{
    "paper_title": "MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation",
    "authors": [
        "Longtao Zheng",
        "Yifan Zhang",
        "Hanzhong Guo",
        "Jiachun Pan",
        "Zhenxiong Tan",
        "Jiahao Lu",
        "Chuanxin Tang",
        "Bo An",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos remain significant challenges. To address these challenges, we propose Memory-guided EMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Our approach is built around two key modules: (1) a memory-guided temporal module, which enhances long-term identity consistency and motion smoothness by developing memory states to store information from a longer past context to guide temporal modeling via linear attention; and (2) an emotion-aware audio module, which replaces traditional cross attention with multi-modal attention to enhance audio-video interaction, while detecting emotions from audio to refine facial expressions via emotion adaptive layer norm. Extensive quantitative and qualitative results demonstrate that MEMO generates more realistic talking videos across diverse image and audio types, outperforming state-of-the-art methods in overall quality, audio-lip synchronization, identity consistency, and expression-emotion alignment."
        },
        {
            "title": "Start",
            "content": "MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation Longtao Zheng2, Yifan Zhang1, Hanzhong Guo, Jiachun Pan1, Zhenxiong Tan3, Jiahao Lu3, Chuanxin Tang1, Bo An1,2, Shuicheng Yan1 1Skywork AI, 2Nanyang Technological University, 3National University of Singapore longtao001@e.ntu.edu.sg, {yifan.zhang7,shuicheng.yan}@kunlun-inc.com Project Page: https://memoavatar.github.io 4 2 0 2 5 ] . [ 1 8 4 4 4 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos remain significant challenges. To address these challenges, we propose Memory-guided EMOtionaware diffusion (MEMO), an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Our approach is built around two key modules: (1) memory-guided temporal module, which enhances long-term identity consistency and motion smoothness by developing memory states to store information from longer past context to guide temporal modeling via linear attention; and (2) an emotion-aware audio module, which replaces traditional cross attention with multimodal attention to enhance audio-video interaction, while detecting emotions from audio to refine facial expressions via emotion adaptive layer norm. Extensive quantitative and qualitative results demonstrate that MEMO generates more realistic talking videos across diverse image and audio types, outperforming state-of-the-art methods in overall quality, audio-lip synchronization, identity consistency, and expression-emotion alignment. 1. Introduction Audio-driven talking video generation [41, 55, 64] has gained significant attention due to its broad impact on areas like virtual avatars, digital content creation, and real-time communication, offering transformative possibilities in entertainment, education, and e-commerce. However, compared to text-to-video generation [17, 42, 44] or image-toWork done during the internship at Skywork AI. Authors contributed equally. Project Lead. Figure 1. Our MEMO generates talking videos with improved identity consistency, audio-lip alignment, and motion smoothness. In contrast, existing diffusion methods (e.g., Hallo2 [12]) are prone to temporal error accumulation during autoregressive generation, especially when the last 2-4 generated frames used as temporal conditions contain artifacts, leading to inconsistent identity. Please refer to the supplementary material for video demos. video generation [3], audio-driven talking video generation presents unique challenges. It requires not only generating synchronized lip movements and realistic head motions from audio, but also preserving long-term identity consistency of the reference image and producing natural expressions that align with the emotional tone of audio. Balancing these demands while ensuring generalization across diverse driving audio and reference images makes this task especially challenging. Recent advances in video diffusion models [9, 12, 55, 63] have enabled more realistic audio-driven talking video generation. Most of these methods use cross attention to incorporate audio to guide video generation, and typically condition on past 2-4 generated frames for autoregressive generation to improve motion smoothness [55, 63]. Additionally, some incorporate single, human-defined emotion label for the whole video to specify the emotion of the generated video [54, 64]. However, these approaches face chal1 lenges with audio-lip synchronization, maintaining longterm identity consistency, and achieving natural expressions aligned with the audio. Specifically, cross attention relies on fixed audio features and limits audio-video interaction, while conditioning on limited number of past frames can lead to temporal error accumulation, especially when those frames contain artifacts (cf. Figure 1). Moreover, using fixed emotion label for the whole video can result in facial expressions that fail to capture the dynamic emotional shifts inherent in audio. As result, these methods struggle with audio-lip synchronization, expression-audio alignment, and long-term identity preservation. In this paper, we propose Memory-guided EMOtionaware diffusion (MEMO), an end-to-end audio-driven portrait animation approach. As shown in Figure 2, MEMO is built around two key modules: (1) memory-guided temporal module and (2) an emotion-aware audio module. To ensure consistent facial identity and smooth transitions across long-duration videos, MEMO develops memory-guided temporal module (cf. Section 4.1) that maintains memory states across longer previously generated frames. This allows the model to use long-term motion information to guide temporal modeling through linear attention, resulting in more coherent facial movements and mitigating the error accumulation issue that may occur in existing diffusion methods (cf. Figure 1). Moreover, to improve audio-lip synchronization and align facial expressions with the audio emotion, MEMO introduces an emotion-aware audio module (cf. Section 4.2). This module replaces the traditional cross-attention audio module in previous diffusion methods with more dynamic multi-modal attention mechanism, enabling better interaction between audio and video during the diffusion process. Meanwhile, by dynamically detecting emotion cues from the audio at the video subsegment level, this module helps to subtly refine facial expressions via the emotion adaptive layer norm, enabling the generation of expressive talking videos. Extensive quantitative results and human evaluations demonstrate that our approach consistently outperforms state-of-the-art methods in overall quality, audio-lip synchronization, expression-audio alignment, identity consistency, and motion smoothness (cf. Table 1 and Figure 5). Additionally, diverse qualitative results highlight MEMOs strong generalization across various types of audio, images, languages, and head poses (cf. Figures 7-10), further showcasing the effectiveness of our method. Lastly, ablation studies further validate the distinct contributions of the memory-guided temporal module (cf. Figure 11), which enhances long-term identity consistency and motion smoothness, and the emotion-aware audio module (cf. Figure 1213), which significantly improves audio-lip alignment and expression naturalness. 2. Related Work Audio-driven talking video generation aims to synthesize realistic and synchronized talking videos given driving audio and reference face image. Early approaches only focused on learning audio-lip mapping while keeping other facial attributes static [8, 10, 41, 53, 65]. These methods, however, cannot capture comprehensive facial expressions and natural head movements. To resolve this, later research used intermediate motion representations (e.g., landmark coordinates, 3D facial mesh, and 3D morphable models) and decomposed the generation process into two stages, i.e., audio to motion and motion to video [9, 51, 58, 60, 67, 72]. However, they often generate inaccurate intermediate representations from audio, which restricts the expressiveness and realism of the resulting videos. Recent end-to-end methods, like EMO [55] and Hallo [63], can generate vivid portrait videos by fine-tuning pre-trained diffusion models [44]. However, they need specific modules (e.g., face locator) to constrain head stability, which makes it impossible for the model to achieve naturally large head motions. Similar issues exist in the methods that learned specific face latent space [12, 18, 33, 64, 66]. In contrast, our work does not depend on any facial inductive biases, which unlocks the possibilities for generating more expressive head motions in talking videos. Moreover, most of these methods use 2-4 past frames [12, 55, 63] as temporal conditions for autoregressive generation of long videos. However, such limited frame history can result in error accumulation over time when artifacts appear in the past 2-4 frames. Very recently, Loopy [24] increased the number of past frames to reduce this dependence, and used temporal segment module to model cross-clip relationships. However, it still uses limited past frames, whereas our proposed memory-guided linear attention module allows utilizing possibly all past frames to provide more comprehensive temporal guidance, thus mitigating error accumulation and enhancing long-term identity consistency. Besides, unlike previous diffusion-based methods [12, 55, 63] that used cross-attention mechanism to integrate audio features, our method enhances the audio-lip synchronization and expression-audio alignment based on newly developed emotion-aware multi-modal diffusion. More related studies of diffusion models are provided in Appendix A. 3. Problem and Preliminaries Problem statement. Given reference image and audio as inputs, audio-driven talking video generation [41, 55] aims to output vivid video that closely aligns with the input audio and authentically replicates real human speech and facial movements. This task is challenging because it requires seamless audio-lip synchronization, realistic head movements, long-term identity consistency, and natural ex2 Figure 2. Overview of MEMO, which is structured with Reference Net and Diffusion Net. The core innovations of MEMO reside in two key modules within the Diffusion Net: the memory-guided temporal module and the emotion-aware audio module. These modules work in tandem to deliver enhanced audio-video synchronization, sustained identity consistency, and more natural expression generation. pressions that align with audio. Most existing diffusionbased approaches [9, 55, 63] struggle with issues such as error accumulation, inconsistent identity preservation over time, limited audio-lip synchronization, unnatural expressions, and poor generalization. Latent diffusion models and rectified flow loss. Our method is built upon the Latent Diffusion Model (LDM) [44], framework designed to efficiently learn generative processes in lower-dimensional latent space rather than directly operating on pixel space. During training, LDM first employs pre-trained encoder E() to map high-dimensional images into compressed latent space, producing latent features z0 = E(I). Then, following the principles of Denoising Diffusion Probabilistic Models (DDPM) [21], Gaussian noise ϵ is progressively added to the latent features over discrete timesteps, resulting in noisy latent features zt = 1 αtϵ, where αt is variance schedule controlling how much noise is added. The diffusion model is then trained to reverse this noise-adding process by taking the noisy latent representation zt as input and predicting the added noise ϵ. The objective function for training can be expressed as: = Ezt,c,ϵN (0,1),t[ϵ ϵθ(zt, t, c)2 2], where ϵθ represents the noise prediction made by the U-Net network, and represents conditioning information such as audio, or motion frames in the context of talking video generation. αtz0 + Recently, Stable Diffusion 3 (SD3) [15] refines this process by incorporating rectified flow loss [31], which modifies the traditional DDPM objective to: = Ezt,c,ϵN (0,1),t[λ(t)ϵ ϵθ(zt, t, c)2 2], where λ(t) = 1/(1 t)2 and zt is reparameterized using linear combination as zt = (1 t)z0 + tϵ. This formulation leads to both better training stability and more efficient inference. In light of these advantages, we adopt the rectified flow loss from SD3 in our training. (1) 3 4. Method Similar As illustrated in Figure 2, MEMO is an end-to-end audiodriven diffusion model for generating identity-consistent to previous and expressive talking videos. diffusion-based approaches [55, 63], MEMO is built around two main components: Reference Net and Diffusion Net. The main contributions of MEMO lie in two key modules within the Diffusion Net: the memory-guided temporal module (cf. Section 4.1), and the emotion-aware audio module (cf. Section 4.2), which work together to achieve superior audio-video synchronization, long-term identity In addiconsistency, and natural expression generation. tion, MEMO introduces new data processing pipeline (cf. Section 4.3) for acquiring high-quality talking head videos, along with decomposed training strategy (cf. Section 4.4) to optimize diffusion model training. 4.1. Memory-Guided Temporal Module Most existing diffusion methods [9, 55, 63] generate talking videos in an autoregressive manner by segmenting the audio into clips of 12-16 frames and using the past 2-4 generated frames to condition the generation of the next video clip. They concatenate the past frame features with the current noisy latent features along the temporal dimension and apply temporal self-attention to model sequential information. While this approach can model short-term dependencies, it often struggles with maintaining consistency over longer sequences, even with the use of Reference Net. If artifacts are introduced in the past 2-4 conditioned frames, these errors tend to accumulate as generation progresses, leading to visual distortions that degrade both video quality and identity consistency (e.g., Hallo2 [12] in Figure 1). Motivated by the idea that leveraging more complete memory of motion information, rather than relying solely on the most recent 2-4 frames, can provide richer guidance occupy constant GPU memory irrespective of . Here, γ is decay factor (0 < γ < 1) that modulates the influence of past frames, with more recent frames exerting greater impact, reflected through the exponentiation by i. After each generation of frames, we update the memory by incorporating information from these newly generated frames. In formal, the memory update when adding the latest frames to the memory with past frames is: a+b a+b KV γaM γaM KV + (cid:80)a + (cid:80)a j=1 γjϕ(Kh,j)V h,j, j=1 γjϕ(Kh,j). (3) (4) Figure 3. Memory-guided temporal module. for enhancing identity consistency and motion smoothness, we propose memory-guided temporal module. The key of this module is memory-guided linear attention, which is designed to improve temporal coherence and maintain consistent facial identity. Linear attention for temporal modeling. Previous approaches use self-attention [24, 55] to capture temporal information across frames. However, self-attention requires storing all key-value pairs, leading to increasing GPU memory overhead as the number of past frames grows, making it impractical to use longer motion information. To address this limitation, we replace self-attention with linear attention [26] and include memory update mechanism into linear attention to model long-term temporal information efficiently. Denoting query as Q, key as K, and value as , the output of linear attention for i-th frame is computed as: outi = ϕ(Qi)(cid:0) (cid:80)f ϕ(Qi) (cid:80)f j=1 ϕ(Kj)V j=1 ϕ(Kj) (cid:1) , (2) where is the frame number and ϕ is an activation function (we use softmax in this work). Memory update mechanism with history decay. To incorporate motion information from longer past context to guide video generation, we develop memory update mechanism. Specifically, let the latent features of past frames as Rf and the latent features of current frames as Rf d, where is the dimension of latent features. As shown in Figure 3, linear attention processes these latent features via learnable matrices, which transform them into queries (Qh), keys (Kh, Km), and values (Vh, Vm). ory states for the past frames as two matrices: (cid:80)f To memorize motion information, we define the memKV = i=1 γiϕ(Km,i), which i=1 γiϕ(Km,i)V m,i and K = (cid:80)f Here, the decay scheme is critical, as unified positional encoding across different video subsegments is infeasible. Instead, we use causal memory decay to provide implicit positional encoding, which enables more effective memory updates to capture long-term dependencies. During training, we set the temporal context to 16 past frames; at inference, this memory decay scheme allows the model to naturally extend memory updates to longer temporal contexts. Memory-guided linear attention. When generating the current video clip, we use the memory to guide temporal modeling. Let HKV = ϕ(Kh)V and HK = ϕ(Kh). The output of the memory-guided temporal module is: out = ϕ(Qh) (HKV + MKV ) ϕ(Qh) (HK + MK) . (5) This enables the model to leverage an extended memory of possibly all past frames, offering comprehensive motion information beyond just the most recent 2-4 generated frames. By drawing on this richer context, the model mitigates temporal error accumulation from recent artifacts, thus enhancing temporal coherence and maintaining identity consistency in talking video generation. 4.2. Emotion-Aware Audio Module Most existing diffusion-based approaches [9, 55, 63] use cross-attention mechanisms to incorporate audio guidance in video generation, while others [54, 64] apply single, human-defined emotion label to generate emotionally talking videos during inference. However, cross attention relies on fixed audio features, limiting the depth of audio-video interaction during the diffusion process. Meanwhile, using fixed, human-defined emotion label for the whole video fails to capture the dynamic emotional shifts in audio, resulting in facial expressions that do not naturally align with the audio emotion. To address these issues, we develop new emotion-aware audio module to improve audio-lip consistency and align facial expressions with the audio emotion. As shown in Figure 4, there are three key strategies: multi-modal attention, audio emotion-aware diffusion, and emotion decoupling training. 4 where is the classifier-free guidance scale to control the influence of the emotion condition. Please note that the overall emotional tone of the generated talking video is largely inferred from the facial expression of the reference image. Here, the audio emotion aims to function mainly as subtle adjustment to enhance or moderately alter the emotion when prompted by the audio. It is not intended to induce complete emotional shift that would override the facial expression conveyed by the reference image. Emotion decoupling training. To further improve the effect of audio emotion on talking videos, we introduce an emotion decoupling training strategy that separates the expression in the reference image from the audio emotion. Specifically, for training video clips sourced from MEAD [59]which provides both speaker identity and emotion labelswe avoid using reference image from the same video clip. Instead, we randomly select reference image of the same person but with different emotion. This encourages better disentanglement between the reference images expression and the audio-induced emotion, allowing our emotion-aware audio module to better refine facial expressions in alignment with the audio. Moreover, our method also supports replacing the detected audio emotion label with manually specified emotion label, if desired. 4.3. Data Processing Pipeline We collect comprehensive set of open-source datasets, such as HDTF [69], VFHQ [61], CelebV-HQ [73], MultiTalk [52], and MEAD [59], along with additional data collected by ourselves. The total duration of these raw videos exceeds 2,200 hours. However, as illustrated in Appendix D, we find that the overall quality of the data is poor, with numerous issues such as audio-lip misalignment, missing heads, multiple heads, occluded faces by subtitles, extremely small face regions, and low resolution. Directly using these data for model training results in unstable training, poor convergence, and terrible generation quality. To further obtain high-quality talking head data, we developed dedicated data processing pipeline for talking head generation. The pipeline consists of five steps: First, we perform scene transition detection and trim video clips to length of less than 30 seconds. Second, we apply face detection, filtering out videos with no faces, partial faces, or multiple heads, and use the resulting bounding boxes to extract talking heads. Third, we use an Image Quality Assessment model [49] to filter out low-quality and low-resolution videos. Fourth, we apply SyncNet [41] to remove videos with audio-lip synchronization issues. Lastly, we manually assess the audio-lip synchronization and overall video quality for subset of the data to ensure more accurate filtering. After completing the entire pipeline, the total duration of our processed high-quality videos is about 660 hours. Figure 4. Emotion-aware audio module. Multi-modal attention. Our emotion-aware audio module replaces the traditional cross attention with more dynamic multi-modal attention mechanism. Specifically, cross attention aligns video and audio by conditioning the process of video features on audio features a. This approach can be formalized as minimizing the loss function Lθva = Et,ϵN (0,I)[λ(t)ϵθ(vta) ϵ2 2]. In contrast, as shown in Figure 4, we explore multi-modal attention, which jointly processes both video and audio inputs by minimizing the loss function Lθva = Et,ϵN (0,I)[λ(t)ϵθ(vt, a) ϵ2 2]. Such mechanism enables better video-audio interaction during the diffusion process. Audio emotion-aware diffusion. We then dynamically detect audio emotions to guide audio-video interaction via newly trained emotion detection model. Specifically, the model is trained on diverse dataset to extract emotion from audio (See Appendix for more details), recognizing eight distinct emotions: angry, disgusted, fearful, happy, neutral, sad, surprised, and others. To improve the robustness of the detected emotion labels, emotion detection is conducted at the audio subsegment level. Each subsegments emotion is determined by the most frequently detected emotion across all its frames, where each frames emotion is evaluated using audio features from 3-second sliding window centered on that frame. The detected emotion of each subsegment is then projected into emotion embeddings, and integrated into each layer via emotion-adaptive layer normalization (cf. Figure 4) to guide multi-modal attention. This process results in the following emotion-conditioned flow loss: Lθvae = Et,ϵN (0,I)[λ(t)ϵθ(vt, ae) ϵ2 2]. (6) During inference, we use classifier-free guidance [20] to control the impact of the dynamically detected emotion on the generated output. The emotion-aware output is ϵθ(vt, ae) = (1 + w)ϵθ(vt, ae) wϵθ(vt, a), (7) 5 Table 1. Quantitative results of video quality and audio-lip synchronization on two OOD test datasets. MEMO consistently outperforms existing talking video baselines. Method VoxCeleb2 test set Collected OOD dataset FVD FID Sync-D FVD FID Sync-D SadTalker [67] AniPortrait [60] V-Express [58] Hallo [63] Hallo2 [12] EchoMimic [9] 397.0 333.2 418.9 330.4 302.0 293.9 MEMO (Ours) 254.3 71.7 45.5 58.9 41.6 41.6 43.8 31.7 8.6 11.0 8.2 8.0 8.0 10.1 7.4 288.7 238.7 315.2 231.1 223.1 223. 161.1 48.3 31.2 46.7 31.9 29.8 39.9 24.9 10.6 10.5 9.5 9.3 9.3 9.8 9.2 4.4. Training Strategy Decomposition The training of MEMO is divided into two progressive stages, each with specific objectives. Stage 1: Face domain adaptation. Following [9, 55, 63], we initialize Reference Net and the spatial module of Diffusion Net with the weights of SD 1.5 [44]. In this stage, we adapt Reference Net, the spatial attention modules of Diffusion Net, and the original text cross-attention module to the face domain with the rectified flow loss (cf. Eq. 1), ensuring these components capture facial features effectively. Stage 2: Emotion-decoupled robust training. We then integrate the emotion-aware audio module and memoryguided temporal module into the Diffusion Net. Initially, we perform warm-up training phase for the newly added modules, keeping the modules in Stage 1 fixed. After the warm-up, all modules are jointly trained. In this stage, we use the emotion-conditioned flow loss (cf. Eq. 6) and scale up the dataset to include all processed data for more comprehensive training. Here, we adopt the emotion decoupling training strategy (cf. Section 4.2) only when the training video clips are sourced from MEAD. Moreover, we found that some noisy data persisted even after applying our data processing pipeline (cf. Section 4.3), making diffusion training unstable and leading to biased model optimization. To mitigate this, we further develop robust training strategy that filters out data points with loss values suddenly exceeding specific large value (0.1 in our case), as the emotion-conditioned flow loss in our method typically converges and fluctuates around 0.03. 5. Experiments 5.1. Experimental Setup Evaluation benchmarks. We create two out-ofdistribution (OOD) datasets to evaluate MEMOs performance and generalization capabilities. For the first OOD dataset, We sample 150 video clips from the VoxCeleb2 [37] test set, which contains videos of various celebrities. Similarly, we create the second OOD dataset with 150 clips across more diverse set of audio, backgrounds, ages, genders, and languages. Figure 5. Human preferences among MEMO and baselines, where users select the best method in terms of each evaluation metric. Evaluation metrics. We adopt suite of metrics to evaluate the overall quality and audio-lip synchronization of the generated videos. The Fréchet Video Distance (FVD) [56] measures the distance between the distributions of real and generated videos, providing an assessment of overall video quality. The Fréchet Inception Distance (FID) [19] evaluates the quality of individual frames by comparing feature distributions extracted from pre-trained model. SyncNet Distance (Sync-D) [11] measures audio-lip synchronization using pre-trained discriminator model. Baselines. We compare our method against several state-ofthe-art baselines with publicly available model checkpoints. The baselines include both two-stage methods with intermediate representations and end-to-end diffusion methods. VExpress [58], AniPortrait [60] and EchoMimic [9] are twostage methods using intermediate representations like landmarks, while Hallo [63] and Hallo2 [12] are recent end-toend diffusion-based models. More implementation details of MEMO are in Appendix B. 5.2. Quantitative Results Performance on two OOD test sets. Table 1 reports the quantitative results on two OOD test sets. Our method consistently outperforms all baselines in terms of FVD, FID, and Sync-D metrics, indicating better video quality and audio-lip synchronization. These results also demonstrate the improved generalization abilities of MEMO to unseen identities and audio. Human evaluation. To better benchmark the quality of generated talking videos, we conduct human studies based on five subjective metrics in several challenging scenarios, e.g., singing, rap, and multi-lingual talking video generation. Specifically, our analyses are based on the overall quality, motion smoothness, expression-audio alignment, audio-lip synchronization, and identity consistency. As shown in Figure 5, our method achieves the highest scores across all criteria in human evaluations, much higher than compared methods. This further demonstrates the effectiveness of our approach. 6 Figure 6. MEMO can generate talking videos featuring wider range of smooth head movements and more emotional facial expressions, illustrated in both visualization and heatmaps. Please refer to the supplementary material for video demos. Figure 7. The generated videos with various types of driving audio. Please refer to the supplementary material for video demos. Figure 8. The generated videos with various types of reference images. Please refer to the supplementary for video demos. 5.3. Qualitative Results Diversity of head motions. Figure 6 shows that MEMO can generate talking videos with higher diversity in head motions, compared to existing methods. The improved motion diversity contributes to better naturalness and expressiveness of talking videos. Generalization to different types of audio. We evaluate MEMO on various audio types, including speeches, songs, and raps. In Figure 7, MEMO consistently generates synchronized lip movements across diverse audio types. It performs well with both expressive songs, which demand nuanced emotional alignment, and raps, which require rapid audio-lip synchronization. This verifies MEMOs strong generalization to various types of driving audio. Generalization to different styles of reference images. Figure 8 shows MEMOs performance on challenging reference images of diverse styles, such as portraits, sculpture, and digital art images. Despite these styles deviating significantly from our training data, MEMO maintains robust generation quality without producing noticeable artifacts, demonstrating its ability to generalize to various OOD reference images. Generalization to multilingual audio. As shown in Figure 9, our method demonstrates robust generalization across multilingual audio inputs, such as English, Chinese, Spanish, Japanese, and Korean. Despite most of our training data being in English, our approach effectively generates lip movements synchronized with the given multilingual audio while capturing rich, realistic facial expressions. Generalization to different head poses. Figure 10 shows the generated videos of MEMO with reference images of varying head poses, including frontal views and multiple side angles. This demonstrates that our method can generate realistic talking videos across different angles while maintaining consistency in facial appearance and expression. 5.4. Ablation Studies To more comprehensively evaluate the effects of our methods components, we conduct ablation studies based on human evaluation and qualitative analysis. Effects of memory module. We evaluate the impacts of our memory module by ablating the length of past frames as temporal guidance. As shown in Figure 11, longer memory significantly improves temporal coherence, overall quality, motion smoothness, identity consistency, and audiolip alignment, while short motion frames lead to worse performance. This demonstrates the effectiveness of our memory-guided temporal module and also explains why our method can alleviate temporal error accumulation in Figure 1, whereas Hallo2 is prone to error accumulation. Figure 9. The generated videos on driving audio with different languages. See the supplementary for video demos. Figure 10. The generated videos with reference images of different head poses. See the supplementary for video demos. Figure 11. Ablation on the number of past frames (f ) during inference via human evaluation, where 16+f indicates our memory-guided inference with context beyond 16 frames. Figure 12. Human preference comparison between multimodal attention and cross attention for integrating audio conditions in the audio module. Effects of multi-modal attention. We further investigate the impact of the multi-modal attention through human evaluations. Results in Figure 12 underscore the effectiveness of multi-modal attention over cross attention in terms of the overall video quality and audio-lip alignments. Effects of emotion-aware scheme. To investigate the effects of our emotion-aware scheme (i.e. emotion-aware diffusion and emotion decoupling training), we conduct an ablation study by manually replacing the detected audio emotion with fixed, human-defined emotion label. In Figure 13, using the same reference image and different emotion labels, we compare the frames generated by our method with and without emotion decoupling training at the same audio moment. The results demonstrate that our emotionaware module with emotion decoupling training can effectively refine facial expressions to align with the specified emotion labels. This finding suggests that, with our detected emotion labels, MEMO can generate facial expressions that match the audio emotion. Moreover, this comparison also validates the necessity of our emotion decoupling training. 6. Conclusion This work has presented MEMO, state-of-the-art talking video generation method. Specifically, MEMO effectively alleviates temporal error accumulation and enhances longFigure 13. Ablation of our emotion-aware module with or without emotion decoupled training, given various human-defined emotion labels. Refer to the supplementary for video demos. term identity consistency via new memory-guided temporal module, while generating videos with high audio-lip and expression-audio alignment via new emotion-aware audio module. Moreover, MEMO does not need face-related inductive biases in the model architecture, allowing it to be extended to broader applications, such as talking body generation tasks. In the future, it is interesting to explore Diffusion Transformer [39] with better identity preservation strategies for talking video generation."
        },
        {
            "title": "References",
            "content": "[1] Adaeze Adigwe, Noé Tits, Kevin El Haddad, Sarah Ostadabbas, and Thierry Dutoit. The emotional voices database: Towards controlling the emotion dimension in voice generation systems. arXiv preprint arXiv:1806.09514, 2018. 12 [2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised In Advances in Neural learning of speech representations. Information Processing Systems, pages 1244912460, 2020. 12 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [4] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The MTG-Jamendo dataset for automatic music tagging. In International Conference on Machine Learning Workshop, 2019. 12, 13 [5] Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter Sendlmeier, Benjamin Weiss, et al. database of German emotional speech. In Interspeech, 2005. 12 [6] Salih Firat Canpolat, Zuhal Ormanoglu, and Deniz Zeyrek. In LREC Turkish emotion voice database (TurEV-DB). Workshop Language Resources and Evaluation Conferenc, pages 368375, 2020. 12 [7] Fabio Catania. Speech emotion recognition in Italian using wav2vec 2. Authorea Preprints, 2023. 12 [8] Lele Chen, Zhiheng Li, Ross Maddox, Zhiyao Duan, and Chenliang Xu. Lip movements generation at glance. In European Conference on Computer Vision, pages 520535, 2018. 2 [9] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. arXiv preprint arXiv:2407.08136, 2024. 1, 2, 3, 4, 6 [10] Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, and Nannan Wang. VideoRetalking: Audio-based lip synchronization for talking head video editing in the wild. In SIGGRAPH Asia, pages 19, 2022. 2 [11] Joon Son Chung and Andrew Zisserman. Out of time: Automated lip sync in the wild. In Asian Conference on Computer Vision Workshops, pages 251263. Springer, 2017. 6 [12] Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, and Jingdong Wang. Hallo2: Long-duration and high-resolution audio-driven portrait image animation. arXiv preprint arXiv:2410.07718, 2024. 1, 2, 3, 6 [13] Kate Dupuis and Kathleen Pichora-Fuller. Toronto emotional speech set (tess)-younger talker_happy. 2010. 12 [14] Mathilde Duville, Luz Alonso-Valerdi, and David Ibarra-Zarate. The Mexican emotional speech database (mesd): Elaboration and assessment based on machine learning. In Annual International Conference of the IEEE Engineering in Medicine & Biology Society, pages 16441647, 2021. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. 3 [16] Philippe Gournay, Olivier Lahaie, and Roch Lefebvre. Canadian French emotional speech dataset. In ACM Multimedia Systems Conference, pages 399402, 2018. 12 [17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-toimage diffusion models without specific tuning. In International Conference on Learning Representations, 2023. 1, 12 [18] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, et al. GAIA: Zero-shot talking avatar generation. In International Conference on Learning Representations, 2023. 2, 12 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. In Advances in Neural Information Processing Systems, 2017. 6 [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, pages 68406851, 2020. 3, 12 [22] Philip Jackson and SJUoSG Haq. Surrey audio-visual expressed emotion (savee) database. University of Surrey: Guildford, UK, 2014. 12 [23] Jesin James, Li Tian, and Catherine Watson. An open source emotional speech corpus for human robot interaction applications. In Interspeech, 2018. 12 [24] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. 2, 4 [25] Dorota Kaminska, Tomasz Sapinski, and Adam Pelikant. In Proceedings Polish emotional natural speech database. of the Conference: Signal Processing Symposium, 2015. 12 [26] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 51565165. PMLR, 2020. [27] Leila KERKENI, Catherine CLEDER, Youssef Serrestou, and Raood. French emotional speech database-oréau, 2020. 12 [28] Dejoli Landry, Qianhua He, Haikang Yan, and Yanxiong Li. ASVP-ESD: dataset and its benchmark for emotion recognition using both speech and non-speech utterances. Global Scientific Journals, 8:17931798, 2020. 12, 13 [29] Siddique Latif, Adnan Qayyum, Muhammad Usman, and Junaid Qadir. Cross lingual speech emotion recognition: Urdu vs. Western languages. In International Conference on Frontiers of Information Technology, pages 8893, 2018. 12, 13 9 [30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 14 [31] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023. 3 [32] Steven Livingstone and Frank Russo. The ryerson audio-visual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english. PloS One, 13(5): e0196391, 2018. 12 [33] Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng. Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767, 2023. 2 [34] Ziyang Ma, Mingjie Chen, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, Xiquan Li, Jiaxin Ye, Xie Chen, and Thomas Hain. EmoBox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark. In Interspeech, 2024. 12 [35] Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, and Xie Chen. emotion2vec: Selfsupervised pre-training for speech emotion representation. In Findings of the Association for Computational Linguistics, pages 1574715760, 2024. 13 [36] Olivier Martin, Irene Kotsia, Benoit Macq, and Ioannis Pitas. The enterface05 audio-visual emotion database. In International Conference on Data Engineering Workshops, pages 88, 2006. [37] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. Voxceleb: Large-scale speaker verification in the wild. Computer Speech & Language, 60:101027, 2020. 6 [38] Kari Ali Noriy, Xiaosong Yang, and Jian Jun Zhang. Emns/imz/corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels. arXiv preprint arXiv:2305.13137, 2023. 12 [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In International Conference on Computer Vision, pages 41954205, 2023. 8 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations, 2023. 12 [41] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In ACM International Conference on Multimedia, pages 484492, 2020. 1, 2, 5, 12, 14 [42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [43] Ephrem Afele Retta, Eiad Almekhlafi, Richard Sutcliffe, Mustafa Mhamed, Haider Ali, and Jun Feng. new amharic speech emotion dataset and classification benchmark. ACM Transactions on Asian and Low-Resource Language Information Processing, 22(1):122, 2023. 12 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Computer Vision and Pattern Recognition, pages 1068410695, 2022. 1, 2, 3, 6, 12 [45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International Confernonequilibrium thermodynamics. ence on Machine Learning, pages 22562265. PMLR, 2015. 12 [46] Tomás Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In ACM International Conference on Multimedia, pages 1121811221, 2024. 13 [47] Ingmar Steiner, Marc Schröder, and Annette Klepp. The PAVOQUE corpus as resource for analysis and synthesis of expressive speech. Proceedings of Phonetik & Phonologie, 9, 2013. [48] Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads: Diffusion models beat gans on talking-face generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 50915100, 2024. 12 [49] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by self-adaptive hyper network. In Computer Vision and Pattern Recognition, 2020. 5, 14 [50] Sadia Sultana, Shahidur Rahman, Reza Selim, and Sust bangla emotional speech corpus Zafar Iqbal. (subesco): An audio-only emotional speech corpus for bangla. Plos one, 16(4):e0250173, 2021. 12 [51] Xusen Sun, Longhao Zhang, Hao Zhu, Peng Zhang, Bang Zhang, Xinya Ji, Kangneng Zhou, Daiheng Gao, Liefeng Bo, and Xun Cao. Vividtalk: One-shot audio-driven talking head generation based on 3D hybrid prior. arXiv preprint arXiv:2312.01841, 2023. 2 [52] Kim Sung-Bin, Lee Chae-Yeon, Gihun Son, Oh Hyun-Bin, Janghoon Ju, Suekyeong Nam, and Tae-Hyun Oh. MultiTalk: Enhancing 3D talking head generation across lanarXiv preprint guages with multilingual video dataset. arXiv:2406.14272, 2024. 5, 13 [53] Supasorn Suwajanakorn, and Ira Kemelmacher-Shlizerman. Synthesizing obama: Learning lip sync from audio. ACM Transactions on Graphics, 36(4): 113, 2017. Steven Seitz, [54] Shuai Tan, Bin Ji, and Ye Pan. Flowvqtalker: High-quality emotional talking face generation through normalizing flow and quantization. In Computer Vision and Pattern Recognition, pages 2631726327, 2024. 1, 4 [55] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. EMO: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, 2024. 1, 2, 3, 4, 6, 12 10 In Advisual models via contrast-regularized fine-tuning. vances in Neural Information Processing Systems, 2021. 13 [69] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with highresolution audio-visual dataset. In Computer Vision and Pattern Recognition, pages 36613670, 2021. 5, 13 [70] Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen Liu, Qin Jin, Xinchao Wang, and Haizhou Li. M3ed: Multimodal multi-scene multi-label emotional dialogue database. In Association for Computational Linguistics, pages 5699 5710, 2022. [71] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Seen and unseen emotional style transfer for voice conversion with new emotional speech dataset. In International Conference on Acoustics, Speech and Signal Processing, pages 920924. IEEE, 2021. 12 [72] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk: Speaker-aware talking-head animation. ACM Transactions On Graphics, 39(6):115, 2020. 2, 12 [73] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebVIn EuHQ: large-scale video facial attributes dataset. ropean Conference on Computer Vision, pages 650667. Springer, 2022. 5, 13 [56] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new metric for video generation. 2019. 6 [57] Nikolaos Vryzas, Rigas Kotsakis, Aikaterini Liatsou, Charalampos Dimoulas, and George Kalliris. Speech emotion recognition for performance interaction. Journal of the Audio Engineering Society, 66(6):457467, 2018. 12 [58] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-Express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. 2, [59] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. MEAD: large-scale audio-visual dataset for emotional talking-face generation. In European Conference on Computer Vision, pages 700717. Springer, 2020. 5, 12, 13 [60] Huawei Wei, Zejun Yang, and Zhisheng Wang. AniPortrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. 2, 6, 12, 15 [61] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. VFHQ: high-quality dataset and benchIn Computer Vision mark for video face super-resolution. and Pattern Recognition, pages 657666, 2022. 5, 13 [62] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. DynamiCrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 12 [63] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 1, 2, 3, 4, 6, 12, 15 [64] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. VASA-1: Lifelike audio-driven talking faces generated in real time. In Advances in Neural Information Processing Systems, 2024. 1, 2, 4, 12 [65] Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, and Yujiu Yang. StyleHeat: One-shot high-resolution editable talking face generation via pre-trained stylegan. In European Conference on Computer Vision, pages 85101. Springer, 2022. 2 [66] Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, and Jiashi Feng. Dream-talk: Diffusion-based realistic emotional audio-driven method for single image talking face generation. arXiv preprint arXiv:2312.13578, 2023. 2 [67] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. SadTalker: Learning realistic 3D motion coefficients for stylized audiodriven single image talking face animation. In Computer Vision and Pattern Recognition, pages 86528661, 2023. 2, [68] Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, and Jiashi Feng. Unleashing the power of contrastive self-supervised 11 A. More Related Studies on Diffusion Models Diffusion models [21, 45] are highly expressive generative models, demonstrating remarkable capabilities in image synthesis [40, 44] and video generation [17, 62]. Stable Diffusion [44] employs UNet architecture and generates high-resolution images in the latent space, which is extended to video domains by AnimateDiff [17] by adding temporal attention layers. These models generate images or videos based on text prompts, where the text guidance from the pre-trained text encoder is introduced through cross attention. In the domain of talking video generation, diffusion models also show promising results in generation quality [18, 48, 55, 60, 63, 64], outperforming previous GANbased methods [41, 72]. Instead of using text prompts, most of these diffusion-based methods condition diffusion models on image and audio embeddings extracted from pretrained image encoder and audio encoder, respectively. B. More Implementation Details Both Reference Net and the spatial module of Diffusion Net are initialized with the weights of SD 1.5 [44]. The temporal module of Diffusion Net is initialized from AnimateDiff [17]. Here, the Reference Net provides identity information from reference images for spatial modeling of the Diffusion Net and offers temporal information from past frames for temporal modeling of the Diffusion Net. For both Reference Net and Diffusion Net, we replace the text cross-attention with image cross-attention. We add two projection modules to convert the audio embedding and image embedding into the dimensions required by our attention module. The audio embedding consists of all the hidden states from the Wav2Vec 2.0 model [2]. The training videos are center-cropped and resized to resolution of 512 512 pixels. With fixed learning rate of 1e-5, we train MEMO for 15k and 600k steps at training stages 1 and 2, respectively. During Stage 2, fixed number of 16 past frames is used to compute memory states as motion context. Alternatively, the number of past frames can be dynamically chosen from 16, 32, or 48 for training, enabling the model to handle longer context scenarios more effectively. Nevertheless, thanks to the memory update mechanism with causal history decay (cf. Section 4.1), such dynamic past-frame training is unnecessary. Moreover, emotion embeddings, reference images, audio embeddings, and past frames are randomly dropped with probability of 5% for classifier-free inference. At inference, we set the frame rate to 30 frames per second (FPS) and employ autoregressive generation, producing 16 frames per iteration. The classifier-free guidance scale is set to 3.5. Table 2. Statistics of the emotion detection Dataset. The source column represents the origin of the samples, and the language column specifies the datasets language. #Emo indicates the number of emotion categories, #Utts shows the total number of utterances, and #Hrs represents the total hours of training data Speech Emotion Recognition datasets Dataset Source Language #Emo #Utts #Hrs AESDD [57] ASED [43] ASVP-ESD [28] CaFE [16] EMNS [38] EmoDB [5] EmoV-DB [1] Emozionalmente [7] eNTERFACE [36] ESD [71] JL-Corpus [23] M3ED [70] MEAD [59] MESD [14] Oreau [27] PAVOQUE [47] Polish [25] RAVDESS [32] SAVEE [22] SUBESCO [50] TESS [13] TurEV-DB [6] URDU [29] Act Act Media Act Act Act Act Act Act Act Act TV Act Act Act Act Act Act Act Act Act Act Talk show Urdu Greek Amharic Mix French English German English Italian English Mix English Mandarin English Spanish French German Polish English English Bangla English Turkish 5 5 12 7 8 7 5 7 6 5 5 7 8 6 7 5 3 8 7 7 7 4 4 604 2,474 13,964 936 1,181 535 6,887 6,902 1,263 35,000 2,400 24,437 31,729 862 434 7,334 450 1,440 480 7,000 2,800 1,735 400 0.7 2.1 18.0 1.2 1.9 0.4 9.5 6.3 1.1 29.1 1.4 9.8 37.3 0.2 0.3 12.2 0.1 1.5 0.5 7.8 1.6 0.5 0.3 Music Emotion Recognition Datasets Dataset Source Lang Emo #Utts #Hrs RAVDESS-Song [32] Act MTG-Jamendo [4] Media English Mix 6 56 1,012 5,022 1.31 299.47 C. Audio Emotion Detection To improve the natural expression of talking videos, we develop an emotion detection model to detect emotion labels from audio. In this appendix, we first introduce the data collection and processing strategies for audio emotion recognition, followed by the details of our emotion detector. C.1. Dataset Collection and Processing Dataset collection. To achieve robust emotion detection across both speech and music audio sources, we collect large-scale dataset encompassing both speech and music segments, each annotated with emotion labels. detailed overview of the datasets used in our training process is provided in Table 2. For speech audio, we collect data from recent Speech Emotion Recognition benchmark, EmoBox [34], which incorporates 23 datasets from various origins, covering 12 distinct languages. Regarding music audio, we gather data from the RAVDESS-song [32] and MTG-Jamendo [4] datasets, including songs with and without background music. All data underwent standardized processing protocol, converted to monophonic format with sampling rate of 16,000 Hz. Each utterance is uniquely annotated with 12 an emotion label. For datasets containing lengthy samples, such as MTG-Jamendo, we divide them into shorter segments of 30 seconds to align with the typically shorter length of other datasets, assigning the same label to all segments. Each dataset was then split into training and testing sets with ratio of 3:1. Label merging. major challenge in integrating different datasets is aligning their label spaces, as each dataset often features distinct emotion categories. For instance, the URDU dataset [29] contains only four emotion labels: happy, sad, angry, and neutral. In contrast, the ASVP-ESD dataset [28] includes 12 emotion labels, covering less common emotions such as boredom and pain. For music emotion recognition datasets like MTG-Jamendo [4], there are 56 mood/theme tags, not all of which correspond to emotional labels, and each sample can be assigned multiple tags. These discrepancies and overlaps in category spaces across different datasets present significant challenges for emotion detection. To establish generalized and streamlined label space, we designed our module to perform an 8-class classification task, selecting labels that are both commonly recognized and easily distinguishable: angry, disgusted, fearful, happy, neutral, sad, surprised, and others. We meticulously reviewed and mapped the original labels from each dataset to fit within this new label space. For instance, samples labeled as pleasure in the ASVPESD dataset were mapped to the happy category due to their semantic similarity. Labels that did not clearly correspond to specific emotion were categorized under the others label. C.2. Audio Emotion Detector We implemented an 8-way classifier for our task, drawing inspiration from state-of-the-art methods in speech and music emotion detection. Our solution builds upon Emotion2vec [35], robust universal speech emotion representation model. The feature extractor employs multiple convolutional layers and Transformer blocks and is trained using teacher-student online distillation self-supervised learning approach. The feature extractor backbone of Emotion2vec is pre-trained on large-scale multilingual speech corpus. For our classification task, we use the fixed Emotion2vec backbone as the feature extractor and train 5-layer MLP as the classification head. To stabilize the training process, we apply gradient clipping, constraining the gradient updates within an l2 norm of 1.0. To enhance the models generalization ability, we incorporate contrastive learning technique [68]. The test accuracy for each dataset, as well as the overall accuracy, is reported in Table 3. We compare with the original Emotion2vec [35] as the baseline, where it adopted single linTable 3. Accuracy comparison of audio emotion detection between Emotion2vec [35] and our learned emotion detector. Dataset Emotion2vec Ours AESDD ASED ASVP-ESD CaFE EMNS EmoDB EmoV-DB Emozionalmente eNTERFACE ESD JL-Corpus M3ED MEAD MESD Oreau PAVOQUE Polish RAVDESS SAVEE SUBESCO TESS TurEV-DB URDU RAVDESS-Song MTG-Jamendo Total 75.84 86.20 52.55 73.30 57.98 88.41 77.84 66.61 28.21 94.83 71.92 42.59 61.74 40.65 50.96 85.15 44.89 82.36 83.33 78.43 76.29 47.45 54.00 43.58 65.30 68. 78.52 85.23 55.99 100.00 61.87 100.0 91.22 71.02 32.05 99.94 100.00 41.52 71.45 41.12 42.31 92.74 100.00 100.00 100.00 100.00 95.14 53.47 56.00 100.00 74.50 78.26 ear layer after the feature extraction backbone for downstream emotion detection. D. Data Processing Pipeline We collect comprehensive set of open-source datasets, such as HDTF [69], VFHQ [61], CelebV-HQ [73], MultiTalk [52], and MEAD [59], along with additional data we collected ourselves. The total duration of these raw videos exceeds 2,200 hours. However, as illustrated in Figure 14, we find that the overall quality of the data is poor, with numerous issues such as audio-lip misalignment, missing heads, multiple heads, occluded faces by subtitles, extremely small face regions, and low resolution. Directly using these data for model training results in unstable training, poor convergence, and terrible generation quality. To further obtain high-quality talking video data, we developed dedicated data processing pipeline for talking head generation. The pipeline consists of five steps: First, we perform scene transition detection based on TransNet V2 [46] and trim video clips to length of less than 30 seconds. 13 Figure 14. Examples of issues in the raw dataset. Figure 15. Distribution of the Sync-C in the CelebV-HQ. Second, we apply face detection based on Grounding DINO [30], filtering out videos with no faces, partial faces, or multiple heads, and use the resulting bounding boxes to extract talking heads. To ensure that the cropped areas encompass more than just the human faces, we apply scaling factor 1.1 to the bounding box regions. Third, we use HyperIQA [49], an image quality assessment model, to filter out low-quality and low-resolution videos. We apply HyperIQA to the first frame of each video and find that when the IQA score exceeds 40, there is noticeable improvement in overall video quality. Therefore, we use an IQA score of 40 as selection threshold, but this threshold can be dynamically adjusted depending on the dataset quality requirements. Fourth, we utilize SyncNet [41] to filter out videos with audio-lip synchronization issues. The SyncNet Confidence (Sync-C) metric is used as the basis for filtering. Figure 15 illustrates the confidence distribution of the CelebVHQ dataset, where threshold of 5 is applied for filtering. This threshold, like others, can be dynamically adjusted based on the dataset quality requirements. Lastly, we manually check the audio-lip synchronization and overall video quality for more accurate filtering for subset of the data. After completing the entire pipeline, the total duration of the processed high-quality videos is about 660 hours. Figure 16. Ablation of the classifier-free guidance scale. Please refer to the supplementary for video demos. E. More Ablation Studies guidance scale. By Classifier-free the classifier-free guidance scale, we observe variations in the expressiveness of the generated faces. As shown in Figure 16, higher guidance scales lead to more pronounced emotional expressions. adjusting F. More Qualitative Results Comparisons with baselines. Figure 17 showcases comparisons between talking videos generated by MEMO and baseline models on sampled out-of-distribution (OOD) data. Specifically, some baseline models (e.g., Hallo and Hallo2) tend to produce artifacts and fail to preserve the original identity and fine details. While certain methods (e.g., AniPortrait and V-Express) generate videos with fewer artifacts, they suffer from poor audio-lip synchronization and motion smoothness. In contrast, our method demonstrates the ability to produce more natural facial expressions and head movements that are well-aligned with the audio input. Additionally, the videos generated by MEMO exhibit superior overall visual quality and stronger identity consistency. More visualization of emotion-guided generation. The facial expressions of the generated talking video are influenced by both the expressions in the reference image and the emotional tone of the audio. As discussed in Section 4.2, the overall emotional tone of facial expressions is inferred mainly from the facial expression of the reference image, while our audio emotion-aware module functions mainly as subtle adjustment to enhance or moderately alter the Figure 19. MEMO can generate long-duration videos with alleviated error accumulation and maintain identity consistency. Please refer to the supplementary for video demos. the emotional state of each reference expression. As shown in Figure 18, our method adapts seamlessly to diverse emotional states, generating highly expressive and emotionally consistent talking videos. These results highlight the robustness and versatility of MEMO in leveraging both reference expressions and audio cues to create emotionally nuanced talking videos. Long-duration talking video generation. Figure 19 demonstrates that MEMO can generate long-duration videos while consistently maintaining the subjects facial features and expression fidelity over thousands of frames. The resulting videos exhibit smooth motion and high temporal coherence, showcasing the robustness of our approach for long video synthesis. These capabilities make MEMO superior choice over existing methods for applications requiring extended video content with stable and consistent output quality. Figure 17. Visualization of generated videos on the OOD dataset. Existing methods either have poor audio-lip synchronization (e.g., AniPortrait [60]) or suffer from error accumulation (e.g., Hallo [63]). In contrast, MEMO generates talking videos with natural head motion and accurate audio-lip synchronization without artifacts. Please refer to the supplementary for video demos. Figure 18. More visualization of expressive talking videos generated by MEMO based on reference images with various emotions. Please refer to the supplementary for video demos. emotion when prompted by the audio. Figure 13 in Section 5.4 has demonstrated that given fixed reference image, MEMO can refine the facial expressions of talking videos based on the given audio emotion. In this appendix, we further explore the flexibility of our method by evaluating its ability to generate expressive talking videos using reference images depicting the same person with different emotional expressions, such as neutral, angry, happy, and sad. To isolate the effect of reference image expressions, we set the audio emotion label to match"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "National University of Singapore",
        "Skywork AI"
    ]
}