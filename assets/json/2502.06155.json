{
    "paper_title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "authors": [
        "Hangliang Ding",
        "Dacheng Li",
        "Runlong Su",
        "Peiyuan Zhang",
        "Zhijie Deng",
        "Ion Stoica",
        "Hao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism."
        },
        {
            "title": "Start",
            "content": "EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Hangliang Ding * 1 Dacheng Li * 2 Runlong Su 3 Peiyuan Zhang 3 Zhijie Deng 4 Ion Stoica 2 Hao Zhang 3 5 2 0 2 0 1 ] . [ 1 5 5 1 6 0 . 2 0 5 2 : r Abstract Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate new family of sparse 3D attention that holds linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate fewstep generation capacities. We further devise three-stage training pipeline to conjoin the lowcomplexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4 7.8 faster for 29 and 93 frames 720p video generation with marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91 speedup when running on 4 GPUs with sequence parallelism. 1. Introduction Diffusion Transformers (DiTs) based video generators can synthesize long-horizon, high-resolution, and high-fidelity videos (Peebles & Xie, 2023; OpenAI, 2024; Kuaishou, *Equal contribution 1Tsinghua University 2University of California, Berkeley 3University of California, San Diego 4Shanghai Jiao Tong University. Correspondence to: Hao Zhang <haozhang@ucsd.edu>. Preprint. 1 2024; Lab & etc., 2024; Zheng et al., 2024; Esser et al., 2023; Yang et al., 2024b). The 3D attention is core module of such models. It flattens both the spatial and temporal axes of the video data into one long sequence for attention computation and reports state-of-the-art generation quality (Lab & etc., 2024; Yang et al., 2024b; Huang et al., 2024). Computation of 3D attention often consumes the majority of the time during the entire forward propagation of 3D DiT, especially with long sequences when generating extended videos. Thus, existing 3D DiTs suffer from prohibitively slow inference due to the slow attention computation and the multi-step diffusion sampling procedure. This paper tackles the issue by simultaneously sparsifying 3D attention and reducing sampling steps to accelerate 3D DiTs. To explore the redundancies in video data (recall that by nature videos can be efficiently compressed), we examine 3D DiT attention states and identify an intriguing phenomenon, referred to as the Attention Tile. As shown in Fig. 1a, the attention maps exhibit uniformly distributed and repetitive tile blocks, where each tile block represents the attention between latent frames1. This repetitive pattern suggests that not every latent frame needs to attend to all others. Moreover, the Attention Tile pattern is almost independent of specific input (Fig. 1d). With these, we propose solution that replaces the original attention with fixed set of sparse attention mask during inference (3.3). Specifically, we constrain each latent frame to only attend to constant number of other latent frames, which reduces the complexity of attention computation from quadratic to linear. We then consider shortening the sampling process of video from 3D DiT to further amplify the acceleration effect. Inspired by the recent advance in diffusion distillation (Salimans & Ho, 2022; Song et al., 2023; Kim et al., 2023; Liu et al., 2023b; Sauer et al., 2023; Yin et al., 2024; Heek et al., 2024; Xie et al., 2024), we adopt simple yet effective multi-step consistency distillation (MCD) (Heek et al., 2024) technique into our approach to achieve the efficient generation of compelling videos. In particular, we split the entire sampling trajectory into adjacent segments and perform consistency distillation within each one. We 1we use the term latent because DiTs compute in the latent space of VAEs (Rombach et al., 2022b). EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Figure 1: We observe the Attention Tile pattern in 3D DiTs. (a) the attention map can be broken down into smaller repetitive blocks. (b) These blocks can be classified into two types, where attention weights on the diagonal blocks are noticeably larger than on off-diagonal ones. (c) These blocks exhibit locality, where the attention score differences between the first frame and later frames gradually increases. (d) The block structure is stable across different data points, but varies across layers. We randomly select 2 prompts (one landscape and one portrait) and record the important positions in the attention map that accounted for 90% (95%, 99%) of the total. We printed the proportion of stable overlap of important positions across layers. also progressively decrease the number of segments to improve the generation quality at rare steps. Due to the orthogonality between sparse attention and MCD, naive combination is possible, such as directly distilling sparse student 3D DiT from pre-trained model. However, the initial gap between the sparse student and the teacher can be large so that the training suffers from cold start. To tackle this issue, we introduce more refined model acceleration process named EFFICIENT-VDIT. Initially, MCD is utilized to generate student model with the same architecture but fewer sampling steps than the teacher. Subsequently, we determine the optimal sparse attention pattern for each head of the student and then apply knowledge distillation procedure to the sparse model to maintain performance. With 0.1% the pretraining data, we train Open-Sora-Plan-1.2 models into variants that are 7.8 and 7.4 faster, with marginal performance tradeoff in VBench. (Huang et al., 2024). In addition, we provide evidence that our approach is amenable to advances in distributed inference systems, achieving an additional 3.91 speedup when running on 4 GPUs. In summary, our contribution are: 1. We discover and analyze the phenomenon of Attention Tile in 3D full attention DiTs, and propose family of sparse attention mask with linear complexity to address the redundancy. 2. We design framework EFFICIENT-VDIT based on our analysis of Attention Tile, which turns pre-trained 3D DiT to fast variant in data efficient manner. 3. We evaluate on two Open-Sora-Plan 1.2 models for EFFICIENT29 frames and 93 frames generation. VDIT achieves up to 7.8 speedup with little performance trade-off on VBench and CD-FVD. We further demonstrate the potential of integrating our method with advanced distributed inference techniques, achieving additional 3.91 with 4 GPUs. 2. Related Work Video Diffusion Transformers There is rich line of research in diffusion based models for video generation (Ho et al., 2022; He et al., 2022; Luo et al., 2023; Wang et al., 2023c; Ge et al., 2023a; Chen et al., 2024b; Guo et al., 2023; 2024). More recently, Peebles & Xie (2023) introduces the architecture of Diffusion Transformers (DiTs), and several popular video generation models have been developed using the DiTs backbone, for instance, Ma et al. (2024); Zheng et al. (2024); Lab & etc. (2024); Yang et al. (2024b). More specifically, Lab & etc. (2024); Yang et al. (2024b) has explored the use of 3D Full Attention Transformers, which jointly model spatial and temporal relationship, instead of previous models that separately model spatial and temporal relationship (e.g. one Transformer layer with spatial attention and the other with temporal attention (Zheng et al., 2024; Ma et al., 2024)). The design of 3D full attention has gained increasing popularity due to their promising performance. In this work, we tackle the efficiency problem specifically for 3D full attention diffusion Transformers. In addition, there is line of research that combines video diffusion model with sequential or autoregressive generation. These methods may also achieve speedup due to their use of shorter sequence length. EFFICIENT-VDIT aims to speedup in single diffusion forward, which is compatible with orthogonal to autoregressive manner methods (Henschel et al., 2024; Xiang et al., 2 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Figure 2: EFFICIENT-VDIT takes in pre-trained 3D Full Attention video diffusion transformer(DiT), with slow inference speed and high fidelity. It then operates on three stages to greatly accelerate the inference while maintaining the fidelity. In Stage 1, we modify the multi-step consistency distillation framework from (Heek et al., 2024) to the video domain, which turned DiT model to CM model with stable training. In Stage 2, EFFICIENT-VDIT performs searching algorithm to find the best sparse attention pattern for each layer. In stage 3, EFFICIENT-VDIT performs knowledge distillation procedure to optimize the fidelity of the sparse DiT. At the end, EFFICIENT-VDIT outputs DiT with linear attention, high fidelity and fastest inference speed. 2024; Chen et al., 2024a; Valevski et al., 2024). Accelerating diffusion inference Many work in diffusion models have been proposed to reduce the number of sampling steps to accelerate diffusion inference (Song et al., 2020; Lu et al., 2022a;b) (Liu et al., 2024). Song et al. (2023) proposes the consistency models which distills multiple steps ODE to one step. Wang et al. (2023b) extends CMs to video generation model. Li et al. (2024b) further extends the idea with reward model to speed up video diffusion model inference. Another line of research that accelerates diffusion models inference utilize multiple devices (Li et al., 2024c; Wang et al., 2024a; Chen et al., 2024d; Zhao et al., 2024). These works exploit the redundancy between denoising steps and use stale activations in distributed inference to hide communication overhead, and are naturally incompatible with work that reduce the redundancy between steps. In this work, we exploit the redundancy in attention computation, which is orthogonal to works that leverage distributed acceleration and redundancy between denoising steps. Our pipeline integrates multi-step CM approach (Xie et al., 2024) by default, and in experiment, we show that it can also seaminglessly integrate with parallel inference. Sparsity in Transformer inference has been investigated in the context of Large Language Models (LLMs) inference, which can be decomposed into pre-filling and decoding stages (Yu et al., 2022). StreamingLLM discovers the pattern of Attention Sink, and keeps combination of first few tokens and recent decoded tokens during decoding phrase (Xiao et al., 2023). Zhang et al. (2024a;b) adaptively identify the most significant tokens during test time. Video DiTs have different workload than LLMs, where DiTs perform single forward in each diffusion step without decoding phrase. In particular, our paper is among the first to explore sparse attention in the context of 3D Full Attention DiTs. In addition, our finding that Attention Tile is data-independent motivates us to design solution which does not require inference time adaptive searching, which is bottleneck in work such as Zhang et al. (2024b). Sparsity has also been studied in Gan and other diffusion-based models, yet we focus on the new architecture 3D DiT (Li et al., 2020; 2022). recent paper (Wang et al., 2024b) also discusses the redundancy in DiTs models, but no performance has been shown. 3. EFFICIENT-VDIT EFFICIENT-VDIT is framework that takes in 3D full attention DiT model , and outputs DiT that runs efficiently during inference TFast. EFFICIENT-VDIT consists of three stages. The first stage (3.2) performs multi-step consistency distillation and outputs TMCM, following the method developed in image diffusion models (Xie et al., 2024). The second stage (3.3) takes in TMCM, performs one-time search to decide the optimal sparse attention mask for each layer, and outputs model TSparse with the optimal sparse attention mask. The last step(3.4) performs knowledge distillation to preserve the model performance, using TMCM as the teacher and TSparse as the student, following the distillation design in (Gu et al., 2024; Jiao et al., 2019). In this section, we first introduce the characteristics of Attention Tile that motivate the design of the sparse patterns in Section 3.1. Then, we will introduce the framework EFFICIENT-VDIT by stages. 3.1. Preliminary: Characteristics of Attention Tile In 1, we briefly describe that the attention map consists of repetitive tile blocks. In this section, we dive into three characteristics that lead to our design and usage of family 3 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile to the endpoint. MLCD generalize CM by dividing the entire ODE trajectory in latent space into segments and carrying out consistency distillation for each segment independently which reduce the difficulty for training dramatically. MLCD obtains set of milestone states marked as {ts s=0. The loss for MLCD is: step}S LMLCD = (cid:13) (cid:13) DDIM(cid:0)ztm, fθ(ztm, tm), tm, ts (cid:13) (cid:13) step (cid:1) nograd (cid:16) DDIM(cid:0)ztn, fθ(ztn , tn), tn, ts step (cid:1)(cid:17)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (1) where is uniformly sampled from {0, . . . , S}, tm is unistep, ts+1 formly sampled from [ts step ], tn is uniformly sampled from [ts step, tm], DDIM(ztm, fθ(ztm, tm), tm, ts step) means one-step DDIM transformation from state ztm at timestep tm to timestep ts step with the estimated denoised image fθ(ztm , tm) and nograd refers to one-step diffusion without guidance scale. 3.3. Stage 2: Layer-wise Search for optimal Sparse attention mask Sparse Attention Masks Following our analysis in 3.1, desired sparse attention mask should separately treat on and off diagonal tile blocks, leverages the repetitive pattern In in off-diagonal tile blocks while considering locality. this paper, we aim on family of masks that achieve linear compute complexity while prioritizing simplicity and implementation efficiency. Specifically, we simply keep tile blocks in the main diagonals(marked as golden color in Figure 3). For off-diagonal tile blocks, we keep constant number of latent frames, and only retain attention between against these global reference frames (mark as blue color in Figure 3). Since is constant, the overall complexity of the attention is linear with respect to the number of latent frames. For simplicity, we choose these reference frames uniformly from all latent frames. For clarity, we denote mask with two numbers - : k. For example, the example figure 3 shows an attention mask of 2 : 6. Figure 3: Exemplar attention mask (2 : 6). It maintains the attention in the main diagonals and against 2 global reference latent frames. Tile blocks in white are not computed. of sparse attention masks. Large Diagonals Tile blocks on the main diagonals has higher attention scores than off-diagonal ones. In Figure 1(b), we plot the attention scores at the main diagonal tile blocks, compared to attention scores at the off-diagonal blocks, on Open-Sora-Plan-1.2 model (Lab & etc., 2024). We find that on average the main diagonal blocks contain values 2.80 higher than the off-diagonal ones. This suggests separate treatment of tile blocks on and off the main diagonals. Locality Off-diagonal tile blocks are similar, but the similarity decreases with further distance. In Figure 1(c), we plot the relative differences between the first latent frame and subsequent latent frames. We find that the differences increase monotonically. This indicates need to retain the computation of several tile blocks (i.e. more than one) to accommodate information in distant tile blocks. Data Independent The structure of the tile is relatively stable across different inputs. We plot the overlap of indices for largest attention scores for different prompts. We observe that roughly 90% of them coincide. This suggests reusing fixed set of attention masks during inference for different inputs. Motivated by the above characteristics, we develop family of sparse attention masks where we keep the attention computation in the main diagonal and the attention with constant number of global reference latent frames. Figure 3 visualizes one instance of the attention mask. The formulation will be introduce formally in 3.3. 3.2. Stage 1: Multi-Step Consistency Distillation We follow (Xie et al., 2024) to perform multi-step latent consistency distillation(MLCD) procedure to obtain TMCM as classic CM map from an arbitrary ODE trajectory state Figure 4: Search results for Open-Sora-Plan v1.2 model (29 frames). We verify that different layers have different sparsity in 3D video DiTs. Layer-wise Searching For Attention Masks Previous EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Algorithm 1 Searching for the optimal set of sparse attention masks Require: Available mask list from dense to sparse [Mask1, Mask2, ..., Maskn], teacher model MT , student model , loss function L, number of timestep samples m. Require: Forward function FORWARD, threshold r, which is the maximum tolerance for L. Initialize best mask None for from 1 to do Require: 1: for each layer in model layers do 2: 3: 4: 5: 6: Apply Maski to the current layer (l) Initialize Lmax for each timestep sampled times from From dense to sparse Uniform(0, 1) do tention TSparse as the student (Hinton, 2015). We follow similar design as knowledge distillation methods in Transformer models for Languages (Gu et al., 2024; Jiao et al., 2019), which combines the loss from attention output and hidden states output, over total layers. Ltotal ="
        },
        {
            "title": "1\nL",
            "content": "(cid:32) (cid:88) (cid:16) i=1 attention + L(i) L(i) mlp (cid:33) (cid:17) + λLdiffusion, (2) where each term is defined as follows: Attention Loss Lattention: To calculate L(i) attention, we apply the MSE loss between the output of the students selfattention layer ˆO(i) attn and the teachers self-attention layer output O(i) attn: 7: 8: 9: 10: 11: 12: ˆy FORWARD(M (l) Compute Li(t) L(y, ˆy) Update Lmax max(Lmax , Maski, t) , Li(t)) end for if Lmax < then best mask Maski mask within threshold Update the best else break end if 13: 14: 15: 16: 17: 18: end for end for Assign best mask to the current layer (l) studies has suggested that different layers exhibit different amount of sparsity (Wang et al., 2023a; Ge et al., 2023b; Yang et al., 2024a). Using the MSE difference of the final hidden states as guidance, we develop searching method to find the best combinations of attention masks across layers (Algorithm 1). Intuitively, we first perform profiling process on TM CM . The profiling step loops over layers, and greedily selects the largest which does not incur higher MSE difference than predefined threshold r. dynamic programming based alternative is also described in Appendix A, where given runtime constraint, the minimum possible maximum loss difference is computed. In the experiment section ( 4), we show evidence that this is key to maintaining video quality. For simplicity, we apply the greedy version of the search throughout the main paper. Fig. 4 shows an exemplar algorithm output. attention = MSE( ˆO(i) L(i) attn, O(i) attn). (3) MLP Loss Lmlp: We calculate L(i) tween the outputs of the students MLP layer ˆO(i) teachers MLP layer output O(i) mlp: mlp as the MSE bemlp and the mlp = MSE( ˆO(i) L(i) mlp, O(i) mlp). (4) In addition, we keep the diffusion loss Ldiffusion for the student model. In practice, we observed that the diffusion loss tends to be an order of magnitude smaller compared to other losses. To balance the contribution of the diffusion loss during the training process, we scale it by factor λ, ensuring it has comparable impact on the overall loss function. 4. Experiment We first present our experiment settings and evaluation metrics in 4.1. We then discuss system performance in 4.2, demonstrating the effectiveness on single GPU and applicable to multiple GPUs. In 4.3, we compare the video quality with and without variants of our methods with VBench and CD-FVD (Huang et al., 2024; Ge et al., 2024). Finally, we show visualization results in 4.4 of the generation quality for the original model, the MLCD model, and the final model. 4.1. Experiment setup 3.4. Stage 3: Knowledge Distillation with TT CM Stage 2 introduces performance drop since we significantly modify the attention mask. In Stage 3, we apply the method of knowledge distillation, using the model with full attention TM CM as the teacher, and the model with sparse atModels. We use the 29 and 93 frames models of the popular 3D DiT based Open-Sora-Plan family (Lab & etc., 2024). The model uses VAE inherits weights from the SD2.1 VAE (Rombach et al., 2022a), with compression ratio of 4x8x8 (temporal, height and width). For the text encoder, it uses mt5-XXL as the language model, and it 5 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile incorporates RoPE as the positional encoding (Xue, 2020; Su et al., 2024). In addition to the VAE encoder, videos are further processed by patch embedding layer that downsamples the spatial dimensions by factor of 2. The videos tokens are finally flattened into one-dimensional sequence across the frame, width, and height dimensions. Metrics. We evaluate video quality using VBench and Content-Debiased Frechet Video Distance (CDFVD) (Huang et al., 2024; Ge et al., 2024). VBench assesses the quality of video generation by aligning closely with human perception , computed for each frame of the video and then averaged across all frames, providing comprehensive assessment. CD-FVD measures the distance between the distributions of generated and real videos toward per-frame quality over temporal realism. Baselines. We consider two models as the major basethe original Open-Sora-Plan model and the model lines: after consistency distillation. Following the default settings of Open-Sora-Plan models (Lab & etc., 2024), we use 100 DDIM steps for the original model, which is consistent across all experiments and training in the paper. For the MLCD model, we select the checkpoint with 20 inference steps as we empirically find that it achieves the best qualitative result. Implementation details. We use FlexAttention from PyTorch 2.5.0 (Ansel et al., 2024) as the attention backend. We provide more detailed description on how to leverage FlexAttention to implement our method in Appendix B. We generate videos based on the VBench standard prompt list for VBench evaluation. To avoid potential data contamination in CD-FVD evaluation, we use set of 2000 samples from the Panda-70M (Chen et al., 2024c) test set to build our real-world data comparison. As we use the CD-FVD score between real-world data and generated videos to evaluate the capacity of DiT models, the prompt style needs to align with the real-world data clip samples. Therefore, we randomly select prompts from the Panda-70M test set caption list for video generation by the models. Training details. All models are trained using the first 2000 samples from the Open-Sora-Plans mixkit dataset.The global batch size is set to 2, and training is conducted for total of 10000 steps, equivalent to 10 epochs of dataset. The learning rate is 1e-5, and the gradient accumulation steps is set to 1. The diffusion scale factor λ is 100. The MLCD model is trained with 100 DDIM steps of the original model. The final model is trained with 20-step MLCD model checkpoint. 4.2. System Performance The major target of EFFICIENT-VDIT accelerates inference in single GPU by using multi-step consistency distillation Table 1: Speedup with different masks."
        },
        {
            "title": "Frames Mask",
            "content": "Sparsity (%) Time(ms)"
        },
        {
            "title": "Speedup",
            "content": "29 93 full 4:4 3:5 2:6 1:7 full 12:12 8:16 6:18 4:20 3:21 0.00 17.60 29.88 45.47 64.38 0.00 21.51 40.30 51.88 64.98 72. 58.36 46.52 40.08 31.35 20.65 523.61 397.72 303.90 244.13 179.74 142.77 1.00 1.25 1.46 1.86 2.83 1.00 1.32 1.72 2.14 2.91 3.67 and sparse attention. In 4.2.1, we demonstrate the system speedup with various settings. In addition, we demonstrate an advantage of our method that it can be seaminglessly integrate with advanced parallel method, i.e. sequence parallelism, in 4.2.2. 4.2.1. EFFICIENT-VDIT SPEEDUP ON SINGLE GPU We test our approach on single A100-SXM 80GB GPU. Table 1 shows the computation time for single sparse attention kernel, while Table 2 presents the average execution time of all layers after layerwise search in Algorithm 1. 2:6 refers to 2 global reference frames in Fig.3. Sparsity refers to the proportion of elements in the kernel that can be skipped. During testing, we consider only the attention operation, where the inputs are query, key, value, and mask, and the output is the attention output. We do not account for the time of VAE, T5, or embedding layers. The measurement method involves 25 warmup iterations, followed by 100 runs. The median of the 20th to 80th percentile performance is used as the final result. In Table 1, we observe that as the sparsity increases, the computation time decreases significantly. For instance, with 2:6 attention mask, corresponding to sparsity level of 45.47%, the execution time reduces to 31.35 ms, resulting in 1.86 speedup compared to the full mask. In Table 2, the effect of increasing threshold on speedup is evident. As increases, the sparsity grows, leading to greater reduction in computation time and corresponding increase in speedup. For example, with = 0.050, the sparsity reaches 37.78%, achieving speedup of 1.64. When is further increased to 0.400, the sparsity level rises to 55.07%, and the speedup improves to 2.25. This positive correlation between r, sparsity, and speedup highlights the efficiency gains that can be achieved by leveraging higher sparsity levels. 6 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Table 2: Open-Sora-Plan with 29 frames and 720p resolution results on VBench, CD-FVD metrics and kernel speedup evalutation. r=0.1 indicates that this checkpoint is trained using the layerwise search strategy described in Algorithm 1, with threshold of r=0.1. We selects some dimensions for analysis, with the remaining dimensions provide in the Table 6. We also shows kernel different speedup with threshold r. Model Base MLCD Oursr=0.025 Oursr=0.050 Oursr=0.100 Oursr=0.200 Oursr=0.400 Final Score Aesthetic Quality Motion Smoothness Temporal Flickering Object Class Subject Consistency CD-FVD Sparsity (%) Kernel Time(ms) Kernel Speedup Speedup 76.12% 58.34% 76.81% 58.92% 76.14% 57.21% 76.01% 57.57% 76.00% 56.59% 75.02% 55.71% 75.30% 55.79% 99.43% 99.41% 99.37% 99.15% 99.13% 99.03% 98.93% 99.28% 99.42% 64.72% 63.37% 60.36% 99.49% 99.56% 58.70% 57.12% 99.54% 55.22% 99.50% 54.98% 99.46% 98.45% 98.37% 98.26% 97.58% 97.73% 97.28% 97.71% 172.64 190.50 186.84 195.55 204.13 223.75 231. 0.00 0.00 23.51 37.78 45.08 51.55 55.07 58.36 58.36 43.50 35.58 31.54 27.91 25.96 1.00 1.00 1.34 1.64 1.85 2.09 2.25 1.00 5.00 5.85 6.60 7.05 7.50 7.80 4.2.2. EFFICIENT-VDIT SPEEDUP IN DISTRIBUTED"
        },
        {
            "title": "SETTING",
            "content": "Table 3: EFFICIENT-VDIT with sequence parallelism on Open-Sora-Plan model. Time as wall-clock-time per step. EFFICIENT-VDIT utilize sparse attention and consistency distillation to achieve speedup. These methods are orthogonal to the recent advances in distributed systems, mainly sequence parallelism based solution in LLMs (Liu et al., 2023a; Li et al., 2024a; Jacobs et al., 2023) and model parallelism (or with hybrid sequence parallelism) based solution in diffusion Transformers (Li et al., 2024c; Wang et al., 2024a; Chen et al., 2024d). We consider sequence parallelism in this section for is simplicity and empirical lower overhead (Li et al., 2024a;c; Xue et al., 2024). Implementation We utilize the All-to-All communication primitives to implement sequence parallelism (Jacobs et al., 2023). In the attention computation, the system partitions the operations along the head dimension while keeping the entire sequence intact on each GPU, allowing simple implementation of EFFICIENT-VDIT by applying the same attention mask as in the one GPU setting 2. As result, EFFICIENT-VDIT is natively compatible with Allto-All sequence parallelism. We conduct scaling experiment with sequence parallelism on 4x A100-SXM 80GB GPUs, interconnected with NVLink. We observe speedup of 3.68 - 3.91 for 29 and 93 frames generation on 4 GPUs, which is close to theoretical speedup of 4  (Table 3)  . If reported 29 frames generation on multi-GPUs, Oursr=0.100 can achieve 25.8x speedup on 4 GPUs and 13.0x speedup on 2 GPUs. 4.3. Video Quality benchmark In this section, we first evaluate EFFICIENT-VDIT with layerwise searching on CD-FVD and VBench (Huang et al., 2024; Ge et al., 2024). We compare with the baseline of the original Open-Sora-Plan 1.2 model, and the model we obtain only using the MLCD method. We then conduct two 2The difference is that the attention mask is applied to fewer number of attention heads."
        },
        {
            "title": "Frames",
            "content": "# GPUs Time (s)"
        },
        {
            "title": "Speedup",
            "content": "29 93 1 2 4 1 2 4 5.56 2.98 1.52 39.06 20.00 10. 1.00 1.87 3.68 1.00 1.95 3.91 ablation experiments to understand the effectiveness of the MLCD method, and our layerwise searching algorithm. Table 2 demonstrates the main result of the 29 frames model. In VBench, We find that the results of all our search models are within 1% final score against the Base model with no noticeable drop in several key dimensions. the At higher acceleration ratios, such as Oursr=0.400, model maintains stable performance, with minimal deviations from the Base model, demonstrating the robustness of our approach while achieving significant speedups. However, we note that the imaging quality and subject class are lower than those of the base model. The reason why the VBench score remains within 1% difference is that our model improves the dynamic degree. With more sparsity, our pipeline has the characteristics of being able to capture richer motions between frames, but trading off some degrees of aesthetic quality and subject class accuracy. In CD-FVD, our models with smaller acceleration ratios achieve better scores than MLCD model. For example, Oursr=0.025 achieves score of 186.84 with speedup of 5.85, outperforming the MLCD model. As the acceleration ratio increases, the score degrades as expected. Oursr=0.400 reaches score of 231.68 with speedup of 7.80, showing trade-off between acceleration and performance. Our models maintain performance with minimal performance drop and achieve significant speedup. EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Extension to MM-DiT architecture We demonstrate our methods generalizability by applying it to CogVideoX5B (Yang et al., 2024b), which is based on the MM-DiT architecture that differs from Open-Sora-Plans cross attention module, where its attention module concatenates text tokens with video token. For MM-DiT, we only apply sparse mask to the video-video part considering that the text tokens length are very small compared to video tokens. Our approach achieves comparable performance, maintaining the final VBench score within 1% of the baseline as shown in Table 4. Detailed analysis and additional results can be found in Appendix E. Table 4: CogVideoX-5B with 49 frames and 480p resolution results on VBench."
        },
        {
            "title": "Model",
            "content": "Final Score Aesthetic Quality Motion Smoothness Temporal Flickering"
        },
        {
            "title": "Speedup",
            "content": "Base Oursr=5 77.91% 57.91% 77.15% 51.18% 97.83% 96.67% 97.34% 97.18% 1.00 1.34 Order of MLCD and KD We claim that knowledge distillation and consistency distillation are orthogonal processes. To verify this, we conducted an ablation experiment on the distillation order. We first applied attention distillation based on the original model, then used this model to perform multi-step latent consistency distillation (MLCD). The results in Table 5 support our hypothesis, showing minimal differences in VBench and CD-FVD scores regardless of the distillation sequence. We also show qualitative samples in Appendix Fig. 6 to illustrate the video quality. Table 5: Quantitative evaluation on distillation order for MLCD and layerwise knowledge distillation."
        },
        {
            "title": "Final\nScore",
            "content": ""
        },
        {
            "title": "Temporal\nFlickering",
            "content": "CD-FVD 4.4. Qualitative result As illustrated in Fig.5, we compare the video results generated by three methods: the original model, after applying MLCD, and after knowledge distillation. The generation settings are consistent with those in Table 2, demonstrating that both the MLCD and knowledge distillation methods maintain the original quality and details. More qualitvative samples are listed in Appendix F. Figure 5: Qualitative samples of our models. We compare the generation quality between the base model, MLCD model, and after knowledge distillation. Frames shown are equally spaced samples from the generated video. EFFICIENT-VDIT is shortened as E-vdit for simplicity. More samples can be found in Appendix F. MLCD + KD 76.00% 56.59% KD + MLCD 75.50% 56.38% 99.13% 99.12% 99.54% 99.40% 204.13 203.52 5. Conclusion Separate Effect of MLCD and Layerwise Search. We evaluate the effectiveness of MLCD and our layerwise search strategy separately. MLCD achieves comparable or better performance across most VBench metrics (76.81% overall score) with 5.00 speedup, maintaining consistent performance after knowledge distillation. For layerwise search, compared to uniform masking patterns (e.g., 4:4, 3:5 splits), our approach with various thresholds (r = 0.025, 0.050, 0.100) achieves better VBench scores (76.00%) and speedup (7.05 vs. 5.80), while maintaining CD-FVD scores below 250. Detailed analysis and additional results can be found in Appendix D.1. 8 In this paper, we first describe the phenomenon of Attention Tile, and dive into its characteristics of repetitive, large diagonals, locality, and data independent. Then we describe class of sparse attention pattern tailored to address the efficiency problem in Attention Tile. Lastly, we introduce our overall framework that leveraged this class of sparse attention, which further leverages multi-step consistency distillation, layerwise searching, and knowledge distillation for faster generation and high performance. Experiments on two varaints of the Open-Sora-Plan model has demonstrated that our method can achieve similar performance, with 0.1% the pre-training data, and up to 7.8 speedup. Further ablation study has shown that our method can be natively integrated with advanced parallelism method to EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile achieve further speedup. 6. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. As highlighted in (Mirsky & Lee, 2020), such generative technologies can impact media authenticity, privacy, and public trust. We acknowledge these potential impacts and emphasize that our research is intended to advance the scientific understanding of machine learning while encouraging responsible development and deployment of these technologies."
        },
        {
            "title": "References",
            "content": "Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., and Chintala, S. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode TransforIn 29th ACM Intermation and Graph Compilation. national Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). ACM, April 2024. doi: 10.1145/ 3620665.3640366. URL https://pytorch.org/ assets/pytorch2-2.pdf. Chen, B., Monso, D. M., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Next-token prediction meets full-sequence diffusion. arXiv preprint arXiv:2407.01392, 2024a. Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., and Shan, Y. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73107320, 2024b. Chen, T.-S., Siarohin, A., Menapace, W., Deyneka, E., Chao, H.-w., Jeon, B. E., Fang, Y., Lee, H.-Y., Ren, J., Yang, M.-H., and Tulyakov, S. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024c. Chen, Z., Ma, X., Fang, G., Tan, Z., and Wang, X. Asyncdiff: Parallelizing diffusion models by asynchronous denoising. arXiv preprint arXiv:2406.06911, 2024d. Esser, P., Chiu, J., Atighehchian, P., Granskog, J., and Germanidis, A. Structure and content-guided video synIn Proceedings of the thesis with diffusion models. IEEE/CVF International Conference on Computer Vision, pp. 73467356, 2023. Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs, D., Huang, J.-B., Liu, M.-Y., and Balaji, Y. Preserve your own correlation: noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22930 22941, 2023a. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023b. Ge, S., Mahapatra, A., Parmar, G., Zhu, J.-Y., and Huang, J.-B. On the content bias in frechet video distance. arXiv preprint arXiv:2404.12391, 2024. Gu, Y., Dong, L., Wei, F., and Huang, M. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2024. Guo, Y., Yang, C., Rao, A., Agrawala, M., Lin, D., and Dai, B. Sparsectrl: Adding sparse controls to text-to-video arXiv preprint arXiv:2311.16933, diffusion models. 2023. Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D., and Dai, B. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. He, Y., Yang, T., Zhang, Y., Shan, Y., and Chen, Q. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Heek, J., Hoogeboom, E., and Salimans, T. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. Henschel, R., Khachatryan, L., Hayrapetyan, D., Poghosyan, H., Tadevosyan, V., Wang, Z., Navasardyan, S., and Shi, H. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. Hinton, G. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 9 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21807 21818, 2024. Jacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song, S. L., Rajbhandari, S., and He, Y. Deepspeed ulysses: System optimizations for enabling training of extreme arXiv preprint long sequence transformer models. arXiv:2309.14509, 2023. Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. Tinybert: Distilling bert arXiv preprint for natural language understanding. arXiv:1909.10351, 2019. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probabilarXiv preprint ity flow ode trajectory of diffusion. arXiv:2310.02279, 2023. Kuaishou. Kling, 2024. URL https://kling. kuaishou.com/en. Accessed: [2024]. Lab, P.-Y. and etc., T. A. Open-sora-plan, April 2024. https://doi.org/10.5281/zenodo. URL 10948109. Li, D., Shao, R., Xie, A., Xing, E. P., Ma, X., Stoica, I., Gonzalez, J. E., and Zhang, H. Distflashattn: Distributed memory-efficient attention for long-context llms training. In First Conference on Language Modeling, 2024a. Li, J., Feng, W., Fu, T.-J., Wang, X., Basu, S., Chen, W., and Wang, W. Y. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv preprint arXiv:2405.18750, 2024b. Li, M., Lin, J., Ding, Y., Liu, Z., Zhu, J.-Y., and Han, S. Gan compression: Efficient architectures for interactive conditional gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 52845294, 2020. Li, M., Lin, J., Meng, C., Ermon, S., Han, S., and Zhu, J.-Y. Efficient spatially sparse inference for conditional gans and diffusion models. Advances in neural information processing systems, 35:2885828873, 2022. Liu, H., Zaharia, M., and Abbeel, P. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023a. Liu, H., Xie, Q., Deng, Z., Chen, C., Tang, S., Fu, F., Zha, Z.-j., and Lu, H. Scott: Accelerating diffusion models with stochastic consistency distillation. arXiv preprint arXiv:2403.01505, 2024. Liu, X., Zhang, X., Ma, J., Peng, J., et al. Instaflow: One step is enough for high-quality diffusion-based text-toimage generation. In The Twelfth International Conference on Learning Representations, 2023b. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Luo, Z., Chen, D., Zhang, Y., Huang, Y., Wang, L., Shen, Y., Zhao, D., Zhou, J., and Tan, T. Videofusion: Decomposed diffusion models for high-quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.-F., Chen, C., and Qiao, Y. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. Mirsky, Y. and Lee, W. The creation and detection of deepfakes: survey. ACM Computing Surveys, 2020. doi: 10.1145/3425780. OpenAI. Sora, 2024. URL https://openai.com/ index/sora/. Accessed: [2024]. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022a. Li, M., Cai, T., Cao, J., Zhang, Q., Cai, H., Bai, J., Jia, Y., Li, K., and Han, S. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 71837193, 2024c. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022b. 10 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022."
        },
        {
            "title": "Progressive distillation for\narXiv preprint",
            "content": "Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Valevski, D., Leviathan, Y., Arar, M., and Fruchter, S. arXiv Diffusion models are real-time game engines. preprint arXiv:2408.14837, 2024. Wang, H., Agarwal, S., Tanaka, Y., Xing, E., Papailiopoulos, D., et al. Cuttlefish: Low-rank model training without all the tuning. Proceedings of Machine Learning and Systems, 5:578605, 2023a. Wang, J., Fang, J., Li, A., and Yang, P. Pipefusion: Displaced patch pipeline parallelism for inference of diffusion transformer models. arXiv preprint arXiv:2405.14430, 2024a. Xue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y., Tang, H., Yang, S., Liu, Z., et al. Longvila: Scaling longcontext visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. Xue, L. mt5: massively multilingual pre-trained textto-text transformer. arXiv preprint arXiv:2010.11934, 2020. Yang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and Zhao, H. Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. arXiv preprint arXiv:2405.12532, 2024a. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024. Yu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.- G. Orca: distributed serving system for {TransformerIn 16th USENIX SympoBased} generative models. sium on Operating Systems Design and Implementation (OSDI 22), pp. 521538, 2022. Wang, J., Ma, A., Feng, J., Leng, D., Yin, Y., and Liang, X. Qihoo-t2x: An efficiency-focused diffusion transformer via proxy tokens for text-to-any-task. arXiv preprint arXiv:2409.04005, 2024b. Zhang, Z., Liu, S., Chen, R., Kailkhura, B., Chen, B., and Wang, A. Q-hitter: better token oracle for efficient llm inference via sparse-quantized kv cache. Proceedings of Machine Learning and Systems, 6:381394, 2024a. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024b. Zhao, X., Jin, X., Wang, K., and You, Y. Real-time video arXiv generation with pyramid attention broadcast. preprint arXiv:2408.12588, 2024. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/ Open-Sora. Wang, X., Zhang, S., Zhang, H., Liu, Y., Zhang, Y., Gao, C., and Sang, N. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023b. Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al. Lavie: Highquality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023c. Xiang, J., Liu, G., Gu, Y., Gao, Q., Ning, Y., Zha, Y., Feng, Z., Tao, T., Hao, S., Shi, Y., et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Xie, Q., Liao, Z., Deng, Z., Tang, S., Lu, H., et al. Mlcm: Multistep consistency distillation of latent diffusion model. arXiv preprint arXiv:2406.05768, 2024. 11 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile A. Extened layerwise search algorithm In this section, we explore how to balance the trade-off between inference speedup and output image quality. Intuitively, as the attention map becomes sparser, the inference time decreases, but the output image quality also degrades. With this model, we can answer the key question: Given target speedup or inference time, how can we achieve the highest possible image quality? This problem is well-suited to latency constrained case because, in real-world applications, speedup can be precisely measured. Adjusting the generation quality within these constraints is therefore meaningful. Additionally, solving this problem allows us to approximate continuous speedup ratios as closely as possible using discrete masks, further validating the robustness of our algorithm. A.1. Estimation and Quantitative Analysis The inference time can be quantitatively computed. Given time limitation Ttarget. Suppose we have series of masks M1, M2, . . . , Mk. For each mask, we can pre-profile its runtime as T1, T2, . . . , Tk. If layer uses mask aj [1, k], the total inference time is given by = (cid:80) Taj Ttarget. On the other hand, quantifying image quality is challenging. To address this, we make an assumption: the impact of different layers on image quality is additive. We use the loss as the value function, representing the output image quality as = (cid:80) Lj,aj , where Lj,aj denotes the loss value when layer uses mask type aj. A.2. Lagrangian Relaxation Method By introducing Lagrange multiplier λ, we construct the Lagrangian function: L(λ) = (cid:88) Lj,aj + λ (cid:88) Taj Ttarget . Our goal is to minimize L(λ), that is: min aj L(λ) = min aj (cid:88) Lj,aj + λ (cid:88)"
        },
        {
            "title": "Taj",
            "content": "λTtarget. Since Ttarget is constant, the optimization problem can be simplified into independent subproblems for each layer j: (cid:0)Lj,aj + λTaj (cid:1) . min aj A.3. Lagrangian Subgradient Method Input: Initial Lagrange multiplier λ(0), learning rate αt, maximum iterations . Output: Approximate optimal solution {aj} and Lagrange multiplier λ. 1. Initialization: Set iteration counter = 0. 2. While < and not converged: (a) Step 1: Solve Subproblems For each layer j, solve the subproblem: a(t) = arg min aj (cid:16) Lj,aj + λ(t)Taj (cid:17) . 12 (5) (6) (7) (8) EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile (b) Step 2: Calculate Subgradient Compute the subgradient: (c) Step 3: Update Lagrange Multiplier Update λ using the subgradient: (d) Update = + 1. g(t) = (cid:88) Ta(t) Ttarget. λ(t+1) = λ(t) + αtg(t). Output: Return the approximate solution {aj} and the final Lagrange multiplier λ. B. FlexAttention implementation details (9) (10) The attention we design can be efficiently implemented by the native block-wise computation design in FlexAttention. Compared to dynamic implementations, our computations are static, allowing us to leverage static CUDA graphs for capturing or use PyTorchs compile=True feature. FlexAttention employs block-based mechanism that allows for efficient handling of sparse attention patterns. Specifically, when an empty block is encountered, the module automatically skips the attention computation, leveraging the sparsity in the attention matrix to accelerate calculations. The ability to skip computations in this manner results in significant speedups while maintaining efficient memory usage. Additionally, FlexAttention is optimized by avoiding the need to materialize the entire mask. This mechanism enables FlexAttention to operate efficiently on large-scale models without incurring significant memory costs. For example, the additional memory usage of model with 32 layers and 29 frames mask is only 0.278GB, while 93 frames mask requires 0.715GB of additional memory, which is considered minimal for large-scale models. By not needing to store or process the full mask, we save both memory and computation time, leading to improved performance, especially in scenarios where the attention matrix is highly sparse. C. Supplemental Vbench Evaluation Table 6: Supplemental VBench evaluation for main result."
        },
        {
            "title": "Model",
            "content": "Base MLCD"
        },
        {
            "title": "Spatial\nRelationship",
            "content": "23.25% 54.00% 94.47% 34.72% 19.21% 56.00% 94.12% 41.67% Oursr=0.025 Oursr=0.050 Oursr=0.100 Oursr=0.200 Oursr=0.400 18.83% 55.00% 96.25% 52.78% 11.74% 58.00% 92.11% 58.33% 18.98% 56.00% 93.65% 63.89% 17.99% 53.00% 51.82% 59.72% 15.32% 54.00% 92.64% 65.28% 43.49% 40.57% 46.02% 39.81% 43.88% 36.14% 37.05%"
        },
        {
            "title": "Scene",
            "content": "18.60% 22.67% 12.35% 22.31% 15.77% 13.88% 12.06%"
        },
        {
            "title": "Imaging\nQuality",
            "content": "19.88% 20.46% 20.31% 20.25% 20.20% 20.29% 20.24% 18.45% 18.21% 18.17% 17.71% 17.98% 17.97% 18.19% 19.69% 19.77% 19.11% 19.45% 19.29% 18.97% 19.22% 97.64% 97.98% 97.70% 97.71% 97.55% 97.62% 97.66% 64.75% 65.55% 58.90% 56.86% 54.88% 54.07% 54.36% 13 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile D. Ablation study D.1. Ablation study of the effect of MLCD and layerwise search Effect of MLCD We conduct tests on VBench and CD-FVD, first comparing the differences between the Base model and the MLCD model, and then evaluating the compatibility of CM with the attention mask. As shown in Table 7, the MLCD model performs as well as or better than the Base model across most dimensions on VBench, achieving an overall VBench score of 76.81%. Due to the MLCD model requiring fewer sampling steps than the Base model, it achieves 5.00 speedup. Furthermore, we observe that the MLCD model, even after undergoing knowledge distillation, maintains performance without any drop in quality. The VBench score and CD-FVD trends are consistent, indicating that the MLCD model supports attention mask operations effectively, similar to the original model. Therefore, the MLCD model continues to deliver high-quality performance while offering significant acceleration benefits. Table 7: Ablation experiments on the effect of MLCD. Model Base Base4:4 Base3:5 Base2:6 Base1: MLCD MLCD4:4 MLCD3:5 MLCD2:6 MLCD1:7 Final Score Aesthetic Quality Motion Smoothness Temporal Flickering Object Class Subject Consistency Imaging Quality CD-FVD Speedup 76.12% 58.34% 76.57% 58.64% 75.53% 55.47% 76.33% 57.14% 77.15% 57.53% 76.81% 58.92% 75.90% 57.84% 75.41% 57.19% 75.23% 57.45% 75.84% 56.83% 99.43% 99.38% 99.01% 99.06% 98.67% 99.41% 99.38% 99.36% 99.29% 98.99% 99.28% 99.20% 98.96% 99.02% 98.66% 99.42% 99.50% 99.50% 99.48% 99.23% 64.72% 66.38% 62.26% 56.17% 60.68% 63.37% 63.03% 57.04% 54.59% 52.77% 98.45% 98.26% 97.42% 97.58% 96.96% 98.37% 98.21% 98.12% 98.37% 97.54% 64.75% 63.56% 59.67% 61.10% 61.91% 65.55% 58.47% 58.84% 57.35% 56.42% 172.64 171.62 197.35 201.61 322.28 190.50 175.47 190.92 213.72 294. 1.00 1.16 1.26 1.45 1.77 5.00 5.80 6.30 7.25 8."
        },
        {
            "title": "Model",
            "content": "Base Base4:4 Base3:5 Base2:6 Base1:7 23.25% 54.00% 94.47% 34.72% 32.01% 55.00% 90.94% 43.06% 15.85% 53.00% 88.88% 58.33% 21.65% 56.00% 93.27% 56.94% 17.76% 54.00% 93.02% 75.00% MLCD MLCD4:4 MLCD3:5 MLCD2:6 MLCD1:7 19.21% 56.00% 94.12% 41.67% 22.79% 53.00% 92.69% 50.00% 22.10% 50.00% 90.82% 43.06% 18.60% 53.00% 92.52% 44.44% 16.92% 53.00% 91.92% 63.89%"
        },
        {
            "title": "Scene",
            "content": "18.60% 17.30% 14.53% 18.31% 19.99% 22.67% 17.51% 21.44% 16.21% 17.22%"
        },
        {
            "title": "Background\nConsistency",
            "content": "19.88% 20.21% 20.13% 19.87% 19.95% 20.46% 19.89% 19.97% 19.89% 19.94% 18.45% 18.41% 17.46% 18.23% 18.25% 18.21% 18.32% 17.68% 17.84% 18.56% 19.69% 19.48% 18.43% 18.94% 19.41% 19.77% 19.06% 19.75% 20.12% 19.85% 97.64% 97.17% 97.28% 97.27% 97.30% 97.98% 97.30% 97.47% 97.70% 97.45% 43.49% 45.42% 44.38% 49.90% 44.75% 40.57% 39.80% 43.48% 43.36% 43.27% Effect of Layerwise Search We conduct tests on VBench and CD-FVD, selecting the MLCD model as the baseline. We compare applying uniform mask across all layers (e.g., 4:4, 3:5) with the layerwise mask from Algorithm 1. As shown in Table 8, in VBench, using the layerwise mask with (r = 0.025, 0.050, 0.100) achieve score exceeding 76.00%, significantly outperforming the results without layerwise masking, while also providing better speedup (7.05 vs. 5.80). In CD-FVD, the layerwise mask consistently results in scores below 250. However, as sparsity increases, the score without layerwise masking exceeds 250, indicating decrease in video generation quality. Therefore, the layerwise approach enhances the quality of generated videos. EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Table 8: Ablation experiments on the effect of our layerwise searching algorithm. Model Final Score Aesthetic Quality Motion Smoothness Temporal Flickering Object Class Subject Consistency Imaging Quality CD-FVD Speedup MLCD MLCD4:4 MLCD3:5 MLCD2:6 MLCD1:7 Oursr=0.025 Oursr=0.050 Oursr=0.100 Oursr=0.200 Oursr=0.400 76.81% 58.92% 75.90% 57.84% 75.41% 57.19% 75.23% 57.45% 75.84% 56.83% 76.14% 57.21% 76.01% 57.57% 76.00% 56.59% 75.02% 55.71% 75.30% 55.79% 99.41% 99.38% 99.36% 99.29% 98.99% 99.37% 99.15% 99.13% 99.03% 98.93% 99.42% 99.50% 99.50% 99.48% 99.23% 63.37% 63.03% 57.04% 54.59% 52.77% 99.49% 60.36% 99.56% 58.70% 57.12% 99.54% 55.22% 99.50% 54.98% 99.46% 98.37% 98.21% 98.12% 98.37% 97.54% 98.26% 97.58% 97.73% 97.28% 97.71% 65.55% 58.47% 58.84% 57.35% 56.42% 58.90% 56.86% 54.88% 54.07% 54.36% 190.50 175.47 190.91 213.71 294.09 186.84 195.55 204.13 223.75 231.68 5.00 5.80 6.30 7.25 8.85 5.85 6.60 7.05 7.50 7."
        },
        {
            "title": "Spatial\nRelationship",
            "content": "MLCD MLCD4:4 MLCD3:5 MLCD2:6 MLCD1:7 Oursr=0.025 Oursr=0.050 Oursr=0.100 Oursr=0.200 Oursr=0.400 19.21% 56.00% 94.12% 41.67% 22.79% 53.00% 92.69% 50.00% 22.10% 50.00% 90.82% 43.06% 18.60% 53.00% 92.52% 44.44% 16.92% 53.00% 91.92% 63.89% 18.83% 55.00% 96.25% 52.78% 11.74% 58.00% 92.11% 58.33% 18.98% 56.00% 93.65% 63.89% 17.99% 53.00% 51.82% 59.72% 15.32% 54.00% 92.64% 65.28% 40.57% 39.80% 43.48% 43.36% 43.27% 46.02% 39.81% 43.88% 36.14% 37.05%"
        },
        {
            "title": "Scene",
            "content": "22.67% 17.51% 21.44% 16.21% 17.22% 12.35% 22.31% 15.77% 13.88% 12.06%"
        },
        {
            "title": "Background\nConsistency",
            "content": "20.46% 19.89% 19.97% 19.89% 19.94% 20.31% 20.25% 20.20% 20.29% 20.24% 18.21% 18.32% 17.68% 17.84% 18.56% 18.17% 17.71% 17.98% 17.97% 18.19% 19.77% 19.06% 19.75% 20.12% 19.85% 19.11% 19.45% 19.29% 18.97% 19.22% 97.98% 97.30% 97.47% 97.70% 97.45% 97.70% 97.71% 97.55% 97.62% 97.66% Table 9: VBench evaluation result for ablation study on distillation order for MLCD and layerwise knowledge distillation. Model Final Score Aesthetic Quality Dynamic Degree Motion Smoothness Temporal Flickering Object Class Subject Consistency Imaging Quality CD-FVD MLCD + KD 76.00% 56.59% KD + MLCD 75.50% 56.38% 63.88% 54.16% 99.13% 99.12% 99.54% 99.40% 57.12% 54.67% 97.73% 97.71% 54.88% 57.97% 204.13 203.52 Model Multiple Objects Human Action Color Spatial Relationship Scene Appearance Style Temporal Style Overall Consistency Background Consistency MLCD + KD 18.97% KD + MLCD 17.22% 0.56% 93.65% 0.53% 93.14% 43.87% 39.87% 15.77% 17.65% 20.20% 20.11% 17.98% 18.01% 19.29% 19.17% 97.55% 97.69% 15 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Figure 6: Qualitative samples of ablation of distillation order. sampled from VBench prompts. We show that both MLCD and EFFICIENT-VDIT model can simliar quality on these samples. In two consecutive videos, the top shows results from MLCD + CD model followed by KD + MLCD model. 16 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile E. Attention distill on CogVideoX model We show that attention distillation also works well on the CogVideoX (Yang et al., 2024b) model. CogVideoX is based on the MM-DiT architecture, where its attention module concatenates text tokens with video tokens, which differs from Open-Sora-Plans cross attention module. This demonstrates that our method works effectively on both MM-DiT and cross attention architectures. Our experiments are conducted on the CogVideoX-5B model with 49-frame generation capability. Implementation Details CogVideoX-5B is profiled using Algorithm 1. For training, the model is trained for total of 10,000 steps, equivalent to 10 epochs of the dataset. The learning rate is set to 1e-7, and the gradient accumulation step is set to 1. The diffusion scale factor λ is set to 1. Kernel Performance We analyze the computation time for single sparse attention kernel in Table 10. The results show that as sparsity increases, computation time decreases significantly. For instance, with 2:11 attention mask, the execution time reduces to 15.16ms, achieving 1.72 speedup compared to the full mask. Table 10: CogvideoX-5B model speedup with different masks."
        },
        {
            "title": "Mask",
            "content": "Sparsity (%) Time(ms)"
        },
        {
            "title": "Speedup",
            "content": "full 1 2 3 4 6 12 0.00 14.50 29.29 38.30 48.66 60.15 74.11 26.03 24.12 23.68 20.51 17.77 14.08 9.99 1.00 1.08 1.10 1.27 1.47 1.85 2.60 Evaluation For quantitative analysis, we show the VBench evaluation results of the knowledge distillation model in Table 11. The results of our model are within 1% of the final score with no noticeable drop in several key dimensions. Our model achieves comparable performance to the original model. For qualitative analysis, we present sample visualizations in Figure 7 to demonstrate the video generation quality. These evaluations show that our method maintains similar video quality while achieving significant speedup, validating its effectiveness across different video diffusion model architectures. Table 11: CogVideoX-5B with 49 frames and 480p resolution results on VBench. r=4.0 indicates that this checkpoint was trained using the layerwise search strategy described in Algorithm 1, with threshold of r=4.0."
        },
        {
            "title": "Final\nScore",
            "content": ""
        },
        {
            "title": "Speedup",
            "content": "Base Oursr=5 77.91% 57.91% 77.15% 51.18% 76.39% 86.11% 97.83% 96.67% 97.34% 97.18% 71.99% 77.06% 92.27% 90.89% 57.78% 55.75% 1.00 1."
        },
        {
            "title": "Background\nConsistency",
            "content": "Base Oursr=5 48.62% 84.00% 86.71% 39.17% 90.00% 83.58% 48.47% 46.00% 38.01% 36.92% 22.99% 23.20% 23.22% 23.40% 26.13% 26.02% 95.01% 93.95% F. Qualitative samples of dynamic scenes and large-scale motion In this section, we compare the generation quality between the base model and the distilled model. For better demonstration of EFFICIENT-VDIT, we highly recommend viewing the video file in the supplementary material. For the figures listed below, in Fig. 8, we demonstrate that our model is capable of generating large-scale motion effects such as centralized radiating explosions. In Figs. 9 and 10, we show series of samples from VBench prompts, demonstrating our models motion generation capabilities. EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Figure 7: Qualitative samples of CogvideoX-5B (Yang et al., 2024b) distillation from its sample prompts. We show that our attention distill is capable of MM-DiT model architecture. In two consecutive videos, the top shows results from the base model, followed by the distillation model. 18 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Figure 8: Based on Open-Soras examples (Zheng et al., 2024) , we selected dynamic prompts featuring centralized explosions and radiating energy, demonstrating dramatic transitions from focal points to expansive environmental transformations, emphasizing large-scale motion. EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Figure 9: Qualitative samples of dynamic scenes from VBench prompts. We show that both MLCD and EFFICIENTVDIT model can generate dynamic videos while maintaining video quality. In three consecutive videos, the top shows results from the base model, followed by the MLCD model, and the EFFICIENT-VDIT model. 20 EFFICIENT-VDIT: Efficient Video Diffusion Transformers with Attention Tile Figure 10: Qualitative samples of dynamic scenes from VBench prompts. We show that both MLCD and EFFICIENTVDIT model can generate dynamic videos while maintaining video quality. In three consecutive videos, the top shows results from the base model, followed by the MLCD model, and the EFFICIENT-VDIT model."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "University of California, Berkeley",
        "University of California, San Diego"
    ]
}