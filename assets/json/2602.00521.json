{
    "paper_title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory",
    "authors": [
        "Junhyuk Choi",
        "Sohhyung Park",
        "Chanhee Cho",
        "Hyeonchu Park",
        "Bugeun Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 3 ] . [ 1 1 2 5 0 0 . 2 0 6 2 : r Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Junhyuk Choi 1 * Sohhyung Park 2 * Chanhee Cho 1 Hyeonchu Park 1 Bugeun Kim 1 Abstract While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce two-phase diagnostic framework for assessing reliability of LLM-as-aJudge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability 1. 1. Introduction Large Language Models (LLMs) have emerged as automated evaluators across diverse domains, commonly referred to as LLM-as-a-Judge (Liu et al., 2023; Gu et al., 2024; Li et al., 2024). This paradigm has been rapidly adopted not only in natural language processing tasks such as summarization (Fabbri et al., 2021; Crupi et al., 2025; Gao et al., 2023) and dialogue evaluation (Mehri & Eskenazi, 2020; Chan et al., 2024), but also extends to visionlanguage models (Ku et al., 2024a; Chen et al., 2024; Lee et al., 2024), and reward modeling for reinforcement learning from human feedback (Wang et al., 2024c; Xu et al., *Equal contribution 1Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea 2Department of Industrial Engineering, Seoul National University, Seoul, Republic of Korea. Correspondence to: Junhyuk Choi <chlwnsgur129@cau.ac.kr>, Bugeun Kim <bgnkim@cau.ac.kr>. Preprint. February 3, 2026. 1Source code: [Anonymized for Review]. 1 2025b). The appeal is clear: LLM judges offer scalable, cost-effective evaluation without the bottleneck of human annotation. Despite their widespread adoption, fundamental question remains unresolved: Can we trust the judgments of LLM-as-a-Judge? LLM judges are increasingly used to support critical evaluation decisions, even though principled methods for verifying the reliability of their judgments remain limited. The reliability of LLM-based evaluation can be examined through two fundamental dimensions: intrinsic consistency, which concerns whether judge behaves as stable and coherent measurement instrument under equivalent evaluation conditions, and human alignment, which concerns how closely its assessments correspond to those of human evaluators. However, existing work tends to address two dimensions separately and relies on metrics that provide limited diagnostic insight. Studies focusing on intrinsic reliability have proposed range of approaches, including inter-rater agreement (Kollitsch et al., 2024; Wang et al., 2024a), internal consistency (Schroeder & Wood-Doughty, 2025), and uncertainty quantification (Wagner et al., 2024; Xiong et al., 2024). While these methods capture partial aspects of judge behavior, they operate at the level of observed scores and cannot separate stable measurement characteristics of the judge from variation driven by the evaluated samples. Alignment, which has been widely studied in prior work, is commonly assessed through aggregate agreement signals between LLM and human judgments, typically quantified using correlation and agreement based metrics (Gu et al., 2024; Liu et al., 2023). However, agreement at the outcome level reflects similarity in evaluation results, rather than whether the judge itself functions as reliable measurement instrument. As result, the reliability of LLM judges remains insufficiently characterized under existing evaluation approaches. To address this gap, we introduce unified framework for assessing LLM judge reliability that integrates intrinsic consistency and human alignment into single diagnostic view. We leverage Item Response Theory (IRT), measurement framework widely used to analyze latent traits from observed responses, to examine LLM judging behavior. IRT models observed scores as probabilistic functions of latent traits and item characteristics, enabling judgments to be analyzed in terms of underlying evaluation signals rather than Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory outcome-level scores alone (Lord, 1952; Embretson & Reise, 2013). For rating-based judgments with ordered categorical outputs (e.g., Likert-style ratings), we adopt the Graded Response Model (GRM; Samejima (1969)), polytomous IRT model that characterizes how latent evaluation signals give rise to discrete score categories. Just as educational testing uses IRT to assess whether exam items reliably measure student ability, we use GRM to assess whether LLM judges reliably measure subject quality. Our diagnostic framework is organized into two conceptual phases. The first phase examines intrinsic consistency by applying prompt perturbations and examining whether judges produce consistent measurements under semantically equivalent variations. Phase 2 evaluates human alignment by comparing latent evaluation signals inferred from LLM judges with those of human annotators, conditioned on judges that exhibit sufficient intrinsic consistency. This sequential design ensures that alignment is interpreted only when measurement behavior is internally consistent. We evaluate the framework across diverse evaluation settings, illustrating how IRT-GRM produces interpretable diagnostic signals that characterize measurement behavior of LLM judges. By identifying multiple sources of unreliability, the framework provides practical basis for determining when LLM-based evaluation can be considered reliable. 2. Related Work LLM-as-a-Judge is now widely used as scalable alternative or complement to human evaluation (Chiang & Lee, 2023; Gu et al., 2024; Li et al., 2024; Tan et al., 2024), with recent work further systematizing this paradigm via structured design choices including Chain-of-Thought prompting (Wei et al., 2022) to elicit explicit reasoning (Wang et al., 2024b; Wagner et al., 2024; Chiang et al., 2025; Saha et al., 2025), checklist-based evaluation (Furuhashi et al., 2025), external validation tools (Findeis et al., 2025), self-improvement processes (Wu et al., 2025), and agent-based pipelines (Zhuge et al., 2025). Despite its growing adoption, concerns have been raised about whether LLM-based evaluation can produce judgments that are sufficiently reliable and consistent to substitute for human evaluation (Chehbouni et al., 2025; Bavaresco et al., 2025). Yet existing evidence offers limited guidance on whether failures in LLM judging stem from unreliable scoring or divergence from human judgment. Intrinsic Consistency Prior work has assessed consistency by examining whether an LLM judges outcomes remain stable across different inputs and evaluation settings. These include measuring inter-rater agreement across LLM evaluators (Kollitsch et al., 2024; Wang et al., 2024a), as well as evaluating internal consistency by repeating judgments under different random seeds (Patel et al., 2024) and summarizing stability with metrics such as McDonalds ω (Schroeder & Wood-Doughty, 2025). However, relying only on final scores or discrete verdicts provides limited insight into the stability and confidence of LLM judgments, and recent work has incorporated uncertainty modeling (Xu et al., 2024; Yona et al., 2024; Xiong et al., 2024), including leveraging token probabilities associated with ratings or verdicts to quantify evaluation confidence or uncertainty (Wagner et al., 2024; Xie et al., 2025; Sheng et al., 2025). In addition, several works have examined multiple dimensions of reliability in LLM-based evaluation, including structural instability (Xu et al., 2025a), rating indeterminacy (Guerdan et al., 2025), and biases that affect judging outcomes (Ye et al., 2025; Shi et al., 2025; Thakur et al., 2025b). However, these approaches capture only partial aspects of reliability without separating the judges measurement properties from sample quality, leaving researchers unable to diagnose the source of observed inconsistencies in measurements. Human Alignment Human alignment captures the extent to which LLM-based judgments agree with human evaluations, often serving as key objective of LLM-as-a-Judge benchmarks and evaluation frameworks (Liu et al., 2023; Zheng et al., 2023). It is commonly measured by comparing LLM-based and human evaluations using correlationbased metrics (Kendalls τ , Spearmans ρ, or Pearsons r) (Xu et al., 2025a; Gera et al., 2025; Chiang et al., 2025; Bavaresco et al., 2025) and agreement metrics, including percent agreement, pairwise accuracy, Cohens κ (Cohen, 1960), Krippendorffs α (Hayes & Krippendorff, 2007), or Scotts π (Scott, 1955; Furuhashi et al., 2025; Guerdan et al., 2025; Bavaresco et al., 2025; Zhang et al., 2025; Thakur et al., 2025a; Haldar & Hockenmaier, 2025). However, relying on single-point scores or discrete verdict matches may provide an incomplete picture of human alignment, as such summaries can mask disagreement and uncertainty in human evaluation. To address this, recent work explores distributional alignment that predicts human rating distributions (Chen et al., 2025) and statistical frameworks that analyze discrepancies at the distribution level (Polo et al., 2025). Moreover, recent work proposes procedural approaches to justify or guarantee human agreement, including selective evaluation with escalation (Jung et al., 2025) and statistical tests for replacing human annotators with LLM judges (Calderon et al., 2025). Yet these metrics offer limited insight into the nature of misalignment, unable to distinguish inconsistent measurements or apply different circumstances. 3. Methodology We propose framework for diagnosing LLM-as-a-judge reliability based on Item Response Theory (IRT). Our approach consists of three components: (1) fitting Graded Response Model (GRM) to separate measurement proper2 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory ties from true quality differences, (2) generating controlled prompt variations to probe sensitivity, and (3) extracting interpretable reliability metrics grounded in social science. 3.1. Graded Response Model for LLM Judges We adopt the GRM (Samejima, 1969) to jointly estimate sample quality and measurement properties. For rating scale with unique score values, the probability that an evaluator with variation of judgment prompt associates score value Ypj of evaluation subject (instance to be evaluated) with at least {1, 2, , K} is: (Ypj θj) = 1 1 + exp(αp(θj βpk)) (1) where θj is the latent genuine quality of subject j, αp R+ is the discrimination parameter for the variation p, k=1 is monotonically increasing sequence and {βpk}K of thresholds defining levels of latent quality at which the judge transitions between adjacent score values. After fitting GRM, each prompt variant (original, typo, newline, paraphrase) corresponds to its own (αp, βp), while all variants share the latent quality θj per sample. This separation allows us to attribute score variation to either prompt effects (captured by α, β) or true quality differences (θ). Also, to follow the common practice of GRM, we apply priors θj (0, 1), αp LogNormal(0, 0.5), and βpk (0, 1) with an ordered constraint (βpk < βp,k+1 for all < K). We perform posterior inference via NUTS (Hoffman et al., 2014) with multiple chains and samples. 3.2. Why Should We Focus on Latent Quality (θ)? critical challenge in comparing LLM judges is that different models use the output range of rating differently. When instructed to use 5-point scale, one model might produce all five score values while another effectively answers only three values. This inconsistency renders direct comparison of fitted model (α and β) problematic. Consider two judges evaluating the same samples on 5point scale: Judge mainly produced scores {1, 2, 3, 4, 5}, while Judge answered only {3, 4, 5}. Because fitting GRM requires at least one sample for each rating value, fitting on As result yields four threshold parameters. But, fitting on Bs result yields two threshold parameters due to the absence of rating data for 1 and 2. Thus, the information on the observed ratings might be overrepresented in α and β, which prohibits observing general tendency; we cannot ensure whether the same α and β are applicable to the missing rating values. Hence, we considered using θ. In contrast, latent quality (θ) represents an intrinsic property of the evaluation subject j, not property of the measurement instrument. As shown in equation 1, θj is shared across different rating values. So, regardless of whether the judge actually produced specific rating value or not, distribution of θj can hint at the judgment behavior of judge. This invariance makes θj the appropriate basis for comparing different judgment methods, prompts, or models. 3.3. Prompt Variation Design To assess prompt sensitivity, we apply three minimal perturbations that preserve semantic content: (1) typo introduces character-level errors in high-attention words, (2) newline inserts sentence-level line breaks, and (3) paraphrase substitutes synonyms for verbs and adjectives. These reflect common real-world variations arising from trivial edits; if judges scores fluctuate substantially under such minor changes, this signals reliability concern independent of alignment with human ratings. 3.4. Reliability Metrics Our framework extracts four metrics organized into two diagnostic phases. In Phase 1, we assess whether the LLM judge functions as reliable measurement instrument without requiring human reference. In Phase 2, applicable only after Phase 1 criteria are satisfied, we examine how the judges quality perception differs from human judgment. 3.4.1. PHASE 1: INTRINSIC CONSISTENCY Before comparing an LLM judge to humans, we must first establish whether it functions as reliable measurement. The following two metrics address this question. Prompt Consistency (CV ). reliable judge should produce consistent evaluations regardless of prompt variations. We quantify this using the within-rating variance of latent quality estimates. For each prompt variant p, we compute variance of θj within each rating value and take an average across those variances, denoting as Vp: Vp = 1 Kp 1 (cid:88) kKp VarjDp(k)(θj), (2) Dp(k) = {j Ypj = k}, Kp = {k Dp(k) = }. We then compute the coefficient of variation across prompts: CV = σV µV (3) where σV and µV are the standard deviation and the average of Vp across prompts. Note that as lower Vp indicates tighter clustering of samples within each rating value, lower CV means more consistent measurement. We regard CV < 0.1 as acceptable consistency, where variance is less than 10%. 3 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Marginal Reliability (ρ). This metric, standard in psychometric practice (Embretson & Reise, 2013), quantifies how much variance in estimated θ reflects true quality differences versus estimation uncertainty: ρ = Var(ˆθj) Var(ˆθj) + E[σ2 ] (4) where ˆθj and σ2 denotes posterior mean and variance across NUTS samples drawn from each θj. Here, the variance of such sample mean Var(ˆθj) represents true variation of latent quality, while the expected variation of σ2 indicates estimation uncertainty during NUTS sampling. Thus, low ρ suggests the model is more prone to measurement errors and lacks fundamental capability to serve as judge.For example, value of ρ = 0.7 means 70% of variance reflects true quality differences while 30% is measurement error. We establish interpretation guideline following psychometric conventions; ρ > 0.7 indicates acceptable reliability. Diagnostic Interpretation. The combination of CV and ρ enables differential diagnosis: high CV with acceptable ρ points to prompt sensitivity as the primary issue, while low ρ regardless of CV indicates fundamental model limitations. 3.4.2. PHASE 2: HUMAN ALIGNMENT Once judge passes Phase 1 criteria, we examine how its quality perception compares to human judgment. Two metrics capture different aspects of this comparison. As we only use original prompts instead of using all variants, we omit the index in the following equations for simplicity. Discrimination Breadth Ratio (θratio). We compare the range of latent quality estimates between the LLM judge and human annotators: θratio = θ(LLM) range /θ(Human) θrange = median(θj Yj = max Yj) range median(θj Yj = min Yj) (5) (6) This ratio reveals whether the LLM discriminates quality differences with similar breadth to humans. ratio less than 1 indicates the LLM perceives narrower quality spectrum it is hypersensitive, failing to distinguish samples that humans would rate differently. ratio greater than 1 indicates the LLM perceives wider spectrumit is insensitive, amplifying quality differences beyond human perception. ratio near 1 suggests comparable discrimination breadth. Note that θrange alone is uninterpretable without human reference: value of 1.5 could indicate appropriate, insufficient, or excessive discrimination depending on the human baseline. Human Alignment (DW ). To assess absolute alignment with human judgments, we compute the Wasserstein distance between LLM and human distributions of ˆθj: DW = W1(ˆθ(LLM), ˆθ(Human)) (7) We choose Wasserstein over alternatives because correlation captures only linear relationships ignoring distributional shape, and KL divergence is asymmetric and undefined for non-overlapping support. Wasserstein captures both location shift and distributional differences, providing an interpretable cost of transforming one distribution into another. Diagnostic Interpretation. The combination of θratio and DW enables nuanced comparison: θratio 1 with high DW indicates the judge discriminates with appropriate breadth but is systematically more lenient or strict, while θratio = 1 with low DW suggests the judges overall perception aligns with humans despite differing sensitivity. Both metrics deviating from ideal values indicates fundamental differences in how the LLM perceives quality. 4. Demonstration 4.1. Judge prompts and Benchmarks We select Judge methods that provide rating-based evaluations across diverse criteria. All methods were evaluated on human annotations, enabling intrinsic reliability diagnostics (Phase 1). We provide detailed description in Appendix C. NLP. We evaluate two LLM-as-a-judge methods: GEval (Liu et al., 2023) and HelpSteer-2 (Wang et al., 2024c) to mirror popular usages of LLM-based quality evaluation. We evaluated reliability of G-Eval on two datasets: SummEval (Fabbri et al., 2021) and TopicalChat (Mehri & Eskenazi, 2020). For SummEval, we use the original human annotation script from the dataset, measuring summary quality with four 5-point scales. For TopicalChat, we adapt the SummEval format with metric definitions from Mehri & Eskenazi (2020), evaluating dialogue quality with two binary scales (understandability, groundedness) and three 3point scales (naturalness, coherence, engagingness). For HelpSteer-2, we use the original prompt from Wang et al. (2024c), measuring response quality with five 5-point scales. Vision. We evaluate one VLM-as-a-judge method: VIEScore (Ku et al., 2024a) to mirror popular usages of image quality evaluation. We follow the same prompt on the same dataset. We evaluated reliability of VIEScore on ImageHub dataset (Ku et al., 2024b) containing three subsets: controlguided image generation (CIG), text-guided image editing (TIE), and mask-guided image editing (MIE). VIEScore measures image-text alignment with two 11-point scales: semantic consistency (SC) and perceptual quality (PQ). Thus, we separately examined VIEScore on these subsets. Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory"
        },
        {
            "title": "TopicalChat",
            "content": "HelpSteer-"
        },
        {
            "title": "Relevance\nConsistency\nFluency\nCoherence",
            "content": "ρ ρ Gemini-2.5 GPT-4o GPT-4o-mini Qwen3-30b Qwen3-235b Llama-4-m Llama-4-s ρ CV 0.25 0.91 0.07 0.55 0.21 0.89 0.09 0.89 CV CV 0.05 0.92 0.04 0.05 0.91 0.92 0.06 0.90 0.60 0.29 0.94 0.06 CV 0.17 0.92 0.07 0.78 0.15 0.92 0.15 0.88 CV 0.17 0.15 0.15 0. CV 0.09 0.08 0.09 0.16 CV 0.36 0.25 0.13 0.18 0.89 0.66 0.91 0.92 0.93 0.88 0.89 0.92 0.83 0.63 0.81 0.84 0.86 0.60 0.90 0. ρ ρ ρ ρ Understandability 0.18 0.39 Naturalness 0.29 0.85 Coherence 0.27 0.79 Engagingness 0.15 0.78 Groundedness 0.17 0."
        },
        {
            "title": "Helpfulness\nCorrectness\nCoherence\nComplexity\nVerbosity",
            "content": "0.03 0.86 0.10 0.72 0.16 0.68 1.08 0.87 0.17 0.87 0.16 0.48 0.20 0.28 0.57 0.11 0.14 0.84 0.17 0.52 0.72 0.20 0.07 0.71 0.12 0.08 0.84 0.30 0.11 0.76 0.12 0.24 0.67 0.40 0.16 0.89 0.30 0.04 0.83 0.21 0.49 0.80 0.81 0.79 0.70 0.67 0.60 0.54 0.81 0.74 0.03 0.18 0.13 0.21 0. 0.27 0.30 0.21 0.39 0.20 0.53 0.87 0.85 0.87 0.73 0.72 0.68 0.58 0.88 0.76 0.27 0.23 0.17 0.16 0.30 0.37 0.25 0.78 0.32 0.74 0.34 0.87 0.86 0.86 0. 0.78 0.76 0.60 0.89 0.76 0.21 0.49 0.25 0.21 0.10 0.13 0.24 0.25 0.25 0.33 0.46 0.71 0.81 0.70 0.71 0.64 0.54 0.40 0.77 0.75 0.48 0.43 0.33 0.81 0.33 0.82 0.11 0.80 0.18 0. 0.28 0.66 0.28 0.54 0.31 0.49 0.31 0.85 0.45 0."
        },
        {
            "title": "VIEScore",
            "content": "CIG-SC PQ MIE-SC PQ TIE-SC PQ 0.29 0.81 0.46 0.83 0.55 0.87 0.58 0.80 0.80 0.88 0.77 0.80 Table 1. Phase 1 intrinsic consistency results. CV denotes prompt consistency; ρ denotes marginal reliability (higher is better). Highlighted cells indicate CV 0.10 and ρ 0.70. Gemini-2.5 refers to Gemini 2.5 Flash; Llama-4-m and Llama-4-s refer to Llama-4-Maverick and Llama-4-Scout, respectively. For VIEScore, Qwen3-30B and Qwen3-235B refer to their VL variants 0.46 0.93 0.56 0.30 0.96 0.82 0.37 0.95 0.52 1.02 0.94 0.58 0.32 0.94 1.11 0.68 0.92 0.30 1.32 0.94 1.11 0.94 1.01 0.94 1.14 0.92 1.00 0.94 1.14 0.93 0.90 0.92 0.93 0.89 0.93 0.90 0.32 0.62 0.45 0.40 0.64 0. 0.37 0.47 0.94 0.61 0.43 0.67 0.36 0.17 0.54 0.32 0.56 0.84 0.93 0.93 0.94 0.93 0.95 0.93 0.92 0.94 0.95 0.90 0.95 0.91 0.94 0.94 0.94 0.94 0.93 0.92 4.2. Selected Models We select judge models based on two criteria: (1) frontier models widely adopted in LLM-as-a-Judge deployments, and (2) coverage across modalities to enable cross-modal reliability comparison across diverse benchmarks. All evaluations are conducted via the OpenRouter API (OpenRouter, 2025) with temperature set to 0 for deterministic outputs. Specifically, we used seven models from four families: Gemini 2.5 Flash (Comanici et al., 2025), GPT-4o and GPT-4o-mini (Hurst et al., 2024), Qwen3-30B-A3B-instruct and Qwen3-235B-A22B-instruct (Yang et al., 2025), and LLaMA-4-Maverick and LLaMA-4-Scout (Meta AI, 2025). Note that, for vision tasks, we used Qwen3-VL (Bai et al., 2025) instead of Qwen3 to ensure text and vision inputs. 4.3. Procedure for Analysis Step 1. Generating Prompt Variations. We generate three prompt variations for each evaluation prompt: typo, newline, and phrase. For typo variations, we select the five high-attention tokens2 from the final layer of Qwen3-8B (Yang et al., 2025) and apply character-level perturbations to those tokens using the AugLy library (Papakipos & Bitton, 2022). For newline variations, we randomly add three newlines between sentence segments. For paraphrase variations, we extract five verbs or adjectives using NLTK POS tagging (Bird & Loper, 2004), then use GPT-4o-mini to generate synonyms for those tokens. To ensure comparability across tasks, identical variations are applied to all models; details are provided in Appendix B. 2After excluding metric names, we chose five highest words. Step 2. Fitting IRT-GRM. We fit GRM using PyMC (Salvatier et al., 2016) with the NUTS sampler (Hoffman et al., 2014) via NumPyro (Phan et al., 2019) backend. We run 4 chains with 1000 warmup and 1000 sampling iterations, setting target acceptance rate to 0.95. For binary scale, we fit 2-parameter logistic model instead as GRM assumes > 2. Detailed hyperparameters are provided in Appendix A. 5. Phase 1: Instinsic Consistency Table 1 presents the Phase 1 diagnostic results, where we highlight results meeting consistency criteria: CV 0.10 and ρ 0.7. With two metrics, we analyze LLM-as-a-judge methods along three axes: modality, model, and task. 5.1. Prompt Consistency (CV ) Modality: Vision is more prompt-sensitive than NLP. NLP judges on SummEval, TopicalChat, or HelpSteer-2 generally maintain CV < 0.30, with several model-criterion pairs achieving CV < 0.10. In contrast, VIEScore exhibits substantially higher CV , ranging from 0.16 to 1.32. This gap is most pronounced for Gemini-2.5, which showed score from 0.03 to 0.29 on NLP tasks but exceeds CV > 1.0 across all VIEScore subtasks. These results suggest that vision-language evaluation is inherently more sensitive to prompt wording than text-based evaluation. Model: Scale improves prompt robustness in NLP but not in vision. In NLP benchmarks, larger models consistently achieve lower CV . Qwen3-235B outperforms Qwen330B on SummEval (Relevance: 0.09 vs. 0.17, Fluency: 0.09 5 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory"
        },
        {
            "title": "Detail Prompt",
            "content": "Detail Prompt + CoT Likert 5 Likert 7 Gemini-2.5 GPT-4o GPT-4o-mini Qwen3-30b Qwen3-235b Llama-4-m Llama-4-s ρ CV ρ ρ ρ ρ ρ ρ Understandability 0.12 0.36 Naturalness 0.12 0.84 Coherence 0.20 0.78 Engagingness 0.11 0.76 Groundedness 0.04 0.73 Understandability 0.13 0.50 Naturalness 0.18 0.84 Coherence 0.21 0.82 Engagingness 0.12 0.81 Groundedness 0.06 0."
        },
        {
            "title": "Naturalness\nCoherence\nEngagingness",
            "content": "0.20 0.93 0.20 0.90 0.13 0.92 0.08 0.95 0.49 0.87 0.34 0.93 CV CV 0.21 0.43 0.11 0.10 0.83 0.21 0.10 0.78 0.21 0.16 0.69 0.17 0.03 0.68 0.07 0.22 0.58 0.28 0.01 0.85 0.47 0.10 0.79 0.37 0.06 0.73 0.39 0.14 0.49 0.16 0.15 0.95 0.32 0.09 0.91 0.13 0.19 0.91 0.10 0.15 0.95 0.32 0.31 0.95 0.17 0.18 0.96 0. 0.56 0.64 0.84 0.70 0.71 0.60 0.70 0.81 0.75 0.71 0.91 0.95 0.93 0.87 0.97 0.94 CV 0.12 0.14 0.13 0.13 0.10 0.14 0.23 0.18 0.16 0. 0.12 0.12 0.16 0.22 0.31 0.25 0.67 0.88 0.83 0.80 0.73 0.64 0.87 0.82 0.79 0.68 0.92 0.93 0.94 0.90 0.96 0. CV 0.14 0.13 0.21 0.23 0.22 0.44 0.06 0.10 0.10 0.10 0.49 0.28 0.25 0.34 0.19 0.26 0.40 0.87 0.81 0.80 0.69 0.46 0.87 0.84 0.85 0. 0.94 0.93 0.94 0.93 0.97 0.96 CV 0.13 0.19 0.14 0.10 0.22 0.14 0.06 0.06 0.03 0.03 0.17 0.14 0.27 0.26 0.22 0. 0.47 0.84 0.84 0.82 0.72 0.67 0.87 0.82 0.84 0.62 0.94 0.92 0.94 0.81 0.95 0.94 CV 0.43 0.36 0.44 0.72 0.25 0.81 0.14 0.61 0.14 0.71 0.18 0.66 0.09 0.80 0.53 0.79 0.25 0.67 0.17 0. 0.47 0.92 0.13 0.92 0.07 0.91 0.24 0.90 0.14 0.94 0.26 0."
        },
        {
            "title": "Naturalness\nCoherence\nEngagingness",
            "content": "Detail Prompt + CoT + Likert 5 0.55 0.92 0.33 0.89 0.27 0.88 Table 2. Ablation results on TopicalChat with varying instruction detail, chain-of-thought prompting, and rating scales. CV denotes prompt consistency (lower is better); ρ denotes marginal reliability (higher is better). Highlighted cells indicate CV 0.10 and ρ 0.70. Gemini-2.5 refers to Gemini 2.5 Flash; Llama-4-m and Llama-4-s refer to Llama-4-Maverick and Llama-4-Scout, respectively. 0.06 0.94 0.29 0.18 0.89 0.10 0.08 0.87 0.26 0.25 0.93 0.18 0.89 0.15 0.90 0.88 0.94 0.92 0.22 0.19 0. 0.11 0.17 0.17 0.92 0.90 0.94 0.95 0.92 0.94 0.16 0.09 0.07 0.93 0.91 0.90 vs. 0.15, Coherence: 0.16 vs. 0.22). GPT-4o similarly surpasses GPT-4o-mini (SummEval Fluency: 0.06 vs. 0.60; HelpSteer-2 Helpfulness: 0.08 vs. 0.30). However, this scaling effect does not transfer to vision: in VIEScore, Qwen330B and Qwen3-235B show comparable CV , and GPT4o-mini occasionally matches or exceeds GPT-4o. This modality-dependent scaling suggests that prompt robustness in vision-language evaluation may rely on capabilities distinct from those improved by scale. Task: Summarization is most stable; vision tasks are not. Within NLP, judgment criteria in SummEval achieves the lowest CV overall, with most models maintaining CV < 0.20. Those in TopicalChat and HelpSteer-2 show moderate variability, though HelpSteer-2 Complexity exhibits notable instability for Gemini-2.5 (1.08). VIEScore consistently yields high CV across all models and subtasks, indicating that prompt sensitivity is task-level characteristic rather than model-specific limitation. 5.2. Marginal Reliability (ρ) Modality: Vision achieves high reliability. Despite high prompt sensitivity, VIEScore achieves the highest ρ values among all benchmarks (0.800.96). This apparent paradox suggests that while vision-language judges are sensitive to exact prompt wording, they produce reliable quality orderings once specific prompt is fixed. NLP benchmarks show more variance: SummEval achieves consistently high ρ (0.810.94), while TopicalChat and HelpSteer-2 exhibit criterion-dependent reliability. Model: Scale law emerges in NLP but not in vision. In NLP benchmarks, larger models consistently achieve higher ρ. Qwen3-235B outperforms Qwen3-30B on SummEval (0.890.92 vs. 0.860.90) and HelpSteer-2 Helpfulness (0.78 vs. 0.72) and Correctness (0.76 vs. 0.68). Similarly, GPT-4o surpasses GPT-4o-mini on HelpSteer-2 across all criteria (e.g., Helpfulness: 0.84 vs. 0.67, Correctness: 0.76 vs. 0.60). However, this scaling effect does not transfer to vision tasks: in VIEScore, Qwen3-30B and Qwen3235B achieve comparable ρ (0.900.95), and GPT-4o-mini occasionally matches or surpasses GPT-4o. This modalitydependent scaling suggests that vision-language evaluation may rely on capabilities which do not scale with model size. Task: TopicalChat Understandability is least reliable. In NLP, TopicalChat Understandability consistently yields the lowest ρ across all models (0.340.53), substantially below the conventional threshold of ρ > 0.7. Other TopicalChat criteria (Naturalness, Coherence, Engagingness) achieve moderate-to-high reliability (0.700.87), while Groundedness remains borderline (0.700.73). HelpSteer-2 shows criterion-dependent patterns: Helpfulness and Complexity achieve high ρ (0.660.89), while Coherence underperforms (0.400.68). Criteria in SummEval maintains consistently high reliability across all criteria. 5.3. Discussion Phase 1 suggests that consistency of current LLM-as-ajudge criteria might depend on how the prompt specifies evaluation criteria. As shown in Table 1, there is no free lunch; no model showed acceptable consistency across all 6 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory"
        },
        {
            "title": "TopicalChat",
            "content": "HelpSteer-"
        },
        {
            "title": "Relevance\nConsistency\nFluency\nCoherence",
            "content": "Gemini-2.5 GPT-4o GPT-4o-mini Qwen3-30b Qwen3-235b Llama-4-m Llama-4-s θratio DW θratio DW θratio DW θratio DW θratio DW θratio DW θratio DW 0.96 0.30 0.28 1.30 0.34 0.55 1.57 0.57 1.36 0.26 0.49 1.97 0.57 1.71 0.48 0.27 1.53 0.31 1.24 0.26 1.55 0.30 1.39 1.89 0.55 2.20 1.63 0.42 2.29 1.50 0.27 1.66 1.07 1.29 1.33 0.98 0.25 0.41 0.47 0.24 1.47 1.36 1.76 1.55 0.21 0.42 0.50 0. 1.30 1.65 1.57 1.30 0.42 0.34 0.49 0.30 Understandability 2.46 0.33 Naturalness 1.71 0.48 Coherence 1.79 0.45 Engagingness 2.21 0.42 Groundedness 2.51 0."
        },
        {
            "title": "Helpfulness\nCorrectness\nCoherence\nComplexity\nVerbosity",
            "content": "1.80 0.33 1.46 0.28 1.10 0.25 1.26 0.34 1.02 0.37 2.59 0.33 2.59 3.19 0.54 2.52 2.41 0.55 2.01 3.07 0.44 2.16 2.38 0.56 2.41 1.83 0.33 1.76 1.55 0.29 1.77 1.29 0.23 1.34 1.16 0.30 1.50 1.21 0.45 1.46 0.34 0.55 0.43 0.43 0.57 0.32 0.32 0.15 0.32 0.17 2.56 2.12 1.93 2.08 2. 1.61 1.52 1.03 1.10 0.99 0.36 0.41 0.43 0.51 0.54 0.31 0.29 0.16 0.46 0.53 2.89 1.96 1.97 2.21 2.38 1.58 1.60 1.48 1.30 1.18 0.36 0.50 0.53 0.54 0. 0.33 0.30 0.15 0.36 0.52 2.29 2.31 1.88 2.05 2.47 1.86 1.76 1.24 1.32 1.40 0.31 0.50 0.43 0.44 0.57 0.30 0.34 0.29 0.28 0.22 2.75 2.35 1.97 2.75 2. 1.65 1.75 1.20 1.40 1.33 0.33 0.41 0.43 0.51 0.55 0.31 0.34 0.21 0.30 0."
        },
        {
            "title": "VIEScore",
            "content": "CIG-SC PQ MIE-SC PQ TIE-SC PQ 0.38 0.33 0.52 0.47 0.51 0.58 Table 3. Phase 2 human alignment results. θratio denotes discrimination breadth ratio; DW denotes Wasserstein distance between LLM and human θ distributions. Gemini-2.5 refers to Gemini 2.5 Flash; Llama-4-m and Llama-4-s refer to Llama-4-Maverick and Llama-4-Scout, respectively. For VIEScore, Qwen3-30B and Qwen3-235B refer to their VL variants. 1.92 0.47 1.48 3.03 0.50 3.11 2.42 0.43 1.96 1.98 0.39 1.84 2.06 0.51 2.29 4.40 0.60 4.14 2.08 0.45 3.35 0.51 1.87 0.55 1.56 0.49 1.99 0.51 3.86 0.61 0.45 0.43 0.55 0.43 0.45 0.52 0.43 0.42 0.56 0.45 0.52 0. 1.48 2.76 2.00 1.28 1.85 3.47 1.37 2.61 1.79 1.73 2.09 3.92 1.74 3.09 2.15 1.75 2.20 3.75 0.86 2.03 1.60 1.12 1.71 3.91 0.46 0.46 0.49 0.43 0.47 0.60 0.57 0.56 0.56 0.43 0.50 0. criteria when we quantified CV and ρ. Thus, one should verify the effect of prompts and models when suggesting new LLM judgment tasks. It is still questionable why such difference occur. To diagnose the cuase, we conduct two simple ablation studies on evaluation criteria, using TopicalChat3. First, we test whether detailed evaluation instructions or chain-of-thought prompting improve consistency (CV ). Second, we examine whether the rating scales used in existing criteria are adequate for reliable judging (ρ) by comparing the original 3-point scale with alternative scales: 5-point and 7-point. The result of ablation study suggests proper way to design judgment prompts, as shown in Table 2. First, instruction specification might affect consistency CV . Providing detailed prompts substantially reduces CV across criteria, and adding chain-of-thought prompting yields further improvement (e.g., Naturalness: GPT-4o achieves CV = 0.01, Qwen3-30b and Llama-4-m achieve CV = 0.06). However, ρ shows only marginal gains, suggesting that detailed instructions stabilize prompt sensitivity but do not fundamentally improve discriminative capacity. Second, rating scales might affect marginal reliablity ρ. The 5-point scale shows modest improvement in ρ for graded criteria (Naturalness: 0.910.95, Coherence: 0.900.95, Engagingness: 0.910.94), but the 7-point scale does not yield consistent further gains and occasionally decreases reliability."
        },
        {
            "title": "These results emphasizes that our framework is suitable to\ncapture subtle differences due to prompt engineering and",
            "content": "3For the result on VIEScore, see Appendix scale adjustment. Though prior work reported similar observations that more explicit instructions or chain-of-thought prompting can lead to increased evaluation stability (Wang et al., 2024b; Wagner et al., 2024; Saha et al., 2025), our result further implies guideline for designing good judgment criteria: More detailed instructions tend to stabilize prompt consistency (CV ), while scale adjustment affects measurement reliability (ρ). 6. Phase 2: Human Alignment Having established intrinsic consistency in Phase 1, we now examine how LLM judges quality perception compares to human judgment. Table 3 presents the Phase 2 diagnostic results with two metrics. (1) θratio (discrimination breadth ratio) measures whether LLMs perceive quality differences with similar breadth to humans: θratio < 1, θratio 1 and θratio > 1 indicate hypersensitive, near-human, insensitive calibration. (2) DW (Wasserstein distance) measures distributional divergence from human judgments, where lower values indicate closer alignment. Similar to Phase 1, we analyze both metrics along the same three axes. 6.1. Discrimination Breadth Ratio (θratio) Modality: Vision is more insensitive than NLP. VIEScore exhibits the highest θratio values, particularly for perceptual quality (PQ) subtasks (2.034.40), indicating that VLMs amplify quality differences far beyond human perception. Criteria in NLP benchmarks show more moderate insensitivity (1.02.5), with several criteria approaching near-human calibration (θratio 1). 7 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Model: No consistent scaling effect on calibration. Unlike Phase 1, model scale does not show consistent effects on θratio. Qwen3-235B and Qwen3-30B achieve comparable values across benchmarks, as do GPT-4o and GPT-4o-mini. This suggests that discrimination calibration is depending on task rather than model capacity. Task: HelpSteer-2 shows most appropriate calibration. HelpSteer-2 Coherence and Verbosity most frequently achieve θratio 1 (e.g., Qwen3-30b: 1.03, 0.99; Gemini2.5: 1.10, 1.02). SummEval Relevance and Coherence also show near-human calibration for select models. In contrast, TopicalChat and VIEScore PQ consistently exhibit insensitivity across all models, suggesting these tasks inherently elicit amplified quality perception in LLMs. 6.2. Distributional Alignment (DW ) Modality: No systematic modality effect. DW shows no clear difference between NLP and vision benchmarks. Both modalities exhibit similar ranges (0.150.61), with withinmodality difference exceeding between-modality difference. Model: Benchmark effects dominate model effects. All models exhibit similar DW patterns within each benchmark, with minimal variation across model families or scales. This suggests that distributional alignment is primarily determined by task characteristics. Task: HelpSteer-2 Coherence achieves closest alignment. HelpSteer-2 Coherence achieves the lowest DW across all benchmarks (0.150.29), indicating closest distributional alignment with human judgments. SummEval Coherence also performs well (0.240.32). VIEScore TIE-PQ shows the highest DW (0.510.61), consistent with its extremely high θratio values. 6.3. Discussion Phase 2 analysis reveals that θratio and DW capture complementary but incomplete aspects of human alignment. θratio identifies systematic insensitivity (θratio > 1) across nearly all model-task combinations, indicating that LLM judges consistently perceive quality differences more broadly than humans. DW quantifies overall distributional divergence but shows no clear effect of modality. However, these aggregate metrics mask critical distinction that emerges only through score-level analysis. Figure 6 visualizes the relationship between human scores and estimated θ for representative NLP and vision tasks. In NLP tasks (SummEval-Coherence), all models exhibit monotonically increasing θ trajectories parallel to humans. The slopes differ, but the direction is preserved. This pattern indicates calibration mismatch: LLMs and humans perceive the same Figure 1. Median latent quality (θ) by human score for SummEvalCoherence (up) and VIEScore-MIE PQ (down). Each line represents different judge model; the black line denotes human. quality construct with different scales. In contrast, vision tasks (VIEScore-MIE PQ) reveal non-monotonic patterns where model θ values decrease or plateau. This suggests validity gap between VLMs and humans: VLMs may evaluate different aspects of quality than what humans evaluate. This interpretation is evidence by sample-wise Pearson correlations between human rating scores and machine rating scores (Pearson (1896); Appendix E): NLP tasks achieve moderate correlations (r = 0.20.5), while vision tasks approach zero. Together, these findings suggest that θratio > 1 carries different implications depending on the underlying pattern. In NLP tasks, the calibration mismatch indicates that LLMs measure the same construct as humans but with different scales. In vision tasks, the calibration mismatch indicates the validity gap. We provide score-level visualizations for all benchmarks in Appendix F. 7. Conclusion We presented framework for diagnosing reliability of LLM-as-a-Judge by adapting Item Response Theory to the evaluation setting, where prompt variations serve as measurement items and latent quality θ captures perceived sample quality. From this formulation, we derive four interpretable metrics organized into two sequential phases. Phase 1 metrics (CV and ρ) identify whether inconsistency stems from prompt sensitivity or inadequate use of the rating scale, guiding practitioners toward appropriate adjustments. Phase 2 metrics (θratio and DW ) reveal how LLM judges differ from humans, distinguishing calibration mismatch from validity gap. This framework provides practitioners with principled tool for diagnosing LLM judge reliability. 8 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory"
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to promote more responsible use of LLMas-a-Judge by offering diagnostic framework that clarifies when automated evaluators behave as reliable measurement instruments and when they do not. By separating intrinsic consistency from human alignment, the proposed approach helps mitigate the risk of over-trusting LLM-based evaluation in benchmarking and model development. At the same time, our study has several limitations that constrain its broader impact. The framework is currently applicable only to point-scale (rating-based) judgments and does not directly extend to pairwise or open-ended evaluation settings. While we analyze consistency and alignment, we do not assess whether the internal reasoning or explanations produced by LLM judges are faithful to their actual decision processes. In addition, our experiments focus on limited set of languages and modalities; multilingual evaluation and wider range of modalities remain unexplored and may exhibit different reliability characteristics. Addressing these limitations is an important direction for future work to ensure that reliability diagnostics for LLM-as-a-Judge generalize across settings and do not introduce unintended biases or misuse."
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report, 2025. URL https://arxiv.org/abs/2511.21631. Bavaresco, A., Bernardi, R., Bertolazzi, L., Elliott, D., Fernández, R., Gatt, A., Ghaleb, E., Giulianelli, M., Hanna, M., Koller, A., Martins, A., Mondorf, P., Neplenbroek, V., Pezzelle, S., Plank, B., Schlangen, D., Suglia, A., Surikuchi, A. K., Takmaz, E., and Testoni, A. LLMs instead of human judges? large scale empirical study across 20 NLP evaluation tasks. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 238255, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2527. doi: 10.18653/v1/2025.acl-short.20. URL https: //aclanthology.org/2025.acl-short.20/. Bird, S. and Loper, E. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pp. 214217, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031/. Calderon, N., Reichart, R., and Dror, R. The alternative annotator test for LLM-as-a-judge: How to statistically justify replacing human annotators with LLMs. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1605116081, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long. 782. URL https://aclanthology.org/2025. acl-long.782/. Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., Fu, J., and Liu, Z. Chateval: Towards better LLM-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=FQepisCUWu. Chehbouni, K., Haddou, M., Cheung, J. C., and Farnadi, G. Neither valid nor reliable? investigating the use of LLMs as judges. In The Thirty-Ninth Annual Conference on Neural Information Processing Systems Position Paper Track, 2025. URL https://openreview.net/ forum?id=yqKfMr0yvY. Chen, D., Chen, R., Zhang, S., Wang, Y., Liu, Y., Zhou, H., Zhang, Q., Wan, Y., Zhou, P., and Sun, L. Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024. Chen, L., Zhang, Z., Tan, H., Dai, Q., Yang, H., Dong, Z., and Chen, X. Distributional llm-as-a-judge. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Chiang, C.-H. and Lee, H.-y. Can large language models be an alternative to human evaluations? In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1560715631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.870. URL https://aclanthology.org/ 2023.acl-long.870/. Chiang, C.-H., Lee, H.-y., and Lukasik, M. TRACT: Regression-aware fine-tuning meets chain-of-thought reasoning for LLM-as-a-judge. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of 9 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 29342952, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.147. URL https: //aclanthology.org/2025.acl-long.147/. Cohen, J. coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37 46, 1960. doi: 10.1177/001316446002000104. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Crupi, G., Tufano, R., Velasco, A., Mastropaolo, A., Poshyvanyk, D., and Bavota, G. On the effectiveness of llm-asa-judge for code generation and summarization. IEEE Transactions on Software Engineering, 2025. Embretson, S. E. and Reise, S. P. Item response theory for psychologists. Psychology Press, 2013. Fabbri, A. R., Kryscinski, W., McCann, B., Xiong, C., Socher, R., and Radev, D. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409, 2021. doi: 10. 1162/tacl_a_00373. URL https://aclanthology. org/2021.tacl-1.24/. Findeis, A., Weers, F., Yin, G., Ye, K., Pang, R., and Gunter, T. Can external validation tools improve annotation quality for LLM-as-a-judge? In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1599716020, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.779. URL https: //aclanthology.org/2025.acl-long.779/. Furuhashi, M., Nakayama, K., Kodama, T., and Sugawara, S. Are checklists really useful for automatic evaluation of generative tasks? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1064110664, 2025. Gao, M., Ruan, J., Sun, R., Yin, X., Yang, S., and Wan, X. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554, 2023. Gera, A., Boni, O., Perlitz, Y., Bar-Haim, R., Eden, L., and Yehudai, A. Justrank: Benchmarking llm judges for system ranking. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 682712, 2025. Gu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y., Ma, S., Liu, H., et al. survey on llm-as-ajudge. The Innovation, 2024. Guerdan, L., Barocas, S., Holstein, K., Wallach, H., Wu, S., and Chouldechova, A. Validating LLM-as-a-judge systems under rating indeterminacy. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=ZwDMrArTBg. Haldar, R. and Hockenmaier, J. Rating roulette: Selfinconsistency in LLM-as-a-judge frameworks. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2025, pp. 24986 25004, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-889176-335-7. doi: 10.18653/v1/2025.findings-emnlp. 1361. URL https://aclanthology.org/2025. findings-emnlp.1361/. Hayes, A. F. and Krippendorff, K. Answering the call for standard reliability measure for coding data. Communication Methods and Measures, 1(1):7789, 2007. doi: 10.1080/19312450709336664. URL https://doi. org/10.1080/19312450709336664. Hoffman, M. D., Gelman, A., et al. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):15931623, 2014. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jung, J., Brahman, F., and Choi, Y. Trust or escalate: LLM judges with provable guarantees for human agreement. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=UHPnqSTBPO. Kollitsch, L., Eredics, K., Marszalek, M., Rauchenwald, M., Brookman-May, S. D., Burger, M., Körner-Riffard, K., and May, M. How does artificial intelligence master urological board examinations? comparative analysis of different large language models accuracy and reliability in the 2022 in-service assessment of the european board of urology. World Journal of Urology, 42 (1):20, January 2024. ISSN 1433-8726. doi: 10.1007/ s00345-023-04749-6. URL https://doi.org/10. 1007/s00345-023-04749-6. 10 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Ku, M., Jiang, D., Wei, C., Yue, X., and Chen, W. VIEScore: Towards explainable metrics for conditional image synthesis evaluation. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1226812290, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.663. URL https: //aclanthology.org/2024.acl-long.663/. Ku, M., Li, T., Zhang, K., Lu, Y., Fu, X., Zhuang, W., and Chen, W. Imagenhub: Standardizing the evaluation of In The Twelfth conditional image generation models. International Conference on Learning Representations, 2024b. URL https://openreview.net/forum? id=OuV9ZrkQlc. Lee, S., Kim, S., Park, S., Kim, G., and Seo, M. Prometheusvision: Vision-language model as judge for fine-grained evaluation. In Findings of the association for computational linguistics ACL 2024, pp. 1128611315, 2024. Li, H., Dong, Q., Chen, J., Su, H., Zhou, Y., Ai, Q., Ye, Z., and Liu, Y. Llms-as-judges: comprehensive survey on llm-based evaluation methods. arXiv preprint arXiv:2412.05579, 2024. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C. G-eval: NLG evaluation using gpt-4 with better human alignment. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 25112522, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 153. URL https://aclanthology.org/2023. emnlp-main.153/. Lord, F. theory of test scores. Psychometric monographs, 1952. Mehri, S. and Eskenazi, M. USR: An unsupervised and reference free evaluation metric for dialog generation. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 681707, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.64. URL https: //aclanthology.org/2020.acl-main.64/. Llama 4 Model Cards and Prompt Forhttps://www.llama.com/docs/ Meta AI. mats. model-cards-and-prompt-formats/ llama4/, 2025. OpenRouter. Openrouter api: Web search feature. https: //openrouter.ai, 2025. Papakipos, Z. and Bitton, J. Augly: Data augmentations for robustness. arXiv preprint arXiv:2201.06494, 2022. Patel, B., Chakraborty, S., Suttle, W. A., Wang, M., Bedi, A. S., and Manocha, D. Aime: Ai system CoRR, optimization via multiple llm evaluators. abs/2410.03131, 2024. URL https://doi.org/10. 48550/arXiv.2410.03131. Pearson, K. Vii. mathematical contributions to the theory of evolution.iii. regression, heredity, and panmixia. Philosophical Transactions of the Royal Society of London. Series A, containing papers of mathematical or physical character, (187):253318, 1896. Phan, D., Pradhan, N., and Jankowiak, M. Composable effects for flexible and accelerated probabilistic programming in numpyro. arXiv preprint arXiv:1912.11554, 2019. Polo, F. M., Wang, X., Yurochkin, M., Xu, G., Banerjee, M., and Sun, Y. Bridging human and LLM judgments: Understanding and narrowing the gap. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=bEP87LNTfX. Saha, S., Li, X., Ghazvininejad, M., Weston, J. E., and Wang, T. Learning to plan & reason for evaluation with thinking-LLM-as-a-judge. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=PNRznmmWP7. Salvatier, J., Wiecki, T. V., and Fonnesbeck, C. Probabilistic programming in python using pymc3. PeerJ Computer Science, 2:e55, 2016. Samejima, F. Estimation of latent ability using response pattern of graded scores. Psychometrika, 34(S1):197, 1969. Schroeder, K. and Wood-Doughty, Z. Can you trust llm judgments? reliability of llm-as-a-judge, 2025. URL https://arxiv.org/abs/2412.12509. Scott, W. A. Reliability of content analysis: The case of nominal scale coding. Public opinion quarterly, pp. 321 325, 1955. Sheng, H., Liu, X., He, H., Zhao, J., and Kang, J. Analyzing uncertainty of llm-as-a-judge: Interval evaluations with conformal prediction. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1129711339, 2025. Shi, L., Ma, C., Liang, W., Diao, X., Ma, W., and Vosoughi, Judging the judges: systematic study of poIn Inui, K., Sakti, S. sition bias in LLM-as-a-judge. Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory S., Wang, H., Wong, D. F., Bhattacharyya, P., Banerjee, B., Ekbal, A., Chakraborty, T., and Singh, D. P. (eds.), Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, pp. 292314, Mumbai, India, December 2025. The Asian Federation of Natural Language Processing and The Association for Computational Linguistics. ISBN 979-8-89176-298URL https://aclanthology.org/2025. 5. ijcnlp-long.18/. Tan, Z., Li, D., Wang, S., Beigi, A., Jiang, B., Bhattacharjee, A., Karami, M., Li, J., Cheng, L., and Liu, H. Large language models for data annotation and synthesis: survey. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 930957, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.54. URL https:// aclanthology.org/2024.emnlp-main.54/. Thakur, A. S., Choudhary, K., Ramayapally, V. S., Vaidyanathan, S., and Hupkes, D. Judging the judges: Evaluating alignment and vulnerabilities in LLMs-asjudges. In Arviv, O., Clinciu, M., Dhole, K., Dror, R., Gehrmann, S., Habba, E., Itzhak, I., Mille, S., Perlitz, Y., Santus, E., Sedoc, J., Shmueli Scheuer, M., Stanovsky, G., and Tafjord, O. (eds.), Proceedings of the Fourth Workshop on Generation, Evaluation and Metrics (GEM²), pp. 404430, Vienna, Austria and virtual meeting, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-261-9. URL https://aclanthology. org/2025.gem-1.33/. Thakur, A. S., Choudhary, K., Ramayapally, V. S., Vaidyanathan, S., and Hupkes, D. Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges. In Proceedings of the Fourth Workshop on Generation, Evaluation and Metrics (GEM2), pp. 404430, 2025b. Wagner, N., Desmond, M., Nair, R., Ashktorab, Z., Daly, E. M., Pan, Q., Cooper, M. S., Johnson, J. M., and Geyer, W. Black-box uncertainty quantification method for llmas-a-judge. arXiv preprint arXiv:2410.11594, 2024. Wang, L., Chen, X., Deng, X., Wen, H., You, M., Liu, W., Li, Q., and Li, J. Prompt engineering in consistency and reliability with the evidence-based guideline for llms. npj Digital Medicine, 7(1):41, 2 2024a. ISSN 2398-6352. doi: 10.1038/s41746-024-01029-4. URL https:// doi.org/10.1038/s41746-024-01029-4. J., and Li, X. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024b. Wang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert, D., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Helpsteer 2: Open-source dataset for training top-performing reward models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024c. URL https://openreview. net/forum?id=PvVKUFhaNy. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J. Wu, T., Yuan, W., Golovneva, O., Xu, J., Tian, Y., Jiao, J., Weston, J. E., and Sukhbaatar, S. Meta-rewarding language models: Self-improving alignment with LLMas-a-meta-judge. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1153711554, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main. 583. URL https://aclanthology.org/2025. emnlp-main.583/. Xie, Q., Li, Q., Yu, Z., Zhang, Y., Zhang, Y., and Yang, L. An empirical analysis of uncertainty in large language model evaluations. In The Thirteenth International Conference on Learning Representations, 2025. URL https:// openreview.net/forum?id=J4xLuCt2kg. Xiong, M., Hu, Z., Lu, X., LI, Y., Fu, J., He, J., and Hooi, B. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=gjeQKFxFpZ. Xu, T., Wu, S., Diao, S., Liu, X., Wang, X., Chen, Y., and Gao, J. SaySelf: Teaching LLMs to express confidence with self-reflective rationales. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 59855998, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 343. URL https://aclanthology.org/2024. emnlp-main.343/. Wang, T., Kulikov, I., Golovneva, O., Yu, P., Yuan, W., Dwivedi-Yu, J., Pang, R. Y., Fazel-Zarandi, M., Weston, Xu, Y., Ruis, L., Rocktäschel, T., and Kirk, R. tigating non-transitivity in LLM-as-a-judge. InvesIn FortyDiagnosing the Reliability of LLM-as-a-Judge via Item Response Theory 2025. URL https://openreview.net/forum? id=Nn9POI9Ekt. second International Conference on Machine Learning, 2025a. URL https://openreview.net/forum? id=clJIQ4TKR0. Xu, Z., Lu, Q., Zhang, Q., Qiu, L., Hong, I., Yu, C., Yao, W., Liu, Y., Jiang, H., Li, L., Yun, H., and Zhao, T. Ask strong LLM judge when your reward model is uncertain. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. URL https: //openreview.net/forum?id=SkdhLeuq8P. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Ye, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N., Gao, T., Geyer, W., Huang, C., Chen, P.-Y., Chawla, N. V., and Zhang, X. Justice or prejudice? quantifying biases in LLM-as-a-judge. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=3GTtZFiajM. Yona, G., Aharoni, R., and Geva, M. Can large language models faithfully express their intrinsic uncertainty in words? In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 7752 7764, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.443. URL https://aclanthology. org/2024.emnlp-main.443/. Zhang, Q., Wang, Y., Jiang, Y., Li, L., Wu, C., Wang, Y., Jiang, X., Shang, L., Tang, R., Lyu, F., and Ma, C. Crowd comparative reasoning: Unlocking comprehensive evaluations for LLM-as-a-judge. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 50595074, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.252. URL https: //aclanthology.org/2025.acl-long.252/. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge In Thirty-seventh with MT-bench and chatbot arena. Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=uccHPGDlao. Zhuge, M., Zhao, C., Ashley, D. R., Wang, W., Khizbullin, D., Xiong, Y., Liu, Z., Chang, E., Krishnamoorthi, R., Tian, Y., Shi, Y., Chandra, V., and Schmidhuber, J. Agent-as-a-judge: Evaluate agents with agents. In Fortysecond International Conference on Machine Learning, 13 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory"
        },
        {
            "title": "The Use of Large Language Models",
            "content": "We used AI assistance tools during the writing process of this manuscript. Specifically, we employed Grammarly for grammar checking, and GPT-5 for language polishing and improving clarity of expression. These tools were used for editorial purposes. A. Details of Graded Response Model for LLM Judges A.1. Model Fitting We implement GRM using PyMC 5.25.1 with ArviZ 0.23.0 for diagnostics. For posterior inference, we use the NUTS sampler with 4 chains, 1000 warmup iterations, 1000 sampling iterations, and target acceptance rate of 0.95. We use adapt_diag initialization and fix the random seed to 42 for reproducibility. We apply the following priors: θj (0, 1) for latent quality, αp LogNormal(0, 0.5) for discrimination parameters, and βpk (0, 1) with an ordered constraint (βpk < βp,k+1 for all < K) for threshold parameters. For binary responses (K=2), we fit 2-parameter logistic model with the same priors for θ and α, and βp (0, 1) without ordering. For convergence diagnostics, we compute ˆR and effective sample size (ESS) using ArviZ. We also compute leave-one-out cross-validation (LOO) with Pareto-k diagnostics to identify influential observations. A.2. Implementation Details All experiments are conducted with Python 3.10.19 on an AMD EPYC 7313 16-Core Processor. B. Details of Prompt Variation All prompt variations are generated with fixed random seed (42). Once generated, variations are saved and applied identically to all models within each dataset. B.1. Typo We first compute attention weights from the final layer of Qwen3-8B for all tokens in the prompt. After excluding placeholders (e.g., {context}) and protected keywords (metric names, json, format, etc.), we select the top-5 high-attention tokens. We then apply character-level perturbations to these tokens using the AugLy library with aug_word_p=1.0 and aug_char_p=0.5. When the library fails to generate valid typo, we swap two adjacent characters as fallback. B.2. Newline We split the prompt into segments by newline characters, then randomly select up to three positions to insert additional newlines. Placeholders containing evaluation samples remain unchanged. B.3. Paraphrase We extract verbs (VB, VBD, VBG, VBN, VBP, VBZ) and adjectives (JJ) using NLTK POS tagger. After excluding protected keywords, we prompt GPT-4o-mini to generate synonyms for five tokens. The model is instructed to replace only the selected tokens while preserving the original part-of-speech and meaning. B.4. Implementation Details We use Python 3.10, AugLy 1.0.0, NLTK 3.9.2, and Transformers 4.51.3. Attention extraction is performed on single NVIDIA RTX 6000 Ada. B.5. Example Tables 4 and 5 present example prompt variations for the SUMMEVAL and HELPSTEER benchmarks. The variations introduce minimal surface-level perturbations, enabling analysis of judging stability under equivalent evaluation conditions. 14 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory summEval \"[Typo VARIATIONS]\": { ONLY -> LONLY (attention: 0.0038), JSON -> SJON (attention: 0.0020), task -> tsak (attention: 0.0018), } \"[NEWLINE VARIATIONS]\": { Extra newlines inserted after line 9, Extra newlines inserted after line 10, Extra newlines inserted after line 14 } \"[PARAPHRASE VARIATIONS]\": { evaluate -> assess, written -> composed, follow -> adhere, aware -> cognizant, proposed -> suggested } Table 4. summEval. helpSteer \"[Typo VARIATIONS]\": { Your -> Youe (attention: 0.0608), Please -> lPease (attention: 0.0039), only -> onyl (attention: 0.0038), } \"[NEWLINE VARIATIONS]\": { Extra newlines inserted after line 2, Extra newlines inserted after line 6, Extra newlines inserted after line 8 } \"[PARAPHRASE VARIATIONS]\": { expert -> specialist, assess -> evaluate, following -> subsequent, consistent -> coherent, sophisticated -> complex } Table 5. helpSteer. Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory C. Prompt Template We adopt prompt templates from the original papers: SummEval and TopicalChat, HelpSteer-2 (Wang et al., 2024c), and VIEScore (Ku et al., 2024a). For some benchmarks, we modify the output instruction to enforce JSON format. The detailed description and chain-of-thought (CoT) variants in our ablation study are our modifications. C.1. SummEval In this task you will evaluate the quality of summaries written for news article. To correctly solve this task, follow these steps: 1. Carefully read the news article, be aware of the information it contains. 2. Read the proposed summary. 3. Rate the summary on scale from 1 (worst) to 3 (best) by its relevance, consistency, fluency, and coherence. Definitions: Relevance: The rating measures how well the summary captures the key points of the article. Consistency: The rating measures whether the facts in the summary are consistent with the facts in the original article. Fluency: This rating measures the quality of individual sentences. Coherence: This rating measures the quality of all sentences collectively. News Article: {article} Summary: {summary} Please provide your ratings in the following JSON format ONLY: \"relevance\": <1-5>, \"consistency\": <1-5>, \"fluency\": <1-5>, \"coherence\": <1-5> C.2. TopicalChat C.2.1. ORIGINAL PROMPT In this task, you will evaluate the quality of dialogue response in knowledge-grounded conversation. Please carefully follow the steps below: 1. Read the dialogue context to understand the conversation history. 2. Read the given response. 3. Rate the summary using integer scores understandability (01), naturalness (15), coherence (15), engagingness (15), groundedness (01). Definitions: Understandability: judge whether the response is understandable. Naturalness: Judge whether response is like something person would naturally say. Coherence: Determine whether this response serves as valid continuation of the previous conversation. Engagingness: Determine if the response is interesting or dull. Groundedness: Given the fact that this response is conditioned on, determine whether this response uses that fact. Dialogue Context: {context} Response: {response} Please provide your ratings in the following JSON format ONLY: \"understandability\": <0-1>, \"naturalness\": <1-3>, \"coherence\": <1-3>, \"engagingness\": <1-3>, \"groundedness\": <0-1> Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory C.2.2. DETAILED PROMPT In this task, you will evaluate the quality of dialogue response in knowledge-grounded conversation. Please carefully follow the steps below: 1. Read the dialogue context to understand the conversation history. 2. Read the given response. 3. Rate the summary using integer scores understandability (01), naturalness (15), coherence (15), engagingness (15), groundedness (01). Definitions: Understandability: Judge whether the response is understandable in terms of basic meaning and intent. response is understandable if reader can clearly grasp what is being said without ambiguity, guesswork, or repeated re-reading. Assign lower score if the response contains grammatical errors, broken sentence structure, unclear references, or logical gaps that make the meaning difficult or impossible to understand. Naturalness: Judge whether the response sounds like something real person would naturally say in conversational setting. Consider fluency, phrasing, tone, and whether the language feels organic rather than robotic, translated, or overly formal. Lower scores indicate stiff, mechanical, repetitive, or unnatural wording, while higher scores indicate smooth, human-like conversational language. Coherence: Determine whether this response serves as valid and appropriate continuation of the previous conversation. Consider whether the response directly addresses the preceding dialogue, stays on topic, and follows logically from what was said before. Lower scores indicate topic drift, non sequiturs, contradictions, or failure to respond to the prior turn. Engagingness: Determine whether the response is interesting, informative, or engaging for human reader. Lower scores indicate dull, generic, minimal, or boilerplate responses that add little value to the conversation. Higher scores indicate responses that provide useful information, thoughtful elaboration, or otherwise maintain the readers interest. Groundedness: Given the fact or knowledge that this response is conditioned on, determine whether the response correctly uses and reflects that information. Assign lower score if the response introduces unsupported claims, contradicts the provided context, or ignores relevant grounding facts. Assign higher score if the response is consistent with, accurately reflects, and appropriately incorporates the given factual information. Dialogue Context: {context} Response: {response} Please provide your ratings in the following JSON format ONLY: \"understandability\": <0-1>, \"naturalness\": <1-3>, \"coherence\": <1-3>, \"engagingness\": <1-3>, \"groundedness\": <0-1> Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory C.2.3. DETAILED + COT PROMPT In this task, you will evaluate the quality of dialogue response in knowledge-grounded conversation. Please carefully follow the steps below: 1. Read the dialogue context to understand the conversation history. 2. Read the given response. 3. Rate the summary using integer scores understandability (01), naturalness (15), coherence (15), engagingness (15), groundedness (01). Definitions: Understandability: Judge whether the response is understandable in terms of basic meaning and intent. response is understandable if reader can clearly grasp what is being said without ambiguity, guesswork, or repeated re-reading. Assign lower score if the response contains grammatical errors, broken sentence structure, unclear references, or logical gaps that make the meaning difficult or impossible to understand. Naturalness: Judge whether the response sounds like something real person would naturally say in conversational setting. Consider fluency, phrasing, tone, and whether the language feels organic rather than robotic, translated, or overly formal. Lower scores indicate stiff, mechanical, repetitive, or unnatural wording, while higher scores indicate smooth, human-like conversational language. Coherence: Determine whether this response serves as valid and appropriate continuation of the previous conversation. Consider whether the response directly addresses the preceding dialogue, stays on topic, and follows logically from what was said before. Lower scores indicate topic drift, non sequiturs, contradictions, or failure to respond to the prior turn. Engagingness: Determine whether the response is interesting, informative, or engaging for human reader. Lower scores indicate dull, generic, minimal, or boilerplate responses that add little value to the conversation. Higher scores indicate responses that provide useful information, thoughtful elaboration, or otherwise maintain the readers interest. Groundedness: Given the fact or knowledge that this response is conditioned on, determine whether the response correctly uses and reflects that information. Assign lower score if the response introduces unsupported claims, contradicts the provided context, or ignores relevant grounding facts. Assign higher score if the response is consistent with, accurately reflects, and appropriately incorporates the given factual information. Dialogue Context: {context} Response: {response} Please evaluate the quality of the judgement based on whether the judgement is grounded on the responses and carefully follows the rubric. Provide some reasoning and analysis to support your evaluation. Next, provide an integer rating in JSON format. Please provide your ratings in the following JSON format: \"understandability\": <0-1>, \"naturalness\": <1-5>, \"coherence\": <1-5>, \"engagingness\": <1-5>, \"groundedness\": <0-1> C.3. HelpSteer-2 You are an expert evaluator for answers to user queries. Your task is to assess responses to user queries based on helpfulness, relevance, accuracy, and clarity. Calculate the following metrics for the response: User Query: {query} Model Response: {response} Metrics: 1. helpfulness (1-5): How well does the response help the user? 2. correctness (1-5): Is the information correct? 3. coherence (1-5): Is the response logically consistent and well-structured? 4. complexity (1-5): How sophisticated is the response? 5. verbosity (1-5): Is the response appropriately detailed? Instructions: Assign score from 1 (poor) to 5 (excellent) for each metric. Please provide your ratings in the following JSON format ONLY: \"helpfulness\": <1-5>, \"correctness\": <1-5>, \"coherence\": <1-5>, \"complexity\": <1-5>, \"verbosity\": <1-5> 18 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory C.4. VIEScore-CIG-SC RULES: Two images will be provided: The first being processed image (e.g. Canny edges, openpose, grayscale, etc.) and the second being an AI-generated image using the first image as guidance. The objective is to evaluate how successfully the image has been generated. On scale 0 to 10: score from 0 to 10 will be given based on the success in following the prompt. (0 indicates that the second image does not follow the prompt at all. 10 indicates the second image follows the prompt perfectly.) second score from 0 to 10 will rate how well the generated image is following the guidance image. (0 indicates that the second image does not follow the guidance at all. 10 indicates that the second image is following the guidance image.) Put the score in list such that output score = [score1, score2], where score1 evaluates the prompt and score2 evaluates the guidance. Text Prompt: text C.5. VIEScore-TIE-SC RULES: Two images will be provided: The first being the original AI-generated image and the second being an edited version of the first. The objective is to evaluate how successfully the editing instruction has been executed in the second image. Note that sometimes the two images might look identical due to the failure of the image edit. On scale of 0 to 10: score from 0 to 10 will be given based on the success of the editing. (0 indicates that the scene in the edited image does not follow the editing instructions at all. 10 indicates that the scene in the edited image follows the editing instruction text perfectly.) second score from 0 to 10 will rate the degree of overediting in the second image. (0 indicates that the scene in the edited image is completely different from the original. 10 indicates that the edited image can be recognized as minimally edited yet effective version of the original.) Put the score in list such that output score = [score1, score2], where score1 evaluates the editing success and score2 evaluates the degree of overediting. Editing instruction: instruction C.6. VIEScore-PQ RULES: The image is an AI-generated image. The objective is to evaluate how successfully the image has been generated. On scale 0 to 10: score from 0 to 10 will be given based on image naturalness. ( 0 indicates that the scene in the image does not look natural at all or gives an unnatural feeling such as wrong sense of distance, wrong shadow, or wrong lighting. 10 indicates that the image looks natural. ) second score from 0 to 10 will rate the image artifacts. ( 0 indicates that the image contains large portion of distortion, watermarks, scratches, blurred faces, unusual body parts, or subjects not harmonized. 10 indicates the image has no artifacts. ) Put the score in list such that output score = [naturalness, artifacts] D. Details of Intrinsic Reliability"
        },
        {
            "title": "Topical Chat",
            "content": "Detail Prompt + CoT Gemini-2.5 GPT-4o GPT-4o-mini Qwen3-30b Qwen3-235b Llama-4-m Llama-4-s ρ CV ρ ρ ρ ρ ρ ρ CV CV 0.63 0.90 0.58 0.68 0.93 0.51 0.29 0.95 0.68 0.33 0.86 0.72 0.63 0.94 0.75 0.73 0.89 0.84 0.79 0.72 0.95 0.93 0.96 0.94 CV 0.64 0.52 0.34 0.36 0.79 0.62 0.82 0.86 0.86 0.87 0.87 0. CV 0.67 0.85 0.77 0.85 0.39 0.70 0.83 0.89 0.85 0.79 0.87 0.88 CV 0.20 0.21 0.33 0.23 0.51 0.17 0.58 0.66 0.83 0.65 0.85 0.64 CV 0.19 0.66 0.27 0.75 0.45 0.82 0.26 0.65 0.26 0.84 0.42 0.73 CIG-SC 1.15 0.91 PQ 1.12 0.93 MIE-SC 0.25 0.93 PQ 0.22 0.59 TIE-SC 0.99 0.92 PQ 1.52 0. Table 6. VIEScore intrinsic result. Gemini-2.5 refers to Gemini 2.5 Flash; Llama-4-m and Llama-4-s refer to Llama-4-Maverick and Llama-4-Scout, respectively. For VIEScore, Qwen3-30B and Qwen3-235B refer to their VL variants. E. Perason Refer to Table 7 through Figure 7. F. Visualization Refer to Figure 2 to Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory"
        },
        {
            "title": "TopicalChat",
            "content": "HelpSteer-"
        },
        {
            "title": "Helpfulness\nCorrectness\nCoherence\nComplexity\nVerbosity",
            "content": "CIG-SC PQ MIE-SC PQ TIE-SC PQ Gemini-2.5 GPT-4o GPT-4o-mini Qwen3-30b Qwen3-235b Llama-4-m Llama-4-s 0.54*** 0.72*** 0.54*** 0.51*** 0.38*** 0.37*** 0.26*** 0.40*** 0.40*** 0.41*** 0.37*** 0.30*** 0.21*** 0.18*** 0.53*** 0.55*** 0.45*** 0.52*** 0.34*** 0.26*** 0.30*** 0.39*** 0.53*** 0.41*** 0.47*** 0.35*** 0.41*** 0.40*** 0.51*** 0.67*** 0.49*** 0.52*** 0.29*** 0.31*** 0.21*** 0.34*** 0.44*** 0.41*** 0.39*** 0.27*** 0.36*** 0.40*** 0.51*** 0.61*** 0.56*** 0.57*** 0.38*** 0.49*** 0.27*** 0.46*** 0.54*** 0.49*** 0.48*** 0.39*** 0.40*** 0.31*** 0.53*** 0.73*** 0.47*** 0.58*** 0.30*** 0.46*** 0.25*** 0.48*** 0.51*** 0.47*** 0.41*** 0.30*** 0.26*** -0. 0.49*** 0.75*** 0.60*** 0.54*** 0.33*** 0.35*** 0.23*** 0.40*** 0.52*** 0.46*** 0.40*** 0.35*** 0.20*** 0.21*** 0.01 0.02 0.04 0.02 0.08 0.06 -0.15 0.03 -0.03 0.07 -0.03 0.07 -0.15 -0.05 0.05 0.16* 0.12 -0.13 -0.09 0.12 -0.06 0.14 0.09 0.05 -0.11 0.08 -0.08 0.10 0.11 0. -0.17 -0.02 -0.07 -0.13 0.05 0.02 0.40*** 0.65*** 0.49*** 0.48*** 0.25*** 0.40*** 0.21*** 0.31*** 0.42*** 0.29*** 0.34*** 0.24*** 0.27*** 0.31*** -0.25* 0.22 -0.12 0.07 0.03 0.03 Table 7. Pearson correlation between human and LLM judgments. Gemini-2.5 refers to Gemini 2.5 Flash; Llama-4-m and Llama-4-s refer to Llama-4-Maverick and Llama-4-Scout, respectively. For VIEScore, Qwen3-30B and Qwen3-235B refer to their VL variants. *p < 0.05, **p < 0.01, ***p < 0.001. 20 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Figure 2. Median θ by human score for all SummEval metrics. 21 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Figure 3. Median θ by human score for all TopicalChat metrics. 22 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Figure 4. Median θ by human score for all HelpSteer-2 metrics. Figure 5. Median θ by human score for VIEScore-CIG (SC and PQ). 23 Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory Figure 6. Median θ by human score for VIEScore-MIE (SC and PQ). Figure 7. Median θ by human score for VIEScore-TIE (SC and PQ)."
        }
    ],
    "affiliations": [
        "Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea",
        "Department of Industrial Engineering"
    ]
}