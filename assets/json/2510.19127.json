{
    "paper_title": "Steering Autoregressive Music Generation with Recursive Feature Machines",
    "authors": [
        "Daniel Zhao",
        "Daniel Beaglehole",
        "Taylor Berg-Kirkpatrick",
        "Julian McAuley",
        "Zachary Novack"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model's internal gradients to produce interpretable \"concept directions\", or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MusicGen's hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code to encourage further exploration on RFMs in the music domain."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 7 2 1 9 1 . 0 1 5 2 : r Preprint."
        },
        {
            "title": "STEERING AUTOREGRESSIVE MUSIC GENERATION\nWITH RECURSIVE FEATURE MACHINES",
            "content": "Daniel Zhao, Daniel Beaglehole, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack University of California, San Diego {djzhao, dbeaglehole, tberg, jmcauley, znovack}@ucsd.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Controllable music generation remains significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, framework that adapts Recursive Feature Machines (RFMs) (Radhakrishnan et al., 2023) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze models internal gradients to produce interpretable concept directions, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MUSICGENs hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code 1 to encourage further exploration on RFMs in the music domain."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large autoregressive (AR) models, powered by neural audio codecs, have made remarkable strides in text-to-music (TTM) generation, producing audio with impressive fidelity and coherence (Copet et al., 2024; Yuan et al., 2025). Despite growing body of work in conditioning TTM AR models on time-varying controls (Novack et al., 2024b;a; Wu et al., 2024; Lin et al., 2023; Koo et al., 2025), achieving precise control over fine-grained music-theoretic (e.g. pitch classes and chord qualities) content across time in generations remains challenging. Current approaches often focus on broad temporal controls like dynamics or polyphonic melody, and may either require intense finetuning runs or costly per-step optimization during inference to avoid large-scale training. We argue that more direct and principled path to controllability lies in activation-space intervention. If we can identify directions within models hidden states that reliably correspond to humaninterpretable music-theoretic concepts, such as specific pitches, chord qualities, or tempo, we can then steer the generation along these axes, guiding the creative process without retraining the base model or altering its decoding procedure. The critical question then becomes how to discover these semantic directions in robust and interpretable manner. Recursive Feature Machines (RFMs) provide powerful answer (Radhakrishnan et al., 2023; Beaglehole et al., 2025a;b). By forming an Average Gradient Outer Product (AGOP) from lightweight task probes, RFMs yield set of orthogonal, eigenvalue-ranked directions that capture the most salient axes of variation for given concept within models representation space. These directions directly represent the models principal axes of sensitivity to specific features. In this work, we introduce MusicRFM, framework that adapts RFMs to steer frozen MUSICGEN-Large model directly in its activation space. Our approach is twofold: first, we train extremely 1https://github.com/astradzhao/music-rfm Preprint. lightweight, layer-wise RFM probes on the SYNTHEORY dataset (Wei et al., 2024) to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the models residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across MUSICGENs 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layers probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of target note from 0.23 to over 0.82, while CLAP score for text alignment remains within 0.02 of the unsteered baseline. In brief, MusicRFM establishes general and efficient framework for fine-grained, interpretable control in text-to-music generation, requiring only the lightweight training of small RFM probes, with no model finetuning or costly optimization at inference time. Its layerand time-aware mechanisms, coupled with support for multi-direction steering, enable flexible and controllable modulation of attributes while preserving high audio fidelity."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Research on controllable generation spans several communities, from activation-level steering in large language models to decoding-time control methods and controllable music generation. Our work, MusicRFM, builds on and unifies these threads by adapting RFMs to the domain of music while adding new temporal and architectural control mechanisms. 2.1 CONTROLLABLE MUSIC AND AUDIO GENERATION We focus particularly on text-to-music (TTM) generation that relies on neural audio codecs and autoregressive sequence models in architectures like MUSICGEN, MUSICLM, and JUKEBOX (Copet et al., 2024; Defossez et al., 2022; Agostinelli et al., 2023; Dhariwal et al., 2020; Yuan et al., 2025; Team et al., 2025). Additionally, number of controllable TTM systems exist in the parallel diffusion domain (Novack et al., 2024b;a; Wu et al., 2024; Nistal et al., 2024a;b). Most existing controllable methods for AR focus on multi-modal controls (e.g. video) (Kim et al., 2025) or common musical controls like piano rolls (Lin et al., 2024). These approaches, while generally performant, still require reasonably compute-heavy finetuning runs and thus necessitate changing the base model, potentially breaking its core generative capabilities if the finetuning data is ill-chosen. 2.2 ACTIVATION-LEVEL STEERING IN GENERATIVE MODELS Beyond music, growing body of work investigates activation-level steering in language models. Activation Addition (ACTADD) constructs steering vectors from paired prompts and injects them into hidden states for sentiment or style shifts, without retraining or optimization (Turner et al., 2024). Contrastive Activation Addition (CAA) extends this idea by contrasting positive/negative contexts to obtain more targeted steering directions in Llama-style models (Panickssery et al., 2024). These methods illustrate broader trend: interpretable steering can often be achieved by modifying internal activations, rather than logits or decoding heuristics. Within music, existing approaches either focus solely on binary controls (Facchiano et al., 2025) or broad concepts like instrument presense (Koo et al., 2025), and thus it remains to be seen whether such approaches can extended to time-varying, music-theoretic control. 2.3 RECURSIVE FEATURE MACHINES (RFMS) Recursive Feature Machines (RFMs) (Radhakrishnan et al., 2023) were introduced as probing methods that iteratively recondition features via AGOP matrices to uncover task-sensitive subspaces. More recently, RFM-derived directions have been re-injected into activations for steering in LLMs (Beaglehole et al., 2025b). We extend this paradigm to autoregressive music generation with three 2 Preprint. innovations: (i) layer-based control through top-K and exponential weighting across 48 layers, (ii) time-based control using dynamic schedules, and (iii) multi-direction control via simultaneous or staggered application of concept directions."
        },
        {
            "title": "3 METHODS",
            "content": "Our overall goal is to enable fine-grained, interpretable control in AR music generation, steering towards concepts like specific notes, chord types, or slow/fast tempo. To do this, we train lightweight RFM probes to extract concept-aligned directions and re-inject them into MUSICGEN activations at inference time. This framework allows us to generate music samples that still follow text conditioning with high accuracy, while also reflecting controlled variations in targeted musical attributes."
        },
        {
            "title": "3.1 BACKGROUND ON RECURSIVE FEATURE MACHINES",
            "content": "We first provide some more background on Recursive Feature Machines before describing our application to music generation. RFMs (Radhakrishnan et al., 2023) were originally proposed as probing method, iteratively reconditioning features with Average Gradient Outer Product (AGOP) matrii=1 and predictor : Rd R, ces to identify task-sensitive subspaces. Given training data {(xi, yi)}n define per-sample gradients gi = xf (xi) Rd and the AGOP 1 (cid:88) i=1 gig Rdd. (1) is PSD, with eigendecomposition = QΛQ. Directions {qj} are orthonormal, with eigenvalues λj 0 measuring sensitivity: λj = M qj = 1 (cid:88) (q gi)2. (2) i=1 RFM implements feature learning by iterating: (i) train base learner (kernel ridge regression as described in App. A) on features x(t) to obtain (t), (ii) compute (t) via equation 1, and (iii) update features with x(t+1) = (t)x(t), (t) = Q(t)(Λ(t))α(Q(t)), where α > 0 amplifies high-sensitivity directions. Importantly, this process is backpropagation-free. Recent work has extended RFMs to steering: injecting concept direction qj back into hidden activations biases frozen model toward that attribute during inference (Beaglehole et al., 2025b). In practice, steering is implemented by registering hooks on subset of layers and adding broadcast control vector to each residual stream: t,ℓ = ht,ℓ + ηℓ(t) qℓ,j , (3) where qℓ,j Rdℓ is reshaped to (1, 1, dℓ). Steering only uses the top component per direction. 3.2 MUSICRFM: RFM STEERING FOR MUSIC GENERATION We adapt RFMs to steer MUSICGEN-large (L=48 decoder blocks), Transformer over EnCodec tokens conditioned on text (Copet et al., 2024; Defossez et al., 2022). Our pipeline has three stages: (i) audio ENCODEC codes, (ii) layerwise RFM probes that yield AGOP eigendirections, and (iii) steering applied at inference as described above. Synthetic Dataset for Probe Training. SYNTHEORY (Wei et al., 2024) is recently designed synthetic dataset made to study interpretable representations of music theory concepts in large models, divided into 7 categories: tempo, notes, chord progressions, chord types, scales, intervals, and time signatures. Compared to prior music datasets, SYNTHEORY offers clean, fine-grained supervision of musical properties, enabling controlled experiments on model interpretability and controllability. This dataset is particularly well-suited for probing approaches, as its labeled attributes align directly with theoretical concepts that can be mapped onto latent representations. In our setting, SYNTHEORY allows us to train lightweight RFM probes on layerwise activations of MusicGen, yielding gradient-based directions that correspond to human-interpretable musical attributes. 3 Preprint. Feature Extraction. Audio clips are resampled to 32 kHz, encoded with ENCODEC, and passed t,ℓ Rdℓ, through MUSICGEN. For clip and layer ℓ, we mean-pool over tokens, xi,ℓ = 1 yielding clip-level vectors. Unlike last-token pooling used in text-based RFMs (Beaglehole et al., 2025b;a), mean pooling better captures temporal structure and improves probe performance. t=1 h(i) (cid:80)T Probe training and steering. For each concept and layer ℓ, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use {0, 1} labels and regression targets are z-normalized. The resulting eigendirections qℓ,j form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq. 3. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs. 3.3 IMPROVING ROBUSTNESS IN AUDIO-DOMAIN STEERING As we extend the existing text-steering framework of RFMs provided by Beaglehole et al. (2025b) to audio domain music, we introduce additional modifications to help reduce out-of-distribution behavior and improve control. In particular, given the difference between the discrete, variablesampling rate nature of text and the continuous, fixed-sampling rate nature of audio-domain music, we found that many of the algorithmic choices made by Beaglehole et al. (2025b) were ill-suited for TTM generation. All modifications are only applied during inference time. 3.3.1 LAYER PRUNING Naıve steering, where we inject RFM directions uniformly across all L=48 layers at every step as is done in the original RFM paper (Beaglehole et al., 2025a), leads to noticeable degradation in audio quality and weaker alignment to text prompts. To address this, we introduce layer pruning strategies at inference time that prioritize informative layers and downweight noisy ones, thereby improving both perceptual fidelity and controllability (see App. for full results). Top-K selection. We rank each layer ℓ {1, . . . , L} by its validation probe performance AUCℓ, then restrict steering to the top-K layers. Exponential weighting. Instead of hard pruning, we also apply continuous weighting across layers. For each layer ℓ, we normalize its probe score sℓ into ˆsℓ [0, 1], and define wℓ = w0 ˆs1/κ with κ (0, 1). This concentrates steering strength on high-performing layers, reducing unwanted artifacts and incorrect directions produced by the lower-scoring ones. ℓ 3.3.2 TIME-CONTROL SCHEDULES We modulate steering strength over time as ηℓ(t) = η0 wℓ ϕ(t) ψp(t), where η0 is global coefficient, wℓ layer weight, ϕ(t) deterministic schedule, and ψp(t) an optional stochastic gate. Deterministic schedules ϕ(t). Linear/logistic rise, linear/exponential decay, and sinusoidal modulation let us increase or decrease concepts influence over time (e.g., fade out note class, ramp in chord progression, or periodically modulate tempo). Closed-form expressions are given in App. E. Stochastic application ψp(t). At each step, apply control with probability (Bernoulli gating). Similarly to layer pruning, this method reduces over-steering and cumulative artifacts while preserving the expected bias toward the target. Ablations are in App. C. 3.3.3 MULTI-DIRECTION AND STAGGERED CONTROL m=1 in parallel. At each step we inject We further extend MusicRFM to support multi-direction steering, combining multiple concept vec- (cid:2)η0,m wℓ ϕm(t) ψp(t)(cid:3) qℓ,jm, tors {qℓ,jm}M where each direction has its own coefficient η0,m and schedule ϕm(t). This enables both (i) simultaneous enforcement of multiple attributes and (ii) staggered control where different concepts are activated at different times. For example, one schedule may enforce tempo strongly during the opening segment, while another gradually ramps in harmonic structure later. t,ℓ = ht,ℓ+(cid:80)M m=1 Preprint."
        },
        {
            "title": "4 CLASSIFICATION RESULTS",
            "content": "With MusicRFM, we additionally train separate multiclass probes (different from the binary probes used to steer) to compare RFM clasification against the original probing methods used in SYNTHEORY. We see that RFMs have better or comparable performance to the 2-layer FFN probes used in the original SYNTHEORY paper across all categories. We highlight that RFMs beat the baseline probes in accuracy on scales, progressions, and intervals and in R2 score on the tempo dataset, resulting in higher overall average score. We justify the mean pooling strategy by ablating RFMs over only classifying on last-token activations, showing much better performance. Furthermore, we argue that FFNs do not naturally yield orthogonal, eigenvalue-ranked directions suitable for steering. In contrast, RFMs produce PSD AGOP matrix whose eigenvectors correspond to stable, interpretable axes of sensitivity. These axes can be directly injected into the model at inference, making RFMs uniquely suited for controlled generation. Model Notes Intervals Scales Chords Prog. Time Sig. Tempos Avg. MusicRFM - mean pooled (ours) RFM (last token) Linear Probe Syntheory FFN 0.850 0.734 0.761 0. 0.975 0.743 0.618 0.972 0.956 0.546 0.158 0.905 0.984 0.866 0.834 0. 0.943 0.811 0.725 0.901 0.900 0.771 0.729 0.905 0.985 0.959 0.972 0. 0.942 0.776 0.685 0.929 Table 1: Classification results for base SYNTHEORY FFN (in Wei et al. (2024)), simple linear probes, RFMs trained on last-token activations, and MusicRFM (ours). We report R2 score on the tempos dataset and accuracy on the others. We dont record performance on logistic probes as some fail to converge. Bold indicates best performing model per category."
        },
        {
            "title": "5 SINGLE-DIRECTION MUSICRFM STEERING RESULTS",
            "content": "We report results on how well binary directions trained using MusicRFM are able to steer generations towards interpretable concepts, exploring both quantitative and subjective metrics. 5.1 QUANTITATIVE METRICS We first quantify audio similarity and distributional shift of generations steered along single concept direction using three standard metrics as function of the control coefficient η0: (i) Frechet Distance (FD) (lower is better), and (ii) Maximum Mean Discrepancy (MMD) (lower is better), (iii) CLAP alignment (higher is better). For the tempos category, the control coefficient η0 is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table 2. All results are reported on generations steered with RFM probes using stochastic application ψp(t) with = 0.3 and exponential layer weighting with w0 = 1 and κ = 0.95; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations. Results. The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As η0 increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignmentmeasuring similarity to the conditioning text prompt remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of η0 can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App. D. 5.2 STEERING ACCURACY ANALYSIS VIA RFM PROBES To evaluate whether MusicRFM steering actually enforces the intended musical concepts, we classify generated samples using the same multiclass RFM probes described in Sec. 4. For each attribute, 5 Preprint. FD MMD CLAP Probe Acc. Category Control coefficient η0 0.15 0.30 0.45 0. 0.15 0.30 0.45 0.60 0.15 0. 0.45 0.60 0.15 0.30 0.45 0. Chords Intervals Notes Scales Progressions Tempos Time signatures 0.162 0.264 0.402 0.492 0.356 1.046 1.980 2.647 0.320 0.317 0.278 0.264 0.172 0.204 0.238 0.245 0.116 0.114 0.110 0.119 0.063 0.086 0.040 0.095 0.324 0.326 0.319 0.326 0.271 0.288 0.320 0.344 0.110 0.128 0.169 0.232 0.078 0.119 0.400 0.817 0.315 0.324 0.311 0.307 0.121 0.156 0.187 0.223 0.113 0.130 0.138 0.180 0.052 0.127 0.217 0.476 0.315 0.311 0.318 0.303 0.231 0.461 0.684 0.824 0.114 0.115 0.114 0.119 0.052 0.075 0.061 0.081 0.318 0.328 0.322 0.324 0.154 0.157 0.161 0.176 0.131 0.142 0.173 0.207 0.157 0.233 0.443 0.650 0.315 0.309 0.296 0.297 0.070 0.079 0.096 0.114 0.122 0.150 0.206 0.377 0.112 0.324 0.717 1.880 0.328 0.325 0.307 0.280 Table 2: Single-direction steering metrics. Steering uses RFM probes with ψp(t), = 0.3 and exponential layer weighting with w0 = 1 and κ = 0.95. Lower is better for FD/MMD, higher better for CLAP and Probe Accuracy (mean per-class). Ground-truth MUSICGEN-LARGE has CLAP 0.332. Probe accuracy is not defined for tempos (regression). Accuracies should be interpreted only as relative trends due to probe training on synthetic SYNTHEORY data rather than real music. Bold indicates best control coefficient per category. Steering Type Chords Intervals Notes Tempo No Steering Naıve RFM (ours) MusicRFM (ours, optimal) 59.71 6.01 69.21 5.25 73.46 4.18 54.75 5.52 62.58 5.84 70.33 4. 57.08 6.37 68.13 5.97 72.88 5.67 55.75 7.08 73.33 4.35 73.38 4.75 Table 3: Listening test results (mean standard deviation) across musical attributes. we varied the control coefficient η0 {0.15, 0.30, 0.45, 0.60} and measured probe accuracy on all generations. The last column in Table 2 summarizes results across six categories. Results and trends. We observe that classification accuracy is generally low for all attributes except notes, where probe accuracy climbs sharply from 0.23 at η0=0.15 to 0.82 at η0=0.60, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy monotonically increases with the control coefficient in every category. For example, chords rise from 0.27 0.34, intervals from 0.12 0.22, and time signatures from 0.17 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute. Interpretation and caveats. These results highlight both the promise and the limitations of probebased evaluation. On the one hand, the monotonic response to η0 validates that MusicRFM manipulates representations in directions aligned with the intended probes. On the other hand, the absolute values of accuracy should not be over-interpreted: the RFM probes were trained exclusively on SYNTHEORY, highly synthetic dataset with simplified musical attributes. When applied to naturalistic generations, these probes may (i) fail to generalize, or (ii) misclassify samples that do in fact satisfy the target property. Thus, the reported accuracies should be understood primarily as relative trends across control coefficients, not as reliable ground-truth measures of musical validity. 5.3 LISTENING TEST AND AUDIO SAMPLES We provide results of listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), naıve RFM generations, and optimal RFM generations (steering with = 0.3 and exponential layer weighting with w0 = 1 and κ = 0.95). We show mean and STD of each type of steering in Table 3. Overall, the results indicate that both naıve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline. To the reader, we also provide representative audio samples from the listening test. Each clip is paired with its text prompt and steering metadata (η0, schedule), where all clips are steered with the optimal parameters listed above. An interactive demo of some of the clips used in our listening test is available at the project page.2 2https://astradzhao.github.io/MusicRFMPage/ 6 Preprint."
        },
        {
            "title": "5.4 EVALUATION ON MUSICBENCH (REAL MUSIC)",
            "content": "FD MMD CLAP Acc. To test transfer beyond synthetic data, we evaluate RFM probes on MUSICBENCH (Melechovsky et al., 2024), real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec. 3.2, we mean-pool MUSICGEN-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach 75.3% accuracy on notes and 67.5% on keys, while tempo regression proves difficult (MSE 0.862). Steering experiments  (Table 4)  mirror SYNTHEORY: higher η0 increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty. η0 0.15 0.424 0.30 0.495 0.45 0.576 0.60 0.717 Table 4: RFM steering on MUSICBENCH (key). 0.478 0.908 1.563 2.615 0.315 0.308 0.276 0. 0.148 0.264 0.479 0."
        },
        {
            "title": "6 MULTI-DIRECTION AND TIME-BASED STEERING RESULTS",
            "content": "We also evaluate MusicRFM when (i) multiple concept directions are injected simultaneously and (ii) when steering strength varies over time. We report the same quantitative metrics used in the single-direction setting and for (ii) introduce temporal analyses based on RFM probe softmax scores. 6.1 MULTI-DIRECTION STEERING: PAIRWISE CROSS-CATEGORY CONTROL To test whether MusicRFM can jointly enforce multiple musical attributes, we examine all pairwise combinations among {notes, chords, intervals}. For each pair (a, b), we sample random target class from category (e.g., note C) and random class from category (e.g., major chord), then generate music conditioned on both controls simultaneously. At inference, we inject two steering directions per selected layer, one for each concept, following Sec 3.3.3. Each direction is scaled by an independent global coefficient η0 {0.3, 0.6}. We evaluate all four cross-combinations {[0.3,0.3], [0.3,0.6], [0.6,0.3], [0.6,0.6]}, where the first value corresponds to category and the second to category b. For each pair, we generate = 100 samples per configuration, yielding 3 4 = 1200 total generations. To report results concisely, we reorganize outputs by attribute rather than by pair. For instance, all samples where notes were steered with coefficient 0.3regardless of whether they were paired with chords or intervalsare averaged together. This gives us per-category summaries across all pairings, shown in Table 5. Concept Chords Chords Intervals Intervals Notes Notes η0 0.3 0.6 0.3 0.6 0.3 0. FD MMD CLAP Probe Acc. 0.604 0.747 0.572 0.8861 0.566 0.927 2.564 3. 2.351 4.470 2.394 4.725 0.207 0.167 0.209 0.134 0.205 0.133 0.385 0. 0.298 0.300 0.770 0.920 Table 5: Multi-direction (pairwise) steering. Each cell reports the average over 200 generations. Findings. We observe several trends: (i) Probe accuracy still rises with stronger coefficients. For notes in particular, accuracy increases from 0.770 at η0 = 0.3 to 0.920 at η0 = 0.6, indicating that control strength directly improves enforcement even in multi-direction cases. (ii) Distributional metrics and CLAP scores degrade at higher strengths. Both FD and MMD grow substantially as η0 increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) Accuracy in multi-direction steering exceeds single-direction. We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect. 7 Preprint. Interpretation. These results highlight that multi-direction steering can indeed enforce multiple concepts, but doing so amplifies distributional drift and weakens prompt adherence. Notably, notes remain most controllable (large probe gains with modest η0), while more abstract concepts like intervals yield smaller improvements. This suggests that balancing coefficients across attributes or staggering them temporally (Sec. 3.3.3), may be necessary for high-quality joint control."
        },
        {
            "title": "6.2 TIME-DEPENDENT CONTROL: SMOOTH SCHEDULES",
            "content": "To study temporal schedules in isolation, we analyze the notes dataset with per-step steering strength ηℓ(t) = η0 ρℓ ϕ(t) and track the softmax score of the ground-truth note class under the RFM probe as function of time (generation steps). For the experiments in this section, we only analyze on notes because are they are the highest quality in terms of following control, and also can give us measurable accuracy when evaluating with RFM probes. We use per-direction coefficients η0,m and schedules ϕm(t) [0, 1], so ηm(t) = η0,m ϕm(t). The schedules we ablate are exponential decay, linear decay & increase, logistic increase, and sine wave. We put formulas used in Appendix E, and record FD, MMD, and CLAP scores in Table 6. FD MMD CLAP Schedule 0.358 Linear increase Linear decay 0.321 Exponential decay 0.229 0.360 Logistic increase 0.413 Sine modulation 1.917 1.636 1.052 1.999 2.347 0.227 0.257 0.312 0.208 0.225 Correct note probability over time. For each schedule we plot the probe softmax of the correct note over time in Figure 1a. We see that the distribution over time follows exactly what we would expect from each of the smooth scheduling functions - exponential & linear decay look like decay functions, sine is very similar to sine wave, and logistic & linear increase show an increase in predicted probability. Table 6: Metrics on time-dependent controlled generations Crossfading Between Concepts. We additionally study controlled cross-fade between two notes n1 n2 using complementary schedules over fixed window of 01500 steps: for n1 we decay from η0 = 0.45 to 0, and for n2 we rise from 0 to 0.45. Formally, η(n1) ℓ (t) = η0 ϕdecay(t), η(n2) ℓ (t) = η0 ϕrise(t), [0, 1500]. We then log and display the RFM-probe softmax scores for both n1 and n2 at each timestep in Figure 1b. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of 0.350, MMD of 1.922, and CLAP alignment of 0.250, indicating modest distributional drift but stable prompt adherence. (a) Temporal softmax traces (notes). Curves show the probe probability of the ground-truth note across timesteps for different schedules (linear/exp rise/decay, log. increase, sine). We choose the probe on the best performing layer (37) as our representative probe. (b) Two-note crossfade (softmax probabilities). The score for n1 decays (red) while n2 rises (blue). We again choose layer 37 as our representative probe and average over 500 samples. Figure 1: Time-based steering analyses. (a) Probe softmax follows prescribed schedules faithfully. (b) Crossfade experiments show expected decayrise patterns between two target notes. Preprint."
        },
        {
            "title": "7 LIMITATIONS & FUTURE WORK",
            "content": "While MusicRFM demonstrates that RFM-derived directions can steer music generation in interpretable ways, several limitations remain. First, our probes rely on mean-pooled features, which discard temporal ordering. This limits performance on concepts with strong sequential dependencies, such as scales, chord progressions, and time signatures, where the temporal dynamics are essential for accurate classification and control. As result, RFM probes underperform on these attributes compared to temporally local concepts like notes or chords. Future work should explore temporally aware pooling strategies (e.g., attention pooling, recurrent aggregation, convolutional pooling) or sequence-level RFMs that directly model time-evolving representations. Similarly, extending beyond the top eigenvector to incorporate multiple components could capture richer subspaces of variation, but we have not yet performed variance analyses to quantify how much information higher-order components retain. Another promising direction is to modify the steering process itself: by selectively masking portions of the autoregressive context during inference, one could reduce model over-dependence on previously generated tokens, thus increasing the models sensitivity to injected steering signals. Such approaches could make activation-space interventions both more controllable and also reduce the number of unwanted artifacts, particularly for longer generations. Additionally, our experiments so far are limited to SYNTHEORY-based, symbolic music-theoretic concepts such as notes, chords, and tempo. Future work could extend MusicRFM to attributes more directly tied to perceptual or production-level qualities, including instrument identity, timbre, or articulation style. While we perform preliminary analysis on MUSICBENCH, extended RFM training and steering on real-music-based datasets remains an open direction. These studies would connect RFM steering more directly to interpretability in real-world generation tasks. Finally, our experiments target MUSICGEN-large, but other large audio models open complementary directions for RFM steering. OpenAIs JUKEBOX (Dhariwal et al., 2020) uses multi-scale VQ-VAE codes and hierarchical AR decoders, while Googles recent MAGENTA-RT (Team et al., 2025) framework supports real-time audio generation. Applying RFMs in these contexts would require adapting probe extraction to multi-level codebooks (for Jukebox) and to low-latency streaming architectures (for Magenta-RT). In particular, real-time models highlight the possibility of real-time steering: dynamically injecting directions during live playback, enabling interactive control (i.e. live DJ-ing). Extending MusicRFM into these setups could bridge interpretability with performancecritical generative applications such as interactive music tools and live performance systems."
        },
        {
            "title": "8 CONCLUSION",
            "content": "We presented MusicRFM, framework that leverages RFM-derived, eigenvalue-ranked directions to steer frozen MUSICGEN-large model directly in activation space. By combining conceptaligned directions with layer-aware weighting and time-dependent schedules, MusicRFM enables fine-grained, interpretable control over attributes such as notes, chords, and tempo without modifying the base model or relying on per-step optimization. Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient η0: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias. By enhancing fine-grained controllability, this line of research can significantly broaden the practical applications of generative models. In the long term, improving the steerability and interpretability of generative models will expand their usefulness in domains like music production and game audio. 9 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023. URL https://arxiv. org/abs/2301.11325. Daniel Beaglehole, David Holzmuller, Adityanarayanan Radhakrishnan, and Mikhail Belkin. xrfm: Accurate, scalable, and interpretable feature learning models for tabular data. arXiv preprint arXiv:2508.10053, 2025a. Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser`a, and Mikhail Belkin. Toward universal steering and monitoring of ai models, 2025b. URL https://arxiv.org/ abs/2502.03708. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation, 2024. URL https://arxiv.org/ abs/2306.05284. Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: generative model for music. arXiv:2005.00341, 2020. Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression, 2022. URL https://arxiv.org/abs/2210.13438. Simone Facchiano, Giorgio Strano, Donato Crisostomi, Irene Tallini, Tommaso Mencattini, Fabio Galasso, and Emanuele Rodol`a. Activation patching for interpretable steering in music generation, 2025. URL https://arxiv.org/abs/2504.04479. Haven Kim, Zachary Novack, Weihan Xu, Julian McAuley, and Hao-Wen Dong. Video-guided textto-music generation using public domain movie collections. arXiv preprint arXiv:2506.12573, 2025. Junghyun Koo, Gordon Wichern, Francois Germain, Sameer Khurana, and Jonathan Le Roux. IEEE Smitin: Self-monitored inference-time intervention for generative music transformers. Open Journal of Signal Processing, 2025. Liwei Lin, Gus Xia, Junyan Jiang, and Yixiao Zhang. Content-based controls for music large language modeling. arXiv:2310.17162, 2023. Liwei Lin, Gus Xia, Yixiao Zhang, and Junyan Jiang. Arrange, inpaint, and refine: Steerable longterm music audio generation and editing via content-based controls. arXiv:2402.09508, 2024. Jan Melechovsky, Zixun Guo, Deepanway Ghosal, Navonil Majumder, Dorien Herremans, and Soujanya Poria. Mustango: Toward controllable text-to-music generation, 2024. URL https: //arxiv.org/abs/2311.08355. Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, and Stefan Lattner. Diff-a-riff: Musical accompaniment co-creation via latent diffusion models. arXiv:2406.08384, 2024a. Javier Nistal, Marco Pasini, and Stefan Lattner. Improving musical accompaniment co-creation via diffusion transformers. arXiv:2410.23005, 2024b. Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO-2: Distilled diffusion inference-time t-optimization for music generation. In ISMIR, 2024a. Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO: Diffusion inference-time T-optimization for music generation. In ICML, 2024b. Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition, 2024. URL https://arxiv. org/abs/2312.06681. 10 Preprint. Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features, 2023. URL https://arxiv.org/abs/2212.13881. Lyria Team, Antoine Caillon, Brian McWilliams, Cassie Tarakajian, Ian Simon, Ilaria Manco, Jesse Engel, Noah Constant, Yunpeng Li, Timo I. Denk, Alberto Lalama, Andrea Agostinelli, ChengZhi Anna Huang, Ethan Manilow, George Brower, Hakan Erdogan, Heidi Lei, Itai Rolnick, Ivan Grishchenko, Manu Orsini, Matej Kastelic, Mauricio Zuluaga, Mauro Verzetti, Michael Dooley, Ondrej Skopek, Rafael Ferrer, Zalan Borsos, Aaron van den Oord, Douglas Eck, Eli Collins, Jason Baldridge, Tom Hume, Chris Donahue, Kehang Han, and Adam Roberts. Live music models, 2025. URL https://arxiv.org/abs/2508.04651. Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid. Steering language models with activation engineering, 2024. URL https://arxiv.org/abs/2308.10248. Megan Wei, Michael Freeman, Chris Donahue, and Chen Sun. Do music generation models encode music theory?, 2024. URL https://arxiv.org/abs/2410.00872. Shih-Lun Wu, Chris Donahue, Shinji Watanabe, and Nicholas J. Bryan. Music ControlNet: Multiple IEEE/ACM Transactions on Audio, Speech, and time-varying controls for music generation. Language Processing (TASLP), 2024. Ruibin Yuan, Hanfeng Lin, Shuyue Guo, Ge Zhang, Jiahao Pan, Yongyi Zang, Haohe Liu, Yiming Liang, Wenye Ma, Xingjian Du, et al. Yue: Scaling open foundation models for long-form music generation. arXiv preprint arXiv:2503.08638, 2025. Preprint."
        },
        {
            "title": "A OVERVIEW OF KERNEL RIDGE REGRESSION",
            "content": "Kernel ridge regression (KRR) is the base model with which we apply the RFM procedure for iterative feature learning via the AGOP. We briefly explain the KRR model. Let Rnd denote training samples with x(i)T denoting the sample in the ith row of for [n] and Rnc denote training labels, where is the number of output channels (e.g. one-hot encoded classes for > 2 classes). Let : Rd Rd denote kernel function (a positive semi-definite, symmetric function), such as the Gaussian/RBF kernel (K(x, z) = exp(x z2 2)/L2), or the Laplace kernel (K(x, z) = exp(x z2)/L) used in this work. Given ridge regularization parameter λ 0, KRR solved on the data (X, y) gives predictor, ˆf : Rd Rc, of the form: ˆf (x) = K(x, X)α , where α is the solution to the following linear system: (K(X, X) + λI)α = . (4) (5) Here the notation K(x, X) R1n is the n-dimensional row vector with K(x, X)i = K(x, x(i)) and K(X, X) Rnn denotes the kernel matrix of pair-wise kernel evaluations K(X, X)ij = K(x(i), x(j)). The advantage of kernel functions in the context of this work is that the predictor admits closed form solution, which can be robustly computed and generally has fast training times for datasets under 70k samples."
        },
        {
            "title": "B TUNING PROCEDURE FOR RFM PROBING",
            "content": "We use 70/15/15 train/valid/test split on RFM training, 15 RFM iterations, and mean pooling over all timesteps. For multiclass training of simple progressions, we use 700 examples per class (there are 1100 per class in dataset, but we cannot fit them given our A6000 GPU memory size. However, we note that even without all training data, we still get significantly better accuracy than baseline in this category). For all other classes, we use the entire dataset for our training & validation. We use 100 random choices of hyperparameters listed below for layer-wise probes and 300 for aggregation. We maximize on AUC for layer-wise probes and accuracy for aggregation. When tuning the number of components calculated with our RFM probes, we tried lower number of components (2-10) for categories with less data points and less perceived complexity (e.g. notes, time signatures). For categories with larger dataset size and higher perceived complexity (e.g. simple progressions, scales), we choose number of components ranging from 8 to 24. Hyperparameter Layer-wise Aggregation model Bandwidth (L) Center gradients Exponent Kernel Type (when kernel type is Kp,q) Regularization log U(1, 100) {False, True} U(0.7, 1.4) K2,q log U(105, 10) log U(1, 100) {False, True} U(0.7, 1.4) Kp,q U(q, 2) log U(105, 10) Table 7: Search spaces for MusicRFM on individual layers and for the aggregation model. p/Lq) (indicated Note we tune over more general class of kernels Kp,q(x, x) = exp(x xq by the kernel type hyperparameter) for the aggregation model, which has been shown to improve the performance of RFM on tabular datasets (Beaglehole et al., 2025a). We also tune over whether to center the gradients in each iteration of RFM, which can help de-noise the gradients in highdimensional settings (Beaglehole et al., 2025b). Gradient centering modifies the AGOP computation to give the following centered matrix in the RFM iteration, where = 1 i=1 gi: (cid:80)n (t) = 1 (cid:88) i=1 (gi g)(gi g) . (6) Preprint."
        },
        {
            "title": "C STEERING ABLATIONS",
            "content": "For generation, we ablate two steering knobs that most strongly impact generation quality and concept alignment: (i) the effective number of layers contributing control via both flat top-k value and an exponential, score-weighted layer scheme (layer pruning), and (ii) per-timestep injection probability that sparsifies when control is applied. C.1 SETUP AND METRICS We follow Sec. 3.2 and inject layerwise RFM directions into the residual stream with strength where for ablations we set ϕ(t) 1 and vary layer weighting and p. ηℓ(t) = η0 ρℓ ϕ(t) ψp(t), C.2 ABLATING LAYER PRUNING We study three strategies that control how many (and how strongly) layers contribute to steering: (i) exponential score-weighted steering, (ii) simple linear score-weighted scheme, and (iii) hard top-K selection. We show results in Table 9 and Table 8. Continuous weighting (Linear vs. Exponential). Given base scale w0, we instantiate the perlayer weight ρℓ in ηℓ(t) = η0 ρℓ ϕ(t) using either: Linear: wlin ℓ = w0 ˆsℓ, Exponential: wexp ℓ = w0 ˆs 1/κ ℓ , where κ (0, 1) is decay rate (smaller κ increases contrast, concentrating weight on high-scoring layers). Linear is the minimal from 1 to 0 mapping; exponential recovers linear as κ 1 and becomes more selective as κ . Discrete selection (Top-K). We also ablate hard selection mask m(K) layers by ˆsℓ: ℓ {0, 1} over the top-K m(K) ℓ = 1[ℓ TopK(ˆs, K)] , wtop-K ℓ = w0 m(K) ℓ . We sweep {4, 8, 12, 16, 24, 32, 48}, with K=48 meaning all layers. Scheme Hyperparams FD MMD CLAP Classification Acc. Linear Exponential Exponential (ours) Exponential Uniform (naive) wℓ = w0ˆsℓ κ = 0.98 κ = 0.95 κ = 0.92 0.482 0.487 0.465 0.483 0.599 2.701 2.710 2.575 2.687 3.44 0.166 0.186 0.194 0.175 0.155 0.959 0.954 0.961 0.954 0.964 Table 8: Layer weighting ablation (continuous schemes). Exponential decay κ interpolates between flat (κ 1) and highly concentrated (κ 0). Linear maps the best layer to w0 and the worst to 0. Top-K FD MMD CLAP Classification Acc. = 4 = 8 = 12 = 16 = 24 = 32 = 48 (naive) 0.109 0.157 0.225 0.347 0.555 0.586 0.599 0.192 0.448 0.919 1.781 3.218 3.395 3.44 0.309 0.291 0.263 0.225 0.158 0.158 0.155 0.398 0.678 0.882 0.941 0.967 0.958 0. Table 9: Layer selection ablation (top-K hard pruning). controls the effective number of controlled layers. 13 Preprint. C.3 ABLATING INJECTION PROBABILITY At each generation step t, we sample gate bt Bernoulli(p) and apply: t,ℓ = ht,ℓ + bt ηℓ(t) qℓ, so control fires stochastically with probability p. We show results in Table 10. FD MMD CLAP Classification Acc. 0.15 0.30 (ours) 0.45 0.6 0.75 0.9 1.0 (naive) 0.108 0.118 0.197 0.281 0.399 0.510 0.599 0.163 0.272 0.769 1.343 2.145 2.853 3. 0.348 0.306 0.287 0.265 0.207 0.172 0.155 0.348 0.697 0.884 0.931 0.961 0.962 0.964 Table 10: Injection probability ablation. Lower reduces artifacts but may weaken alignment; higher increases control strength but risks over-steering."
        },
        {
            "title": "D SINGLE DIRECTION METRIC GRAPHS",
            "content": "Figure 2: Single-direction steering metrics as function of control coefficient η0. Top Left: Frechet Distance (FD; ) increases with stronger control. Top Right: Maximum Mean Discrepancy (MMD; ) shows similar trend. Bottom Left: CLAP alignment () to the text prompt remains relatively stable for most categories. Bottom Right: Probe accuracy shows that, despite poor performance on generated data, there is an upwards trend in accuracy as we increase the control coef η0. 14 Preprint."
        },
        {
            "title": "E CONTROL SCHEDULES USED FOR TIME CONTROL ABLATIONS ON NOTE",
            "content": "CLASSIFICATION ϕlin(t) = min (cid:16) max( (cid:17) 1500 , 0), 1 , ϕlin(t) = 1 min (cid:16) max( (cid:17) , 1500 , 0), 1 ϕexp(t) = λt, (λ = 0.998), ϕlog(t) = 1 1+exp((t750)/200) , ϕsin(t) = 1 2 (cid:0)1 + sin(2πt/1500)(cid:1)."
        }
    ],
    "affiliations": [
        "University of California, San Diego"
    ]
}