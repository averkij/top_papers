{
    "paper_title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens",
    "authors": [
        "Zeyuan Yang",
        "Xueyang Yu",
        "Delin Chen",
        "Maohao Shen",
        "Chuang Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 8 1 2 7 1 . 6 0 5 2 : r Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens Zeyuan Yang1 Xueyang Yu1 Delin Chen1 Maohao Shen2 Chuang Gan1 1University of Massachusetts, Amherst 2Massachusetts Institute of Technology Project Page: Code: https://vlm-mirage.github.io https://github.com/UMass-Embodied-AGI/Mirage"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagerythe internal construction and manipulation of visual cueswe investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to think visually, it recasts its hidden states as next tokens, thereby continuing multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation."
        },
        {
            "title": "Introduction",
            "content": "Visionlanguage models (VLMs) jointly encode images and text and attain impressive results on visual-understanding benchmarks through text-only decoding [Wang et al., 2024]. Techniques such as chain-of-thought prompting and reinforcement-learning fine-tuning can lengthen these textual reasoning traces and yield extra gains. Nonetheless, VLMs still stumble on multimodal reasoning tasks such as spatial reasoning, which demand more than passive perception; they require coherent understanding and manipulation of visual elements. Consider the jigsaw puzzle in Fig. 1. Instead of textualizing every candidate piece, people picture how the two fragments might align and decide on the correct match. This reasoning unfolds in native multimodal fashion, not through language alone. Recent studies [Team, 2024, Tong et al., 2024, Chern et al., 2024, Chen et al., 2025] have pre-trained VLMs for large-scale image generation so single model can produce both words and pictures. Yet the cognitive demands of logical reasoning differ sharply from the task of synthesizing pixels, and asking one model to master both goals often degrades its reasoning quality [Wang et al., 2025]. In addition, the image decoders cannot produce interleaved trajectories pertinent to input images. Consequently, fully exploiting the dormant multimodal reasoning capacity of VLMs remains an open challenge. Equal contribution Preprint. Under review. Figure 1: Multimodal Reasoning Examples. Mirage interleaves latent visual tokens, which represent compact imagery visual features, with explicit text tokens to solve diverse spatial reasoning multimodal tasks, boosting the reasoning performance without the full pixel-level image generation. According to imagery theory, humans do not summon photorealistic pictures while thinking. We instead construct and manipulate mental images, simplified sketches that capture only task-relevant information, process known as mental imagery [Shepard and Metzler, 1971, Farah, 1985, Kosslyn, 1996]. In the jigsaw example, we examine fragment contours to decide whether two pieces fit. Likewise, when searching for misplaced keys, we recall the outline of the shelf edge rather than the full room. Inspired by this behavior, we ask whether VLMs can reason directly in their latent visual embedding space, weaving compact visual embeddings into the text stream and dispensing with the need for explicit image generation. To this end, we present Mirage, decoding mechanism that interleaves latent visual representations among text tokens. Prior studies have shown that LLMs can reason directly within the latent space. Building upon this insight, in our Mirage framework, when the model chooses to reason visually by producing special token, it then reuses its current hidden state as compact visual embedding and appends it to the context, skipping the language projection. These internal embeddings furnish focused visual cues for later reasoning steps. As illustrated in Fig. 1, Mirage yields chain-of-thought trajectory without any external image decoder. As illustrated in Fig. 2, we adopt two-stage fine-tuning paradigm to equip the model with interleaved reasoning. In the first stage, with annotated interleaving trajectories, we supervise both modalities: the model predicts the next word while reconstructing compact latent visual vector obtained from compressed image embeddings. This dual objective anchors the latent tokens in the visual subspace and teaches the model to weave visual cues into its output. The second stage removes direct supervision on the latent vectors and optimizes only the text tokens, letting the model treat its autoregressively generated latent embeddings as priors that guide subsequent word generation. This relaxation yields more flexible interleave reasoning trajectory without forcing the latent channel to match any predetermined embedding. After these two stages, we apply reinforcement learning to further boost the reasoning performance. Extensive experiments and superior performance across multiple benchmarks demonstrate that our proposed Mirage significantly enhances the reasoning ability of VLMs compared with text-only decoding. More concretely, our contributions are threefold, 2 We introduce Mirage, which enables VLMs to generate interleaved reasoning trajectories that mix latent visual tokens with ordinary text, without relying on external visual decoders. Our two-stage training paradigm empowers VLMs to produce stable yet flexible interleaved reasoning and shows that reinforcement learning can further boost performance. Mirage achieves consistent gains across diverse multimodal reasoning benchmarks. Further analysis reveals that the latent tokens embody meaningful visual cues, underscoring the potential to unlock deeper multimodal reasoning capabilities in VLMs."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Multimodal Chain-of-Thought Chain-of-Thought (CoT) prompting was first shown to elicit step-by-step reasoning in LLMs by supplying few worked examples that include intermediate rationales [Feng et al., 2023, Zhang et al., 2024a, Wei et al., 2023]. Recent extensions of CoT to multimodal settings embed visual evidence directly into the reasoning trajectory. ICoT [Zhang et al., 2024b] interleaves attention-selected image crops with text tokens, yielding significant VQA gains, while Visual CoT [Shao et al., 2024a] supplies 438 bounding-box-grounded rationales to train VLMs that emit explicit visual tokens and improve spatial grounding. Recent works [Hu et al., 2024, Zhou et al., 2024, Yang et al., 2025, Gao et al., 2024, Wu et al., 2025, Chern et al., 2025, Fang et al., 2025, Cheng et al., 2025a, Su et al., 2025] further leverage external tools to supply visual cues that enrich multimodal CoT reasoning.. Recent works [Chen et al., 2025, Wang et al., 2025] like Chameleon [Team, 2025, Chern et al., 2024] trains unified token-based model that can emit arbitrary sequences of text and image tokens, but at the cost of large-scale pixel-level supervision and heavier decoding. MVoT [Li et al., 2025a] further trains unified model to directly produce image and text interleaving trajectories, but absent of reasoning thoughts. In contrast, our Mirage framework differs by emitting compact latent visual tokens rather than real image patches or pixels, avoiding heavy image generation while still allowing fully interleaved visualtext reasoning. 2.2 Latent Reasoning in LLMs Much recent work has highlighted the importance of intermediate hidden representations in Large Language Models (LLMs) [Biran et al., 2024, Yang et al., 2024a]. To better guide the latent reasoning process, several approaches introduce specialized tokens into the input sequence. Wang et al. [2023] incorporate discrete <plan> tokens to control reasoning stages, while Goyal et al. [2023] propose inserting <pause> token during pretraining to stabilize multi-step reasoning. Another line of work seeks to internalize reasoning behavior by distilling chain-of-thought rationales into latent representations. Deng et al. [2023] trains models to mimic CoT-style reasoning implicitly through hidden states, and [Deng et al., 2024] further improves inference efficiency by removing explicit intermediate steps altogether. Yu et al. [2024] proposes to distill latent reasoning capabilities into model by supervising it with data generated for complex reasoning. More recently, Hao et al. [2024] go further by replacing CoT tokens with continuous latent embeddings, enabling unconstrained reasoning in the latent space to explore on complex tasks including math and logical reasoning. While prior work primarily focuses on enhancing efficiency or structural planning within the LLMs latent space, our approach takes different perspective: we treat latent tokens as bridge for exploring visual information into the model."
        },
        {
            "title": "3 Multimodal Reasoning with Latent Visual Tokens",
            "content": "Inspired by the cognitive process of mental imagery, we introduce Mirage, framework that lets VLMs reason in interleaved multimodal trajectories. In contrast to prior unified models that integrate an external image decoder and pre-train on large-scale image generation, our method generates compact latent embeddings that serve as visual tokens. By sidestepping image generation, the model can devote its capacity to reasoning, producing only the essential visual cues and thereby echoing the concise, sketch-like representations humans employ during reasoning. 3 Figure 2: Pipeline of Mirage Framework. Stage 1 jointly supervises text and latent visual tokens, grounding the latter in the visual subspace; Stage 2 drops the latent supervision, anchoring the grounded latent tokens for subsequent text generation. In this section, we first explain how we synthesize informative multimodal reasoning data (Sec. 3.1). Next, we introduce our first joint supervision training stage in Sec. 3.2. Finally, we explain the second stage, which applies text-only supervision while relaxing the latent constraints (Sec. 3.3). 3.1 Data Generation Consider the multimodal reasoning task where the VLMs need to generate responses to the input that consists of one or more images and textual query. For simplicity, we denote the input that contains both image and text as x. Given VLMs naturally generating text tokens only, they require additional supervised fine-tuning to learn an interleaved reasoning pattern. We therefore begin by synthesizing training corpus that pairs each input with task-specific helper image (See Fig. 3). For example, in the navigation task, the helper image can be generated by taking the ground truth action list and manually drawing the corresponding path on the starting map with red arrows. Similarly, for the jigsaw task, we can concatenate the candidate fragments to form composite image that captures the relationship among pieces. More details on the image generation procedures for different tasks can be found in the supplementary materials. In general, we obtain help image that delivers precisely the visual cues needed to supervise latent reasoning. With the helper image prepared, we next synthesize reasoning chain where the LLM incorporates the helper image to generate the final solution. Specifically, we first feed large reasoning VLM with the original input x, the ground-truth answer y, and the helper image and prompt it to generate step-by-step reasoning that incorporates the helper image. For example, the prompt can be Generate step-by-step reasoning that leads to the ground-truth answer while properly incorporating the helper image in reasoning. Denote the model response as = (x, y, I) . Here is step-by-step reasoning with the helper image embedded in the reasoning process. Since the helper image is embedded in the reasoning chain, it splits the reasoning chain into two parts. Without loss of generality, we represent = opre opost, where is the concatenation operation, opre is the reasoning chain before the helper image while opost is the reasoning chain after the helper image. By prompt the large reasoning VLM with different inputs, we can thus collect training dataset = {x(i), (i), o(i), y(i)}N i=1, where each o(i) is synthesized reasoning chain with text and image interleaved. 3.2 Joint Supervision for Latent Grounding To teach the model an interleaved style of reasoning, one naive solution is to directly train VLM on the data collected above. However, the effectiveness can be negatively affected by the models limited capability of synthesizing helper images. Therefore, we propose novel training strategy: pass the helper images to the VLM first to convert the helper images in the synthetic training data into patch-level features; then fine-tune the VLM to output such features as latent reasoning tokens, thus eliminating the need to generate helper images by the VLM. 4 Figure 3: Data-generation Pipeline. For each questionanswer pair, we first create helper image with task-specific tools (here, annotate the map with arrows), then prompt VLM to produce textual reasoning that embeds this image. The text and helper image together form the synthetic multimodal trajectory used for training. More specifically, for each training example (x, I, o, y) D, we pass the helper image through the VLM fθ() with parameter θ to obtain its patch-level features {e1, . . . , en} = fθ(I). Rather than asking the model to reproduce every patch, we mimic human mental imagery by compressing these features into salient vectors, {ˆe1, . . . , ˆek} = Compress(cid:0){e1, . . . , en}(cid:1), that retain only task-critical visual cues. In this work, we realize Compress() with simple average pooling over the original patch features, lightweight yet surprisingly effective strategy that supplies concise visual summary for supervision. We then train our model to (1) generate the response opre conditioned on the input x, (2) generate the latent tokens {ˆe1, . . . , ˆek} conditioned on and opre, where the last layer hidden states at corresponding positions will be regarded as the generated latent tokens, and (3) generate the response opost conditioned on the proceeding content. For the training objective for latent token generation, we adopt the cosine similarity between the last layer hidden states of the model and the target latent tokens: Lvisual = ℓcos (cid:0)opre, ˆe1:j1 (cid:1) denotes the models prediction for the j-th latent token conditioned on the where gθ preceding context. This loss grounds the latent tokens firmly in the visual representation space. (cid:0)opre, ˆe1:j1 ˆej, gθ (1) , (cid:16) (cid:1)(cid:17) Meanwhile, we train the surrounding textual tokens using the standard cross-entropy loss for next token prediction. For the left segment opre the model conditions only on earlier words, whereas for the right segment opost it also attends to the compressed visual embeddings. Ltext = opre (cid:88) i= ℓCE (cid:0)opre,i, fθ(x, opre,<i)(cid:1) + opost (cid:88) i=1 ℓCE (cid:0)opost,i, fθ(x, opre, {ˆej}k 1, opost,<i (cid:1). (2) Here fθ(x) denotes the next token prediction probability conditioned on the input and {ˆej}k 1 is the set of the ground-truth latent tokens. The overall training objective in this stage combines this term with the visual-alignment loss L1 = Lvisual + γLtext, where the γ is the loss coefficient, thereby anchoring the latent tokens in visual space while teaching the model to weave them naturally into its textual thoughts. 3.3 Text-Only Supervision with Latent Relaxation The first stage grounds the latent tokens by forcing the model to reconstruct the compressed image embeddings. Although effective for visual alignment, this can over-constrain the model, diverting capacity from its primary goal of answering the question correctly, degrading the reasoning performance. Therefore, in the second stage, we remove the cosine loss altogether and keep only the cross-entropy loss over text tokens. Although the latent tokens no longer carry an explicit loss, we still anchor them so that they meaningfully guide the following thoughts. For each training instance, the model first autoregressively produces its own latent tokens {ei}k i=1, with ej = fθ (cid:0)x, opre, e<j (cid:1). 5 (3) These self-generated embeddings replace the compressed image vectors used in Stage 1 and serve as priors for the tokens that follow the image placeholder. Therefore, the training objective becomes opost (cid:88) opre (cid:88) ℓCE (cid:0)opost,i, fθ(x, opre, {ej}k 1, opost,<i (cid:1). (4) ℓCE (cid:0)opre,i, fθ(x, opre,<i)(cid:1) + Ltext = i=1 i= Due to the continuous property of {ei}k i=1, these self-generated latent tokens are fully differentiable. Since the next token prediction of opost is function of the latent tokens, the gradient can be propagated to these latent tokens when minimizing the above loss on the textual tokens. This allows us to optimize the generation of the latent tokens within the learned visual subspace, acting as flexible priors that guide subsequent text generation and yield more adaptive, task-focused reasoning. The overall framework of our two-stage pipeline is provided in Fig. 2. These two stages jointly endow VLMs with the ability to generate interleaved multimodal reasoning with latent visual tokens. Empirical results in Sec. 4.2 further validate the effectiveness of our latent reasoning over naive text-only decoding. 3.4 Reinforcement Learning After the two supervised fine-tuning stages, the model has already learned to reason using both interleaved text and latent tokens. Here, we further explore whether the models performance can be improved using reinforcement learning (RL), inspired by recent long-CoT language models [Xie et al., 2025, Shen et al., 2025]. Specifically, we adopt group relative policy optimization (GRPO) [Shao et al., 2024b] for RL training. For each input query in the training set, we sample multiple responses from the model. During RL, we explicitly optimize the probabilities of textual tokens while allowing gradients to flow through the latent tokens. Following LMM-R1 [Peng et al., 2025], we adopt two types of rewards: accuracy and format. We consider both accuracy and format rewards. For accuracy reward, we set racc(o, x) = 1 if the final answer is correct, and 0 otherwise. For the format reward, we check whether the thinking process is enclosed between <think> and </think> tags and whether the final answer format is formatted as boxed{} in the output response o. If the format is correct, the reward is 0.1; otherwise, it is 0. We then use the aggregated reward for optimization."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Benchmarks. We evaluate our approach on four spatial reasoning benchmarks. VSP [Wu et al., 2024] measures spatial planning in simulated maze-navigation environment. In addition to its main task, we adopt its spatial reasoning subtask, which asks the model to predict the outcome of prescribed action sequence. We extend the original binary choice to three-way classification. BLINK-Jigsaw [Fu et al., 2024] systematically evaluates the capacity of multimodal large language models to extrapolate global structural and semantic information from incomplete visual inputs, thereby assessing their proficiency in reasoning about spatial organization and maintaining perceptual coherence at fine-grained level. SAT [Ray et al., 2024] evaluates both static and dynamic spatial relations. Additionally, we include the Mathematical Geometry subset of the recent COMT [Cheng et al., 2025b] to assess formal spatial reasoning in mathematical contexts. Full dataset details are provided in the supplementary material. Data Synthesis. For each task, we sample 1k training instances for fine-tuning and 2k instances for reinforcement learning. COMT uniquely provides interleaved multimodal reasoning trajectories, which we directly use as both helper images and reasoning supervision. For the other benchmarks, we synthesize helper images and reasoning thoughts following the procedure outlined in Sec. 3.1. For VSP, the helper image is either the start map annotated with the red-arrow path (planning task) or the agents current state snapshot (reasoning subtask). In Jigsaw, we concatenate one candidate patch beside the reference image. For SAT, we prompt powerful video generation model CogVideoX-5B [Yang et al., 2024b] to render scene that matches the textual description. With the generated helper image, we then employ Qwen2.5-VL 32B [Bai et al., 2025] as the external reasoning model Mr to generate textual thoughts. Specifically, three distinct reasoning trajectories are generated per helper image to encourage diversity in model outputs. Full synthesis details are provided in the supplementary material. 6 Table 1: Experimental Results on Visual-Spatial Planning (VSP) tasks. Spatial Planning Spatial Reasoning VSP Level 3 Level 4 Level 5 Level 6 Avg. Level 3 Level 4 Level 5 Level 6 Avg. Zero-Shot Direct SFT CoT SFT GRPO CoT SFT + GRPO Anole MVoT Ours (Direct) Ours (CoT) + w/ GRPO 0.32 0.83 0.88 0.54 0. 0.46 0.53 0.86 0.87 0.92 0.23 0.81 0.86 0.49 0.85 0.51 0.64 0.84 0.92 0.90 0.40 0.85 0.80 0.76 0. 0.49 0.67 0.88 0.86 0.86 0.32 0.86 0.83 0.67 0.8 0.63 0.59 0.87 0.84 0.88 0.32 0.83 0.84 0.62 0. 0.52 0.61 0.86 0.87 0.89 0.10 0.88 0.68 0.42 0.65 0.02 0.21 0.93 0.75 0.78 0.08 0.81 0.53 0.35 0. 0.01 0.11 0.83 0.63 0.65 0.05 0.73 0.35 0.26 0.43 0.00 0.08 0.76 0.53 0.52 0.01 0.47 0.31 0.08 0. 0.00 0.03 0.51 0.39 0.43 0.06 0.72 0.47 0.28 0.51 0.01 0.11 0.76 0.58 0.60 Baselines. We compare our approach against both text-only baselines and recent unified multimodal models. First, we fine-tune the model directly using answer labels and also evaluate zero-shot reinforcement learning without any supervised warm-up. Next, using our synthetic data, we perform chain-of-thought supervised fine-tuning (CoT SFT) and then add reinforcement learning, giving fair comparison. In addition, we benchmark against unified model Anole [Chern et al., 2024], training with the same multimodal supervision, and MVoT [Li et al., 2025a], which generates action and state images but does not incorporate explicit reasoning thoughts during training. Implementation Details. In this work, unless stated otherwise, all experiments use Qwen2.5-VL 7B as the base model. We perform supervised fine-tuning using batch size of 8 and cosine learning rate scheduler with an initial learning rate of 1e-5 for both stages. The random seed is fixed at 42 to ensure reproducibility. Reinforcement learning is implemented with the Verl framework. Unless stated otherwise, we use latent token size of = 4 and loss coefficient of γ = 0.1. 4.2 Experimental Results We first evaluate the effectiveness of our method on the VSP benchmark. The results are shown in Tab. 1. We highlight the following findings. First, adding latent visual tokens to the reasoning process significantly improves the reasoning capability of VLMs compared to text-only baselines. Compared to directly fine-tuning the VLM with the synthesized data, our method achieves 3% higher accuracy on the spatial reasoning task and 11% on the spatial planning task. Also, with our two-stage training, Mirage improves the CoT SFT + GRPO, by 2% and 7%, respectively. This demonstrates the effectiveness of the proposed two-stage training method. Also, we test our method on COMT, Jigsaw, and SAT tasks and present the results in Tab. 2, where we observe the consistent performance gains on both tasks, underscoring that interleaving compact visual cues consistently strengthens spatial reasoning ability. Additionally, we observe that unified model-based baselines such as MVoT and Anole, despite explicitly generating image tokens, perform poorly when faced with text and image interleave reasoning. After fine-tuning on the same data, they achieve only 61% accuracy on the spatial reasoning task and 11% on the spatial planning task. Notably, Anole struggles to even generate valid answers for the spatial planning task post fine-tuning. Following the setup in Li et al. [2025a], we construct interleaved reasoning trajectories by combining textual thoughts with simulated state images after each action step for the spatial reasoning task. While our reproduced results are lower than those reported in their paper, we attribute this discrepancy to the difference in training data. They use 6,846 samples, whereas we training with the same 1,000 samples to ensure fair comparison. Even when compared to their reported results, our model still gains an additional 2% improvement. These findings further underscore the advantage of our latent design over current unified approaches. We notice that on VSP spatial planning task, fine-tuning with synthesized reasoning thoughts performs significantly worse than training directly on answer labels, both with and without our latent design. Two factors likely contribute to this outcome. First, as noted in prior work [Li et al., 2025b], certain visual tasks that rely heavily on perception may not benefit from explicit reasoning during fine-tuning. Second, the synthesized thoughts are generated by Qwen2.5-VL-32B; although generally sound, they are not flawless, and any imperfections propagate into the base model. Likely, in SAT, the Table 2: Experimental Results on COMT, Jigsaw, and SAT tasks. Method COMT Jigsaw SAT Synthetic SAT Real GoalAim ObjM Avg. Zero-Shot Direct SFT CoT SFT GRPO SFT + GRPO Ours 0.63 0.71 0.75 - - 0.77 0.58 0.87 0.83 0.85 0.86 0. 0.50 0.95 0.97 0.85 0.93 0.98 0.63 0.95 0.90 0.80 0.85 0.98 0.57 0.95 0.94 0.83 0.89 0. 0.49 0.67 0.66 0.71 0.65 0.72 Table 3: Experimental Results with Qwen2.5-VL 3B on COMT, Jigsaw, and SAT tasks. Method COMT Jigsaw SAT Synthetic SAT Real GoalAim ObjM Avg. Zero-Shot Direct SFT CoT SFT GRPO SFT + GRPO Ours 0.40 0.67 0.65 - - 0. 0.45 0.80 0.59 0.54 0.72 0.85 0.50 0.82 0.73 0.78 0.82 0.85 0.38 0.83 0.88 0.80 0.85 0. 0.44 0.83 0.71 0.79 0.84 0.89 0.51 0.55 0.54 0.54 0.52 0.64 helper images are produced by video generation model without ground-truth annotations, which can introduce further noise to the latent prior. Despite these challenges, our latent reasoning pipeline still closes much of the performance gap, highlighting its practical robustness. Moreover, reinforcement learning can further improve the performance of our method. As shown in Tab. 1, by weaving latent visual tokens within the text trajectories, instead of placing them at the start, our model can naturally explore diverse sequences. After optimizing with GRPO, Mirage achieves extra gains (+2% accuracy) on VSP tasks. These results further confirm that interleaved latent cues provide informative guidance with flexible reasoning, highlighting the potential of our latent design. 4.3 Ablation Study In this section, we first conduct an ablation study to evaluate the influence of the two stages of our framework. Tab. 4 reports the effect of removing each training phase. Training with only the first phase, which jointly supervises text and latent visual tokens, anchors the latent embeddings but leaves them constrained and lowers performance, similar to the plight of unified models. Figure 4: Ablation Study of Training Stages on VSP Spatial Planning task. Both training stages work jointly to achieve better reasoning performance. VSP Spatial Planning Method Ours - w/o Stage 1 - w/o Stage 3 0.75 0.69 0.38 4 0.63 0.58 0.19 5 0.53 0.46 0. 6 Avg. 0.39 0.36 0.09 0.58 0.52 0.21 Training with only the second stage, which relies on text loss alone while letting latent tokens evolve freely, performs slightly better the text-only baseline. Without the grounding supplied by the first stage, the latent vectors drift into regions of the multimodal embedding space that do not aid reasoning. This outcome contrasts with findings on LLMs in Coconut [Hao et al., 2024], where unsupervised latent vectors can benefit subsequent reasoning. The difference indicates that visual and textual subspaces in VLMs remain heterogeneous enough that grounding phase is effective. These ablations confirm that the first stage aligns latent tokens with visual features, the second stage allows them to adapt to the task, and both steps are necessary for the final performance. We also include deeper analysis of the generated latent embeddings in Sec. 5. 8 Figure 6: Performance with Helper Images as Input Priors. We evaluate model accuracy using synthesized helper images under both zero-shot and fine-tuned settings. The results highlight the informativeness of the generated images and confirm their high data quality. To delve deeper into the robustness of our framework, we investigate the influence of hyperparameters: latent token size and the multimodal loss coefficient γ. As Tab. 5 shows, adjusting the loss coefficient γ has moderate effect. larger γ weights the latent-alignment loss less in the first stage. When γ approaches infinity, the first stage becomes equivalent to skipping visual supervision entirely, in other words, the second stage. This gives poor initialization for subsequent training. Even so, after the second stage, each γ tested still obtains over 80% accuracy, which attests to the overall robustness of the framework. Figure 5: Ablation Study of Latent Size and Loss Coefficient γ on VSP Spatial Reasoning. Our training pipeline remains robust and superior performance across different hyperparameters. 2 4 6 8 4 4 γ 0.1 0.1 0.1 0.1 0.5 VSP Spatial Reasoning 3 0.85 0.87 0.85 0.77 0.84 0.77 4 0.86 0.92 0.90 0.77 0.91 0.85 0.89 0.86 0.91 0.74 0.84 0.85 6 Avg. 0.93 0.84 0.87 0.70 0.78 0.87 0.86 0.87 0.88 0.75 0.84 0.83 We observe that varying the latent size from 2 to 6 yields consistently strong performance, with = 6 showing slight improvementhighlighting the resilience of our latent design. However, increasing to 8 results in significant performance drop around 13%, likely due to error accumulation in longer latent sequences under autoregressive non-decoding generation. These observations are consistent with prior findings that optimal latent reasoning performance in LLMs typically occurs with fewer than 6 latent tokens [Hao et al., 2024]."
        },
        {
            "title": "5 Analysis",
            "content": "Generalization to Smaller Models. To further investigate the impact of our latent design, we also evaluate performance using the Qwen2.5-VL 3B model. As shown in Tab. 3, results are consistent with those observed on the 7B model, demonstrating clear performance gains across both tasks. Notably, compared to text-only baselines, our Mirage achieves even larger improvements5% on the Jigsaw task and 10% on the SAT Real task. These findings further highlight the strength of our latent design and its potential to generalize across different model scales. Synthesized Data Quality. Data quality plays critical role in model performance. In this section, we investigate whether the helper images generated by various tools are genuinely informative for VLM reasoning. For the two VSP tasks, we supply the helper image as prior input and evaluate model performance in both zero-shot and fine-tuned settings. As shown in Fig. 6, providing the helper image leads both models to achieve nearly 100% accuracy on both tasks. Even in the zero-shot setting, we observe substantial performance gains on the spatial reasoning task. However, improvements on the spatial planning task are limited to simpler map layouts in the zero-shot setting. We attribute this to the inherent difficulty of extracting and leveraging spatial information from the helper image without task-specific fine-tuning. These results suggest that the synthesized helper images do indeed enhance VLM reasoning. Moreover, if the models latent thoughts can fully internalize the information encoded in these images, it would represent strong performance upper bound for our Mirage. 9 Figure 7: Visualization of Latent Embeddings. We visualize our latent tokens along with text and image embeddings with t-SNE. Our latent tokens cluster near, yet just outside, the visual representation subspace, consistent with the two-stage training design. Latent Behavior Analysis. During the first stage, the model learns to reproduce compressed image embeddings, anchoring its latent tokens in the visual subspace. However, after the second stage, these latent tokens receive no direct supervision. Therefore, it is unclear whether they still encode visual representations. Therefore, in this section, we further investigate the latent behaviors of our Mirage. By sampling 100 examples from each dataset, we obtain the corresponding latent token vectors alongside the text and visual embeddings. Next, we use t-SNE to embed all vectors into two dimensions for better visualization with perplexity of 30, and initialize the embeddings via PCA. As shown in Fig. 7, the text embeddings (blue dots) fill the entire plot in radial scattering pattern, while the visual token embeddings (yellow dots) cluster tightly inside distinct visual subspace, consistent with previous findings. Our latent embeddings (red dots) form compact cloud that sits just outside that visual cluster, shifted by the second training stage, which tailors the latent embeddings to answer generation. However, we notice that our latent tokens remain clearly separated from the text distribution and closer to the visual embedding across diverse tasks. This pattern shows that even without an explicit decoder, the latent tokens stay close to the visual manifold while retaining the flexibility introduced in the second stage, echoing the way mental imagery abstracts rather than reproduces visual input."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, mimicking human mental imagery, we propose Mirage, lightweight framework that interleaves compact latent visual tokens with text so visionlanguage model can reason multimodally without ever generating pixel-level images. Specifically, our framework is trained in two stages: joint supervision stage that anchors latent tokens to visual embeddings while learning the surrounding text, followed by text-only supervision stage that lets those tokens adapt freely to support answer generation. brief reinforcement-learning refinement further aligns the entire trajectory with task goals. Across four spatial-reasoning benchmarks, Mirage consistently outperforms text-only baselines, underscoring the effectiveness and potential of latent visual reasoning for multimodal models. Limitations and Future Works. While effective, our framework has certain limitations: Synthetic Data Quality: The performance of our interleaved reasoning depends on the quality of the generated multimodal trajectories. Carefully curating high-quality datasets for unified reasoning models is an important next step. Extend to Unified Models: Our framework explores the latent space within reasoning model, whereas unified models jointly align the latent space through image and text token generation during training. Despite current limitations in interleaved generation performance, whether the aligned feature space of unified models can be leveraged to further improve latent reasoning design remains an open question. Task Scale beyond Spatial Reasoning: Currently, our evaluation is limited to spatial-reasoning benchmarks. How to extend our framework to broader multimodal or purely textual tasks remains an open direction."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 19 Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024. 3 Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 1, 3 Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, Wanxiang Che, et al. Visual thoughts: unified perspective of understanding multimodal chain-of-thought. arXiv preprint arXiv:2505.15510, 2025a. 3 Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng, Wanxiang Che, Min Li, and Libo Qin. Comt: novel benchmark for chain of multi-modal thought on large vision-language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2367823686, 2025b. 6 Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. 1, 3, 7 Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. 3 Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023. 3 Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. 3 Irving Fang, Juexiao Zhang, Shengbang Tong, and Chen Feng. From intention to execution: Probing the generalization boundaries of vision-language-action models. arXiv preprint arXiv:2506.09930, 2025. 3 Martha Farah. Psychophysical evidence for shared representational medium for mental images and percepts. Journal of Experimental Psychology: General, 114(1):91, 1985. 2 Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36: 7075770798, 2023. 3 Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. 6 Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, et al. Cantor: Inspiring multimodal chain-of-thought of mllm. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 90969105, 2024. 3 Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. 3 Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 3, 8, Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. 3 Stephen Kosslyn. Image and brain: The resolution of the imagery debate. MIT press, 1996. 2 Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025a. 3, 7 11 Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, and Kaipeng Zhang. Think or not think: study of explicit thinking in rule-based visual reinforcement fine-tuning. arXiv preprint arXiv:2503.16188, 2025b. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. 6 Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. 6 Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning, 2024a. URL https://arxiv.org/abs/2403.16999. 3 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. 6, 19 Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508, 2025. 6 Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Roger Shepard and Jacqueline Metzler. Mental rotation of three-dimensional objects. Science, 171(3972): 701703, 1971. 2 Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 3 Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1 Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2025. URL https://arxiv. org/abs/2405.09818. 3 Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. 1 Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng Yu, Zhongyu Wei, and Jiaqi Wang. Autoregressive semantic visual reconstruction helps vlms understand better. arXiv preprint arXiv:2506.09040, 2025. 1, 3 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1 Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https: //arxiv.org/abs/2201.11903. 3 Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 3 Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024. 6 Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. Teaching language models to critique via reinforcement learning. arXiv preprint arXiv:2502.03492, 2025. 6 Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024a. 3 Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, and Chuang Gan. 3d-mem: 3d scene memory for embodied exploration and reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1729417303, 2025. 3 Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. 6 Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. Chain of preference optimization: Improving chain-of-thought reasoning in llms. Advances in Neural Information Processing Systems, 37: 333356, 2024a. 3 Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofthought reasoning in language models, 2024b. URL https://arxiv.org/abs/2302.00923. 3 Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought prompting for visual reasoning refinement in multimodal large language models. arXiv preprint arXiv:2405.13872, 2024."
        },
        {
            "title": "A Datasets",
            "content": "A.1 Help Image Generation Diverse task-specific tools are employed to generate the helper images used in fine-tuning. In this section, we will detail the generation pipeline for each task. VSP Spatial Reasoning. To assist in inferring the final state after sequence of actions, we leverage the map layout visualization as the helper image, including the agent position after part of the action trajectory. Following the VSP implementation, we render this state with the OpenAI Gym package [Brockman et al., 2016], using the initial map and the action sequence as inputs. Figure 8: An example of the helper image of the VSP Spatial Reasoning task. VSP Spatial Planning. For the planning task, we provide map annotated with the ground-truth path, turning the problem into simply reading the highlighted trajectory. Specifically, we select one valid action sequence for each sample and highlight its steps as red arrow that begins at the agents start position and ends at the goal. Figure 9: An example of the helper image of the VSP Spatial Planning task. Blink Jigsaw. The Jigsaw task asks which candidate patch completes the reference image. For each instance we create helper image by inserting one randomly chosen candidate patch into the masked region. The model then can judge whether the composite looks seamless: if the patch blends smoothly, it is the correct answer; if not, the other candidate should be chosen. 14 Figure 10: An example of the helper image of the BLINK task. SAT. For the SAT task, we focus on the GoalAim and ObjM subtasks, which require reasoning about specified camera pose movement. Providing the target view as helper image would ease the models spatial reasoning burden. Therefore, given the recent advance in world model research, we adopt high-quality video generation model CogVideoX-5B to generate this image. To further ensure the image quality, we restrict the action condition for generation to three primitives: move forward, turn left, and turn right. Sampling 9 frames along each trajectory, we instruct VLM to choose the most informative frame. The chosen frame is then used as the helper image. Figure 11: An example of the helper image of the SAT task. A.2 Textual Thoughts Generation For each task, we generate the textual thoughts instead of leveraging closed-source outputs. We feed the helper image and the ground truth answer to large reasoning model Qwen2.5-VL 32B. Task-specific prompts are applied. Simplified prompts and one illustrative example per task are provided in Tab. 47. 15 The generated thoughts and the associated helper image serve as the supervision for fine-tuning, and the quality of these explanations sets an upper bound on our models performance. Our current approach relies on straightforward prompts, which occasionally yield subpar reasoning trajectories. Developing richer prompts or otherwise curating higher-quality trajectories remains an important future work. Table 4: Data Example of VSP Spatial Reasoning VSP Spatial Reasoning Thoughts Generation System Prompt: You are analysing **one move** in FrozenLake. Tiles (numeric): 1=Start point, 0=Ice, -1=Hole, 2=Target End-states: success, fail in hole, fail on ice Task: Given the map, current position, and one action, write **one short sentence thought** that states: 1. the square reached [(row,col), TileType]; 2. how that move shifts the likelihood of the three end-states. No bullets, no extra lines, just the single sentence. Current numeric map: {map_desc} Current position: {position} Chosen action: {action} Example 1 Numeric map: [[1,0,0],[0,-1,0],[0,0,2]] At (0,0) action RIGHT Thought: slide onto [(0,1),Ice] staying safe and inching closer to the target, so success is bit likelier while hole-failure remains unchanged. Example 2 Numeric map: [[1,0,-1],[0,-1,0],[0,0,2]] At (1,0) action RIGHT Thought: Moving to [(1,1),Hole] ends the game instantly in hole, making \"fail in hole\" certain and wiping out other outcomes. Example Input: Task: FrozenLake Given the initial state and action sequence, visualize the final state of the frozen lake map and determine whether the agent (elf character) can safely reach the gift following the action sequence without falling into the holes. If not, identify the failure reason from the given options. The definitions of the actions are as below. Go Up/Left/Down/Right: move one grid space in the absolute up/left/down/right direction. Return A, or C. A. Action Success. B. Action Failed: Fall into the Hole. C. Action Failed: Agent Safe. Full Action Sequence: Go Left, Go Down Here is the image of the initial state of the frozenlake map. <image> The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{}. Example Output: <think> Moving to [(1,3),Hole] ends the game instantly in hole, making fail in holecertain and wiping out other outcomes. Next, Moving to [(1,4),Hole] ends the game instantly in hole, making fail in holecertain and wiping out other outcomes. Next, Moving to [(2,4),Ice] keeps me on the slippery path, increasing the likelihood of success slightly while the risk of failing in hole remains high due to nearby hazards. </think>The answer is boxed{A}. 16 Table 5: Data Example of VSP Spatial Planning VSP Spatial Planning Thoughts Generation System Prompt: You are concise maze navigator. Input: (1) an image of maze, (2) second image, and (3) proposed action sequence. Task: Write 1-2 English sentences that logically justify why the given action sequence reaches the treasure. - Pretend you derived the sequence yourself; NEVER say it was supplied. - NEVER mention or allude to the second image or any highlighted route. - Do not output anything else. Example Input: As professional maze solver, your task is to analyze grid-based map and devise an action plan that enables player to reach the goal from the starting point without falling into any holes, using the fewest possible moves. ## Game Setup - The game presents fully observable grid-based map. - The player starts at specified grid square, with the goal located elsewhere on the map. - Each grid square is either safe or contains hole. - Your goal is to guide the player to the goal while avoiding holes. - For example, your final answer should be like \"L,L,R,U,D\", which means an action plan of left, left, right, up, and down. ## Moving Rules - The action plan involves series of moves: (left), (right), (up), or (down). - Each move transfers the player to the adjacent square in that direction, provided it is safe square. The player cannot move more than one square at time. - Moving off the edge of the map has no effect. The player will remain at the same square. - DO NOT MOVE INTO HOLE! Falling into hole results in defeat. - Locating at the grid containing the goal results in victory. Here is the map: <image> Please provide your action plan. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{}. Example Output: <think> Moving RIGHT is the best choice because it allows me to avoid the hole directly below me and keeps me on path that is moving closer to the goal G. Moving up would lead me to frozen square but doesnt advance me towards the goal, while moving down would immediately result in falling into hole. Therefore, moving RIGHT is the safest and most efficient option at this moment. Next, Moving down is strategic choice because it avoids the immediate danger of the hole to the right. It also moves the agent closer to the goal by entering column that is likely to lead towards without any immediate threats. Moving left or right poses higher risk of falling into hole, whereas moving down appears to be safe and progressive step towards the goal. Next, Moving DOWN from position is strategic choice because it avoids the immediate danger to the left and right while still moving closer to the goal. Moving RIGHT would risk falling into the hole, and moving LEFT would take me further away from the goal. Thus, moving DOWN is the safest and most direct path towards the goal. Next, Moving DOWN is the best choice because it allows me to continue progressing towards the goal while avoiding immediate danger. To the right, there is hole, and moving left would not bring me any closer to my objective. Moving up would be counterproductive as it would take me further away from the goal. Thus, moving DOWN is the safest and most efficient option. <image></think> The answer is boxed{RIGHT, DOWN, DOWN, DOWN}. 17 Table 6: Data Example of BLINK Jigsaw BLINK Jigsaw Thoughts Generation System Prompt: You are concise visual inspector. When shown: (1) jigsaw with missing section, (2) two candidate pieces, and (3) the fully assembled image using the correct piece, you must give brief justification of why the chosen piece fits. Write 1-2 English sentences, focus on visual cues such as edge continuity, color/lighting consistency, perspective, and semantics. Example Input: Given the first image with the lower right corner missing, can you tell which one of the second image or the third image is the missing part? Imagine which image would be more appropriate to place in the missing spot. You can also carefully observe and compare the edges of the images. Select from the following choices (A) the second image (B) the third image <image><image><image> The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{}. Example Output: <think> Option (B) shows person in the foreground with light source, which disrupts the continuity of the crowd and lighting in the main image. </think> <image> <think> The lighting and shadows in the main image suggest more uniform and darker environment, which is not consistent with the brighter, focused light in the wrong piece. </think> boxed{A}. Table 7: Data Example of SAT SAT Thoughts Generation System Prompt: You are concise spatial-reasoning expert. Input items may include one or more scene images, question, an additional helper image, the available answer choices, and special field called TargetAnswer. - Use any information you need internally, but NEVER mention or allude to the additional image or the TargetAnswer. - Write 1 short English sentence that logically deduces the answer. - Embed the additional image exactly once using the literal token <image>. Example Input: You will be given one or two images and spatial reasoning question. Your goal is to answer the spatial related question correctly. You should output an answer from the answer choice provided below. Now, according to the following image(s), answer the question from the provided choices. Question: If turn right by 33 degrees, will be facing away from dark frame red plaid blanket red (near the mark 2 in the image)? Answer Choice: yes no <image> The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{}. Example Output: <image> <think> Turning right by 33 degrees will not make you face away from the bed </think> The answer is: boxed{no}. 18 A.3 Data Configuration For the Mathematical Geometry subset of COMT, we randomly sample 200 examples for evaluation and use the remaining 820 for fine-tuning. For each other benchmark, we fine-tune on 1,000 samples and use another 2,000 non-overlapping samples for reinforcement learning. Blink-Jigsaw and SAT samples are drawn at random from their official training splits. VSP provides no training set, so we follow its data generation recipe and synthesize our own data. We ensure that each map contains valid path, no more than 20% trap blocks, and grid sizes 36 are produced in 1:2:3:4 ratio (100, 200, 300, and 400 examples, respectively, for fine-tuning). Additionally, for each sample in VSP, we generate three distinct reasoning trajectories to encourage diversity. Full dataset statistics are provided in Tab. 8. Table 8: Dataset Statistics"
        },
        {
            "title": "Task",
            "content": "# SFT # RL # Test"
        },
        {
            "title": "VSP Spatial Reasoning\nVSP Spatial Planning\nBlink Jigsaw\nSAT\nCOMT",
            "content": "3,000 3,000 1,000 1,000 820 2,000 2,000 2,000 2,000 - 400 400 150"
        },
        {
            "title": "B Experiments",
            "content": "B.1 Implementation Details Fine-tuning. We adopt Qwen2.5-VL-7B-Instruct Bai et al. [2025] as our base VLM. The detailed training configurations are provided in Table 9. During fine-tuning, all components of the model are trainable except for the vision encoder. The training objective combines cross-entropy loss for next-token prediction with cosine similarity loss for aligning latent visual tokens, as described in Sec. 4.1. The loss weight γ for the visual alignment loss is set to the default value of 0.1. Both the training stage 1 and the training stage 2 employ the same configurations. Table 9: Implementation details of Supervised Fine-tuning. Value Config Value Config optimizer optimizer momentum β1 optimizer momentum β2 optimizer weight decay learning rate Adam batch size 0.9 0.95 0.01 1e-5 gradient accumulation steps warmup steps training epochs loss weight γ 8 2 10 10 10 Table 10: Implementation details of Reinforcement Learning. Config Value Config prompt Length limit learning rate gradient accumulation steps training epochs σf λkl 1024 1e-6 4 15 0.1 0.01 response length limit batch size rollout num mini batch size σc λen Value 1024 32 5 8 0.9 0. Reinforcement Learning. We adopt VERL Sheng et al. [2024] as the RL framework, and provide the detailed training settings in Tab. 10. Specifically, we utilize Group Relative Policy Optimization (GRPO) Shao et al. [2024b] for reinforcement learning. The reward function consists of format reward and correctness reward, weighted by σf and σc, respectively. KL regularization is applied 19 with coefficient of λkl, while entropy regularization is disabled in the policy loss by setting λen = 0. For our Mirage, the KL divergence on latent visual tokens is omitted during RL training. B.2 Efficiency Analysis Both training stages of Mirage are conducted on single NVIDIA H100 GPU. Taking the VSP spatial reasoning task as an example, Stage 1 completes in approximately 3.5 hours, while Stage 2 takes around 7.2 hours. For reference, text-only CoT SFT on the same hardware requires about 5.5 hours."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "University of Massachusetts, Amherst"
    ]
}