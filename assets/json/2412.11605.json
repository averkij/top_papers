{
    "paper_title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models",
    "authors": [
        "Jiale Cheng",
        "Xiao Liu",
        "Cunxiang Wang",
        "Xiaotao Gu",
        "Yida Lu",
        "Dan Zhang",
        "Yuxiao Dong",
        "Jie Tang",
        "Hongning Wang",
        "Minlie Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often optimized by preference learning. However, existing methods often directly sample multiple independent responses from the model when creating preference pairs. Such practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), interfering with the goal of teaching models to recognize the key differences that lead to improved instruction following. In light of this, we introduce SPaR, a self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions. By playing against itself, an LLM employs a tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations. Our experiments show that a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities. Furthermore, SPaR demonstrates promising scalability and transferability, greatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree search would impact model performance. Our code and data are publicly available at https://github.com/thu-coai/SPaR."
        },
        {
            "title": "Start",
            "content": "SPAR: SELF-PLAY WITH TREE-SEARCH REFINEMENT TO IMPROVE INSTRUCTION-FOLLOWING IN LARGE LANGUAGE MODELS Jiale Cheng1,2, Xiao Liu2,3, Cunxiang Wang2,3 , Xiaotao Gu2 , Yida Lu1,2, Dan Zhang3 , Yuxiao Dong3 , Jie Tang3 , Hongning Wang1 , Minlie Huang1 1The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University 2Zhipu AI 3The Knowledge Engineering Group (KEG), Tsinghua University chengjl23@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Instruction-following is fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often optimized by preference learning. However, existing methods often directly sample multiple independent responses from the model when creating preference pairs. Such practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), interfering with the goal of teaching models to recognize the key differences that lead to improved instruction following. In light of this, we introduce SPAR, self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions. By playing against itself, an LLM employs tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations. Our experiments show that LLaMA38B model, trained over three iterations guided by SPAR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities. Furthermore, SPAR demonstrates promising scalability and transferability, greatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree search would impact model performance. Our code and data are publicly available at https://github.com/thu-coai/SPaR. 4 2 0 2 6 1 ] . [ 1 5 0 6 1 1 . 2 1 4 2 : r Figure 1: An example of the interfering factors (story content) in independently sampled multiple responses (Left). Refined response pairs exclude these factors, highlight the key difference (ending sentence), and lead to improved performance on iteratively trained LLaMA3-8B-Instruct (Right). Equal contributions. Work done when JC and YL interned at Zhipu AI. Corresponding author"
        },
        {
            "title": "INTRODUCTION",
            "content": "To date, Large Language Models (LLMs) have achieved great success in wide range of tasks (Brown et al., 2020; Zeng et al., 2022; Chowdhery et al., 2023; Touvron et al., 2023; GLM et al., 2024). As LLMs are applied to various scenarios, their instruction-following capability becomes crucial (Ouyang et al., 2022; Bai et al., 2022), especially to follow instructions with multiple constraints (Zeng et al., 2023; Zhou et al., 2023; Jiang et al., 2023b). The failure to accurately follow instructions can even lead to safety issues (Ruan et al., 2023). Subtle nuances can determine the success of instruction-following tasks (Zhou et al., 2023), making preference learning (Rafailov et al., 2024; Hou et al., 2024) well-suited solution. However, existing methods usually sample multiple independent responses from the target model (Yuan et al., 2024; Wu et al., 2024; Dong et al., 2024), inadvertently introducing irrelevant variations to whether the instruction was successfully followed. As illustrated in Figure 1, given the instruction: Write story and end it with The devil is in the details, sampling multiple independent responses from an LLM can result in responses as different as the story Little Red Riding Hood vs. Hansel and Gretel. This variation in the narrative content can interfere with the models ability to learn how to realize the critical requirementthe specified ending sentenceand ultimately mislead the comparison within the preference pair. Therefore, effective learning from preference pairs necessitates excluding these extraneous factors and focusing on the key differences that drive the success of instruction-following. In this paper, we propose SPAR, self-play method integrated with tree-search refinement to enhance instruction-following capabilities of LLMs. The key lies in iteratively teaching LLMs to learn instruction-following from nuances by playing against itself with structured tree search. In each turn of self-play, an LLM takes two different roles: the actor and the refiner, which are both initialized from the same model. The actor executes complex instructions while the refiner critiques and refines the actors responses. During the iteration, we first collect the actors responses which fail to follow the instructions accurately, as judged by the refiner. Starting from those failed responses, we apply tree-search algorithm for refinement, which ensures consistent improvements against previous turns and naturally creates valid comparison counterparts for model training. We conduct experiments on several LLMs, LLaMA3 series (MetaAI, 2024), GLM-4-9B (GLM et al., 2024), and Mistral-7B-Instruct (Jiang et al., 2023a), over multiple iterations. Through extensive experiments, we demonstrate significant improvements in the models instruction-following capability, outperforming other self-improvement methods such as self-rewarding (Yuan et al., 2024) and meta-rewarding (Wu et al., 2024). Notably, after three iterations, SPAR improves LLaMA3-8BInstruct over GPT-4-Turbo on the IFEval benchmark (Zhou et al., 2023). Moreover, scaling testtime compute by integrating tree-search refinement during inference can further improve the quality of instruction following. Additionally, we find that with several iterations, the refiners judgment and refinement capabilities can match or even exceed those of the distilled LLM, indicating great potential for continuous self-improvement without being limited by the initial bootstrapping data. Ablation studies demonstrate the importance of each component within our framework. Importantly, our method does not degrade performance on general benchmarks. In summary, our contributions are: We reveal that preference pairs derived from independently sampled responses often contain interfering factors, hampering preference learning to improve instruction following. As result, performing solution has to minimize such interference and highlight the key differences contributing to the success of instruction following. We introduce SPAR, novel self-play framework that enables continuous self-improvement in instruction-following tasks. Through three iterations, our method boosts LLaMA3-8B-Instruct to achieve GPT4-level performance and scales effectively to enhance LLaMA3-70B-Instruct. We construct high-quality dataset with 43K complex instruction-following prompts and an SFT dataset that can improve the instruction-following capabilities of LLMs."
        },
        {
            "title": "2 METHOD",
            "content": "We introduce SPAR, an automated and scalable approach designed for self-improvement of instruction-following tasks through self-play. The core idea is to create paired responses with 2 Figure 2: SPAR iterative training framework. At iteration t, the refiner Rt first judges the generated responses from the actor Mt to collect negative data. Next, tree-search algorithm is employed to refine these imperfect responses. Finally, using the data from the above steps, we can optimize the actor and refiner for the next iteration, aiming for continuous self-improvement. minimal irrelevant variations, thereby highlighting the key differences that manifest the success of instruction-following. 2.1 OVERALL FRAMEWORK The overall framework of SPAR is illustrated in Figure 2. Briefly, our framework involves an actor model and refiner model, which are both initialized from the same base model. The actor generates responses to given instructions while the refiner judges and refines these responses. This iterative self-play process, involving response generation, judgment, and refinement, fosters continuous selfimprovement. Formally, in each iteration, given an instruction from the prompt set, the actor generates response y. The refiner identifies the responses that do not follow the instructions accurately, termed as negative responses. Our objective is to refine the negative response (represented as y0 in Figure 2) into correct response (represented as y8 in the figure). These generated refinement pairs, e.g., (x, y8 > y0), are collected and used to optimize the actor via Direct Preference Optimization (DPO) (Rafailov et al., 2024). Simultaneously, we apply Rejection-sampling Fine-Tuning (RFT) (Yuan et al., 2023) to improve the refiner. This process prepares both models for the next iteration of self-improvement. In this iterative process, we face two major challenges: the scarcity of complex instruction-following data and the difficulty of achieving successful refinements. To address the lack of high-quality, multiconstraint instruction-following datasets, we generate complex instructions using taxonomy-based approach and create corresponding SFT datasets to initialize the actor and refiner models (2.2). To ensure high success rate in refining negative responses, we employ tree search strategy that systematically explores refinement paths and facilitates subsequent training (2.3). 2.2 DATA CONSTRUCTION 2.2.1 PROMPT CREATION Given the scarcity of high-quality data for instruction-following tasks, especially those with multiple constraints, we start by creating high-quality dataset of instruction-following prompts. Seed Prompts. To ensure the quality and diversity of our dataset, and to prevent issues like insufficient diversity or even model collapse (Liu et al., 2024; Shumailov et al., 2024), we use seed set of prompts derived from the Infinity-Instruct dataset (Zhao et al., 2024), which contains ten million high-quality conversations. After applying rule-based filtering based on length, keywords, and self-BLEU, we obtain approximately 50k seed prompts. Taxonomy-based Prompt Construction. Complex prompts constructed without human intervention tend to be poorly diversified, as the types of constraints added are often distributed unevenly (Sun et al., 2024). Therefore, we adopt taxonomy-based mechanism to make constraint types com3 prehensive and balanced. Our taxonomy for instruction-following constraints is derived from Cheng et al. (2024) and further refined to be more comprehensive. After building the constraint taxonomy, we employ it to construct complex instruction-following tasks based on seed prompts. We sample main constraint type and employ strong LLM to add several other constraints to make the original prompt more complex. Moreover, we leverage the strong LLM to assess the validity of the generated prompt, ensuring that the constraints do not conflict with each other or create unreasonable scenarios with the original task. The detailed taxonomy and prompt can be found in Appendix A."
        },
        {
            "title": "2.2.2 ACTOR AND REFINER INITIALIZATION",
            "content": "The taxonomy-based prompt construction results in about 43k prompts. We utilize 8k prompts for actor initialization, another 5k for the refiner, and save 30k for further self-play training. Actor Data Creation. To bootstrap the actor model with strong instruction-following capabilities, we first collect strong LLMs responses to these complex prompts, thereby producing supervised fine-tuning (SFT) data (x, y) DActor for the actor model, where is the complex instruction and is the strong LLMs response. Then, we fine-tune the base model to get an initial actor M0. Refiner Data Creation. To bootstrap the refiner model with strong judgment and refinement capability, we sample responses from the initial actor M0. Then, we collect the judgments from strong LLM to form dataset, (x, y, j) DJSFT. We collect responses that are judged not to accurately follow instructions and term them as negative responses. For these negative responses, we use the strong LLM to correct them with minimal revisions to avoid irrelevant variations. In this way, we get refinement dataset, (x, ynegative, j, yrefined) DRSFT. The refiner is then trained with DRefiner = DJSFT DRSFT to create the initial refiner R0. Training Strategy. For both actor and refiner models, we use standard supervised fine-tuning with the loss function: = 1 N (cid:88) i=1 logP (riq, r<i), (1) where denotes the input, signifies the target response, and represents the length of r. For actor training, we have input = and target = y. When it comes to the refiner, we use input = (x, y) and target = for DJSFT, and input = (x, ynegative, j) and target = yrefined for DRSFT. 2.3 TREE-SEARCH INTEGRATED SELF-PLAY TRAINING After initializing the actor and refiner models, we embark on an iterative process for continuous self-improvement. In each iteration, we first collect the negative data, where the responses fail to accurately follow the instructions (2.3.1). Then, we utilize tree-search algorithm to refine the negative responses (2.3.2) and form the training data for the next iteration of the actor (2.3.3) and refiner (2.3.4). This iterative self-play pipeline allows us to continuously improve both models. 2.3.1 NEGATIVE DATA COLLECTION For each prompt x, we first sample responses {y1, y2, . . . , yK} from the actor model. This step ensures that there are enough negative responses to support subsequent learning. Then, for each prompt and response pair, we utilize the refiner to generate judgment, which contains two parts: label suggesting whether the response follows the instruction and an explanation about the assessment. To make this judgment more accurate, we incorporate the self-consistency mechanism (Wang et al., 2022), which is also applied in the subsequent refinement process. Specifically, we obtain multiple judgments from the refiner and determine the final label through majority voting, as detailed in Appendix D.4. After majority voting, we randomly select one judgment that matches the voted label to serve as the final judgment. This process allows us to identify challenging prompts that elicit responses that do not accurately follow the instructions, yielding tuples in the form of (x, ynegative, j), where ynegative is the incorrect response and is its corresponding judgment."
        },
        {
            "title": "2.3.2 TREE-SEARCH REFINEMENT",
            "content": "After collecting these negative instances, the core step is to refine the responses to form preference pairs. These self-refined pairs are crucial for highlighting the subtle differences that can determine the success of instruction-following tasks, thereby facilitating effective learning. Given that direct refinement often results in low success rate, we employ tree-search approach. We implement both breadth-first search (BFS) and depth-first search (DFS) strategies for this refinement. Detailed algorithms for these methods are provided in Appendix B. To illustrate our process, we take BFS as an example and illustrate the procedure in Figure 2. Starting with an incorrect instruction-response pair and its judgment as the root node, we expand the search tree level-by-level until correct response is found. At each intermediate node, we generate potential refinements for the current response and evaluate its correctness using the refiner. The number of generated refinements corresponds to the number of branches. Specifically, at level of the tree, judges the the refiner: 1). generates potential refinements for each node in the current level; 2). correctness of these refinements. This creates set of child nodes with new responses and their corresponding judgments. The search process continues until we obtain tuple (x, ynegative, yrefined), where yrefined is the newly refined, correct response. Importantly, SPAR combines the strengths of both tree-search and self-refinement, exploring multiple refinement paths while minimizing the interfering factors, producing effective preference learning data. 2.3.3 ACTOR TRAINING To optimize the actor model, we leverage the refinement pairs for preference learning using DPO. At iteration t, we train the actor model Mt with refinement pairs (ynegative, yrefined), treating ynegative as the rejected response (yl) and yrefined as the chosen response (yw). The training dataset is denoted as Dt dpo and the DPO loss is described as follows: LDPO(πt θ; πref) = E(x,yw,yl)Dt dpo (cid:20) (cid:18) log σ β log πt θ(ywx) πref(ywx) β log (cid:19)(cid:21) πt θ(ylx) πref(ylx) (2) where πt fixed during the training process. This results in new actor model, Mt+1, for the next iteration. θ represents the actor model Mt, and the reference model πref initialized with Mt remains 2.3.4 REFINER TRAINING Given that the input for the refiner is templated, we use RFT to obtain the new refiner Rt+1. The RFT training data consists of two components: the refinement data and the judgment data for improving the refiners corresponding capabilities. Refinement Training Data. The refinement training data consists of tuples that capture the process of refining incorrect responses. For each incorrect response from the tree-search based refinement step, we collect tuples in the form of (x, yp, jp, yrefined), where (x, yp, jp) represents the parent node of the final correct response in the refinement tree, and yrefined is the correctly refined response. Judgment Training Data. The judgment training data is derived both from the negative data collection and nodes of the tree-search process. This dataset consists of tuples (x, yi, ji), where is the prompt, yi is response to x, and ji is the judgment consistent with majority voting. Then, we perform supervised fine-tuning using the constructed training data. For the refinement data Dt refine we use the tuples (x, yp, jp, yrefined) with input = (x, yp, jp) and target = yrefined. For the judgment data Dt judge, we use the tuples (x, yi, ji) with input = (x, yi) and target = ji. The supervised fine-tuning loss is given by Eq (1). By employing this self-play training process with the tree-search based self-refinement strategy, SPAR iteratively enhances both the actor and refiner models, aiming for continuous self-improvement in instruction-following tasks."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EXPERIMENT SETUP Backbone Models. We have conducted experiments on several popular LLMs: 5 LLaMA3 Series (MetaAI, 2024) are the best-performing models of their size, showcasing toptier instruction-following capabilities among open-source LLMs. GLM-4-9B-Chat (GLM et al., 2024) excels in instruction-following tasks, offering competitive performance under 10B parameters. Mistral-7B-Instruct (Jiang et al., 2023a) is one of the most popular LLMs and has shown good performance across wide range of tasks. Settings. In this work, we focus on enhancing the instruction-following abilities of LLMs in selfplay fashion. We utilize SFT to bootstrap models under 10B parameters as actor and refiner models. For the more advanced LLaMA3-70B-Instruct, we directly employ it in both roles. Following this, we perform three-iteration self-play training using 10k prompts per iteration from our generated dataset. In each iteration, we apply DPO for the actor and RFT for the refiner. We refer to the trained LLaMA3-8B-Instruct as SPAR-8B, LLaMA3-70B-Instruct as SPAR-70B, GLM-4-9B-Chat as SPAR-9B, and Mistral-7B-Instruct as SPAR-7B. More implementation details are provided in Appendix C. Baselines. We compare our method with five popular self-improvement approaches, including: AutoIF (Dong et al., 2024) incorporates code feedback and online DPO training to improve instruction-following ability in both distillation and self-evolution settings. SELF (Lu et al., 2023) proposes leveraging language feedback to guide response generation in order to achieve iterative self-improvement. Self-rewarding (Yuan et al., 2024) proposes to combine the reward model and policy model to enhance alignment capabilities simultaneously. Meta-rewarding (Wu et al., 2024) further introduces meta-judge to address judgment capability limitations, building on the self-rewarding framework. Humpback (Li et al., 2023a) proposes training an instruction generation model to synthesize high-quality data using web resources. 3.2 EVALUATION BENCHMARKS As both the actor and refiner continually evolve within our framework, its crucial to comprehensively evaluate both of their capabilities. Actors Instruction-following Capability. To assess the actors ability to follow instructions, we rely on two widely-used benchmarks: IFEval (Zhou et al., 2023) and FollowBench (Jiang et al., 2023b). IFEval offers 541 verifiable instructions specifically designed for code-based evaluation. These instructions cover 25 verifiable types, including tasks like Keyword Frequency and Number of Words. FollowBench, on the other hand, encompasses five categories of more subjective constraints: Content, Situation, Style, Format, and Example. This dataset features 820 meticulously curated instructions across five difficulty levels and utilizes hybrid assessment approach combining rulebased and LLM-as-judge evaluations. Refiners Judgment and Refinement Capability. For assessing the refiners judgment capability, we turn to LLMBar (Zeng et al., 2023), dataset designed to measure the assessment ability of LLMs in the context of instruction-following tasks. LLMBar includes 419 instruction-response pairs, categorized into two subsets: Natural and Adversarial. Originally, the task involves pair-wise comparisons to identify successful and failed responses. We adapted it to point-wise judgment task, asking the model to determine whether each instruction-following task is successful. To evaluate the refiners capability in refinement, we split 200 samples from the DRSFT to create test set, and we employ both GPT-4o and SPAR-8B-RFT-iter3, the refiner after three rounds of training, as judges to evaluate whether the refined responses are accurately following the instructions. 3.3 ACTOR EVALUATION RESULTS SPAR significantly improves instruction-following ability. As illustrated in Table 1, the iteratively trained LLMs demonstrate substantial improvements in both the IFEval and FollowBench 6 Table 1: Main results of iteratively trained LLMs on instruction-following benchmarks (Cf. Table 6 for full results). stands for prompt level, and represents instruction level. and denote loose and strict evaluations, respectively. Avg. indicates average results and Lv means level. Results using inference-time tree search are highlighted in green . The highest results for each backbone model is bolded. Scores marked with are sourced directly from the original paper. Model IFEval FollowBench (SSR) (L) (L) (S) (S) Avg. Lv-1 Lv-2 Lv-3 Lv-4 Lv-5 Avg. LLaMA3-8B-Instruct AutoIF-8B SELF Humpback Self-Rewarding Meta-Rewarding SPAR-8B-SFT SPAR-8B-DPO-iter1 SPAR-8B-DPO-iter2 SPAR-8B-DPO-iter3 w/ tree search GLM-4-9B-Chat SPAR-9B-SFT SPAR-9B-DPO-iter3 LLaMA3-70B-Instruct AutoIF-70B SPAR-70B-DPO-iter 77.6 43.1 78.2 72.5 77.3 77.8 75.4 78.0 78.9 79.9 82.4 71.5 71.5 77.3 83.7 85.6 85. 84.5 56.0 84.5 80.2 84.2 84.1 82.5 84.7 85.0 85.4 87.5 79.9 80.5 84.1 88.9 90.4 90. LLaMA3-8B Models 70.6 28.8 76.0 70.1 74.1 75.4 73.4 75.8 77.1 78.0 79.5 78.9 42.2 82.9 78.1 81.7 82.3 80.6 82.6 83.3 83.7 85.3 77.9 42.5 80.4 75.2 79.3 79. 78.0 80.3 81.1 81.8 83.7 GLM-4-9B Models 68.0 68.8 73.6 77.2 78.1 81. 74.2 74.7 79.1 69.4 54.6 68.3 66.8 72.8 73.9 73.9 75.3 73.9 73.0 73.9 80.8 79.4 82. LLaMA3-70B Models 77.1 80.2 81.3 83.8 86.7 87.3 83.4 85. 86.1 77.1 71.0 80.3 62.2 52.1 65.7 66.1 66.6 71.9 67.4 67.7 71.9 72.3 71.7 75. 70.9 76.7 72.5 67.2 75.7 63.1 50.0 65.2 67.2 66.8 66.0 68.1 67.6 69.1 70.0 70.3 67. 68.2 67.9 69.4 66.2 71.4 61.9 49.0 62.2 60.2 64.9 62.3 63.1 64.7 64.0 64.1 66.8 64. 65.1 68.3 68.7 64.6 73.7 60.9 43.7 62.4 62.6 64.1 62.6 61.3 62.3 62.2 64.7 64.1 65. 63.7 64.2 66.3 63.5 70.5 63.5 49.9 64.8 64.6 67.0 67.3 66.8 67.5 68.2 68.8 69.4 70. 69.5 72.0 70.8 66.5 74.3 benchmarks. Remarkably, after three training iterations, SPAR-8B-DPO-iter3 even surpasses GPT4-Turbo (81.3% average accuracy) on IFEval. Moreover, incorporating the tree-search refinement technique during the inference stage significantly boosts performance. Additionally, the SPAR showcases excellent scalability with respect to model size, which substantially enhances the instruction-following abilities of the LLaMA3-70B-Instruct model. SPAR does not damage general abilities. As shown in Appendix D.2, we assessed each iterations performance on general benchmarks, including GSM8k (Cobbe et al., 2021), TriviaQA (Joshi et al., 2017), MMLU (Hendrycks et al., 2020), and HumanEval (Chen et al., 2021). The results indicate that SPAR maintains or even improves general performance, particularly on GSM8k and HumanEval benchmarks, demonstrating that enhanced instruction-following capabilities support overall LLM alignment. SPAR outperforms other baselines significantly. Figure 3 demonstrates the improvements on IFEval with each training iteration. In every iteration, SPAR outperforms other methods. Notably, even after three iterations, other methods fail to surpass the performance of SPARs first iteration. Generally, our method and SELF outperform self-rewarding and metarewarding approaches, underscoring the importance of learning from refinement and excluding the interfering factors in instructionfollowing tasks. Furthermore, SPARs superior performance compared to SELF indicates that Figure 3: Comparison with baseline methods across iterations (Cf. Figure 9 for SPAR-7B). SPAR-8B consistently surpasses all baselines. 7 Table 2: Evaluation of judgment capability for iteratively trained LLMs on LLMBar. (Cf. Table 8 for Mistral-7B-Instruct results.) Acc. denotes accuracy. The highest scores for each base model are highlighted in bold. Model Natural Acc. F1 Adversarial GPTInst F1 Acc. GPTOut F1 Acc. Manual F1 Acc. Neighbor F1 Acc. Average F1 Acc. Average Acc. F1 GPT-4o-Mini 74.5 70.5 69. 61.6 60.9 51.4 59.8 51.9 72. 66.4 65.7 57.8 67.4 60.4 LLaMA3-8B Models LLaMA3-8B-Instruct SELF Self-Rewarding Meta-Rewarding SPAR-8B-SFT SPAR-8B-RFT-iter1 SPAR-8B-RFT-iter2 SPAR-8B-RFT-iter3 60.0 69.5 71.0 70.5 68.5 68.5 70.5 70.5 51.8 61.6 66.3 66.3 60.9 63.2 64.2 65. 55.4 62.0 70.1 68.5 67.9 66.8 66.8 70.7 46.1 50.7 66.7 64.6 62.4 60.6 61.6 66.7 47.9 64.9 63.8 64.9 59.6 63.8 66.0 63. 39.5 54.8 59.5 60.2 50.0 55.3 60.0 57.5 51.1 57.6 62.0 64.1 63.0 62.0 65.2 68.5 36.6 41.8 55.7 58.3 54.1 53.3 57.9 63. 54.5 64.6 67.5 69.0 68.3 66.8 69.0 68.3 45.0 51.3 61.7 63.1 59.3 59.0 62.4 62.2 52.2 62.2 65.9 66.6 64.7 64.9 66.8 67. 41.8 49.6 60.9 61.6 56.5 57.1 60.5 62.4 53.8 63.7 66.9 67.4 65.5 65.6 67.5 68.3 43.8 52.0 61.9 62.5 57.3 58.3 61.2 63. GLM-4-9B Models GLM-4-9B-Chat 74.5 76.5 74.5 75. 57.4 62.3 53.3 56.6 69.8 72. 63.7 66.7 65.9 68.6 SPAR-9B-SFT SPAR-9B-RFT-iter3 70.5 71. 65.5 68.8 72.8 75.5 70.2 74.6 59.6 58.5 55.8 55.2 64.1 68. 53.5 64.2 71.3 68.7 67.2 65.9 66.9 67.8 61.7 64.9 67.7 68. 62.5 65.7 LLaMA3-70B Models LLaMA3-70B-Instruct 75.0 71.9 73. 69.6 69.1 66.7 66.3 60.8 69. 63.4 69.5 65.1 70.6 66.5 SPAR-70B-RFT-iter 78.0 74.7 78.8 76.9 64.9 61. 67.4 59.5 72.4 68.1 70.9 66. 72.3 68.1 contrastive refinement response pairs can highlight key differences, which are difficult to learn using only correct responses. Additionally, only SPAR-8B-SFT outperforms the original LLaMA38B-Instruct, which suggests that incorporating the judgment SFT or refinement SFT data would reduce performance, likely due to the huge task gap and reduced diversity in the data. 3.4 REFINER EVALUATION RESULTS SPAR iteratively enhances judgment capability. Our analysis in Table 2 shows that SPAR iterations notably improve the models ability to evaluate instruction-following tasks. By iteration three, the refiner SPAR-8B-RFT-iter3 surpasses GPT-4o-Mini, the model used to construct the judgment SFT dataset. This finding highlights the potential for continuous self-improvement, as the supervised fine-tuning data is not bottleneck. Interestingly, our refiner greatly outperforms GPT-4o-Mini on adversarial test sets, suggesting that the similar positive and negative examples generated during tree search can make our model more robust against adversarial samples. SPAR progressively improves refinement capability. Table 3 demonstrates continuous improvement in refinement accuracy (success rate) of LLaMA3-8B-Instruct with each training iteration, eventually matching the level of GPT-4o-Mini, the strong LLM for SFT data construction. This further showcases promising way for selfevolution in instruction-following tasks. However, it also points to potential issue of self-evaluation bias: when the refiner self-evaluates refinement accuracy, it performs significantly better than when evaluated by GPT-4o. Table 3: Refinement evaluation results. Acc-GPT uses GPT-4o as judge; -SPAR uses SPAR-8B-RFT-iter3. Model Acc-GPT Acc-SPAR GPT-4o-Mini SPAR-8B-SFT SPAR-8B-RFT-iter SPAR-8B-RFT-iter2 SPAR-8B-RFT-iter3 79.0 73.5 77.5 74. 79.0 71.0 71.0 77.0 76.0 90. 3.5 ABLATIONS AND ANALYSIS Refinement preference pairs enhance instruction-following capability more effectively. To verify that the interfering factors indeed affect preference learning and motivate the need to highlight the key differences, we have conducted synthetic data experiment featuring two tasks: 8 Figure 4: Synthetic data experiment results: Character Sequence Generation (left) and Start/End Story Generation (right). For Character Sequence Generation, interfering pairs show rapid learning of the uppercase ratio (interfering factor) but perform worse than refinement pairs. In the Start/End Story Generation task, refinement pairs outperform interfering pairs, which even underperform the original model at step 0. Table 4: Ablation study on the actor. Table 5: Ablation study on the refiner. Model IFEval FollowBench (SSR) Model Prompt(S) Instruction(S) SPAR-8B-DPO-iter3 78.0 w/o Tree Search w/o Iterative Training w/o Refinement -2.0 -0.9 -2.6 83.7 -0.8 -0.2 -1. Avg. 68.8 -1.7 -2.0 -3.1 Natural Adversarial Acc. F1 Acc. F1 SPAR-8B-RFT-iter3 70.5 65.9 67. 62.4 w/o Tree Search w/o Iterative Training -0.5 -0.5 -1.2 -2. -4.3 -1.7 -8.2 -3.5 Character Sequence Generation: The model needs to generate specified number of given letters, with no restrictions on letter case, such as generating 12 letters a. For each prompt, we first construct negative response in lowercase. In order to introduce disturbing factors, we have the correct response in uppercase for interfering pairs while maintaining refined pairs lowercase correctness. Start/End Story Generation: The model is asked to generate story that starts with sentence 1 and ends with sentence 2. The negative response lacks either sentence 1 or 2. Interfering pairs have different story concatenated with these sentences; refined pairs keep the same story intact. Figure 4 shows that refinement pairs significantly outperform interfering pairs in both tasks, with larger and more effective improvements. Particularly in story generation, diverging stories results in worse accuracy than the original model. Moreover, in the character generation task, we can clearly observe that the interfering factor (uppercase ratio) is learned quickly. However, the task is not performed as well as the refinement setting, highlighting the necessity of focusing on key differences and excluding possible interfering factors. Furthermore, the ablation study on actors performance in Table 4 further reveals significant drop when refinement data is omitted. SPARs superiority over self-rewarding and meta-rewarding methods in Table 1 also underscores the importance of using refinement pairs to eliminate interfering factors. Additionally, the string-level similarity of refinement response pairs is 0.90, much higher than 0.85 of the independently sampled response pairs. Each element is crucial in SPAR. The primary elements of SPAR include the tree-search refinement process and iterative training. We thus conduct ablation studies to assess the significance of these elements. For the tree-search process, as shown in Table 4, excluding tree search significantly reduces the actors performance. This might be due to lack of difficult samples that require more iterations to refine and reduced number of preference pairs. Table 10 illustrates that tree search greatly outperforms greedy decoding in response refinement and surpasses other methods, such as best-of-N refinement or simple iterative refinement. Furthermore, tree search is essential for improving judgment capability, especially against adversarial inputs, as indicated in Table 5. Simi9 lar responses with opposite labels generated during the tree-search process can enhance robustness against challenging scenarios. Moreover, the results presented in Tables 4 and 5 underscore the importance of iterative training for both the actor and the refiner. This iterative training process ensures mutual improvement, which is crucial for the overall effectiveness of our framework. significantly Scaling compute test-time boosts model performance. Inspired by the recent developments in test-time compute scaling (Snell et al., 2024), we investigate various decoding strategies during inference on SPAR-8B-DPO-iter3. Figure 5 shows that increasing inference times remarkably enhances model performance, outperforming the results of greedy decoding. Notably, while tree search refinements performance growth is slower, it ultimately achieves superior results compared to best-of-N generation. This indicates that refinement is more powerful than generation and could be better suited for scaling test-time compute in the instruction-following task."
        },
        {
            "title": "4 RELATED WORK",
            "content": "4.1 INSTRUCTION FOLLOWING Figure 5: Comparison of decoding strategies. Model performance improves with increased inference times. Instruction-following is fundamental capability of LLMs and is central to LLM alignment (Ouyang et al., 2022; Cheng et al., 2023; Lou et al., 2024). Many studies have evaluated instruction-following capabilities from various perspectives (Li et al., 2023b; Zheng et al., 2023; Zeng et al., 2023; Liu et al., 2023a; Xia et al., 2024). With the expanding application of LLMs, the tasks they are expected to perform become more intricate (Liu et al., 2023b), often involving composite instructions with numerous constraints. Consequently, several benchmarks have been developed to test LLMs ability to follow these complex instructions (Zhou et al., 2023; Jiang et al., 2023b; Qin et al., 2024; Wen et al., 2024). Additionally, multiple studies have focused on enhancing LLMs instructionfollowing capabilities (Lou et al., 2023; Zhou et al., 2024; Sun et al., 2024). One crucial aspect of the instruction-following task is that subtle differences in responses can significantly impact their correctness (Zhou et al., 2023). Considering this, we introduce SPAR framework to construct preference pairs that reduce extraneous elements to highlight these subtle variations for effective improvements. 4.2 AUTONOMOUS LLM ALIGNMENT Given the high cost of manually collecting alignment data, many studies focus on exploring autonomous LLM alignment methods (Cao et al., 2024). One common strategy involves using data distilled from advanced models to improve less powerful ones (Peng et al., 2023; Xu et al., 2023; Cheng et al., 2024). Alternatively, as the LLMs become stronger, several studies (Wang et al., 2023; Yuan et al., 2024; Zhang et al., 2024) investigate how to self-evolving LLMs capabilities. SelfInstruct (Wang et al., 2023) generates instructions by employing the models in-context learning ability. Reinforced Self-Training (Gulcehre et al., 2023) samples data from an LLM policy and utilizes the dataset to enhance the policy through offline RL algorithms. Moreover, recent research has incorporated feedback from diverse sources. SELF (Lu et al., 2023) trains LLMs to acquire metaskills of self-feedback and self-refinement, enabling the models to self-evolve iteratively. AutoIF (Dong et al., 2024) introduces the code execution feedback. Self-rewarding (Yuan et al., 2024) and Meta-rewarding (Wu et al., 2024) leverage the LLM-as-judge ability to evaluate its own responses, thereby constructing preference pairs. However, these methods usually direct sample multiple independent responses from the actor model, which is likely to introduce the interfering factors and thus affect the models capture of the key differences. Thus, we propose new framework that constructs preference pairs by self-refining the models responses, minimizing extraneous elements, and promoting more effective autonomous improvement."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this study, we introduce new self-play framework, SPAR, designed to improve the instructionfollowing capabilities of LLMs through training with refinement pairs. We reveal that, unlike traditional approaches that rely on sampling multiple independent responses from the model to construct preference pairs, refining preference pairs to minimize extraneous factors and highlight key differences lead to significant improvements in instruction-following tasks. Remarkably, the LLaMA38B-Instruct model, trained iteratively using our framework, outperforms GPT-4-Turbo on IFEval. With inference time compute scaling, its performance can be further improved. Moreover, the iterative enhancement of instruction-following, judgment, and refinement abilities brought about by SPAR underscores promising path to continuous self-improvement."
        },
        {
            "title": "REFERENCES",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, et al. Towards scalable automated alignment of llms: survey. arXiv preprint arXiv:2406.01252, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang. Black-box prompt optimization: Aligning large language models without model training. arXiv preprint arXiv:2311.04155, 2023. Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. Autodetect: Towards unified framework for automated weakness detection in large language models. arXiv preprint arXiv:2406.16714, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. arXiv preprint arXiv:2406.13542, 2024. Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. 11 Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Zhenyu Hou, Yiin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, et al. Chatglm-rlhf: Practices of aligning large language models with human feedback. arXiv preprint arXiv:2404.00934, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. Followbench: multi-level fine-grained constraints following benchmark for large language models. arXiv preprint arXiv:2310.20410, 2023b. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, 2017. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023a. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023b. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for language models. arXiv preprint arXiv:2404.07503, 2024. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023a. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023b. Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, and Wenpeng Yin. Muffin: Curating multi-faceted instructions for improving instruction-following. arXiv preprint arXiv:2312.02436, 2023. Renze Lou, Kai Zhang, and Wenpeng Yin. Large language model instruction following: survey of progresses and challenges. Computational Linguistics, pp. 110, 2024. Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao Wang, Lifeng Shang, and Qun Liu. Self: Language-driven self-evolution for large language model. arXiv preprint arXiv:2310.00533, 2023. MetaAI. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 12 Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Identifying the risks of lm agents with Dubois, Chris Maddison, and Tatsunori Hashimoto. an lm-emulated sandbox. arXiv preprint arXiv:2309.15817, 2023. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. Conifer: Improving complex constrained instruction-following ability of large language models. arXiv preprint arXiv:2404.02823, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, 2023. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, et al. Benchmarking complex instruction-following with multiple constraints composition. arXiv preprint arXiv:2407.03978, 2024. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. Fofo: benchmark to evaluate llms format-following capability. arXiv preprint arXiv:2402.18667, 2024. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. 13 Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641, 2023. Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm selftraining via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024. Hanyu Zhao, Li Du, Yiming Ju, Chengwei Wu, and Tengfei Pan. Beyond iid: Optimizing instruction learning from the perspective of instruction interaction and dependency. 2024. URL https: //arxiv.org/abs/2409.07045. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "A DATASET INFORMATION",
            "content": "Constraint Taxonomy. We take the taxonomy from Cheng et al. (2024), and further refine it to be more comprehensive to ensure the diversity of our prompts. The refined taxonomy is shown in Figure 6. Figure 6: The detailed taxonomy of constraints for prompt evolution. Prompt Template. Here, we give the prompt for constructing complex prompts in Figure 7. For the refiner, the prompt template for judgment is provided in Figure 8. As for the refinement task, we form it as multi-turn task after judgment, with the prompt template provided in Figure 8. 14 Figure 7: The prompt template applied for prompt evolution. Figure 8: The prompt template applied for the refiners judgment and refinement. 15 TREE-SEARCH ALGORITHM We show the detailed process of BFS and DFS refinement in Algorithm 1 and Algorithm 2. Algorithm 1 BFS-Refinement Require: Instruction x, Response y, Judgment j, Refiner RN , depth limit d, branch limit b. S0 {x, y, j} for = 1, , do {[x, y] St1, RN (s, b)} Vt RN (S t) St {[x, y, j] t, Vt(s)} get judgment end for return arg maxsST VT (s) Algorithm 2 DFS-Refinement Require: Current state s, depth t, Refiner RN , depth limit d, threshold vth, branch limit if > then record output = (x, y, j) end if for RN (s, b) do refinement judgment if RN (s) < vth then DFS(s, + 1) end if end for"
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "The SFT dataset for the actor comprises 8k examples, while the refiner dataset includes approximately 9k examples for judgment training and 3k for refinement training, formatted as multi-turn task following the first turns judgment. These two datasets are both constructed with GPT-4o-Mini. Both the actor and refiner are trained with learning rate of 2e-6 and warmup ratio of 0.1, using the AdamW optimizer with β1 = 0.9 and β2 = 0.999. The actor is trained over 5 epochs with batch size of 64, and the refiner is trained for 3 epochs with the same batch size. In the data construction process, we set tree search budget of 15 to strike balance between performance and efficiency. The average number of expanded tree nodes is around 3.7 in our experiments, which is an acceptable level. Specifically, for LLaMA3-8B-Instruct, the average expanded node numbers are 4.3, 3.7, and 3.4 across different iterations, demonstrating decreasing trend as the model becomes stronger. For the actor iterative training, each iteration uses around 5k examples for DPO. To enhance training stability as suggested by (Hou et al., 2024), an additional SFT loss is added to the chosen response with weight of 0.1. Here, the learning rate is set to 2e-7, β to 0.1, with warmup ratio of 0.1, and training is conducted for 1 epoch with batch size of 32. For the refiner, each iteration utilizes about 10k examples, including 4k refinement samples. We ensure the judgment training dataset maintains balance of positive and negative samples. The training configuration remains the same as for SFT, except the learning rate is set to 1e-6. All experiments are performed on an 880G Nvidia A100 setup. For our baseline methods, we have maintained uniform settings to ensure fairness. For SELF, we initialize with our constructed datasets, DActor and DRef iner. In the case of self-rewarding and meta-rewarding, we start with DActor and DJSF . For Humpback, we create the seed dataset by combining about 3k data from the Oasst1 dataset and 5k data from DActor. We also control the number of training samples to be nearly identical for fair comparisons."
        },
        {
            "title": "D EXPERIMENT RESULTS",
            "content": "D.1 INSTRUCTION-FOLLOWING EVALUATION RESULTS. The evaluation results on instruction-following benchmarks are shown in Table 6. Our method outperforms all baselines on these benchmarks and show substantial improvements in each iteration (Figure 9). D.2 GENERAL PERFORMANCE EVALUATION Our analysis in Table 7 reveals that SPAR training not only doesnt harm general performance, but it can also even bring enhancements. 1https://huggingface.co/datasets/OpenAssistant/oasst 16 D.3 JUDGMENT EVALUATION RESULTS. As shown in Table 8, the judgment capability improves in each iteration and the accuracy outperforms all baselines. D.4 ABLATION STUDY ON JUDGMENT CAPABILITY. In our experiments, we employ majority voting for iterative improvements for judgment capability. We show the results of the refiner SPAR-8B-SFTs sampling times and performance on LLMBar in Table 9. To balance the performance and computation time, we choose majority voting@5. D.5 ABLATION STUDY ON REFINEMENT CAPABILITY. Table 10 shows the results of different decoding strategies for the refinement task on SPAR-8B. For methods except greedy decoding, we use the same inference budget. We can see that the tree search algorithms largely outperform other methods, verifying the importance of incorporating tree search refinement. D.6 INFERENCE-TIME SCALING COMPARISON Figure 10 presents comparison between SPAR and self-rewarding, focusing on their scalability with regard to inference times, measured by the number of response generations in our study. Our analysis includes both the LLaMA3-8B-Instruct and Mistral-7B-Instruct models. The results demonstrate that SPAR outperforms the self-rewarding method when additional computational resources are allocated for inference time, leading to enhanced performance. Figure 9: Comparison with baseline methods across iterations. SPAR-7B consistently surpasses all baselines. Figure 10: Inference-time scaling comparison on IFEval. The left panel showcases results for LLaMA3-8B-Instruct, while the right panel presents findings for Mistral-7B-Instruct. 17 Table 6: Full results of SPAR-7B, SPAR-9B, and SPAR-70B on instruction-following benchmarks. stands for prompt level, and represents instruction level. and denote loose and strict evaluations, respectively. Avg. indicates average results and Lv means level. Scores marked with are sourced directly from the original paper. Model Mistral-7B-Instruct SELF Humpback Self-Rewarding Meta-Rewarding SPAR-7B-SFT SPAR-7B-DPO-iter1 SPAR-7B-DPO-iter2 SPAR-7B-DPO-iter GLM-4-9B-Chat SPAR-9B-SFT SPAR-9B-DPO-iter1 SPAR-9B-DPO-iter2 SPAR-9B-DPO-iter3 LLaMA3-70B-Instruct AutoIF-70B SPAR-70B-DPO-iter1 SPAR-70B-DPO-iter2 SPAR-70B-DPO-iter3 IFEval FollowBench (SSR) (L) (L) (S) (S) Avg. Lv-1 Lv-2 Lv-3 Lv-4 Lv-5 Avg. 55.1 71.3 60.4 64.3 65.1 62.7 68.2 70.0 74.1 71. 71.5 73.8 76.7 77.3 83.7 85.6 84.5 85.0 85.6 64.9 79.7 71.0 73.5 74.7 72.3 76.6 78.1 80.9 79. 80.5 81.2 83.3 84.1 88.9 90.4 89.2 89.4 90.2 Mistral-7B Models 49.9 68.0 56.6 61.0 61.0 59.3 64.7 65.8 69. 60.2 76.9 67.6 70.7 71.1 68.7 73.6 74.2 77.1 57.5 74.0 63.9 67.4 68.0 65.8 70.8 72.0 75.5 GLM-4-9B Models 68. 68.8 70.6 73.2 73.6 77.2 78.1 78.5 80.9 81.4 74.2 74.7 76.0 78.5 79.1 65.1 71.5 70.7 70.8 73. 74.4 73.2 72.2 74.6 80.8 79.4 82.6 80.4 82.7 LLaMA3-70B Models 77.1 80.2 80.2 81.5 81. 83.8 86.7 85.7 87.2 87.3 83.4 85.7 84.9 85.8 86.1 77.1 71.0 77.6 80.4 80. 61.6 64.2 63.9 64.8 64.6 64.3 64.6 65.7 63.8 75.1 70.9 76.0 76.6 76.7 72.5 67.2 74.0 76.4 75. 61.6 60.8 63.8 62.3 64.5 62.5 63.1 61.4 66.1 67.4 68.2 67.9 67.4 67.9 69.4 66.2 70.2 69.9 71. 56.8 58.0 59.8 61.9 60.6 58.2 60.3 62.4 61.0 64.3 65.1 64.9 68.7 68.3 68.7 64.6 70.6 73.7 73. 57.2 57.0 57.9 58.3 57.6 55.0 56.6 57.5 58.0 65.4 63.7 63.6 64.1 64.2 66.3 63.5 66.9 70.2 70. 60.4 62.3 63.2 63.6 64.1 62.9 63.6 63.8 64.7 70.6 69.5 71.0 71.4 72.0 70.8 66.5 71.9 74.1 74. Table 7: Performance on general benchmarks. SPAR maintains the models general capabilities. Model GSM8k TriviaQA MMLU HumanEval Average Mistral-7B-Instruct SPAR-7B-SFT SPAR-7B-DPO-iter1 SPAR-7B-DPO-iter2 SPAR-7B-DPO-iter3 LLaMA3-8B-Instruct SPAR-8B-SFT SPAR-8B-DPO-iter1 SPAR-8B-DPO-iter2 SPAR-8B-DPO-iter3 GLM-4-9B-Chat SPAR-9B-SFT SPAR-9B-DPO-iter1 SPAR-9B-DPO-iter2 SPAR-9B-DPO-iter Mistral-7B Models 72.5 72.8 72.2 72.1 71.6 57.9 56.7 55.3 55.8 55.1 LLaMA3-8B Models 75.9 76.0 75.2 74.9 75.1 63.6 64.0 63.8 63.1 63. GLM-4-9B Models 69.7 69.4 68.8 68.9 69.0 71.9 71.8 71.6 71.8 72.1 42.9 56.4 55.6 54.4 58.2 75.4 75.6 78.8 77.0 77.7 80.6 82.9 82.6 82.8 83. LLaMA3-70B Models LLaMA3-70B-Instruct SPAR-70B-DPO-iter1 SPAR-70B-DPO-iter2 SPAR-70B-DPO-iter3 92.2 92.5 92.9 93.4 80.8 81.0 80.4 80.6 87.2 90.4 89.5 86.7 32.9 44.5 46.3 45.1 46.3 55.5 61.6 60.4 60.4 60.9 74.3 73.8 75.0 73.8 73.2 79.3 79.3 78.7 79.9 51.6 57.6 (+6.0) 57.4 (+5.8) 56.9 (+5.3) 57.8 (+6.2) 67.6 69.3 (+1.7) 69.6 (+2.0) 68.9 (+1.3) 69.2 (+1.6) 74.1 74.5 (+0.4) 74.5 (+0.4) 74.3 (+0.2) 74.3 (+0.2) 84.9 85.8 (+0.9) 85.4 (+0.5) 85.2 (+0.3) Table 8: Judgment evalution results on LLMBar for SPAR-7B. Acc. stands for accuracy. Model Natural Acc. F1 Adversarial GPTInst F1 Acc. GPTOut F1 Acc. Manual F1 Acc. Neighbor F1 Acc. Average F1 Acc. Average Acc. F1 Mistral-7B-Instruct SELF Self-Rewarding Meta-Rewarding SPAR-7B-SFT SPAR-7B-RFT-iter1 SPAR-7B-RFT-iter2 SPAR-7B-RFT-iter3 58.0 68.0 68.0 67.5 69.5 67.0 68.0 71. 69.1 65.2 64.0 62.4 63.9 62.1 64.4 66.7 57.1 71.2 69.0 71.7 71.7 66.3 68.5 72.3 68.8 68.7 63.7 68.7 67.5 62.7 64.6 67. 50.0 56.4 59.6 56.4 55.3 56.4 60.6 57.4 64.1 56.8 53.7 51.8 48.8 52.9 57.5 55.6 45.6 62.0 63.0 63.0 55.4 60.9 62.0 60. 61.5 52.6 57.5 56.4 45.3 52.6 52.1 51.4 47.8 67.5 69.4 66.8 69.4 64.2 64.2 68.3 62.6 62.3 64.3 62.1 62.3 60.7 60.0 62. 50.1 64.3 65.3 64.5 63.0 61.9 63.8 64.7 64.3 60.1 59.8 59.7 56.1 57.2 58.5 59.2 51.7 65.0 65.8 65.1 64.3 63.0 64.7 66. 65.2 61.1 60.6 60.3 57.6 58.2 59.7 60.7 Table 9: Comparison of decoding strategies on LLMBar. Method Natural Adversarial Acc. F1 Acc. F1 Greedy Decoding 68.0 60.7 63.9 Majority Voting@3 69.0 60.8 63.7 Majority Voting@5 68.5 60.9 64.7 Majority Voting@7 66.5 58.8 65. Majority Voting@9 69.0 61.2 65.8 55.1 54.5 56.5 56.7 57. Table 10: Comparison of different decoding strategies for refinement task. Acc-GPT stands for the accuracy of using GPT-4o as judge, and Acc-SPAR for the accuracy of using SPAR-8B-RFT-iter3 as judge. Method Acc-GPT Acc-SPAR Greedy Decoding Best of Iterative Refinement BFS DFS 65.0 80.0 82.0 90. 90.0 69.5 74.0 71.0 79.0 79."
        }
    ],
    "affiliations": [
        "The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University",
        "The Knowledge Engineering Group (KEG), Tsinghua University",
        "Zhipu AI"
    ]
}