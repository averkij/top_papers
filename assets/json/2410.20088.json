{
    "paper_title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
    "authors": [
        "Atula Tejaswi",
        "Yoonsang Lee",
        "Sujay Sanghavi",
        "Eunsol Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space."
        },
        {
            "title": "Start",
            "content": "RARE: RETRIEVAL AUGMENTED RETRIEVAL WITH IN-CONTEXT EXAMPLES , Yoonsang Lee Atula Tejaswi The University of Texas at Austin atutej@utexas.edu , Sujay Sanghavi , Eunsol Choi , Seoul National University , New York University 4 2 0 2 6 2 ] . [ 1 8 8 0 0 2 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space."
        },
        {
            "title": "INTRODUCTION",
            "content": "In-context learning (ICL) (Brown et al., 2020) has emerged as powerful paradigm enabling diverse applications without parameter updates in large language models (LLMs). By conditioning on inputoutput examples that demonstrate specific task, LLMs can generate predictions while maintaining fixed parameters. While in-context learning has been extensively studied for LLMs (Xu et al., 2023; Min et al., 2022a; Dong et al., 2024), its potential for retriever models remains unexplored. We study how in-context examples can be effectively leveraged to enhance performance in retriever models. Unlike in decoder-only LLMs where in-context examples expand model capacity at generation time, in-context examples may primarily provide task-relevant information rather than increasing model capacity. Specifically, we study injecting in-context examples to build dense retriever model (Karpukhin et al., 2020) which embeds queries and documents into shared representational space for efficient search over large corpus. Text retrieval is core component of many natural language processing (NLP) tasks, serving as key component for retrieval-augmented language language models (Lewis et al., 2021). State-of-the-art retriever models started to leverage decoder-only models as backbone (Wang et al., 2024b; BehnamGhader et al., 2024; Muennighoff et al., 2024; Meng et al., 2024; Lee et al., 2024a), further motivating our study of applying in-context examples. We begin by naively prepending in-context examples to the target query and provide it to existing retriever models (BehnamGhader et al., 2024; Wang et al., 2024b; Meng et al., 2024), observing that this leads to significant performance drop. We propose new approach to construct retrieval models that can leverage in-context examples, which we name as RARe: Retrieval Augmented Retrieval with In-Context Examples. Our approach modifies the query format of retrieval systems by providing in-context examples whose query is semantically similar to the target query. Then, we apply standard continued fine-tuning with contrastive loss. We conduct comprehensive initializing from both evaluation of new query format across various experimental settings, Equal advising Figure 1: Overview Prior work augments task-specific instruction to given query as input to the Retriever. In RARe, we further leverage set of in-context exemplars that contain pairs of queries and relevant documents. These in-context examples are augmented with the original query as input to the retriever along with the instruction. decoder-only checkpoints and pre-trained retriever model checkpoints. We demonstrate that RARe outperforms baseline models across multiple tasks, achieving up to +1.41% nDCG@10 on standard retrieval benchmarks (Thakur et al., 2021) and demonstrating even larger gains (+2.72%) on reasoning-oriented retrieval tasks (Xiao et al., 2024). Our contributions can be summarized as follows: We introduce RARe, an approach that adapts pre-trained checkpoints to use in-context examples for retrieval. We demonstrate that this recipe can improve the performance of various base architectures, including decoder-only models and existing retriever models. We provide detailed analyses on how the quality, quantity, and selection of in-context examples affect performance, contextualizing the sources of our experimental gains. All our code and model checkpoints are publicly released."
        },
        {
            "title": "2 SETUP & EXISTING APPROACHES",
            "content": "Standard Retrieval Setup We consider standard dense retriever (Karpukhin et al., 2020), where input queries and documents are encoded with an embedder E() into fixed-dimensional embedding. The embedder E() is trained on training set which consists of multiple retrieval tasks + {D1, D2, , DT }, where each task contains training examples of the form (q, , ) (Wang et al., 2024b; BehnamGhader et al., 2024). Here, is the input query, is positive (relevant) document, and is hard-negative (irrelevant) document, which allows for contrastive-loss based training. + + = {d + + 2 , ..., 1 , The evaluation task Dtest consists of corpus of documents C, as well as test pairs (q, ), where + m} is set of relevant document(s) for the query (Thakur et al., 2021). from the corpus using the embedder E(). The aim is to retrieve these relevant documents Specifically, an index Ce of the corpus with document embeddings E(d), is created. Then, the embedding E(q) of test query is used to retrieve the documents whose embedding E(d) is closest to E(q), typically with the cosine (cos) similarity function. + + Existing Methods Current architectures (Asai et al., 2023; BehnamGhader et al., 2024) prepend task-specific instruction ti, [1, 2, , ] to the query to contextualize the task: qinst = Instruct: {ti}; Query: {q}, Di (1) Then, the embedder E() is trained with standard contrastive loss (Izacard et al., 2022; Karpukhin Di, along in-batch negatives N, where repreet al., 2020), incorporating qinst, and , + 1Code is available at: https://github.com/atutej/RARe 2 Algorithm 1: RARe - Training Input: Training set D, embedder E(), BM25, the number of in-context examples k, + , Sample mini-batch of size from ) do for (ti, q, mini-batch size B. 1: for each training iteration do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Output: Trained embedder E() In-Context Example Retrieval: {qic {dic+ Dic Query Augmentation: qinst+ic = Instruct: {ti}; Query: {qic 2 , . . . , qic 2 , . . . , dic+ 1 , dic+ 1 , qic 1 , dic+ {(qic } {d 1 ), . . . , (qic + (q , dic+ )} , Training with Contrastive Loss: Compute the mini-batch contrastive loss LRARe as described in Equation 5. Update E() by minimizing LRARe. 1 }; Document: {dic+ 1 } ; Query: {q} } Retrieve nearest neighbor queries of from using BM25 + ) D, {qic 1 , . . . , qic }} sents the set of in-batch negatives, eqinst = E(qinst ); = log + ed+ = E(d ); ed = E(d exp(cos(eqinst, ed+ )) ); en = E(n) (2) (3) exp(cos(eqinst, ed+ )) + exp(cos(eqinst , ed )) + nN exp(cos(eqinst, en)) During evaluation on Dtest, each test query is augmented with task-specific instruction ttest."
        },
        {
            "title": "3 OUR METHOD â€“ RARE",
            "content": "RARe consists of two main components (a) We enhance the query representation by incorporating in-context examples, which provide additional query-specific guidance to the model, (b) We finetune E() on to learn to leverage these in-context examples. Given query q, we use BM25 (Robertson & Zaragoza, 2009), sparse retrieval technique that ranks documents based on keyword matching, and find closest queries qj from Di to obtain in-context examples Dic 1 ), (qic )}. As shown in Figure 1, we then augment these examples to the original query qinst to obtain the final query qinst+ic, 2 ), , (qic = {(qic 2 , dic+ 1 , dic+ , dic+ qinst+ic = Instruct: {ti}; Query: {qic 1 }; Document: {dic+ 1 } ; Query: {q} We then train embedder E() with the same loss as Equation 3, but with qinst+ic instead of qinst, LRARe = log exp(cos(eqinst+ic, ed+ )) + exp(cos(eqinst+ic, ed )) + nN exp(cos(eqinst+ic, en)) exp(cos(eqinst+ic, ed+ )) (4) (5) Algorithm 1 presents our training procedure in detail. At inference time, we similarly perform search to find nearest in-context examples to form an augmented query. Algorithm 2 in the Appendix provides an overview of the inference procedure."
        },
        {
            "title": "4.1 FINE-TUNING",
            "content": "Base Models We explore two training setups: fine-tuning decoder-only models for retrieval, and fine-tuning existing retriever models. For the first setup, we train the Llama-3 family of models, 3 following the training methodology outlined by Ma et al. (2023); Weller et al. (2024b). For the second setup, we use two high-performing publicly available embedding models that were trained with task-specific instructions: LLM2Vec-Llama-3-8b-Supervised (BehnamGhader et al., 2024) and E5-Mistral-Instruct (Wang et al., 2024b). We chose these two models because, unlike some other strong performers (Meng et al., 2024; de Souza P. Moreira et al., 2024), they were not trained on most of the datasets used in our downstream benchmarks. The LLM2Vec-Llama-3-8b-Supervised model is initially trained using an unsupervised text reconstruction objective and then fine-tuned with supervised contrastive learning on public subset of the E5 dataset, which incorporates various supervised training datasets (Gao et al., 2021; Nguyen et al., 2016; Kwiatkowski et al., 2019). In contrast, E5-Mistral-Instruct undergoes further training on synthetic data that is not publicly available. These models are chosen to assess the impact of additional supervised training on an existing retriever model versus training generative model for retrieval from scratch. Training Data For fine-tuning existing retriever models, we follow prior work (BehnamGhader et al., 2024) and train on publicly available portion of E5 dataset (Springer et al., 2024; Wang et al., 2024b), which contains MS-MARCO (Nguyen et al., 2016) NLI (Gao et al., 2021), ELI5 (Fan et al., 2019), FEVER (Thorne et al., 2018), HotpotQA (Yang et al., 2018), NQ (Kwiatkowski et al., 2019), SQuAD (Rajpurkar et al., 2016), Quora Duplication Questions (DataCanary et al., 2017). For finetuning decoder-only models from scratch, we use the MS-MARCO (Nguyen et al., 2016) passage ranking dataset and train without task-specific instruction prefix, following Ma et al. (2023). Constructing In-Context Examples During training, we provide each training example with five in-context examples from the dataset that it belongs to (k=5). Specifically, the set of examples Dic for each task is drawn from the training set Di, Dic ."
        },
        {
            "title": "4.2 EVALUATION",
            "content": "Datasets We evaluate on the widely used BeIR retrieval benchmark (Thakur et al., 2021). For ablative experiments, we follow prior work and focus on low-resource datasets (Wang et al., 2023) that potentially benefit more from few-shot examples. Since the BeIR benchmark contains few datasets whose training sets are in the E5 dataset mixutre, we categorize them as in-domain and out-of-domain i.e. not seen during training. See Table 7 in the Appendix for list of in-domain and out-of-domain datasets from BeIR. We also evaluate on subset of the RAR-b (Xiao et al., 2024) benchmark, which requires complex reasoning for retrievers. Specifically, we evaluate on HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-C (Clark et al., 2018), TempReasonL1 (Tan et al., 2023), WinoGrande (Sakaguchi et al., 2021), Î±-NLI (Bhagavatula et al., 2020), SiQA (Sap et al., 2019), and Quail (Rogers et al., 2020). Unlike BeIR, some RAR-b queries are composed of sentences with (multiple) indicators (e.g., Start:, End:). Each dataset is associated with task-specific instruction, following prior work (Muennighoff et al., 2023; Wang et al., 2024b; BehnamGhader et al., 2024). We provide additional preprocessing details in Appendix A. Constructing In-Context Examples We construct Dic test from the training/development set of each datasets. For datasets on BeIR that do not have either of these, we use synthetically generated collection of document-query pairs (GenQ) from Thakur et al. (2021). For all experiments, we use k=5 in-context examples. Metrics We use standard metrics for retrieval benchmarks. Following Thakur et al. (2021), we report nDCG@10, which measures the ranking quality of the top 10 retrieved documents, taking into account both the relevance and position of each retrieved document (Wang et al., 2013)."
        },
        {
            "title": "5 RESULTS",
            "content": "We evaluate in-context example augmented queries in three settings. First, we evaluate the performance after inference-only modification, where we take existing pre-trained retrievers and simply provide in-context examples at inference time (Section 5). Second, we evaluate training retriever with in-context examples from an LLM (decoder-only) backbone (Section 5.1). Third, we compare training retriever models with in-context examples from pre-trained retriever (Section 5.2). 4 Figure 2: Inference-only modification does not work Performance after adding in-context examples to the query without updating model parameters. We see that embedding models are not able to leverage in-context examples out of the box, as opposed to decoder-only models. Inference-only Modification Figure 2 illustrates the impact of incorporating in-context examples at inference time. Here, we simply modify the query format with retrieved in-context examples (i.e. qinst+ic, Eq. 4) at inference time and compare its performance with the query format that does not have retrieved in-context examples (i.e. qinst, Eq. 1). We evaluate the performance on three retriever models: SFR-Embedding-2-R (Meng et al., 2024), LLM2Vec-Llama-3-8B-Supervised (BehnamGhader et al., 2024), and E5-Mistral-7B-Instruct (Wang et al., 2024b). Unlike in autoregressive LLMs, these embedding models generally exhibit decreased performance when in-context examples are added, with LLM2Vec-Llama-3-8B-Supervised showing the largest drops in performance, except on one dataset (SciFact), where 2 out of 3 models show marginal gains over providing only instructions. Our experiments, which include adding more in-context examples and using nearest-neighbor examples, extend the findings of Muennighoff et al. (2024), where in-context examples led to decrease in performance on the GritLM models. Table 1: Training from decoder-only (LLM) checkpoint. Performance is measured by nDCG@10. RARe shows up to +2.72% absolute gain on average over Promptriever, demonstrating that starting from an existing embedding model is not requirement. We provide breakdown of In-Domain (ID) and Out-of-Domain (OOD) performance. Method Base model Training Data Llama-2 Llama-3 Llama-3 RepLLaMA RepLLaMA RARe RepLLaMA Llama-3.1-Instruct Promtpriever Llama-3.1-Instruct MS-MARCO + Synthetic RARe MS-MARCO MS-MARCO MS-MARCO MS-MARCO Llama-3.1-Instruct MS-MARCO ID OOD Average MS-MARCO BeIR RAR-b 42.00 43.56 44.77 43.67 42.70 42. 53.69 53.99 55.87 54.34 56.10 56.05 20.23 18.50 22.34 19.20 20.95 23.67 38.64 38.68 40.99 39.07 39.94 40."
        },
        {
            "title": "5.1 TRAINING FROM LLM CHECKPOINTS",
            "content": "Next, we present the results of applying our approach when training from LLM checkpoint. This might preserve in-context learning capacity of the LLM, which can be lost during standard IR training, which compresses query and passage into fixed dimensional vector. We experiment with three LLM checkpoints (Llama-2 (Touvron et al., 2023), Llama-3 (Dubey et al., 2024), Llama-3.1Instruct) to enable comparison with prior work Ma et al. (2023); Weller et al. (2024b). Comparison Systems We compare training with our in-context example augmented query with two baselines. The first baseline is vanilla query (Eq. 1), which was explored in RepLLaMA (Ma et al., 2023). The second baseline is Promptriever (Weller et al., 2024b) which augments queryspecific instructions using synthetically generated training set from MS-MARCO. In all these 5 Table 2: Training from retriever checkpoint. Performance (nDCG@10) on BeIR (Thakur et al., 2021) and RAR-b (Xiao et al., 2024) benchmarks when fine-tuning retriever model on E5 dataset. We report breakdown of performance on In-Domain (ID) and Out-of-Domain (OOD) tasks on BeIR. We consider all RAR-b tasks as OOD. LLM2Vec-Llama-3-8b-Supervised E5-Mistral-Instruct Method BeIR RAR-b BeIR RAR-b ID OOD All ID OOD All Base Instruct RARe 71.31 70.46 71.67 49.28 47.79 49.30 56.63 55.35 56.76 21.55 23.44 23.10 71.95 72.91 72.98 49.33 48.98 50. 56.87 56.96 58.28 22.17 24.12 25.79 systems, the task-specific instruction is null string (Ma et al., 2023) as we train on single task (MS-MARCO). Results Table 1 presents the performance on downstream benchmarks when training from LLM checkpoints. Comparing within the same base LLM checkpoint, our apporach outperforms both baselines (RepLLaMA and Promptriever). Our performance is competitive to that of Promptriever (Weller et al., 2024b), without incorporating synthetic data during training. Specifically, RARe achieves an absolute gain of +2.7% over Promptriever on the RAR-b benchmark."
        },
        {
            "title": "5.2 TRAINING FROM RETRIEVER CHECKPOINTS",
            "content": "Lastly, we continue training retriever models LLM2Vec-Llama-3-8B-Supervised (BehnamGhader et al., 2024), E5-Mistral-Instruct (Wang et al., 2024b) on training set where queries are augmented with in-context examples. As these initial checkpoints have already been trained on the training dataset, the extent that retrievers adapt to new query format can be limited. Comparison Systems We first report the initial retriever performance (Base) without any modification. Then, we compare continued fine-tuning with the task-specific instruction query format (Eq. 1) which only prepends the task specific instruction (Instruct, qinst) to our in-context example augmented query format (Eq. 4). Results Table 2 reports experimental results in this setting. Overall, both fine-tuning approaches provides gains over the base checkpoints. Comparing two settings, Instruct (qinst) vs. RARe (qinst+ic), our method achieves notable improvement with E5-Mistral-Instruct base model (1.95% over Instruct on out-of-domain tasks, and 1.32% overall). Our method performs similar to Instruct (qinst) setting when trained with the LLM2Vec base model. It is hard to attribute why experimental results varies based on the base retriever checkpoint, but we note the following differences between the two models. LLM2Vec-Llama-3-8b-Supervised is the only model in our experiments where further fine-tuning with only instructions led to decrease in in-domain performance. E5-Mistral-Instruct employs causal attention with last token pooling, and trains on proprietary synthetic dataset, LLM2Vec-Llama-3-8b-Supervised uses bidirectional attention with mean pooling, training only on the E5 public portion. The effectiveness of learning with in-context examples may depend on the model architecture or data setting, and further investigation can be explored in future work."
        },
        {
            "title": "6.1 CHOICE OF IN-CONTEXT EXAMPLES",
            "content": "Retrieved (Similar) vs. Random In-Context Examples In Figure 3, we study the impact of retrieving the nearest neighbor query-document pairs as examples against randomly chosen examples during training and evaluation. We observe that using retrieved examples during both training and evaluation (Retrieved, Retrieved) consistently outperforms other configurations across most 6 Figure 3: Retrieved vs. Random In-context Examples. Change in performance (nDCG@10) on E5-Mistral-Instruct with RARe (qinst+ic) from the baseline setting (qinst both during training and evaluation time). Using retrieved examples during training and inference enhance model performance in most benchmark datasets. Figure 4: Change in performance (nDCG@10) from the base model (E5-Mistral-Instruct) for varying similarity between the closest in-context example query and target query (Score@Top-1). datasets. (Random, Retrieved) shows second best overall performance, and generally outperforms (Random, Random), suggesting retrieved examples during evaluation is advantageous even when trained with randomly paired in-context examples. Our findings align with prior work in in-context learning that the incorporation of semantically similar examples is beneficial (Agrawal et al., 2022; Rubin et al., 2022). Does Having Semantically Relevant In-Context Example Help? For some test examples, augmented in-context examples are very relevant, and for others, much less so. In this section, we group the evaluation examples by the maximum similarity of in-context query and the test query measured by an off-the-shelf sentence embedding model (Score@Top-1).2 and plot the performances for each group. Figure 4 presents the performance of our system (RARe) and baseline (Instruct). On NFCorpus and SciFact datasets, we observe that when the closest in-context example has high similarity with the target query, RARe demonstrates over 10% gains compared to the base model. On the other hand, fine-tuning exhibits relatively lower performance gains with increasing similarity thresholds. For ArguAna, and FiQA2018, RARes gains with increasing Score@Top-1 are less pronounced, but generally matches the performance of the base model. How Many In-Context Examples Are Sufficient? We analyze the performance of RARe when varying the number of in-context examples provided during training and inference. Table 3 shows that increasing the number of in-context examples generally enhances performance. On ArguAna, we observe that 0 examples are optimal, which is likely due to the mismatch in the lengths of the queries used as in-context examples3 (which are significantly shorter) versus the actual test queries. However, the impact is not uniformly positive across all datasets, suggesting that the optimal number of in-context examples may be dataset-dependent. We observe similar trends when we fix the number of in-context examples to five during training and vary the number of examples provided during inference, which are provided in Table 12 in the Appendix. 2https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 3https://huggingface.co/datasets/BeIR/arguana-generated-queries 7 Table 3: Impact of the number of in-context examples (k) during training and evaluation. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of examples depends on the task. Arguana CQADupStack FiQA2018 NFCorpus SciFact Touche2020 Average Instruct (0) 61.19 1 3 5 10 60.47 62.98 60.87 58.85 44.82 46.76 47.12 48.46 48. 57.39 56.07 57.08 57.31 57.03 40.99 40.67 40.77 42.28 42.24 77.28 81.47 83.71 84.79 87. 29.35 29.78 27.12 28.70 28.29 51.84 52.54 53.13 53.74 53.82 Table 4: In-Context Format Comparing variants of in-context example format on E5-MistralInstruct. Instruct refers to the baseline which does not use any in-context examples. Fine-Tuning IC Eval Setting ArguAna CQA FiQA2018 NFCorpus SciFact Touche2020 Average Instruct - 61.19 44.82 57.39 RARe Queries-Only Doc-Only Shuffle-NC Shuffle-C Regular 58.88 57.54 60.17 58.97 60.87 46.66 48.28 45.78 47.97 48.46 54.44 56.02 54.25 55.98 57.31 40.99 41.42 41.62 41.17 41.78 42. 77.28 78.84 79.80 80.70 80.51 84.79 29.35 28.09 29.01 29.18 28.97 28.70 51.83 51.39 52.05 51.88 52.36 53. Ablating Content and Format of In-context Examples One can view in-context examples as form of query expansion (Lv & Zhai, 2009; Wang et al., 2023), providing useful keywords to improve the performance. In Table 4, we analyze the impact of various formats of in-context examples. All models are trained with the same format that they are evaluated on. Query-Only and Doc-Only contain only queries and documents of in-context examples, respectively. For ShuffleC, we randomly shuffle the mapping between and d. On the other hand, for Shuffle-NC, we do not assume any structure, meaning that query can be followed by query as well as document. First, we observe that Query-Only shows larger performance drop over Doc-Only, suggesting incontext documents might contain more useful contents than in-context queries. Second, we observe that shuffling the pairings (Shuffle-C) marginally hurts in-context learning in RARe, as opposed to Shuffle-NC. Our findings align with prior study in decoder-only models (Min et al., 2022b) which showed strict correspondence between and is not required for performance gains from in-context examples. We observe similar trends on keeping the training format fixed (Regular), and varying only the evaluation format see Table 14 in the Appendix. Table 5: Impact of adding negative documents in the in-context prompt. All results are on E5Mistral-Instruct. Negative documents (d ) in the prompt do not enhance performance. Training / Eval Setting ArguAna CQA FiQA2018 NFCorpus SciFact Touche2020 Average RARe-qinst+ic RARe-qinst+ic+neg 60.87 48.46 57.31 42.28 84.79 61. 48.09 56.89 41.58 82.37 28.70 30. 53.74 53.44 + Negative Documents in the Query So far, we have used (q, ) i.e (Query, Positive Document) pairs as the in-context prompt. Therefore, we study the impact of including negative documents. + Specifically, the augmented query qinst+ic+neg includes examples of the form (q, ), where the documents are prefixed with the term Positive Document: and Negative Document: respectively. Table 5 presents the downstream performance comparison between RARe variants trained solely on positive examples and those trained with augmented negative documents. The results indicate no performance gains from including negative documents. In fact, training with negative examples led to slight decrease in performance. , Table 6: Latency breakdown (in seconds) of each stage in the retrieval pipeline for qinst and qinst+ic evaluation settings. # Corpus denote the number of documents and Avg len. denote the average number of query tokens split by whitespace. Table 11 in the Appendix provides numbers on additional datasets. Dataset # Corpus Eval Setting Avg len. NN Query Search Total Inc. NFCorpus FiQA2018 57638 TRECCOVID 171332 Touche2020 382545 Quora 522931 DBPedia 4635922 qinst qinst+ic qinst qinst+ic qinst qinst+ic qinst qinst+ic qinst qinst+ic qinst qinst+ic 3.3 866.0 10.9 1016.6 10.6 722. 6.6 1287.8 9.5 129.5 5.5 158.2 0 3.30 0.20 152. 0 6.92 0.45 278.62 0 0.31 0 0.20 1.83 21. 1.42 40.32 0.54 0.57 7.92 8.83 4.08 4.30 9.29 10.50 3.84 153. 14.84 287.90 5.91 26.39 10.71 51.02 0 113.93 986.42 1100.35 3.19 530.33 982.61 1516.13 0 0. 36.93 46.21 588.38 709.27 625.31 755.67 - 40.04 - 19.40 - 4.47 - 4.76 - 1.38 - 1."
        },
        {
            "title": "6.2 EFFICIENCY ANALYSIS",
            "content": "In Table 6, we present breakdown of the latency of each stage of the retrieval pipeline for both baseline (qinst) and in-context (qinst+ic) settings. We measure the total time required to obtain nearest-neighbour in-context examples (NN) from BM25, compute query embeddings (Query), and perform search with FAISS (Douze et al., 2024) with encoded query embeddings on the pre-computed document index (Search). We observe that the largest contributing factors to latency are the average length of input queries (Avg len.), and the size of the index (# Corpus). For large query length and small corpus sizes, the in-context setting demonstrates significant increase in total latency (19.40-40.04 for FiQA2018 and NFCorpus, respectively). However, for smaller average query lengths, this latency diminishes, as seen for Quora (1.38) and DBPedia (1.21). Moreover, the added latency due to the in-context setting also diminishes when the corpus size grows, as the time required for search outweighs the time to encode the query. For example, on Touche2020 with larger corpus of 380K documents, the increase in latency is 4.76 compared to FiQA2018 (19.40) for similar query lengths."
        },
        {
            "title": "7 RELATED WORK",
            "content": "In-context learning ICL (Brown et al., 2020) allows models to adapt to new tasks in few-shot manner by conditioning on the input data and the context provided at inference time. ICL has been effectively applied to wide range of tasks such as classification (Milios et al., 2023), translation (Zhu et al., 2024), mathematical reasoning (Wei et al., 2022; Zhou et al., 2022), and code generation (Li et al., 2023a). Recent advancements have enhanced the ICL capabilities of language models through additional training procedures (Huang et al., 2022; Gu et al., 2023; Shi et al., 2024). Min et al. (2022a) and Chen et al. (2022) perform meta-learning with in-context examples on wide collection of tasks, with the goal of adapting to new task at inference time through few-shot in-context examples. Other works have explored improving performance through more principled approaches to select in-context examples during inference (Zhang et al., 2022; Sorensen et al., 2022; Wang et al., 2024c; Qin et al., 2024; Lee et al., 2024c). simple and popular approach is to retrieve examples that are most similar to the input (Liu et al., 2022; Rubin et al., 2022; Li et al., 2023c). Providing in-context examples to re-ranking models has been studied in prior work (Drozdov et al., 2023), but the potential of augmenting retrievers themselves by leveraging in-context examples remains unexplored. Muennighoff et al. (2024) explored providing an in-context example out-of-the-box, but showed an overall decrease in performance compared to zero-shot inference. 9 Retrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023; Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b; BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2022; Wang et al., 2024a). Despite these advancements, challenge that remains is the ability to tailor retrieval systems to specific tasks or queries. To address this, recent line of work explores incorporating instructions into retrieval by training models to use task-specific instructions along with the query (Su et al., 2023; Asai et al., 2023). Oh et al. (2024) and Weller et al. (2024a) further propose using instructions that are specific to each query. Another well-established technique in retrieval is query expansion (Jagerman et al., 2023; Li et al., 2023b; Chen et al., 2024), where the query is augmented with additional terms to enrich the embedding as form of relevance feedback (Lv & Zhai, 2009). Recent efforts have focused on applying LLMs to expand the original query before retrieval (Wang et al., 2023; Shen et al., 2024). These techniques are not mutually exclusive, and can be integrated with our approach."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this paper, we explored augmenting in-context examples to retrieval models. Building on the limitations of existing retriever models in following in-context examples, we introduced RARe, simple strategy that equips retrievers with the ability to leverage in-context examples by training with semantically similar in-context examples. Through detailed experiments and analyses, we demonstrated that RARe consistently improves performance across various architectures and downstream retrieval tasks, demonstrating the effectiveness of in-context learning for retriever models."
        },
        {
            "title": "9 LIMITATIONS AND FUTURE WORK",
            "content": "+ Similar to in-context settings in autoregressive models, limitation of our approach is the require- ) pairs at inference time. RARe also ment for set of in-context examples in the form of (q, introduces additional latency at inference time due to the encoding of in-context examples in the augmented query. This latency becomes more pronounced with longer documents, resulting in correspondingly extended queries. While the overhead is particularly significant for small indexes, it diminishes as the size of the index grows. To address these challenges, future research could explore several avenues, such as using efficient long-context retrievers (Saad-Falcon et al., 2024; Zhang et al., 2024) as backbone, or developing extractive and/or abstractive compression techniques on incontext documents to reduce query length. In this work, we used BM25 due to its lightweight nature to retrieve nearest neighbour examples. Future work could explore stronger models and approaches to reduce latency. Our current experiments are limited to English-language tasks, with potential to expand the scope to multilingual settings. Future work could explore curating synthetic data, an increasingly popular area of study for embedding models (Lee et al., 2024b; Wang et al., 2024b; Weller et al., 2024b), but for training with in-context examples. Future work could also explore developing new contrastive objectives to provide better signals during training with in-context examples."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The work is partially supported by NSF grant IIS-2312948. We also thank Fangyuan Xu, Michael Zhang, Anuj Diwan, and other members of the UT NLP community for insightful feedback."
        },
        {
            "title": "REFERENCES",
            "content": "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. Incontext examples selection for machine translation, 2022. URL https://arxiv.org/abs/ 2212.02437. Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.findings-acl.225. 10 Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders, 2024. URL https://arxiv.org/abs/2404.05961. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. Abductive commonsense In International Conference on Learning Representations, 2020. URL https: reasoning. //openreview.net/forum?id=Byg1v1HKDB. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. Xinran Chen, Xuanang Chen, Ben He, Tengfei Wen, and Le Sun. Analyze, generate and refine: In Findings of the Association Query expansion with LLMs for zero-shot open-domain QA. for Computational Linguistics ACL 2024, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. findings-acl.708. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning, 2022. URL https://arxiv.org/abs/2110.07814. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. Quora question pairs, 2017. URL https://kaggle.com/competitions/quora-question-pairs. Gabriel de Souza P. Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. Nv-retriever: Improving text embedding models with effective hard-negative mining, 2024. URL https://arxiv.org/abs/2407.15831. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. survey on incontext learning, 2024. URL https://arxiv.org/abs/2301.00234. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library, 2024. URL https://arxiv.org/abs/2401.08281. Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew McCallum, Donald Metzler, and Kai Hui. Parade: Passage ranking using demonstrations with large language models, 2023. URL https://arxiv.org/abs/ 2310.14408. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, July 2019. URL https: //aclanthology.org/P19-1346. 11 Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of senIn Proceedings of the 2021 Conference on Empirical Methods in Natutence embeddings. ral Language Processing. Association for Computational Linguistics, 2021. URL https: //aclanthology.org/2021.emnlp-main.552. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, July 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.acl-long.267. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Yukun Huang, Yanda Chen, Zhou Yu, and Kathleen McKeown. In-context learning distillation: Transferring few-shot learning ability of pre-trained language models, 2022. URL https:// arxiv.org/abs/2212.10670. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learnISSN 2835-8856. URL https: ing. Transactions on Machine Learning Research, 2022. //openreview.net/forum?id=jKN1pXi7b0. Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. Query expansion by prompting large language models, 2023. URL https://arxiv.org/abs/2305. 03653. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answerIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language ing. Processing (EMNLP). Association for Computational Linguistics, 2020. URL https:// aclanthology.org/2020.emnlp-main.550. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7, 2019. URL https://aclanthology.org/ Q19-1026. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models, 2024a. URL https://arxiv.org/abs/2405.17428. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar Naim. Gecko: Versatile text embeddings distilled from large language models, 2024b. URL https://arxiv.org/abs/2403.20327. Yoonsang Lee, Pranav Atreya, Xi Ye, and Eunsol Choi. Crafting in-context examples according to lms parametric knowledge, 2024c. URL https://arxiv.org/abs/2311.09579. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL https: //arxiv.org/abs/2005.11401. 12 Jia Li, Ge Li, Chongyang Tao, Jia Li, Huangzhao Zhang, Fang Liu, and Zhi Jin. Large language model-aware in-context learning for code generation, 2023a. URL https://arxiv.org/ abs/2310.09748. Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, and Michael Bendersky. Generate, filter, and fuse: Query expansion via multi-step keyword generation for zero-shot neural rankers. arXiv preprint arXiv:2311.09175, 2023b. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, In Proceedings of and Xipeng Qiu. Unified demonstration retriever for in-context learning. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, July 2023c. Association for Computational Linguistics. URL https: //aclanthology.org/2023.acl-long.256. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.deelio-1.10. Yuanhua Lv and ChengXiang Zhai. comparative study of methods for estimating query language models with pseudo feedback. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 09, pp. 18951898, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585123. doi: 10.1145/1645953.1646259. URL https: //doi.org/10.1145/1645953.1646259. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval, 2023. URL https://arxiv.org/abs/2310.08319. Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Sfr-embedding-mistral:enhance text retrieval with transfer learning. and Semih Salesforce URL https://blog.salesforceairesearch.com/ Yavuz. AI Research Blog, 2024. sfr-embedded-mistral/. Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. In-context learning for text classification with many labels. In Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, Singapore, December 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.genbench-1.14. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seattle, United States, July 2022a. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.naacl-main.201. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022b. URL https://arxiv.org/abs/2202.12837. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 2014 2037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.eacl-main.148. URL https://aclanthology.org/2023.eacl-main.148. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning, 2024. URL https://arxiv. org/abs/2402.09906. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: human generated machine reading comprehension dataset. CoRR, abs/1611.09268, 2016. URL http://arxiv.org/abs/1611.09268. 13 Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin, Hansol Jang, Changwook Jun, and Minjoon Instructir: benchmark for instruction following of information retrieval models, 2024. Seo. URL https://arxiv.org/abs/2402.14334. Chengwei Qin, Aston Zhang, Chen Chen, Anirudh Dagar, and Wenming Ye. In-context learning with iterative demonstration selection, 2024. URL https://arxiv.org/abs/2310. 09881. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, November 2016. URL https://aclanthology.org/D16-1264. Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333389, apr 2009. ISSN 1554-0669. URL https://doi.org/ 10.1561/1500000019. Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to ai complete question answering: set of prerequisite real tasks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 87228731, 2020. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.naacl-main.191. Jon Saad-Falcon, Daniel Y. Fu, Simran Arora, Neel Guha, and Christopher Re. Benchmarking and building long-context retrieval models with loco and m2-bert, 2024. URL https://arxiv. org/abs/2402.07440. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, 2019. Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Yibin Lei, Tianyi Zhou, Michael Blumenstein, and Daxin Jiang. Retrieval-augmented retrieval: Large language models are strong zero-shot retriever. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 1593315946, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.943. URL https://aclanthology.org/2024.findings-acl.943. Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Gergely Szilvasy, Rich James, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. Incontext pretraining: Language modeling beyond document boundaries, 2024. URL https: //arxiv.org/abs/2310.10638. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. acl-long.60. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings, 2024. URL https://arxiv.org/abs/ 2402.15449. 14 Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned In Findings of the Association for Computational Linguistics: ACL 2023, text embeddings. Toronto, Canada, July 2023. Association for Computational Linguistics. URL https:// aclanthology.org/2023.findings-acl.71. Qingyu Tan, Hwee Tou Ng, and Lidong Bing. Towards benchmarking and improving the temporal reasoning capability of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1482014835, 2023. Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=wCu6T5xFjeJ. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: largescale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, June 2018. URL https://aclanthology.org/N18-1074. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language modIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Proels. cessing, Singapore, December 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.emnlp-main.585. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024a. URL https://arxiv.org/abs/2212.03533. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models, 2024b. URL https://arxiv.org/abs/ 2401.00368. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are latent variable models: Explaining and finding good demonstrations for incontext learning, 2024c. URL https://arxiv.org/abs/2301.11916. Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, and Wei Chen. theoretical analysis of ndcg type ranking measures, 2013. URL https://arxiv.org/abs/1304.6480. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Chain-of-thought prompting elicits reasoning in large In Advances in Neural Information Processing Systems, volume 35, URL https://proceedings.neurips.cc/paper_files/paper/2022/ Quoc Le, and Denny Zhou. language models. 2022. file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. 15 Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models to follow instructions, 2024a. URL https://arxiv.org/abs/2403.15246. Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, and Jack Hessel. Promptriever: Instruction-trained retrievers can be prompted like language models, 2024b. URL https://arxiv.org/abs/2409.11136. Chenghao Xiao, Thomas Hudson, and Noura Al Moubayed. Rar-b: Reasoning as retrieval benchmark, 2024. URL https://arxiv.org/abs/2404.06347. Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. knn prompting: Beyond-context learning with calibration-free nearest neighbor inference, 2023. URL https://arxiv.org/abs/2303.13824. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018. Association for Computational Linguistics. URL https: //aclanthology.org/D18-1259. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can In Anna Korhonen, David Traum, and LluÄ±s M`arquez machine really finish your sentence? (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10. 18653/v1/P19-1472. URL https://aclanthology.org/P19-1472. Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao. Mamba retriever: Utilizing mamba for effective and efficient dense retrieval, 2024. URL https://arxiv.org/abs/2408. 08066. Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.622. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning, 2022. URL https://arxiv. org/abs/2211.09066. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical In Findings of the Association for Computational Linguistics: NAACL results and analysis. 2024, Mexico City, Mexico, June 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-naacl.176."
        },
        {
            "title": "APPENDIX",
            "content": "The appendix is organized as follows: In Appendix A, we present details on additional data preprocessing and other training details. In Appendix B, we present additional results and experiments."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 TRAINING DETAILS Hyperparameters For fine-tuning Llama-3-8B, we follow the setting outlined in Ma et al. (2023). We train on 4 H100 GPUs with per-device batch size 8 and gradient accumulation steps 4. We 16 Algorithm 2: RARe - Inference Input: list of test queries Qtest Dtest, Corpus Dtest, embedder E(), the number of in-context examples k, Training dataset of target task DT , task instruction ttest. [i] )] do 1 , qic 1 , dic+ test {(qic 1: Ce Construct document index as E(d), C. 2: Dpred [] 3: for [0, len(Qtest = Qtest 4: In-Context Example Retrieval: 5: {qic 6: {dic+ 7: 8: Dic 9: 10: 11: 12: 13: 14: Dpred.append({d1, d2, . . . , dK}) Output: Predictions Dpred. 2 , . . . , qic + (q 2 , . . . , dic+ , , dic+ 1 , dic+ )} Query Augmentation / Encoding: qinst+ic = Instruct: {ttest}; Query: {qic eq E(qinst+ic Prediction: {d1, d2, . . . , dK} argtop-KdC exp(cos(eq, ed)) } {d 1 ), . . . , (qic ) D, ) } Retrieve nearest neighbor queries of from DT using BM25 {qic 1 , . . . , qic }} + 1 }; Document: {dic+ 1 } ; Query: {q} apply LoRA (Hu et al., 2021) with r=32, temperature of 0.01, learning rate 1e-4 with 100 warmup steps. We use sequence length of 512 for documents and 1024 for queries as in-context augmented queries are longer. For RARe we use mixture of 70% examples with in-context examples and 30% without Table 16. (E5-Mistral-Instruct, LLM2Vec-Llama-3-8BWhen fine-tuning existing retriever models Supervised), we follow setting similar to BehnamGhader et al. (2024). We train on 8 H100 GPUs with largest possible per-device batch size of 32 along with 2 gradient accumulation steps. We consider random subset of 100K examples from the public E5 dataset mixture (Springer et al., 2024; Wang et al., 2024b). We use learning rate of 2e-4, maximum sequence length 1024, warmup ratio 0.1 for 1 epoch. For E5-Mistral-Instruct, we apply LoRA (Hu et al., 2021) r=16, and r=4 for LLM2Vec-Llama-3-8B-Supervised since higher rank was leading to severe overfitting on the instruction baseline. A.2 DATA PROCESSING RAR-b Since RAR-b benchmark provides only test split, we parse the original training data for each dataset to use as in-context examples. We exclude datasets without any training splits and 2 datasets that were mixture of multiple tasks or datasets, thereby being difficult to parse. This results in 8 datasets to evaluate on. We preprocess the training split to match the format of RAR-b test split, without excluding any instances. An exception is made for Î±-NLI, where there were multiple identical instances. Therefore, we removed such duplicates, resulting in 72,046 in-context candidates. Furthermore, some RAR-b queries are composed of sentences with (multiple) indicators (e.g., Start:, End:). To address this, we make minor modification in formatting, enclosing the queries in brackets. The final query representation is qinst+ic = Instruct: {t}; Query: [{qic 1 }]; Document: {dic+ 1 } ; Query: [{q}]. Inference Algorithm Algorithm 2 provides detailed outline of inference with RARe. Promptriever Promptriever(Weller et al., 2024b) employs 10 different prompts and reports the highest score for each dataset. We apply the prompt that works the best (outperforms 5/15 datasets), which is as follows: document that meets these criteria is considered relevant, while document that does not meet these criteria is considered non-relevant."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTS",
            "content": "Table 7: Performance (nDCG@10) on BeIR (Thakur et al., 2021) when fine-tuning retriever model on E5 dataset. We report breakdown of performance on In-Domain (ID) and Out-of-Domain (OOD) tasks on BeIR. LLM2Vec-Llama-3-8b-Supervised E5-Mistral-Instruct ID OOD Category Dataset Instruct qinst RARe qinst qinst+ic Base qinst 90.20 71.76 64.21 87.16 43. FEVER HotpotQA NQ QuoraRetrieval MSMARCO 62.78 ArguAna ClimateFEVER 34.27 48.25 CQADupStack 48.34 DBPedia 55.33 FiQA2018 41.83 NFCorpus 22.96 SCIDOCS 78.22 SciFact 20.50 Touche2020 80.34 TRECCOVID Average 56.63 88.43 73.83 65.00 87.88 40.77 59.54 34.67 49.10 48.41 54.26 41.61 22.92 77.70 22.71 78. 86.62 79.09 66.13 87.63 38.88 57.05 34.73 49.93 49.09 52.82 41.84 23.35 81.77 19.54 82.78 88.12 72.50 63.63 87.85 40.19 60.51 34.49 49.76 48.61 52.99 41.92 23.97 76.89 22.11 68.37 55.35 56. 56.76 56.87 Base qinst 87.84 75.72 63.53 89.61 43.06 61.65 38.35 42.97 48.89 56.81 38.58 16.32 76.42 26.27 87.03 Instruct qinst RARe qinst qinst+ic 91.50 73.91 67.44 89.82 41.89 61.19 39.03 44.82 48.92 57.39 40.99 17.94 77.28 29.35 72.89 56. 90.18 72.18 68.15 89.59 41.88 62.90 38.99 45.57 49.24 56.33 41.19 18.71 77.11 27.56 77.03 90.48 75.95 67.66 88.95 41.88 60.87 37.50 48.46 49.65 57.31 42.28 20.19 84.79 28.7 79.58 57.11 58. Table 8: Performance on reasoning-focused IR benchmark RAR-b (Xiao et al., 2024) when finetuning existing retriever models. LLM2Vec-Llama-3-8b-Supervised E5-Mistral-Instruct Dataset ARC-C Î±-NLI HellaSwag PIQA Quail SiQA TempReason-L1 WinoGrande Average Base qinst 18.81 26.59 34.32 33.57 6.83 6.99 5.24 40.02 21.55 Instruct qinst RARe qinst qinst+ic 18.28 25.25 34.19 38.12 5.57 4.39 5.55 48.47 17.02 23.66 33.29 39.72 4.25 4.55 7.87 54.44 18.77 27.29 34.19 37.07 6.06 5.34 5.89 52.88 23.44 Base qinst 19.00 26.04 35.38 39.80 8.40 5.66 3.60 39.48 Instruct qinst 20.37 25.70 35.99 39.35 10.94 5.45 4.71 50.41 24.12 RARe qinst 22.72 24.19 35.07 37.22 15.34 5.75 4.55 44.26 qinst+ic 26.44 23.23 36.29 41.35 14.69 6.15 4.67 53.50 23.64 25.79 22. 23.10 22.17 B.1 PERFORMANCE ON BEIR AND RAR-B Table 7 and Table 8 provide detailed numbers on each dataset from BeIR and RAR-b respectively when training from retriever checkpoints. Table 9 and Table 10 provide detailed numbers on each dataset from BeIR and RAR-b respectively when training from decoder-only LLMs. B.2 EFFICIENCY EVALUATION Table 11 provides breakdown of latency on additional datasets. B.3 CHOICE OF IN-CONTEXT EXAMPLES Table 13 provides detailed numbers for varying in-context examples on all OOD BeIR tasks. Table 15 provides detailed numbers for various prompt formats on all OOD BeIR tasks. 18 Table 9: Performance (nDCG@10) on BeIR when training decoder-only models. Llama2 Llama Llama-3.1-Instruct Dataset ArguAna ClimateFEVER CQADupStack DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ Quora SCIDOCS SciFact TRECCOVID Touche2020 Average RepLLaMA RepLLaMA RARe RepLLaMA Promptreiver qinst+ic qinst qinst qinst qinst 48.60 29.30 37.91 44.80 82.90 45.00 68.80 42.00 36.00 63.00 86.00 16.10 75.30 83.90 34.10 52.91 52.83 32.52 42.59 45.62 81.79 44.31 72.24 43.56 37.73 62.70 88.34 19.66 75.02 83.15 27. 53.99 49.48 32.12 42.96 45.79 83.66 47.13 72.72 44.77 39.34 65.96 87.65 19.45 77.20 85.76 32.89 55.13 51.38 33.13 41.58 44.73 79.22 44.50 70.90 43.67 38.77 61.09 86.84 19.26 75.38 83.15 30.77 53.62 58.90 29.80 42.18 46.00 85.50 47.20 71.70 42.70 38.50 63.80 87.30 20.80 77.50 84.50 31. 55.21 RARe qinst qinst+ic 54.77 35.91 42.55 45.87 80.05 44.36 70.55 41.65 38.16 60.92 87.95 20.02 74.59 77.52 25.47 52.83 34.24 43.31 45.95 81.84 46.20 74.01 42.93 39.74 65.20 87.65 19.52 76.54 85.30 32. 53.36 55.18 Table 10: Performance (nDCG@10) on datasets from RAR-b when training decoder-only models. Llama2 Llama3 Llama-3.1-Instruct Dataset ARC-C Î±-NLI HellaSwag PIQA Quail SiQA TempReason-L1 WinoGrande Average RepLLaMA RepLLaMA RARe RepLLaMA Promptreiver qinst+ic qinst qinst qinst qinst 11.79 25.40 30.83 31.56 6.40 2.82 1.49 51.58 20.23 11.65 24.35 31.47 32.84 6.21 2.61 1.75 37.11 18. 13.48 30.38 30.27 34.12 5.98 3.87 3.61 57.01 22.34 11.68 24.96 31.03 33.42 5.71 2.75 2.05 42.01 19.20 14.63 24.70 32.57 34.80 7.80 3.53 4.32 45.25 20. RARe qinst qinst+ic 13.24 27.34 31.42 34.23 6.92 2.18 4.84 44.72 15.02 31.58 28.81 35.59 6.91 3.14 6.59 61.69 20. 23.67 B.4 MIXTURE OF TRAINING DATA In Table 16, we analyze the impact of training with only in-context examples when starting from decoder-only LLMs. As opposed to starting from existing retriever models, which have been trained without in-context examples, we observe that performance drops in the instruction-only setting. This can be largely mitigated by considering mixture of in-context and instruction-only queries. 19 Table 11: Latency breakdown (in seconds) of each stage in the retrieval pipeline for qinst and qinst+ic evaluation settings. # Corpus denote the number of documents and Avg len. denote the average number of query tokens split by whitespace. Dataset # Corpus Eval Setting Avg len. NN Query Search Total Inc. SciFact 5183 SCIDOCS 25657 CQADupStack 38100 ClimateFEVER 5416593 qinst qinst+ic qinst qinst+ic qinst qinst+ic qinst qinst+ic 12.5 1250.7 0 4.52 0.25 212. 0.61 0.61 5.74 5.79 0 11.29 0.67 354.82 0 10.80 1.33 570. 10.20 10.30 5.13 213.21 17.03 361.28 21.01 581.67 0 26. 1725.90 1751.96 3.55 651.76 1723.84 2379.15 - 41.56 - 21.21 - 27.69 - 1.36 9.4 901. 8.6 678.2 20.2 831.3 Table 12: Impact of the number of in-context examples (k) at inference time. = 5 during training. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of examples can vary by task. Dataset Instruct (0) 1 ArguAna ClimateFEVER CQADupStack DBPedia FiQA2018 NFCorpus SCIDOCS SciFact Touche2020 TRECCOVID Average 61.19 39.03 44.82 48.92 57.39 40.99 17.94 77.28 29.35 72.89 48.98 62.90 38.99 45.57 49.24 56.33 41.19 18.71 77.11 27.56 77. 61.24 38.27 47.49 49.79 57.61 41.48 19.83 83.56 27.53 76.96 49.46 50.38 # Examples 3 60.99 37.97 48.33 48.34 57.42 42.10 20.17 84.45 27.70 78. 50.65 5 61.18 37.50 48.46 49.65 57.31 42.28 20.19 84.79 28.70 79.58 10 60.37 37.67 48.48 49.82 57.38 42.29 20.20 85.12 30.77 78.77 50. 51.09 Table 13: Impact of the number of in-context examples (k) during training and inference. All results are on E5-Mistral-Instruct. In general, performance increases when increasing the number of examples, and the optimal number of in-context examples can vary by task. Dataset Instruct (0) 0 Arguana ClimateFEVER CQADupStack DBPedia FiQA2018 NFCorpus SCIDOCS SciFact Touche2020 TRECCOVID Average 61.19 39.03 44.82 48.92 57.39 40.99 17.94 77.28 29.35 72.89 48.98 62.90 38.99 45.57 49.24 56.33 41.19 18.71 77.11 27.56 77.03 60.47 37.94 46.76 47.70 56.07 40.67 20.01 81.47 29.78 78. 49.46 50.18 20 # Examples 3 62.98 36.45 47.12 49.05 57.08 40.77 19.28 83.71 27.12 73. 48.83 5 60.87 37.50 48.46 49.65 57.31 42.28 20.19 84.79 28.70 79.58 10 58.85 36.54 48.92 47.95 57.03 42.24 21.54 87.61 28.29 86.11 51. 53.16 Table 14: In-Context Format Comparing variants of in-context example format on E5-MistralInstruct during inference only. Training is done with the Regular format. Instruct refers to the baseline which does not use any in-context examples. Instruct RARe Dataset - Query-Only Doc-only Shuffle-NC Shuffle-C Regular 61.19 ArguAna ClimateFEVER 39.03 44.82 CQADupStack 48.92 DBPedia 57.39 FiQA2018 40.99 NFCorpus 17.94 SCIDOCS 77.28 SciFact 29.35 Touche2020 72.89 TRECCOVID Average 48.98 57.36 38.35 39.56 49.14 55.67 41.00 19.06 77.46 27.04 75.11 47. 60.35 38.32 48.43 49.69 56.85 42.09 20.06 81.88 29.02 79.97 50.67 55.64 37.44 47.70 49.72 56.64 42.02 19.98 81.51 28.60 79.07 49.83 60.49 37.84 48.27 50.04 57.41 41.92 20.25 82.20 29.31 80.03 60.87 37.50 48.46 49.65 57.31 42.28 20.19 84.79 28.70 79. 50.78 50.93 Table 15: In-Context Format Comparing variants of in-context example format on E5-MistralInstruct. Instruct refers to the baseline which does not use any in-context examples. Instruct RARe Dataset - Query-Only Doc-Only Shuffle-NC Shuffle-C Regular 61.19 ArguAna ClimateFEVER 39.03 44.82 CQADupStack 48.92 DBPedia 57.39 FiQA2018 40.99 NFCorpus 17.94 SCIDOCS 77.28 SciFact 29.35 Touche2020 72.89 TRECCOVID Average 48.98 58.88 36.21 46.66 49.98 54.44 41.42 20.04 78.84 28.09 79. 49.41 57.54 35.59 48.28 49.08 56.02 41.62 20.12 79.80 29.01 83.29 50.04 60.17 30.83 45.78 50.93 54.25 41.17 20.35 80.70 29.18 82.14 49.55 58.97 35.71 47.97 50.24 55.98 41.78 20.11 80.51 28.97 82. 60.87 37.50 48.46 49.65 57.31 42.28 20.19 84.79 28.70 79.58 50.32 50.93 Table 16: Performance (nDCG@10) on datasets from the BeIR benchmark Thakur et al., 2021 when training decoder-only model (Llama3). Applying RARe with only in-context examples can lead to degradation of performance in the zero-shot setting (qinst), but this is easily mitigated my including mixture of qinst and qinst+ic data (30% and 70%) respectively. Training Eval NQ Quora NFCorpus SciFact SCIDOCS FiQA2018 CQA Average RepLLaMA-qinst qinst 62.70 88.34 37.73 RARe-qinst+ic RARe-qinst + qinst+ic qinst 39.64 88.39 qinst+ic 65.19 86.79 qinst 63.68 87.84 qinst+ic 65.96 87. 35.42 38.87 38.06 39.34 75.02 74.52 78.41 76.07 77.20 19.66 21.04 19.70 20.11 19.45 44. 42.59 52.91 30.44 46.58 46.02 47.13 37.74 43.75 42.99 42.96 46.74 54.18 53.54 54."
        }
    ],
    "affiliations": [
        "New York University",
        "Seoul National University",
        "The University of Texas at Austin"
    ]
}