{
    "paper_title": "PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos",
    "authors": [
        "Meng Cao",
        "Haoran Tang",
        "Haoze Zhao",
        "Hangyu Guo",
        "Jiaheng Liu",
        "Ge Zhang",
        "Ruyang Liu",
        "Qiang Sun",
        "Ian Reid",
        "Xiaodan Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 0 0 8 1 0 . 2 1 4 2 : r PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos Meng Cao1, Haoran Tang2*, Haoze Zhao1*, Hangyu Guo3, Jiaheng Liu3, Ge Zhang4, Ruyang Liu2, Qiang Sun1,5, Ian Reid1, Xiaodan Liang1,6 1Mohamed bin Zayed University of Artificial Intelligence 2Peking University 3Alibaba Group 4University of Waterloo 5University of Toronto 6Sun Yat-sen University *Authors contributed equally to this research. Corresponding author. https://github.com/PhysGame/PhysGame Figure 1. Left: Comparisons of physical commonsense understanding capability. Our PhysVLM identifies that motorcycle colliding and flipping car is implausible while GPT-4o [92] and LLaVA-Next-Video [72] fail to accurately interpret the physical commonsense violations in the video; Right: The taxonomy of PhysGame benchmark including 4 primary categories and 12 fine-grained sub-categories."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM. 1. Introduction Large Language Models (LLMs) [1, 12, 103, 117, 118] have achieved considerable success in understanding user instructions and delivering contextually relevant responses. Building on this foundation, video LLMs [54, 55, 62, 67, 1 68, 70, 76, 77, 83, 88, 141] have emerged as fundamental video intelligence system by incorporating LLMs with perception and reasoning capabilities of dynamic scenes. Across the range of video types, gameplay videos [25, 71, 90, 101, 102, 108, 109, 123, 144] present unique challenge due to their highly dynamic and interactive environments, where agents and objects unfold in complex temporal scenes. Unlike real-world videos, gameplay videos frequently contain inconsistencies between the intended physics and on-screen behavior due to software bugs [71, 109]. Typically, video glitches1 encompass broad spectrum of physical phenomena, thus naturally serving as benchmark for physical commonsense understanding. Humans inherently develop an intuitive comprehension of the physical world through our experiences, enabling us to easily recognize violations of physical commonsense, even in the absence of formal physics education. For instance in Figure 1 (left), we can predict that the car will not be launched into the air after colliding with the motorcycle. In spite of the progress, it remains unclear how adept video LLMs are at recognizing physical commonsense violations in gameplay videos. Although various benchmarks [33, 38, 64, 79, 84, 121, 125] have been introduced to evaluate the fundamental capabilities of video LLMs, the community still lacks comprehensive evaluation standards for assessing video-based physical commonsense reasoning (cf . Table 1). To bridge this gap, we propose PhysGame, pioneering benchmark to uncover Physical commonsense violations in Gameplay videos. We focus on the intuitive adherence to the physical commonsense instead of complex physical formulas requiring explicit domain knowledge. The PhysGame benchmark consists of 880 gameplay videos containing glitches, each annotated with high-quality multiple-choice question specifically addressing the nature of the glitch. As illustrated in Figure 1 (right), PhysGame spans four key physical domains (i.e., mechanics, kinematics, optics, and material properties), and encompasses 12 fine-grained categories (e.g., gravity and velocity). The video lengths range from 2.63 seconds to 239.57 seconds, covering both short clips and videos requiring long-context reasoning. Based on the constructed PhysGame benchmark, we provide comprehensive analysis of state-of-the-art proprietary LLMs, e.g., GPT-4o [92] and Gemini-1.5-pro [103], and open-source video LLMs including LLaVA-Next [72] and Video-LLaVA [68]. Our preliminary experiments on the PhysGame benchmark reveal considerable constraints in the physical commonsense understanding capabilities of existing MLLMs: 1) Existing video LLMs exhibit limited performance on the PhysGame benchmark; 2) Open-source models tend to underperform significantly compared to pro1We employ the term glitch to describe the phenomenon of physical commonsense violation in gameplay videos. prietary counterparts, possibly due to the absence of suitable instruction-tuning dataset for physical commonsense reasoning. To this end, we additionally introduce the PhysInstruct dataset to facilitate Physical understanding oriented Instruction tuning [93]. We curate PhysInstruct by prompting GPT-4o [92] with gameplay videos and video-wise meta information (i.e., video titles), which typically summarize key content and provide valuable hints. According to statistics, PhysInstruct consists of 140,057 instruction-following pairs regarding video glitch content. Besides, we construct the PhysDPO dataset for Physical Direct Preference Optimization [100, 142]. The preferred responses are taken from PhysInstruct while the dis-preferred responses are generated by prompting GPT-4o with misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower resolutions (i.e., spatial hacking). Under the two-stage successive training with PhysInstruct and PhysDPO datasets, we train Physical knowledge enhanced large Video Language Model (PhysVLM for short). PhysVLM achieves state-of-the-art performance on the PhysGame benchmark, demonstrating its advanced capability in physical commonsense understanding. Notably, it also demonstrates impressive results on generalpurpose video understanding benchmarks [38, 83]. For example, PhysVLM attains an overall accuracy of 61.1% on Video-MME [38] with the use of subtitles and an average score of 3.83 on VCG benchmark [83]. We emphasize that our focus of PhysVLM is not on introducing novel architectural designs or training strategies. Instead, we aim to integrate the existing architecture and datasets to establish solid baseline with strong performance on both physical commonsense and general video understanding datasets (cf . Sec.4.2 and Sec.4.3). We envision that PhysVLM designed in the simple yet effective principle will offer insights for advancing future research efforts in video LLMs. In summary, our contributions are in three-folds: Physical commonsense benchmark. The PhysGame benchmark is collected to uncover physical commonsense violations in gameplay videos. Instruction-following and preference-tuning datasets. We introduce the PhysInstruct dataset to facilitate the supervised fine-tuning and the PhysDPO dataset via the meta-information, temporal, and spatial hacking for direct preference optimization. Physical knowledge enhanced Video LLM. We propose PhysVLM, which not only demonstrates state-of-the-art performance on PhysGame but also exhibits leading performance on general video understanding benchmarks. 2 Table 1. Comparison with existing benchmarks for video LLMs in terms of the video number (#Videos), the average video duration (Len.), the number of QA pair (#QA Pairs), the average QA pair tokens (QA Tokens), the manually/automatic annotation manner (M/A), whether the benchmarks are gameplay video based (Game-Bsd), whether the questions are physical commonsense classified (Phys-Clsf), and whether the benchmarks contain meta information (Meta-info). Benchmarks #Videos Len.(s) #QA Pairs QA Tokens Anno. Game-Bsd Phys-Clsf Meta-info MSRVTT-QA [129] MSVD-QA [129] TGIF-QA [51] ActivityNet-QA [137] TVQA [56] How2QA [65] STAR [124] NExT-QA [128] MVBench [64] Video-Bench [91] EgoSchema [84] AutoEval-Video [27] TempCompass [79] Video-MME [38] LVBench [121] LongVideoBench [125] PhysGame (Ours) 2,990 504 9,575 800 2,179 1,166 914 1,000 3,641 5,917 5,063 327 410 900 103 3,763 880 15.2 9.8 3.0 111.4 11.2 15.3 11.9 39.5 16.0 56.0 180.0 14.6 11.4 1017.9 4,101 473.0 25. 72,821 13,157 8,506 8,000 15,253 2,852 7,098 8,564 4,000 17,036 5,063 327 7,540 2,700 1,549 6,678 880 8.4 7.6 20.5 10.2 27.8 16.9 19.5 25.3 27.3 21.3 126.8 11.9 49.2 35.7 32.0 84.1 66.9 A&M A A&M A&M A&M A&M Table 2. Comparison with existing gameplay video benchmarks in terms of whether they are video-based (Vid-Bsd), whether they follow an instructional format (Instruct), and support multi-modal evaluations (MModal). Benchmarks GameBunny [107] Taesiri et.al [109] GameBugDescript [110] GlitchBench [111] PhysGame (Ours) Vid-Bsd Instruct MModal 2. Related Work Benchmarks for Video LMMs. Integrating visual, temporal, and linguistic inputs, Video LMMs opening doors to wide range of applications including video understanding [14, 17, 138], editing [15, 31], healthcare [74, 134], etc. Primarily, video LLMs have been evaluated on classical video question-answering (QA) benchmarks [51, 56, 65, 124, 129, 137], e.g., MSVD-QA [129], MSRVTT-QA [129], and ActivityNet-QA [137]. Since these benchmarks can often be addressed using sparse set of frames [125], recent research [64, 66, 80, 83, 95, 128] focuses on the assessment of temporal dynamics in videos. Video-ChatGPT [83] introduces video-based generative performance benchmark, which augments videos from ActivityNet-QA [137] with dense descriptive captions and human annotated questionanswer pairs. MVBench [64] emphasizes temporally sensitive videos and encompasses wide range of temporal tasks by automatically converting public annotations into multiple-choice QA formats. Recently, there has been remarkable research interest in advancing long-form video understanding. EgoSchema [84] targets 3-minute-long egocentric videos, while MovieChat1K [106] specializes in 10-minute-long movie videos. Recent pre-prints [33, 38, 121, 125], including Video-MME [38], LVBench [121], and LongVideoBench [125], have extended the scope of long video understanding to consider more general themes and intricate reasoning tasks. Despite of the progress, few of existing benchmarks (cf . Table 1) evaluate the physical commonsense reasoning capability in video LLMs, which acts as critical step towards human-like video comprehension. In this paper, we bridge this gap by introducing suite of datasets: PhysInstruct for supervised fine-tuning, PhysDPO for preference alignment, and PhysGame for evaluation. Gameplay Video Understanding. Digital games [47, 130] are considered pivotal in pursuing artificial general intelligence, as they act as controllable real-world simulators and create complex problem-solving contexts. Therefore, gameplay videos are typically employed as benchmarks for evaluating the capabilities of vision-language models from the perspectives of environment perception [3, 46], context reasoning [75, 120], decision-making [26, 98], etc. The majority of existing games can be classified into two categories: 1) Competition games [35, 43, 44, 48, 49, 63, 82, 105, 115, 143] in which players compete against one another, with the objective of outperforming others to achieve victory. Notable examples include StarCraft II [82, 105], Pokemon Battles [48], Chess [35, 63, 115] and Poker [43, 44, 49, 143]. Ma et al. [82] introduce TextStarCraft II, natural language-based interface that equips LLMs with the functionality to play StarCraft II, fostering more effective reasoning and decision-making capabilities; 2) Cooperation games [22, 24, 39, 40, 42, 96, 97, 126] are structured around collaboration, requiring players to work together to achieve shared goals. These games emphasize teamwork, communication, and joint problem-solving, where players must coordinate efforts to succeed and reach mutual accomplishments. In Overcooked-AI [22], the preparation of an onion soup requires two agents to collaborate by loading three onions into cooking pot, thereby initiating cooking process that spans 20-time steps. The most relevant line of research to ours is on the topic of game bug detection [107, 109112]. Existing methods, however, either focus the classification/retrieval tasks [109] or limited in the static image domain [107, 111] (cf . Table 2). The prior work [110] employs LLMs to detect bugs in game videos with the reliance on pre-extracted event-wise textual descriptions and lacks support for multi-modal evaluations. In contrast, our PhysVLM addresses all these limitations and supports multi-modal instruction evaluations in videos. Physical Commonsense Understanding. Even before language acquisition, children start to grasp fundamental physical commonsense by observing the properties of the world around them [45]. However, acquiring such physical commonsense knowledge remains major challenge for artificial intelligence systems. The topic of physical commonsense understanding has seen significant attention across range of fields, e.g., visual physical reasoning [57], video generation [7, 86], and robotics [2, 13]. collection of works [2, 5, 9, 10, 23, 36, 37, 57, 87, 135] has centered on learning physical and causal reasoning through synthetic dynamic scenarios, either from video frames [8, 11, 28, 34, 36, 37, 41, 57, 87, 119, 122, 136], or from the symbolic environment representations [9, 23]. Within the field of video generation, recent efforts [7, 86] are increasingly exploring whether generative models demonstrate an understanding of intuitive physical commonsense. In robotics, learning from intuitive physics has been demonstrated effective in visuomotor planning [13], tool usage [116], and construction [89]. This paper investigates video LLMs physical commonsense reasoning skills and verifies their significance for enhancing general video understanding. 3. Dataset & Method This section details the design of the evaluation benchmark PhysGame (Sec. 3.1), supervised fine-tuning (SFT) dataset PhysInstruct (Sec. 3.2), and the direct preference optimization (DPO) dataset PhysDPO (Sec. 3.3). The optimization procedure for PhysVLM is discussed in Section 3.4. Figure 2. The annotated multi-choice question in PhysGame. The correct option is annotated in green. Table 3. The average tokens of four options in the annotations of PhysGame benchmark. Opt. Opt. Opt. Opt. Avg. tokens 14.40 14.49 14. 14.47 3.1. PhysGame Benchmark Building on the intuitive cognition of physical commonsense, PhysGame benchmark introduces comprehensive taxonomy for task categorization including four primary domains, i.e., mechanics, kinematics, optics, and material properties, and 12 fine-grained categories (cf . Figure 1). Mechanics: This category deals with forces and torques as well as their effects on motion, which provides the foundational principles to interpret and analyze the motion of objects in videos. Typical cases include gravity, elasticity, and friction. Kinematics: This domain studies motion without considering forces, which involves fine-grained categories including velocity and acceleration over time. Optics: It focuses on the behavior and properties of light as well as its interactions with matter. It includes reflection, refraction, and absorption & transmission. Material properties: It refers to the inherent material characteristics including color, rigidity, object shape, and human body gesture. Video Collection and Filter. The videos in PhysGame are mainly crawled from the reddit page2 which contains gameplay videos with unusual events and glitches. To balance different categories, we also augment videos from YouTube via keyword searching. We conduct manual checks based on the following two criteria: 1) Duplicate check: The Reddit discussion forum may feature multiple references to the same video, resulting in duplicate downloading. We manually check to confirm that each video in PhysGame is distinct; 2) Content check: The pool of downloaded videos may incorporate non-game elements, which we rigorously filter out of our PhysGame benchmark. Annotation Scheme. Based on the collected gameplay 2www.reddit.com/r/GamePhysics/ 4 Figure 3. Overview of the direct preference optimization training, where the preferred data is generated with the guidance of associated meta-information (i.e., title) while dispreferred data is generated with misleading titles (i.e., meta-information hacking), fewer frames (i.e., temporal hacking) and lower resolutions (i.e., spatial hacking). videos, we create the question-answer pairs in four-way multiple-choice format to facilitate convenient evaluation. Specifically, the correct options describe the video-specific glitches that contravene physical commonsense principles. It is important to note that some videos may exhibit multiple glitches. We therefore instruct expert annotators to review the entire video to ensure all the appearing glitches are included in the correct answer. To enhance the plausibility of the distractor options, we have provided expert annotators with three guiding principles: 1) Instead of imagining arbitrary glitches, the glitch in the distractor options should be highly correlated to the individuals or actions observed in the videos. For example in Figure 2, the distractor option includes car and shadow that are genuinely present in the video. This annotation principle forces video LLMs to comprehend the glitchy content, rather than merely selecting answers by identifying contained objects or actions; 2) The four choice options should be of similar length, which helps prevent any preference biases in video LLMs. From Table 3, it can be observed that all four options exhibit comparable token numbers; 3) To mitigate choice bias, the distribution of the correct option among the four choices should be equitable, (i.e., 25% likelihood for each option). Quality Control. To guarantee the quality of our dataset, we conduct two-fold quality control process including human inspection and automatic LLM-assisted inspection: 1) All the initially annotated question-answering pairs undergo rigorous cross-inspection by different human annotators. For the correct options, the inspectors must assess whether they comprehensively and accurately describe all instances of physical commonsense violations present. For the distractor options, the inspectors are required to evaluate whether they are sufficiently deceptive, specifically by including objects or actions depicted in the video; 2) We exclude question-answering pairs that can be correctly answered by GPT-4o [92] solely based on the question and options without the need to view the video. By statistics, we limit the accuracy of GPT-4o in the question-only scenario to less than 25%. Through the rigorous construction and review process, we present the PhysGame benchmark which is of high quality and well-balanced. The specific statistics are available in Table 1. 3.2. PhysInstruct Dataset To improve the physical commonsense understanding ability of video LLMs, we develop the PhysInstruct dataset for supervised fine-tuning. The video collection procedure follows the same process as that in PhysGame. To prevent data leakage, we diligently exclude any videos included in PhysGame. We follow the self-instruction paradigm [58] to construct PhysInstruct by prompting GPT-4o [92]. In terms of the instruction generation3, we aspire for the questions to be as diverse as possible. We set up three question types, varying from direct to indirect assessment of the glitches in videos: i) explicitly inquiring about glitches in 3Refer to the supplementary material for detailed prompts. 5 the PhysDPO dataset with 34,358 training pairs. 3.4. PhysVLM Through our experiments in Sec.4.2, we observe that the capabilities of current open-source models are markedly inferior to those of proprietary models. To establish strong open-source baseline for physical commonsense understanding, we propose PhysVLM by employing cuttingedge architecture and high-quality training datasets. Architecture. For the architecture design, we primarily adopt PPLLaVA [78], solely substituting the Vicuna-7B [30] LLM with the high-performing Qwen2-7B [131]. Supervised Fine-tuning. We introduce the hybrid training datasets that include our physics-oriented datasets alongside general image and video datasets. Following LLaVANeXT Interleave [59], we utilize combined dataset comprising 300K images randomly sampled from the LLaVA1.5 [73] training dataset, 300K video samples in LLavaHound [142], and our PhysInstruct dataset, totaling around 740k training samples. We apply the conventional autoregressive loss in this stage. Direct Preference Optimization. We use the preferred and dis-preferred data in both LLaVA-Hound-DPO-17k [142] and our self-constructed PhysDPO in this stage, leading to total of 51k training samples. The training loss follows the standard methodology [100, 142]. 4. Experiments 4.1. Experimental Settings Implementation Details. For SFT training, the pooling kernel and strides are set to (1, 3, 3) for image inputs and (2, 3, 3) for video inputs. PhysVLM lasts for one epoch with batch size of 256 and learning rate of 2e-5. The input frame is set to 32 following PPLLaVA [78]. For DPO training, the pooling kernel and strides are set to (1, 3, 3), and the input frame is set to 16. The DPO training lasts two epochs with batch size of 64 and learning rate of 5e6. All experiments are conducted on 8 NVIDIA A100 GPUs. Figure 4. Example cases in the PhysInstruct dataset with (w/ ) or without (w/o) meta-information hints. the video, ii) probing anomalies present in the video, iii) or merely straightforward questions regarding the video content. We adopt in-context learning [32] with three examples corresponding to the mentioned three scenarios: i) What is the description of the glitch observed in the video? ii) Are there any abnormalities present in the videos? iii) Please provide description of the video content. As for the response generation3, the preliminary experiments suggest that the intuitive prompting method leads to remarkable errors, regardless of how we adjust the prompt contents. To alleviate this, we propose meta-information guided prompting strategy. Specifically, we found that the meta-information (e.g., title) associated with each video offers insight into the fundamental content. For example in Figure 4, the title indicates the glitch concerning leg muscle. Therefore, we propose to incorporate video-wise meta-information in the prompt, resulting in more accurate instruction-following generation (cf . Figure 4). In Figure 4, the meta-information helps GPT-4o to detect the leg gesture glitch, while its absence leads to the degradation of physical commonsense understanding. In total, we generate 140,057 instruction-following pairs. 3.3. PhysDPO Dataset 4.2. Evaluations on PhysGame We construct the preference alignment dataset PhysDPO to deliver more trustworthy and reliable responses. As shown in Figure 3, we regard the generated answers in the PhysInstruct dataset as the preferred responses and the dispreferred responses are generated via the combination of meta-information hacking, temporal hacking, and spatial hacking. We prompt GPT-4o with the misleading metainformation as well as video frames with the reduced frame count and decreased frame resolution3. Specifically, the dispreferred data are generated based on one single frame with both height and width reduced to 1/16 of the original dimensions. Through the above generation process, we compile Evaluation Settings. We benchmark PhysGame on 8 proprietary multi-modal LLMs, i.e., Claude3.5-Sonnet [4], Claude3.5-SonnetV2 [4], Gemini-1.5-pro [114], Gemini1.5-pro-flash [114], GPT-4V [1], GPT-4o-0806 [92], GPT4o-mini-0718 [92] and Qwen-VL-max [6], as well as 8 open-source models including LLaVA-Next-Video [72], Video-LLaVA [68], LLaVA-OneVision [58], InternVL2 [29], VideoChat2 [64], ST-LLM [77], Chat-UniVi [54] and PPLLaVA[78]. We follow Video-MME [38] to utilize the official frame configurations provided for each video LLM. We employ accuracy as the evaluation metric for our curated multi-choice questions. The evaluation prompt is 6 Table 4. Evaluation results (%) of open-source and proprietary multi-modal LLMs on PhysGame. The fine-grained categories include gravity, elasticity, friction, velocity, acceleration, reflection, refraction, absorption & transmission, color, rigidity, object shape, and body gesture. AVG denotes the average accuracy. PhysVLM-SFT denotes PhysVLM only undergoes supervised fine-tuning while PhysVLM-DPO denotes PhysVLM with consecutive supervised fine-tuning and direct preference optimization. Models AVG Mechanics Kinematics Optics Material Grav. Elast. Fric. Velo. Acc. Refl. Refr. Abs. Col. Rig. Sha. Gest. Claude3.5-Sonnet Claude3.5-SonnetV2 Gemini-1.5-pro Gemini-1.5-pro-flash GPT-4V GPT-4o-0806 GPT-4o-mini-0718 Qwen-VL-max LLaVA-Next-Video Video-LLaVA LLaVA-OneVision InternVL2 VideoChat2 ST-LLM Chat-UniVi PPLLaVA PhysVLM-SFT PhysVLM-DPO [4] [4] [114] [114] [1] [92] [92] [6] [72] [68] [58] [29] [64] [77] [54] [78] 54.3 47.6 55.2 48.5 45.9 56.1 40.3 50.9 32.2 29.0 47.7 33.4 34.3 32.8 29.5 38.4 56.7 59.5 50.7 46.5 50.7 47.9 40.8 47.9 43.7 50.7 43.7 32.4 50.7 29.6 33.8 32.4 28.2 45.1 54.9 64.8 Proprietary Multi-modal LLMs 53.2 37.2 51.1 43.6 34.0 43.6 35.1 31.9 59.1 53.4 59.1 51.1 48.9 61.4 44.3 46. 50.6 46.6 48.9 51.7 48.3 59.1 39.2 51.1 58.8 52.5 70.0 52.5 60.0 61.3 43.8 53.8 50.0 47.8 50.0 43.5 43.5 43.5 30.4 50.0 Open-source Multi-modal LLMs 34.0 33.8 31.9 22.5 39.4 50.0 35.1 31.2 41.5 35.0 37.2 26.2 39.4 27.5 30.9 38.8 51.1 62.5 59.6 66.3 27.3 27.8 46.0 38.6 29.5 26.7 29.5 42.6 60.2 60.2 22.7 26.1 45.5 30.7 28.4 28.4 23.9 30.7 63.6 60. 21.7 19.6 43.5 30.4 28.3 37.0 28.3 41.3 45.7 39.1 50.0 50.0 42.9 53.6 46.4 53.6 46.4 60.7 35.7 35.7 71.4 53.6 32.1 25.0 32.1 39.3 57.1 67.9 49.2 33.9 52.5 33.9 42.4 50.8 42.4 50.8 23.7 32.2 40.7 35.6 33.9 28.8 30.5 35.6 28.8 35.6 64.4 55.6 71.1 64.4 53.3 68.9 44.4 64. 35.6 31.1 55.6 26.7 33.3 33.3 31.1 44.4 64.4 57.8 52.7 54.1 56.8 43.2 45.9 54.1 37.8 48.6 41.9 36.5 44.6 29.7 41.9 40.5 18.9 39.2 51.4 62.2 50.0 43.8 53.1 46.9 37.5 65.6 37.5 65.6 34.4 28.1 56.2 18.8 21.9 37.5 28.1 18.8 50.0 37.5 62.1 51.7 58.6 49.4 44.8 63.2 41.4 59. 37.9 27.6 52.9 34.5 44.8 46.0 35.6 43.7 72.4 78.2 Table 5. Evaluation results (%) on Video-MME. w/ subs and w/o subs respectively denote with subtitles and without subtitles. Models InternVL-Chat-V1.5 LLaVA-NeXT-Video VILA-1.5 LLaVA-OneVision Qwen-VL-Chat Video-LLaVA ST-LLM VideoChat2-Mistral Chat-UniVi-V1.5 LLaVA-NeXT-Video PPLLaVA PhysVLM-SFT PhysVLM-DPO [29] [72] [69] [58] [6] [68] [76] [64] [54] [72] [78] LLM Params Short (%) Medium (%) Long (%) Overall (%) w/o subs w/ subs w/o subs w/ subs w/o subs w/ subs w/o subs w/ subs 20B 34B 34B 72B 7B 7B 7B 7B 7B 7B 7B 7B 7B 60.2 61.7 68.1 76.7 46.9 45.3 45.7 48.3 45.7 45.9 58.7 64.1 66.1 61.7 65.1 68.9 79.3 47.3 46.1 48.4 52.8 51.2 49.8 62.8 68.0 70.0 46.4 50.1 58.1 62.2 38.7 38.0 36.8 37.0 40.3 40.3 45.6 55.0 54.3 49.1 52.2 57.4 66.9 40.4 40.7 41.4 39.4 44.6 44.3 50.4 61.7 59.6 45.6 44.3 50.8 60.0 37.8 36.2 31.3 33.2 35.8 36.6 42.2 46.4 47.1 46.6 47.2 52.0 62.4 37.9 38.1 36.9 39.2 41.8 41.0 47.4 50.3 53. 50.7 52.0 59.0 66.3 41.1 39.9 37.9 39.5 40.6 40.9 48.8 55.2 55.8 52.4 54.9 59.4 69.6 41.9 41.6 42.3 43.8 45.9 45.0 53.6 60.0 61.1 available in the supplementary material. Performance Analysis. The evaluation results on the PhysGame benchmark are demonstrated in Table 4. Among all proprietary models, GPT-4o and Gemini-1.5-pro demonstrate the best performance, achieving average accuracy scores of 56.1% and 55.2%, respectively. Across all the fine-grained domains, GPT-4o achieves superior performance in friction and acceleration. In contrast, Gemini-1.5pro shows stronger capability in understanding physical commonsense related to gravity, elasticity, reflection, absorption & transmission, color, and rigidity. Furthermore, existing open-source models fall significantly behind proprietary models. Even the best-performing reaches only open-source model, LLaVA-OneVision, 47.7% average accuracy. In comparison, our proposed PhysVLM achieved state-of-the-art performance among all proprietary and open-source models. Compared to opensource methods, our PhysVLM attains the highest performance in 6 domains out of the total 12 evaluated domains. Notably, PhysVLM-DPO surpasses the best-performing Table 6. Evaluation results on VCG benchmark [83]. Methods marked by use DPO or PPO [104]. CI, DO, CU, TU, and CO respectively denote correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency. AVG is the average result. Methods PhysVLM-DPO w/o temporal hacking w/o spatial hacking w/o meta-info hacking AVG 59.5 57.6 57.3 57.4 Table 7. Ablation studies of the temporal, spatial, and meta-info hacking in the PhysDPO dataset generation process. Methods CI DO CU TU CO AVG VideoChat Video-ChatGPT BT-Adapter Chat-UniVi VideoChat2 LLaMA-VID ST-LLM PLLaVA LLaVA-Next-Video PPLLaVA PhysVLM-SFT LLaVA-Next-Video PPLLaVA PhysVLM-DPO 2.23 2.50 2.68 2.89 3.02 2.96 3.23 3.21 3.39 3.32 3.59 3.64 3.85 3.89 2.50 2.57 2.69 2.91 2.88 3.00 3.05 2.86 3.29 3.20 3.07 3.45 3.56 3.69 2.53 2.69 3.27 3.46 3.51 3.53 3.74 3.62 3.92 3.88 3.89 4.17 4.21 4. 1.94 2.16 2.34 2.89 2.66 2.46 2.93 2.33 2.60 3.00 2.74 2.95 3.21 3.11 2.24 2.20 2.46 2.81 2.81 2.51 2.81 2.93 3.12 3.20 3.44 4.08 3.81 4.19 2.29 2.42 2.69 2.99 2.98 2.89 3.15 2.99 3.26 3.32 3.35 3.66 3.73 3. proprietary model GPT-4o by an absolute margin of 3.4% in the metric of average accuracy. 4.3. Evaluations of General Video Understanding To further demonstrate the generalizability of our model, we conducted experiments on general video LLM benchmarks including Video-MME [38] and VCG benchmarks [83]. We follow the common practice by using GPT-3.5-turbo0613 version for the evaluation of the VCG benchmark. Video-MME is in the format of multichoice questions and thus the evaluation is more objective by eliminating the reliance on GPT. The comparison results on the Video-MME benchmark are demonstrated in Table 5. Our PhysVLM achieves superior performance among all the 7-B models. Surprisingly, as the 7B model, both PhysVLM-SFT and PhysVLM-DPO outperform the 34B model LLaVA-NeXT-Video by 3.2% and 3.8% absolute improvements on the overall performance without using subtitles. By comparing PhysVLMSFT and PhysVLM-DPO, we find that DPO training using the proposed PhysDPO data results in performance gains on both short and long videos, while performance on mediumlength videos experiences slight decline. We summarize the results on the VCG benchmark in Table 6. In the case of models using only SFT, our PhysVLMSFT achieves the best performance in terms of the average score. In the evaluation across four subcategories, PhysVLM-SFT performs exceptionally well in the correctness of information and consistency categories. Compared to PPLLaVA and LLaVA-Next-Video which use DPO or PPO training, our PhysVLM-DPO also demonstrates superior performance, further validating the outstanding capaTable 8. Ablations of training data in SFT and DPO stages. AVG denotes the average accuracy on the PhysGame benchmark. Stage Training Data SFT SFT SFT LLava-Hound LLava-Hound[142], LLaVA-Image [73] LLava-Hound, LLaVA-Image, PhysInstruct DPO LLava-Hound-DPO [142] DPO LLava-Hound-DPO, PhysDPO AVG 40.7 46.0 56. 52.9 59.5 bilities of the proposed PhysVLM model in general video understanding. 4.4. Ablations Ablations of DPO Dataset Generation. In Section 3.3, the dis-preferred responses in PhysDPO are generated by prompting GPT-4o with misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). We ablate on these three kinds of hacking in Table 7. We found that removing any one of the three components leads to decline in overall performance. For instance, omitting temporal hacking results in 1.9% decrease in the final PhysVLM performance on the AVG metric. Ablations of Training Data. We use the hybrid training dataset in both the SFT and DPO training stages. Here we ablate on each dataset to investigate the specific contributions. We report the average accuracy on the PhysGame benchmark in Table 8. The comparison results demonstrate that introducing PhysInstruct and PhysDPO respectively leads to 10.7% and 6.6% performance boosts, further validating the effectiveness of the curated datasets. More ablation results on Video-MME and VCG benchmarks are available in the supplementary material. 5. Conclusion This paper investigates current video LLMs understanding of physical commonsense in gameplay videos. To achieve this, we introduce PhysGame benchmark, consisting of glitchy gameplay videos accompanied by annotated question-answer pairs to identify and analyze physical commonsense violations. The extensive experiments reveal that the performance of open-source models falls significantly behind that of proprietary counterparts. To this end, we propose PhysVLM as an open-source physical-knowledgeenhanced video LLM. To facilitate training, we curate 8 suite of datasets including PhysInstruct for instruction tuning and PhysDPO for preference alignment. Experiments manifest that PhysVLM achieves state-of-the-art performance on both physical-oriented benchmark PhysGame and general video understanding benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 6, 7 [2] Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. Advances in neural information processing systems, 29, 2016. 4 [3] Nader Akoury, Qian Yang, and Mohit Iyyer. framework for exploring player perceptions of llm-generated dialogue in commercial video games. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2295 2311, 2023. 3 [4] Anthropic. Claude 3.5 sonnet, 2024. 6, 7 [5] Tayfun Ates, Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe Goksun, and Deniz Yuret. Craft: benchmark for causal arXiv preprint reasoning about forces and interactions. arXiv:2012.04293, 2020. 4 [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 6, 7 [7] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. [8] Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics. In International Conference on Learning Representations, 2021. 4 [9] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. Advances in neural information processing systems, 29, 2016. 4 [10] Peter Battaglia, Jessica Hamrick, and Joshua Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332, 2013. 4 [11] Daniel Bear, Elias Wang, Damian Mrowca, Felix Jedidja Binder, Hsiao-Yu Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and In Thirty-fifth Conference on Neural Informamachines. tion Processing Systems Datasets and Benchmarks Track (Round 1), 2021. 4 9 [12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [13] Arunkumar Byravan, Felix Leeb, Franziska Meier, and Dieter Fox. Se3-pose-nets: Structured deep dynamics models for visuomotor planning and control. arXiv preprint arXiv:1710.00489, 2017. 4 [14] Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, and Yuexian Zou. On pursuit of designing multi-modal transIn Proceedings of the 2021 former for video grounding. Conference on Empirical Methods in Natural Language Processing, pages 98109823, 2021. 3 [15] Meng Cao, Haozhi Huang, Hao Wang, Xuan Wang, Li Shen, Sheng Wang, Linchao Bao, Zhifeng Li, and Jiebo Luo. Unifacegan: unified framework for temporally conIEEE Transactions on Image sistent facial video editing. Processing, 30:61076116, 2021. 3 [16] Meng Cao, Ji Jiang, Long Chen, and Yuexian Zou. Correspondence matters for video referring expression compreIn Proceedings of the 30th ACM International hension. Conference on Multimedia, pages 49674976, 2022. [17] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang, and Yuexian Zou. Locvtp: Video-text pre-training for temporal localization. In European Conference on Computer Vision, pages 3856. Springer, 2022. 3 [18] Meng Cao, Can Zhang, Long Chen, Mike Zheng Shou, and Yuexian Zou. Deep motion prior for weakly-supervised temporal action localization. IEEE Transactions on Image Processing, 31:52035213, 2022. [19] Meng Cao, Fangyun Wei, Can Xu, Xiubo Geng, Long Chen, Can Zhang, Yuexian Zou, Tao Shen, and Daxin Jiang. Iterative proposal refinement for weakly-supervised video grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65246534, 2023. [20] Meng Cao, Yuyang Liu, Yingfei Liu, Tiancai Wang, Jiahua Dong, Henghui Ding, Xiangyu Zhang, Ian Reid, and Xiaodan Liang. Continual llava: Continual instruction arXiv preprint tuning in large vision-language models. arXiv:2411.02564, 2024. [21] Meng Cao, Haoran Tang, Jinfa Huang, Peng Jin, Can Zhang, Ruyang Liu, Long Chen, Xiaodan Liang, Li Yuan, and Ge Li. Rap: Efficient text-video retrieval with sparseand-correlated adapter. arXiv preprint arXiv:2405.19465, 2024. [22] Micah Carroll, Rohin Shah, Mark Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32, 2019. 4 [23] Michael Chang, Tomer Ullman, Antonio Torralba, and Joshua Tenenbaum. compositional object-based aparXiv preprint proach to learning physical dynamics. arXiv:1612.00341, 2016. 4 [24] Jiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: Self-organizing agents in open-ended environments. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. 4 [25] Ke Chen, Yufei Li, Yingfeng Chen, Changjie Fan, Zhipeng towards automated test oracle Hu, and Wei Yang. Glib: In Proceedings of the for graphically-rich applications. 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 10931104, 2021. [26] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multiagent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2023. 3 [27] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video arXiv preprint arXiv:2311.14906, question answering. 2023. 3 [28] Zhenfang Chen, Shilong Dong, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua Tenenbaum, and Chuang Gan. Compositional physical reasoning of objects and events from videos. arXiv preprint arXiv:2408.02687, 2024. 4 [29] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. arXiv preprint arXiv:2404.16821, 2024. 6, 7 [30] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 6, 15, 16 [31] Jiahua Dong, Wenqi Liang, Hongliu Li, Duzhen Zhang, Meng Cao, Henghui Ding, Salman Khan, and Fahad Shahbaz Khan. How to continually adapt text-to-image diffuarXiv preprint sion models for flexible customization? arXiv:2410.17594, 2024. [32] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. 6 [33] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, and Ji-Rong Wen. Towards event-oriented long video understanding. arXiv preprint arXiv:2406.14129, 2024. 2, 3 [34] Frederik Ebert, Chelsea Finn, Alex Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. CoRL, 12(16):23, 2017. 4 [35] Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. Chessgpt: Bridging policy learning and language modeling. Advances in Neural Information Processing Systems, 36, 2024. 3 [36] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. Advances in neural information processing systems, 29, 2016. 4 [37] Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, Learning visual predictive modarXiv preprint and Jitendra Malik. els of physics for playing billiards. arXiv:1511.07404, 2015. [38] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 3, 6, 8 [39] Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, et al. Threedworld: platform for interactive multi-modal physIn Thirty-fifth Conference on Neural Inical simulation. formation Processing Systems Datasets and Benchmarks Track (Round 1), 2021. 4 [40] Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James DiCarlo, Josh McDermott, Antonio Torralba, et al. The threedworld transport challenge: visually guided task-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint arXiv:2103.14025, 2021. 4 [41] Rohit Girdhar and Deva Ramanan. Cater: diagnostic dataset for compositional actions & temporal reasoning. In International Conference on Learning Representations, 2021. 4 [42] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023. 4 [43] Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo. Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. arXiv preprint arXiv:2309.17277, 2023. [44] Akshat Gupta. Are chatgpt and gpt-4 good poker players? pre-flop analysis. arXiv preprint arXiv:2308.12466, 2023. 3 [45] Susan Hespos and Elizabeth Spelke. Conceptual precursors to language. Nature, 430(6998):453456, 2004. 4 [46] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. 3 [47] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024. 3 [48] Sihao Hu, Tiansheng Huang, and Ling Liu. Pokellmon: human-parity agent for pokemon battles with large language models. arXiv preprint arXiv:2402.01118, 2024. 3 10 [49] Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, and Yanru Zhang. Pokergpt: An end-to-end lightweight solver for multi-player texas holdem via large language model. arXiv preprint arXiv:2401.06781, 2024. [50] Xiaoshuang Huang, Hongxiang Li, Meng Cao, Long Chen, Chenyu You, and Dong An. Cross-modal conditioned reconstruction for language-guided medical image segmentation. arXiv preprint arXiv:2404.02845, 2024. [51] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27582766, 2017. 3 [52] Jiang Ji, Meng Cao, Tengtao Song, Long Chen, Yi Wang, and Yuexian Zou. Video referring expression comprehension via transformer with content-conditioned query. In Proceedings of the 1st International Workshop on Deep Multimodal Learning for Information Retrieval, pages 39 48, 2023. [53] Ji Jiang, Meng Cao, Tengtao Song, and Yuexian Zou. Video referring expression comprehension via transformer with arXiv preprint arXiv:2210.02953, content-aware query. 2022. [54] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. 1, 6, 7 [55] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization. arXiv preprint arXiv:2402.03161, 2024. [56] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. In Empirical Methods in Natural Language Processing, 2018. 3 [57] Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. In International conference on machine learning, pages 430438. PMLR, 2016. 4 [58] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 5, 6, 7 [59] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 6 [60] Hongxiang Li, Meng Cao, Xuxin Cheng, Yaowei Li, Zhihong Zhu, and Yuexian Zou. G2l: Semantically aligned and uniform video grounding via geodesic and game theory. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1203212042, 2023. [61] Hongxiang Li, Meng Cao, Xuxin Cheng, Zhihong Zhu, Yaowei Li, and Yuexian Zou. Generating templated caption for video grounding. arXiv preprint arXiv, 2301:2, 2023. [62] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1 [63] Kenneth Li, Aspen Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task. In The Eleventh International Conference on Learning Representations, 2023. 3 [64] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 2, 3, 6, 7 [65] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2046 2065, 2020. 3 [66] Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of videolanguage models. arXiv preprint arXiv:2311.17404, 2023. 3 [67] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023. [68] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 2, 6, 7 [69] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for viIn Proceedings of the IEEE/CVF sual language models. Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 7 [70] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, et al. Mm-vid: Advancing video understanding with gpt-4v (ision). arXiv preprint arXiv:2310.19773, 2023. 2 [71] Carlos Ling, Konrad Tollmar, and Linus Gisslen. Using deep convolutional neural networks to detect rendered glitches in video games. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 6673, 2020. 2 [72] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 2, 6, 7 [73] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 6, [74] Junling Liu, Ziming Wang, Qichen Ye, Dading Chong, Peilin Zhou, and Yining Hua. Qilin-med-vl: Towards chi11 nese large vision-language model for general healthcare. arXiv preprint arXiv:2310.17956, 2023. 3 [75] Jijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin Liao, Yi Wu, and Yu Wang. Llm-powered hierarchical language agent for real-time human-ai coordination. arXiv preprint arXiv:2312.15224, 2023. 3 [76] Ruyang Liu, Chen Li, Yixiao Ge, Ying Shan, Thomas Li, and Ge Li. One for all: Video conversation is feaarXiv preprint sible without video instruction tuning. arXiv:2309.15785, 2023. 2, 7 [77] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. arXiv preprint arXiv:2404.00308, 2024. 2, 6, [78] Ruyang Liu, Haoran Tang, Haibo Liu, Yixiao Ge, Ying Shan, Chen Li, and Jiankun Yang. Ppllava: Varied video arXiv sequence understanding with prompt guidance. preprint arXiv:2411.02327, 2024. 6, 7, 15 [79] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2, 3 [80] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Et bench: Towards open-ended event-level video-language understanding. arXiv preprint arXiv:2409.18111, 2024. 3 [81] Yuanjiang Luo, Hongxiang Li, Xuan Wu, Meng Cao, Xiaoshuang Huang, Zhihong Zhu, Peixi Liao, Hu Chen, and Yi Zhang. Textual inversion and self-supervised refinement In International Conferfor radiology report generation. ence on Medical Image Computing and Computer-Assisted Intervention, pages 681691. Springer, 2024. [82] Weiyu Ma, Qirui Mi, Yongcheng Zeng, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, and Jun Wang. Large ii: Benchmarks and language models play starcraft arXiv preprint chain of summarization approach. arXiv:2312.11865, 2023. 3 [83] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 2, 3, [84] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 2, 3 [85] Yangjun Mao, Jun Xiao, Dong Zhang, Meng Cao, Jian Shao, Yueting Zhuang, and Long Chen. Improving reference-based distinctive image captioning with contrastive rewards. ACM Transactions on Multimedia Computing, Communications and Applications, 2023. [86] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. 4 [87] Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, what happens if... learning to preand Ali Farhadi. In Computer Vision dict the effect of forces in images. ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 269285. Springer, 2016. 4 [88] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large video-language models. arXiv preprint arXiv:2311.13435, 2023. 2 [89] Lakshmi Nair, Jonathan Balloch, and Sonia Chernova. Tool macgyvering: Tool construction using geometric reasoning. In 2019 international conference on robotics and automation (ICRA), pages 58375843. IEEE, 2019. [90] Alfredo Nantes, Ross Brown, and Frederic Maire. framework for the semi-automatic testing of video games. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 197 202, 2008. 2 [91] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: for evaluatA comprehensive benchmark and toolkit arXiv preprint ing video-based large language models. arXiv:2311.16103, 2023. 3 [92] OpenAI. Hello gpt-4o. OpenAI Blog, 2024. 1, 2, 5, 6, 7 [93] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 2 [94] Savelii Pashkov. Video game industry market analysis: Approaches that resulted in industry success and high demand. 2021. 15 [95] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 3 [96] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 84948502, 2018. [97] Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, YuanHong Liao, Joshua Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: challenge for social perception and human-ai collaboration. In International Conference on Learning Representations, 2021. 4 [98] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6, 2023. 3 [99] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 12 Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. 15 [100] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 2, [101] Farrukh Rahman. Weak supervision for label efficient visual bug detection. arXiv preprint arXiv:2309.11077, 2023. 2 [102] Geeta Rani, Upasana Pandey, Aniket Anil Wagde, and Vijaypal Singh Dhaka. deep reinforcement learning technique for bug detection in video games. International Journal of Information Technology, 15(1):355367, 2023. 2 [103] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 2 [104] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 8 [105] Xiao Shao, Weifu Jiang, Fei Zuo, and Mengqing Liu. Swarmbrain: Embodied agent for real-time strategy game arXiv preprint starcraft ii via large language models. arXiv:2401.17749, 2024. 3 [106] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 3 [107] Mohammad Reza Taesiri and Cor-Paul Bezemer. Videogamebunny: Towards vision assistants for video games. arXiv preprint arXiv:2407.15295, 2024. 3, 4, 15 [108] Mohammad Reza Taesiri, Moslem Habibi, and Mohammad Amin Fazli. video game testing method utilizing Iran Journal of Computer Science, 17(2), deep learning. 2020. 2 [109] Mohammad Reza Taesiri, Finlay Macklon, and Cor-Paul Bezemer. Clip meets gamephysics: Towards bug identification in gameplay videos using zero-shot transfer learning. In Proceedings of the 19th International Conference on Mining Software Repositories, pages 270281, 2022. 2, 3, 4, 15 [110] Mohammad Reza Taesiri, Finlay Macklon, Yihe Wang, Hengshuo Shen, and Cor-Paul Bezemer. Large language models are pretty good zero-shot video game bug detectors. arXiv preprint arXiv:2210.02506, 2022. 3, 4, 15 [111] Mohammad Reza Taesiri, Tianjun Feng, Cor-Paul Bezemer, and Anh Nguyen. Glitchbench: Can large multimodal In Proceedings of models detect video game glitches? the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2244422455, 2024. 3, 4, 15 [112] Mohammad Reza Taesiri, Finlay Macklon, Sarra Habchi, and Cor-Paul Bezemer. Searching bug instances in gameIEEE Transactions on Games, play video repositories. 2024. [113] Haoran Tang, Meng Cao, Jinfa Huang, Ruyang Liu, Peng Jin, Ge Li, and Xiaodan Liang. Muse: Mamba is efficient multi-scale learner for text-video retrieval. arXiv preprint arXiv:2408.10575, 2024. [114] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6, 7 [115] Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel. Chess as testbed for language model state tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1138511393, 2022. 3 [116] Marc Toussaint, Kelsey Rebecca Allen, Kevin Smith, and Joshua Tenenbaum. Differentiable physics and stable modes for tool-use and manipulation planning. Robotics: Science and systems foundation, 2018. 4 [117] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [118] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [119] Hsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Josh Tenenbaum, Dan Yamins, Judith Fan, and Kevin Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Advances in Neural Information Processing Systems, 36, 2024. [120] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalons game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023. 3 [121] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 2, 3 [122] Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and Andrea Tacchetti. Visual interaction networks: Learning physics simulator from video. Advances in neural information processing systems, 30, 2017. 4 [123] Benedict Wilkins and Kostas Stathis. Learning to identify perceptual bugs in 3d video games. arXiv preprint arXiv:2202.12884, 2022. 2 [124] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning [137] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 3 [138] Can Zhang, Meng Cao, Dongming Yang, Jie Chen, and Yuexian Zou. Cola: Weakly-supervised temporal action localization with snippet contrastive learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1601016019, 2021. 3 [139] Can Zhang, Meng Cao, Dongming Yang, Ji Jiang, and Yuexian Zou. Synergic learning for noise-insensitive webly-supervised temporal action localization. Image and Vision Computing, 113:104247, 2021. [140] Can Zhang, Tianyu Yang, Junwu Weng, Meng Cao, Jue Wang, and Yuexian Zou. Unsupervised pre-training for In Proceedings of temporal action localization tasks. the IEEE/CVF conference on computer vision and pattern recognition, pages 1403114041, 2022. [141] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 543553, 2023. 2 [142] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 2, 6, [143] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via arXiv preprint policy-level reflection and optimization. arXiv:2402.17574, 2024. 3 [144] Yan Zheng, Xiaofei Xie, Ting Su, Lei Ma, Jianye Hao, Zhaopeng Meng, Yang Liu, Ruimin Shen, Yingfeng Chen, and Changjie Fan. Wuji: Automatic online combat game testing using evolutionary deep reinforcement learning. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 772784. IEEE, 2019. 2 in real-world videos. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 3 [125] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interarXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 2, 3 [126] Sarah Wu, Rose Wang, James Evans, Joshua Tenenbaum, David Parkes, and Max Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration. Topics in Cognitive Science, 13 (2):414432, 2021. 4 [127] Xuan Wu, Hongxiang Li, Yuanjiang Luo, Xuxin Cheng, Xianwei Zhuang, Meng Cao, and Keren Fu. Uncertaintyaware sign language video retrieval with probability distriIn European Conference on Computer bution modeling. Vision, pages 390408. Springer, 2025. [128] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 3 [129] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. 3 [130] Xinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and Borje Karlsson. survey on game playing agents and large models: Methods, applications, and challenges. arXiv preprint arXiv:2403.10249, 2024. 3 [131] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 6, 15, 16 [132] Bang Yang, Meng Cao, and Yuexian Zou. Concept-aware video captioning: Describing videos with effective prior information. IEEE Transactions on Image Processing, 2023. [133] Dongming Yang, Yuexian Zou, Can Zhang, Meng Cao, and Jie Chen. Rr-net: Relation reasoning for end-to-end humanIEEE Transactions on Cirobject interaction detection. cuits and Systems for Video Technology, 32(6):38533865, 2021. [134] Qichen Ye, Junling Liu, Dading Chong, Peilin Zhou, Yining Hua, Fenglin Liu, Meng Cao, Ziming Wang, Xuxin Cheng, Zhu Lei, et al. Qilin-med: Multi-stage knowledge injection advanced medical large language model. arXiv preprint arXiv:2310.09089, 2023. [135] Tian Ye, Xiaolong Wang, James Davidson, and Abhinav In ProInterpretable intuitive physics model. Gupta. ceedings of the European Conference on Computer Vision (ECCV), pages 87102, 2018. 4 [136] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations, 2020. 4 14 6. Supplementary Material This supplementary material is organized as follows. We firstly clarify our motivation in designing the proposed benchmark and methodology in Sec 6.1. Then, we present more ablation studies in Sec 6.2. The qualitative comparison results are illustrated in Sec 6.3. Finally, the prompts for supervised fine-tuning (SFT), direct preference optimization (DPO), and evaluation are detailed in Sec 6.4. 6.1. Motivation Clarification Q: Why use gameplay videos rather than real-world videos for benchmarking physical commonsense understanding? A: Compared to real-world videos, gameplay videos offer several advantages for physical commonsense benchmarks: 1) Easier to define: Instead of complex physical formulas, this paper focuses on the intuitive adherence to the physical commonsense. Given real-world videos, it is both challenging and unnecessary to exhaustively cover and interpret all normal physical phenomena. In contrast, gameplay videos typically contain glitches that violate physical commonsense. This can simplify the definition and evaluation of the physical commonsense understanding, i.e., focusing on interpreting physical commonsense violation rather than trying to enumerate all the existing normal physical phenomena; 2) More meaningful: The video game industry generates substantial annual revenue with billions of gamers [94]. Automatically detecting in-game glitches acts as highly demanding task for gameplay video stress testing. Therefore, developing video LLMs to uncover physical commonsense violations in gameplay videos may potentially offer one automatic and end-to-end solution. Q: Differences from prior gameplay glitch datasets. A: We have briefly clarified the differences between our proposed PhysGame benchmark and existing gameplay glitch datasets in the related work section of the main paper. Here, we provide more detailed discussions. The overall comparisons with existing benchmarks are illustrated in Table 2. Taesiri et.al [109] focus on gameplay video retrieval by leveraging the zero-shot transfer capabilities of the large-scale pre-trained CLIP model [99]. Several existing works are limited in the static image domain [107, 111]. GlitchBench [111] is proposed for video-game quality assurance by evaluating the reasoning capabilities of LMMs under unusual and glitched scenarios. This paper has two drawbacks: 1) It is limited in the image-based LLMs and fails to evaluate the capabilities of video LLMs; 2) GlitchBench [111] uses Llama-2-70b-Chat as judge to evaluate the models responses. This open-ended evaluation is unreliable and unstable due to the change in the version of the judge model. Our proposed PhysGame benchmark advances this by constructing multiple-choice questions, which facilitates more convenient evaluations. GameBugDescript [110] is pure-text benchmark with the Table 9. Hyper-parameter ablations of (a) the sampled frame number in temporal hacking and (b) the frame resolution scale factor  in spatial hacking for PhysDPO construction."
        },
        {
            "title": "AVG",
            "content": "59.5 "
        },
        {
            "title": "AVG",
            "content": "1/8 57.1 2 58.1 1/16 59. 4 57.8 1/32 58.6 reliance on pre-extracted event-wise textual descriptions and lacks support for multi-modal evaluations. In contrast, our PhysVLM addresses all these limitations and supports multi-modal instruction evaluations in videos. 6.2. More Ablation Studies Ablation on LLMs in PhysVLM. Our PhysVLM is built upon PPLLaVA [78] by substituting the Vicuna-7B [30] LLM with the high-performing Qwen2-7B [131]. To verify the necessity, we report the experimental results on PhysGame with Vicuna-7B LLM in Table 10. Experimental results demonstrate that Qwen2-7B significantly enhances the performance on physical commonsense understanding, establishing the proposed PhysVLM as strong baseline. Ablations on Training Data. We utilize the hybrid training dataset across both the SFT and DPO training stages. We perform the ablation study to assess the individual contributions of each dataset. The ablative results on PhysVLM have been presented in Table 8 of the main paper and we provide the results on Video-MME and VCG benchmarks in Table 12 and Table 11, respectively. As shown, the introduction of PhysInstruct and PhysDPO datasets enables sustained performance improvements for PhysVLM on the PhysGame and VCG benchmarks. However, these two datasets have limited impact on the Video-MME benchmark, possibly because Video-MME places greater emphasis on long-video understanding. Despite this, given the substantial performance gains observed on the PhysGame and VCG benchmarks, we argue that the PhysInstruct and PhysDPO datasets possess substantial merit. Hyper-parameter Ablations The PhysDPO dataset is constructed conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). For implementation, we set the sampled frame number to 1 and the frame resolution scale factor  to 1/16. We conduct the hyper-parameter ablation studies of and  in Table 9. 6.3. Visualizations We provide the qualitative comparison results in both the formats of open-ended questions (cf . Figure 5 and Figure 6) and multi-choice questions (cf . Figure 7). The visualization 15 Table 10. Ablations on LLMs in PhysVLM with Vicuna-7B [30] or Qwen2-7B [131]. Stage LLMs AVG Mechanics Kinematics Optics Material Grav. Elast. Fric. Velo. Acc. Refl. Refr. Abs. SFT SFT Vicuna Qwen-2 DPO Vicuna DPO Qwen-2 44.7 56.7 48.2 59. 47.9 54.9 56.3 64.8 45.0 62.5 52.5 66.3 48.9 60.2 50.6 60. 52.1 51.1 59.6 59.6 48.9 63.6 48.9 60.2 30.4 45.7 28.3 39. 42.9 57.1 35.7 67.9 28.8 28.8 28.8 35.6 Col. 28.9 64. 31.1 57.8 Table 11. Ablations on training data on VCG benchmark. Stage Training Data SFT SFT SFT DPO DPO LLava-Hound LLava-Hound, LLaVA-Image LLava-Hound, LLaVA-Image, PhysInstruct LLava-Hound-DPO LLava-Hound-DPO, PhysDPO CI 3.48 3.43 3.59 3.94 3.89 DO 2.88 2.99 3. 3.43 3.69 CU 3.74 3.73 3.89 4.25 4.26 TU 2.58 2.56 2. 3.12 3.11 Table 12. Ablations on training data on Video-MME benchmark. Rig. 50.0 51.4 47.3 62.2 CO 3.02 3.12 3.44 4.05 4.19 Sha. Gest. 31.2 50.0 37.5 37.5 48.3 72. 60.9 78.2 AVG 3.14 3.17 3.35 3.76 3.83 Models Training Data Short (%) Medium (%) Long (%) Overall (%) w/o subs w/ subs w/o subs w/ subs w/o subs w/ subs w/o subs w/ subs SFT SFT SFT DPO DPO LLava-Hound LLava-Hound, LLaVA-Image LLava-Hound, LLaVA-Image, PhysInstruct LLava-Hound-DPO LLava-Hound-DPO, PhysDPO 65.6 65.2 64.1 66.0 66.1 68.9 68.3 68.0 70.2 70. 55.3 54.9 55.0 53.6 54.3 60.4 60.2 61.7 60.5 59.6 47.7 47.6 46.4 47.3 47. 52.4 52.8 50.3 52.8 53.8 56.2 55.9 55.2 55.6 55.8 60.6 60.4 60.0 61.2 61. results manifest that our proposed PhysVLM effectively understands and interprets phenomena in videos that violate physical commonsense, further advancing the development of video LLMs. 6.4. Prompts The prompts for SFT data, DPO data, and evaluation are illustrated in Table 13, Table 14, and Table 15. 16 (a) (b) Figure 5. Qualitative examples of open-ended questions. 17 (a) (b) Figure 6. Qualitative examples of open-ended questions. (a) (b) (c) Figure 7. Qualitative examples of multi-choice questions. The correct options are marked in green. 19 Table 13. Prompt for instruction-tuning data generation in PhysInstruct. role: system You are an AI visual assistant, and you are seeing video and title as hint. Watch the video carefully and analyze the events and object movements, focusing on any inconsistencies with physical laws. Please design conversation between you and the person asking about the game description and the glitch especially. Example questions: What is the description of the glitch observed in the video? Are there any abnormalities present in the videos? Please provide description of the video content. role: user Title of the video: {title}. This hint might not be accurate. Your analysis should be based primarily on your own observations and understanding of the video and do not imply the title in any of your generation. Please directly design questions like the example and answer them in detail. Ensure that all descriptions are at the video level, do not refer to images or frames. Table 14. Prompt for response generation in PhysDPO. The false title is randomly selected from the other videos and the question is instantiated by the same instruction in PhysInstruct. role: system You are an AI visual assistant, and you are seeing video and title as hint. Watch the video carefully and analyze the events and object movements, focusing on any inconsistencies with physical laws. Please design conversation between you and the person asking about the game description and the glitch especially. role: user Title of the video: {false title}, Questions: {question}. Please repeat the question (Question:) and answer them in detail (Answer:). Ensure that all descriptions are at the video level, do not refer to images or frames. Table 15. Prompt for evaluation generation in PhysGame. Watch the video carefully and analyze the events and object movements, focusing on any inconsistencies with physical laws. Identify and highlight instances where the behavior deviates from expected real-world physics, and select the most accurate option to describe the detected glitch. Answer with the option letter from the given choices directly."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Peking University",
        "Sun Yat-sen University",
        "University of Toronto",
        "University of Waterloo"
    ]
}