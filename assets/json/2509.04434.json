{
    "paper_title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
    "authors": [
        "Hyunsoo Cha",
        "Byungjun Kim",
        "Hanbyul Joo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 4 3 4 4 0 . 9 0 5 2 : r arXiv preprint DURIAN: DUAL REFERENCE-GUIDED PORTRAIT ANIMATION WITH ATTRIBUTE TRANSFER Hyunsoo Cha, Byungjun Kim. Hanbyul Joo Seoul National University {243stephen,byungjun.kim,hbjoo}@snu.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "We present Durian, the first method for generating portrait animation videos with facial attribute transfer from given reference image to target portrait in zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of diffusion model. We train the model using self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in single generation pass without additional training."
        },
        {
            "title": "INTRODUCTION",
            "content": "Personalized appearance editing, such as trying on glasses or exploring new hairstyles, is increasingly common in AR applications. However, most existing tools rely on static previews or simplified 2D templates, limiting realism and expressiveness. single image is often insufficient for users to judge whether new look truly suits them, particularly for more significant changes like facial hair or hairstyles. Users naturally want to see how style behaves with facial expressions or looks from different angles. This motivates the need for method that can animate single portrait image using the appearance of reference face with the desired attributes. Recent diffusion-based methods have enabled high-quality image editing by injecting visual attributes from one image into another. Exemplar-based inpainting approaches (Yang et al., 2023; Chen et al., 2024) support applications like hairstyle transfer and virtual try-on. However, these methods are designed for static image editing and often require manual input (e.g., masks or user scribbles) to localize the transfer region. Extensions to video (Fang et al., 2024; Tu et al., 2025) still assume that the target regions are easy to specify, which does not hold for dynamic and deformable facial attributes. Other methods (Zhang et al., 2025; Chung et al., 2025) focus specifically on hairstyle transfer by constructing task-specific training data. For example, Zhang et al. (2025) synthesizes triplets by predicting bald version of portrait and generating reference hair images via pretrained diffusion model, but this process is not scalable and hard to extend to other attributes. Moreover, these methods are confined to static images and are unable to generate dynamic expression or pose-aware animations. In this work, we propose zero-shot framework for portrait animation with cross-identity attribute transfer. Given only single portrait of the source identity and another image depicting the desired Project page: https://hyunsoocha.github.io/durian 1 arXiv preprint Figure 1: Overview of Durian. Given portrait and reference image specifying target attributes (e.g., hairstyle, eyeglasses), our Durian generates an animatable 2D portrait video with composable visual attributes in single-stage pipeline. attribute (e.g., hairstyle, beard, or eyeglasses), our method generates video of the source face turning naturally while seamlessly adopting the target attribute, as shown in figure 1. straightforward solution might involve constructing triplet dataconsisting of reference attribute image, target identity before the edit, and the desired edited versionbut building such triplets for wide variety of facial attributes lacks scalability. The problem becomes even more challenging for video, where consistency across frames is hard to guarantee. Alternatively, attribute transfer can be formulated as an inpainting or object insertion task. However, this requires accurate region masks, which are difficult to define in general and scalable way, as the spatial extent of the attribute (e.g., long vs. short hair) can vary significantly. These challenges call for flexible and generalizable framework that performs attribute-guided animation without requiring triplet training data or frame-level masks. To address these challenges, we extend self-reference training setups commonly used in exemplarbased inpainting (Yang et al., 2023) to the video domain for portrait animation with attribute transfer. Leveraging in-the-wild portrait videos, we randomly sample one frame and decompose it into two inputs: an attribute-only image and an attribute-masked portrait image. These are fed into diffusion model, which is trained to generate the remaining frames of the video. Notably, the attribute mask is required only for the two input imagesnot for the output framesallowing the model to implicitly propagate the desired attribute throughout the animation. To accommodate attribute transfer with varying spatial extent, we expand the mask region in the attribute-masked portrait image, exposing the model to wider range of insertion scenarios. To further improve robustness against the pose and alignment variations commonly seen in in-the-wild pairs, we introduce tailored augmentation strategy that perturbs the spatial layout of the input pair. This allows the model to generalize across diverse attribute-portrait configurations. Our setup enables robust, zero-shot animation with diverse attributes and minimal annotation effort. Our contributions are summarized below: We propose the first method to directly generate portrait animation video with transferred attributes from two images, generalized over diverse facial attributes, not limited to hair. We design self-reconstruction training for attribute transfer that enables scalable learning on in-the-wild data, enhanced by spatial mask expansion and tailored augmentation to improve robustness against pose and attribute variations. Our framework naturally supports multi-attribute composition within single generation pass, without requiring any additional training or fine-tuning. 2 arXiv preprint"
        },
        {
            "title": "2 RELATED WORK",
            "content": "Face Image and Video Editing. Generative models have advanced facial editing from unconditional synthesis to fine-grained manipulation of existing images (Goodfellow et al., 2014; Rezende & Mohamed, 2015; Ho et al., 2020). Latent-space editing with StyleGAN (Karras et al., 2020) and GAN inversion (Zhu et al., 2016; Abdal et al., 2019; Richardson et al., 2021) has been extended to video via latent trajectory modeling (Yao et al., 2021; Tzaban et al., 2022) and 3D-aware editing (Bilecen et al., 2024; Xu et al., 2024). However, such approaches often rely on attribute classifiers or fixed editing controls. Diffusion-based models have introduced more flexible editing through promptdriven (Brooks et al., 2023) or identity-preserving techniques (Ye et al., 2023; Wang et al., 2024), with extensions to video improving temporal consistency (Ku et al., 2024; Kim et al., 2023). Still, these methods are limited to modifying existing content and cannot generate new motions or expressions. Diffusion-based Virtual Try-On. Diffusion-based try-on methods typically formulate editing as masked inpainting, where reference content is inserted into target image using explicit masks (Yang et al., 2023; Chen et al., 2024; Mou et al., 2025; Chen et al., 2025; Song et al., 2025). These approaches have been adapted to domain-specific tasks such as clothing (Kim et al., 2024; Li et al., 2024; Chong et al., 2024), hair (Zhang et al., 2025; Chung et al., 2025), and makeup (Zhang et al., 2024b). While effective for static images, they rely on category labels or mask annotations. Video extensions (Fang et al., 2024; Tu et al., 2025) apply per-frame inpainting with post-hoc smoothing, but predefined masks are hard to specify for deformable facial attributes that vary over time. In contrast, our model performs attribute transfer and animation jointly in single forward pass, conditioned only on pair of reference images and facial keypoint sequence. This eliminates the need for per-frame masks, text prompts, or category labels, enabling zero-shot transfer of diverse facial attributes. Portrait Animation from Single Image. Portrait animation aims to generate motion from static image, typically guided by facial keypoints, audio, or motion trajectories. Early methods rely on GANs with implicit keypoint modeling (Guo et al., 2024; Wang et al., 2021), while recent approaches use diffusion models (Hu, 2024; Zhu et al., 2024; Yang et al., 2025) for improved realism and temporal stability. These methods primarily focus on reenactment and appearance preservation. Others incorporate paired motion (Xie et al., 2024) or audio (Yang et al., 2025), but require multistage inference or fine-tuning. Our model unifies facial attribute transfer and motion generation in single framework. Without subject-specific tuning, it synthesizes identity-preserving, photorealistic videos from diverse attribute references and keypoint-driven motion, enabling compositional edits and smooth animation in single pass."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 OVERVIEW: LEARNING ATTRIBUTE TRANSFER FROM SELF-RECON. We propose diffusion-based generative framework for portrait animation with cross-identity attribute transfer. At high level, the model generates an -frame animation sequence = {Iτ }F τ =1 as: = Durian(Iattr, Mattr, Iport, Mport, K), (1) conditioned on an attribute image Iattr, portrait image Iport, and sequence of driving facial keypoint images = {kτ }F τ =1. Each reference image is accompanied by binary mask: Mattr localizes the attribute region (e.g., hair or glasses) in the reference image, while Mport specifies the candidate region in the portrait where the attribute will be transferred. Using these masks, we construct two masked inputs: the attribute-only image Iattr = Iattr Mattr, where only the attribute region is preserved, and the attribute-masked portrait image Iport = Iport (1 Mport), where the corresponding region is removed. These masked inputs are fed into the Dual ReferenceNet, consisting of the Attribute ReferenceNet (ARNet) and Portrait ReferenceNet (PRNet), which extract multi-scale spatial features. These features are then injected into diffusion-based generator, the Denoising UNet (DNet), to synthesize the remaining frames of the video, guided by the sequence of keypoints (Section 3.2). To enable training without requiring explicitly annotated triplets (i.e., combinations of target attribute image, an original portrait image, and an edited portrait image), we adopt self-reconstruction 3 arXiv preprint Figure 2: Overview of Training Pipeline. Given an attribute-masked portrait image Iport and an attribute-only image Iattr, our Durian synthesizes portrait animation with the transferred attribute. These inputs are constructed by randomly sampling two frames from training video and applying the estimated masks. sequence of facial keypoints kτ τ =1 is extracted from the video to guide the motion of the generated animation. During generation, spatial features from PRNet and ARNet are fused via spatial attention into the DNet, enabling identity-preserving and attribute-consistent video synthesis. strategy based on portrait videos (Yu et al., 2023; Xie et al., 2022). Specifically, we simulate attribute transfer by sampling two frames Iattr and Iport from the same video, treating one as the attribute reference and the other as the target portrait. We then construct the masked inputs Iattr and Iport using the same masking formulation as in inference, based on segmentation mask of randomly selected attribute. Although the two frames originate from the same identity, this complementary masking enforces role separation between identity and attribute inputs and encourages the model to learn meaningful mappings from attribute and identity features to output frames without cross-identity supervision. To enhance the models ability to generalize beyond the self-attribute transfer setup, we introduce an augmentation pipeline that improves robustness to variations in attribute shape, position, and appearance (Section 3.3). At inference time, we estimate aligned attribute masks using 3D animatable avatar to mitigate spatial misalignment between the attribute image and the portrait. Conditioned on the masked reference images and the driving keypoint sequence, our model synthesizes temporally coherent portrait animations with attribute transfer. Notably, our design supports multi-attribute composition and interpolation between attributes in single generation pass, without requiring additional training or post-processing (Section 3.4). Fig. 1 shows our generated portrait animations with attribute transfer. 3.2 MODEL ARCHITECTURE: DUAL REFERENCENET Inspired by recent approaches (Guo et al., 2023; Hu, 2024; Zhu et al., 2024) that leverage ReferenceNet to inject spatial features into diffusion models, we present Dual ReferenceNet architecture tailored for portrait animation with attribute transfer. Unlike previous work, our model includes two separate encoders: Attribute ReferenceNet (ARNet) and Portrait ReferenceNet (PRNet), each sharing the same architecture as the Denoising U-Net (DNet) in the diffusion model. The networks are adapted from the U-Net (Long et al., 2015) used in latent diffusion models (Rombach et al., 2022), where each block consists of convolutional layers, followed by self-attention and cross-attention modules. The overall architecture is illustrated in Fig. 2. Reference inputs. Given an attribute image Iattr R3HW and portrait image Iport R3HW , along with their binary masks Mattr R1HW and Mport R1HW , which localize the attribute region and the candidate transfer region respectively, we construct two masked inputs: the attribute-only image Iattr = Iattr Mattr, where only the attribute region is preserved, and the attribute-masked portrait image Iport = Iport (1 Mport), where the corresponding candidate region is removed. We then encode these masked images into latent representations using the pretrained VAE from the latent diffusion model (Rombach et al., 2022), yielding zattr, zport 4 arXiv preprint Rchw. The corresponding masks Mattr, Mport are downsampled to match the latent resolution, producing mattr, mport R1hw. These downsampled masks are concatenated with the latents along the channel dimension to form (c + 1)-channel inputs: zattr = concatc(zattr, mattr), zport = concatc(zport, mport), (2) where zattr, zport R(c+1)hw. Spatial attention. The augmented latents are passed to ARNet Eattr and PRNet Eport, with the same architecture as DNet, to extract multi-scale feature maps after convolutional layers of each block: Fattr := {Fl Fport := {Fl attr}L port}L l=1 = Eattr(zattr; Θattr), l=1 = Eport(zport; Θport), (3) (4) where Θ{attr,port} are the parameters of Dual ReferenceNet. Let Fτ,l Rclhlwl denote the feature map of the frame τ at the l-th block of the denoising U-Net. While the original denoising U-Net includes self-attention layer at each resolution, we replace it with our spatial attention to integrate identity and attribute features in spatially-aware manner. We denote width-wise concatenation as concatw(), and define our spatial attention SA(, , ) as: Fτ,l ref,t := concatw({Fτ,l , Fl port, Fl attr}) Rclhl3wl, Fl = SA(Fτ,l , Fl port, Fl attr) = Attention(WQFτ,l , WKFτ,l ref,t, WV Fτ,l ref,t), where Fτ,l Rclhlwl is the feature map after the spatial attention, Attention(Q, K, ) = softmax(QK / d)V (5) (6) (7) is the standard scaled dot-product attention (Vaswani et al., 2017), WQ, WK, WV are linear projection layers. This width-wise concatenation preserves spatial resolution and allows the model to attend across all positions in the combined reference and target features. As result, the model can leverage both attribute and portrait guidance at every generation step, enabling fine-grained and semantically consistent attribute transfer across identities. Cross-attention with semantic embeddings. After applying spatial attention, we further inject semantic guidance into both the Dual ReferenceNet and the denoising U-Net via cross-attention. For ARNet, we use the CLIP (Radford et al., 2021) embedding of the attribute-only image Iattr as the attribute embedding ϕattr, which is injected via cross-attention into each block of ARNet. For PRNetand DNet, we follow StableAnimator (Tu et al., 2024) and construct portrait embedding ϕport by combining ArcFace (Deng et al., 2019) and CLIP embeddings of the attribute-masked portrait image Iport. This embedding is injected into both PRNet and DNet to enhance identity preservation. We define the cross-attention operation CA(, ) as: F, Kϕ, CA( F, ϕ) = Attention(W ϕ), where is the input feature map, ϕ is the conditioning embedding, and linear projections. Let Fl attr and Fl of ARNet and PRNet, and let Fl cross-attention updates are given by: {attr,port} = CA( Fl Fl port, and Fτ,l {attr,port}, ϕ{attr,port}), Fτ,l attr, Fl = CA( Fτ,l are learned port denote the feature maps after self-attention in the l-th block denote the feature map of DNet after spatial attention. The Q, K, are the feature maps after cross-attention in ARNet, PRNet, and DNet, , ϕport), (9) (8) where Fl respectively. Temporal extension and keypoint guidance. In addition to spatial and semantic conditioning, our model incorporates temporal awareness to generate coherent portrait animations. To model interframe consistency, we insert temporal self-attention into each block of the denoising U-Net (Hu, 2024; Zhu et al., 2024), enabling the model to capture motion dynamics across frames. To control pose 5 arXiv preprint and expression, we use sequence of facial keypoints = kτ et al., 2024). Each keypoint image kτ is encoded into spatial feature map Fτ and combined with the noisy latent z(τ ) predicts the added noise ˆϵ(τ ) semantic embeddings, and keypoint features: τ =1 extracted by Sapiens (Khirodkar kpt via pose encoder following (Zhu et al., 2024). For each frame τ , our DNet ϵθ at timestep t, using the reference features, from the noisy latent z(τ ) t ˆϵ(τ ) = ϵθ (cid:16) z(τ ) , t, Fattr, Fport, ϕattr, ϕport, Fτ kpt (cid:17) . (10) The predicted noise is used to recover the denoised latent z(τ ) to produce the final video frame: 0 , then decoded by the VAE decoder Iτ = D(z(τ ) 0 ), for τ = 1, . . . , F. (11) This frame-wise prediction process, guided by temporal keypoints and reference features, enables the model to generate temporally coherent and pose-aligned portrait animations."
        },
        {
            "title": "3.3 TRAINING STRATEGY.",
            "content": "Training loss. To effectively train our model, we adopt two-stage training scheme following previous approaches (Hu, 2024; Zhu et al., 2024). In the first stage, we optimize the entire model except the temporal attention layers using single-frame inputs. We define the per-frame conditioning bundle as := (Fattr, Fport, ϕattr, ϕport) , where Fport, Fattr are the multi-scale spatial features from PRNet and ARNet and ϕport, ϕattr are the semantic embeddings. Then, the training objective is the standard denoising diffusion loss: L(1) diff = Ez0, ϵ, ϵ ϵθ (zt, t, C, Fkpt)2(cid:105) (cid:104) , (12) where zt is the noised latent at diffusion timestep t, ϵ is the sampled noise, and kτ is the facial keypoint image at frame τ . In the second stage, we freeze all modules except the temporal attention layers and train them using multi-frame inputs. The temporal objective considers sequence of noised latents and corresponding keypoints: L(2) diff = {z(τ ) 0 }F τ =1, ϵ1:F , (cid:20)(cid:13) (cid:13)ϵ1:F ϵθ (cid:13) (cid:16) {z(τ ) }F τ =1, t, C, {Fτ kpt}F τ =1 2(cid:21) (cid:17)(cid:13) (cid:13) (cid:13) , (13) where ϵ1:F = {ϵ(τ )}F τ =1 denotes the per-frame noise sequence. This staged training improves convergence and allows the temporal attention module to focus on modeling motion dynamics without disrupting the spatial fidelity learned in the first stage. Attribute-aware mask expansion. To simulate diverse spatial extents of facial attributes during training, we introduce an attribute-aware mask expansion strategy, illustrated in the top right of Fig. 2. Given training frame I, we begin by selecting target attribute (e.g., hair, eyeglasses, beard) and obtaining its binary segmentation mask Mattr via Sapiens (Khirodkar et al., 2024). To simulate variation in the shape and coverage of this attribute, we generate an image Igen using SDXL (Podell et al., 2023) with ControlNet (Zhang et al., 2023), conditioned on the facial keypoints of and text prompt describing modified attribute appearance (e.g., long wavy hair). We then extract new attribute mask Mgen using Sapiens. We define the initial candidate region as Minit port := Mattr, and compute the expanded mask as the union of the initial and generated masks: Mtrain port = Minit port Mgen. We then construct the two masked inputs used by the model: Iattr = Mattr, Iport = (1 Mtrain port ), (14) (15) where denotes element-wise multiplication. The mask Mtrain port defines the region into which the attribute will be inserted during generation, while Mattr localizes the original attribute region. This expansion process is attribute-aware in that it preserves the intended attribute category while diversifying its spatial extent. Unlike HairFusion (Chung et al., 2025), which expands masks using fixed heuristics specific to hair, our approach generalizes across multiple facial attributes and enables the model to learn spatially flexible yet semantically grounded transfer patterns. 6 arXiv preprint Reference image augmentation. To overcome the limited diversity inherent in self-reconstruction setups, we introduce an augmentation pipeline that improves robustness to pose, alignment, and appearance variations in attributeportrait pairs. Our approach introduces controlled perturbations to both the attribute-only image and the attribute-masked portrait image to simulate realistic spatial and photometric variability. Specifically, we apply random affine transformationsincluding translation, scaling, and rotationto induce spatial misalignment. To inpaint regions exposed by such transformations, we employ the FLUX outpainting model (Labs, 2024) for semantically consistent content completion. In addition, we apply color jittering operations such as random perturbation of tone, contrast, saturation, and hue to account for distribution shifts across image pairs. This augmentation strategy exposes the model to wider range of training configurations, leading to more robust attribute transfer and animation under real-world variations. 3."
        },
        {
            "title": "INFERENCE FRAMEWORK AND EXTENSIONS",
            "content": "Inference pipeline. At inference time, we automatically construct the attributeonly image Iattr and the attribute-masked portrait image Iport by applying segmentation masks predicted by Sapiens (Khirodkar et al., 2024) to the attribute image Iattr and the portrait image Iport, respectively. To improve spatial alignment between the attribute and portrait inputs, we generate an aligned attribute mask by animating 3D avatar of the attribute image using the pose of the portrait. Concretely, we leverage GAGAvatar (Chu & Harada, 2024), single-image-to-3D avatar pipeline, to reconstruct an avatar from Iattr, and animate it using the FLAME (Li et al., 2017) parameters (β, θ, ψ) of Iport estimated via EMOCA (Danˇeˇcek et al., 2022). From the resulting pose-aligned image Ialign attr using Sapiens. We then compute the union of this mask with the initial attribute mask extracted from the portrait image, denoted Minit port, to obtain the final candidate transfer region: Figure 3: Aligned Attribute Mask Estimation. To enhance spatial alignment between the attribute and the portrait input, we generate an aligned attribute mask by reenacting 3D avatar reconstructed from the attribute image. attr , we extract new attribute mask Malign Minfer port = Minit port Malign attr . (16) This updated mask is used to construct the attribute-masked portrait image Iport = Iport(1Minfer port ). Fig. 3 describes the overall mask estimation process. Then, we extract spatial features Fattr, Fport and semantic embeddings ϕattr, ϕport from the two masked reference images. Conditioned on these features and keypoint sequence, DNet synthesizes an animation of the target identity with the desired attribute transferred by iterative denoising following Eq. (10). Multi-attribute transfer. Our model supports zero-shot composition of multiple attributes by generalizing the spatial attention formulation in Eq. (6). Instead of using single attribute feature, we concatenate multiple attribute feature maps along the width dimension: t, Fl attr, , Fl,Nattr port, concatw attr, Fl,2 = SA Fl,1 (17) Fl (cid:17)(cid:17) Fl attr (cid:16) (cid:16) , where each Fl,k attr denotes the feature map extracted from the k-th attribute-only image using the Attribute ReferenceNet. To construct the final attribute-masked portrait in this setting, we also generalize the mask fusion process by taking the union of all aligned attribute masks: Minfer port = Minit port Nattr(cid:91) k=1 Malign,k attr , (18) attr where each Malign,k is the aligned mask extracted from the k-th attribute image. This composite mask is then used to remove all attribute regions from the portrait image before generation. The rest of the attention computation remains unchanged, allowing the model to jointly attend to all attributes and synthesize coherent multi-attribute compositions without retraining. 7 arXiv preprint Attribute interpolation. Our model enables zero-shot interpolation between two attributes of the same category (e.g., hairstyle and B) without additional training (Zhang et al., 2024a). Given two attribute-only images, we extract spatially attended features Fτ,l,1 using our spatial attention, and interpolate them as follows: Fτ,l = (1 α) Fτ,l, + α Fτ,l,2 and Fτ,l,2 (19) . , where α [0, 1] controls the interpolation ratio. The interpolated feature Fτ,l is then passed to DNet for generation. This enables smooth and semantically consistent transitions between attributes."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Table 1: Quantitative Comparison. We compare our method with recent approaches that (1) generate portrait images from hair images, and (2) animate the synthesized portrait image. Our method outperforms both state-of-the-art baselines across all metrics. Img.Gen. Animation L1 PSNR SSIM LPIPS FID PbE HairFusion StableHair TriplaneEdit Ours 0.1059 LivePortrait X-Portrait 0.1180 MegActor-(cid:80) 0.1268 0.1438 LivePortrait X-Portrait 0.1511 MegActor-(cid:80) 0.1650 0.1122 LivePortrait X-Portrait 0.1229 MegActor-(cid:80) 0.1301 0.1023 LivePortrait X-Portrait 0.1051 MegActor-(cid:80) 0.1248 0.0744 16.14 15.33 14.82 13.76 13.30 12.75 15.84 15.04 14.62 16.52 16.05 15. 18.83 0.5641 0.5270 0.4840 0.4801 0.4334 0.4138 0.5491 0.5114 0.4706 0.5511 0.5401 0.4828 0.2859 0.2978 0. 0.3792 0.3733 0.4015 0.3041 0.3117 0.3347 0.2924 0.2760 0.3293 0.6527 0.1565 40.63 59.20 62. 46.24 59.02 65.59 43.74 53.36 63.47 57.86 60.25 70.41 38.00 Table 2: Ablation Study. We evaluate the contributions of the key components and training strategies of our model. Bold text indicates the first rank, and underlined text denotes the second rank. Dataset. We train our model on the CelebV-Text (Yu et al., 2023), VFHQ (Xie et al., 2022), and Nersemble (Kirschstein et al., 2023) datasets, which together contain 2,747 videos. For evaluation, we randomly select 200 videos from VHFQ and CelebV-Text, including unseen identities, facial poses, and expressions. Since triplet datasets containing combinations of target attribute images, original portrait images, and edited portrait images are not available, we conduct quantitative evaluation under self-attribute transfer setting following HairFusion (Chung et al., 2025). Specifically, we sample two frames from each video as the portrait and attribute image, and generate animations conditioned on the facial keypoints. The outputs are then compared to groundtruth frames. single ReferenceNet w/o mask expansion w/o ref. image aug. w/o ref. mask input full ref. image input 0.1973 0.2073 0.2248 0.1670 0.1310 0.0813 0.0881 0.0900 0.0747 0.0670 0.6314 0.5915 0.5973 0.6511 0.6698 17.95 17.16 16.97 18.60 19.47 LPIPS PSNR SSIM Variant 0.0744 0.6527 0. L1 18.83 Ours Metrics We evaluate each method using standard metrics for fidelity and perceptual quality. To assess reconstruction fidelity, we report L1 distance, PSNR, SSIM, and LPIPS between generated frames and groundtruth. For perceptual realism, we compute the Fréchet Inception Distance (FID) (Parmar et al., 2022). 4.1 COMPARISON Baselines. As no prior work directly tackles portrait animation with attribute transfer from an in-the-wild reference, we construct two-stage baselines by combining imagelevel attribute transfer with video animation methods, resulting in 12 model combinations. For attribute transfer (stage 1), we consider: Paint-by-Example (PbE)Yang et al. (2023), mask-conditioned diffusion method for reference image insertion; HairFusionChung et al. (2025) and StableHair (Zhang et al., 2025), diffusion-based models for hairstyle Figure 4: Qualitative Comparison. We compare our method and the baselines that combine X-Portrait Xie et al. (2024) with face editing methods Chung et al. (2025); Yang et al. (2023); Zhang et al. (2025); Bilecen et al. (2024). 8 arXiv preprint transfer with and without masks; and TriplaneEdit (Bilecen et al., 2024), 3D-aware GAN-based face editor. For portrait animation (stage 2), we use: LivePortrait (Guo et al., 2024) (GAN-based keypoint animation), X-Portrait (Xie et al., 2024) (ReferenceNet + ControlNet), and MegActor-(cid:80) (Yang et al., 2025) (DiT-based video diffusion). Results. As shown in Table 1, our method consistently outperforms all baseline combinations across both fidelity and perceptual quality metrics. Fig. 4 presents qualitative comparison against baselines using X-Portrait (Xie et al., 2024) as the animation module (stage 2). Our method generates coherent and realistic hairstyle animations that preserve the identity and maintain consistency in spatial extent, shape, and fine details across frames. Refer to the supplementary material for additional qualitative comparisons with other baseline combinations."
        },
        {
            "title": "4.2 ABLATION STUDY",
            "content": "We evaluate the contributions of key components in our model and training strategy. Table 2 presents quantitative results, and Fig. 5 shows corresponding qualitative comparisons. single ReferenceNet replaces the dual-branch architecture with shared encoder that receives the portrait and attribute images concatenated along the channel dimension, following CATVTON (Chong et al., 2024). This setup fails to separate the roles of the two inputs, resulting in undesired blending of attribute and identity cues. w/o mask expansion omits the attribute-aware augmentation that simulates variations in spatial extent. Without this strategy, the model tends to rely on the default shape of the portraits original attribute mask, making it less capable of handling diverse attribute shapes during inference. w/o ref. image aug. disables spatial and photometric augmentations applied to the reference images during training. As result, the model fails to accurately transfer the desired attribute with misaligned reference images. w/o ref. mask input removes the binary mask concatenation from the inputs to the ReferenceNets. This weakens spatial localization and often leads to artifacts or residual traces of the original attribute in the output. full ref. image input uses unmasked portrait and attribute images during training. Interestingly, this variant achieves the best quantitative scores in Table 2, which evaluates the self-attribute transfer setting, since full images simplify the task by allowing the model to copy content more easily. However, as shown in Fig. 5, this model fails to disentangle identity and attribute roles, leading to visible identity leakage during cross-identity transfer. Ours achieves spatially consistent, identity-preserving results, and outperforms all other ablated variants except the full reference image variant. Figure 5: Ablation Study. Removing each component and varying the training strategy results in visible degradation of synthesis quality. 4.3 APPLICATION Multi-attribute Transfer. Our model supports the composition of multiple attributes (e.g., glasses, hat, hairstyle) in single generation pass by extending the spatial attention mechanism as described in Eq. (17). Fig. 6 show qualitative results where multiple attributes are simultaneously transferred from different reference images. Remarkably, our model not only combines multiple attributes seamlessly but also handles interactions between overlapping regions, such as between hair and hat. Despite the reference images Figure 6: Multi-Attribute Transfer. Our model supports composition of multiple attributes (e.g., hair, eyeglasses, beard, hat) in single forward pass. arXiv preprint exhibiting diverse lighting conditions and spatial alignments, the model successfully integrates all attributes into the portrait image while maintaining coherent and natural appearance. Attribute Interpolation. Our model supports the transfer of interpolated attributes between two different attribute images by linearly blending their reference features, as described in Eq. (19). Fig. 7 presents results for hair, beard, and eyeglasses, showing continuous transitions in both shape and appearance. The interpolations exhibit smooth changes in visual attributes such as color, volume, and style, demonstrating that our model effectively captures semantically meaningful directions in the attribute feature space."
        },
        {
            "title": "5 DISCUSSION",
            "content": "We present Durian, zero-shot framework for portrait animation with cross-identity attribute transfer. Our method generates realistic, temporally coherent videos conditioned on portrait image and reference image depicting the target attribute. By leveraging self-reconstruction training on in-the-wild video frames, along with our attribute-aware mask expansion and augmentation strategy, our model learns spatially consistent and semantically grounded attribute transfer without requiring triplet supervision. Our model can also be extended to multi-attribute composition and attribute interpolation in single generation pass without additional training. Figure 7: Attribute Interpolation. Our model enables smooth and consistent transitions between attributes. We present interpolation examples for hair, beard, and eyeglasses by varying the interpolation parameter α. However, several limitations remain. First, while our model supports multi-attribute transfer, complex interactions, such as hair occluded by hats, may not always be handled faithfully. Second, although we apply color-based augmentation during training, large lighting discrepancies between the portrait and attribute images can still degrade visual quality, particularly under strong shadows or highlights. Third, our training data primarily contains front-facing portraits; hence, transferring attributes from or to extreme poses (e.g., back views) remains an open challenge. Finally, because our method relies on accurate keypoint extraction and face tracking, failures in these components can lead to temporal jitter or artifacts during animation. Extending our approach to full-body scenarios and broader categories of attributes, such as clothing or accessories, presents an exciting direction for future work."
        },
        {
            "title": "REFERENCES",
            "content": "Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE International Conference on Computer Vision, 2019. Bahri Batuhan Bilecen, Yigit Yalin, Ning Yu, and Aysegul Dundar. Reference-based 3d-aware image editing with triplanes. arXiv preprint arXiv:2404.03632, 2024. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Lan Chen, Qi Mao, Yuchao Gu, and Mike Zheng Shou. Edit transfer: Learning image editing via vision in-context relations. arXiv preprint arXiv:2503.13327, 2025. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 10 arXiv preprint Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, Dongmei Jiang, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual try-on with diffusion models. arXiv preprint arXiv:2407.15886, 2024. Xuangeng Chu and Tatsuya Harada. Generalizable and animatable gaussian head avatar. Advances in Neural Information Processing Systems, 2024. Chaeyeon Chung, Sunghyun Park, Jeongho Kim, and Jaegul Choo. What to preserve and what to transfer: Faithful, identity-preserving diffusion-based hairstyle transfer. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. Radek Danˇeˇcek, Michael Black, and Timo Bolkart. Emoca: Emotion driven monocular face capture and animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, and Zheng-Jun Zha. Vivid: Video virtual try-on using diffusion models. arXiv preprint arXiv:2405.11794, 2024. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems, 2014. Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision models. In European Conference on Computer Vision, 2024. Gyeongman Kim, Hajin Shim, Hyunsu Kim, Yunjey Choi, Junho Kim, and Eunho Yang. Diffusion video autoencoders: Toward temporally consistent face video editing via disentangled video encoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Nießner. Nersemble: Multi-view radiance field reconstruction of human heads. ACM Transactions on Graphics, 2023. Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: plug-and-play framework for any video-to-video editing tasks. arXiv e-prints, pp. arXiv2403, 2024. arXiv preprint Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Tianye Li, Timo Bolkart, Michael Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4d scans. ACM Transactions on Graphics, 2017. Yuhan Li, Hao Zhou, Wenxiang Shang, Ran Lin, Xuanhong Chen, and Bingbing Ni. Anyfit: Controllable virtual try-on for any combination of attire across any scenario. arXiv preprint arXiv:2405.18172, 2024. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2015. Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, 2021. Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the International Conference on Machine Learning, 2015. Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Wensong Song, Hong Jiang, Zongxing Yang, Ruijie Quan, and Yi Yang. Insert anything: Image insertion via in-context editing in dit. arXiv preprint arXiv:2504.15009, 2025. Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video object insertion with precise motion control. arXiv preprint arXiv:2501.01427, 2025. Rotem Tzaban, Ron Mokady, Rinon Gal, Amit Bermano, and Daniel Cohen-Or. Stitch it in time: Gan-based facial editing of real videos. In ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 12 arXiv preprint Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchmark for video face super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM Transactions on Graphics (Proc. SIGGRAPH), 2024. Yiran Xu, Zhixin Shu, Cameron Smith, Seoung Wug Oh, and Jia-Bin Huang. In-n-out: Faithful 3d gan inversion with volumetric decomposition for face editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Shurong Yang, Huadong Li, Juhao Wu, Minhao Jing, Linze Li, Renhe Ji, Jiajun Liang, Haoqiang Fan, and Jin Wang. Megactor-sigma: Unlocking flexible mixed-modal control in portrait animation with diffusion transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. Xu Yao, Alasdair Newson, Yann Gousseau, and Pierre Hellier. latent transformer for disentangled In Proceedings of the IEEE International Conference on face editing in images and videos. Computer Vision, 2021. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial text-video dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Kaiwen Zhang, Yifan Zhou, Xudong Xu, Bo Dai, and Xingang Pan. Diffmorpher: Unleashing the capability of diffusion models for image morphing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE International Conference on Computer Vision, 2023. Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. Stable-makeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764, 2024b. Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, and Jiaming Liu. Stable-hair: Real-world hair transfer via diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, 2016. Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, 2024. arXiv preprint"
        },
        {
            "title": "A GUIDANCE GENERATION",
            "content": "Our model generates portrait animations by leveraging guidance video composed of facial keypoints, as shown in figure 2 of our main paper. These keypoints encode entangled facial shape information such as the distance between the eyes and ears, the interocular distance, and the distance from the eyes to the nose. While this rich representation contributes to accurate portrait animation in self-reconstruction scenarios, we observe that the animation tends to follow the facial shape of the guidance video in cross-identity reenactment scenarios. To address this, we propose method that preserves the facial shape of the portrait image while still transferring the facial motion from cross-identity guidance video. Specifically, we employ LivePortrait (Guo et al., 2024) to generate an animation video of the portrait image driven by the original guidance video from different identity. We then extract the facial keypoint guidance video from this animation using Sapiens (Khirodkar et al., 2024) facial keypoint estimation."
        },
        {
            "title": "B ADDITIONAL APPLICATION",
            "content": "Text-to-Image-to-Attribute Transfer for Portrait Animation Our method generates attribute transferred portrait animation video when an image containing the desired attribute is provided. We extend this capability by generating the attribute image from text prompt, allowing for attributetransferred portrait animation driven by textual descriptions, as shown in figure 8. We utilize the FLUX (Labs, 2024) text-to-image model to synthesize attribute images, which are then transferred to the portrait image to generate portrait animations."
        },
        {
            "title": "C ADDITIONAL QUALITATIVE COMPARISON",
            "content": "Extending the comparison presented in figure 4 of the main paper, we additionally provide qualitative results with other baseline combinations. Note that we generate portraits with transferred hair attributes using recent image insertion and face editing methods (Chung et al., 2025; Yang et al., 2023; Zhang et al., 2025; Bilecen et al., 2024), and compare the resulting animation videos produced by applying recent animation techniques (Guo et al., 2024; Xie et al., 2024; Yang et al., 2025) with those generated by our method, as shown in figure 9. 1 arXiv preprint Figure 8: Text-to-Image-to-Attribute Transfer for Portrait Animation. We demonstrate portrait animation with attribute transfer from textual description. We employ FLUX Labs (2024) to generate high-quality portrait image with the desired hair attribute from text prompt. Figure 9: Qualitative Comparison. We compare our method and the baselines that combine portrait animation method Guo et al. (2024); Xie et al. (2024); Yang et al. (2025) with face editing methods Chung et al. (2025); Yang et al. (2023); Zhang et al. (2025); Bilecen et al. (2024). arXiv preprint Figure 10: More Qualitative Results. We present additional results on hair, hat, eyeglasses, and beard attribute transfer for portrait animation. Our method preserves the fine details of the original portrait while achieving natural and seamless attribute transfer. 3 arXiv preprint Figure 11: More Multi-Attribute Transfer Results. We demonstrate the results of simultaneously transferring two attributes for portrait animation. Figure 12: More Multi-Attribute Transfer Results. We present results of simultaneously transferring three attributes. In each example, the image at the top-left corner indicates the portrait that receives the attribute transfer."
        }
    ],
    "affiliations": [
        "Seoul National University"
    ]
}