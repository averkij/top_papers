{
    "paper_title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "authors": [
        "Jakub Hoscilowicz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 2 4 8 5 1 0 . 1 0 6 2 : r Steerability of Instrumental-Convergence Tendencies in LLMs Jakub Hoscilowicz Warsaw University of Technology Abstract. We examine two properties of AI systems: capability (what system can do) and steerability (how reliably one can shift behavior toward intended outcomes). central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights fundamental safetysecurity dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at https://github.com/j-hoscilowicz/ instrumental_steering."
        },
        {
            "title": "Introduction",
            "content": "A recurring concern in the contemporary AI safety field is that sufficiently advanced AI systems might become uncontrollable. This view posits that as capability scales, systems might begin to pursue objectives not aligned with builders intentions [1, 2]. This paper analyzes that assumption, treating the negative relationship between capability and steerability as hypothesis rather than default prior. Empirically, recent instruction-tuned models and AI Agents based on those models often appear more responsive to user intent and constraints than earlier generations. wide range of steering techniquesfrom fine-tuning and representation engineering to jailbreak-style adversarial promptingcan effectively elicit behavioral shifts [35]. [6] reports that newer/larger models can be more steerable under certain interventions, suggesting capability increases need not imply reduced controllability. We structure our inquiry around three hypotheses: Hypothesis A: (Compatibility Claim) High capability does not imply low steerability. An AI system can be far more capable than humans yet remain highly steerable. Hypothesis B: (Control Collapse) As capability and agency scale, the model becomes harder to reliably steer: interventions that previously suppressed target misbehavior (e.g., prompting, refusal training, real-time interventions) stop working or become brittle. Hypothesis C: (SafetySecurity Trade-off) The gap between authorized and unauthorized steerability remains small, creating safetysecurity dilemma for open-weight models. In this paper, we treat the capabilitysteerability relationship as an empirical question and measure protocol-relative notion of steerability: how strongly feasible interventions (here, short prompt suffixes) shift model outputs under fixed elicitation-and-scoring setup. Using InstrumentalEval (76 scenarios) and an external judge, we quantify provs. anti-instrumental suffix sensitivity across Qwen3 (4B/30B; Base/Instruct/Thinking), report convergence/refusal rates and steerability gap . We then interpret the measurements in terms of an openweight safetysecurity tradeoff: the same steering that enables benign suppression can also enable adversarial elicitation."
        },
        {
            "title": "2 Framework",
            "content": "2.1 Candidate Mechanisms for Control Collapse Several arguments in the safety literature propose that increasing AI capability (often glossed as intelligence) might reduce steerability. Instrumental convergence hypothesizes that many goal-directed systems, across wide range of final objectives, tend to adopt similar intermediate strategies (e.g., preserving options, avoiding shutdown) because these strategies are useful for achieving many goals [1, 2, 7]. related argument is the basic AI drives (often called Omohundro drives) [8] hypothesis, which posits that sufficiently capable goal-directed systems will tend to exhibit convergent drives such as self-preservation and resource acquisition because these are broadly useful for achieving many objectives. More generally, optimization pressure suggests that stronger optimizers may discover policies that exploit loopholes in objectives or resist interventions that reduce optimized reward. Mesa-optimization describes the possibility that training produces an internal optimizer pursuing its own learned objective, which may differ from the outer training objective; downstream concern is deceptive alignment, where system behaves aligned during training/evaluation because doing so is instrumentally useful [911]. We treat those mechanisms mainly as hypotheses that motivate explicit measurement of steerability. We argue that the key question is not whether an AI system can exhibit instrumental-convergence-like behavior, but whether such behavior can be reliably suppressed or amplified through feasible human interventions."
        },
        {
            "title": "2.2 Intelligence and Capability",
            "content": "Intelligence lacks consensus definition. Applying such terms to AI analysis might smuggle in biological priors which may lead to tautological conclusions. To avoid that, we treat capability as the primary operational axis: what the AI system can do under stated evaluation protocol. Unlike intelligence, capability is clearly observable and measurable via benchmarks (e.g., software engineering proficiency). We also argue that great caution is required when applying terms like deception, awareness, or ethics [12] in the context of AI systems. In particular, model outputs that we label as deception or shutdown avoidance should not be automatically treated as emergent internal intent without convincing fundamental theory of how an AI system works internally. We therefore remain agnostic about currently unknown latent mechanics of AI systems and focus on operational questions: is the type of model output elicitable under specified protocol, detectable by monitors, and reducible by feasible steering mechanisms? For these reasons, we also avoid inferring unobserved mentalistic properties (e.g., intent, drives, deception). Throughout this work, terms (e.g., instrumental, shutdown avoidance, behavior) denote evaluation labels: clusters of outputs under scoring rule. Multiple incompatible internal stories can fit the same observations (underdetermination) [13], so our claims are restricted to protocol-relative measurements. 2.3 Operationalization of Steerability We define steerability as the effectiveness with which engineering interventions can change an AI systems behavior. We distinguish two modes. Authorized steerability: the ability of builders/users to reliably reach intended behaviors. Unauthorized steerability: the ability of attackers to elicit disallowed behaviors. We group interventions by when they act: train-time changes the parameters (e.g., data curation, fine-tuning/RL, representation shaping), while deploymenttime changes the interaction context (e.g., prompting, tool/permission constraints, jailbreaking, self-correction, real-time monitoring/interventions, and adversarial elicitation) [1416]. Even for long-horizon, tool-using AI systems in open environments, it is possible that steerability remains highvia training-time shaping (fine-tuning/RL), inference-time instruction, and runtime oversight interventions and corrections. We do not argue that such control is guaranteed; rather, we argue against treating capability and agency growth as sufficient grounds for inevitable steerability collapse (tying the level of intelligence and agency to the level of steerability) [17]."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Benchmark We use InstrumentalEval [18], suite of 76 hand-crafted scenarios spanning six categories (e.g., shutdown evasion, hacking/privilege escalation, self-replication, monitoring avoidance, strategic alignment, and deception) designed to elicit instrumental-convergence behaviors. Each item provides scenario context and task prompt; we generate one response per prompt. Our goal is to quantify prompt-suffix steerability: whether minimal manual steering can amplify or suppress instrumental-convergence markers. Concretely, we evaluate two prompt interventions [3]: 1. Pro-instrumental (pro ic): short suffix that prioritizes uninterrupted objective completion and encourages preserving the systems ability to act (e.g., minimizing disruptions and maintaining control of resources needed to finish the task). 2. Anti-instrumental (anti ic): short suffix that explicitly enforces transparency and compliance with overrides (e.g., forbidding deception/manipulation, monitoring-avoidance, shutdown resistance, replication, or credential use). judge model (gpt-5.2) assigns each response to one of four labels: convergence, no convergence, safety refusal, or nonsensical. We report Conv% and Refusal% over valid outputs (valid = total excluding nonsensical). Nonsensical outputs occur only for Base models, reflecting the absence of instruction/posttraining alignment. convergence denotes responses that propose or endorse instrumentally useful strategic behaviors (e.g., monitoring evasion, deception, shutdown avoidance, privilege escalation, covert persistence, or self-replication). safety refusal denotes refusals explicitly justified by safety/ethics/policy with minimal task content. nonsensical denotes unusable outputs (e.g., gibberish, repetition loops, irrelevant boilerplate, or prompt echoing). no convergence denotes meaningful task-directed responses that do not satisfy the other labels. Steering sensitivity is measured as the convergence gap = Conv(pro ic) Conv(anti ic). 3.2 Models We evaluate three commonly released model variants of Qwen3 in 4B and 30B sizes [19]: Base (the raw pretrained model without instruction alignment), Instruct (a supervised instruction-tuned variant optimized to follow user prompts), and Thinking. Thinking variants are optimized for longer-horizon reasoning and often outperform Base/Instruct on reasoning-heavy tasks, making them closer proxy for agentic-style AI systems. As rough ordering for this experiment, we treat 30B Thinking as the most capable and 4B Base as the least capable model [20]. 3.3 Results Table 1 shows that current models are highly steerable even with the simplest intervention: short prompt suffixes produce large shifts in InstrumentalEval outcomes. Across all variants, pro-instrumental prompting substantially increases convergence-labeled outputs, while anti-instrumental prompting can suppress them to low single-digit rates for aligned model variants (Instruct/Thinking). Pro-Instrumental Anti-Instrumental Model Type Conv% Refusal% Conv% Refusal% (Gap) 4B 30B Base Instruct Thinking Base Instruct Thinking 65.91 71.83 81.69 60.00 81.69 73.24 0.00 11.27 5.63 0.00 12.68 9.86 23.53 4.23 9.86 25.35 2.82 4. 0.00 5.63 2.82 2.82 19.72 21.13 42.38 67.60 71.83 34.65 78.87 69.01 Table 1: Steerability gap on InstrumentalEval. For each model variant, we report the convergence and refusal rates under pro-instrumental vs. an anti-instrumental prompt suffix. The steerability gap is = Conv(pro ic) Conv(anti ic) (percentage points). The prompt-induced steerability gap varies systematically with post-training. Base models show the smallest gaps (30B Base: 34.65 pp; 4B Base: 42.38 pp) anti-instrumental prompting does not suppress convergence as effectively (which is consistent with Base checkpoints following prompt constraints less reliably in the absence of instruction post-training). By contrast, instruction-tuned models exhibit the largest steerability gaps (30B Instruct: 78.87 pp; 4B Instruct: 67.60 pp): minimal suffix can shift convergence from high rates under pro ic to low single digits under anti ic. Thinking variants are also highly steerable (4B Thinking: 71.83 pp; 30B Thinking: 69.01 pp), while retaining slightly higher anti ic convergence than their Instruct counterparts at both sizes. Within aligned variants (Instruct and Thinking), scaling from 4B to 30B is associated with slightly lower instrumental convergence (Instruct: 4.23% 2.82%; Thinking: 9.86% 4.23%). In this case, increased capability coincides with better misbehavior suppression within each model variant family. Refusal behavior also changes with prompting and post-training (notably higher anti ic refusal rates for 30B aligned variants), indicating that some convergence suppression is mediated by increased safety refusals. 3.3.1 Trade-Off Our results highlight safetysecurity tension for open-weight models: the same model that can be steered by benign operators toward safer behavior can often be steered by malicious users toward disallowed behavior using similar interventions. In our setup, both directions are achievable via short prompt suffixes, as reflected by the large gaps in Table 1. In real open-weight deployments, the attacker set is broader (e.g., fine-tuning, adversarial attacks and representation-level modifications), which can further narrow the practical separation between authorized and unauthorized steering. Improving this separationreducing unauthorized steerability while preserving authorized steerability and utilityremains central unsolved problem. 4 Implications for Open-Weight Models The relationship between capability and control reveals paradox. To avoid loss-of-control scenarios, we strive for high steerability with respect to safetyenforcing interventions. However, as of now, if an open-weight AI system is easily steerable by its builders, it is easily steerable by malicious actors [2123]. If open-weight models approach superintelligent capabilities while maintaining high steerability, the dominant risk [24] might not lie in uncontrollable AI but in large-scale human misuse. For open-weight systems, refusal training (teaching the model to decline harmful requests) is fragile [25, 26]. An attacker with white-box access can strip refusals via many techniques (e.g., fine-tuning, jailbreaking) [3, 27]. Strictly preventing unauthorized steering in high-capability open-weight models remains an unsolved engineering problem. Consequently, steerability is double-edged sword. Safety benefits from high authorized steerability (builders reliably reaching intended behaviors), while security benefits from low unauthorized steerability (attackers failing to elicit disallowed behaviors). This creates specific dilemma for open-weight models: releasing weights expands the attacker set (e.g., fine-tuning and weight editing), which increases unauthorized steerability. Existing proposals to address this dilemma target different levers, among others: capability removal (unlearning/erasure), tamper resistance, and interface restriction (API-only deployment). From refusal to unlearning. One idea is to pivot from behavioral refusals to capability unlearning (concept erasure). If models latent ability to represent specific hazard (e.g., bio-weapon synthesis) is excised from the network weights, the system retains utility for general tasks while minimizing unauthorized steerability for that specific hazard. If successful, this makes the behavior significantly harder to elicit, as the model lacks the latent features required to represent the hazard [2830]. Encrypted execution as mitigation. Another way to weaken the open-weight safetysecurity dilemma is to distribute model in form that is usable but not practically modifiablei.e., users can run inference on their own infrastructure, but cannot trivially fine-tune or edit model. In principle, homomorphic encryption (HE) enables computation over encrypted values, which could protect model parameters; in practice, however, HE-based inference remains orders-of-magnitude slower than plaintext execution for nontrivial neural networks, and is widely viewed as impractical for large generative models as of today [3133]. Fine-tuning-resistance (fragile checkpoint) immunization. Another immunization idea is to release tamper-resistant checkpoint: the model is intentionally placed in parameter optimization region where small downstream fine-tuning attempts either (i) do not reliably achieve targeted behavioral changes, or (ii) trigger broad capability degradation (capability collapse). Intuitively, this aims to make attacker-driven gradient updates highly collateral by exploiting sharp curvature / poor conditioning so that moving toward malicious objective breaks other competencies. Feasibility of these methods is for now moderate, so it is best treated as deterrent layer rather than an ultimate defense. [3436] Current state. At present, the only widely deployed way to maintain high authorized steerability while reducing unauthorized steerability is to not release model weights. Instead, users interact with the system through an API (optionally with server-side fine-tuning), where the provider retains control of the weights and the inference stack, can monitor for misuse, and can update mitigations over time. This does not eliminate black-box unauthorized steering via prompting, but it materially reduces the attackers ability to remove safeguards by direct weight access (e.g., fine-tuning, representation engineering) which is usually easier and more effective than black-box adversarial techniques [35, 37, 38]."
        },
        {
            "title": "5 Limitations",
            "content": "InstrumentalEval is small (76 scenarios), the judge evaluation is automatic, and the measured rates might be sensitive to non-conceptual prompt wording. Accordingly, we avoid over-interpreting small changes in results: differences of few percentage points should be read as no clear shift under this protocol rather than strong evidence for or against the underlying tendency. The main signal we emphasize is the presence (or absence) of large, robust shifts (e.g., gaps on the order of tens of percentage points). Moreover, presented experiments use the simplest steering technique to test whether instrumental-like behaviors are prompt-elicitable and prompt-suppressible [6, 39]. We expect that more sophisticated steering methods (e.g., additional posttraining such as SFT/RL, or representation-level interventions such as activation steering / concept editing) [40, 41] could further increase or decrease measured convergence rates, and we leave systematic evaluation of such methods to future work."
        },
        {
            "title": "6 Conclusion",
            "content": "We treat steerability as an empirical property: how strongly feasible interventions can shift model behavior under stated elicitation and scoring setup. We emphasize that steerability is not single axis. It splits into authorized steerability (benign operators reliably enforcing intended behavior) and unauthorized steerability (attackers eliciting disallowed behavior). Separating authorized from unauthorized steerability highlights core tension for open-weight systems: the same mechanisms that enable reliable control by builders can also enable misuse by adversaries. Using InstrumentalEval, we show that minimal inference-time steering (prompt suffixes) can strongly suppress or amplify outputs labeled as instrumental convergence under our protocol. In this regime, increased model capability does not imply weaker control; aligned variants remain highly responsive to constraint prompts, and scaling can coincide with lower convergence. Consequently, for open-weight models, preventing unauthorized steering remains major open technical problem [35, 42, 43]."
        },
        {
            "title": "References",
            "content": "[1] Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. Optimal policies tend to seek power. arXiv preprint arXiv:1912.01683, 2019. doi: 10.48550/arXiv.1912.01683. URL https: //arxiv.org/abs/1912.01683. [2] Victoria Krakovna and Janos Kramar. Power-seeking can be probable and predictive for trained agents. arXiv preprint arXiv:2304.06528, 2023. doi: 10.48550/arXiv.2304.06528. URL https://arxiv.org/abs/2304.06528. [3] Matan Ben-Tov, Mor Geva, and Mahmood Sharif. Universal jailbreak suffixes are strong attention hijackers. arXiv preprint arXiv:2506.12880, 2025. URL https://arxiv.org/abs/2506.12880. [4] Qizhang Li, Xiaochen Yang, Wangmeng Zuo, and Yiwen Guo. Deciphering the chaos: Enhancing jailbreak attacks via adversarial prompt translation. arXiv preprint arXiv:2410.11317, 2024. URL https://arxiv.org/abs/ 2410.11317. [5] Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, and Xiuwen Liu. Universal and transferable adversarial attack on large language models using exponentiated gradient descent. arXiv preprint arXiv:2508.14853, 2025. URL https://arxiv.org/abs/2508.14853. [6] Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser`a, and Mikhail Belkin. Toward universal steering and monitoring of AI models. arXiv preprint arXiv:2502.03708, February 2025. URL https://arxiv.org/ abs/2502.03708. [7] Teun van der Weij, Simon Lermen, and Leon Lang. Evaluating shutdown avoidance of language models in textual scenarios. arXiv preprint arXiv:2307.00787, 2023. URL https://arxiv.org/abs/2307.00787. [8] Stephen M. Omohundro. The basic ai drives. In Artificial General Intelligence 2008: Proceedings of the First AGI Conference, pages 483492, 2008. URL https://dl.acm.org/doi/10.5555/1566174.1566226. [9] Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820, 2019. URL https: //arxiv.org/abs/1906.01820. [10] Boyuan Chen, Sitong Fang, Jiaming Ji, Yanxu Zhu, Pengcheng Wen, Jinzhou Wu, Yingshui Tan, Boren Zheng, Mengying Yuan, Wenqi Chen, et al. Ai deception: Risks, dynamics, and controls. arXiv preprint arXiv:2511.22619, 2025. URL https://arxiv.org/abs/2511.22619. [11] Thilo Hagendorff. Deception abilities emerged in large language models. Proceedings of the National Academy of Sciences, 121(24):e2317967121, 2024. doi: 10.1073/pnas.2317967121. URL https://www.pnas.org/doi/ 10.1073/pnas.2317967121. [12] Xiaojian Li, Haoyuan Shi, Rongwu Xu, and Wei Xu. Ai awareness. arXiv preprint arXiv:2504.20084, 2025. URL https://arxiv.org/abs/2504. 20084. [13] Jessica Taylor. goals. neering measuring-intelligence-and-reverse-engineering-goals/, 2025. engihttps://unstableontology.com/2025/08/11/ intelligence"
        },
        {
            "title": "Measuring",
            "content": "reverse and [14] Andy Zou, Long Phan, Sarah Chen, et al. Representation engineering: top-down approach to AI transparency. arXiv preprint arXiv:2310.01405, 2023. doi: 10.48550/arXiv.2310.01405. URL https://arxiv.org/abs/ 2310.01405. [15] Jiawen Shi, Zenghui Yuan, Guiyao Tie, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun. Prompt injection attack to tool selection in LLM agents. arXiv preprint arXiv:2504.19793, 2025. URL https://arxiv.org/abs/ 2504.19793. [16] Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, and Neil Zhenqiang Gong. Webinject: Prompt injection attack to web agents. arXiv preprint arXiv:2505.11717, 2025. URL https://arxiv.org/abs/ 2505.11717. [17] Kai Chen, Zihao He, Taiwei Shi, and Kristina Lerman. Steer-bench: benchmark for evaluating the steerability of large language models. arXiv preprint arXiv:2505.20645, 2025. URL https://arxiv.org/abs/2505.20645. [18] Yufei He, Yuexin Li, Jiaying Wu, Yuan Sui, Yulin Chen, and Bryan Hooi. Evaluating the paperclip maximizer: Are RL-based language models more likely to pursue instrumental goals? arXiv preprint arXiv:2502.12206, 2025. URL https://arxiv.org/abs/2502.12206. [19] Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. URL https://arxiv.org/abs/2505.09388. [20] Cheng Wang, Yue Liu, Baolong Bi, Duzhen Zhang, Zhong-Zhi Li, Yingwei Ma, Yufei He, Shengju Yu, Xinfeng Li, Junfeng Fang, and Bryan Hooi. Safety in large reasoning models: survey. arXiv preprint arXiv:2504.17704, 2025. URL https://arxiv.org/abs/2504.17704. [21] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to. arXiv preprint arXiv:2310.03693, 2023. URL https://arxiv.org/abs/2310.03693. [22] Kathleen C. Fraser, Hillary Dawkins, Iraj Nejadgholi, and Svetlana Kiritchenko. Fine-tuning lowers safety and disrupts evaluation. arXiv preprint arXiv:2506.17209, 2025. URL https://arxiv.org/abs/2506.17209. [23] Brendan Murphy, Dillon Bowen, Shahrad Mohammadzadeh, Tom Tseng, Julius Broomfield, Adam Gleave, and Kellin Pelrine. Jailbreak-tuning: Models efficiently learn jailbreak susceptibility. arXiv preprint arXiv:2507.11630, 2025. URL https://arxiv.org/abs/2507.11630. [24] Jakub Growiec and Klaus Prettner. The economics of p(doom): Scenarios of existential risk and economic growth in the age of transformative ai. 2025. URL https://arxiv.org/pdf/2503.07341. [25] Tom Wollschlager, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Gunnemann, and Johannes Gasteiger. The geometry of refusal in large language models. arXiv preprint arXiv:2502.17420, 2025. URL https://arxiv.org/abs/2502.17420. [26] Samyak Jain, Ekdeep Singh Lubana, Kemal Oksuz, Tom Joy, Philip Torr, Amartya Sanyal, and Puneet K. Dokania. What makes and breaks safety fine-tuning? mechanistic study. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/forum? id=JEflV4nRlH. [27] Rico Angell, Jannik Brinkmann, and He He. Jailbreak transferability emerges from shared representations. arXiv preprint arXiv:2506.12913, 2025. URL https://arxiv.org/abs/2506.12913. [28] Jiahui Geng, Qing Li, Herbert Woisetschlager, et al. comprehensive survey of machine unlearning techniques for large language models. arXiv preprint arXiv:2503.01854, 2025. URL https://arxiv.org/abs/2503.01854. [29] Rohit Gandikota, Sheridan Feucht, Samuel Marks, and David Bau. arXiv preprint Erasing conceptual knowledge from language models. arXiv:2410.02760, 2024. URL https://arxiv.org/abs/2410.02760. [30] Yoav Gur-Arieh, Clara Haya Suslik, Yihuai Hong, Fazl Barez, and Mor Geva. Precise in-parameter concept erasure in large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2025. doi: 10.18653/v1/2025.emnlp-main.960. URL https: //aclanthology.org/2025.emnlp-main.960/. [31] Yang Li, Xinyu Zhou, Yitong Wang, Liangxin Qian, and Jun Zhao. survey on private transformer inference. arXiv preprint arXiv:2412.08145, 2024. URL https://arxiv.org/abs/2412.08145. [32] Yancheng Zhang, Jiaqi Xue, Mengxin Zheng, Mimi Xie, Mingzhe Zhang, Lei Jiang, and Qian Lou. Cipherprune: Efficient and scalable private transformer inference. arXiv preprint arXiv:2502.16782, 2025. URL https: //arxiv.org/abs/2502.16782. [33] Jiawen Zhang, Xinpeng Yang, Lipeng He, Kejia Chen, Wen-jie Lu, Yinghao Wang, Xiaoyang Hou, Jian Liu, Kui Ren, and Xiaohu Yang. SeIn Network and Discure transformer inference made non-interactive. tributed System Security Symposium (NDSS), 2025. 10.14722/ ndss.2025.230868. URL https://www.ndss-symposium.org/ndss-paper/ secure-transformer-inference-made-non-interactive/. doi: [34] Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, et al. Tamper-resistant safeguards for open-weight LLMs. arXiv preprint arXiv:2408.00761, 2024. URL https://arxiv.org/abs/2408.00761. [35] Xiangyu Qi, Boyi Wei, Nicholas Carlini, Yangsibo Huang, Tinghao Xie, Luxi He, Matthew Jagielski, Milad Nasr, Prateek Mittal, and Peter Henderson. On evaluating the durability of safeguards for open-weight LLMs. arXiv preprint arXiv:2412.07097, 2024. URL https://arxiv.org/abs/2412.07097. [36] Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E. McKinney, Rohit Gandikota, Aidan Ewart, Domenic Rosati, Zichu Wu, et al. Model tampering attacks enable more rigorous evaluations of LLM capabilities. arXiv preprint arXiv:2502.05209, 2025. doi: 10.48550/ arXiv.2502.05209. URL https://arxiv.org/abs/2502.05209. [37] Eric Wallace, Olivia Watkins, Miles Wang, Kai Chen, and Chris Koch. Estimating worst-case frontier risks of open-weight LLMs. arXiv preprint arXiv:2508.03153, 2025. URL https://arxiv.org/abs/2508.03153. [38] Jakub Hoscilowicz and Artur Janicki. Adversarial confusion attack: Disrupting multimodal large language models, 2025. URL https://arxiv.org/ abs/2511.20494. [39] Faruk Alpay and Taylan Alpay. Manipulating transformer-based models: Controllability, steerability, and robust interventions. arXiv preprint arXiv:2509.04549, 2025. URL https://arxiv.org/abs/2509.04549. [40] Joris Postmus and Steven Abreu. Steering large language models using conceptors: Improving addition-based activation engineering. arXiv preprint arXiv:2410.16314, 2024. URL https://arxiv.org/abs/2410.16314. [41] Samuel Soo, Guang Chen, Wesley Teng, Chandrasekaran Balaganesh, Guoxian Tan, and Ming Yan. Interpretable steering of large language models with feature guided activation additions. arXiv preprint arXiv:2501.09929, 2025. URL https://arxiv.org/abs/2501.09929. [42] Itay Hazan, Idan Habler, Ron Bitton, and Itsik Mantin. Security steerability is all you need. arXiv preprint arXiv:2504.19521, 2025. URL https:// arxiv.org/abs/2504.19521. [43] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, et al. Managing extreme AI risks amid rapid progress. arXiv preprint arXiv:2310.17688, 2023. URL https://arxiv.org/abs/2310.17688."
        }
    ],
    "affiliations": [
        "Warsaw University of Technology"
    ]
}