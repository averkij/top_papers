{
    "paper_title": "TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning",
    "authors": [
        "Giuseppe Paolo",
        "Abdelhakim Benechehab",
        "Hamza Cherkaoui",
        "Albert Thomas",
        "Balázs Kégl"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two levels or require centralized training, which limits their practical applicability. We introduce TAME Agent Framework (TAG), a framework for constructing fully decentralized hierarchical multi-agent systems.TAG enables hierarchies of arbitrary depth through a novel LevelEnv concept, which abstracts each hierarchy level as the environment for the agents above it. This approach standardizes information flow between levels while preserving loose coupling, allowing for seamless integration of diverse agent types. We demonstrate the effectiveness of TAG by implementing hierarchical architectures that combine different RL agents across multiple levels, achieving improved performance over classical multi-agent RL baselines on standard benchmarks. Our results show that decentralized hierarchical organization enhances both learning speed and final performance, positioning TAG as a promising direction for scalable multi-agent systems."
        },
        {
            "title": "Start",
            "content": "TAG: Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning Giuseppe Paolo 1 Abdelhakim Benechehab 1 Hamza Cherkaoui 1 Albert Thomas 1 Balázs Kégl 1 5 2 0 2 4 2 ] . [ 2 5 2 4 5 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two levels or require centralized training, which limits their practical applicability. We introduce TAME Agent Framework (TAG), framework for constructing fully decentralized hierarchical multi-agent systems. TAG enables hierarchies of arbitrary depth through novel LevelEnv concept, which abstracts each hierarchy level as the environment for the agents above it. This approach standardizes information flow between levels while preserving loose coupling, allowing for seamless integration of diverse agent types. We demonstrate the effectiveness of TAG by implementing hierarchical architectures that combine different RL agents across multiple levels, achieving improved performance over classical multiagent RL baselines on standard benchmarks. Our results show that decentralized hierarchical organization enhances both learning speed and final performance, positioning TAG as promising direction for scalable multi-agent systems. 1. Introduction Human societies are organized as hierarchical networks of agents, ranging from organizational structures (junior employees middle managers CEO) to ontological relationships (individuals families nations). This hierarchical organization facilitates complex coordination by decomposing problems across multiple scales while ensuring robustness through localized failure handling. As *Equal contribution nologies France. <giuseppe.g.paolo@gmail.com>. 1Noahs Ark Lab, Huawei TechCorrespondence to: Giuseppe Paolo TAG codebase available at: https://github.com/ GPaolo/TAG_Framework 1 Figure 1: Threeand two-level hierarchical agents used in the four-agent MPE-Spread environment. Yellow boxes represent the hierarchy levels, while blue connections indicate what each agent perceives as its environment. Red connections illustrate how the agents in the real environment are controlled, and green boxes represent the goals that the agents must reach. proposed in the TAME approach (Levin, 2022), biological systems also function as hierarchical networks of agents, where higher-level agents coordinate lower-level ones. Each level exhibits varying degrees of cognitive sophistication, corresponding to the scale of the goals it can pursue. From single cells managing basic homeostasis to tissues coordinating morphogenesis to brains overseeing complex behaviors, each level builds upon and integrates the intelligence of its components to achieve increasingly sophisticated cognitive capabilities. However, implementing similar hierarchical structures in artificial systems presents several key challenges: (1) coordinating information flow between levels without centralized control, (2) enabling efficient learning despite the non-stationarity introduced by the simultaneous adaptation of agents at multiple levels, and (3) maintaining scalability as the depth of the hierarchy increases. Formally, we consider the challenge of learning in multiagent systems where agents must collaborate to solve complex tasks, each maximizing their own expected returns. In this setting, each agent receives its own reward. As inTAG: TAME Agent Framework creases, the joint action and state spaces grow exponentially, rendering centralized approaches intractable. Moreover, agents must learn to coordinate across different temporal and spatial scales, ranging from immediate reactive behaviors to long-term strategic planning. Current AI systems predominantly rely on monolithic architectures that limit their adaptability and scalability in addressing these challenges. This is evident in large language models (LLMs) and traditional reinforcement learning (RL) approaches where agents are typically defined as single, end-to-end trainable instances. Such monolithic designs present several limitations: they require complete retraining when conditions change, lack the natural compositionality of hierarchical systems, and scale poorly with increasing task complexity. Traditional multi-agent approaches based on centralized training with decentralized execution or twolevel hierarchies with manager/worker structures struggle in such situations due to the high dimensionality of the states, limiting their applicability to small number of agents. At the same time, strategies consisting of independent learners with communication protocols are less afflicted by this, but suffer from possible communication overhead. Our key insight is that biological systems address similar coordination challenges through flexible, multi-scale hierarchical organization. We propose that future intelligent systems should be structured more like societies of agents than as monolithic entities. Our long-term goal is to build agents that resemble hierarchical and dynamic networks of sub-agents, rather than static structures. In this work, we take the first step in that direction with the introduction of the TAME Agent Framework (TAG), which draws inspiration from TAMEs biological insights (Levin, 2022) to create hierarchical multi-agent RL framework that enables the construction of arbitrarily deep agent hierarchies. The core innovation of TAG is the LevelEnv abstraction, which facilitates the construction of multi-level multi-agent systems. Through this abstraction, each agent in the hierarchy interacts with the level below as if it were its environmentobserving it through state representations, influencing it through actions, and receiving rewards based on the lower levels performance. The resulting system consists of multiple horizontal levels, as shown in Fig. 1, each containing one or more sub-agents, loosely connected to both their upper-level counterparts and their lower-level components. This structure reduces communication overhead and state space size by connecting agents locally within the hierarchy. TAG introduces several key innovations: 1. LevelEnv abstraction that standardizes information flow between levels while preserving agent autonomy, by presenting each level of the hierarchy as the environment to the level above; 2. flexible communication protocol that enables coordination without requiring centralized control; 3. Support for heterogeneous agents across levels, allowing different learning algorithms to be deployed where most appropriate. This approach enables more efficient learning by naturally decomposing tasks across multiple scales while maintaining scalability through loose coupling between levels. We demonstrate the effectiveness of TAG through empirical validation on standard multi-agent reinforcement learning (MARL) benchmarks, where we instantiate multiple twoand three-level hierarchies. The experiments show improved sample efficiency and final performance compared to both flat and shallow multi-agent baselines. In the following sections, we first review related work in both MARL (Sec.2.1) and HRL (Sec.2.2). We then present the TAG framework, including our key LevelEnv abstraction, in Sec.3. Sec.5 provides empirical validation on standard benchmarks for multiple instantiations of agents. We conclude with discussion of implications and future directions in Sec.6 and Sec.7. 2. Related Works 2.1. Multi-Agent Reinforcement Learning Research in multi-agent systems has gained significant attention in recent years (Nguyen et al., 2020; Oroojlooy & Hajinezhad, 2023). Leibo et al. (2019) proposed that innovation in intelligent systems emerges through social interactions via autocurriculanaturally occurring sequences of challenges resulting from competition and cooperation between adaptive units, which drive continuous innovation and learning. The authors argue that advancing intelligent systems requires strong focus on multi-agent research. To support this growing field, several benchmarks have emerged (Samvelyan et al., 2019; Hu et al., 2021; Bettini et al., 2024; Terry et al., 2021). Terry et al. (2021) introduced PettingZoo, which provides standardized OpenAI Gym-like (Brockman, 2016) interface for multi-agent environments, while Bettini et al. (2024) introduced BenchMARL, which addresses fragmentation and reproducibility challenges by offering comprehensive benchmarking tools and standardized baselines. MARL approaches can be broadly categorized into three main groups based on their coordination strategy: 1. Independent learners operate without inter-agent communication, with each agent maintaining its own learning algorithm and treating other agents as part of the environment. Common examples include IPPO (De Witt et al., 2020), IQL (Thorpe, 1997), and ISAC (Bettini 2 TAG: TAME Agent Framework et al., 2024), which are independent adaptations of their single-agent counterparts: PPO (Schulman et al., 2017), Q-Learning (Watkins & Dayan, 1992), and SAC (Haarnoja et al., 2018) respectively; 2. Parameter sharing approaches have agents share components like critics or value functions, as in MAPPO (Yu et al., 2022), MASAC (Bettini et al., 2024), and MADDPG (Lowe et al., 2017); 3. Communicating agents actively exchange information, either through consensus-based approaches (Cassano et al., 2020; Zhang et al., 2018) where agents must reach agreement over communication network, or through learned communication protocols (Foerster et al., 2016; Jorge et al., 2016). For comprehensive taxonomy and review, we refer readers to Oroojlooy & Hajinezhad (2023). significant challenge in MARL is the non-stationarity of the environment from each agents perspective. As other agents learn and change their behaviors, the state transition dynamics also change. This impacts experience replay mechanisms, as stored experiences quickly become obsolete (Foerster et al., 2016). The dominant paradigm of centralized learning with decentralized execution (Oroojlooy & Hajinezhad, 2023) attempts to address these challenges through shared learning components. However, this approach constrains the architecture during training and limits applicability to lifelong learning scenarios. 2.2. Hierarchical Reinforcement Learning Hierarchical organization is fundamental to intelligent behavior in nature. Human infants naturally decompose complex tasks into hierarchical goal structures (Spelke & Kinzler, 2007), enabling both temporal and behavioral abstractions. This hierarchical approach offers two key advantages: it improves credit assignment through abstractionbased value propagation and enables more semantically meaningful exploration through temporal and state abstraction (Hutsebaut-Buysse et al., 2022). Nachum et al. (2019) demonstrates that this enhanced exploration capability is one of the major benefits of hierarchical RL over flat RL approaches. The foundational approaches to HRL focus on two-level architectures. The Options framework formalizes temporal abstraction through Semi-Markov Decision Processes (SMDPs), where temporally-extended actions (\"options\") consist of policy, termination condition, and initiation set (Sutton et al., 1999). The framework supports concurrent option execution and allows for option interruption, providing flexibility beyond simple hierarchical structures. While options were initially predefined (Sutton et al., 1999), later work enabled learning them with fixed high-level policies (Silver & Ciosek, 2012; Mann & Mannor, 2014) or through end-to-end training, as in Option-Critic (Bacon et al., 2017). An alternative approach, Feudal RL (Dayan & Hinton, 1992; Kumar et al., 2017; Vezhnevets et al., 2017), implements manager-worker architecture where managers provide intrinsic goals to lower-level workers. This creates bidirectional information hidingmanagers need not represent low-level details, while workers focus solely on their immediate intrinsic rewards without requiring access to high-level goals. These approaches face common challenge: the nonstationarity of the lower level during learning complicates value estimation for the higher level. Model-based approaches attempt to address thisXu & Fekri (2021) learn symbolic models for high-level planning, while Li et al. (2017) build on MAXQs value function decomposition by breaking down the global MDP into taskspecific local MDPs. However, these typically require handspecified state abstractions or task decompositions. Recent work focuses on learning stability, with Luo et al. (2023) introducing attention-based reward shaping to guide exploration, and Hu et al. (2023) developing uncertainty-aware techniques to handle distribution shifts between levels. The multi-agent setting introduces additional complexity, as hierarchical coordination must now handle both temporal and agent-to-agent dependencies. Tang et al. (2018) addresses this through temporal abstraction with specialized replay buffers to handle the resulting non-stationarity. Meanwhile, Zheng & Yu (2024) introduces hierarchical reward machines but require significant domain knowledge. The scarcity of work combining HRL and MARL highlights the challenges of stable learning with multiple sources of non-stationarity. Our approach, TAG, departs from traditional hierarchical frameworks by directly learning to shape lower-level observation spaces, rather than explicitly assigning goals like Feudal RL. This is directly inspired by the work of Levin (2022), which proposes that in biological systems, local environmental changes drive coordinated responses without central control. The closest approach to our work is FMH (Ahilan & Dayan, 2019), but in this work, the agent is limited to shallow two-depth hierarchies and has only topbottom information flow in the form of goals. In contrast, TAG supports arbitrary-depth hierarchies without requiring explicit task specifications, and the communication across levels relies on bottom-up messages and top-down actions modifying the observations of the agents, rather than providing them goals. In this way, TAG offers flexible solution for multi-agent coordination. 3 TAG: TAME Agent Framework 3. TAG Framework The TAG framework addresses scenarios where multiple agents collaborate to maximize individual rewards over Markov Decision Process (MDP), which we refer to as the real environment. Inspired by biological systems, as described in TAME (Levin, 2022), TAG implements hierarchical multi-agent architecture where higher-level agents coordinate lower-level ones, each with varying cognitive sophistication matching their goal complexity. As shown in Fig. 1, at its core, TAG organizes agents into levels, where each level perceives and interacts only with the level directly below it. While agents at the lowest level operate directly in the real environment MDP, agents at higher levels perceive and interact with increasingly abstract representations of the system through the LevelEnv construct. This structure facilitates both horizontal (intra-level) and vertical (inter-level) coordination, allowing higher levels to maintain strategic oversight without requiring detailed knowledge of lowerlevel behaviors, while influencing lower levels through actions that modify their environmental observations. The frameworks key innovation is the LevelEnv abstraction, which transforms each hierarchical layer into an environment for the agents above it. This abstraction reshapes the original MDP into series of coupled decision processes, with each level operating on its own temporal and spatial scale. Within this structure, agents optimize their individual rewards while contributing to the overall system performance through the hierarchical arrangement. feedback TAG enables bidirectional information flow: moves upward through the hierarchy via agent communications, while control flows downward through actions that shape lower-level observations. This design preserves modularity between levels while facilitating coordination and integrates heterogeneous agents whose capabilities match the complexity requirements of their respective levels. 3.1. Formal Framework Definition TAG-hierarchy consists of ordered levels, with each 1, . . . , ωl level containing Nl parallel agents [ωl ]. Within Nl the hierarchy, each agent ωl is connected to agents in the levand 1 els immediately above and below. We define +1 as the sets of indices of agents connected to ωl from levels + 1 and 1, respectively. Each agent ωl is characterized by an observation space Ol lower-level agents into single observation: ol [ml1j]j that aggregates messages from = ; ) {Get messages} i, rl ϕl ) {Get actions} (Actions from level above) , ol1 Algorithm 1 LevelEnv .step() 1: Input: al+1 i(al+1 2: al πl 3: ol1, rl1 env.step(al) {Step lower level} , rl1 i(ol1 4: ml 5: if training then for agent ωl 6: 7: 8: 9: 10: end if 0, . . . , ml 11: ol = [ml Nl 0, . . . , rl 12: rl = [rl ] {Make reward} Nl 13: Return: ol, rl agent.store(al+1 agent.update() Level do , ol1 ] {Make observation} i, rl1 end for , al ) ); flowing messages and rewards based on observarewards, and internal states: mli, rli = tions, i(ol1 , rl1 ϕl policy πl that selects actions based on loweri = level observations and higher-level actions: al i(al+1 πl , ol1 ). The reward structure reflects this hierarchical decomposition: while the lowest-level agents receive rewards directly from the real environment, higher-level agents (ωl) receive rewards computed by the communication function ϕl1 of the agents in the levels below, based on their own performance. This creates cascade of reward signals that aligns the objectives of the individual agents with the overall goal of the system, which is optimizing performance in the real environment. During training, each agent stores its experiences and updates its policy based on the received rewards, enabling the entire hierarchy to learn coordinated behavior. The LevelEnv abstraction standardizes information exchange between levels while preserving their independence. As detailed in Alg. 1, at each step, agents at level generate messages and rewards through their communication functions and influence lower levels through their policies. This enables coordinated behavior through bidirectional information flow while maintaining the autonomy of the implementation of each level. 3.2. Information Flow and Agent Interactions Information in TAG flows through continuous cycle between adjacent levels, facilitated by the LevelEnv abstraction. This flow can be characterized by two distinct pathways: bottom-up and top-down, as illustrated in Fig. 2. an action space Al lower-level agents; for influencing the observations of communication function ϕl that generates upwardBottom-up Flow Information ascends the hierarchy from the real environment at the bottom through all the successive levels until the top. At each timestep, agents at level receive 4 TAG: TAME Agent Framework The actions do not directly control the agents at lower levels but instead modify their observation space, subtly influencing their behavior while preserving their autonomy. This indirect influence mechanism is crucial as it allows higher levels to guide lower levels toward desired behaviors without needing to specify exact goals, similar to how biological systems maintain coordination across scales, while preserving the environmental abstraction at each level. 3.3. Learning and Adaptation The learning process in TAG naturally accommodates the hierarchical structure instantiated by the framework. Each agent learns two key functions: policy π for generating actions, and communication function ϕ for generating messages and rewards. The policy learns to map the combination of received actions and observations to actions for the level below, while the communication function learns to extract and transmit relevant information to higher levels. The modular design of the framework allows agents at each level to learn independently using appropriate algorithms for their specific roles. This flexibility accommodates wide range of learning approaches, from simple Q-learning to sophisticated policy gradient methods. During training, each agent stores its experiences and updates its policy based on received rewards, as shown in Alg. 1. This independent learning capability enables the framework to adapt more easily to different scenarioslower levels might employ basic reactive policies, while higher levels can use advanced planning algorithms. Figure 2: Representation of the information flows between level with two agents and the levels above and below. The top-down flow of actions is shown in blue. The bottom-up flux of messages and rewards is shown in red and green, respectively. messages ml1 and rewards rl1 from level 1, defined as: (cid:40) ol1 = [ml1 0 rl1 = [rl , . . . , ml1 Nl1 , . . . , rl1 ] Nl1 0 ] where Nl1 represents the number of agents at level 1. Each message ml1 encodes both environmental state and internal agent state information. Agents ωl through their communication function: process information from their subordinate agents (ml i, rl i) = ϕl i(ol1 , rl1 ), 3.4. Scalability and Flexibility ]jI 1 = [ml1 where ol1 represent the collections of messages and rewards directed to agent i. Finally, level returns to level + 1 its observations ol = [ml ] and rewards rl = [rl = [rl1 and rl1 ]jI 1 ]. i 0, ..., ml Nl 0, ..., rl Nl The strength of this framework lies in how messages are processed and transformed. Rather than simply relaying raw observations, agents can learn to extract and communicate relevant features that are crucial for coordination. For example, an agent might learn to signal when it needs assistance from other agents or when it has achieved subgoal that contributes to the larger objective. Top-bottom Flow Control information descends the hierarchy through actions, starting at the top level. Each level receives actions al+1 = [al+1 ] from level + 1, 0 where each component corresponds to the action input for agent ωl i. These actions influence lower-level behavior through the policy function: , . . . , al+1 Nl The architecture of TAG enables scaling to arbitrary depths while maintaining computational efficiency through several mechanisms. First, the loose coupling between levels allows each layer to operate at its own temporal scale, similar to how biological systems separate strategic planning from reactive control. Higher levels can make decisions at lower frequencies than lower levels, reducing computational overhead while maintaining effective coordination. Second, standardized interfaces, implemented through the LevelEnv abstraction, naturally handle the integration of heterogeneous agents with varying capabilities and learning algorithms. This standardization ensures effective communication and coordination regardless of the implementation of individual agents. In practice, the LevelEnv implementation follows the PettingZoo API (Terry et al., 2021), providing two primary interface functions: .reset() and .step()1. The first, .reset(), initializes the system state from the real environment through all hierarchy levels and returns the initial = πi(al+1 al , ol1 ). 1Code will be released upon acceptance. 5 TAG: TAME Agent Framework observation, starting the upward flow of information. The .step() function accepts dictionary of actions and returns dictionaries containing observations, rewards, termination conditions, and additional information for each agent in the level. It is during the call to .step() that actions for the lower level are generated, the .step() of the lower level is called, and the agents are updated, as detailed in Alg. 1. 4. Empirical Validation 4.1. Multi Level Hierarchy Examples To demonstrate the effectiveness of TAG, we implement multiple concrete examples consisting of twoand threelevel hierarchical systems using PPOand MAPPObased agents. Their structures are shown in Fig. 1. We focus on on-policy algorithms as the lack of the replay buffer helps in dealing with the changing distributions in the environment (Foerster et al., 2016). 5. Empirical Validation 5.1. Examples of Multi-Level Hierarchy To demonstrate the effectiveness of TAG, we implement multiple concrete examples consisting of twoand threelevel hierarchical systems using PPOand MAPPO-based agents. Their structures are shown in Fig. 1. We focus on on-policy algorithms, as the lack of replay buffer helps address the changing distributions in the environment (Foerster et al., 2016). As shown in Fig. 1(a), the three-level architecture consists of bottom level comprising four agents, each directly controlling an actor in the environment. These agents must learn to translate high-level directives into concrete actions while adapting to local conditions. The middle level contains two agents, each coordinating pair of bottom-level agents. Finally, the top level contains single agent that learns to provide strategic direction to the entire system. In contrast, the two-level hierarchy consists of four low-level agents interacting with the real environment and coordinated by single high-level manager. For each of these topologies, we instantiate one homogeneous system, containing only PPO-based agents, and one heterogeneous system, with PPO-agents at the bottom and MAPPO-agents at the upper levels. We refer to these agents as 3PPO and 2MAPPO-PPO for the three-level systems, and 2PPO and MAPPO-PPO for the two-level systems. Except for the agents at the bottom level, whose action space depends on the environment, all the PPO-based agents in 2PPO and 3PPO produce one-dimensional discrete actions in the range [0, . . . , 5]. Given that PPO is not MARL algorithm, it cannot control multiple agents in the level below the hierarchy without adaptation. To overcome this, we design the action space of each PPO agent in the upper levels of 2PPO and 3PPO to be the combination of the input action spaces of level 1, resulting from the subset of agents in 1 connected to it. For example, if level 1 contains two agents, each with an input action space of size K, the PPO agent at level will have an action space of size K. In the heterogeneous hierarchies of 2MAPPO-PPO and MAPPO-PPO, each MAPPO-based agent produces two-dimensional continuous action for each of the agents to which it is connected. In this case, since MAPPO is MARL algorithm by design, we did not modify its outputs. The agents in all these four systems (2PPO, 3PPO, MAPPO-PPO and 2MAPPO-PPO) only learn their policy πi, while the communication function ϕi is ] 1 hand-designed to return as message ml , corresponding to the concatenation of the observations from the level below, and as reward the sum of the rewards from 1: rl . Moreover, in the three-level agents, the top two levels provide new action once every two steps of the level below, making each level effectively work at different frequencies compared to the levels below. = [ml1 = (cid:80) rl1 jI 1 i Finally, we implement 3PPO-comm, version of 3PPO in which the communication function ϕi is learned. This consists of two-layer AutoEncoder (AE) (Bank et al., 2023) with ReLU activation functions between the layers and Sigmoid on its feature space. The AE is continually trained together with the PPO agents, on the same batch, to reconstruct ol1 corresponds to the representation of ol1 in the 8-dimensional feature space of the trained autoencoder. As with the other agents, the reward returned by ϕi is the sum of rewards from the level below. The hyperparameters of all the implemented systems are presented in App. A. by minimizing the MSE. The message ml 5.2. Experimental Design and Results We evaluate TAG-based systems across two standard multiagent environments that test different aspects of coordination and scalability. The first is the Simple Spread environment from the MPE suite (Lowe et al., 2017; Mordatch & Abbeel, 2017), where agents must maximize area coverage while avoiding collisions, testing both coordination and spatial reasoning. The second is the Balance environment from the VMAS suite (Bettini et al., 2022), which tests synchronized control by requiring agents to maintain collective stability through coordinated actions. Both environments operate with four agents and limit episodes to 100 time-steps. We compare our approach against three baselines: MAPPO (Yu et al., 2022), I-PPO (De Witt et al., 2020), and classic PPO (Schulman et al., 2017). Being in multi-agent setting, 6 TAG: TAME Agent Framework Figure 3: Mean average reward in the MPE-Spread environment (a) and Balance environment (b). Mean is calculated over 5 random seeds. Shaded areas represent 95% confidence intervals. Dotted red line in (a) shows the performance of an hand-designed heuristic. we adapted PPO by expanding its action space to encompass the combined action spaces of all agents in the real environment. Additionally, for the MPE-Spread environment, we developed hand-designed heuristic that assigns and directs each agent to specific goal along the shortest path from their initial position. The average performance of this heuristic across 10 episodes is indicated by red dotted line in Fig. 3.(a). Fig. 3 shows the average reward obtained by all tested algorithms in both benchmark environments over 5 random seeds. The shaded areas represent 95% confidence intervals. The results demonstrate that increasing the depth of the hierarchy improves both final performance and sample efficiency. This improvement is particularly pronounced in the MPE-Spread environment (Fig. 3.(a)), where only the depth-three agents, 3PPO and 2MAPPO-PPO, match the hand-designed heuristic performance, while all other agents achieve lower rewards. We particularly focus on 3PPOcomm due to its performance in the Balance environment (Fig. 3.(b)). Its ability to achieve significantly higher average rewards compared to other baselines suggests that learned communication is crucial for proper coordination in certain settings. However, the implementation and learning of communication require careful consideration. While simple AE might suffice for the Balance task, 3PPO-comm shows lower performance in MPE-Spread compared to methods using the identity function as their communication function ϕ. Currently, the learning of ϕ occurs independently of agent performance. We believe incorporating performance-related communication between agents could significantly enhance both performance and communication quality, which we leave for future work. Regarding the baselines, while MAPPO and I-PPO eventually reach similar performance levels as the two-level TAG-based agents, they require more training time. Notably, PPO struggles to achieve performance similar to the other baselines in both environments, highlighting the limitations of monolithic approaches when dealing with large action and observation spaces. These results demonstrate two key advantages of the TAG approach. First, the hierarchical structure enables more efficient learning compared to flat architectures, as the division of labor across levels allows each agent to focus on manageable subset of the overall problem, leading to increased sample efficiency. Second, the framework shows improved scalability; as we increase the number of agents, the hierarchical structure helps maintain coordination without the exponential complexity growth typical of flat architectures. 5.3. Analysis of Communication Mechanisms In this section, we analyze the learned communication mechanism between hierarchy levels by examining correlations between the actions of connected agents. The presence of such correlations would indicate that agents can effectively use the modifications to their observations from higher-level agents. We focus our analysis on the action relationships between the top and bottom levels of 2PPO and MAPPOPPO in the MPE-Spread environment, where all agents in the hierarchy have discrete action space of 5. Figs. 4 and 5 display the discrete actions of one low-level agent on the y-axis and the training episodes on the x-axis. The colors indicate which top-level action was most frequently chosen (mode) when the bottom-level agent performed each of its actions during an episode. This is calculated as follows: for each episode, we: 1) look at every instance when the bottomlevel agent performs specific action, 2) record which action the top-level agent chose in each of these instances, and 3) determine which top-level action occurred most often (mode) for that bottom-level action. White spaces represent episodes where the low-level agent did not select the corresponding action. constant mode across multiple episodes indicates an association between the actions of agents across two levels. As shown in Figs. 4.(a) and 5.(a), there is strong correlation between the actions selected by the top agent ω2 for the bottom agent ω1 for both 2PPO and MAPPO-PPO, evidenced by the mode remaining constant across multiple episodes. While this association evolves and the actions of ω1 7 TAG: TAME Agent Framework of levels and agents per level currently relies on empirical tuning, presenting an important area for future research. Another key consideration emerges from the definition of our communication function. While most of our baselines use the identity function for inter-level communication, our experiments with learned communication functions reveal promising improvements in performance. These results underscore the need for more thorough investigation into learning optimal communication between agents. Understanding how to effectively learn and shape this communication could significantly enhance information flow between hierarchical levels and potentially reduce coordination overhead. particularly promising direction is adapting the hierarchical structure automatically. The current implementation requires pre-specifying the number of levels and inter-agent connections. Extending TAG to dynamically adjust its structure based on the demands of the task could enhance its flexibility and efficiency. This development could draw inspiration from biological systems, where hierarchical organization typically emerges through self-organization rather than external specification. The success of TAG in enabling scalable multi-agent coordination extends beyond pure reinforcement learning. Its principles of loose coupling between levels and standardized information flow could inform the design of other complex systems, from robotic swarms to distributed computing architectures. Additionally, the capability of the framework to handle heterogeneous agents suggests potential applications in human-AI collaboration, where artificial agents must coordinate with human operators across multiple levels of abstraction. Several promising avenues for future research emerge from this work. First, investigating theoretical guarantees for learning convergence in deep hierarchies could provide valuable insights for designing more robust systems, particularly regarding the stability of learning across multiple hierarchical levels. Second, enabling the creation of autonomous hierarchies and composing the team dynamically would enhance practical applicability by allowing agents to join or leave the hierarchy during operation. Furthermore, integrating model-based planning at higher levels while maintaining reactive control at lower levels could improve performance in complex domains. This could include incorporating LLM-based agents at the highest levels to enhance reasoning capabilities and facilitate natural interaction with human operators. The study of how agents learn to communicate effectively within the hierarchy represents another crucial direction, as our preliminary results with learned communication functions suggest significant potential for improving coordination efficiency and system performance. Figure 4: Action distributions between top and bottom agents in MAPPO-PPO. (a) The bottom agent receives actions from the top. (b) The bottom agent does not receive actions from the top. Figure 5: Action distributions between top and bottom agents in 2PPO. (a) Bottom agent receives actions from the top. (b) Bottom agent does not receive actions from the top. and the actions of ω1 throughout training, it maintains clear definition. In contrast, Figs. 4.(b) and 5.(b) show the correlation between actions selected by ω2 for ω1 , with = i. If the correlations observed earlier were merely coincidental rather than due to meaningful communication, we would expect to see similar patterns even between unconnected agents. Nonetheless, no correlation is present, as indicated by the mode changing every episode. The absence of correlation in this case confirms that the patterns observed between connected agents reflect actual information flow through the hierarchy. These results demonstrate that higher-level agents learn to provide useful feedback that lower-level agents can build on, confirming that the hierarchical structure and information flow instantiated by TAG are beneficial. 6. Discussion and Future Work Our results demonstrate the benefits of TAG for hierarchical coordination, while highlighting several important considerations. The framework excels in tasks requiring coordination between multiple agents, though determining the optimal hierarchy configuration specifically, the number 8 TAG: TAME Agent Framework 7. Conclusion TAG represents step toward more scalable and flexible multi-agent systems. By providing principled framework for hierarchical coordination while maintaining agent autonomy, it enables complex collective behaviors to emerge from relatively simple components, similar to biological systems. The demonstrated success in our comprehensive evaluation across standard multi-agent benchmarks, including both cooperative navigation and manipulation tasks, suggests its potential for addressing increasingly challenging multiagent problems. Having heterogeneous agents and arbitrary depths of hierarchy, while maintaining stable learning, poses several key challenges in multi-agent reinforcement learning. As we move toward increasingly complex multi-agent systems, frameworks like TAG that enable principled hierarchical organization will become increasingly important. Impact statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "gence rates. IEEE Transactions on Automatic Control, 66 (4):14971512, 2020. Dayan, P. and Hinton, G. E. Feudal reinforcement learning. Advances in neural information processing systems, 5, 1992. De Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H., Sun, M., and Whiteson, S. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020. Foerster, J., Assael, I. A., De Freitas, N., and Whiteson, S. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29, 2016. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. PMLR, 2018. Hu, J., Jiang, S., Harding, S. A., Wu, H., and Liao, S.-w. Rethinking the implementation tricks and monotonicity constraint in cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2102.03479, 2021. Ahilan, S. and Dayan, P. Feudal multi-agent hierarchies for cooperative reinforcement learning. arXiv preprint arXiv:1901.08492, 2019. Hu, W., Wang, H., He, M., and Wang, N. Uncertainty-aware hierarchical reinforcement learning for long-horizon tasks. Applied Intelligence, 53(23):2855528569, 2023. Bacon, P.-L., Harb, J., and Precup, D. The option-critic architecture. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. Bank, D., Koenigstein, N., and Giryes, R. Autoencoders. Machine learning for data science handbook: data mining and knowledge discovery handbook, pp. 353374, 2023. Bettini, M., Kortvelesy, R., Blumenkamp, J., and Prorok, A. Vmas: vectorized multi-agent simulator for collective robot learning. In International Symposium on Distributed Autonomous Robotic Systems, pp. 4256. Springer, 2022. Bettini, M., Prorok, A., and Moens, V. Benchmarl: Benchmarking multi-agent reinforcement learning. Journal of Machine Learning Research, 25(217):110, URL http://jmlr.org/papers/v25/ 2024. 23-1612.html. Brockman, G. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Cassano, L., Yuan, K., and Sayed, A. H. Multiagent fully decentralized value function learning with linear converHutsebaut-Buysse, M., Mets, K., and Latré, S. Hierarchical reinforcement learning: survey and open research challenges. Machine Learning and Knowledge Extraction, 4 (1):172221, 2022. Jorge, E., Kågebäck, M., Johansson, F. D., and Gustavsson, E. Learning to play guess who? and inventing grounded language as consequence. arXiv preprint arXiv:1611.03218, 2016. Kumar, A., Swersky, K., and Hinton, G. Feudal learning for large discrete action spaces with recursive substructure. In Proceedings of the NIPS Workshop Hierarchical Reinforcement Learning, Long Beach, CA, USA, volume 9, 2017. Leibo, J. Z., Hughes, E., Lanctot, M., and Graepel, T. Autocurricula and the emergence of innovation from social interaction: manifesto for multi-agent intelligence research. arXiv preprint arXiv:1903.00742, 2019. Technological approach to mind everyLevin, M. An experimentally-grounded framework where: for understanding diverse bodies and minds. Frontiers in Systems Neuroscience, 16, 2022. ISSN 1662-5137. doi: 10.3389/fnsys.2022.768201. URL 9 TAG: TAME Agent Framework https://www.frontiersin.org/articles/ 10.3389/fnsys.2022.768201. Li, Z., Narayan, A., and Leong, T.-Y. An efficient approach to model-based hierarchical reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017. Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel, O., and Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. Luo, S., Chen, J., Hu, Z., Zhang, C., and Zhuang, B. Hierarchical reinforcement learning with attention reward. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pp. 2804 2806, 2023. Mann, T. and Mannor, S. Scaling up approximate value iteration with options: Better policies with fewer iterations. In International conference on machine learning, pp. 127135. PMLR, 2014. Mordatch, I. and Abbeel, P. Emergence of grounded compositional language in multi-agent populations. arXiv preprint arXiv:1703.04908, 2017. Nachum, O., Tang, H., Lu, X., Gu, S., Lee, H., and Levine, S. Why does hierarchy (sometimes) work so well in reinforcement learning? arXiv preprint arXiv:1909.10618, 2019. Sutton, R. S., Precup, D., and Singh, S. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2): 181211, 1999. Tang, H., Hao, J., Lv, T., Chen, Y., Zhang, Z., Jia, H., Ren, C., Zheng, Y., Meng, Z., Fan, C., et al. Hierarchical deep multiagent reinforcement learning with temporal abstraction. arXiv preprint arXiv:1809.09332, 2018. Terry, J., Black, B., Grammel, N., Jayakumar, M., Hari, A., Sullivan, R., Santos, L. S., Dieffendahl, C., Horsch, C., Perez-Vicente, R., et al. Pettingzoo: Gym for multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:1503215043, 2021. Thorpe, T. Multi-agent reinforcement learning: Independent vs. cooperative agents. PhD thesis, Masters thesis, Department of Computer Science, Colorado State University, 1997. Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K. Feudal networks for hierarchical reinforcement learning. In International conference on machine learning, pp. 35403549. PMLR, 2017. Watkins, C. J. and Dayan, P. Q-learning. Machine learning, 8:279292, 1992. Xu, D. and Fekri, F. Interpretable model-based hierarchical reinforcement learning using inductive logic programming. arXiv preprint arXiv:2106.11417, 2021. Nguyen, T. T., Nguyen, N. D., and Nahavandi, S. Deep reinforcement learning for multiagent systems: review of challenges, solutions, and applications. IEEE transactions on cybernetics, 50(9):38263839, 2020. Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:2461124624, 2022. Oroojlooy, A. and Hajinezhad, D. review of cooperative multi-agent deep reinforcement learning. Applied Intelligence, 53(11):1367713722, 2023. Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019. Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. Fully decentralized multi-agent reinforcement learning with networked agents. In International conference on machine learning, pp. 58725881. PMLR, 2018. Zheng, X. and Yu, C. Multi-agent reinforcement learning with hierarchy of reward machines. arXiv preprint arXiv:2403.07005, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Silver, D. and Ciosek, K. Compositional planning using optimal option models. arXiv preprint arXiv:1206.6473, 2012. Spelke, E. S. and Kinzler, K. D. Core knowledge. Developmental science, 10(1):8996, 2007."
        },
        {
            "title": "Appendix",
            "content": "TAG: TAME Agent Framework A. Hyperparameters The hyperparameters of the actor critic networks of all our PPO-based agents are the following: Component Number of Layers Input Layer Activation 1 Hidden Layer Activation 2 Output Layer Output Init std Action Type Actor 3 Critic 3 Observation Size 64 Observation Size Tanh 64 64 Tanh 64 Actions Size 0.01 Discrete Tanh 64 64 Tanh 64 1 1.0 The hyperparameters of the actor critic networks of all our MAPPO-based agents are the following: Component Number of Layers Input Layer Activation 1 Hidden Layer Activation 2 Output Layer Output Type Init Method Output Init Action Type Actor 3 Critic Observation Size 64 N_agents * Observation Size 64 ReLU 64 64 ReLU 64 Actions Size Normal Distribution Orthogonal gain = 0.01 Continuous ReLU 64 64 ReLU 64 1 Value Orthogonal default A.1. Hyperparameters of 2PPO The training hyperparameters of 2PPO are the following: Value Parameter 2, 000, 000 Total training steps 0.001 Learning rate true Anneal learning rate 0.5 Max grad norm 2, 048 Buffer size 4 Number minibatches 4 Update epochs 0.99 Gamma 0.95 GAE lambda true Norm advantage 0.2 Clip coef ratio true Clip value loss 0.0 Entropy loss coef Value Function loss Coef 0.5 Target KL None 11 The size of the observation and action spaces for the agents in the hierarchy are: TAG: TAME Agent Framework Environment Bottom agents Observation Size Bottom agents number of Actions Top agent Observation Size Top agent number of Actions Bottom level action frequency wrt to top Simple Spread Balance 25 5 96 625 17 9 64 625 1 A.2. Hyperparameters of 3PPO The training hyperparameters of 3PPO are the following: Value Parameter 2, 000, 000 Total training steps 0.001 Learning rate true Anneal learning rate 0.5 Max grad norm 2, 048 Buffer size 8 Number minibatches 4 Update epochs 0.99 Gamma 0.95 GAE lambda true Norm advantage 0.1 Clip coef ratio true Clip value loss 0.01 Entropy loss coef Value Function loss Coef 0.5 Target KL 0.015 The size of the observation and action spaces for the agents in the hierarchy are: Environment Bottom agents Observation Size Bottom agents number of Actions Middle agents Observation Size Middle agents number of Actions Top agent Observation Size Top agent number of Actions Bottom level action frequency wrt to middle Middle level action frequency wrt to top 12 Simple Spread Balance 25 5 34 25 32 25 2 2 17 9 34 25 64 625 2 2 A.3. Hyperparameters of 3PPO-comm The training hyperparameters of 3PPO-comm are the following: TAG: TAME Agent Framework Value Parameter 2, 000, 000 Total training steps 0.001 Learning rate true Anneal learning rate 0.5 Max grad norm 2, 048 Buffer size 8 Number minibatches 4 Update epochs 0.99 Gamma 0.95 GAE lambda true Norm advantage 0.1 Clip coef ratio true Clip value loss 0.01 Entropy loss coef Value Function loss Coef 0.5 Target KL 0.015 The Autoencoder has the following hyperparameters: Component Input Layer Activation 1 Output Layer Activation 2 Loss Training epochs Encoder Observation Shape 32 ReLU 32 8 Sigmoid Decoder 8 32 ReLU 32 Observation Shape None MSE Loss 50 The size of the observation and action spaces for the agents in the hierarchy are: Environment Bottom agents Observation Size Bottom agents number of Actions Middle agents Observation Size Middle agents number of Actions Top agent Observation Size Top agent number of Actions Bottom level action frequency wrt to middle Middle level action frequency wrt to top 13 Simple Spread Balance 25 5 34 25 32 25 2 17 9 34 25 64 625 2 2 A.4. Hyperparameters of MAPPO-PPO The training hyperparameters of MAPPO-PPO are the following: TAG: TAME Agent Framework Parameter Value 2, 000, 000 Total training steps 0.001 Learning rate Anneal learning rate true 0.5 Max grad norm 10, 000 MAPPO Buffer size 2, 048 PPO Batch size 4 Number minibatches 4 Update epochs 0.99 Gamma 0.95 GAE lambda Norm advantage true 0.2 Clip coef ratio Clip value loss true 0.0 Entropy loss coef Value Function loss Coef 0.5 Target KL None The size of the observation and action spaces for the agents in the hierarchy are: Environment Bottom agents Observation Size Bottom agents number of Actions Top agent Observation Size Top agent Action Size Bottom level action frequency wrt to top Simple Spread Balance 26 5 24 2 1 18 9 16 2 1 A.5. Hyperparameters of 2MAPPO-PPO The training hyperparameters of 2MAPPO-PPO are the following: Value Parameter 2, 000, 000 Total training steps 0.001 Learning rate true Anneal learning rate 0.5 Max grad norm 10, 000 MAPPO Buffer size 2, 048 PPO Batch size 4 Number minibatches 4 Update epochs 0.99 Gamma 0.95 GAE lambda true Norm advantage 0.2 Clip coef ratio true Clip value loss 0.01 Entropy loss coef Value Function loss Coef 0.5 0.5 Max Grad Norm 0.015 Target KL 14 The size of the observation and action spaces for the agents in the hierarchy are: TAG: TAME Agent Framework Environment Bottom agents Observation Size Bottom agents number of Actions Middle agent Observation Size Middle agent Action Size Top agent Observation Size Top agent Action Size Bottom level action frequency wrt to middle Middle level action frequency wrt to top Simple Spread Balance 26 5 26 2 48 2 2 18 9 18 2 32 2 2"
        }
    ],
    "affiliations": [
        "Noahs Ark Lab, Huawei Tech"
    ]
}