{
    "paper_title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
    "authors": [
        "Sreyan Ghosh",
        "Zhifeng Kong",
        "Sonal Kumar",
        "S Sakshi",
        "Jaehyeon Kim",
        "Wei Ping",
        "Rafael Valle",
        "Dinesh Manocha",
        "Bryan Catanzaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/."
        },
        {
            "title": "Start",
            "content": "Audio Flamingo 2: An Audio-Language Model with Long-Audio"
        },
        {
            "title": "Understanding and Expert Reasoning Abilities",
            "content": "Sreyan Ghosh * 1 2 Zhifeng Kong 1 Sonal Kumar 2 Sakshi 2 Jaehyeon Kim 1 Wei Ping 1 Rafael Valle 1 Dinesh Manocha 2 Bryan Catanzaro"
        },
        {
            "title": "Abstract",
            "content": "Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) multistage curriculum learning strategy. AF2 achieves state-of-the-art performance with only 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Finetuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/ labs/adlr/AF2/ 5 2 0 2 6 ] . [ 1 3 8 9 3 0 . 3 0 5 2 : r 1. Introduction Understanding non-speech sounds, non-verbal speech, and music (collectively referred to as audio in this paper) is essential for real-world applications such as detecting anomalies in industrial environments, recognizing emotional cues, and improving assistive technologies for the impaired. While Large Language Models (LLMs) have *Work done during an internship at NVIDIA. 1NVIDIA, Santa Clara, CA, USA 2University of Maryland, College Park, MD, USA. Correspondence to: Sreyan Ghosh <sreyang@umd.edu>, Zhifeng Kong <zkong@nvidia.com>. Preliminary work. Under review. Copyright 2025 by the author(s). Figure 1: Audio Flamingo 2 versus previous SOTA ALMs on audio understanding and reasoning benchmarks (values normalized). AF2 outperforms all baselines and has smaller model footprints. demonstrated remarkable reasoning capabilities through language, extending these systems to comprehend audio is key to building intelligent systems capable of reasoning with contextual auditory cues (Kong et al., 2024). Verbal speech, inherently tied to language, benefits significantly from (L)LM advancements (Watanabe et al., 2018; Chen et al., 2024a); however, the potential to enhance perception and reasoning over non-verbal audio remains largely underexplored (Ghosh et al., 2024c). Audio-Language Models (ALMs) extend language models with audio understanding capabilities. Contrastive Language-Audio Pre-training (CLAP) (Elizalde et al., 2023a) was among the first encoderonly ALMs to bridge audio and language with contrastive learning. Building on this, subsequent efforts introduced Large ALMs (LALMs), which integrate audio encoders with pre-trained decoder-based LLMs, enabling open-ended Audio Question Answering (AQA) and free-form response generation (Gong et al., 2021; Tang et al., 2024; Ghosh et al., 2024c). However, despite these developments, even the most advanced LALMs continue to underperform on expertlevel reasoning tasks compared to foundational tasks like event classification. For example, Gemini-1.5-Pro (Team et al., 2024), one of the most advanced models, achieves only 54.4% and 48.5% on the MMAU sound and music Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities subsets (Sakshi et al., 2024), benchmark for evaluating expert-level audio reasoning. This underscores the challenges of improving an LLMs ability to understand and reason over audio, which we attribute to the lack of highquality training data and robust audio representations. Main Contributions: In this paper, we propose Audio Flamingo 2 (AF2), parameter-efficient ALM that combines 3B-parameter small decoder LM with 203Mparameter audio encoder, achieving state-of-the-art audio understanding and reasoning capabilities. There are three major innovations in our method: (1) Data: Recent studies highlight that improving data quality can rival or even surpass the performance gains achieved by scaling compute and model size (Abdin et al., 2024). To this end, we propose AudioSkills (Section 4.1), large-scale, skill-specific AQA training dataset featuring complex, reasoning-intensive questions paired with each audio. We design questions that target seven distinct skills, with the primary aim of improving finegrained reasoning capabilities in ALMs. (2) Audio Encoding: We propose AF-CLAP (Section 3.1), where we scale CLAP training to over 8M audio-caption pairs, incorporating synthetic data, and propose an improved contrastive loss for better representational quality and robustness. (3) Training Strategy: We propose novel 3-stage curriculum training strategy (Section 5) for improved performance. Additionally, for the first time, we extend audio understanding to long audios, moving beyond 30-second clips to audios lasting up to 5 minutes. To enable the model to comprehend and reason over long audio, we introduce LongAudio, novel dataset comprising over 260k carefully curated AQA instances with audios ranging from 30 seconds to 5 minutes. LongAudio spans 10+ audio categories and supports 6 tasks, including captioning and 5 reasoning-based QA tasks. Next, we introduce LongAudioBench, an expert-annotated benchmark for evaluating ALMs on long audio understanding. AF2, trained on LongAudio, significantly outperforms our baseline. In summary, our main contributions are: 1. We present Audio Flamingo 2, SOTA ALM with advanced audio understanding and reasoning capabilities. 2. We propose innovations in data generation, architecture design, representation learning, and training strategies. 3. We introduce the long audio understanding task and create dedicated training and evaluation datasets to drive progress in this area. 4. Audio Flamingo 2 outperforms larger and proprietary LALMs across over 20 benchmarks, despite being smaller and trained exclusively on public datasets. 5. We conduct systematic ablation studies to demonstrate the impact of each design choice. 2. Related Work Audio-Language Models: ALMs can be classified into 2 two broad categories: 1) Encoder-only ALMs: Encoderonly ALMs are class of Multi-Modal Language Models (MLLM) that learn shared space between the audio and language modalities with an encoder-only LM and an audio encoder. CLAP, pioneering encoder-based ALM inspired by CLIP (Radford et al., 2021), showed stateof-the-art performance on audio-language tasks like retrieval, zero-shot classification, etc. Following this, several attempts have been made to improve CLAP by scaling data (Wu et al., 2022b), incorporating additional training objectives (Ghosh et al., 2024d), or with synthetic data (Ghosh et al., 2025). Other notable works include Wav2CLIP (Wu et al., 2022a), AudioClip (Guzhov et al., 2022) and CoLLAT (Silva et al., 2023). 2) Decoder-based ALMs: With the advent of LLMs, Pengi (Deshmukh et al., 2023), pioneering decoder-based ALM, achieved SOTA results on variety of audio classification tasks. Following Pengi, large number of Large ALMs were introduced, including fully open-source models like LTU (Gong et al., 2024), LTU-AS (Gong et al., 2023a), SALMONN (Tang et al., 2024), AudioGPT (Huang et al., 2024), GAMA (Ghosh et al., 2024c), Audio Flamingo (Kong et al., 2024), and open-access models like Qwen-Audio (Chu et al., 2023) and Qwen-2-Audio (Chu et al., 2024). majority of the advances have focused on scaling model size and datasets, with very little advancements in data quality or audio encoder representations. This has eventually translated to performance advancing on foundational tasks like classification and captioning but under-performing on expert-level reasoning, the skill required for advancing towards Artificial General Intelligence (AGI) (Morris et al., 2024). Long Audio Understanding: Current ALMs are limited to perceiving at most 30 seconds of audio (Chu et al., 2024), with majority confined to at most 10 seconds (Ghosh et al., 2024c; Gong et al., 2024). This can be attributed to the fact that these models employ audio encoders that support only short audio encoding and datasets where majority of the data is only at most 10 seconds. While long speech has received some attention (Chiu et al., 2019; Arumugam et al., 2023), and the field on long video understanding has seen advancements recently (Weng et al., 2025; Xue et al., 2024), to the best of our knowledge, no current work or datasets exist for long audio understanding and reasoning. 3. Audio Flamingo 2 Architecture Fig. 2 summarizes the AF2 architecture, consisting of four primary components: i) AF-CLAP: CLAP-based audio encoder with sliding window feature extraction, ii) audio representation transformation layers for additional capacity, iii) decoder-only language model, and iv) gated crossattention (XATTN-Dense) layers for audio conditioning. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 2: Overview of Audio Flamingo 2s cross-attention architecture and three-stage curriculum training. 3.1. AF-CLAP Audio Encoder CLAP, pre-trained with contrastive loss on audio-caption pairs, shows strong audio understanding and natural language alignment (Wu et al., 2022b; Elizalde et al., 2023a). These make CLAP suitable choice for building ALMs. However, CLAP is less favored in prior works (Tang et al., 2023; Chu et al., 2023) due to its under-performance compared to SSL pre-trained audio encoders (Gong et al., 2023b; LI et al., 2024; Ghosh et al., 2024c). We hypothesize this is due to the limited availability of high-quality audio-caption pairs, which causes CLAP representations to struggle with compositional reasoning (Ghosh et al., 2024d) and linguistic variations in captions (Selvakumar et al., 2024). In this section, we address these issues and introduce an improved version of CLAP called AF-CLAP. Specifically, we focus on: i) constructing large-scale, high-quality training dataset, and ii) improving the training objective to for better representational robustness. 3.1.1. AF-CLAP TRAINING DATASET We scale the training dataset for AF-CLAP to over 8M (10second) audio-caption pairs. We collect these data from open-source audio and video datasets, with an emphasis on audio diversity and caption accuracy. Existing models like Laion-CLAP Wu et al. (2022b) and MS-CLAP (Elizalde et al., 2023b) rely heavily on singlelabel audio classification datasets for captions. This limits their ability to generalize to complex, real-world audio with diverse compositions. Automated captioning efforts (e.g., Yuan et al. (2024)) yield only about 1.4M pairs, lack diversity, and under-represent critical domains like home sounds. Inspired by the recent success of training vision LMs on images from long videos (Venkataramanan et al., 2024), we collect audio from open long-video datasets. Specifically, we select 100k diverse videos from MiraData (Ju et al., 2024) and Video Recap (Islam et al., 2024) (see Appendix H.1 for selection details). We segment these videos into 10second clips and generate video captions using Qwen2-VL2B-Instruct and audio captions using Qwen2-Audio. To ensure diversity and reduce redundancy, we filter out segments with audio-visual similarity above threshold p. We then prompt GPT-4o(2024-05-13) (Prompts 23 and 22) to generate audio-centric captions that exclude visual attributes and emphasize sound events. Using this approach, we collect approximately 5.5M new audio-caption pairs, including 4M from unlabeled short audios and 1.5M from long videos. Detailed dataset statistics are provided in Table 9. 3.1.2. AF-CLAP TRAINING OBJECTIVE Compared to the standard contrastive loss in audio-language pre-training (Wu et al., 2022b; Elizalde et al., 2023a), we improve the training objective for better robustness to linguistic variations and compositional reasoning abilities. Improving Linguistic Invariance: CLAP-like models struggle to generalize to linguistic variations in captions that humans easily understand (Selvakumar et al., 2024) (e.g., failing to equate helicopter and chopper). To address this, for every caption in the dataset, we generate 1 linguistically varied captions with identical semantics and composition. These variations, along with the ground-truth caption, are treated as positives. Improving Compositional Reasoning: Captions with different word orders or structures often convey distinct relationships between acoustic events, such as temporal sequencing or attribute binding. However, CLAP-like models struggle to capture these nuances (e.g., differentiating whether one sound follows or precedes another) (Ghosh et al., 2024d). To address this, we introduce compositionaware negatives. For every positive captions, we generate variations with modified temporal or attribute compositions and use them as negatives. An example is below: Original Caption: dog barking followed by the sound of train approaching. Positive: dog barking followed by the sound of railcar approaching. Negative: dog barking preceded by the sound of railcar approaching. Contrastive Loss. Each sample in the training data now has positives {P(x)m}M m=1 and negatives Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities feature representations, each with 8 heads and an inner dimension of 2048 (Kong et al., 2024; Vaswani et al., 2017). Gated Cross-Attention. Following Audio Flamingo, we use gated cross-attention dense (XATTN-Dense) layers from Flamingo (Alayrac et al., 2022) to condition audio representations on the LLM. Each layer consists of two blocks: (1) residual block with cross-attention and tanh gating and (2) residual block with dense layer and tanh gating. These layers are inserted before each LLM block. The XATTN-Dense layers reduces the quadratic attention complexity in prefix tuning to linear complexity. For instance, let l1 = 80 be the text token length and l2 = 30 64 be the number of audio embeddings. Prefix tuning requires self-attention complexity of (l1 + l2)2 = 4 106, whereas our cross-attention complexity is around l1 l2 1.5105. Frozen Language Model. Our architecture uses Qwen2.53B (Yang et al., 2024a), decoder-only causal LLM with 3B parameters, 36 hidden layers, and 16 attention heads. We find this model to have the best cost-performance ratio, as it has enough capacity for audio understanding and is light enough (see Section 6.6 for comparison of LLM sizes). 4. Audio Flamingo 2 Training Data Table 23 summarizes the datasets used to train AF2. We first convert common benchmark datasets used in Audio Flamingo to AQA format (see Appendix F.1). In addition, we propose two datasets: AudioSkills for audio expert reasoning and LongAudio for long audio understanding. 4.1. AudioSkills: An Expert Audio Reasoning Dataset Expert-level reasoning requires mastery of diverse and specialized skills (Ericsson, 2003; Huang & Chang, 2022). However, most existing audio datasets focus on surface-level properties, such as acoustic events or category classification. This limitation extends to QA pairs derived from these datasets, which often fail to require expert-level reasoning. To address this gap, we introduce AudioSkills, highquality, skill-specific synthetic dataset designed to prioritize the development of reasoning and problem-solving abilities. This dataset is carefully curated to ensure diversity and relevance, grounded in the hypothesis that expert reasoning emerges from the mastery of various relevant skills and world knowledge. Specifically, we use combination of open-source sound and music datasets and synthetic audio and prompt GPT-4o, along with any available metadata, to create QA pairs. AudioSkills focuses on audios 30 seconds long to enable effective scaling on open-source It spans seven distinct skills selected for their datasets. plausibility, relevance, and ablation insights as follows: Figure 3: Illustration of AF-CLAP training process. We collect long and short videos, segment it into 10-second clips, caption it, and prompt an LLM to generate audio captions. The data is then used to train CLAP with modified contrastive loss. {N (x)m,n}M,N m=1,n=1. Let A(x) be audio embedding from the HTSATlarge audio encoder (Chen et al., 2022) with MLP projection, and (x) be the text embedding from Flan-T5 text encoder (Raffel et al., 2020; Chung et al., 2024) with MLP projection. Let s(u, v) = exp(uv/τ ) be the similarity function with temperature τ . Our training objective is: = 1 (cid:88) i=1 log S(i, i) Sneg(i) + (cid:80)B j=1 S(j, i) , where S(j, i) = (cid:88) s(T (P(xj)m), A(xi)), Sneg(i) = (cid:88) m,n s(T (N (xi)m,n), A(xi)), where is the batch size. Our approach encourages CLAP to learn both linguistic invariance and compositional reasoning, aligning its capabilities more closely with human-like understanding and thus providing more human-aligned representations. AF-CLAP achieves SOTA performance on various retrieval and audio classification benchmarks. Since comparing CLAP performance is beyond our scope, we compare results in Tables 7 and 10. 3.2. Audio Conditioning Architecture & LLM Audio Feature Extraction. For each 10-second segment, we extract dense audio features R642048 from the penultimate layer of AF-CLAP. This approach yields higherquality features compared to mean-pooled representations from the final layer (see Appendix B.2). For longer audio, we use non-overlapping sliding windows to compute and concatenate audio features. The maximum number of sliding windows varies across training stages, with up to 30 windows (5 minutes) when training on LongAudio. Once the sliding window features are obtained, we apply RoPE (Su et al., 2023) with base of 4096 to encode temporal information into the features. Representation Transformation Layers. To expand model capacity, we apply three self-attention layers to the audio 1. Temporal Reasoning: Understanding temporal relationships between acoustic events in the audio. We 4 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 4: Examples from AudioSkills. Compared to other AQA datasets, questions in AudioSkills require deliberate reasoning. generate 4 types of MCQ-type QA pairs: i) Temporal Order: Understanding the sequential order of events or sounds; ii) Temporal Attribute: Understanding how sound attributes change over time; iii) Temporal Referring: Answering questions referring to sounds at specific temporal locations in the audio (e.g., start, middle, end); iv) Temporal Grounding: Identifying the temporal location of specific acoustic events. 2. Attribute Identification: Recognizing attributes of events in multi-event audio, such as sound characteristics (e.g., loud bang) or gender (e.g., male speech). We generate QA pairs using attributes extracted via the method proposed by Kumar et al. (2024). 3. Counting: Counting occurrences of specific sounds in an audio. We use the Synthio TTA model (Ghosh et al., 2024b) to create QA pairs at 3 difficulty levels: i) Level 1: Single sounds concatenated; count the event occurrences. ii) Level 2: Multiple interleaved sounds; count the main sound occurrences. iii) Level 3: Same as Level 2 but referenced by their temporal position. 4. Contextual Sound Event Reasoning: Identifying the purpose of sound or acoustic event in the context of other sounds and events in the audio. This skill requires audio understanding, temporal reasoning, world knowledge, and various other skills. Inspired by CompA-R in GAMA (Ghosh et al., 2024c), we expand the dataset from 200k to 350k QA pairs. 5. Contextual Speech Event Reasoning: Similar to Contextual Sound Event Reasoning but focused on identifying the purpose of spoken utterances in relation to other sounds or events. 6. Information Extraction: Focuses on understanding characteristics of the audio beyond just surface-level properties, detailed content analysis, and the application of external world knowledge when necessary. 7. General Reasoning: This category encompasses questions that do not fall into the specific skill types above but require combination of multiple skills or unique abilities, such as identifying relationships between multiple events or interpreting complex scenarios. In total, we generate 4.2M QA pairs. Figure 4 compares AudioSkills to other open-source AQA datasets, highlighting that these datasets lack the complexity required for deliberate reasoning. While training on such QA pairs is useful for alignment (Zhou et al., 2024; Wolf et al., 2023), it does not equip models with the specialized skills needed to handle complex questions (Sakshi et al., 2024; Ghosh et al., 2024a). Additional details, including statistics, metadata, and prompts, are provided in Appendix D.1 and Table 14. 4.2. LongAudio: Long Audio Understanding Dataset We construct LongAudio, the first large-scale long audio understanding dataset, which is comprised of over 80K unique audios and approximately 263K AQA pairs. Audios are sourced from open long-video datasets, including subset of MiraData(Ju et al., 2024) (featuring diverse content from natural scenes to gaming) and the entire Video ReCap(Islam et al., 2024) (egocentric videos of daily activities). To ensure diversity, we filter MiraData by clustering videos and carefully selecting samples from each cluster (see Appendix H.1). Fig. 5 (left) categorizes these videos and topics across several domains. We generate captions for video and audio segments using Qwen2-VL-2B-Instruct and Qwen2-Audio, respectively. GPT-4o is then used to generate questions that ask for captions, challenge models to reason, or extract information from long audios. Fig. 6 outlines the LongAudio creation process. Fig.5 (middle) shows the distribution of these categories as outlined below: 1. Captioning: The task is to generate detailed and accurate descriptions of long audio, focusing on capturing 5 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 5: The proportion of video categories (for sourcing audios) (left), question categories (middle), and distribution of durations (right) for the LongAudio dataset with 262,928 unique AQA and 80k unique audios. We target captioning and reasoning tasks. Figure 6: The pipeline for generating LongAudio. The process begins by segmenting the long video into short video and audio clips, each 10 seconds. These clips are individually annotated with captions. Subsequently, an LLM is employed to generate question-and-answer pairs based on the captions of these clips. subset of the data goes through expert review to construct LongAudioBench. the essence of the entire audio. 2. Plot QA: The task is to answer questions related to the overarching narrative or storyline of the audio, requiring an understanding of temporal and causal relationships between individual acoustic events. 3. Temporal QA: The task is to identify the temporal positions of acoustic events and their relationships, such as order or overlap within the audio, including how certain attributes change with time. 4. Needle QA: The task is to locate and reason about specific needle segment embedded within longer audio haystack, ensuring the response is tied explicitly to the needle. 5. Subscene QA: The task is to answer questions about specific subscene within the audio, requiring identification and understanding of localized events. 6. General QA: The task is to address broad, openended questions about the audio that may span multiple events, themes, or contexts, demonstrating overall comprehension and reasoning. Fig.5 (right) shows the distribution of audio lengths, and Table 15 shows examples of each category. LongAudioBench: We sample 10% (3k) from LongAudio, proportionally across QA types, for expert human verification. Annotators manually verify and correct these instances, discarding any incorrect ones. Subsequently, three authors conduct quality check on the corrected samples. The full annotation process is detailed in Appendix E.7. The final version of LongAudioBench has 2,429 instances. For evaluation, following wealth of prior work (Ghosh et al., 2024c; Yang et al., 2024b), we use an LLM-as-a-judge framework (Zheng et al., 2023): we prompt GPT-4o with the model response and ground truth answer using prompt 25 and score the response on scale of 1 to 10. 5. Audio Flamingo 2 Training Strategy We train AF2 using 3-stage curriculum learning strategy, progressively increasing the audio context length and improving data quality: Stage 1: Pre-training. This stage focuses on multi-modal alignment to align audio representations with the LLM. For training, we leverage large-scale and plausibly noisy classification and captioning datasets. Inspired by (Cheng et al., 2024), we include small amount of high-quality QA data to improve pre-training. During this stage, CLAP and LLM layers are frozen, and only the audio representation transformation and gated cross-attention layers are trainable. The audio context is restricted to = 3 windows (30 seconds). Stage 2: Fine-tuning. This stage focuses on improving audio understanding and reasoning by learning skills. For training, we use high-quality shortaudio classification, captioning, and QA datasets. This stage is often referred to as instruction-tuning (Chu et al., 2024; Ghosh et al., 2024c). During this stage, only the LLM layers are frozen, and the CLAP model, audio representation transformation layers, and gated cross-attention layers are 6 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 1: Comparison of Audio Flamingo 2 with previous SOTA LALMs on foundational audio understanding benchmarks. CAP = Captioning, AQA = Audio Question Answering, CLS = Classification, ZS=Zero-Shot. Also, see note in Section C. Dataset Task Previous SOTA Ours Dataset Task Previous SOTA Ours AQA - ACC 74.9% - Qwen2-A 86.9%+12.0% Clotho-v2 ClothoAQAunan. ClothoAQAnon-bin AQA - ACC 49.5% - AF MusicAVQAaudio NonSpeech7k CochlScene NSsource NSinstrument FSD50k AQA - ACC 72.1% - Qwen-A CLS - ACC CLS - ACC CLS - ACC CLS - ACC CLS - mAP 83.9% - AF 91.6% - Pengi 60.1% - Pengi 78.8% - Qwen-A 47.9% - GAMA AudioCapsZS CREMA-DZS RavdessZS GTZANZS 52.6%+3.1% 72.3%+0.2% 84.3%+0.4% 82.1%9.5% 62.0%+1.9% Medley-solos-DBZS 71.1%7.7% 49.2%+1.3% US8KZS ESC50ZS CAP - CIDEr CAP - CIDEr CLS - ACC CLS - ACC CLS - ACC CLS - ACC CLS - ACC CLS - ACC 0.45 - Qwen2-A 0.46+0.01 0.58+0.12 0.46 - AF 26.5% - AF 36.6%+10.1% 26.3%+5.4% 20.9% - AF 69.1%+3.9% 65.2% - AF 85.6% - GAMA 85.8%+0.2% 71.2% - AF 68.0%3.2% 80.6% - GAMA 83.9%+3.3% trainable. The audio context length is increased to = 9 windows (1.5 minutes). Stage 3: Long Fine-tuning. This stage focuses on context-length extension and teaching skills specific to enabling reasoning on long audios. For training, we employ our proposed LongAudio dataset, keep the audio representation transformation and gated cross-attention layers trainable, and increase = 30 windows (5 minutes). 6. Experiments 6.1. Experimental Setup We train our model using 128 NVIDIA A100 80GB GPUs. During pre-training, we use an effective batch size of 1024, the AdamW optimizer (learning rate = 104, weight decay = 0.1), and bf16 with automatic mixed precision for efficiency. For fine-tuning and long fine-tuning, we adopt dynamic batching based on audio length, ensuring batch sizes are multiples of 2, with effective batch sizes ranging from 128 to 1024 (see Appendix H.2). We benchmark our model against recent SOTA LALMs, including GAMA, Audio Flamingo, Qwen-Audio, Qwen2-Audio, LTU, LTU-AS, SALMONN, AudioGPT, and Gemini (Flash v2 and Pro v1.5), GPT-4oaudio and report results for the best model. For zero-shot evaluations, we exclude corresponding datasets from all training stages, consistent with prior work (Gong et al., 2024; Ghosh et al., 2024c). For LongAudioBench, as current LALMs (except Gemini) do not support audio inputs of length 30 seconds, we adopt two-step cascaded approach. First, we generate captions for short segments of the input audio, and then we prompt the same LALM to answer the question using these captions. Our experiments with passing the original long audio consistently outperformed our cascaded approach. For Gemini, we prompt it with the entire original long audio. 6.2. Smaller but Better We employ standard benchmark datasets for evaluation  (Table 24)  . Foundational benchmarks include ClothoAQA, MusicAVQA (audio-only), NonSpeech7k, NSynth, CREMAD, Ravdess, GTZAN, Medley-solos-DB and USD8K and Clotho-v2 and AudioCaps. Reasoning benchmarks inTable 2: Comparison of Audio Flamingo 2 with previous SOTA LALMs on audio reasoning benchmarks, all AQA-based. Dataset Previous SOTA Ours MMAU Sound MMAU Music AE Clotho AE AudioCaps CompA-R-test MuchoMusic OpenAQA MusicInstruct (Long) MusicQA CMM Hallucination LongAudioBench (ours) 65.1%+3.4% 61.7% - Gemini v2 72.9%+16.4% 56.5% - Gemini v2 92.5%+9.2% 83.3% - Qwen-A 93.3%+29.1% 64.2% - Qwen-A 96.4%+16.4% 80.0% - GAMA-IT 56.5%+5.1% 51.4% - Qwen-A 86.0%+6.0% 80.0% - GAMA-IT 90.2%+4.1% 86.1% - MusiLingo 93.0%+3.0% 90.0% - MusiLingo 76.0% - SALMONN 82.0%+6.0% 64.2%+18.9% 45.3% - Gemini v2 clude MMAU (sound and music subsets), Audio Entailment, OpenAQA-test, MuchoMusic, CompA-R-test, MusicInstruct (Long subset), MusicQA, CMM (audio-language subset) and LongAudioBench. Foundational Audio Understanding: Table 1 presents the performance of AF2 on foundational audio understanding benchmarks. For evaluation, we follow the similarity-based retrieval approach using our CLAP model, proposed by Deshmukh et al. (2023) and widely adopted. While foundational audio understanding is not our primary focus, AF2 shows competitive results against SOTA LALMs while having half of their size (e.g., Qwen(2)-A, GAMA, and LTU are equipped with 7B LLMs and large audio encoders). Expert Reasoning: Table 2 compares the performance of AF2 on audio reasoning benchmarks. We employ the original evaluation strategy and metrics. With much smaller LLM, AF2 outperforms all LALMs by large margins. We provide fine-grained results and failure cases of AF2 on LongAudioBench in Table 13 and Table 22. 6.3. Enhanced Audio Features Boost Performance Table 3 compares the performance of AF2 using AF-CLAP against various SOTA CLAP models on benchmark datasets. The MMAU score is averaged across the sound and music subsets. AF-CLAP 630k refers to AF-CLAP trained using the same strategy but with Laion-CLAPs 630k audio-text pairs. AF-CLAP w/ con. represents our model trained 7 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities with the same data but using the standard contrastive loss formulation. The results show that replacing AF-CLAP with other CLAP models results in performance drop across benchmarks, highlighting the importance of robust audio representations for improving performance. Table 3: Benchmark results on various CLAP as audio encoders. Model AudioCaps GTZAN MuchoMusic MMAU (avg) Laion-CLAP MS-CLAP AF-CLAP 630k AF-CLAP w/ con. AF-CLAP 0.51 0.55 0.51 0.53 0.58 65.2 65.8 66.0 66.2 69. 52.3 53.1 54.6 54.4 56.5 63.8 64.3 63.9 66.8 69.0 6.4. High Quality Data Boosts Reasoning Abilities Table 4 compares the impact of training data composition on performance. w/ 1/2 data and w/ 1/3 data refer to experiments using random subsets comprising half and onethird of the total instances from each dataset. Key findings include: (1) AudioSkills significantly enhances AF2s reasoning performance and shows notable improvements when combined with other datasets. (2) For challenging tasks like reasoning and zero-shot classification, data diversity is crucial, as using 1/2 of the data outperforms using only OpenAQA. (3) More data consistently improves performance. Table 4: Comparison of training on different data compositions. Model AudioCaps GTZAN MuchoMusic MMAU (avg) w/ 1/2 data w/ 1/3 data w/o AudioSkills w/ OpenAQA +AudioSkills Original 0.48 0.41 0.58 0.55 0.55 0.58 51.3 47.7 68.8 19.3 22.6 69.1 46.7 43.9 42.6 39.8 51.3 56. 49.3 45.1 48.6 38.2 57.5 69.0 6.5. Cross-Attention Outperforms Prefix-Tuning Table 5 compares GAMA, SOTA 7B LALM using prefix tuning, trained on the same data and strategy (excluding stage 3), with AF2. AF2 significantly outperforms GAMA, attributed to superior audio representations and the shift cross-attention-based conditioning. Table 5: Comparison of AF2 with GAMA, SOTA LALM, trained on our same data and same training recipe. Model AudioCaps GTZAN MuchoMusic MMAU (avg) GAMA (orig.) GAMA (ours) AF2 0.67 0.41 0.58 13.8 54.7 69.1 33.7 40.3 56.5 38.1 52.8 69. 6.6. Effect of Scaling LLM Fig. 7 illustrates the performance of AF2 across various LLM sizes, ranging from 0.5B to 7B, trained with and without AudioSkills. The results demonstrate that data quality often surpasses the performance gains achieved by simply scaling compute. Training with AudioSkills not only delivers superior performance overall but also significantly boosts reasoning capabilities, even at smaller LLM sizes. In contrast, when AudioSkills is excluded, reasoning performance heavily depends on model size, with performance scaling more gradually as parameters increase. This highlights the critical role of high-quality, skill-specific data like AudioSkills in driving robust reasoning capabilities, regardless of model size. Figure 7: Performance comparison of AF2 on different LLM sizes, w/ and w/o AudioSkills. More results in Table 19. 6.7. Effect of Training Schedules Table 6 compares 10 training schedules and their impact on AF2 performance. For 1and 2-stage training, data from later stages is combined with earlier stages. Additionally, we evaluate 4-stage curriculum where stage 2 is repeated with LLM fine-tuning and long fine-tuning is shifted to stage 4. Key findings include: (1) Fine-tuning the LLM improves classification and captioning tasks due to the style memorization effect (Ghosh et al., 2024a), which benefits retrieval-based evaluation (see Appendix C) but reduces performance on reasoning tasks. (2) Pre-training alignment is essential before fine-tuning. (3) Gradual context-length extension is critical for effective long-audio understanding. Table 6: Overview of 10 training schedules detailing whether CLAP, XATTN, and LLM components (in order) are frozen or unfrozen during each stage and their impact on performance. Training Stages Benchmarks AudioCaps GTZAN MMAU LongAudioB 1 stage 2 stage 3 stage 4 stage - - - - - - - - - - - - - - - - 0.50 0.50 0.56 0.59 0.59 0.52 0.58 0.45 0.62 0. 56.7 52.5 68.9 70.9 72.3 65.7 69.1 62.6 74.2 74.0 58.8 56.3 68.0 63.2 60.7 63. 69.0 59.1 64.5 62.0 5.1 4.9 5.1 5.7 5.7 5.3 6.4 6.2 6.4 6. 7. Conclusion, Limitations and Future Work In this paper, we introduce Audio Flamingo 2, an ALM designed for long audio understanding and expert reasoning. Our model leverages custom CLAP, trained on large8 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities scale dataset with novel objective function, and is trained on synthetic reasoning AQA data to develop unique skills essential for real-world reasoning. Audio Flamingo 2 achieves SOTA performance in audio understanding and reasoning despite being small in model footprints. Additionally, we propose LongAudio and LongAudioBench to advance the field of ALM reasoning over long audio contexts. For future work, we aim to address current limitations, including: (1) enhancing speech content understanding capabilities, (2) scaling AudioSkills to include more diverse datasets, and (3) developing audio encoders inherently capable of processing long audio."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R. J., Javaheripi, M., Kauffmann, P., et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., and Vijayanarasimhan, S. Youtube8m: large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016. Adigwe, A., Tits, N., Haddad, K. E., Ostadabbas, S., and Dutoit, T. The emotional voices database: Towards controlling the emotion dimension in voice generation systems. arXiv preprint arXiv:1806.09514, 2018. Agostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M., Caillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., Sharifi, M., Zeghidour, N., and Frank, C. Musiclm: Generating music from text, 2023. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for fewshot learning. Advances in neural information processing systems, 35:2371623736, 2022. Arumugam, G. P., Chang, S.-Y., Prabhavalkar, T. N. S. R., Wang, Q., and Bijwadia, S. Improved long-form speech recognition by jointly modeling the primary and nonprimary speakers. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 18. IEEE, 2023. Barros, P., Churamani, N., Lakomkin, E., Siqueira, H., Sutherland, A., and Wermter, S. The omg-emotion behavior dataset. In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 17. IEEE, 2018. Bittner, R. M., Salamon, J., Tierney, M., Mauch, M., Cannam, C., and Bello, J. P. Medleydb: multitrack dataset In ISMIR, volfor annotation-intensive mir research. ume 14, pp. 155160, 2014. Bogdanov, D., Won, M., Tovstogan, P., Porter, A., and Serra, X. The mtg-jamendo dataset for automatic music tagging. ICML, 2019. Cao, H., Cooper, D. G., Keutmann, M. K., Gur, R. C., Nenkova, A., and Verma, R. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing, 5(4):377390, 2014. Cartwright, M., Mendez, A. E. M., Cramer, A., Lostanlen, V., Dove, G., Wu, H.-H., Salamon, J., Nov, O., and Bello, J. Sonyc urban sound tagging (sonyc-ust): multilabel dataset from an urban acoustic sensor network. 2019. Chen, C., Hu, Y., Yang, C.-H. H., Siniscalchi, S. M., Chen, P.-Y., and Chng, E.-S. Hyporadise: An open baseline for generative speech recognition with large language models. Advances in Neural Information Processing Systems, 36, 2024a. Chen, C., Peng, P., Baid, A., Xue, Z., Hsu, W.-N., Harwath, D., and Grauman, K. Action2sound: Ambient-aware generation of action sounds from egocentric videos. arXiv preprint arXiv:2406.09272, 2024b. Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. Vggsound: large-scale audio-visual dataset, 2020. Chen, K., Du, X., Zhu, B., Ma, Z., Berg-Kirkpatrick, T., and Dubnov, S. Hts-at: hierarchical token-semantic audio transformer for sound classification and detection. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2022. Cheng, D., Gu, Y., Huang, S., Bi, J., Huang, M., and Wei, F. Instruction pre-training: Language models are supervised multitask learners. arXiv preprint arXiv:2406.14491, 2024. Chiu, C.-C., Han, W., Zhang, Y., Pang, R., Kishchenko, S., Nguyen, P., Narayanan, A., Liao, H., Zhang, S., Kannan, A., et al. comparison of end-to-end models for longform speech recognition. In 2019 IEEE automatic speech recognition and understanding workshop (ASRU), pp. 889896. IEEE, 2019. Chu, Y., Xu, J., Zhou, X., Yang, Q., Zhang, S., Yan, Z., Zhou, C., and Zhou, J. Qwen-audio: Advancing universal audio understanding via unified large-scale audioarXiv preprint arXiv:2311.07919, language models. 2023. Chu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng, Y., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. 9 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. mediating mechanisms through deliberate practice. The psychology of problem solving, pp. 3183, 2003. Defferrard, M., Benzi, K., Vandergheynst, P., and Bresson, X. Fma: dataset for music analysis. arXiv preprint arXiv:1612.01840, 2016. Deng, Z., Ma, Y., Liu, Y., Guo, R., Zhang, G., Chen, W., Huang, W., and Benetos, E. Musilingo: Bridging music and text with pre-trained language models for music captioning and query response. arXiv preprint arXiv:2309.08730, 2023. Deshmukh, S., Elizalde, B., and Wang, H. Audio retrieval with wavtext5k and clap training. arXiv preprint arXiv:2209.14275, 2022. Deshmukh, S., Elizalde, B., Singh, R., and Wang, H. Pengi: An audio language model for audio tasks, 2023. Deshmukh, S., Han, S., Bukhari, H., Elizalde, B., Gamper, H., Singh, R., and Raj, B. Audio entailment: Assessing deductive reasoning for audio understanding. In The 39th Annual AAAI Conference on Artificial Intelligence, 2024. URL arXivpreprintarXiv:2407.18062. Doh, S., Choi, K., Lee, J., and Nam, J. Lp-musiccaps: Llm-based pseudo music captioning. arXiv preprint arXiv:2307.16372, 2023. Drossos, K., Lipping, S., and Virtanen, T. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 736740. IEEE, 2020. Elizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H. Clap learning audio concepts from natural language suIn ICASSP 2023-2023 IEEE International pervision. Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023a. Fonseca, E., Favory, X., Pons, J., Font, F., and Serra, X. Fsd50k: an open dataset of human-labeled sound events. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:829852, 2021. Fonseca, E., Favory, X., Pons, J., Font, F., and Serra, X. Fsd50k: An open dataset of human-labeled sound events, 2022. Font, F., Roma, G., and Serra, X. Freesound technical demo. In Proceedings of the 21st ACM international conference on Multimedia, pp. 411412, 2013. Foster, P., Sigtia, S., Krstulovic, S., Barker, J., and Plumbley, M. D. Chime-home: dataset for sound source recognition in domestic environment. In 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pp. 15. IEEE, 2015. Gardner, J., Durand, S., Stoller, D., and Bittner, R. M. Llark: multimodal foundation model for music. arXiv preprint arXiv:2310.07160, 2023. Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 776780. IEEE, 2017. Ghosh, S., Evuru, C. K. R., Kumar, S., S, R., Aneja, D., Jin, Z., Duraiswami, R., and Manocha, D. closer look at the limitations of instruction tuning. In Fortyfirst International Conference on Machine Learning, 2024a. URL https://openreview.net/forum? id=XkHJo8iXGQ. Elizalde, B., Deshmukh, S., and Wang, H. Natural language supervision for general-purpose audio representations, 2023b. URL https://arxiv.org/abs/ 2309.05767. Ghosh, S., Kumar, S., Kong, Z., Valle, R., Catanzaro, B., and Manocha, D. Synthio: Augmenting small-scale audio classification datasets with synthetic data. arXiv preprint arXiv:2410.02056, 2024b. Engel, J., Resnick, C., Roberts, A., Dieleman, S., Eck, D., Simonyan, K., and Norouzi, M. Neural audio synthesis of musical notes with wavenet autoencoders, 2017a. Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi, M., Eck, D., and Simonyan, K. Neural audio synthesis of musical notes with wavenet autoencoders. In International Conference on Machine Learning, pp. 10681077. PMLR, 2017b. Ericsson, K. A. The acquisition of expert performance as problem solving: Construction and modification of Ghosh, S., Kumar, S., Seth, A., Evuru, C. K. R., Tyagi, U., Sakshi, S., Nieto, O., Duraiswami, R., and Manocha, D. GAMA: large audio-language model with advanced audio understanding and complex reasoning abilities. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 62886313, Miami, Florida, USA, November 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 361. URL https://aclanthology.org/2024. emnlp-main.361/. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Ghosh, S., Seth, A., Kumar, S., Tyagi, U., Evuru, C. K. R., S, R., Sakshi, S., Nieto, O., Duraiswami, R., and Manocha, D. Compa: Addressing the gap in compositional reaIn The Twelfth Insoning in audio-language models. ternational Conference on Learning Representations, 2024d. URL https://openreview.net/forum? id=86NGO8qeWs. Ghosh, S., Kumar, S., Evuru, C. K. R., Nieto, O., Duraiswami, R., and Manocha, D. Reclap: Improving zero shot audio classification by describing sounds. 2025. Gong, Y., Chung, Y.-A., and Glass, J. Ast: Audio spectrogram transformer. arXiv preprint arXiv:2104.01778, 2021. Gong, Y., Liu, A. H., Luo, H., Karlinsky, L., and Glass, J. Joint audio and speech understanding. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 18, 2023a. doi: 10.1109/ASRU57964. 2023.10389742. Gong, Y., Rouditchenko, A., Liu, A. H., Harwath, D., Karlinsky, L., Kuehne, H., and Glass, J. R. Contrastive audio-visual masked autoencoder. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum? id=QPtMRyk5rb. Gong, Y., Luo, H., Liu, A. H., Karlinsky, L., and Glass, In The Twelfth J. R. Listen, think, and understand. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=nBZBPXdJlC. Guzhov, A., Raue, F., Hees, J., and Dengel, A. Audioclip: In ICASSP Extending clip to image, text and audio. 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 976980. IEEE, 2022. Heittola, T., Mesaros, A., and Virtanen, T. Tau urban acoustic scenes 2019 openset, development dataset.. Hershey, S., Ellis, D. P., Fonseca, E., Jansen, A., Liu, C., Moore, R. C., and Plakal, M. The benefit of temporallystrong labels in audio event classification. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 366370. IEEE, 2021. Huang, J. and Chang, K. C.-C. Towards reasoning in arXiv preprint large language models: survey. arXiv:2212.10403, 2022. talking head. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 2380223804, 2024. Islam, M. M., Ho, N., Yang, X., Nagarajan, T., Torresani, L., and Bertasius, G. Video recap: Recursive captioning of hour-long videos. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1819818208, 2024. doi: 10.1109/CVPR52733.2024. 01723. James, J., Tian, L., and Watson, C. An open source emotional speech corpus for human robot interaction applications. Interspeech 2018, 2018. Jeong, I.-Y. and Park, J. Cochlscene: Acquisition of acoustic scene data using crowdsourcing. In 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pp. 1721. IEEE, 2022. Ju, X., Gao, Y., Zhang, Z., Yuan, Z., Wang, X., Zeng, A., Xiong, Y., Xu, Q., and Shan, Y. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119132, 2019. Koepke, A. S., Oncescu, A.-M., Henriques, J. F., Akata, Z., and Albanie, S. Audio retrieval with natural language queries: benchmark study. IEEE Transactions on Multimedia, 25:26752685, 2022. Kong, Z., Goel, A., Badlani, R., Ping, W., Valle, R., and Catanzaro, B. Audio flamingo: novel audio language model with few-shot learning and dialogue abilities. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/ forum?id=WYi3WKZjYe. Kulesza, A., Taskar, B., et al. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 5(23):123286, 2012. Kumar, S., Seetharaman, P., Salamon, J., Manocha, D., and Nieto, O. Sila: Signal-to-language augmentation for enhanced control in text-to-audio generation. arXiv preprint arXiv:2412.09789, 2024. Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., et al. Audiogpt: Understanding and generating speech, music, sound, and Law, E., West, K., Mandel, M. I., Bay, M., and Downie, J. S. Evaluation of algorithms using games: The case of music tagging. In ISMIR, pp. 387392. Citeseer, 2009. 11 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Melechovsky, J., Guo, Z., Ghosal, D., Majumder, N., Herremans, D., and Poria, S. Mustango: Toward controllable text-to-music generation. arXiv preprint arXiv:2311.08355, 2023. Li, G., Wei, Y., Tian, Y., Xu, C., Wen, J.-R., and Hu, D. Learning to answer questions in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19108 19118, 2022. LI, Y., Yuan, R., Zhang, G., Ma, Y., Chen, X., Yin, H., Xiao, C., Lin, C., Ragni, A., Benetos, E., Gyenge, N., Dannenberg, R., Liu, R., Chen, W., Xia, G., Shi, Y., Huang, W., Wang, Z., Guo, Y., and Fu, J. MERT: Acoustic music understanding model with large-scale selfsupervised training. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=w3YZ9MSlBu. Lipping, S., Sudarsanam, P., Drossos, K., and Virtanen, T. Clotho-aqa: crowdsourced dataset for audio question answering. In 2022 30th European Signal Processing Conference (EUSIPCO), pp. 11401144. IEEE, 2022. Liu, S., Hussain, A. S., Sun, C., and Shan, Y. Music understanding llama: Advancing text-to-music generation In ICASSP with question answering and captioning. 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 286290. IEEE, 2024. Livingstone, S. R. and Russo, F. A. The ryerson audiovisual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english. PloS one, 13(5):e0196391, 2018. Lostanlen, V., Cella, C.-E., Bittner, R., and Essid, S. Medley-solos-DB: cross-collection dataset for musical instrument recognition, February 2019. URL https: //doi.org/10.5281/zenodo.1344103. Lotfian, R. and Busso, C. Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings. IEEE Transactions on Affective Computing, 10(4):471483, 2017. Martin Morato, I. and Mesaros, A. Diversity and bias in audio captioning datasets. 2021. Mei, X., Meng, C., Liu, H., Kong, Q., Ko, T., Zhao, C., Plumbley, M. D., Zou, Y., and Wang, W. Wavcaps: chatgpt-assisted weakly-labelled audio captioning dataset IEEE/ACM for audio-language multimodal research. Transactions on Audio, Speech, and Language Processing, 2024. Morato, I. M. and Mesaros, A. Macs - multi-annotator captioned soundscapes, July 2021. URL https:// doi.org/10.5281/zenodo.5114771. Morris, M. R., Sohl-Dickstein, J., Fiedel, N., Warkentin, T., Dafoe, A., Faust, A., Farabet, C., and Legg, S. Position: Levels of AGI for operationalizing progress on the path to AGI. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 3630836321. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/morris24b.html. Oncescu, A.-M., Koepke, A., Henriques, J. F., Akata, Z., and Albanie, S. Audio retrieval with natural language queries. arXiv preprint arXiv:2105.02192, 2021. Pichora-Fuller, M. K. and Dupuis, K. Toronto emotional speech set (tess). Scholars Portal Dataverse, 1:2020, 2020. Piczak, K. J. ESC: Dataset for Environmental Sound ClasIn Proceedings of the 23rd Annual ACM sification. Conference on Multimedia, pp. 10151018. ACM Press, 2015. ISBN 978-1-4503-3459-4. doi: 10.1145/2733373. 2806390. URL http://dl.acm.org/citation. cfm?doid=2733373.2806390. Poria, S., Hazarika, D., Majumder, N., Naik, G., Cambria, E., and Mihalcea, R. Meld: multimodal multi-party dataset for emotion recognition in conversations. arXiv preprint arXiv:1810.02508, 2018. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. 12 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Rafii, Z., Liutkus, A., Stoter, F.-R., Mimilakis, S. I., and Bittner, R. Musdb18-hq-an uncompressed version of musdb18. (No Title), 2019. et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Rahman, M. A., Hakim, Z. I. A., Sarker, N. H., Paul, B., and Fattah, S. A. Sonics: Synthetic or notidentifying counterfeit songs. arXiv preprint arXiv:2408.14080, 2024. Rashid, M. M., Li, G., and Du, C. Nonspeech7k dataset: Classification and analysis of human non-speech sound. IET Signal Processing, 17(6):e12233, 2023. Sakshi, S., Tyagi, U., Kumar, S., Seth, A., Selvakumar, R., Nieto, O., Duraiswami, R., Ghosh, S., and Manocha, D. Mmau: massive multi-task audio understanding and reasoning benchmark. arXiv preprint arXiv:2410.19168, 2024. Salamon, J., Jacoby, C., and Bello, J. P. dataset and taxonomy for urban sound research. In Proceedings of the 22nd ACM international conference on Multimedia, pp. 10411044, 2014. Selvakumar, R., Kumar, S., Giri, H. K., Anand, N., Seth, A., Ghosh, S., and Manocha, D. Do audio-language models understand linguistic variations? arXiv preprint arXiv:2410.16505, 2024. Silva, A., Whitehead, S., Lengerich, C., and Leather, H. J. CoLLAT: On adding fine-grained audio understanding to language models using token-level locked-language In Thirty-seventh Conference on Neural Intuning. formation Processing Systems, 2023. URL https: //openreview.net/forum?id=2NncD8AaFK. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Venkataramanan, S., Rizve, M. N., Carreira, J., Asano, Y. M., and Avrithis, Y. Is imagenet worth 1 video? learning strong image encoders from 1 long unlabelled video. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=Yen1lGns2o. Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., Soplin, N. E. Y., Heymann, J., Wiesner, M., Chen, N., et al. Espnet: End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015, 2018. Weck, B., Manco, I., Benetos, E., Quinton, E., Fazekas, G., and Bogdanov, D. Muchomusic: Evaluating music understanding in multimodal audio-language models. arXiv preprint arXiv:2408.01337, 2024. Weng, Y., Han, M., He, H., Chang, X., and Zhuang, B. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pp. 453470. Springer, 2025. Wolf, Y., Wies, N., Avnery, O., Levine, Y., and Shashua, A. Fundamental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082, 2023. Sturm, B. L. The gtzan dataset: Its contents, its faults, their effects on evaluation, and its future use. arXiv preprint arXiv:1306.1461, 2013. Wu, H., Seetharaman, P., Kumar, K., and Bello, J. P. Wav2clip: Learning robust audio representations from clip. In ICASSP 2022, 2022a. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., MA, Z., and Zhang, C. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=14rn7HpKVk. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., Wu, H.-H., Nieto, O., Bello, J. P., and Salamon, J. Audiotext models do not yet leverage natural language. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023a. Wu, Y., Chen, K., Zhang, T., Hui, Y., BergLarge-scale conKirkpatrick, T., and Dubnov, S. trastive language-audio pretraining with feature fuICASSP sion and keyword-to-caption augmentation. 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15, 2022b. URL https://api.semanticscholar. org/CorpusID:253510826. Wu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T., and Dubnov, S. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023 - 2023 IEEE International 13 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15, 2023b. doi: 10.1109/ICASSP49357. 2023.10095969. Xue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y., Tang, H., Yang, S., Liu, Z., et al. Longvila: Scaling longcontext visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Yang, Q., Xu, J., Liu, W., Chu, Y., Jiang, Z., Zhou, X., Leng, Y., Lv, Y., Zhao, Z., Zhou, C., et al. Air-bench: Benchmarking large audio-language models via generative comprehension. arXiv preprint arXiv:2402.07729, 2024b. Yuan, Y., Jia, D., Zhuang, X., Chen, Y., Liu, Z., Chen, Z., Wang, Y., Wang, Y., Liu, X., Kang, X., et al. Soundvecaps: Improving audio generation with visual enhanced captions. In Audio Imagination: NeurIPS 2024 Workshop AI-Driven Speech, Music, and Sound Generation, 2024. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. 14 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities A. Table of Contents 1. : AF-CLAP 2. : Limitations of CLAP-Retrieval-Based Evaluation 3. : AudioSkills 4. : LongAudio 5. : Prompts 6. : More Results and Design Analysis 7. G.1 : Effect of RoPE 8. G.2 : Effect of Audio Transformation Layers 9. G.3 : Effect of Cross-Attention Frequency 10. : More Details B. AF-CLAP B.1. Training Hyper-parameters We train AF-CLAP on 8 A100 80GB GPUs. We train it with learning rate of 5e-4, =N =3, and an effective batch size of 256 for 12 epochs. This batch size is smaller than that in the literature, but we do so due to computational constraints. B.2. Feature Extraction Layer As described in Section 3.2, unlike Audio Flamingo which uses CLAP audio embeddings directly, AF2 discards the CLAP head and rather leverages dense features from the final layer of the audio encoder (HTS-AT in our case). Table 8 compares the performance of AF2 trained with CLAP head features versus dense audio features from the encoders last layer. The results clearly demonstrate that using dense features significantly improves AF2s performance. B.3. Training Datasets Table 9 provides detailed statistics of datasets used for training AF-CLAP. B.4. Comparison with prior-art Table 7 presents the performance of our CLAP on audio-totext and text-to-audio retrieval tasks using the Clotho and AudioCaps datasets. AF-CLAP 630k refers to AF-CLAP trained using our proposed strategy but with Laion CLAPs 630k audio-text pairs. AF-CLAP w/ van. con. represents AF-CLAP trained with the same data but using the standard contrastive loss formulation. AF-CLAP w/o noise red. represents our AF-CLAP trained using the same data and method but without our noise reduction step. Our final proposed AF-CLAP achieves state-of-the-art performance across all metrics. Overall, AF-CLAP 630k does not lead to any improvements over Laion-CLAP, as our data augmentation and cleaning are only applicable to captions obtained from complex real-world audios, which represents only significantly small portion of Laion-CLAP. Table 10 highlights the performance of AF-CLAP on zeroshot audio classification benchmarks, where it consistently achieves SOTA results. We make the same conclusion for Laion-CLAP as previously stated. We emphasize that benchmark datasets do not holistically evaluate CLAP models capabilities, as highlighted in several works (Ghosh et al., 2024d; Wu et al., 2023a; Selvakumar et al., 2024; Ghosh et al., 2025). While AF-CLAP outperforms baselines on benchmark datasets, its audio features are significantly more robust and enhance the audio perception capabilities of (L)ALMs (see also Table 3). C. Limitations of CLAP Retrieval Based"
        },
        {
            "title": "Evaluation",
            "content": "We attribute the suboptimal performance of AF2 on some datasets to the limitations of the CLAP-based evaluation method, which often fails to retrieve the correct label corresponding to the models open-ended generation response. we follow the evaluation scheme introduced by Deshmukh et al. (2023) and widely adopted in prior works (Gong et al., 2024; Ghosh et al., 2024c). This approach uses CLAP model to retrieve label from the label set by comparing the models open-ended generation and assigning the label with the highest similarity as the predicted output. We show some examples below where even correct prediction by AF2 leads to incorrect retrieval and therefore lower accuracy: Dataset: GTZAN, Correct Label: Rock, Predicted Label: Punk Metal, Retrieved Label: Metal Dataset: ESC50, Correct Label: Pouring water, Predicted Label: liquid, , Retrieved Label: Water drops However, fine-tuning the LLM leads to style memorization, which favors this evaluation method. We show some examples of prediction shift and how this leads to increase in accuracy: Dataset: GTZAN, Correct Label: rock, Predicted Label: rock, Retrieved Label: rock Dataset: ESC50, Correct Label: clock alarm, Predicted Label: clock alarm, Retrieved Label: clock alarm D. AudioSkills D.1. Dataset Statistics Table 14 presents detailed category-wise statistics on our proposed AudioSkills dataset. We also list the meta-data used for data generation. For meta-data, transcripts were 15 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 7: Performance comparison of AF-CLAP with baselines on Text-to-Audio and Audio-to-Text retrieval on AudioCaps and Clotho. Model Text-to-Audio Audio-to-Text Text-to-Audio Audio-to-Text R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@ AudioCaps Clotho MMT ML-ACT CLAP CompA-CLAP Laion-CLAP AF-CLAP-630k AF-CLAP w/ van. con. AF-CLAP w/o noise red. AF-CLAP (ours) 36.1 33.9 34.6 36.1 36.1 36.3 36.8 35.4 37. 72.0 69.7 70.2 72.6 71.8 71.8 72.0 70.7 72.9 84.5 82.6 82.0 81.6 83.9 83.9 84.0 81.6 84.0 39.6 39.4 41.9 45.2 46.8 46.3 45.2 45.0 46. 76.8 72.0 41.9 80.1 82.9 83.0 83.5 82.6 84.1 86.7 83.9 84.6 86.7 90.7 90.9 90.0 91.0 91.9 6.7 14.4 16.7 16.8 16.1 16.1 16.5 16.3 17. 21.6 36.6 41.1 43.5 38.3 38.3 41.9 40.5 43.9 33.2 49.9 54.1 56.1 51.1 51.4 55.3 52.7 56.8 7.0 16.2 20.0 19.7 22.7 22.7 22.9 22.8 23. 22.7 37.6 44.9 45.2 48.5 48.7 51.0 51.2 51.2 34.6 50.2 58.7 55.6 60.8 60.8 63.2 59.4 63.5 Table 8: Performance comparison of AF2 with different audio feature extraction methods from CLAP. Head refers to audio features extracted from the CLAP head, and Dense refers to dense features extracted from the last layer of the audio encoder. Model Head R1256 Dense R64 AudioCaps GTZAN MuchoMusic MMAU 0.50 0.58 54.7 69.1 42.6 56.5 47.0 69.0 Table 9: Statistics of audio-caption datasets used for CLAP training. indicates the dataset was collected by us. Furthermore, all datasets with captions were made to go through our cleaning stage to reduce noise, as mentioned in Section 3.1.1. Dataset YouTube-8M (Abu-El-Haija et al., 2016) Sound-VECaps (Yuan et al., 2024) MiraData (Ju et al., 2024) Action2sound (Chen et al., 2024b) NSynth (Engel et al., 2017b) Freesound (Font et al., 2013) AudioSet Strong (Hershey et al., 2021) VGGSound (Chen et al., 2020) FMA (Defferrard et al., 2016) Video Recap (Islam et al., 2024) CochlScene (Jeong & Park, 2022) FSD50k (Fonseca et al., 2022) MACS (Morato & Mesaros, 2021) BBC 1 MagnaTagATune (Law et al., 2009) SoundDescs (Koepke et al., 2022) Clotho (Drossos et al., 2020) TAU-Urban (Heittola et al.) MusicCaps (Agostinelli et al., 2023) WavText5K (Deshmukh et al., 2022) SONICS (Rahman et al., 2024) SoundBible2 MUSDB18 (Rafii et al., 2019) Medleybd-Pitch (Bittner et al., 2014) Total #Audio-Text Pairs 3,947,057 1,657,029 748,320 306,602 289,205 256,695 216,622 185,161 106,412 64,627 60,855 40,966 31,675 31,201 25,863 23,085 19,195 14,400 5,479 4,347 1,602 935 276 103 8,037,712 obtained from Whisper Large-v3 (Radford et al., 2023). We generate all audio captions from Qwen2-Audio and visual captions from Qwen2-VL. 16 E. LongAudio E.1. Detailed Statistics Table 11 presents detailed statistics of LongAudio and LongAudioBench, categorized into the various types of QAs. E.2. Examples Table 15 shows category-wise examples from LongAudio. E.3. Comparison with other datasets Table 12 compares the duration statistics of LongAudio with other AQA datasets. LongAudio stands out with the longest average audio durations. E.4. Fine-grained results for AF2 Table 13 presents category-wise fine-grained results of AF2 on LongAudioBench. While AF2 demonstrates strong performance, fine-tuning on LongAudio further improves scores across all categories, particularly in tasks unique to LongAudioBench, such as NeedleQA and SubsceneQA, which were not encountered during AF2s two-stage training. These results emphasize the significance of our proposed LongAudio dataset in effectively extending (L)ALM context length and improving long-audio reasoning. E.5. Success and Failure Cases on LongAudioBench Table 22 presents success and failure cases of AF2 on LongAudioBench. E.6. Metadata For MiraData, metadata includes visual captions (e.g., main object, background, and style) as detailed in the original paper and shown in prompt 9. For Video ReCap, metadata focuses on action captions describing activities, as shown in prompt 15. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 10: Performance comparison of our CLAP with baselines on Zero-shot Audio classification benchmarks. Model Wav2CLIP AudioClip CLAP Laion-CLAP CoLLAT CompA-CLAP AF-CLAP-630k AF-CLAP w/ van. con. AF-CLAP w/o noise red. AF-CLAP (ours) ESC-50 US8K VGGSound FSD50K TUT AudioSet NSynth 41.4 69.4 82.6 88.2 84.0 86.5 88.1 91.0 91.1 91.3 40.4 65.3 73.2 74.1 77.0 88. 74.3 91.8 91.3 92.3 10.0 9.9 16.4 21.2 - 21.9 21.2 23.5 23.9 24.1 3.0 6.6 14.0 22.4 19.0 19.6 22.0 28.1 26.9 27.2 28.2 29.5 29.6 58.4 29.0 56. 57.5 63.0 63.2 63.2 5.0 3.7 5.1 20.8 9.0 21.6 19.3 21.8 22.8 23.7 5.9 6.8 9.9 11.8 - 11.8 12.0 17.4 15.5 15.9 Table 11: Dataset statistics for LongAudio and LongAudioBench. Category LongAudio LongAudioBench Captioning Plot QA Complex QA Subscene QA Temporal QA General QA 67,498 61,511 50,315 7,908 66,836 8,860 917 237 361 176 253 485 Total 262,928 2,429 Table 12: Comparison of duration statistics of LongAudio with other AQA datasets. LongAudio stands out with the longest average audio durations. Method Clotho-AQA (Lipping et al., 2022) AudioEntailment (Deshmukh et al., 2024) CompA-R (Ghosh et al., 2024c) OpenAQA (Gong et al., 2024) MU-LLAMA (Liu et al., 2024) Salmonn (Tang et al., 2024) LongAudio (ours) Min 15.00 15.00 0.47 0.06 29.12 0.47 5.00 Max Avg 30.00 30.00 10.01 180.00 29.12 1069.04 1797.71 22.53 22.49 9.87 12.01 29.12 10.62 117.08 Table 13: Fine-grained scores of AF2 on LongAudioBench w/ and w/o fine-tuning on our dataset LongAudio. Category AF2 AF2 w/o LongAudio 63.75% Captioning Plot QA 68.02% Temporal QA 62.61% Needle QA 63.13% Subscene QA 64.03% 63.61% General QA Avg 64.19% 46.02% 44.15% 51.00% 35.92% 33.61% 42.39% 50.22% E.7. Human Verification for LongAudioBench The human verification process has been approved by our institutions Institutional Review Board (IRB). Annotation Method. The annotation process for LongAudioBench was carried out in two stages by group of experts. In the first stage, experts corrected and verified each QA pair, including discarding completely erroneous pairs. In the second stage, experts reviewed QA pairs corrected by Figure 8: Snapshot of the annotation tool used for LongAudio annotation. each other. Thus, each QA pair was at least annotated by 2 individuals. Their primary role in both stages was to annotate the data while ensuring accuracy, consistency, and adherence to the predefined guidelines (mentioned next). We build an annotation tool for this purpose, and Fig. 8 provides snapshot of the tool. This multi-stage process, involving annotation followed by detailed review, was designed to enhance the reliability and depth of the annotated dataset, leveraging the combined expertise and experience of the annotators. Annotation Guidelines. The authors of the paper put forward the following annotation guidelines for annotating and verifying LongAudioBench: Accuracy and Consistency: Ensure all annotations Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 14: Detailed statistics of AudioSkills, categorized into individual reasoning types, together with details on open-source datasets, additional meta-data, and prompt used for QA generation. Reasoning Type Size Datasets Used Meta-Data Used Temporal Attribute Identification Counting Contextual Sound Event Reasoning Contextual Speech Event Reasoning Information Extraction General Reasoning AudioSet-SL, MusicBench AudioSet-SL Synthetic AudioSet-SL, MusicBench, FMA 188,799 201,455 50,493 982,847 1,272,782 AudioSet-SL 858,639 704,040 AudioSet-SL, MusicBench, MTG-Jamendo, MusicNet Time Stamped Events, Caption Time Stamped Events, Caption AudioSet-SL, MusicBench, MusicNet, FMA Time Stamped Events (GT), Caption Attribute values from Kumar et al. (2024), Caption Transcript, Caption Time Stamped Events, Caption Transcript, Caption Prompt Reference Fig. 9, 10 Fig. 26 pythonic Fig. 13,27, 28, 29 Fig. 30 Fig. 31, 32, 33, 34, 37, 36 Fig. 27, 33, 34, 35, 36 are accurate and consistent, strictly adhering to the predefined standards and guidelines. Two-Stage Process: Ensure all QAs go through the 2-stage verification process mentioned above. Listening Requirements: Listen to the complete audio before annotating to ensure the QA pair is contextually accurate. QA Pair Validation: i) Ensure the audio in the QA pair is valid, not corrupt, and corresponds to the question. ii) Discard or flag QA pairs containing irrelevant or ambiguous content. Question Format: i) All questions must be in English. ii) Check if the tagged QA type (e.g., captioning, PlotQA) is correct and relevant. iii) Avoid mentioning identifiable information about the audio, such as names or metadata. Answer Format: i) All answers must follow the MCQ or open-ended format, pre-defined for each question category. ii) Additionally, annotators should ensure answers are comprehensive and free of ambiguities. Tool Usage: i) Use the annotation tool  (Fig. 8)  for all corrections, reviews, and annotations. ii) Document and report any technical issues encountered during the process. Quality Assurance: i) Each QA pair must be reviewed by at least two individuals to ensure reliability and adherence to the guidelines. ii) Maintain detailed logs of discarded QA pairs and reasons for exclusion. iii) Escalate ambiguous or complex QA pairs to the team for collective review. Collaboration: i) Maintain open communication with other annotators to clarify task-specific doubts or resolve disagreements during review. ii) Share feedback on ambiguous QA pairs to iteratively improve the annotation process. F. Prompts F.1. Prompts for foundational datasets AQA format for training AF2. For the answers, only the label was retained, excluding any additional text. Initially, we experimented with adding prefix to the labels to convert them into sentence format, but this approach did not yield favorable results. F.2. Prompts for other tasks Table. 26 and 26 list prompts we used to convert foundational audio classification and captioning datasets into AQA format for training AF2. 1. Prompt 9: Prompt used for generating Temporal QA for LongAudio from MiraData. 2. Prompt 11: Prompt used for generating Plot QA for LongAudio from MiraData. 3. Prompt 12: Prompt used for generating Caption QA for LongAudio from MiraData. 4. Prompt 13: Prompt used for generating Contextual Sound Event Reasoning QA for AudioSkills. 5. Prompt 14: Prompt used for generating Caption QA for Video-ReCap. 6. Prompt 15: Prompt used for generating Subscene QA for LongAudio from Video-ReCap. 7. Prompt 16: Prompt used for generating General QA for LongAudio from Video-ReCap. 8. Prompt 17: Prompt used for generating Subscene QA for LongAudio from MiraData. 9. Prompt 18: Prompt used for generating Temporal Reasoning QA for AudioSkills. 10. Prompt 19: Prompt used for generating linguistically variant positives for short-audio captions for CLAP training. 11. Prompt 20: Prompt used for cleaning and removing noise from synthetic short-audio captions. 12. Prompt 21: Prompt used for generating compositionally different negatives for short-audio captions for CLAP training. 13. Prompt 22: Prompt used for generating synthetic short-audio captions from Video ReCAP. Tables 26 and 27 list the prompts used to convert foundational audio classification and captioning datasets into the 14. Prompt 23: Prompt used for generating synthetic short-audio captions from MiraData. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities"
        },
        {
            "title": "Answer",
            "content": "Table 15: Category-wise examples from LongBench. How would you describe the scene from the audio?"
        },
        {
            "title": "Captioning",
            "content": "What is taking place in the audio? What is the predominant mood conveyed by the background music and sound effects in the audio segments?"
        },
        {
            "title": "Plot QA",
            "content": "Two women converse in car with ambient bird chirping and occasional mechanical noises in the background. The audio features ambient wind, distant motor vehicle hums, and occasional background noises creating an eerie atmosphere. Sad and suspenseful. What is the primary theme conveyed by the audio in the initial segments? The audio conveys solemn and spiritual theme with classical music and religious references. Temporal QA What is the correct order of sounds in the audio? Choose the correct option among the options below: (A) clock ticking, flute melody, woman speaking in English (B) Footsteps outdoors, man speaking, piano melody (C) flute melody, woman speaking in English, footsteps outdoors (D) clock ticking, man speaking, piano melody When can the sound of horse galloping be heard? Choose the correct option among the options below: (A) At the beginning (B) In the middle (C) Towards the end (A) clock ticking, flute melody, woman speaking in English (B) In the middle What audio event indicates an interruption during the conversation between the man and woman? The background sound of phone ringing from 0.53 seconds to 7.40 seconds. Needle QA What audio element creates sense of suspense in the dark room scene? The cinematic strings playing scary tune along with church bell and distant horror scream. What happened between the sound of ocean waves and the sound of mechanisms functioning? The sound of phone being dropped on table followed by mechanical fan noises. What sounds occur between brushing teeth and using phone? Music playing continuously with occasional running water and shower sound. What continuous background noise is heard while the person is using the laptop? The sound of an engine idling consistently. Subscene QA General QA What was the continuous background noise throughout the audio? Engine idling. 15. Prompt 24: Prompt used for rewriting audio captions 17. Prompt 26: Prompt used for generating Attribute QA and removing implausible acoustic elements. for AudioSkills. 16. Prompt 25: Prompt used for evaluating responses for 18. Prompt 27: Prompt used for generating Complex QA questions in LongAudioBench. for AudioSkills. 19 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities 19. Prompt 35: Prompt used for generating QA for AudioSkills using FMA. 20. Prompt 36: Prompt used for generating QA for AudioSkills using MusicNet. 21. Prompt 37: Prompt used for generating QA for AudioSkills using MTG-Jamendo. 22. Prompt 38: Prompt used for self-verification of generTable 18: Comparison of performance with various crossattention conditioning frequencies. Frequency AudioCaps GTZAN MuchoMusic MMAU (avg) 6 3 1 0.50 0.58 0.58 56.7 70.0 69.1 50.1 54.4 56. 61.7 65.7 69.0 ated QA pairs for MiraData. G.4. Results on different LLM Sizes 23. Prompt 39: Prompt used for self-verification of generated QA pairs for ReCap dataset. Table 19 compares the performance of AF2 on various LLM sizes, ranging from 0.5B to 7B. G. More Results and Design Analysis G.1. Effect of RoPE Table 16 compares the performance of AF2 with and without applying RoPE in the audio transformation layers. We see drop in performance in LongAudioBench and MMAU without the application of RoPE, which highlights its importance in long-context and reasoning capabilities. Table 16: Performance comparison of AF2 with and without RoPE in the audio transformation layer. Frequency AudioCaps GTZAN LongAudioB MMAU (avg) w/o RoPE w/ RoPE 0.56 0.58 67.2 69.1 4.3 6.4 59.1 69.0 G.2. Effect of Audio Transformation Layers Table 17 compares the performance of AF2 with and without the audio transformation layers. We see drop in performance without the audio transformation layers, which highlights its importance in expanding audio representation learning and adaptation capacity. Table 17: Performance comparison of AF2 with and without the audio transformation layer. Frequency AudioCaps GTZAN MuchoMusic MMAU (avg) w/o transform w/ transform 0.55 0.58 64.3 69. 51.8 56.5 59.0 69.0 G.3. Effect of Cross-Attention Frequency Table 18 compares AF2s performance across different cross-attention conditioning frequencies. As reminder, conditioning the audio representations using cross-attention after every LM layer (frequency of 1) yields the best performance. However, conditioning every 3rd layer performs competitively, with noticeable performance drop when conditioning is reduced to every 6th layer. Table 19: Results for AF2 on different LLM sizes, ranging from 0.5B - 7B. Dataset AF2-0.5B AF2-1.5B AF2-3B AF2-7B AudioCaps GTZAN ClothoAQA MMAU Sound MMAU Music MuchoMusic CompA-R-test 0.46 65.6 80.4 61.0 68.0 49.7 82.4 0.50 66.1 82.7 65.0 70.9 52.4 92.4 0.58 69.1 86.9 64.4 72.9 56.5 96.4 0.58 68.8 85.1 65.0 73.1 57.2 95. H. More Details H.1. Topic-wise Video Segmentation To segment videos into topic-specific clusters, we adopted multi-step clustering approach utilizing both textual of video captions. Below, we detail the methodology: 1. Feature Extraction: We extracted semantic feature embeddings from video captions to represent each video. To achieve this, we employed NV-Embed (Lee et al., 2024). 2. Clustering: Next, we applied K-Means Clustering to group videos based on their feature representations. 3. Cluster Selection and Diversity: After clustering, we analyzed the clusters to ensure relevance and diversity. For each cluster, representative subset of videos was selected using combination of random sampling and Determinantal Point Processes (DPPs) (Kulesza et al., 2012), diversitybased scoring method. This ensured that selected videos captured the range of topics present in each cluster while avoiding redundancy. 4. Manual Review and Refinement: To further ensure the quality and relevance of the selected videos, we conducted manual review of random sample from each cluster. Videos with ambiguous or low-quality content were discarded or reassigned as needed. 20 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities far. If adding the next audio clip violates either max tokens or max sentences, the current batch is finalized, and new batch is started. Additionally, if the batch size is multiple of bsz mult, we consider finalizing the batch early to meet this alignment requirement. Any remaining examples at the end of the data list form the final batch. This two-stage approachfirst weighted bucketed blending, followed by dynamic batchingensures: 1. Multiple datasets contribute examples proportionally to their desired weights. 2. Effective shuffling to prevent overfitting. 3. Efficient grouping of audio clips by length, reducing wasted padding and speeding up training. Effectiveness. Our dynamic batching scheme reduces the percentage of paddings in each batch from 58% to 16%. H.3. Computational Analysis Training Cost. We trained Audio Flamingo 2 on 128 NVIDIA A100 80GB GPUs for each stage and variant. For the original version with the 3B model, stage 1 takes about 4 days to train, stage 2 about 5 days, and stage 3 about 12 hours. We provide detailed cost analysis in Table 20. Table 20: Final model training cost comparison of Audio Flamingo 2. Costs are estimated using AWS (e.g., https: //aws.amazon.com/ec2/instance-types/p4/), and excludes LM training costs and other failures. The cost of experimentation is about 5 the final model training cost. LLM Size Total Params GPU Hours Estimated Cost 0.5B 1.5B 3B 7B 1.0B 2.5B 4.7B 11.0B 104 160 228 USD 55K USD 84K USD 120K USD 268K H.2. Data Loader AF2 is trained on audio datasets with durations ranging from 0.5 seconds to 10 minutes. Using generic data loader would result in excessive padding within batches, leading to unstable losses. To address this, we implement dynamic batching scheme that groups audio samples of similar lengths, minimizing padding. This approach is further constrained by predefined maximum duration for each training job. The algorithm is detailed in Algorithm 1 and 2, with its two main components explained below: Weighted Bucketed Blending. We begin with several datasets, each assigned specific weight that determines the relative number of items sampled from the dataset per epoch. For each dataset, audio clips are divided into buckets based on their duratione.g., short clips are grouped in one bucket, medium-length clips in another, and so on. This bucketing process minimizes padding overhead by ensuring that only audio clips of similar lengths are grouped together. During training epoch e, the number of items to sample from each bucket is determined by the following indices: start idx = bucket size weight % bucket size, end idx = (e + 1) bucket size weight% bucket size. These indices define slice of the bucket for the current epoch, ensuring that each dataset and bucket contributes If the index completes controlled number of items. an entire bucket (i.e., when mod bucket total = 0), the bucket is shuffled deterministically using seed derived from the dataset name and the current epoch. This approach maintains randomness across epochs while ensuring reproducibility. Dynamic Batching. After blending all datasets and gathering their examples into single list, we apply Dynamic Batching to form more efficient mini-batches. The primary goal is to group audio clips of similar lengths to minimize the total number of padded frames per mini-batch. Each audio examples token count (e.g., frame count or another relevant measure of length) is used to incrementally add examples to batch, subject to the following constraints: max sentences: The maximum number of examples allowed in batch. max tokens: The maximum total frames/tokens permitted in batch, calculated as batch size max audio length. bsz mult: An optional alignment constraint requiring that the batch size either be smaller than this multiplier or multiple of it, often for multi-GPU efficiency. At each step, we compute the total frames/tokens in the tentative new batch, considering the maximum frame length so 21 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 21: List of fine pre-training and fine-tuning datasets together with their training composition."
        },
        {
            "title": "Audio Length",
            "content": "#Audio-Text Pairs"
        },
        {
            "title": "SFT Epochs",
            "content": "AudioSkills CompA-R MusicBench Mu-LLAMA Salmonn AQA ClothoAQA OpenAQA Clotho-v2 MACS FSD50k CochlScene NonSpeech 7k Chime-home Sonyc-UST Emov-DB JL-Corpus Tess OMGEmotion MusicAVQAaudio-only MusicQA LP-MusicCapsMSD LP-MusicCapsMTT LP-MusicCapsMC MusicCaps NSynth MTG-Jamendo MusDB-HQ FMA OpenAQA Laion630kBBCSoundEffects Laion630kFreesound SoundDescs WavCaps AudioSet WavText5K MSP-Podcast MELD MusicAVQAaudio-visual MusicQA LP-MusicCapsMSD MTG-Jamendo - 159 hrs 115.5 hrs 62.9 hrs 800 hrs 7.4 hrs 693.2 hrs 24.0 hrs 10.9 hrs 80.8 hrs 169.0 hrs 6.2 hrs 5.0 hrs 34.9 hrs 7.8 hrs 1.4 hrs 1.6 hrs 3.0 hrs 77.1 hrs 62.9 hrs 5805.7 hrs 126.4 hrs 7.4 hrs 7.4 hrs 321.3 hrs 3768.9 hrs 29.1 hrs 860.7 hrs 693.2 hrs 456.9 hrs 2494.8 hrs 749.7 hrs 3793.3 hrs 2617.8 hrs 23.8 hrs 73.9 hrs 8.7 hrs 142.4 hrs 62.9 hrs 5805.7 hrs 3768.9 hrs 4200K 350k 686k 70k 270k 9.7K 1959.8K 19.2K 17.3K 41.0K 60.9K 6.3K 4.5K 27.9K 6.8K 2.4K 2.8K 1.7K 5.7K 70K 1331.8K 46.9K 7.9K 2.6K 289.2K 55.6K 10.2K 104.2K 1959.8K 15.1K 306.5K 23.1K 402.6 950.8K 4.3K 45.1K 32.9K 17.9K 70K 1331.8K 55.6K 22 - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1.0 5.0 1.0 1.0 1.0 - 3.0 3.0 3.0 3.0 1.2 - 1.0 2.0 2.0 2.0 2.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 6.0 2.5 3.0 3.0 - 0.026 1.0 2.0 6.0 1.5 1.0 2.0 1.0 1.0 - - - - - - - - - - - - Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 22: Examples of Highest and Lowest Scores for Different Reasoning Types Reasoning Type Prompt Output Model Response Score 10 Continuous background noise with occasional sounds of objects being moved and water running in an apartment. Continuous background noise with occasional sounds of water running and objects being moved. Background noise and occasional speech are heard as someone moves around house, decorates, and watches television. man speaks in English with neutral tone, accompanied by background noise and occasional mechanical sounds. 1 (A) man speaking in English (A) man speaking in English 10 Give description of the atmosphere in the audio. Captioning Give general description of the audio content. Temporal QA What can be heard at the beginning of the audio? Choose the correct option among the options below: (A) man speaking in English, (B) Classical music with drums, (C) Instrumental electronic piece, (D) pop song. What indicates transition to an intense and momentous scene during the group of men riding horses in the desert? The wide brass melody over powerful string section creates cinematic and epic atmosphere, suggesting an important moment. The sound of train horn honking followed by train passing by. What do you hear between the piano playing and the guitar strumming? hear male voice speaking in English with sad mood. male voice speaking in English with sad mood. Subscene QA What is happening between the sound of game music and the idling engine? The scene consists of person playing with the remote control with background whirring and clicking sounds. man is speaking in English. Needle QA What sudden auditory change occurs that might indicate shift in the persons physical state while navigating the snow? increase in significant panting and breathing sounds, suggesting physical exertion or difficulty. The sound of breathing becomes prominent, suggesting the person may be exerting effort or experiencing discomfort. What does the woman mention about the view from the bedroom? She mentions seeing the red and yellow cliffs from her bedroom in the morning. She mentions that the view from the bedroom is amazing. What is the mood conveyed by the male voice singing in the audio? Sad. Plot QA What innovative feature does the man highlight about the Pulse smart watch? It can DJ without using phone. The male voice singing in the audio conveys deep and resonant mood. the The man highlights smartwatchs to ability monitor heart rate and sleep patterns. General QA What natural sound is consistently present throughout the entire audio? Birds chirping lightly. Birds chirping lightly. What sound is heard consistently throughout the audio clip? Continuous noise. background The sound of rooster crowing. 1 10 1 9 2 1 10 1 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Algorithm 1 Weighted Bucketed Blending & Dynamic Batching Require: all data: dictionary of datasets, each with contents (bucketed examples) and weight > 0 Require: epoch: Current epoch index Require: IsBrokenFile(): Checks if an audio file is unreadable Require: ShuffleDictFixedRand(, seed): Shuffles the bucketed data deterministically Require: batch by size fn(): Function implementing dynamic batching by size (Algorithm 2) function BLENDDATA(all data, epoch) self data {data : {}, total num : 0} for dataset name all data do contents all data[dataset name].contents weight all data[dataset name].weight for (bucket idx, bucket data) contents do bucket total bucket data if bucket total = 0 then continue end if start idx (cid:4)epoch bucket total weight(cid:5) mod bucket total end idx (cid:4)(epoch + 1) bucket total weight(cid:5) mod bucket total for idx start idx to end idx 1 do if (idx > 0) (idx mod bucket total = 0) then seed SumOfCharCodes(dataset name epoch idx/bucket total) bucket data ShuffleDictFixedRand(bucket data, seed) end if key idx mod bucket total item bucket data[key] if ISBROKENFILE(item[name]) then continue end if self data[data][self data[total num]] item self data[total num] self data[total num] + 1 end for end for end for return self data Skip empty buckets Skip broken files end function function DYNAMICBATCHING(indices, num tokens fn, max tokens, max sentences, bsz mult) High-level wrapper for dynamic batching batches batch by size fn(indices, num tokens fn, max tokens, max sentences, bsz mult) return batches end function 24 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Algorithm 2 Dynamic Batching by Size (Pseudocode for batch by size fn) Require: indices: Array of example indices Require: numTokensVec[i]: Number of tokens (or frames) for indices[i] Require: max tokens, max sentences: Batch constraints Require: bsz mult: Required multiple of batch size (e.g., for multi-GPU alignment) function BATCH BY SIZE VEC(indices, numTokensVec, max tokens, max sentences, bsz mult) batches [ ] batchStart 0 tailMaxTokens 0 batchMaxTokens 0 for pos 0 to indices 1 do List of subarrays of indices tailMaxTokens max(tailMaxTokens, numTokensVec[pos]) newBatchEnd pos + 1 newBatchMaxTokens max(batchMaxTokens, tailMaxTokens) newBatchSentences newBatchEnd batchStart newBatchNumTokens newBatchSentences newBatchMaxTokens overflow (newBatchSentences > max sentences > 0) (newBatchNumTokens > max tokens > 0) sizeOk (newBatchSentences < bsz mult) (newBatchSentences mod bsz mult = 0) if overflow then Finalize the current batch and start new one batches.APPEND(indices[batchStart : pos]) batchStart pos batchMaxTokens numTokensVec[pos] tailMaxTokens numTokensVec[pos] else if sizeOk then Optionally finalize if batch size is multiple of bsz mult batches.APPEND(indices[batchStart : newBatchEnd]) batchStart newBatchEnd batchMaxTokens 0 tailMaxTokens 0 else Continue accumulating examples in the current batch batchMaxTokens newBatchMaxTokens end if end for if batchStart < indices then batches.APPEND(indices[batchStart :]) end if return batches end function Add any leftover examples in the final batch 25 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 23: All datasets used to train our model. We mark the datasets used in Stage 11, Stage 22, Stage 33 or multiple stages of training1,2,3. Datasets marked with were added to AF2 over the ones already present in Audio Flamingo. Audio Type Task CAP Datasets WavCaps1 (Mei et al., 2024), Macs2 (Martin Morato & Mesaros, 2021), SoundDescs1 (Oncescu et al., 2021), Clotho-v22 (Drossos et al., 2020), WavText5K1 (Deshmukh et al., 2022), Laion-630k1 (Wu et al., 2023b) General Sound Clotho-AQA2 (Lipping et al., 2022), Open-AQA2 (Gong et al., 2023a), AQA Salmonn AQA2* (Tang et al., 2023), AudioEntailment2* (Deshmukh et al., 2024) CompA-R2* (Ghosh et al., 2024c), AudioSkills1,2* (ours), LongAudio3* (ours) CLS AudioSet 2 (Gemmeke et al., 2017), FSD50k2 (Fonseca et al., 2021), CochlScene2 (Jeong & Park, 2022), NonSpeech7K 2(Rashid et al., 2023), Chime-Home2 (Foster et al., 2015), Sonyc-UST2 (Cartwright et al., 2019) #Audio-Text Pairs 829 1970 1091 CAP LP-MusicCaps2 (Doh et al., 2023), MusicCaps2 (Agostinelli et al., 2023) 1389 Music AQA MusicQA2 (Liu et al., 2024), MusicAVQA2 (Li et al., 2022) MusicBench2* (Melechovsky et al., 2023), Mu-LLAMA2* (Liu et al., 2024) AudioSkills1,2* (ours), LongAudio3* (ours) CLS NSynth2 (Engel et al., 2017b), MTG-Jamendo2 (Bogdanov et al., 2019), FMA2 (Defferrard et al., 2016), MusDB-HQ2 (Rafii et al., 2019), Speech CLS MSP-Podcast1 (Lotfian & Busso, 2017), Emov-DB2 (Adigwe et al., 2018) JL-Corpus2 (James et al., 2018), Tess2 (Pichora-Fuller & Dupuis, 2020), MELD2 (Poria et al., 2018), OMGEmotion2 (Barros et al., 2018) 94 459 92 Table 24: All datasets used for evaluation in AF2, with additional usage/contextual notes for foundational datasets. Dataset Type Audio Type Task Datasets CAP Clotho-v2 (Drossos et al., 2020), AudioCaps (Kim et al., 2019) Sound CLS Foundational UrbanSound8K (USD8K) (Salamon et al., 2014) (environmental sound classification), ESC50 (Piczak, 2015) (environmental sound classification), CochlScene (Jeong & Park, 2022) (acoustic scene classification), FSD50k (Fonseca et al., 2021) (sound event classification) CREMA-D (Cao et al., 2014) (emotion classification), Ravdess (Livingstone & Russo, 2018) (emotion classification), NonSpeech7k (Rashid et al., 2023) (non-speech audio classification) AQA Clotho-AQA (Lipping et al., 2022) (audio question answering) CLS Music NSynth (NS)source (Engel et al., 2017a) (instrument classification), NSynth (NS)instrument (Engel et al., 2017a) (instrument classification), (genre classification), Medley-solosGTZAN (Sturm, 2013) DB (Lostanlen et al., 2019) (instrument classification) AQA MusicAVQAaudio (Li et al., 2022) (music audio question answering) Reasoning Sound AQA Music AQA OpenAQA (Gong et al., 2024), MMAU Sound (Sakshi et al., 2024), AudioEntailment (AE) Clotho (Deshmukh et al., 2024), AudioEntailment (AE) AudioCaps (Deshmukh et al., 2024), CompA-R-test (Ghosh et al., 2024c) MMAU Music (Sakshi et al., 2024), MuchoMusic (Weck et al., 2024), MusicInstruct (MI) (Long) (Deng et al., 2023), MusciQA (Liu et al., 2024) 26 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Category Example Table 25: More examples from AudioSkills. Temporal Relationship Identification Order: In what sequence do the sounds appear in the audio? (A) Car engine, honk, music (B) Music, honk, car engine Attribute: When does the breathing sound change over time? (A) Disappear (B) Get louder (C) Get soft Grounding: When does the sound of bird chirp occur in the audio? (A) Beginning (B) Middle (C) End Referring: What sound appears first in the audio? (A) Music (B) Car honk (C) Dog bark Order: In what sequence do the footsteps appear in the audio? (A) Slow footsteps, running, door slam (B) Running, door slam, slow footsteps (C) Door slam, slow footsteps, running Attribute: How does the volume of the rain sound change over time? (A) Increases (B) Decreases (C) Stays the same Grounding: When does the sound of baby crying occur in the audio? (A) Beginning (B) Middle (C) End Referring: What sound appears immediately after the thunder? (A) Wind blowing (B) Rainfall (C) Car alarm Contextual Speech Event Reasoning How might the playful interaction between the boy and the goat be affected by the tone of the speech and the background sounds? Contextual Event Reasoning Sound What can be deduced about the environment and the relationship between the boy and the goat from the audio events? What might the presence of the music and its continuous play suggest about the overall atmosphere of the scene? Based on the timing and sequence of impact sounds and male speech, identify the possible work environment depicted in the audio. Counting Level 1: How many times was the hammer sound heard? Level 2: How many times did the first sound occur in the entire audio? Information Extraction What key is the main melody of the audio played in? (A) major (B) major (C) A# major (D) G# major General Reasoning How does the melody in the audio contribute to the hypnotic effect of the music? (A) By changing frequently (B) By maintaining consistent and repetitive loop (C) By using multiple key changes Considering the chord sequence Bm7, D, Bm7, in the audio, what is the role of the melody in relation to these chords? (A) It follows the progression (B) It decorates the progression How does the melody in the audio contribute to the hypnotic effect of the music? (A) By changing frequently (B) By maintaining consistent loop Attribute Identification Which event has the highest pitch in the audio? Which is the least loud event in the audio? 27 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 9: Prompt 1 used for generating Temporal QA for LongAudio from MiraData. 28 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 10: Prompt 2 used for generating Temporal QA. 29 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 11: Prompt used for generating Plot QA for LongAudio from MiraData. 30 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 12: Prompt used for generating Caption QA for LongAudio from MiraData. 31 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 13: Prompt used for generating Contextual Sound Event Reasoning QA for AudioSkills. 32 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 14: Prompt used for generating Caption QA for Video-ReCap. 33 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 15: Prompt used for generating Subscene QA for LongAudio from Video-ReCap. 34 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 16: Prompt used for generating General QA for LongAudio from Video-ReCap. 35 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 17: Prompt used for generating Subscene QA for LongAudio from MiraData. 36 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 18: Prompt used for generating Temporal Reasoning QA for AudioSkills. 37 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 19: Prompt used for generating linguistically variant positives for short-audio captions for CLAP training. Figure 20: Prompt used for cleaning and removing noise from synthetic short-audio captions. Figure 21: Prompt used for generating compositionally different negatives for short-audio captions for CLAP training. 38 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 22: Prompt used for generating synthetic short-audio captions from Video ReCAP. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 23: Prompt used for generating synthetic short-audio captions from MiraData. Figure 24: Prompt used for rewriting audio caption and removing implausible acoustic elements. Figure 25: Prompt used for evaluating responses for questions in LongAudioBench. 40 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 26: Prompt used for generating Attribute QA for AudioSkills. 41 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 27: Prompt 1 used for generating Contextual Sound Event Reasoning QA for AudioSkills. Figure 28: Prompt 2 used for generating Contextual Sound Event Reasoning QA for AudioSkills. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 29: Prompt 3 used for generating Contextual Sound Event Reasoning QA for AudioSkills. Figure 30: Prompt 4 used for generating Contextual Speech Event Reasoning QA for AudioSkills. 43 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 31: Prompt 1 used for generating Information Extraction QA for AudioSkills. Figure 32: Prompt 3 used for generating Information Extraction QA for AudioSkills. 44 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 33: Prompt 4 used for generating Information Extraction QA for AudioSkills. Figure 34: Prompt 5 used for generating Information Extraction QA for AudioSkills. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 35: Prompt used for generating QA for AudioSkills using FMA. Figure 36: Prompt used for generating QA for AudioSkills using MusicNet, borrowed from Gardner et al. (2023). 46 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 37: Prompt used for generating QA for AudioSkills using MTG-Jamendo, borrowed from Gardner et al. (2023). Figure 38: Prompt used for self-verification of generated QA pairs for MiraData. 47 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Figure 39: Prompt used for self-verification of generated QA pairs for ReCap dataset. 48 Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 26: Prompts designed for specific audio processing tasks, used to transform foundational audio understanding datasets into QA pairs. Task Name Instrument Classification Audio Captioning Music Captioning Speech Emotion Classification Prompts What instrument is playing in the audio?, Identify the instrument in this audio clip., Classify the instrument heard in the music., What is the primary instrument in this track?, Provide the instrument tag for this audio., Which instrument is most prominent in the clip?, What instrument is featured in this music?, What is the instrument label for this audio., Identify the type of instrument being played., Classify the instrument based on the audio content., What is the main instrument in this track?, What musical instrument is audible in the audio?, Provide the instrument classification for this audio., What type of instrument is represented in this clip?, What instrument is most recognizable in this music?, Classify the music by its primary instrument., Which instrument defines the sound of this track?, Identify the instrument most prominently heard in the audio., What instrument category does this audio belong to?, Determine the instrument used in the music clip. Caption the input audio., Describe the sounds in the audio., Provide caption for the audio., What is happening in this audio clip?, Summarize the audio content., Describe the events in the audio., What sounds can you hear in this audio?, Give detailed description of the audio scene., Caption the sounds in the audio., What is the main action or event in the audio?, How would you describe the sounds in this audio?, Describe the background sounds in the audio., What can be heard in the audio?, Give brief description of the sounds in this audio., Describe the setting based on the audio., Provide caption describing the audio scene., What events are occurring in this audio?, Give general description of the audio content., Whats going on in the audio clip?, Describe any notable sounds in the audio., Provide summary of the audio sounds., What is taking place in the audio?, Give description of the atmosphere in the audio., Describe the main sounds in the audio., What does the audio depict?, Describe the audible events in this audio., Summarize what you hear in the audio., Provide descriptive caption for the audio clip., What is the soundscape in this audio?, How would you describe the scene from the audio?, Describe the key elements heard in the audio. Describe the music in the audio., Provide caption for the music., Summarize the characteristics of the music., Summarize the music content in sentence., Caption the input music. Identify the emotion in the utterance., What is the emotion of the utterance?, Describe the emotional tone in this audio., What emotion is expressed in the audio?, What is the primary emotion in this recording?, Identify the feeling conveyed in the utterance., How would you describe the emotion in the audio?, What emotion stands out in this audio clip?, Describe the mood of the speaker., What sentiment is present in the audio?, What is the dominant emotion in this audio?, Classify the emotion expressed in the clip., What is the emotional state of the speaker?, What feeling does the speaker convey?, Identify the mood in this audio., What is the overall emotion of this recording?, How would you classify the emotion in the clip?, What kind of emotion is detectable in the audio?, Describe the emotional expression of the speaker., What is the perceived emotion in the recording?, Determine the emotion conveyed by the speaker., What is the underlying emotion in this utterance?, Classify the speakers emotional tone., What emotional state is reflected in the audio?, Describe the speakers feeling in this clip., What is the prevailing emotion in this recording?, Identify the primary mood of the utterance., What emotional quality does the audio suggest?, What is the affective tone of the speaker?, What emotion can be inferred from the speakers tone? Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities Table 27: Prompts designed for specific audio processing tasks, used to transform foundational audio understanding datasets into QA pairs. Task Name Audio Event Classification Sentiment Classification Prompts What are the unique sounds in the audio?, Provide comma separated list of all sounds you hear in the input audio., What sounds can you hear in the audio?, List all the sounds present in the audio., Identify the distinct sounds in the audio clip., What specific sounds are detectable in the audio?, Give list of sounds you can identify in this audio., What types of sounds are included in the audio?, List the audible elements in the audio., What are the main sounds you notice in this audio?, Provide list of all recognizable sounds in the audio., What sounds are prominent in the audio clip?, List all distinct sounds in the audio., What are the different sounds occurring in this audio?, Identify and list each sound in the audio., Provide detailed list of sounds heard in the audio., What sounds stand out in the audio?, List each unique sound in this audio clip., Which sounds are identifiable in the audio?, What sounds are repeated in the audio?, What environmental sounds can you hear in this audio?, List all background sounds in the audio., Identify the primary sounds in the audio., What are the foreground sounds in the audio?, List any musical or rhythmic sounds present., Provide list of natural sounds in the audio., What artificial or mechanical sounds can be heard?, What ambient sounds are in the background?, What noticeable sounds can be identified in this clip?, List all sound sources in the audio. Identify the sentiment of the utterance., What is the sentiment of the utterance?, What is the primary sentiment of the utterance in this recording? Music Understanding and Classification Generate music tags including genre, instrument, and mood, Identify the genre, instrument, and theme of this music, What are the tags for genre, instrument, and mood for this track, Describe the genre, instruments, and theme of the audio, What is the genre and mood of the music, Provide tags for the genre, mood, and instruments in the track, What music tags best describe this audio, including genre and theme, Generate tags for the genre and atmosphere of the music, What genre and instruments are featured in this track, What are the primary genre and theme of this audio, Describe the mood, genre, and instrumentation in the music, Identify the genre and musical style of the audio, What genre tags and mood fit this music, What genre and theme are represented in this track, Provide tags for genre, style, and mood of the audio, Generate descriptive tags for genre and atmosphere in the music, Identify the genre, theme, and instrumental tags for this track, What tags describe the genre and emotional tone of this audio, Provide genre and theme tags for the music, What genre, mood, and instruments define this audio clip, What genre and style does the music represent, Describe the tags for genre and instruments in the audio, What genre and theme best describe this track, Provide tags that include genre and emotional tone for the music, What genre and mood characterize the music in this audio, Generate tags for genre, style, and mood in the track, What is the genre and overall theme of the audio, Identify genre and instrumentation tags for this music, What genre, theme, and mood does the audio convey, Provide tags that describe the genre and atmosphere of the track. What is the genre of the music, Identify the genre of this audio clip, Classify the genre of the music in the audio, What musical genre does this clip represent, Provide the genre tag for this music, Which genre best describes the audio, What is the genre label for this music clip, Classify the genre based on the audio content, What category of music does this audio belong to, Determine the genre of the audio, Identify the genre of this track, What is the musical genre for the given audio, What kind of music genre is this, Provide the genre classification for this audio, What type of genre is represented in this audio clip, What genre does this music fall under, Classify the music by its genre, What style or genre does the audio represent, What is the most suitable genre for this track, Provide genre tag for the music in this clip. Genre Classification"
        }
    ],
    "affiliations": [
        "NVIDIA, Santa Clara, CA, USA",
        "University of Maryland, College Park, MD, USA"
    ]
}