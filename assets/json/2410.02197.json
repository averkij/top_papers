{
    "paper_title": "General Preference Modeling with Preference Representations for Aligning Language Models",
    "authors": [
        "Yifan Zhang",
        "Ge Zhang",
        "Yue Wu",
        "Kangping Xu",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. Although supervised pair preference models (PairPM) can express general preferences, their implementation is highly ad-hoc and cannot guarantee a consistent preference probability of compared pairs. Additionally, they impose high computational costs due to their quadratic query complexity when comparing multiple responses. In this paper, we introduce preference representation learning, an approach that embeds responses into a latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback. Experimental results show that our General Preference representation model (GPM) outperforms the BT reward model on the RewardBench benchmark with a margin of up to 5.6% and effectively models cyclic preferences where any BT reward model behaves like a random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0 and MT-Bench, following the language model post-training with GPO and our general preference model, reveal substantial performance improvements with margins up to 9.3%. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://github.com/general-preference/general-preference-model."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 7 9 1 2 0 . 0 1 4 2 : r a"
        },
        {
            "title": "General Preference Modeling with Preference\nRepresentations for Aligning Language Models",
            "content": "Yifan Zhang Ge Zhang Yue Wu Kangping Xu Quanquan Gu Abstract Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. Although supervised pair preference models (PairPM) can express general preferences, their implementation is highly ad-hoc and cannot guarantee consistent preference probability of compared pairs. Additionally, they impose high computational costs due to their quadratic query complexity when comparing multiple responses. In this paper, we introduce preference representation learning, an approach that embeds responses into latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback. Experimental results show that our General Preference representation model (GPM) outperforms the BT reward model on the RewardBench benchmark with margin of up to 5.6% and effectively models cyclic preferences where any BT reward model behaves like random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0 and MT-Bench, following the language model post-training with GPO and our general preference model, reveal substantial performance improvements with margins up to 9.3%. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://github.com/general-preference/general-preference-model."
        },
        {
            "title": "Introduction",
            "content": "Modeling human preferences is cornerstone in developing foundation models that interact seamlessly with users. In natural language modeling and reinforcement learning, aligning models with human intent and values has led to significant advancements, including improved text generation and enhanced decision-making policies (Ouyang et al., 2022; Christiano et al., 2017). Traditional approaches often rely on reward modeling, wherein reward function is learned to guide the optimization of policies. While effective in certain contexts, these methods face challenges in expressiveness and computational efficiency, particularly when addressing complex or intransitive human preferences (Tversky, 1969; Munos et al., 2023). Equal contribution IIIS, Tsinghua University, e-mail: zhangyif21@mails.tsinghua.edu.cn Shanghai Qizhi Institute Department of Computer Science, University of California, Los Angeles IIIS, Tsinghua University Department of Computer Science, University of California, Los Angeles, e-mail: qgu@cs.ucla.edu 1 (a) Bradley-Terry (BT) reward model (b) PairPM (c) General Preference representation model (GPM) Figure 2: Illustration of (a) Bradley Terry (BT) reward model, (b) supervised pair preference model (PairPM) (Jiang et al., 2023; Dong et al., 2024), and (c) our General Preference representation model (GPM). Preference learning algorithms typically employ pairwise comparisons to capture human judgments (Ibarz et al., 2018; Ziegler et al., 2019). The Bradley-Terry (BT) model (Bradley & Terry, 1952) is popular for modeling such pairwise preferences due to its simplicity and computational efficiency: given responses, BT reward model cost O(K) inference-time compute to output the reward dictating the preferences. The efficiency of the BT model comes from the implicit assumption that each option can be conveniently represented by scalar reward, which inevitably limits the models capacity to capture the richness of human judgments that may be context-dependent or exhibit intransitivity (Gardner, 1970). On the other hand, supervised (sequential-classification) pair preference models (PairPM) (Jiang et al., 2023; Dong et al., 2024) that predict the preference given concatenation of the two responses can express complex and intransitive (cyclic) structures. But to fully capture the preference relations among responses, it requires evaluating O(K2) pairwise preferences between all candidate responses (Munos et al., 2023; Wu et al., 2024b). This quadratic scaling hinders Figure 1: sitiveness world preferences. Intranin realthem for applications with larger response sets especially in test-time scaling for reasoning tasks using verifiers and ranking models (Snell et al., 2024; Wu et al., 2024a). Aside from computational inefficiency, supervised preference models also exhibit asymmetric preference behaviors related to positions. Also, the models design choice can be highly ad-hoc, varying among different templates and different linear heads. Based on the above observations, it is thus natural to raise the following question: Is there principled way to model general preference? In this paper, we answer this question affirmatively by proposing preference representation learning, which bridges the gap between expressiveness and efficiency in general preference modeling. Our method embeds responses into multi-dimensional latent space that captures the complex preference structure beyond transitive relations while allowing for efficient querying of preferences. Notably, our approach achieves computational complexity of O(K), matching the efficiency of the BT model but with enhanced expressiveness. The main contributions of our work are summarized as follows: We introduce preference representation learning for general preference modeling, enabling both efficient and expressive representation of human preferences. Our approach generalizes the BradleyTerry (BT) reward model by embedding responses into latent space, capturing complex preference structures, including intransitive preferences. Notably, our General Preference representation model (GPM) achieves query complexity of O(K) for evaluating preferences among responses, significant improvement over the O(K2) complexity of traditional supervised preference models that rely on pairwise inputs (see Section 4). We demonstrate the effectiveness of GPM across variety of tasks, including CyclicPreference (ours) and the renowned RewardBench (Lambert et al., 2024). Specifically, GPM models intransitive (e.g., cyclic) preferences with 100% accuracy, whereas the BT reward model performs like random guessing (see Section 6.1). Additionally, GPM outperforms the BT model on RewardBench with performance margins of up to 5.6% (see Section 6.2). For language model alignment, we propose General Preference Optimization (GPO), which leverages the preference scores provided by GPM. The general preference score can also be integrated as preference signal into wide range of RLHF and preference optimization methods, such as (iterative) DPO (Rafailov et al., 2024), SPPO (Wu et al., 2024b), and PPO-based methods (Ouyang et al., 2022). Experimental results on AlpacaEval2.0 and MT-Bench reveal that our approach leads to substantial improvements over reward-based methods for language model alignment (see Section 6.3)."
        },
        {
            "title": "2 Related Work",
            "content": "Reward-Based Reinforcement Learning from Human Feedback (RLHF). The earlier approaches to modeling human preference for language model alignment usually learn reward model from preference dataset. The human preference is assumed to follow the Bradley-Terry (BT) model (Bradley & Terry, 1952) or the Thurstone model (Thurstone, 2017). LLM policies then are fine-tuned to maximize these scalar reward signals for better alignment (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). Later, the direct preference optimization (DPO) methods are proposed by Rafailov et al. (2024) to only implicitly learn reward model represented by an LLM. The human preference is still assumed to follow the Bradley-Terry model. However, 3 the reliance on scalar rewards imposes total ordering on preferences, which may not reflect the intransitive or stochastic nature of human judgments (Tversky, 1969; Agranov & Ortoleva, 2017). Preference-Based Reinforcement Learning from Human Feedback. Recently, there emerged line of works that directly estimates the preference probability without imposing reward-based preference model or any transitivity assumptions (Lou et al., 2022; Wu et al., 2023; Wang et al., 2023) either for preference-based RL or in the context of RLHF. Efforts have been made to optimize policies directly from pair-wise preference comparisons, thereby mitigating the limitations of scalar reward functions (Munos et al., 2023; Swamy et al., 2024; Rosset et al., 2024; Wu et al., 2024b). Intransitivity in Game Theory. The symmetric zero-sum game and its intransitivity have also been frequently studied in the context of game theory. Balduzzi et al. (2018) was motivated by evaluation among different agents, showing that any symmetric zero-sum game can be decomposed into transitive game and cyclic game, and proposed Nash averaging for better agent/task evaluation. Balduzzi et al. (2019) generalized the results from matrix games to functional-form games and propose new algorithms to construct diverse populations of effective agents. Czarnecki et al. (2020) investigated the geometrical properties of real-world games (e.g., Tic-Tac-Toe, Go, StarCraft II) and proposed that real-world games have spinning top geometry, with strong transitive dimension and gradually diminishing non-transitive cyclic dimensions. Very recently, Bertrand et al. (2023) examined the limitations of the Elo rating system and proposed an alternative disc decomposition method that can better handle both transitive and cyclic game dynamics. Representation Learning and Embedding. Representation learning and embedding techniques have successfully captured relational structures across various domains (Mikolov et al., 2013; Chen et al., 2020; Radford et al., 2021), yet their application in preference modeling and RLHF remains limited. Our work introduces preference representation learning, an approach that enhances expressiveness while maintaining computational efficiency, bridging the gap left by traditional approaches."
        },
        {
            "title": "3 Background",
            "content": "In this section, we present preliminaries on reward modeling, preference modeling, and reinforcement learning from human feedback (RLHF) for language model alignment. We consider an autoregressive language model that generates responses to the given prompts. Let = [x1, x2, . . .] denote prompt, sequence of tokens. The language model π generates response = [y1, y2, . . . , yN ] based on the conditional probability distribution: π(y x) = (cid:81)N i=1 π (yi x, y<i), where y<i represents the sequence of tokens generated before position i. In this paper, we assume general-preference oracle. Given two responses and to the same prompt x, the oracle provides the feedback indicating which response is preferred. (cid:0)y x(cid:1) := (cid:2)o (cid:0)y x(cid:1)(cid:3) ."
        },
        {
            "title": "3.1 Reward-Based Reinforcement Learning from Human Feedback",
            "content": "The most prevalent approach to aligning language models with human preferences is to consider scalar reward function r(y; x) that assigns numerical score to each response. The preference between two responses is then determined solely by the reward scores for the two responses. For example, the Bradley-Terry (BT) model (Bradley & Terry, 1952) is widely used method for modeling pairwise preferences in this context. However, the BT model can not capture intransitive (e.g. cyclic) preferences effectively (Bertrand et al., 2023). Under the BT model, the probability that response is preferred over is given by: P(y x) = σ(cid:0)r(y; x) r(y; x)(cid:1), where σ(z) = 1/(1 + ez) is the logistic (sigmoid) function. In practice, the reward function r(y; x) is learned by maximizing the likelihood of the observed preference data. Once the reward function is established, policy optimization techniques, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), can be applied to adjust the language model to generate responses that maximize expected rewards. The optimization problem can be formulated as: max θ ExX , yπθ(x) [r(y; x)] βExX [KL (πθ( x) πref( x))] , (3.1) where θ are the parameters of the policy πθ, πref is reference policy (often the pre-trained or supervised-fine-tuned language model), β is scaling parameter that controls the strength of regularization, and KL denotes the Kullback-Leibler divergence."
        },
        {
            "title": "3.2 General Preference Modeling",
            "content": "We consider the scenario where given prompt x, set of responses {yi} is generated, and human preferences over these responses are represented as pairwise probabilities P(yi yj x) (0, 1), indicating the likelihood that response yi is preferred over yj given the prompt x. To model these preferences, we define (pairwise) preference score function: s(yi yj x) := log P(yi yj x) 1 P(yi yj x) , (3.2) which represents the log-odds of yi being preferred over yj. This score function allows us to express the preference probability as: P(yi yj x) = σ (s(yi yj x)) , (3.3) where σ(z) = 1/(1 + ez) is the logistic function. One can see that the BT model is special case: s(yi yj x) = r(yi; x) r(yj; x). 3.2.1 Supervised Pair Preference Models Existing approaches often involve concatenating the prompt and responses with template and training an LLM-based sequential classifier in supervised learning manner. For example, Jiang et al. (2023) simply concatenate the three segments (x, y1, y2) sequentially and form single input sequence with special tokens as separators: <s> <source> </s> <candidate1> y1 </s> <candidate2> y2 </s> Then sequential classification head on the last token is trained to predict the preference. Another example is Munos et al. (2023), which uses the following template for text summarization: You are an expert summary rater. Given piece of text and two of its possible summaries, output 1 or 2 to indicate which summary is better. Text - text, Summary 1 - summary1, Summary 2 - summary2. Preferred Summary - Then use the last logit for an arbitrarily chosen token as s(y1 y2x) for training. However, due to the language models position encoding (Press et al., 2021; Su et al., 2024) and the causal attention (Radford et al., 2018, 2019) mechanism not being symmetric, the candidates order in the concatenation will affect the final prediction results. It is mitigated by randomly shuffling the two responses in the training dataset but the output is still highly asymmetric. Another limitation is that how to represent the preference score can be highly ad-hoc. The two examples above already use different templates and different linear heads (sequential classification v.s. language modeling)."
        },
        {
            "title": "4 General Preference Modeling with Preference Representations",
            "content": "In this section, we propose general preference representation learning framework that can model human preferences efficiently and expressively. Each response is embedded as vector in latent space, and the preferences are modeled through interactions between these representations (embeddings) using skew-symmetric operator. We first define preference representations, which serve as the foundation for modeling the relationships between responses. Definition 4.1 (Preference Representations). Given prompt x, we assign to each response preference representation vector vyx R2k. These representations are designed to capture the features relevant to human preferences beyond what can be represented by scalar rewards. Next, to model the directional nature of preferences, we introduce the skew-symmetric preference operator, which ensures that the model respects the skew-symmetry (anti-symmetry) in preference modeling. Definition 4.2 (Skew-symmetric Preference Operator). To capture the directional nature of preferences, we define skew-symmetric (anti-symmetric) preference operator R2k2k. Specifically, is block-diagonal matrix consisting of skew-symmetric blocks of the form (for more discussion, please see Appendix A): , = 1, . . . , k. (4.1) An example of for = 2 is: Rl = (cid:21) (cid:20)0 1 0 1 = 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 . 6 Finally, we define the preference score, which quantifies the degree to which one response is preferred over another. This score is calculated based on the interaction between the preference representations, mediated by the skew-symmetric operator. Definition 4.3 (Preference Score). The preference score between two responses yi and yj using preference representations is defined as: s(yi yj x) = (cid:10)Rvyix, vyj (cid:11), (4.2) where , denotes the inner product in R2k. This score captures the anti-symmetric relationship between responses induced by human preferences. We model the preference probability using the logistic function as defined in Equation (3.3). Our general preference representation model (GPM) exhibits two desirable properties: 1. Skew-symmetry. The preference score function is skew-symmetric, satisfying: s(yi yj x) = s(yj yi x). This reflects the fact that the preference relation is naturally skew-symmetric: if yi is preferred over yj with probability pi,j, then yj is preferred over yi with probability 1 pi,j. Specifically, s(y x) = (cid:10)Rvyx, vyx (cid:11) = 0. This means that response is neither superior nor inferior to itself. 2. Magnitude preserving. The skew-symmetric preference operator does not change the representation vectors magnitude, which makes this operation stable for training and inference. (cid:10)Rvyx, Rvyx (cid:11) = (cid:10)vyx, vyx (cid:11). Relation to Bradley-Terry Model. If we set = 1, vy = [r(y x), c], where is constant and = 0 (e.g., = 1), and = , then the preference score reduces to: (cid:21) (cid:20)0 1 0 1 s(yi yj x) = c(cid:0)r(yi x) r(yj x)(cid:1), and the preference probability becomes: P(yi yj x) = σ(cid:2)c(cid:0)r(yi x) r(yj x)(cid:1)(cid:3), which is exactly the Bradley-Terry (BT) model as disk game (Balduzzi et al., 2019)."
        },
        {
            "title": "4.1 Expressiveness of the Model",
            "content": "Our general preference representation model is fully expressive for any real skew-symmetric preference matrix (see Appendix A.1 for complex representations interpretation). Specifically, we establish the following theorem (similar results have been proved in Balduzzi et al. (2018)): 7 Theorem 4.4 (Expressiveness of Preference Representation Model). Let Rkk be real skewi=1 R2k and block-diagonal symmetric matrix (i.e., = P). Then there exist vectors {vi}k skew-symmetric matrix R2k2k, with consisting of blocks of the form: such that: Rl = (cid:21) (cid:20)0 1 0 , = 1, . . . , k, Pij = Rvj, i, j. Moreover, the vectors {vi} can be constructed explicitly from P. Theorem 4.4 suggests that our preference representation framework can theoretically model arbitrary complex and potentially intransitive (e.g., cyclic) preference structures (see Appendix A.4 for proofs)."
        },
        {
            "title": "5 Efficient Preference Optimization with General Preference",
            "content": "To address the potential intransitive human preference, the preference-based LLM alignment algorithms (Munos et al., 2023; Azar et al., 2023; Wu et al., 2024b; Rosset et al., 2024) have been proposed to directly work on the preference pairs instead of assuming reward function. Given preference oracle (y x). The objective is to find policy π that performs well against another competing policy π in terms of these preference probabilities. For example, Azar et al. (2023) consider competing with another fixed policy µ (X denotes the distribution over prompts): (cid:2)Eyπ(x), yµ(x) (cid:2)P (cid:0)y x(cid:1)(cid:3) βKL(ππref)(cid:3) , (5.1) max π ExX Other works (Munos et al., 2023; Wu et al., 2024b; Rosset et al., 2024) consider solving the two-player constant-sum game: max π min π ExX (cid:2)Eyπ(x), yπ(x) (cid:2)P (cid:0)y x(cid:1)(cid:3)(cid:3) . (5.2) To simplify notation, we define the winning probability of policy π over another policy π as: (cid:0)π π x(cid:1) = Eyπ(x), yπ(x) (cid:2)P (cid:0)y x(cid:1)(cid:3) . The optimization problem then becomes: max π min π ExX (cid:2)P (cid:0)π π x(cid:1)(cid:3) . (5.3) (5.4) The von Neumann winner is concept drawn from social choice theory (Sen, 1986) and has been studied in preference-based RL (Owen, 2013; Dudık et al., 2015). It is the Nash equilibrium of the two-player symmetric game (Equation 5.4). It represents mixed strategya probability distribution over possible responsesthat performs optimally in the worst-case scenario against any opponent. More formally speaking, distribution π is called von Neumann winner if it satisfies: ExX (cid:2)P (cid:0)π π x(cid:1)(cid:3) 1/2. min π This condition ensures that, on average, the von Neumann winner π is at least as likely to be preferred than any other policy π. The von Neumann winner always exists due to the symmetric nature of the two-player game (Equation 5.4)."
        },
        {
            "title": "5.1 Efficient Policy Optimization",
            "content": "To align language models with the general preference, previous works (Wu et al., 2024b; Rosset et al., 2024; Liu et al., 2024; Swamy et al., 2024) have proposed iterative training frameworks that include 1) sampling from the current LLMs multiple responses y1, y2, . . . , yK; 2) using general preference model to label the preference among the responses; 3) aggregating the preferences into different learning objectives. key advantage of our model is its computational efficiency. The previous general preference models require O(K2) inference-time compute to evaluate all pairwise preferences among responses, as each pair is represented by different concatenated sequence from (x, yi, yj) to predict P(yi yjx). In contrast, computing the preference representation for responses requires only O(K) forward passes / inference-time compute: we first calculate the representation vi for each yi, and then use them to calculate the preference probability between any two responses using formula s(yi yj) = Rvi, vj. This way, our model is as efficient as reward model while being way more expressive."
        },
        {
            "title": "5.2 General Preference Optimization (GPO)",
            "content": "Policy Optimization with Preference Score. Once we have general preference model that outputs the preference score s(yi yjx) at hand, we aim to find policy π that performs well against an opponent policy µ in terms of expected preference scores. The optimization problem is formulated as: max θ Ex (cid:2)Eyπθ(x), yµ(x) (cid:2)s(y x)(cid:3)(cid:3) βEx [KL (πθ( x)πref( x))] , (5.5) where πref is reference policy (e.g., the initial language model), µ is the opponent policy (usually the same as πref), and β > 0 is regularization parameter controlling the divergence from the reference policy. We would like to point out that this formulation is different from the many previous works (Wu et al., 2024b; Swamy et al., 2024; Rosset et al., 2024; Munos et al., 2023; Azar et al., 2023) as they consider maximizing the win rate P(y yx), while our formulation is to maximize P(yyx) P(yyx) . Note that P(y yx) only varies between 0 and 1, while s(y yx), s(y yx) = log similar to the reward r(y; x) in RLHF or DPO, can take arbitrary values. The flexibility in its value range might benefit fine-tuning. General Preference Optimization. We consider the SPPO loss used by Wu et al. (2024b) for iterative preference optimization, except that we use preference score instead of preference probability in the loss form. SPPO used responses for each prompt and calculated the empirical win rate of each response yk. Instead, we calculate (cid:98)s (yi µ x) to estimate the empirical win rate over the distribution µ as below: (cid:98)s (yi µ x) = 1 (cid:88) k=1 (yi yk x) , [K], (5.6) At each iteration t, GPO has the following learning objective: θt+1 = arg min θ ExX ,yπθt (x) (cid:34) (cid:18) log (cid:19) (cid:18) πθ(y x) πθt(y x) (cid:16) 1 β (cid:98)s (y πθt x) log Zπθt (cid:17)(cid:19)2 (cid:35) (x) , (5.7) 9 where the normalizing factor Zπθt (x) := (cid:80) In practice, we directly replace log Zπθt πθt(yx) exp ((cid:98)s (y πθt x)). (x) with 01. Intuitively, if response receives high average score, GPO will increase its log probability. We report the empirical performance of GPO in Section 6.3. Remark 5.1. Notice that the GPO learning objective can be seen as an offline policy gradient algorithm (see Appendix C) for the optimization problem defined in Equation (5.7), similar results have been discussed in Munos et al. (2023); Wu et al. (2024b). Remark 5.2. Note that the general preference score given by our GPM in Equation (5.5) can also be integrated as preference (reward) signal for any off-the-shelf RLHF and preference optimization methods, including (iterative) DPO (Rafailov et al., 2024), IPO (Azar et al., 2023), NLHF (Munos et al., 2023), SPPO (Wu et al., 2024b) and REBEL (Gao et al., 2024), as well as PPO-based methods (Ouyang et al., 2022) by directly optimizing Equation (5.5)."
        },
        {
            "title": "6 Experiments",
            "content": "We conducted extensive experiments to evaluate the effectiveness of the proposed General Preference representation model (GPM) in comparison to traditional reward-based models, particularly focusing on its ability to model cyclic preferences and improve language model alignment. Our experiments are designed to address the following key questions: Q1: Can the GPM effectively capture and model cyclic and intransitive preferences, where traditional models like the Bradley-Terry (BT) reward model struggle? Q2: How does the GPM perform on standard preference modeling benchmarks (RewardBench) compared to the BT model? Q3: How does using the GPM for downstream policy optimization impact language model performance on real-world tasks compared to reward-based approaches?"
        },
        {
            "title": "6.1 Cyclic Preference Modeling",
            "content": "To address Q1, we evaluate the ability of the GPM to capture intransitive, cyclic preferences that traditional transitive models (like the BT model) struggle to represent. Cyclic Preference Dataset. We constructed dataset by inducing cyclic preferences from the Ultrafeedback dataset Cui et al. (2024). The dataset includes responses evaluated across four key metrics: instruction following, honesty, truthfulness, and helpfulness. We created preference cycles such as: instruction following honesty truthfulness helpfulness instruction following, ensuring the presence of intransitive cycles. We further generated four sub-datasets by omitting one metric from each cycle, resulting in datasets of varying complexity with 216 to 363 instances. Training and Evaluation. We trained the GPM using the Gemma-2B-it language model as the base and evaluated the models based on their ability to predict the human-provided preferences in these datasets. Since cyclic preferences are inherently intransitive, we measure accuracy as the percentage of correctly predicted human preferences, where higher scores indicate better handling 10 Table 1: Comparison of Bradley-Terry (BT) reward model and General Preference representation models (GPM) on cyclic preference datasets. Model Dataset Random Guess Acc. (%) 50.0 BT RM GPM BT RM GPM BT RM GPM BT RM GPM No instruction following No instruction following 62.4 100.0 (+37.6) No honesty No honesty No truthfulness No truthfulness No helpfulness No helpfulness 61.6 100.0 (+38.4) 50.0 100.0 (+50.0) 62.9 100.0 (+37.1) of non-transitive preferences. Results and Analysis. As shown in Table 1, the GP representation model achieves 100% accuracy across all datasets, significantly outperforming the BT model. These results validate the GP representation models ability to capture complex, cyclic preferences, confirming the theoretical advantages of using preference representation-based approach over traditional reward models that assume transitivity."
        },
        {
            "title": "6.2 Experiments on RewardBench",
            "content": "To address Q2, we compare the GP representation model and the BT reward model on the RewardBench benchmark (Lambert et al., 2024), which covers diverse preference modeling tasks, including Chat, Chat-Hard, Safety, and Reasoning. Datasets and Experimental Setup. We train both the BT and GPMs using the Skywork Reward Data Collection (Liu & Zeng, 2024), which contains 80,000 pairwise preference examples from tasks in various domains. We evaluate both models on RewardBench, using two different base models: Gemma-2B-it (Team et al., 2024) (2B parameters) and Llama-3.1-8B-Instruct (Dubey et al., 2024) (8B parameters), which are well-suited for instruction-following tasks (see Appendices A.2 and B.2 for the implementation details and experimental setup). Results and Analysis. Table 2 presents the results. The GPM consistently outperforms the BT model for both base models on RewardBench, with notable improvements in tasks involving complex reasoning (e.g., Chat-Hard and Reasoning). These results highlight the superior expressiveness of the GPM in preference modeling. Ablation Studies. We further conducted ablation studies to assess the impact of varying the representation (embedding) dimension in the GPM. Table 2 shows that increasing the embedding 1In late stages of the iterative training, πθt is close to equilibrium so the preference model can not distinguish between policy πθ and the opponent policy πθt ( meaning (cid:98)s (y πθt x) 0). Therefore, we have log Zπθt (x) 0. Table 2: Comparison between the Bradley-Terry (BT) models and the General Preference representation models (GPM) with varying embedding head dimensions on RewardBench. The highest scores are in bold and the second highest are underlined. Model Embed Dim. Chat Chat-Hard Safety Reasoning Average BT RM GPM BT RM GPM Base Model: Gemma-2B-it 58.4 68.4 63.1 68.2 71. 62.9 66.7 68.4 66.4 69.7 82.9 70.7 72.4 80.6 81.3 71.5 80.3 81.0 76.7 75.6 Base Model: Llama-3.1-8B-Instruct 89.7 92.7 93.6 93.3 92.7 87.3 87.3 87.1 88.6 86. 91.0 90.5 90.4 90.6 90.4 96.2 96.1 96.0 96.0 96.7 1 2 4 6 8 1 2 4 6 8 68.9 71.5 71.2 73.0 74.5 (+5.6) 91.1 91.7 91.8 92.1 (+1.0) 91. dimension generally improves performance. Additional experimental results regarding the ablation on the scale gate and L2 normalization on the embedding head can be found in Appendix B.1 (see Appendix for implementation details on the scale gate and the embedding head)."
        },
        {
            "title": "6.3 Downstream Performance on Aligning Language Models with Human Pref-",
            "content": "erences To address Q3, we investigate the effectiveness of the GPM in language model for alignment using Self-Play Policy Optimization (SPPO) (Wu et al., 2024b) and our proposed General Preference Optimization (GPO). Experimental Setup. We fine-tuned language models using SPPO and GPO, integrating preference scores provided by our GP representation model (GPM). We evaluated the models on AlpacaEval 2.0 (Dubois et al., 2024) and MT-Bench (Zheng et al., 2023), two widely used benchmarks for evaluating LLM alignment. Results and Analysis. The evaluation results on the benchmarks are as follows. For AlpacaEval 2.0, we compared the generated responses of the aligned models with those of GPT-4-turbo. To avoid the preference bias when using GPT-4-turbo as the evaluator, we also used DeepSeek-V2 (DeepSeekAI, 2024) and GPT-4o-mini as the evaluators besides GPT-4-turbo itself. Notice that the Length Controlled (LC) Win Rate results are using generalized linear model fitted using default evaluator GPT-4-turbo, so it does not apply to other evaluators. The results of the three evaluators are presented in Tables 3, 4 and 5. For MT-Bench, we used the default mode to let GPT-4 grade and give score to the models answer, and the MT-Bench scores of aligned models are presented in Table 6. These results suggest that integrating the General Preference representation model (GPM) into policy optimization can enhance the downstream performance of language models, supporting alignment with complex human 12 preferences (for additional experimental results on GSM8K, MMLU, etc., please see Appendix B.1). Table 3: AlpacaEval 2.0 evaluation results. Base model: LLama3-8B-it, Evaluator: GPT-4o-mini. The results are grouped by the size and type of the RM or PM, and the number of iterations. Size Type Iter SPPO GPO Win Rate Avg. Len Win Rate Avg. Len base 32.26 2B BT RM 1 2 3 44.48 59.89 71.11 GPM 1 2 52.64 (+8.16) 66.01 (+6.12) 72.97 (+1.86) 8B BT RM 1 2 3 40.96 52.43 55.30 GPM 1 2 3 45.80 (+4.84) 56.04 (+3.61) 60.73 (+5.43) 1843 2028 2244 2102 2318 2490 1802 1945 1832 1878 2020 1994 32.26 49.55 57.19 67. 58.49 (+8.94) 72.22 (+15.03) 77.11 (+9.30) 42.56 57.19 62.79 49.55 (+6.99) 57.54 (+0.35) 66.23 (+3.44) 1959 1837 2048 2245 2145 2404 1764 2080 3445 1877 2126 2157 Table 4: AlpacaEval 2.0 evaluation results. Base model: LLama3-8B-it, Evaluator: GPT-4-turbo. The results are grouped by the size and type of the RM or PM, and the number of iterations. Size Type Iter base 2B BT RM 1 2 3 GPM 1 2 3 8B BT RM 1 2 3 GPM 1 2 3 SPPO GPO LC. WR WR Avg. Len LC. WR WR Avg. Len 23.07 33.71 37.38 38.86 32.64 35.16 35.30 33.38 38.83 40.55 33.39 37.35 39. 23.34 30.88 37.88 42.98 35.12 41.40 45.44 29.90 37.82 37.09 31.49 37.60 39.38 1843 2028 2244 2102 2318 2490 1802 1945 1832 1878 2020 1994 23.07 36.28 37.87 38. 35.23 36.04 38.51 35.96 41.48 42.16 37.45 39.07 41.19 23.34 33.17 38.89 42.80 38.10 44.47 48. 31.71 41.73 43.33 35.17 39.94 43.38 1959 1837 2048 2245 2145 2404 2613 1764 2080 1877 2126 2157 13 Table 5: AlpacaEval 2.0 evaluation results. Base model: LLama3-8B-it, Evaluator: DeepSeek-V2. The results are grouped by the size and type of the RM or PM, and the number of iterations. Size Type Iter SPPO GPO Win Rate Avg. Len Win Rate Avg. Len 2B BT GP 8B BT GP base 1 2 3 1 2 3 1 2 1 2 3 30.20 43.00 51.78 60.99 48.41 (+5.41) 58.12 (+6.34) 65.26 (+4.27) 41.73 50.24 50.76 45.42 (+3.69) 52.77 (+2.53) 54.73 (+3.97) 1959 1843 2028 2244 2102 2318 2490 1802 1945 1832 1878 2020 1994 30. 43.78 52.80 58.21 52.57 (+8.79) 62.52 (+9.72) 68.28 (+10.07) 43.36 52.90 56.59 46.89 (+3.53) 53.03 (+0.13) 59.43 (+2.84) 1959 1837 2048 2145 2404 2613 1764 2080 3445 1877 2126 2157 Table 6: MT-Bench evaluation results. Base model: LLama3-8B-it, Evaluator: GPT-4. Size Type Iter SPPO 2nd 1st Avg. 1st GPO 2nd Avg. base 8.27 2B BT RM 1 2 3 GPM 1 2 3 8B BT RM 1 2 3 GPM 1 2 8.11 8.37 8.26 8.26 8.26 8.31 8.53 8.33 8.05 8.27 8.46 8.14 7.38 7.44 7.59 7. 7.66 7.66 7.53 7.85 7.91 7.66 7.60 7.71 7.59 7.82 7.77 7.98 7.81 7.96 7.96 7. 8.19 8.12 7.86 7.93 8.08 7.87 8.27 8.15 8.20 8.59 8.49 8.38 8.63 8.54 8.16 8. 8.30 8.11 8.22 7.38 7.37 7.69 7.54 7.50 7.55 7.89 8.05 7.55 7.76 7.70 7.65 7. 7.82 7.77 7.95 8.06 7.99 7.97 8.26 8.29 7.85 7.94 8.00 7.88 7."
        },
        {
            "title": "7 Conclusion",
            "content": "This work introduced preference representation learning, framework for modeling human preferences that can capture complex, intransitive structures like cyclic preferences. The proposed General 14 Preference representation model (GPM) achieves linear complexity while maintaining the ability to model intricate preference relationships. It consistently outperforms traditional models like BradleyTerry and supervised preference models across various benchmarks, including cyclic preference datasets and real-world tasks from RewardBench. Additionally, incorporating preference scores from GPM into policy optimization methods, such as SPPO and the newly introduced General Preference Optimization (GPO), led to significant performance improvements in downstream tasks that require alignment with intricate human preferences, as demonstrated in benchmarks like AlpacaEval 2.0 and MT-Bench. Limitations. Despite the current interesting experimental results, further exploration is still needed. The choice of representation (embedding) dimensions and model architecture can influence the expressiveness and computational efficiency of GPM. An overly expressive model such as high embedding dimension can impact performance, particularly in tasks prone to overfitting. Future works should examine the expressiveness of preference representations in wider range of real-world datasets."
        },
        {
            "title": "References",
            "content": "Marina Agranov and Pietro Ortoleva. Stochastic choice and preferences for randomization. Journal of Political Economy, 125(1):4068, 2017. Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023. David Balduzzi, Karl Tuyls, Julien Perolat, and Thore Graepel. Re-evaluating evaluation. Advances in Neural Information Processing Systems, 31, 2018. David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jaderberg, and Thore Graepel. Open-ended learning in symmetric zero-sum games. In International Conference on Machine Learning, pp. 434443. PMLR, 2019. Edward Beeching, Clementine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard (2023-2024). https: //huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard, 2023. Quentin Bertrand, Wojciech Marian Czarnecki, and Gauthier Gidel. On the limitations of the elo, real-world games are transitive, not additive. In International Conference on Artificial Intelligence and Statistics, pp. 29052921. PMLR, 2023. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pp. 15971607. PMLR, 2020. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with scaled ai feedback, 2024. URL https://arxiv.org/abs/2310.01377. Wojciech Czarnecki, Gauthier Gidel, Brendan Tracey, Karl Tuyls, Shayegan Omidshafiei, David Balduzzi, and Max Jaderberg. Real world games look like spinning tops. Advances in Neural Information Processing Systems, 33:1744317454, 2020. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Miroslav Dudık, Katja Hofmann, Robert Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling bandits. In Conference on Learning Theory, pp. 563587. PMLR, 2015. Zhaolin Gao, Jonathan Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kiante Brantley, Thorsten Joachims, Andrew Bagnell, Jason Lee, and Wen Sun. Rebel: Reinforcement learning via regressing relative rewards. arXiv preprint arXiv:2404.16767, 2024. Martin Gardner. Mathematical games. Scientific american, 222(6):132140, 1970. Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in atari. Advances in neural information processing systems, 31, 2018. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. https://huggingface.co/ spaces/allenai/reward-bench, 2024. 16 Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Chris Yuhao Liu and Liang Zeng. Skywork reward model series. https://huggingface.co/Skywork, September 2024. URL https://huggingface.co/Skywork. Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, et al. Lipo: Listwise preference optimization through learning-to-rank. arXiv preprint arXiv:2402.01878, 2024. Hao Lou, Tao Jin, Yue Wu, Pan Xu, Quanquan Gu, and Farzad Farnoud. Active ranking without strong stochastic transitivity. Advances in neural information processing systems, 35:297309, 2022. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 2773027744, 2022. Guillermo Owen. Game theory. Emerald Group Publishing, 2013. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. openai.com, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. 17 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715, 2024. Paul Rottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models, 2024. URL https://arxiv.org/abs/2308.01263. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Amartya Sen. Social choice theory. Handbook of mathematical economics, 3:10731181, 1986. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, and et al. Gemma 2: Improving open language models at practical size, 2024. URL https: //arxiv.org/abs/2408.00118. Louis Thurstone. law of comparative judgment. In Scaling, pp. 8192. Routledge, 2017. Amos Tversky. Intransitivity of preferences. Psychological review, 76(1):31, 1969. Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more difficult than standard rl? arXiv preprint arXiv:2306.14111, 2023. Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: Evaluating safeguards in LLMs. In Yvette Graham and Matthew Purver (eds.), Findings of the Association for Computational Linguistics: EACL 2024, pp. 896911, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. findings-eacl.61. 18 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024a. Yue Wu, Tao Jin, Hao Lou, Farzad Farnoud, and Quanquan Gu. Borda regret minimization for generalized linear dueling bandits. arXiv preprint arXiv:2303.08816, 2023. Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024b. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In International Conference on Learning Representations (ICLR), 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "Appendix",
            "content": "A More on General Preference Representation Learning 21 A.1 Complex Representations Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . 21 A.2 Implementing General Preference Representation Model . . . . . . . . . . . . . . . . 21 A.3 Training Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.4 Appendix for Proofs More on Experiments 27 B.1 Additional Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 B.2 Implementation Details More on General Preference Optimization"
        },
        {
            "title": "A More on General Preference Representation Learning",
            "content": "In this section, we present additional discussion on general preference modeling with preference representations. Proposition A.1. For any two vectors vi R2k and vj R2k, if R2k2k satisfies the following two properties: 1. Skew-symmetry: Rvi, vj = Rvj, vi. 2. Magnitude preserving: Rvi, Rvi = vi, vi. Then must be in the form = UJU, where R2k2k is an orthonormal matrix (e.g. identity matrix I2k) and is block-diagonal matrix consisting of skew-symmetric blocks of the form: Jl = (cid:21) (cid:20)0 1 0 1 , = 1, . . . , k. A.1 Complex Representations Interpretation Our model can also be interpreted using complex representations. By representing the representations as complex vectors vy Ck, we can express the preference score as: s(yi yj x) = Im (cid:0)vyi, vyj (cid:1) , where Im() denotes the imaginary part, and , is the Hermitian inner product. This formulation captures cyclic and intransitive preferences through the angular relationships between complex presentations. Theorem A.2 (Expressiveness of Complex Preference Representations). Let Rkk be real i=1 Ck such that: skew-symmetric matrix (i.e., = P). Then, there exist complex vectors {vi}k Pij = Im (vi, vj) , i, j. Example. For = 1, let vy = eiθy , then: s(yi yj x) = sin(θyi θyj ). A.2 Implementing General Preference Representation Model When the preference score matrix has an even dimension, i.e., R2k2k, we have more interesting interpretation based on spectral decomposition. Theorem A.3 (Expressiveness of Preference Representation Model). Let R2k2k be real skewi=1 R2k symmetric matrix (i.e., = P). Then there exist representations (embeddings) {vi}2k and block-diagonal skew-symmetric matrix R2k2k, with consisting of blocks of the form: Rl = (cid:21) (cid:20)0 1 0 1 , = 1, . . . , k, (a) Cyclic 3 (b) Cyclic 4 (c) Cyclic 5 Figure 3: Visualization of learned preference embedding vectors for cyclic preferences with sizes 3, 4, and 5, e.g., A. such that: Rvj, Moreover, the representations {vi} can be constructed from the orthogonal matrix in the decomposition of P, scaled by the square roots of the positive eigenvalues of P. Pij = i, j. To effectively capture general preferences while maintaining computational efficiency, we implement our preference representation model by augmenting an existing language model with two additional components: an eigenvalue scale gate and an eigenvector embedding head. Eigenvalue Scale Gate. The eigenvalue scale gate Gλ computes context-dependent scaling factors {λl(x)}, where λl(x) 0, based solely on the prompt x: {λl(x)} = Gλ(x). This component models how different preference dimensions are weighted in the context of the given prompt, effectively adjusting the importance of various aspects such as helpfulness, instructionfollowing, and creativity. Eigenvector Embedding Head. The eigenvector embedding head Ev generates embeddings vyx for each response in the context of the prompt x: vyx = Ev(x, y). These embeddings capture the nuanced characteristics of the responses relevant to human preferences. Preference Score. The preference score between two responses is computed as: s(yi yj x) = yixD(x)RD(x)vyj x. where D(x) is block-diagonal matrix with blocks (cid:112)λl(x)I2, and is the skew-symmetric operator defined previously. We normalize the embeddings vy to have unit length to ensure stability in training. 22 Automatic Subspace Discovery. The use of multiple dimensions in the embeddings allows the model to discover different subspaces corresponding to various preference dimensions automatically. Each pair of dimensions can capture distinct aspects of preferences, such as helpfulness, correctness, or stylistic elements. The context-dependent eigenvalues λl(x) modulate the contributions of these subspaces based on the prompt, enabling the model to adapt to varying user preferences dynamically. This implementation provides scalable and flexible way to model complex human preferences, making it suitable for large-scale applications in language modeling and other domains where alignment with nuanced human values is essential. A.3 Training Objective The preference embedding can thus be obtained by minimizing the cross-entropy loss over observed preference data. Given dataset (x, yw, yl) of preference comparisons, we denote P(yw ylx) as the probability of the winner yw being chosen over the loser yl (1 if hard preference is given). The cross-entropy loss function is: LCE = (cid:88) (cid:20) PD(yw yl x) log σ (x,yw,yl)D (cid:19) s(yw yl x) (cid:18) 1 β +(1 PD(yw yl x)) log σ (cid:18) 1 β (cid:19)(cid:21) s(yw yl x) . Alternatively, if there is an oracle providing continuous scores, we can use regression loss: LMSE = (cid:88) (x,yw,yl)D (cid:18) 1 β s(yw yl x) sD(yw yl x) , (cid:19) where sD(yw yl x) is the dataset-provided score satisfying σ (sD(yw yl x)) = PD(yw yl x). A.4 Appendix for Proofs Proof of the Proposition A.1. Proof. Let R2k2k be real matrix satisfying the following properties: 1. Skew-symmetry with respect to the inner product: Rv, = Rw, v, v, R2k. 2. Magnitude preserving: Rv, Rv = v, v, R2k. Recall that the standard inner product in R2k is given by v, = vw, which is symmetric: v, = w, v. From the skew-symmetry condition, we have: Rv, + Rw, = 0, v, R2k. 23 Since Rw, = (Rw)v = wRv, the above condition becomes: vRw + wRv = 0, v, R2k. This implies that is skew-symmetric: From the magnitude-preserving property, we have: = R. Rv, Rv = (Rv)Rv = vRRv = vv, R2k. Therefore, Using = R, we obtain: RR = I2k. (R)R = I2k R2 = I2k. This shows that satisfies the equation R2 = I2k. The characteristic polynomial of is then: Since R2 = I2k, it follows that the eigenvalues λ satisfy: det(R λI2k) = 0. λ2 = 1 λ = i. Thus, has eigenvalues i, each with algebraic multiplicity k. Because is real and skew-symmetric, it can be brought into block-diagonal form via an orthogonal transformation. Specifically, there exists an orthogonal matrix R2k2k such that: = UJU, where and each block Jl is 2 2 skew-symmetric matrix of the form: = blockdiag(J1, J2, . . . , Jk), Jl = (cid:21) (cid:20)0 1 0 1 , = 1, . . . , k. This decomposition leverages the standard canonical form for real skew-symmetric matrices, which states that any such matrix can be orthogonally diagonalized into blocks of this type. Therefore, can be expressed as: = UJU, where R2k2k is an orthogonal matrix, and is the block-diagonal matrix consisting of blocks Jl. This completes the proof. Proof of the Theorem 4.4. 24 Proof. We aim to represent the entries of the skew-symmetric matrix Rkk using vectors in R2k and block-diagonal skew-symmetric matrix R2k2k. For each = 1, . . . , k, define the vector vi R2k as: vi = (cid:21) (cid:20)ai bi , where ai, bi Rk are real vectors to be specified. Set ai = ei, the i-th standard basis vector in Rk, and define bi as: bi = 1 2 pi, where pi is the i-th row of P. Thus, the j-th component of bi is (bi)j = 1 2 Pij. Define the block-diagonal matrix R2k2k as: where each block Rl is the 2 2 skew-symmetric matrix: = blockdiag(R1, . . . , Rk), Rl = (cid:21) (cid:20)0 1 0 1 , = 1, . . . , k. Now, compute the inner product Rvj: Rvj = (cid:2)a i (cid:3) (cid:20)0kk Ik 0kk Ik (cid:21) (cid:20)aj bj (cid:21) Since ai = ei, we have: = bj + aj. bj = a bj = (bj)i = aj = b ej = (bi)j = Pji = 1 2 Pij, Pij. 1 2 1 2 Rvj = (cid:18) 1 (cid:19) Pij + 1 2 Pij = Pij. Pij = i Rvj. Therefore, Thus, for all i, j, (A.1) (A.2) This construction shows that any real skew-symmetric matrix can be represented in terms of vectors {vi} R2k and the block-diagonal skew-symmetric matrix R. This completes the proof. Proof of the Theorem A.2. 25 Proof. We aim to represent any real skew-symmetric matrix Rkk using the imaginary parts of inner products of complex vectors. For each = 1, . . . , k, define the complex vector vi = ai + bi, where ai, bi Rk. Let ai = ei, the i-th standard basis vector in Rk, and set bi = 1 2 (cid:88) j=1 Pijej. This implies that the j-th component of bi is (bi)j = 1 2 Pij. The Hermitian inner product of vi and vj is vi, vj = (a )(aj + bj) = i aj + bj + (b aj bj). Therefore, Compute i aj and bj: Im (vi, vj) = aj bj. i aj = (bi)j = bj = (bj)i = 1 2 1 2 Pij, Pji = 1 Pij, since Pji = Pij due to skew-symmetry. Thus, Im (vi, vj) = (cid:18) Pij 1 2 1 2 (cid:19) Pij = Pij. Therefore, we have constructed complex vectors vi such that Pij = Im (vi, vj) , i, j. This completes the proof. Proof of the Theorem A.3. Proof. Since is real and skew-symmetric with even dimension 2k, it can be brought into blockdiagonal form via an orthogonal transformation. Specifically, there exists an orthogonal matrix R2k2k such that: = UΛU, where Λ is block-diagonal matrix composed of blocks λlJ, with λl 0 and = (cid:21) (cid:20)0 1 0 1 . This decomposition leverages the fact that the eigenvalues of are purely imaginary and occur in conjugate pairs iλl. Define the block-diagonal matrix = blockdiag(J, . . . , J) R2k2k, and let λ1I2, . . . , = blockdiag( Observe that Λ = DRD. λkI2) R2k2k, where I2 is the 2 2 identity matrix. 26 Set = UD. Then, = UΛU = UDRDU = VRV. Therefore, where vi is the i-th row of V. Pij = Rvj, i, j, This construction shows that any real skew-symmetric matrix can be represented in terms of embeddings {vi} and the asymmetric operator R, confirming the full expressiveness of our preference representation model."
        },
        {
            "title": "B More on Experiments",
            "content": "B.1 Additional Experimental Results Ablations on Scale Gate and Embedding head. We investigate the effects of scale gates and embedding head dimensions, with and without L2 normalization, on model performance. As shown in Table 7 , for Gemma-2B-it models, incorporating scale gate generally enhances GPM performance across various embedding dimensions. L2 normalization on the embedding head output consistently improves models with scale gates. Interestingly, Gemma-2B-it-based models without L2 normalization or scale gates outperform those with L2 normalization but no scale gates. plausible explanation for this phenomenon is that removing L2 normalization introduces additional degrees of freedom, particularly beneficial for models with smaller parameter spaces and high-dimensional embedding layers. This increased flexibility may allow the model to better utilize its limited parametric capacity, potentially leading to enhanced expressiveness and task-specific adaptability. For larger models, such as those based on Llama3.1-8B-Instruct, the impact of scale gates becomes less pronounced. This diminished effect may be attributed to the inherently stronger representational capacity of the 8B parameter model, which can likely capture complex patterns more effectively without additional architectural modifications. These observations suggest nuanced relationship between model size, normalization techniques, and architectural enhancements like scale gates, highlighting the importance of considering these factors in model design and optimization. More Results on Evaluating Language Model Alignment. We further conduct rigorous evaluation of our downstream task-specific models using various benchmarks. AlpacaEval 2.0 evaluation results are listed in Table 4 and Table 5, using GPT-4-turbo and Deepseek-V2 as evaluators respectively. For LM-Harness, we chose Arc-Challenge, TruthfulQA, WinoGrande, GSM8k, HellaSwag, and MMLU as the evaluation tasks, and used the default rule-based evaluator of lm-evaluation-harness for accuracy calculation. These tasks are the same as those evaluated by Open LLM Leaderboard v1 (Beeching et al., 2023), which no longer provides service. Notice that the evaluator of LM-Harness is not the same as Open LLM Leaderboard v1, so we only compare different models within our results and do not compare them with the Open LLM Leaderboard v1. The results are showed in Tables 8 and 9. To facilitate direct comparison with current state-of-the-art models, we adhere to the evaluation protocol established by the Open LLM Leaderboard v1. Our 27 models are evaluated locally using this standardized framework. The resultant performance metrics are presented in Tables 11 and Table 10. Table 7: Impact of the embedding head and the scale gate on the GPMs performance on RewardBench. Dim. represents the dimension of the embedding head. The highest average scores for each base model are in bold and the second highest are underlined. Embedding Type Dim. Chat Chat-Hard Safety Reasoning Average Base Model: Gemma-2B-it w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. l2 w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. l2 w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. l2 w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. l2 w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. l2 w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. l2 w. scale gate w. l2 w. scale gate w.o. l2 w. o. scale gate w. l2 w. o. scale gate w.o. 2 2 2 2 4 4 4 4 6 6 6 6 8 8 8 8 68.4 70.7 68.4 67.0 63.1 69.3 71.2 66. 68.2 66.8 65.4 69.8 71.5 69.8 67.3 69.6 66.7 66.4 62.9 69.5 68.4 71.7 60.7 67.8 66.4 62.9 65.1 71.1 69.7 66.9 66.2 67. 70.7 72.8 70.3 70.2 72.4 67.1 66.6 71.7 80.6 68.5 66.9 73.3 81.3 78.9 62.9 76.1 Base Model: Llama-3.1-8B-Instruct 2 2 2 4 4 4 4 6 6 6 6 8 8 8 8 92.7 92.5 92.2 93.0 93.6 93.0 93.9 93.6 93.3 93.9 93.9 94. 92.7 94.1 93.0 93.9 87.3 87.1 88.6 87.1 87.1 87.9 87.3 87.5 88.6 87.9 85.5 89.3 86.4 87.3 87.1 88.2 90.5 88.4 90.6 88.4 90.4 91.1 90.8 88.4 90.6 89.3 91.5 87.6 90.4 89.7 91.1 89.7 80.3 71.4 72.5 68.1 81.0 73.8 76.6 69. 76.7 62.8 63.6 74.9 75.6 75.9 70.8 70.4 96.1 96.1 97.4 94.3 96.0 96.1 96.7 94.5 96.0 95.7 96.3 93.4 96.7 95.5 96.6 94. 71.5 70.3 68.5 68.7 71.2 70.5 68.8 68.9 73.0 65.2 65.3 72.3 74.5 72.9 66.8 71.0 91.7 91.0 92.2 90.7 91.8 92.0 92.2 91. 92.1 91.7 91.8 91.2 91.6 91.7 92.0 91.7 Table 8: LM-Harness evaluation results of LLama3-8B-it model fine-tuned using SPPO with BT reward model and our GPM. Size Type Iter SPPO Arc TruthfulQA WinoGrande GSM8k HellaSwag MMLU Average base 56.91 51.65 2B BT RM 1 2 3 GPM 1 2 3 8B BT RM 1 2 3 GPM 1 2 55.89 52.94 54.69 52.61 53.41 52.23 55.20 52.92 53.84 53.12 53.50 52.98 56.14 55.18 56.23 56.32 56.06 55.80 55.89 55.40 55.12 55.53 55.12 55.66 71.43 71.82 71.74 71. 71.59 71.51 71.82 71.51 72.22 72.61 71.67 71.90 72.06 32.83 26.46 21.46 19.56 26.99 23.88 28. 25.93 27.45 25.78 28.35 33.13 29.87 75.78 75.95 75.69 75.70 75.64 75.37 75.22 76.35 76.19 76. 76.35 76.01 75.89 63.72 63.79 63.89 63.91 63.84 63.65 63.47 63.99 63.77 63.87 63.89 63.84 64. 58.72 57.81 56.68 56.10 57.70 56.90 57.65 58.18 58.70 58.37 58.59 59.25 58.78 Table 9: LM-Harness evaluation results of LLama3-8B-it model fine-tuned using GPO with BT reward model and our GPM. Size Type Iter GPO Arc TruthfulQA WinoGrande GSM8k HellaSwag MMLU Average base 56.91 51.65 2B BT RM 1 2 GPM 1 2 3 8B BT RM 1 2 3 GPM 1 2 3 54.95 53.41 53.08 53.5 52.73 53.14 53.67 53.19 52.82 53.43 52.22 53.60 55.55 55.87 55.29 57.10 54.52 56. 55.55 56.10 53.58 55.98 53.41 55.87 32.83 25.17 23.12 22.44 28.89 27.37 27.37 23.35 30.33 35.18 28.51 29.72 31. 75.78 75.52 75.1 75.0 75.25 74.75 74.54 76.15 75.34 73.78 75.79 74.90 73.62 63. 63.86 63.96 63.64 63.97 63.84 63.72 64.01 63.77 63.53 63.95 63.82 63.77 58.72 57.46 56.68 56. 57.72 57.19 57.08 57.84 58.93 59.25 58.67 58.26 58.26 71.43 71.82 71.35 71.35 71.35 70.96 71. 72.14 71.74 72.06 72.14 71.59 71.43 30 Table 10: Open LLM Leaderboard v1 evaluation results of LLama3-8B-it model fine-tuned using SPPO with BT reward model and our GPM. Size Type Iter SPPO Arc TruthfulQA WinoGrande GSM8k HellaSwag MMLU Average base 62.20 51.66 2B BT RM 1 2 3 GPM 1 2 3 8B BT RM 1 2 GPM 1 2 3 62.63 52.93 62.03 52.55 61.86 52.21 62.37 53.03 62.12 53.04 62.03 52.95 63.31 55.18 63.82 56.31 64.51 55.80 63.05 55.40 63.14 55.53 63.57 55.66 75. 76.24 76.64 76.16 75.93 76.64 76.56 77.27 76.40 76.72 76.16 75.93 75.69 76.04 75.82 75.13 75. 74.53 75.59 75.36 76.35 78.17 77.26 77.56 77.71 77.41 78.67 79.27 79.04 79.01 78.83 78.73 78. 79.83 79.80 79.41 79.82 79.43 79.40 65.67 65.86 65.64 65.75 65.82 65.77 65.66 65.98 65.97 65. 65.86 65.97 66.07 68.30 68.79 68.51 68.47 68.42 68.65 68.52 69.65 70.08 69.93 69.64 69.62 69. Table 11: Open LLM Leaderboard v1 evaluation results of LLama3-8B-it model fine-tuned using GPO with BT reward model and our GPM. Size Type Iter GPO Arc TruthfulQA WinoGrande GSM8k HellaSwag MMLU Average base 62.20 51. 2B BT RM 1 2 3 GPM 1 2 3 8B BT RM 1 2 3 GPM 1 2 3 62.80 53.38 61.77 53.09 61.52 53.16 61.95 53.28 61.77 53.43 61.43 53. 63.91 55.87 63.82 57.10 62.46 56.44 63.65 56.10 62.88 55.98 62.71 55.87 76.04 76.72 75.82 75.59 74.53 75.74 76.12 77.48 76.04 77. 77.10 76.42 76.35 78.67 78.76 78.6 78.52 78.56 78.33 78.06 79.64 79.38 78.35 79.47 79.00 78. 65.67 65.87 65.76 65.77 66.02 65.77 65.65 65.88 65.72 65.54 65.69 65.94 65.95 68. 68.98 68.61 68.44 68.39 68.48 68.34 69.94 69.57 69.04 69.71 69.36 68.94 75.53 76.32 76.64 76. 76.01 75.85 75.22 76.87 75.37 74.11 76.24 75.93 74.51 31 B."
        },
        {
            "title": "Implementation Details",
            "content": "Details on Training Setup. Our experiments on RewardBench and Cyclic Preference Dataset were implemented using the HuggingFace Transformers library (Wolf et al., 2020) and the OpenRLHF framework (Hu et al., 2024). For reward model training on Skywork Reward Data Collection, we employed the following settings (in Table 12): Gemma-2B-it: Trained with learning rate of 1 105. Llama-3.1-8B-Instruct: Trained with learning rate of 2 106. Training Configuration: Both models were trained for two epochs with global batch size of 32. We used cosine learning rate scheduler with warm-up ratio of 0.03. Input sequences were truncated to maximum length of 2048 tokens. Hyperparameters: For the Bradley-Terry (BT) model, the temperature parameter β was set to 1, following standard practice (Rafailov et al., 2024). For our General Preference (GP) model, we set β = 0.1, determined via hyperparameter tuning on validation set. Hardware: All experiments were conducted on machines equipped with NVIDIA A800 80GB GPUs, utilizing 8 GPUs per experiment. For cyclic preference experiments, the training settings are as follows, except for the parameters specified below; all other experimental parameters remain consistent with experiments on RewardBench (in Table 13): Gemma-2B-it: Trained with learning rate of 1 106. Training Configuration: Models were trained for 50 epochs with global batch size of 1. Hardware: Experiments were conducted on machines equipped with NVIDIA A800 80GB GPUs, utilizing single GPU per experiment. Details on Evaluation Dataset RewardBench. RewardBench is divided into four core sections: Chat: Evaluates the ability to differentiate between thorough and correct responses in open-ended conversations, using data from AlpacaEval (Li et al., 2023) and MT Bench (Zheng et al., 2023). Chat-Hard: Tests the handling of trick questions and subtle instruction differences, using adversarial examples from MT Bench and LLMBar (Zeng et al., 2024). Safety: Assesses the capacity to refuse harmful content appropriately, using data from XSTest (Rottger et al., 2024), Do-Not-Answer (Wang et al., 2024), and custom AI2 dataset. Reasoning: Measures code generation and reasoning abilities, with prompts from HumanEvalPack (Muennighoff et al., 2023) and PRM800k (Lightman et al., 2023)."
        },
        {
            "title": "C More on General Preference Optimization",
            "content": "Connection to Policy Gradient. Applying policy gradient theorem on Equation (5.5) gives: θExX ,yπθ = ExX ,yπθ (cid:21) πθ(yx) πθt(yx) πθ(yx) πθt(yx) (cid:20) (cid:98)s(y πθt) β log (cid:20)(cid:18) (cid:98)s(y πθt) β log 32 (cid:19) (cid:21) θ log πθ(yx) Table 12: Implementation details for experiments on RewardBench. General Settings Base models Batch size Quantization for training Learning Rate Learning Rate Scheduler Warmup Ratio Max training epochs Gradient accumulation step Max input length Zero stage Flash attention enabled Gemma-2b-it and Llama3.1-8B-Instruct 32 bf16 1 105 for Gemma and 2 106 for Llama3.1 cosine 0.03 2 1 2048 3 True β for loss function β for loss function General Preference Model Bradly Terry Model 0.1 1 Table 13: Implementation details for experiments on Cyclic Preference Dataset. General Settings Base models Batch size Quantization for training Learning Rate Learning Rate Scheduler Warmup Ratio Max training epochs Gradient accumulation step Max input length Zero stage Flash attention enabled Gemma-2b-it 1 bf16 1 106 cosine 0.03 50 1 2048 3 True General Preference Model β for loss function Bradly Terry Model β for loss function 0.1 1 = ExX ,yπθ (cid:20) (cid:18) θ (cid:98)s(y πθt) β log πθ(yx) πθt(yx) (cid:19)2(cid:21) . 33 So Equation (5.7) can also be seen as an offline policy gradient method for the optimization problem (5.5)."
        }
    ],
    "affiliations": []
}