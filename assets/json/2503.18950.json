{
    "paper_title": "Target-Aware Video Diffusion Models",
    "authors": [
        "Taeksoo Kim",
        "Hanbyul Joo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis."
        },
        {
            "title": "Start",
            "content": "Target-Aware Video Diffusion Models Taeksoo Kim1 Hanbyul Joo1,2 1Seoul National University 2RLWRLD {taeksu98, hbjoo}.snu.ac.kr https://taeksuu.github.io/tavid/ 5 2 0 2 2 ] . [ 2 0 5 9 8 1 . 3 0 5 2 : r Figure 1. Target-Aware Video Diffusion Models. Given an input image, our target-aware video diffusion model generates video in which an actor accurately interacts with specified target. The target is indicated to the model via segmentation mask, while the desired action with the target is described using text prompt."
        },
        {
            "title": "Abstract",
            "content": "We present target-aware video diffusion model that generates videos from an input image in which an actor interacts with specified target while performing desired action. The target is defined by segmentation mask and the desired action is described via text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actors movements toward the target, our target-aware model requires only simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce special token that encodes the targets spatial information within the text prompt. We then fine-tune the model with our curated dataset using novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our targetaware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis. 1. Introduction Video diffusion models have shown remarkable capability in simulating complex scenes. To effectively leverage the priors of these generative models for real-world simulations, it is essential to generate videos with control over both content and motion. Image-to-video diffusion models [24, 50, 94] typically generate videos from an input image by using text prompts as control. However, controlling the actor in an image to perform specific actions with the target through text descriptions remains challenging, as demonstrated in Fig. 8 (top row). While generated videos generally follow the semantics of the input text prompts, they often hallucinate objects instead of accurately performing interac1 tions with specified targets, resulting in unintended outputs. To improve control, several approaches incorporate dense structural or motion cues, such as depth maps [17, 104], edges [14, 42], optical flow [11, 22, 63], motions [70, 92], or drags [15, 71, 76], obtained from source videos or provided with interactive user inputs. While these methods facilitate precise controls for simple translations or viewpoint changes, they are less effective for actor-target interaction scenarios, where providing structural action guidance for the actors is non-trivial. Moreover, we seek the potential to leverage generative priors of video diffusion models for high-level action planning to infer plausible interaction cues for the actor (agent) within the current scene, as explored in recent robotics research [3, 6, 16, 62]. In this context, we consider video diffusion model as planner generating plausible actions as outputs, rather than shader [22] taking heavy motion conditions as inputs. In this paper, we present target-aware video diffusion model that generates videos from an input image, where an actor interacts with specified target while performing desired action. The target is defined by segmentation mask that can easily be obtained using an off-the-shelf tool [49], and the desired action is described with text prompt, as shown in Fig. 1. In contrast to previous approaches that depend on dense structural cues over frames to control actor motions, our method takes only the target object mask, allowing the models generative priors to autonomously infer plausible actor motions. Our method significantly simplifies the controllable video generation pipeline, removing the need for additional motion conditions from source videos or detailed user inputs. Furthermore, the resulting motions extracted from the models generative priors can serve as plausible action cues for other controllable video generation approaches or downstream applications. To integrate spatial information of the target mask, we extend base image-to-video diffusion model [94] to take the mask as an additional input, and fine-tune it on our newly curated dataset. However, simply fine-tuning the model with the extra mask input does not ensure the targetawareness of the model. To address this, we introduce special token, [TGT], into the text prompt to describe the target and enforce an alignment between the [TGT] tokens cross-attention maps and the input target mask using our novel cross-attention loss. Our cross-attention loss enables the model to associate the [TGT] token with the spatial information of the target, improving the precision of the generated interactions with the target. We selectively apply this loss to specific transformer blocks and attention regions that are most semantically relevant for effective supervision. Experimental results demonstrate that our target-aware video diffusion model outperforms existing solutions in synthesizing videos where actors precisely engage with designated targets. To further demonstrate the strength of our target-aware model, we apply our model to two downstream applications: (1) video content creation, generating longform videos covering navigations and interactions with simple user inputs, and (2) zero-shot 3D HOI motion synthesis, simulating physical agents performing plausible actions in given environment. Our contributions can be summarized as follows: We present target-aware video diffusion model that generates videos of interactions between the actor and the target using segmentation mask and text prompt. We propose novel cross-attention loss that enables the model to effectively utilize the additional mask input, and provide comprehensive analysis of its effects across different parts of the model. We present newly curated dataset specifically designed to train and evaluate our target-aware model. We demonstrate two real-world applications of our targetaware model: video content creation and zero-shot 3D HOI motion synthesis for controlling physical agents. We will release our code and dataset for future research. 2. Related Work Controllable Video Generation Building on early work [17, 23, 32] that extends text-to-image diffusion models to video generation, the community has shown significant interest in producing videos with enhanced controls. Several methods, inspired by ControlNet [99], have been adapted for videos, where additional structural cues, such as depth maps [17, 104], edge information [14, 42], optical flow [22, 63], or motion [70, 92], are integrated into the generation process via additional modules to produce structureconsistent outputs. Another line of research focuses on manipulating the internal representations of diffusion models to achieve controls without extra modules. Attention modulation approaches [36, 83, 93] adjust cross-attention maps of predefined regions to steer subject movements. Inversionbased feature injection methods [37, 56, 79] edit videos in zero-shot manner by leveraging cross-attention control, allowing for content editing without training. Other work [15, 76, 84, 97] extends drag-based image editing techniques [64, 71, 72] to video, by either adding an extra dragembedding module or optimizing video latents to align with user drag inputs. While existing approaches focus on generating videos that faithfully follow the dense input cues, either from source video or user inputs, we aim to extract those motion cues from video diffusion models with minimal extra input, segmentation mask of the target. Human-Scene Interaction Synthesis. Synthesizing natural human motions within given scene remains challenging, requiring high-level semantic understanding of human-scene interactions and affordances. Early work primarily focuses on posing static 3D human in 3D environment [26, 28, 34, 47, 55, 68, 101, 103, 105] or gen2 erating short-term, predefined motions, such as reaching or sitting, given static object [7375, 100, 102]. More recent work [27, 40, 44, 51, 77, 78] has extended this to synthesizing human motions in 3D scenes with multiple objects, while other work has explored human motion generation involving dynamic objects [20, 41, 52, 53, 89 91]. These methods leverage 3D motion-scene paired datasets [26, 45, 60, 68, 96] or 3D human-object interaction datasets [5, 39, 46, 74, 80, 87] to enable such sceneconditioned motion generation. To address the scarcity of large-scale 3D datasets, some approaches [25, 43, 54] leverage the knowledge of 2D generative or vision language models, yet are limited to static human-scene interactions. In this work, we synthesize 3D HOI motions from the 2D scene, utilizing our target-aware video diffusion model. 3. Preliminaries: Video Diffusion Models Building on the success of text-to-image latent diffusion models [7, 18, 67], recent text-to-video (T2V) diffusion models [8, 24, 94] generate videos in latent space. Given video x, an encoder maps it to its latent representation z. During the forward process, Gaussian noise is added to at each timestep t, as zt = αtz + σtϵ, where ϵ (0, I) and αt, σt are noise scheduling coefficients. In the reverse process, the model is trained to predict the noise added to the video latent, guided by input conditions such as text prompt, by minimizing the following objective: LVDM = (cid:2)ϵ ϵθ(zt; y, t)2 2 (cid:3) , (1) where denotes the text prompt. T2V diffusion models can be fine-tuned for image-to-video (I2V) tasks by conditioning the model on an extra input image, allowing the video to start from the given image [24, 50, 88, 94]. In this work, we use CogVideoX [94], one of the SOTA open-sourced video diffusion models based on diffusion transformers [65]. 4. Target-Aware Video Diffusion Models Given an image, segmentation mask of the target, and text prompt describing desired action, our target-aware video diffusion model generates videos where an actor accurately interacts with the specified target. We first extend our base video diffusion model to accept the segmentation mask as an additional input (Sec. 4.1). To make our model utilize the extra mask information, we augment the text prompt by adding sentence, The person interacts with [TGT] object., where the [TGT] token is used to encode the spatial information of the target. We introduce novel cross-attention loss to align the cross-attention maps of the [TGT] token with the mask input and make our model target-aware (Sec. 4.2). This loss is selectively applied to specific transformer blocks and cross-attention regions to maximize its effectiveness Figure 2. Injecting the Extra Mask Condition. We condition the noisy video latent with binary segmentation mask of the target to incorporate the spatial information of the target during generation. (Sec. 4.3). Finally, we curate dedicated dataset tailored to train our target-aware model (Sec. 4.4). 4.1. Specifying the Target with Mask To encode the spatial information of the target, we extend our base I2V diffusion model [94] to incorporate binary segmentation mask of the target. Our base model takes an input image RHW 3 and generates an output video RF HW 3, where denotes the number of frames. To enforce the input image as the first frame of the video, the latent noise zt Rf hwc is concatenated channelwise with the latent encoding of the input image E(I) R1hwc for the first frame, with zero-padding applied for the remaining frames. The concatenated representation is then projected through an image projection layer to align with the text embedding dimension. To integrate the target mask RHW 1 into this process, we downsample it to Rhw1 and concatenate it alongside the input image condition, again applying zeropadding for the other frames. To support the extra mask channel, we extend the original image projection layer by adding an input channel, and initialize the new weights to zero while preserving the pre-trained parameters, following InstructPix2Pix [10]. The overall pipeline is shown in Fig. 2. 4.2. Target Awareness via Cross-Attention Loss We make our model target-aware by introducing novel cross-attention loss that aligns the models attention on the target with the additional input mask during fine-tuning. For every text prompt in our training dataset, we append general sentence The person interacts with [TGT] object., where the [TGT] token is intended to encode the targets spatial information. We then encourage the cross-attention weights between the latent noise corresponding to the first frame of the video and the [TGT] token to align with the provided target mask as demonstrated in Fig. 3. Specifically, we 3 Figure 3. Target-aware video diffusion models. We fine-tune the pretrained image-to-video diffusion model with additional crossattention loss to make the model utilize the additional mask input. Figure 4. Effect of our cross-attention loss. Our cross-attention loss effectively guides the model to focus on the target region. minimize the following loss: Lattn = (cid:2)A(z0 , [TGT]) 2 2 (cid:3) , (2) where A(z0 , [TGT]) denotes the cross-attention weights between the latent noise for the first frame of the video and the [TGT] token. In addition to our cross-attention loss, we employ the standard diffusion objective: Lrec = (cid:2)ϵ ϵθ(zt; y, I, M, t) 2 (cid:3) , (3) where notations remain consistent with those used in Eq. (1). Our overall training objective is defined as: Ltotal = Lrec + λattnLattn, (4) where λattn balances the two loss terms. During inference, we prepend the [TGT] token to words referring to the target, enabling the model to leverage the spatial cue provided by the segmentation mask. As presented in Fig. 4, our crossattention loss effectively guides the [TGT] token to focus on the target region, enabling precise interactions with it. 4.3. Selective Cross-Attention Loss For effective and efficient supervision, we selectively apply our cross-attention loss to the model by identifying (1) the 4 Figure 5. Selective cross-attention loss. We apply our loss on particular transformer blocks that effectively capture semantic information and cross-attention areas that significantly impact the models target awareness. transformer blocks that best capture semantic details and (2) the cross-attention regions that most influence target awareness of the model. Selective Transformer Blocks. Motivated by prior work [30], we observe that certain transformer blocks of the model capture richer semantic details than others, as shown in the bottom row of Fig. 5. Therefore, we apply our cross-attention loss to those blocks whose cross-attention maps closely resemble the segmentation masks of the corresponding token. By generating 100 videos from 100 input images and computing the mean squared error between each blocks cross-attention map and the segmentation mask for predefined token, we evaluate the semantic alignment of each transformer block. We find that blocks 5 through 23 of our base model [94] exhibit the smallest errors and consequently apply our loss to every nth block within this range. Selective Cross-Attention Regions. The multi-modal diffusion transformer architecture, employed by state-of-the-art image and video diffusion models [7, 18, 50] including our base model [94], concatenates text and video embeddings into unified sequence and computes attention over the combined representation. This process yields four distinct attention regions: text-to-text self-attention, text-to-video (T2V) cross-attention, video-to-text (V2T) cross-attention, and video-to-video self-attention. While both T2V and V2T cross-attention maps encode semantic information (Fig. 5 top row), we apply our cross-attention loss on V2T crossattention regions to maximize its impact since V2T attention directly influences the video latent representations. Details on attention mechanisms are provided in the Supp. Mat.. 4.4. Dataset To train our target-aware model, we require videos that satisfy two conditions: (1) the initial frame should depict scene where an actor is present but not yet interacting with the target, and (2) subsequent frames must capture the actor engaging with the target. By utilizing the segmentation mask of the target in the first frame, indicating the object the actor eventually interacts with, our model learns target awareness. Figure 6. Video content creation pipeline. Given images of person and scene, we perform depth-based 3D insertion of the person into the scene and render them together to produce frames for video diffusion input. We interpolate generated initial and final frames to synthesize navigation contents, and utilize our target-aware video diffusion model to synthesize action and manipulation contents. To this end, we present video dataset that captures such interaction scenarios, with segmentation masks for the target in the initial frame and corresponding text prompts. We curate and preprocess set of videos of person interacting with target object in scene, sourced from BEHAVE [5] and Ego-Exo4D [21] datasets. BEHAVE dataset features videos where single person interacts with clearly defined target object in relatively simple setting, whereas Ego-Exo4D contains more complex scenarios such as cooking or bike repairing, where multiple objects, including those of the same type, may be present. In total, we extract 1290 clips that meet our criteria. We obtain the mask for the target object in the initial frame using an off-the-shelf segmentation model [49] and generate text prompts with CogVLM2-Caption [94], the same captioning tool used for training our base model. While it is ideal to prepend [TGT] tokens to the target object nouns during caption generation, we find that current video captioning tools cannot reliably identify the target object in complex scenes. Therefore, we add general sentence, The person interacts with [TGT] object. to the generated captions as described in Sec. 4.2, which we find sufficient to train our target-aware model. 5. Application The core advantage of our target-aware video diffusion model is its ability to produce plausible interaction motions between actors and specified target objects, without requiring additional guidance. We demonstrate its practical value in two downstream applications: (1) video content creation and (2) zero-shot 3D human-object interaction (HOI) motion synthesis for target object. 5.1. Video Content Creation Our target-aware video diffusion model can serve as crucial component in video content creation pipeline, enabling precise control over an actors actions in 2D scene. We introduce simple yet effective pipeline that combines video interpolation technique between keyframes and our targetaware video diffusion model to realize two action types: navigating the scene and interacting with target objects. for details. Navigating The Scene. To enable navigation, we interpolate between two keyframes where each keyframe is constructed by placing the actor at desired location. Instead of using 2D-based approaches such as inpainting, we build depthaware 3D insertion method. Given 2D scene image and 2D human actor image as input, we first uplift each into 3D [4, 9], place the uplifted 3D actor at the specified location within the reconstructed 3D scene, and render them back to 2D. Our approach effectively handles occlusions and preserves identities when inserting humans into the scene. See the Supp. Mat. In practice, users only need to specify the desired 2D locations for placing actors. Once keyframes are generated, we utilize an off-the-shelf video diffusion model for the frame interpolation task [19] to synthesize natural navigation actions. Interacting with Target Object. To generate HOI actions with specified target object, we first position the actor using the technique described above at the desired location, where HOI actions can be initiated. Next, we specify the target object with an off-the-shelf segmentation tool [49] and utilize our target-aware model to synthesize realistic interaction scenes. Note that interpolation-based models cannot replace our model, since obtaining the end keyframe for HOI actions is challenging. By combining these two components, users can generate long-form video contents featuring diverse range of actions with minimal control input. The overall pipeline is presented in Fig. 6. 5.2. Zero-Shot 3D HOI Motion Synthesis Another key advantage of our target-aware model is its ability to leverage the robust generalization of video diffusion models in generating plausible HOI actions in given scene. We demonstrate its effectiveness in zero-shot 3D HOI motion synthesis pipeline. Given an actor in 2D scene and desired target object, our target-aware model forecasts plausible HOI actions, aligned with the given text prompts, thereby providing strong planning cues for robotics control [3, 6, 16, 62]. To verify this connection, we present pipeline by first applying an off-the-shelf 3D human pose estimator [69] on videos generated with our model to obtain 3D human motions. We 5 ing are non-trivial. We therefore compare our approach to three simple baselines. First, we evaluate against our base image-to-video diffusion model, CogVideoX [94]. Next, we assess against version of CogVideoX fine-tuned on our dataset to isolate the impact of our method from that of the additional data (CogVideoX w. data). Finally, we adapt the attention modulation method from Direct-a-video [93], which controls subjects trajectory by amplifying crossattention weights within predefined bounding box regions (Attn. Mod.). In our setting, where trajectory data for actors and targets is unavailable, we prepend the keyword target to the object description in the prompt and amplify crossattention weights in the target object mask region for the new keyword. Intuitively, this modulation mimics our fine-tuning approach without additional training. 6.2. Qualitative Evaluation Target Alignment. Fig. 8 compares our method with baselines in terms of targeting accuracy. In rows 1 and 2, baseline methods occasionally hallucinate the target described in the prompt, rather than incorporating the actual target from the input image. In contrast, our approach generates videos where the actor accurately interacts with the specified target. Comparison with Drag-Based Methods. Fig. 9, provides qualitative comparison with two drag-based editing methods [11, 71], which require additional user inputs. DragDiffusion [71] optimizes spatial positions of image latents based on input drags. Since we find single large drag insufficient to induce significant changes, we instead move the actor gradually toward the target using multiple small drags. While DragDiffusion produces reasonable outputs for minor translations, it fails when larger adjustments are needed. Go-with-the-Flow [11] allows users to control motions by warping the initial random noise sequence to follow desired flow. We generate this warped noise by dragging the actors segmentation mask toward the target. Although this method enables the actor to make contact with the target, the output video lacks plausible motion due to its coarse conditioning. In comparison, our approach yields accurate and realistic interactions using only simple mask of the target. Multiple Objects of the Same Type. Fig. 10 highlights our key advantage where the scene contains multiple target objects of the same type. By enabling the usage of an explicit segmentation mask to identify the target, our method ensures precise selection and manipulation of the intended target. Non-Human Interactions. While our model is fine-tuned on human-scene interactions, it generalizes well to interactions involving non-human subjects, as shown in Fig. 11. Specifying Both the Actor and the Target. Our approach enables simultaneous control over both the source actor and the target object, as demonstrated in Fig. 12. To achieve this, we extend our model to accept two separate segmentation masks as additional inputs and introduce two tokens, [SRC] Figure 7. Zero-shot 3D HOI motion synthesis. We perform imitation learning on 3D poses of person interacting with target in the scene, obtained from videos generated with our model. then apply physics-based imitation learning [80] to train policy that mimics plausible HOI human motions in the Isaac Gym simulator [59]. The results are shown in Fig. 7. 6. Experiments 6.1. Experimental Setup Dataset. We construct benchmark set of 50 images depicting scenes with person, where each image is paired with text prompt describing an interaction between the person and target object. For all pairs, we ensure that the target can be distinctly distinguished with text prompts using noun, color descriptor, or spatial detail (e.g., soda bottle on the table, blue box at the center). Text prompts follow the format The person {action} with {object}. for baselines and The person {action} with [TGT] {object}. for ours. These prompts are further refined using GPT-4o, following the prompt enhancement procedure of CogVideoX [94]. For each image, we generate five videos and compare the results. Metric. We evaluate our approach along two dimensions: target alignment and generation quality. To measure target alignment, we assess whether the generated video captures an accurate interaction between the person and the target object. Specifically, we employ an off-the-shelf contact detector [61] to detect human-object contact in each frame of the generated video and consider the interaction accurate if the detected contact regions overlap the target objects mask in at least one frame. We report the rate of accurate interactions over all generated videos (Contact Score). In addition, we perform two types of user studies to further assess target alignment of each method. For generation quality, we adopt the evaluation metrics from VBench [35] which break down video quality into subject consistency (SS), background consistency (BC), dynamic degree (DD), motion smoothness (MS), aesthetic quality (AQ), and imaging quality (IQ). Baselines. Since our method relies on single target object mask to specify the target, direct comparisons with state-ofthe-art approaches that require extensive temporal condition-"
        },
        {
            "title": "Video Quality",
            "content": "Contact Score Hum. Eval. User Pref. SC BC DD MS AQ IQ Avg. CogVideoX [94] CogVideoX w.data Attn. Mod. Ours 0.592 0.692 0.613 0.896 0.456 0.596 0.508 0.892 28.4% 36.2% 22.2% (100%-above) 0.914 0.915 0.878 0.938 0.903 0.900 0.887 0.914 0.950 0.956 0.827 0.956 0.988 0.990 0.984 0.905 0.482 0.467 0.473 0.471 0.720 0.681 0.707 0. 0.826 0.818 0.793 0.812 Table 1. Quantitative comparison. Our method enables generation of videos containing accurate interactions with the specified targets. We also report generation quality with measures from VBench [35], confirming that our approach does not compromise overall video quality. Figure 8. Qualitative comparison on target alignment. Each set displays generated videos using different methods. While baselines tend to hallucinate the target, our target-aware model produces videos where the actor interacts accurately with the actual target in the scene. Figure 9. Qualitative comparison with drag-based methods. Our approach achieves accurate and realistic interactions with minimal inputs compared to drag-based editing methods [11, 71]. and [TGT]. Each token is encouraged to attend to each mask with our cross-attention loss during fine-tuning. During inference, we prepend each token to the actor and target descriptions, respectively. 6.3. Quantitative Evaluation Target Alignment and Video Quality. As presented in Tab. 1, our method substantially outperforms all baselines in generating accurate interactions with target objects. We also confirm that our method is not detrimental to the video generation quality by reporting scores on different evaluation dimensions in the same table. Attention modulation Figure 10. Multiple objects of the same type. In scenes with several objects of the same type, our method enables actors to accurately interact with the intended target by leveraging its mask. Figure 11. Non-human interactions. Our target-aware model generalizes to non-human cases. approach fails to maintain the temporal consistency of videos since the amplified cross-attention values adversely affect 7 Contact Score Video Quality T2V Cross-Attn. Both Cross-Attn. Ours (V2T Cross-Attn.) 0.784 0.872 0.896 0.817 0.814 0.812 Table 3. Cross-attention loss on selective attention regions. We apply our cross-atention loss to the attention regions that most influence target awareness of the model. Contact Score Video Quality λattn = 0.0 λattn = 0.05 λattn = 0.1 λattn = 1.0 0.688 0.756 0.896 0. 0.820 0.812 0.812 0.804 Table 4. Effects of different cross-attention loss weights. We demonstrate that incorporating our cross-attention loss is crucial for achieving target awareness. cross-attention for better target alignment. This stems from the fact that V2T cross-attention weights directly impact the video latents, whereas T2V weights affect the video latents in subsequent blocks. See Supp. Mat. for details. Cross-Attention Loss Weight. In Tab. 4, we analyze the impact of cross-attention loss coefficient λattn. When λattn = 0.0, meaning the model is trained solely with the reconstruction loss, the target alignment performance is nearly identical to that of the CogVideoX fine-tuned on our dataset (Tab. 1 second row). This clearly demonstrates that incorporating the cross-attention loss is essential for target awareness of the model. As we increase the loss weight, we observe saturation of the contact score and set λattn = 0.1. 7. Discussion We presented target-aware video diffusion model that synthesizes video in which an actor plausibly interacts with specified target. The target is specified by segmentation mask in the first frame and the desired action is described with text prompt, eliminating the need for complex motion guidance. Our core motivation is to generate plausible actortarget interaction motions in zero-shot manner by leveraging pretrained video models. For such task, our target-aware model naturally guides the actor to interact with the target objects. Our experiments demonstrate the strengths and advantages of our model compared to baseline methods and alternative solutions. We further present key downstream tasks of video content creation and zero-shot 3D HOI motion synthesis using our target-aware video diffusion model. Limitations and Future Work. The quality of our generated videos is inherently constrained by existing open-sourced video models, which often produce noticeable visual artifacts when synthesizing complex appearances. Given that closedsourced commercial models [1, 2] yield more convincing results, we expect this limitation to be alleviated as more advanced open-sourced models become available. NevertheFigure 12. Control over multiple entities. Our model, extended to take in two masks, enables specifying both the source actor and the target object. The actor is indicated with red mask and the target is indicated with green mask in the initial frame. Contact Score Video Quality Random Equally-Spaced Ours 0.840 0.839 0.896 0.807 0.806 0.812 Table 2. Cross-attention loss on selective blocks. We apply our cross-attention loss to the blocks that best capture semantics. the self-attention values, resulting in low contact scores. Additional details are provided in the Supp. Mat.. User Study. We conduct two types of user study via CloudResearch Connect to assess our methods effectiveness on target alignment. In the first study (Hum. Eval.), participants are asked to evaluate whether generated video exhibits accurate interactions between the actor and the target, given an input image specified with the target. Fifty participants evaluated 10 videos per method. In the second study (User Pref.), participants are shown an input image along with two videos generated from the image: one produced by our method and the other by baseline method. Fifty subjects are asked 10 questions per baseline to select the video they perceive as exhibiting more accurate interaction with the target. The results are reported in Tab. 1. 6.4. Ablation Studies Cross-Attention Loss on Selective Blocks. We evaluate the impact of applying cross-attention loss on different transformer blocks. For all experiments, we fix the number of blocks receiving the loss per training step to seven. We compare three strategies: (1) random seven blocks at each training step, (2) seven equally spaced blocks, and (3) blocks chosen using our proposed method. As shown in Tab. 2, our approach shows improved target alignments. Cross-Attention Loss on Selective Regions. In Tab. 3, we examine how applying cross-attention loss on various regions of the cross-attention influences performance. The results indicate that the loss should be applied on the V2T 8 less, enhancing video quality by incorporating interaction motion cues [12, 38] could be an interesting future work. Also, due to the static camera setting of our dataset, videos generated by our model tend to exhibit fixed camera trajectories. Given the scarcity of interaction datasets with dynamic cameras, integrating camera control techniques [29, 81, 93] into our model could be possible future direction. Acknowledgements: This work was supported by RLWRLD, NRF grant funded by the Korean government (MSIT) (No. 2022R1A2C2092724), and IITP grant funded by the Korea government (MSIT) [No. RS-202400439854, No. RS-2021-II211343, and No.2022-0-00156]. We thank Jeonghwan Kim for implementing physics-based imitation learning and rendering those results. We also thank Inhee Lee for proofreading the paper. H. Joo is the corresponding author."
        },
        {
            "title": "References",
            "content": "[1] Kling, 2024. https://kling.kuaishou.com/en. 8 https : / / deepmind . google / [2] Veo2, 2024. technologies/veo/veo-2. 8 [3] Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal. Compositional foundation models for hierarchical planning. NeurIPS, 2023. 2, 5 [4] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d human digitization with shape-guided diffusion. In Proc. ACM SIGGRAPH Asia, 2023. 5, 13 [5] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. Behave: Dataset and method for tracking human object interactions. In Proc. CVPR, 2022. 3, 5 [6] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained imageediting diffusion models. Proc. ICLR, 2024. 2, [7] Black"
        },
        {
            "title": "Forest",
            "content": "Labs, 2023. blackforestlabs.ai/. 3, 4, 15 https : / / [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [9] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv, 2024. 5, [10] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proc. CVPR, 2023. 3 [11] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, and Ning Yu. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise, 2025. 2, 6, 7 [12] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. 9 Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 9 [13] Minghao Chen, Iro Laina, and Andrea Vedaldi. Trainingfree layout control with cross-attention guidance. In Proc. WACV, 2024. [14] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. 2 [15] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. Proc. ECCV, 2024. 2 [16] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. NeurIPS, 2023. 2, 5 [17] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proc. CVPR, 2023. 2 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proc. ICML, 2024. 3, 4, 15 [19] Fei, Zhengcong, 2024. https : / / github . com / feizc/CogvideX-Interpolation. 5, 15 [20] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Imos: Intent-driven full-body motion synthesis for human-object interactions. In Proc. Eurographics, 2023. 3 [21] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proc. CVPR, 2024. 5, 15 [22] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. 2, 15, 17 [23] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toimage diffusion models without specific tuning. Proc. ICLR, 2024. [24] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 1, 3 [25] Sookwan Han and Hanbyul Joo. Chorus: Learning canonicalized 3d human-object spatial relations from unbounded synthesized images. In Proc. ICCV, 2023. 3 [26] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael Black. Resolving 3d human pose ambiguities with 3d scene constraints. In Proc. CVPR, 2019. 2, 3 [27] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael Black. Stochastic sceneaware motion prediction. In Proc. ICCV, 2021. 3 [28] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael Black. Populating 3d scenes by learning human-scene interaction. In Proc. CVPR, 2021. 2 [29] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. Proc. ICLR, 2025. 9 [30] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. In Proc. ICLR, 2022. 4, [31] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop, 2022. 13 [32] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 2022. 2 [33] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In Proc. ICLR, 2022. 13 [34] Chun-Hao Huang, Hongwei Yi, Markus Hoschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael Black. Capturing and inferring dense full-body human-scene contact. In Proc. CVPR, 2022. 2 [35] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proc. CVPR, 2024. 6, [36] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via maskeddiffusion. In Proc. CVPR, 2024. 2 [37] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zeroshot grounded video editing using text-to-image diffusion models. Proc. ICLR, 2024. 2 [38] Hyeonho Jeong, Chun-Hao Paul Huang, Jong Chul Ye, Niloy Mitra, and Duygu Ceylan. Track4gen: Teaching video diffusion models to track points improves video generation. Proc. CVPR, 2025. 9 [39] Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan Zhang, Yixin Chen, He Wang, Yixin Zhu, and Siyuan Huang. In Proc. Full-body articulated human-object interaction. ICCV, 2023. 3 [40] Nan Jiang, Zimo He, Zi Wang, Hongjie Li, Yixin Chen, Siyuan Huang, and Yixin Zhu. Autonomous character-scene interaction synthesis from text instruction. In Proc. ACM SIGGRAPH Asia, 2024. 3 [41] Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, and Siyuan Huang. Scaling up dynamic human-scene interaction modeling. In Proc. CVPR, 2024. [42] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. In Proc. CVPR, 2023. 2 [43] Hyeonwoo Kim, Sookwan Han, Patrick Kwon, and Hanbyul Joo. Beyond the contact: Discovering comprehensive affordance for 3d objects from pre-trained 2d diffusion models. In Proc. ECCV, 2024. 3 [44] Hyeonwoo Kim, Sangwon Beak, and Hanbyul Joo. David: Modeling dynamic affordance of 3d objects using pre-trained video diffusion models. arXiv preprint arXiv:2501.08333, 2025. 3 [45] Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, and Hanbyul Joo. Parahome: Parameterizing everyday home activities towards 3d generative modeling of human-object interactions. arXiv preprint arXiv:2401.10232, 2024. 3 [46] Taeksoo Kim, Shunsuke Saito, and Hanbyul Joo. Ncho: Unsupervised learning for neural 3d composition of humans and objects. In Proc. ICCV, 2023. 3 [47] Vladimir Kim, Siddhartha Chaudhuri, Leonidas Guibas, and Thomas Funkhouser. Shape2pose: Human-centric shape analysis. ACM TOG, 2014. [48] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proc. ICCV, 2023. 16 [49] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In Proc. ICCV, 2023. 2, 5, 13 [50] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 3, 4, 15 [51] Jiye Lee and Hanbyul Joo. Locomotion-action-manipulation: Synthesizing human-scene interactions in complex 3d environments. In Proc. CVPR, 2023. 3 [52] Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and Karen Liu. Controllable human-object interaction synthesis. In Proc. ECCV, 2023. 3 [53] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM TOG, 2023. 3 [54] Lei Li and Angela Dai. Genzi: Zero-shot 3d human-scene interaction generation. In Proc. CVPR, 2024. 3 [55] Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, MingHsuan Yang, and Jan Kautz. Putting humans in scene: Learning affordance in 3d indoor environments. In Proc. 10 CVPR, 2019. 2 [56] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. Proc. CVPR, 2024. 2 [57] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. [58] Wan-Duo Kurt Ma, J. P. Lewis, Avisek Lahiri, Thomas Leung, and W. Bastiaan Kleijn. Directed diffusion: Direct control of object placement through attention guidance, 2023. 16 [59] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021. 6 [60] Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin interaction-guided Yumer, and Niloy Mitra. scene mapping from monocular videos. ACM TOG, 2019. 3 [61] Supreeth Narasimhaswamy, Trung Nguyen, and Minh Hoai. Detecting hands and recognizing physical contact in the wild. In NeurIPS, 2020. 6, 14 imapper: [62] Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Jiashun Liu, Yan Zheng, Bin Wang, and Yuzheng Zhuang. Generate subgoal images before act: Unlocking the chain-of-thought reasoning in diffusion model for robot manipulation with multimodal prompts. In Proc. CVPR, 2024. 2, 5 [63] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proc. CVPR, 2023. 2 [64] Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In Proc. ACM SIGGRAPH, 2023. 2 [65] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proc. ICCV, 2023. 3 [66] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proc. CVPR, synthesis with latent diffusion models. pages 1068410695, 2022. 13 [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proc. CVPR, 2022. 3 [68] Manolis Savva, Angel Chang, Pat Hanrahan, Matthew Fisher, and Matthias Nießner. Pigraphs: learning interaction snapshots from observations. ACM TOG, 2016. 2, 3 [69] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In Proc. ACM SIGGRAPH Asia, 2024. 5, 15 [70] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In Proc. ACM SIGGRAPH, 2024. 2 [71] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proc. CVPR, 2024. 2, 6, 7 [72] Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. Instantdrag: Improving interactivity in drag-based image editing. In Proc. ACM SIGGRAPH Asia, 2024. [73] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene interactions. ACM TOG, 2019. 3 [74] Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. Grab: dataset of whole-body human grasping of objects. In Proc. ECCV, 2020. 3 [75] Omid Taheri, Vasileios Choutas, Michael Black, and Dimitrios Tzionas. Goal: Generating 4d whole-body motion for hand-object grasping. In Proc. CVPR, 2022. 3 [76] Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. Drag-a-video: Non-rigid video editing with point-based interaction. arXiv preprint arXiv:2312.02936, 2023. 2 [77] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In Proc. CVPR, 2021. 3 [78] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Sceneaware generative network for human motion synthesis. In Proc. CVPR, 2021. [79] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. 2 [80] Yinhuai Wang, Jing Lin, Ailing Zeng, Zhengyi Luo, Jian Zhang, and Lei Zhang. Physhoi: Physics-based imitation of dynamic human-object interaction. arXiv preprint arXiv:2312.04393, 2023. 3, 6, 13, 15 [81] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In Proc. ACM SIGGRAPH, 2024. 9 [82] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In Proc. CVPR, 2024. 13 [83] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. NeurIPS, 2024. 2, 16 [84] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In Proc. ECCV, 2024. 2 [85] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proc. CVPR, 2024. [86] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proc. ICCV, 2023. 16 11 [103] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael Black, and Siyu Tang. Generating 3d people in scenes without people. In Proc. CVPR, 2020. 2 [104] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. Proc. ICLR, 2024. 2 [105] Kaifeng Zhao, Shaofei Wang, Yan Zhang, Thabo Beeler, and Siyu Tang. Compositional human-scene interaction synthesis with semantic control. In Proc. ECCV, 2022. 2 [87] Xianghui Xie, Bharat Lal Bhatnagar, Jan Eric Lenssen, and Gerard Pons-Moll. Template free reconstruction of humanobject interaction with procedural interaction generation. In Proc. CVPR, 2024. [88] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. Proc. ECCV, 2024. 3 [89] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Interdiff: Generating 3d human-object interactions Gui. with physics-informed diffusion. In Proc. CVPR, 2023. 3 [90] Sirui Xu, Ziyin Wang, Yu-Xiong Wang, and Liang-Yan Gui. Interdreamer: Zero-shot text to 3d dynamic human-object interaction. In NeurIPS, 2024. [91] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liangyan Gui. Intermimic: Towards universal whole-body control for physics-based human-object interactions. In Proc. CVPR, 2025. 3 [92] Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion-conditioned image animation for video editing. arXiv preprint arXiv:2311.18827, 2023. 2 [93] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn ACM directed camera movement and object motion. TOG, 2024. 2, 6, 9, [94] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 3, 4, 5, 6, 7, 13, 15, 16 [95] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 13 [96] Hongwei Yi, Justus Thies, Michael Black, Xue Bin Peng, and Davis Rempe. Generating human interaction motions in scenes with text control. In Proc. ECCV, 2024. 3 [97] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2 [98] Jiyao Zhang, Mingdong Wu, and Hao Dong. Genpose: Generative category-level object pose estimation via diffusion models. NeurIPS, 2023. 13 [99] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proc. ICCV, 2023. 2, 15 [100] Menghe Zhang, Yuting Ye, Takaaki Shiratori, and Taku Komura. Manipnet: neural manipulation synthesis with hand-object spatial representation. ACM TOG, 2021. 3 [101] Siwei Zhang, Yan Zhang, Qianli Ma, Michael Black, and Siyu Tang. Place: Proximity learning of articulation and contact in 3d environments. In Proc. 3DV, 2020. 2 [102] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard Pons-Moll. Couch: Towards controllable human-chair interactions. In Proc. ECCV, 2022. 12 A. Implementation Details A.1. Training Details. We use the CogVideoX-5B-I2V model [94] as our base image-to-video diffusion model, producing output videos at resolution of 720 480 with total of 49 frames. For model fine-tuning, we employ LoRA [33] with rank of 128 and α = 64 to the diffusion transformer and designate the word target as our [TGT] token. We optimize the added LoRA layers and the image projection layer while keeping the other parts of the model frozen and train for 2,000 steps using an AdamW optimizer with learning rate of 1 104, and an effective batch size of 4. We set the cross-attention loss coefficient λattn = 0.1 and apply the loss to the video-to-text (V2T) cross-attention regions of every third transformer block from the 5th block to the 23rd block of the total 42 blocks. We average the attention maps of these selected blocks and regions and normalize them to [0, 1]. The overall training takes approximately 6 hours on 4 NVIDIA A100 GPUs. For inference, we employ DPM sampler [57] with = 50 sampling steps and set the classifier-free guidance scale [31] to 6 with the same dynamic guidance strategy as the original work [94]. The inference for one video approximately takes 4 minutes on single NVIDIA A100 GPU. A.2. Details on Applications. Application 1: Inserting Humans into Scenes. As demonstrated in Fig. 13, we perform human insertion in 3D space rather than in 2D pixel space to handle depth ordering and occlusions between the human and objects in the scene. Given an input human image, we use single-view 3D human reconstruction method [4] to obtain 3D reconstruction of the person. For the input scene image, we first apply segmentation tool [49] to identify and segment the ground. We then use metric-depth estimation method [9] to generate the real-scale 3D pointcloud. From the 3D pointcloud, we extract the points that belong to the ground when projected to the image and perform RANSAC-based plane fitting on these points to derive 3D ground plane. Using the mapping between pixels and the 3D pointcloud, we obtain the 3D coordinates of the pixel to place the human. The reconstructed 3D human is positioned at its 3D point, perpendicular to the ground plane. To manage occlusions between the 3D human and the 3D scene pointcloud, we discard cases where significant overlap occurs and ask the user for alternative input coordinates. Once occlusion handling is complete, we render them together to obtain the human-inserted scene images. Compared to 2D-based solutions using inpainting, where specific region of the scene is masked and the person is inserted via personalized diffusion models [66, 95], our 3D approach better preserves the appearance of the original inputs. As shown in Fig. 14, inpainting-based methods often Figure 13. Inserting humans into scenes. We perform 3D depth-based insertion of the human into the scene by performing single-view 3D reconstruction of the human and estimating the depth of the scene. Figure 14. Comparison with 2D inpainting. Our 3D-based human insertion effectively inserts the human into the scene while preserving both identities and handling occlusion. fail to maintain consistency with the original scene, resulting in undesired removal of objects in the scene. Additionally, inpainting can generate random details for occluded parts of the person in the input image, leading to inconsistencies between frames. Application 2: Physics-Based Imitation Learning. We use the official code of PhysHOI [80] to implement physicsbased imitation learning on 3D human poses extracted from our output videos. Since our goal is to learn policy for human motion, we disable modules related to object motions during training. Joint training of full modules by obtaining paired data of 3D human pose and 3D object pose via off-theshelf object 6D pose estimator [82, 98] could be possible extension. Also, our current imitation learning outputs as shown in Fig. 22 are manually aligned with the 3D scene due to different scales between estimated 3D human pose translations and the scene. Since the 3D location of the initial pose is given through 3D insertion of humans, we may adjust the scale of the subsequent translations leveraging the depth information, which we leave as future work. 13 Figure 15. Evaluation data example. We show subset of our evaluation dataset, where the target is indicated with green mask. We confirm that the target is fully distinguishable with the input text prompt. Figure 16. Contact score based on detection. Green masks in the input image indicate the target and yellow masks in output video frames indicate the detected object contact regions. We consider the interaction with target accurate if the detected object contact region overlaps with the target mask. A.3. Evaluation Details Evaluation Dataset. We construct set of images of scene containing person paired with the prompt depicting the interaction between the person and the target. In the prompt, the person is described to interact with (1) an object placed at different locations of the scene relative to the persons position, or (2) specific object among several different objects in the scene. For all images, we ensure that the target can be precisely determined with text prompts by noun, color description, or spatial description. Some samples of our evaluation dataset are presented in Fig. 15. Contact Score. To detect physical hand contact between the actor and objects, we use the official code of ContactHands [61]. We set the hand detection threshold to 0.5 and the object contact detection threshold to 0.5. We consider the interaction between actor and the target to be successful, when the detected object contact region overlaps with the segmentation mask of the target. Detection results for success and failure cases are presented in Fig. 16, where the yellow masks indicate the detected object contact regions. User Study. We conducted two types of user studies to validate our comparisons. First, we perform human evaluation Figure 17. Human evaluation on target alignment. We ask participants to assess whether the person in the video accurately interacts with the target. Figure 18. User preference study on target alignment. We ask participants to select the video that better demonstrates accurate target alignment. on videos generated by each method to assess whether the depicted person accurately interacts with the target. Fifty participants are presented with 10 videos per method. In each video, we show an input image with the target object highlighted with green mask for 2 seconds, followed by the generated video. screenshot of the human evaluation interface is shown in Fig. 17. We also perform an A/B test to measure user preference of our method over each baseline method in terms of target alignment. Fifty participants are asked 10 questions per baseline method, where each question displays an input image with the target in green mask, alongside the generated videos from our method and base14 Figure 19. Targeting objects in complex scenes. We compare results of original CogVideoX [94] and our target-aware model in complex scenes. Our model successfully generates target-aligned videos even when the target appears small in complex scenes. Best viewed with zoom. line in random order. The screenshot of the user preference study interface is shown in Fig. 18. B. Additional Qualitative Results Targeting Objects in Complex Scenes. In Fig. 19, we present additional results of our target-aware video diffusion model applied to complex scenes where describing the target with text prompts is challenging. We also present the results of the original CogVideoX [94]. Our model successfully generates videos that capture accurate interactions with the target object, even when the target occupies only small portion of complex scene. Note that the scene, sourced from the Ego-Exo4D dataset [21] is unseen during fine-tuning. Providing Motion. The output videos produced with our method, where the actor precisely interacts with the target, can serve as source of motion data for existing controllable video generation approaches [22]. Diffusion as Shader [22] uses 3D tracking video [85] of source clip to condition the motion in generated videos such that they follow the motion of the source. However, acquiring appropriate source videos for complex motions, such as human-object interactions or robot manipulations, is often challenging. In such cases, our method can generate the desired interactions and provide sufficient motion conditions, as demonstrated in Fig. 21. We use Flux [7] with canny-edge ControlNet [99] to generate the initial frames for running Diffusion as Shader. Applications. Fig. 22 is an extended figure of Fig. 6 and Fig. 7 in the main paper, demonstrating the downstream applica15 Figure 20. Attention mechanisms in MM-DiTs. Attentions of MM-DiTs can be divided into text-to-text self-attention, text-tovideo (T2V) cross-attention, video-to-text (V2T) cross-attention, and video-to-video self-attention. Since V2T cross-attention weights directly influence the values of video latents, we apply our cross-attention loss on V2T cross-attention regions. tions of our method. Given images of person and scene, we first synthesize human-inserted images as described in Appendix A.2. As mentioned in the main paper, to achieve human navigation contents, we use frame interpolation video diffusion model [19] to interpolate two synthesized images where the person is inserted in different positions of the scene. For human action or manipulation content, we similarly start from human-inserted image and utilize our target-aware video diffusion model to achieve precise interaction with the target. From the generated contents, we extract the 3D human pose sequences [69] and use them to learn policy via physics-based imitation learning [80] given target motion. C. Discussions Attention Mechanisms in MM-DiTs. State-of-the-art diffusion models [7, 18, 50] including our base model [94], utilize multi-modal diffusion transformers (MM-DiT) [18] for denoising. In MM-DiTs, text and video latents are concatenated into single sequence, and attention computations are performed over the combined representation. Specifically, given query features Q, key features and value features V, each obtained by passing the combined representation through separate linear layers, the attention in transformer block is computed as, Attn(Q, K, V) = Softmax( QKT )V, (5) where is the channel dimension of Q. The resulting Attn(Q, K, V) is normalized, projected through linear layers, and used as the combined representation for the next transformer block. The attention weights are formed by stage. Recently, the technique has been extended to textto-video diffusion models, enabling control over object trajectories in generated videos by modulating attention map weights of every frame [83, 93]. As mentioned in the main paper, we adapt this attention modulation concept as baseline. In our setting, since trajectories for actors and targets are not available, we add the word target to the object description in the prompt and amplify cross-attention weights in target mask regions for the new keyword. This modulation should mirror our approach without additional training. Since attention modulation modifies the internal attention computation during the denoising process, it is highly sensitive to hyperparameters such as the attention control weight λ and the cut-off timestep τ . We report the contact scores of each setting in Tab. 5, evaluated by generating three videos per image from our evaluation dataset. The results consistently show that the scores remain low even compared to the original CogVideoX [94] without any modification. This degradation stems from the attention mechanisms in MM-DiTs: since the attention computation contains row-wise softmax operation, modulating the crossattention values affects the self-attention values of the video, ultimately leading to degraded output video quality and low contact scores, highlighting the necessity of our method for building target-aware models. λ = 10 λ = 25 λ = 50 λ = 100 τ = 0.80T τ = 0.85T τ = 0.90T τ = 0.95T 0.493 0.480 0.533 0.487 0.473 0.507 0.520 0.533 0.527 0.520 0.573 0.613 0.513 0.480 0.520 0.553 Table 5. Contact scores for attention modulation. We report contact scores for different combinations of attention control weights and cut-off timesteps. Softmax( QKT ), where the value at index [i, j] indicates the influence of the i-th token on the j-th token. As illustrated in Fig. 20, this process results in four distinct attention regions: text-to-text self-attention, text-to-video (T2V) cross-attention, video-to-text (V2T) cross-attention, and video-to-video self-attention. As discussed in the main paper, while both T2V and V2T cross-attention maps encode semantic information, we find that applying our loss to the V2T cross-attention is more effective for enhancing target awareness. As demonstrated in Fig. 20, V2T cross-attention weights directly influence the video latents during the dot product computation of the attention weights and value features, whereas T2V crossattention weights primarily affect the text latents. Although the influenced text latents can affect subsequent V2T crossattention weights through QKT computation, their impact is diminished, as shown in our main paper. Attention Modulation. Prior work on controllable textto-image generation [13, 30, 48, 58, 86] demonstrates that by modifying cross-attention maps during inference, it is possible to control the placement of subjects in specific regions of the output image. Cross-attention modulation is applied as follows: CrossAttnMod(Q, K, V) = Softmax( QKT + λS )V, (6) where λ denotes attention control weight, is the modulation term with the same dimensions as the attention maps. The modulation term takes positive values within the desired region for the subject and negative values outside that region. Formally, given bounding box that specifies the desired region for the object, is defined as follows: S[i, j] = QKT , 1 0, , if B, P, τ if B, P, < τ otherwise (7) where is the size of the bounding box, QKT is the number of elements in QKT , represents the indices of prompt tokens for subjects, and τ is cut-off timestep. Since diffusion models form the subject layout in earlier steps [30, 86], the amplification is only applied in the early 16 Figure 21. Acting as source video. Our outputs can provide sufficient motion data for existing controllable video generation methods [22] that require dense structural conditions over frames. Figure 22. Applications. Given images of person and scene, we perform 3D insertion of the person into the scene and render them together to produce frames for video diffusion input. We interpolate synthesized initial and final frames to generate locomotion contents and utilize our Target-Specified video diffusion model to generate action and manipulation contents. We further demonstrate that extracted 3D human poses from our generated contents can be used as training data for physics-based imitation learning."
        }
    ],
    "affiliations": [
        "RLWRLD",
        "Seoul National University"
    ]
}