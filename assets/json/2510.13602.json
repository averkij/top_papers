{
    "paper_title": "NOSA: Native and Offloadable Sparse Attention",
    "authors": [
        "Yuxiang Huang",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave a crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, a trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and query-agnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to a 2.3x improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2)."
        },
        {
            "title": "Start",
            "content": "NOSA: Native and Offloadable Sparse Attention Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu Department of Computer Science and Technology, Tsinghua University huang-yx21@mails.tsinghua.edu.cn, {xcj,han-xu,liuzy}@tsinghua.edu.cn 5 2 0 2 5 1 ] . [ 1 2 0 6 3 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Trainable sparse attention has emerged as promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and queryagnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to 2.3 improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2)."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advancements of large language models (LLMs) (OpenAI, 2025; Anthropic, 2025; Deepseek-AI, 2025) in processing long contexts have transformed the landscape of artificial intelligence, enabling LLM-based agent systems (Qin et al., 2024; Luo et al., 2025) and empowering them to handle complex reasoning tasks (DeepSeek-Team, 2025; Guo et al., 2025). As the application of long-context LLMs evolves from merely encoding long inputs to also decoding long outputs (Bai et al., 2024), the decoding process, whose efficiency is heavily constrained by memory I/O, emerges as the primary bottleneck of LLM inference. As the input sequence length increases, the memory-bound bottleneck limits the efficiency of LLM decoding due to the O(n)-level growth of memory access, thereby slowing down inference and reducing decoding throughput. Consequently, improving decoding throughput remains significant challenge in long-context scenarios. To achieve higher inference efficiency, trainable sparse attention methods (Yuan et al., 2025; Lu et al., 2025; Zhao et al., 2025) offer promising direction, as they reduce both computation and memory access through dynamically learned attention sparsity. Since these methods are learned jointly with the entire process of language modeling, they can maintain nearly lossless task performance compared with the full attention mechanism, and outperform other attention-based optimization techniques such as training-free sparse attention (Li et al., 2025; Xu et al., 2025) and KV cache optimization (Zhang et al., 2023; Li et al., 2024). However, trainable sparse attention can only mitigate the memory-bound bottleneck, but cannot reduce the size of the KV cache. Consequently, since the KV cache needs to be entirely stored, trainable sparse attention provides no improvement for the maximum achievable batch size. This limitation becomes more pronounced for processing long contexts, as larger sizes of single-sequence KV cache lead to smaller inference"
        },
        {
            "title": "Working in Progress",
            "content": "batch sizes and lower compute utilization, thereby lowering the decoding throughput for applications such as cloud-side LLM service (Kwon et al., 2023; Zheng et al., 2024) and sample generation for LLM reinforcement learning (Sheng et al., 2024; Fu et al., 2025). Fortunately, as observed in Section 2.2, trainable sparse attention naturally exhibits the inherent locality in token (or block) selection across consecutive decoding steps. Intuitively, the set of tokens attended to by the current decoding step largely overlaps with those attended to by the previous step. This inherent locality enables seamless integration of KV cache offloading with attention, allowing for memory optimization without modifying the underlying attention computation. In contrast, traditional methods that adopt offloading-based KV cache optimization (Tang et al., 2024; Xiao et al., 2024a; Sun et al., 2024) rely on manually-crafted training-free block-sparse patterns to improve inference throughput, which often leads to performance degradation due to the mismatch between training and inference sparsity patterns, since such patterns are not inherently produced during model training. However, relying solely on inherent locality is insufficient, as trainable sparse attention provides no explicit guarantee of maintaining locality. In low-locality inference scenarios, frequent GPUCPU data transfers still occur, resulting in degraded throughput. To overcome this limitation, promising approach is to design offloading-aware trainable sparse attention, in which locality constraints are explicitly incorporated during training LLMs to yield consistently offloadable sparse attention pattern. Based on this perspective, we propose NOSA, trainable sparse attention whose KV cache is natively offloadable. We summarize our contributions as follows. Contribution 1: We present detailed analysis of the locality property in trainable sparse attention and provide theoretical analysis of decoding throughput when offloading is applied. Building upon these insights, we build NOSA based on typical trainable sparse attention InfLLM-V2 (Zhao et al., 2025), introducing an explicit locality constraint by decomposing the block selection into query-aware and queryagnostic components for higher decoding throughput. Contribution 2: We train models integrated with NOSA and conduct comprehensive empirical evaluations across both short and long context benchmarks. Experimental results demonstrate that NOSA achieves nearly lossless task performance compared with the original trainable sparse attention (InfLLM-V2). Contribution 3: We implement an inference system for NOSA, featuring carefully designed communication operators and GPUCPU memory manager optimized for offloading efficiency. We further evaluate the decoding throughput of NOSA under various input lengths and batch sizes, achieving approximately 2 decoding throughput over non-offloading baselines under large batch sizes and 13.6% throughput improvement compared with applying vanilla offloading to InfLLM-V2."
        },
        {
            "title": "2 Background",
            "content": "In this section, we first introduce the preliminaries and notations in Section 2.1, followed by two key observations presented in Section 2.2. 2.1 Preliminaries 2.1.1 Trainable Sparse Attention Transformer-based LLMs (Vaswani et al., 2017) employ attention modules to capture cross-token relationships and long-range dependencies. The attention mechanism takes Rnd as input, where and denote the input sequence length and model dimension, respectively. The query, key, and value matrices are then obtained through linear projections: The attention computation is then formulated as below, where {0, }nn is the attention mask. = HWQ, = HWK, = HWV . = + QK, = Softmax(P), = AV. 2 (1) (2)"
        },
        {
            "title": "Working in Progress",
            "content": "Trainable sparse attention methods, such as NSA (Yuan et al., 2025), MoBA (Lu et al., 2025), and InfLLMV2 (Zhao et al., 2025), commonly adopt block-sparse attention pattern. Since we build NOSA based on InfLLM-V2, we briefly review its notations here. Before performing the attention computation, the key matrix is first partitioned into blocks of size nb, and each block is compressed into single representation using compression function fc : Rnbd R1d. This process can be formalized as follows:"
        },
        {
            "title": "The selection score Sq",
            "content": "c = (sq ij)n nb Kc = fc(K) nb d. (3) between each token and all blocks is then computed as: (4) Since InfLLM-V2 also incorporates attention sinks and sliding windows, we denote their lengths as ns and nw, respectively. Then, the attention mask = (mij)nn is computed as: = QK Sq Rn nb . mij = 0, (i j) (j < ns) (i < nw) (cid:18) (cid:16) , otherwise. ij Topk(sq sq i,i/nb) (cid:17) (cid:19) ; (5) Finally, we compute the attention mechanism following Equation 2. 2.1.2 Selecting KV Cache for Sparse Attention KV cache selection is widely used in sparse attention mechanisms and KV cache optimizations, where limited number of KVs are accessed and computed. Mainstream approaches to KV cache selection can be broadly categorized into two types: query-aware selection (Yuan et al., 2025; Zhao et al., 2025; Li et al., 2024) and query-agnostic eviction (Kim et al., 2024; Huang et al., 2024; Yao et al., 2024). Query-aware selection. The attention output of the i-th token can be approximated by removing the value vectors vj whose attention weights aij from the i-th query are relatively small. To perform this selection, we first compute the selection score Sq = QK, and then choose the top-k positions, where mij = 0 if sq ij Topk(sq i,i). Note that when applied to block-sparse patterns, we compute the dot product from the query tokens to the compressed key representations, i.e., Sq = QK . Query-agnostic eviction. Evicting unimportant KVs without awareness of specific query token is common technique in KV cache eviction methods, particularly in scenarios where query tokens are not immediately available. Therefore, scoring functions based on the hidden state at position are used to estimate the importance of (ki, vi) for processing subsequent context. To perform this selection, we first = fe(hi) R, and then compute the importance score for each KV cache unit using scoring function se select the top-k positions, where mij = 0 if se i). Note that the importance score is independent of the current query position i, making this selection query-agnostic. Topk(se Query-agnostic selection yields fixed eviction pattern, i.e., once token is evicted, it will not be reused. Such feature maintains strong locality and enables zero communication when offloaded. However, it fails to capture long-range dependencies. In contrast, query-aware selection excels in retrieval tasks but lacks locality constraints. NOSA combines both to establish lower bound on selection locality, making offloading feasible. 2.2 Observations One crucial reason why traditional offloading systems achieve higher decoding efficiency is that memory access is reduced through post-training sparsity, despite communication over slower interconnects. Such post-training sparsity can hardly be applied when the models attention is natively sparse, as the selected KV blocks are already limited and further sparsification would harm task performance. Therefore, if all selected KV blocks are fetched from GPUs, simply communicating these blocks over PCIe (30 GB/s) instead of highbandwidth memory (HBM, 2 TB/s on NVIDIA A100-80GB) would result in non-negligible bottleneck. Notably, we observe strong locality between consecutive decoding steps in token (or block) selection of trainable sparse attention mechanisms."
        },
        {
            "title": "Working in Progress",
            "content": "(a) Memory Hierarchy (b) Attention Time Ratio (c) Theoreotical Throughput Figure 1: The memory hierarchy of NVIDIA A100-40GB, the ratio of attention mechanism among the complete inference time, and the theoretical decoding throughput with respect to various cache hit rates. represents the batch size. Observation 1: Locality in Query-aware Token Selection Trainable sparse attention naturally exhibits inherent locality in query-aware token (or block) selection across consecutive decoding steps. Given token position t, we define the set of selected tokens as Γ(t) = {i : mij = 0}, (6) where = (mij)nn denotes the attention mask. The locality (overlapping rate) between adjacent decoding steps is defined as γ(t) = Γ(t) Γ(t 1) Γ(t) . (7) We measure the overlapping rate for each transformer layer using an input sequence of 16K tokens, among which 4K tokens are selected (sliding window size nw = 1024, attention sink size ns = 64). Figure 2 shows that, for most layers, the sets of selected tokens at steps and t1 overlap by at least 80% (i.e., γ(t) 0.8). This strong locality implies that fewer than 20% of the KV entries change per step and thus require communication, making KV cache offloading possible. Notably, this locality emerges naturally from the trained sparse attention without any explicit constraints. Figure 2: The overlapping rate γ(t) of InfLLM-V2. Observation 2: PCIe Communication Bound Query-aware offloading inference remains bounded by the communication latency of the PCIe bus. Although Observation 1 shows that the high overlapping rate of selected tokens between adjacent decoding steps enables the potential for offloading by reusing previously selected tokens as cache, we find that an overlap of approximately 80% is still insufficient. As illustrated in Figure 1, more than 80% of the total decoding time is spent within the attention mechanism, suggesting that throughput remains constrained by PCIe communication. To further analyze this, we simulate the theoretical inference time and decoding throughput under varying cache hit rates (i.e., overlapping ratios), based on the efficiency estimation model introduced in Yuan et al. (2024). The results in Figure 1 indicate that increasing the cache hit rate substantially reduces the attention time ratio, thereby improving the theoretical decoding throughput. Building on Observations 1 and 2, we contend that enforcing constraints to promote higher cache hit rate during training, i.e. by developing native and offloadable sparse attention, is the key to achieving superior decoding throughput."
        },
        {
            "title": "Working in Progress",
            "content": "Figure 3: The framework of NOSA. By introducing locality constraint, NOSA selects larger proportion of GPU-resident blocks compared with vanilla offloading, thereby reducing PCIe communication volume. For simplicity, we omit the selected blocks from the attention sink and sliding window attention."
        },
        {
            "title": "3 NOSA",
            "content": "In this section, we first describe the algorithmic design of NOSA, followed by detailed ablation studies and corresponding analyses. The overall framework of NOSA is illustrated in Figure 3. 3.1 Offloading-aware Trainable Sparse Attention Design 3.1.1 Adding Locality Constraint Aiming to maintain desired cache hit rate (overlapping rate) γ(t) across all token positions t, we impose lower bound γ0 (0, 1) on the overlapping rate of selected tokens between adjacent decoding steps, i.e., we design an algorithm such that {2, , n}, γ(t) γ0. (8) To achieve this goal, we divide the selected tokens Γ(t) into two subsets: the query-aware selection Γq(t) and the query-agnostic selection Γe(t). For query-aware selection Γq(t), we do not add any constraints to preserve the models performance. As described in Section 2.1.2, such selection imposes an eviction constraint, i.e., if token is not selected at decoding step t, it must not be selected at any subsequent step > t. This constraint can be formulated as follows. t1, t2 {1, , 1}, t1 < t2, then Γe(t2) Γe(t1) {t1 + 1, t1 + 2, , t2}. (9) R. We implement the query-agnostic selection by assigning each KV pair an importance score se When selecting query-agnostic tokens, Top-k function is applied to these scores, defined as Γe(t) = ArgTopk({se i=0). To further enhance performance, we adopt the design of DMA (Shi et al., 2025), where the importance score is computed as }n = τ (vjW1)W2, W1 Rdheadnhead , W2 Rnhead1 se (10) where τ is the non-linear function, W1, W2 are trainable parameters, and dhead, nhead are the attention head dimension and number of attention heads, correspondingly. We remove the final exp from the original"
        },
        {
            "title": "Working in Progress",
            "content": "design to achieve better performance. For the query-aware selection, we adopt sq score to preserve the models ability to retrieve previously evicted tokens. tj = qtk as the selection At each decoding step, total number of is selected. We assign two budgets kq, ke for query-aware and query-agnostic selection, such that = kq + ke. We first select the query-aware tokens by sq tj. We then set these places to infinity and select the query-agnostic tokens by se j. Such process can be followed by taking Top-k selection on the following score: stj = ( , se j, Topkq ({sq sq otherwise. }t j=1); , Γ(t) = ArgTopk({sj}t j=1). (11) This design inherently ensures locality, as stated in Theorem 1. The proof is provided in Appendix A. Theorem 1. If the above selection process has budget = kq + ke, we have {2, , n}, γ(t) ke ."
        },
        {
            "title": "3.2 Attention Calculation",
            "content": "At the decoding step for generating the (t+1)-th token, the hidden input is ht R1d, with its corresponding query, key, and value vectors qt, kt, vt R1d, and the KV cache K, Rtd. We first compute the importance scores Se = (se j)t according to Equation 10. Next, we compress these scores in the same manner as when generating block representations and compute the query-aware selection scores, formalized as follows. Kc = fc(K) nb d, Se = fc(Se) nb , Sq = qtK nb . (12) We then utilize the selection process defined in Equation 11 to generate the selected positions Γ(t) and the corresponding attention mask M, where mj = 0 if Γ(t) and mj = otherwise. To make this selection differentiable and trainable, we introduce an attention bias vector Rt similar to Shi et al. (2025), where each element is assigned as bj = se c,j/nb. Finally, the attention output at Rd is computed as aj = mj + exp(bj) exp(qtk )vj (cid:0)ml + exp(bl) exp(qtk )(cid:1) . (13) We observe that the selection process based on Se is highly sensitive to numerical precision. Therefore, we omit the exponential (exp) operation before token selection; that is, we select the top-k indices based on se c,j/nb rather than on exp(bj). Since this operation is deferred to the attention computation, we refer to this optimization as ED-DMA (Exp-Delayed DMA). As shown in the ablation studies in Section 3.3, EDDMA yields the most stable and effective results, whereas alternative approaches lead to noticeably greater performance degradation. 3.3 Ablation Studies and Analysis Here we provide brief discussion on the detailed designs and sparse features of NOSA, to show that the design of NOSA is effective and efficient. 3.3.1 Eviction Head We begin by ablating the implementation of the eviction head to evaluate the effectiveness of NOSAs design. The eviction head assigns an importance score se to each token at position t, which serves as the metric for selecting query-agnostic tokens in NOSA. There are several possible approaches to implement this mechanism. straightforward approach is to learn the importance score through lightweight MLP that takes the hidden state of each token as input, similar to the retaining head proposed in Locret (Huang et al., 2024). Alternatively, DMA (Shi et al., 2025) employs single-layer gated projection based on vt, the value vector at the t-th token, to estimate importance. In NOSA, we perform query-agnostic selection using the pre-exponential value from DMA, motivated by our observation that this step is highly sensitive"
        },
        {
            "title": "Working in Progress",
            "content": "to numerical precision. This variant is referred to as Exp-Delayed DMA (ED-DMA). For comparison, we also include variant where no explicit attention bias is applied during forward passes, and only its gradient contribution is preserved during backpropagation. This simplified version is termed Simple DMA (S-DMA). We list the bias calculation before query-agnostic selection and the attention calculation in Table 1. Abbreviation Full Name Bias Calculation Attention Calculation Retaining Retaining Head = σ(hjW1)W2 DMA Dynamic Mask Attention = exp(τ (vjW1) W2) ED-DMA Exp-Delayed DMA = τ (vjW1) W2 S-DMA Simple-DMA = τ (vjW1) W2 aij = aij = aij = +bl)) +bj )vj mij +exp(qik l(mil+exp(qik mij +bj exp(qik )vj l(mil+bl exp(qik )) mij +exp(bj ) exp(qik )vj l(mil+exp(bl) exp(qik aij = )) mij +exp(bj bj .detach()) exp(qik )vj l(mil+exp(blbl.detach()) exp(qik P )) Table 1: Details of various implementations of the eviction head. Eviction Head SG1 SG2 SG3 MK1 MK2 MK3 MV MQ VT CWE FWE QA1 QA2 Avg. InfLLM-V2 100.0 100.0 100. 84.0 50.0 18.0 92.5 84.5 26. Retaining DMA S-DMA ED-DMA 100.0 100.0 100.0 100.0 100.0 100.0 96.0 98.0 84.0 98.0 98.0 98.0 68.0 66.0 58.0 70.0 44.0 42.0 44.0 42. 4.0 8.0 6.0 14.0 76.5 73.5 93.0 90.5 80.5 75.0 80.0 82.5 24.0 4.4 19.6 35.6 0.8 2.6 2.0 2.6 2. 62.7 36.0 30.0 60.3 66.0 74.7 60.7 69.3 40.0 34.0 30.0 36. 36.0 34.0 34.0 36.0 55.8 54.7 55.5 59.6 Table 2: RULER evaluation results with various implementations of the eviction head. We first pretrain 1B-parameter model with an input sequence length of 8K under dense attention, and then perform long-context continuous pretraining with NOSA using different eviction head implementations, where the sequence length is extended to 16K. We set the total selection budget to 4096, with 64 attention sink tokens, 1024 sliding window tokens, and kq = 1024. The resulting models are evaluated on RULER (Hsieh et al., 2024). As shown in Table 2, ED-DMA achieves the best performance, remaining nearly lossless compared to the original InfLLM-V2. Other implementations exhibit noticeable performance degradation, particularly on the MV, MQ, and VT tasks. Notably, merely shifting the exponential operator from the bias calculation to the attention computation yields 5% improvement in accuracy. Based on these results, we adopt ED-DMA as the eviction head implementation for NOSA. 3.3.2 Cache Hit Rate In the training setup described in Section 3.3.1, the lower bound of the cache hit rate γ0 is given by (k kq)/k = 0.75, which is lower than the inherent cache hit rate observed in InfLLM-V2 (see Observation 1). natural question arises: can explicitly enforcing this constraint further improve the locality of token selection? Figure 4 presents the measured locality γ(t) of NOSA. The results indicate that introducing NOSA leads to higher locality compared with the original InfLLM-V2, yielding an approximate 5.5% improvement (equivalent to nearly 2 reduction in cache miss rate). These results demonstrate the effectiveness of incorporating locality constraint to enhance the cache hit rate."
        },
        {
            "title": "4 Inference System Implmentation",
            "content": "Figure 4: Comparing locality γ(t) between NOSA and InfLLM-V2. We design carefully optimized offloading inference system to fully exploit the potential of NOSA in improving decoding throughput. In this section, we present the key system-level techniques incorporated into the framework, followed by detailed ablation studies that demonstrate the effectiveness of each component."
        },
        {
            "title": "4.1 Memory Layout and Communication Design",
            "content": "Since trainable sparse attention mechanisms typically operate in block-sparse pattern, we adopt PagedAttention (Kwon et al., 2023) as the foundation for our attention kernel implementation. Accordingly, KV blocks are managed in paged manner on both GPU and CPU, with block-to-block communication occurring during decoding. However, because most trainable sparse attention mechanisms activate distinct block sets across different attention heads, the resulting contiguous block size is smaller than that in vanilla PagedAttention, leading to increased overhead in both memory management and communication. Figure 5: Effective throughput of communication kernels. To address this issue, inspired by Zhou et al. (2025), we adopt memory layout of (Nnum, H, nb, dhead), where Nnum denotes the total number of available blocks on the GPU or CPU, is the number of attention heads, nb is the block size in sparse attention, and dhead is the head dimension. This layout enables contiguous memory access when transferring block from CPU to GPU. However, the fragmented distribution of small blocks can still cause bandwidth inefficiency during PCIe communication, significantly slowing down KV transfers. To mitigate this, we implement custom Triton kernels leveraging unified virtual address (UVA) supported by modern GPUs, allowing parallelized CPUto-GPU block transfers for higher throughput.1 As shown in Figure 5, both directions of communication achieve bandwidths exceeding 20 GB/s (out of 31.5GB/s for PCIe bandwidth) when the number of blocks communicated per head is greater than 128. This condition is easy to satisfy in practice; for instance, given batch size of = 32, only 16 blocks need to be communicated. 4.2 Efficient Memory Manager Since trainable sparse attention selects different blocks for different attention heads and each block is relatively small, the block tables, i.e., the mappings from logical positions to physical memory locations, can become extremely large. Managing these block tables efficiently is crucial, as they can easily turn into the bottleneck of the decoding process. To address this, we design memory manager that maintains two mapping tables, linking logical block positions (b, h, i) (the i-th block of batch and head h) to their corresponding physical block indices (h, j) (the j-th physical block of head h) on both GPU and CPU. The memory manager is implemented in C++ for efficiency, and both block tables are maintained entirely on the CPU. As illustrated in Figure 6, this implementation achieves remarkable efficiency, delivering over 35 speedup compared with Python-based implementation."
        },
        {
            "title": "5 Experiments",
            "content": "Figure 6: Wall time per KV cache update step under the configuration = 64, = 2, = 8192, = 128, and nb = 64. To comprehensively demonstrate the effectiveness and efficiency of NOSA, we conduct extensive benchmarks in this section, focusing on two main aspects: (1) the task performance of models trained with NOSA, and (2) the end-to-end decoding efficiency. 1These techniques are also employed in SparseServe (Zhou et al., 2025). However, since its implementation has not been publicly released, we reimplemented the kernel ourselves in approximately 100 lines of Triton code."
        },
        {
            "title": "Working in Progress",
            "content": "Method MMLU MMLU Pro BBH GSM8K MATH DROP MBPP HumanEval Avg. Infllm-V2 NOSA 48.53 48.36 24.22 24.22 38.26 37. 46.85 46.32 11.70 11.60 5.23 5.02 40.00 40.60 12.80 12.20 28.45 28. Table 3: Benchmark results of short-context evaluation. Method gov_report triviaqa narrativeqa qmsum musique 2wikimqa multifieldqa_en repobench-p Infllm-V2 NOSA 28.73 27.90 73.33 71.23 14.95 13. 23.11 22.84 8.69 9.51 19.80 17.74 42.73 43.76 55.51 55.44 Method qasper hotpotqa multi_news trec passage_retrieval_en passage_count samsum lcc Avg. Infllm-V2 NOSA 29.06 29.73 18.58 19.74 25.66 24. 67.50 67.50 9.50 12.50 2.00 1.50 5.23 5.41 56.02 55.88 30.03 29. Table 4: LongBench evaluation results. Method SG1 SG2 SG3 MK1 MK2 MK3 MV MQ VT CWE FWE QA1 QA Avg. Infllm-V2 NOSA 100.0 100.0 100.0 96.0 100.0 100.0 86.0 80. 38.0 42.0 20.0 16.0 90.5 87.0 93.5 88.5 26.8 20.8 1.0 4. 59.33 66.67 36.0 34.0 30.0 32.0 60.09 59.00 Table 5: RULER evaluation results. 5.1 Task Performances 5.1.1 Setup Models. We pretrain 1B-parameter model with NOSA to evaluate its task performance. During pretraining, the model is first trained with full attention using an input sequence length of 8K. We then perform long-context continuous pretraining phase with 16K input sequence length, where NOSA is applied. In this phase, we configure total of = 4096 selected tokens (including 64 attention sink tokens and 1024-token sliding window), consisting of kq = 1024 query-aware tokens and ke = 3072 query-agnostic tokens. Finally, we conduct supervised fine-tuning (SFT) stage to further refine the model. Detailed model architecture and training configurations are provided in Appendix B. Benchmarks. We benchmark the trained model on both short-context and long-context evaluations. For short-context evaluation, we test the model on MMLU (Hendrycks et al., 2020), MMLU Pro (Wang et al., 2024), BBH (Suzgun et al., 2022), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), DROP (Dua et al., 2019), MBPP (Austin et al., 2021), and HumanEval (Chen et al., 2021). For long-context evaluation, we the the model on LongBench (Bai et al., 2024) and RULER (Hsieh et al., 2024). Baselines. Since NOSA is built upon InfLLM-V2 (Zhao et al., 2025), we compare its task performance with InfLLM-V2 to examine the performance degradation introduced by the added locality constraint. 5.1.2 Discussion The benchmark results are summarized in Table 3, Table 4, and Table5. As shown in Table 3, NOSA achieves nearly lossless performance compared with InfLLM-V2. Table 4 further demonstrates that NOSA shows no significant performance degradation on long-context benchmarks, and only marginal (1%) accuracy drop is observed on RULER  (Table 5)  . These results indicate that introducing locality constraint into trainable sparse attention does not lead to noticeable performance degradation on either shortor long-context tasks, highlighting NOSAs stability and robustness across downstream evaluations."
        },
        {
            "title": "Working in Progress",
            "content": "Method Off. SeqLen=8K SeqLen=12K M=8.75GB M=13.13GB M=17.50GB M=13.13GB M=17.07GB M=21.00GB InfLLM-V2 InfLLM-V2 NOSA 374.28 (20) 457.65 (40) 494.11 (40) 533.23 (30) 582.18 (60) 609.81 (60) SeqLen=14K 683.15 (40) 668.44 (80) 738.36 (80) 267.78 (13) 429.02 (40) 469.85 (40) 331.76 (17) 486.16 (52) 552.78 (52) SeqLen=16K 396.16 (21) 537.10 (64) 626.09 (64) Method Off. InfLLM-V2 InfLLM-V2 NOSA M=15.32GB M=16.85GB M=18.38GB M=17.50GB M=19.25GB M=21.00GB 222.89 (11) 415.86 (40) 455.59 (40) 239.47 (12) 445.13 (44) 485.78 (44) 257.04 (13) 459.79 (48) 505.67 (48) 202.79 (10) 418.63 (40) 446.19 (40) 218.66 (11) 441.95 (44) 485.38 (44) 240.77 (12) 440.66 (48) 500.76 (48) Table 6: Decoding throughput (tok/s) under different sequence lengths and memory constraints. SeqLen denotes the sequence length, represents the memory constraint for the KV cache, and Off. represents whether offloading is used. The numbers in parentheses indicate the batch sizes, where each batch size is maximized under the given memory constraint. We report the throughput of 1B model on the PG19 dataset, averaging results over 20 runs for each setting. 5.2 Efficiency Benchmarks 5.2.1 Setup We evaluate decoding efficiency by measuring the decoding throughput of the trained 1B-parameter model under various memory constraints and input sequence lengths. For each memory constraint, we maximize the batch size to fully utilize available GPU resources. We compare NOSA against two baselines: InfLLM-V2 without offloading (where all KV caches are stored on the GPU) and InfLLM-V2 with vanilla offloading (without applying locality constraints). Experiments are conducted on single NVIDIA A800-80GB GPU paired with an Intel(R) Xeon(R) Platinum 8470 CPU (52 cores), connected via 4th-generation PCIe. For each configuration, we select 20 samples from the PG19 dataset (Rae et al., 2019) and decode 4 tokens per sample under the specified input lengths. The benchmark results are summarized in Table 6. 5.2.2 Discussion The throughput results reported in Table 6 show that NOSA achieves the highest decoding efficiency across all experimental settings. Due to the inherent locality of trainable sparse attention, applying vanilla offloading to InfLLM-V2 already yields substantial improvement in decoding speed. However, NOSA further enhances throughput by incorporating explicit locality constraints during training, enabling even faster decoding than InfLLM-V2. We also observe that the efficiency gain becomes more pronounced with larger batch sizes and longer input sequences, where NOSA achieves up to 2.3 throughput improvement over vanilla InfLLM-V2."
        },
        {
            "title": "6 Related Works",
            "content": "To accommodate the growing trend of extending the input context length of LLMs, numerous attentionfocused optimizations have been proposed. In this section, we briefly review two primary directions for improving LLM inference efficiency: sparse attention mechanisms and offloading systems. 6.1 Sparse Attention Mechanisms Sparse and approximate attention mechanisms have been proposed to alleviate the O(n2) computational burden and O(n) memory access, thereby accelerating LLM inference. Early approaches, including StreamingLLM (Xiao et al., 2023) and LM-Infinite (Han et al., 2023), employ simple and fixed sparsity patterns, such as attention sinks and sliding windows, to reduce attention complexity. More advanced methods (Ge et al., 2023; Jiang et al., 2024; Xu et al., 2025; Xiao et al., 2024b; Zhang et al., 2025a;d) introduce richer sparsity structures for improved task performance, often"
        },
        {
            "title": "Working in Progress",
            "content": "combined with other optimizations like low-bit quantization (Zhang et al., 2025b;c; Yang et al., 2024). Among these methods, query-aware sparsification (selecting only subset of query-related KV pairs) is widely adopted strategy (Li et al., 2024; Zhang et al., 2023; Liu et al., 2023). In contrast, query-agnostic selection methods (Huang et al., 2024; Kim et al., 2024; Yao et al., 2024) focus on selecting essential KV pairs without queries, but suffer from performance degradation when directly applied to large-scale LLMs. More recent research demonstrates that integrating attention sparsity directly into model training can yield nearly lossless performance while maintaining substantial sparsity and speedup. NSA (Yuan et al., 2025), SeerAttention (Gao et al., 2024), and MoBA (Lu et al., 2025) introduce trainable block-sparse patterns tailored for long-context scenarios, though these approaches tend to slow down short-context inference. InfLLM-V2 (Zhao et al., 2025) addresses this limitation by unifying multiple sparsity patterns within single attention kernel, enabling dynamic adaptation between short and long contexts. FSA (Yan et al., 2025) further improves kernel efficiency for smaller query-head number, while DSA (DeepSeek-AI, 2025) introduces element-wise trainable sparsity suitable for ultra-large LLMs. DMA (Shi et al., 2025) extends this line of work by incorporating eviction-based sparse selection. Beyond natural language processing, trainable sparse attention has also been successfully applied to long-video generation (Zhang et al., 2025e; Zhan et al., 2025), signal processing (Wang et al., 2025), and computational biology (Yoshai et al., 2025). 6.2 Offloading Systems for LLMs Offloading is widely adopted technique for LLM inference in memory-constrained environments or when supporting extra-large batch sizes. Early approaches (Sheng et al., 2023; Song et al., 2024) primarily focus on parameter offloading, enabling the inference of larger LLMs on consumer-grade GPUs. Methods such as Quest (Tang et al., 2024) and InfLLM (Xiao et al., 2024a) introduce block-wise KV offloading strategies to handle longer input sequences. ShadowKV (Sun et al., 2024) and MagicPig (Chen et al., 2024) aim to improve decoding throughput by dynamically offloading non-activated KV blocks to the CPU. More recent studies (Yang et al., 2025; Zhou et al., 2025) explore the integration of sparse attention into cloud-based serving systems, further enhancing decoding efficiency in large-scale deployments."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose NOSA, trainable attention mechanism natively designed for KV-cache offloading. Building on the observation that trainable sparse attention naturally exhibits token-selection locality during training, we design an offloading system that reuses the tokens selected in the previous decoding step as cache for the current step. To further improve the cache hit rate, we introduce locality constraint that enforces lower bound on the degree of locality. We pretrain 1B-parameter model with NOSA to evaluate both its task performance and decoding efficiency. Extensive benchmarks show that NOSA maintains nearly lossless task performance while achieving up to 2.3 improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2)."
        },
        {
            "title": "8 Near-Term Future Work",
            "content": "This work is still in progress, and we are actively developing the final version. Our plans are as follows: (1) Conduct more experiments on models of varying scales with more baselines. (2) Explore the scaling behavior of NOSA when applied to pretraining. (3) Perform deeper analysis of the sparsity patterns in the trained models. (4) Develop more optimized offloading system for efficient deployment. We are continuously advancing along these directions and will release further results soon."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude-sonnet-4.5, 2025. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, et al. Magicpig: Lsh sampling for efficient llm generation. arXiv preprint arXiv:2410.16179, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Deepseek-AI. Deepseek-v3.2-exp, 2025. DeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context efficiency with deepseek sparse attention, 2025. DeepSeek-Team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948, 2025. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, et al. R-bench: Graduate-level multi-disciplinary benchmarks for llm & mllm complex reasoning evaluation. arXiv preprint arXiv:2505.02018, 2025. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. 2023."
        },
        {
            "title": "Working in Progress",
            "content": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, and Zhiyuan Liu. Locret: Enhancing eviction in long-context llm inference with trained retaining heads on consumer-grade devices. arXiv preprint arXiv:2410.01805, 2024. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. Minsoo Kim, Kyuhong Shim, Jungwook Choi, and Simyung Chang. Infinipot: Infinite context processing on memory-constrained llms. arXiv preprint arXiv:2410.01518, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pp. 611626, 2023. Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, et al. Mminference: Accelerating pre-filling for long-context vlms via modality-aware permutation sparse attention. Proceedings of ICML, 2025. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Proceedings of NeurIPS, 2024. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36:5234252364, 2023. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv:2502.13189, 2025. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: survey on methodology, applications and challenges. arXiv:2503.21460, 2025. OpenAI. Gpt-5, 2025. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al. Tool learning with foundation models. ACM Computing Surveys, 2024. Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/ 1911.05507. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024."
        },
        {
            "title": "Working in Progress",
            "content": "Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with single gpu. In International Conference on Machine Learning, pp. 3109431116. PMLR, 2023. Jingze Shi, Yifan Wu, Bingheng Wu, Yiran Peng, Liangdong Wang, Guang Liu, and Yuyu Luo. Trainable dynamic mask sparse attention. arXiv preprint arXiv:2508.02124, 2025. Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with consumer-grade gpu. In Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles, pp. 590606, 2024. Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, and Beidi Chen. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference. arXiv preprint arXiv:2410.21465, 2024. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Proceedings of NeurIPS, 2017. Hesong Wang, Juan Liu, Zheng Chen, Yi Zhang, and Cheng Li. Sparse attention diffusion model for pathological micrograph deblurring. In International Conference on Artificial Neural Networks, pp. 275 286. Springer, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. Advances in Neural Information Processing Systems, 37:119638119661, 2024a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024b. Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. Proceedings of ICML, 2025. Ran Yan, Youhe Jiang, and Binhang Yuan. Flash sparse attention: More efficient natively trainable sparse attention. arXiv preprint arXiv:2508.18224, 2025. Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, and Song Han. Lserve: Efficient long-sequence llm serving with unified sparse attention. arXiv preprint arXiv:2502.14866, 2025. Shuo Yang, Ying Sheng, Joseph Gonzalez, Ion Stoica, and Lianmin Zheng. Post-training sparse attention with double sparsity. arXiv preprint arXiv:2408.07092, 2024. Yao Yao, Zuchao Li, and Hai Zhao. Sirllm: Streaming infinite retentive llm. arXiv preprint arXiv:2405.12528, 2024."
        },
        {
            "title": "Working in Progress",
            "content": "Elad Yoshai, Dana Yagoda-Aharoni, Eden Dotan, and Natan Shaked. Hierarchical sparse attention framework for computationally efficient classification of biological cells. arXiv preprint arXiv:2505.07661, 2025. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. Proceedings of ACL, 2025. Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, and Kurt Keutzer. Llm inference unveiled: Survey and roofline model insights, 2024. Chenlu Zhan, Wen Li, Chuyu Shen, Jun Zhang, Suhui Wu, and Hao Zhang. Bidirectional sparse attention for faster video diffusion training. arXiv preprint arXiv:2509.01085, 2025. Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML), 2025a. Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, and Jianfei Chen. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. arXiv preprint arXiv:2505.11594, 2025b. Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR), 2025c. Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, and Jianfei Chen. Sageattention2++: more efficient implementation of sageattention2. arXiv preprint arXiv:2505.21136, 2025d. Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025e. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Weilin Zhao, Zihan Zhou, Zhou Su, Chaojun Xiao, Yuxuan Li, Yanghao Li, Yudi Zhang, Weilun Zhao, Zhen Li, Yuxiang Huang, et al. Infllm-v2: Dense-sparse switchable attention for seamless short-to-long adaptation. arXiv preprint arXiv:2509.24663, 2025. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. Qihui Zhou, Peiqi Yin, Pengfei Zuo, and James Cheng. Sparseserve: Unlocking parallelism for dynamic sparse attention in long-context llm serving. arXiv preprint arXiv:2509.24626, 2025."
        },
        {
            "title": "A Locality",
            "content": "Theorem 1. If the selection process of NOSA has budget = kq + ke, we have {2, , n}, γ(t) ke . Proof. Such statement equals to Γ(t 1) Γ(t) ke. Denote the token indexes of query-aware selection at step 1 as Γq(t 1) = {a1, a2, , akq }, such that sq a1 sq a2 sq akq . Denote the indexes of the Top-k largest importance score as ArgTopk({se }n i=1) = {b1, b2, , bk}, such that se se b2 se bk . (14) (15) According to the Equation 11, we have ArgTopk+kq ({si}n i=1) ArgTopk+kq ({si}n ArgTopk({si}n the number of elements in ArgTopk({se construction, we have bkp+1, bkp+2, , bk / Γ(t 1). Therefore, we have i=1) = {b1, , bk; a1, , akq }. Since Γ(t 1) = i=1), there is Γ(t 1) {b1, , bk; a1, , akq }. Here, we denote }n i=1) that are not included in Γ(t 1) as p. Due to the monotonic b1, b2, , bkp Γ(t 1). (16) Since 0 kq, we must have b1, b2, , bke Γ(t 1). Similarly, we also have b1, b2, , bke Γ(t), thereby Γ(t 1) Γ(t) ke."
        },
        {
            "title": "B Hyperparameters",
            "content": "We list the hyperparameters of the 1B-parameter model in Table 7. Model Size Architecture Vocabulary Size Number of Layers Hidden Size Number of Attention Heads Number of KV Heads FFN Intermediate Size 1B Llama-2 73448 28 2048 16 2 6144 Pretraining Tokens Long-Context Training Tokens SFT Tokens 1.5T 2B 630M Table 7: Model architecture and training details."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University"
    ]
}