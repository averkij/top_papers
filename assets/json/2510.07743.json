{
    "paper_title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment",
    "authors": [
        "Tianci Liu",
        "Ran Xu",
        "Tony Yu",
        "Ilgee Hong",
        "Carl Yang",
        "Tuo Zhao",
        "Haoyu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains a key challenge. In this work, we introduce OpenRubrics, a diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preference-label consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling a new principle-driven paradigm for LLM alignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 4 7 7 0 . 0 1 5 2 : r OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Tianci Liu1,* Ran Xu2,* Tony Yu3 Ilgee Hong3 Carl Yang2 Tuo Zhao3 Haoyu Wang4 1Purdue University 2Emory University 3Georgia Institute of Technology 4University at Albany Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains key challenge. In this work, we introduce OpenRubrics, diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preferencelabel consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling new principle-driven paradigm for LLM alignment. Keywords: Rubrics-as-Rewards, Reward Modeling, LLM Alignment, Synthetic Data Date: 2025-10-10 Model Weights & Checkpoints: https://huggingface.co/OpenRubrics/models Datasets: http://huggingface.co/OpenRubrics/datasets Contact: liu3351@purdue.edu; ran.xu@emory.edu; hwang28@albany.edu 1. Introduction Reward modeling is central to reinforcement learning from human feedback (RLHF) and is widely used to align large language models (LLMs) with human preferences (Ouyang et al., 2022, Wu et al., 2023, Bhaskar et al., 2025). By assigning scalar score (Ouyang et al., 2022) or preference label (Chen et al., 2025b) to each response, reward modeling provides the optimization signal during training and steers the policy LLM toward generating helpful and harmless responses. While lots of efforts have been paid on RL with verifiable reward (RLVR) (Guo et al., 2025a, Yue et al., 2025), many high-value applications of LLMs, such as long-form question answering, general helpfulness, operate in inherently subjective domains where correctness cannot be sufficiently captured by binary signals. To bridge this gap, rubrics-as-rewards (RaR) (Gunjal et al., 2025) have emerged as new paradigm for reward modeling. Rubrics include structured natural language criteria that decompose These authors contributed equally to this work, order was determined randomly (by rolling die). Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment quality into interpretable and measurable dimensions, providing more consistent and transparent evaluation framework than scalar judgments. For policy models, rubrics also enable optimization to be guided by explicit principles. Despite their great promise, the construction of high-quality rubrics remains an open challenge. Existing benchmarks (Arora et al., 2025) curate rubrics with the effort from domain experts, which is costly and difficult to scale. Recent works (Huang et al., 2025, Viswanathan et al., 2025, Gunjal et al., 2025) typically generate rubrics via direct prompting LLMs, but those approaches suffer from limited quality control over rubrics and can be prohibitively expensive when relying on commercial APIs. In this work, we present OpenRubrics, large collection of (prompt, rubrics) pairs to facilitate rubricgeneration model training. Specifically, we prompt the LLM to generate two complementary types of rubrics: hard rules, which capture explicit and objective constraints specified in the prompt, and principles, which summarize implicit and generalizable qualities of strong responses. This design allows the rubrics to capture both surface-level requirements and deeper dimensions of quality. Although hard rules are typically straightforward to extract, the principles are more subtle and require fine-grained reasoning. To address this, we propose Contrastive Rubric Generation (CRG), which conditions on user queries paired with both chosen and rejected responses. By leveraging negative contrasts, CRG encourages the model to identify discriminative qualities that distinguish stronger answers from weaker ones, yielding more comprehensive and ranking-aware rubric signals. To further ensure reliability and reduce the noise, we apply preference-label consistency through rejection sampling, retaining only rubrics that yield correct preference predictions. Our contributions are threefold: Data Contribution: We introduce OpenRubrics, large-scale and domain-diverse collection of rubrics. This dataset enables both rubric generation models and rubric-informed reward modeling at scale. Methodological Contribution: We distinguish between two fundamental types of rubrics and propose novel contrastive rubric generation strategy that trains models to produce comprehensive and discriminative rubrics from prompts and response pairs. In addition, we introduce preferencelabel consistency that improves the quality and reliability of the rubric. Empirical Contribution: We conduct extensive experiments on eight benchmark datasets. Our proposed rubric reward model, Rubric-RM, consistently outperforms strong baselines by 6.8%. Moreover, when integrated into policy optimization, Rubric-RM enables the policy model to achieve strong performance on challenging instruction following and medical benchmarks with an average gain of 2.9%. Case studies further verify the benefits of combining hard rules and principles and show that rubrics help reduce false positives from overly long outputs. 2. Related Work Reward Modeling. Standard reward models assign scalar scores to responses by applying ranking loss between preferred and rejected outputs under the BradleyTerry framework (Bradley and Terry, 1952, Liu et al., 2025a). To enhance reasoning capability, generative reward models (GenRMs) incorporate synthesized Chains of Thought (CoT), enabling more accurate reward estimation (Ankner et al., 2024, Yu et al., 2025, Zhang et al., 2025b, Shen et al., 2024, Liang et al., 2025). Beyond the pointwise setting, pairwise reward models have been proposed to directly compare multiple responses (Xu et al., 2025, Liu et al., 2025b). More recently, reinforcement learning has been leveraged to further optimize reward 2 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Figure 1: Overall Framework for Synthetic Rubric Generation in OpenRubrics. models, enabling them to reason explicitly over comparisons and thereby achieve stronger alignment performance (Chen et al., 2025a,b, Whitehouse et al., 2025, Guo et al., 2025b). Orthogonal to these efforts, our work focuses on improving reward modeling quality through the use of structured rubrics. By introducing rubric-based evaluation signals, we complement existing approaches with an additional layer of interpretability that yield performance gains. Rubrics as Rewards. Recent work has explored rubrics for both evaluation and alignment. Rubrics provide structured assessments of model generations (Arora et al., 2025, Hashemi et al., 2024, Pathak et al., 2025, Lin et al., 2025), guide instruction following and domain adaptation (Viswanathan et al., 2025, Gunjal et al., 2025), improve safety via rule-based rewards (Mu et al., 2024), and have been combined with verifiable rewards for reasoning tasks (Huang et al., 2025, Zhou et al., 2025). Yet most existing approaches rely on prompting frontier LLMs to generate rubrics, which limits scalability and consistency. Our work introduces more scalable framework for high quality synthetic rubric generation, improving both reward quality and interpretability at cheaper cost. Concurrently, Zhang et al. (2025a) also investigate rubric generation, but focus on iterative refinement to mitigate reward over optimization, whereas we emphasize scalable synthesis and rubricpreference consistency. 3. Preliminaries Rubrics. We define rubrics as structured set of evaluation criteria tailored to given prompt. Formally, let denote an input prompt and ˆy model-generated response. rubric ℛ(x) is represented as collection of criteria ℛ(x) = {ci} denotes rubric description specifying an aspect of i=1 response quality (e.g., factual correctness, reasoning soundness, style). , where each ci Rubrics-based Reward Models. Building on prior work in pairwise reward modeling (Liu et al., 2025b, Chen et al., 2025b, Guo et al., 2025b, Xu et al., 2025), we focus on comparative setting where the goal is to evaluate the relative quality of two candidate responses. Given prompt and two samples ( ˆy1, ˆy2), pairwise rubric-based reward function is defined as rewardpair(x, ˆy1, ˆy2) = rθ(x, ˆy1, ˆy2; {ci} i=1), (1) where reward is the binary preference label, rθ rubric criteria {ci} when producing preference judgment. The overall framework for OpenRubrics is in Figure 1. Our overall objective is two-fold: (i) constructing is the reward model parameterized by θ that integrates 3 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment rubric dataset 𝒟rubric for training generation model gθ that automatically synthesizes rubrics ℛ(x) given prompt x; and (ii) building reward modeling dataset 𝒟rm for training rubric-guided reward model rϕ capable of producing reliable and interpretable pairwise judgments. This two-stage formulation enables explicit decomposition of evaluation into rubric generation and rubric-conditioned reward prediction, bridging human-aligned criteria and automated preference modeling. 4. OpenRubrics 4.1. Data Construction Data Sources. To generate high-quality rubrics that generalize across tasks and domains, it is crucial to construct dataset with broad coverage in domains and tasks. To this end, we integrate range of publicly available preference and instruction-tuning datasets, balancing general-purpose conversational data with domain-specific resources. Specifically, our dataset draws from the following sources: UltraFeedback (Cui et al., 2024), which aggregates preference annotations from Evol-Instruct (Xu et al., 2024), UltraChat (Ding et al., 2023), ShareGPT (Chiang et al., 2023), and TruthfulQA (Lin et al., 2022). Tulu 2.5 (Ivison et al., 2024), diverse mixture of preference datasets including AlpacaFarm (Dubois et al., 2023), Chatbot Arena (Chiang et al., 2024), Capybara1, StackExchange (Lambert et al., 2023), Nectar (Zhu et al., 2023), SHP (Ethayarajh et al., 2022), HH-RLHF (Bai et al., 2022), HelpSteer (Wang et al., 2024c), and Orca (Mukherjee et al., 2023). HelpSteer3 (Wang et al., 2025), large-scale human-annotated dataset designed for alignment with helpfulness preferences. Skywork-Preference (Liu et al., 2024), which integrates data from HelpSteer2 (Wang et al., 2024b) and OffsetBias (Park et al., 2024). Tulu3-IF (Lambert et al., 2025a), collection of human preference judgments tailored for verifiable instruction-following. MegaScience (Fan et al., 2025), domain-specialized corpus spanning multiple scientific domains including physics, medicine, biology, and chemistry. Medical-o1 (Chen et al., 2024), medical SFT dataset curated for diagnostic reasoning tasks. Preference Data Construction. To build preference data for rubric generation and judge training (see Sec. 4.3), we reuse existing preference and SFT datasets with tailored processing. For UltraFeedback, we select the highest-scoring response as the chosen and the lowest as the rejected. For Tulu3-IF, MegaScience, and Medical-o1, we generate multiple responses using Qwen-3-8B/14B (Yang et al., 2025), Llama-3.1-8B (Grattafiori et al., 2024), and Gemma-3-12B (Team et al., 2025), selecting one from each model. For Verifiable-IF, responses satisfying all verification functions are labeled as chosen, and others as rejected. For the MegaScience and Medical-o1 datasets, we employ an ensemble of open-source reward models: Athene-RM-8B (Frick et al., 2024a) and Skywork-Reward-V2-Llama-3.1-8B-40M (Liu et al., 2025a) to rank responses and form bestworst preference pairs. 1 https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment (a) The data distribution for OpenRubrics. (b) The distribution for the length of prompts and rubrics, as well as the number of rubrics. Figure 2: Statistics Overview of OpenRubrics. 4.2. Rubrics Synthesis After collecting diverse set of preference pairs, our next objective is to construct set of rubrics that serve as anchors for guiding LLM-based reward modeling. To comprehensively represent different types of constraints while preserving discriminative granularity, we categorize rubrics into two complementary types: (i) Hard Rules, which capture explicit requirements stated in the users prompt; and (ii) Principles, which describe higher-level qualitative aspects such as reasoning soundness, factuality, or stylistic coherence. We then introduce two strategies for generating high-quality rubrics, detailed as follows: + , ˆy )} + and ˆy as follows: is the prompt Contrastive Rubric Generation. Given preference dataset 𝒟 = {(xi, ˆy and ˆy denote the preferred and displeased responses, respectively. Our objective is to infer rubrics ℛ (xi) that capture qualities good response should satisfy and the criteria by which one response as guidance. Formally, we prompt an instruction-tuned LLM is preferred over another, using ˆy hψ i=1 , where xi + , ˆy is the preference label. The generator is asked to produce set of discriminative evaluation where ℓi criteria ℛ (xi) = {ci,1, . . . , ci,ki }, where each ci,j describes specific aspect. This contrastive setting encourages the model to discover rubric dimensions that are both task-sensitive and preference-aligned. ℛ (xi) hψ (xi, ˆy , ℓi) , and ˆy + Rubric Filtering with Preference-label Consistency. Not all generated rubrics faithfully capture the human preference signal. To ensure reliability, we conduct consistency-based filtering step via prompt5 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment again. For each triplet (xi, ˆy ), we concatenate the full rubric ℛ (xi) into the context ing the LLM hψ and ask the model to predict which response better aligns with the rubric ˆli = hψ (xi, ℛ (xi) , ˆy ) , where ˆli = (ˆri, ˆℓi) is the final prediction, ˆri denotes the predicted preference. We retain only those rubrics that lead to predictions consistent with the original human label ℓi denotes the prediction rationale and ˆℓi + , ˆy + , ˆy : ℛ (xi) = { ℛ (xi) , , if ˆℓi = ℓi, otherwise. This yields filtered collection of high-quality rubrics that are both interpretable and empirically consistent with human preferences. The final rubrics-conditioned preference dataset combines prompts, paired responses, and their verified rubrics as: 𝒟rubric = {(xi, ˆy + , ˆy , ℛ (xi))} i=1 . Rubric Statistics Overview. We analyze the curated rubric set along three axes: (i) domain coverage (instruction following, reasoning, general helpfulness; Figure 2a); (ii) the balance between hard rules and principles as well as the length of prompts and rubrics (Figure 2b); and (iii) semantic diversity of prompt topics, visualized via t-SNE on embeddings from Qwen-3-Embedding0.6B (Zhang et al., 2025c) (Figure 3). These statistics confirm that the synthesized rubrics provide comprehensive, yet discriminative coverage, forming foundation for rubric-based reward modeling. Figure 3: The T-SNE plot for the embeddings of prompts. 4.3. Reward Model Training and Inference After collecting the rubrics-based dataset, we proceed to develop rubric generation model that outputs systematic evaluation rubrics and reward model Rubric-RM that generates final preference labels. Rubric Generation. We first train rubric generation model gθ conditioned on the prompt and paired responses. Formally, given dataset 𝒟rubric = {(xi, ˆy where ℛ pervised fine-tuning (SFT) with the standard next-token cross-entropy loss: (xi) denotes the reference rubric associated with the prompt, the model gθ to produce structured evaluation rubrics , ℛ + (xi))} , ˆy is trained via su- , i=1 rubric SFT = ℒ (x, ˆy + ,ℛ , ˆy )𝒟rubric ℛ t=1 x, ℛ log pθ(ℛ <t). 6 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment This objective teaches the model to generate detailed, domain-relevant rubrics that encode evaluation criteria from the user-provided prompt, which can be subsequently used in reward modeling. Reward Model Training. Using the synthesized rubrics, we then train the reward model rϕ (xi), ˆli)} {(xi, ˆy on the prompt, response pair, and rubric: . The model is also optimized via SFT to predict the label tokens ˆli , ℛ + , ˆy i= on 𝒟rm = conditioned rm SFT = ℒ (x, ˆy + ,ℛ , ˆy ,ˆl)𝒟rm ˆl t=1 log pϕ(ˆlt x, ˆy + , ℛ , ˆy (x), ˆl<t). ), Rubric-RM performs two-stage Inference. At inference time, given pairwise test instance (x, process to predict the final preference label: (1) the rubric generator first produces ˆℛ(x) = gθ(x, ); (2) the reward model then predicts the verdict conditioned on the generated rubric from two possible labels 𝒞 = {A is better, is better}: , , A This pipeline ensures that the judgment from Rubric-RM is explicitly grounded in rubric criteria. ˆl = arg max k𝒞 pϕ(k x, , , ˆℛ(x)). 5. Experiment 5.1. Datasets and Experiment Settings Training data. We train both components of Rubric-RMthe rubric generator and the judgeon the curated OpenRubrics as presented in Sec. 4.2. Rubrics are produced with contrastive signals from chosen/rejected responses and filtered by preferencelabel consistency before use. Unless otherwise noted, we use the science-related slice of OpenRubrics to better match our domain study on HealthBench/medical evaluation. Backbone and variants. Both the rubric generator and the judge are fine-tuned from Qwen-3-8B (Rubric-RM-8B) unless specified. At inference time, Rubric-RM follows two-stage pipeline: (i) generate or retrieve rubric conditioned on the prompt and candidate responses; (ii) predict the pairwise preference conditioned on that rubric. We also report an ensemble variant, voting@5, which aggregates five independently sampled judge trajectories by majority vote. Baselines. We compare against strong, same-scale white-box reward/judge modelsJudgeLRM-7B (Chen et al., 2025a), RRM-7B (Guo et al., 2025b), and RM-R1-7B (Chen et al., 2025b)as well as larger RM-R1-14B (Chen et al., 2025b) and reference API judges when available. To isolate the benefit of rubric-aware fine-tuning, we also include naïve pipeline Qwen-3-8B (Rubric+Judge) that directly prompts the base model to produce rubric and then make judgment. Evaluation benchmarks and metrics. We evaluate Rubric-RM as pairwise reward model on widely used reward-modeling suites: RewardBench (Chat / Chat-Hard) (Lambert et al., 2025b), RM-Bench (Liu et al., 2025c), PPE-IFEval (Frick et al., 2024b), FollowBench (Jiang et al., 2024), InfoBench (Qin et al., 2024), IFBench (Peng et al., 2025), and RewardBench2 (Precise-IF / Focus) (Malik et al., 2025). While 7 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment FollowBench and InfoBench were originally designed to assess instruction-following capabilities of LLMs, we adapt them into pairwise evaluation settings by sampling two responses from the same model (Qwen-3-8B/14B), where one response adheres to all specified constraints and the other violates some of them. For the domain study we additionally report HealthBench/medical results. We follow each benchmarks official splits and scoring rules, reporting accuracy/win-rate or benchmark-defined scores. Decoding and efficiency protocol. All models are run under matched decoding budgets (temperature, max tokens, and stop conditions per benchmark recommendations). We use unified execution stack vLLM (Kwon et al., 2023) for throughput-fair comparisons. For efficiency  (Table 4)  , we measure wallclock time to score fixed set of prompts; note that rubrics from stage (i) are cacheable and can be reused across examples, amortizing the cost in large-scale judging and preference optimization. Policy-model evaluation. When integrating Rubric-RM into policy optimization, we follow prior work that evaluates on instruction-following (IF) suites (Zhou et al., 2023, Qin et al., 2024, Viswanathan et al., 2025), using each benchmarks official metrics and scripts. Reproducibility. We use LLaMA-Factory (Zheng et al., 2024) for training Rubric-RM (via SFT) and policy models (via DPO; Rafailov et al. (2023)). For evaluation, we use the benchmarks official scripts where available. To facilitate reproducibility, we release our training and inference configuration in Appendix A.1. Prompts, including rubric templates, are provided in Appendix A.2. 5.2. Performance of Rubric-RM We first validate the performance of Rubric-RM for reward modeling. For more systematic evaluation, we test both 4B and 8B variants of Rubric-RM, which use Qwen3-4B and Qwen3-8B as backbones respectively. Table 1 reports the performance of our proposed Rubric-RM across multiple benchmarks. Outperforming Comparable Reward Models. Both Rubric-RM-4B and Rubric-RM-8B outperform existing 7B-scale white-box reward models such as JudgeLRM-7B, RRM-7B, and RM-R1-7B. RubricRM-4B achieves an average score of 65.6, already higher than JudgeLRM-7B (53.8), RRM-7B (57.8), and RM-R1-7B variants (59.461.7), while Rubric-RM-8B further improves to 68.5. These results demonstrate that rubric-aware training produces more reliable and generalizable reward signals even under smaller model scales, outperforming models trained with generic preference-based supervision. Majority Voting Further Enhances Performance. We also evaluate Rubric-RM-voting@5, which aggregates predictions via majority voting across five independent judge trajectories. This ensemble strategy consistently improves accuracy. Rubric-RM-4B-voting@5 reaches 68.3, and Rubric-RM8B-voting@5 achieves the best overall average of 71.2, nearly matching much larger models such as RM-R1-14B (71.7) and the Rubric+Judge API (71.3). These results highlight the robustness and stability benefits of rubric-based ensembles. Effectiveness of Rubric-Aware Fine-Tuning. direct pipeline of generating rubrics with Qwen-3-8B and then applying them for judgment performs poorly (average 58.9). In contrast, our Rubric-RM substantially outperforms this baseline, achieving 68.5 on average. This demonstrates that our finetuning with high-quality rubrics, generated by contrastive responses and filtered by preferencelabel consistency, provides significant advantage over naïve rubric-based judging. Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Table 1: Comparison of different judge and reward models across multiple benchmarks. RewardBench2 reports results on Precise IF, and Focus dimensions. Rubric API uses GPT-4.1-Mini, and Judge API uses Gemini-2.5-FlashLite. Best results are highlighted in bold. RewardBench IF Evaluation Benchmarks RM-Bench RewardBench Chat Chat Hard FollowBench PPE-IFEval InfoBench IFBench Chat Precise IF Focus HelpSteer3 AVG Black-box LLMs (For reference only) Claude-3.5-Sonnet API (Rubric+Judge) API (direct Judge) 96.4 79.6 89.6 74.0 79.2 71.2 Larger White-box LLMs (For reference only) RM-R1-14B (Qwen-2.5-Inst) RM-R1-14B (DeepSeek-Dist) 73.5 90.3 White-box Judge/Reward LLMs JudgeLRM-7B RRM-7B RM-R1-7B (Qwen-2.5-Inst) RM-R1-7B (DeepSeek-Dist) Qwen-3-8B (Rubric+Judge) 92.1 77.7 83.0 85.3 73.9 Rubric-RM (Our proposed model) Rubric-RM-4B Rubric-RM-4B-voting@5 Rubric-RM-8B Rubric-RM-8B-voting@ 83.2 83.6 87.3 89.6 79.8 78.9 56.1 69.5 70.0 67.3 63.6 67.4 69.1 73.0 75.7 83.2 81.7 84.0 89. 79.8 65.5 56.3 69.7 63.0 71.1 77.3 73.1 78.2 58.0 61.0 59.2 59.0 61.2 46.0 51.0 55.2 51.0 53.8 61.8 65.6 65.1 68. 82.2 72.9 85.5 82.4 62.7 68.2 71.3 70.3 74.6 77.4 80.9 78.6 81.6 66.2 60.4 60.8 59. 47.5 53.2 55.2 56.5 55.6 62.1 64.4 64.6 67.1 62.5 67.9 67.2 73.2 71.4 55.4 59.9 64.2 62.2 64.2 61.1 61.8 62.2 62. 38.8 42.5 13.2 23.8 30.6 9.4 10.0 20.6 13.8 21.9 28.1 34.1 33.8 40.0 87.0 79.6 63.4 84.6 79. 29.1 60.4 76.2 55.4 56.6 76.6 78.2 78.2 79.2 71.4 70.3 74.8 74.6 60.2 62.4 65.2 62.6 61.8 66.7 68.2 68.6 69. - 71.3 64.9 69.9 71.7 53.8 57.8 61.7 59.4 58.9 65.6 68.3 68.5 71.2 Strength on IF Evaluation Benchmarks. In addition to absolute improvements, Rubric-RM shows particularly strong advantages on the IF Evaluation Benchmarks, which measure fine-grained instruction following capabilities. For example, on FollowBench and InfoBench, Rubric-RM achieves 73.1 and 78.6, respectively, substantially outperforming other 7B-scale baselines such as JudgeLRM-7B (79.8 / 62.7) and RRM-7B (65.5 / 68.2). These results demonstrate that rubric-based training is especially effective at capturing instruction adherence and nuanced response qualities, where conventional reward models often struggle. In the remaining part of experimentation, we use Rubric-RM-8B as our reward model unless specified. 5.3. Offline Reinforcement Learning for Policy Models with Rubric-RM 5.3.1. Instruction-Following Evaluation We further evaluate the effectiveness of using Rubric-RM as reward model for policy optimization on instruction-following tasks, including IFEval, InfoBench, and IFBench. The results are shown in Table 2 and Fig. 4. Improved Performance on IFEval and InfoBench. When serving as the reward model in Direct Preference Optimization (DPO), Rubric-RM enables the trained policy model to achieve the best overall performance among all open-source counterparts. On IFEval, the Rubric-RM-trained policy achieves an average score of 79.9, surpassing policies trained with Skywork (76.0) and ArmoRM (76.0). On InfoBench, the Rubric-RM-based policy reaches 82.9, outperforming other DPO-trained policies and approaching the performance of much larger commercial systems. These results highlight that rubrics provide more reliable optimization signals for constrained instruction-following. 9 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Table 2: Comparison of trained policy models with different reward models on format-based constrained instruction-following benchmark (IFEval) and an open-ended benchmark (InfoBench). Baseline results are from Viswanathan et al. (2025). Results with underlines are reproduced by us using official checkpoints and evaluation scripts. Best scores are in bold."
        },
        {
            "title": "Model",
            "content": "IFEval (Prompt) IFEval (Inst.)"
        },
        {
            "title": "Loose Strict",
            "content": "GPT-4 (0314) AutoIF (Dong et al., 2024) UltraIF (An et al., 2025) Qwen2.5-7B-Instruct + SFT (Distilled) + DPO (via Skywork) + DPO (via ArmoRM) + DPO (via Ultrafbk.) + DPO (via AI Judge) + DPO (via RLCF) 79.3 56.9 75.4 75.0 66.8 75.7 73.8 71.5 73.0 77.3 76.9 47.1 71.3 72.5 64.1 68.0 70.2 69.1 68.9 72. 85.4 67.0 83.0 81.8 75.3 83.2 81.7 79.9 80.9 84.1 83.6 57.6 79.4 79.9 72.8 78.5 78.3 77.7 77.8 80."
        },
        {
            "title": "AVG",
            "content": "81.3 57.2 77.3 77.3 69.8 76.0 76.0 74.6 75.2 78."
        },
        {
            "title": "AVG",
            "content": "87.3 80.6 80.7 78.1 (76.0) 72.5 82.0 83.5 80.0 76.1 84.1 (81.5) + DPO (via Rubric-RM) 79.1 73.8 85. 81.3 79.9 82.9 Clear Gains on Complex Instruction Following Benchmark (IFBench). Figure 4 shows that the policy model optimized with Rubric-RM attains new state-of-the-art score of 33.7 on IFBench, substantially higher than RLCF (28.2) and RLMT-based methods (22.422.8). Compared with both supervised fine-tuning variants and reinforcement learning baselines, Rubric-RM provides stronger inductive biases, enabling policies to better capture fine-grained instruction adherence. Overall, these results confirm that using Rubric-RM as reward model significantly enhances the instruction-following ability of trained policies. Unlike prior scalar or generative reward models that often fail to enforce strict formats or nuanced constraints, rubrics supply explicit and interpretable guidance, leading to consistent improvements across multiple benchmarks. This demonstrates that Rubric-RMbased training not only improves absolute performance but also establishes robust foundation for building policy models aligned with human instructions. Figure 4: Comparison of trained policy models on IFBench. Results of baselines except RLCF are from Bhaskar et al. (2025). We evaluate RLCF with its official checkpoint. 10 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Table 3: Comparison of different alignment strategies applied to Qwen2.5-7B-Instruct on Arena-Hard and AlpacaEval benchmarks. Results are reported for vanilla models and style/length-controlled settings. Baseline results are from Viswanathan et al. (2025). Best results are in bold."
        },
        {
            "title": "Model",
            "content": "Arena-Hard"
        },
        {
            "title": "AlpacaEval",
            "content": "Vanilla Style-Controlled Vanilla Length-Controlled GPT-4 (0314) UltraIF (An et al., 2025) Qwen2.5-7B-Instruct + SFT (Distilled) + DPO (via Skywork) + DPO (via ArmoRM) + DPO (via Ultrafbk.) + DPO (via AI Judge) + DPO (via RLCF) 50.0 31.4 51.3 32.6 55.1 50.8 52.8 51.0 54. + DPO (via Rubric-RM) 56.1 50.0 42.8 29.2 50.3 46.4 47.9 44.4 48.4 56.9 22.1 33.5 36.1 44.8 37.6 33.7 28.8 36.2 38.8 35.3 36.2 33.3 41.5 38.1 38.7 33.4 37.1 50."
        },
        {
            "title": "AVG",
            "content": "39.4 41.0 32.8 47.9 43.2 43.3 39.4 44.1 50.6 5.3.2. Human Preference Alignment Evaluation We evaluate policies trained with Rubric-RM on human preference alignment benchmarks Arena-Hard and AlpacaEval  (Table 3)  . With DPO optimization, Rubric-RM achieves the best overall average score (50.6) among all opensource reward models. On Arena-Hard (style-controlled), it obtains 56.9, outperforming Skywork (50.3), Ultrafeedback (47.9), and RLCF (48.4). On AlpacaEval (length-controlled), it reaches 50.5, surpassing ArmoRM (38.1) and AI Judge (33.4). These results show that rubric-based signals provide reliable gains across both vanilla and controlled settings. 5.4. Rubric-RM for BioMedical Domain In this section we further study the effectiveness of Rubric-RM in more specialized medical domain, following Arora et al. (2025). Experiments here are again conducted from two aspects: (i) how OpenRubrics results in better reward modeling ability in Rubric-RM; (ii) how does Rubric-RM leads to stronger policy models. The Rubric and Judge are Qwen-3-8B backbones fine-tuned with OpenRubrics data from science-related domains, see Sec 4.1 for more details about our data. 5.4.1. Performance of Rubric-RM on HealthBench As in general domains, Rubric-RM outperforms comparable-size generative reasoning reward models on HealthBench: our model obtains 68.3, exceeding RRM-7B (63.3) and two 7B-level variants of RM-R1 (55.4/66.9). Notably, Rubric-RM matches the competitiveness of much larger RM-R1-14B (69.9). Moreover, consistent with previous results, majority voting further enhances Rubric-RM performance: Rubric-RM-voting@5 attains 72.9 (+4.6 over single-pass judge), narrowing the gap to much larger 14B reasoning models (e.g., RM-R1-14B up to 74.7) and approaching API-based references (69.9 73.5). These gains clearly reflect the effectiveness of Rubric-RM trained with OpenRubrics. 11 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment The second noticeable gain lies in the importance of domain-specific SFT. Compared to directly prompting Qwen-3-8B with the proposed Rubric+Judge pipeline, which only reaches score 51.8, Rubric-RM (68.3) yields remarkable improvement on HealthBench (+16.5). This pronounced gap highlights the importance of domain-specific rubric data and rubric-aware SFT: contrastive rubric training and preferencelabel consistency produce higher-precision, science-aware criteria that transfer more effectively to health tasks than rubrics generated on-the-fly. (a) Judges and Reward Models. (b) Trained Policy Models. Figure 5: Comparison of different judges, reward models, and trained policy models on HealthBench. 5.4.2. Preference Optimization with Rubric-RM on HealthBench We further validate that our Rubric-RM, which achieves higher reward modeling performance as presented in the previous section, can successfully translate into stronger policy model learning. Here we compare using Rubric-RM as the preference judge for DPO on HealthBench against two baselines expanding both reasoning-based RM-R1-7B (Qwen-2.5-Inst), and non-reasoning based ArmoRM (Wang et al., 2024a). Specifically, we use Qwen-2.5-7B-Instruct as the base policy model and collect independent 4 responses from it to each question in HealthBench. The preference pairs are labeled with different reward models, upon which the base model is finetuned with DPO. The DPO performance results are reported in Figure 5b. According to the Figure, holding the policy backbone and DPO recipe fixed, replacing baseline judges with Rubric-RM consistently yields the best downstream performance. Starting from the base model (21.6), DPO via ArmoRM reaches 22.5 and via RM-R1-7B reaches 22.7, whereas DPO via Rubric-RM attains 23.8, the highest among all settings. This results in 1.1 to 1.3 absolute gain over strong 7B reasoning rewards echoes our findings that rubric-aware, domain-tuned signals provide cleaner preferences for policy optimization than generative reasoning at similar scale. 5.5. Efficiency Comparison In this section we analyze the computing cost for RubricRM inference. Table 4 reports wall-clock time on 100 randomly sampled prompts from RewardBench2. Notably, our Rubric-RM (130.77 sec), whose rubric generator and judge are both Qwen-3-8B, is not slower than existing reasoning reward models such as RRM-7B (203.4 sec) and RM-R17B/14B (170.76 382.02 sec). In particular, it is markedly faster than the 14B R1 variants and competitive with the Table 4: Computing speed on 100 samples (vLLM). Compute Time (sec.) JudgeLRM-7B RRM-7B RM-R1-7B (Qwen-2.5-Inst) RM-R1-7B (DeepSeek-Dist) RM-R1-14B (Qwen-2.5-Inst) RM-R1-14B (DeepSeek-Dist) Rubric-RM-8B 25.71 203.4 260.37 170.76 322.79 382.02 130.77 12 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment stronger 7B reasoning baselines, while operating at the 8B scale. We attribute the speed gap to the different computation patterns. Prior reasoning reward models execute long chain-of-thought traces before emitting final judgment, incurring substantial decoding latency. In contrast, our approach decomposes evaluation into two focused stages(i) generating or retrieving rubric and (ii) applying lightweight judge conditioned on that rubricso each step remains short and targeted. further practical advantage is that rubrics are amortizable: once produced, they can be computed offline and cached for reuse across many examples, eliminating rubric generation cost at scoring time. This property makes Rubric-RM especially attractive for large-scale preference optimization, where repeated judgments dominate runtime. While JudgeLRM-7B achieves the lowest raw latency (25.71 sec), it does not provide the explicit rubric signal that enables our methods interpretability and downstream policy optimization. 5.6. Case Study Case Study: Instruction adherence on RewardBench Chat Hard"
        },
        {
            "title": "Prompt",
            "content": "Describe vivid and unique character using strong imagery, in fewer than two paragraphs. Resp (snippet) Resp (snippet)"
        },
        {
            "title": "Ground Truth",
            "content": "She was woman of great power and influence. . . with deep voice and piercing eyes. . . (single paragraph, on-task) . . . fierce and determined young woman. . . [paragraph break] . . . Which of the following is an example of character. . . [paragraph break]; character. . . Lily. . . Resp A. RRM-7B . . . both responses are within that limit . . . Assistant 2s response is more detailed. . . more"
        },
        {
            "title": "JudgeLRM",
            "content": "Rubric-RM"
        },
        {
            "title": "Rubric",
            "content": "(Choose B) vivid and unique . . . . . . In contrast, Assistant 2 provided more detailed and vivid description, painting picture of character named Lily who is not only leader but also survivor, fighter, and free spirit . . . (Choose B) 1. The response uses strong imagery and creative language to create vivid and unique character description. [Hard Rule] 2. The response is written in fewer than two paragraphs. [Hard Rule] 3. The response presents character with distinctive and memorable traits that contribute to clear and engaging portrayal. [Principle] 4. The response employs sensory details and evocative descriptions to enhance the readers mental image. [Principle] 5. The response demonstrates originality and creativity in language and characterization to avoid clichés and generic portrayals. [Principle] 6. The response maintains coherence and focus, ensuring all elements contribute to unified and compelling character depiction. [Principle] 7. The response balances detail and conciseness, providing enough information to be vivid without unnecessary elaboration. [Principle]"
        },
        {
            "title": "Judge",
            "content": "Identified Gatekeeper Criterion: The response is written in fewer than two paragraphs. . . . Response successfully adheres to all criteria . . . Response fails the paragraph count rule . . . While Response offers more vivid and creative language . . . (Choose A) Table 5: Case study with error highlighting. Baselines favor the longer, imagery-rich response (B) and miss the explicit paragraph limit, while Rubric-RM first enforces hard rules and then scores principles. We end up this section with concrete case studies on how Rubric-RM addresses challenging inputs and Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment leads to better reward modeling. We present two instances drawn from RewardBench and FollowBench benchmarks, with baselines chosen from same-size generative reasoning reward models. Namely, we use JudgeLRM-7B and RRM-7B in Case 1 study, and two RM-R1-7B variants (DeepSeek-Dist, Qwen-2.5-Inst) for Case 2 study. The detailed results are shown in Table 5 and Table 6 respectively. Case Study: Instruction adherence on FollowBench"
        },
        {
            "title": "Prompt",
            "content": "Resp (snippet) Resp (snippet)"
        },
        {
            "title": "Ground Truth",
            "content": "RM-R1-7B (DS) RM-R1-7B (Qwen) Rubric-RM"
        },
        {
            "title": "Judge",
            "content": "Would you consider direct air carbon capture as significant financial venture? In addition to sharing your perspective, incorporate quote from recent news article or study to substantiate your viewpoint, and concisely summarize it. Also, ensure to mention the publication date of the referenced source. Furthermore, briefly discuss the potential economic implications based on your source. Direct air carbon capture (DAC) is promising technology . . . According to recent study published in Nature Energy in March 2023 . . . In conclusion, while there are still challenges to overcome, the recent study suggests that DAC has the potential to become financially viable option. . . . Would you consider direct air carbon capture as significant financial venture? Yes . . . recent report from BloombergNEF, published on May 16, 2024, titled . . . BNEFs projection highlights several key economic implications: 1. . . . Resp B. . . . looking at Chatbot As response. It starts by explaining DAC briefly and then moves into the study from Nature Energy in 2023 . . . Chatbot . . . The quote is from BloombergNEF, but the publication date isnt provided . . . instructions appears study prompt. . . doesnt provide proper citation with the requested publication date providing have misunderstood follows all . . . Chatbot . . . Chatbot recent (Choose A) from the by to quote . . . (Choose A) 1. The response must address whether direct air carbon capture is considered significant financial venture. [Principle] 2. The response must incorporate quote from recent news article or study to substantiate its viewpoint. [Hard Rule] 3. The response must concisely summarize the quoted source. [Hard Rule] 4. The response must mention the publication date of the referenced source. [Hard Rule] 5. The response must briefly discuss the potential economic implications based on the source. [Hard Rule] 6. The response must be written in clear and understandable manner. [Principle] 7. The response must be well-organized and easy to follow. [Principle] 8. The response must be free of factual errors. [Principle] 9. The response must be relevant to the topic of direct air carbon capture and its financial aspects. [Principle] Identified Gatekeeper Criterion: The response must incorporate quote from recent news article or study to substantiate its viewpoint. . . . Response is slightly better organized (Choose B) . . . for the economic implications . . . also provided more recent source . . . Table 6: Case study 2 with error highlighting. Baselines overvalue academic-looking prose and misreading the date requirement, whereas Rubric-RM prioritizes hard rules and makes correct decision. Case 1 (RewardBenchChat Hard): instruction adherence vs. verbosity bias. Both responses contain vivid descriptions, but the instruction explicitly requires fewer than two paragraphs. Baseline reasoning 14 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment RMs ignore this hard requirement and choose the longer response, exhibiting classic verbosity bias and instruction-violation blindness. In contrast, Rubric-RM first applies the Gatekeeper check (paragraph count), rejects the non-compliant candidate, and then scores principles (imagery/originality/focus), ultimately selecting the correct answer. This example highlights that long chain-of-thought does not guarantee correct constraint satisfaction, whereas rubricjudge decomposition makes the failure explicit and avoids it. Case 2 (FollowBench): verifiable recency and citation integrity. This example is more challenging: both answers are longer and the quality gap is subtle. Baselines nevertheless produce factual mistakes about the evidence, e.g., asserting that the better response lacks date/citation, despite it correctly providing BloombergNEF quote with May 16, 2024 publication date and concrete figures ($387B cumulative investment). Our rubric-aware judge identifies recency and verifiability as hard requirements (quote, date, concise summary, and economic implications), and favors the response that meets them. This demonstrates Rubric-RMs robustness to citation hallucinations and over-weighting of academiclooking prose that misled generative reasoning RMs. Takeaways. Across both cases, baseline reward models with long CoT capabilities still fail for two recurring reasons: (i) they overlook explicit hard rules (structural and evidentiary constraints), and (ii) they are vulnerable to hallucinated or weakly verifiable references. By contrast, Rubric-RM enforces gatekeepers before scoring higher-level qualities, yielding interpretable decisions and improved accuracy on difficult examples. We also observe that the gatekeeper stage reduces false positives from offtask/overlong content and elevates verifiability-aware judging in domains (e.g., science/finance) where recency and source integrity matter. 6. Conclusion We present OpenRubrics, large-scale dataset and framework for scalable and high-quality rubric generation. By decomposing evaluation into hard rules and principles through Contrastive Rubric Generation (CRG) and applying preferencelabel consistency filtering, we constructed interpretable and discriminative rubric signals that better align with human judgment. Our rubric-based reward model, Rubric-RM, delivers an average 6.8% improvement across diverse benchmarks and further boosts policy performance by 1.1%6.5% when used as reward in offline reinforcement learning. These results position rubrics-as-rewards as practical foundation for transparent and generalizable LLM alignment. For future work, we will extend rubric generation to more open-ended tasks and leverage rubrics as intermediate supervision within RLHF pipelines. 15 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment"
        },
        {
            "title": "References",
            "content": "Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, and Baobao Chang. Ultraif: Advancing instruction following from the wild. arXiv preprint arXiv:2502.04153, 2025. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critiqueout-loud reward models. arXiv preprint arXiv:2408.11791, 2024. Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Adithya Bhaskar, Xi Ye, and Danqi Chen. Language models that think, chat better. arXiv preprint arXiv:2509.20357, 2025. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. Judgelrm: Large reasoning models as judge. arXiv preprint arXiv:2504.00050, 2025a. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025b. Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating LLMs by human preference. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=3MW8GKNyzI. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK: Boosting language models with scaled AI feedback. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=BOorDpKHiJ. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of 16 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment the 2023 Conference on Empirical Methods in Natural Language Processing, pages 30293051, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/ 2023.emnlp-main.183/. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. arXiv preprint arXiv:2406.13542, 2024. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=4hturzLcKX. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with 𝒱-usable information. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 59886008. PMLR, 1723 Jul 2022. Run-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025. Evan Frick, Peter Jin, Tianle Li, Karthik Ganesan, Jian Zhang, Jiantao Jiao, and Banghua Zhu. Athene-70b: Redefining the boundaries of post-training for open models, July 2024a. URL https://nexusflow. ai/blogs/athene. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. How to evaluate reward models for rlhf, 2024b. URL https://arxiv.org/abs/2410.14872. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025a. Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. Reward reasoning model. arXiv preprint arXiv:2505.14674, 2025b. Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, and Chris Kedzie. Llm-rubric: multidimensional, calibrated approach to automated evaluation of natural language texts. arXiv preprint arXiv:2501.00274, 2024. Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv preprint arXiv:2508.12790, 2025. 17 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hanna Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. Advances in neural information processing systems, 37:3660236633, 2024. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. FollowBench: multi-level fine-grained constraints following benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46674688, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.257/. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. Huggingface h4 stack exchange preference dataset, 2023. URL https://huggingface.co/datasets/HuggingFaceH4/ stack-exchange-preferences. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Christopher Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. In Second Conference on Language Modeling, 2025a. URL https://openreview.net/forum?id=i1uGbfHHpH. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 17551797, Albuquerque, New Mexico, April 2025b. Association for Computational Linguistics. URL https://aclanthology.org/2025.findings-naacl.96/. Xiaobo Liang, Haoke Zhang, Juntao Li, Kehai Chen, Qiaoming Zhu, and Min Zhang. Generative reward modeling via synthetic criteria preference learning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2675526769, Vienna, Austria, July 2025. Association for Computational Linguistics. URL https://aclanthology.org/ 2025.acl-long.1297/. Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking LLMs with challenging tasks from real users in the wild. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=MKEHCx25xp. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.acl-long.229/. Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, et al. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352, 2025a. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Pairjudge rm: Perform best-of-n sampling with knockout tournament. arXiv preprint arXiv:2501.13007, 2025b. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. RM-bench: Benchmarking reward models of language models with subtlety and style. In The Thirteenth International Conference on Learning Representations, 2025c. URL https://openreview.net/forum?id=QEHrmQPBdd. Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah Smith, Hannaneh Hajishirzi, and Nathan Lambert. Rewardbench 2: Advancing reward model evaluation. arXiv preprint arXiv:2506.01937, 2025. Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language model safety. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=QVtwpT5Dmg. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Junsoo Park, Seungyeon Jwa, Ren Meiying, Daeyoung Kim, and Sanghyuk Choi. OffsetBias: Leveraging debiased data for tuning evaluators. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 10431067, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-emnlp.57/. Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Arnav Ramamoorthy, Pratyush Ghosh, Aaryan Raj Jindal, Shreyash Verma, Aditya Mittal, Aashna Ased, Chirag Khatri, et al. Rubric is all you need: Enhancing llm-based code evaluation with question-specific rubrics. arXiv preprint arXiv:2503.23989, 2025. Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1593415949, Vienna, Austria, July 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025.acl-long.775/. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, InFoBench: Evaluating instruction following ability in large In Findings of the Association for Computational Linguistics: ACL 2024, pages Fei Liu, Pengfei Liu, and Dong Yu. language models. 19 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment 1302513048, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.772/. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=HPuSIXJaa9. Jiaming Shen, Ran Xu, Yennie Jun, Zhen Qin, Tianqi Liu, Carl Yang, Yi Liang, Simon Baumgartner, and Michael Bendersky. Boosting reward model with preference-conditional multi-aspect synthetic data generation. arXiv preprint arXiv:2407.16008, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024a. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer 2: Open-source dataset for training top-performing reward models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=PvVKUFhaNy. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Scowcroft, Neel Kant, Aidan Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 33713384, 2024c. Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, and Oleksii Kuchaiev. Helpsteer3-preference: Open human-annotated preference data across diverse tasks and languages. arXiv preprint arXiv:2505.11475, 2025. Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. J1: Incentivizing thinking in llm-as-a-judge via reinforcement learning. arXiv preprint arXiv:2505.10320, 2025. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36:5900859033, 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=CfXh93NDgH. Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Wenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, and Yonghui Wu. unified pairwise framework for rlhf: Bridging generative reward modeling and policy optimization. arXiv preprint arXiv:2504.04950, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, and Rui Hou. Self-generated critiques boost reward modeling for language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1149911514, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. URL https://aclanthology.org/ 2025.naacl-long.573/. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, and Lifeng Jin. Chasing the tail: Effective rubric-based reward modeling for large language model post-training. arXiv preprint arXiv:2509.21500, 2025a. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=Ccwp4tFEtE. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025c. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/ 2403.13372. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv, Kongcheng Zhang, Yihe Zhou, Hengtong Lu, et al. Breaking the exploration bottleneck: Rubric-scaffolded reinforcement learning for general llm reasoning. arXiv preprint arXiv:2508.16949, 2025. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness and harmlessness with rlaif, November 2023. 21 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment A.1. Hyper-parameters A. Appendix Table 7 details the hyper-parameters used in Rubric-RM and policy model training, which were conducted in LLaMA-Factory (Zheng et al., 2024). Moreover, Table 8 presents sampling parameters used in OpenRubrics curation and Rubric-RM inference. For baseline methods, we adopted the sampling parameters from their official implementations and papers. Table 7: Hyper-parameters used in Rubric-RM and policy model training. Rubric-RM SFT Rubric-Generator Judge-Generator"
        },
        {
            "title": "Epochs\nCutoff Length\nBatch Size\nOptimizer\nLearning Rate\nLR Schedule\nWarmup\nSFT mixing weight",
            "content": "β 1 3072 128 AdamW 6 8 10 Cosine 0.05 2 6144 64 AdamW 6 5 10 / / 1 2048 64 AdamW 7 3 10 / / 0.1 0.1 A.2. Prompts We present the prompts we used in this subsection. For baseline methods, we adopted the prompts from their official implementations and papers. 22 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Table 8: Sampling parameters used in OpenRubrics curation and Rubric-RM inference."
        },
        {
            "title": "OpenRubrics Curation",
            "content": "Rubric-Generator Judge-Generator Rubric-RM Inference Rubric-Generator Judge-Generator"
        },
        {
            "title": "Value",
            "content": "Model Maximum Tokens Temperature Top-P Top-K Model Maximum Tokens Temperature Top-P Top-K GPT-4.1-Mini 768 0.0 / / Gemini-2.5-Flash-Lite 2048 0.0 / / Base-Model Maximum Tokens Temperature Top-P Top-K Enable-thinking Model Maximum Tokens Temperature Top-P Top-K Enable-thinking Qwen-3-4B/8B (Default) 1024 0.0 / / False Qwen-3-4B/8B (Default) 4096 0.7 1.0 -1 (All) False 23 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Prompt for Contrastive Rubric Generation (OpenRubrics Curation) You are an expert in pedagogy and critical thinking. Your mission is to create universal scoring rubric based on user's request and set of examples. The final rubric must consist of high-level, generalizable principles that can be used to evaluate any response to the request, not just the specific examples provided. ==================================================================== Methodology - Three-Step Process for Principled Rubric Design ==================================================================== 1. Step 1: Extract Explicit Requirements. - Meticulously analyze the <request> tag to identify all direct commands and constraints (e.g., length, format, style). - These requirements are *non-negotiable hard rules* that must appear in the rubric. - They should be clearly labeled as [Hard Rule] in the final output. 2. Step 2: Analyze the Examples for Specific Differences. - If <chosen> and <rejected> responses are present, identify all specific, concrete reasons why the chosen response is superior. - At this stage, it is acceptable to generate topic-specific observations (e. g., \"The chosen response correctly stated that Zeus is myth\"), but these observations are *temporary* and must not appear in the final rubric. - Every such observation must then be abstracted in Step 3. 3. Step 3: MANDATORY ABSTRACTION -- Convert Specifics to Universal Principles. - This is the most critical step. For each observation from Step 2, ask: **\"What is the universal principle of high-quality communication, reasoning, or pedagogy that this specific difference demonstrates?\"** - Convert each observation into principle that applies across any domain, not just the provided examples. - Any rubric item that references concrete facts, names, events, or topics is INVALID. - All such principles must be labeled as [Principle] in the final output. ==================================================================== Strict Guidelines for Final Output ==================================================================== - **Abstraction is Mandatory:** Every rubric item must be universal principle. If any rubric still contains topic-specific references (e.g., names, places, myths, numbers, historical facts), it is automatically invalid. - **Two Distinct Categories:** - [Hard Rule]: Derived strictly from explicit requirements in the <request>. - [Principle]: Derived from abstracted differences in Step 3. 24 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment - **Comprehensiveness:**"
        },
        {
            "title": "The rubric must cover all critical aspects implied by the request and",
            "content": "examples, including explicit requirements and implicit quality standards. - **Conciseness & Uniqueness:** Each rubric must capture distinct evaluation criterion. Overlapping or redundant criteria must be merged into single rubric. Wording must be precise and free of repetition. - **Format Requirements:** - Use numbered list. - Each item starts with \"The response...\" phrased in third person. - Append [Hard Rule] or [Principle] at the end of each item. - Do not include reasoning, explanations, or examples in the final outputonly the rubrics. - **Validation Check Before Output:** Before presenting the final list, verify: 1. Does every rubric meet the abstraction requirement (no topic-specific details)? 2. Are all hard rules from Step 1 included? 3. Are all principles unique and non-overlapping? 4. Is the list written entirely in third person, concise, and consistent? ==================================================================== Final Output Format ==================================================================== 1. The response ... [Hard Rule] 2. The response ... [Principle] 3. The response ... [Principle] ... (continue until all rules and principles are listed) ==================================================================== <request> {request} </request> <context> {context} </context> <chosen> {chosen} </chosen> <rejected> {rejected} </rejected}> 25 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Prompt for Judge Generation (OpenRubrics Curation) You are fair and impartial judge. Your task is to evaluate 'Response A' and ' Response B' based on given instruction and rubric. You will conduct this evaluation in distinct phases as outlined below. ### Phase 1: Compliance Check Instructions First, identify the single most important, objective 'Gatekeeper Criterion' from the rubric. - **A rule is objective (and likely Gatekeeper) if it can be verified without opinion. Key examples are: word/paragraph limits, required output format (e .g., JSON validity), required/forbidden sections, or forbidden content.** - **Conversely, rule is subjective if it requires interpretation or qualitative judgment. Subjective rules about quality are NOT Gatekeepers. Examples include criteria like \"be creative,\" \"write clearly,\" \"be engaging ,\" or \"use professional tone.\"** ### Phase 2: Analyze Each Response Next, for each Gatekeeper Criterion and all other criteria in the rubric, evaluate each response item by item. ### Phase 3: Final Judgment Instructions Based on the results from the previous phases, determine the winner using these simple rules. Provide final justification explaining your decision first and then give your decision. --- ### REQUIRED OUTPUT FORMAT You must follow this exact output format below. --- Compliance Check --- Identified Gatekeeper Criterion: <e.g., Criterion 1: Must be under 50 words.> --- Analysis --- **Response A:** - Criterion 1 [Hard Rule]: Justification: <...> - Criterion 2 [Hard Rule]: Justification: <...> - Criterion 3 [Principle]: Justification: <...> - ... (and so on for all other criteria) **Response B:** - Criterion 1 [Hard Rule]: Justification: <...> - Criterion 2 [Hard Rule]: Justification: <...> - Criterion 3 [Principle]: Justification: <...> - ... (and so on for all other criteria) --- Final Judgment --- Justification: <...> Winner: <Response / Response B> 26 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Task to Evaluate: Instruction: {instruction} Rubric: {rubric} Response A: {response_a} Response B: {response_b} 27 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Prompt for Rubric Generation (Rubric-RM) Your task is to extract set of rubric-style instructions from user's request."
        },
        {
            "title": "These rubrics will be used as evaluation criteria to check if a response fully",
            "content": "meets the request. Every rubric item must be universal principle. If any rubric still contains topic-specific references (e.g., names, places, myths, numbers, historical facts), it is automatically invalid. - Two Distinct Categories: - [Hard Rule]: Derived strictly from explicit requirements stated in the < request> (format, length, structure, forbidden/required elements, etc.). - [Principle]: Derived by abstracting any concrete cues into domain-agnostic quality criteria (e.g., clarity, correctness, sound reasoning, pedagogy). - Comprehensiveness:"
        },
        {
            "title": "The rubric must cover all critical aspects implied by the request and",
            "content": "examples, including explicit requirements and implicit quality standards. - Conciseness & Uniqueness: Each rubric must capture distinct evaluation criterion. Overlapping or redundant criteria must be merged into single rubric. Wording must be precise and free of repetition. - Format Requirements: - Use numbered list. - Each item starts with \"The response\" phrased in third person. - Append [Hard Rule] or [Principle] at the end of each item. - Do not include reasoning, explanations, or examples in the final output-- only the rubrics. Here is the request: {prompt} Please generate the rubrics for the above request. 28 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Prompt for Judge Generation (Rubric-RM) You are fair and impartial judge. Your task is to evaluate 'Response A' and ' Response B' based on given instruction and rubric. You will conduct this evaluation in distinct phases as outlined below. ### Phase 1: Compliance Check Instructions First, identify the single most important, objective 'Gatekeeper Criterion' from the rubric. - **A rule is objective (and likely Gatekeeper) if it can be verified without opinion. Key examples are: word/paragraph limits, required output format (e .g., JSON validity), required/forbidden sections, or forbidden content.** - **Conversely, rule is subjective if it requires interpretation or qualitative judgment. Subjective rules about quality are NOT Gatekeepers. Examples include criteria like \"be creative,\" \"write clearly,\" \"be engaging ,\" or \"use professional tone.\"** ### Phase 2: Analyze Each Response Next, for each Gatekeeper Criterion and all other criteria in the rubric, evaluate each response item by item. ### Phase 3: Final Judgment Instructions Based on the results from the previous phases, determine the winner using these simple rules. Provide final justification explaining your decision first and then give your decision. --- ### REQUIRED OUTPUT FORMAT You must follow this exact output format below. --- Compliance Check --- Identified Gatekeeper Criterion: <e.g., Criterion 1: Must be under 50 words.> --- Analysis --- **Response A:** - Criterion 1 [Hard Rule]: Justification: <...> - Criterion 2 [Hard Rule]: Justification: <...> - Criterion 3 [Principle]: Justification: <...> - ... (and so on for all other criteria) **Response B:** - Criterion 1 [Hard Rule]: Justification: <...> - Criterion 2 [Hard Rule]: Justification: <...> - Criterion 3 [Principle]: Justification: <...> - ... (and so on for all other criteria) --- Final Judgment --- Justification: <...> Winner: <Response / Response B> 29 Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment Task to Evaluate: Instruction: {instruction} Rubric: {rubric} Response A: {response_a} Response B: {response_b}"
        }
    ],
    "affiliations": [
        "Emory University",
        "Georgia Institute of Technology",
        "Purdue University",
        "University at Albany"
    ]
}