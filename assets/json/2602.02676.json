{
    "paper_title": "AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process",
    "authors": [
        "Xintong Zhang",
        "Xiaowen Zhang",
        "Jongrong Wu",
        "Zhi Gao",
        "Shilin Yan",
        "Zhenxin Diao",
        "Kunpeng Gao",
        "Xuanyan Chen",
        "Yuwei Wu",
        "Yunde Jia",
        "Qing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 6 7 6 2 0 . 2 0 6 2 : r AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Xintong Zhang1,2, Xiaowen Zhang2,3, Jongrong Wu2, Zhi Gao1,2,4,, Shilin Yan5, Zhenxin Diao1, Kunpeng Gao1, Xuanyan Chen1, Yuwei Wu1,4, Yunde Jia4 Qing Li2, 1Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology 2State Key Laboratory of General Artificial Intelligence, BIGAI 3Xidian University 4Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University 5Alibaba Group Core contribution, Project supervisor, Equal contribution, Corresponding authors Project Page: https://adaptmmbench.github.io/"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Adaptive multimodal reasoning has emerged as promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures. Correspondence to: Zhi Gao <gaozhi@bit.edu.cn>, Yuwei Wu <yuwei.wu@bit.edu.cn>, Qing Li <liqing@bigai.ai>. Preprint. February 4, 2026. Figure 1. Comparative Analysis of Accuracy, Reasoning Mode Selection, and Reasoning Process. Closed-source models achieve stronger performance in accuracy and mode selection, while reasoning process quality is analyzed on open-source models due to limited access to closed-source reasoning traces. Vision Language Models (VLMs) have evolved from passive observers of static visual inputs to proactive models capable of dynamic information seeking. This evolution makes shift from direct perception and textual chain-ofthought (CoT) to the tool-augmented visual reasoning (i.e., thinking with images) (OpenAI, 2025), where models iteratively manipulate the visual content using visual tools, such as zoom-in, and enhancement(e.g., contrast and rotation) to acquire more visual information and resolve ambiguities. (Zheng et al., 2025; Hu et al., 2024). However, this capability introduces significant computational redundancy. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Lacking mechanism to discern task necessity, models often fall into tool-invocation trap, applying intensive visual tools to tasks solvable by direct perception or text reasoning. Consequently, adaptive multimodal reasoning is promising direction for VLMs, which balances the necessity of such tool-augmented visual reasoning against text reasoning (Lin et al., 2025b; Wang et al., 2025a). Despite the emergence of adaptive multimodal reasoning formulations, evaluating adaptive multimodal reasoning remains an open problem. Most existing evaluations rely on token-level reduction, coarse tool-call statistics, or final accuracy as proxies for adaptive intelligence. While intuitive, these metrics primarily reflect observable outcomes rather than evaluating the internal reasoning process itself. In particular, they fail to disentangle adaptive reasoning mode selection from subsequent reasoning execution. The ability to select an appropriate reasoning mode is crucial, as it reflects difficulty-aware meta-cognition. From the data perspective, adaptive reasoning is commonly evaluated on domain-specific logic tasks (e.g., math and knowledge reasoning) or high-resolution perception benchmarks (Wu & Xie, 2024; Wang et al., 2025c; Zhang et al., 2025b). These benchmarks lack hierarchy of difficulty, limiting their effectiveness in evaluating adaptive reasoning. Recent efforts such as Omni-AutoThink (Yang et al., 2025a) attempt to quantify adaptiveness through thinking rates under predefined difficulty levels, as shown in Fig. 2. While this encourages increased reasoning effort on harder tasks, predefined difficulty levels are not universally applicable across models, leading to evaluation bias. Moreover, existing evaluations largely overlook reasoning process quality, losing detailed analyzes to guide future multimodal reasoning research. To bridge these gaps, we propose AdaptMMBench to quantify adaptive multimodal reasoning in VLMs. AdaptMMBench includes 1420 samples across five domains: realworld, OCR, GUI, knowledge, and math. Each domain contains both text-only solvable tasks and complex scenarios of varying difficulties requiring proactive visual tool invocation. AdaptMMBench enables separate evaluation of adaptive reasoning mode selection and the reasoning process. Specifically, it adopts the Matthews Correlation Coefficient (MCC) to evaluate mode selection by dynamically identifying task difficulties based on model performance boundaries. For reasoning process evaluation, we assess key step coverage, tool invocation effectiveness, and efficiency to measure reasoning coherence, tool correctness, and computational cost alongside accuracy. We evaluate closed-source and open-source VLMs on AdaptMMBench, results shown in Fig. 1. Experiments reveal relatively weak correlation between adaptive mode selection performance and final accuracy, whereas closed-source and larger models demonstrate stronger adaptive capability. By 2 contrast, key step coverage correlates more closely with accuracy, and tool execution effectiveness varies substantially across models. Our contributions are summarized as follows. (1) We propose the AdaptMMBench to quantify the adaptive multimodal reasoning capabilities of VLMs, which contains 1420 samples across five domains with detailed reasoning annotations for comprehensive evaluations. (2) We establish suite of metrics for adaptive multimodal reasoning, which disentangle the adaptive capability from other model capabilities and assess three aspects of the reasoning process, providing detailed and in-depth evaluations. (3) We analyze current VLMs from the perspective of adaptive reasoning, highlighting that the relationship between mode selection performance and final accuracy is relatively small, while closed-source and larger models exhibit stronger adaptive behavior. In contrast, key step coverage correlates more closely with accuracy, and tool execution effectiveness varies substantially across models. 2. Related Work 2.1. Multimodal Reasoning in VLMs Early VLMs predominantly rely on text-only reasoning over fixed visual encodings, imposing first-glance bottleneck that limits access to fine-grained visual details (Lu et al., 2023; Huang et al., 2025; Zhang et al., 2023; Yang et al., 2025b). Recent advanced models, including GPT5 (Singh et al., 2025), Qwen3-VL (Bai et al., 2025), and InternVL (Zhu et al., 2025) have shifted multimodal reasoning from passive visual interpretation toward active, tool-augmented information seeking. Under this thinking with images paradigm, models acquire additional visual information through mechanisms such as multi-turn visual search (OpenAI, 2025; Zheng et al., 2025), region zoomin (Wang et al., 2025b; Lai et al., 2025), and self-generated visual cues (Li et al., 2025a; Chern et al., 2025). In parallel, adaptive multimodal reasoning models have emerged to selectively invoke tools, trading off between text-only and tool-based reasoning to improve inference efficiency (Lin et al., 2025b; Zhang et al., 2025a; Wang et al., 2025a; Li et al., 2025d;e). More advanced systems further incorporate agentic workflows and code generation to support precise execution (Hong et al., 2025; Zhang et al., 2025c). While these works emphasize improvements in precision and efficiency, they offer limited evaluation of whether models invoke tool-based reasoning until text-only reasoning is insufficient, avoiding unnecessary computational overhead. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 2. Illustration of our model-specific difficulty evaluation. Existing methods rely on static difficulty levels, while difficulty is inherently model-dependent. 2.2. Benchmarks for VLMs 3.1. Data Formulation Traditional VLM benchmarks mainly assess multimodal reasoning in structured domains with coarse visual content, such as chart understanding (Mathew et al., 2021; Masry et al., 2022), mathematical problem solving (Lu et al., 2023; Xiao et al., 2024), and other general-purpose VQA (Liu et al., 2024; Chen et al., 2024). MME-CoT (Jiang et al., 2025) further evaluates the correctness of the text reasoning process. As VLM capabilities improve, more recent benchmarks (Wu & Xie, 2024; Wang et al., 2025c) introduce higher-resolution images to better reflect complex conditions. Building on this trend, benchmarks such as VisualProbe (Lai et al., 2025), InSight-o3 (Li et al., 2025b), and TIR-Bench (Li et al., 2025c) emphasize fine-grained visual understanding and active visual reasoning through operations like region zoom-in and iterative exploration, implicitly requiring models to thinking with images. In parallel, generative benchmarks including VTBench (Lin et al., 2025a) and AuxSolidMath (Guo et al., 2025a) evaluate multimodal reasoning via self-produced auxiliary visual cues, extending visual reasoning beyond the information directly available in the input image. However, these visualgrounded benchmarks (Li et al., 2025b; Lin et al., 2025a) largely focus on task accuracy, overlooking the problem of redundant computation where models use visual tools for tasks already solvable through text-only reasoning. Relying solely on token reduction for efficiency evaluation fails to evaluate the adaptive decisions and the reasoning quality. 3. AdaptMMBench AdaptMMBench focuses on two perspectives: adaptive reasoning mode selection and reasoning process. Formally, AdaptMMBench is constructed as set of samples = {di}N i=1, where each data sample di is defined as: di = (I, Q, A, E, K). (1) Here, RHW 3 denotes the input image, is the textual query, and is the ground-truth answer. To support adaptive evaluation, we provide the visual tool annotation that specifies how essential visual information can be obtained, including the coordinates of target regions as well as the required image transformations such as rotation and contrast adjustment. = {k1, . . . , km} is an ordered sequence of human-verified key reasoning steps describing the solution path from (I, Q) to A. During inference, the model only observes the image and the query Q. Acquiring the visual information specified by requires invoking visual tool t(I, τ ) via code execution or function calls, where denotes tool from the predefined toolset and τ its execution arguments. 3.2. Data Collection AdaptMMBench encompasses 1,420 samples spanning five domains: real-world, OCR, GUI, math, and knowledge, enabling comprehensive evaluation of adaptive reasoning across diverse scenarios, as detailed in Fig. 3. To ensure that AdaptMMBench contains both samples solvable via text-only reasoning and samples that require visual tool invocation under adaptive reasoning, we deliberately construct the dataset with diverse difficulty levels during data collection. One subset consists of samples solved by Qwen2.5-VL-7B under text-only reasoning. second subset includes samples that Qwen2.5-VL-7B fails but can be 3 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 3. An Overview of AdaptMMBench. The benchmark contains data from five domains. Each domain includes samples requiring zoom-in and enhancement tools. We annotate zoom-in regions, enhancement arguments, and key reasoning steps. is made by the model itself, as detailed in Sec. 4. Building on prior adaptive reasoning methods (Chern et al., 2025; Zhang et al., 2025c; Zhao et al., 2025), AdaptMMBench evaluates diverse visual tools beyond zoom-in, including geometric transformations for orientation correction and photometric adjustments for visual enhancement. During data construction, these requirements are induced via controlled distortions such as changes in contrast, brightness, and orientation, with zoom-in and transformation samples with ratio of 5:2. We further include 120 samples requiring auxiliary-line generation, suggesting that reasoning with self-generated images constitutes an important extension of the think-with-images paradigm. Figure 4. Domains and category of AdaptMMBench. 3.3. Annotation and Quality Control Visual Tool & Key Step Annotation. solved by Qwen3-VL-235B based on adaptive reasoning. small portion remains unsolved even by Qwen3-VL-235B. The relative proportions of these three subsets are approximately 24%, 70%, and 6%. Notably, these subsets are introduced only to ensure difficulty diversity and do not determine the ground truth reasoning mode during evaluation. The reasoning mode selection label in adaptive mode We collect initial data from existing benchmarks, with annotators providing bounding-box annotations for key regions, while visual enhancement annotations are generated through predefined transformations. Distortion parameters are constrained to maintain recoverability. GPT-5 is used to generate key reasoning steps K, which are manually verified. These components form annotated quintuples 4 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 5. Evaluation pipeline for mode selection and reasoning process. (I, Q, A, E, K). 4.2. Adaptive Mode Selection Evaluation Quality Control. Benchmark quality is ensured through multi-stage verification pipeline. First, three independent annotators cross-validate each QA pair to remove ambiguity and verify correctness. Annotated image transformations and generated key reasoning steps are then reviewed by additional annotators for precision. Inaccurate instances are iteratively refined or re-annotated. This process ensures high-fidelity ground truth with precise pixel-level annotations and reliable key reasoning steps for comprehensively evaluating adaptive reasoning. More statistical information of AdaptMMBench can be found in Appendix A. 4. Evaluation Strategy 4.1. Evaluation Modes Following the formulation defined in Sec. 3.1, we define three evaluation modes to systematically assess the models adaptive reasoning capabilities. Text-Reasoning Mode: Given (I, Q), the model relies solely on text reasoning over the given image, without invoking active visual transformations, providing baseline for assessing tool necessity. Adaptive Reasoning Mode: Given (I, Q), the model adaptively selects between text-only reasoning and tool-augmented visual reasoning with tools. It generates reasoning trajectory and records all tool invocation parameters, enabling evaluation of both its ability to decide when tool usage is required and the correctness of the reasoning process. Oracle-Visual Mode: Given (I, Q, IE), where IE denotes gold-standard visual evidence from annotation E, the model performs text-only reasoning over the provided visual evidence, providing an upper-bound performance estimate under perfect visual acquisition. Adaptive intelligence depends on models ability to assess whether its current information is sufficient to solve task. Consequently, the appropriateness of reasoning mode should be evaluated independently of answer correctness. Under this principle, the necessity of tool invocation is determined by the outcome of text-only reasoning. If task can be solved using text reasoning alone, it is labeled as Tool-Redundant, indicating that visual tool invocation is unnecessary and may introduce noise. Conversely, tasks that cannot be solved via text-only reasoning are labeled as Tool-Required, indicating that visual tool invocation is necessary to obtain additional information. This categorization defines the mode selection labels used in our evaluation, as detailed in Fig. 5. Accordingly, tool invocation decisions are evaluated using confusion matrix: TP denotes ToolRequired cases where the model invokes tools, FN denotes Tool-Required cases where the model does not invoke tools, TN denotes Tool-Redundant cases where the model selects text-only reasoning, and FP denotes Tool-Redundant cases where the model unnecessarily invokes tools. Matthews Correlation Coefficient (MCC). In adaptive mode selection, the proportions of tool-redundant and toolrequired cases are model-dependent, leading to varying degrees of class imbalance in the resulting confusion matrix. To ensure robust evaluation, we adopt the MCC, MCC = N N (cid:112)(T + )(T + )(T + )(T + ) + ϵ , (2) where ϵ is small constant for numerical stability. MCC ranges from [1, 1], with 1 indicating perfect agreement with the optimal mode selection, 0 denoting the chance-level performance, and 1 indicating complete misalignment. Adaptive Label Robustness. We analyze the effects of minor prompt variations on text and adaptive reasoning. 5 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Only 0.02% of samples show inconsistent outcomes between text reasoning mode and text-only reasoning in adaptive mode. This indicates that the performance difference is stable under prompt variations, and adaptive reasoning rarely degrades text-solvable samples. 4.3.3. REASONING EFFICIENCY Efficiency is evaluated in terms of token numbers, reasoning turns, and tool usage frequency, collectively capturing the conciseness of reasoning and the computational cost of adaptive execution. 4.3. Reasoning Process Evaluation While MCC measures the quality of mode selection, it does not assess the validity of the reasoning process. Models may produce correct answers despite logical errors or improper tool usage. To address this limitation, we introduce three process-oriented metrics to evaluate reasoning coherence and tool execution fidelity. reasoning trajectory is formalized as an interleaved sequence of reasoning steps and tool invocations: = {(s1, t1), (s2, t2), . . . , sn}, (3) where si is the reasoning at step and ti represents the corresponding tool invocation. The trajectory terminates at the final reasoning step sn, and produces the answer. 4.3.1. KEY STEPS COVERAGE Following the evaluation paradigm of (Jiang et al., 2025), we assess whether models reasoning chain {si}n i=1 covers the essential human-annotated key steps defined in Sec. 3.1. We employ GPT-5 as an evaluator to identify the presence of these key steps within the generated reasoning, and define the key step coverage as: KCoverage = 1 max j (cid:89) i=1 (cid:104) ki GPT-5 (cid:105) {s1, . . . , sn} . (4) This metric measures how far the models reasoning progresses along the key steps. Rather than penalizing skipped or compressed steps, KCoverage captures the maximum extent to which the reasoning aligns with the solution structure, allowing different reasoning styles and reflecting how close the model comes to correct solution. 4.3.2. TOOL EXECUTION EFFECTIVENESS To assess the precision of tool usage, we evaluate whether each tool invocation is semantically appropriate for its corresponding reasoning step and free of execution errors. The tool effectiveness is defined as: Effecttool ="
        },
        {
            "title": "1\nNtool",
            "content": "Ntool(cid:88) i=1 validGPT-5(ti si), (5) where Ntool denotes the total number of tool invocations, ti is the tool invoked at step i, and validGPT-5() {0, 1} is semantic validity judgment provided by GPT-5. Table 1. Evaluation of mode selection performance across models. We report TP, FP, TN, FN, and the MCC to assess meta-cognitive calibration in adaptive reasoning mode. Best and second-best scores in each category are highlighted in blue and green . Model TP FP TN FN MCC Open-Source Models PixelReasoner Deepeyes Thyme PyVision Deepeyes v2 AdaptVision Qwen3-vl-8B-Instruct Qwen3-vl-32B-Instruct Qwen3-vl-235B-Instruct 280 662 20 540 623 385 328 348 286 196 638 20 405 676 279 381 646 434 0 655 231 1 375 351 245 487 390 0 605 124 0 261 240 61 90 Closed-Source Models GPT-5 Gemini-3-Pro 482 284 392 376 296 50 17 0.11 0.00 0.01 0.20 0.03 0.17 0.06 0.14 0.26 0.41 0.24 Table 2. Comprehensive evaluation of reasoning process, including key step coverage (Key Step Cov.), tool effectiveness (Tool Effect.), and efficiency. This assesses the logical rigor of the reasoning paths alongside their computational efficiency. Model Key Step Cov. (%) Tool Effect. (%) Efficiency Steps Tools Tokens PixelReasoner Deepeyes Thyme PyVision Deepeyes v2 AdaptVision Qwen3-vl-8B Qwen3-vl-32B Qwen3-vl-235B 76.02 75.56 77.14 77.43 75.14 72.60 78.40 83.79 84. 56.51 50.99 56.50 62.02 56.79 81.70 91.62 92.98 89.64 1.37 2.00 1.05 2.76 2.09 1.51 1.76 2.42 2.04 0.37 1.68 0.06 1.76 1.09 0.51 1.20 1.44 1.04 4229.00 7601.45 6708.47 2481.00 6918.90 4175.96 8282.40 7725.99 7531.95 5. Experiments We conduct comprehensive quantitative evaluation of adaptive reasoning on AdaptMMBench, focusing on three complementary dimensions: (i) reasoning mode selection capability, (ii) quality and efficiency of reasoning process, and (iii) accuracy across reasoning modes. 5.1. Experiment Setting We evaluate set of VLMs to establish baselines for AdaptMMBench. For closed-source models, we select GPT5 (Singh et al., 2025) and Gemini3 (Google DeepMind, 2025). For open-source models, we include the Qwen3-VL family (Bai et al., 2025) at multiple scales (8B, 32B, and 235B). In addition, we evaluate several specialized adaptive reasoning models, including DeepEyes (Zheng et al., 2025; 6 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Table 3. Accuracy across different domains under three reasoning modes. Results are reported on 1,300 AdaptMMBench samples, with auxiliary-line tasks evaluated separately. * indicates that the model supports enhancement operations. w/o enh. denotes results without enhancement-based data transformations (e.g., rotation and contrast). Model Mode Real-world OCR GUI Knowledge Math (w/o aux) Overall Accuracy w/o enh. All w/o enh. All w/o enh. All w/o enh. All w/o enh. All w/o enh. All PixelReasoner Deepeyes Thyme PyVision Deepeyes v2 AdaptVision Qwen3-vl -8B-Instruct Qwen3-vl -32B-Instruct Qwen3-vl -235B-Instruct GPT-5 Gemini-3-Pro Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle 42.08 53.75 70.83 50.00 54.17 67. 55.00 60.83 75.83 40.00 50.83 94.58 58.75 61.25 75.42 45.83 49.17 74.17 56.25 57.50 83.75 55.83 63.33 87. 59.58 64.17 87.92 46.67 70.83 97.92 59.17 80.42 87.50 38.00 51.33 67.67 48.33 53.00 66.33 51.67 58.00 73. 38.00 47.33 84.67 55.00 56.33 74.33 43.00 46.33 70.67 50.00 52.33 78.00 51.00 57.33 81.67 52.33 56.33 80. 45.67 64.67 88.00 53.33 74.00 80.33 58.75 61.25 72.92 50.42 57.08 62.92 58.33 61.67 66.67 62.08 72.50 79. 58.33 59.58 70.83 62.92 64.17 71.25 64.17 68.33 79.17 82.92 85.42 92.92 81.25 82.08 90.83 77.08 89.17 90. 87.92 89.58 92.59 Open-Source Models 58.33 62.33 75.00 51.33 57.67 66.67 57.67 60.00 69.00 60.33 70.67 80. 58.33 57.67 74.33 61.67 64.00 76.00 62.33 65.67 80.67 78.00 82.67 92.00 78.00 80.00 91.33 48.33 61.67 66. 52.50 51.25 61.67 50.00 55.83 63.75 75.00 77.92 90.00 54.58 57.08 68.33 48.75 52.92 62.92 57.50 65.83 68. 76.67 77.00 81.67 84.58 87.08 97.08 48.00 53.00 58.67 53.33 52.33 64.00 49.67 53.67 64.67 70.00 73.33 86. 54.67 55.67 69.33 47.67 54.33 65.67 54.00 59.67 67.00 71.33 70.00 75.67 76.67 80.00 91.33 Closed-Source Moddels 73.67 86.33 90.00 87.33 89.67 93.00 79.17 88.75 91.67 90.83 92.08 94.17 74.33 85.33 88.00 86.67 90.00 92. 56.88 58.13 65.62 50.62 53.12 66.88 55.00 58.75 64.38 35.62 58.75 60.00 48.12 59.38 65.00 54.37 60.62 71. 72.50 78.75 80.62 83.75 84.38 96.25 83.12 93.75 96.25 60.00 92.50 96.88 85.00 92.50 92.50 55.50 59.00 67. 48.50 50.00 69.00 51.00 51.00 67.50 34.50 52.50 62.50 47.50 53.50 65.50 54.00 54.50 73.00 66.00 71.50 81. 77.00 79.00 95.00 78.50 86.50 96.50 54.50 86.00 94.50 83.00 93.50 94.00 46.25 51.88 62.50 43.12 55.00 61. 51.88 51.88 63.12 35.00 35.00 58.13 41.88 50.00 63.12 45.00 49.38 68.75 55.62 69.38 80.00 76.25 81.88 90. 83.12 85.62 96.88 44.38 76.88 90.62 81.88 93.12 93.75 43.00 50.00 64.00 41.50 50.50 62.00 48.00 50.00 63. 31.00 31.00 53.50 39.00 49.00 64.00 44.50 45.00 69.00 50.50 62.00 80.00 68.00 73.50 87.50 73.00 76.50 94. 39.00 71.00 83.00 75.50 87.00 91.00 50.29 55.19 65.96 49.71 54.13 64.04 54.13 58.17 67.21 51.73 60.87 79. 53.46 57.88 69.23 51.63 55.29 69.71 60.77 67.02 78.17 74.33 77.79 89.04 77.60 81.44 93.37 62.88 83.46 93. 80.58 89.04 91.92 48.46 55.23 66.69 49.15 53.08 65.69 51.92 55.15 67.85 48.92 57.00 75.85 52.08 54.92 70. 50.31 53.31 70.85 56.31 61.54 76.85 68.54 71.92 85.62 71.08 75.00 90.08 59.08 78.69 88.69 76.85 86.31 89. Hong et al., 2025), PixelReasoner (Wang et al., 2025b), Thyme (Zhang et al., 2025c), PyVision (Zhao et al., 2025), and AdaptVision (Lin et al., 2025b). For all evaluated models, we follow the implementation details provided in their official codebases. For evaluations under different reasoning modes, we apply unified and minimal modification to the prompts, as detailed in the Appendix D. 5.2. Adaptive Reasoning Mode Selection Capability closer analysis of mode selection capability reveals clear differences across models. As shown in Table 1 and Table 3, mode selection capability does not exhibit strong correlation with final task accuracy. For example, AdaptVision achieves relatively modest accuracy, yet demonstrates strong mode selection behavior with an MCC of 0.17, outperforming all other models trained on Qwen2.5-VL-7B backbones. In contrast, GPT-5 attains the highest MCC of 0.41, demonstrating good mode selection capability. Model scaling improves mode selection. Table 1 demonstrates clear scaling trend within the Qwen3-VL family, where larger models exhibit more reliable mode selection. This pattern suggests that increased model capacity contributes to improved calibration when determining whether tool-based reasoning is necessary. Similarly, large-scale closed-source models outperform open-source models. Imbalanced mode selection behavior is observed in some models. Several specialized adaptive models exhibit imbalanced mode selection behavior, either invoking tools excessively or rarely. For example, Deepeyes v2 invokes tools in all but one of the 1,300 samples in AdaptMMBench, whereas Thyme triggers tool usage in only about 3% of 7 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process cases. Such imbalanced patterns are associated with lower mode selection performance, despite competitive accuracy. Table 4. Experimental results on geometric auxiliary-line problems across different reasoning modes. 5.3. Quality and Efficiency of the Reasoning Process Since intermediate reasoning steps of closed-source models (e.g., GPT-5 and Gemini-3-Pro) are not accessible, we restrict process-level analysis to open-source models. Table 2 evaluates key step coverage, tool effectiveness, and efficiency. Consistent with Table 3, key step coverage shows similar ranking, with Qwen3-VL-235B among the top models. Larger models also demonstrate stronger tool effectiveness, better aligning tool usage with reasoning intent. Tool effectiveness varies with models. Qwen3 family shows strong performance, while some smaller models are less effective. This may stem from repeated or unnecessary tool calls, as well as code-based tool invocation in Deepeyes v2, Thyme, and PyVision, which introduces more complexity than the function-call interface used by Qwen models. Token usage is not positively correlated with steps or tool calls. Considering efficiency, token usage varies across models and does not correspond to the number of reasoning steps or tool calls. For example, Thyme uses the fewest steps and tool invocations, yet consumes more tokens than PyVision, which has the most steps. This shows that fewer steps or tool calls do not necessarily reduce token cost. 5.4. Accuracy across Reasoning Modes We analyze model performance across different reasoning modes, including text-only, adaptive, and oracle tool reasoning. The oracle tool reflects upper-bound performance. As shown in Table 3, adaptive reasoning consistently improves accuracy over text-only baselines for all evaluated models. Significant performance gap between adaptive and oracle reasoning. Although adaptive reasoning yields clear gains, oracle tool reasoning reveals substantial remaining headroom. For example, GPT-5 improves from 78.69% under adaptive reasoning to 88.69% in the oracle setting, with similar trends observed in open-source models. These results indicate that current performance is mainly limited by imperfect tool invocation rather than reasoning capability. Moreover, the high oracle-visual accuracy of 90.08% indicates the reliability and accuracy of our visual annotations. Generation-Based Tools Are Beneficial for Certain Tasks. We conduct an exploratory analysis on self-generated auxiliary-line tasks as shown in Table 4. As current opensource models cannot generate visual representations, adaptive reasoning shows limited or negative gains over textonly reasoning, while oracle-visual inputs bring substantial improvements. This highlights the importance of visual generation for future adaptive reasoning models. Model Text Acc Adaptive Acc Oracle Acc Open-Source VLMs Thyme PyVision Deepeyes v2 Qwen3-vl-8B Qwen3-vl-32B Qwen3-vl-235B 21.67 15.83 19.17 50.00 63.33 62.50 21.67 29.17 19.17 46.67 58.33 68. Closed-Source Models Gemini-3-Pro GPT-5 85.00 75.00 78.33 86.67 24.17 32.50 25.83 62.50 79.17 84.17 94.17 89. 5.5. Error Analysis Figure 6. Error Analysis on GPT-5. In this section, we analyze the causes of incorrect predictions made by GPT-5 under the adaptive mode to understand the gap between adaptive reasoning and oracle-visual mode. As shown in Fig. 6, most errors are related to tool usage. Specifically, 42.3% of the errors stem from visual reasoning failures, such as zoom-in into incorrect regions or applying wrong image transformations. Another 7.3% of errors occur even when visual reasoning is correct. Since these samples are solvable in the oracle-visual mode, this suggests that intermediate images in multi-step reasoning may introduce visual noise affecting the final prediction. In addition, 8.3% of errors are caused by incorrect mode selection, where text reasoning is sufficient but the model unnecessarily invokes tools, leading to degraded performance. For cases without tool usage, forcing tool invocation corrects 7.0% of the errors, while 6.3% remain incorrect. The remaining 28.8% of errors exceed the capability of the GPT-5 model. 6. Conclusion In this paper, we present AdaptMMBench, benchmark for evaluating adaptive multimodal reasoning in VLMs. AdaptMMBench covers diverse domains and reasoning AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process scenarios, and enables model-dependent identification of tool-redundant and tool-required cases by comparing performance across reasoning modes. We further propose set of metrics that assess mode selection quality, reasoning process quality, and efficiency. Through systematic evaluation of state-of-the-art models, we observe that high accuracy does not necessarily imply strong reasoning mode selection capability. The substantial performance gap between adaptive and oracle-visual reasoning further suggests that performance is often limited by suboptimal tool invocation. This highlights adaptive tool selection as key challenge for future multimodal reasoning models."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37: 2705627087, 2024. Chern, E., Hu, Z., Chern, S., Kou, S., Su, J., Ma, Y., Deng, Z., and Liu, P. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. Google DeepMind."
        },
        {
            "title": "A New Era",
            "content": "gence with Gemini 3, //blog.google/products-and-platforms/ products/gemini/gemini-3/. 2025. of IntelliURL https: Guo, S., Pang, L., Wang, X., Wang, Y., Shen, H., and Zhang, J. Geovlmath: Enhancing geometry reasoning in visionlanguage models via cross-modal reward for auxiliary line creation. arXiv preprint arXiv:2510.11020, 2025a. Guo, Z., Zhang, R., Chen, H., Gao, J., Jiang, D., Wang, 9 J., and Heng, P.-A. Sciverse: Unveiling the knowledge comprehension and visual reasoning of lmms on multimodal scientific problems. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 19683 19704, 2025b. Hong, J., Zhao, C., Zhu, C., Lu, W., Xu, G., and Yu, X. Deepeyesv2: Toward agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025. Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. Huang, W., Jia, B., Zhai, Z., Cao, S., Ye, Z., Zhao, F., Xu, Z., Hu, Y., and Lin, S. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Jiang, D., Zhang, R., Guo, Z., Li, Y., Qi, Y., Chen, X., Wang, L., Jin, J., Guo, C., Yan, S., et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Lai, X., Li, J., Li, W., Liu, T., Li, T., and Zhao, H. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969, 2025. Li, C., Wu, W., Zhang, H., Xia, Y., Mao, S., Dong, L., Vulic, I., and Wei, F. Imagine while reasoning in space: arXiv preprint Multimodal visualization-of-thought. arXiv:2501.07542, 2025a. Li, K., Yao, L., Wu, J., Yu, T., Chen, J., Bai, H., Hou, L., Hong, L., Zhang, W., and Zhang, N. L. Insight-o3: Empowering multimodal foundation models with generalized visual search. arXiv preprint arXiv:2512.18745, 2025b. Li, M., Zhong, J., Zhao, S., Zhang, H., Lin, S., Lai, Y., Wei, C., Psounis, K., and Zhang, K. Tir-bench: comprehensive benchmark for agentic thinking-with-images reasoning. arXiv preprint arXiv:2511.01833, 2025c. Li, X., Li, X., Gao, J., Pi, R., Hu, S., and Zhang, W. Look less, reason more: Rollout-guided adaptive pixel-space reasoning. arXiv preprint arXiv:2510.01681, 2025d. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Li, Z., Zhao, Y., Zhang, J., Wang, S., Yao, Y., Zhao, R., Song, J., Zheng, B., and Wei, Z. Mixture-of-visualthoughts: Exploring context-adaptive reasoning mode selection for general visual reasoning. arXiv preprint arXiv:2509.22746, 2025e. Lin, H., Geng, T., Xu, Z., and Zhao, W. Vtbench: Evaluating visual tokenizers for autoregressive image generation. arXiv preprint arXiv:2505.13439, 2025a. Lin, Z., Liu, Y., Yang, Y., Tao, L., and Ye, D. Adaptvision: Efficient vision-language models via adaptive visual acquisition. arXiv preprint arXiv:2512.03794, 2025b. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Masry, A., Do, X. L., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pp. 22632279, 2022. Masry, A., Islam, M. S., Ahmed, M., Bajaj, A., Kabir, F., Kartha, A., Laskar, M. T. R., Rahman, M., Rahman, S., Shahmohammadi, M., et al. Chartqapro: more diverse and challenging benchmark for chart question answering. arXiv preprint arXiv:2504.05506, 2025. Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: In Proceedings dataset for vqa on document images. of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. o"
        },
        {
            "title": "Openai\nTechnical",
            "content": "system OpenAI. 2025. card. https://cdn.openai.com/pdf/ URL 2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. o4-mini OpenAI, and report, Qiao, R., Tan, Q., Dong, G., MinhuiWu, M., Sun, C., Song, X., Wang, J., Gongque, Z., Lei, S., Zhang, Y., et al. Wemath: Does your large multimodal model achieve humanlike mathematical reasoning? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2002320070, 2025. 10 Shi, C., Yu, Z., Gao, Z., Feng, R., Liu, E., Wu, Y., Jia, Y., Xiang, L., He, Z., and Li, Q. Gui knowledge bench: Revealing the knowledge gap behind vlm failures in gui tasks. arXiv preprint arXiv:2510.26098, 2025. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Wang, C., Feng, K., Chen, D., Wang, Z., Li, Z., Gao, S., Meng, M., Zhou, X., Zhang, M., Shang, Y., et al. Adatooler-v: Adaptive tool-use for images and videos. arXiv preprint arXiv:2512.16918, 2025a. Wang, H., Su, A., Ren, W., Lin, F., and Chen, W. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025b. Wang, W., Ding, L., Zeng, M., Zhou, X., Shen, L., Luo, Y., Yu, W., and Tao, D. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 79077915, 2025c. Wu, J., Yin, W., Jiang, Y., Wang, Z., Xi, Z., Fang, R., Zhang, L., He, Y., Zhou, D., Xie, P., et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025. Wu, P. and Xie, S. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Xiao, Y., Sun, E., Liu, T., and Wang, W. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Xu, W., Wang, J., Wang, W., Chen, Z., Zhou, W., Yang, A., Lu, L., Li, H., Wang, X., Zhu, X., et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. Yang, D., Liu, S., Wang, D., Wang, Y., Wan, G., and Meng, H. Omni-autothink: Adaptive multimodal reasoning via reinforcement learning. arXiv preprint arXiv:2512.03783, 2025a. Yang, S., Han, C., Luo, S., and Hovy, E. Magic-vqa: Multimodal and grounded inference with commonsense knowledge for visual question answering. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1696716986, 2025b. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Qiao, Y., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Zhang, X., Gao, Z., Zhang, B., Li, P., Zhang, X., Liu, Y., Yuan, T., Wu, Y., Jia, Y., Zhu, S.-C., et al. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025a. Zhang, Y., Zhang, H., Tian, H., Fu, C., Zhang, S., Wu, J., Li, F., Wang, K., Wen, Q., Zhang, Z., Wang, L., and Jin, R. Mme-realworld: Could your multimodal LLM challenge high-resolution real-world scenarios that are difficult for humans? In The Thirteenth International Conference on Learning Representations (ICLR), 2025b. Zhang, Y.-F., Lu, X., Yin, S., Fu, C., Chen, W., Hu, X., Wen, B., Jiang, K., Liu, C., Zhang, T., et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025c. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Zhao, S., Zhang, H., Lin, S., Li, M., Wu, Q., Zhang, K., and Wei, C. Pyvision: Agentic vision with dynamic tooling. arXiv preprint arXiv:2507.07998, 2025. Zheng, Z., Yang, M., Hong, J., Zhao, C., Xu, G., Yang, L., Shen, C., and Yu, X. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian, H., Duan, Y., Su, W., Shao, J., et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 11 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process A. More Data Details A.1. Data Source Distribution Real-World VQA. We target high-resolution natural scenes by leveraging VisualProbe (Lai et al., 2025) for small-object search and custom SA-1B (Kirillov et al., 2023) subset for large-scale object reasoning. Queries are explicitly designed to evaluate attributes, spatial, counting, physical state and text-recognition across distinct scales. Moreover, statistics of bounding box sizes are presented in Fig. 7. Text-Rich VQA. This domain covers diverse charts, tables, and documents. We aggregate standard samples from ChartQA (Masry et al., 2022) and DocVQA (Mathew et al., 2021) with high-resolution challenges from ChartQA-Pro (Masry et al., 2025), MM-RealWorld (Zhang et al., 2025b), and Insight-o3 (Li et al., 2025b) to demand precise visual inspection and deep reasoning. Math VQA. To assess mathematical reasoning in visual contexts, we consolidate high-quality samples from spectrum of established benchmarks including MathVista (Lu et al., 2023), MathVerse (Zhang et al., 2024), We-Math (Qiao et al., 2025), LogicVista (Xiao et al., 2024), Visulogic (Xu et al., 2025), AuxSolidMath, and VTBench (Lin et al., 2025a). GUI VQA. We construct cross-platform suite covering iOS, Android, Web, macOS, Windows, and Linux. This is achieved by integrating generic datasets like GUI-Knowledge-Bench (Shi et al., 2025) and MMBench-GUI (Liu et al., 2024) with domain-specific samples from WebWalker (Wu et al., 2025). Knowledge VQA. This category is sourced from disciplinary benchmarks across Physics, Chemistry, and Biology. Specifically, we incorporate expert-level samples from MMMU (Yue et al., 2024) and SciVerse (Guo et al., 2025b) to evaluate the models ability to integrate specialized domain knowledge with visual reasoning. Figure 7. Statistics of bounding box sizes in AdaptMMBench. Figure 8. Overview of the data curation and annotation process. A.2. Data Construction Pipeline The construction workflow of AdaptBench is depicted in Figure 8. Initially, raw data is partitioned based on reasoning complexity, specifically separating tasks that necessitate external tool intervention from those amenable to text-only inference. 12 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process To further challenge model adaptability, we augment the visual inputs with diverse transformations, thereby mandating finegrained perception. Finally, we implement multi-stage verification pipeline involving expert annotation of transformation logic, key reasoning steps, and rigorous human review, ensuring high-fidelity ground truth for the final benchmark. B. Transform Results Tab. 5 serves as supplementary detailed analysis to the main experimental results presented in Tab. 3. While Tab. 3 reports the model performance on the original dataset (denoted as w/o enh.) and the aggregated dataset (All), it does not explicitly isolate the performance on the transformed data. To provide comprehensive view of model robustness against data variations, Tab. 5 exclusively presents the accuracy results for the Transformed subset across all five domains. The highlighting scheme follows the same convention as the main table to facilitate direct comparison of the autonomous decision-making capabilities in the Adaptive mode. Table 5. Main experimental results on the Transform subset of AdaptBench. We report accuracy (%) across five specific domains and overall aggregates. Performance for Text and Oracle modes is displayed in deep gray to prioritize adaptive results. * indicates that the model supports enhancement operations. Model Mode Real-world OCR GUI Knowledge Math Overall Accuracy Open-Source Models PixelReasoner Deepeyes Thyme PyVision Deepeyes v2 AdaptVision Qwen3-VL -8B-Instruct Qwen3-VL -32B-Instruct Qwen3-VL -235B-Instruct GPT-5 Gemini-3-Pro Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle 46.67 61.67 66.67 56.67 56.67 73.33 48.33 45.00 68.33 50.00 55.00 70.00 55.00 50.00 73.33 43.33 60.00 76. 40.00 35.00 60.00 50.00 40.00 51.67 45.00 51.67 68.33 55.00 71.67 73.33 70.00 81.67 86.67 56.67 66.67 83. 55.00 60.00 81.67 55.00 53.33 78.33 53.33 63.33 85.00 58.33 50.00 88.33 56.67 63.33 95.00 55.00 55.00 86. 58.33 71.67 88.33 65.00 71.67 93.33 60.00 75.00 88.33 85.00 90.00 95.00 21.67 41.67 55.00 41.67 48.33 63. 38.33 46.67 63.33 30.00 33.33 45.00 40.00 36.67 70.00 31.67 35.00 56.67 25.00 31.67 55.00 31.67 33.33 60. 23.33 25.00 51.67 41.67 40.00 48.33 30.00 48.33 51.67 50.00 62.50 75.00 40.00 37.50 77.50 35.00 20.00 80. 30.00 27.50 72.50 45.00 30.00 67.50 52.50 30.00 77.50 40.00 42.50 82.50 50.00 57.50 90.00 60.00 57.50 97. 30.00 42.50 70.00 35.00 32.50 65.00 32.50 42.50 62.50 15.00 15.00 35.00 27.50 45.00 67.50 42.50 27.50 70. 30.00 32.50 80.00 35.00 40.00 77.50 32.50 40.00 82.50 Closed-Source Models 32.50 60.00 85.00 75.00 97.50 100. 17.50 47.50 52.50 50.00 62.50 80.00 13 41.15 55.38 69.62 46.92 48.85 72.31 43.08 43.08 70. 37.69 41.54 62.69 46.54 43.08 74.23 45.00 45.38 75.38 38.46 39.62 71.54 45.38 48.46 71.92 45.00 49.23 76. 43.85 59.62 69.62 61.92 75.38 81.54 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process C. Category Results In this section, we provide fine-grained analysis of model performance across all specific categories defined in our benchmark. To ensure legibility and accommodate the wide range of sub-domains, the detailed accuracy results are presented in two separate tables: Tab. 6 reports the performance metrics for the GUI and Realworld domains. Tab. 7 covers the Knowledge, Math, and OCR domains. In both tables, the first row (labeled as N) denotes the number of test samples available for each corresponding category. All accuracy values are reported in decimal format. Table 6. Detailed Accuracy (Part 1/2): GUI and Realworld domains. * indicates that the model supports enhancement operations. Model PixelReasoner Deepeyes Thyme PyVision Deepeyes v2 AdaptVision Qwen3-VL -8B-Instruct Qwen3-VL -32B-Instruct Qwen3-VL -235B-Instruct GPT-5 Gemini-3-Pro Mode And. Lin. Mac. Web Win. iOS Attr. Count Integ. Phys. Spat. GUI Realworld 0.53 0.57 0.67 0.45 0.49 0.63 0.41 0.45 0.55 0.76 0.80 0.88 0.59 0.53 0.76 0.49 0.55 0. 0.51 0.57 0.67 0.57 0.67 0.73 0.88 0.78 0.94 0.75 0.88 0.80 0.86 0.90 0.90 0.47 0.50 0.57 0.57 0.53 0.63 0.50 0.57 0.60 0.77 0.90 0.83 0.57 0.63 0.77 0.53 0.60 0. 0.53 0.67 0.60 0.77 0.83 0.77 0.73 0.77 0.90 0.90 0.90 0.90 0.90 0.93 0.93 0.45 0.55 0.68 0.55 0.58 0.65 0.57 0.64 0.73 0.51 0.58 0.87 0.60 0.58 0.72 0.48 0.54 0. 0.55 0.58 0.70 0.56 0.64 0.78 0.61 0.60 0.81 0.62 0.77 0.92 0.58 0.82 0.83 0.12 0.50 0.75 0.38 0.44 0.69 0.62 0.75 0.75 0.44 0.44 0.88 0.50 0.69 0.81 0.25 0.25 0. 0.44 0.50 0.88 0.56 0.50 0.81 0.50 0.50 0.75 0.44 0.62 0.88 0.38 0.69 0.75 0.35 0.49 0.69 0.44 0.50 0.69 0.51 0.54 0.79 0.24 0.36 0.80 0.52 0.55 0.79 0.40 0.40 0. 0.47 0.44 0.84 0.50 0.54 0.86 0.50 0.57 0.83 0.31 0.55 0.82 0.50 0.69 0.78 0.50 0.70 0.50 0.30 0.30 0.40 0.40 0.40 0.60 0.40 0.50 0.70 0.70 0.50 0.80 0.60 0.60 0. 0.70 0.70 0.80 0.50 0.80 0.70 0.40 0.50 0.60 0.40 0.60 0.80 0.50 0.90 0.70 0.34 0.43 0.63 0.51 0.57 0.69 0.34 0.51 0.60 0.40 0.51 0.94 0.46 0.49 0.63 0.40 0.46 0. 0.40 0.57 0.77 0.34 0.46 0.83 0.34 0.46 0.83 0.46 0.57 0.97 0.54 0.60 0.86 (Count) Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle 0.51 0.51 0.59 0.59 0.48 0.67 0.53 0.54 0.61 0.81 0.83 0.92 0.46 0.49 0.64 0.48 0.51 0. 0.52 0.61 0.70 0.81 0.76 0.80 0.80 0.83 0.92 0.86 0.89 0.96 0.90 0.93 0.94 0.45 0.50 0.68 0.45 0.36 0.50 0.45 0.55 0.68 0.64 0.59 0.77 0.45 0.45 0.68 0.32 0.36 0. 0.45 0.64 0.59 0.77 0.77 0.68 0.77 0.77 0.91 0.82 0.86 0.95 0.86 0.86 0.91 0.31 0.41 0.36 0.36 0.44 0.46 0.36 0.41 0.56 0.82 0.82 0.92 0.46 0.44 0.51 0.28 0.41 0. 0.49 0.46 0.51 0.56 0.51 0.62 0.79 0.87 0.95 0.87 0.90 0.82 0.92 0.95 0.97 0.52 0.61 0.63 0.63 0.68 0.75 0.60 0.64 0.80 0.47 0.51 0.79 0.68 0.71 0.77 0.59 0.68 0. 0.64 0.63 0.77 0.75 0.68 0.83 0.65 0.76 0.88 0.47 0.75 0.84 0.79 0.84 0.91 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Table 7. Detailed Accuracy (Part 2/2): Knowledge, Math, and OCR domains. * indicates that the model supports enhancement operations. Model PixelReasoner Deepeyes Thyme PyVision Deepeyes v2 AdaptVision Qwen3-VL -8B-Instruct Qwen3-VL -32B-Instruct Qwen3-VL -235B-Instruct GPT-5 Gemini-3-Pro Knowledge Math OCR Mode Bio. Chem. Geo. Phys. Alg. Geo. Log. Stat. Chart Diag. Doc. Tab. (Count) Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle Text Adaptive Oracle 57 0.68 0.70 0.82 0.61 0.65 0.79 0.65 0.67 0. 0.51 0.72 0.56 0.60 0.65 0.84 0.63 0.61 0.84 0.75 0.75 0.91 0.84 0.82 0.95 0.81 0.91 0. 0.63 0.88 0.93 0.86 0.93 0.95 58 0.47 0.45 0.62 0.38 0.40 0.66 0.40 0.41 0. 0.16 0.36 0.74 0.43 0.48 0.53 0.47 0.55 0.67 0.62 0.71 0.79 0.78 0.76 0.95 0.84 0.79 0. 0.45 0.83 0.98 0.86 0.95 0.91 10 0.70 0.50 0.60 0.70 0.70 1.00 0.70 0.60 0. 0.60 0.60 0.70 0.30 0.50 0.50 0.70 0.80 0.90 0.40 0.70 0.60 1.00 0.90 0.90 0.60 1.00 0. 0.60 0.80 0.90 0.90 1.00 0.90 75 0.51 0.63 0.61 0.44 0.44 0.60 0.47 0.45 0. 0.33 0.49 0.57 0.44 0.49 0.63 0.51 0.45 0.67 0.65 0.69 0.77 0.68 0.77 0.96 0.75 0.87 0. 0.55 0.88 0.93 0.77 0.92 0.96 64 0.44 0.52 0.69 0.45 0.47 0.66 0.53 0.52 0. 0.28 0.25 0.48 0.48 0.50 0.66 0.41 0.45 0.70 0.55 0.69 0.86 0.67 0.67 0.95 0.73 0.66 0. 0.38 0.67 0.80 0.72 0.86 0.94 75 0.44 0.49 0.69 0.48 0.59 0.64 0.52 0.53 0. 0.33 0.39 0.52 0.40 0.57 0.69 0.59 0.52 0.72 0.47 0.56 0.77 0.68 0.77 0.84 0.75 0.83 0. 0.43 0.64 0.84 0.76 0.91 0.91 13 0.38 0.38 0.46 0.31 0.31 0.54 0.31 0.54 0. 0.38 0.31 0.46 0.15 0.31 0.31 0.15 0.46 0.46 0.31 0.38 0.38 0.31 0.38 0.38 0.23 0.77 0. 0.15 0.23 0.46 0.46 0.31 0.62 48 0.42 0.52 0.54 0.29 0.48 0.58 0.40 0.42 0. 0.29 0.27 0.65 0.31 0.40 0.62 0.35 0.33 0.69 0.56 0.69 0.88 0.79 0.85 0.96 0.83 0.81 0. 0.42 1.00 0.96 0.88 0.98 0.96 171 0.57 0.65 0.72 0.49 0.58 0.63 0.56 0.61 0. 0.59 0.72 0.74 0.58 0.61 0.74 0.59 0.63 0.70 0.65 0.68 0.78 0.81 0.83 0.91 0.79 0.80 0. 0.75 0.87 0.86 0.86 0.86 0.89 40 0.70 0.62 0.88 0.52 0.62 0.78 0.55 0.57 0. 0.62 0.75 0.85 0.68 0.55 0.80 0.70 0.65 0.98 0.75 0.70 0.98 0.82 0.90 1.00 0.78 0.92 1. 0.72 0.90 0.98 0.95 0.98 1.00 55 0.53 0.55 0.73 0.51 0.55 0.64 0.55 0.56 0. 0.58 0.64 0.87 0.51 0.51 0.73 0.58 0.64 0.78 0.36 0.53 0.69 0.62 0.75 0.91 0.67 0.73 0. 0.67 0.78 0.93 0.82 0.89 0.98 34 0.62 0.62 0.79 0.62 0.53 0.76 0.76 0.65 0. 0.68 0.71 0.97 0.62 0.53 0.71 0.71 0.68 0.76 0.76 0.71 0.91 0.82 0.85 0.91 0.91 0.79 0. 0.76 0.94 0.97 0.94 1.00 0.97 15 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process D. Reasoning Mode Prompt Here we provide the detailed prompts used in our experiments. Text-Reasoning Mode Prompts {question} [Multiple Choice] Question: Options: Please think step-by-step and give the final answer following the format: reasoning process </think> <answer> the options letter </answer> {options} <think> [Short Answer] Question: Please think step-by-step and give the final answer following the format: reasoning process </think> <answer> single word or phrase </answer> {question} <think> Oracle-Visual Mode Prompts [Zoom-in Setting] The first image is the global view, and the second image is the key region (zoomed-in) to help answer the question. Question: Please think step-by-step and give the final answer following the format: reasoning process </think> <answer> single word or phrase </answer> {question} <think> [Transformed Setting] The first image is the original input which might be distorted (rotated or dark), and the second image has been corrected and enhanced to show the true content. Question: Please think step-by-step and give the final answer following the format: reasoning process </think> <answer> single word or phrase </answer> {question} <think> [Auxiliary Line Setting] The first image is the original image, and the second image is the same image with auxiliary lines to help solve the problem. Question: Please think step-by-step and give the final answer following the format: reasoning process </think> <answer> single word or phrase </answer> {question} <think> 16 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process"
        },
        {
            "title": "LLM Judge Prompt",
            "content": "Your task is to determine whether the models predicted will give you question related to the image, the ground truth answer, and the model predicted answer. answer and the ground truth answer are consistent, and output the Judgement. Note that [Model Predicted Answer] is consistent with [Ground Truth Answer] whenever they are essentially the same. considered consistent, for example, pink and it is pink. If they are consistent, the Judgement is 1; if they are different, the Judgement is 0. Output Format: [Question]: {question} [Ground Truth Answer]: [Model Predicted Answer]: [Judgement]: Just output the Judgement and dont output anything else. If the meaning is expressed in the same way, it is {ground truth} {prediction} E. Error Analysis In this section, we provide detailed visualizations of the failure modes discussed in the Error Analysis (Sec. 5.5 of the main paper. By examining the intermediate reasoning steps, we offer concrete examples across different categories of tool-related errors. Visual Reasoning Failures. As noted in the main text, 42.3% of errors stem from the models inability to correctly manipulate or locate visual information. We present two representative scenarios: Wrong Image Transformations: Figure 9 illustrates case where the model repeatedly fails to correct the image orientation. This visual reasoning failure propagates to the OCR stage, causing the model to misread 831K as 83K and producing an incorrect prediction. Incorrect Region Selection: Figure 10 demonstrates spatial grounding failure in dense document. The model zooms into an incorrect region (Question 235 instead of Question 238), leading to reasoning that is logically valid but based on irrelevant visual evidence. Context Noise in Multi-step Reasoning. Figure 11 depicts the specific error type (accounting for 7.3% of cases) where visual perception is initially correct but overridden by context noise. In this example, the model successfully enhances the image and identifies the correct number of objects (two) in the intermediate step. However, distracted by the accumulated visual and textual context from the multi-step process, it becomes overly cautious and hallucinates negation, resulting in failure. Correction via Forced Tool Invocation. Figure 12 illustrates specific scenario (representative of the 7.0% of corrected errors) where forcing tool invocation rectifies an initial estimation failure. In this example, the model originally relies on imprecise visual intuition, incorrectly identifying Cerulean Blue as the answer. However, when forced to invoke tools, it bypasses the typical spatial zooming approach and adopts creative programmatic strategy: using Python to perform pixel-level RGB count. By rigorously verifying consistency across multiple tolerance thresholds, the model successfully overrides its initial hallucination and derives the correct answer based on quantitative data. Performance Degradation due to Incorrect Mode Selection. Figure 13 exemplifies the 8.3% of cases caused by incorrect mode selection, where the model unnecessarily invokes tools for tasks solvable by direct visual inspection. In this example, accurate icon counting is achievable via standard OCR or visual recognition (as seen in the Text-CoT mode). However, in the adaptive mode, the model complicates the task by adopting an unreliable engineering approach: using OpenCV edge detection to count squares. This strategy proves fragile, as the model struggles with parameter tuningfirst detecting excessive noise and then over-filtering actual targetsultimately leading to hallucinated final count due to the confused tool outputs. 17 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 9. failure case caused by incorrect image transformation (rotation) leading to OCR errors. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 10. failure case caused by zooming into an incorrect region (spatial misalignment). 19 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 11. failure case where correct intermediate visual grounding is overridden by context noise. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 12. Example of correcting visual estimation errors via code-based pixel analysis. 21 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 13. Example of performance degradation caused by unnecessary tool usage in simple visual task. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process F. Process Evaluation Example To better understand our evaluation protocol, we present detailed cases of process reasoning quality assessment in Figure 14 and Figure 15. Figure 14. Illustration of key step coverage evaluation. 23 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process Figure 15. Illustration of tool effectiveness evaluation. 24 AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process G. Process Evaluation Prompt To ensure reproducible and standardized assessment, we leverage LLM-based judges with specialized prompts for process auditing. Specifically, we employ the tool invocation effectiveness prompt (detailed in Figure 16) is used to audit the functional correctness and intent alignment of each tool call within the adaptive process. Subsequently, the key step coverage Prompt (detailed in Figure 17) to verify the logical completeness of the models reasoning trajectory against annotated ground truth."
        },
        {
            "title": "Tool Invocation Effectiveness Prompt",
            "content": "The above content contains the models multi-step reasoning process. top-tier visual reasoning audit expert. and can accurately evaluate whether models tool usage aligns with its stated reasoning intent. All Available Tools: {tools} Each step in the reasoning process includes: You are You possess strong logical analysis skills [Step] textual reasoning step. [Tool] tool invocation (All Available Tools above). [Tool Execution Output] The execution output (image, text, or highlighted region). Evaluation Criteria: 1. Tool Invocation Correctness: Check if the invocation is valid, properly formatted, and consistent with the tools definition and the corresponding Step ID. 2. Tool Execution Output Validation: Check if the output satisfies the intent of the corresponding Step ID and Tool ID. The output only needs to fulfill the specific steps purpose, not the overall problem. Evaluate based on the global image. Attention: Only steps satisfying ALL criteria are listed as correct. Do not analyze for errors unless the output explicitly indicates failure. Ignore image resolution/size info; assume the tool operates on the original source image. Output Format (JSON-like): Correct Tools: Brief Analysis: others are incorrect] [Tool1, Tool2, Tool3...] [Briefly explain why these tool invocations are correct and why Figure 16. Prompt used for evaluating tool invocation effectiveness. AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process"
        },
        {
            "title": "Key Step Coverage Prompt",
            "content": "1. The original question/task. Segment the provided solution into logical reasoning steps. You are an expert system for verifying solutions to image-based problems. is to: each ground truth middle step with the solution steps. Problem: INPUT FORMAT: 1. paragraph containing the models reasoning. required for correct answer. TASK: Step 1: Divide the solution paragraph into distinct logical reasoning steps. represent coherent reasoning unit. Step 2: For each ground truth step, determine if it is matched in any of the solution steps based on the following: Match Ground Truth Steps Segment the Solution Essential steps Ground Truth: Solution: Your task Match continuous 3. 2. 2. Each step should Content Match: Must match specific values and details. Tool Integrity: If tool execution has errors, the step is \"Unmatched\". Implicit Logic: Reasonable logical skips (like identifying obvious objects or using background knowledge) are permissible. OUTPUT FORMAT (JSON): { \"solution_steps\": [\"Step 1...\", \"Step 2...\"], \"judgments\": [ \"step_index\": <integer>, \"judgment\": \"Matched\" \"Unmatched\", \"reason\": \"Brief explanation.\" { } ] } ADDITIONAL RULES: 1. ground truth step in order without omission. DATA: Only output the JSON object with no extra text. 2. Judge each [Problem] {question} [Answer] {answer} [Solution] {solution} [Ground Truth Information] {gt annotation} Figure 17. Prompt used for key step coverage evaluation."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology",
        "Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University",
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "Xidian University"
    ]
}