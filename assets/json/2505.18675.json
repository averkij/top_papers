{
    "paper_title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps",
    "authors": [
        "Sicheng Feng",
        "Song Wang",
        "Shuyi Ouyang",
        "Lingdong Kong",
        "Zikai Song",
        "Jianke Zhu",
        "Huan Wang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 5 7 6 8 1 . 5 0 5 2 : r Can MLLMs Guide Me Home? Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps 1Westlake University Sicheng Feng1,2,, Song Wang3,2,, Shuyi Ouyang3,2, Lingdong Kong2, Zikai Song4,2, Jianke Zhu3, Huan Wang1,, Xinchao Wang2 2National University of Singapore 4Huazhong University of Science and Technology Dataset & Toolkit: https://fscdc.github.io/Reason-Map Equal contribution. Corresponding author. 3Zhejiang University"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce REASONMAP, benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. REASONMAP encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1, 008 question-answer pairs spanning two question types and three templates. Furthermore, we design two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) [1, 2, 3, 4, 5] have recently achieved notable advancements across range of vision-language tasks, including visual grounding [6, 7], reasoning segmentation [8, 9, 10, 11], and text-image alignment [12, 13]. Building upon these developments, reasoning MLLMs [14, 15, 16, 17, 18, 19, 20] have further improved performance on complex visual reasoning tasks such as visual math problems [21, 22], visual question answering (VQA) [22, 23, 24], and spatial reasoning [23, 25, 26]. These capabilities are critical for wide range of real-world applications, including embodied AI, autonomous agents, and decision-making systems such as autonomous driving [27, 28, 29]. As multimodal tasks grow in complexity and practical relevance, the need for rigorous benchmarks to assess fine-grained visual reasoning becomes increasingly essential. To address the growing demand for robust evaluation of multimodal reasoning, several benchmarks have been proposed. Datasets such as MathVQA [22] and MMMU [24] incorporate multimodal questions but often permit models to succeed via shallow heuristics, without requiring genuine visual grounding. MathVerse [30] mitigates this limitation by introducing diverse problem variants that Preprint. Figure 1: Overview of REASONMAP. REASONMAP is benchmark dataset designed to evaluate finegrained visual reasoning abilities of MLLMs, encompassing 1, 008 questionanswer pairs constructed over high-resolution transit maps from 30 cities, spanning two question types and three templates. encourage reliance on visual input. VisuLogic [31] further enforces visual reasoning by explicitly eliminating language-only shortcuts. Other efforts, such as VisualPuzzles [32], VGRP-Bench [33], and R-Bench [34], target logical and structural reasoning, while CityBench [35] and DriveBench [36] focus on domain-specific applications like urban tasks and autonomous driving. VBench [37] emphasizes detailed visual understanding. Despite these advances, systematic evaluation of finegrained visual reasoning remains limited, especially for structured and information-rich diagrams like high-resolution transit maps, leaving critical gap in existing benchmarks. In this paper, we introduce REASONMAP (Figure 1), benchmark designed to evaluate the finegrained visual understanding and spatial reasoning capabilities of MLLMs using high-resolution transit maps. As structured and information-dense visual artifacts, transit maps inherently require precise spatial interpretation, making them well-suited for assessing detailed visual reasoning. REASONMAP comprises 1, 008 human-verified question-answer pairs spanning 30 cities across 13 countries. Each instance includes map, two stops, two questions (short and long), multiple reference routes, and two difficulty labels (map and question difficulty). The questions cover two types and three prompting templatesone for short and two for long questionscapturing both coarse and fine-grained spatial reasoning. To ensure data quality, we perform manual route verification, promote question diversity, and balance difficulty distribution. For evaluation, we propose two-level framework that independently measures answer correctness (via accuracy) and quality (via proposed map score), reflecting both feasibility and efficiency in route planning. We conduct comprehensive experiments on 15 widely-used MLLMs, encompassing base and reasoning models. Our results reveal counterintuitive finding: among open-source models, base variants outperform their reasoning counterparts, whereas the opposite holds for closed-source models. Moreover, when only textual inputs are provided, models can still answer some questions based on inner knowledge, but in most cases, their performance noticeably drops. This highlights critical limitation in the current model behavior. While some models can leverage prior knowledge and textual cues to solve certain tasks, the tasks (e.g., fine-grained visual reasoning tasks) requiring genuine visual understanding still necessitate effective integration of multimodal information for robust reasoning. 2 Table 1: Comparison between REASONMAP and existing multimodal reasoning datasets. For entries in the dataset size column with notation like (n), each base problem has multiple versions to enforce visual grounding. Specifically, VGRP-Bench is constructed by sampling over 20 core puzzles. Name Year Dataset Size Avg. Resolution Training Set Step Evaluation Multilingual (Count) MMMU [24] MathVerse [30] VisuLogic [31] VisualPuzzles [32] VGRP-Bench [33] R-Bench [34] VBench [37] REASONMAP 2024 2024 2025 2025 2025 2025 2023 2025 11.5k 2, 612 (6) 1, 003 1, 168 20 (5) 665 191 684 246 577 487 601 331 767 464 790 790 629 348 2, 246 1, 1, 008 (2) 5, 839 5, 449 (2) (2) (4) Our main contributions are summarized as follows: We develop an extensible, semi-automated pipeline for dataset construction, facilitating scalable expansion to additional maps and cities. Using this pipeline, REASONMAP is constructed to evaluate fine-grained visual reasoning capabilities in MLLMs. We propose structured two-level evaluation framework that separately quantifies answer correctness and quality using accuracy and the proposed map score, respectively, enabling fine-grained answer assessment. comprehensive benchmarking study is conducted across 15 MLLMs, providing insights into model performance, robustness, and the interplay between visual and textual cues, thereby informing future research on multimodal reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Reasoning in LLMs & MLLMs. Recent advances in large language models (LLMs) have demonstrated significant improvements in reasoning capabilities through reinforcement fine-tuning paradigms [14, 15, 38, 39], which leverage GRPO [40] to unlock the reasoning potential of LLMs. This paradigm has also been extended to the multimodal domain, with increasing interest in applying reinforcement learning (RL) to visual reasoning [41, 42, 43, 44, 45]. Both open-source and closed-source communities have introduced advanced reasoning MLLMs built upon earlier systems [2, 3, 46, 47]. Notable open-source models include Kimi-VL [16], Skywork-R1V [17, 18], and Qwen-QVQ [20], while Doubao-1.5-Pro [19], OpenAI o3 [47], OpenAI 4o [48], and Gemini [49] represent leading closed-source efforts. Despite recent progress, systematic evaluation of fine-grained visual reasoning in MLLMs remains limited, as existing benchmarks primarily target coarse-grained tasks and fail to capture model performance on complex real-world visual content. Multimodal Reasoning Datasets. As multimodal reasoning has rapidly progressed, various benchmarks have emerged to evaluate MLLMs across different reasoning dimensions (see summary in Table 1). Datasets such as VBench [37], VisualPuzzles [32], VisuLogic [31], and VGRP-Bench [33] primarily examine abstract visual reasoning through synthetic tasks involving logic, structure, and pattern recognition. In parallel, CityBench [35] and DriveBench [36] shift focus to real-world spatial reasoning, assessing model performance on complex urban or autonomous driving scenarios. For mathematical reasoning, benchmarks like MathVQA [22], MMMU [24], and MathVerse [30] integrate multimodal inputs, with MathVerse notably introducing varied problem formats to strengthen visual dependence. Unlike these works, we first introduce benchmark for evaluating fine-grained visual reasoning capacities with high-resolution transit maps. Map-based Spatial Reasoning. Among the many directions of multimodal reasoning, map-based spatial reasoning has emerged as crucial area, with broad applications in navigation, urban planning, and autonomous systems [50, 51, 52, 53]. Recent efforts have focused on enabling models to interpret and reason over various types of map data. CityBench [35] provides dataset for evaluating urban scene understanding, while MapLM [54] introduces benchmark for map and traffic scene understanding. PlanAgent [55] and PReP [56] explore embodied planning in environments that require interpreting map information. MapEval [26] proposes structured evaluation suite for maprelated reasoning, and GeoNav [52] investigates geospatial navigation using LLMs. Most existing methods [26, 35, 55] depend on external tools (e.g., map services or APIs) to complete spatial tasks, which often bypasses the need for genuine visual reasoning. However, spatial reasoning based 3 Figure 2: The building pipeline of REASONMAP consists of three main stages: (1) data collection and preprocessing, (2) questionanswer pair construction, and (3) quality control. Steps (2-4) in the figure correspond to the questionanswer pair construction stage. solely on visual understanding remains essential. Our work targets to fill this gap by evaluating such capabilities without tool assistance."
        },
        {
            "title": "3 REASONMAP Construction",
            "content": "In this section, we first present the complete dataset building pipeline as shown in Figure 2, which consists of three main stages: (1) data collection and preprocessing; (2) questionanswer pair construction; and (3) quality control. We then report comprehensive statistics of the dataset. 3.1 REASONMAP Building Pipeline 3.1.1 Data Collection and Preprocessing We collect and manually select 30 high-resolution transit maps covering 30 cities across 13 countries from publicly available online sources, in compliance with relevant licenses and regulations, ensuring diversity and balanced range of map difficulty. We then leveraged MLLMs to extract the names of transit lines and their corresponding stops, followed by manual correction to ensure correctness. Special cases such as transfer stops and branch-starting stops were annotated in standardized format appended to the respective stop entries. Finally, for subsequent usage, all route and stop information was saved in unified JSON format, referred to as the Map Metadata. 3.1.2 Question-Answer Pair Construction The construction of questionanswer pairs involves three key steps: (1) Question Generation, where we formulate questions based on predefined templates; (2) Reference Route Collection, where we 4 obtain corresponding reference routes using Gaode Map1 and Google Map2; and (3) Label Annotation, where we assign difficulty levels for both the maps and the questions. Question Generation. We randomly select two stops (refer to stop1 and stop2) from the current highresolution transit map. We then generate one short question and one long question based on predefined question templates and two stops (Figure 2). The short question has only one fixed template, while the long question is randomly assigned one of two available templates during generation. Additionally, the two long question templates differ in focus: one asks for the number of via stops, while the other requires identifying each via stop (see detailed templates in Appendix A.1). Reference Route Collection. For each question, we query all valid transit routes between stop1 and stop2 using APIs from map services (e.g., Gaode Map for Chinese cities and Google Map for other cities). The retrieved routes are stored in unified format containing relevant metadata (e.g., route name, departure stop, arrival stop, via stops, and number of via stops). We discard routes that cannot be visually traced on the map, ensuring consistency with the visual content. Label Annotation. Two levels of difficulty labeling are included in this stage. For map difficulty, we manually assign each map to one of three difficulty levels (easy, medium, hard), ensuring balanced split across 30 maps, with 10 maps per level. For question difficulty, we assign difficulty based on the number of transfers in the reference route: routes with no transfers are labeled as easy, those with one transfer as medium, and all others as hard. To ensure balance, we set fixed difficulty distribution threshold of 20 : 15 : 5 (easy:medium:hard) for each map, generating 40 questions. Once the quota for difficulty level is reached on given map, no additional questions of that level are retained. 3.1.3 Quality Control To ensure the reliability and balance of the dataset, we perform quality control from three perspectives: correctness, diversity, and difficulty balance. Incorrect questionanswer pairs are either manually corrected or discarded. We then involve both automatic checks and manual adjustments to ensure consistency and coverage across all difficulty levels. One reserved example is shown in Figure 1. 3.2 Dataset Statistics The REASONMAP consists of 30 high-resolution transit map images (see map sources in Appendix A.2) with an average resolution of 5, 839 5, 449 pixels. In total, it contains 1, 008 questionanswer pairs, including stop names in four languages (e.g., English, Hungarian, Chinese, and Italian). The distribution of question difficulty is as follows: 57.7% are labeled as easy, 34.4% as medium, and 7.8% as hard. Additionally, subset of 312 samples is manually selected as the test set for the benchmark experiments described in Section 5, while the remaining samples serve as training set for future use. To ensure diversity and difficulty balance, the test set includes 11 cities with 4 : 3 : 4 map difficulty ratio and question difficulty distribution (181 easy, 108 medium, 23 hard) that maintains consistency with the full dataset."
        },
        {
            "title": "4 Evaluation Framework",
            "content": "This section systematically introduces two-level evaluation framework for assessing model performance on the REASONMAP. This framework separately evaluates the correctness and quality of answers produced by models. Specifically, we quantify correctness using accuracy and design map score to measure the quality of answers, considering multiple factors (e.g., route efficiency and alignment with the reference routes from map services). Preparation for Evaluation. We first parse the model-generated answers according to the required format. Answers that do not comply with the specified format or cannot be parsed due to model hallucination [57] are marked as invalid. Invalid responses are excluded from subsequent evaluations, with accuracy and map score set to zero. For the correctness evaluation, we utilize the Map Metadata mentioned in Section 3.1.1 as ground truth. For the quality evaluation, we adopt the collected reference routes as presented in Section 3.1.2 as the ground truth. 1https://console.amap.com/dev/index 2https://developers.google.com/maps/apis-by-platform 5 4.1 Correctness Evaluation We evaluate the correctness of the answer using Algorithm 1 in Appendix B. Specifically, the evaluation checks the correctness of the overall departure and arrival stops (stop1 and stop2), verifies if the route name of each segment exists in the Map Metadata, ensures the departure and arrival stops are valid for each segment, and confirms that transfer stops between consecutive segments are consistent. An answer is considered correct only if all the above checks are satisfied. Additionally, we apply the same correctness evaluation algorithm to the answers of short and long questions. 4.2 Quality Evaluation To evaluate the quality of the answers, we introduce unified scoring metric, referred to as the map score, which is applied to both short and long questions using the evaluation procedure (see Algorithm 2 in Appendix B). The overall evaluation framework for route quality follows structure similar to that used in Section 4.1. The following evaluation procedure assumes single reference route for simplicity. In practice, if multiple reference routes are available, the answer is evaluated against each of them, and the highest score is taken as the final map score. For short questions, the map score solely focuses on route-level and endpoint consistency, excluding all long-question-specific parts. We compute the score by comparing segment pairs in the answer and reference route. Specifically, correctly matching stop1 and stop2 contributes one point, matching the route name adds two points, and matching the departure and arrival stops within each route segment provides one point each. The final score is capped at 10, and an additional bonus is awarded if the answer is judged correct based on the correctness evaluation procedure described in Section 4.1. This design ensures that correct answer always receives higher score than any incorrect one. For long questions, the evaluation incorporates additional scoring components tailored to the two question templates introduced in Section 3.1.2. These components are designed to capture the increased reasoning depth required in long-form responses. As with short questions, bonus score is also added for correct answers. The two additional scoring components are detailed below. Via Stop Count Evaluation. For long questions that require models to predict the number of via stops for each segment, we introduce the num_via_stop_score. This score compares the via stop count of the answer and reference route by computing the absolute error and mapping it to fixed score (4). perfect match yields full points, while larger discrepancies receive proportionally lower scores. The score is then capped at 10 for the full route. Specific Via Stop Evaluation. For long questions that require explicit enumeration of intermediate stops, we compute via_stop_score using combination of two factors: the number of correctly matched via stops, and the intersection-over-union (IoU) between via stop sets of the answer and reference route. The final score for this component is obtained by averaging the IoU score (scaled to 10) and the exact match count (capped at 10), and then clipped to maximum of 10."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setups We conduct extensive benchmark experiments on REASONMAP using 15 popular MLLMs under different inference settings, analyzing their performance and comparing results. Several interesting insights emerge from this comparison. The detailed experimental settings are described below. Evaluated Models. We evaluate diverse set of MLLMs categorized into two groups based on whether they are reasoning-oriented models with long-thinking process. Reasoning models include: Skywork-R1V-38B [17, 18], QvQ-72B-Preview [20], Kimi-VL-A3B-Thinking/Instruct [16], OpenAI o3 [47], Gemini-2.5-Flash [49], Doubao-1-5-thinking-vision-pro-250428 (Doubao-428), and Doubao1.5-Thinking-Pro-M-250415 (Doubao-415) [19]. Base models include: Qwen2.5-VL series (3B, 32B, 72B) [2], InternVL3 series (38B, 78B) [3], OpenAI 4o [48], and Doubao-1.5-Vision-Pro-32k-250115 (Doubao-115) [19]. Additionally, the Doubao 1.5 Pro series has an activated parameter size of 20B. Inference Settings. For open-source models, we set the max output token limit to 2, 048, while keeping other parameters consistent with the official HuggingFace configurations. All open-source models are deployed using PyTorch and the HuggingFace Transformers library on 8 NVIDIA A100 6 Table 2: Evaluations of various MLLMs on REASONMAP. S. represents results for short questions, while L. denotes results for long questions. The map score is capped at 20 for short questions, while for long questions, the maximum score is 40. Bold indicates the best results among open-source and closed-source models, respectively, while underline represents the second best. Model Type Weighted Acc. (S.) #Tokens (S.) Weighted Acc. (L.) #Tokens (L.) Weighted Map Score (S. / L.) Qwen2.5-VL-3B-Instruct [2] Qwen2.5-VL-32B-Instruct [2] Qwen2.5-VL-72B-Instruct [2] InternVL3-38B [3] InternVL3-78B [3] Kimi-VL-A3B-Instruct [16] Kimi-VL-A3B-Thinking [16] Reasoning Skywork-R1V-38B [17, 18] Reasoning Reasoning QvQ-72B-Preview [20] Base Base Base Base Base Base Doubao-115 [19] OpenAI 4o [48] Doubao-415 [19] Doubao-428 [19] Gemini-2.5-Flash [49] OpenAI o3 [47] Base Base Reasoning Reasoning Reasoning Reasoning 8.68% 16.49% 26.65% 14.84% 25.35% 12.76% 5.47% 6.86% 9.03% 34.20% 41.15% 43.14% 37.15% 46.09% 63.02% Open-source Models 42 36 33 43 33 41 754 645 1, 279 Closed-source Models 32 34 536 532 806 1, 7.99% 15.71% 24.22% 13.45% 19.62% 12.33% 5.47% 3.21% 4.25% 38.02% 42.80% 46.09% 37.85% 29.86% 59.11% 151 112 104 68 62 41 1, 287 842 1, 619 118 58 1, 796 2, 167 1, 419 2, 372 2.75 / 3.70 3.88 / 6.84 5.09 / 8.80 3.48 / 6.31 4.80 / 7.50 3.30 / 5.37 2.44 / 3.17 2.11 / 3.11 1.59 / 1.55 5.25 / 11.96 6.84 / 13.57 7.33 / 14.67 5.52 / 11.73 7.64 / 9.98 9.53 / 17. GPUs. For closed-source models, we access their official APIs for evaluation and follow the default settings provided by each models official documentation. We further discuss the diverse image processing strategies when handling high-resolution visual inputs in Appendix D. Difficulty-Aware Weighting. To better reflect the varying complexity of different samples, we adopt difficulty-aware weighting strategy based on the combination of question difficulty and map difficulty. Specifically, each difficulty pair is assigned predefined weight, with harder combinations receiving higher values. The complete weight matrix is provided in Appendix B.2. Both accuracy and map score are evaluated using this weighting scheme, ensuring that models are more strongly rewarded for correctly solving more challenging examples. 5.2 Experimental Results 5.2.1 Performance of MLLMs with Full Input (a) Accuracy for short questions (b) Accuracy for long questions Figure 3: Accuracy across difficulty combinations for four representative MLLMs (Qwen2.5-VL72B-I, InternVL3-78B, OpenAI o3, and Doubao-415). Each difficulty combination is denoted by pair (e.g., easy-hard), where the first term indicates question difficulty and the second term represents map difficulty. The pair (hard-middle) contains only one sample, leading to an accuracy of 100%. We summarize the number of evaluation samples in each difficulty bucket: 55 samples for easy-easy, 46 for easy-middle, 28 for middle-easy, 7 for hard-easy, 23 for middle-middle, 80 for easy-hard, 1 for hard-middle, 57 for middle-hard, and 15 for hard-hard. The principal results are summarized in Table 2. Notably, we observe counterintuitive phenomenon: among open-source models, reasoning models consistently underperform their base counterparts, whereas the opposite holds in the closed-source setting3. Prior work suggests that RL may improve sample efficiency without introducing fundamentally new reasoning abilities [58, 59, 60], while 3Although the comparison across closed-source models may not be fair due to lack of transparency in details, the reasoning variants exhibit stronger performance in this category. 7 Table 3: Evaluations of various MLLMs on REASONMAP without visual inputs. S. represents results for short questions, while L. denotes results for long questions. The map score is capped at 20 for short questions, while for long questions, the maximum score is 40. Bold indicates the best results among open-source and closed-source models, respectively, while underline represents the second best. Green highlights improved results compared to the full input setting  (Table 2)  , while red indicates performance drops. Model Type Weighted Acc. (S.) #Tokens (S.) Weighted Acc. (L.) #Tokens (L.) Weighted Map Score (S. / L.) Open-source Models Qwen2.5-VL-3B-Instruct [2] Qwen2.5-VL-72B-Instruct [2] Kimi-VL-A3B-Instruct [16] Kimi-VL-A3B-Thinking [16] Reasoning Base Base Base 9.38% 0.7% 16.41% 10.24% 11.81% 0.95% 4.17% 1.30% 47 28 41 1, 039 9.72% 1.73% 15.71% 8.51% 9.81% 2.52% 2.08% 3.39% 147 108 49 1, 755 2.93 0.18 / 4.51 0.81 4.03 1.06 / 6.49 2.31 3.37 0.07 / 5.32 0.05 2.06 0.38 / 1.64 1.53 Doubao-115 [19] Doubao-415 [19] Base 13.72% 20.48% Reasoning 21.53% 21.61% 34 352 13.98% 24.04% 17.19% 28.90% 99 1, 047 3.50 1.75 / 6.48 5.48 4.85 2.48 / 7.68 6.99 Closed-source Models RL-trained models tend to bias their output distributions toward high-reward trajectories, which helps produce more correct responses but may simultaneously constrain the models exploration capacity and reduce its ability to leverage broader foundational knowledge. In addition, recent studies indicate that multimodal models may sometimes rely on inner knowledge priors instead of truly attending to visual inputs [30, 61, 62, 63]. This tendency is further supported by the results in Section 5.2.2, where open-source models still maintain part of their performance even without visual input, indicating limited visual grounding. In contrast, closed-source reasoning models outperform their base variants. One possible explanation lies in the broader knowledge coverage and better visual integration observed in these models [19, 47, 49]. We further analyze the effect of model size by examining performance within the same architecture series. Qwen2.5-VL and InternVL series show consistent trend: larger models achieve better accuracy with fewer tokens, suggesting that the scaling law [64] continues to hold even in finegrained visual reasoning tasks. Figure 3 presents accuracy distributions across different combinations of question and map difficulty. As expected, performance degrades as task complexity increases. Additionally, Figure 4 illustrates accuracy variation across cities. We observe negative correlation between map difficulty and accuracy. Moreover, model performance varies notably even among cities with comparable map difficulty levels. This disparity can be partially attributed to factors such as city prominence and the language used for stop names, both of which are closely tied to the models pretrained knowledge. For instance, OpenAI o3 performs significantly better on complex cities like Singapore compared to Hangzhou, likely due to Singapores higher international visibility and the use of English stop names, whereas Hangzhou is less prominent and its stop names are Chinese. (a) Accuracy for short questions (b) Accuracy for long questions Figure 4: Accuracy across different cities for four representative MLLMs (Qwen2.5-VL-72B-I, InternVL3-78B, OpenAI o3, and Doubao-415). Each city is marked with the corresponding map difficulty and the country flag. Each city in the test set provides specific number of samples per model: 32 samples for Auckland, 34 for Los Angeles, 7 for Miami, 35 for Lisboa, 18 for Geneva, 40 for Beijing, 39 for Hangzhou, 17 for Budapest, 39 for Singapore, 40 for Rome, and 11 for Toronto. Figure 5: Error case analysis of various MLLMs using REASONMAP. For reasoning models, the reasoning process is explicitly marked with <think> and </think> tags. We highlight error contents in the answers with red and categorize them accordingly. 5.2.2 Performance of MLLMs without Visual Input To further investigate the reliance of MLLMs on visual input, we selected representative open-source and closed-source models for additional experiments, where the visual input was masked. The results are reported in Table 3. We observe that while most models can leverage internal knowledge to answer certain questions, their performance generally declines to varying degrees when visual input is removed, with the decline being more pronounced among closed-source models. Model performance is positively correlated with the performance drop after masking visual inputs, indicating effective use of visual information. In contrast, models like Qwen2.5-VL-3B-I show minimal or even improved performance, suggesting reliance on inner knowledge rather than genuine visual reasoning. 5.3 Error Analysis Figure 5 presents representative failure cases from REASONMAP, revealing several recurring error types. common issue is visual confusion, where the model misidentifies the transit line due to similar colors or adjacent layouts, for instance, mistaking Line 9 for Line 16 (OpenAI o3, left column; Doubao-428, right column). Another frequent problem is format errors, where responses deviate from the required structure, making them unprocessable despite containing correct route information (Doubao-115 and QvQ-72B-Preview, left column). We also observe instances of hallucination [57], where the model repeats the correct answer (Kimi-VL-A3B-Thinking, middle column) or generates information that is not present in the input, such as mentioning image generation, as seen in SkyworkR1V-38B (right column). Refusal cases are also present, where models explicitly decline to answer (Skywork-R1V-38B, middle and right column). Notably, these errors may occasionally co-occur within single response (Skywork-R1V-38B, right column). These behaviors highlight the limitations in visual grounding and response robustness, especially when handling fine-grained visual details (see more case analyses in Appendix C)."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce REASONMAP, benchmark designed to evaluate the fine-grained visual understanding and spatial reasoning capabilities of MLLMs using high-resolution transit maps. Through semi-automated and scalable data building pipeline, we curate diverse set of humanverified question-answer pairs across 30 cities. Our two-level evaluation framework enables nuanced assessment of both correctness and quality. Experimental results on 15 popular MLLMs reveal key insights into model behavior, highlighting performance gaps between base and reasoning models, as well as the crucial role of visual input. These findings underscore the need for more rigorous evaluation and training approaches to advance visual reasoning in multimodal systems."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [4] Yangliu Hu, Zikai Song, Na Feng, Yawei Luo, Junqing Yu, Yi-Ping Phoebe Chen, and Wei Yang. Sf2t: Self-supervised fragment finetuning of video-llms for fine-grained understanding. arXiv preprint arXiv:2504.07745, 2025. [5] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. [6] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [7] Jingru Yang, Huan Yu, Yang Jingxin, Chentianye Xu, Yin Biao, Yu Sun, and Shengfeng He. Visuallinguistic agent: Towards collaborative contextual object reasoning. arXiv preprint arXiv:2411.10252, 2024. [8] Yi-Chia Chen, Wei-Hua Li, Cheng Sun, Yu-Chiang Frank Wang, and Chu-Song Chen. Sam4mllm: Enhance multi-modal large language model for referring expression segmentation. In ECCV, 2024. [9] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In NeurIPS, 2024. [10] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In CVPR, 2024. [11] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. [12] Xinli Yue, JianHui Sun, Junda Lu, Liangchao Yao, Fan Xia, Tianyi Wang, Fengyun Rao, Jing Lyu, and Yuetang Deng. Instruction-augmented multimodal alignment for image-text and element matching. arXiv preprint arXiv:2504.12018, 2025. [13] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving text-image alignment evaluation. In NeurIPS, 2023. [14] OpenAI. OpenAI o1. https://openai.com/o1/, 2024. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. 10 [17] Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025. [18] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599, 2025. [19] ByteDance. doubao-1.5-pro. https://seed.bytedance.com/en/special/doubao_1_5_pro, 2025. [20] Qwen Team. Qvq: To see the world with wisdom. https://qwenlm.github.io/blog/ qvq-72b-preview/, 2024. [21] Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, and Jie Tang. Mathglm-vision: Solving mathematical problems with multi-modal large language model. arXiv preprint arXiv:2409.13729, 2024. [22] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In NeurIPS, 2024. [23] Fatemeh Shiri, Xiao-Yu Guo, Mona Golestan Far, Xin Yu, Gholamreza Haffari, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. arXiv preprint arXiv:2411.06048, 2024. [24] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. [25] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [26] Mahir Labib Dihan, Md Tanvir Hassan, Md Tanvir Parvez, Md Hasebul Hasan, Md Almash Alam, Muhammad Aamir Cheema, Mohammed Eunus Ali, and Md Rizwan Parvez. Mapeval: map-based evaluation of geo-spatial reasoning in foundation models. arXiv preprint arXiv:2501.00316, 2024. [27] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. survey of embodied ai: From simulators to research tasks. TETCI, 6(2):230244, 2022. [28] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [29] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. survey on multimodal large language models for autonomous driving. In WACV, 2024. [30] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In ECCV, 2024. [31] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. [32] Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025. [33] Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine SÃ¼sstrunk, and Filippos Kokkinos. Vgrp-bench: Visual grid reasoning puzzle benchmark for large vision-language models. arXiv preprint arXiv:2503.23064, 2025. [34] Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, et al. R-bench: Graduate-level multi-disciplinary benchmarks for llm & mllm complex reasoning evaluation. arXiv preprint arXiv:2505.02018, 2025. [35] Jie Feng, Jun Zhang, Junbo Yan, Xin Zhang, Tianjian Ouyang, Tianhui Liu, Yuwei Du, Siqi Guo, and Yong Li. Citybench: Evaluating the capabilities of large language model as world model. arXiv preprint arXiv:2406.13945, 2024. 11 [36] Shaoyuan Xie, Lingdong Kong, Yuhao Dong, Chonghao Sima, Wenwei Zhang, Qi Alfred Chen, Ziwei Liu, and Liang Pan. Are vlms ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives. arXiv preprint arXiv:2501.04003, 2025. [37] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In CVPR, 2024. [38] Sicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903, 2025. [39] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021. [40] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [41] R1-V Team. R1-V. https://github.com/Deep-Agent/R1-V?tab=readme-ov-file, 2025. [42] EvolvingLMMs Lab. Open R1 Multimodal. https://github.com/EvolvingLMMs-Lab/ open-r1-multimodal, 2025. [43] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [44] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [45] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [46] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [47] OpenAI. OpenAI o3 and o4-mini System Card. https://cdn.openai.com/pdf/ 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf, 2025. [48] OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024. [49] Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [50] Zhibin Bao, Sabir Hossain, Haoxiang Lang, and Xianke Lin. review of high-definition map creation methods for autonomous driving. Engineering Applications of Artificial Intelligence, 122:106125, 2023. [51] Ari Seff and Jianxiong Xiao. Learning from maps: Visual common sense for autonomous driving. arXiv preprint arXiv:1611.08583, 2016. [52] Haotian Xu, Yue Hu, Chen Gao, Zhengqiu Zhu, Yong Zhao, Yong Li, and Quanjun Yin. Geonav: Empowering mllms with explicit geospatial reasoning abilities for language-goal aerial navigation. arXiv preprint arXiv:2504.09587, 2025. [53] Song Wang, Wentong Li, Wenyu Liu, Xiaolu Liu, and Jianke Zhu. Lidar2map: In defense of lidar-based semantic map construction using online camera distillation. In CVPR, 2023. [54] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can Cui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang, James Rehg, et al. Maplm: real-world large-scale vision-language benchmark for map and traffic scene understanding. In CVPR, 2024. [55] Yupeng Zheng, Zebin Xing, Qichao Zhang, Bu Jin, Pengfei Li, Yuhang Zheng, Zhongpu Xia, Kun Zhan, Xianpeng Lang, Yaran Chen, et al. Planagent: multi-modal large language agent for closed-loop vehicle motion planning. arXiv preprint arXiv:2406.01587, 2024. [56] Qingbin Zeng, Qinglong Yang, Shunan Dong, Heming Du, Liang Zheng, Fengli Xu, and Yong Li. Perceive, reflect, and plan: Designing llm agent for goal-directed city navigation without instructions. arXiv preprint arXiv:2408.04168, 2024. 12 [57] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. [58] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [59] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. [60] Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon. Med-rlvr: Emerging medical reasoning from 3b base model via reinforcement learning. arXiv preprint arXiv:2502.19655, 2025. [61] Botian Jiang, Lei Li, Xiaonan Li, Zhaowei Li, Xiachong Feng, Lingpeng Kong, Qi Liu, and Xipeng Qiu. Understanding the role of llms in multimodal evaluation benchmarks. arXiv preprint arXiv:2410.12329, 2024. [62] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. [63] Aarti Ghatkesar, Uddeshya Upadhyay, and Ganesh Venkatesh. Looking beyond language priors: Enhancing visual comprehension and attention in multimodal models. arXiv preprint arXiv:2505.05626, 2025. [64] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [65] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In ECCV, 2024. [66] Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding. arXiv preprint arXiv:2412.09616, 2024."
        },
        {
            "title": "Appendix",
            "content": "We provide comprehensive overview in the Appendix, covering key components of our dataset, methodology, and analysis. Specifically, we include the question templates used during dataset construction, sources of transit maps from 30 cities, detailed descriptions of the evaluation algorithm, experimental setup, and in-depth analyses of both correct and erroneous cases. In addition, we further discuss diverse image processing strategies employed by various MLLMs when handling high-resolution visual inputs. We also address the stated limitations of our work, potential broader impacts, ethical considerations related to the dataset, and licensing information for the MLLMs used in our experiments. 14 14 15 15 16 16 17 17 19 20 20 20 Dataset Construction Details A.1 Question Template Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Map Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation Details B.1 Correctness and Quality Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Details about Difficulty-Aware Weighting. . . . . . . . . . . . . . . . . . . . . . . Case Analysis Further Discussions D.1 High-Resolution Image Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . D.2 Limitations and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Broader Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . License and Consent Information E.1 Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Public Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Dataset Construction Details",
            "content": "A.1 Question Template Summary We present one short question template and two long question templates as follows. Short Question Template According to the subway map, how do get from [Stop 1] to [Stop 2]? Provide only one optimal route, with only the line name and the departure and arrival stations. The format should be strictly followed: Route Name: Line Departure Stop: xx Station Arrival Stop: xx Station -- Route Name: Line Departure Stop: xx Station Arrival Stop: xx Station Table A1: Source links for city transit maps used in REASONMAP. We present total of 30 cities sourced from 13 countries. City Source City Source City Source Budapest Lisboa Auckland Kuala Lumpur New York Guiyang Nanchang Hangzhou Hefei Jinan Link Link Link Link Link Link Link Link Link Link Oslo Geneva Sydney Los Angeles Toronto Shanghai Nanning Dalian Beijing Xian Link Link Link Link Link Link Link Link Link Link Rome Dubai Singapore Miami Washington Huhehaote (Hohhot) Shenzhen Kunming Changzhou Changshang Link Link Link Link Link Link Link Link Link Link Long Question Template 1 According to the subway map, how do get from [Stop 1] to [Stop 2]? Provide only one optimal route, and include the number of via stops for each route section (excluding the departure and arrival stops). The format should be strictly followed: Route Name: Line Departure Stop: xx Station Arrival Stop: xx Station Number of Via Stops: -- Route Name: Line Departure Stop: xx Station Arrival Stop: xx Station Number of Via Stops: Long Question Template 2 According to the subway map, how do get from [Stop 1] to [Stop 2]? Provide only one optimal route, including all the via stops. The format should be strictly followed: Route Name: Line Departure Stop: xx Station Arrival Stop: xx Station Via Stops: xx Station, xx Station, xx Station -- Route Name: Line Departure Stop: xx Station Arrival Stop: xx Station Via Stops: xx Station A.2 Map Source We provide the sources of all maps included in REASONMAP for further reference (Table A1)."
        },
        {
            "title": "B Evaluation Details",
            "content": "B.1 Correctness and Quality Evaluation We present the detailed algorithms for evaluating answer correctness and quality in Section 4 (Algorithm 1 for correctness evaluation and Algorithm 2 for quality evaluation). 15 Algorithm 1: Correctness Evaluation Initialize acc 1; if departure stop of first segment = stop1 or arrival stop of last segment = stop2 then acc 0; foreach segment in predicted route do if route name not in the Map Metadata then acc 0; if departure or arrival stop not in the stop list of the route then acc 0; if not the last segment then if arrival stop of current segment = departure stop of next segment then acc 0; return acc B.2 Details about Difficulty-Aware Weighting. Each difficulty pair is assigned predefined weight that reflects its relative challenge level. The full weight matrix is shown below, where the first element in each pair denotes the question difficulty and the second denotes the map difficulty: (\"easy\", \"easy\"): 1.0 (\"easy\", \"middle\"): 1.5 (\"easy\", \"hard\"): 2.0 (\"middle\", \"easy\"): 1.5 (\"middle\", \"middle\"): 2.0 (\"middle\", \"hard\"): 2. (\"hard\", \"easy\"): 2.0 (\"hard\", \"middle\"): 2.5 (\"hard\", \"hard\"): 3.0 This weighting scheme rewards models more for correctly solving harder questionmap combinations, reflecting the increased reasoning complexity they entail, while maintaining moderate differences between buckets to prevent excessive score variance and preserve evaluation stability."
        },
        {
            "title": "C Case Analysis",
            "content": "We provide additional case analyses covering both correct and incorrect predictions, along with detailed comparisons of their respective reasoning processes. We compare Doubao-415 and Doubao-428 (Figure A1), both of which reach the correct destination (from Augustins Station to Poterie Station) but via distinct reasoning paths. Doubao-415 correctly identifies early that both stations are on Line 18 and efficiently converges on the optimal, direct route without transfers. In contrast, Doubao-428 misclassifies Augustins as being on Line 12 and, assuming Poterie is on Line 18, proposes transfer route via Plainpalaisfunctionally correct but suboptimal due to unnecessary complexity. Both models engage in extensive self-correction (7270 tokens for Doubao-428; 4474 for Doubao-415), highlighting the significant downstream impact of earlystage misjudgments. Moreover, visual reasoning limitations persist: despite correctly recognizing Augustins on Line 12, Doubao-415 commits to transfer path and fails to re-evaluate the possibility of direct connection. This indicates room for improvement in both early visual grounding and global route optimality awareness. We then analyze the observed pattern when comparing the full input and text-only variants in the case (in Figure A2). The model with full visual access accurately identifies both stations on the Yellow Line and outputs the optimal direct route with the correct number of via stops. In contrast, the text-only variant makes an early misclassification, placing both stations on the Blue Line (Azul) and constructing plausible but entirely incorrect sequence of intermediate stops. Although the final answer format appears coherent, the underlying logic is flawed due to the initial error in line recognition. This further illustrates the importance of visual input in spatial reasoning tasks, where even minor misinterpretations can lead to fundamentally incorrect conclusions. Additionally, some models, such as the InternVL3 series, default to rejection when visual input is absent. We further present several error cases (in Figure A3), where Doubao-415 still exhibits visual confusion. In contrast, Qwen2.5-VL-32B-I, when lacking visual input, behaves differently from the InternVL3 16 Algorithm 2: Quality Evaluation Initialize map_score 0; if departure stop of first segment = stop1 and arrival stop of last segment = stop2 then map_score map_score + 1; /* Long-question-specific part Initialize Vunion, Vintersection ; Initialize via_stop_score, num_via_stop_score 0; foreach segment pair (answer route, reference route) do if answer route name = reference route name then map_score map_score + 2; if answer departure stop = reference departure stop then map_score map_score + 1; if answer arrival stop = reference arrival stop then map_score map_score + 1; */ /* Long-question-specific part Calculate absolute difference (error) in the number of via stops; num_via_stop_score num_via_stop_score + max(0, 4 error/ max(number of answer via stops, number of reference via stops) 4); */ if answer route name = reference route name then Update Vunion, Vintersection with answer and reference via stops respectively; via_stop_score via_stop_score + number of correctly matched via stops; /* Long-question-specific part via_stop_score min(10, via_stop_score); num_via_stop_score min(10, num_via_stop_score); via_stop_score average( Vintersection/Vunion 10, via_stop_score) map_score map_score + Option(via_stop_score or num_via_stop_score); /* 10 for short question; 20 for long question map_score min(10, map_score)/min(20, map_score) ; if correctness evaluation (acc) = 1 then map_score map_score + 10/map_score + 20; return map_score; */ */ series: rather than rejecting the query outright, it attempts to reason over the available information without producing final answer, while explicitly notifying the user of the missing visual input."
        },
        {
            "title": "D Further Discussions",
            "content": "D.1 High-Resolution Image Preprocessing. We compare how different Multimodal Large Language Models (MLLMs) handle high-resolution image inputs in Table A2. Specifically, we examine three key components in their preprocessing pipelines: dynamic resolution handling, positional encoding, and token compression. 1. Dynamic resolution handling refers to whether the model can directly accept images of arbitrary sizes without resizing or cropping. Most recent models support native resolution processing, enabling them to preserve fine-grained spatial information. In contrast, models like Gemini [49] rely on image tiling and resizing to fit fixed input constraints. 2. Positional encoding helps the model retain spatial structure among visual tokens. Common strategies include 2D Rotary Positional Encoding (2D-RoPE) [65], as seen in Qwen2.517 Figure A1: Case analysis of various MLLMs using REASONMAP (Case N1). For reasoning models, the reasoning process is explicitly marked with <think> and </think> tags. We highlight error contents in the answers with red and correct contents in green. Table A2: Comparison of high-resolution image preprocessing strategies across different MLLMs. We use to denote unspecified or unclear content. Model Dynamic Resolution Handling Positional Encoding Token Compression Qwen2.5-VL series [2] QVQ-72B-Preview [20] InternVL3 series [3] Kimi-VL series [16] Skywork-R1V-38B [17, 18] Gemini [49] Doubao-1.5-Pro series [19] (Tiling+Resize) 2D-RoPE 2D-RoPE V2PE 2D-RoPE - - 2D-RoPE (2 2 Concat + MLP) (2 2 Concat + MLP) (Unshuffle + MLP) (Shuffle + MLP) (2 2 Pooling + MLP) VL [2] and Doubao [19], or flexible alternatives like V2PE [66] in InternVL3 [3]. Some models (e.g., Gemini, Skywork-R1V [17, 18]) do not explicitly disclose their positional encoding scheme, which we mark as in the table. 3. Token compression aims to reduce the number of visual tokens for more efficient processing. Different models adopt different strategies: Qwen2.5-VL and QVQ [20] compress tokens via 2 2 patch concatenation followed by an MLP; InternVL3 [3] and Kimi-VL [16] utilize spatial transformations like pixel unshuffle or shuffle, also followed by MLPs; Doubao averages over 2 2 patches before projection. Models without token compression may incur higher memory and computation costs when processing high-resolution inputs. 18 Figure A2: Case analysis of various MLLMs using REASONMAP (Case N2). For reasoning models, the reasoning process is explicitly marked with <think> and </think> tags. We highlight error contents in the answers with red and correct contents in green. D.2 Limitations and Future Work While REASONMAP provides carefully curated benchmark for evaluating fine-grained visual reasoning with high-resolution transit maps, we acknowledge that it represents only one type of structured visual diagram. As such, caution should be taken when generalizing observations to other domains that involve different types of visual content or reasoning styles. Additionally, although efforts were made to ensure diversity across cities and languages, the current version may not fully capture all geographic or linguistic variations. Future iterations could further expand coverage and explore additional forms of reasoning to enhance generality. D.3 Broader Impact Advancing the capabilities of MLLMs in fine-grained visual reasoning has the potential to benefit wide range of real-world applications, including navigation systems, urban planning tools, and assistive technologies for visually impaired individuals. By offering structured and rigorous benchmark, REASONMAP encourages the development of MLLMs that can more effectively interpret complex visual artifacts and perform spatial reasoning. This could contribute to the long-term goal of building intelligent agents that interact more naturally and safely with human environments. Furthermore, the datasets emphasis on high-resolution, globally sourced transit maps promotes research that is inclusive of diverse visual formats and geographic contexts. We hope REASONMAP can serve as step toward more transparent, robust, and generalizable multimodal systems. 19 Figure A3: Case analysis of various MLLMs using REASONMAP (Case N3). For reasoning models, the reasoning process is explicitly marked with <think> and </think> tags. We highlight error contents in the answers with red and correct contents in green."
        },
        {
            "title": "E License and Consent Information",
            "content": "E.1 Ethical Considerations All experiments are conducted on REASONMAP, which is built using publicly available transit maps collected in compliance with relevant licenses and usage terms. The maps are selected to ensure geographic diversity and legal validity. Upon code release, we provide the source of each map for further reference. REASONMAP is intended solely for academic research on fine-grained visual understanding and spatial reasoning in MLLMs. It does not redistribute any copyrighted map images. All annotations are based on public information, contain no personal data, and are created under academic oversight. The benchmark is not intended for safety-critical use. We take care to ensure fairness, legal compliance, and responsible data handling. E.2 Public Implementation We benchmark the visual understanding and reasoning performance on REASONMAP across diverse set of publicly available MLLMs: KimiVL [16]4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License Skywork-R1V [17, 18]5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License QVQ-72B-Preview [20]6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Qwen License Gemini-2.5-Flash [49]7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Closed-Source InternVL-3.0 [3]8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License 4https://github.com/MoonshotAI/Kimi-VL. 5https://huggingface.co/Skywork/Skywork-R1V2-38B. 6https://huggingface.co/Qwen/QVQ-72B-Preview. 7https://deepmind.google/technologies/gemini. 8https://github.com/OpenGVLab/InternVL. 20 Qwen2.5-VL [2]9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache 2.0 License Doubao-Pro 1.5 [19]10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Closed-Source OpenAI o3 [47]11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Closed-Source OpenAI 4o [48]12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Closed-Source To ensure fair and reproducible evaluation, we implement all inference procedures by adhering closely to the official documentation and recommended practices of each model. The code is released under the MIT License to support transparency and reproducibility. Additionally, we provide detailed usage instructions on the project website to ensure easy access and reproducibility for future users. 9https://github.com/QwenLM/Qwen2.5-VL. 10https://www.volcengine.com/product/doubao. 11https://platform.openai.com/docs/models/o3. 12https://platform.openai.com/docs/models/gpt-4o."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "National University of Singapore",
        "Westlake University",
        "Zhejiang University"
    ]
}