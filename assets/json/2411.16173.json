{
    "paper_title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis",
    "authors": [
        "Junho Kim",
        "Hyunjun Kim",
        "Hosu Lee",
        "Yong Man Ro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence. In this paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process. We address two main challenges to achieve it: (i) We present the SceneWalk dataset, a high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context. (ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries. Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses. Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 3 7 1 6 1 . 1 1 4 2 : r SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis Junho Kim* Hyunjun Kim*"
        },
        {
            "title": "Hosu Lee",
            "content": "Yong Man Ro Integrated Vision and Language Lab, KAIST, South Korea {arkimjh, kimhj709, leehosu01, ymro}@kaist.ac.kr https://ivy-lvlm.github.io/SALOVA"
        },
        {
            "title": "Abstract",
            "content": "Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence. In this paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process. We address two main challenges to achieve it: (i) We present the SceneWalk dataset, high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context. (ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries. Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses. Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences. 1. Introduction advancements Recent in Large Language Models (LLMs) [22, 43, 44] have brought us one step closer to achieving Artificial General Intelligence (AGI). Next following step, the current trend is shifting toward modular systems that integrate various multi-modality, leveraging *Equal contribution. Corresponding author. the exceptional generalization and reasoning capabilities of LLMs to evolve into Large Multi-modal Models (LMMs). Accordingly, users can unrestrictedly interact with the models across various modalities beyond text, expanding the scope of machine understanding and enhancing user engagement. Especially, considering the widespread adoption of long-form videos across various web platforms, the importance of understanding long, untrimmed video has become increasingly prominent in the multi-modal domain. After the pioneer works [14, 37, 38] utilizing visual instruction tuning to augment vision perception into LLMs, remarkable strides [11, 17, 65] have been made in aligning cross-modal consistencyespecially between vision and language domains. Albeit more recent models [29, 69] integrate various vision modalities all at once, current approaches still face significant challenges in understanding untrimmed and long-form video content. The main challenge is attributed to the limited context length of LMMs, which is an inherent structural limitation that restricts the models to process only finite number of tokens as the input sequences. We can exemplify that LLaVA series [29, 30], when processing video data, require 144 visual tokens per each frame, where numerical approximation is only maximum of 56 frames using 8K max context length LMMs, which is still limited to handle long sequnce data. Accordingly, current video-LMMs [12, 35, 39] rely on (i) sparse frame sampling to represent entire videos [26, 72], (ii) dense compression of visual tokens into smaller size to manage the excessive number of frames [34, 41], and (iii) adaptive pooling strategies [59, 60] based on the SlowFast approach [19], all aimed at fitting the long video sequences within the limited context window of LMMs. Several studies focusing on the long video understanding task have presented memory-augmented generation [23, 53] utilizing an additional buffer to embed long-term information, or have extended the context using RoPE-based frequency extension during the training [70]. Despite of such endeavors, when handling massive video frames, previous works still confront restricted context size and significant mem1 ory overhead, which leads to substantial visual information loss. As critical events may be overlooked by the models, this hinders their ability to fully capture context changes in lengthy videos, resulting in inaccurate and irrelevant responses for the user queries. Starting from the intuitive insight outlined below, in this paper, we propose retrieval-driven approach for long video understanding with LLMs. Analogous to the recent Retriever-Augmented Generation (RAG) systems [28] (widely adopted in LLMs), which retrieve relevant information from external factual knowledge, humans naturally employ similar strategies when seeking specific information, efficiently locating and referring necessary materials to answer targeted questionse.g., imagine that we are taking open-book exams or searching for certain recipe in cookbook. Given long and untrimmed video, mirroring the targeted retrieval processes, we introduce novel framework, Segment-Augmented LOng Video Assistant (SALOVA) to effectively handle the long sequence visual inputs by retrieving the relevant video segments. To construct our video segments retrieval framework, central challenge hinges on establishing two main components: (i) Densely captioned video data, which consists of video-caption pairs with progressively-captioned descriptions that change throughout each video used to train the model to accurately identify relevant video segments. (ii) Dynamic routing mechanism, which selects pertinent video segments for the queries, followed by being connected to LLMs. To address that, our approach is outlined as follows. Data. (3) Recently several video-text paired datasets [4, 6, 8, 10, 56, 62] have been released, but they are inadequate for handling long and untrimmed video data, where only partial video moments are described with limited word length as compared in Fig. 1(a). To handle such insufficiency of detailed descriptions within the videos and the short durations of both videos and texts, we introduce the SceneWalk dataset, new high-quality video dataset with thorough captioning for each video. It includes dense and detailed descriptions for every video segment across the entire scene context. The SceneWalk dataset, sourced from long and untrimmed 87.8K YouTube videos (avg. 486 seconds each), features frequent scene transitions across total of 11.8K hrs video duration and 1.3M massively segmented video clips. Each video segment in the dataset is provided with detailed description (avg. 137.5 word length), generated by combining pre-trained models [54, 73] and manual curation from human. Architecture. (4) Utilizing the constructed video dataset, SALOVA learns to identify relevant video segments for the given queries within each video source and then auto-regressively predicts the next token. To do so, we present two architectural designs to seamlessly incorporate the retrieved segments in an end-to-end training: SpatioTemporal Connector and Segment Retrieval Router. By focusing on the relevant segments, our framework can perform deeper reasoning without being constrained by context length limitations. Additionally, we present FocusFast approach, which intensively analyzes the selected segments for detailed comprehension (focus pathway), while quickly accessing overall contextual information with routing tokens obtained from the entire video segments (fast pathway). The strategy ensures SALOVA to maintain comprehensive video understanding while prioritizing details where it is most needed, effectively enhancing long and untrimmed video interpretation. Through extensive experiments and analyses, we corroborate that competitive performance of SALOVA to the existing video-LMM models in understanding complex longform videos. Also, our results show significant reductions in the loss of crucial visual information and lower risk of omitting important events, demonstrating the effectiveness of our proposed method across various video benchmarks. Our contribution can be summarized into three-fold: We introduce the SceneWalk dataset, high-quality and densely-captioned video dataset with detailed segmentlevel descriptions from 87.8K long and untrimmed video sources. The proposed dataset provides rich context and scene continuity, enabling effective training for long-form video understanding. We propose Segment-Augmented LOng Video Assistant (SALOVA), novel video-LMM framework designed to enhance long video comprehension by targeting relevant video segments in lengthy videos, optimizing the models focus on essential segment targets for the given queries. Through extensive evaluation, we validate that SALOVA improves overall long video understanding capabilities by effectively integrating relevant video segments, thus optimizing to handle long and untrimmed video content. 2. Related Work 2.1. Large Multi-modal Models After the emergence of LLMs [5, 55], which can actively interact with users through back-and-forth conversations, as next leap, various research efforts [2, 25, 31] integrate different modalities into the LLMs, utilizing their core reasoning and zero-shot capabilities. Building on the opensourced models [13, 55], seminal works [14, 38, 65] have bridged image and text modality under the visual instruction tuning and presented multi-modal assistant models that possess visual perception and QA capabilities. Since then, numerous research studies have been introduced to (i) enhance vision understanding with advanced architectures [17] or higher resolutions [30, 37], (ii) implement more sophisticated alignment layers [7, 42] between modalities, and (iii) train the models with more high-quality data and larger 2 (a) Video-Text Dataset Comparison (b) SceneWalk Statistics (c) Overall Pipeline for Data Collection Figure 1: The overview of the SceneWalk dataset includes (a) dataset comparison, (b) detailed statistics, and (c) the annotation pipeline for description and score collection. Note that the scale of circles in Fig. 1(a) indicates the data size, and the color distribution in Fig. 1(b) denotes the video duration in each video categorybrighter colors correspond to shorter video durations. Further details about the dataset are provided in Appendix A. model parameters. Recent focus has shifted towards more unified modality processing following the release of omnivorous models [46]. Some recent omni-versions of LMMs [29, 64] can handle combinatorial subsets from various modality sources, such as images, videos, audio, speech, and depth. However, the current LMMs for video [32, 35, 41] still lack of capturing the necessary details to effectively process video information due to their sparse frame sampling strategy. While such approach is seemingly adequate for relatively shorter videos, it may fail to capture comprehensive spatio-temporal information, potentially compromising the accuracy of model responses to user queries. In this paper, SALOVA first retrieves relevant video segments, then concentrates on more granular video cues. Such targeted focus allows the model to effectively understand complex analysis within the videos, significantly improving its ability to provide contextual-aware and accurate responses. 2.2. Long Video Understanding In parallel, we detailedly introduce video-specialized models [12, 23, 53] integrated with LLMs, which have also been widely explored these days to enhance video understanding and reasoning. Here, the most challenging part of current video-LMMs lies in handling long video sequences, mainly due to the limited context length of the LLMs. This limitation compels the models to sparsely sample the video frames in only limited sizes (e.g., typically 8 or 16 frames), potentially missing important spatial and temporal information. To address this, several studies have focused on compressing visual tokens into more manageable size, proposing aggregation [34, 41] or pooling methods [35, 59] with advanced vision encoder structures [67, 73]. In addition, memory-augmented methods [23, 53] first stored long-term information in memory bank, then responded to specific queries by loading memory features from the stored buffer. On the other hand, among more recent approaches, Li et al. [70] have directly extended the LLMs context length by exploiting RoPE-based frequency interpolation, and Xue et al. [61] have introduced sequence parallelism that can be implemented on multiple GPUs by modifying backend systems. However, we argue that current approaches inherently cannot be free from the fixed context length and provoke intensive memory demands when processing more Instead, by focusing on the relevant seglonger videos. ments within the entire video, SALOVA can efficiently handle the limited context length, enabling targeted processing of key moments without the need for excessive memory consumption, thereby enhancing performance on longer video sequences. 3. SceneWalk Dataset In this section, we elaborate on how we collected the SceneWalk dataset. The overall pipeline for building the dataset and summarized statistics are illustrated in Fig. 1. While several video SFT datasets [32, 41] are widely used during the instruction tuning stage, they often fail to capture comprehensive details within the scenes. This stems from the nature of instruction-type questions, which provide only partial information, and the brief lengths of both videos and texts in QAs. In contrast, the SceneWalk dataset offers densely captioned video-text pairs that cover long sequence videos in full details, as shown in Fig. 1(a). For further detailed data statistics, please see Appendix A. 3.1. Data Gathering and Processing Video Source & Filtering. For the long and untrimmed video sources, we primarily focus on three key aspects to build densely captioned video dataset: (i) extensive video length with diverse video source categories, (ii) high-quality video contents, (ii) frequent scene transitions within each 3 video. Accordingly, our data collection is mainly sourced from YouTube, ensuring rich dynamic content that better reflects real-world complexities experienced by global usershere, because our main goal for video gathering is on complex scene understanding, we exclude low-quality and user-uploaded aesthetic videos (e.g., WebVid, Pixabay, Pexels, and etc,.) that are rather beneficial for video generation tasks, despite their merits for easy collection. We have collected YouTube urls from [27] and downloaded the whole video in untrimmed states. Among the total 32 coarse and diverse video categories YouTube API provided, we selectively curated 10 categories, excluding categories such as News & Politics, Classics, and Documentary, due to their static nature, which provides only sparse temporal information in the videos. We further supplemented the dataset with additional Movie & Drama videos sourced from [21, 53], totaling 87, 867 video sources with 11.87 Khrs video duration (avg. 486.5-seconds). Segmenting Video into Clips. Next, for the collected long and untrimmed video sources, we cut the lengthy videos into small segments to densely caption the entire video in Instead of adopting the bottom-up approach next phase. used in the ShareGPT4Video dataset [8], which segments videos into fixed time intervals (2-seconds) in advance and then merges adjacent frames based on their CLIP similarity [48], we directly employed PysceneDetect 1 to segment the videos, dynamically adjusting the threshold based on the raw-level video information to reliably detect scene changes. At the end, the total number of 1.29M of video segments with 33.11-seconds average video length is extracted from the original video sources. 3.2. Captioning and Scoring Dense Segment Captioning. After obtaining the massive video segments, our next goal is to caption each segment with visual details and narrative context to capture the scene-specific explanations, which can enrich scene-level interpretation. To achieve this, we plan to utilize pre-trained LMMs to generate detailed descriptions for the partial video segments. As the captioner, we empirically found that VILA-1.5 (13B) [36] shows competent descriptive quality than other open-sourced models, and used to generate dense captions for each video segment with randomly sampled instructions for detailed descriptions. As result, we acquire 1.29M pairs of detailed descriptions corresponding to the video segments, each description with average 137.5 word length. Please see instruction details and qualitative examples of generated captions in Appendix A. Scoring Video-Text Correspondence. Lastly, we score the correspondence between the video segments and the paired 1We use AdaptiveDetector with default setup in https://github. com/Breakthrough/PySceneDetect dense descriptions, which will later be used as explicit supervision to robustly train our retrieval framework. What we must not overlook here is that the paired video-text relationship is not solely one-to-one correspondence but is more akin to generalized bipartite matching. That is, within the long and untrimmed video source, each video segment can be connected to other descriptions with additional edges. Therefore, for the Nv number of video segments and their paired segments, we can construct {Nv}2 correspondence matrix between video-text (V2T). To measure each correspondence, we employ LanguageBind [73] due to its competitive alignment capabilities across various modalities. In addition, we build another {Nv}2 matrix to provide doubly robust measure for the correspondence scores among adjacent descriptions (T2T) by comparing similarity within the textual context using the SBERT model [54]. 4. Segment-Augmented LOng Video Assistant Network Overview. For given set of Nv video segments sampled at 1 FPS v={vi}Nv , where each segments vi RTiHW has varying video length (summing up to the total time of long and untrimmed video), SALOVA consists of four main architecture components as illustrated in Fig. 2: Vision Encoder: We use CLIP-ViT-L-336px [48] to extract visual features, followed by 2x2 average pooling, resulting in 144 visual tokens for each frame. Spatio-Temporal Connector: To handle spatio-temporal features of varying lengths from the vision encoder, we employ the Perceiver Resampler [2], which consists of 2-layer Transformer architecture followed by 2-layer MLP with GELU activation as projector. This resampler embed each video segment feature into fixed size latent features that are connected to LLMs. Segment Retrieval Router: For the given textual queries, retrieval structure (2-layer Transformer) gathers representative information (i.e., routing tokens) from each video segment and then routes the query-relevant video features into the LLMs. Note that the router architecture is trained in an end-to-end manner. Large Language Model: We select two open-sourced sizes, LLaMA-3.2 LLMs with varying parameter (3B) [18], Phi-3.5 (3.8B) [1] and Qwen-2.5 (7B) [63], both of which are instruction-tuned models that possess QA assistant capabilities. 4.1. Long Video Processing and Pipeline 4.1.1. Spatio-Temporal Connector The first component of our model, Spatio-Temporal Connector, efficiently handles long and variable-length input video segments by extracting each segments visual semantics in fixed-size latent vector. As illustrated in Fig. 2(b), 4 Figure 2: The network overview of SALOVA. Our framework consists of four structural components: vision encoder, STconnector, SR-router, and LLMs. Using the FocusFast strategy, our model can concentrate on more detailed local information while maintaining context awareness. we first sample video frames at 1 FPS from each video segments, then visual features are acquired with 2 2 pooling (thus, 144 tokens from each frame). After that, the visual features are flattened and fed into the ST-Connector with additional positional and temporal encoding. Here, when the long video is processed, the number of unfolded patch tokens becomes extremely large, leading to exhaustive computations. To address this, we employ dynamic token drop technique to reduce computational load. Dynamic Token Drop. To effectively manage long video sequences, the token drop has been utilized in video generation tasks [16, 40]. Expanding such approach, in our framework, the dropout rate is dynamically adjusted based on the length of the input sequence Ti in the input visual feature fv Ti HpWp d, which allows for more efficient processing of longer sequences by reducing computational demands, while still preserving dense visual semantics in shorter videos. Additionally, to retain spatio-temporal information from the dropped patches, we add positional embeddings separately along the spatial and temporal axes. This enables more refined extraction of spatio-temporal visual semantics even after reducing the number of tokens. 4.1.2. Segment Retrieval Router Next, the key to conveying the pertinent video information to LLMs is retrieving relevant video segments by querying sentence. To densely cue the similarities between the video and sentence information, we introduce routing framework, Segment Retrieval Router, which consists of 2layer Transformer as illustrated in Fig. 2(c). After obtaining the routing tokens R={Ri}Nv RNvD from entire video segments, we aggregate them and feed into the SR5 Router as queries. For the given sentence, we employ the same text encoder used for the vision encoder and project it into the shared embedding space to obtain sentence features RNtD, where Nt indicates textual length. Using the cross attention mechanism (q: R; k/v: S), we can estimate similarity scores between the video segments and given sentence queries (i.e., V-T similarity). The scores enable the SR-Router to prioritize and select the most relevant video segments that align with the sentence query. Retrieval Objective. To seamlessly train the SR-Router with the mainstream flows of SALOVA in an end-to-end manner, we have designed similarity loss function Lsim that minimizes the distance between the high-dimensional embeddings of the video segments and sentence queries. Here, we use the correspondence scores (aforementioned in Sec. 3.2) as retrieval supervision signal yi, after applying one-hot encoding. We incorporate simple margin-based loss, commonly used in contrastive learning settings [33], which enables the model to learn off-diagonal relaxation in the correspondence matrices between video segments and sentences. As mentioned earlier, the relationship between paired videos and sentences is closer to generalized bipartite matching than to one-to-one matching, so relaxation learning helps to accommodate the inherent complexity in aligning correspondence. In conclusion, with the binary crossentropy loss and the score margin loss, we can formulate the similarity loss as follows: Lsim = Lbce(yi, si)Nv i=1 (cid:125) max (cid:0)0, δ (sp (cid:123)(cid:122) score margin loss (cid:123)(cid:122) point-wise CE + 1 Ns (cid:124) )(cid:1) (cid:125) sn (cid:80) (cid:124) , where sp and sn (1) indicate randomly sampled scores from positive and negative pairs, respectively, and δ denotes the margin parameter (set as 0.2). Note that the similarity loss is trained in conjunction with the auto-regressive loss Lar from subsequent LLMs in an end-to-end manner. 4.1.3. FocusFast Pathways: Integration to LLMs Using the routing tokens, we can calculate the similarities of each video segment for the given query. Leveraging the similarities, SALOVA efficiently retrieves the specific video features that exhibit the highest relevance score to the textual query, where the indexed video features are then directly integrated into the LLM architecture. Here, extending the SlowFast pathways concept [19], we present the FocusFast mechanism to effectively manage the processing pathway for the retrieved video segments: (i) Focus pathway concatenates the top-K most pertinent features to construct comprehensive video representation, capturing local details across retrieved segments and enabling detailed interactions with textual queries to enhance handling complex video information. (ii) Fast pathway focuses on the more broader-level context by employing segment-wide routing tokens as the condensed global representation. It effectively contains dynamic spatio-temporal changes throughout the video stream, thereby allowing SALOVA to understand the overall video content and scene-level continuity awareness. Once the most pertinent features are retrieved, they are delivered to the LM backbone for the final processing as in Fig. 2(a), integrating video-specific details into the models responses. By effectively handling long and untrimmed videos with the proposed retrieval and routing mechanism, SALOVA can maintain the flow of salient information without the processing overhead for less related data, thus generating more context-aware responses. 4.2. Training Strategies The current training strategies for LMMs predominantly consist of two-step training: (i) cross-modal alignment and (ii) visual instruction tuning. Recently, Li et al. [29] have emphasized the importance of high-quality knowledge learning between the two training stages (thus stage 1.5), pointing out that the models cannot enoughly learn necessary knowledge during the alignment with the lowqualitative web-scale image-text data. As the similar approach of using rephrased descriptions for additional knowledge learning [29], we employ the newly collected SceneWalk dataset as the parametric knowledge injection step, which enables the SALOVA to learn detailed spatial and temporal representation from the long sequence video data before the instruction tuning. Accordingly, our training recipes and data configuration can be divided into three steps as follow (Please see the training details in Appendix B): Stage 1: Cross-modality Alignment. For the initial step in modality alignment, we utilize 790K image/video-text paired dataset: (i) 558K image-text pairs from the CC3M dataset [52], filtered by LLaVA [38] and (ii) video-text pairs sampled from the WebVid 2.5M subset [4]. We freeze vision encoder and LLMs during the training, and mainly focus on optimizing the connector and router to map the visual information into the textual space. Stage 1.5: Long Video Knowledge Injection. As an intermediate training step, we use the SceneWalk dataset to train the SALOVA, unfreezing all trainable parameters except for the vision encoder. During training, we input the long and untrimmed video instances and follow the processing pipeline shown in Fig. 2. By training the model with densely captioned video-description pairs, it acquires high-quality parametric knowledge of both spatial and temporal information. In addition, through the aforementioned retrieval process, the model learns to target video segments that are mostly relevant to the video description. Stage 2: Video Instruction Tuning. To possess QA capabilities in SALOVA, we use extensive video instructiontuning data as the final training step. The instruction data are mainly sourced from four different datasets: LLaVAVideo-178K [71], NeXT-QA [58], ActivityNetQA [66], and PerceptionTest [47]comprising total of 1.4M videoinstruction QA data, including caption entries, open-ended QA, and multiple-choice QA. Note that we train all the network parameters during this stage and auto-regressively update the instruction-following assistants response for the next word prediction. 5. Experiments 5.1. Experimental Details Implementation. For the vision encoder and text encoder of the SR-Router, we utilize the CLIP-ViT-L model [48] with resolution size of 336. We employ 2-layer transformer with head size of 2 for the ST-Connector, which has latent dimension of 256. The token drop mechanism is dynamically applied according to video length, with varying maximum drop rates for each training stageStage 1 has no token drop, Stage 1.5 up to 0.7, and Stage 2 up to 0.4. For the configuration of SR-Router, we set 2-layer of transformers with single head, and top-K number is set to 5 during the stage 2. Following [30, 37], the projector layer consists of 2-layer MLP with GELU. Our LLM backbones are (i) 3B: Llama-3.2-3B [18], (ii) 3.8B: Phi-3.5-mini [1], and (iii) 7B: Qwen2.5-7B [63]. Training Details. For the each training stage, we train SALOVA for 1 epoch with 1 node of 8 A100 GPUs. The total training hours for 3B/3.8B and 7B models roughly take 5 and 7 days, respectively. We employ FlashAttention-2 [15], gradient checkpointing [9], and ZeRO-2 [49] to minimize 6 Model #param Video-MME LVBench Short Medium Long Overall Acc. (val) Model #param ActivityNetQA VideoChatGPT MVBench test (acc/score) test (acc) test (acc) Proprietary LMMs GPT-4V [45] GPT-4o [46] Gemini 1.5 Pro [50] Open-sourced LMMs ST-LLM [39] VideoChat2 [32] ShareGPT4Video [8] Video-LLaVA [35] Chat-UniVi-V1.5 [26] Qwen-VL-Chat [3] ShareGemini [51] SliME [72] PLLaVA [59] VideoLLaMA2 [12] Ours SALOVA-Llama SALOVA-Phi SALOVA-Qwen n/a n/a n/a 7B 7B 8B 7B 7B 7B 7B 8B 7B 8B 3B 3.8B 7B 70.5 80.0 81. 45.7 48.3 48.3 45.3 45.7 46.9 49.1 53.3 - 56.0 48.3 47.1 52.3 55.8 70.3 74.3 36.8 37.0 36.3 38.0 40.3 38.7 41.3 42.7 - 45.4 46.3 48.8 50.9 53.5 65.3 67. 31.3 33.2 35.0 36.2 35.8 37.8 39.1 39.8 - 42.1 41.1 44.1 46.8 59.9 71.9 75.0 37.9 39.5 39.9 39.9 40.6 41.1 43.2 45.3 - 47.9 45.3 46.7 50.0 - 66.7 64. - 39.3 39.7 39.1 - - - - 40.2 - 41.4 41.6 43.5 Table 1: Detailed results for the Video-MME benchmark (w/o subtitles) and LongVideoBench. The best results are highlighted in bold and the runner-up results are underlined. the memory footprint associated with model components (i.e., gradient, activation, and optimizer states). Additionally, we fine-tune the trainable parameters at each step without employing LoRA [24]. For the extended training configuration, we have attached the details in Appendix C. Evaluation Benchmarks. We evaluate our model using two types of video analysis benchmarkslong video understanding and general video understanding, categorized based on the video length. For the long video benchmark, we primarily utilize Video-MME [20] and LongVideoBench [57], both of benchmarks includes videos up to two hours long duration. As the general video analysis evaluations, we employ various benchmarks such as ActivityNetQA [66], VideoChatGPT [41], and MVBench [32]. Note that the same pipeline is used to obtain video segments for each benchmark, and all benchmarks are sampled at 1 FPS without token drop during inference. As comparison baseline, considering academic budget constraints, we evaluate against models that have similar parameter size. 5.2. Experimental Results Results on Long Video Understanding. Video-MME [20] evaluates LMMs with focus on video analysis across variety of video types and durations. We primarily compare the benchmark results in settings without subtitles, relying solely on video frames. Therefore it can assess the LMMs visual comprehension capabilities rigorously, based purely on visual content. Also, LongVideoBench [57] is designed to assess LMMs understanding of long-duration videos up to two hours. It includes diverse collection of videos, challenging the models ability to process and interpret extensive visual and contextual information across variety of themes. As shown in Tab. 1, our model shows competent video understanding performance across all video Proprietary LMMs GPT-4V [45] GPT-4o [46] Gemini 1.5 Pro [50] Open-sourced LMMs VideoLLaMA [68] VideoChatGPT [41] MovieChat [53] Chat-UniVi [26] LLaMA-VID [34] VideoChat2 [32] VideoLLaMA2 [12] Ours SALOVA-Llama SALOVA-Phi SALOVA-Qwen n/a n/a n/a 7B 7B 7B 7B 7B 7B 8B 3B 3.8B 7B 57.0 61.9 57. 12.4 35.2 45.7 46.1 47.4 49.1 50.2 52.6 51.1 55.6 - - - 1.1 2.7 - 3.2 3.3 3.3 3.3 3.4 3.5 3.6 4.06 - - 2.16 2.42 2.67 2.99 2.89 2.98 3.13 3.08 2.83 3.09 43.5 - - 34.1 32.7 - - 41.3 51.1 54.6 51.7 46.4 52.6 Table 2: Comparison results for generic video understanding benchmarks. The best results are highlighted in bold and the runner-up results are underlined. length distributions in Video-MME and lengthy video instances in LongVideoBench. Notably, we highlight that SALOVA achieved significant performance in the medium (average 562.7 seconds) and long (average 2385.8 seconds) length categories in Video-MME benchmark, even with more smaller size of backbone LM parameters compared with the baseline models. Such performance gains in long video instances are attributed to our models dynamic capability to retrieve and process only the relevant video segments, enabling it to handle lengthy video content efficiently without being constrained by the limited context length. Especially, the routing mechanism in SALOVA strategically prioritizes video segments that are likely to contain crucial visual and contextual cues relevant to the query. This selective routing mechanism reduces the computational load and minimizes the information loss that commonly occurs in current videoLMMs trying to process extensive video data in entirety. Results on General Video Understanding. Using benchmarks such as ActivityNetQA [66], VideoChatGPT [41], and MVBench [32], SALOVA was evaluated across various video types to assess its general video understanding capabilities. As shown in Tab. 2, SALOVA demonstrated competent performance, comparable to existing video-LMMs, especially in dynamic and shorter video sequences. On ActivityNetQA, the model effectively utilized its segment retrieval strategy to provide focused and contextually appropriate responses, which helped maintain accuracy. This approach was similarly effective in the multi-modal settings of VideoChatGPT and MVBench, where SALOVA showed consistent performance in handling dialogues and visual cues. These outcomes highlight SALOVAs capability to process general video content efficiently through its dynamic routing mechanism, offering reliable solution that balances computational resources with output quality. 7 Ablation frm sample 8 frm 16 frm 1 fps 1 / 1.5 / 2 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) FastFocs (cid:33) Video-MME Short: 2m Mid: 4-15m Long: 30-60m Overall : Video frame sampling (w/o SR-Router) 42.0 42.8 46.3 48.3 50.0 48.3 37.2 38.0 41.1 42.5 43.6 45.3 : Train stage - Long video knowledge injection 45.6 48.3 43.7 46.3 : Local-global video representation 36.4 48.3 38.6 46.3 40.2 41. 35.6 41.1 43.6 45.3 36.9 45.3 (a) SALOVA-Llama-3B (16 frm sample) Table 3: Ablation studies on SALOVA configuration. We utilize SALOVA-Llama model for the experiments. 5.3. Additional Analyses on SALOVA Ablation Study. We conduct ablation studies on three components as follows: (i) different video frame sampling strategies, (ii) intermediate training stage for long video knowledge injection, and (iii) the FocusFast mechanism to understand branched local-global representation in videos. As shown in Table 3, we first observe that using more frames significantly enhances performance, particularly in understanding long-form videos. This aligns with our key insights on managing long videos, suggesting that higher frame count can provide more spatio-temporal information and improve the models response without losing contextual information within the video. Additionally, we compare with baseline trained with stage 1-2 (skipping stage 1.5). Here, we highlight the effectiveness of the SceneWalk dataset as an intermediate training step to enhance parametric knowledge for the long video analysis by allowing the model to learn from high-quality and densely captioned scene-level information, which is crucial for adapting to various lengths and contexts. Lastly, we conduct an analysis on the FocusFast method and demonstrate its efficacy in analyzing not only local details from relevant video segments but also in understanding the global video context through the simultaneous use of routing tokens, thereby facilitating more comprehensive understanding of video content. Analysis of Retrieving Segments. By retrieving relevant video segments for the given queries, SALOVA can effectively target salient information in the long video and retain long context information. To further demonstrate the models targeting capabilities beyond numerical performance in long video analysis, we explore our models application in the Visual Needle-In-A-Haystack (V-NIAH) task [70], which extends the Needle-in-a-Haystack (NIAH) evaluation for LLMs to vision-level benchmark. This task is particularly challenging as it requires models to not only detect but also precisely retrieve the sparse yet crucial visual cues scattered across lengthy videos. As shown in Fig. 3, we compare our model to baseline trained on sparsely sampled frames (16 frm, without SR- (b) SALOVA-Llama-3B (1 fps sample) Figure 3: Comparison results of V-NIAH. The x/y-axis indicates the total video frames and the location of needle image within the video, respectively. Router). Our framework effectively identifies and extracts relevant video segments from densely packed content, even when handling long context lengths. These results highlight SALOVAs robustness in managing complex, longform videos, maintaining contextual continuity and relevance by strategically focusing on critical segments in response to user queries. 6. Discussion and Conclusion Discussion. Despite SALOVAs competence in handling extended video sequences, it is important to recognize scenarios where its complex architecture may not be necessary. Specifically, for shorter videos where sparse sampling suffices to capture essential spatio-temporal information, simpler models could potentially outperform the efficiency of SALOVA without necessitating its extensive processing capabilities. This suggests future avenue for integrating hybrid approach based on our framework by dynamically adjusting the complexity of the retrieval and processing mechanisms based on the video length and content density. Conclusion. In this paper, we introduce SALOVA, novel framework designed to enhance the comprehension of long and untrimmed video by leveraging retrievaldriven approach with new densely captioned dataset, the SceneWalk dataset. SALOVA strategically targets and processes only the relevant video segments, effectively addressing the structural limitations of current Video-LMMs with its Spatio-Temporal Connector and Segment Retrieval Router. Through extensive evaluation on various benchmarks, SALOVA exhibits its robust performance in interpreting complex video content, enhancing efficiency, and improving the understanding of extended videos."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 4, 6 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 2, 4, 3 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. 7 [4] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 2, 6 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 2 [7] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. arXiv preprint arXiv:2312.06742, 2023. 2 [8] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. 2, 4, 7, [9] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. arXiv Training deep nets with sublinear memory cost. preprint arXiv:1604.06174, 2016. 6 [10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 2 [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1 [12] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1, 3, [13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 2 [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose visionIn Advances in language models with instruction tuning. Neural Information Processing Systems, 2023. 1, 2 [15] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 6 [16] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Ibrahim AlabdulJoan Puigcerver, Robert Geirhos, mohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024. [17] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprearXiv preprint hension in vision-language large model. arXiv:2401.16420, 2024. 1, 2 [18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 4, 6 [19] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 62026211, 2019. 1, 6 [20] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 7, 6 [21] Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, and Ivan Laptev. Short film dataset (sfd): benchmark for storylevel video understanding. arXiv preprint arXiv:2406.10221, 2024. 4, 1 [22] Google. Gemini, 2023. 1 [23] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model In Proceedings of the for long-term video understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514, 2024. 1, 3 [24] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 9 [25] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36, 2024. 2 [26] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. 1, 7 [27] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. 4 [28] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. 2 [29] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 3, [30] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 1, 2, 6 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational Conference on Machine Learning. PMLR, 2023. 2 [32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 3, 7 [33] Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, and Yongdong Zhang. Momentdiff: Generative video moment retrieval from random to real. Advances in neural information processing systems, 36, 2024. 5 [34] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2025. 1, 3, 7 [35] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1, 3, 7 [36] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 4, 1 [37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 2, 6, 3 [38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, 2023. 1, 2, 6 [39] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 118. Springer, 2025. 1, 7 [40] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 5 [41] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1, 3, [42] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 2 [43] OpenAI. ChatGPT. https://openai.com/blog/ chatgpt/, 2023. 1 [44] OpenAI. Gpt-4 technical report, 2023. 1, 3 [45] OpenAI. GPT-4V(ision) System Card, 2023. 7 [46] OpenAI. Hello gpt-4o, 2024. 3, 7 [47] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 6 [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4, 6 [49] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [50] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 7, 3 [51] Share. Sharegemini: Scaling up video caption data for multimodal large language models, 2024. 7 10 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50365045, 2022. 2 [63] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 4, [64] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding arXiv preprint large language models. in multi-modal arXiv:2408.04840, 2024. 3 [65] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 1, 2 [66] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 6, 7 [67] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 3 [68] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 7 [69] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 1 [70] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 1, 3, [71] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 6 [72] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 1, 7 [73] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In The Twelfth International Conference on Learning Representations. 2, 3, 4 [52] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. 6 [53] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 1, 3, 4, 7 [54] Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence In Proceedings of the 2021 Conference of scoring tasks. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 296310, Online, 2021. Association for Computational Linguistics. 2, [55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [56] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2 [57] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. interLongvideobench: benchmark for long-context arXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 7, 3 [58] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 6 [59] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 1, 3, 7 [60] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Slowfast-llava: strong training-free baseDehghan. arXiv preprint line for video large language models. arXiv:2407.15841, 2024. [61] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. 3 [62] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with In Proceedings of the large-scale video transcriptions. 11 SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Details of SceneWalk Dataset A.2. Pipeline for Dense Caption A.1. Detailed Data Statistics We provide comprehensive analysis of the proposed SceneWalk dataset, focusing on detailed data statistics, including video duration, categorical distribution, and segmentlevel descriptions. The information emphasizes the versatility and diversity of the dataset, ensuring its applicability for training our video-LLM. Dataset Composition. The SceneWalk dataset comprises 87,867 long-form video sources, spanning total of 11.87 Khrs (average video duration: 486.5 seconds). The video sources are collected from curated selection of 10 diverse categories as in Fig. 1 sourced primarily from YouTube, with additional contributions from Movie & Drama datasets [21, 53]. This ensures wide range of real-world scenarios, avoiding static categories. Figure 4: Detailed video duration range statistics for each video category in the SceneWalk dataset. Video Duration Distribution. The collected videos can be split into three distinct duration ranges to analyze temporal diversity: (i) 0240 seconds (short): This range constitutes about 24.4% of all segments, (ii) 240600 seconds (long): This intermediate range accounts for the largest proportion, approximately 46.1% of the dataset, and (iii) 6002280 seconds (extreme-long): The longest duration range comprises around 29.5% of the dataset. The distribution of video durations for each video category is illustrated in Fig. 1(b) (outer circle), and more detailed duration distributions can be found in Fig. 4. Splitting into Video Segments To divide untrimmed and long video sources into massive 1.29M video segments, we directly utilize PySceneDetect with the AdaptiveDetector using the default adaptive threshold (3.0), which compares the difference in content between adjacent frames similar using rolling average of adjacent frame changes. This can help mitigate false detections in situations such as fast camera motions. Instructions of Dense Segment Captioning. To generate detailed descriptions for each video segment obtained from the above process, we mainly use pre-trained LMM (VILA-1.5-13B [36]). Below Tab. 4 includes the instructions for generating those captions. We randomly select one from the list and use it as query for the model. Video Caption Generation Instruction: Provide detailed description of both the visual content and the storyline depicted in the video. Thoroughly describe the scenes, actions, and characters featured in the video Elaborate on the visual and narrative elements of the video in detail. Describe what is happening in the video in detail, including both the visual details and the narrative context. Describe every element of the video, from visual details to the unfolding narrative, including how each aspect interacts to enhance the storytelling. Offer granular analysis of the video, detailing the scenes, character actions, and dialogue, alongside any symbolic visual elements that add depth to the story. Narrate the unfolding events in the video with attention to both the visual composition and the plot, describing how each scene visually portrays the narrative tensions or themes. Table 4: The list of instructions for detailed description for each video segment. Captioning and Scoring. Each segment is densely captioned, generating highly detailed textual descriptions that average 137.5 words per segment. Please see the densely captioned video examples in Fig. 6 and Fig. 7. To ensure alignment quality, generalized bipartite matching frame- (i) Video-to-Text (V2T) Corresponwork is employed: ing the presence, actions, and interactions of individuals within scene. These terms highlight that the dataset prioritizes detailed portrayals of people as central subjects, providing rich context about their appearance, activities, and relationships with the surrounding environment. Furthermore, the inclusion of descriptive spatial and contextual terms (e.g., stage, floor, light, tree, etc,.) illustrates how the dataset prioritizes capturing environmental details alongside subject interactions. This level of granularity ensures that the visual-textual mappings are comprehensive, enabling the dataset to serve as robust resource for training models that require an in-depth understanding of scene composition and narrative continuity. By focusing on such fine-grained visual details, the SceneWalk dataset can provide generic scene descriptions, encapsulating nuanced visual content that is critical for multimodal tasks. The highlighted terms reflect not only the datasets diversity but also its deliberate emphasis on actionable visual semantics, making it particularly valuable for an intermediate training step, as proposed in Sec. 4.2, by enabling models to effectively learn and represent long video knowledge, including scene comprehension and nuanced understanding. B. Training Details of SALOVA Training Config. In this section, we elaborate the training process of SALOVA. All variations of SALOVA undergo training with unified settings, though per-device batch sizes differ slightly due to hardware limitations. To equalize the global batch size across these variations, gradient accumulation is implemented, facilitating consistent training timeline for each variant. The detailed training configuration for each step can be found in Tab. 5, which optimizes the use of available GPU memory for batch sizing and ensures efficient training dynamics with limited hardware resource. config input modality input frame input resolution optimizer lr schedule training precision DeepSpeed train warmup epochs trainable params lr {vision, text} lr {LLM, others} global batch size total epochs Max token drop Stage1 image, video Stage2 video Stage1.5 video 1 FPS 336 336 AdamW (β1, β2=0.9, 0.999) cosine decay BFloat16 ZeRO-2 0.03 full 2e-6 2e-5 8 1 0.7 full 2e-6 2e-5 64 1 0.4 connectors - 1e-3 256 1 0.0 Table 5: Training hyper-parameters for different stages. Here, connectors indicates SR-Router and ST-Connector 2 Figure 5: WordCloud analysis of the SceneWalk dataset. dence: matrix evaluates the alignment between video segments and their paired captions using LanguageBind [73], and (ii) Text-to-Text (T2T) Context Similarity: The textual coherence among adjacent captions is assessed using SBERT [54], enhancing overall alignment robustness. Supervision from Correspondence Scores. To derive the supervision signal yi for training, we leverage the correspondence scores SV2T (Video-to-Text) and ST2T (Text-toText), as discussed in Sec. 4. For each correspondence score matrix, we apply thresholding to extract meaningful relationships. Specifically, we define thresholds τV2T and τT2T for the two matrices, and elements with scores exceeding these thresholds are treated as positive correspondences (th: 0.18 (τV2T) and 0.8 (τT2T), respectively). These positive correspondences are then one-hot encoded to form binary matrices YV2T and YT2T, where each element indicates whether specific correspondence is valid. Finally, we compute the union of these binary matrices to produce the final supervision signal (e.g., yi=YV2T YT2T). The union operation ensures that any correspondence deemed valid by either of the two modalities contributes to the final supervision. This approach captures both the multi-modal alignment (Videoto-Text) and intra-modal coherence (Text-to-Text), providing robust supervision signal for the retrieval task. The resulting yi is then incorporated into the similarity loss function Lsim as described in Eq. (1), ensuring that the model effectively learns the nuanced relationships between video segments and their corresponding textual descriptions. By combining SV2T and ST2T in this manner, we account for the complexity of generalized bipartite matching and enhance the models ability to align correspondences across and within modalities. A.3. Word Cloud Analysis. The Word Cloud visualization in Fig. 5 highlights the richness and diversity of visual cues captured within the SceneWalk dataset. The prominent keywords such as man, woman, person, and group reflect the datasets strong emphasis on human-centric descriptions, focusing on capturAblation Top-k 1 5 9 13 Video-MME Short: 2m Mid: 4-15m Long: 30-60m Overall : Number of Video Segments for Retrieval 44.4 45.0 46.3 44.7 39.1 39.2 41.1 39. 48.1 48.1 48.3 48.1 43.9 44.1 45.3 44.1 Table 7: Ablation studies on retrieval number for video segments. We utilize SALOVA-Llama model for the experiments. varying the number of video segments affects performance results. Note that the maximum number of video segments in Video-MME is 13. As in Tab. 7, we clearly observe that increasing the number of frames tends to enhance performance. However, performance saturates after the retrieval number reaches 9. This saturation may be related to the fact that excessive input information becomes more disruptive than helpful for reasoning about partial scenes in the video. Qualitative Results. We provide qualitative results with varying video lengths to clearly demonstrate the effectiveness of SALOVA across short, medium, and long videos as in Fig. 8. For example, short video in Fig. 8, our model accurately retrieved scene of the Moon colliding with the Earth, depicting an astronomical disaster based on the given question. Similarly, in medium-length videos, SALOVA effectively identified the scene where male judge selects card corresponding to the question. Specifically, even in videos longer than 40 minutes, SALOVA accurately identifies scenes related to the correct answer, such as people eating BBQ after exploring the history of the foods origin, based solely on the query input and the video content. These consistent qualitative results across all lengths indicate that the successful retrieval of pertinent video segments relevant to the input query significantly contributes to the models efficiency. As demonstrated in our analysis, SALOVA effectively handles different amounts of video data, which supports robust scene understanding and reasoning. C. Architecture Details of SALOVA Network Config. Here, we explain our network configurations in detail. For the first part of our architecture, the Spatio-Temporal Connector, we employ the Perceiver Resampler [2] architecture (but, smaller size), which consists of 2-layer, 2-head Transformer architecture followed by 2-layer MLP with GELU activation as projector. For the connectors latent features, we set the number of latent features to 256 and the hidden size to 1024. Next, the second module, the Segment Retrieval Router, consists of 2-layer, single head Transformer architecture. The Transformer uses model of 1024 and PReLU as the activation function. D. Additional Experiments Results of LongVideoBench. Due to the page limit of the main manuscript, in this additional section, we elaborate on both validation and test set results of LongVideoBench [57] for further demonstration. As in Tab. 6, it shows an analogous tendency for Video-MME benchmark, which exhibits significant performance increase after the short duration video (15s). We highlight again that such trend is mainly due to the retrieval capability of SALOVA, which excels in associating visual content with contextual information, even as video lengths increase. Model Size Proprietary LMMs GPT-4o [46] Gemini 1.5 Pro [50] GPT-4-Turbo [44] Open-sourced LMMs VideoChat2 [32] VideoLLaVA [35] PLLaVA [59] LLaVA-1.5 [37] ShareGPT4Video [8] Ours SALOVA-Llama SALOVA-Phi SALOVA-Qwen 5 1 - 8 71.6 70.2 66.4 38.1 43.1 45.3 45.0 46.9 - - - 7B 8B 7B 7B 7B 3B 46.3 3.8B 45.3 46.0 7B LongVideoBench 0 0 6 - 0 8 1 66.7 65.0 61.7 33.5 36.4 38.5 40.1 40.0 41.9 42.6 44. 0 0 6 3 - 0 0 9 61.6 59.1 54.5 33.6 34.4 35.2 37.0 38.7 39.8 40.6 42.1 0 6 - 5 1 76.8 75.3 71. 40.5 44.6 47.3 47.4 50.1 46.7 48.3 50.7 t 66.7 64.4 60.7 35.1 37.6 39.2 40.4 41. 42.2 42.9 44.5 l 66.7 64.0 59.1 36.0 39.1 40.2 40.3 39.7 41.4 41.6 43. Table 6: Comparison results for LongVideoBench. The best results are highlighted in bold and the runner-up results are underlined. Ablation Study for Retrieval Number. In addition, we conduct an analysis of the number of video segments used for inference on the Video-MME benchmark. In our architectural design, the number of video segments can be dynamically set based on retrieval estimates from the SRRouter, which forwards partial yet pertinent spatio-temporal information from the video to the LMMs. We compare how 3 Figure 6: Examples of the SceneWalk dataset (i). 4 Figure 7: Examples of the SceneWalk dataset (ii). 5 Figure 8: Qualitative examples on Video-MME [20] with SALOVA-Qwen. Note that the red dashed lines indicates the top-1 relevant video segment estimation for the question."
        }
    ],
    "affiliations": [
        "Integrated Vision and Language Lab, KAIST, South Korea"
    ]
}