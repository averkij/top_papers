{
    "paper_title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "authors": [
        "Z-Image Team",
        "Huanqia Cai",
        "Sihan Cao",
        "Ruoyi Du",
        "Peng Gao",
        "Steven Hoi",
        "Zhaohui Hou",
        "Shijie Huang",
        "Dengyang Jiang",
        "Xin Jin",
        "Liangchen Li",
        "Zhen Li",
        "Zhong-Yu Li",
        "David Liu",
        "Dongyang Liu",
        "Junhan Shi",
        "Qilong Wu",
        "Feng Yu",
        "Chi Zhang",
        "Shifeng Zhang",
        "Shilin Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models."
        },
        {
            "title": "Start",
            "content": "-Image November 27, 2025 Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer Z-Image Team, Alibaba Group"
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 8 ] . [ 3 9 9 6 2 2 . 1 1 5 2 : r The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro [27] and Seedream 4.0 [64]. Leading open-source alternatives, including Qwen-Image [76], Hunyuan-Image-3.0 [8] and FLUX.2 [35], are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle from curated data infrastructure to streamlined training curriculum we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models. GitHub https://github.com/Tongyi-MAI/Z-Image ModelScope Model https://modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo HuggingFace Model https://huggingface.co/Tongyi-MAI/Z-Image-Turbo ModelScope Demo Online Demo (ModelScope) HuggingFace Demo Online Demo (HuggingFace) Image Gallery Online Gallery Offline Gallery Figure 1 Showcases of Z-Image-Turbo in photo-realistic image generation. All related prompts can be found in Appendix A.1. 2 Figure 2 Showcases of Z-Image-Turbo in bilingual text-rendering. All related prompts can be found in Appendix A.2. 3 Figure 3 Showcases of Z-Image-Edit in various image-to-image tasks. Each arrow represents an edit from the input to output images. All related prompts can be found in Appendix A.3. 4 Figure 4 Showcases of comparison between Z-Image-Turbo and currently state-of-the-art models [58, 76, 8, 27, 35, 64, 21, 26]. Z-Image-Turbo shows conspicuous photo-realistic generation capacity."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Data Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Data Profiling Engine . . 2.2 Cross-modal Vector Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 World Knowledge Topological Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Active Curation Engine . 2.5 Efficient Construction of Editing Pairs with Graphical Representation . . . . . . . . . . . . . . . . . . . 3 Image Captioner 3.1 Detailed Caption with OCR Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Multi-Level Caption with World Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Difference Caption for Image Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Model Training . . . . . . . . . . . . . . . . . . . . . . . . Supervised Fine-Tuning (SFT) . . 4.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Training Efficiency Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Few-Step Distillation . 4.5.1 Decoupled DMD: Resolving Detail and Color Degradation . . . . . . . . . . . . . . 4.5.2 DMDR: Enhancing Capacity with RL and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.3 Results and Analysis . 4.6 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . 4.6.1 Reward Annotation and Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stage 1: Offline Alignment with DPO on Objective Dimensions . . . . . . . . . . . 4.6.2 Stage 2: Online Refinement with GRPO . . . . . . . . . . . . . . . . . . . . . . . . . 4.6.3 4.7 Continued Training for Image Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8 Prompt Enhancer with Reasoning Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Performance Evaluation 5.2 Quantitative Evaluation . 5.1 Human Preference Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Elo-based Human Preference Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Human Preference Comparison with Flux 2 dev . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Text-to-Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Instruction-based Image Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.1 Superior Photorealistic Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.2 Outstanding Bilingual Text Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . Instruction-following Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.3 5.3.4 Enhanced Reasoning Capacity and World Knowledge through Prompt Enhancer . 5.3.5 Emerging Multi-lingual and Multi-cultural Understanding Capacity . . . . . . . . . . . . 5.3 Qualitative Evaluation . 6 Conclusion 7 Authors 7.1 Core Contributors . . 7.2 Contributors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . References Prompts Used in the Report . . A.1 Figure 1 . A.2 Figure 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 8 9 10 11 12 12 13 14 15 15 16 16 17 18 19 20 21 22 22 22 22 22 23 24 24 25 25 25 27 27 27 32 32 32 32 33 33 33 49 49 49 49 50 56 56 A.3 Figure 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 7 1. Introduction The field of text-to-image (T2I) generation has witnessed remarkable advancements in recent years, evolving from generating rudimentary textures to producing photorealistic imagery with complex semantic adherence [57, 18, 34, 76, 64, 8, 4]. However, as the capabilities of these models have scaled, their development and accessibility face significant barriers. The current landscape is increasingly characterized by two divergent trends: on one side, state-of-the-art commercial closed-source models such as Nano Banana Pro [27] and Seedream 4.0 [64] remain enclosed within black boxes, offering high performance but limited transparency or reproducibility. On the other side, open-source models, while fostering democratization, often resort to massive parameter scaling approaching tens of billions of parameters (e.g., Qwen-Image [76] (20B), FLUX.2 [35] (32B) and Hunyuan-Image-3.0 [8] (80B) imposing prohibitive computational costs for both training and inference. In this context, distilling synthetic data from proprietary models has emerged as an appealing shortcut to train high-performing models at lower cost, becoming prevalent approach for resource-constrained academic research [13, 20]. However, this strategy risks creating closed feedback loop that may lead to error accumulation and data homogenization, potentially hindering the emergence of novel visual capabilities beyond those already present in the teacher models. In this work, we present Z-Image, powerful diffusion transformer model that challenges both the scale-at-all-costs paradigm and the reliance on synthetic data distillation. We demonstrate that neither approach is necessary to develop top-tier image generation model. Instead, we introduce the first comprehensive end-to-end solution that systematically optimizes every stage of the model lifecycle from data curation and architecture design to training strategies and inference acceleration enabling efficient, low-cost development on purely real-world data without distilling results from other models. Most notably, this methodological efficiency allows us to complete the entire training workflow with remarkably low computational overhead. As detailed in Table 1, the complete training pipeline for Z-Image requires only 314K H800 GPU hours, translating to approximately $628K at current market rates (about $2 per GPU hour [37]). In landscape where leading models often demand orders of magnitude more resources, this modest investment demonstrates that principled design can effectively rival brute-force scaling. Table 1 Training costs of Z-Image, assuming the rental price of H800 is about $2 per GPU hour. The rental price refers from [37]. Training Costs Low-res. Pre-Training Omni-Pre-Training Post-Training in H800 GPU Hours in USD 147.5K $295K 142.5K $285K 24K $48K Total 314K $628K This breakthrough in cost-efficiency is underpinned by systematic methodology built on four pillars: Efficient Data Infrastructure: In resource-constrained scenarios, an efficient data infrastructure is pivotal; it serves to maximize the rate of knowledge acquisition per unit of time thereby accelerating training efficiency while simultaneously establishing the upper bound of model capabilities. To achieve this, we introduce comprehensive Data Infrastructure composed of four synergistic modules: Data Profiling Engine for multi-dimensional feature extraction, Cross-modal Vector Engine for semantic deduplication and targeted retrieval, World Knowledge Topological Graph for structured concept organization, and an Active Curation Engine for closed-loop refinement. By granularly profiling data attributes and orchestrating the training distribution, we ensure that the right data is aligned with the right stage of model development. This infrastructure maximizes the utility of real-world data streams, effectively eliminating computational waste arising from redundant or low-quality samples. Efficient Architecture: Inspired by the remarkable scalability of decoder-only architectures in large language models [6], we propose Scalable Single-Stream Multi-Modal Diffusion Transformer (S3-DiT). Unlike dual-stream architectures that process text and image modalities in isolation, our design facilitates dense cross-modal interaction at every layer. This high parameter efficiency enables Z-Image to achieve superior performance within compact 6B parameter size, significantly lowering the hardware requirements for both training and deployment. The compact model size is also made possible in part by our use of prompt enhancer (PE) to augment the models complex world knowledge comprehension and prompt understanding capabilities, further mitigating the limitations of the modest parameter count. Furthermore, this early-fusion transformer architecture ensures superior versatility by treating tokens from different modalities uniformly including text tokens, image VAE tokens, and image semantic tokens enabling seamless handling of diverse tasks such as text-to-image generation and image-to-image editing within unified framework. Efficient Training Strategy: We design progressive training curriculum composed of three strategic phases: (1) Low-resolution Pre-training, which bootstraps the model to acquire foundational visual-semantic alignment and synthesis knowledge at fixed 2562 resolution. (2) Omni-pretraining, unified multi-task stage that consolidates arbitrary-resolution generation, text-to-image synthesis, and image-to-image manipulation. By amortizing the heavy pre-training budget across these diverse capabilities, we eliminate the need for separate, resource-intensive stages. (3) PEaware Supervised Fine-tuning, joint optimization paradigm where Z-Image is fine-tuned using PE-enhanced captions. This ensures seamless synergy between the Prompt Enhancement module and the diffusion backbone without incurring additional LLM training costs, thereby maximizing the overall development efficiency of the Z-Image system. Efficient Inference: We present Z-Image-Turbo, which delivers exceptional aesthetic alignment and high-fidelity visual quality in only 8 Number of Function Evaluations (NFEs). This performance is unlocked by the synergy of two key innovations: Decoupled DMD [45], which explicitly disentangles the quality-enhancing and training-stabilizing roles of the distillation process, and DMDR [31], which integrates Reinforcement Learning by employing the distribution matching term as an intrinsic regularizer. Together, these techniques enable highly efficient generation without the typical trade-off between speed and quality. Building upon this robust foundation and efficient workflow, we have successfully derived two specialized variants that address distinct application needs. First, our few-shot distillation scheme with reinforcement learning yields Z-Image-Turbo, an accelerated model that achieves exceptional aesthetic alignment in just 8 NFEs. It offers sub-second inference1 latency on enterprise GPUs and fits within the memory constraints of consumer-grade hardware (<16GB VRAM). Second, leveraging the multi-task nature of our omni-pre-training, we introduce Z-Image-Edit, model specialized for precise instructionfollowing image editing. Extensive qualitative and quantitative experiments demonstrate the superiority of the Z-Image family. As illustrated in Figure 1 and Figure 2, Z-Image delivers strong capabilities of photorealistic generation and exceptional bilingual (Chinese/English) text rendering, matching the visual fidelity of much larger models. Figure 3 showcases the capabilities of Z-Image-Edit, highlighting its precise adherence to editing instructions. Furthermore, qualitative comparisons in Figure 4 and Section 5.3 reveal that our model rivals top-tier commercial systems, proving that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly generative models. 2. Data Infrastructure While the remarkable capabilities of state-of-the-art text-to-image models are underpinned by large-scale training data, achieving optimal performance under constrained computational resources necessitates paradigm shift from data quantity to data efficiency. Simply scaling the dataset size often leads to diminishing returns; instead, an efficient training pipeline requires data infrastructure that maximizes the information gain per computing unit. To this end, an ideal data system must be strictly curated to be conceptually broad yet non-redundant, exhibit robust multilingual text-image alignment, and crucially, be structured for dynamic curriculum learning, ensuring that the data composition evolves to match the models training stages. To realize this, we have designed and implemented an integrated Efficient Data Infrastructure. Far from static repository, this system operates as dynamic engine architected to maximize the rate of knowledge acquisition within fixed training budget. As the cornerstone of our pipeline, this infrastructure is composed of four core, synergistic modules: 1FlashAttention-3 [65] and torch.compile [1] is necessary for achieving sub-second inference latency. 9 1. Data Profiling Engine: This module serves as the quantitative foundation for our data strategy. It extracts and computes rich set of multi-dimensional features from raw data, spanning lowlevel physical attributes (e.g.image metadata, clarity metrics) to high-level semantic properties (e.g., anomaly detection, textual description). These computed profiles are not merely for basic filtering; they are the essential signals used to quantify data complexity and quality, enabling the programmatic construction of curricula for our dynamic learning stages. 2. Cross-modal Vector Engine: Built on billions of embeddings, this module is the engine for ensuring efficiency and diversity. It directly supports our goal of non-redundant dataset through large-scale semantic deduplication. Furthermore, its cross-modal search capabilities are critical for diagnosing and remediating model failures. This allows us to pinpoint and prune data responsible for specific failure cases and strategically sample to fill conceptual gaps. 3. World Knowledge Topological Graph: This structured knowledge graph provides the semantic It directly underpins our goal of conceptual breadth by backbone for the entire infrastructure. organizing knowledge hierarchically. Crucially, this topology functions as semantic compass for data curation. It allows us to identify and fill conceptual voids in our dataset by traversing the graph to find underrepresented entities. Furthermore, it provides the structured framework needed to precisely rebalance the data distribution across different concepts during training, ensuring more efficient and comprehensive learning process. 4. Active Curation Engine: This module operationalizes our infrastructure into truly dynamic, selfimproving system. It serves two primary, synergistic functions. First, it acts as frontier exploration engine, employing automatic sampling to identify concepts on which the model performs poorly or lacks knowledge (hard cases\"). Second, it drives closed-loop data annotation pipeline. This ensures that every iteration not only expands conceptual breadth of the dataset with high-value knowledge but also continuously refines the data quality, maximizing the learning efficiency of the entire training process. Collectively, these components forge robust data infrastructure that not only fuels the training of textto-image models but also establishes versatile infrastructure for broader multimodal model training. Leveraging this system, we successfully facilitate the training of various critical components, including captioners, reward models, and our image editing model (i.e., Z-Image-Edit). In particular, we construct dedicated data pipeline specifically for Z-Image-Edit upon this infrastructure, the details of which are elaborated in Section 2.5. 2.1. Data Profiling Engine The Data Profiling Engine is engineered to systematically process massive, uncurated data pool, comprising large-scale internal copyrighted collections. It computes comprehensive suite of multidimensional features for each image-text pair, enabling principled data curation. Recognizing that different data sources exhibit unique biases, our engine supports source-specific heuristics and sampling strategies to ensure balanced and high-quality training corpus. The profiling process is structured across several key dimensions: Image Metadata. We begin by caching fundamental properties for each image. This includes elementary metadata like resolution (width and height) and file size, which facilitate efficient filtering based on resolution and aspect ratio. Simultaneously, we compute perceptual hash (pHash) from the images byte stream. This hash acts as compact visual fingerprint, enabling rapid and effective low-level deduplication to remove identical or near-identical images. Together, these pre-computed attributes form the first layer of data selection. Technical Quality Assessment. The technical quality of an image is critical determinant of model performance. Our engine employs multi-faceted approach to quantify and filter out low-quality assets: Compression Artifacts: To identify over-compressed images, we calculate the ratio of the ideal uncompressed file size (derived from resolution and bit depth) to the actual file size. low ratio indicates potential quality degradation due to excessive compression. Visual Degradations: We utilize an in-house trained quality assessment model to score images 10 on range of degradation factors, including color cast, blurriness, perceptible watermarks, and excessive noise. Information Entropy: To maximize the density of meaningful content seen during training, we filter out low-entropy images. This is achieved through two complementary methods: (1) analyzing the variance of border pixels to detect images with large, uniform-color backgrounds or frames, and (2) performing transient JPEG re-encoding and using the resulting bytes-per-pixel (BPP) as proxy for image complexity. Semantic and Aesthetic Content. Beyond technical quality, we profile the high-level semantic and aesthetic properties of images: Aesthetic Quality: We leverage an aesthetics scoring model, trained on labels from professional annotators, to quantify the visual appeal of each image. AIGC Content Detection: Following the findings of Imagen 3 [3], we trained dedicated classifier to detect and filter out AI-generated content. This step is crucial for preventing degradation in the models output quality and physical realism. High-Level Semantic Tagging: We have trained specialized Vision-Language Model (VLM) to generate rich semantic tags. These tags include general object categories, human-centric attributes (e.g.number of people), and culturally specific concepts, with particular focus on elements relevant to Chinese culture. The same model also performs safety assessment by assigning Not-Safe-for-Work (NSFW) scores, allowing for the unified filtering of both semantically irrelevant and inappropriate content. Cross-Modal Consistency and Captioning. The alignment between an image and its textual description is paramount. Text-Image Correlation: We use CN-CLIP [86] to compute the alignment score between an image and its associated alt caption. Pairs with low correlation scores are discarded to ensure the relevance of textual supervision. Multi-Level Captioning: For all images selected for pre-training, we generate structured set of captions, including concise tags, short phrases, and detailed long-form descriptions. Notably, diverging from prior works [21, 64, 76] that use separate modules for Optical Character Recognition (OCR) and watermark detection, our approach leverages the powerful inherent capabilities of our VLM. We explicitly prompt the VLM to describe any visible text or watermarks within the image, seamlessly integrating this information into the final caption. This unified strategy not only streamlines the data processing pipeline but also enriches the textual descriptions with critical visual details, as further elaborated in Section 3. 2.2. Cross-modal Vector Engine We enhance the de-duplication method proposed in Stable Diffusion 3 [18], reformulating it as scalable, graph-based community detection task. Addressing the severe scalability bottleneck of the original ð‘Ÿð‘Žð‘›ð‘”ð‘’_ð‘ ð‘’ð‘Žð‘Ÿð‘â„Ž function, we substitute it with highly efficient k-nearest neighbor (k-NN) ð‘ ð‘’ð‘Žð‘Ÿð‘â„Ž function. We construct proximity graph from the k-NN distances and subsequently apply the community detection algorithm [68]. This methodology closely approximates the original algorithms output for sufficiently large while drastically reducing time complexity. Our fully GPU-accelerated [60] pipeline achieves processing rate of approximately 8 hours per 1 billion items on 8 H800s, encompassing index construction and 100-NN querying. This approach not only ensures non-redundant dataset by identifying dense clusters for effective de-duplication but also extracts semantic structures via modularity levels, facilitating fine-grained data balancing. Furthermore, we constructed an efficient retrieval pipeline leveraging multimodal features [86] combined with state-of-the-art index algorithm [54]. This systems cross-modal search capabilities are critical for both data curation and active model remediation. Beyond identifying distributional voids for strategically sampling to fill conceptual gaps thereby enabling targeted augmentation for balanced pre-training distribution this engine is instrumental in diagnosing model failures. By querying the 11 system with failure cases (e.g., problematic generated images or text prompts), we can pinpoint and prune the underlying data clusters responsible for the erroneous behavior. This iterative refinement process, targeting both data gaps and model failures, ensures dataset robustness and is pivotal for sourcing high-quality candidates for complex downstream tasks. 2.3. World Knowledge Topological Graph The construction of our knowledge graph follows three-stage process. Initially, we build comprehensive but redundant knowledge graph from all Wikipedia entities and their hyperlink structures. To refine this graph, we employ two-pronged pruning strategy: first, centrality-based filtering removes nodes with exceptionally low PageRank [56] scores, which represent isolated or seldom-referenced concepts; second, visual generatability filtering uses VLM to discard abstract or ambiguous concepts that cannot be coherently visualized. Subsequently, to address the limited conceptual coverage of the pruned graph, we augment it by leveraging large-scale internal dataset of captioned images. We extract tags and corresponding text embeddings from all available captions. Inspired by [71], we then perform an automatic hierarchical strategy on these embeddings. Each parent node is named by using VLM to summarize its child nodes. This not only supplements the graph with new concept nodes but also organizes them into structured taxonomic tree, significantly enhancing the structural integrity of the graph. In the final stage, we perform weight assignment and dynamic expansion to align the graph with practical applications. This involves manually curating and up-weighting high-frequency concepts from user prompts, and proactively integrating novel, trending concepts not yet present in our data pool to maintain the relevance and timeliness of the graph. In application, this graph underpins our semantic-level balanced sampling strategy. We map the tags within each training caption to their corresponding nodes in the knowledge graph. By considering both the BM25 [62] score of tag and its hierarchical relationships (i.e., parent-child links) within the graph, we compute semantic-level sampling weight for each data point. This weight then guides our data engine to perform principled, staged sampling from the data pool, enabling fine-grained control over the training data distribution. Figure 5 Overview of the Active Curation Engine. The pipeline refines uncurated data through crossmodal embedding, deduplication, and rule-based filtering to construct high-quality augmented dataset. feedback mechanism leverages the Z-Image model to diagnose long-tail distribution deficiencies, dynamically guiding cross-modal retrieval to reinforce the data collection process. The Squirrel Fish (æ¾é¼ é³œé±¼) case illustrates classic long-tail challenge: it is actually the name of Chinese cuisine but the model lacks the specific concept for this dish and may rely on compositional reasoning (combining Squirrel (æ¾é¼ ) and Fish (é³œé±¼)), leading to erroneous generations absent of domain-specific training data. 2.4. Active Curation Engine To systematically elevate data quality and address long-tail distribution challenges, we deploy comprehensive Active Curation Engine (Figure 5). This framework incorporates filtering tool and Z-Image as Figure 6 Illustration of the Human-in-the-Loop Active Learning Cycle. Data sampled from the media pool undergoes concept and quality balancing before being assigned pseudo-labels . dual-verifier system (Human and AI) filters these proposals: approved samples pass directly, while rejected cases trigger manual correction phase . This feedback loop iteratively refines the annotations and updates the topology graph to ensure high-precision alignment. diagnostic generative prior. The pipeline begins by processing uncurated data through cross-modal embedding and deduplication, followed by rule-based filtering to eliminate low-quality samples. To support the continuous evolution of Z-Image, we establish human-in-the-loop active learning cycle (Figure 6) where the reward model and captioner are progressively optimized. In this pipeline, we first employ the topology graph (Section 2.3) and the initial reward model to curate balanced subset from the unlabeled media pool. The current captioner and reward model then assign pseudo-labels to these samples. hybrid verification mechanism comprising both human and AI verifiers verifies these proposals; rejected samples trigger manual correction phase by human experts to refine captions or scores. This high-quality annotated data is then used to retrain the captioner and reward model, thereby creating virtuous cycle of our whole data infrastructure enhancement. 2.5. Efficient Construction of Editing Pairs with Graphical Representation Input Image Edited Images (a) Graphical Representation (b) Paired Image from Videos (c) Rendering for Text Editing Figure 7 Data construction for image editing using different strategies: (a) arbitrarily permuting and combining different edited versions of the same input image where the green arrow represents the pair constructed by task-specfic expert models and the red arrow denotes the pair generated by combination and permutation, (b) collecting images with inherent relationship from video frames, and (c) controllable text rendering system for text editing. Collecting editing pairs that exhibits precise instruction following is challenging, owing to the requirement of consistency maintaining and the diverse and complex nature of editing operations. Through scalable 13 and controllable strategies as shown in Figure 7, we construct large-scale training corpus from diverse sources. Mixed Editing with Expert Models. To guarantee broad task coverage, we begin by curating diverse taxonomy of editing tasks, and then leverage task-specific expert models to synthesize high-quality training data for each category. To improve the training efficiency, we construct mixed-editing data, where multiple editing actions are integrated into one editing pair. Thus, the model can enhance its ability in multiple editing tasks from only single composite pair, instead of relying on multiple ones. Efficient Graphical Representation. For an input image, we synthesize multiple edited versions corresponding to different editing tasks, enabling us to further scale the training data at zero cost through arbitrary pairwise combination [41] (e.g., 2(cid:0)ð‘+1 (cid:1) pairs are constructed from one input image and its ð‘ 2 edited versions). Apart from scaling the quantity, this strategy 1) creates mixed-editing training data by combining two edited versions to enhance the training efficiency, and 2) yields inverse pairs to improve data quality, i.e., transforming real, undistorted input image to an output image. Paired Images from Videos. Constructing image editing pairs from predefined tasks suffers from limited diversity. To overcome this issue, we leverage naturally grouped images collected from large scale video frames in our media pool. These images, by sharing inherent relatedness (e.g., common subjects, scenes, or styles), implicitly define complex editing relationships among themselves. Building on this, we refine the data by calculating the cosine similarity between image embeddings using CN-CLIP [86], allowing us to filter for pairs with high semantic relevance within each image group. The resulting dataset of video frame pairs offers three key advantages: 1) high task diversity, 2) inherent coupling of multiple edit types (e.g., simultaneous changes in human pose and background), and 3) superior scalability. Rendering for Text Editing. The acquisition of high-quality training data for text editing presents substantial challenges, where natural images suffer from the scarcity and imbalance of textual content, and text editing requires paired samples with precise operation annotations. To address these challenges, we develop controllable text rendering system [76] that grants us precise control over not only the textual content but also its visual attributes, such as font, color, size, and position. This approach enables us to systematically generate large-scale dataset of paired images, where the ground-truth editing instruction are known by the rendering operation, thereby directly overcoming the aforementioned data limitations. 3. Image Captioner World Knowledge Single Image Text-to-Image Captions Tagging Caption Short Caption Long Caption Step1: Caption Step2: Analysis Step3: Instruction Image Editing Instructions Z-Captioner Model OCR Augmentation Image Pair Figure 8 Pipeline for generating text-to-image captions and image editing instructions. OCR results (obtained through CoT) and world knowledge (from meta information) are explicitly included into the captions. We build an all-in-one image captioner, Z-Captioner, by incorporating multiple types of image caption. 14 Figure 9 Single image caption and difference caption examples. Left: for single image, we have captions of different types and lengths, and notably, OCR results (all the texts transcribed in their original languages) and world knowledge (explicitly and correctly recognizing the famous beauty spot, West Lake, Hangzhou, China, in this example) is included. Right: difference captions are composed step-by-step. As revealed in previous works [49], different captioning tasks can benefit each other as they share the same goal of understanding and depicting images. Our model is designed not only to describe visual elements, but also to leverage extensive world knowledge to interpret the semantic context of the image. The integration of world knowledge is particularly critical for the downstream text-to-image synthesis task, as it enables the model to accurately render images involving specific named entities. Figure 8 shows our pipeline for generating text-to-image captions and image editing instructions. 3.1. Detailed Caption with OCR Information First, we specially emphasize that according to our experiments, including explicit OCR information in image captions is inextricably bound with accurate text rendering in the generated images. Therefore, we employ way that shares the same spirit as Chain-of-Thought (CoT) [73], by first explicitly recognizing all optical characters in the image and then generating caption based on the OCR results. This effectively mitigates missing texts compared to directly generating caption that encapsulates everything, especially for the cases where texts are very long/dense. In addition, we force the OCR results to remain in their original languages without any translation, avoiding them being falsely rendered in their translated languages. 3.2. Multi-Level Caption with World Knowledge We design five different types of image captions in total, including long, medium and short captions, as well as tags and simulated user prompts. With the data infrastructure in Section 2, we include world knowledge in all five types of captions by performing image captioning conditioned on meta information. This significantly alleviates hallucinations when our captioner identifies and names specific entities such as public figures, famous landmarks, or known events. To be specific, for relatively long captions, we include very dense information of the images, in order that the model could learn mapping from the text to the image as accurate as possible. These captions contain full OCR results as mentioned above, along with subjects, objects, background, location information, et al. 15 We deliberately adopt plain and objective linguistic style for our descriptions, strictly confining them to factual information observable in the image. By inhibiting subjective interpretations and imaginative associations, our purpose is to enhance data efficiency for the image generation task by eliminating non-essential information. On the other hand, short captions, tags and simulated user prompts are designed for the model to adapt to real user prompts (which are usually short and unspecific) for better user experience. Notably, most of the simulated user instructions are incomplete prompts. They differ from short captions in that short caption provides relatively complete and comprehensive description of the entire image. In contrast, short simulated prompt may mimic user behavior by focusing only on specific parts of interest to the user, while making no mention of the rest of the image. 3.3. Difference Caption for Image Editing Difference caption is concise editing instruction specifying the transformation from source to target image. To generate this, we employ three-step CoT process that systematically breaks down the comparative task [100]. 1. Step1: Detailed Captioning. We first generate comprehensive, OCR-inclusive caption for both the source and target images respectively. This step provides structured, detailed representation of each images content. 2. Step2: Difference Analysis. The model then performs comparative analysis, leveraging both the raw images and their generated captions, to tell all discrepancies from visual and textual perspectives. 3. Step3: Instruction Synthesis. Finally, the model generates concise editing instruction based on the identified differences. This step-by-step process helps the model create clear and useful instructions by moving from understanding, to comparing, and finally to generating the instructions. 4. Model Training This section presents the complete training pipeline of Z-Image and Z-Image-Edit. We begin by introducing our Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture (Section 4.1) and training efficiency optimizations (Section 4.2), followed by multi-stage training process: pre-training (Section 4.3), supervised fine-tuning (Section 4.4), few-step distillation (Section 4.5), and reinforcement learning with human feedback (Section 4.6). Finally, we describe the continued training strategy for image editing capabilities (Section 4.7) and our reasoning-enhanced prompt enhancer (Section 4.8). The overall training pipeline is summarized in Figure 11. And in Figure 12, we present intermediate generation results throughout Z-Images training process to demonstrate the benefits contributed by each stage. 4.1. Architecture Efficiency and stability are the core objectives guiding the design of Z-Image. To achieve this, we employ the lightweight Qwen3-4B [85] as the text encoder, leveraging its bilingual proficiency to align complex instructions with visual content. For image tokenization, we utilize the Flux VAE [34] selected for its proven reconstruction quality. Exclusively for editing tasks, we augment the architecture with SigLIP 2 [69] to capture abstract visual semantics from reference images. Inspired by the scaling success of decoder-only models, we adopt Single-Stream Multi-Modal Diffusion Transformer (MM-DiT) paradigm [18]. In this setup, text, visual semantic tokens, and VAE image tokens are concatenated at the sequence level to serve as unified input stream, maximizing parameter efficiency compared to dual-stream approaches [18, 76]. We employ 3D Unified RoPE [58, 78] to model this mixed sequence, wherein image tokens expand across spatial dimensions and text tokens increment along the temporal dimension. Crucially, for editing tasks, the reference image tokens and target image tokens are assigned aligned spatial RoPE coordinates but are separated by unit interval offset in the temporal dimension. Additionally, different time-conditioning values are applied to the reference and target images to distinguish between clean and noisy images. 16 Figure 10 Architecture overview of the Z-Image series. The S3-DiT consists of single-stream FFN blocks and single-stream attention blocks. It processes inputs from different modalities through lightweight modality-specific processors, then concatenates them into unified input sequence. This modalityagnostic architecture maximizes cross-modal parameter reuse to ensure parameter efficiency, while providing flexible compatibility for varying input configurations in both Z-Image and Z-Image-Edit. As illustrated in Figure 10, the specific architecture of our S3-DiT (Scalable Single-Stream DiT) commences with lightweight modality-specific processors, each composed of two transformer blocks for initial modal alignment. Subsequently, tokens enter the unified single-stream backbone. To ensure training stability, we implement QK-Norm to regulate attention activations [32, 50, 24, 53] and Sandwich-Norm to constrain signal amplitudes at the input and output of each attention / FFN blocks [16, 99]. For conditional information injection, input condition vectors are projected into scale and gate parameters to modulate the normalized inputs and outputs of both Attention and FFN layers. To reduce parameter overhead, this projection is decomposed into low-rank pair: shared, layer-agnostic down-projection layer followed by layer-specific up-projection layers. Finally, RMSNorm [91] is uniformly utilized for all the aforementioned normalization operations. Table 2 Architecture Configurations of S3-DiT. Configuration Total Parameters Number of Layers Hidden Dimension Number of Attention Heads FFN Intermediate Dimension (ð‘‘ð‘¡, ð‘‘â„Ž, ð‘‘ð‘¤) S3-DiT 6.15B 30 3840 32 10240 (32, 48, 48) 4.2. Training Efficiency Optimization To optimize training efficiency, we implemented multi-faceted strategy targeting both computational and memory overheads. 17 Figure 11 The training pipeline of Z-Image and Z-Image-Edit. The low-resolution pre-training and omni-pre-training stages provide suitable initialization for image generation and editing tasks, after which separate post-training processes yield the Z-Image and Z-Image-Edit models respectively. For distributed training, we employed hybrid parallelization strategy. We applied standard Data Parallelism (DP) to the VAE and Text Encoder, as they remain frozen and incur minimal memory footprint. In contrast, for the large DiT model, where optimizer states and gradients consume substantial memory, we utilized FSDP2 [96] to effectively shard these overheads across GPUs. Furthermore, we implemented gradient checkpointing across all DiT layers. This technique trades an acceptable increase in computational cost for significant memory savings, enabling larger batch sizes and improved overall throughput. To further accelerate computation and optimize memory usage, the DiT blocks were compiled using torch.compile, just-in-time (JIT) compiler [1]. In addition to system-level optimizations, we addressed inefficiencies arising from mixed-resolution training. Grouping samples with significantly different sequence lengths into single batch typically results in excessive padding, which significantly impedes overall training speed. To mitigate this, we designed sequence length-aware batch construction strategy. Prior to training, we estimate the sequence length of each sample based on the resolution (height and width) recorded in the metadata. The sampler then groups samples with similar sequence lengths into the same batch, thereby minimizing computational waste. Crucially, we additionally employ dynamic batch sizing mechanism: smaller batch sizes are assigned to long-sequence batches to prevent Out-Of-Memory (OOM) errors, while larger batch sizes are used for short sequences to avoid resource vacancy. This approach ensures maximal hardware utilization across varying resolutions. 4.3. Pre-training Z-Image is trained using the flow matching objective [44, 48], where noised inputs are first constructed through linear interpolation between Gaussian noise ð‘¥0 and the original image ð‘¥1, i.e., ð‘¥ð‘¡ = ð‘¡ ð‘¥1 + (1 ð‘¡) ð‘¥0. The model is then trained to predict the velocity of the vector field that defines the path between them, i.e., ð‘£ð‘¡ = ð‘¥1 ð‘¥0. The training objective can be formulated as: = Eð‘¡,ð‘¥0,ð‘¥ 1,ð‘¦ [ð‘¢(ð‘¥ð‘¡, ð‘¦, ð‘¡; ðœƒ) (ð‘¥1 ð‘¥0)2], (1) Where ðœƒ as the learnable parameters and ð‘¦ as the conditional embedding. Following SD3 [18], we employ the logit-normal noise sampler to concentrate the training process on intermediate timesteps. Additionally, to account for the variations in Signal-to-Noise Ratio (SNR) arising from our multi-resolution training setup, we adopt the dynamic time shifting strategy as used in Flux [34]. This ensures that the noise level is appropriately scaled for different image resolutions, leading to more effective training. The pre-training of Z-Image can be broadly divided into two phases: low-resolution pre-training and omni-pre-training. Low-resolution Pre-training. This phase consists of single stage, conducted exclusively at 2562 resolution on the text-to-image generation task. The primary emphasis of this stage is on efficient crossmodal alignment and knowledge injection equipping the model with the capability to generate diverse range of concepts, styles, and compositions, which is consistent with the initial stage of conventional multi-stage training protocols. As shown in Figure 1, this phase accounts for over half of our total pre18 training compute. This allocation is based on the rationale that the majority of the models foundational visual knowledge (e.g., Chinese text rendering) is acquired during this low-resolution training stage. Omni-pre-training. The omni here signifies three key aspects: Arbitrary-Resolution Training: We design an arbitrary-resolution training strategy in which the original image resolution is mapped to predefined training resolution range through resolutionmapping function. The model is then trained on images with diverse resolutions and aspect ratios. This enables the learning of cross-scale visual information, mitigates information loss caused by downsampling to fixed resolution, and improves overall data efficiency. Joint Text-to-Image and Image-to-Image Training: We integrate the image-to-image task into the pre-training framework. By leveraging the substantial compute budget available during pretraining, we can effectively exploit large-scale, naturally occurring, and weakly aligned image pairs, as discussed in Section 2.5. Learning the relationships between natural image pairs provides strong initialization for downstream tasks such as image editing. Importantly, we observe that this joint pre-training scheme does not introduce any noticeable performance degradation on the text-to-image task. Multi-level and Bilingual Caption Training: It is widely recognized that high-quality captions are crucial for training text-to-image models [4]. To ensure both bilingual understanding and strong native prompt-following capability, we employ Z-Captioner to generate bilingual, multi-level synthetic captions (including long, medium, and short descriptions, as well as tags and simulated user prompts). In addition, the original textual metadata associated with each image is incorporated with small probability to further enhance the models acquisition of world knowledge. The use of captions at different granularities and from diverse perspectives provides broad mode coverage, which is beneficial for subsequent stages of training. Moreover, for image-to-image tasks, we randomly sample either the target images caption or the pairwise difference caption with certain probability, corresponding to reference-guided image generation and multi-task image editing, respectively. Working with our data infrastructure, the omni-pre-training phase is conducted in multiple stages. Upon completion of the final stage, the model becomes capable of generating images at arbitrary resolutions up to the 1k-1.5k range and can condition its output on both image and text inputs. This provides suitable starting point for the subsequent training of Z-Image and Z-Image-Edit. 4.4. Supervised Fine-Tuning (SFT) Distribution Narrowing via High-Quality Alignment. While the omni-pre-training stage establishes broad world understanding and mode coverage, the resulting distribution inevitably exhibits high variance, reflecting the noisy nature of web-scale data. Consequently, the primary objective of Supervised Fine-Tuning (SFT) is not merely to correct local artifacts, but to narrow the generation distribution towards focused, high-fidelity sub-manifold [67]. This phase aims for rapid convergence to fixed distribution characterized by consistent visual aesthetics and precise instruction following. To achieve this, we transition from the noisy supervision of pre-training to curriculum dominated by highly curated images filtering by our data infrastructure and super detailed, grounded captions. This rigorous supervision acts as an anchor, forcing the model to discard low-quality modes (e.g., unstable stylization or inconsistent rendering) and align strictly with detailed textual descriptions, shifting the model from diversity-maximizing regime to quality-maximizing operating point. Concept Balancing with Tagged Resampling. critical challenge in narrowing the distribution is the risk of catastrophic forgetting, particularly for long-tail concepts that are prone to being overshadowed by dominant modes during convergence. To address this, we enforce strict class balancing throughout the SFT phase. We employ dynamic resampling strategy guided by world knowledge topological graph in Section 2. Specifically, we maintain target prior over concepts and utilize BM25-based retrieval to compute rarity scores for training samples on the fly. Mini-batches are constructed by up-weighting under-represented concepts such as rare entities or specific artistic styles while down-weighting over-represented ones. This mechanism ensures that while the model converges to the target high-quality 19 Figure 12 Intermediate generation results throughout Z-Image-Turbos training process, echoing our analysis of each stages contribution. distribution, the marginal distribution over concepts remains uniform, effectively preserving the semantic diversity of the pre-trained model. Robustness via Model Merging. Despite balanced training, SFT on specific high-quality datasets can introduce subtle biases or trade-offs between capabilities (e.g., photorealism vs. stylistic flexibility). To achieve Pareto-optimal solution without complex inference routing, we employ Model Merging [75, 93] as the final refinement step. We fine-tune multiple SFT variants initialized from the same backbone, each slightly biased towards different capability dimensions (e.g., strict instruction following or aesthetic final = (cid:205)ð‘– ð›¼ð‘–ðœƒð‘–. rendering). We then perform linear interpolation of their weights in the parameter space: ðœƒ This lightweight merging strategy effectively smooths the loss landscape, neutralizing individual biases and resulting in final model that exhibits superior stability and robustness across diverse prompts compared to any single SFT checkpoint. 4.5. Few-Step Distillation The goal of the Few-Step Distillation stage is to reduce the inference time of our foundational SFT model, achieving the efficiency demanded by real-world applications and large-scale deployment. While our 6B foundational model represents significant leap in efficiency compared to larger counterparts, the inference cost remains non-negligible. Due to the inherent iterative nature of diffusion models, our standard SFT model requires approximately 100 Number of Function Evaluations (NFEs) to generate high-quality samples using Classifier-Free Guidance (CFG) [29]. To bridge the gap between generation quality and interactive latency, we implemented few-step distillation strategy. Fundamentally, the distillation process involves teaching student model to mimic the teachers denoising dynamics across fewer timesteps along its sampling trajectory. The core challenge lies in reducing the inherent uncertainty of this trajectory, allowing the student to collapse its probabilistic path into deterministic and highly efficient inference process. Therefore, the key to enable stable few-step integrator is to meticulously control the distillation process. We initially selected the Distribution Matching Distillation (DMD) [88, 89] paradigm due to its promising performance in academic works. However, in practice, we encountered persistent artifacts such as the loss of high-frequency details and noticeable color shifts issues that have been increasingly documented by the community. These observations signaled need for algorithmic refinement. Through deeper exploration of the distillation mechanism, we gained new 20 insights into the underlying dynamics of DMD, leading to two key technical advancements: Decoupled DMD [45] and DMDR [31]. We refer interested readers to the respective academic papers for full technical details. Below, we introduce the practical application of these techniques in building Z-Image-Turbo. Figure 13 Few-Step Distillation visualization results across different distillation strategies: (a) the original SFT model; (b) Standard DMD; (c) Decoupled DMD (D-DMD); and (d) D-DMD+DMDR (Z-Image-Turbo). The proposed approach achieves real-time 8-step inference while attaining superior perceived quality and aesthetic appeal. 4.5.1. Decoupled DMD: Resolving Detail and Color Degradation Our investigation revealed core insight: the effectiveness of existing DMD methods is not monolithic phenomenon but the result of two independent, collaborating mechanisms: CFG-Augmentation (CA): This acts as the primary engine driving the distillation process, efficiently building up the few-step generation capabilities of the student model. Despite its dominant role, this factor has been largely overlooked in previous literature. Distribution Matching (DM): This functions primarily as powerful regularizer, ensuring the stability of the training process and removing the emerging artifacts. By recognizing and decoupling these two mechanisms, we were able to study and optimize them in isolation. This motivation led to the development of Decoupled DMD, an improved distillation framework that features decoupled application of renoising schedules tailored specifically for the CA and DM terms. In practice, Decoupled DMD effectively addresses the pain points of traditional DMD, ensuring sharp detail preservation and color fidelity. Notably, the resulting distilled model not only matches the original multi-step teacher but even surpasses it in terms of photorealism and visual impact. 21 4.5.2. DMDR: Enhancing Capacity with RL and Regularization To further push the performance boundaries of our few-step model, we incorporate Reinforcement Learning (RL) into the few-step distillation process. Applying RL to generative models typically faces the risk of \"reward hacking\", where the model exploits the reward function to generate high-scoring but visually nonsensical images. To mitigate this, external regularization is usually required. Our insight from Decoupled DMD provides natural solution: since we established that the Distribution Matching (DM) term functions as high-quality regularizer, it can be organically combined with RL objectives. This synthesis gives rise to DMDR (Distribution Matching Distillation meets Reinforcement Learning) [31]. In this framework, RL unlocks the student models capacity to align with human preferences, while the DM term acts as robust constraint, effectively preventing reward hacking. This synergy allows Z-Image-Turbo to achieve superior aesthetic alignment and semantic faithfulness while maintaining strict generative stability. 4.5.3. Results and Analysis The efficacy of our Decoupled DMD and DMDR distillation strategy is visualized in Figure 13. The original SFT model (a) sets high baseline but suffers from high latency. Standard DMD (b), while fast, exhibits characteristic degradation: blurred textures and shifted color tones. Our Decoupled DMD (c) successfully resolves these artifacts, restoring sharp details and accurate colors. Finally, Z-Image-Turbo (d), refined via combination of Decoupled DMD and DMDR, represents the optimal convergence of speed and quality. It achieves 8-step inference that is not only indistinguishable from the 100-step teacher but frequently surpasses it in perceived quality and aesthetic appeal. In summary, our Few-Step Distillation framework resolves the long-standing tension between inference speed and visual fidelity. 4.6. Reinforcement Learning with Human Feedback (RLHF) Following the previous stages, the model has acquired strong foundational capabilities but may still exhibit inconsistencies in aligning with nuanced human preferences. To bridge this gap, we introduce comprehensive post-training framework leveraging Reinforcement Learning with Human Feedback (RLHF). This framework hinges on powerful, multi-dimensional reward model, which provides targeted feedback for online optimization. Guided by these feedback signals, our approach is structured into two sequential stages: an initial offline alignment phase using Direct Preference Optimization (DPO) [59], followed by an online refinement phase with Group Relative Policy Optimization (GRPO) [66, 46]. This two-stage strategy allows us to first efficiently instill robust adherence to objective standards and then leverage the fine-grained signals from our reward model for optimizing more subjective qualities. As illustrated in Figure 14, this comprehensive process yields substantial improvements in photorealism, aesthetic quality, and instruction following. 4.6.1. Reward Annotation and Training As an indispensable and critical component of the RLHF pipeline, our reward model is designed to evaluate the models performance along three key dimensions: instruction-following capability, AIContent Detection perception, and aesthetic quality. The reward model is then trained specifically to provide targeted feedback along these axes. For instruction following, we perform syntactic and semantic decomposition of the prompt into structured hierarchy that includes (i) core subject entities, (ii) attribute specifications, (iii) action or interaction requirements, (iv) spatial or compositional constraints, and (v) stylistic or rendering conditions. During annotation, human raters simply click on the elements that are not satisfied by the models output. We then compute the ratio of satisfied elements to obtain the final instruction-following score, which is used as the target reward. 4.6.2. Stage 1: Offline Alignment with DPO on Objective Dimensions While manually curating preference pairs for DPO is feasible for capturing human aesthetic judgments, scaling this process to large, high-quality dataset presents significant bottleneck in real practice. Sourcing consistently informative preference pairs across subjective dimensions (e.g., aesthetics, style) 22 Figure 14 Visual comparison between Few-Step Distillation (FSD, top row) and RLHF (bottom row). Building upon the strong foundation of the FSD model, RLHF further enhances photorealism, aesthetic quality, and instruction following. is slow and requires extensive expert annotation. To address this scalability challenge and enhance annotation efficiency, our DPO strategy pivots to focus exclusively on objective, verifiable dimensions. These dimensions, such as text rendering and object counting, offer clear and binary correctness criteria that are highly amenable to automated evaluation by modern Vision-Language Models (VLMs). For instance, given prompt requiring specific text, an image with accurately rendered characters is designated as the positive sample (chosen), while an image with typographical errors becomes the negative sample (rejected). We leverage VLMs to programmatically generate large corpus of such candidate preference pairs. This VLM-generated dataset is then subjected to streamlined human verification and cleaning process, ensuring high fidelity. This hybrid VLM-human pipeline dramatically increases annotation throughput and consistency compared to purely human manual curation. Furthermore, to smooth the learning curve, we implement curriculum learning strategy for DPO training. The process begins with prompts of low complexity (e.g., rendering single word, generating small number of objects) and progressively advances to more challenging instructions involving multiple elements, complex layouts, or difficult styles. During this process, we also optimized our pair selection strategy. We observed that DPOs convergence is sensitive to the differentiation between positive and negative samples. To maximize training efficiency, our curriculum initially prioritizes pairs with moderate differentiation and gradually introduces more challenging pairs exhibiting larger or more subtle differences, which we found accelerate convergence and improve the final performance. 4.6.3. Stage 2: Online Refinement with GRPO Building upon the robust foundation established by DPO, the second stage employs online reinforcement learning with GRPO. Guided by our reward model, this stage is designed to significantly enhance the models capability for photorealistic image generation, alongside improving aesthetic quality and nuanced instruction-following. During the GRPO training loop, we compute composite advantage function by aggregating the scores from our reward model (e.g., realism, aesthetics, instruction following, etc.). This multi-faceted feedback mechanism enables targeted, fine-grained optimization [84]. By providing distinct signals for different aspects of the generation, GRPO can simultaneously enhance photorealistic image generation, aesthetic quality, improve semantic accuracy, and reduce undesirable artifacts. This integrated approach proved to be significantly more effective than optimizing against single reward, allowing the model to achieve 23 better balance across multiple, often competing, quality dimensions. 4.7. Continued Training for Image Editing Starting from the base model, the continued pre-training for image editing consists of two stages, as shown in Figure 10. In the continued pre-training stage, we train the model with our constructed editing pairs (see Section 2.5), together with our text-to-image SFT data to ensure high image quality. We first train the whole amount of editing data in resolution of 5122 for few thousand steps for quick adaptation to editing tasks, and then increase the image resolution to 10242 for high generation quality. Because image editing data pairs are expensive and difficult to acquire, their total volume is significantly smaller and far less diverse than that of text-to-image data. Therefore, we suggest relatively higher ratio of text-to-image data (e.g., text-to-image:image-to-image = 4 : 1) to avoid performance degradation during training. In the following SFT stage, task-balanced, high-quality subset of the training data is manually constructed to further improve the models overall performance, especially its instruction following ability. However, synthetic data (e.g., the rendered text data for text editing), though easy-to-acquire and guaranteed to be 100% accurate in terms of instruction following, are far from the distribution of real-world user input, and are thus heavily downsampled in this final training stage. 4.8. Prompt Enhancer with Reasoning Chain Figure 15 PE visualization. We compare generation results between PE without reasoning (middle column) and PE with reasoning (right column). As shown in the top row, the reasoning chain enables the model to decipher raw coordinates into specific scenic context (e.g., West Lake) rather than simply rendering the coordinate text. In the second row, the reasoning module plans detailed steps for \"brewing Pu-erh tea,\" allowing the model to generate specific illustrations for each step instead of generic list. This demonstrates that the reasoning chain effectively injects world knowledge and provides fine-grained content planning for complex user prompts. Due to limited model size (6B parameters), Z-Image exhibits limitations in world knowledge, intent understanding, and complex reasoning. However, it serves as powerful text decoder capable of translating detailed prompts into realistic images. To address the cognitive gaps, we propose equipping Z-Image with Prompt Enhancer (PE), powered by system prompt and pretrained VLM, to improve its reasoning and knowledge capabilities. Distinct from other methods, we keep the large VLM fixed during alignment. Instead, we process all input prompts (and input images for Z-Image-Edit) through our PE model during the Supervised Fine-Tuning (SFT) stage. This strategy ensures that Z-Image aligns effectively with the Prompt Enhancer during SFT. Furthermore, we identify the structured reasoning chain as key factor for injecting reasoning and world knowledge. As shown in Figure 15, without reasoning, the PE merely renders coordinate text onto the image when given geolocation data; with reasoning, it infers the location (e.g., West Lake) to generate the correct scene. Similarly, in generating journal-style instructions, the lack of reasoning leads to monotonous outputs, whereas the reasoning-enhanced model enriches the result by generating specific illustrations for each step. 5. Performance Evaluation 5.1. Human Preference Evaluation 5.1.1. Elo-based Human Preference Evaluation Figure 16 Z-Image-Turbo secured the 8th position among all evaluated models in the Text-to-Image Elo rankings provided by the Artificial Analysis AI Arena. To rigorously benchmark Z-Image-Turbos capabilities against the competitive landscape of generative models, we participated in Artificial Analysis Image Arena2 and Alibaba AI Arena3, two public-facing, independent benchmarking platforms powered by Elo-based large-scale human judgment. Unlike automated metrics that frequently misalign with human perception, the Elo-based evaluation provides 2https://artificialanalysis.ai/image/leaderboard/text-to-image 3https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=T2I 25 Figure 17 Z-Image-Turbo ranks first among open-source models in the Text-to-Image Elo rankings from the Artificial Analysis AI Arena. dynamic and unbiased model rankings based on thousands of pairwise comparisons, making it an ideal venue for objective performance assessment. The evaluation protocol is built upon the Elo rating system well-established method for ranking competitors based on head-to-head outcomes. In each round, two images generated from the same text prompt by different models are displayed side-by-side with identities hidden. Evaluators are asked to select the image they perceive as superior in terms of visual coherence, detail rendering, prompt alignment, and artistic quality. Each vote updates the global Elo leaderboard dynamically, ensuring that rankings reflect collective human judgment over time. In the evaluation, Z-Image-Turbo our high-efficiency diffusion architecture with 6B parameters and low inference cost of 8 NFEs was benchmarked against other leading models. These included top-tier closed-source systems such as Nano Banana Pro [27], Imagen 4 Ultra Preview 0606 [26] (Google), Flux.2 (Black Forest Labs) [36], Seedream (ByteDance) [64], GPT Image 1 [High] (OpenAI) [55], and FLUX.1 Kontext [Pro] (Black Forest Labs) [36], as well as open-source baselines like HunyuanImage 3.0 [8] and Qwen-Image [76]. According to the Artificial Analysis leaderboard in Figure 16 and Figure 17, Z-Image-Turbo achieved an Elo score of 1,161, ranking 8th overall. This result is notable for two primary reasons. Firstly, Z-ImageTurbo is established as the top-ranked open-source model on this highly competitive platform, with performance comparable to that of leading proprietary systems like Googles Imagen 4 and ByteDances Seedream. Secondly, the model exhibits exceptional efficiency: among the top ten models, it features not only the smallest parameter count (6B) but also the lowest inference cost ($5.0 per 1,000 images). This unique combination of high-quality output and low computational overhead is defining characteristic of our architecture. These findings are further corroborated by its performance on the Alibaba AI Arena (see Table 3), where Z-Image-Turbo secures higher rank of 4th overall while again leading the open-source category. 26 In summary, these results establish Z-Image-Turbo as one of the leading open text-to-image models in terms of both quality and efficiency. More than high-performing generator, it represents new baseline for efficiency-oriented architecture design, demonstrating that compact models can achieve elite-level performance without compromising usability. This combination of speed, fidelity, and openness enables deployment in resource-constrained environments, interactive applications, and community-driven innovation. Table 3 Elo rankings of Text-to-Image models from Alibaba AI Arena. Z-Image-Turbo achieves 4th globally and 1st among open-source models. Rank Model Name Company Type 95% CI Elo Score Win Rate 1 2 3 4 5 6 7 8 9 Imagen 4 Ultra Preview 0606 gemini-2.5-flash-image-preview Seedream 4.0 Z-Image-Turbo Seedream 3.0 Qwen-Image GPT Image 1 FLUX.1 Kontext Pro Ideogram 3. Google Google ByteDance Alibaba ByteDance Alibaba OpenAI Black Forest Labs Ideogram Closed-source Closed-source Closed-source Open-source (6B) Closed-source Open-source (20B) Closed-source Closed-source Closed-source -16/+16 -16/+14 -17/+16 -15/+17 -15/+19 -16/+16 -14/+17 -15/+14 -15/+16 1048 1046 1039 1025 1012 1008 986 950 936 48% 47% 46% 45% 41% 41% 38% 32% 29% 5.1.2. Human Preference Comparison with Flux 2 dev To benchmark our Z-Image model against the latest state-of-the-art open-source model, Flux 2 dev [35], we conducted an additional user study. The study involved evaluating set of 222 samples generated by Z-Image and Flux 2 dev with user-style prompts. Each sample was assessed by three independent annotators to ensure robustness and reduce bias in the evaluation process. The results, as summarized in Table 4, demonstrate significant advantage for Z-Image model. Specifically, the \"G Rate\" (i.e., Good Rate) achieve 46.4%, indicating high proportion of satisfactory generations. Concurrently, the Rate (i.e., Same Rate) is 41.0%. Consequently, the combined G+S Rate reached an impressive 87.4%, suggesting that Z-Image consistently produces high-quality or acceptable outputs from user prompts. In contrast, the Rate (i.e., Bad Rate) was only 12.6%. Remarkably, these superior results were achieved with Z-Image having only 1/5 of the parameters compared to Flux 2 dev (6B vs. 32B parameters). Table 4 User Preference Evaluation Results: G, S, B, and G+S Rates for Z-Image vs. Flux 2 dev Comparison Rate Rate Rate G+S Rate 46.4% 41.0% 12.6% 87.4% 5.2. Quantitative Evaluation To comprehensively evaluate the generation and editing capabilities of Z-Image and its variants (ZImage-Turbo and Z-Image-Edit), we conducted extensive experiments across multiple authoritative benchmarks. These evaluations cover general image generation, fine-grained instruction following, text rendering in both English and Chinese, and instruction-based image editing. 5.2.1. Text-to-Image Generation CVTG-2K. To evaluate our models performance on text rendering tasks, we conduct quantitative experiments on the CVTG-2K benchmark [17]. CVTG-2K is specialized benchmark designed for Complex Visual Text Generation, encompassing diverse scenarios with varying numbers of text regions. As presented in Table 5, our model achieves superior performance on CVTG-2K across all evaluation metrics. Specifically, Z-Image attains the highest average Word Accuracy score of 0.8671, outperforming competitive baselines such as GPT-Image-1 [55] (0.8569) and Qwen-Image [76] (0.8288). Notably, our model 27 demonstrates robust performance across varying levels of complexity, maintaining consistent accuracy even as the number of text regions increases from 2 to 5. Furthermore, Z-Image-Turbo, our efficient variant, achieves the highest CLIP Score of 0.8048 among all models while maintaining competitive text accuracy (0.8585 average Word Accuracy), striking an effective balance between generation quality and inference efficiency. These results demonstrate the effectiveness of our approach in complex visual text generation scenarios. Table 5 Quantitative evaluation results of English text rendering on CVTG-2K [17]. Rank Model NED CLIPScore 1 2 3 4 5 6 7 8 9 10 11 12 Z-Image Z-Image-Turbo GPT Image 1 [High] [55] Qwen-Image [76] TextCrafter [17] SD3.5 Large [18] Seedream 3.0 [21] FLUX.1 [dev] [36] 3DIS [98] RAG-Diffusion [39] TextDiffuser-2 [10] AnyText [70] 0.9367 0.9281 0.9478 0.9116 0.8679 0.8470 0.8537 0.6879 0.6505 0.4498 0.4353 0.4675 0.7969 0.8048 0.7982 0.8017 0.7868 0.7797 0.7821 0.7401 0.7767 0.7797 0.6765 0. Word Accuracy 2 regions 3 regions 4 regions 5 regions average 0.9006 0.8872 0.8779 0.8370 0.7628 0.7293 0.6282 0.6089 0.4495 0.4388 0.5322 0.0513 0.8722 0.8662 0.8659 0.8364 0.7628 0.6825 0.5962 0.5531 0.3959 0.3316 0.3255 0.1739 0.8652 0.8628 0.8731 0.8313 0.7406 0.6574 0.6043 0.4661 0.3880 0.2116 0.1787 0.1948 0.8512 0.8347 0.8218 0.8158 0.6977 0.5940 0.5610 0.4316 0.3303 0.1910 0.0809 0.2249 0.8671 0.8585 0.8569 0.8288 0.7370 0.6548 0.5924 0.4965 0.3813 0.2648 0.2326 0.1804 LongText-Bench. To further assess our models capability in rendering longer texts, we evaluate its performance on LongText-Bench [22], specialized benchmark focusing on evaluating the performance of rendering longer texts in both English and Chinese. As shown in Table 6, our models demonstrate strong and consistent performance across both language settings. On LongText-Bench-EN, Z-Image achieves competitive score of 0.935, ranking third among all evaluated models, while on LongText-Bench-ZH, it attains score of 0.936, securing second place. Z-Image-Turbo also delivers impressive results, scoring 0.917 on the English benchmark and 0.926 on the Chinese benchmark, demonstrating strong efficiencyperformance trade-offs. This consistent performance across both languages highlights our models robust bilingual text rendering capability. Table 6 Quantitative evaluation results on LongText-Bench [22]. Rank Model LongText-Bench-EN LongText-Bench-ZH 1 2 3 4 5 6 7 8 9 10 11 12 13 Qwen-Image [76] Z-Image Z-Image-Turbo Seedream 3.0 [21] X-Omni [22] GPT Image 1 [High] [55] Kolors 2.0 [33] BAGEL [15] OmniGen2 [78] HiDream-I1-Full [7] BLIP3-o [11] Janus-Pro [14] FLUX.1 [Dev] [36] 0.943 0.935 0.917 0.896 0.900 0.956 0.258 0.373 0.561 0.543 0.021 0.019 0. 0.946 0.936 0.926 0.878 0.814 0.619 0.329 0.310 0.059 0.024 0.018 0.006 0.005 OneIG. We utilize the OneIG benchmark [9] to assess fine-grained alignment. As reported in Tables 7 and 8, Z-Image achieves the highest overall score (0.546) on the English track, surpassing Qwen-Image (0.539) and GPT Image 1 [High] (0.533). Notably, Z-Image sets new state-of-the-art in text rendering reliability with an English Text score of 0.987 and Chinese Text score of 0.988, significantly outperforming competitors. On the Chinese track, Z-Image ranks second overall (0.535), confirming its multi-lingual robustness. Additionally, our distilled version, Z-Image-Turbo, demonstrates impressive efficiency, maintaining strong performance with only marginal decrease compared to the base model. 28 Table 7 Quantitative evaluation results on OneIG-EN [9]. The overall score is the average of the five dimensions. Rank Model Alignment Text Reasoning Style Diversity Overall 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Z-Image Qwen-Image [76] GPT Image 1 [High] [55] Seedream 3.0 [21] Z-Image-Turbo Imagen 4 [26] Recraft V3 [61] HiDream-I1-Full [7] OmniGen2 [78] SD3.5 Large [18] CogView4 [97] FLUX.1 [Dev] [36] Kolors 2.0 [33] Imagen 3 [3] BAGEL [15] Lumina-Image 2.0 [58] SANA-1.5-4.8B [81] SANA-1.5-1.6B [81] BAGEL+CoT [15] SD 1.5 [63] SDXL [57] Show-o2-7B [83] BLIP3-o [11] Show-o2-1.5B [83] Janus-Pro [14] 0.881 0.882 0.851 0.818 0.840 0.857 0.810 0.829 0.804 0.809 0.786 0.786 0.820 0.843 0.769 0.819 0.765 0.762 0.793 0.565 0.688 0.817 0.711 0.798 0.553 0.987 0.891 0.857 0.865 0.994 0.805 0.795 0.707 0.680 0.629 0.641 0.523 0.427 0.343 0.244 0.106 0.069 0.054 0.020 0.010 0.029 0.002 0.013 0.002 0.001 0.280 0.306 0.345 0.275 0.298 0.338 0.323 0.317 0.271 0.294 0.246 0.253 0.262 0.313 0.173 0.270 0.217 0.209 0.206 0.207 0.237 0.226 0.223 0.219 0. 0.387 0.418 0.462 0.413 0.368 0.377 0.378 0.347 0.377 0.353 0.353 0.368 0.360 0.359 0.367 0.354 0.401 0.387 0.390 0.383 0.332 0.317 0.361 0.317 0.276 0.194 0.197 0.151 0.277 0.139 0.199 0.205 0.186 0.242 0.225 0.205 0.238 0.300 0.188 0.251 0.216 0.216 0.222 0.209 0.429 0.296 0.177 0.229 0.186 0.365 0.546 0.539 0.533 0.530 0.528 0.515 0.502 0.477 0.475 0.462 0.446 0.434 0.434 0.409 0.361 0.353 0.334 0.327 0.324 0.319 0.316 0.308 0.307 0.304 0.267 Table 8 Quantitative evaluation results on OneIG-ZH [9]. The overall score is the average of the five dimensions. Rank Model Alignment Text Reasoning Style Diversity Overall 1 2 3 4 5 6 7 8 9 10 11 12 13 Qwen-Image [76] Z-Image Seedream 3.0 [21] Z-Image-Turbo GPT Image 1 [High] [55] Kolors 2.0 [33] BAGEL [15] Cogview4 [97] HiDream-I1-Full [7] Lumina-Image 2.0 [58] BAGEL+CoT [15] BLIP3-o [11] Janus-Pro [14] 0.825 0.793 0.793 0.782 0.812 0.738 0.672 0.700 0.620 0.731 0.719 0.608 0.324 0.963 0.988 0.928 0.982 0.650 0.502 0.365 0.193 0.205 0.136 0.127 0.092 0.148 0.267 0.266 0.281 0.276 0.300 0.226 0.186 0.236 0.256 0.221 0.219 0.213 0. 0.405 0.386 0.397 0.361 0.449 0.331 0.357 0.348 0.304 0.343 0.385 0.369 0.264 0.279 0.243 0.243 0.134 0.159 0.333 0.268 0.214 0.300 0.240 0.197 0.233 0.358 0.548 0.535 0.528 0.507 0.474 0.426 0.370 0.338 0.337 0.334 0.329 0.303 0.240 GenEval. As shown in Table 9, we evaluate object-centric generation using GenEval [23]. Z-Image achieves an overall score of 0.84, securing three-way tie for second place alongside Seedream 3.0 [21] and GPT Image 1 [High] [55], trailing only Qwen-Image [76] (0.87). Notably, Z-Image-Turbo delivers highly competitive performance with an overall score of 0.82, maintaining only 2-point gap from the base model. These results indicate that our model possesses robust capability for generating accurate and distinct entities. DPG-Bench. Table 10 presents the comparison on the DPG-Bench benchmark [30], which evaluates the ability of prompt following in dense prompts. Z-Image achieves strong global performance, ranking third overall with score of 88.14, closely trailing Seedream 3.0 [21] and Qwen-Image [76]. Notably, our model demonstrates robust performance in the Attribute dimension (93.16), surpassing the leading QwenImage (92.02) and Seedream 3.0 (91.36). Furthermore, our 8-step distillation model (Z-Image-Turbo), maintains competitive performance while achieving high efficiency. Table 9 Quantitative Evaluation results on GenEval [23]. Rank Model Single Object Two Object Counting Colors Position Attribute Binding Overall 1 2 2 2 5 6 7 8 9 10 11 12 13 14 15 Qwen-Image [76] Z-Image Seedream 3.0 [21] GPT Image 1 [High] [55] HiDream-I1-Full [7] Z-Image-Turbo Janus-Pro-7B [14] Lumina-Image 2.0 [58] SD3.5-Large [18] FLUX.1 [Dev] [36] JanusFlow [51] SD3 Medium [18] Emu3-Gen [72] Show-o [82] PixArt-ð›¼ [13] 0.99 1.00 0.99 0.99 1.00 1.00 0.99 - 0.98 0.98 0.97 0.98 0.98 0.95 0. 0.92 0.94 0.96 0.92 0.98 0.95 0.89 0.87 0.89 0.81 0.59 0.74 0.71 0.52 0.50 0.89 0.78 0.91 0.85 0.79 0.77 0.59 0.67 0.73 0.74 0.45 0.63 0.34 0.49 0.44 0.88 0.93 0.93 0.92 0.91 0.89 0.90 - 0.83 0.79 0.83 0.67 0.81 0.82 0.80 0.76 0.62 0.47 0.75 0.60 0.65 0.79 - 0.34 0.22 0.53 0.34 0.17 0.11 0.08 0.77 0.77 0.80 0.61 0.72 0.68 0.66 0.62 0.47 0.45 0.42 0.36 0.21 0.28 0.07 0.87 0.84 0.84 0.84 0.83 0.82 0.80 0.73 0.71 0.66 0.63 0.62 0.54 0.53 0. Table 10 Quantitative evaluation results on DPG [30]. Rank Model Global Entity Attribute Relation Other Overall 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Qwen-Image [76] Seedream 3.0 [21] Z-Image Lumina-Image 2.0 [58] HiDream-I1-Full [7] GPT Image 1 [High] [55] Z-Image-Turbo Janus-Pro-7B [14] SD3 Medium [18] FLUX.1 [Dev] [36] DALL-E 3 [4] Janus-Pro-1B [14] Emu3-Gen [72] PixArt-Î£ [12] Janus [77] Hunyuan-DiT [40] Playground v2.5 [38] SDXL [57] Lumina-Next [99] PixArt-ð›¼ [13] SD1.5 [63] 91.32 94.31 93.39 - 76.44 88.89 91.29 86.90 87.90 74.35 90.97 87.58 85.21 86.89 82.33 84.59 83.06 83.27 82.82 74.97 74. 91.56 92.65 91.22 91.97 90.22 88.94 89.59 88.90 91.01 90.00 89.61 88.63 86.68 82.89 87.38 80.59 82.59 82.43 88.65 79.32 74.23 92.02 91.36 93.16 90.20 89.48 89.84 90.14 89.40 88.83 88.96 88.39 88.17 86.84 88.94 87.70 88.01 81.20 80.91 86.44 78.60 75.39 94.31 92.78 92.22 94.85 93.74 92.63 92.16 89.32 80.70 90.87 90.58 88.98 90.22 86.59 85.46 74.36 84.08 86.76 80.53 82.57 73.49 92.73 88.24 91.52 - 91.83 90.96 88.68 89.48 88.68 88.33 89.83 88.30 83.15 87.68 86.41 86.41 83.50 80.41 81.82 76.96 67.81 88.32 88.27 88.14 87.20 85.89 85.15 84.86 84.19 84.08 83.84 83.50 82.63 80.60 80.54 79.68 78.87 75.47 74.65 74.63 71.11 63.18 TIIF. Table 11 details the performance on the TIIF benchmark testmini [74], which systematically evaluates instruction-following capabilities. Z-Image and Z-Image-Turbo achieve the 4th and 5th ranks, respectively. These results demonstrate that both the base and distilled versions possess exceptional capabilities in interpreting and executing complex user instructions across diverse categories. PRISM-Bench. We evaluate our models on PRISM-Bench [19], VLM-powered benchmark assessing reasoning and aesthetics across seven tracks. On the English track  (Table 12)  , Z-Image-Turbo achieves the 3rd rank (77.4), outperforming the base model and Qwen-Image, which highlights its superior efficiency and generation quality. On the Chinese track  (Table 13)  , Z-Image ranks 2nd (75.3), demonstrating robust multi-lingual performance with exceptional scores in Text Rendering (83.4) and Composition (88.6). 30 Table 11 Quantitative evaluation results on TIIF Bench testmini [74]. Rank Model Overall Basic Following Advanced Following short long short long short long short long Avg Attribute Relation Reasoning long short Avg short long Attr.+Rela. long short Attr.+Reas. long short Rela.+Reas. long short Style Text short long short long Designer Real World long short 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 GPT Image 1 [High] [55] Qwen-Image [76] Seedream 3.0 [21] Z-Image Z-Image-Turbo DALL-E 3 [4] FLUX.1 [dev] [36] FLUX.1 [Pro] [36] Midjourney V7 [52] SD 3 [18] SANA 1.5 [81] Janus-Pro-7B [14] Infinity [28] PixArt-Î£ [12] Show-o [82] LightGen [79] Hunyuan-DiT [40] Lumina-Next [99] 89.15 86.14 86.02 80.20 77.73 74.96 71.09 67.32 68.74 67.46 67.15 66.50 62.07 62.00 59.72 53.22 51.38 50.93 88.29 86.83 84.31 83.04 80.05 70.81 71.78 69.89 65.69 66.09 65.73 65.02 62.32 58.12 58.86 43.41 53.28 52.46 90.75 90.18 87.07 78.36 81.85 78.72 83.12 79.08 77.41 78.32 79.66 79.33 73.08 70.66 73.08 66.58 69.33 64.58 89.66 87.22 84.93 82.79 81.59 78.50 78.65 78.91 76.00 77.75 77.08 78.25 75.41 75.25 75.83 47.91 69.00 66.08 91.33 90.50 90.50 79.50 86.50 79.50 87.05 78.83 77.58 83.33 79.83 79.33 74.33 69.33 74.83 55.83 65.83 56.83 87.08 91.50 90.00 86.50 87.00 79.83 83.17 81.33 81.83 79.83 77.83 82.33 76.83 78.83 79.83 47.33 69.83 59. 84.57 88.22 89.85 80.45 82.88 80.82 87.25 82.82 82.07 82.07 85.57 78.32 72.82 75.07 78.82 74.82 78.07 67.57 84.57 90.78 85.94 79.94 79.99 78.82 80.39 83.82 76.82 78.82 83.57 73.32 77.57 77.32 78.32 45.82 73.82 71.82 96.32 79.81 80.86 75.13 76.17 75.82 75.01 75.57 72.57 71.07 73.57 80.32 72.07 67.57 65.57 69.07 64.07 69.32 97.32 79.38 78.86 81.94 77.77 76.82 72.39 71.57 69.32 74.07 69.82 79.07 71.82 69.57 69.32 50.57 63.32 67.07 88.55 79.30 79.16 72.89 68.32 73.39 65.79 61.10 64.66 61.46 61.50 59.71 56.64 57.65 53.67 46.74 42.62 44.75 88.35 80.88 80.60 77.02 74.69 67.27 68.54 65.37 60.53 59.56 60.67 58.82 54.98 49.50 50.38 41.53 45.45 45. 87.07 79.21 79.76 72.91 72.04 73.45 67.07 62.32 67.20 61.07 65.32 66.07 60.44 65.20 60.95 62.44 50.20 51.44 89.44 78.94 81.82 77.56 75.24 67.20 73.69 65.57 62.70 64.07 56.57 56.20 55.57 56.57 56.82 40.82 41.57 43.20 87.22 78.85 77.23 66.99 60.22 72.01 73.84 69.84 81.22 68.84 69.96 70.46 74.22 66.96 68.59 61.71 59.22 51.09 83.96 81.69 78.85 73.82 73.33 71.34 73.34 71.47 71.59 70.34 73.09 70.84 64.71 61.72 68.96 50.47 61.84 59.72 85.59 75.57 75.64 73.89 68.90 63.59 69.09 65.96 60.72 50.96 62.96 67.22 60.22 66.59 66.46 50.34 47.84 44.72 83.21 78.59 78.64 75.62 71.92 60.72 71.59 67.72 64.59 57.84 65.84 59.97 59.71 54.59 56.22 45.34 51.09 54. 90.00 100.00 100.00 90.00 83.33 89.66 66.67 63.00 83.33 66.67 80.00 60.00 80.00 83.33 63.33 53.33 56.67 70.00 93.33 100.00 93.33 93.33 93.33 86.67 66.67 63.00 80.00 76.67 80.00 70.00 73.33 70.00 66.67 53.33 73.33 66.67 89.83 92.76 97.17 94.84 83.71 66.83 43.83 35.83 24.83 59.83 17.83 28.83 10.83 1.83 3.83 0.00 0.00 0.00 86.83 89.14 87.78 93.21 84.62 54.83 52.83 55.83 20.83 20.83 15.83 33.83 23.83 1.83 2.83 6.83 0.83 0.83 89.73 90.30 83.21 88.06 85.82 72.93 70.72 71.80 68.83 63.23 71.07 65.84 54.28 62.11 55.02 50.92 40.10 47.56 93.46 91.42 83.58 85.45 77.24 60.99 71.47 68.80 63.61 67.34 68.83 60.25 56.89 52.41 50.92 50.55 44.20 49. Table 12 Quantitative results on PRISM-Bench [19] evaluated by Qwen2.5-VL-72B [2]. Rank Model Imagination Entity Text rendering Style Affection Composition Long text Overall Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 GPT-Image-1 [High] [55] Gemini 2.5-Flash-Image [25] Z-Image-Turbo Seedream 3.0 [21] Z-Image Qwen-Image [76] FLUX.1-Krea-dev [36] HiDream-I1-Full [7] SD3.5-Large [18] HiDream-I1-Dev [7] FLUX.1-dev [36] SD3.5-Medium [18] SD3-Medium [18] FLUX.1-schnell [36] Janus-Pro-7B [14] Bagel [15] Bagel-CoT [15] Playground [38] SDXL [57] SD2.1 [63] SD1.5 [63] 79.8 84.7 65.7 75.8 68.0 75.5 69.6 73.0 66.7 68.8 65.5 65.1 64.3 62.8 65.0 68.0 68.0 59.0 54.5 48.9 40.7 53.3 38.1 50.1 38.0 47.3 37.4 43.1 44.0 43.4 45.8 42.9 34.7 37.7 35.6 38.8 45.0 44.1 39.0 34.1 28.4 23.7 66.6 61.4 57.9 56.9 57.6 56.5 56.3 58.5 55.0 57.3 54.2 49.9 51.0 49.2 51.9 56.5 56.0 49.0 44.3 38.6 32.2 87.3 86.0 75.7 81.3 75.0 79.5 72.2 76.3 76.8 73.5 70.6 72.5 69.4 64.8 68.6 67.6 67.6 69.4 71.1 66.0 61.2 81.0 76.7 82.3 74.2 74.4 64.5 70.7 72.8 72.7 68.1 61.9 70.9 63.3 56.8 63.5 53.4 53.4 56.7 65.0 57.6 52. 84.1 81.3 79.0 77.7 74.7 72.0 71.4 74.5 74.8 70.8 66.2 71.7 66.3 60.8 66.0 60.5 60.5 63.0 68.0 61.8 56.9 66.7 72.8 59.6 58.8 59.3 57.9 51.7 60.5 53.6 56.7 52.3 36.6 38.5 54.3 23.1 29.4 29.4 15.3 18.6 16.7 11.4 86.8 84.3 84.9 74.0 81.6 71.2 76.1 76.4 73.1 75.7 73.0 64.5 63.3 68.1 50.3 42.3 42.3 31.9 37.3 31.4 24.1 76.8 78.5 72.2 66.4 70.4 64.5 63.9 68.4 63.3 66.2 62.6 50.5 50.9 61.2 36.7 35.8 35.8 23.6 27.9 24.0 17.8 87.3 89.5 76.7 84.4 78.0 86.6 80.0 81.4 77.3 70.2 72.6 75.5 74.6 70.3 70.7 69.0 69.0 74.6 71.7 62.7 56.7 87.8 87.8 88.2 84.1 89.0 84.4 86.6 81.5 78.2 77.4 74.2 80.0 79.5 71.5 75.2 69.7 69.7 74.6 72.6 66.5 61. 87.5 88.6 82.4 84.2 83.5 85.5 83.3 81.4 77.7 73.8 73.4 77.7 77.0 70.9 72.9 69.3 69.3 74.6 72.1 64.6 59.1 88.1 94.3 85.1 90.5 84.3 89.9 82.6 90.0 85.6 88.2 86.0 81.8 80.5 75.4 80.7 87.1 87.1 88.8 78.7 68.5 66.9 79.8 74.8 87.4 74.6 80.1 70.4 78.7 76.6 73.9 74.3 72.9 73.9 75.5 65.9 68.0 66.7 66.7 66.0 66.5 62.1 60.7 84.0 84.5 86.2 82.5 82.2 80.1 80.6 83.3 79.7 81.2 79.4 77.9 78.0 70.6 74.3 76.9 76.9 77.4 72.6 65.3 63.8 92.2 91.2 89.0 93.6 89.1 93.9 90.8 88.5 87.8 84.7 87.4 85.4 85.6 81.7 82.4 86.6 86.6 72.2 72.2 64.8 57.5 84.9 88.2 90.2 85.1 85.1 79.5 87.1 80.3 80.9 78.5 75.8 81.0 79.5 75.6 71.1 69.2 69.2 61.3 67.8 58.3 53. 88.5 89.7 89.6 89.3 87.1 86.7 88.9 84.4 84.3 81.6 81.6 83.2 82.5 78.6 76.7 77.9 77.9 66.7 70.0 61.5 55.4 77.2 76.3 69.8 76.2 70.6 76.8 73.6 66.3 65.8 64.0 70.5 63.5 63.4 68.7 63.9 64.5 64.5 56.0 54.1 50.7 47.3 77.5 80.6 79.0 76.4 76.6 70.9 73.4 48.6 52.2 49.3 53.8 50.6 50.3 54.4 49.0 50.2 50.2 35.3 34.5 29.8 26.8 77.4 78.4 74.4 76.3 73.6 73.8 73.5 57.4 59.0 56.6 62.1 57.0 56.8 61.5 56.4 57.3 57.3 45.6 44.3 40.2 37.0 82.7 85.0 74.5 80.1 74.9 80.0 74.4 76.6 73.4 72.3 72.1 68.6 68.0 68.3 64.9 67.5 67.5 62.2 60.1 54.0 48.8 78.7 75.8 80.3 72.3 76.2 68.3 73.7 68.6 67.8 67.0 64.9 65.1 64.2 61.1 59.4 56.6 56.5 52.1 54.0 47.7 43. 80.7 80.4 77.4 76.2 75.6 74.1 74.0 72.6 70.6 69.6 68.5 66.8 66.1 64.7 62.1 62.0 62.0 57.1 57.0 50.8 46.0 Table 13 Quantitative results on PRISM-Bench-ZH [19] evaluated by Qwen2.5-VL-72B [2]. Rank Model Imagination Entity Text rendering Style Affection Composition Long text Overall Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. Ali. Aes. Avg. 1 2 3 4 5 6 7 8 9 GPT-Image-1 [High] [55] Z-Image Z-Image-Turbo Seedream 3.0 [21] Qwen-Image [76] Bagel-CoT [15] Bagel [15] HiDream-I1-Full [7] HiDream-I1-Dev [7] 73.0 69.5 64.1 71.4 71.4 64.4 64.6 51.2 48.3 37.6 34.1 37.2 36.6 29.9 36.6 36.3 30.8 24.6 55.3 51.6 50.7 54.0 50.7 50.5 50.5 41.0 36.5 80.4 70.6 72.9 74.8 74.7 62.6 62.7 60.1 52. 82.1 73.7 82.4 73.8 67.8 53.8 55.5 61.3 54.1 81.3 72.2 77.6 74.3 71.3 58.2 59.1 60.7 53.4 73.1 76.8 69.4 70.7 64.3 25.2 18.6 20.7 18.6 89.9 90.0 89.7 88.0 73.1 51.9 26.3 40.6 35.3 81.5 83.4 79.6 79.4 68.7 38.6 22.5 30.7 27.0 77.1 74.1 72.9 74.1 75.2 65.4 66.0 64.5 59. 92.4 88.2 89.2 88.0 83.2 76.7 76.6 73.8 68.3 84.8 81.2 81.0 81.1 79.2 71.1 71.3 69.2 63.7 78.0 77.6 74.0 79.0 77.3 74.0 74.9 65.2 65.9 77.8 73.5 80.9 71.4 64.5 65.0 66.2 69.1 62.3 77.9 75.5 77.5 75.2 70.9 69.5 70.6 67.2 64.1 91.9 89.3 87.2 90.3 89.8 81.3 81.3 72.4 66. 85.7 88.0 85.8 83.2 74.1 71.3 72.2 69.0 64.6 88.8 88.6 86.5 86.8 82.0 76.3 76.8 70.7 65.6 72.4 71.6 71.7 73.0 72.6 61.4 62.4 57.1 54.2 76.3 75.6 74.8 71.2 65.8 46.6 47.3 42.8 38.6 74.4 73.6 73.3 72.1 69.2 54.0 54.9 50.0 46.4 78.0 75.7 73.1 76.2 75.0 62.0 61.5 55.9 52. 77.4 74.9 77.1 73.2 65.5 57.4 54.3 55.3 49.7 77.7 75.3 75.1 74.7 70.3 59.7 57.9 55.6 50.9 Table 14 Quantitative Evaluation results on ImgEdit [87]. Rank Model Add Adjust Extract Replace Remove Background Style Hybrid Action Overall UniWorld-V2 [42] 4.29 1 Qwen-Image-Edit [2509] [76] 4.32 2 Z-Image-Edit 4.40 3 Qwen-Image-Edit [76] 4 4.38 4.61 GPT-Image-1 [High] [55] 5 4.25 FLUX.1 Kontext [Pro] [36] 6 3.57 OmniGen2 [78] 7 3.82 UniWorld-V1 [43] 8 3.56 BAGEL [15] 9 3.88 Step1X-Edit [47] 10 3.58 11 ICEdit [94] 3.47 12 OmniGen [80] 3.44 13 UltraEdit [95] 3.18 14 AnyEdit [90] 2.84 15 MagicBrush [92] 2.45 Instruct-Pix2Pix [5] 4.44 4.36 4.14 4.16 4.33 4.15 3.06 3.64 3.31 3.14 3.39 3.04 2.81 2.95 1.58 1.83 4.32 4.04 4.30 3.43 2.90 2.35 1.77 2.27 1.70 1.76 1.73 1.71 2.13 1.88 1.51 1.44 4.69 4.64 4.57 4.66 4.35 4.56 3.74 3.47 3.30 3.40 3.15 2.94 2.96 2.47 1.97 2.01 31 4.72 4.52 4.13 4.14 3.66 3.57 3.20 3.24 2.62 2.41 2.93 2.43 1.45 2.23 1.58 1. 4.41 4.37 4.14 4.38 4.57 4.26 3.57 2.99 3.24 3.16 3.08 3.21 2.83 2.24 1.75 1.44 4.91 4.84 4.85 4.81 4.93 4.57 4.81 4.21 4.49 4.63 3.84 4.19 3.76 2.85 2.38 3.55 3.83 3.39 3.63 3.82 3.96 3.68 2.52 2.96 2.38 2.64 2.04 2.24 1.91 1.56 1.62 1.20 4.83 4.71 4.50 4.69 4.89 4.63 4.68 2.74 4.17 2.52 3.68 3.38 2.98 2.65 1.22 1.46 4.49 4.35 4.30 4.27 4.20 4.00 3.44 3.26 3.20 3.06 3.05 2.96 2.70 2.45 1.90 1.88 Table 15 Quantitative Evaluation results on GEdit-Bench [47]. Rank Model GEdit-Bench-EN GEdit-Bench-CN G_SC G_PQ G_O G_SC G_PQ G_O UniWorld-V2 [42] Qwen-Image-Edit [2509] [76] Z-Image-Edit Qwen-Image-Edit [76] GPT-Image-1 [High] [55] Step1X-Edit [47] BAGEL [15] OmniGen2 [78] FLUX.1 Kontext [Pro] [36] FLUX.1 Kontext [Dev] [36] OmniGen [80] UniWorld-V1 [43] 1 2 3 4 5 6 7 8 9 10 11 12 13 MagicBrush [92] 14 Instruct-Pix2Pix [5] AnyEdit [90] 8.39 8.15 8.11 8.00 7.85 7.66 7.36 7.16 7.02 6.52 5.96 4.93 4.68 3.58 3.18 8.02 7.86 7.72 7.86 7.62 7.35 6.83 6.77 7.60 7.38 5.89 7.43 5.66 5.49 5.82 7.83 7.54 7.57 7.56 7.53 6.97 6.52 6.41 6.56 6.00 5.06 4.85 4.52 3.68 3.21 - 8.08 8.03 7.82 7.67 7.20 7.34 - 1.11 - - - - - - - 7.89 7.80 7.79 7.56 6.87 6.85 - 7.36 - - - - - - - 7.54 7.54 7.52 7.30 6.86 6.50 - 1.23 - - - - - - 5.2.2. Instruction-based Image Editing ImgEdit. Table 14 shows the evaluation of Z-Image-Edit on the ImgEdit Benchmark [87], where the metric combines instruction completion and visual quality. Across 9 common editing tasks, Z-Image-Edit shows competitive editing performance with leading models , especially object addition and extraction. GEdit. We also evaluate Z-Image-Edit on the GEdit-Bench [47], which evaluates visual naturalness (G_PQ) and bilingual instruction following (G_SC). GEdit-Bench-EN abd GEdit-Bench-CN adopt English and Chinese instructions in the evaluation, respectively. As shown in Table 15, Z-Image-Edit achieves 3rd rank, demonstrating robust bilingual editing capabilities. 5.3. Qualitative Evaluation To further demonstrate the visual generation capacity of Z-Image 4, we first give the qualitative comparison against state-of-the-art open-source models (Lumina-Image 2.0 [58], Qwen-Image [76], HunyuanImage 3.0 [8], and FLUX 2 dev [35]) and close-source models (Imagen 4 Ultra [26], Seedream 4.0 [64] and Nano Banana Pro [27]). We then show the editing capacity of our Z-Image-Edit. We next show the examples of how reasoning capacity and world knowledge are injected by our prompt enhancer. We finally show that the emerging multi-lingual and multi-cultural understanding capacity of our Z-Image. 5.3.1. Superior Photorealistic Generation As shown in Figure 18 and 19, Z-Image-Turbo shows excellent character close-up generation (e.g., the skin details on mans face and girls tears). When asked to generate multi-expression portraits of one person (Figure 20), Z-Image-Turbo can produce images that are more aesthetically pleasing and have more realistic expressions, while Qwen-Image, HunyuanImage3.0, FLUX 2 dev, and Seedream 4.0 would sometimes generate exaggerated and unrealistic expressions, thus lacking authenticity and beauty. Moreover, when generating scene captured by mobile phone (Figure 21 and 22), Z-Image-Turbo shows strong performance in the authenticity of both the person and the background, as well as the aesthetic appeal of layout and posture. while Qwen-Image, HunyuanImage3.0, and FLUX 2 dev would generate unrealistic things (e.g., clothes that remain completely unsoaked in the heavy rain). 5.3.2. Outstanding Bilingual Text Rendering Figure 23 and Figure 24 show the qualitative comparison of Chinese and English text rendering. As shown in Figure 23 and Figure 24, Z-Image-Turbo accurately rendered the required text while maintaining the aesthetic appeal and authenticity of other parts (e.g., the authenticity of the human face in Figure 23 4In the section, all results of Z-Image are generated by our Turbo version. and the layout of the scene in Figure 24). Note that this is comparable to the leading closed-source model Nano Banana Pro, and surpasses other candidates. When rendering text in poster design (Figure 25 and Figure 26), Z-Image-Turbo not only presents correct text rendering, but also designs more aesthetically pleasing and realistic poster. For example, as shown in Figure 26), Qwen-Image, HunyuanImage3.0, FLUX 2 dev, and Imagen 4 Ultra make errors when rendering very small characters, Seedream4.0 and Nano Banana Pro make errors of repeatedly rendering the text, while Z-Image-Turbo gets the poster with the right rendered text and satisfactory design. 5.3.3. Instruction-following Editing The first two columns of Figure 27 demonstrates the ability of Z-Image-Edit to handle complex composite prompts. For example, the top simultaneously switches the background to the Sydney Opera House, inserting specific object (a sign reading Z-Image), and removing the characters backpack. The bottom row also illustrates precise control over multiple subjects. The last two columns of Figure 27 also illustrates that Z-Image-Edit can accurately modify textual content according to bounding-boxbased location constraint (left) and keep characters consistent when transforming the image (right). 5.3.4. Enhanced Reasoning Capacity and World Knowledge through Prompt Enhancer As demonstrated in Figure 15 and Figures 28-29, our prompt enhancer leverages structured reasoning chain comprising core subject analysis, problem solving/world knowledge injection, aesthetic enhancement, and comprehensive description to equip the model with logical reasoning and world knowledge capabilities. This allows the model to handle diverse tasks, ranging from solving complex logical puzzles (e.g., the chicken-and-rabbit problem) and interpreting user intent (e.g., visualizing classical poetry or inferring scenes from coordinates) to performing text rendering and question answering. In the context of image editing, prompt enhancer is also crucial for addressing ambiguous or unclear intentions, as well as for injecting world knowledge and enabling reasoning, similar to how it functions in text-to-image generation, as shown in Figure 30-31. For example, in Figure 31, the wrong dish is made because of lack of reasoning about the relationship between the ingredients and the dish, while prompt enhancer can make up for this. 5.3.5. Emerging Multi-lingual and Multi-cultural Understanding Capacity After trained with bilingual data, we are surprised to find that Z-Image has initially emerged with the ability to handle multilingual input. As shown in Figure 32, Z-Image can not only understand prompts in multiple languages but also generate images that align with local cultures and landmarks. 33 Figure 18 Comparison of close-up portrait generation, which indicates that Z-Image exhibits strong capabilities in character emotion and skin texture rendering. Better to zoom in to check the subtle expressions and the texture of the skin. 34 Figure 19 Comparison of close-up portrait generation, which indicates that Z-Image exhibits strong capabilities in character emotion and skin texture rendering. Better to zoom in to check the subtle expressions and the texture of the skin. 35 Figure 20 Comparison of complex close-up portrait generation, which indicates that Z-Image-Turbo has strong ability in rendering character expressions and skin textures, as well as generating aesthetic images. Better to zoom in to check the subtle expressions. 36 Figure 21 Comparison of scene shooting, which indicates that Z-Image-Turbo shows strong performance in the authenticity of both the person and the background, as well as the aesthetic appeal of layout and posture. Better to zoom in to check the texture of the clothes and hair. 37 Figure 22 Comparison of scene shooting, which indicates that Z-Image-Turbo shows strong performance in the authenticity of both the person and the background, as well as the aesthetic appeal of layout and posture. Better to zoom in to check the details. 38 Figure 23 Comparison of complex Chinese text rendering. It shows that only Z-Image-Turbo and Nano Banana Pro can accurately generates the expected Chinese couplet. Better to zoom in to check the correctness of the rendered text and the authenticity of the person. 39 Figure 24 Comparison of complex English text rendering. It shows that only Z-Image-Turbo and Nano Banana Pro can accurately generates the expected English couplet. Better to zoom in to check the correctness of the rendered text and the layout of the scene. 40 Figure 25 Comparison of Chinese text rendering in poster design. Z-Image-Turbo not only presents correct text rendering, but also designs more aesthetically pleasing and realistic poster. Better to zoom in to check the correctness of the rendered text and the fidelity of the food. 41 Figure 26 Comparison of English text rendering in poster design. Only Z-Image-Turbo presents correct text rendering with pleasing and realistic poster. Better to zoom in to check the correctness of the rendered text and the details of the poster. 42 Figure 27 The first two columns: Mixed-instruction editing across various tasks in Z-Image-Edit. The last two columns: Text editing (with bounding box) and identity-preservation editing in Z-Image-Edit. 43 Figure 28 Showcases of prompt enhancer for logical reasoning. 44 Figure 29 Showcases of prompt enhancer for world knowledge injection. Given the poem title \"After Passing the Imperial Examination\" (ç™»ç§‘åŽ), the baseline (Left) lacks cultural context. Our method (Right) leverages LLM priors to retrieve specific historical details (e.g., the galloping horse, red official robe) and the famous couplet: \"æ˜¥é£Žå¾—æ„é©¬è¹„ç–¾ä¸€æ—¥çœ‹å°½é•¿å®‰èŠ±\", the reasoning module (center) translates these literary semantics into visual cues, ensuring culturally faithful rendering with precise text transcription. 45 Figure 30 Showcases of prompt enhancer in image editing for handling ambiguous and unclear instructions. 46 Figure 31 Showcases of prompt enhancer in image editing for world knowledge injection and reasoning. 47 Figure 32 Emerging Multi-lingual and Multi-cultural Understanding Capacity of Z-Image-Turbo. It shows that Z-Image-Turbo can not only understand prompts in multiple languages but also leverage its world knowledge to generate images that align with local cultures and landmarks. 48 6. Conclusion In this report, we introduce the Z-Image series, suite of high-performance 6B-parameter models built upon Scalable Single-Stream Diffusion Transformer (S3-DiT). Challenging the prevailing scale-atall-costs paradigm, we propose holistic end-to-end solution anchored by four strategic pillars: (1) curated, efficient data infrastructure; (2) scalable single-stream architecture; (3) streamlined training strategy; and (4) advanced optimization techniques for high-quality and efficient inference, encompassing PEaware supervised fine-tuning, few-step distillation, and reward post-training. This synergy allows us to complete the entire workflow within 314K H800 GPU hours at total cost of under $630K, delivering top-tier photorealistic synthesis and bilingual text rendering. Beyond the robust base model, our pipeline yields Z-Image-Turbo, which enables sub-second inference (<1s) on an enterprise-grade H800 GPU and fits comfortably within 16G VRAM consumer-grade hardware. Additionally, we develop Z-Image-Edit, an editing model efficiently derived via our omni-pretraining paradigm. Through this pipeline, we provide the community with blueprint for developing accessible, budget-friendly, yet state-of-the-art generative models. 7. Authors 7.1. Core Contributors5 Huanqia Cai, Sihan Cao, Ruoyi Du, Peng Gao, Steven Hoi, Zhaohui Hou, Shijie Huang, Dengyang Jiang, Xin Jin, Liangchen Li, Zhen Li, Zhong-Yu Li, David Liu, Dongyang Liu, Junhan Shi, Qilong Wu, Feng Yu, Chi Zhang, Shifeng Zhang, Shilin Zhou 7.2. Contributors6 Chenglin Cai, Yujing Dou, Yan Gao, Minghao Guo, Songzhi Han, Wei Hu, Yuyan Huang, Xu Li, Zefu Li, Heng Lin, Jiaming Liu, Linhong Luo, Qingqing Mao, Jingyuan Ni, Chuan Qin, Lin Qu, Jinghua Sun, Peng Wang, Ping Wang, Shanshan Wang, Xuecong Wang, Yi Wang, Yue Wang, Tingkun Wen, Junde Wu, Minggang Wu, Xiongwei Wu, Yi Xin, Haibo Xing, Xiaoxiao Xu, Ze Xu, Xunliang Yang, Shuting Yu, Yucheng Zhao, Jianan Zhang, Jianfeng Zhang, Jiawei Zhang, Qiang Zhang, Xudong Zhao, Yu Zheng, Haijian Zhou, Hanzhang Zhou 5Core Contributors are listed in alphabetical order of the last name. 6Contributors are listed in alphabetical order of the last name."
        },
        {
            "title": "References",
            "content": "[1] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929947, 2024. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] Qi Cai, Yehao Li, Yingwei Pan, Ting Yao, and Tao Mei. Hidream-i1: An open-source high-efficient image generative foundation model. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1363613639, 2025. [8] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. [9] Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arXiv:2506.07977, 2025. [10] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In European Conference on Computer Vision, pages 386402. Springer, 2024. [11] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [12] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-ðœŽ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [13] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-ð›¼: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations. [14] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [15] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining, 2025. URL https://arxiv. org/abs/2505.14683. 50 [16] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. [17] Nikai Du, Zhennan Chen, Zhizhou Chen, Shan Gao, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes. arXiv preprint arXiv:2503.23461, 2025. [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024. [19] Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, and Hongsheng Li. Flux-reason-6m and prism-bench: million-scale text-toimage reasoning dataset and comprehensive benchmark. arXiv preprint arXiv:2509.09680, 2025. [20] Peng Gao, Le Zhuo, Chris Liu, , Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Luminat2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [21] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. [22] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. [23] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:52132 52152, 2023. [24] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 43674375, 2018. [25] Google. Gemini 2.5 flash & 2.5 flash image model card. https://storage.googleapis.com/d eepmind-media/Model-Cards/Gemini-2-5-Flash-Model-Card.pdf, 2025. [26] Google. Imagen 4 model card. https://storage.googleapis.com/deepmind-media/Mod el-Cards/Imagen-4-Model-Card.pdf, 2025. [27] Google. Nano banana pro. https://storage.googleapis.com/deepmind-media/Model-C ards/Gemini-3-Pro-Image-Model-Card.pdf, 2025. [28] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1573315744, 2025. [29] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. Advances in Neural Information Processing Systems Workshops (NeurIPS Workshops), 2021. [30] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [31] Dengyang Jiang, Dongyang Liu, Zanyi Wang, Qilong Wu, Xin Jin, David Liu, Zhen Li, Mengmeng Wang, Peng Gao, and Harry Yang. Distribution matching distillation meets reinforcement learning. arXiv preprint arXiv:2511.13649, 2025. [32] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 51 [33] Kuaishou Kolors Team. Kolors 2.0. https://app.klingai.com/cn/, 2025. [34] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2023. [35] Black Forest Labs. FLUX.2: State-of-the-Art Visual Intelligence. https://bfl.ai/blog/flux-2, 2025. [36] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [37] LeaderGPU. Gpu server rental pricing. https://www.leadergpu.com/, 2025. Accessed: November 2025. [38] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [39] Yuhan Li, Xianfeng Tan, Wenxiang Shang, Yubo Wu, Jian Wang, Xuanhong Chen, Yi Zhang, Hangcheng Zhu, and Bingbing Ni. Ragdiffusion: Faithful cloth generation via external knowledge assimilation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1748517495, 2025. [40] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [41] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context learning. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2025. [42] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. [43] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. [44] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [45] Dongyang Liu, David Liu, Peng Gao, Ruoyi Du, Zhen Li, Qilong Wu, Xin Jin, Sihan Cao, Shifeng Zhang, Hongsheng Li, and Steven Hoi. Decoupled dmd: Cfg augmentation as the spear, distribution matching as the shield. arXiv preprint, 2025. [46] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [47] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [48] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [49] Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin, Xinyue Li, Le Zhuo, Licheng Wen, Dongyang Liu, Yuewen Cao, et al. Omnicaptioner: One captioner to rule them all. arXiv preprint arXiv:2504.07089, 2025. 52 [50] Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui Ren, and Qiang Yang. Cosine normalization: Using cosine similarity instead of dot product in neural networks. In International conference on artificial neural networks, pages 382391. Springer, 2018. [51] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [52] Midjourney. Midjourney v7. https://www.midjourney.com/home, 2025. [53] Quang-Huy Nguyen, Cuong Nguyen, Dung Le, and Hieu Pham. Enhancing few-shot image classification with cosine transformer. IEEE Access, 11:7965979672, 2023. [54] Hiroyuki Ootomo, Akira Naruse, Corey Nolet, Ray Wang, Tamas Feher, and Yong Wang. Cagra: Highly parallel graph construction and approximate nearest neighbor search for gpus. In 2024 IEEE 40th International Conference on Data Engineering (ICDE), pages 42364247. IEEE, 2024. [55] OpenAI. Gpt-image-1. https://openai.com/zh-Hans-CN/index/introducing-4o-image -generation/, 2025. [56] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford infolab, 1999. [57] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [58] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. [59] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [60] rapidsai. cuGraph - RAPIDS Graph Analytics Library. https://github.com/rapidsai/cugr aph, 2018. Accessed: 2025-11-12. [61] Recraft. Recraft v3. https://www.recraft.ai/docs/recraft-models/recraft-V3, 2024. [62] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. [63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [64] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. [65] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. [66] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [67] FLUX-Krea Team. Flux.1 krea [dev]. https://github.com/krea-ai/flux-krea, 2025. [68] Vincent Traag, Ludo Waltman, and Nees Jan Van Eck. From louvain to leiden: guaranteeing well-connected communities. Scientific reports, 9(1):112, 2019. 53 [69] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [70] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. 2023. [71] Huy Vo, Vasil Khalidov, TimothÃ©e Darcet, ThÃ©o Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, et al. Automatic data curation for self-supervised learning: clustering-based approach. arXiv preprint arXiv:2405.15613, 2024. [72] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [73] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [74] Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. [75] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pages 2396523998. PMLR, 2022. [76] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [77] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Janus: Decoupling visual encoding for unified In Proceedings of the Computer Vision and Pattern Zhenda Xie, Xingkai Yu, Chong Ruan, et al. multimodal understanding and generation. Recognition Conference, pages 1296612977, 2025. [78] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [79] Xianfeng Wu, Yajing Bai, Haoze Zheng, Harold Haodong Chen, Yexin Liu, Zihao Wang, Xuran Ma, Wen-Jie Shu, Xianzu Wu, Harry Yang, et al. Lightgen: Efficient image generation through knowledge distillation and direct preference optimization. arXiv preprint arXiv:2503.08619, 2025. [80] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025. [81] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng YU, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inferencetime compute in linear diffusion transformer. In Forty-second International Conference on Machine Learning. [82] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations. 54 [83] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. [84] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. [85] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [86] An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. Chinese clip: Contrastive vision-language pretraining in chinese. arXiv preprint arXiv:2211.01335, 2022. [87] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. [88] Tianwei Yin, MichaÃ«l Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and In Improved distribution matching distillation for fast image synthesis. William Freeman. NeurIPS, 2024. [89] Tianwei Yin, MichaÃ«l Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [90] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. [91] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [92] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [93] Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. [94] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. [95] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [96] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. [97] Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogview3: Finer and faster text-to-image generation via relay diffusion. arXiv preprint arXiv:2403.05121, 2024. [98] Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis-flux: simple and efficient multi-instance generation with dit rendering. arXiv preprint arXiv:2501.05131, 2025. 55 [99] Le Zhuo, Ruoyi Du, Xiao Han, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems (NeurIPS), 2024. [100] Le Zhuo, Songhao Han, Yuandong Pu, Boxiang Qiu, Sayak Paul, Yue Liao, Yihao Liu, Jie Shao, Xi Chen, Si Liu, et al. Factuality matters: When image generation and editing meet structured visuals. arXiv preprint arXiv:2510.05091, 2025. A. Prompts Used in the Report Here we summarize the prompts/instructions used in Figure 1-3, which can be directly input into Z-Image-Turbo (with PE disabled) to reproduce our generation results. A.1. Figure 1 Column #1 Case #1: ä¸€å¼ ä¸­æ™¯æ‰‹æœºè‡ªæ‹ç…§ç‰‡æ‹æ‘„äº†ä¸€ä½ç•™ç€é•¿é»‘å‘çš„å¹´è½»ä¸œäºšå¥³å­åœ¨ç¯å…‰æ˜Žäº®çš„ç”µæ¢¯å†…å¯¹ç€é•œå­è‡ªæ‹ å¥¹ç©¿ç€ä¸€ä»¶å¸¦æœ‰ç™½è‰²èŠ±æœµå›¾æ¡ˆçš„é»‘è‰²éœ²è‚©çŸ­ä¸Šè¡£å’Œæ·±è‰²ç‰›ä»”è£¤å¥¹çš„å¤´å¾®å¾®å€¾æ–œå˜´å”‡å˜Ÿèµ·åšäº²å» çŠ¶éžå¸¸å¯çˆ±ä¿çš®å¥¹å³æ‰‹æ‹¿ç€ä¸€éƒ¨æ·±ç°è‰²æ™ºèƒ½æ‰‹æœºé®ä½äº†éƒ¨åˆ†è„¸åŽç½®æ‘„åƒå¤´é•œå¤´å¯¹ç€é•œå­ ç”µæ¢¯å¢™å£ç”±æŠ›å…‰ä¸é”ˆé’¢åˆ¶æˆåå°„ç€å¤´é¡¶çš„è§å…‰ç¯å’Œä¸»ä½“å·¦ä¾§å¢™ä¸Šæœ‰ä¸€ä¸ªå¸¦æœ‰è®¸å¤šåœ†å½¢æŒ‰é’®å’Œå° åž‹æ•°å­—æ˜¾ç¤ºå±çš„åž‚ç›´é¢æ¿åœ¨æŒ‰é’®ä¸‹æ–¹å¯ä»¥çœ‹åˆ°ä¸€ä¸ªé‡‘å±žæ‰¶æ‰‹åŽå¢™ä¸Šè´´ç€å¸¦æœ‰æ–‡å­—çš„é•¿æ–¹å½¢æ ‡ å¿—åœ°é¢é“ºç€å¸¦æœ‰ç™½è‰²çº¹ç†çš„æ·±è‰²å¤§ç†çŸ³ç“·ç –æ•´ä½“ç…§æ˜Žä¸ºäººé€ å…‰æ˜Žäº®å…·æœ‰ç”µæ¢¯å†…éƒ¨çš„ç‰¹å¾ (Translation: mid-range phone selfie captured young East Asian woman with long black hair taking selfie in front of mirror in brightly lit elevator. She was wearing black off shoulder short top with white floral pattern and dark jeans. Her head tilted slightly, and her lips curled up in kiss, very cute and playful. She held dark gray smartphone in her right hand, covering part of her face, with the rear camera facing the mirror. The elevator walls are made of polished stainless steel, reflecting the fluorescent lights and main body above the head. There is vertical panel on the left wall with many circular buttons and small digital display screen. Below the button, you can see metal armrest. There is rectangular sign with text on the back wall. The ground is covered with dark marble tiles with white texture. The overall lighting is artificial light, bright, and has the characteristics of an elevator interior.) Case #2: ä¸€å¼ å……æ»¡åŠ¨æ„Ÿçš„è¿åŠ¨æ‘„å½±ç…§ç‰‡æ•æ‰åˆ°ä¸€åæ©„æ¦„çƒè¿åŠ¨å‘˜åœ¨æ¯”èµ›ä¸­å¥”è·‘çš„çž¬é—´ä»–æ­£å‘å³å†²åˆºå·¦ æ‰‹æŠ±ç€æ©„æ¦„çƒä¸­å¿ƒäººç‰©æ˜¯ä¸€å30å¤šå²çš„é»‘äººç”·æ€§æ©„æ¦„çƒè¿åŠ¨å‘˜èº«ç©¿ç»¿ç™½æ¡çº¹çƒè¡£çƒè¡£æ­£é¢ æœ‰Turboå­—æ ·èƒ¸å£ç™½è‰²å°å­—æ¸…æ™°çš„å†™ç€MAIæ²¡æœ‰å…¶ä»–æ˜Žæ˜¾logoç™½è‰²è¢œå­å¸¦æœ‰ç»¿è‰²æ¡çº¹ è„šç©¿ç™½è‰²é’‰éž‹ä»–çš„å·¦ä¾§ä¸¤åèº«ç©¿é»‘è‰²å’Œé»„è‰²æ¡çº¹çƒè¡£çš„å¯¹æ‰‹æ­£æœç›¸åæ–¹å‘å¥”è·‘ä¸¤ä¸ªäººçƒè¡£æ­£ é¢åˆ†åˆ«å†™ç€æœ‰Baseå’ŒEditå­—æ ·æ­é…é»„è‰²è¢œå­å’Œç™½è‰²é’‰éž‹å‡ç•¥å¾®å¤±ç„¦èƒŒæ™¯æ˜¯æ¨¡ç³Šçš„ä½“è‚²åœº è§‚ä¼—å¸­è§‚ä¼—ç©¿ç€å„è‰²æœè£…è¿˜æœ‰è“è‰²å’Œç™½è‰²çš„ä½“è‚²åœºåº§æ¤…ä»¥åŠä¸€å—æ©™è‰²å¹¿å‘Šç‰Œä¸Šé¢æœ‰éƒ¨åˆ†å¯ è§çš„ç™½è‰²æ–‡å­—Tongyi Labå’Œä¸€ä¸ªé»„è‰²åœ†å½¢æ ‡å¿—å‰æ™¯æ˜¯ç»´æŠ¤è‰¯å¥½çš„ç»¿è‰²æ©„æ¦„çƒåœºåŠ¨ä½œæ‘„å½±ä½“ è‚²æ‘„å½±æµ…æ™¯æ·±ä¸­å¿ƒçƒå‘˜å¯¹ç„¦æ¸…æ™°èƒŒæ™¯è™šåŒ–è‡ªç„¶å…‰ç…§æ˜Žè‰²å½©é²œè‰³é«˜å¯¹æ¯”åº¦æž„å›¾åŠ¨æ„Ÿ å¯¹æ‰‹çƒå‘˜æœ‰è¿åŠ¨æ¨¡ç³Šå……æ»¡æ´»åŠ›ç«žæŠ€æ°›å›´æˆ·å¤–ä½“è‚²åœºåœºæ™¯ (Translation: dynamic sports photography shot capturing rugby player running during game, sprinting to the right with rugby ball tucked under his left arm. The central figure is Black male rugby player in his 30s, wearing green and white striped jersey with text \"Turbo\" on it, the white text \"MAI\" clearly visible on the chest with no other prominent logos, white socks with green stripes, and white cleats. To his left, two opponents in black and yellow striped jerseys are running in the opposite direction, their jerseys displaying \"Base\" and \"Edit\" on the front, paired with yellow socks and white cleats, both slightly out of focus. The background shows blurred stadium crowd wearing various colored clothing, blue and white stadium seats, and an orange advertising board with partially visible white text \"Tongyi Lab\" and yellow circular logo. The foreground features well-maintained green rugby field. Action photography, sports photography, shallow depth of field, central player in sharp focus, blurred background, natural lighting, vibrant colors, 56 high contrast, dynamic composition, motion blur on opponent players, energetic, competitive atmosphere, outdoor stadium scene.) Case #3: ä¸€å¼ å¹¿è§’å¹³è§†è§’åº¦çš„ç…§ç‰‡æ•æ‰åˆ°äº†ä¸€ä¸ªå……æ»¡æ´»åŠ›çš„è¡—æ™¯åœ°ç‚¹æ˜¯ä¸€æ¡é“ºç€ä¸å¹³æ•´é¹…åµçŸ³çš„ç‹­çª„å¤ è€å°å··å°å··ä¸¤æ—æ˜¯ä¸¤åˆ°ä¸‰å±‚çš„çº¢ç –å»ºç­‘å…·æœ‰ä¼ ç»Ÿå»ºç­‘ç‰¹è‰²çš„æ·±è‰²æœ¨é—¨çª—æ¡†å’Œæ‚¬æŒ‘çš„æ¥¼å±‚åœ¨ å·¦è¾¹ä¸¤åå¥³å­ç«™åœ¨ä¸€ç‰‡é˜³å…‰ä¸‹äº¤è°ˆä¸€åå¥³å­åœ¨æ ¼å­è¡¬è¡«å’Œæ·±è‰²è£¤å­å¤–å¥—ç€çº¢è‰²å›´è£™å¦ä¸€åå¥³ å­åˆ™æŠ«ç€æ·±è‰²æŠ«è‚©ä¸€åªé»‘è‰²å°ç‹—èººåœ¨å¥¹ä»¬è„šä¸‹æ¸©æš–çš„çŸ³å¤´ä¸Šåœ¨å‰æ™¯ä¸­ä¸€åªä½“åž‹è¾ƒå¤§æœ‰ç€è“¬ æ¾å·æ›²å°¾å·´çš„é‡‘è‰²ç‹—æ­£åœ¨å—…æŽ¢é¹…åµçŸ³è·¯é¢æ²¿ç€å°å··çš„ä¸­å¿ƒå†å¾€å‰ä¸€ä¸ªäººæ­£éª‘ç€ä¸€è¾†å°åž‹æ‘©æ‰˜è½¦ è¿œåŽ»å¦ä¸€åªé»‘è‰²å°ç‹—åˆ™ååœ¨è¡—é“çš„å³ä¾§æ˜Žäº®çš„é˜³å…‰å’Œæ·±é‚ƒçš„é˜´å½±åœ¨æ•´ä¸ªåœºæ™¯ä¸­å½¢æˆé²œæ˜Žå¯¹æ¯” çªæ˜¾äº†ç –å—å’ŒçŸ³å¤´çš„çº¹ç†å°å··å°½å¤´å¯è§çš„å¤©ç©ºæ˜¯è‹ç™½çš„é˜´ç™½è‰² (Translation: wide-angle, head up photo captures vibrant street scene in narrow ancient alley paved with uneven pebbles. On both sides of the alley are two to three story red brick buildings with traditional architectural features such as dark wooden doors, window frames, and cantilevered floors. On the left, two women are standing in sunny area talking. woman is wearing red apron over checkered shirt and dark pants, while another woman is draped in dark shawl. small black dog lay on the warm stone beneath their feet. In the foreground, large golden dog with fluffy and curly tail is sniffing the cobblestone road surface. Continuing along the center of the alley, person is riding small motorcycle away, while another black puppy is sitting on the right side of the street. The bright sunlight and deep shadows create sharp contrast throughout the scene, highlighting the texture of the bricks and stones. The sky visible at the end of the alley is pale and gloomy white.) Case #4: ä¸€å¼ å®é™çš„å…¨æ™¯æ¨ªå‘ç…§ç‰‡æ•æ‰äº†ä¸€ä¸ªå°å­©ä¾§èº«ç«™åœ¨éƒéƒè‘±è‘±çš„ç»¿è‰²è‰å²¸ä¸Šæ—è¾¹æ˜¯å¹³é™çš„æ°´ ä½“åœºæ™¯è®¾ç½®åœ¨é»„é‡‘æ—¶åˆ»å¾ˆå¯èƒ½æ˜¯æ—¥è½æ—¶åˆ†è‰²è°ƒæŸ”å’Œè€Œæ¸©æš–å­©å­ä½äºŽç”»é¢å·¦ä¾§æˆ´ç€ä¸€é¡¶æµ… è‰²çš„ç¼–ç»‡è‰å¸½åœ¨æµ…è“ç™½æ ¼çº¹é•¿è¢–è¡¬è¡«å¤–ç©¿ç€ä¸€ä»¶æ©„æ¦„ç»¿è‰²çš„çŸ­è¢–èƒŒå¿ƒä¸‹èº«æ˜¯å®½æ¾çš„æ·±è“è‰²ç‰›ä»” è£¤è£¤è„šå·èµ·éœ²å‡ºæ£•è‰²çš„éž‹å­å­©å­çš„å³æ‰‹æ‹¿ç€ä¸€æœµé»„è‰²å°èŠ±çš„èŒŽå·¦æ‰‹æç€ä¸€ä¸ªé“¶è‰²çš„å°å·é•€ é”Œé‡‘å±žå–·å£¶ä»–/å¥¹æ­£æœå³è¾¹æœ›åŽ»çœ‹å‘æ°´é¢å‰æ™¯æ˜¯ç‚¹ç¼€ç€é»„è‰²å°é‡ŽèŠ±çš„è‰å¡ä¸­æ™¯æ˜¯æ²³æµæˆ–æ±  å¡˜çš„é™æ°´å€’æ˜ ç€å¤©ç©ºæ¸©æš–çš„ç²‰æ©™è‰²è°ƒå¯¹å²¸æœ‰ç»¿è‰²æ¤è¢«å’Œä¸€å †ç°è‰²å²©çŸ³èƒŒæ™¯æ˜¯æŸ”å’Œæ¨¡ç³Šçš„å±• çŽ°äº†å¹¿é˜”çš„ç»¿è‰²ç”°é‡Žè¿œå¤„çš„æ ‘æž—çº¿ä»¥åŠä¸€äº›æ¨¡ç³Šçš„å»ºç­‘è½®å»“è¿™ä¸€åˆ‡éƒ½åœ¨ä¸€ç‰‡å¹¿é˜”çš„å¤©ç©ºä¸‹å¤© ç©ºå¸ƒæ»¡äº†æŸ”å’Œçš„å¸¦æœ‰ç²‰è‰²å’Œæ©™è‰²æ¸å˜çš„äº‘å½©æ‘„å½±é£Žæ ¼çš„ç‰¹ç‚¹æ˜¯æµ…æ™¯æ·±åœ¨èƒŒæ™¯ä¸­åˆ›é€ äº†æ˜¾è‘—çš„ æ•£æ™¯æ•ˆæžœä½¿ä¸»ä½“çªå‡ºå…‰çº¿è‡ªç„¶è€Œæ¼«å°„è¥é€ å‡ºå¹³å’Œç”°å›­è¯—èˆ¬å’Œæ€€æ—§çš„æ°›å›´å­©å­çš„è„šè¢«ç”»é¢ çš„åº•éƒ¨è¾¹ç¼˜è½»å¾®æˆªæ–­ (Translation: peaceful, panoramic horizontal photo captures child standing sideways on lush green grassy bank, with calm body of water beside it. The scene is set in prime time, most likely at sunset, with soft and warm tones. The child is located on the left side of the screen, wearing light colored woven straw hat, an olive green short sleeved vest over light blue and white checkered long sleeved shirt, and loose dark blue jeans with rolled up hemlines revealing brown shoes. The child holds stem of small yellow flower in their right hand and silver small galvanized metal spray can in their left hand. He/she is looking to the right, towards the water surface. The prospect is grassy slope adorned with small yellow wildflowers. The central view is the still water of river or pond, reflecting the warm pink orange tones of the sky. There is green vegetation and pile of gray rocks on the opposite bank. The background is soft and blurry, showing vast green fields, distant forest lines, and some blurry building contours, all under vast sky filled with soft, pink and orange gradient clouds. The characteristic of photography style is shallow depth of field, which creates significant bokeh effect in the background, making the subject stand out. The natural and diffuse light creates peaceful, pastoral, and nostalgic atmosphere. The childs feet were slightly cut off by the bottom edge of the screen.) Case #5: ä¸€å¼ å¹¿è§’é£Žæ™¯ç…§ç‰‡æ‹æ‘„äºŽé˜´å¤©çš„å®‰å¾½å®æ‘å¤æ‘è½ç”»é¢è¢«å¹³é™æ°´ä½“çš„å²¸çº¿æ°´å¹³åˆ†å‰²å½¢æˆäº†æ‘åº„ ä¸Žå¤©ç©ºè¿‘ä¹Žå®Œç¾Žçš„é•œé¢å€’å½±åœ¨ä¸­æ™¯éƒ¨åˆ†ä¸€ç°‡å¯†é›†çš„ä¼ ç»Ÿå¾½æ´¾å»ºç­‘æ²¿æ°´è¾¹æŽ’åˆ—å…·æœ‰ç‹¬ç‰¹çš„ç™½å¢™ å’Œæ·±ç°è‰²ç“¦é¡¶å‡ æ ‹å»ºç­‘çš„å±‹æªä¸‹æ‚¬æŒ‚ç€çº¢è‰²çš„çº¸ç¯ç¬¼åœ¨æŸ”å’Œçš„èƒŒæ™¯ä¸­å¢žæ·»äº†é²œè‰³çš„è‰²å½©ç‚¹ç¼€ æ°´è¾¹çš„çŸ³æ¿è·¯ä¸Šå’Œæˆ¿å±‹ä¹‹é—´æ•£å¸ƒç€è®¸å¤šå…‰ç§ƒç§ƒçš„è½å¶æ ‘ä¸€äº›èº«å½±å¾®å°çš„äººæ²¿ç€è¿™æ¡å°è·¯è¡Œèµ°æˆ–å ç€åœ¨èƒŒæ™¯ä¸­ä¸€ç‰‡æœ¦èƒ§çš„è“ç»¿è‰²å±±è„‰åœ¨æ·¡ç°è‰²çš„å¤©ç©ºä¸‹è¿žç»µèµ·ä¼å³ä¾§å±±å¡ä¸Šå¯ä»¥çœ‹åˆ°ä¸€ä¸ªå°åž‹ è¾“ç”µå¡”åœ¨ç”»é¢ä¸­å¿ƒåå³çš„ä¸€æ ‹å»ºç­‘ä¸Šé—¨æ¥£ä¸Šæ–¹æŒ‚ç€ä¸€å—æ¨ªå‘çš„æœ¨åŒ¾ä¸Šé¢æœ‰é»‘è‰²çš„æ±‰å­—ä¸–å¾· å ‚è¯¥æ‘„å½±ä½œå“çš„é£Žæ ¼ç‰¹ç‚¹æ˜¯æž„å›¾å¯¹ç§°å…‰çº¿æŸ”å’Œæ¼«å°„æ™¯æ·±è¾ƒå¤§æ•´ä¸ªåœºæ™¯éƒ½æ¸…æ™°é”åˆ©è‰²è°ƒ æ¸…å†·è€Œå®é™ä»¥ç™½è‰²ç°è‰²å’Œè“è‰²ä¸ºä¸»çº¢è‰²ä½œä¸ºå¼ºçƒˆçš„ç‚¹ç¼€è‰²æ•´ä½“æ°›å›´å¹³å’Œå®‰è¯¦ä¸”å…·æœ‰æ°¸æ’ 57 æ„Ÿ (Translation: wide-angle landscape photo taken on cloudy day in the ancient village of Hongcun, Anhui. The screen is horizontally divided by the calm shoreline of the water, forming nearly perfect mirror reflection of the village and the sky. In the central area, dense cluster of traditional Huizhou style buildings are arranged along the waters edge, featuring unique white walls and dark gray tiled roofs. Red paper lanterns hang under the eaves of several buildings, adding vibrant color accents to the soft background. There are many bare deciduous trees scattered between the stone roads and houses by the waters edge. Some small figures walked or sat along this path. In the background, hazy blue-green mountain range undulates continuously under light gray sky. small transmission tower can be seen on the right slope. On building to the right of the center of the screen, there is horizontal wooden plaque hanging above the lintel, with black Chinese characters \"ä¸–å¾·å ‚\" on it. The stylistic features of this photography work are symmetrical composition, soft and diffuse lighting, large depth of field, clear and sharp entire scene, cool and peaceful tones, mainly white, gray and blue, with red as strong accent color. The overall atmosphere is peaceful, serene, and has sense of eternity.) Case #6: ä¸€å¼ å……æ»¡æ´»åŠ›çš„å¹¿è§’å¤œæ™¯ç…§ç‰‡æ•æ‰äº†ä¸­å›½å¹¿å·žçŒŽå¾·å¤§æ¡¥ä¸Šç©ºå£®è§‚çš„çƒŸèŠ±è¡¨æ¼”åœºæ™¯è®¾ç½®åœ¨æ¼†é»‘çš„ å¤œç©ºä¸‹è¢«å¤šæœµå·¨å¤§çš„çƒŸèŠ±çˆ†ç‚¸çž¬é—´ç…§äº®çƒŸèŠ±ä¸»è¦ä¸ºç™½è‰²å’Œçº¢è‰²/ç²‰è‰²åœ¨ç”»é¢çš„ä¸ŠåŠéƒ¨åˆ†å½¢æˆ äº†ç¿çƒ‚çš„ç‰¡ä¸¹èŠ±çŠ¶å›¾æ¡ˆå‘¨å›´çŽ¯ç»•ç€æµ“æµ“çš„ç¡çƒŸçŒŽå¾·å¤§æ¡¥ä¸€åº§çŽ°ä»£åŒ–çš„æ–œæ‹‰æ¡¥åœ¨ä¸­æ™¯å¤„æ¨ªè·¨ ç æ±Ÿå…¶ç‹¬ç‰¹çš„æ‹±å½¢ä¸­å¤®æ¡¥å¡”è¢«æ¸©æš–çš„é»„è‰²ç¯å…‰ç…§äº®åœ¨è¿™ä¸ªä¸­å¤®æ¡¥å¡”çš„æ­£é¢å¯ä»¥çœ‹åˆ°ä¸€ä¸ªè¢«éƒ¨ åˆ†é®æŒ¡çš„çº¢è‰²å°æ ‡å¿—æ¡¥é¢ä¹Ÿè¢«è·¯ç¯ç…§äº®åœ¨å‰æ™¯ä¸­é»‘æš—çš„æ±Ÿæ°´æ˜ ç…§å‡ºçƒŸèŠ±å’Œæ¡¥ç¯çš„ç¼¤çº·å€’å½± å·¦ä¸‹è§’å¯ä»¥çœ‹åˆ°ä¸€è‰˜å°èˆ¹çš„é»‘è‰²å‰ªå½±è¿œå¤„è¿˜æ•£å¸ƒç€å…¶ä»–æ›´å°çš„èˆ¹åªèƒŒæ™¯æ˜¯é—ªé—ªå‘å…‰çš„çŽ°ä»£åŒ–åŸŽ å¸‚å¤©é™…çº¿æ‘©å¤©å¤§æ¥¼å’Œå…¶ä»–å»ºç­‘ä¸Šçš„æ— æ•°ç¯å…‰ç‚¹ç¼€å…¶é—´è¯¥æ‘„å½±é£Žæ ¼ä»¥é•¿æ›å…‰ä¸ºç‰¹ç‚¹è¿™ä»ŽçƒŸèŠ±çš„ è½¨è¿¹ä¸­å¯ä»¥æ˜Žæ˜¾çœ‹å‡ºè¥é€ å‡ºä¸€ç§åŠ¨æ„Ÿå’Œå–œåº†çš„æ°›å›´å›¾åƒå¯¹æ¯”åº¦é«˜å¯¹ç„¦æ¸…æ™°åœ¨é»‘æš—çš„çŽ¯å¢ƒä¸­ å‘ˆçŽ°å‡ºé²œè‰³çš„è‰²å½© (Translation: vibrant wide-angle night view photo captures the spectacular fireworks display over the Liede Bridge in Guangzhou, China. The scene is set in the pitch black night sky, instantly illuminated by multiple huge fireworks explosions. The fireworks are mainly white and red/pink, forming brilliant peony shaped pattern in the upper part of the picture, surrounded by thick gunpowder smoke. Liede Bridge, modern cable-stayed bridge, crosses the the Pearl River in the middle view. Its unique arched central bridge tower is illuminated by warm yellow lights. On the front of this central bridge tower, partially obscured red small sign can be seen. The bridge deck is also illuminated by streetlights. In the foreground, the dark river reflects the colorful reflections of fireworks and bridge lights. In the lower left corner, black silhouette of small boat can be seen, with other smaller boats scattered in the distance. The background is sparkling modern city skyline, adorned with countless lights from skyscrapers and other buildings. This photography style is characterized by long exposures, which can be clearly seen from the trajectory of fireworks, creating dynamic and festive atmosphere. The image has high contrast, clear focus, and presents bright colors in dark environments.) Column #2 Case #1: stylish young woman sits casually on an unmade bed bathed in soft daylight, wearing pastel yellow oversized T-shirt with subtle white text and cozy light gray sweatpants. Her skin glows fresh beneath glossy deep lavender hydrogel under-eye patches, while her hair is tied back loosely with scrunchie, complemented by delicate gold hoop earrings. Nearby, tube of hand cream and an open laptop rest casually atop soft, slightly rumpled sheets. The natural window light gently illuminates her radiant skin and the subtle sheen of the hydrogel patches, enhancing the cozy textures of her loungewear and bedding. Shot from top-down selfie angle, the framing captures her face, shoulders, and upper torso with realistic iPhone grain, conveying an authentic, relaxed self-care morning moment in softly lit bedroom scene skincare selfie, shot on iPhone. Case #2: ä¸€å¼ é€¼çœŸçš„å¹´è½»ä¸œäºšå¥³æ€§è‚–åƒä½äºŽç”»é¢ä¸­å¿ƒåå·¦çš„ä½ç½®å¸¦ç€æµ…æµ…çš„å¾®ç¬‘ç›´è§†è§‚è€…å¥¹èº«ç€ä»¥æµ“ éƒçš„çº¢è‰²å’Œé‡‘è‰²ä¸ºä¸»çš„ä¼ ç»Ÿä¸­å¼æœè£…å¥¹çš„å¤´å‘è¢«ç²¾å¿ƒç›˜èµ·é¥°æœ‰ç²¾è‡´çš„çº¢è‰²å’Œé‡‘è‰²èŠ±å‰å’Œå¶å½¢å‘ é¥°å¥¹çš„çœ‰å¿ƒä¹‹é—´é¢å¤´ä¸Šç»˜æœ‰ä¸€ä¸ªå°å·§åŽä¸½çš„çº¢è‰²èŠ±å‰å›¾æ¡ˆå¥¹å·¦æ‰‹æŒä¸€æŠŠä»¿å¤æ‰‡å­æ‰‡é¢ä¸Šç»˜ æœ‰ä¸€ä½èº«ç€ä¼ ç»Ÿæœé¥°çš„å¥³æ€§ä¸€æ£µæ ‘å’Œä¸€åªé¸Ÿçš„åœºæ™¯å¥¹çš„å³æ‰‹å‘å‰ä¼¸å‡ºæ‰‹æŽŒå‘ä¸Šæ‰˜ç€ä¸€ä¸ªæ‚¬ æµ®çš„å‘å…‰çš„éœ“è™¹é»„è‰²é—ªç”µäºšå…‹åŠ›ç¯ç‰Œè¿™æ˜¯ç”»é¢ä¸­æœ€äº®çš„å…ƒç´ èƒŒæ™¯æ˜¯æ¨¡ç³Šçš„å¤œæ™¯å¸¦æœ‰æš–è‰²è°ƒçš„ äººå·¥ç¯å…‰ä¸€åœºæˆ·å¤–æ–‡åŒ–æ´»åŠ¨æˆ–åº†å…¸åœ¨è¿œå¤„çš„èƒŒæ™¯ä¸­å¥¹å¤´éƒ¨çš„å·¦ä¾§ç•¥åæ˜¯ä¸€åº§é«˜å¤§å¤šå±‚ è¢«æš–å…‰ç…§äº®çš„è¥¿å®‰å¤§é›å¡”ä¸­æ™¯å¯è§å…¶ä»–æ¨¡ç³Šçš„å»ºç­‘å’Œç¯å…‰æš—ç¤ºç€ä¸€ä¸ªç¹åŽçš„åŸŽå¸‚æˆ–æ–‡åŒ–èƒŒæ™¯ å…‰çº¿æ˜¯ä½Žè°ƒçš„é—ªç”µç¬¦å·ä¸ºå¥¹çš„è„¸éƒ¨å’Œæ‰‹éƒ¨æä¾›äº†æ˜¾è‘—çš„ç…§æ˜Žæ•´ä½“æ°›å›´ç¥žç§˜è€Œè¿·äººäººç‰©çš„å¤´ éƒ¨æ‰‹éƒ¨å’Œä¸ŠåŠèº«å®Œå…¨å¯è§ä¸‹åŠèº«è¢«ç”»é¢åº•éƒ¨è¾¹ç¼˜æˆªæ–­å›¾åƒå…·æœ‰ä¸­ç­‰æ™¯æ·±ä¸»ä½“æ¸…æ™°èšç„¦èƒŒ æ™¯æŸ”å’Œæ¨¡ç³Šè‰²å½©æ–¹æ¡ˆæ¸©æš–ä»¥çº¢è‰²é‡‘è‰²å’Œé—ªç”µçš„äº®é»„è‰²ä¸ºä¸» (Translation: realistic portrait of young East Asian woman, located to the left of the center of the image, looking directly at the viewer with faint smile. She was dressed in traditional Chinese clothing dominated by rich red and gold colors. Her hair was carefully styled, adorned with delicate red and gold flowers and leaf shaped hair accessories. There is small and gorgeous red floral pattern painted on her forehead between her eyebrows. She held an antique style fan in her left hand, with scene of woman dressed in traditional clothing, tree, and bird painted on the fan surface. Her right hand extended forward, palm up, holding suspended glowing neon yellow lightning acrylic light tag, which was the brightest element in the picture. The background is blurry night scene with warm toned artificial lighting, representing an outdoor cultural event or celebration. In the distant background, to the left of her head is tall, multi-layered, warm lit Xian Big Wild Goose Pagoda. Other blurry buildings and lights can be seen in the middle of the scene, implying bustling city or cultural background. The light is low-key, and the lightning symbol provides significant illumination for her face and hands. The overall atmosphere is mysterious and charming. The head, hands, and upper body of the character are fully visible, while the lower body is cut off by the bottom edge of the screen. The image has moderate depth of field, the subject is clearly focused, and the background is soft and blurry. The color scheme is warm, with red, gold, and bright yellow of lightning as the main colors.) Case #3: full-body, eye-level photograph of young, beautiful East Asian woman posing cheerfully inside brightly lit LEGO store or brand exhibition space. The woman, positioned slightly right of center, has long dark hair and is smiling at the camera. She wears vibrant yellow ribbed beanie, white diamond-quilted puffer jacket over white t-shirt, and medium-wash blue jeans with cuffs rolled up at the ankles. She is wearing white lace-up sneakers and white socks, with small red heart visible on her left sock. In her left hand, she holds black structured handbag. Her pose is playful, with her left leg kicked up behind her. To her left is large, multi-tiered display stand in bright yellow, which features the official LEGO logo white text in red square with black and yellow outline in the upper left corner. On this stand are two large-scale LEGO Minifigure statues: policeman in blue uniform and hat stands in the foreground, and behind him is Santa Claus figure in red. The background shows more yellow retail shelving stocked with various LEGO sets and products. The floor is made of large, light grey tiles, and white dome security camera is visible on the ceiling. The image is sharp, well-lit snapshot with vibrant color palette, dominated by yellow, red, and blue, creating joyful and commercial atmosphere. Case #4: candid mid-2010s-style snapshot featuring pale young woman with icy platinum hair styled casually loose, seated on metal bench inside monochrome concept store. She wears huge black hoodie, sheer tights, and maroon platform creepers, complemented by beanie embroidered with Z-Image Real & Fast The subjects relaxed expression gazes off to the side, conveying subtle, ambiguous emotion. The lighting is cold and matte with soft shadows stretching along wooden floor, intentionally exhibiting muted color saturation, softened contrast, and distinctly cool-toned bluish-gray shadows. Visible textures include realistic skin details, detailed fabric grain of the hoodie and tights, individual icy hair strands, and clear accessory textures. The framing is slightly off-center and casually tilted, capturing spontaneous intimacy and informal snapshot aesthetics characteristic of mid-2010s casual youth photography. Column # Case #1: ä¸€ä½ç”·å£«å’Œä»–çš„è´µå®¾çŠ¬ç©¿ç€é…å¥—çš„æœè£…å‚åŠ ç‹—ç‹—ç§€å®¤å†…ç¯å…‰èƒŒæ™¯ä¸­æœ‰è§‚ä¼— (Translation: man and his poodle participated in dog show wearing matching costumes, with 59 indoor lighting and an audience in the background.) Case #2: ä¸€å¼ ç‰¹å†™é€¼çœŸçš„ä¸œäºšå©´å„¿è‚–åƒå©´å„¿ç©¿ç€ä¸€ä»¶å°æœ‰å¿ƒå½¢å›¾æ¡ˆçš„å¥¶æ²¹è‰²è“¬æ¾å†¬å­£è¿žä½“è¡£ç›´è§†è§‚ è€…å©´å„¿æ‹¥æœ‰æ·±è‰²å¤´å‘å’Œçº¢æ‰‘æ‰‘çš„è„¸é¢Šå©´å„¿æ‰‹è¾¹éƒ¨åˆ†å¯è§ä¸€ä¸ªè‰²å½©é²œè‰³çš„çŽ©å…·èƒŒæ™¯æ¨¡ç³Šå¤„æœ‰ä¸€ ä½ç©¿ç€æ ¼å­è¡¬è¡«çš„äººå®¤å†…å…‰çº¿å…·æœ‰æŸ”å’Œçš„é˜´å½±å’Œé«˜å…‰è¥é€ å‡ºæ¸©æš–çš„è‰²è°ƒå©´å„¿è„¸éƒ¨æ¸…æ™°èšç„¦ èƒŒæ™¯æŸ”å’Œæ¨¡ç³Šä½Žé¥±å’Œåº¦é¢—ç²’æ„Ÿè€èƒ¶ç‰‡é£Žæ ¼ (Translation: close-up, realistic portrait of an East Asian baby wearing creamy fluffy winter jumpsuit with heart-shaped pattern, looking straight at the viewer. Babies have dark hair and rosy cheeks. brightly colored toy can be seen near the babys side, with person wearing checkered shirt in blurry background. The indoor lighting features soft shadows and highlights, creating warm tone. The babys face is clearly focused, and the background is soft and blurry. Low saturation, graininess, and vintage film style.) Case #3: åŒ—äº¬å›½å®¶ä½“è‚²åœºé¸Ÿå·¢çš„ç…§ç‰‡è“å¤©èƒŒæ™¯ä¸‹ä½“è‚²åœºçš„å¤–è§‚ç”±å¤æ‚çš„äº¤ç»‡é’¢ç»“æž„å½¢æˆç½‘çŠ¶å›¾æ¡ˆä¸» å¯¼å‰æ™¯ä¸­ä¸€ä¸ªäººç©¿ç€ä¼‘é—²è£…ç•¥å¾®åä¸­å¿ƒä½ç½®è¡Œèµ°èƒŒæ™¯é€šè¿‡é’¢ç»“æž„å¯ä»¥çœ‹åˆ°ä½“è‚²åœºå†…éƒ¨çš„çº¢ è‰²åº§ä½åŒºA30ç”¨çº¢è‰²æ ‡è®°åœ¨é’¢ç»“æž„çš„å·¦ä¸‹è§’å›¾åƒä»Žä½Žè§’åº¦æ‹æ‘„çªæ˜¾å»ºç­‘çš„å®ä¼Ÿå’Œè§„æ¨¡ç…§ ç‰‡é«˜å¯¹æ¯”åº¦æˆå‰§æ€§å…‰çº¿è“å¤©ä½Žè§’åº¦è§†è§’å»ºç­‘æ‘„å½±èšç„¦æ¸…æ™°çŽ°ä»£è®¾è®¡ç²¾ç»†é’¢ç»“æž„ é²œè‰³çº¢è‰²ç‚¹ç¼€è§†è§‰å†²å‡»åŠ›å¼ºæž„å›¾å¹³è¡¡ (Translation: photo of the Beijing National Stadium (Birds Nest), with blue sky background, the appearance of the stadium is dominated by complex interwoven steel structure forming mesh pattern. In the foreground, person is wearing casual clothing and walking slightly off center. The background shows the red seating area inside the stadium through the steel structure. \"A30\" is marked in red on the bottom left corner of the steel structure. The image is taken from low angle to highlight the grandeur and scale of the building. Photos, high contrast, dramatic lighting, blue sky, low angle perspective, architectural photography, clear focus, modern design, fine steel structure, bright red accents, strong visual impact, balanced composition.) A.2. Figure 2 Row #1 Case #1: æ‚å¿—å°é¢è®¾è®¡ æ–‡æ¡ˆå¤§æ ‡é¢˜é€ ç›¸Z-Image å°æ ‡é¢˜Winter Release. Spring for Generative Art. ç‰ˆæœ¬å· VOL 1.0 ä¸­é—´åº•éƒ¨æžå°å­—é€šä¹‰å¤šæ¨¡æ€äº¤äº’å‡ºç‰ˆç¤¾ æ‹‰å¼€ä¸€ç‰‡ç™½é›ªèŒ« èŒ«ä¸‹çš„æ‹‰é“¾æ‹‰é“¾ä¸‹æ¼å‡ºç»¿è‰é²œèŠ±çš„æ˜¥å¤©ç§»è½´å¾®è·æ‹‰é“¾æ˜¯ä¸€ä¸ªå†’ç€ç™½çƒŸè¿œåŽ»çš„ç«è½¦å¤´ç²¾ç¾Žæž„ å›¾å¤¸å¼ çš„ä¿¯è§†è§†è§’è§†è§‰å†²å‡»åŠ›é«˜å¯¹æ¯”åº¦é«˜é¥±å’Œåº¦ (Translation: Magazine cover design. Copy: Headline \"é€ ç›¸Z-Image\". Subtitle: \"Winter Release. Spring for Generative Art. Version number: \"VOL 1.0\". The extremely small font at the bottom of the middle reads é€šä¹‰å¤šæ¨¡æ€äº¤äº’å‡ºç‰ˆç¤¾. Pulling open zipper under vast expanse of white snow, the spring of green grass and flowers peeks out from under the zipper. Moving the axis macro, the zipper is locomotive emitting white smoke far away, with exquisite composition, exaggerated top-down perspective, visual impact, high contrast, and high saturation.) Case #2: ä¸€å¹…åž‚ç›´æž„å›¾é£Žæ ¼åŒ–çš„æ•°å­—æ’ç”»è®¾è®¡ä¸ºä¸€å¼ åŠ±å¿—æµ·æŠ¥åœºæ™¯æç»˜äº†å¤œé—´çš„æ²™æ¼ æ™¯è§‚å¤´é¡¶æ˜¯å¹¿ é˜”æ— åž ç¹æ˜Ÿå¯†å¸ƒçš„å¤©ç©ºå…¶ä¸­é“¶æ²³æ¸…æ™°å¯è§å‰æ™¯å’Œä¸­æ™¯ä»¥æ·±è“è‰²è¿‘ä¹Žé»‘è‰²çš„å‰ªå½±ä¸ºç‰¹è‰²å·¦ ä¾§ä¸€æ£µå·¨å¤§è€Œç»†èŠ‚ä¸°å¯Œçš„çº¦ä¹¦äºšæ ‘å‰ªå½±å æ®äº†ç”»é¢ä¸»å¯¼æ›´è¿œå¤„å¯ä»¥çœ‹åˆ°ä¸¤æ£µè¾ƒå°çš„çº¦ä¹¦äºšæ ‘ å³ä¾§ä¸¤ä¸ªäººçš„å‰ªå½±ç«™åœ¨ä¸€ä¸ªå°å±±ä¸˜ä¸Šä»°æœ›ç€å¤©ç©ºå¤©ç©ºä»Žåº•éƒ¨çš„æ·±æµ·å†›è“è¿‡æ¸¡åˆ°é¡¶éƒ¨çš„æµ…è“ è‰²å¸ƒæ»¡æ˜Ÿè¾°æ˜Žäº®çš„é“¶æ²³å¸¦ä»¥æŸ”å’Œçš„ç™½è‰²ç´«è‰²å’Œè“è‰²è°ƒä»Žå³ä¸Šè§’åˆ’è¿‡å›¾åƒä¸Šè¦†ç›–æœ‰äº”å¤„ç‹¬ ç«‹çš„æ¸²æŸ“ä¸­æ–‡å­—æ ·é¡¶éƒ¨æ˜¯ç™½è‰²å¤§å·å­—ä½“å†…å®¹ä¸ºäºŽæ— åž é»‘æš—ä¸­å¯»è§ä½ çš„å¾®å…‰åœ¨ä¸­é—´é è¿‘äºº ç‰©çš„ä½ç½®æœ‰è¾ƒå°çš„é»‘è‰²å­—ä½“å†™ç€å¿ƒä¹‹æ‰€å‘å®‡å®™å›žå“åœ¨æœ€åº•éƒ¨æ˜¯ç™½è‰²å¤§å·è‰ºæœ¯å­—çš„ä¸»æ ‡ é¢˜ä»°æœ›é€æ¢¦å…¶ä¸‹æ–¹æ˜¯ç¨å°çš„ç™½è‰²å­—ä½“å¿ƒçš„æ—…ç¨‹ç”±æ­¤å¼€å§‹åœ¨å±±ä¸˜ä¸Šé è¿‘äººç‰©çš„åœ°æ–¹æœ‰ä¸€ ä¸ªéžå¸¸å°å‡ ä¹Žéšè—çš„é»‘è‰²ç­¾åè§‚æ˜Ÿè€…æ•´ä½“é£Žæ ¼å›¾å½¢åŒ–ä¸”ç®€çº¦å°†æ‰å¹³çš„å‰ªå½±ä¸Žç»†èŠ‚æ›´ä¸°å¯Œ å¯Œæœ‰ç»˜ç”»æ„Ÿçš„å¤©ç©ºç›¸ç»“åˆè¥é€ å‡ºä¸€ç§æ·±æ²‰å¼•äººæ·±æ€ä¸”å……æ»¡å¸Œæœ›çš„æ°›å›´ (Translation: vertically composed and stylized digital illustration designed as an inspirational poster. The scene depicts desert landscape at night, with vast and starry sky overhead, among which the Milky Way is clearly visible. The foreground and middle ground are characterized by 60 deep blue and almost black silhouettes. On the left, large and detailed silhouette of Joshua tree dominates the scene. Two smaller Joshua trees can be seen further away. On the right, silhouettes of two people stand on small hill, looking up at the sky. The sky transitions from deep sea navy blue at the bottom to light blue at the top, filled with stars, and the bright Milky Way streaks across in soft white, purple, and blue tones from the top right corner. The image is covered with five independent rendered Chinese characters. At the top is large white font that reads äºŽæ— åž é»‘æš— ä¸­å¯»è§ä½ çš„å¾®å…‰. In the middle, near the character, there is small black font that reads å¿ƒä¹‹ æ‰€å‘å®‡å®™å›žå“. At the bottom, there is the main title \"ä»°æœ›é€æ¢¦\" in large white artistic font, and below it is slightly smaller white font \"å¿ƒçš„æ—…ç¨‹ç”±æ­¤å¼€å§‹\". Near the character on the hill, there is very small, almost hidden black signature called è§‚æ˜Ÿè€…. The overall style is graphical and minimalist, combining flat silhouettes with more detailed and picturesque sky, creating deep, thought-provoking, and hopeful atmosphere.) Case #3: ä¸€å¼ å……æ»¡æ´»åŠ›çš„è§†è§‰ä½œå“é›†å¹³é¢è®¾è®¡æµ·æŠ¥æ•´å¼ å›¾ç‰‡ä»¥éžå¸¸å°çš„é€æ˜Žæ£‹ç›˜æ ¼ä¸ºèƒŒæ™¯å±•ç¤ºäº†ä¸€ ä¸ª3Dæ¸²æŸ“çš„å¡é€šäººç‰©ç”»é¢å·¦ä¾§æ˜¯ä¸€ä½å¹´è½»å¥³æ€§çš„åŠèº«åƒå¥¹çš®è‚¤ç™½çš™ç•™ç€æ·±æ£•è‰²é•¿å·å‘æˆ´ ç€ç²‰è‰²è¾¹æ¡†çš„çœ¼é•œçœ¼é•œåŽæ˜¯æ£•è‰²çš„å¤§çœ¼ç›å¥¹ç¬‘å®¹ç¿çƒ‚éœ²å‡ºç‰™é½¿æˆ´ç€å°å·§çš„é“¶è‰²è€³é’‰å¥¹çš„ ç€è£…åŒ…æ‹¬ä¸€ä»¶æµ…ç°è‰²è¥¿è£…å¤–å¥—ä¸€ä»¶ç™½è‰²ç¿»é¢†è¡¬è¡«å’Œä¸€æ¡çº¢è‰²é¢†å¸¦å¥¹æ‰‹ä¸­æ§ç€ä¸€æŸç”±å››æœµé²œè‰³ çš„é»„è‰²å‘æ—¥è‘µç»„æˆçš„èŠ±æŸèŠ±èŒŽä¸ºç»¿è‰²è¯¥è§’è‰²è¢«ä¸€åœˆç²—ç™½çš„è½®å»“çº¿åŒ…å›´ä½¿å…¶ä»ŽèƒŒæ™¯ä¸­å‡¸æ˜¾å‡º æ¥æµ·æŠ¥çš„å³ä¾§ä¸»è¦æ˜¯å¤§åž‹è‰ºæœ¯å­—ä¸»æ ‡é¢˜è§†è§‰ä½œå“é›†é‡‡ç”¨ç²—å¤§çš„é»„è‰²ç¬”åˆ·é£Žæ ¼å­—ä½“å…¶ä¸Šå  åŠ ç€ä¸€è¡Œçº¤ç»†çš„çº¢è‰²è‰ä¹¦è‹±æ–‡Personalizationä¸‹æ–¹æ˜¯åœ†æ¶¦æ°”æ³¡çŠ¶çš„é»„è‰²å°ä¸€å·å­—ä½“VISUAL PORTFOLIO å…¶ä¸‹å†™å‡ºäº†ä¸‰ä¸ªäº®ç‚¹ ä¸­è‹±æ¸²æŸ“å­—å­—å¦‚åˆ» Bilingual Rendering ä¸æ­¢çœŸ å®žæ›´æ‡‚ç¾Žå­¦ Realism & Aesthetic è¯»æ‡‚å¤æ‚ç”Ÿæˆç²¾å¦™ Complexity & Elegance è¿™é‡Œ ä¸­æ–‡æ˜¯ç™½è‰²æ‰‹å†™ä½“å¤§å­—è‹±æ–‡æ˜¯åŠé€æ˜Žçš„å°åˆ·ä½“å°å­—æµ·æŠ¥åŒ…å«å¤šä¸ªæ–‡æœ¬å—å’Œæ ‡å¿—ä¸­ä¸Šéƒ¨å…ˆæ˜¯ é»„è‰²çš„æ–‡å­—Z-Image xä¸­é—´æ˜¯ä¸€ä¸ªæˆ´ç€è€³æœºçš„å¡é€šå¤´åƒçš„é»„è‰²çº¿æ¡ç”»æ ‡å¿—åŽé¢è·Ÿç€æ–‡å­—x Designåœ¨å³ä¸‹è§’æœ‰ä¸€ä¸ªå¯çˆ±çš„æ‹ŸäººåŒ–æ‰©éŸ³å™¨å®ƒæœ‰ä¸¤åªå¤§å¤§çš„çœ¼ç›é¢œè‰²ä¸ºæµ…ç»¿è‰²å’Œå¥¶æ²¹è‰² åº•éƒ¨æœ‰ä¸€æœµå°é›èŠæ•´ä½“é£Žæ ¼æ˜¯3Dè§’è‰²æ¸²æŸ“å’Œå¹³é¢è®¾è®¡çš„ç»“åˆç‰¹ç‚¹æ˜¯æ°›å›´æ„‰å¿«å¯¹æ¯”åº¦é«˜å¹¶ é‡‡ç”¨äº†é»„é»‘ç™½ä¸ºä¸»çš„é…è‰²æ–¹æ¡ˆ (Translation: vibrant visual portfolio graphic design poster, with very small transparent checkerboard pattern as the background, showcasing 3D rendered cartoon character. On the left side of the screen is half body portrait of young woman with fair skin, long curly dark brown hair, wearing pink framed glasses, and brown big eyes behind the glasses. She had bright smile, revealing her teeth and wearing small silver earrings. Her attire includes light gray suit jacket, white collared shirt, and red tie. She held bouquet of four bright yellow sunflowers in her hand, with green stems. The character is surrounded by thick white outline, making it stand out from the background. On the right side of the poster are mainly large artistic characters. The main title \"Visual Works Collection\" adopts thick yellow brush style font. On top of it is thin line of red cursive English word \"Personalization\". Below is round, bubble shaped yellow font with one size smaller reading VISUAL PORTFOLIO. Below are three highlights: ä¸­è‹±æ¸²æŸ“å­—å­—å¦‚åˆ» Bilingual Rendering ä¸æ­¢çœŸå®žæ›´æ‡‚ç¾Žå­¦ Realism & Aesthetic è¯»æ‡‚å¤æ‚ç”Ÿæˆç²¾å¦™ Complexity & Elegance The Chinese characters here are white handwritten large characters, while the English characters are semi transparent printed small characters. The poster contains multiple text blocks and logos. The upper part is first marked with yellow text \"Z-Image x\", followed by yellow line drawn logo of cartoon avatar wearing headphones, and then the text \"x Design\". In the bottom right corner, there is cute anthropomorphic amplifier with two big eyes in light green and cream colors, and small daisy at the bottom. The overall style is combination of 3D character rendering and graphic design, characterized by pleasant atmosphere, high contrast, and predominantly yellow, black, and white color schemes.) Row #2 Case #1: ä¸€å¼ è™šæž„çš„è‹±è¯­ç”µå½±å›žå¿†ä¹‹å‘³The Taste of Memoryçš„ç”µå½±æµ·æŠ¥åœºæ™¯è®¾ç½®åœ¨ä¸€ä¸ªè´¨ æœ´çš„19ä¸–çºªé£Žæ ¼åŽ¨æˆ¿é‡Œç”»é¢ä¸­å¤®ä¸€ä½çº¢æ£•è‰²å¤´å‘ç•™ç€å°èƒ¡å­çš„ä¸­å¹´ç”·å­æ¼”å‘˜é˜¿ç‘Ÿå½­å“ˆ åˆ©æ ¹é¥°ç«™åœ¨ä¸€å¼ æœ¨æ¡ŒåŽä»–èº«ç©¿ç™½è‰²è¡¬è¡«é»‘è‰²é©¬ç”²å’Œç±³è‰²å›´è£™æ­£çœ‹ç€ä¸€ä½å¥³å£«æ‰‹ä¸­æ‹¿ ç€ä¸€å¤§å—ç”Ÿçº¢è‚‰ä¸‹æ–¹æ˜¯ä¸€ä¸ªæœ¨åˆ¶åˆ‡èœæ¿åœ¨ä»–çš„å³è¾¹ä¸€ä½æ¢³ç€é«˜é«»çš„é»‘å‘å¥³å­æ¼”å‘˜åŸƒèŽ‰ è¯ºä¸‡æ–¯é¥°å€šé åœ¨æ¡Œå­ä¸Šæ¸©æŸ”åœ°å¯¹ä»–å¾®ç¬‘å¥¹ç©¿ç€æµ…è‰²è¡¬è¡«å’Œä¸€æ¡ä¸Šç™½ä¸‹è“çš„é•¿è£™æ¡Œä¸Šé™¤ 61 äº†æ”¾æœ‰åˆ‡ç¢Žçš„è‘±å’Œå·å¿ƒèœä¸çš„åˆ‡èœæ¿å¤–è¿˜æœ‰ä¸€ä¸ªç™½è‰²é™¶ç“·ç›˜æ–°é²œé¦™è‰å·¦ä¾§ä¸€ä¸ªæœ¨ç®±ä¸Šæ”¾ ç€ä¸€ä¸²æ·±è‰²è‘¡è„èƒŒæ™¯æ˜¯ä¸€é¢ç²—ç³™çš„ç°ç™½è‰²æŠ¹ç°å¢™å¢™ä¸ŠæŒ‚ç€ä¸€å¹…é£Žæ™¯ç”»æœ€å³è¾¹çš„ä¸€ä¸ªå°é¢ä¸Š æ”¾ç€ä¸€ç›å¤å¤æ²¹ç¯æµ·æŠ¥ä¸Šæœ‰å¤§é‡çš„æ–‡å­—ä¿¡æ¯å·¦ä¸Šè§’æ˜¯ç™½è‰²çš„æ— è¡¬çº¿å­—ä½“ARTISAN FILMS PRESENTSå…¶ä¸‹æ–¹æ˜¯ELEANOR VANCEå’ŒACADEMY AWARD WINNERå³ä¸Šè§’å†™ ç€ARTHUR PENHALIGONå’ŒGOLDEN GLOBE AWARD WINNERé¡¶éƒ¨ä¸­å¤®æ˜¯åœ£ä¸¹æ–¯ ç”µå½±èŠ‚çš„æ¡‚å† æ ‡å¿—ä¸‹æ–¹å†™ç€SUNDANCE FILM FESTIVAL GRAND JURY PRIZE 2024ä¸» æ ‡é¢˜THE TASTE OF MEMORYä»¥ç™½è‰²çš„å¤§å·è¡¬çº¿å­—ä½“é†’ç›®åœ°æ˜¾ç¤ºåœ¨ä¸‹åŠéƒ¨åˆ†æ ‡é¢˜ä¸‹æ–¹æ³¨ æ˜Žäº†A FILM BY Tongyi Interaction Labåº•éƒ¨åŒºåŸŸç”¨ç™½è‰²å°å­—åˆ—å‡ºäº†å®Œæ•´çš„æ¼”èŒå‘˜åå•åŒ… æ‹¬SCREENPLAY BY ANNA REIDCULINARY DIRECTION BY JAMES CARTERä»¥åŠArtisan FilmsRiverstone Pictureså’ŒHeritage Mediaç­‰ä¼—å¤šå‡ºå“å…¬å¸æ ‡å¿—æ•´ä½“é£Žæ ¼æ˜¯å†™å®žä¸»ä¹‰é‡‡ç”¨æ¸© æš–æŸ”å’Œçš„ç¯å…‰æ–¹æ¡ˆè¥é€ å‡ºä¸€ç§äº²å¯†çš„æ°›å›´è‰²è°ƒä»¥æ£•è‰²ç±³è‰²å’ŒæŸ”å’Œçš„ç»¿è‰²ç­‰å¤§åœ°è‰²ç³»ä¸ºä¸»ä¸¤ ä½æ¼”å‘˜çš„èº«ä½“éƒ½åœ¨è…°éƒ¨è¢«æˆªæ–­ (Translation: movie poster for the fictional English movie The Taste of Memory. The scene is set in rustic 19th century style kitchen. In the center of the screen, middle-aged man with reddish brown hair and small beard (played by actor Arthur Penhaligan) stands behind wooden table. He is wearing white shirt, black vest, and beige apron, looking at woman holding large piece of raw red meat with wooden cutting board below. On his right, black haired woman with high bun (played by actress Eleanor Vance) leaned against the table and smiled gently at him. She was wearing light colored shirt and long skirt with white on top and blue on the bottom. On the table, in addition to chopping board with chopped onions and shredded cabbage, there is also white ceramic plate and fresh herbs. On the left side, there is wooden box with string of dark grapes. The background is rough gray white plaster wall with landscape painting hanging on it. On the far right countertop is vintage oil lamp. There is lot of textual information on the poster. The white sans serif font \"ARTISAN FILMS PRESS\" is located in the upper left corner, with \"ELEANOR VANCE\" and \"ACADEMY AWARD\" below it WINNER. In the upper right corner are written \"ARTHUR PENHALIGON\" and \"GOLDEN GLOBE\" AWARD WINNER. At the top center is the crown emblem of Sundance Film Festival, with the words SUNDANCE FILM FESTIVAL GRAND JURY PRIZE 2024 written below. The main title \"THE TASTE OF Memory\" is prominently displayed in large white serif font in the lower half. The title reads FILM BY Tongyi Interaction Lab.. The bottom area lists the complete cast and crew list in small white font, including \"SCREENPLAY BY ANNA REID\", \"CULINARY Directing BY JAMES CARTER\", as well as many production company logos such as Artisan Films, Riverstone Pictures, and Heritage Media. The overall style is realism, using warm and soft lighting schemes to create an intimate atmosphere. The color scheme is dominated by earthy tones such as brown, beige, and soft green. The bodies of both actors were severed at the waist.) Case #2: ä¸€å¼ ç«–ç‰ˆæ—¥æœ¬è‰ºæœ¯å±•æµ·æŠ¥èƒŒæ™¯ä¸ºæ·±è“è‰²è®¾è®¡ä»¥é†’ç›®çš„é»„è‰²æ–‡å­—å’Œä¸ƒå¹…æ°´å½©ç”»æ‹¼è´´ä¸ºä¸»é¡¶éƒ¨æ˜¯ æ—¥æ–‡å’Œè‹±æ–‡æ ‡é¢˜æ—¥æ–‡éƒ¨åˆ†ä½¿ç”¨å¤§å·é»„è‰²å®‹ä½“é£Žæ ¼å­—ä½“å†…å®¹ä¸ºè°·å· æ­£å­£ - æ°´å½©ç”»ã®ä¸–ç•Œ -å…¶ ä¸‹æ–¹æ˜¯è¾ƒå°çš„é»„è‰²æ— è¡¬çº¿å­—ä½“-The world of watercolor-ä¸»æ ‡é¢˜ä¸­å¤©å ‚ä»¥éžå¸¸å¤§çš„é£Žæ ¼åŒ–é»„ è‰²å­—ä½“çªå‡ºæ˜¾ç¤ºå…¶ä¸‹æ˜¯è‹±æ–‡ç¿»è¯‘HEAVEN OF DREAMåŒæ ·ä¸ºé»„è‰²æ— è¡¬çº¿å­—ä½“å†ä¸‹ä¸€è¡Œæ˜¯ æ—¥æ–‡å‰¯æ ‡é¢˜æˆ‘ãŒå¿ƒã®æ¡‚æž—å­—ä½“è¾ƒå¤§åŽè·Ÿå…¶è‹±æ–‡ç¿»è¯‘GUILIN IN MY MINDå­—ä½“è¾ƒå° æµ·æŠ¥ä¸­å¤®æ˜¯ç”±ä¸ƒå¹…æç»˜æ¡‚æž—å–€æ–¯ç‰¹åœ°è²Œä¸åŒåœºæ™¯çš„æ°´å½©ç”»ç»„æˆçš„ç½‘æ ¼è¿™äº›ç”»ä½œå±•ç¤ºäº†äº‘é›¾ç¼­ç»•çš„ ç¾¤å±±èœ¿èœ’ç©¿è¿‡å±±è°·çš„æ²³æµå€’æ˜ åœ¨æ°´é¢ä¸Šçš„ç»šä¸½æ—¥è½äººä»¬åœ¨èˆ¹ä¸Šæç€ç¯ç¬¼çš„å¤œæ™¯ä»¥åŠå…¶ä»–å¯Œæœ‰ æ°›å›´çš„é£Žæ™¯æµ·æŠ¥åº•éƒ¨ä¸‰åˆ†ä¹‹ä¸€å¤„ç”¨è¾ƒå°çš„é»„è‰²æ–‡å­—åˆ—å‡ºäº†æ´»åŠ¨è¯¦æƒ…åŒ…æ‹¬2025.11.11(å…­) 17(äº”) 9:00 20:00é˜¿é‡Œå·´å·´äº‘è°·å›­åŒº(021)-34567890æ•´ä½“é£Žæ ¼æ˜¯ä¼˜é›…çš„å¹³é¢è®¾è®¡é‡‡ç”¨äº†æ·±è“è‰² å’Œé»„è‰²çš„é«˜å¯¹æ¯”åº¦åŒè‰²è°ƒè‰²æ¿ (Translation: vertical Japanese art exhibition poster with dark blue background. The design mainly features eye-catching yellow text and seven watercolor collages. At the top are Japanese and English titles. The Japanese section uses large yellow Song style fonts and the content is \"è°·å· æ­£å­£ - æ°´å½©ç”»ã®ä¸–ç•Œ -\". Below it is smaller yellow sans serif font that reads The world of watercolor - . The main title \"ä¸­å¤©å ‚\" is highlighted in very large stylized yellow font. Below is the English translation \"HEAVEN OF DREAM\", also in yellow sans serif font. The next line is the Japanese subtitle æˆ‘ãŒå¿ƒã®æ¡‚æž—, with larger font size, followed by its English translation GUILIN IN MY MIND, with smaller font size. In the center of the poster is grid composed of seven watercolor paintings depicting different scenes of Guilins karst landscape. These paintings showcase misty mountains, winding rivers through valleys, stunning sunsets reflected on the water, night scenes of 62 people carrying lanterns on boats, and other atmospheric landscapes. The activity details are listed in small yellow text at the bottom third of the poster, including \"2025.11.11(å…­) 17(äº”) 9:00 20:00\", \"é˜¿é‡Œå·´å·´äº‘è°·å›­åŒº\", and \"(021) -34567890\". The overall style is an elegant graphic design, featuring high contrast dual tone palette of dark blue and yellow.) Case #3: ä¸€å¼ æ–¹å½¢æž„å›¾çš„ç‰¹å†™ç…§ç‰‡ä¸»ä½“æ˜¯ä¸€ç‰‡å·¨å¤§çš„é²œç»¿è‰²çš„æ¤ç‰©å¶ç‰‡å¹¶å åŠ äº†æ–‡å­—ä½¿å…¶å…·æœ‰æµ·æŠ¥ æˆ–æ‚å¿—å°é¢çš„å¤–è§‚ä¸»è¦æ‹æ‘„å¯¹è±¡æ˜¯ä¸€ç‰‡åŽšå®žæœ‰èœ¡è´¨æ„Ÿçš„å¶å­ä»Žå·¦ä¸‹è§’åˆ°å³ä¸Šè§’å‘ˆå¯¹è§’çº¿å¼¯æ›² ç©¿è¿‡ç”»é¢å…¶è¡¨é¢åå…‰æ€§å¾ˆå¼ºæ•æ‰åˆ°ä¸€ä¸ªæ˜Žäº®çš„ç›´å°„å…‰æºå½¢æˆäº†ä¸€é“çªå‡ºçš„é«˜å…‰äº®é¢ä¸‹æ˜¾éœ² å‡ºå¹³è¡Œçš„ç²¾ç»†å¶è„‰èƒŒæ™¯ç”±å…¶ä»–æ·±ç»¿è‰²çš„å¶å­ç»„æˆè¿™äº›å¶å­è½»å¾®å¤±ç„¦è¥é€ å‡ºæµ…æ™¯æ·±æ•ˆæžœçªå‡º äº†å‰æ™¯çš„ä¸»å¶ç‰‡æ•´ä½“é£Žæ ¼æ˜¯å†™å®žæ‘„å½±æ˜Žäº®çš„å¶ç‰‡ä¸Žé»‘æš—çš„é˜´å½±èƒŒæ™¯ä¹‹é—´å½¢æˆé«˜å¯¹æ¯”åº¦å›¾åƒ ä¸Šæœ‰å¤šå¤„æ¸²æŸ“æ–‡å­—å·¦ä¸Šè§’æ˜¯ç™½è‰²çš„è¡¬çº¿å­—ä½“æ–‡å­—PIXEL-PEEPERS GUILD Presentså³ä¸Šè§’åŒ æ ·æ˜¯ç™½è‰²è¡¬çº¿å­—ä½“çš„æ–‡å­—[Instant Noodle] æ³¡é¢è°ƒæ–™åŒ…å·¦ä¾§åž‚ç›´æŽ’åˆ—ç€æ ‡é¢˜Render Distance: Maxä¸ºç™½è‰²è¡¬çº¿å­—ä½“å·¦ä¸‹è§’æ˜¯äº”ä¸ªç¡•å¤§çš„ç™½è‰²å®‹ä½“æ±‰å­—æ˜¾å¡åœ¨...ç‡ƒçƒ§å³ä¸‹è§’æ˜¯è¾ƒå°çš„ç™½ è‰²è¡¬çº¿å­—ä½“æ–‡å­—Leica Glow Unobtanium X-1å…¶æ­£ä¸Šæ–¹æ˜¯ç”¨ç™½è‰²å®‹ä½“å­—ä¹¦å†™çš„åå­—è”¡å‡  (Translation: close-up photo with square composition, featuring large, bright green plant leaf and overlaid with text to give it the appearance of poster or magazine cover. The main subject being photographed is thick, waxy leaf that curves diagonally through the frame from the bottom left corner to the top right corner. Its surface has strong reflectivity, capturing bright direct light source and forming prominent highlight, revealing parallel fine leaf veins under the bright surface. The background is composed of other dark green leaves that are slightly out of focus, creating shallow depth of field effect and highlighting the main leaf of the foreground. The overall style is realistic photography, with high contrast between bright leaves and dark shadow backgrounds. There are multiple rendered texts on the image. In the upper left corner is the white serif font text \"PIXEL-PEEPERS GUIDE Gifts\". The text in white serif font in the upper right corner reads [Instant Noodle] æ³¡é¢è°ƒæ–™åŒ…. The title \"Render Distance: Max\" is vertically arranged on the left side in white serif font. In the bottom left corner are five large white Song typeface Chinese characters that read æ˜¾å¡åœ¨...ç‡ƒçƒ§. The smaller white serif font text \"Leica Glow\" is located in the bottom right corner Unobtanium X-1 Above it is the name \"è”¡å‡ \" written in white Song typeface.) Row # Case #1: vertical digital illustration depicting serene and majestic Chinese landscape, rendered in style reminiscent of traditional Shanshui painting but with modern, clean aesthetic. The scene is dominated by towering, steep cliffs in various shades of blue and teal, which frame central valley. In the distance, layers of mountains fade into light blue and white mist, creating strong sense of atmospheric perspective and depth. calm, turquoise river flows through the center of the composition, with small, traditional Chinese boat, possibly sampan, navigating its waters. The boat has bright yellow canopy and red hull, and it leaves gentle wake behind it. It carries several indistinct figures of people. Sparse vegetation, including green trees and some bare-branched trees, clings to the rocky ledges and peaks. The overall lighting is soft and diffused, casting tranquil glow over the entire scene. Centered in the image is overlaid text. At the top of the text block is small, red, circular seal-like logo containing stylized characters. Below it, in smaller, black, sans-serif font, are the words Zao-Xiang * East Beauty & West Fashion * Z-Image. Directly beneath this, in larger, elegant black serif font, is the word SHOW & SHARE CREATIVITY WITH THE WORLD. Among them, there are \"SHOW & SHARE\", \"CREATIVITY\", and \"WITH THE WORLD\" Case #2: vertical movie poster for the film \"Come Back Home Often.\" created by Master of Oil painting. The artwork is unified digital painting with heavy impasto texture, mimicking thick oil paint strokes applied with palette knife. The central focus is massive, abstract figure rendered in thick, textured white paint, resembling giant bird or stylized human form. This white shape is set against dark navy blue background that is densely covered with small, stylized flowers painted in vibrant red and white, with green stems. In the bottom right corner, two elderly people are depicted from behind, walking away from the viewer. One person, slightly ahead, wears purple jacket and uses wooden cane. The other, slightly behind, wears greyish-blue jacket. Their 63 bodies are truncated at the ankles by the bottom edge of the frame. The overall style is surreal and symbolic, with high-contrast color palette dominated by deep navy, white, and red. Text control: all lettering is fully integrated into the painted surface with identical heavy impasto, each character exhibiting raised ridges and knife-scraped edges that catch ambient light. In the top left corner, in white sans-serif strokes sculpted with thick, palette-knife ridges, the words Z-Image appear, and directly beneath them, still in raised impasto, Visionary Creator. In the bottom left, the Chinese title is rendered in large, white, cursive calligraphy (è‰ä¹¦ style) built up from layered, knife-pressed paint: å¸¸å›žå®¶çœ‹çœ‹, its down-strokes showing visible paint peaks. Below this, in smaller white serif font whose letterforms are similarly embossed with raised impasto, reads the English title: Come Back Home Often. Case #3: ä¼ ç»Ÿä¸­å›½æ°´å¢¨ç”»ç…§ç‰‡æç»˜äº†è§ç‘Ÿç§‹æ—¥é»„æ˜æ™¯è±¡ä½äºŽé¡µé¢å·¦ä¾§ç”»ä½œç«–å‘æŽ’åˆ—ç”¨æž¯ç¬”å‹¾å‹’ç›˜æ›² è€è—¤ç¼ ç»•å¤æ ‘æµ“å¢¨ç‚¹æŸ“æ –æ¯æ˜é¸¦æ·¡å¢¨æ™•æŸ“æš®è‰²å¤©ç©ºæž¯è—¤å¦‚é¾™è›‡èˆ¬æ”€é™„åœ¨è™¬æ›²æ ‘å¹²ä¸Šä¸‰ä¸¤åª ä¹Œé¸¦åœé©»æžå¤´å‰ªå½±èˆ¬çš„è½®å»“è¿œå¤„éšçº¦å¯è§å°æ¡¥æµæ°´å’Œå¤æœ´äººå®¶å±‹èˆç‚ŠçƒŸè¢…è¢…è¿‘æ™¯æœ‰ä¸€æ¡é»„ åœŸå¤é“ç˜¦é©¬ä½Žå¤´ç¼“è¡Œç”»é¢ä¸Šæ–¹æœ‰å‡ è¡Œé»‘è‰²è¡Œä¹¦ä¹¦æ³•é¢˜å†™å…ƒæ›²åå¥å³ä¸Šè§’é’¤ä¸€æžšæœ±çº¢æ–¹å° åœ¨ç”»ä½œå³ä¾§æœ‰ä¸¤åˆ—ç«–æŽ’ä¸­æ–‡æ–‡å­—çº¯ç™½è‰²èƒŒæ™¯è‰ä¹¦å­—ä½“ç¬¬ä¸€åˆ—å†™ç€ \"å¤©å‡€æ²™ç§‹æ€\"ç¬¬äºŒåˆ— å†™ç€æž¯è—¤è€æ ‘æ˜é¸¦å°æ¡¥æµæ°´äººå®¶ ä¼ ç»Ÿä¸­å¼æ–‡äººç”»é£Žæ ¼æ°´å¢¨å•è‰²ç³»å¸¦é£žç™½æž¯ç¬”æ•ˆæžœç¬”è§¦ è‹åŠ²æœ‰åŠ›æž„å›¾ç–å¯†æœ‰è‡´ç•™ç™½å¤„ç†çªå‡ºå……æ»¡è§ç‘Ÿè‹å‡‰çš„ç¾Žå­¦æ„è•´æ°›å›´å­¤å¯‚è€Œæ‚ è¿œå…·æœ‰æµ“åŽš çš„å¤å…¸è¯—æ„å’Œæ–‡åŒ–éŸµå‘³ (Translation: The traditional Chinese ink painting photo depicts bleak autumn dusk scene, located on the left side of the page. The painting is arranged vertically, using dry brush to outline the winding old vines around the ancient trees, with thick ink coloring the roosting crows, and light ink blending the twilight sky. The withered vine clung to the winding tree trunk like dragon or snake, with three or two crows perched on the branches, forming silhouette like silhouette. In the distance, small bridges, flowing water, and quaint houses can be faintly seen, with smoke rising from cooking. In the close-up, there is loess ancient road, and thin horses are walking slowly with their heads down. There are several lines of black cursive calligraphy above the screen, inscribed with famous lines of Yuan opera, and vermilion square seal is stamped in the upper right corner. On the right side of the painting, there are two vertical columns of Chinese characters with pure white background and cursive font. The first column reads \"å¤©å‡€æ²™ç§‹æ€\", and the second column reads \"æž¯è—¤è€æ ‘æ˜é¸¦å°æ¡¥æµæ°´äººå®¶\". The traditional Chinese literati painting style features single color ink wash with flying white and withered pen effect. The brushstrokes are vigorous and powerful, the composition is dense and orderly, and the white space treatment is prominent. It is full of desolate and desolate aesthetic connotations, creating lonely and distant atmosphere with strong classical poetry and cultural charm.) Row #4 Case #1: ç«–æŽ’è¡Œä¹¦ä¹¦æ³•ä½œå“ç‰¹å†™ä»¥ç±³ç™½è‰²å¸¦æµ…æ·¡è‚Œç†çš„åŠç”Ÿç†Ÿå®£çº¸ä¸ºè½½ä½“çº¸å¼ å¸¦æœ‰è‡ªç„¶è½»å¾®è¤¶çš±è´¨æ„Ÿ æ¸©æ¶¦æŸ”å’Œé»‘è‰²å¢¨æ±ä¹¦å†™çš„è¡Œä¹¦å­—ä½“ç¬”é”‹ç²—ç»†å˜åŒ–çµåŠ¨å¦‚äººå­—æºç¬”èˆ’å±•åŠ²æŒºå¿—å­—è¿žç¬”å©‰ è½¬æµç•…å¢¨è‰²å±‚æ¬¡ä¸°å¯Œéƒ¨åˆ†ç¬”ç”»å¸¦è‡ªç„¶é£žç™½æ•ˆæžœå°½æ˜¾è‹åŠ²æ´’è„±çš„ä¹¦å†™å¼ åŠ›æ–‡å­—æŒ‰ä¼ ç»Ÿä»Žå³è‡³ å·¦ç«–åˆ—æŽ’å¸ƒå¯è§äººæ‰¶æˆ‘é’å¿—æˆ‘è‡ªè¸é›ªè‡³å±±å·…ç­‰è¯å¥å·¦ä¾§é…æœ‰é€ ç›¸å¤§å¸ˆè½æ¬¾å°å­—çº¸é¢ç‚¹ ç¼€å¤šæžšæœ±çº¢æ–¹å½¢ç¯†åˆ»å°ç« å°æ³¥è‰²æ³½é¥±æ»¡å°æ–‡çº¿æ¡æ¸…æ™°å¤šå¼ ä¹¦æ³•çº¸å‘ˆè½»å¾®é‡å çš„é”™è½æ‘†æ”¾èƒŒ æ™¯éšçº¦éœ²å‡ºå…¶ä»–çº¸å¼ çš„æ·¡è‰²å­—è¿¹è¥é€ å‡ºéšæ€§çš„åˆ›ä½œæ°›å›´å…‰çº¿ä¸ºæŸ”å’Œè‡ªç„¶å…‰å‡åŒ€é“ºæ´’åœ¨çº¸é¢ å‡¸æ˜¾å¢¨è‰²çš„å…‰æ³½ä¸Žçº¸å¼ çš„çº¹ç†è¤¶çš±é€ æ¢¦å¸ˆçš„è¯—æ„è§†è§‰é£Žæ ¼æ•´ä½“æ°›å›´é›…è‡´å¤æœ´å…¼å…·æ‰‹å†™ä¹¦æ³•çš„ çµåŠ¨éšæ€§ä¸Žä¼ ç»Ÿæ–‡æˆ¿çš„æ²‰é™è´¨æ„Ÿ (Translation: Close up of vertical cursive calligraphy works, using semi ripe rice paper with light texture and off white color as the carrier. The paper has natural slight wrinkles and warm and soft texture; The running script font written in black ink has dynamic changes in stroke thickness, such as the stretching and vigorous strokes of the \"äºº\" character and the smooth and graceful strokes of the \"å¿—\" character. The ink layers are rich, and some strokes have natural flying white effect, showcasing the vigorous and free spirited writing tension; The text is arranged vertically from right to left according to tradition, with phrases such as \"äººæ‰¶æˆ‘é’å¿—\" and \"æˆ‘è‡ª è¸é›ªè‡³å±±å·…\" visible. On the left side, there is small signature of \"é€ ç›¸å¤§å¸ˆ\", and the paper is decorated with multiple vermilion square seal seals. The ink color is full and the lines of the seal are clear; Multiple calligraphy papers are arranged in slightly overlapping and staggered manner, with the background faintly revealing the light colored handwriting of other papers, creating casual creative atmosphere; The light is soft natural light, evenly spread on the paper surface, highlighting the luster of ink color and the texture wrinkles of the paper. The poetic visual style of the dream maker creates an elegant and rustic atmosphere, combining the agility and casualness of handwritten calligraphy with the calm texture of traditional study rooms.) Case #2: ä¸€å¼ åž‚ç›´æž„å›¾çš„å¹³é¢è®¾è®¡æµ·æŠ¥èƒŒæ™¯æ˜¯çº¯ç²¹è€Œé²œè‰³çš„å®è“è‰²é¡¶éƒ¨çš„å·¨å¤§æ— è¡¬çº¿å­—ä½“ä¸»æ ‡é¢˜ä¸Š åŠéƒ¨åˆ†ä¸ºæµ…ç°è‰²çš„Sofa Montain Slummerfestä¸‹åŠéƒ¨åˆ†ä¸ºç™½è‰²çš„Annual Napping Festival 2025å…¶ä¸‹æ–¹æ˜¯å·¨å¤§çš„é»‘è‰²ä¹¦æ³•å­—ä½“ä¸­æ–‡æ ‡é¢˜æ²™å‘å±±æ‰“å‘¼èŠ‚æµ·æŠ¥çš„ä¸‹åŠéƒ¨åˆ†ç”±ä¸€å¹…å·¨å¤§çš„ æ’ç”»é£Žæ ¼çš„è€è™Žæ’ç”»å æ®å®ƒæ­£è¶´ç€é¢å‘è§‚ä¼—çœ¼ç›æ˜¯é»„è‰²çš„å…¶çš®æ¯›ç”±æ©™é»‘ç™½ä¸‰è‰²æž„æˆä¸€ ä¸ªåŒ…å«çº¢è‰²çˆ±å¿ƒçš„æ€æƒ³æ³¡æ³¡æ¼‚æµ®åœ¨å®ƒçš„å¤´é¡¶æµ·æŠ¥ä¸Šå¸ƒæ»¡äº†è¯¦å°½çš„æ´»åŠ¨æ–‡å­—å·¦æ ç”¨ç™½è‰²å­—ä½“åˆ—å‡º äº†ä»¥çŒ«ä¸ºä¸»é¢˜çš„ä¹é˜Ÿåå¦‚The Fluffy Paws Grumbers (æ¯›çˆªå’•å™œ)DJ Meow Mixä¹å‘½æ€ª çŒ« (Nine Lives)æ¿€å…‰ç¬”è¿½é€è€… (The Laser Dots)çº¸ç®±çˆ±å¥½è€… (Cardbock Box Lovers)å‘¼ å™œç¥žæ•™ (The Purr-fectionists)çŒ«è‰æˆç˜¾è€… (The Catnip Junkies)DJ Chairman Meow (çŒ«ä¸» å¸­)ä»¥åŠåƒVarh Radator Fesidenl Paw-Fiveè¿™æ ·çš„æ— æ„ä¹‰çŸ­è¯­å³æ åˆ—å‡ºäº†æ´»åŠ¨ç»†èŠ‚å…¶ä¸­è®¸å¤š éƒ½å¸¦æœ‰æ»‘ç¨½çš„æ‹¼å†™é”™è¯¯æˆ–æ— æ„ä¹‰å†…å®¹åŒ…æ‹¬æ—¥æœŸ4/1 MONDAY SUNL SUNSETåœ°ç‚¹ä¸Šæµ·å¸‚æµ¦ ä¸œæ–°åŒºçŒ«æŠ“æ¿è·¯1å·é¡¶æ¥¼é˜³å°ä»¥åŠç¥¨åŠ¡ä¿¡æ¯å¦‚ADV. 1 CAN OF TUNA, DOOR 2 CANS, KITTENS FREE!åœ¨æœ€åº•éƒ¨æ˜¯ä¸€æŽ’è™šæž„çš„èµžåŠ©å•†æ ‡å¿—åç§°åŒ…æ‹¬Catberdå¥½ä¸»äººç½ç½æœ‰é™å…¬å¸ (Good Oinar Canned Food Ltd)å’ŒiNONEPAWS (Translation: vertically composed graphic design poster with pure and vibrant navy blue background. The main title features large sans serif font at the top, with the upper half in light gray reading \"Sofa Montain Slummerfest\" and the lower half in white reading \"Annual Napping Festival 2025\". Below it is huge black calligraphy font with the Chinese title \"æ²™å‘å±±æ‰“å‘¼èŠ‚\". The lower part of the poster is occupied by huge, illustrated tiger illustration, which is lying face down to the audience with yellow eyes. Its fur is composed of three colors: orange, black, and white. thought bubble containing red heart floats above its head. The poster is filled with detailed activity text. The left column lists the names of cat themed bands in white font, such as \"The Fluffy Paws Grumbers (æ¯›çˆªå’•å™œ)\", \"DJ Meow Mix, \"ä¹å‘½æ€ªçŒ« (Nine Lives)\", \"æ¿€å…‰ç¬”è¿½é€è€… (The Laser Dots)\", \"çº¸ç®±çˆ±å¥½è€… (Cardbock Box Lovers)\", \"å‘¼å™œç¥žæ•™ (The Purr-fectionists)\", \"çŒ«è‰æˆç˜¾è€… (The Catnip Junkies)\", \"DJ Chairman Meow (çŒ«ä¸»å¸­)\" and meaningless phrases like \"Varh Radator Feseidel Paw Five. The right column lists the details of the event, many of which have humorous spelling errors or meaningless content, including the date \"4/1 MONDAY SUNL SUNSET\", location \"ä¸Šæµ·å¸‚æµ¦ä¸œ æ–°åŒºçŒ«æŠ“æ¿è·¯1å·é¡¶æ¥¼é˜³å°\", and ticketing information such as \"ADV. 1 CAN OF TUNA, DOOR 2 CANS, KITTENS FREE. At the bottom is row of fictional sponsor logos, with names including \"Cattered,\" \"å¥½ä¸»äººç½ç½æœ‰é™å…¬å¸\" and \"iNONEPAWS\".) Case #3: ä¸€å¼ æ¤ç‰©å±•è§ˆçš„å¹³é¢è®¾è®¡æµ·æŠ¥èƒŒæ™¯ä¸ºç´ å‡€çš„ç±³ç™½è‰²æµ·æŠ¥ä¸Šæœ‰å¤šå¹…æ°´å½©æ’ç”»æç»˜äº†å„ç§è‹”è—“å’Œ è•¨ç±»æ¤ç‰©ç”¨è‰²ä»¥ç»¿è‰²æ£•è‰²å’Œé»„è‰²ä¸ºä¸»å¹¶é…æœ‰ç²¾è‡´çš„é»‘è‰²å¢¨æ°´è½®å»“çº¿ç”»é¢ä¸­å¤®æ˜¯ä¸€å¹…å·¨å¤§è€Œ ç²¾ç»†çš„ç»¿è‰²åœ°é’±æ’ç”»ä¸Šé¢æœ‰æ£•è‰²çš„å­¢å­ä½“å…¶ä»–è¾ƒå°çš„æ’å›¾æ•£å¸ƒåœ¨å‘¨å›´åŒ…æ‹¬å·¦ä¸Šè§’çš„ç»†å¶æ»¡æ±Ÿ çº¢Fissidensé¡¶éƒ¨ä¸­å¤®çš„å·¨å¶çº¢èŒŽè—“Rhodobryum giganteumå³ä¸Šè§’çš„é’è‹”å±žBryum sp.ä»¥åŠå³ä¸‹è§’çš„å‡¤å°¾è—“Marchantia formosanaæµ·æŠ¥å¸ƒå±€å¹²å‡€ç®€çº¦æ–‡å­—åž‚ç›´å’Œæ°´å¹³ æŽ’åˆ—å³ä¸Šè§’æ˜¯çºµå‘æŽ’åˆ—çš„é»‘è‰²å®‹ä½“å¤§å­—è‹”ç—•ä¸‹è¾¹æ˜¯æ¨ªå‘æŽ’åˆ—çš„æ— è¡¬çº¿å­—ä½“è‹±æ–‡Moss Exhibitionå·¦ä¾§æ˜¯é»‘è‰²æ— è¡¬çº¿å­—ä½“Elkhorn Fern LifeStyleå·¦ä¸‹è§’å†™ç€Alishan Moss Ecological æ—¥æœŸå’Œæ—¶é—´åœ¨åº•éƒ¨çªå‡ºæ˜¾ç¤º2001ä¸ºé»‘è‰²å¤§å·è¡¬çº¿å­—ä½“å…¶åŽæ˜¯è¾ƒå°çš„æ— è¡¬çº¿å­—ä½“04.22 [Apr.] am. 09:00å’Œ05.22 [May] pm. 17:00æ¯å¹…æ¤ç‰©æ’å›¾éƒ½é™„æœ‰å…¶å­¦åä½¿ç”¨å°å·ç°è‰²æ— è¡¬çº¿å­—ä½“ ä¹¦å†™ä¾‹å¦‚FissidensRhodobryum giganteumBryum sp.BartramiaceaeAlishan Moss EcologicalMarchantia formosanaå’ŒAstrocella yoshinaganaæ•´ä½“é£Žæ ¼ä¼˜é›…ä¸”å…·æœ‰æ•™ è‚²æ„ä¹‰å°†ç§‘å­¦æ’ç”»ä¸ŽçŽ°ä»£æŽ’ç‰ˆç›¸ç»“åˆ (Translation: graphic design poster for plant exhibition, with plain beige background. The poster features multiple watercolor illustrations depicting various mosses and ferns, primarily using green, brown, and yellow colors, with delicate black ink outlines. In the center is large and intricate illustration of green liverwort with brown sporophytes on top. Other smaller illustrations are scattered around, including Fissidens in the upper left corner, Rhodobryum giganteum in the top center, Bryum sp. in the upper right corner, and Marchantia formosana in the lower right corner. The poster layout is clean and minimalist, with text arranged both vertically and horizontally. In the upper right corner is the vertically arranged black Song-style characters \"è‹”ç—•\", below which is the horizontally arranged sans-serif English text \"Moss Exhibition\". On the left side is the black sans-serif text \"Elkhorn Fern LifeStyle\". The lower left corner reads \"Alishan Moss Ecological\". The date and time are prominently displayed at the bottom: \"2001\" in large black serif font, followed by smaller sans-serif text \"04.22 [Apr.] am. 09:00\" and \"05.22 [May] pm. 17:00\". Each plant illustration is accompanied by its scientific name, written in small gray sans-serif font, such as \"Fissidens\", \"Rhodobryum giganteum\", \"Bryum sp.\", \"Bartramiaceae\", \"Alishan Moss Ecological\", \"Marchantia formosana\", and \"Astrocella yoshinagana\". The overall style is elegant and educational, combining scientific illustration with modern typography.) A.3. Figure 3 Row #1 Case #1: å¤´å‘å˜æˆæ·¡ç´«è‰²å·å‘å‘ä¸è¾¹ç¼˜æ•£å‘å‡ºé‡‘å…‰ (Translation: The hair becomes light purple curls, with golden light emanating from the edges of the hair strands.) Case #2: è®©è¿™ä¸ªå¥³æ€§ç›˜è…¿ååœ¨é…’åº—æˆ¿é—´çš„åœ°æ¯¯ä¸Šé¢å¸¦å¾®ç¬‘çœ¼ç¥žæ¸©æŸ”åœ°æ³¨è§†ç€é•œå¤´èƒŒæ™¯æ˜¯æ•´æ´çš„ç™½è‰²å¤§ åºŠå’Œæ¸©é¦¨çš„åºŠå¤´ç¯æ•´ä½“æ°›å›´å®é™è€Œä¼˜é›… (Translation: Have this woman sit cross-legged on the hotel room carpet, with smile on her face and gentle gaze looking at the camera. The background features neat white bed and warm bedside lamp, with an overall atmosphere that is tranquil and elegant.) Case #3: å˜æˆæ°´å½©é£Žæ ¼ (Translation: Transform into watercolor style.) Case #4: æŠŠè€é¹°å˜æˆçŽ»ç’ƒæè´¨åŒæ—¶æŠŠå¤©ç©ºå˜æˆæ©™è‰²çš„å¤•é˜³ (Translation: Transform the eagle into glass material, while changing the sky into an orange sunset.) Row # Case #1: å‚è€ƒçŒ«çš„å½¢è±¡ç”Ÿæˆä¸‰å®«æ ¼æ•…äº‹ç”»é¢åŒ…æ‹¬æµ·æ»©è¿œå±±è½æ—¥å¤•é˜³ä¸‰ä¸ªå®«æ ¼ä»Žä¸Šåˆ°ä¸‹å®« æ ¼1é¡¶éƒ¨: çŒ«èƒŒå¯¹ç€é•œå¤´ååœ¨æ²™æ»©ä¸Šå‡æœ›ç€è¿œæ–¹çš„å¤•é˜³å’Œæµ·é¢å­—å¹•å±±çš„é‚£è¾¹æ˜¯ä»€ä¹ˆå®« æ ¼2ä¸­éƒ¨çŒ«å’ªè½¬è¿‡èº«æ¥ä¾§è„¸å¯¹ç€é•œå¤´å­—å¹•ä½ ä¸å¿…è¯´å®«æ ¼3åº•éƒ¨çŒ«è„¸ç‰¹å†™æ­£è„¸ ç›´è§†é•œå¤´å­—å¹•æˆ‘æ ¹æœ¬ä¸æƒ³çŸ¥é“ (Translation: Referencing the cats image, generate three-panel story. The scenes include beach, distant mountains, sunset, and evening glow, with three panels arranged from top to bottom. Panel 1 (top): The cat sits on the beach with its back to the camera, gazing at the distant sunset and sea, with subtitle \"å±±çš„é‚£è¾¹æ˜¯ä»€ä¹ˆ\"; Panel 2 (middle): The cat turns around, showing its profile to the camera, with subtitle \"ä½ ä¸å¿…è¯´\"; Panel 3 (bottom): Close-up of the cats face, looking directly at the camera, with subtitle \"æˆ‘æ ¹æœ¬ä¸æƒ³çŸ¥é“\".) Case #2: åˆ¶ä½œæµ·æŠ¥èƒŒæ™¯å˜æˆå…¬è·¯å’Œè“å¤©ç™½äº‘ä¸¤ä¾§æ˜¯å¼€é˜”çš„ç”°é‡Žè‡ªè¡Œè½¦ä¿æŒå§¿æ€ä¸å˜æ”¾åœ¨å…¬è·¯ä¸­å¤® æµ·æŠ¥ä¸Šæ–¹æ˜¯ä¸»æ ‡é¢˜çŽ¯çƒéª‘è¡Œæ´¾å¯¹ä½¿ç”¨ç²—çŠ·åŠ¨æ„Ÿçš„é»‘è‰²å­—ä½“å…¶æ­£ä¸‹æ–¹æ˜¯å‰¯æ ‡é¢˜From 2.30 to 2.31 (Translation: Create poster with the background transformed into highway with blue sky and white clouds, with open fields on both sides. The bicycle remains in its original posture, placed in the center of the road. At the top of the poster is the main title \"çŽ¯çƒéª‘è¡Œæ´¾å¯¹\" in bold, dynamic black font, with the subtitle \"From 2.30 to 2.31\" directly below it.) Case #3: æŠŠçŒ«æ¢æˆä¸€åªæœ‰ç€ç›¸åŒå§¿åŠ¿çš„å“ˆå£«å¥‡æ–‡å­—Love Catä¿®æ”¹ä¸ºLove Dogä»¥åŠæ–‡å­—å–µå–µæ”¹ ä¸ºæ±ªæ±ªåŒæ—¶åœ¨å·¦ä¸‹è§’çš„å® ç‰©é¤ç›˜é‡ŒåŠ æ»¡ç‹—ç²® 66 (Translation: Replace the cat with husky in the same pose, change the text \"Love Cat\" to \"Love Dog\", and change the text \"å–µå–µ\" to \"æ±ªæ±ª\". Additionally, fill the pet food bowl in the lower left corner with dog food.) Row #3 Case #1: è®©çŒ«å’Œç‹—åˆ†åˆ«ç©¿ä¸Šç²‰è‰²å’Œå’Œç»¿è‰²çš„æ¯›è¡£èƒŒæ™¯æ”¹ä¸ºæµ·è¾¹çš„æˆ·å¤–å’–å•¡åº—çŒ«å’Œç‹—ååœ¨æ¡Œå­æ—çš„æ¤…å­ä¸Š å–å’–å•¡ (Translation: Have the cat and dog wear pink and green sweaters respectively, change the background to seaside outdoor cafÃ©, with the cat and dog sitting on chairs at the table drinking coffee.) Case #2: æŠŠè¿™å¼ å›¾å˜æˆä¸€å¹…ç”»é‡‘è‰²è¾¹æ¡†æŒ‚åœ¨ç”»å±•çš„å¢™ä¸Šæ—è¾¹æœ‰ä¸€äº›äººåœ¨æ¬£èµè¿™å¹…ç”» (Translation: Transform this image into painting with golden frame, hanging on the wall of an art exhibition, with some people standing beside it appreciating the painting.) Row #4 Case #1: è®©ä¸€ä¸ªä¸œæ–¹ç¾Žå¥³ç©¿ä¸Šè¿™ä¸ªè¿žè¡£è£™å’Œé»‘è‰²çš„éž‹å¹¶æˆ´ä¸Šè¿™é¡¶å¸½å­ç¾Žå¥³ç«™åœ¨å…¬å›­è‰åœ°ä¸ŠèƒŒæ™¯æœ‰å‡ é¢— æ¡ƒæ ‘ (Translation: Have an East Asian woman wear this dress and black shoes, and put on this hat. The woman stands on the park lawn, with several peach trees in the background.) Row #5 Case #1: å‚è€ƒå›¾åƒç”Ÿæˆä¸€ä¸ªç‹ç‹¸çŽ©å¶çš„å½©è‰²å›¾åƒçŽ©å¶æ”¾åœ¨å…¬å›­è‰åœ°ä¸ŠèƒŒæ™¯æœ‰ä¸€äº›æ ‘æœ¨ (Translation: Referencing the image, generate color image of fox plush toy, with the toy placed on the park lawn and some trees in the background.) Case #2: æ—‹è½¬çŽ©å¶å±•çŽ°ä»–çš„ä¾§é¢ (Translation: Rotate the toy to show its side profile.) Case #3: ç”Ÿæˆä¸¤ä¸ªè¿™ä¸ªç‹ç‹¸çŽ©å¶åœ¨è¶…å¸‚è´­ç‰©çš„ç”»é¢åƒäººä¸€æ ·æŽ¨ç€è´­ç‰©è½¦è´­ç‰©è´­ç‰©è½¦é‡Œæ”¾æ»¡äº†æ°´æžœ (Translation: Generate an image of two of these fox plush toys shopping in supermarket, pushing shopping carts like humans, with the carts filled with fruits.)"
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}