{
    "paper_title": "Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling",
    "authors": [
        "Jinghan Li",
        "Zhicheng Sun",
        "Fei Li",
        "Cao Sheng",
        "Jiazhong Yu",
        "Yadong Mu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions into long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with better scaling for inference computation. Code is available at https://github.com/Singularity0104/equilibrium-planner."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 4 0 4 4 1 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "CLOSED-LOOP LONG-HORIZON ROBOTIC PLANNING VIA EQUILIBRIUM SEQUENCE MODELING Jinghan Li1, Zhicheng Sun1, Fei Li2, Cao Sheng2, Jiazhong Yu2, Yadong Mu1 1Peking University, 2China Tower li.jh@stu.pku.edu.cn,{sunzc,myd}@pku.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "In the endeavor to make autonomous robots take actions, task planning is major challenge that requires translating high-level task descriptions into long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate self-refining scheme that iteratively refines draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in simple supervised learning fashion. Meanwhile, nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with better scaling for inference computation. Code is available at https:// github.com/Singularity0104/equilibrium-planner."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in large language models (LLMs) have spurred tremendous progress in robotic planning (Huang et al., 2022; Li et al., 2022; Singh et al., 2023; Driess et al., 2023; Ahn et al., 2023; Huang et al., 2023; Zhao et al., 2023; Hu et al., 2024). Based on their extensive world knowledge, LLM agents seem close to autonomously performing robotic tasks, such as in household scenarios. However, growing evidence shows that existing LLM agents struggle with task planning (Kaelbling & Lozano-Perez, 2011) that decomposes high-level task into mid-level actions. While this problem requires long-horizon planning as well as consideration of environmental feedback, LLMs are often limited by: (1) unidirectional dependency: due to autoregressive generation, previous tokens cannot attend to future tokens, resulting in limited ability to plan ahead (Wu et al., 2024a); (2) lack of error correction for existing outputs, unless with heavy system 2; (3) fixed forward process hindering the allocation of more inference computation to further improve planning performance. These inherent limitations of LLMs lead to inefficiency in the closed-loop long-horizon robotic planning. To address above challenges of LLM planners in closed-loop long-horizon planning, we advocate the approach of self-refinement (Welleck et al., 2023; Shinn et al., 2023; Kim et al., 2023; Madaan et al., 2023) that iteratively improves previously generated plan. The reasons behind are threefold: (1) bidirectional dependency: since the output is conditioned on previous draft plan, it can attend to all tokens in the plan (from an old version), thus improving its ability to plan ahead; (2) internal error correction which allows implicit self-correction in forward pass without an explicit, heavy system 2; (3) dynamic computation allocation by iterating through self-refinement process until convergence. However, such self-refining strategy imposes significant training difficulties because it requires backpropagation through infinite self-refining steps (Werbos, 1990). This may be seen as an extreme case that reflects some of the general challenges in teaching LLMs to plan and reason. Existing solutions include curating process supervision (Uesato et al., 2022; Lightman et al., 2024) or applying reinforcement learning (Zelikman et al., 2024; OpenAI, 2024; Kumar et al., 2024), but they are considerably more complex than supervised training and difficult to implement. Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "(a) Equilibrium in planning (b) Iterative planning until an equilibrium is reached Figure 1: Illustration of the equilibrium point in planning. We view planning as self-refinement process in which the ideal plan emerges as an equilibrium point, remaining unchanged by any refinement attempts even with newer information (e.g. feedback from the environment or world model). This enables us to tackle robotic planning from an optimization perspective around its equilibrium. This work proposes an simple learning framework for planning via self-refinement. Specifically, we formulate the self-refining process as fixed-point problem that recursively refines the plan until the equilibrium point, as illustrated in Fig. 1. While the forward process of this fixed-point problem could be solved efficiently using root-finding methods, more interestingly, its backpropagation can be skipped since its gradient is explicated by the implicit function theorem (Krantz & Parks, 2002) as in deep equilibrium models (Bai et al., 2019; Geng & Kolter, 2023). It is noted that the derived gradient term may be further simplified through Jacobian-free approximation (Fung et al., 2022) to facilitate training. These analytical techniques allow end-to-end supervised training of the LLM planner to accomplish self-refinement without the need for additional verifiers or reward models in reinforcement learning-based counterparts, greatly enhancing simplicity and practicality. And after training, our equilibrium model-based planner is capable of dynamically allocating more inference computation in the equilibrium solving process to achieve better planning performance. Another important cue for self-refinement in robotic tasks is closed-loop feedback from the environment. To efficiently incorporate environmental feedback, we devise nested equilibrium sequence modeling procedure consisting of inner and outer loops, where the inner loop iteratively refines plan using previous feedback, while the outer loop updates the feedback by interacting with the environment. This enables closed-loop planning from even few environmental interactions. Moreover, the nested equilibrium solving process is accelerated by reusing the previously derived equilibrium. We further implement the above design within an LLM agent framework, seamlessly integrating the equilibrium model-based planner, an experience memory buffer containing past plans and feedback, and world model to estimate feedback in the absence of environmental interactions, thus enabling it to operate effectively in robotic planning. The contributions of our work are summarize as follows: We present equilibrium sequence modeling, simple training approach for self-refining LLMs based on equilibrium models, allowing for end-to-end learning in supervised manner. nested equilibrium solving process is proposed to efficiently incorporate closed-loop feedback into the equilibrium sequence modeling, reusing previous equilibrium solutions to alleviate inference computation. It is further implemented with world model to improve practicality. Our method is evaluated on the VirtualHome-Env benchmark (Puig et al., 2018; Liao et al., 2019), demonstrating its advantageous performance with better scaling w.r.t. inference computation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "LLMs for Planning. Scaling up inference computation to improve LLMs performance on planning and reasoning tasks has received increasing attention (Brown et al., 2024; Snell et al., 2024; Wu et al., 2024b; OpenAI, 2024). Existing techniques involving chain-of-thought (Wei et al., 2022; Zelikman et al., 2022; 2024), repeated sampling (Wang et al., 2023; Brown et al., 2024) and tree search (Yao et al., 2023a; Zhao et al., 2023) showed preliminary results with handcraft system 2. Alternatively, method called self-refinement (Welleck et al., 2023; Shinn et al., 2023; Kim et al., 2023; Madaan et al., 2023) suggests recursively refining the existing LLM output in an autonomous manner, but it relies heavily on prompting or intricate training procedures. To fully exploit its potential, we propose an end-to-end optimization method for self-refinement through deep equilibrium models."
        },
        {
            "title": "Preprint",
            "content": "Deep Equilibrium Models (Bai et al., 2019) are infinite-depth neural networks specified by fixedpoint problems = fθ(x), where fθ is an equilibrium layer. While their inference can take infinite steps by the fixed-point iteration, their gradients are estimated using implicit differentiation (Krantz & Parks, 2002) without backpropagating through all layers, thus enabling memory-efficient training. They have been extensively applied to tasks such as visual understanding (Bai et al., 2020; 2022) and image generation (Pokle et al., 2022; Geng et al., 2023; Bai & Melas-Kyriazi, 2024). In this paper, we apply the fixed-point formulation of deep equilibrium models to the self-refinement process in LLM planners, allowing for simple supervised training of LLMs to refine themselves. More detailed introduction to deep equilibrium models is presented in Appendix A."
        },
        {
            "title": "3 METHOD",
            "content": "We study the problem of robot task planning which aims to decompose high-level task description into long-horizon mid-level action sequences (Kaelbling & Lozano-Perez, 2011). In the following, we first discuss the limitations of LLM planners in self-refinement (Section 3.1) and address them with novel equilibrium sequence modeling scheme (Section 3.2), which is compatible with various feedback from the designs in Section 3.3. Lastly, practical implementations are given in Section 3.4. 3.1 PRELIMINARIES ON SELF-REFINEMENT The prevailing LLMs are intrinsically limited in planning, as their unidirectional dependency results in limited capability to plan ahead (Wu et al., 2024a), and the lack of error correction hinders closedloop planning. These reasons call for alternative mechanisms to address robot task planning. Recently, Welleck et al. (2023); Shinn et al. (2023); Kim et al. (2023); Madaan et al. (2023) proposed self-refinement, which uses an LLM fθ to iteratively refine the previous LLM output. This strategy naturally addresses the above limitations, since it introduces bidirectional token dependency and dynamic error correction mechanism. Formally, let xt denote draft plan and ct denote context (e.g. environmental feedback), then planning may be viewed as self-refinement process as follows: xt+1 = fθ(xt, ct). However, self-refinement via prompting (Shinn et al., 2023; Kim et al., 2023; Madaan et al., 2023) has been found to be very limited by Huang et al. (2024). Alternative training-based methods require careful curation of training sequences (Welleck et al., 2023; Havrilla et al., 2024) or reinforcement learning (Qu et al., 2024; Kumar et al., 2024) and are therefore difficult to train. Overall, they remain deficient for robotic planning compared to system 2-based alternatives, as shown in Hu et al. (2024). (1) 3.2 SELF-REFINEMENT AS AN EQUILIBRIUM MODEL To address the training inefficiency of self-refinement approaches, this section proposes equilibrium sequence modeling, simple supervised training scheme for teaching LLM planners to self-refine through the lens of deep equilibrium models (Bai et al., 2019; Geng & Kolter, 2023). Let us first consider simplified scenario of self-refinement without environmental feedback, namely that the context ct is fixed, e.g. to predefined system message c. Then, the self-refinement process in Eq. (1) reduces to fixed-point problem concerning only the plan xt. Denote the initial plan by x0 = and the equilibrium plan, i.e. the endpoint, by x, then its trajectory can be expressed as: (x0, c) . . . (xt, c) . . . (x, c). Although its forward process is tractable with existing root-solving techniques, such as the classic fixed-point iteration or alternative numerical methods (Broyden, 1965; Anderson, 1965), its training requires recurrent backpropagation through multiple self-refining steps (Werbos, 1990). This results in an extremely inefficient and unstable computational process where end-to-end training fails. (2) Instead, we approach it directly from an analytical perspective. Assuming access only to an outcome supervision L(, y) on the plan, e.g. its distance to the ground truth plan y, then self-refinement is formulated as an optimization problem minimizing the loss function of the equilibrium plan: L(x, y) min θ s.t. = fθ(x, c). 3 (3)"
        },
        {
            "title": "Preprint",
            "content": "(a) Inference: solving equilibrium via fixed-point iteration (b) Training Figure 2: Illustration of equilibrium sequence modeling. Prior to training, our model first undergoes iterative inference to reach an equilibrium plan. It is then pushed away from the equilibrium towards ground truth by supervised training loss. This process enables self-refinement of the model. Interestingly, the above optimization problem can be solved without backpropagating over the entire inference process. As the following theorem indicates, we can directly differentiate through its fixed point regardless of the solution path, with only constant computational and memory cost. Theorem 1 (Implicit Function Theorem (Bai et al., 2019; Krantz & Parks, 2002)) Assuming that (cid:0)I fθ (cid:1) is invertible, then the loss gradient of Eq. (3) w.r.t. θ is given by: θ = x (cid:18) fθ (cid:19)1 fθ θ . (4) Its proof is given in Appendix A.4. It is noteworthy that the inverse Jacobian term = (I fθ )1 within the above gradient is difficult to compute exactly, for which existing work often approximates through the damped fixed-point unrolling or the Neumann series (Geng et al., 2021b). For computational efficiency, we drop the inverse Jacobian term using as in Fung et al. (2022); Geng et al. (2021a); Choe et al. (2023), the latter having been validated on Transformer-based LLMs: θ = x fθ θ . (5) Equilibrium Sequence Modeling. Based on the simplified gradient estimation, we reformulate its training into supervised learning problem. According to the chain rule, the derived gradient is exactly the gradient of the following optimization problem associated with the equilibrium x: min θ L(fθ(x, c), y). (6) This new formula represents new equilibrium sequence modeling scheme that can be implemented in two stages as in Fig. 2. In the first stage, we iterate over the fixed-point problem within Eq. (3) to solve the equilibrium plan x. Then, it is paired with the ground truth plan as training sequence, which is used as in the standard supervised finetuning pipeline to teach the LLM to self-refine. It features two intuitive advantages: (1) instead of directly regressing the ground truth, it only adjusts the equilibrium point, which reduces overfitting compared to the vanilla supervised finetuning; (2) it teaches self-refinement by simple supervised loss, without requiring additional value functions or reward models (Welleck et al., 2023; Havrilla et al., 2024; Qu et al., 2024; Kumar et al., 2024). 3.3 EQUILIBRIUM MODELS WITH FEEDBACK This section extends the derived equilibrium sequence modeling to more practical scenario where the environment may provide some closed-loop feedback, e.g. failure details, during plan execution. Such auxiliary information would be an effective cue for planners to further refine their plan."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Inference of Equilibrium Planner Require: planner fθ, environment or world model Env, number of iterations . Initialize start point x0 and feedback c0. for 0 to or converged do Solve inner equilibrium loop to obtain . Update next plan xt+1 and feedback ct+1 with Env. Ensure: generated plan x. Eq. (8) Eqs. (9) and (10) To take into account environmental feedback, we consider an adaptive context ct that is influenced by the plan xt rather than fixed. Then, the previously concerned equilibrium solving process of Eq. (2) should be revised as an iterative process that couples plan and feedback, starting from x0 = c0 = : (x0, c0) . . . (xt, ct) . . . (x, c). (7) After the modification, the existing derivations only hold when we neglect the derivatives related to c. Fortunately, this is natural choice due to the non-differentiability of most feedbacks. Therefore, the equilibrium planner can be trained in similar supervised way as in Eq. (6) and Fig. 2, and after training it would be able to self-refine based on the latest feedback just by forward passes. However, iteratively interacting with the environment to obtain feedback is costly and may not be recoverable. In response, we devise nested equilibrium solving scheme for more efficient closed-loop planning. Nested Equilibrium Solving. Inspired by the introspection process in human daily life, we propose to divide equilibrium solving into nested loop process. The inner loop introspects on the existing plan and feedback and takes no action, while the outer loop interacts with the environment to update the feedback. Formally, each inner loop is an equilibrium solving process with fixed feedback ct: = fθ(x0 x1 = fθ(x t , ct) , ct). (8) Thanks to this inner-loop introspection mechanism, our equilibrium model can be more efficient in closed-loop planning, achieving superior performance with fewer environmental interactions. Reusing Equilibrium Solution. Another efficiency bottleneck is the equilibrium solving. Considering that its speed depends largely on the initial plan, it is unnecessary to restart from every time. Therefore, we accelerate equilibrium solving by reusing the previously derived equilibrium plan as the starting point of the next iteration, similar to Bai et al. (2022); Bai & Melas-Kyriazi (2024): xt+1 = . (9) which corresponds to the starting point x0 t+1 of the inner loop. Similarly, history feedback could be reused across different inner loops. This is achieved by initializing the context of the next inner loop by concatenating the previous feedback with the latest feedback (paired with its original plan): where denotes concatenation. The whole nested inference procedure is described in Algorithm 1. ct+1 = (xt+1, Env(xt+1)) ct, (10) 3.4 PRACTICAL IMPLEMENTATION This section discusses the implementation of the proposed equilibrium planner. To enable effective training while interacting with the environment, the following two modules are carefully devised to complement the planner: an experience memory that caches all equilibrium plans and their feedback during equilibrium solving, and world model to estimate the closed-loop feedback in the absence of environmental interactions. The planning framework is illustrated in Fig. 3. Equilibrium Experience Memory. During the training process, our equilibrium model interacts with the environment only when the inner loop reaches the equilibrium point. This results in small number of equilibrium points, which may not be sufficient for supervised training. To improve our training efficiency and stability, we opt to cache all previously obtained equilibrium points, along with their environmental feedback, in an experience memory. Thereafter, these equilibrium points"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Illustration of our proposed framework. It incorporates (1) memory containing all the equilibrium experience during inference, (2) self-refining planner trained on equilibrium plans with the ground truth, and (3) world model trained on the memory to simulate environmental feedback. can be sampled repeatedly for versatile training purposes. For example, for the planner, we randomly sample batch of equilibrium points at each training epoch, which are paired with the ground truth for supervised training. In particular, the most recent equilibrium points are sampled more frequently to reduce distribution shift. Next, we describe another crucial component of our framework. Internal Feedback from World Model. Due to inefficiency of interacting with the environment, we construct world model (Ha & Schmidhuber, 2018) to provide the necessary feedback in closedloop planning. Our world model takes the environmental context, task instruction and current plan as inputs and predicts some basic types of feedback. This definition is slightly simpler than the commonly used world model, which requires simulation of the environmental states, and therefore may be easier to train. Concretely, we implement the world model with an LLM and finetune it on the planners equilibrium feedback over all iterations for better generalizability. And during inference, we alternate between using external and internal feedback in closed-loop robotic planning."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Benchmark. The VirtualHome-Env benhmark (Puig et al., 2018; Liao et al., 2019) is adopted during the experiments. It contains 1360 long-horizon tasks with ground truth action sequence annotations (average length 10.8) and provides updated scene graphs after each action, allowing simulation of closed-loop feedback. To analyze the generalizability, we divided the dataset into training set and three test subsets, including the novel scene set, the novel task set, and the novel scene and task set. More statistics and examples about VirtualHome-Env can be found in Appendix B.1. Metrics. We use executability (Exec.), success rate (SR), goal conditions recall (GCR) following Hu et al. (2024). Exec. evaluates whether the plan can be executed in given the environment, SR refers to whether the goal is accomplished, and GCR measures the proportion of goal conditions achieved. To examine the closed-loop planning capabilities, we study two test setups, without error correction or with up to 10 corrections, the latter allowing for self-correction based on environmental feedback. We also evaluate the computational efficiency by measuring TFLOPS at inference time. Baselines. Our method is compared with Tree-Planner (Hu et al., 2024), SELF-REFINE (Madaan et al., 2023), and supervised finetuned planner. They are all reproduced using Llama 3 8B (Dubey et al., 2024). In addition, we consider several baseline methods that call the GPT-3.5 API, including ProgPrompt (Singh et al., 2023), Zero-shot Planner (Huang et al., 2022) and two self-refining planners, Local Replan (Raman et al., 2022; Guo et al., 2023) and Global Replan (Shinn et al., 2023). Their results are presented for reference only. See Appendix B.2 for more details. Implementation Details. Our implementation is consistent with the baseline methods by finetuning from Llama 3 8B (Dubey et al., 2024) on the VirtualHome-Env training set (paired with the equilibrium points). The number of finetuning epochs is set to 6, and the learning rate is set to 0.0002. The world model is finetuned on all planner interactions for 5 epochs using the same learning rate. greedy LLM sampling strategy is used in later refinement steps until convergence. Moreover, we implement the KV cache to speed up inference. Further details are provided in Appendix B.3."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance on VirtualHome-Env without correction. Our planner achieves state-of-the-art performance in most evaluations. Note that the Exec. metrics are marked in gray because they are already high and can easily exceed 99% with simple automated rules (by truncating illegal output). Novel Scene and Task Novel Scene Novel Task Exec. SR GCR Exec. SR GCR Exec. SR GCR GPT-3.5 API: Zero-shot Planner ProgPrompt Iterative-Planner Tree-PlannerN=25 Tree-PlannerN=50 Finetuned Llama 3 8B: Supervised Tree-PlannerN=25 Tree-PlannerN=50 Ours 16.49 35.04 44.54 55.74 49.01 93.55 95.16 94.94 90.32 1.07 12.54 27.04 28.33 28.14 24.19 38.71 38.71 40.32 1.52 19.99 33.25 39.96 35.84 32.55 63.18 63.50 65. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 96.84 96.08 96.06 95.79 41.05 51.58 51.58 65.26 49.81 69.45 69.54 79.47 95.94 95.50 95.40 93.38 26.07 40.38 39.74 41.88 35.53 63.75 63.29 62. Table 2: Performance on VirtualHome-Env with up to 10 corrections. Our planner consistently leads in SR and GCR performance. The Exec. metrics are shown in gray for the same reasons in Table 1."
        },
        {
            "title": "Novel Task",
            "content": "Exec. SR GCR Exec. SR GCR Exec. SR"
        },
        {
            "title": "GCR",
            "content": "79.66 82.09 89.13 88.26 GPT-3.5 API: Local Replan Global Replan Tree-PlannerN=25 Tree-PlannerN=50 Finetuned Llama 3 8B: SELF-REFINE Tree-PlannerN=25 Tree-PlannerN=50 Ours 96.77 95.16 94.94 91.94 37.46 37.93 35.30 41.58 43.55 41.94 43.55 56.45 51.90 52.46 56.65 59. 65.18 56.49 58.91 76.63 - - - - - - - - - - - - - - - - - - - - - - - - 92.63 96.08 96.06 97.89 54.74 55.79 58.95 77.89 70.24 68.82 70.00 87.07 94.44 95.50 95.40 92.31 39.96 42.09 43.38 54. 62.37 57.83 59.79 74.18 4.2 MAIN RESULTS The experimental results in the two planning setups, without correction or with up to 10 corrections, are summarized in Tables 1 and 2. Overall, our method achieves the leading performance on the majority of metrics. Specifically, the experimental results show that: (1) Even without error correction, our self-refining process still brings significant improvement of 14% on SR in the novel scene subset, with other metrics superior or comparable to the previous leading method. (2) By incorporating environmental feedback, our approach improves all metrics by more than 11% and up to 19%, showing clear advantages. (3) Similar to the existing finetuning-based methods, our generated plans exhibit high executability of over 90%, which can be improved to 99% by simply truncating illegal overlength outputs. These results clearly confirm the advantages of our approach. Figure 4 further compares our self-correction process with the baseline methods. As can be seen, our method is better at incorporating environmental feedback to improve the plan, while the baselines fail by simply repeating the previous plan, or by making only local adjustments that are insufficient. This exemplifies the effectiveness of equilibrium sequence modeling for self-correction. 4.3 ABLATION STUDY Effectiveness of equilibrium sequence modeling is quantitatively verified in Table 2. Compared to alternative methods, our approach shows more than 11% improvement over the prompting-based"
        },
        {
            "title": "Preprint",
            "content": "Output log [WALK] <home_office> [WALK] <chair> [FIND] <chair> [GRAB] <chair> [WALK] <dining_room> Task fail, trackback [WALK] <bedroom> Tree traversal end, task fail Output log [WALK] <home_office> [WALK] <chair> [FIND] <chair> [GRAB] <chair> [WALK] <dining_room> Wrong relative pos: <chair> and <floor> <character> and <home_office> [WALK] <home_office> [WALK] <chair> [FIND] <chair> [GRAB] <chair> [WALK] <dining_room> Same plan, task fail Output log [WALK] <home_office> [WALK] <chair> [FIND] <chair> [GRAB] <chair> [PUTBACK] <chair> <home_office> Wrong relative pos: <chair> and <floor> [WALK] <home_office> [WALK] <chair> [FIND] <chair> [GRAB] <chair> [WALK] <floor> [PUTBACK] <chair> <floor> Task success (a) SELF-REFINE (b) Tree-Planner (c) Ours Figure 4: Visualization of our self-correction process compared to the baselines SELF-REFINE and Tree-Planner. The task instruction is Take comfortable chair and place it in the entrance hall. Table 3: Effectiveness of different types of feedback. They are measured under the constraint of up to 10 rounds of internal or external feedback. The last two rows are reported in the main tables. Novel Scene and Task Novel Scene Novel Task World model Env. Exec. SR GCR Exec. SR GCR Exec. SR 88.71 83.87 90.32 91.94 33.87 51.61 40.32 56.45 59.98 75.13 65.40 76. 96.79 96.84 95.79 97.89 49.47 75.79 65.26 77.89 66.60 85.79 79.47 87.07 93.80 92.31 93.38 92.31 34.62 56.62 41.88 54.91 GCR 59.06 75.53 62.76 74.18 self-refinement method SELF-REFINE (Madaan et al., 2023) and more than 12% improvement over the system 2-based Tree-Planner (Hu et al., 2024). Meanwhile, we maintain simplistic supervised finetuning fashion similar to the compared methods, without intricate reinforcement learning. This validates the effectiveness of equilibrium sequence modeling in our robot task planning problem. Effectiveness of various feedback. As can be observed in Table 3, incorporating external feedback from the environment or internal feedback from the world model consistently improves performance. Even though the world model does not provide as much improvement as the real environment, it also increases performance by over 3%. In particular, the synergy of both types of feedback yields the highest performance on most of the metrics, further confirming their effectiveness. In the following analysis, we will focus on our method using only environmental feedback for simplicity. Scaling of performance. Here, we follow Brown et al. (2024); Snell et al. (2024); Wu et al. (2024b); OpenAI (2024) in considering the scaling w.r.t. inference computation. The results in Fig. 5a show that our method achieves better performance-computation tradeoff along with leading scaling w.r.t. inference computation. Thus, more inference budget can be allocated to improve its performance. Furthermore, in Fig. 5b, we show that its performance advantage is largely due to better long-horizon planning capabilities, achieving more than twice the success rate for extremely long plans. Computational efficiency. Although our training is slower than the baselines (36h vs. 12h) due to the equilibrium solving process for synthesizing training pairs (24h), it exhibits competitive inference efficiency. For example, our method takes 16h to evaluate, while Tree-Planner takes 24h."
        },
        {
            "title": "Preprint",
            "content": "(a) Scaling w.r.t. inference computation (b) Performance for long-horizon tasks Figure 5: Performance analysis vs. inference computation and plan length. Our method shows leading scaling w.r.t. inference computation and long-horizon planning capabilities. The inference computation is measured by TFLOPS, and we consider KV cache when computing inference TFLOPS. (a) Effectiveness of reusing equilibrium (b) Dynamic computation allocation Figure 6: Ablation study on computational efficiency. Our nested equilibrium solving process reuses previous equilibrium solutions and dynamically allocates the inference budget for better efficiency. The latter is displayed as the number of fixed-point iterations with mean and standard deviation. This can be attributed to our design of reusing equilibrium in nested equilibrium solving, As illustrates in Fig. 6a, it accelerates the convergence of feedback iterations, achieving better performance (>55%) with significantly fewer interactions. Furthermore, we show in Fig. 6b that our method is able to dynamically allocate inference computation for planning tasks of different complexity."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This work proposes an equilibrium model-based LLM planner that is capable of self-refining plans from external and internal feedback. Unlike existing self-refinement methods based on prompting or sophisticated reinforcement learning, our proposed equilibrium sequence modeling allows simple supervised training of the self-refining planners. Moreover, it also enables the planner to efficiently incorporate environmental feedback or world model for closed-loop planning. We implement the proposed approach on the VirtualHome-Env benchmark, and the experimental results suggest that it can dynamically allocate inference computation to achieve state-of-the-art planning performance. Code: https://github.com/Singularity0104/equilibrium-planner. Acknowledgements. We thank Xingjian Bai and Liyuan Wang for their helpful discussions."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: In Conference on Robot Learning, pp. 287318, Grounding language in robotic affordances. 2023. Donald Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM, 12 (4):547560, 1965. Shaojie Bai, Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems, pp. 690701, 2019. Shaojie Bai, Vladlen Koltun, and Zico Kolter. Multiscale deep equilibrium models. In Advances in Neural Information Processing Systems, pp. 52385250, 2020. Shaojie Bai, Zhengyang Geng, Yash Savani, and Zico Kolter. Deep equilibrium optical flow estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 620630, 2022. Xingjian Bai and Luke Melas-Kyriazi. Fixed point diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94309440, 2024. Stefan Banach. Sur les operations dans les ensembles abstraits et leur application aux equations integrales. Fundamenta Mathematicae, 3(1):133181, 1922. Jerome Bolte, Edouard Pauwels, and Samuel Vaiter. One-step differentiation of iterative algorithms. In Advances in Neural Information Processing Systems, pp. 7708977103, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. https://openai.com/research/video-generation-models-as-worldsimulators, 2024. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Charles Broyden. class of methods for solving nonlinear simultaneous equations. Mathematics of Computation, 19(92):577593, 1965. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. In International Conference on Machine Learning, pp. 52095235, 2024. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, pp. 65726583, 2018. Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, Willie Neiswanger, Pengtao Xie, Emma Strubell, and Eric Xing. Making scalable meta learning practical. In Advances in Neural Information Processing Systems, pp. 2627126290, 2023. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. In International Conference on Machine Learning, pp. 84698488, 2023. Yilun Du, Sherry Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, Leslie Pack Kaelbling, et al. Video language planning. In International Conference on Learning Representations, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024."
        },
        {
            "title": "Preprint",
            "content": "Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Tsai. Implicit deep learning. SIAM Journal on Mathematics of Data Science, 3(3):930958, 2021. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pp. 11261135, 2017. Samy Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and Wotao Yin. JFB: Jacobian-free backpropagation for implicit networks. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 66486656, 2022. Garima, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. In Advances in Neural Information Processing Systems, pp. 19920 19930, 2020. Zhengyang Geng and Zico Kolter. TorchDEQ: library for deep equilibrium models. arXiv preprint arXiv:2310.18605, 2023. Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke Wei, and Zhouchen Lin. Is attention In International Conference on Learning Representations, better than matrix decomposition? 2021a. Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. In Advances in Neural Information Processing Systems, pp. 2424724260, 2021b. Zhengyang Geng, Ashwini Pokle, and Zico Kolter. One-step diffusion distillation via deep equilibrium models. In Advances in Neural Information Processing Systems, pp. 4191441931, 2023. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pretrained large language models to construct and utilize world models for model-based task planning. In Advances in Neural Information Processing Systems, pp. 7908179094, 2023. Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Zheyuan Jiang, and Jianyu Chen. DoReMi: Grounding language model by detecting and recovering from plan-execution misalignment. arXiv preprint arXiv:2307.00329, 2023. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Alexander Havrilla, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. GLoRe: When, where, and how In International Conference on to improve LLM reasoning via global and local refinements. Machine Learning, pp. 1771917733, 2024. Mengkang Hu, Yao Mu, Xinmiao Chelsey Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large language models. In International Conference on Learning Representations, 2024. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, In International and Denny Zhou. Large language models cannot self-correct reasoning yet. Conference on Learning Representations, 2024. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 91189147, 2022. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, pp. 17691782, 2023. Leslie Pack Kaelbling and Tomas Lozano-Perez. Hierarchical task and motion planning in the now. In IEEE International Conference on Robotics and Automation, pp. 14701477, 2011. Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. In Advances in Neural Information Processing Systems, pp. 6696 6707, 2020."
        },
        {
            "title": "Preprint",
            "content": "Patrick Kidger, James Foster, Xuechen Li, and Terry Lyons. Neural SDEs as infinite-dimensional GANs. In International Conference on Machine Learning, pp. 54535463, 2021. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. In Advances in Neural Information Processing Systems, pp. 3964839677, 2023. Taewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, and Jonghyun Choi. ReALFRED: An embodied instruction following benchmark in photo-realistic environments. In European Conference on Computer Vision, 2024. Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. CLLMs: Consistency large language models. In International Conference on Machine Learning, pp. 2542625440, 2024. Steven George Krantz and Harold Parks. The Implicit Function Theorem: History, Theory, and Applications. Birkhauser, 2002. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, , et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyurek, Anima Anandkumar, et al. Pre-trained language models for interactive decisionmaking. In Advances in Neural Information Processing Systems, pp. 3119931212, 2022. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE International Conference on Robotics and Automation, pp. 94939500, 2023. Yuan-Hong Liao, Xavier Puig, Marko Boben, Antonio Torralba, and Sanja Fidler. Synthesizing environment-aware activities via activity sketches. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62916299, 2019. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In International Conference on Learning Representations, 2024. Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. LLM+P: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023. Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In International Conference on Learning Representations, 2019. Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning of continuous regularization hyperparameters. In International Conference on Machine Learning, pp. 29522960, 2016. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. SELF-REFINE: Iterative refineIn Advances in Neural Information Processing Systems, pp. 46534 ment with self-feedback. 46594, 2023. Drew McDermott. The 1998 AI planning systems competition. AI Magazine, 21(2):3535, 2000. OpenAI. Learning to reason with LLMs. https://openai.com/index/learning-toreason-with-llms/, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Ashwini Pokle, Zhengyang Geng, and Zico Kolter. Deep equilibrium approaches to diffusion models. In Advances in Neural Information Processing Systems, pp. 3797537990, 2022."
        },
        {
            "title": "Preprint",
            "content": "Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. VirtualHome: Simulating household activities via programs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84948502, 2018. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: challenge for social perception and human-AI collaboration. In International Conference on Learning Representations, 2021. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024. Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. In Advances in Neural Information Processing Systems Workshops, 2022. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. SayPlan: Grounding large language models using 3d scene graphs for scalable robot task planning. In Conference on Robot Learning, pp. 2372, 2023. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 1233612355, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, pp. 86348652, 2023. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1074010749, 2020. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. In IEEE International Conference on Robotics and Automation, pp. 1152311530, 2023. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. AdaPlanner: Adaptive planning from feedback with language models. In Advances in Neural Information Processing Systems, pp. 5820258245, 2023. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In Advances in Neural Information Processing Systems, pp. 38975 38987, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In Advances in Neural InforŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. mation Processing Systems, pp. 60006010, 2017. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations, 2023."
        },
        {
            "title": "Preprint",
            "content": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, pp. 2482424837, 2022. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In International Conference on Learning Representations, 2023. Paul Werbos. Backpropagation through time: What it does and how to do it. Proceedings of the IEEE, 78(10):15501560, 1990. Wilson Wu, John Morris, and Lionel Levine. Do language models plan ahead for future tokens? In Conference on Language Modeling, 2024a. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024b. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. TravelPlanner: benchmark for real-world planning with language agents. In International Conference on Machine Learning, pp. 5459054613, 2024. Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In International Conference on Learning Representations, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, pp. 1180911822, 2023a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2023b. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Self-taught reasoner bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, pp. 1547615488, 2022. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-STaR: Language models can teach themselves to think before speaking. In Conference on Language Modeling, 2024. Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. In Advances in Neural Information Processing Systems, pp. 31967 31987, 2023. Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, and Yuke Zhu. Hierarchical planning for longhorizon manipulation with geometric and symbolic scene graphs. In IEEE International Conference on Robotics and Automation, pp. 65416548, 2021."
        },
        {
            "title": "A BACKGROUND",
            "content": "A.1 DEEP EQUILIBRIUM MODELS Traditional neural networks are constructed by explicitly stacking layers (i), which can be limited in their expressiveness due to the fixed number of layers and predetermined forward process. Instead, implicit models are defined by an underlying dynamic system to be solved, such as an ordinary differential equation (Chen et al., 2018), controlled differential equation (Kidger et al., 2020), stochastic differential equation (Kidger et al., 2021) or fixed-point problem (Bai et al., 2019). Deep equilibrium models, first introduced in Bai et al. (2019), are representative class of implicit models characterized by fixed-point problems. Given an input and function fθ(x, c) such as Transformer block (Bai et al., 2019) or Transformer (Geng et al., 2023), deep equilibrium models define infinite-level stacking of this function xi+1 = fθ(xi, c) with = 0, 1, . . . , and by solving the solution to the following fixed-point equation defined by fθ and c: = fθ(x, c) (11) The forward pass of deep equilibrium models is root solving for the fixed-point problem. common choice is the fixed-point iteration method, which starts from an initial guess x0 and iteratively applies the transformation xt+1 = fθ(xt, c) until convergence. sufficient condition for its convergence is if fθ is contraction mapping w.r.t. x, namely its Lipschitz constant is less than one (Banach, 1922), which could be relaxed by the well-posedness condition in El Ghaoui et al. (2021). More advanced root solvers include Broydens method (Broyden, 1965) or Anderson acceleration (Anderson, 1965). A.2 TRAINING DEEP EQUILIBRIUM MODELS Unlike traditional neural networks, whose gradient requires backpropagation through time (Werbos, 1990) at high memory and computational cost, the gradient of deep equilibrium models is computed analytically without differentiating over its forward pass. Given an equilibrium point = fθ(x, c) and loss function L(x, y), the loss gradient w.r.t. the model parameters θ is provided by the implicit function theorem (Krantz & Parks, 2002; Bai et al., 2019) as follows: θ = x (cid:18) fθ (cid:19)1 fθ θ . (12) Its proof is given in Appendix A.4. Due to the challenge of exactly computing the inverse Jacobian term = (I fθ )1 in the above gradient, existing work often approximate it via the damped fixed-point unrolling or the Neumann series (Geng et al., 2021b). Recently, Fung et al. (2022); Geng et al. (2021a) propose to drop the inverse Jacobian term using I, with the former justifying it in Theorem 3.1 with strong assumptions. In practice, this approximation has been used extensively in one-step gradient (Bolte et al., 2023; Luketina et al., 2016; Finn et al., 2017; Liu et al., 2019; Garima et al., 2020) and shown to be effective on Transformer-based LLMs (Choe et al., 2023). A.3 TRANSFORMER-BASED DEEP EQUILIBRIUM MODELS Deep equilibrium models are initially proposed on Transformer architecture (Vaswani et al., 2017) for language modeling tasks (Bai et al., 2019). This seminal work considers Transformer block as the basic unit fθ in the equilibrium model. Then, Geng et al. (2021a) investigates improvements over the Transformer block by replacing self-attention with matrix decomposition. They also introduce one-step gradient based on the approximation of for efficiency and stability, assuming that the Lipschitz condition apply to large number of matrix decomposition methods. Recently, following the prevalence of Diffusion Transformers (Peebles & Xie, 2023), deep equilibrium models are extended to image generation tasks. Geng et al. (2023) propose generative equilibrium Transformers consisting of two modules, one using Transformer as the basic unit fθ in the equilibrium model. Their method yields advanced one-step image generation results. (Bai & MelasKyriazi, 2024) replace most of the intermediate Transformer blocks with an equilibrium model, thus significantly reducing the number of parameters and memory usage for training and inference."
        },
        {
            "title": "Preprint",
            "content": "A.4 PROOF OF IMPLICIT FUNCTION THEOREM Theorem 1 (Implicit Function Theorem (Bai et al., 2019; Krantz & Parks, 2002)) Let : Rn Rn be differentiable loss function, and let fθ : Rn Rp Rn be differentiable function parameterized by θ Rq. Consider the following optimization problem: L(x, y) min θ s.t. = fθ(x, c). (13) where x, Rn, and Rp. If (cid:0)I fθ (cid:1) is invertible, then the loss gradient w.r.t. θ is given by: θ = x (cid:18) fθ (cid:19)1 fθ θ . (14) Proof of Theorem 1. To derive the loss gradient w.r.t. θ, we begin by differentiating the equilibrium condition = fθ(x, c) with respect to θ. Applying the chain rule, we have: θ x (cid:1) is invertible, we can rearrange the above equation and solve for θ : Given that (cid:0)I fθ x θ θ (15) = + . The chain rule implies θ = x (cid:18) = θ θ . Substituting the expression for x (cid:19)1 fθ θ fθ . θ , we obtain: θ = x (cid:18) fθ (cid:19)1 fθ θ . (16) (17)"
        },
        {
            "title": "B EXPERIMENTAL SETTINGS",
            "content": "B.1 BENCHMARK Environment. We adopt the robotic planning benchmark VirtualHome-Env (Liao et al., 2019) based on VirtualHome (Puig et al., 2018). It consists of complex set of 292 planning tasks in 7 different indoor scenes, provided with 1360 mid-level action trajectories as ground truth annotations. These action trajectories are typically very long, with an average execution length of 10.8, highlighting its long-horizon characteristic. Moreover, the VirtualHome environment provides detailed feedback after performing each mid-level action, making it an ideal testbed for closed-loop planning. Figure 7 visualizes few examples sampled from the VirtualHome-Env benchmark. In each example, the planner is placed in an environment that spans few indoor rooms and is given detailed description of the environment. The description is originally in the form of scene graph with objects as nodes and spatial relationships as edges, but for simplicity we present the planner with only the object nodes. The planner is then asked to generate semantic action sequence based on short task description. For instance, after receiving an instruction turn on TV . . . , the robot agent must first walk to the table and grab the remote control, and then point at the TV to turn it on. As can be seen, these planning tasks usually involve rather complex scene setup, and the ground truth action sequences are quite long. We provide more detailed statistics for this benchmark in Fig. 8. Compared to alternative embodied planning benchmarks, VirtualHome-Env features long time horizons as well as more diverse closed-loop feedback. For example, ALFRED (Shridhar et al., 2020) and ReALFRED (Kim et al., 2024) are two common embodied instruction following benchmarks, but their plan lengths are relatively short and can be determined by few templates, making them unsuitable for long-horizon planning. PlanBench (Valmeekam et al., 2023) and TravelPlanner (Xie et al., 2024) are recent benchmarks designed specifically for LLM planning, but they do not provide"
        },
        {
            "title": "Preprint",
            "content": "Task description: Bring me red cookbook Ground truth: [WALK] <home_office> [WALK] <bookshelf> [FIND] <novel> [GRAB] <novel> [WALK] <table> [PUTBACK] <novel> <table> Task description: Bring dirty plate to sink Task description: Turn on TV with remote Ground truth: [WALK] <dining_room> [WALK] <table> [FIND] <table> [TURNTO] <table> [FIND] <plate> [GRAB] <plate> [WALK] <dining_room> [WALK] <sink> [FIND] <sink> [PUTBACK] <plate> <sink> Ground truth: [WALK] <home_office> [WALK] <table> [FIND] <remote_control> [GRAB] <remote_control> [FIND] <television> [TURNTO] <television> [POINTAT] <television> [SWITCHON] <television> [PUTOBJBACK] <remote_control> (a) (b) (c) Figure 7: Examples in VirtualHome-Env (Puig et al., 2018; Liao et al., 2019). The planner is given detailed description of the environment (specifically, the objects within each rooms), short task instruction, and is asked to output sequence of mid-level actions associated with the correct objects. (a) Distribution of plan length (b) Distribution of scene graph size Figure 8: Detailed statistics of VirtualHome-Env (Puig et al., 2018; Liao et al., 2019). The benchmark features (a) large set of long-horizon plans with an average length of 10.8, and (b) 7 complex scenes containing more than 280 objects and more than 400 valid relations. For the sake of clarity, we have excluded the CLOSE and FACING relations, which are redundant for most planning tasks. closed-loop feedback during execution, which is an essential element of robotic planning. Therefore, we adopt VirtualHome-Env during the experiments, with further details presented below. Action. The VirtualHome environment (Puig et al., 2018) originally supported animating 12 atomic actions based on the Unity simulator, with the followup work VirtualHome-Env (Liao et al., 2019) adding support for more actions using graph simulator. It currently supports 40 atomic actions, in which 21 actions can be animated through Unity. Each action is defined by an action name and some object arguments, and is implemented by prewritten code executors. In our experiments, we use full set of 40 actions included in the VirtualHome-Env dataset, summarized as follows:"
        },
        {
            "title": "Preprint",
            "content": "1. Actions without object association: SLEEP, STANDUP, WAKEUP. 2. Actions associated with one object: WALK, FIND, GRAB, WASH, WIPE, PULL, PUSH, POUR, TURNTO, POINTAT, WATCH, TOUCH, OPEN, CLOSE, RUN, SIT, READ, PUTON, PUTOFF, DROP, LIE, SWITCHON, SWITCHOFF, DRINK, LOOKAT, TYPE, CUT, PUTOBJBACK, EAT, RINSE, PLUGIN, PLUGOUT, GREET, SCRUB, SQUEEZE. 3. Actions associated with two objects: PUTIN, PUTBACK. Feedback. Because the environment includes graph simulator of the scene graph, it can respond quickly to actions, e.g. changing object attributes, and provide the updated scene graph at each step. In our experiments, we curate several types of closed-loop feedback based on these scene graphs, simulating coarse feedback that may be received in real-world situations. Specifically, we consider the following four categories of environmental feedback associated with task failure: 1. Program format feedback:Your output does not conform to the required format, meaning that the generated action sequence does not conform to the required format. 2. Invalid command feedback: Your output has an invalid command: ..., meaning that the generated action sequence has an illegal command line. 3. Execution feedback: Your output is executed incorrectly in the environment., meaning that the generated action sequence cannot be executed in the environment. 4. Task completion feedback: You have not completed this task. The following objects and corresponding states do not meet the goals: ... The following objects have wrong relative position: ..., meaning that the generated action sequence cannot complete the task, with more details about the task failure. Dataset Split. We randomly divide the VirtualHome-Env dataset into training set and test set in 50:50 ratio. To analyze the generalizability of our method, we mainly study the following three subsets of the test set: novel scene set, novel task set, and novel scene and task set. For instance, the novel scene set consists of seen planning tasks on unseen scenes. Overall, the dataset contains 735 training trajectories, 468 trajectories within the novel task set, 95 trajectories within the novel scene set, 62 trajectories within the novel scene and task set. Our models are first trained on the training set for fixed number of epochs and then evaluated on the three test subsets above. B.2 BASELINES Our method is mainly compared with Tree-Planner (Hu et al., 2024) and SELF-REFINE (Madaan et al., 2023), both reproduced using Llama 3 8B Instruct (Dubey et al., 2024) in line with ours. The former traverses an action tree that is built by repeated plan sampling, while the latter relies on selfrefinement. To reproduce them, we perform supervised finetuning of Llama 3 on the training split of VirtualHome-Env for the same number of epochs as our method, and then follow their original procedures for inference. For instance, Tree-Planner is reproduced with both settings {25, 50} in action tree construction. The system prompts they use are similar to ours in Fig. 9. We also report the results summarized by Hu et al. (2024) for reference. They additionally considered ProgPrompt (Singh et al., 2023), Zero-shot Planner (Huang et al., 2022) and two self-refinement planners, Local Replan (Raman et al., 2022; Guo et al., 2023) and Global Replan (Shinn et al., 2023). Since these baselines were implemented by calling the GPT-3.5 API instead of finetuning Llama 3, we report them in the novel scene and task track for relatively fair comparison. It is worth noting that they adopted smaller subset of actions and feedback, and differed in the curation of partial observations of the environment. Therefore, their results are for reference only. There are several robotic planning baselines that we have not compared due to large environmental differences. For example, LLM-MCTS (Zhao et al., 2023) is representative tree-search (Yao et al., 2023a) based planner. It followed Watch-and-help (Puig et al., 2021) to generate dataset of simple embodied tasks (mostly object rearrangement tasks), while our work considers more complex set of planning tasks. Alternative planners based on symbolic scene graph (Zhu et al., 2021; Rana et al., 2023), code (Liang et al., 2023; Sun et al., 2023), or PDDL (McDermott, 2000; Liu et al., 2023; Guan et al., 2023) are less flexible and difficult to implement in our environment."
        },
        {
            "title": "Preprint",
            "content": "You need to act as task planner, who first draft an initial sub-task sequence and then refine it in the next few iterations. When the the draft sub-task sequence is Null, you should output the initial sub-task sequence. When the the draft sub-task sequence is not Null, You should refine it based on the the draft subtask sequence. If you have previously generated some action sequences and tried to execute them in the environment, their feedback will be provided to you for reference. Each sub-task can be one of the following form: 1. [action_name]; 2. [action_name] <object name 1> ( object id 1); 3. [action_name] <object name 1> (object id 1) <object name 2> (object id 2). The (object id) is used to tell the simulator which object the action should act on. The number of arguments depends on the action type. For action type 1, the available actions are: SLEEP, STANDUP, WAKEUP For action type 2, the available actions are: WALK, FIND, GRAB, WASH, WIPE, PULL, PUSH, POUR, TURNTO , POINTAT, WATCH, TOUCH, OPEN, CLOSE, RUN, SIT, READ, PUTON, PUTOFF, DROP, LIE, SWITCHON, SWITCHOFF, DRINK, LOOKAT, TYPE, CUT, PUTOBJBACK, EAT, RINSE, PLUGIN, PLUGOUT, GREET, SCRUB, SQUEEZE For action type 3, the available actions are: PUTIN, PUTBACK All action_name of the sub-tasks must be chosen from the above actions. You should output the sub-task sequence in succinct form. You must output END after you have output the entire sub-task sequence. Task name: Grab some juice Instructions: go to the fridge, and grab some juice out of it. then get glass, and pour the juice into the glass. There are 4 rooms, and you are an embodied character with ID 198 in bedroom with ID 199. The objects in each room is as follows: Room name: home_office Room ID: 1 Object ID and name in this room: 28 hanger 73 mat ...... Room name: dining_room Room ID: 100 Object ID and name in this room: 116 ceiling 2005 food_food ...... Feedbacks from past executions: Action sequence: [WALK] <dining_room> (100) [WALK] <cupboard> (132) [FIND] <cupboard> (132) [OPEN] <cupboard> (132) [FIND] <cup> (1000) [GRAB] <cup> (1000) [CLOSE] <cupboard> (132) [WALK] <freezer> (141) [OPEN] <freezer> (141) [FIND] <juice> (1001) [GRAB] <juice> (1001) [POUR] <juice> (1001) <cup> (1000) [PUTOBJBACK] <juice> (1001) [CLOSE] <freezer> (141) [END] Feedback: You have not completed this task. The following objects have wrong relative position: (1000, cup) and (128, table). The draft sub-task sequence: [WALK] <dining_room> (100) [WALK] <cupboard> (132) [FIND] <cupboard> (132) [OPEN] <cupboard> (132) [FIND] <cup> (1000) [GRAB] <cup> (1000) [CLOSE] <cupboard> (132) [WALK] <freezer> (141) [OPEN] <freezer> (141) [FIND] <juice> (1001) [GRAB] <juice> (1001) [POUR] <juice> (1001) <cup> (1000) [PUTOBJBACK] <juice> (1001) [CLOSE] <freezer> (141) [END]"
        },
        {
            "title": "Task",
            "content": "Env Feedback ct Draft Plan xt Figure 9: Example of the prompt used by our equilibrium planner. B.3 IMPLEMENTATION DETAILS Prompt. Our approach involves two LLMs, one LLM as the equilibrium planner and an additional LLM as an optional world model. The planners prompt is illustrated in Fig. 9. As can be seen, it consists of system prompt, task definition, environment description, history feedback, and draft plan."
        },
        {
            "title": "Preprint",
            "content": "Notably, the system prompt is modified from Hu et al. (2024), and the environment section describes the initial environment sorted by rooms, including the object names with their IDs within each room. For the additional world model, we adopt similar prompt, except that it receives more information about the initial environment, including edges in the scene graph that correspond to spatial relations The world model is then asked to predict the environmental feedback given generated plan. Finetuning. Both our equilibrium planner and the world model are finetuned in supervised manner. The equilibrium planner is finetuned for 6 iterations with learning rate of 0.0002. At each iteration, we curate training data by pairing fixed points and ground truths, where the fixed points are sampled from an equilibrium memory of past iterations. To prevent overfitting to the history equilibrium, decay ratio of 0.5 is used when sampling from the fixed points of previous iterations. Thereafter, we update the model parameters using gradient descent according to Eq. (6) for one epoch per iteration. For the world model, we collect all interacting experiences between the planner and the environment, including plans and feedback, and finetune it for 5 epochs using the same learning rate of 0.0002. Inference. The inference procedure of our planner was described in Algorithm 1, which involves nested equilibrium solving process. Given the environment and the task instruction, we initialize the draft plan x0 and the feedback c0 as null and iterate through nested loop. Each inner loop reuses the feedback from the outer loop to self-refine the draft plan, and after it converges, we update the feedback by interacting with the environment or world model. This process continues until it reaches an upper bound on the outer loop, which we set to 10 to match Tree-Planner (Hu et al., 2024). We adopt greedy sampling for the LLM, except that the first refinement step uses top-k sampling with = 10 for higher diversity. Since most of the prompt remains unchanged during equilibrium solving, we implement KV cache to accelerate the inference process, which can be further improved with parallel decoding techniques (Santilli et al., 2023; Cai et al., 2024; Kou et al., 2024)."
        },
        {
            "title": "C ADDITIONAL RESULTS",
            "content": "To further illustrate the effectiveness of equilibrium sequence modeling in closed-loop planning, we additionally exemplify our self-correction process on long-horizon robotic planning task in Fig. 10. Compared to SELF-REFINE (Madaan et al., 2023) and Tree-Planner (Hu et al., 2024), our approach is more competent in revising long plan through few forward passes without additional system 2. This is attributed to our efficient training scheme for teaching planners to self-refine. We also compare different types of feedback utilized by our planner in Fig. 11. As can be observed, internal feedback alone cannot enable successful replanning, but it can reduce environmental interactions prior to convergence. This confirms the effectiveness of both internal and external feedback in closed-loop planning and thus justifies the framework design of our equilibrium planner."
        },
        {
            "title": "D LIMITATIONS",
            "content": "While our equilibrium sequence modeling improves the planning capability of LLMs, we identify the following failure scenarios during the experiments: (1) hallucination of the equilibrium planner and the world model as in vanilla LLMs; (2) lack of awareness of history context such as previously grabbed objects. The latter may be resolved with reasoning techniques as in Yao et al. (2023b). In broader sense, our method may be limited in generalizing to new domains because it requires the ground truth and environmental feedback during training. These procedures with the equilibrium solving process results in lower training efficiency. Also, the current formulation only considers the explicit output plan without implicit reasoning steps. Furthermore, our model has only text input and no visual input, which limits its applicability in the real world. This can be resolved by introducing video-based planners (Du et al., 2024) and world models (Yang et al., 2024; Brooks et al., 2024)."
        },
        {
            "title": "Preprint",
            "content": "Output log [WALK] <bedroom> [WALK] <toy1> [FIND] <toy1> [GRAB] <toy1> [FIND] <toy2> [GRAB] <toy2> [FIND] <toy3> [GRAB] <toy3> [FIND] <desk> [PUTBACK] <toy1> <desk> [PUTBACK] <toy2> <desk> [PUTBACK] <toy3> <desk> Wrong relative pos: <toy1> and <filing_cabinet> <toy2> and <filing_cabinet> <toy3> and <filing_cabinet> [WALK] <bedroom> [WALK] <toy1> [FIND] <toy1> [GRAB] <toy1> [FIND] <toy2> [GRAB] <toy2> [FIND] <toy3> [GRAB] <toy3> [FIND] <desk> [PUTBACK] <toy1> <desk> [PUTBACK] <toy2> <desk> [PUTBACK] <toy3> <desk> [FIND] <bookshelf> [PUTBACK] <toy1> <bookshelf> [PUTBACK] <toy2> <bookshelf> [PUTBACK] <toy3> <bookshelf> Wrong relative pos: <toy1> and <filing_cabinet> <toy2> and <filing_cabinet> <toy3> and <filing_cabinet> ... Same plan, task fail Output log [WALK] <bedroom> [WALK] <toy1> [FIND] <toy1> [GRAB] <toy1> [FIND] <toy2> [GRAB] <toy2> Action fail, trackback [WALK] <dresser1> [OPEN] <dresser1> [PUTIN] <toy1> <dresser1> [CLOSE] <dresser1> [FIND] <toy2> [GRAB] <toy2> Task fail, trackback [WALK] <desk> [PUTBACK] <toy1> <desk> Task fail, trackback [WALK] <dresser2> [OPEN] <dresser2> [PUTIN] <toy1> <dresser2> Tree traversal end, task fail Output log [WALK] <bedroom> [WALK] <toy1> [FIND] <toy1> [GRAB] <toy1> [WALK] <table> [PUTBACK] <toy1> <table> [WALK] <toy2> [FIND] <toy2> [GRAB] <toy2> [WALK] <table> [PUTBACK] <toy2> <table> [WALK] <toy3> [FIND] <toy3> [GRAB] <toy3> [WALK] <table> [PUTBACK] <toy3> <table> Wrong relative pos: <toy1> and <filing_cabinet> <toy2> and <filing_cabinet> <toy3> and <filing_cabinet> [WALK] <bedroom> [WALK] <toy1> [FIND] <toy1> [GRAB] <toy1> [WALK] <filing_cabinet> [OPEN] <filing_cabinet> [PUTBACK] <toy1> <filing_cabinet> [FIND] <toy2> [GRAB] <toy2> [WALK] <filing_cabinet> [PUTBACK] <toy2> <filing_cabinet> [FIND] <toy3> [GRAB] <toy3> [WALK] <filing_cabinet> [PUTBACK] <toy3> <filing_cabinet> [CLOSE] <filing_cabinet> Task success (a) SELF-REFINE (b) Tree-Planner (c) Ours Figure 10: Visualization of our self-correction process in comparison with baselines. This example uses only environmental feedback, and we include toy IDs in the presentation for clarity. The task instruction is Pick up all the toys on the floor and put them in their correct storage bin or shelf."
        },
        {
            "title": "Preprint",
            "content": "Output log [WALK] <home_office> [WALK] <laptop> [FIND] <laptop> [SWITCHON] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <mail> [READ] <mail> [TURNTO] <mail> [POINTAT] <mail> [GRAB] <mail> [FIND] <keyboard> [TYPE] <keyboard> World model feedback Wrong state: <character> {SITTING} <computer> {ON} <computer> {PLUGGED_IN} Wrong relative pos: <character> and <chair> [WALK] <home_office> [WALK] <chair> [FIND] <chair> [SIT] <chair> [FIND] <computer> [SWITCHON] <computer> [TURNTO] <computer> [LOOKAT] <computer> [FIND] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <mail> [TURNTO] <mail> [LOOKAT] <mail> [GRAB] <mail> [READ] <mail> [FIND] <keyboard> [TYPE] <keyboard> World model feedback Task success Task fail Output log [WALK] <home_office> [WALK] <laptop> [FIND] <laptop> [SWITCHON] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <mail> [READ] <mail> [TURNTO] <mail> [POINTAT] <mail> [GRAB] <mail> [FIND] <keyboard> [TYPE] <keyboard> Environmental feedback Wrong state: <character> {SITTING} Wrong relative pos: <character> and <mail> <character> and <chair> [WALK] <home_office> [WALK] <laptop> [FIND] <laptop> [SWITCHON] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <chair> [SIT] <chair> [FIND] <mail> [GRAB] <mail> [READ] <mail> [TURNTO] <mail> [POINTAT] <mail> [WRITE] <mail> [SWITCHOFF] <laptop> Environmental feedback Invalid command: [WRITE] <mail> [WALK] <home_office> [WALK] <laptop> [FIND] <laptop> [SWITCHON] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <chair> [SIT] <chair> [TURNTO] <laptop> [POINTAT] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <mail> [GRAB] <mail> [READ] <mail> [TYPE] <laptop> Task success Output log [WALK] <home_office> [WALK] <laptop> [FIND] <laptop> [SWITCHON] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <mail> [READ] <mail> [TURNTO] <mail> [POINTAT] <mail> [GRAB] <mail> [FIND] <keyboard> [TYPE] <keyboard> World model feedback Wrong state: <character> {SITTING} <computer> {ON} <computer> {PLUGGED_IN} Wrong relative pos: <character> and <chair> [WALK] <home_office> [WALK] <chair> [FIND] <chair> [SIT] <chair> [FIND] <computer> [SWITCHON] <computer> [TURNTO] <computer> [LOOKAT] <computer> [FIND] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <mail> [TURNTO] <mail> [LOOKAT] <mail> [GRAB] <mail> [READ] <mail> [FIND] <keyboard> [TYPE] <keyboard> Environmental feedback Wrong relative pos: <character> and <mail> [WALK] <home_office> [WALK] <laptop> [FIND] <laptop> [SWITCHON] <laptop> [TURNTO] <laptop> [LOOKAT] <laptop> [FIND] <chair> [SIT] <chair> [TURNTO] <laptop> [POINTAT] <laptop> [WATCH] <laptop> [FIND] <mail> [TURNTO] <mail> [LOOKAT] <mail> [GRAB] <mail> [READ] <mail> [TYPE] <laptop> World model feedback Task success Task success (a) Ours w/o external feedback (b) Ours w/o internal feedback (c) Ours Figure 11: Visualization of our self-correction process with different types of feedback. The external and internal feedback refer to the feedback from the environment and the world model, respectively. The task instruction is Open email application, open new emails and respond accordingly."
        }
    ],
    "affiliations": [
        "China Tower",
        "Peking University"
    ]
}