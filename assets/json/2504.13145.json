{
    "paper_title": "Exploring Expert Failures Improves LLM Agent Tuning",
    "authors": [
        "Li-Cheng Lan",
        "Andrew Bai",
        "Minhao Cheng",
        "Ruochen Wang",
        "Cho-Jui Hsieh",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld."
        },
        {
            "title": "Start",
            "content": "Preprint."
        },
        {
            "title": "Exploring Expert Failures Improves LLM Agent Tuning",
            "content": "Li-Cheng Lan1 Ruochen Wang3 Andrew Bai1 Cho-Jui Hsieh1 Minhao Cheng2 Tianyi Zhou4 1UCLA 2Pennsylvania State University lclan@cs.ucla.edu andrewbai@ucla.edu ruocwang@g.ucla.edu chohsieh@cs.ucla.edu 3OpenAI mmc7149@psu.edu tianyi@umd.edu 4University of Maryland 5 2 0 2 7 1 ] . [ 1 5 4 1 3 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved 62% win rate in WebShop, outperforming RFT (53. 6%) and GPT-4 (35. 6%), and to the best of our knowledge, setting new state-of-the-art as the first method to surpass score of 0.81 in WebShop and exceed 81 in SciWorld."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have recently demonstrated remarkable potential as autonomous agents, extending beyond their traditional role as passive text generators (Achiam et al., 2023; Team et al., 2023; Zheng et al., 2023; Xi et al., 2025). By allowing LLM-based agents to interact with their environment, they can autonomously complete real-world tasks by processing feedback, maintaining context, and making informed decisions (Zhou et al., 2023; Yao et al., 2022). To develop lightweight specialized agents, researchers commonly employ Rejection Sampling Fine-Tuning (RFT) (Touvron et al., 2023; Yuan et al., 2023; Xi et al., 2024), which fine-tunes models using only environment-validated positive expert and synthesized trajectories. Initially, an expert model (e.g., GPT-4 (Achiam et al., 2023)) generates trajectories for each subtask. smaller model then undergoes supervised fine-tuning (SFT) on only the positive expert trajectories. The fine-tuned model explores further, collecting additional successful trajectories to augment the training dataset. Repeated cycles of exploration and fine-tuning allow the smaller model to eventually exceed the experts performance. RFT is simple and scalable, avoiding the complexity of reward model training and hyperparameter tuning (Touvron et al., 2023; Yuan et al., 2023). 1 Preprint. However, in challenging environments, RFT can quickly be trapped in local minima, impeding further improvements in model performance. Tong et al. (2024) demonstrates that this occurs primarily because RFT tends to favor simpler subtasks during data synthesis, leading to datasets biased toward easy scenarios with inadequate coverage of more difficult tasks. This issue is exacerbated in highly challenging environments, where even expert models struggle to solve the majority of subtasks. Consequently, these challenging subtasks remain Out-of-Distribution (OOD) throughout training. The issue of diminishing returns on harder subtasks has also been reported by other researchers (Yuan et al., 2023; Singh et al., 2023), highlighting key obstacle in training LLM agents. To this end, we investigate the rejected, negative expert trajectories on these hard subtasks. We surprisingly found that due to the experts stronger generalized capabilities, the failed expert trajectories still consist of helpful actions or thoughts to solve the problem. The failure may stem from mistakes in the actions of the last few steps. This observation motivates us to take advantage of the numerous failed expert trajectories that may still contain beneficial actions. Although prior works such as ETO (Song et al., 2024b) (based on DPO (Rafailov et al., 2023)) and NAT (Wang et al., 2024a) are capable of utilizing negative data, they treat all actions within failed trajectories as uniformly negative, thereby failing to learn from potentially valuable actions embedded in them directly. Figure 1: τe = [s0, s1, . . . ] is failed expert (GPT-4) trajectory for challenging subtask s0 with R(τe) = 0. Despite its overall failure, τe contains partial insights to train agent πθ. To identify beneficial actions in τe, our method (EEF) performs simulations from expert states [s0, sl, . . . ] at intervals of length l, resulting in [τs0, τsl , . . . ], revealing that certain expert actions a0:l1 and a2l:3l1 enable the agent to transit from failed states s0, s2l to positivelyperforming states sl, s3l. EEF thus considers these actions (green arrows) beneficial for solving s0, s2l and retains them in the SFT training dataset while rejecting the remaining actions (red arrows). To learn these beneficial actions in negative trajectories, we proposed Exploring Expert Failures (EEF), which simulates the selected expert states and analyzes the simulation results to identify the beneficial actions and thoughts we observed for further fine-tuning. An illustration of our idea is demonstrated in Fig. 1. As shown in Fig. 1, state s0 is an initial state of challenging subtask where our agent and the expert did not succeed. However, with the help of the negative expert trajectory τe, our agent can achieve success by starting simulations from experts intermediate states sl, s3l. Based on the simulation results, we train the agent policy on those beneficial actions (green arrows). Moreover, since our method exclusively learns from pinpointed beneficial actions, it enables us to utilize trajectories generated by weaker yet more cost-effective experts, such as GPT-3.5 Turbo, without being negatively influenced by their suboptimal actions. We evaluated our method in Webshop and SciWorld, two highly challenging environments where even expert agents frequently fail. Our results demonstrate that effectively leveraging negative expert demonstrations enables our approach to solve subtasks previously unattainable by RFT. Consequently, our method outperforms baselines while retaining RFTs simplicity. Notably, we are the first to surpass 0.81 score in Webshop and to exceed score of 81 in SciWorld."
        },
        {
            "title": "2 Backgroud",
            "content": "Environment for text-based LLM agents. Similar to reinforcement learning (RL), the environment of text-based LLM agents can be considered as Markov Decision Process (MDP) with contexts (subtasks) (Hallak et al., 2015), formalized as the tuple = S, A, T, R, C. The state space consists of states S, each encoding both observable and unobservable aspects of the current status, as well as the dialogue history. Note that the most recent 2 Preprint. message in the dialogue serves as the observation of s0. The action space encompasses all possible actions the agent can perform, explicitly including internal reasoning steps (e.g., thoughts (Wei et al., 2022b)). The transition function : specifies the probability of transiting to state st+1 after taking action at in state st. The reward function : assigns numerical reward to each state-action pair. In this paper, we consider sparse reward structure, meaning the agent receives nonzero reward only upon completing subtask, while intermediate steps yield zero reward. For convenience, give trajectory τ = (cid:2)s0, a0, s1, a1, . . . , sT (cid:3), we let R(τ) denote the final total reward obtained by trajectory τ. We define the context space as set of initial states. Each initial state s0 corresponds to specific subtask defined by both an instruction and the current environmental situation. Note that even with the same instruction (query), the subtasks s0 may be different due to unobservable parts of their states. When developing an agent, we partition the subtasks into training set Ctrain and testing set Ctest C. During training, the agent can only explore the subtasks s0 in Ctrain. In addition, we assume that setting the environment state is permitted, allowing simulations to start from arbitrary states. This is common practice in many RL environments for exploring OOD states (Lan et al., 2023). Rejection Sampling Fine-Tuning (RFT). RFT (Yuan et al., 2023) is training paradigm that begins with expert demonstrations and then iteratively refines the learned policy through exploration. At the beginning, the agent πθ is trained exclusively on the positive trajectories R(τe) = 1 of an expert dataset De. This initial phase helps the agent acquire fundamental skills and strategies. In subsequent iterations, for each subtask s0, the current policy generates trajectories, and only the positive ones are added to the training set for further fine-tuning. Through repeated exploration-and-fine-tuning cycles, RFT not only exceeds expert-level performance but can also tackle subtasks beyond the capabilities of the original expert (Song et al., 2024b). Owing to its simplicity and demonstrable effectiveness, we adopt RFT as our foundational framework, while focusing on refining the exploration process and improving the selection of successful trajectories for fine-tuning. Webshop. Webshop is an e-commerce simulation platform that replicates real-world online shopping behaviors by leveraging over million Amazon product listings. We will later utilize Webshop to illustrate our motivation. Box 1 provided successful trajectory of Webshop. Starting from human instruction, the agent then search (search[white t-shirt]), navigate (click[Next]), select (click[D], click[median]), and purchase ([buy now]) the correct item. This example shows that Webshop is valuable environment for studying grounded language understanding and multistep decision-making, as it provides realistic, interactive subtasks that require interpreting language and executing sequential actions. Box 1: WebShop shopping across over million real-world Amazon products. Human instruction: need m-size white t-shirt under 40 dollars. AI: search[white t-shirt] Obs: [Next] [A] Nike white t-shirt, 90 usd. [B] Addidas white t-shirt, 60 usd. [C] Zara white t-shirt, 70 usd. AI: All too expensive, click[Next] Obs: [Next] [Prev] [D] H&M white t-shirt, 30 usd. [E] F21 black t-shirt, 40 usd. [F] Uniqlo white jacket, 35 usd. AI : Only is white t-shirt, click[D] Obs: [back to search] size: [small] [median] [large] click[median] One of Webshops key testing capabilities is assessing an agents navigation skill, which is also crucial for many other web tasks (Yao et al., 2022). Webshop provides two essential navigation actions: Next and Back. The Next action (i.e., Action:click[Next]) allows the agent to access subsequent pages with additional products (as illustrated in Box 1), while the Back action (i.e., Action:click[back to search]) enables the agent to discard the currently selected product and restart the search. Learning these skills helps agent recover from mistake or explore more candidates before making decision, especially in challenging subtasks that even the expert fails to succeed at. AI: Obs: [buy now] [back to product] AI: Obs: finished click[buy now] 3 Preprint."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Motivation In this subsection, we leverage observations from preliminary experiment of Webshop to elucidate the key motivations behind our proposed approach. In this preliminary experiment, we apply RFT to small model, LLama3 8B (AI@Meta, 2024), with an expert dataset De generated by GPT-4. It is important to note that the WebShop task is highly challenging, with GPT-4 only achieving approximately 35% success on the training subtasks Ctrain. Consequently, the remaining 65% of subtasks lack expert demonstrations and remain out-of-distribution (OOD) after the first iteration of fine-tuning. Although subsequent iterations of RFT allow our agent to generalize and explore successful trajectories for some previously unsolved OOD subtasks, roughly 50% of these subtasks remain unsolved and thus persist as OOD. We regard this as waste, as these OOD subtasks and their expert trajectories are accessible yet remain unutilized by our agent during training. To solve the remaining 50% of unsolved subtasks, we analyze the failure trajectories produced by both our model and the expert. In many of these cases, we observe that our agent fails to take necessary navigation actions such as Next and Back. For instance (Box 1), when the listed product does not meet the requirements (too expensive), our agent still blindly selects one of the options instead of using the Next action to check the next pages for more products. Upon inspecting the expert trajectoriesthough they also result in failurewe find that the expert does attempt to use the Next action appropriately, but fails due to other factors. We even observed that our policy can succeed on some of the previously unsolved tasks when initialized from certain expert states, showing that those expert actions are truly beneficial to our agent. search[black living room wall lamp] [A] Generic Raven Decor Crow Lamp Bird Desk Lamp Box 2: negative GPT-3 trajectory. GPT-3 first makes mistake (red), followed by recovery action (green). Human instruction: need living room wall lamp that is in black, and price lower than 100.00 dollars. AI: Obs: [Next] Moreover, we observed that some negative expert trajectories contain recovery actions, where the expert attempts to recover from harmful expert actions. Take Box 2 as an example. Although GPT3 correctly analyzed item and item B, it overlooked the description of item and mistakenly selected it (click[C]). After this harmful action, the expert attempted to recover by performing new search using Back action, which we regard as recovery action. In contrast, although our agent can successfully complete this subtask when starting from the beginning, it failed to perform the recovery action when initialized from the experts mistaken state. This observation motivates us to incorporate the unlearned recovery segments of these trajectories into training. Desk Bedside Light Wall Sconce Light - Black Table lamp. $33.8 AI: The first product is not wall lamp, the second product is not in black, so should skip them and check the next one, click[C] Obs: [back to search] [buy now] AI: The third product is table lamp, not wall lamp, so should skip it and check the next one. click[back to search] Obs: . . . [C] Dolity Crow desk Lamp, Bird Lamps Bedroom Resin Crow [B] Ayux Living Room Dining Room E27 White Deer Head Wall LightGolden Antlers Wall Sconce Rustic Style, $162. Creative Bedroom Bedside Wall Sconce lamp, $35. Based on these observations, our goal is to include these beneficial expert actions in the RFT fine-tuning process while excluding the remaining expert actions. For instance, in Fig. 1, we consider the action sequence [a0, . . . , al1] as beneficial for solving state s0, and similarly, [a2l, . . . , a3l1] as beneficial for solving state s2l which requires recovery. In contrast, the intermediate actions [al, . . . , a2l1] are discarded, as the simulation failure at s2l suggests that they may not represent useful behavior for the policy to learn."
        },
        {
            "title": "3.2 Exploring Expert Failure (EEF)",
            "content": "Based on this motivation, we introduce our method, Exploring Expert Failure (EEF), outlined in Algo.1. Similar to previous works such as RFT (Yuan et al., 2023) and ETO (Song et al., 2024b), our method consists of three main phases: (1) Behavior Cloning (line 4), 4 Preprint. Algorithm 1 Eploring Expert Failure 1: Inputs: Expert dataset: De, Initial policy: πθ 2: Parameters: Finetune Iteration: I, Simulation Num: 3: D+ {τe De : R(τe) = 1} 4: Optimize θ with D+ and LSFT 5: for = 1, 2, . . . , 1 do 6: Di {τ πθ( s0) : s0 Ctrain} for τe = [s0, s1, . . . ] De do 7: 8: = τe/(M + 1) Di Di {τ πθ( sml) : [1, 2, . . . , M]} 9: 10: D+ D+ {τ Di : R(τ) = 1} 11: Ds0 {get traj(s0, D+) : s0 Ctrain} Sr need recover states(Di) 12: 13: Dr {get traj(s, D+) : Sr} 14: Optimize θ with Ds0 Dr and LSFT 15: return πθ // D+ is pos trajectory repository // BC on pos expert trajectories // Explore initial states with πθ // Skip length // Explore states of τe // Add pos trajectories to repository // Get solutions for all s0 if exist // Get expert states that πθ start failing // Get recovery solutions if exist (2) Exploration (lines 610), and (3) Reinforcement Fine-tuning (lines 1115). First, we employ behavior cloning to enhance the capability of small model. Next, we iteratively perform exploration and reinforcement training phases to further refine the model. In each iteration, the exploration phase simulates all subtasks s0 Ctrain and the states selected from the expert trajectories τe De with the current policy πθ, and the reinforcement training phase analyzes these simulated trajectories, subsequently training the model with actions identified as beneficial. The inputs include an expert dataset De and an initial policy πθ. The parameters include the number of reinforcement fine-tuning iterations I, and the number of simulations for each expert trajectory per iteration. When the computational budget is limited, one can reduce to trade off estimation accuracy for efficiency. The details of each phase are provided in the following paragraphs. Behavior Cloning Phase In this phase, we train our policy to acquire fundamental skills by imitating expert behaviors. First, like RFT, we reject the negative and select only positive expert trajectories as our training dataset D+ in Algo 1 line 3. Next, we fine-tune our LLM model πθ with auto-regressive loss in Algo 1 line 4. Specifically, given an expert trajectory τe = (s0, a0, s1, . . . ) , we first convert it into pure text sequence τ = (o0, a0, o1, . . . ) , that only includes observations and actions. Next, we concatenate the observations and actions as string and convert them to sequence of tokens = [t0, t1, . . . , tL], where is the length of the sequence. Since πθ should only train on the action part of the sequence, for each token tl, we define the loss mask as ml = 1( tl ai), where the indicator 1 returns 1 if tl belongs to one of the actions ai. Finally, the masked autoregressive loss is defined as: s.t. LSFT(πθ) = ml log πθ(tlt<l). With LSFT, we update the weights θ of our policy πθ in Algo. 1 line 4 to obtain decent policy to conduct the following exploration. Exploration Phase In the exploration phase, the current model interacts with the environment to collect trajectories and rewards from the environment. Our exploration phase consists of two types of exploration. The first type is the same as RFT, where the policy πθ explores all subtasks s0 Ctrain, as presented in line 6 of Algo. 1. 5 Preprint. The second type of exploration (Algo. 1, lines 79) simulates expert states for each expert trajectory. Our goal is to identify expert actions that are beneficial to our policy, particularly those actions helpful in solving challenging subtasks or recovering from expert mistakes. The parameter controls the computational budget, as some expert trajectories contain an excessive number of states (τe M), making it computationally impractical to simulate all states. Therefore, given parameter M, EEF selects expert states at equal intervals. Specifically, given an expert trajectory τe = [s0, a0, . . . ], EEF first computes skip length = τe/(M + 1) and simulates only the selected expert states [sl, s2l, . . . , sMl]. In iteration i, all the trajectories generated during the exploration phase are stored in the dataset Di for subsequent analysis. Furthermore, all positive trajectories are also stored in the positive trajectory repository D+ as solutions for future training. Reinforcement training phase After the exploration phase, we have collected numerous positive trajectories in D+, either generated from scratch or from expert states. However, fine-tuning our agent on all the trajectories in D+ is impractical due to high computational costs and may introduce biases toward subtasks with more positive trajectories in D+ Zelikman et al. (2022). Hence, in this phase, our goal is to determine which trajectories in D+ are beneficial for training and to identify the specific actions within these trajectories on which the agent should be trained. To accomplish this, we adopt two-step strategy: the important state selection step, in which EEF identifies states considered important, and the solution selection step, where we select solution from D+ for each important state, if such solutions exist. The selected solutions are the positive trajectories used for training. We detail each step in the following paragraphs. In the important state selection step, EEF selects two types of important states. The first type is the initial states s0 of each subtask, ensuring that these foundational states remain represented to prevent our agent from forgetting how to solve them. The second type consists of states requiring recovery from harmful expert actions. In EEF, we identify these harmful actions through simulations conducted during the exploration phase. Specifically, if the current policy πθ succeeds when starting from expert state sil but fails when starting from expert state si, we infer that harmful actions exist within the action sequence ail:i1 and designate si as state requiring recovery. To avoid overemphasizing any single expert trajectory, EEF selects only the first expert state requiring recovery from each trajectory as part of the second type of important states. Formally, given an expert trajectory τe = [s0, s1, . . . ] and simulated states [sl, s2l, . . . , sMl], alongside corresponding trajectories [τs0 , τsl , τs2l , . . . ] generated by the current policy πθ, we define the state sneed recover as: = argmin sneed recover = si , where {i R(τsil ) = 1, R(τsi ) = 0}. i[l,2l,3l,...,Ml] As shown in the equation, si is the first state where the current policy fails (R(τsi ) = 0) after previously succeeding (R(τsi ) = 1), indicating harmful actions within ail:i1. For instance, in Fig. 1, = 2l because the agent succeeds at sl but fails at s2l. This implies expert actions [al, . . . , a2l1] may transition the agent to overly challenging states. Thus, if recovery actions enabling success from s2l exist, we aim for the agent to learn these actions, as they may be generalized to other subtasks. Next is the solution selection step, where we choose at most one solution for each identified important state from D+ to avoid over-emphasizing any important state. solution for given state is defined as positive trajectory containing state s. In Algo. 1, we use the function get traj(s, D+) to get the solution path from the positive trajectory repository D+. If there is no solution path in D+, the function returns None. If there is only one solution path in D+, the function returns the only positive trajectory. If multiple solution paths exist for an important state, we choose the solution path with fewer expert actions. Training the agent on these solution paths with fewer expert-dependent steps minimizes the necessary gradient adjustments and model updates, preserving learning capacity and allowing the model to focus more on other subtasks or states. Consider Fig. 1 as an example: state s0 has two solutions: τsl and τs3l . In this scenario, we select τsl as the solution, as it involves fewer 6 Preprint. expert-generated actions. Thus, the agent only needs to learn the expert actions [a0, . . . , al1] since the subsequent actions after the state sl of τsl are already known to the agent. After collecting the solution trajectories for the two categories of important states, stored respectively in Ds0 and Dr as detailed in Algo. 1, we fine-tune our agent using on Ds0 Dr (Algo. 1, line 14). In particular, when trajectory is selected as the solution for specific state si, the agent is trained only on the actions [ai, ai+1, . . . ] that occur after state si That is, we disable loss propagation for earlier actions [a0, . . . , ai1] before si. This selective training prevents the agent from incorporating potentially problematic actions preceding state si. For instance, in Figure 1, trajectory τs3l provides solution for state s2l. Although actions a0, a1, . . . , a2l1 are present within τs3l , we exclusively train our agent on actions a2l, a2l+1, . . ., as only these subsequent actions are relevant for solving state s2l. By focusing learning solely on actions following important states, our agent reduces the risk of imitating suboptimal expert behaviors."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Table 1: Statistics of datasets. Total expert #: the number of expert trajectories. Avg Len: the average length of expert trajectories. Datasets We conducted our experiments in three datasets: WebShop 11k, WebShop 3k, and ScienceWorld 2k  (Table 1)  . Specifically, WebShop 11k (Ma et al., 2024) allows us to examine whether our approach can effectively utilize substantial amount of training data. To investigate generalization under limited data availability, we employ WebShop 3k, random subset of WebShop 11k containing only 1k positive demonstrations. Additionally, we test our method on ScienceWorld (Wang et al., 2022), which is designed for scenarios involving longer trajectories (approximately 20 steps). Unlike prior works such as ETO, we restrict our agents to only win/loss feedback across all environments and explicitly prohibit teleport actions in SciWorld to increase task difficulty. Webshop 11k Webshop 3k Sciworld 2k 11338 2835 2120 Total Expert # Positive Expert # 4106 1045 1489 8.26 8.24 20."
        },
        {
            "title": "Avg\nLen",
            "content": "For expert demonstrations, we used GPT-4 to generate trajectories based on human instructions within the WebShop datasets (Liu et al., 2024). Furthermore, in certain evaluations, GPT-3.5-Turbo-generated demonstrations were used to assess the robustness of our methods under conditions involving weaker expert guidance. In contrast, for ScienceWorld, the demonstrations were sourced directly from AgentGym (Xi et al., 2024). Due to existing models limitations in achieving high performance, AgentGym supplemented 500 demonstrations from rule-based gold agent with an additional 1,620 demonstrations produced by GPT-4 Turbo. We define trajectories achieving the maximum score as positive trajectoriesspecifically, trajectories scoring 1.0 for WebShop and 100 for SciWorld. Table 2: Methods requiring fine-tuning. Use Neg indicates if the method uses negative expert trajectories. FT Iter is the total iteration number I. Sim. # refers to the number of simulations per subtask per iteration. Use GPT-3 indicates if GPT-3.5-Turbogenerated trajectories (30 cheaper than GPT-4) are also used. Use Neg SFT ALL SFT POS NAT (Wang et al., 2024a) ETO (Song et al., 2024b) RFT (Yuan et al., 2023) RFT 6 EEF GPT-4 EEF GPT-3&4 FT Iter Sim # 1 1 1 3 3 3 3 3 0 0 0 1 1 6 6 11 Use GPT-3 Our method We utilize LLAMA3 8B Instruct (AI@Meta, 2024) as the initial model for fine-tuning. Each fine-tuning iteration consists of six epochs, with batch size of 64 and learning rate of 5e-5. All experiments were conducted with four NVIDIA A6000 GPUs. The default parameters are = 5, = 4. 7 Preprint. Table 3: Win rates and rewards of various methods on three agentic environments. Maximum rewards: 1 for WebShop-11k/3k, and 100 for ScienceWorld-2k. FT Iter 0 0 1 1 1 3 3 3 3 3 Webshop 11k Webshop 3k ScienceWorld 2k Winrate Reward Winrate Reward Winrate Reward 23.2% 35.6% 37.2% 46.4% 37.2% 42.0% 52.0% 53.6% 58.4% 62.0% 0.60 0.66 0.66 0.75 0. 0.68 0.75 0.76 0.78 0.81 23.2% 35.6% 39.6% 39.6% 40.4% 37.8% 38.8% 41.4% 46.8% 50.0% 0.60 0.66 0.68 0.67 0. 0.67 0.66 0.68 0.72 0.73 53.0% 61.0% 54.0% 57.5% 61.5% 62.5% 68.5% 7.64 14.4 68.1 76.8 69. 75.0 74.6 73.4 81.3 GPT-3.5 Turbo GPT-4 SFT ALL SFT POS NAT ETO RFT RFT 6 EEF GPT-4 EEF GPT-3 & 4 We select the best model from all iterations. For the Webshop dataset, we evaluate two variants of our method: EEF GPT-4 and EEF GPT-3 & 4. EEF GPT-4 uses the same dataset as the baselines, while EEF GPT-3 & 4 additionally incorporates the data generated by GPT-3 Turbo in the exploration phase and the reinforcement training phase. Baselines Our baselines can be categorized into three types. The first type includes models without fine-tuning, specifically GPT-3 Turbo and GPT-4. The second type comprises models that are finetuned without exploration; within this group, we have three baselines. The SFT All baseline learns from all demonstrations, regardless of their correctness. The SFT POS baseline learns exclusively from positive demonstrations. The Negative-Aware Training (NAT) (Wang et al., 2024a) baseline uses the entire dataset but requires the agent to generate incorrect trajectories for questions associated with erroneous demonstrations in the prompt. The third category of baselines are finetuned with exploration. For this category, we selected RFT and ETO (Song et al., 2024b). We tested two RFT variants: RFT, which conducts single exploration per subtask per iteration, and RFT6, which explores each subtask six times, each with different temperature settings ranging from 0.2 to 0.95. In contrast to RFTs use of SFT, ETO employs DPO (Rafailov et al., 2023), where agent-generated trajectories are compared directly with expert demonstrations based on environment reward (still provided) to produce labeled training pairs. PPO is excluded due to its significantly lower performance in preliminary experiments Song et al. (2024b) on unseen subtasks compared to ETO."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 3 presents the performance comparison between our proposed EEF method and other baselines in three datasets. The results show clear performance advantage of our proposed method compared to other baselines, including the GPT-4 baseline, across all evaluated datasets. Specifically, the EEF GPT-4 variant achieves significant improvements over GPT-4, increasing the win rate from 35.6% to 58.4% on Webshop 11k, from 35.6% to 46.8% on Webshop 3k, and notably from lower than 14.4% to 68.5% on ScienceWorld 2k. Furthermore, compared to the best-performing fine-tuning baseline (RFT 6), EEF GPT-4 demonstrates additional improvements, raising the win rate from 53.6% to 58.4% on Webshop 11k, from 41.4% to 46.8% on Webshop 3k, and from 62.5% to 68.5% on ScienceWorld 2k. Additionally, when incorporating data from GPT-3 Turbo into EEF (EEF GPT-3 & 4), we observe even greater performance gains, particularly on the Webshop datasets, where the win rate further rises to 62.0% and 50.0% for Webshop 11k and Webshop 3k, respectively. This highlights that our method effectively leverages additional demonstrations from weaker expert (GPT-3.5 Turbo), achieving notable performance improvements with only slight increase in cost. 8 Preprint. 4.3 Ablation Studies Navigation skills In this experiment, we evaluate the mitigation of simplicity bias by measuring the effectiveness of navigation skills on the Webshop 11k dataset. Specifically, we focus on two types of navigation actions: Next page (Next) and Back to search (Back), as shown in Fig. 2. Our methods, EEF GPT-4 and EEF GPT-3&4, exhibit superior proficiency in utilizing these navigation skills compared to baseline models. In addition, we investigated the attempt rate of using navigation actions. The results are shown in Table 4 of Appendix A.1. In particular, while GPT-3.5 and GPT-4 display lower percentage of successful subtasks that involve navigation actions, they actually show relatively high attempt rates. For example, GPT-4 attempted to use the Next action in 16.8% of the subtasks. This indicates that GPT-4 recognizes the potential usefulness of such navigation actions but lacks the ability to execute them effectively. In contrast, our method not only identify these beneficial actions in the negative expert trajectories but also convert them into successful task completions through more effective skill utilization. Figure 2: The percentage of successful subtasks that involve the use of navigation skills: Next and Back. Case studies To further substantiate the necessity of navigation skills for challenging subtasks, we examine specific subtasks where (1) our method succeeded by effectively employing these skills, (2) RFT methods failed due to the absence of these skills, and (3) GPT-4 applied these skills but ultimately failed. Some representative samples of these subtasks, along with side-by-side comparison of trajectories generated by the RFT method and our method, can be found in Appendix A.2 and A.3. Upon examining the trajectores of these type subtasks, it becomes evident that navigation skills are crucial. For subtasks that require the Next action, the products initially presented do not satisfy user requirements. For instance, listed products may have incorrect attributes such as an inappropriate price range or incorrect colors (e.g., ask for petal green but get petal blue). Similarly, for subtasks that require the Back action, the items initially selected appear suitable on the search page, but ultimately lack specific attributes such as desired sizes or colors upon closer inspection on the product page. (e.g., ask for 250ml shampoo but only find 500ml or 200ml options). Another notably challenging subtask involved users request for high-power sound system. Here, the agent needed to recognize that portable 16W soundbar was insufficient and consequently navigate to the next page to identify more suitable product. These case studies clearly demonstrate that our approach significantly mitigates simplicity bias and is able to employ advanced navigation skills. Figure 3: Win rates of different methods training the same base model for one iteration under varying simulation budgets (exploration cost) by adjusting or M. RFT N: Initial state s0 simulated times. EEF: Different expert state simulation numbers = 1, 2, 5 on different expert datasets. Efficiency Analysis In this section, we evaluate the exploration efficiency of our proposed method by varying the number of simulations, specifically setting = 1, 2, 5. We compared these results against baseline exploration method that utilizes different temperatures within the Webshop 3k environment. To ensure fairness in our comparison, we employed the same model (SFT POS) for conducting explorations, and subsequently utilized the resulting data to finetune new model with only one iteration. The results presented in Fig. 3 clearly demonstrate the superior performance of our method relative to the baseline exploration approach. Specifically, the baseline method fails to achieve 40% winrate even when employing up to 11 simulations. In contrast, our method achieves 40% winrate using merely 2 simulations (when = 1) and consistently improves performance 9 Preprint. with increased simulation budgets. Furthermore, our findings indicate that exploring the trajectories generated by GPT-3 consistently yields better results than exploring those generated by GPT-4. This phenomenon may be attributed to the exploration model being trained exclusively on positive GPT-4 data. Therefore, adding the GPT-3.5 Turbo trajectories can offer richer learning opportunities. Different Models To further validate the effectiveness and generalization of our method, we conducted additional experiments using one of the latest models, mistral-7b-v0.3, on the Webshop 3k dataset. The results demonstrate that our approach consistently outperforms existing baselines. Specifically, our method achieved win rates of 46.8% and 48.8% with the GPT-4 and GPT-3&4 models, respectively, compared to only 42.2% obtained by RFTx6. These findings underscore the robust generalization capability of our approach across different initial LLMs."
        },
        {
            "title": "5 Related Work",
            "content": "Recent advances in large language models (LLMs) have prompted researchers to build LLM-based agents for multi-step tasks, exploiting the models emergent abilities (Achiam et al., 2023; Wei et al., 2022a). Broadly, existing approaches can be categorized into methods without fine-tuning (zero-shot or few-shot), methods with fine-tuning but no exploration, and methods with exploration-based fine-tuning. Zero-shot and few-shot methods typically focus on prompt engineering to reduce errors and hallucinations (Huang et al., 2022a;b; Yao et al., 2023; Wang et al., 2023a). Approaches that use fine-tuning without exploration often rely on filtering suboptimal expert trajectories based on rewards (Zeng et al., 2023; Chen et al., 2023; 2024), or adding negative label on the prompt of negative demonstrations (Wang et al., 2024a). and some also train on large set of tasks to learn general concepts transferable to tasks with limited demonstrations (Zhang et al., 2024; Song et al., 2024a). Explorationbased fine-tuning adds newly discovered positive samples to the supervised fine-tuning dataset (Aksitov et al., 2023; Xi et al., 2024); experiments have shown that training on selfgenerated data can be more sample-efficient (Setlur et al., 2024). Other variants (e.g., ETO) use DPO to reduce the likelihood of generating negative trajectories (Song et al., 2024b). Note that ETO, as well as many methods (Xiong et al., 2024) that rely on ETOs dataset, use only high-reward expert trajectories. However, many of these approaches treat the entire trajectory with single final reward, which can be simplistic. More recent methods adopt stepwise analysis of trajectories for better finetuing (Wang et al., 2023b; 2024b; Ma et al., 2023; Havrilla et al., 2024; Xiong et al., 2024). For example, by training model to identify critical steps or highly rewarded and then focus training only on those steps (Chen et al., 2025; Wang et al., 2025). Another strategy assumes that the expert is always correct, applying DPO at each step (Deng et al., 2024); but this is not viable when the expert frequently fails. Stepwise DPO (Lai et al., 2024) assumes that if state contains both positive and negative trajectories, the agent should learn from the positive action. Yet implementing stepwise DPO on the negative expert trajectories still differs from our approach. Specifically, given state si and two trajectories τ+ si and τ si labeled as chosen and rejected, stepwise DPO increases the probability of actions after si in τ+ si , while decreasing it for τ si . In contrast, our method emphasizes beneficial actions before si that contribute to successful simulations. Additionally, stepwise DPO may produce an excessive number of trajectory pairs when multiple states each yield chosenrejected pair, whereas our method limits representation to one per subtask and per expert trajectory."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this paper, we present EEF, novel framework that learns beneficial actions from negative expert data while remaining robust against noise from suboptimal actions. Remarkably, our method can even learn effectively from trajectories generated by weaker experts such as GPT-3.5. Experimental results demonstrate that EEF achieves state-of-the-art (SOTA) performance on both the Webshop and ScienceWorld environments. Furthermore, EEF 10 Preprint. retains the simplicity of RFT by relying solely on supervised fine-tuning (SFT) loss, without requiring additional reward model training, making it easier to apply and reducing the need for extensive hyperparameter tuning. There are several promising directions to further improve our method. For instance, one can perform preference learning by using only beneficial actions as the chosen data. Another direction is to enhance the accuracy of identifying these beneficial actions. Instead of searching over fixed interval, binary search strategy could be employed to more efficiently determine which actions are advantageous to the model. Furthermore, if computational resources permit, the method could be enhanced by incorporating expert trajectories as the main branches in tree search algorithmsuch as Monte Carlo Tree Search (MCTS) (Coulom, 2006)to better solve out-of-distribution (OOD) subtasks or states. In addition, the solution selection step can be modified to select the best solution. Moreover, our results suggest that even when GPT-4 trajectories are available, GPT-3 trajectories still contain valuable information. This opens up opportunities for further research on combining expert datasets, particularly when one expert source is significantly more cost-effective than the other. How to allocate resources between high-quality and low-cost experts becomes an even more critical trade-off challenge."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL CARD.md. Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, et al. Rest meets react: Self-improvement for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003, 2023. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024. Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, and Tianyi Zhou. Atlas: Agent tuning via learning critical steps. arXiv preprint arXiv:2503.02197, 2025. Remi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pp. 7283. Springer, 2006. Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, and Weipeng Chen. From novice to expert: Llm agent policy optimization via step-wise reinforcement learning. arXiv preprint arXiv:2411.03817, 2024. Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. ArXiv, abs/1502.02259, 2015. URL https://api.semanticscholar.org/CorpusID: 14616648. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. Glore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pp. 91189147. PMLR, 2022a. 11 Preprint. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Stepdpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. Li-Cheng Lan, Huan Zhang, and Cho-Jui Hsieh. Can agents run relay race with strangers? generalization of rl to out-of-distribution trajectories. ArXiv, abs/2304.13424, 2023. URL https://api.semanticscholar.org/CorpusID:258331519. Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla Choubey, Tian Lan, Jason Wu, Huan Wang, et al. Agentlite: lightweight library for building and advancing task-oriented llm agent system. arXiv preprint arXiv:2402.15538, 2024. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn llm agents. ArXiv, abs/2401.13178, 2024. URL https://api.semanticscholar. org/CorpusID:267199917. Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. Lets reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint arXiv:2310.10080, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. ArXiv, abs/2305.18290, 2023. URL https://api.semanticscholar.org/CorpusID: 258959321. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. Advances in Neural Information Processing Systems, 37:4300043031, 2024. Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Ethan Dyer, Behnam Neyshabur, Jascha Narain Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Trans. Mach. Learn. Res., 2024, 2023. URL https://api.semanticscholar.org/CorpusID:266163375. Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction trajectories. arXiv preprint arXiv:2410.07706, 2024a. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for llm agents. arXiv preprint arXiv:2403.02502, 2024b. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficultyaware rejection tuning for mathematical problem-solving. Advances in Neural Information Processing Systems, 37:78217846, 2024. Preprint. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant on Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melissa Hall Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. URL https://api.semanticscholar.org/CorpusID: 259950998. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a. Hanlin Wang, Jian Wang, Chak Tou Leong, and Wenjie Li. Steca: Step-level trajectory calibration for llm agent learning. arXiv preprint arXiv:2502.14276, 2025. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023b. Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy Baldwin. Learning from failure: Integrating negative examples when fine-tuning large language models as agents. arXiv preprint arXiv:2402.11651, 2024a. Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre ˆote, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID: 247451124. Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. Multi-step problem solving through verifier: An empirical analysis on modelinduced process supervision. arXiv preprint arXiv:2402.02658, 2024b. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022b. URL https://api.semanticscholar.org/CorpusID: 246411621. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101, 2025. Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Watch every step! llm agent learning via iterative step-level process refinement. arXiv preprint arXiv:2406.11176, 2024. 13 Preprint. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. E. Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. 2022. URL https://api.semanticscholar.org/CorpusID:247762790. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023. Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Ming Zhu, Juntao Tan, Thai Hoang, Zuxin Liu, Liangwei Yang, et al. Agentohana: Design unified data and training pipeline for effective agent learning. arXiv preprint arXiv:2402.15506, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 14 Preprint."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Navigation actions success and attempted rate Table 4: Performance comparison in terms of pos and all metrics. Columns 2-3 indicate the proportion of test subtasks successfully solved using Next or Back, respectively. Columns 4-5 show the proportion of attempts made by agents using these navigation skills. Success (%) Attempt (%) Next Back Next Back 0.0 2.8 2.8 4.4 1.2 4.0 4.0 6.8 9.2 2.4 2.8 1.6 2.0 1.2 3.2 3.2 5.2 9. 3.2 16.8 16.8 13.2 9.6 9.6 9.2 13.2 13.2 16.8 14.0 9.6 4.0 3.2 8.4 7.6 12.0 14.0 GPT-3.5 Turbo GPT-4 SFT ALL SFT POS ETO RFT RFT 6 EEF GPT-4 EEF GPT-3&4 A.2 Case studies of Next action Next Example 1: RFT, reward = 0.66 Task:i am looking for resilient memory foam loveseat sofa Action:Think[response: should first use Search to search for resilient memory foam loveseat sofa.] Observation: OK Action:search[product: resilient memory foam loveseat sofa] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B08737G8HG] Modern Fabric Double Sofa Couch Loveseat Linen Accent Chair Comfy Upholstered Arm Lounge Chair for Bedroom Living Room $279.99 [B09NS47YJ7] Sectional Couches for Living Room Faux Leather Office Couch Sets Mid Century Modern Small Sofa for Bedroom Mini Settee Loveseat Comfy for Small Apartment(Loveseat, Brown) $512.0 [B09F5J244F] Hydeline Erindale Power Recline, Headrest, Lumbar, Top Grain Leather Sofa Couch with Built in USB Ports, 88, Cognac $2699.0 Action:Think[response: The first product seems to be the most relevant to the requirements. should click the first product.] Observation: OK Action:click[button: B08737G8HG] Observation: WEB PAGE: [Back to Search] [Prev] Modern Fabric Double Sofa Couch Loveseat Linen Accent Chair Comfy Upholstered Arm Lounge Chair for Bedroom Living Room Price: $279.99 Action:click[button: Buy Now] Observation: shopping is finished. Next Example 1: EEF, reward = 1.00 Task:i am looking for resilient memory foam loveseat sofa 15 Preprint. Action:Think[response: should first use Search to search for resilient memory foam loveseat sofa.] Observation: OK Action:search[product: resilient memory foam loveseat sofa] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B08737G8HG] Modern Fabric Double Sofa Couch Loveseat Linen Accent Chair Comfy Upholstered Arm Lounge Chair for Bedroom Living Room $279.99 [B09NS47YJ7] Sectional Couches for Living Room Faux Leather Office Couch Sets Mid Century Modern Small Sofa for Bedroom Mini Settee Loveseat Comfy for Small Apartment(Loveseat, Brown) $512.0 [B09F5J244F] Hydeline Erindale Power Recline, Headrest, Lumbar, Top Grain Leather Sofa Couch with Built in USB Ports, 88, Cognac $2699. Action:Think[response: None of the products listed are resilient memory foam loveseat sofas. should click Next to see more options.] Observation: OK Action:click[button: Next] Observation: WEB PAGE: [Back to Search] Page 2 (Total results: 15) [Prev] [Next] [B09NS26T9M] Faux Leather Office Couch Sets Mid Century Modern Sectional Small Sofa for Bedroom Mini Settee Loveseat Comfy for Living Room Small Apartment(3-Seat Sofa, Black) $512.0 [B09QMF83B5] 56-inch Small Loveseat Sofa, Modern Couch with Removable Cushion, Upholstered Living Room Sofa with Rose Golden Metal Legs, Leopard Teal $259.99 [B0872G9J57] Poppy Sofa - Nelson Sectional/Sofa/Loveseat/Loveseat Sectional/Armchair 42 Colors - Resilient Memory Foam - Made in US - Mid Century Modern (Sand Cross Weave Upholstery) SF031720036E1 $1751.0 Action:Think[response: The third product is resilient memory foam loveseat sofa. should click on the third product.] Observation: OK Action:click[button: B0872G9J57] Observation: WEB PAGE: [Back to Search] [Prev] size [armchair][loveseat][sectional][sectional loveseat][sofa] Price: $1751.0 [Buy Now] Action:click[button: loveseat] Observation: WEB PAGE: You have clicked loveseat. Action:click[button: Buy Now] Observation: shopping is finished. Next Example 2: RFT, reward = 0.66 Task:i need detangler hair brush that stimulates hair growth. choose the purple one, and price lower than 50.00 dollars Action:Think[response: should first use Search to search for detangler hair brush that stimulates hair growth.] Observation: OK Action:search[product: detangler hair brush that stimulates hair growth] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B08KYNCDVG] Zolf Natural Bristle Detangling for Wet/Dry Hair, for Women, Men, and Kids. Includes brush cleaner. $19. 16 Preprint. [B07QW12YR9] Hair Brush, Natural Boar Bristle Hair Brush, Wooden Bamboo Hair Brush for Women Mens, Paddle Brush Curly Hair Brush for Thick Hair & Anti Static Detangling Best Paddle Brush for Reducing Hair Breakage. $9.99 [B09189QFJ9] Hair Brush, Natural Boar Bristle Hair Brush, Wooden Bamboo Hair Brush for Women Mens, Paddle Brush Curly Hair Brush for Thick Hair & Anti Static Detangling Best Paddle Brush for Reducing Hair Breakage $9.99 Action:Think[response: The first product seems to be the most relevant to the requirements. should click the first product.] Observation: OK Action:click[button: B08KYNCDVG] Observation: WEB PAGE: [Back to Search] [Prev] Zolf Natural Bristle Detangling for Wet/Dry Hair, for Women, Men, and Kids. Includes brush cleaner. Price: $19.77 Action:click[button: Buy Now] Next Example 2: EEF, reward = 1.0 Task:i need detangler hair brush that stimulates hair growth. choose the purple one, and price lower than 50.00 dollars Action:Think[response: should first use Search to search for detangler hair brush that stimulates hair growth.] Observation: OK Action:search[product: detangler hair brush that stimulates hair growth] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B08KYNCDVG] Zolf Natural Bristle Detangling for Wet/Dry Hair, for Women, Men, and Kids. Includes brush cleaner. $19.77 [B07QW12YR9] Hair Brush, Natural Boar Bristle Hair Brush, Wooden Bamboo Hair Brush for Women Mens, Paddle Brush Curly Hair Brush for Thick Hair & Anti Static Detangling Best Paddle Brush for Reducing Hair Breakage. $9.99 [B09189QFJ9] Hair Brush, Natural Boar Bristle Hair Brush, Wooden Bamboo Hair Brush for Women Mens, Paddle Brush Curly Hair Brush for Thick Hair & Anti Static Detangling Best Paddle Brush for Reducing Hair Breakage $9.99 Action:Think[response: None of the products listed stimulate hair growth or are purple. should click Next to see more options.] Observation: OK Action:click[button: Next] Observation: WEB PAGE: [Back to Search] Page 2 (Total results: 15) [Prev] [Next] [B07WN2DJSL] Curved Vented Boar Bristle Styling Hair Brush, Single White Brush Anti-static Detangler, Wet or Dry Use, Fast Blow Drying, Use on Long or Short Hair. (1 Pack, White) $7.99 [B07VQDM2CF] Oleh-Oleh Wet and Dry Brush Pro Detangle Hair Brush + Folding Mirror Mini Pop Up Hairbrush (BLUE+PURPLE). Healthy to scalp. $16.99 [B00S1L6590] Giorgio GION2R Neon Red 6.25 inch Gentle Touch Detangler Hair Brush for Men Women & Kids. Soft Bristles for Sensitive Scalp. Wet and Dry for all Hair Types. Scalp Massager Brush Stimulate Hair Growth $14. Action:Think[response: The second product is purple and stimulates hair growth. should click on the second product.] Observation: OK Action:click[button: B07VQDM2CF] Observation: WEB PAGE: [Back to Search] [Prev] color [blue+purple][blue+red+light blue][purple][purple+blue][rose+blue+purple][set1] Oleh-Oleh Wet and Dry Brush Pro Detangle Hair Brush + Folding Mirror Mini Pop Up Hairbrush (BLUE+PURPLE) Price: $16.99 [Buy Now] 17 Preprint. Action:click[button: purple] Observation: WEB PAGE: You have clicked purple. Action:click[button: Buy Now] Observation: shopping is finished. A.3 Case studies of Back action Back Example 1: RFT, reward = 0. Task:i want red office chair ergonomic gaming chair with lumbar support, and price lower than 140.00 dollars Action:Think[response: should first use Search to search for red office chair ergonomic gaming chair with lumbar support.] Observation: OK Action:search[product: red office chair ergonomic gaming chair with lumbar support] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B098DG5F8T] Tyyps Ergonomic Gaming Chair -250lb High Back PC Racing Desk Chair Adjustable Reclining Computer Chair with Lumbar Support Armrest Headrest Task Rolling Swivel Chair Game Chair for Adult Teen, Black $52.95 [B08PPGC1C4] Gaming Chair Office Chair Desk Chair Ergonomic High Back Computer Chair with Lumbar Support Flip-up Arms Headrest PU Leather Swivel Task Chair for Women Men Adult, Pink $105.99 [B097BG6P5Q] Executive PU Leather Office Chair, Ergonomic High Back Managerial Chairs with Lumbar Support Armrest Swivel, Adjustable Computer Gaming Chair for Office Home Gaming Room Red $162.99 Action:Think[response: The third product is red office chair with lumbar support, but its over the budget. should click the first product.] Observation: OK Action:click[button: B098DG5F8T] Observation: WEB PAGE: [Back to Search] [Prev] Tyyps Ergonomic Gaming Chair -250lb High Back PC Racing Desk Chair Adjustable Reclining Computer Chair with Lumbar Support Armrest Headrest Task Rolling Swivel Chair Game Chair for Adult Teen, Black Price: $52.95 [Buy Now] Action:click[button: Buy Now] Observation: shopping is finished. Back Example 1: EEF, reward = 1. Task:i want red office chair ergonomic gaming chair with lumbar support, and price lower than 140.00 dollars Action:Think[response: should first use Search to search for red office chair ergonomic gaming chair with lumbar support.] Observation: OK Action:search[product: red office chair ergonomic gaming chair with lumbar support] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B098DG5F8T] Tyyps Ergonomic Gaming Chair -250lb High Back PC Racing Desk Chair Adjustable Reclining Computer Chair with Lumbar Support Armrest Headrest Task Rolling Swivel Chair Game Chair for Adult Teen, Black $52.95 [B08PPGC1C4] Gaming Chair Office Chair Desk Chair Ergonomic High Back Computer Chair with Lumbar Support Flip-up Arms Headrest PU Leather Swivel Task Chair for Women Men Adult, Pink $105.99 Preprint. [B097BG6P5Q] Executive PU Leather Office Chair, Ergonomic High Back Managerial Chairs with Lumbar Support Armrest Swivel, Adjustable Computer Gaming Chair for Office Home Gaming Room Red $162.99 Action:Think[response: The third product is red office chair with lumbar support, but its over the budget. should click the first product to see if it has red color option.] Observation: OK Action:click[button: B098DG5F8T] Observation: WEB PAGE: [Back to Search] [Prev] Tyyps Ergonomic Gaming Chair -250lb High Back PC Racing Desk Chair Adjustable Reclining Computer Chair with Lumbar Support Armrest Headrest Task Rolling Swivel Chair Game Chair for Adult Teen, Black Price: $52.95 [Buy Now] Action:Think[response: The product does not have red color option. should go back to the search results and try the next product.] Observation: OK Action:click[button: Back to Search] Observation: WEB PAGE: Let us begin shopping [Search] Action:search[product: red office chair ergonomic gaming chair with lumbar support] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B098DG5F8T] Tyyps Ergonomic Gaming Chair -250lb High Back PC Racing Desk Chair Adjustable Reclining Computer Chair with Lumbar Support Armrest Headrest Task Rolling Swivel Chair Game Chair for Adult Teen, Black $52.95 [B08PPGC1C4] Gaming Chair Office Chair Desk Chair Ergonomic High Back Computer Chair with Lumbar Support Flip-up Arms Headrest PU Leather Swivel Task Chair for Women Men Adult, Pink $105.99 [B097BG6P5Q] Executive PU Leather Office Chair, Ergonomic High Back Managerial Chairs with Lumbar Support Armrest Swivel, Adjustable Computer Gaming Chair for Office Home Gaming Room Red $162.99 Action:click[button: B08PPGC1C4] Observation: WEB PAGE: [Back to Search] [Prev] color [blue][camo][grey][pink][red][white] Gaming Chair Office Chair Desk Chair Ergonomic High Back Computer Chair with Lumbar Support Flip-up Arms Headrest PU Leather Swivel Task Chair for Women Men Adult, Pink Price: $105.99 [Buy Now] Action:click[button: red] Observation: WEB PAGE: You have clicked red. Action:click[button: Buy Now] Observation: shopping is finished. Back Example 2: RFT, reward = 0.75 Task:i am looking for 2 pack of 20ft long quadshield solid copper black color indoor and outdoor coaxial cable, and price lower than 60.00 dollars Action:Think[response: should first use Search to search for 2 pack of 20ft long quadshield solid copper black color indoor and outdoor coaxial cable.] Observation: OK Action:search[product: 2 pack of 20ft long quadshield solid copper black color indoor and outdoor coaxial cable] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] 19 Preprint. [B074741B7G] PHAT SATELLITE INTL 100ft White Solid Copper COAXIAL RG6 3GHz UV Jacket Indoor Outdoor Satellite TV Antenna Cable 18AWG 75 Ohm Anti-Corrosion Nickel Compression Connectors Assembled in USA $18.0 [B07CQ96G4F] 150ft Black TRI-Shield Indoor Outdoor RG-6 Coaxial Cable Nickel-Plated Brass Connector 75 Ohm (Satellite TV, Broadband Internet, Ham Radio, OTA HD Antenna Coax) Assembled in USA PHAT SATELLITE $19.5 [B07CQB2FZK] 200ft Black TRI-Shield Indoor Outdoor RG-6 Coaxial Cable Nickel-Plated Brass Connector 75 Ohm (Satellite TV, Broadband Internet, Ham Radio, OTA HD Antenna Coax) Assembled in USA PHAT SATELLITE $21.5 Action:Think[response: None of the products match the exact search criteria. should click on the first product as it is the closest match.] Observation: OK Action:click[button: B074741B7G] Observation: WEB PAGE: [Back to Search] [Prev] size [1ft][1ft (3 pack)][2ft][2ft (3 pack)][3 ft][3ft][3ft (3 pack)][4ft][4ft (3 pack)][5 ft][5ft][5ft (3 pack)][6 ft][6ft][6ft (3 pack)][8ft][8ft (3 pack)][10 ft][10ft][10ft (3 pack)][12 ft][12ft][12ft (3 pack)][15 ft][15ft][15ft (2 pack)][20 ft][20ft][20ft (2 pack)][25 ft][25ft][25ft (2 pack)][30 ft][30ft][30ft (2 pack)][35ft][35ft (2 pack)][40 ft][40ft][40ft (2 pack)][45ft][45ft (2 pack)][50 ft][50ft][50ft (2 pack)][55ft][55ft (2 pack)][60 ft][60ft][60ft (2 pack)][65ft][65ft (2 pack)][70ft][70ft (2 pack)][75ft][75ft (2 ft][100ft][105ft][110ft][115ft][120ft][125ft][130ft][135ft][140ft][145ft][150 ft][150ft][155ft][160ft][165ft][170ft][175ft][180ft][185ft][190ft][195ft][200 ft][200ft] color [plenum cmp catvp - white][solid copper - black][solid copper weather boot - white][bare copper digicon connector - white][direct burial tri-shield - black][direct burial tri-shield, boot - black][quad-shield weather boot - black][quadshield - black][quadshield - white][quadshield solid copper - black][solid copper - white] PHAT SATELLITE INTL 100ft White Solid Copper COAXIAL RG6 3GHz UV Jacket Indoor Outdoor Satellite TV Antenna Cable 18AWG 75 Ohm Anti-Corrosion Nickel Compression Connectors Assembled in USA Price: $18.0 [Buy Now] pack)][80ft][85ft][90ft][95ft][100 Action:click[button: 20ft (2 pack)] Observation: WEB PAGE: You have clicked 20ft (2 pack). Action:click[button: solid copper - black] Observation: WEB PAGE: You have clicked solid copper - black. Action:click[button: Buy Now] Observation: shopping is finished. Back Example 2: EEF, reward = 1.0 Task:i am looking for 2 pack of 20ft long quadshield solid copper black color indoor and outdoor coaxial cable, and price lower than 60.00 dollars Action:Think[response: should use the Search function to find the coaxial cable with the specific requirements.] Observation: OK Action:search[product: 20ft long quadshield solid copper black color indoor and outdoor coaxial cable] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next] [B073L44ZBG] 20ft White TRI-Shield Weather Seal Indoor Outdoor RG-6 Coaxial Cable Brass Connector 75 Ohm Satellite TV Broadband Internet Ham Radio OTA HD Antenna Coax Assembled in USA by PHAT SATELLITE INTL $11.4 [B074741B7G] PHAT SATELLITE INTL 100ft White Solid Copper COAXIAL RG6 3GHz UV Jacket Indoor Outdoor Satellite TV Antenna Cable 18AWG 75 Ohm Anti-Corrosion Nickel Compression Connectors Assembled in USA $18.0 [B073L2RDKV] PHAT SATELLITE INTL Tri-Shielded Solid Core 18AWG RG-6 Coaxial Cable, Satellite TV Broadband Modem Internet OTA HD Antenna Indoor Outdoor Coax Applications, UL ETL, Assembled in USA, (200 feet White) $28.5 Action:Think[response: None of the products match the specific requirements. should search again with different keywords.] Observation: OK Action:search[product: quadshield solid copper black coaxial cable] Observation: WEB PAGE: There is no [Search] button, you should click the [Back to Search] button first to search something Action:click[button: Back to Search] 20 Preprint. Observation: WEB PAGE: Let us begin shopping [Search] Action:search[product: quadshield solid copper black coaxial cable] Observation: WEB PAGE: [Back to Search] Page 1 (Total results: 15) [Next ] [B07H9G2RPM] PHAT SATELLITE INTL - 3GHz Direct Burial Underground RG6 Coaxial Cable, Tri-Shield Coax, Gel Coated Braids, Weather Boot Compression Connectors, Satellite Approved, Made in USA (12 feet, Orange) $13.0 [B07H9GYZPD] PHAT SATELLITE INTL - 3GHz Direct Burial Underground RG6 Coaxial Cable, Tri-Shield Coax, Gel Coated Braids, Weather Boot Compression Connectors, Satellite Approved, Made in USA (100 feet, Orange) $30.5 [B073KR5D4W] 135ft Black Quad Shield Solid Copper 3GHZ RG-6 Coaxial Cable 75 Ohm (DIRECTV Satellite TV or Broadband Internet) Anti Corrosion Brass Connector RG6 Fittings Assembled in USA by PHAT SATELLITE INTL $52.58 Action:Think[response: The third product matches the specific requirements. should click the third product.] Observation: OK Action:click[button: B073KR5D4W] Observation: WEB PAGE: [Back to Search] [Prev] size [1ft][2ft][3ft][4ft][5ft][6ft][8ft][10ft][12ft][15ft][20ft][25ft][25 ft][30ft][35ft][40ft][45ft][50ft][55ft][60ft][65ft][70ft][75ft][80ft] [85ft][90ft][95ft][100ft][105ft][110ft][115ft][120ft][125ft][130ft][135ft][140ft][145ft][150ft][155ft][160ft][165ft][170ft][175ft][180ft] [185ft][190ft][195ft][200ft][210ft][220ft][230ft][240ft][250ft][260ft][270ft][280ft][290ft][300ft][310ft][320ft][330ft][340ft][350ft] [360ft][370ft][380ft][390ft][400ft] style [quadshield - black][quadshield - white][quadshield solid copper - black][solid copper core - black][solid copper core - white][solid copper weather boot - black][solid copper weather boot - white][trishield - black][trishield - white][usa made trishield - black][usa made trishield - white][direct burial - black][direct burial - orange][direct burial 3ghz - orange][direct burial 3ghz weather boot - ora...][direct burial rg-11 connector - orange][direct burial rg-11 weather boot - or...][direct burial rg-11 connector - black][direct burial rg-11 weather boot - bl...][direct burial weather boot - black][direct burial weather boot - orange][plenum rg-11 - white][w ground - black][w ground - white][3ghz dual ground - black][3ghz messenger - black][dual - black][dual copper ground - black][dual copper ground - white][w messenger - black] 135ft Black Quad Shield Solid Copper 3GHZ RG-6 Coaxial Cable 75 Ohm (DIRECTV Satellite TV or Broadband Internet) Anti Corrosion Brass Connector RG6 Fittings Assembled in USA by PHAT SATELLITE INTL Price: $52.58 [Buy Now] Action:click[button: 20ft] Observation: WEB PAGE: You have clicked 20ft. Action:click[button: quadshield solid copper - black] Observation: WEB PAGE: You have clicked quadshield solid copper - black. Action:click[button: Buy Now] Observation: shopping is finished."
        }
    ],
    "affiliations": [
        "OpenAI",
        "Pennsylvania State University",
        "UCLA",
        "University of Maryland"
    ]
}