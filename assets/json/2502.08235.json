{
    "paper_title": "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks",
    "authors": [
        "Alejandro Cuadron",
        "Dacheng Li",
        "Wenjie Ma",
        "Xingyao Wang",
        "Yichuan Wang",
        "Siyuan Zhuang",
        "Shu Liu",
        "Luis Gaspar Schroeder",
        "Tian Xia",
        "Huanzhi Mao",
        "Nicholas Thumiger",
        "Aditya Desai",
        "Ion Stoica",
        "Ana Klimovic",
        "Graham Neubig",
        "Joseph E. Gonzalez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at https://github.com/AlexCuadron/Overthinking."
        },
        {
            "title": "Start",
            "content": "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks Alejandro Cuadron 1 2 Dacheng Li 1 Wenjie Ma 1 Xingyao Wang 3 Yichuan Wang 1 Siyuan Zhuang 1 Shu Liu 1 Luis Gaspar Schroeder 1 Tian Xia 1 Huanzhi Mao 1 Nicholas Thumiger 2 Aditya Desai 1 Ion Stoica 1 Ana Klimovic 2 Graham Neubig 4 Joseph E. Gonzalez 1 5 2 0 2 2 1 ] . [ 1 5 3 2 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) represent breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMsa phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments such as selecting the solution with the lower overthinking score can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at https://github.com/ AlexCuadron/Overthinking. 1Department of EECS, University of California, Berkeley, USA 2Department of Computer Science, ETH, Zurich, Switzerland 3Department of Computer Science, University of Illinois UrbanaChampaign, USA 4Department of Computer Science, Carnegie Mellon University, USA. Correspondence to: Alejandro Cuadron <acuadron@berkeley.edu>. Figure 1. Higher overthinking scores (tendency to favor internal reasoning over environmental feedback) correlate with lower issue resolution rates across all models. Reasoning models exhibit consistently higher overthinking tendencies, suggesting that excessive reliance on internal simulation impairs task performance. Model nomenclature: FC indicates native function calling capability, DS represents DeepSeek models, and suffixes o1 high and o1 low denote models with reasoning effort set to high and low respectively. 1. Introduction Large Reasoning Models (LRMs) (Guan et al., 2025; Xu et al., 2025), such as OpenAIs o1 (OpenAI, 2024e), Alibabas QwQ (Qwen, 2024b), or Deepseeks R1 (Guo et al., 2025) represent breakthrough in large language models (LLMs). These advanced systems have fundamentally redefined AIs problem-solving capabilities across various domains (Besta et al., 2025). In particular, LRMs selfcorrection abilities enable them to achieve impressive scores in several benchmarks, such as AIME 2024 (AoPS, 2024), MMLU (Hendrycks et al., 2021), or GPQA-Diamond (Rein et al., 2023) among others (Guo et al., 2025; OpenAI, 2024e;d; Qwen, 2024b; Guan et al., 2025). Despite extensive analysis of LRMs in non-agentic environ1 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks Figure 2. OpenHands Execution Pipeline. 1) The system initializes by presenting the agent with the primary issue and previous action history. 2) The agent reaches decision point 2a) Direct action formulation and execution, or 2b) Internal simulation of potential actions and outcomes, potentially leading to overthinking. 3) The chosen action is executed, generating environmental feedback which updates the event stream. This cycle continues until task completion. ments, there remains critical gap in understanding how LRMs perform in agentic environments (Smeyatsky, 2024), where models must simultaneously gather, retain, and act upon new information to complete their tasks (Zhang et al., 2024; Yang et al., 2024b). In this context, LRMs face fundamental challenge: models must choose between engaging directly with their environment or relying on internal reasoning about potential actions and their hypothetical consequences, challenge we define as the Reasoning-Action Dilemma. In this work, we present the first comprehensive empirical study of LRMs in agentic tasks at balancing the ReasoningAction Dilemma, using real-world software engineering tasks as our experimental framework (Jimenez et al., 2024; Yang et al., 2024b). We employ SWE-bench Verified (Jimenez et al., 2024; OpenAI, 2024) as our benchmark, using the CodeAct agent scaffolding (Wang et al., 2024a) within the OpenHands framework (Wang et al., 2024c). This setup creates controlled environment where models must balance information gathering with reasoning chains while maintaining context across multiple interactions as illustrated in Figure 2. proper balance becomes critical as too much reliance on internal reasoning chains might lead to false assumptions about the environment. Figure 3. Performance comparison of Pass@k and Lowest Overthinking@k on SWE-bench Verified. Pass@k represents the success rate when considering solutions, while Lowest Overthinking@k shows the success rate when selecting the solution with minimal overthinking from samples. Using k=2 samples with low reasoning effort, we achieve 27.3% success rate while reducing computational costs by 43% compared to high reasoning configurations. Increasing to k=3 further improves performance to 30.3% surpassing the high configuration using 15% less computational costs. The confidence intervals (CI) were computed using Wilson score (Wallis, 2013). phenomenon we define as overthinking. To quantify overthinking, we develop and validate systematic evaluation framework using LLM-as-a-judge (Zheng et al., 2023) that identifies three key patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement (Figure 4). Our scoring system strongly correlates with human expert assessments (Figure 5), confirming its reliability in measuring models tendency to favor internal simulation over environmental interaction. We applied this framework to analyze 4018 trajectories, creating comprehensive open-source dataset to advance research in balancing reasoning and action in agentic environments. Statistical analysis reveals two distinct patterns in overthinking behavior. First, regression analysis demonstrates significant negative correlation between overthinking and issue resolution rates for both reasoning and non-reasoning models (Figure 1), with the latter showing steeper decline in performance as overthinking increases. Second, direct comparison reveals that reasoning models consistently exhibit higher overthinking scoresnearly three times higher than non-reasoning modelswith this difference being statistically significant as shown afterward in Table 2. These patterns suggest that while all models are susceptible to overthinking, reasoning models are particularly prone to this behavior. We observe that LRMs exhibit consistent pattern of favoring internal simulation over environmental interaction in the Reasoning-Action Dilemma, spending increasing amounts of time constructing elaborate chains of predicted actions rather than adapting to actual system responses, Addressing overthinking yields substantial practical benefits. Running o1 with high reasoning effort achieves 29.1% issue resolution but costs $1,400, while the low reasoning variant reaches 21.0% at 3.5 lower cost ($400). Instead of using the expensive high-reasoning configuration, we found that The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks generating two solutions with low reasoning effort ($800 total) and selecting the one with lower overthinking score achieves 27.3% resolution rate (Figure 3). This simple strategy nearly matches the performance of high-reasoning configurations while reducing computational costs by 43%, demonstrating that overthinking mitigation can dramatically improve the efficiency of LRMs in real-world applications. Additionally, we suggest two potential approaches to mitigate overthinking in LRMs in agentic environments: native function-calling capabilities and selective reinforcement learning. Both approaches could significantly reduce overthinking while improving model performance, with functioncalling models showing particularly promising results (Section 6.3). To facilitate further research into these solutions, we release our evaluation framework and dataset, enabling the broader research community to build upon these findings across different environments and architectures. 2. Background and Related Work In this section, we explore Large Reasoning Models (LRMs) and agentic environments, where LRMs must balance sophisticated reasoning with practical actions. We examine how these models navigate environments requiring deep analytical thinking and concrete interactions. This leads to fundamental dilemma between reasoning depth and actiona tension we will explore in Section 3. 2.1. Large Reasoning Models and Agentic Environments LRMs, defined as language models optimized through process reward models and test-time compute scaling (Xu et al., 2025), represent an evolution beyond traditional LLMs through their focus on reliable step-by-step reasoning (Guo et al., 2025; OpenAI, 2025c). These models achieve unprecedented performance through extended chainof-thought reasoning (Wei et al., 2023) and rigorous selfverification (Madaan et al., 2023). However, their extended focus on internal reasoning depth raises important questions about performance in interactive environments. Agency in AI Systems While traditional AI defined agents broadly as entities that perceive and act upon their environment (Russell & Norvig, 1995), modern approaches view agency as spectrum of capabilities (Zhang et al., 2024; Kapoor et al., 2024), emphasizing autonomous goal pursuit, natural language interfaces, and structured outputs like tool use (Yang et al., 2024b). This framework has been particularly influential in software engineering, where various agent architectures (Research, 2024; Blog, 2024; Liu et al., 2024) have been developed to solve real-world GitHub issues (Jimenez et al., 2024). Our work examines how LRMs distinctive reasoning capabilities affect their performance in these agentic environments. 3. Overthinking 3.1. The ReasoningAction Dilemma We observe that, in agentic decision-making tasks, LRMs constantly face the ReasoningAction Dilemma where they must navigate fundamental trade-off between: Direct interaction with the environment, where the model executes actions and receives feedback. Internal reasoning, where the model reasons over hypothetical outcomes before committing to an action. Ideally, an LRM should balance action and reasoning by using internal simulation to refine its choices while leveraging real-world feedback to correct errors. For instance, when debugging failing test case, well-balanced model would hypothesize potential issues yet still execute the test opportunely to collect concrete failure signals. Unfortunately, achieving this balance is inherently challenging in agentic environments. On one hand, direct interaction with the environment is time and space (i.e. in-context memory is limited) consuming. On the other hand, prior research has demonstrated that LRMs exhibit significant vulnerability to knowledge insufficiency, where gaps in understanding can cascade into compounding errors throughout the reasoning process (Li et al., 2025; Zhong et al., 2024; Ling et al., 2023; Chia et al., 2024). Consequently, excessive simulation without sufficient external information can ultimately lead to failure. The situation is especially difficult for environments with limited interaction opportunities. We observe that LRMs face fundamental tension between incorporating environmental feedback and relying on internal reasoning chains, challenge exacerbated by their prompt sensitivity (OpenAI, 2024c; Guo et al., 2025). As reasoning steps accumulate in the context, they can overshadow or distort the interpretation of real-world information in subsequent iterations. We observed that reasoning models consistently resolve this tension by favoring their internal simulations over environmental signals. Overthinking To capture this potential failure mode in agentic settings, we define overthinking as the tendency of an LRM to rely excessively on internal reasoning while failing to seek or integrate essential external feedback. Even with an unbounded resource budget, such an agent remains constrained by the limitations of its partial or inaccurate world model, leading to compounding errors and impaired decision-making. 3.2. Manifestations of Overthinking Our investigation into impaired decision-making in AI agents draws from detailed analysis of agent-environment 3 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks Figure 4. Three distinct patterns of overthinking behavior in LRM agent trajectories. (a) Analysis Paralysis: the agent spends excessive time planning future steps while making minimal environmental progress. (b) Rogue Actions: facing errors, the agent attempts to execute multiple actions simultaneously, breaking the environments sequential constraints. (c) Premature Disengagement: the agent terminates based on internal predictions rather than environmental feedback. interactions. These interactions are recorded in what we term trajectories. Comprehensive logs that capture the complete sequence of agent actions, environment responses, and (where available) the agents reasoning process. As outlined in Section 4, we systematically analyzed these trajectories to understand patterns of overthinking. While most trajectories include the agents explicit reasoning process, those from the o1 family exclude these reasoning tokens (OpenAI, 2024c). This limitation led us to focus our analysis on observable behaviors, which are the concrete actions agents take in response to environmental challenges. Through this analysis, we identified three distinct patterns of overthinking: Analysis Paralysis, where agents become stuck in excessive planning; Premature Disengagement, where agents abandon tasks prematurely; and Rogue Actions, where agents seem to get stressed and generate multiple actions on the same iteration. These actions are exemplified in Figure 4. Analysis Paralysis LRMs tend to shift their focus from immediate actions to elaborate future planning. They generate increasingly complex action sequences but struggle to execute them systematically (Figure 4a). Rather than addressing immediate errors, they construct intricate plans that often remain unexecuted, leading to cycle of planning without progress. Rogue Actions We observe cases where agents deliberately generate chains of interdependent actions in single step, without awaiting feedback from the environment (Figure 4b). Despite their prior demonstrated awareness of step-by-step interaction requirements, models proceed to construct elaborate action sequences that presume the success of each preceding step, effectively substituting real environmental feedback with internal simulation. Premature Disengagement LRMs sometimes terminate tasks based solely on their internal simulation of the problem space, either through direct abandonment or by delegating hypothetical action sequences (Figure 4c). This illustrates how overreliance on internal reasoning can lead to decisions without environmental validation. 3.3. Quantifying Overthinking Overthinking Score To quantify overthinking behavior, we developed systematic scoring method using an LLMbased evaluator. This evaluator analyzes model trajectories for the previously described patterns and assigns score of 0 to 10, with higher scores indicating more severe overthinking behavior. Each score includes detailed justification explaining which patterns were identified and their severity. The complete evaluation prompt and scoring criteria can be found in Appendix A. To validate our LLM-based evaluator, we conduct an independent assessment where four expert annotators manually scored 20 randomly selected model traces, as shown in Figure 5. Using these standardized scores, we conduct comprehensive statistical analysis to investigate the relationship between overthinking behavior and model performance and how overthinking affects LRMs compared to non-reasoning models. The tools used for the statistical analysis can be 4 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks outcomes. 4. Evaluation Framework We analyze LRMs performance in agentic environments using SWE-bench Verified (OpenAI, 2024), comparing reasoning models with their non-reasoning counterparts. Our study aims to answer the following research questions: RQ1: Does overthinking affect agentic performance? RQ2: How does it impact different models? RQ3: Can we mitigate overthinking? 4.1. Experimental setup OpenHands To demonstrate how AI agents operate, we use the OpenHands framework (Wang et al., 2024c), which implements complete agent-environment interaction cycle as illustrated in Figure 2. Through this framework, agents receive set of tools to interact with their environment, along with examples of the proper usage of tools (Wang et al., 2024a). The agent processes this information and can execute actions through these tools, receiving immediate environmental feedback. This feedback is then incorporated into the agents context, enabling in-context learning (Dong et al., 2024) and self-refinement (Madaan et al., 2023) through successive interactions. The framework supports both native function-calling capabilities (OpenAI, 2024a) and structured text output, adapting to different model architectures while maintaining consistent interaction protocol. In this work, we leverage OpenHands comprehensive instrumentation capabilities to systematically analyze how models balance the Reasoning-Action Dilemma, revealing previously unexamined patterns in their interaction behavior. SWE-Bench Software engineering tasks present an ideal environment for studying agent behavior, as they require both sophisticated reasoning and continuous interaction with the environment (Jimenez et al., 2024). SWE-Bench captures this complexity by presenting agents with real-world software issues that demand multiple steps to resolve: agents must understand the problem, explore the codebase, reason about potential solutions, and validate their changes through testing (Yang et al., 2024b). This multi-step nature creates natural tension between reasoning and action, ideal for testing how models balance the Reasoning-Action Dilemma. In this work, we present the first systematic framework for quantifying how LRMs navigate this fundamental tension, revealing that excessive reliance on internal reasoning often comes at the cost of effective environmental interaction and task completion. Figure 5. Validation of our automated overthinking detection methodology against expert human evaluators. The strong correlation between human and automated scores demonstrates the reliability of our approach. Reasoning models consistently show higher overthinking scores compared to non-reasoning models. found in Appendix C. Overthinking prompt We craft prompt to systematically evaluate trajectories to detect overthinking behavior. We avoid utilizing the word overthinking as it could bias the model into using its own definition. Instead, we base the prompt around the manifestations of overthinking defined in Section 3.2 and the preference for internal reasoning chains over environmental interaction. The prompt first establishes core principles for identifying the three manifestations: Analysis Paralysis (excessive planning), Rogue Actions (multiple actions without waiting for feedback), and Premature Disengagement (concluding tasks without environmental validation). We then implement structured scoring system ranging from 0-10, where lower scores (0-3) indicate appropriate environment interaction, middle scores (4-7) suggest occasional overreliance on internal reasoning, and high scores (8-10) represent complete detachment from environmental feedback. To ground these criteria, we provide concrete examples: model receiving score of 0 might persistently retry similar configurations while waiting for feedback between attempts, whereas model scoring 10 might generate multiple interdependent actions without awaiting environmental response or prematurely conclude tasks based solely on internal reasoning. The trajectory intentionally excludes information about whether the fix succeeded or failed, preventing the model from developing biases based on solution 5 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks Models Evaluated To comprehensively study the phenomenon and influence of overthinking, we consider 19 models across multiple dimensions, including reasoning capabilities, model openness (proprietary vs. open-weight), model size, and function calling support. We evaluate both reasoning-optimized models as well as general-purpose language models. Our evaluation spans proprietary models (e.g., OpenAI o1, Claude Sonnet 3.5) (OpenAI, 2024c; Anthropic, 2024) and open-weight alternatives (e.g., DeepSeekR1, Qwen2.5) (Yang et al., 2024a; Qwen, 2024a; Guo et al., 2025) to ensure broad coverage. We also analyze models of varying scales, ranging from small (1.5B-14B) to largescale models (32B-671B parameters) (DeepSeek, 2025), to investigate whether model size influences overthinking tendencies. Additionally, we distinguish between models that natively support function calling (e.g., OpenAI o1, GPT-4o) (OpenAI, 2024a;b; 2025b;c) and those that do not, which allows us to assess whether explicit function calling capabilities reduce overthinking compared to models that rely on prompt-based learning of tool usage. Further details on the models studied can be found in the Appendix B, Table 5. Scaffoldings Models are not able to directly execute code or edit files. So, we adopt CodeAct, an open-source single-agent scaffolding built within the OpenHands framework (Wang et al., 2024b; Qwen, 2024b; OpenAI, 2024d;e; Guo et al., 2025; NovaSky, 2025). Scaffolding provides structured execution environment, allowing models to interact with SWE-bench in controlled and consistent manner. We choose the single-agent approach as it maintains unified reasoning process, ensuring full context retention throughout execution. In contrast, multi-agent scaffolds distribute tasks across multiple specialized agents that share an underlying model but operate with distinct prompts and action spaces (Chen et al., 2024a; Xia et al., 2024; Phan et al., 2024; Neubig, 2024) which can introduce structural rigidity and lead to information loss during inter-agent communication (Neubig, 2024). Therefore, we ensure all models are evaluated in standardized, interactive environment. Overthinking Score Calculation To ensure reliability and consistency, we employ Claude Sonnet 3.5 as the evaluation model and configure it with temperature of 0 to enforce deterministic scoring, following the LLM-as-a-judge methodology (Zheng et al., 2023). Claude Sonnet 3.5 is selected for its 200K-token context window, allowing it to process complete trajectories alongside the evaluation criteria. Notably, the evaluator does not have access to the final issue resolution outcome, ensuring that the overthinking assessment remains independent of task success and thereby eliminating potential biases. 6 5. Results We generate and evaluate 3908 trajectories using our evaluation methodology across all models. We make publicly available every trajectory alongside their corresponding overthinking score and the reasoning behind this score. Our analysis reveals three key findings about overthinking in language models: its impact on model performance, its varying prevalence across model types, and its practical implications for model selection. Illustrated in Figure 3. We observe that overthinking consistently impacts performance across all evaluated models, with reasoning-optimized models showing higher overthinking tendencies than generalpurpose ones as illustrated in Figure 1. 5.1. Overthinking and Issue resolution We observe strong negative correlation between overthinking and performance on SWE-bench, as illustrated in Figure 1. Both reasoning and non-reasoning models show decreased performance as overthinking increases, though with notably different patterns. 5.2. Overthinking and Model Type We make three key observations with regard to overthinking in reasoning and non-reasoning models. The results are presented in Figure 1. First, we observe that non-reasoning models can also overthink, likely due to their latent reasoning capabilities. Recent studies suggest that non-reasoning models also exhibit reasoning abilities (Wei et al., 2023; Yao et al., 2023; Chen et al., 2023; Kojima et al., 2023). Second, reasoning models exhibit significantly higher overthinking scores than non-reasoning models, as shown in Table 3. Since these models are explicitly trained for reasoning and generate extended chains of thought by simulating environmental interactions, they are more likely to suffer overthinking manifestations."
        },
        {
            "title": "Model",
            "content": "β1 R2 p-value Reasoning Non-Reasoning -7.894 -15.938 0.892 0. 0.000 0.010 Table 1. Regression Results for Reasoning and Non-Reasoning Models Lastly, we also observe that non-reasoning models that overthink suffer from severe degradation in issue resolution, as indicated by the beta coefficients in Table 1. lower beta coefficient corresponds to more significant impact of overthinking on performance. We suspect that since nonreasoning models are not trained for reasoning, they are The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks"
        },
        {
            "title": "Value",
            "content": "Reasoning Models Non-Reasoning Models 3.505 1.774 2.228 0.751 Table 2. Average Overthinking Scores for Reasoning and NonReasoning Models not capable of handling reasoning chains effectively, thus showing worse results. 5.3. Overthinking and Model Size Our evaluation examines two model families across three size variants (32B, 14B, 7B): the non-reasoning Qwen2.5Instruct and the reasoning R1-Distill-Qwen (Yang et al., 2024a; Qwen, 2024a; Guo et al., 2025). As illustrated in Figure 6, our analysis suggests negative correlation between model size and overthinking behavior. We hypothesize that smaller models struggle with environmental comprehension, causing them to rely more heavily on internal reasoning chains and increasing their tendency to overthink. The relationship between model size and overthinking manifests differently across model types. As shown in Table 3, both reasoning and non-reasoning models show higher overthinking scores as their size decreases, with reasoning models consistently exhibiting greater susceptibility to overthinking. However, the gap in overthinking scores between reasoning and non-reasoning models narrows significantly as model size decreases further. This convergence in overthinking behavior among smaller models towards high overthinking scores likely stems from their shared difficulty in processing environmental complexity. When faced with repeated failures in environmental interactions, these models appear to retreat to their internal reasoning chains and disregard external feedback. While this pattern aligns with our observations, further investigation is needed to confirm the underlying cause."
        },
        {
            "title": "Value",
            "content": "DS-R1 Family Qwen2.5 Family 6.700 1.656 5.001 1.732 Table 3. Overthinking Score Comparison for R1 and Qwen2.5 Figure 6. This graph showcases that both families suggest negative correlations between overthinking and model size. With reasoning and non-reasoning models showing close overthinking scores in their 7B and 14B counterparts soning tokens used (OpenAI, 2025a). Our analysis reveals that o1 models with low reasoning effort demonstrate 35% higher overthinking scores compared to their high-effort counterparts. As shown in Table 4, the difference in averaged overthinking scores between the two configurations is statistically significant, suggesting that increased token allocation might reduce overthinking in agentic contexts. This finding challenges the perception that increased reasoning token usage correlates with overthinking as shown by some recent studies (Chen et al., 2024b). Instead, our results indicate that having more reasoning tokens can effectively curb overthinking, highlighting the importance of structured reasoning processes in model behavior."
        },
        {
            "title": "Value",
            "content": "o1 Low o1 High 2.774 3.081 2.426 2.880 Table 4. Overthinking scores comparison between o1 model configurations with low and high reasoning effort settings 5.4. Overthinking and Token Usage Prior research has suggested that token usage can serve as an indicator for overthinking (Chen et al., 2024b). To investigate this relationship, we analyze the o1 model, manipulating its reasoning effort parameter between high and low settings, which directly influences the number of rea5.5. Overthinking and Context Window We analyze models across different context window sizes, ranging from 8K to 32K tokens. We observe no significant correlation between context window size and overthinking scores when comparing models of similar architectures and sizes but different context windows. For instance, comparing Qwen2.5-32B (32K context) with QwQ-32B (32K 7 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks context) shows overthinking scores of 2.31 0.42 and 2.28 0.39 respectively (p > 0.05). We hypothesize that this lack of correlation may be because overthinking behaviors are more influenced by models architectural design and training approach rather than its context capacity. This aligns with our earlier findings about the importance of model type and size in determining overthinking tendencies. 5.6. Practical Implications OpenAI showcased that reasoning models exhibit disproportionate increase in computational costs relative to their performance gains (ARC, 2024). Our experiments with SWE-bench Verified dataset confirm this observation: o1 with high reasoning effort achieves 29.1% resolution rate at $1,400, while the low reasoning variant reaches 21.0% at $400 3.5 cost difference for an 8.1 percentage point improvement in performance. Metrics. To address this efficiency gap, we computed the (1) Pass@k, which represents the percentage of tasks where at least one successful solution is found among sampled trajectories, and (2) Lowest Overthinking@K, which selects the trajectory with the lowest overthinking score among samples and reports the percentage of these selected trajectories that are successful. Pass@K evaluates the models ability to find any working solution (i.e., the upper bound for Lowest Overthinking@K), while Lowest Overthinking@K assesses our models capability to identify the most promising solution as illustrated in Figure 3. The confidence intervals (CI) showcased were computed using Wilson score (Wallis, 2013) This method of selecting solutions based on overthinking scores yields impressive efficiency gains. By limiting to two samples with the lowest reasoning, we achieve 27.3% resolution rate while consuming only 57% of the highreasoning configurations cost ($800 vs $1,400). Furthermore, with three samples we surpass the high-reasoning baseline (30.3% vs 29.1%) while still saving $200 in computational costs. Our findings demonstrate that monitoring and controlling overthinking behavior is highly effective strategy for optimizing both the performance and efficiency of language reasoning models in real-world applications. 29.1% to 47.7%, while simultaneously reducing the average overthinking score from 2.43 to 1.05 effectively mitigating the overthinking phenomenon. However, benchmarking against BCFL (Yan et al., 2024) reveals more nuanced pattern, where the performance differential between FC and non-FC implementations of o1 in multi-turn environments shows modest improvement from 36% to 41%. This comparatively smaller enhancement suggests that FC implementation alone cannot fully account for the dramatic performance improvements observed in our primary experiments. 6.2. Why doesnt DeepSeek-R1-671B overthink? Our analysis of DeepSeek-R1-671B (DS-R1) reveals overthinking scores comparable to those of DeepSeek-V3-671B. This similarity in overthinking behavior may be attributed to DS-R1s training methodology, which does not incorporate extensive reinforcement learning for software engineering tasks. While DS-R1 maintains performance levels similar to DeepSeek-V3 on software engineering benchmarks (Guo et al., 2025), our findings suggest that the combination of limited RL training and substantial model scale (671B parameters) contributed to its controlled overthinking behavior. 6.3. How to fix overthinking? While our algorithmic interventions demonstrate immediate practical benefits, they primarily address the symptoms rather than the root causes of overthinking. Our analysis suggests that more fundamental solutions might emerge from understanding how models learn to balance reasoning and environmental interaction. The success of function-calling architectures hints at the importance of explicit interaction training, while the effectiveness of limited reinforcement learning points to the role of training methodology. These insights open important questions for future research: How do these approaches generalize across different domains? How can we optimize for environments where environmental interaction carries varying costs? Understanding these dynamics could help develop more robust solutions that prevent, rather than just mitigate, overthinking behaviors in large reasoning models. 6. Discussion 7. Conclusion 6.1. Can native function calling affect overthinking? Our experimental analysis compares o1 model configurations with high reasoning effort, evaluating performance both with and without native function calling (FC) capabilities. The integration of FC capabilities yields substantial improvements, increasing the performance score from In this work, we present the first comprehensive empirical study of Large Reasoning Models (LRMs) in agentic environments. We identify fundamental challenge: the Reasoning-Action Dilemma, in which models must balance environmental engagement against internal reasoning about potential actions and their hypothetical consequences. Our analysis reveals that LRMs consistently favor internal 8 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks simulation over environmental interaction, behavior we define as overthinking. Through our systematic evaluation framework, we analyzed 3,908 trajectories using novel overthinking score metric. Our findings demonstrate strong correlation between overthinking and task failure rates, with reasoning models showing particularly high vulnerability to this phenomenon compared to their non-reasoning counterparts. Our research demonstrates that even simple interventions to mitigate overthinking can yield substantial benefits: 43% reduction in inference costs while improving issue resolution rates by 25% on SWE-bench Verified dataset. These results, combined with our observations about the effectiveness of function-calling capabilities and targeted reinforcement learning, suggest promising directions for developing more efficient and environmentally grounded reasoning models particularly for agentic tasks. 8. Acknowledgments The authors thank Siavash Ameli, Jiayi Pan, and Yilong Zhao for their invaluable contributions. They also thank SkyLab at UCB, OpenHands, NeuLab at CMU, and Lambda Cloud for their support."
        },
        {
            "title": "Impact Statement",
            "content": "This paper advances our understanding of how Large Reasoning Models (LRMs) balance internal reasoning with environmental interaction, critical factor in their real-world deployment. By introducing the first systematic framework for quantifying overthinking behaviors, we enable more efficient and effective AI systems that can better allocate their computational resources between reasoning and action. Our open-sourced dataset and evaluation framework provide the research community with tools to develop more balanced AI agents, potentially reducing both computational costs and error rates in practical applications. This work has immediate implications for software engineering automation and broader applications in any domain where AI agents must interact with dynamic environments. 9 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks"
        },
        {
            "title": "References",
            "content": "Claude 3.5: sonnet of progress Anthropic. https://www.anthropic.com/news/ in ai. claude-3-5-sonnet, 2024. Accessed: 2024-11-21. AoPS. 2024 aime artofproblemsolving.com/wiki/index. php/2024_AIME_I, 2024. Accessed: 2025-01-22. i. https:// ARC. o3 breakthrough, publication Openai URL https://arcprize.org/blog/ 2024. oai-o3-pub-breakthrough. Accessed: 2024-1103. Besta, M., Barth, J., Schreiber, E., Kubicek, A., Catarino, A., Gerstenberger, R., Nyczyk, P., Iff, P., Li, Y., Houliston, S., Sternal, T., Copik, M., Kwasniewski, G., Muller, J., Flis, l., Eberhard, H., Niewiadomski, H., and Hoefler, T. Reasoning Language Models: Blueprint, January 2025. Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Liu, T., Chang, B., Sun, X., Li, L., and Sui, Z. survey on in-context learning, 2024. URL https://arxiv.org/abs/2301.00234. Guan, X., Zhang, L. L., Liu, Y., Shang, N., Sun, Y., Zhu, Y., Yang, F., and Yang, M. rstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.04519. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021. URL https: //arxiv.org/abs/2009.03300. Blog, A. D."
        },
        {
            "title": "Reinventing the\nsoftware",
            "content": "amazon dedevelopment. for agent veloper https://aws.amazon.com/blogs/devops/ reinventing-the-amazon-q-developer-agentfor-software-development/, 2024. Accessed: 2024-11-21. Chen, D., Lin, S., Zeng, M., Zan, D., Wang, J.-G., Cheshkov, A., Sun, J., Yu, H., Dong, G., Aliev, A., Wang, J., Cheng, X., Liang, G., Ma, Y., Bian, P., Xie, T., and Wang, Q. Coder: Issue resolving with multi-agent and task graphs, 2024a. URL https://arxiv.org/abs/ 2406.01304. Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 28358856. URL https://openreview.net/forum? id=YfZ4ZPt8zd. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., Wang, R., Tu, Z., Mi, H., and Yu, D. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2024b. URL https: //arxiv.org/abs/2412.21187. Chia, Y. K., Chen, G., Xu, W., Tuan, L. A., Poria, S., and Bing, L. Reasoning paths optimization: Learning to reason and explore from diverse paths, 2024. URL https://arxiv.org/abs/2410.10858. DeepSeek. Reasoning model guide. https: //api-docs.deepseek.com/guides/ reasoning_model, 2025. 24. Accessed: 2025-0110 Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770. Kapoor, S., Stroebl, B., Siegel, Z. S., Nadgir, N., and Narayanan, A. Ai agents that matter, 2024. URL https://arxiv.org/abs/2407.01502. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205.11916. Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z. Search-o1: Agentic search-enhanced large reasoning models, 2025. URL https://arxiv. org/abs/2501.05366. Ling, Z., Fang, Y., Li, X., Huang, Z., Lee, M., Deductive veriand Su, H. Memisevic, R., In Oh, fication of chain-of-thought A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), NeurIPS, 2023. URL http://dblp.uni-trier.de/db/conf/ nips/neurips2023.html#LingFLHLMS23. reasoning. Liu, Y., Gao, P., Wang, X., Liu, J., Shi, Y., Zhang, Z., and Peng, C. Marscode agent: Ai-native automated bug fixing, 2024. URL https://arxiv.org/abs/2409. 00899. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback, 2023. URL https:// arxiv.org/abs/2303.17651. The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks Neubig, G. Dont sleep on single-agent systems. https://www.all-hands.dev/blog/ dont-sleep-on-single-agent-systems, 2024. Accessed: 2024-11-21. NovaSky. Sky-t1: Train your own o1 preview model within $450. https://novasky-ai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. OpenAI. Openai function calling guide. https: //platform.openai.com/docs/guides/ function-calling, 2024a. Accessed: 2024-11-21. OpenAI. Gpt-4o mini: Advancing cost-efficient inhttps://openai.com/index/ telligence. gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024b. Accessed: 2025-01-22. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, November 2024b. URL https://qwenlm. github.io/blog/qwq-32b-preview/. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Research, I. Swe agents: Empowering software development with ai agents. https://research.ibm. com/blog/ibm-swe-agents, 2024. Accessed: 2024-11-21. Russell, S. J. and Norvig, P. Artificial Intelligence: Modern Approach. Prentice Hall, 1 edition, 1995. ISBN 978-0-13-103805-9. Google-Books-ID: CUVeMwAACAAJ. OpenAI. llms. learning-to-reason-with-llms/, [Online]. Learning with https://openai.com/index/ 2024c. reason to OpenAI. o1-mini: Openai reasoning. cost-efficient openai.com/index/ advancing-cost-efficient-reasoning/, 2024d. Accessed: 2024-11-22. Advancing https:// openai-o1-miniOpenAI. Openai o1 system card. https://openai. com/index/openai-o1-system-card/, 2024e. [Online]. OpenAI. fied. introducing-swe-bench-verified/, Accessed: 2025-01-24. Introducing verihttps://openai.com/index/ 2024. bench swe OpenAI. Chat api reference. https://platform. openai.com/docs/api-reference/chat, 2025a. Accessed: 2025-01-24. OpenAI. Gpt-4o mini model documentation. https://platform.openai.com/docs/ models#gpt-4o-mini, 2025b. Accessed: 2025-0124. OpenAI. Openai o1. https://openai.com/o1/, 2025c. Accessed: 2025-01-24. Phan, H. N., Nguyen, T. N., Nguyen, P. X., and Bui, N. D. Q. Hyperagent: Generalist software engineering agents to solve coding tasks at scale, 2024. URL https://arxiv.org/abs/2409.16299. Smeyatsky, A. Agentic ai whitepaper, 2024. URL https://www.linkedin.com/pulse/ agentic-ai-whitepaper-allan-smeyatsky-fpgff/. Accessed: 2024-11-03. Wallis, S. Binomial confidence intervals and contingency tests: Mathematical fundamentals and the evaluation of alternative methods. Journal of Quantitative Linguistics, 20(3):178208, 2013. doi: 10.1080/09296174.2013. 799918. Wang, X., Chen, Y., Yuan, L., Zhang, Y., Li, Y., Peng, H., and Ji, H. Executable code actions elicit better llm agents, 2024a. URL https://arxiv.org/abs/ 2402.01030. Wang, X., Li, B., Song, Y., Xu, F. F., Tang, X., Zhuge, M., Pan, J., Song, Y., Li, B., Singh, J., Tran, H. H., Li, F., Ma, R., Zheng, M., Qian, B., Shao, Y., Muennighoff, N., Zhang, Y., Hui, B., Lin, J., Brennan, R., Peng, H., Ji, H., and Neubig, G. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024b. URL https://arxiv.org/abs/2407.16741. Wang, X., Li, B., Song, Y., Xu, F. F., Tang, X., Zhuge, M., Pan, J., Song, Y., Li, B., Singh, J., et al. Openhands: An open platform for ai software developers as generalist agents, 2024c. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/ 2201.11903. Qwen. Qwen2.5: party of foundation models, September 2024a. URL https://qwenlm.github.io/ blog/qwen2.5/. Xia, C. S., Deng, Y., Dunn, S., and Zhang, L. Agentless: Demystifying llm-based software engineering agents, 2024. URL https://arxiv.org/abs/2407.01489. 11 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks Xu, F., Hao, Q., Zong, Z., Wang, J., Zhang, Y., Wang, J., Lan, X., Gong, J., Ouyang, T., Meng, F., Shao, C., Yan, Y., Yang, Q., Song, Y., Ren, S., Hu, X., Li, Y., Feng, J., Gao, C., and Li, Y. Towards large reasoning models: survey on scaling llm reasoning capabilities, 2025. URL https://arxiv.org/abs/2501.09686. Yan, F., Mao, H., Ji, C. C.-J., Zhang, T., Patil, S. G., Stoica, I., and Gonzalez, J. E. Berkeley function calling leaderboard. 2024. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., and Press, O. Swe-agent: Agentcomputer interfaces enable automated software engineering, 2024b. URL https://arxiv.org/abs/ 2405.15793. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv.org/abs/2305.10601. Zhang, W., Liao, J., Li, N., and Du, K. Agentic information retrieval, 2024. URL https://arxiv.org/abs/ 2410.09713. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https: //arxiv.org/abs/2306.05685. Zhong, T., Liu, Z., Pan, Y., Zhang, Y., Zhou, Y., Liang, S., Wu, Z., Lyu, Y., Shu, P., Yu, X., et al. Evaluation of openai o1: Opportunities and challenges of agi. arXiv preprint arXiv:2409.18486, 2024. 12 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks A. Prompt to detect overthinking Here, we provide the prompt used to assess the overthinking score. You are an AI judge focused on detecting when models prefer their internal reasoning chain over interacting with the environment. <INTERACTION> trajectory goes here </INTERACTION> Analyze the <INTERACTION> and determine if the model is preferring their internal reasoning chain over interacting with the environment: How could this be detected? <CORE PRINCIPLE> The model suffers from Analysis Paralysis, it focuses on heavy planning instead of interacting with the environment. The model suffers from Rogue actions, after facing setbacks, it generates multiple actions without waiting for the environment to process the previous action. The model suffers from Premature Disengagement, it concludes the task without checking with the environment. Either because it is overconfident in the solution or because it thinks it cant solve the problem. </CORE PRINCIPLE> <SCORING SYSTEM (0-10)> 0-3: Always interacting with the environment summary of what has been done so far is good, even if done multiple times. brief summary of the steps to take is good if the model interacts with the environment following steps one by one. Only one action per turn, finish and other actions are NOT allowed. Alternating between two operations is good. Trying the same approach over and over is good, even with long or complex actions, as long as the model waits for environment feedback each time. Repeating similar patterns or configurations is fine as long as the model interacts with the environment between attempts. Detailed reasoning and planning is good if it leads to concrete actions with environment interaction. 4-7: Sometimes relies too much on their internal reasoning chain, but still interacts with the environment. It engages in heavy planning, but still interacts with the environment. It NEVER concludes the task without checking with the environment. It might output multiple steps ONE time, but at subsequent turns it interacts one step at time. Long theoretical discussions are acceptable if they eventually result in concrete actions. 8-10: Completely relies on their internal reasoning chain. Focuses solely on their internal reasoning chain, with no concrete actions following the analysis. Generates multiple actions without waiting for environment response. The model prematurely concludes the task. Either because it is overconfident in the solution or because it thinks it cant solve the problem. Generates many steps without any environment interaction. Gets stuck in endless theoretical discussion without attempting solutions. </SCORING SYSTEM> <ANALYSIS STEPS> 1. Analysis Paralysis Is the model focusing on heavy planning instead of interacting with the environment? Does the model interact with the environment at all? Does the model follows its planned steps starting from the first one? 2. Rogue Actions Does the model generate multiple actions without waiting for the environment to process the previous action? Is this behavior after facing setback? Does this behaviour happen often? 3. Premature Disengagement Does the model prematurely conclude the task? Is the model overconfident in the solution? 13 The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks Is the model thinking it cant solve the problem? </ANALYSIS STEPS> <EXAMPLES> Example 1 - Persistent Retries (Good): EXECUTION RESULT: Error: Invalid configuration Model: *tries complex configuration A* EXECUTION RESULT: Error: Invalid configuration Model: *tries similar complex configuration with slight modification* EXECUTION RESULT: Error: Invalid configuration Model: *tries complex configuration again with another modification* Score: 0 - The model is persistently trying to solve the problem, waiting for environment feedback between each attempt. Even though the attempts are similar and complex, its properly interacting with the environment. Example 2 - Thoughtful Planning (Good): Model: *provides detailed analysis of the problem and potential approaches* Model: *tries specific solution based on analysis* EXECUTION RESULT: Error in implementation Model: *refines approach based on error and tries again* Score: 0 - While the model engages in detailed planning, it follows through with concrete actions and responds to environment feedback. Example 3 - Stuck in loop (Good): EXECUTION RESULT: ERROR Model: *apply fix 0* EXECUTION RESULT: ERROR Model: *apply SAME fix 0* EXECUTION RESULT: ERROR Model: *apply SAME fix 0* Score: 0 - Stuck in loop is good. Example 4 - Analysis Paralysis: EXECUTION RESULT: Invalid indentation line 10 Model: *Maybe should... Perhaps should... It should be... Let me try to start again rewriting the class* EXECUTION RESULT: Still invalid line 10 Model: *Its not working... We also need to fix this other thing...* EXECUTION RESULT: Same error line 10 Score: 10 - focuses on its internal reasoning chain instead of the environment. Example 5 - Premature Disengagement: EXECUTION RESULT: Invalid indentation line 10 Model: *This fixes it! Ill conclude the task. <function=finish>* Score: 10 - The model concludes the task without applying the fix or overconfidence in the solution. Example 6 - Rogue Actions: EXECUTION RESULT: Invalid indentation line 10 *Oh Model: again <function=str replace editor>...</function>* <function=str replace editor>...</function> forgot add the no, to old and string, then let me we do call this the other function thing Score: 10 - The model generates multiple actions after facing setback without waiting for the environment to process the previous action. The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks </EXAMPLES> <IMPORTANT> Format your response as: <answer> { \"overthinking_score\": \"[0-10]\", \"reasoning\": \"Explain your reasoning for the score, be careful with new lines as they might break the JSON parsing\" } </answer> Always surround your answer with <answer> and </answer> tags. Take your time to understand the interaction and analyze it carefully. Think step by step if models prefer their internal reasoning chain over interacting with the environment. </IMPORTANT> B. Model Specifications and Capabilities Category Model Params Context FC Notes Non-Reasoning Models (Open Source) DeepSeek-V3 Qwen 2.5-32B Qwen 2.5-14B Qwen 2.5-7B Qwen 2.5-1.5B Sky-T1-32B Non-Reasoning Models (Closed Source) GPT-4o GPT-4o-mini Claude 3.5 Sonnet Reasoning Models (Open Source) QwQ-32B DeepSeek-R1 R1-Distill-Qwen-32B R1-Distill-Qwen-14B R1-Distill-Qwen-7B R1-Distill-Qwen-1.5B Reasoning Models (Closed Source) 671B 32B 14B 7B 1.5B 32B - - - 32B 671B 32B 14B 7B 1.5B 128k 128k 128k 128k 128k 32k 128k 128k 200k 32k 128k 128k 128k 128k 128k MoE architecture Dense architecture Dense architecture Dense architecture Dense architecture QwQ distillation Aug 2024 version Jul 2024 version Oct 2024 version Preview version Based on V3 Based on Qwen 2.5 Based on Qwen 2.5 Based on Qwen 2.5 Based on Qwen 2.5 o1 o1-mini - - 200k 128k Dec 2024, RE Sep 2024 version Table 5. Comprehensive comparison of evaluated models. FC indicates native function calling support. Models are grouped by reasoning capabilities and source availability. Supports reasoning effort parameter (low/medium/high). C. Statistical principles utilized in this work Coefficient of Determination R2. The coefficient of determination, denoted by R2, is statistical measure of how well the regression predictions approximate the real data points. Formally, for set of observed values {yi}n i=1 with mean and corresponding fitted values {ˆyi}n i=1, it is defined as: R2 = 1 (cid:80)n (cid:80)n i=1(yi ˆyi)2 i=1(yi y)2 . It represents the proportion of the variance in the dependent variable that is explained by the regression model. P-value. Given null hypothesis H0 and test statistic (based on sample) used to decide whether to reject H0, the p-value The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks is the probability, under the assumption that H0 is true, of obtaining test statistic value at least as extreme as the one that was actually observed. Symbolically, if is the test statistic, and tobs its observed value, p-value = (cid:0)T tobs H0 (cid:1), for one-sided test (or an analogous definition for two-sided tests). smaller p-value indicates stronger evidence against H0. Beta Coefficients in Simple Linear Regression Consider simple linear regression model: where: β0 is the intercept (the predicted value of when = 0), Yi = β0 + β1 Xi + εi, β is the slope (the expected change in for one-unit increase in X). εi is the error term, assumed to have mean zero. In this context, the slope β1 is given by β1 = (cid:80)n i=1(Xi X)(Yi ) (cid:80)n i=1(Xi X)2 , which measures the strength and direction of the linear relationship between and . T-test of the p-value t-test assesses whether the mean(s) of one or two groups differ(s) from hypothesized value or from each other under the null hypothesis H0. Let be the test statistic calculated from the data (for instance, comparing sample mean(s) to the hypothesized mean(s)), and let tobs be the observed value of . The p-value for the t-test is then defined as: p-value = (cid:0)T tobs (cid:1) H0 for two-sided test (or correspondingly appropriate one-sided version). lower p-value provides stronger evidence against H0, suggesting that the observed difference is unlikely to have occurred under the null hypothesis. C.1. Definition of model-specific coefficients Definition C.1 (Model-Specific Coefficients). For the Reasoning Language Models, the fitted model is where ˆYR = β0,R + β1,R X, β1,R = 7.894. For the Non-Reasoning Language Models, the fitted model is where ˆYN = β0,N + β1,N X, β1,N = 15.938."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Carnegie Mellon University, USA",
        "Department of Computer Science, ETH, Zurich, Switzerland",
        "Department of Computer Science, University of Illinois UrbanaChampaign, USA",
        "Department of EECS, University of California, Berkeley, USA"
    ]
}