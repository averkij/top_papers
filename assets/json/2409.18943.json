{
    "paper_title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
    "authors": [
        "Jiaming Li",
        "Lei Zhang",
        "Yunshui Li",
        "Ziqiang Liu",
        "yuelin bai",
        "Run Luo",
        "Longze Chen",
        "Min Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data is available at https://github.com/Geaming2002/Ruler."
        },
        {
            "title": "Start",
            "content": ": Model-Agnostic Method to Control Generated"
        },
        {
            "title": "Length for Large Language Models",
            "content": "Jiaming Li1,2 Lei Zhang1,2 Yunshui Li1,2 Ziqiang Liu1,2 Yuelin Bai1,2 Run Luo1,2 Longze Chen1,2 Min Yang1 1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 2University of Chinese Academy of Sciences {jm.li4, min.yang}@siat.ac.cn 4 2 0 2 1 ] . [ 2 3 4 9 8 1 . 9 0 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The instruction-following ability of large language models enables humans to interact with AI agents in natural way. However, when required to generate responses of specific length, large language models often struggle to meet users needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the models performance in adhering to specified response lengths. Furthermore, we introduce novel, model-agnostic approach called RULER, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, RULER equips LLMs with the ability to generate responses of specified length based on length constraints within the instructions. Moreover, RULER can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of RULER across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of RULER. Our code and data is available at https://github.com/Geaming2002/Ruler."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across variety of natural language tasks and are increasingly being utilized in various fields (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020). primary Min Yang is the corresponding author. Figure 1: Existing LLMs lack the capability to follow instructions for generating texts of specified length. area of interest is the instruction following ability, referring to their capability to execute tasks or generate outputs based on instructions (Ouyang et al., 2022; Wei et al., 2022a). It reflects the models effectiveness in understanding and responding to instructions. The practical challenges highlight the complexity of achieving precise instruction following, particularly when users require control over the outputs length. Users frequently give LLMs various instructions, such as \"Tell me how to make cake in 20 words\", \"Use 50 words to write post\", \"Write 300-word story for me\" and so on. These instructions challenge the instruction following capability of LLMs. To explore how well LLMs handle such challenges, we focus on the scenario where users specify the target length of the responses. The question is posed, \"Can LLMs accurately generate with target length?\" and introduce the Target Length Generation Task (TLG). We create test dataset with various target lengths and introduce two evaluation metrics: Precise Match (PM) and Flexible Match (FM). Our findings reveal that current LLMs generally perform poorly in this task, indicating considerable room for improvement. discussion on the underlying causes is conducted, primarily attributing it to tokenization schemes and model training strategy."
        },
        {
            "title": "2 Related Work",
            "content": "To address aforementioned issues, we introduce RULER, model-agnostic approach designed to enhance the instruction-following capability of LLMs through Meta Length Tokens (MLTs). MLTs are designed to control models responses. By utilizing RULER, LLMs can generate responses that meet target lengths. We create dataset with MLTs DM LT for end-to-end training of LLMs. LLMs learn to generate MLT and the corresponding length response after training. During inference, if target length is provided, RULER can transform it into MLT and generate responses that meet the requirement. If no target length is specified, it first generates MLT, then the response, ensuring its length aligns with the generated MLT. We apply RULER to various large language models and test them on TLG. Each model demonstrates significant improvements. Across all evaluated models, we observe consistent improvement in both PM and FM scores at all Levels. The PM and FM scores across All Level showed an average improvement of 27.97 and 29.57.Furthermore, to rigorously test the capabilities of RULER, we randomly sample the dataset provided by Li et al. (2024a) and assess RULER on multi MLT generation and self-generated MLT experiment to show the its effectiveness and generalizability. Additionally, RULER is tested on six benchmarks to observe whether the models overall performance is affected. Our contributions can be summarized as follows: We introduce the Target Length Generation Task (TLG), which designed to assess the instruction following capability of LLMs. It evaluates how well models generate responses of target lengths as directed by instructions. We propose RULER, novel and modelagnostic approach which employs the Meta Length Tokens (MLTs). Through end-to-end training, it enables models to generate response matching the target lengths indicated by MLTs. We demonstrate that RULER significantly enhances the performance of various models on the TLG. Further experiments have also validated the effectiveness and generalizability of RULER."
        },
        {
            "title": "2.1 Large Language Model",
            "content": "The advent of LLMs has revolutionized the field of natural language processing and become milestone (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020; Zhang et al., 2023a). Large language models have achieved success across various NLP tasks. Models such as GPT-4(Achiam et al., 2023), Llama-3(AI@Meta, 2024), and Qwen(Bai et al., 2023), known for their powerful capabilities, are increasingly serving as the foundation for various applications and making significant inroads into diverse fields, exerting substantial impact. In-context learning enables LLMs to infer and generate responses solely based on the contextual information provided within prompt(Dong et al., 2022; Wei et al., 2022b). This capability allows the models to exhibit high degree of flexibility and adaptability across variety of tasks(Levine et al., 2022; Chen et al., 2022; Zhao et al., 2021). CoT further excavates and demonstrates the powerful logical reasoning capabilities of LLMs(Wei et al., 2022c; Huang and Chang, 2023; Zhang et al., 2023b). 2."
        },
        {
            "title": "Instruction Following",
            "content": "Instruction following refers to the ability of large language models to comprehend and execute given natural language instructions (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022a; Zhou et al., 2023a). This capability enables the models to perform broad spectrum of tasks, from simple query responses to complex problem-solving and content generation, tailored to specific user requests. In practical deployments, models may not adhere to comply with user instructions, exhibiting behaviors that deviate from anticipated outcomes. This includes generating responses unrelated to explicit instructions, emitting redundant or erroneous information, or entirely ignoring specified directives (Gehman et al., 2020; Kenton et al., 2021; Wei et al., 2024). To enhance the instruction following capability of LLMs, open-domain instruction following data is frequently used for training. Several prominent studies have explored the construction of instruct-tuning data, to achieve efficient and costeffective results(Li et al., 2024b; Cao et al., 2024; Liu et al., 2024; Xu et al., 2024). Level Target Length Precise Match (PM) Flexible Match (FM) Level:0 Level: Level:2 10 30 50 80 150 300 500 700 >800 10 10 10 10 20 20 70 (800, ) (0, 20] (20, 40] (40, 60] (60, 100] (100, 200] (200, 400] (400, 600] (600, 800] (800, ) Table 1: Nine target lengths and their corresponding match ranges categorized as Precise Match (PM) and Flexible Match (FM). Target lengths are classified into three categories, Level:0, Level:1, and Level:2."
        },
        {
            "title": "2.3 Meta Token",
            "content": "Recently, an increasing number of studies have employed custom tokens within language models to execute specific functions or enhance performance. Todd et al. (2024) report findings that the hidden states of language models capture representations of these functions, which can be condensed into Function Vector (FV). Furthermore, their research demonstrates that FV can effectively guide language models in performing specific tasks. Numerous studies have utilized meta tokens to compress prompts, thereby enhancing the inference capability of models (Li et al., 2023; Liu et al., 2023; Zhang et al., 2024). Mu et al. (2023)introduce the concept of \"gist tokens\", which can be cached and reused for compute efficiency. Further Jiang et al. (2024) utilize hierarchical and dynamic approach to extend the concept, proposing \"HD-Gist tokens\" to improve model performance."
        },
        {
            "title": "3 Can LLMs Accurately Generate with",
            "content": "Target Length? In this section, we examine the capability of LLMs to generate responses of target length. Initially, we introduce Target Length Generation Task (TLG). Subsequently, we establish various target lengths and two evaluation metrics (3.1). We then detail the experimental setup and assess the ability of LLMs to generate responses at target lengths (3.2). Finally, we present the outcomes of the experiments and analysis the underlying reasons(3.3)."
        },
        {
            "title": "3.1 Target Length Generation Task",
            "content": "To assess the ability of existing LLMs to control the length of generated response, we develop the TLG. This task assesses the models ability in producing responses that match target lengths as directed designed target lengths are detailed in Table 1. Additionally, we divide these nine target lengths into three levels: Level:0, Level:1, and Level:2. Given that generating responses with target lengths is challenging for existing LLMs, we develop two metrics to evaluate the accuracy of response lengths. Precise Match (PM): This metric requires that the length of the generated response be very close to the target length. For different Level, precise tolerance range is set (10, 20, . . . ) necessitating that the response length stringently conforms to these defined limits. Flexible Match (FM): This metric requires broader tolerance interval for target length. the range incrementally For longer texts, widens to meet response generation requirements. For the responses, we assess whether response meets the target length, then calculating the PM and FM scores of the model. (cid:80)N i=1 1 (cid:0)lbP TLi PM = < (ci) ubP TLi (cid:80)N i=1 1 (cid:0)lbF TLi FM = < (ci) ubF TLi (cid:1) (cid:1) (1) (2) where: ci denotes the i-th response generated by LLM. The function L() calculates the word count of the input string. lbP denote the lower and upper bounds of the precise match and ubP TLi TLi Model Params Level:0 Level:1 Level:2 All Level Target Length Generation Task (TLG) PM FM PM FM PM FM PM FM gpt-4-turbo gpt-4o gpt-3.5-turbo claude-3-haiku claude-3.5-sonnet Mistral Gemma Llama InternLM2 DeepSeek-LLM Yi-1.5 Qwen1.5 Closed-source Model1 - - - - - 82.26 74.06 64.41 48.23 75.17 86.36 79.05 69.84 55.21 81.04 46.49 32.32 35. 35.37 42.38 85.06 69.36 75.76 73.78 83.08 Open-source Model 7B 20.29 23. 16.77 48.32 2B 20.95 7B 15.52 8B 34.59 70B 58.76 7B 20B 6.65 8. 7B 28.16 67B 26.94 6B 23.50 9B 25.28 34B 28.82 7B 24.28 14B 28.27 32B 32.59 72B 35.59 23.17 18.85 40.02 64.52 7.21 9. 31.37 30.27 25.83 29.16 33.59 27.38 31.49 36.25 39.69 8.69 11.74 29.73 36.59 8.69 10. 17.68 17.07 16.46 17.38 26.07 14.33 18.45 22.26 18.29 24.24 35.82 65.70 77.90 27.44 34. 44.36 49.54 48.78 44.36 65.40 46.19 43.90 49.39 49.70 40.72 36.22 38.24 44.12 62.67 3. 0.23 0.45 18.10 36.43 19.68 17.42 10.86 9.50 18.10 24.43 21.27 9.05 11.09 21.49 3. 47.51 71.95 45.93 50.45 71.27 61.35 57.75 49.00 43.10 61.65 77.35 74.30 66.50 60.25 79. 5.66 15.45 27.70 0.23 0.45 21.04 41.18 22.40 20. 13.12 11.99 20.36 29.41 25.79 11.99 14.25 25.34 6.11 12.35 10.95 29.35 46.55 10.20 11. 20.90 19.85 20.00 22.50 26.25 17.65 21.25 26.75 22.90 18.45 20.35 44.25 63.75 17.20 20. 31.60 32.55 32.15 34.20 42.30 30.15 31.75 38.15 35.55 Table 2: Overall results of different LLMs of TLG. All open-source models used are either chat or instruct models. In models belonging to the same series but varying in parameter sizes, those with larger parameters typically exhibit superior performance. The best-performing model in each Level is in-bold, and the second best is underlined. range associated with the target length of i-th response. lbF denote the lower and upper bounds of the flexible match range associated with the target length of i-th response. and ubF TLi TLi"
        },
        {
            "title": "3.2 Experimental Setup",
            "content": "Dataset. We employ two-stage data construction method for this study. Initially, we randomly sample 2,000 data from OpenHermes2.5 (Teknium, 2023). To enhance the complexity of the task and prevent data leakage, the second stage involved uses only the questions from these samples. Additionally, we randomly assign one of nine target lengths for the responses. The distribution of target length in the TLG dataset is shown in Figure 3. Further details regarding the format of the TLG dataset are provided in Appendix A.1. Models & Prompt Templates. We conduct extensive experiments with both closed and opensource LLMs, specifically the chat or instruct version. The specific models used are listed in Table 8. We evaluate each model using its own prompt template, as detailed in Table 9. To integrate the target length into the prompt, we modify the sentence The response should have word count of {Target Length} words into each question. For target length >800, we replace this with more than 800. Hardware & Hyperparameters. All experiments are conducted on NVIDIA A100 GPUs. Inference is performed using the vllm (Kwon et al., 2023), with temperature set to 0 and 1The results of all closed-source models are obtained on July 26, 2024. Figure 2: Overview of RULER. The method is divided into two parts: training and inference. The figure illustrates the main content of both sections. Additionally, in the inference section, we show two scenarios: TLG and non-TLG to show the difference. PM and FM scores for each model across various target lengths are detailed in Appendix A.3. The poor performance in TLG can be attributed to discrepancy between the token counts generated by LLMs and the lengths as understood by humans. The discrepancy between the tokens generated by LLMs and the lengths as understood by humans constirbutes to the issue. This mismatch arises due to several factors: Tokenization Schemes: LLMs employ subword tokenization schemes that decompose words into smaller units of varying lengths. For example, single long word might be divided into multiple tokens, complicating the models ability to equate token counts with human-understood word counts (Gage, 1994). Model Training: Most LLMs, particularly those trained using autoregressive language modeling, are not explicitly trained with objectives that prioritize output length. As result, these models often lack strong capabilities for controlling the length of their generated output(Devlin et al., 2019)."
        },
        {
            "title": "Generation",
            "content": "In this section, we first introduce RULER, encompassing the design of the Meta Length Tokens (MLTs), the data collection and the learning process associated with the models (4.1). Subsequently, we detail the difference in the generation of RULER under two scenarios: TLG and non-TLG (4.2)."
        },
        {
            "title": "4.1 Method",
            "content": "RULER. We introduce RULER, as illustrated in Figure 2, to effectively control the response length Figure 3: Target length distribution in TLG dataset. The count of each target length is approximately 200. max_tokens set to 2,048 in the SamplingParams, thereby employing greedy decoding for inference. The model_max_length for all models is consistent with their respective configurations, as shown in Table 8."
        },
        {
            "title": "3.3 Results and Analysis",
            "content": "Table 2 displays the PM and FM scores of opensource models at different Levels. Generally, models with advanced capabilities achieve higher PM and FM scores, indicating stronger adherence to instructions. This observation aligns with human expectations. For most models, scores are lowest at Level:2, suggesting significant potential for enhancement in producing longer responses. While, scores at Level:1 are the highest. This trend may be attributed to the prevalence of shorter responses in the training datasets utilized for model fine-tuning, which influences their generative biases. Desipte potential differences in parameters, performance gap between closed and open source models remains evident. Notably, claude-3.5-Sonnet achieve the best scores across all models at the All Level, with scores of 61.65 and 79.55. Furthermore, the MLT Range of Variation No. in DM LT [MLT:10] [MLT:30] [MLT:50] [MLT:80] [MLT:150] [MLT:300] [MLT:500] [MLT:700] [MLT:>800] [5, 15) [25, 35) [45, 55) [75, 85) [145, 155) [295, 305) [495, 505) [695, 705) (800, ) 20,000 20,000 20,000 20,000 20,000 10,333 2, 497 8,082 Table 3: Meta length tokens in RULER showing their range of variation in data collection and counts in DM LT . of LLMs using MLTs. Ruler employs MLTs to explicitly communicate length requirements within instructions. The MLTs represent the models response length range and aim to enhance its capability on the TLG task. Our end-to-end training enables the LLMs to automatically generate MLTs in various scenarios, regardless of target length requirements. MLTs  (Table 3)  offer more precise control than traditional text prompt methods, which often prove insufficiently constraining. Data collection for RULER. For common finetuning training datasets, the format typically consist of input-output pairs (x, y). Following Zhou et al. (2023b), we calculate the word count of for each entry. Based on the predefined MLTs in Table 3 and their range of variation, we aim to match each to corresponding mlt based on its word count. If match is found, the data is reformatted as (x, mlt, y). This method aids in the construction of the fine-tuning training dataset DM LT , detailed in Algorithm B. RULER learning. To minimize changes to the models generation pattern and ensure stability in non-TLG scenario, we position the MLT immediately before the original response during the construction of fine-tuning data. This strategy maintains the model chat template. Consequently, the combination of mlt and the original response forms new complete response y. We conduct the training of the RULER on the curated corpus DM LT , which is augmented with Meta Length Tokens DM LT , employing the standard next token objective: max E(x,mlt,y)DM LT log pM(mlt, yx) (3) We concatenate the MLT directly to the beginning of to compute the loss and use the MLTs to expand the original vocabulary V."
        },
        {
            "title": "4.2 RULER Inference",
            "content": "In the Target Length Generation TLG scenario. (TLG) scenario, the users instruction specifies target length, decomposed into question and target length. The RULER converts this target length into the corresponding MLT and appends it to the model chat template. Subsequent to the MLT, RULER generates response that aligns with the target length, ensuring compliance with both the users question and the target length, as illustrated in Figure 2. This approach yields superior results compared to controlling outputs solely through prompts. In the non-TLG scenario, non-TLG scenario. users provide straightforward instructions consisting solely of question. RULER integrates these instructions directly into the models chat template for generation. Owing to its innovative design and the use of standard next-token objective in training (Equation 3), RULER autonomously generates MLT prior to producing the textual response. This MLT is designed to match the length of the content generated, thereby ensuring normal generation of the model in non-TLG scenarios, as illustrated in Figure 2."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Dataset DM LT . To ensure balanced frequency distribution of each Meta Length Token (MLT) in DM LT , we set maximum occurrence limit of 20,000 for each MLT. We construct DM LT from three datasets: OpenHermes2.5 (excluding data previously used in TLG) (Teknium, 2023), LongForm (Köksal et al., 2023), and ELI5 (Fan et al., 2019), in accordance with Algorithm 1. This approach aims to create diverse dataset, particularly effective for generating longer content that is relatively rare. in total, DM LT comprises 121,229 entries, with the frequency of each MLT in Table 3. Moreover, we calculate the word count for each response in every dataset, allowing us to statistically analyze the MLT distribution, as detailed in Table 16. LLMs. To comprehensively evaluate the performance of RULER across different models, we consider factors such as model size, open-source availability, and overall model performance. We Model Level:0 Level:1 Level: All Level PM FM PM FM PM FM PM FM Target Length Generation Task (TLG) Mistral-7B-Instruct Mistral-7BR 20.29 70.1849. 23.50 75.0651.56 16.77 35.5218.75 48.32 67.8419.52 3.62 33.7130.09 5.66 36.4330.77 15.45 50.7535. 27.70 64.1536.45 gemma-7b-it gemma-7bR 15.52 59.5344.01 18.85 64.1945.34 11.74 39.3327.59 35.82 68.1432. 0.45 25.3424.89 0.45 27.8327.38 10.95 45.3534.40 20.35 57.4537.10 Llama-3-8B-Instruct Llama-3-8BR 34.59 77.2742. 40.02 80.7140.69 29.73 50.7621.03 65.70 83.8418.14 18.10 19.231.13 21.04 22.851.81 29.35 55.7526. 44.25 68.9524.70 deepseek-llm-7b-chat deepseek-llm-7bR 28.16 68.1840.02 31.37 73.5042.13 17.68 31.1013.42 44.36 68.9024. 10.86 11.540.68 13.12 11.76-1.36 20.90 43.5022.60 31.60 58.3526.75 Yi-1.5-6B-Chat Yi-1.5-6BR Qwen1.5-7B-Chat Qwen1.5-7BR 23.50 67.0743.57 25.83 72.1746.34 16.46 40.4023.94 48.78 76.8328.05 18.10 19.231.13 24.28 59.0934. 27.38 64.4137.03 14.33 29.8815.55 46.19 61.2815.09 9.05 11.542.49 20.36 21.040.68 11.99 14.252. 20.00 47.7527.75 32.15 62.4030.25 17.65 39.0021.35 30.15 52.3022.15 Table 4: Overall results of various LLMs with RULER are presented. Additionally, we also annotate the table with the score changes compared to the chat or instruct model. Consistent improvements in both PM and FM scores are observed across all Levels. select six LLMs are selected: Mistral-7B-v0.3 (Jiang et al., 2023), gemma-7b (Team et al., 2024), Llama-3-8B (AI@Meta, 2024), deepseek-llm-7b (DeepSeek-AI, 2024), Yi-1.5-6B (AI et al., 2024), and Qwen1.5-7B (Bai et al., 2023). We apply the RULER to these base models and compare the results with their corresponding instruct or chat models. Evaluation Metric. Consistent with the TLG and compared to previous results, we also calculate PM and FM scores to assess the effectiveness of RULER."
        },
        {
            "title": "5.2 Main Results",
            "content": "Table 4 presents detailed comparison of PM and FM scores across various LLMs using RULER across different Levels. For information on model training see Appendix C.2. Overall Performance Enhancement. Across all evaluated models, we observe consistent improvement in both PM and FM scores at all Levels. The most significant improvement is observed in 1, with PM and FM scores increasing gemma-7bR by 34.40 and 37.10, respectively. In contrast, the least improvement is noted with PM and FM rising by 21.35 and 22.15. The PM and FM scores across All Level showed an average improvement of 27.97 and 29.57. These improvements indicate that RULER effectively enhances the models ability to generate content of target lengths. This suggests 1Model name with means base model with RULER that using MLT to control output length is more effective than using prompts, as the model learns to generate content of corresponding lengths during fine-tuning. Additionally, RULERs ability to enhance various models demonstrates its generalizability and scalability. Different Level Analysis. At Level:0, all models show significant improvements in both PM and FM scores. Compared to other Level, each model achieves the highest PM and FM score improvements at Level:0. This enhancement occurs because the models are capable of generating responses of this length; however, their coarse length control impedes precise adherence to target length requirements. Our method significantly improves the models capacity to accurately control content length at Level:0 more accurately, better meeting the target length requirements. Moving to Level:1, while the improvements are not as pronounced as at Level:0, the models still exhibit significant gains in both PM and FM scores. At Level:2, the extent of score improvements varies across models. For instance, Mistral7B-v0.3R and gemma-7bR continue to show substantial score increases. Despite these positive trends, only deepseek-llm-7b-chatR, show slight decrease in scores at Level:2. This is attributed to the insufficient data for Level:2 in DM LT . The uneven distribution of data likely contributes to the slight decrease in scores. Model FM of Different Target Length Avg FM 10 30 80 150 300 500 700 >800 Mistral-7B-Instruct-v0.3 Mistral-7B-v0.3R gemma-7b-it gemma-7bR Llama-3-8B-Instruct Llama-3-8BR deepseek-llm-7b-chat deepseek-llm-7bR Yi-1.5-6B-Chat Yi-1.5-6BR Qwen1.5-7B-Chat Qwen1.5-7BR 0.5 72. 13.0 58.0 23.5 84.0 36.5 64.0 26.5 80.5 13.5 69.0 0.0 68. 17.0 63.5 18.0 84.0 16.0 70.0 16.5 66.0 17.0 61.0 0.5 65. 15.5 61.0 12.5 73.0 12.5 62.5 14.5 67.0 9.5 46.5 2.0 76. 26.0 69.5 28.0 80.0 17.5 73.0 14.5 77.0 16.0 68.5 18.5 76. 54.5 72.5 50.5 87.5 23.5 82.0 18.5 83.5 6.5 81.0 50.5 63. 76.5 64.0 76.5 89.5 60.5 86.5 42.5 83.5 51.0 80.5 20.5 28. 17.5 42.0 57.0 71.0 36.5 27.0 35.0 56.0 57.5 38.5 3.0 24. 0.0 17.0 25.5 14.5 16.0 17.0 33.5 22.0 22.5 16.5 2.5 64. 0.0 67.0 30.5 36.5 22.5 40.5 28.5 39.5 4.5 36.5 10.89 59. 24.44 57.17 35.78 68.89 26.83 58.06 25.56 63.89 22.00 55.33 Table 5: Results in multi MLT generation experiment. Generally, the FM scores obtained via RULER surpass those of the baseline models."
        },
        {
            "title": "FM Avg WC",
            "content": "Mistral-7B-v0.3R gemma-7bR Llama-3-8BR deepseek-llm-7bR Yi-1.5-6BR Qwen1.5-7BR 73.40 69.00 88.40 84.40 81.40 81.60 279 347 215 187 236 245 Table 6: The FM score and average word count of RULER with models in self-generated MLT experiment. FM scores are notably high. Specifically, gemma-7bR recorded the lowest at 69.00, while Llama-3-8BR achieved the highest at 88.40. Figure 4: Distribution of MLTs generated by RULER in self-generated MLT experiment. The models demonstrate preference for generating responses with lengths of 150 and 300."
        },
        {
            "title": "5.3 Do MLTs actually influence the length of",
            "content": "baseline. the generated content? To further investigate the effectiveness and scalability of MLTs, we designed two additional experiments: multi MLT generation experiment and self-generated MLT experiment. Multi MLT Generation Experiment. To further validate the efficacy and robustness of RULER, we assess its ability to control response length. We randomly sample 200 entries from Arena-HardAuto (Li et al., 2024a) and subject each to all target lengths  (Table 1)  , culminating in 1,800 entries at last. Subsequently, we calculate the FM scores for each target length, using the original model as The results presented in Table 5 highlight the enhancements in model performance due to RULER. The FM scores achieved by RULER generally surpass those of the baseline models. Notably, even the well-performing Llama-3-8BR shows significant improvements. However, when the target length is 700, RULER shows decline in FM if the baseline model already achieves certain score. In contrast, RULER enhances performance if the baseline model is underperforming. This phenomenon is likely due to an imbalance in the DM LT , where responses of 700 words are infrequent and differ from the fine-tuning data of the baseline, potentially undermining performance. Overall, RULER Model Type ARC (chanllenge/easy) HellaSwag TruthfulQA MMLU Winogrande GSM8K Mistral-7B-v0.3 - gemma-7b - Meta-Llama-3-8B - deepseek-llm-7b-base - Yi-1.5-6B - Qwen1.5-7B - vanilla RULER vanilla RULER vanilla RULER vanilla RULER vanilla RULER vanilla RULER 38.23/67.76 37.97/67.85 35.75/65.66 38.99/67.47 48.63/77.48 49.23/77.99 50.94/79.92 51.37/79. 51.62/79.25 51.28/79.46 46.67/77.53 47.27/76.68 48.57 47.83 45.95 45.40 58.89 59.12 61.48 61. 58.79 58.41 56.39 56.46 46.02 47.12 41.13 45.65 51.41 51.90 39.90 38. 55.32 49.94 53.98 50.18 34.94 37.88 32.44 31.67 50.91 50.16 48.65 48. 54.68 55.13 54.00 54.59 62.04 62.83 57.14 60.30 71.74 71.19 72.93 72. 68.51 68.11 65.98 65.19 26.46 27.52 23.58 25.93 44.96 46.63 38.89 37. 52.01 50.34 44.88 47.01 Table 7: Comparison of the overall performance of six models with RULER or vanilla, with scores computed on ARC, HellaSwag, TruthfulQA, MMLU, Winogrande and GSM8K. The overall performance of models using RULER generally remains consistent with the base models with sft. significantly improves model performance. Self-generated MLT Experiment. To validate RULER in generating MLT and responses under non-TLG scenario, we use the Arena-Hard-Auto dataset without providing MLTs, thereby necessitating autonomous response generation by the model. We evaluate performance by cataloging the types and proportions of generated MLTs (Figure 4) and evaluating response length using FM score at the target lengths corresponding to the MLTs  (Table 6)  . Models show preference for producing responses with target lengths of 150 and 300. This inclination is likely attributable to the complex nature of the queries in the Arena-Hard-Auto, which require longer responses for problem resolution. In the non-TLG scenario, the FM scores are notably high, with the Mistral-7B-v0.3R recording the lowest at 73.40 and Llama-3-8BR achieving the highest at 88.40. The word count across all models varies from 187 words to 347 words."
        },
        {
            "title": "5.4 Evaluation on Overall Performance",
            "content": "To evaluate the impact of RULER on overall performance, we conduct experiments utilizing six benchmark datasets: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), MMLU (Hendrycks et al., 2021), Winogrande (Sakaguchi et al., 2019) and GSM8K (Cobbe et al., 2021). These benchmarks provide comprehensive assessment across different task types. lm-evaluation-harness (Gao et al., 2024) is employed to assess the overall performance. Further details about the experiments on the experiment can be found in Appendix C.4. Table 7 illustrates that RULER marginally reduces performance on several tasks. Overall performance of models using Ruler generally remains consistent with the original models. The variations in scores are minimal, with changes within very small range. Moreover, we observe that some models with Ruler actually show improvements in specific tasks. These improvements suggest that Ruler may contribute positively under certain conditions or in certain task types. This indicates that RULER can significantly enhance the models ability to follow length-based instructions without compromising its performance on the same data."
        },
        {
            "title": "6 Conclusion",
            "content": "This study initially investigate the instruction following abilities of LLMs and introduces Target Length Generation Task (TLG). Additionally, we propose RULER, novel and model-angnostic method that controls generated length for LLMs. RULER utilizes the MLT and end-to-end training to enhance model performance. Experimental results demonstrate that substantial improvements in PM and FM scores across various models. Moreover, two additional experiments are conducted to further validate the efficacy of the proposed method. Finally, we assess overall performance across six different benchmarks to demonstrate its superiority."
        },
        {
            "title": "Limitations",
            "content": "With the emergence of large language models (LLMs), an increasing number of applications are now utilizing LLMs. particularly interesting aspect is the instruction-following capabilities of LLMs. In this paper, we analyze the capabilities of LLMs solely from the perspective of controlling generated length and propose solution through RULER. Instructions, which vary widely and represent real-life scenario or application. We believe addressing the challenges or solving widespread issues across various instructions is crucial. We employ meta token to construct RULER and argue that meta tokens offer more robust control over models than prompts do. Exploring how to develop and utilize models effectively with the help of tokens is profoundly important question."
        },
        {
            "title": "Ethical Statements",
            "content": "This study concentrates on managing the output length of Large Language Models (LLMs). While our primary focus is on the length of generated content, we have not assessed the potential for producing toxic content. The research does not involve human participants, nor does it handle personal or sensitive information. We have used only opensource or suitably licensed resources, thereby complying with relevant standards. Additionally, all training data employed are open-source, ensuring the exclusion of any private or sensitive information."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by National Key Research and Development Program of China (2022YFF0902100), National Natural Science Foundation of China (Grant No. 62376262), the Natural Science Foundation of Guangdong Province of China (2024A1515030166), Shenzhen Science and Technology Innovation Program (KQTD20190929172835662), Shenzhen Basic Research Foundation (JCYJ20210324115614039)."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.ai. AI@Meta. 2024. Llama 3 model card. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. 2024. Instruction mining: Instruction data selection for tuning large language models. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719730, Dublin, Ireland. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. DeepSeek-AI. 2024. Deepseek llm: Scaling opensource language models with longtermism. arXiv preprint arXiv:2401.02954. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. survey on in-context learning. arXiv preprint arXiv:2301.00234. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. Eli5: Long form question answering. In Proceedings of ACL 2019. Philip Gage. 1994. new algorithm for data compression. The Users Journal archive, 12:2338. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 33563369, Online. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 10491065, Toronto, Canada. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Yichen Jiang, Marco Vecchio, Mohit Bansal, and Anders Johannsen. 2024. Hierarchical and dynamic prompt compression for efficient zero-shot API usIn Findings of the Association for Computaage. tional Linguistics: EACL 2024, pages 21622174, St. Julians, Malta. Association for Computational Linguistics. Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. 2021. Alignment of language agents. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze. 2023. Longform: Effective instruction tuning with reverse instructions. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. 2022. The inductive bias of in-context learning: Rethinking pretraining example design. In International Conference on Learning Representations. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024a. From live data to high-quality benchmarks: The arena-hard pipeline. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023. Compressing context to enhance inference efficiency of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 63426353, Singapore. Association for Computational Linguistics. Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Ling-Hao Chen, Junhao Liu, Tongliang Liu, Fei Huang, and Yongbin Li. 2024b. One-shot learning as instruction data prospector for large language models. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland. Association for Computational Linguistics. Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023. TCRA-LLM: Token compression retrieval augmented large language model for inference cost reduction. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 97969810, Singapore. Association for Computational Linguistics. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations. Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. In Thirty-seventh Conference on Neural Information Processing Systems. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. Teknium. 2023. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants. Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron Wallace, and David Bau. 2024. Function vectors in large language models. In The Twelfth International Conference on Learning Representations. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022b. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. 2022c. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. Lei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi yang, Junhao Liu, and Min Yang. 2023a. Marathon: race through the realm of long context with large language models. Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. 2024. Soaring from 4k to 400k: Extending llms context with activation beacon. arXiv preprint arXiv:2401.03462. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pages 1269712706. PMLR. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Instruction-following evaluand Le Hou. 2023b. ation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Target Length Generation Task Deatils",
            "content": "In this section, we present the experimental details of the Target Length Generation (TLG). A.1 TLG Dataset Dataset constructed for the TLG, totaling 2,000 entries."
        },
        {
            "title": "TLG Dataset",
            "content": "{ \"id\":\"0\" \"Instruction\":\"How can generate an AI model that can classify articles of clothing as shorts, skirts, or pants based on their descriptions?\", \"TargetLength\":\"50\" } [...] { \"id\":\"1999\" \"Instruction\":\"You will be given several pieces of information about someone, and you will have to answer question based on the information given.nJohn is taller than Bill. Mary is shorter than John. Question: Who is the tallest person?\", \"TargetLength\":\"30\" } A.2 Models & Prompt Templates In this appendix, we list the models in the TLG, including their fullname, params, context length and vocab size. All models are downloaderd from Huggingface2 and inference is executed using vllm (Kwon et al., 2023)."
        },
        {
            "title": "Gemma",
            "content": "Llama3 Mistral-7B-Instruct-v0.3 gemma-2b-it gemma-7b-it Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct InternLM2 DeepSeek-LLM InternLM2-Chat-7B InternLM2-Chat-20B deepseek-llm-7b-chat deepseek-llm-67b-chat Yi-1.5 Qwen1.5 Yi-1.5-6B-Chat Yi-1.5-9B-Chat Yi-1.5-34B-Chat Qwen1.5-7B-Chat Qwen1.5-14B-Chat Qwen1.5-32B-Chat Qwen1.5-72B-Chat 7B 2B 7B 8B 70B 7B 20B 7B 67B 6B 9B 34B 7B 14B 32B 72B 32,768 32,768 8,192 8,192 8,192 8,192 32,768 32, 4,096 4,096 4,096 4,096 4,096 32,768 32,768 32,768 32,768 256,000 256,000 128,256 128,256 92,544 92, 102,400 102,400 64,000 64,000 64,000 151,936 151,936 151,936 151,936 2https://huggingface.co/ Table 8: All models used in TLG Model Mistral Gemma Llama3 Prompt Template <s>[INST] {Instruction} [/INST] <bos><start_of_turn>usern{Instruction} <end_of_turn>n<start_of_turn>modeln Eos Tokens </s> <eos> <begin_of_text><start_header_id>user <end_header_id>nn{Instruction}<eot_id> <start_header_id>assistant<end_header_id>nn <end_of_text>,<eot_id> InternLM <s><im_start>usern{Instruction} <im_end>n<im_start>assistantn DeepSeek-LLM <begin_of_sentence>User: {Instruction} nnAssistant: </s>, <im_end> <end_of_sentence> Yi-1. Qwen1.5 <im_start>usern{Instruction}<im_end> n<im_start>assistantn <im_end>,<endoftext> <im_start>systemnYou are helpful assistant. <im_end>n<im_start>usern{Instruction} <im_end>n<im_start>assistantn <im_end>, <endoftext> Table 9: Prompt templates and Eos tokens for all models used in TLG. A.3 Results on Different Target Length Here, we present the FM and PM scores of the models at all target lengths. A.3.1 Level:0 The PM and FM scores for each model at Level:0 are shown in Table 11 and Table 10."
        },
        {
            "title": "Params",
            "content": "10 30 50 80 Level:"
        },
        {
            "title": "Gemma",
            "content": "Llama3 InternLM2 DeepSeek-LLM Yi-1.5 Qwen1.5 PM FM PM FM PM FM PM FM 7B 30.73 30.73 18.60 18.60 16. 16.87 15.45 28.64 2B 21.56 7B 12.39 8B 45.41 70B 60.55 7B 17.89 20B 20. 7B 58.26 67B 46.79 6B 39.91 9B 47.71 34B 45.41 7B 31.19 14B 45.87 32B 46.79 72B 39.45 21.56 12.39 45.41 60.55 17.89 20. 58.26 46.79 39.91 47.71 45.41 31.19 45.87 46.79 39.45 30.23 18.14 35.35 66.05 6.98 8. 25.12 20.47 23.72 23.72 27.44 25.58 28.84 33.95 41.86 30.23 18.14 35.35 66.05 6.98 8. 25.12 20.47 23.72 23.72 27.44 25.58 28.84 33.95 41.86 20.88 18.88 33.73 61.45 1.20 2. 17.67 22.09 20.08 17.27 20.48 22.89 26.51 29.32 32.53 20.88 18.88 33.73 61.45 1.20 2. 17.67 22.09 20.08 17.27 20.48 22.89 26.51 29.32 32.53 11.36 12.27 24.09 46.82 1.36 4. 13.18 19.09 10.91 13.64 23.18 17.73 12.27 20.91 29.09 20.45 25.91 46.36 70.45 3.64 8. 26.36 32.73 20.45 29.55 42.73 30.45 25.45 35.91 45.91 Table 10: Results of open-source models of TLG at Level:0. The best-performing model in each target length is in-bold, and the second best is underlined. A.3.2 Level:1 The PM and FM scores for each model at Level:1 are shown in Table 12 and Table 13. Model Params 10 30 50 80 Level: gpt-4-turbo gpt-4o gpt-3.5-turbo claude-3-haiku claude-3.5-sonnet PM FM PM FM PM FM PM FM - - - - - 89.45 83.49 80.73 69.27 82.57 89.45 83.49 80.73 69.27 82.57 86.98 80.47 72.09 54.42 74. 86.98 80.47 72.09 54.42 74.42 82.33 71.08 57.43 42.17 75.50 82.33 71.08 57.43 42.17 75. 70.45 61.82 48.64 28.18 68.18 87.27 82.27 70.91 56.82 92.27 Table 11: Results of closed-source models of TLG at Level:0. The best-performing model in each target length is in-bold, and the second best is underlined. Model Params 150 Level:1 300 500 Mistral"
        },
        {
            "title": "Gemma",
            "content": "Llama3 InternLM2 DeepSeek-LLM Yi-1.5 Qwen1.5 PM FM PM FM PM FM 7B 17. 41.84 14.77 70.04 17.94 30.94 2B 17.35 7B 18. 8B 38.27 70B 55.10 7B 20B 9.18 9.69 7B 15.31 9.18 67B 6B 18.88 9B 12.76 34B 25. 9.69 7B 14B 5.61 32B 20.92 72B 13.27 32.65 42.35 70.92 85.71 20.92 22.96 37.24 34.69 46.94 33.16 58. 29.59 16.84 43.37 35.20 7.17 12.24 27.00 22.36 5.91 9.28 18.14 19.83 12.66 12.66 24. 7.17 10.97 14.77 12.66 33.33 51.90 78.90 88.61 37.55 45.99 60.76 71.73 62.45 53.59 78. 61.60 56.12 53.59 64.98 2.69 4.93 25.11 35.43 11.21 13.90 19.28 21.08 18.39 26.46 28. 26.01 37.67 31.39 28.70 7.17 13.00 47.09 59.64 22.42 32.29 33.18 39.01 35.87 44.39 57. 44.39 54.71 50.22 46.19 Table 12: Results of open-source models of TLG at Level:1. The best-performing model in each target length is in-bold, and the second best is underlined."
        },
        {
            "title": "Params",
            "content": "150 Level:1 300 500 gpt-4-turbo gpt-4o gpt-3.5-turbo claude-3-haiku claude-3.5-sonnet PM FM PM FM PM FM - - - - - 68.37 60.20 54.08 43.88 76.53 93.88 88.78 79.59 76.02 97. 22.36 15.61 15.19 19.41 24.05 83.97 71.31 81.86 75.95 88.61 52.91 25.56 39.46 44.84 31. 78.48 50.22 65.92 69.51 64.57 Table 13: Results of closed-source models of TLG at Level:1. The best-performing model in each target length is in-bold, and the second best is underlined. A.3.3 Level:2 The PM and FM scores for each model at Level:2 are shown in Table 14 and Table 15. Model Params Level:2 700 >800 Mistral"
        },
        {
            "title": "Gemma",
            "content": "Llama3 InternLM2 DeepSeek-LLM Yi-1.5 Qwen1.5 PM 3.04 0.00 0.87 7B 2B 7B 8B 16.09 70B 24.35 7B 18.70 20B 17. 7B 67B 9.13 9.13 6B 12.61 9B 22.17 34B 22.17 7B 12.17 14B 15.22 32B 23.91 6.09 72B FM 6. 0.00 0.87 21.74 33.48 23.91 22.61 13.48 13.91 16.96 31.74 30.87 17.83 21.30 31.30 10. PM 4.25 0.47 0.00 20.28 49.53 20.75 17.45 12.74 9. 24.06 26.89 20.28 5.66 6.60 18.87 1.42 FM 4.25 0.47 0.00 20.28 49. 20.75 17.45 12.74 9.91 24.06 26.89 20.28 5.66 6.60 18.87 1.42 Table 14: Results of open-source models of TLG at Level:2. The best-performing model in each target length is in-bold, and the second best is underlined."
        },
        {
            "title": "Params",
            "content": "Level:2 700 >800 PM FM PM FM gpt-4-turbo gpt-4o gpt-3.5-turbo claude-3-haiku claude-3.5-sonnet - - - - - 49.57 46.09 35. 39.57 36.52 62.61 64.78 50.43 51.74 53.04 31.13 79.72 41.04 49.06 91.04 31.13 79.72 41. 49.06 91.04 Table 15: Results of closed-source models of TLG at Level:2. The best-performing model in each target length is in-bold, and the second best is underlined."
        },
        {
            "title": "C Experiments Details",
            "content": "C.1 MLT in Datasets To obtain data with varying response lengths for composing DM LT , particularly those responses exceeding 500, we integrateg data from OpenHermes2.5 (Teknium, 2023), LongForm (Köksal et al., 2023) and ELI5 (Fan et al., 2019). We calculate the word count for each response in every dataset, allowing us to statistically analyze the MLT distribution, shown in Table 16. Algorithm 1 DM LT Data Creation Require: Word count function L(), meta length tokens LT = {M LT0, LT1, } Input: Initial dataset Output: DM LT 1: DM LT {} 2: for each tuple (x, y) in do 3: mlt None 4: for each LT in LT do 5: 6: 7: 8: 9: 10: 11: if L(y) > lbM LT and L(y) ubM LT then mlt LT break end if end for if mlt is not None then DM LT DM LT {(x, mlt, y)} end if 12: 13: end for 14: return DM LT"
        },
        {
            "title": "MLT",
            "content": "[MLT:10] [MLT:30] [MLT:50] [MLT:80] [MLT:150] [MLT:300] [MLT:500] [MLT:700] [MLT:800] OpenHermes2.5 (Teknium, 2023) LongForm (Köksal et al., 2023) ELI5 (Fan et al., 2019) 28,552 16,860 18,867 18,014 37,515 7,526 1,495 193 1,809 586 1,428 1,236 852 1,037 252 140 101 2, 3,280 14,143 17,597 15,926 19,103 2,555 682 203 3,808 Table 16: MLT distribution in each dataset. The OpenHermes2.5 excludes the data utilized in TLG. The LongForm and ELI5 employs its training, validation, and test sets simultaneously. When multiple answers are available in the dataset, the longest answer is selected as the final response. C.2 More Details of Training More details of training. We use 4*A100 with 80GB Nvidia GPUs to train the models. The training utilizes both bf16 and tensor tf32 precision formats. The per-device training batch size is set to 4, with gradient accumulation is 8 steps. cosine learning rate scheduler is applied, starting with an initial learning rate of 2e-5 and warmup ratio of 0.05. All models are trained for 3 epochs. Additionally, log is set to print every 5 steps. Loss. We document the changes in training loss for all models, as shown in Figure 5. Figure 5: Training loss for models. C.3 Multi MLT generation experiment Here is the results in multi MLT generation experiment. C.4 More Details of Other Tasks We tested the RULERon six benchmarks (ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), MMLU (Hendrycks et al., 2021), Winogrande (Sakaguchi et al., 2019) and GSM8K (Cobbe et al., 2021)) to examine whether the performance of the fine-tuned models varies on different tasks. We employ 25-shot in ARC, 10-shot setting in Hellaswag, 5-shot setting in MMLU, 0-shot setting in TruthfulQA, 5-shot setting in Winogrande and 5-shot in GSM8K."
        }
    ],
    "affiliations": [
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
    ]
}