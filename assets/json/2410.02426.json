{
    "paper_title": "Learning the Latent Rules of a Game from Data: A Chess Story",
    "authors": [
        "Ben Fauber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella \"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also explore the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 6 2 4 2 0 . 0 1 4 2 : r Learning the Latent Rules of Game from Data: Chess Story Ben Fauber* Dell Technologies October 3, 2024 Abstract We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of process from data associated with the process. Inspired by Stefan Zweigs novella Schachnovelle, also known as The Royal Game in English, we show that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also explore the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples."
        },
        {
            "title": "1 Introduction",
            "content": "The novella Schachnovelle, also known as The Royal Game in English, was written by the renowned Austrian author Stefan Zweig and first published in 1942 (Zweig, 1942). The story follows an anonymous narrator on an ocean liner traveling from New York to Buenos Aires. Among the passengers is world chess champion Mirko Czentovic. The narrator and businessman named McConnor challenge Czentovic to chess match, in which Czentovic easily wins the first game. In the next game, mysterious passenger known only as Dr. B. intervenes, confounding Czentovics strategy and unexpectedly forcing draw. As the story progresses, Dr. B. confides in the narrator that he was once prisoner of the Gestapo. To maintain his sanity during his imprisonment, Dr. B. obsessively studied book containing 150 championship chess games. The story culminates when Dr. B. agrees to match against Czentovic. Dr. B. stuns Czentovic by winning the first game but grows increasingly agitated by the slow tempo of the second game. The narrator intervenes before Dr. B.s frustration with the games tempo overwhelms him, prompting Dr. B. to resign and withdraw from the second game as the story concludes. We have shown that instruction fine-tuning small pretrained generative language models (SLMs) leads to highly specialized models capable of accurately performing challenging tasks that the base models cannot perform (Fauber, 2024a;b). Our advances in fine-tuning language models prompted us to revisit Stefan Zweigs fictional character, Dr. B., and his process of mastering chess. We found it compelling that Dr. B., who initially had limited knowledge of chess, mastered the game after intensely studying book of championship chess games during his imprisonment. Even more remarkable was that his book contained no board diagrams and only used standard algebraic chess notation (SAN) to chronicle each game, which Dr. B. initially did not understand at all. Dr. B. described the books contents as, \"...what were to me the almost unintelligible symbols... It all seemed kind of algebra to me, to which could find no key. Only gradually did puzzle out that the numbers stood for the ranks and the letters for the files, so that you could establish the position of each piece.\" Dr. B.s fictitious yet plausible experience led us to explore the real possibility of learning the latent rules of game from data. Herein, we explore: 1) can an instruction fine-tuned language model learn the rules of chess from only standard algebraic chess notation (SAN) game data; 2) how much data is required to learn the rules well, and does learning scale proportionally with the data; and 3) how well do instruction fine-tuned language models play the game of chess? *Correspondence to: Ben.Fauber@dell.com 1 Figure 1: Illustration of our proposed task: prediction of white chess piece move (red arrow) given board state in standard algebraic chess notation (SAN). The chessboard has standard algebraic notation ranks and files along the board edges."
        },
        {
            "title": "2 Background",
            "content": "Chess is strategic board game played between two opponents on an 8 8 grid called chessboard. Each player begins with 16 pieces of single color, usually white or black: one king, one queen, two rooks, two knights, two bishops, and eight pawns. The player with the white pieces makes the first move to initiate the game. The objective is to checkmate the opponents king, meaning the king is in position to be captured (\"in check\") and there is no legal move to escape the threat. Players take turns moving one piece at time, with each type of piece having its own unique movement pattern. The game can also end in draw under various conditions, such as stalemate or insufficient material to achieve checkmate. Strategy and tactics are crucial, as players aim to control the board, capture opponent pieces, and protect their own king (Harkness, 1956). The game of chess is thought to of originated in India circa 600 CE. By 1000, the game had spread throughout Asia, the Middle East, and Europe. The game was widely adopted by nobility in these regions around 1500, and thus became known as the \"Royal Game\" (Lombardy, 1975). Although the game was widely played circa 1500, the rules varied widely by region. The rules of the game were not unified and codified until 1851 during European congress of players organized by St. Georges Chess Club of London at the Crystal Palace (Lombardy, 1975). The international chess code underwent some minor modifications in 1929 and 1954, but has since been largely unchanged (Harkness, 1956). Automated chess game play dates back to 1769 with the advent of the Mechanical Turk automaton by Wolfgang von Kempelen (Hooper & Whyld, 1992). Serious interest in chess software began in the 1950s and gained momentum with the discovery and application of the alpha-beta pruning search algorithm to optimize move evaluation (Newell et al., 1958). significant milestone was IBMs Deep Blue defeating then World Chess Champion Garry Kasparov in 1997, proving computers could compete at the highest levels (Hsu, 2002).1 Computers have since transformed chess by expanding opening theory, providing definitive endgame solutions, and bots/engines that are widely used for online play.2 Deep Blue was an expert system that combined vast database of chess knowledge and heuristics with powerful tree-search algorithm. Most modern and stronger chess engines, such as Stockfish, follow similar approach.3 Notable architectural exceptions include AlphaZero (Silver et al., 2018) and and its open-source counterpart, Leela Chess Zero.4 Both utilize combination of deep neural networks (DNNs) trained on data from millions of chess games, and powerful search algorithms paired with reinforcement learning (RL) to select moves with the highest probability of winning. recent paper described the results of training transformer-based model from scratch on billionsscale move action-value annotated chess dataset (Ruoss et al., 2024). Unlike previous chess engines, this work did not explicitly use reinforcement learning (RL) or search algorithms. Instead, it heavily relied on the search capabilities of the Stockfish chess engine to accurately annotate billions of boards with move 1While Kasparovs defeat by Deep Blue is perhaps the most famous instance of computer beating Grandmaster, Saviely G. Tartakower was actually the first Grandmaster to lose to computer. This happened in 1951 when he was defeated by the Ajedrecista chess computer in Paris. 2https://lichess.org/ (accessed 11Sept2024) 3https://stockfishchess.org/ (accessed 11Sept2024) 4https://github.com/LeelaChessZero/ (accessed 13Sept2024) 2 action-values, thereby teaching the new transformer model the best moves for given board state to maximize the probability of winning."
        },
        {
            "title": "2.1 Our Contribution",
            "content": "Dr. B., the fictitious character in Stefan Zweigs Schachnovelle, led us to explore the real possibility of learning the latent rules of chess from game data. Dr. B. achieved Grandmaster-level chess skills by obsessively studying book containing 150 championship chess games. In his book, each game was described solely in standard algebraic chess notation (SAN), with no accompanying board diagrams. Chess engines, such as Stockfish, have learned game play from board states paired with powerful search algorithms to select the next best move (Hooper & Whyld, 1992). AlphaZero paired that approach with reinforcement learning (RL) to emphasize and learn the strategies with the highest probabilities of winning. The recent approach of combining billions of Stockfish-annotated move action-values with supervised learning differs slightly from the chess engine and reinforcement learning (RL) methods (Ruoss et al., 2024). However, it still utilized billions-scale static move action-values derived from chess engines with the goal of creating Grandmaster-level chess bot. Additionally, to our knowledge, all prior approaches to chess engines/bots have been built/trained from scratch with the sole purpose of playing chess. Unlike these prior approaches, we used only SAN data from chess games and problems to replicate the situation experienced by Dr. B. in Zweigs novella. Further, we did not train our models from the ground-up, rather, we chose to explore the instruction fine-tuning of pretrained generative small language models (SLMs). We have previously demonstrated that instruction fine-tuning small pretrained generative language models leads to highly specialized models capable of accurately performing challenging tasks that the base models cannot perform (Fauber, 2024a;b). In this work, we sought to address the following questions: 1. Can an instruction fine-tuned small generative language model learn the rules of chess from only standard algebraic chess notation (SAN) game data? 2. If fine-tuned model can learn the rules, how much data is required to learn them well, and does learning scale proportionally with the data? 3. How well do instruction fine-tuned small language models play the game of chess?"
        },
        {
            "title": "3 Methods",
            "content": "The python library python-chess provides extensive functionality to analyze portable game notation (PGN) format chess game files.5 The library also contains functions to extract board status of game before/after move and to check the legality of proposed moves given board status."
        },
        {
            "title": "3.1 Standard Algebraic Chess Notation",
            "content": "Standard algebraic chess notation (SAN) assigns unique two-character identifier to each of the 64 squares on chessboard. The first character represents the file (column) of the square, labeled from \"a\" (leftmost or queenside) to \"h\" (rightmost or kingside). The second character represents the rank (row) of the square, numbered from \"1\" (bottom side, whites first rank) to \"8\" (top side, blacks first rank). For example, the initial positions of some pieces are: white queens rook at a1, white king at e1, black queens knight pawn at b7, and black kings rook at h8 (Hooper & Whyld, 1992). SAN identifies each piece by single letter. The standard English values include pawn = \"P\", rook = \"R\", knight = \"N\", bishop = \"B\", queen = \"Q\", and king = \"K\". In our studies, we used uppercase letters to represent white pieces and lowercase letters to represent black pieces. Notably, the letter code for pawn is not used in SAN moves within PGN format move text. However, letter codes for pawns and other pieces are sometimes necessary for certain tag pair and annotation constructs. For example, we used the letter codes for pawns in this study to differentiate between white and black pawn pieces when indicating our board status. An example of SAN move sequences is show in Figure 2, illustrating the \"Queens Gambit Declined\" opening moves (Matanovic, 1978). standard SAN move lists the moving pieces letter (omitted for pawns) followed by the destination square. Capture moves are indicated by lowercase \"x\" before the destination square. For pawn captures, the file letter of the capturing pawns starting square is placed immediately before the \"x\". 5https://github.com/niklasf/python-chess (accessed 03Sept2024) Figure 2: Illustration of the initial array (left board diagram) and the standard algebraic chess notation (SAN) for the \"Queens Gambit Declined,\" popular opening move sequence in the 1920s and 1930s (Encyclopedia of Chess Openings, sequences D30-42). The moves, in SAN, are as follows: 1. d4 d5, 2. c4 e6. The outcome of the two move sequences is shown on the right board diagram. In SAN notation, kingside castling is represented by the letters \"O-O\" and queenside castling by the letters \"O-O-O\". En passant captures are not specially notated; they are written as if the captured pawn were on the capturing pawns destination square. Pawn promotions are indicated by an equal sign \"=\" immediately after the destination square, followed by the letter of the promoted piece (knight, bishop, rook, or queen) in uppercase. If move results in check, plus sign \"+\" is added to the end of the SAN notation. For checkmate, hash sign \"#\" is used instead. There are no special notations for double checks, discovered checks, or drawing moves."
        },
        {
            "title": "3.2 Forsyth-Edwards Board Notation",
            "content": "Forsyth-Edwards notation (FEN) denotes single board status. It does not record the moves that lead to the board status. For example, the starting positions of the game pieces are denoted by the sequence \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR\" (Edwards, 1994). Each row of the board is separated by forward-slash \"/\" character, with uppercase letters representing white pieces and lowercase letters representing black pieces, using the same piece designations as in SAN."
        },
        {
            "title": "3.3 Dataset",
            "content": "Stefan Zweigs novella Schachnovelle was initially published in 1942 (Zweig, 1942). Therefore, we focused on games from prominent chess masters of that era as these were likely the games Dr. B., Zweigs character, might have studied. In the story, Dr. B. specifically mentions only two players: Alekhine and Bogoljubow, and their 1922 game in Pistyan, Slovakia.6 The chess Grandmasters and International Chess Federation or FIDE (Fédération Internationale des Échecs) world champions leading up to the early 1940s included Alexander A. Alekhine (18921946) (Alekhine, 1927; 1939; Kotov, 1975; Bjelica, 1993), José Raúl Capablanca (18881942) (Capablanca, 1935; Reinfeld, 1942; Panov, 1973; Chernev, 1982; Bjelica, 1994; Golembek, 1996), Emanuel Lasker (1868 1941) (Lasker, 1960; 1965; Reinfeld & Fine, 1965; Charushin, 1998), Aron I. Nimzowitsch (18861935) (Nimzowitsch, 1925; 1929), Akiba K. Rubinstein (18801961) (Kmoch, 1960; Razuvaev & Murakhevri, 1980), and Savielly Tartakower (18871956) (Tartakower, 1924). We included many game sources for these notable players. We also included texts for short games (Chernev, 1954), openings (Mednis, 1986), middle games (Leininger, 1997), endings (Chernev, 1969), and esteemed games (Tartakower & du Mont, 1952; Lombardy, 1975; Chernev, 1992; 1995) to cover key tactics of game play. All chess game data was collected from the referenced texts as portable game notation (PGN) format files, standard format for recording single or multiple games in text file.7 The PGN format contains the chess board configuration at the initiation of game, or sequence, where the board state is encoded as Forsyth-Edwards notation (FEN) text string (Edwards, 1994). Each white/black move pair are encoded 6This is different from the often-cited 1922 \"Sacrificing the Queens\" game between Alekhine and Bogoljubow at the Hastings Summer Chess Congress in England. In that game, Alekhine successively sacrificed three queens and was about to promote pawn to fourth queen when Bogoljubow resigned. 7Games in PGN format were downloaded from: http://billwall.phpwebhosting.com/ (accessed 03Sept2024) within the PGN using standard algebraic chess notation (SAN). Illegal moves are not permitted in PGN move text. All games were parsed into single moves for both white and black pieces, along with the board status before each move. The FEN game boards were converted into SAN, ensuring that both the move and board status were in SAN. The conversion to SAN was conducted to align with the experience of Stefan Zweigs character, Dr. B., in Schachnovelle. Parsing of the PGN files associated with all the above-cited publications resulted in total of 20 million board and single-move combinations for white and black pieces. Filtering to only white piece single-moves resulted in 10 million board and move combinations. This data is referred to as the WSM-10M dataset. All moves were checked to ensure they were legal for the given board state. Each game in the WSM-10M dataset contained mean of 39 white piece single-moves, with the longest game in the dataset containing 108 white piece single-moves. Further, there were 80k unique board and white piece single-move combinations in the dataset. This data is referred to as the Unique-WSM-10M dataset (see the Appendix for additional details)."
        },
        {
            "title": "3.4 Data Sampling",
            "content": "Following best practices in machine learning, we randomly divided parent datasets into training/fine-tuning data and test data by sampling without replacement. We varied the number of available fine-tuning data instances from 1,000 to 1,000,000 examples of board states and their subsequent white piece single-moves, by random selection without replacement from the parent pool of fine-tuning data instances for each instance cohort. The fine-tuning data instances were used for language model instruction fine-tuning. The language models were never exposed to the test data (i.e., out-of-sample \"hold-out\" data) during the fine-tuning process to avoid train/test data contamination. We acknowledge that early-game board states are limited, and players frequently use popular openings. Consequently, these common board states and moves appear in both the fine-tuning and test datasets. We decided to keep them in both sets to avoid distributional shifts that could skew our evaluation metrics. All data was formatted into an instruction-based format where the \"instruction\" was the input, and the \"output\" was the desired outcome. For example, the instruction was, \"You are chess Grandmaster and checkmate # is your goal. Predict the next best move on this SAN chess board: h1:K, a2:P, g2:P, h3:P, b4:p, g4:R, f5:r, a6:R, f6:p, b7:r, f7:k,\" and the corresponding output was the white piece single-move, \"Rg3\". The instruction formatting was consistent throughout the fine-tuning and testing datasets unless otherwise noted."
        },
        {
            "title": "3.5 Pretrained Foundational Small Language Models",
            "content": "We selected the OPT (open pretrained transformer) family of pretrained foundational generative language models as the starting point for our studies (Zhang et al., 2022). We also explored the TinyStories (Eldan & Li, 2023) family of language models. The OPT-125M model contained 125M parameters, whereas the TinyStories-28M model contained 28M parameters. Both models provided up to 2,048 positional embeddings for their inputs, permitting context for long string sequences which can be present in our method. In our work, we defined model fine-tuning as initialization of pretrained foundational language model followed by updates to the model weights and biases. In our fine-tuning setting, all language model parameters could undergo gradient updates there were no frozen layers nor adapters. In our prior work (Fauber, 2024b), we found the full fine-tuning approach was superior to adapter-based methods like LoRA (Low-Rank Adaptation) (Hu et al., 2021). Other research groups have since confirmed our initial findings (Biderman et al., 2024). The prompt for the language models was consistent throughout our evaluation and across all models. The language model prompt was general and agnostic to the dataset instructions. The prompt used for our evaluation was: \"Below is an instruction that describes task. Write response that appropriately completes the request. ### Instruction: {instruction} ### Response:\"."
        },
        {
            "title": "3.6 Evaluation of Our Method",
            "content": "We evaluated the performance of our instruction fine-tuned language models on their ability to correctly propose legal white piece single-moves given board state in the test dataset. We also evaluated the ability of our fine-tuned models to correctly predict the next white piece single-move in \"check or checkmate in one move\" chess problem. The problems were drawn from parent dataset of 1,836 chess problems (Polgár, 2013). This data is referred to as the \"Check/Mate-in-1\" dataset. 5 We applied our previously-described instruction fine-tuning and text generation framework to ensure consistent outcomes and scoring (Fauber, 2024b). There were no detectable deviations in our study when replicate training sessions and fine-tuned SLM text generation results were evaluated."
        },
        {
            "title": "4.1 Baseline Performance",
            "content": "Language models can be generalists, or specialists, and it is valuable to understand what is required to create specialist language model. It is important to determine how much fine-tuning data, and which fine-tuning paradigm, are reasonable starting points to create specialist language model. We recognize that fine-tuning language models over multiple epochs may obliterate some portion of information that resides within the pretrained foundational language model. This potential change did not concern us as our objective was to create specialized language models from pretrained foundational language models, with the objective of effectively executing highly specialized task that the original pretrained foundation models were incapable of performing. We evaluated series of pretrained generative foundational language models on their ability to provide legal chess move in SAN given board state in SAN. Many popular pretrained foundational language models (Radford et al., 2019; Black et al., 2021; Scao et al., 2022; Zhang et al., 2022; Eldan & Li, 2023; Jiang et al., 2023; Dubey et al., 2024) were unable to perform this task with any reasonable level of proficiency  (Table 1)  . Pretrained Foundational Language Model Language Model Parameters % Legal Moves roneneldan/TinyStories-28M gpt2 EleutherAI/gpt-neo-125m facebook/opt-125m facebook/opt-350m facebook/opt-1.3b facebook/opt-6.7b mistralai/Mistral-7B-Instruct-v0.3 bigscience/bloom-7b1 meta-llama/Meta-Llama-3.1-8B-Instruct 28M 124M 125M 125M 350M 1.3B 6.7B 7B 7.1B 8B 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% Table 1: Baseline performance of pretrained foundational generative language models on their ability to provide legal chess move in SAN for given board state in SAN. Assessment used 1,000 test instances of board states in SAN drawn from the WSM-10M dataset as the model input. The language models are described by their HuggingFace.co repo names (accessed 13Sept2024) 4."
        },
        {
            "title": "Increasing Dataset Size Improves Performance",
            "content": "Our objective was to expose small pretrained foundational language models to increasing orders of magnitude of domain-specific instruction fine-tuning data and assess their performance against test data with welldefined metrics. In our primary assessment, we evaluated the percentage of legal proposed moves as function of increasing orders of magnitude of instruction fine-tuning data. We observed that increasing the amount of domain-specific instruction fine-tuning data, as well as increasing the pretrained foundational language model parameter count (i.e., model size), improved the fine-tuned language model performance on our task (Figure 3). Thus, learning the latent rules of the game did scale with the data. Further, the OPT-125M model instruction fine-tuned with 1 million examples nearly always proposed legal move. Next, we explored the influence of unique board and move combinations in the instruction fine-tuning dataset. Filtering the WSM-10M dataset to only unique board and move combinations resulted in the UniqueWSM-10M dataset, which contained approximately 80k white piece single-moves. The percentage of legal moves proposed by OPT-125M language models, which were instruction fine-tuned on progressively larger amounts of Unique-WSM-10M data, showed similar performance to the unfiltered WSM-10M fine-tuned models on the same number of example cohorts  (Table 2)  . 6 Figure 3: Influence of increasing instruction fine-tuning examples. Percentage of legal proposed moves versus count of the instruction fine-tuning examples for the TinyStories-28M (blue) and OPT-125M (orange) language models, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of the instruction fine-tuned language models was evaluated using 10,000 test instances of chess board states drawn from WSM-10M to assess the models ability to generate legal proposed move. This result was notable because some examples in the WSM-10M dataset included duplicate board and move state combinations as players typically use similar opening moves and strategies. Therefore, filtering to only unique board and move state combinations did not provide beneficial outcomes when instruction fine-tuning for this task. Pretrained Foundational Language Model Instruction Fine-Tuning Dataset Instruction Fine-Tuning Example Count % Legal Moves OPT-125M OPT-125M Unique-WSM-10M Unique-WSM-10M 1,000 10,000 29% 36% Table 2: Percentage of legal proposed moves versus count of the instruction fine-tuning examples for the OPT-125M language model, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The instruction fine-tuning examples for these instances were drawn exclusively from the Unique-WSM-10M dataset. The Unique-WSM-10M dataset contained only 80k examples. Therefore, our assessment was limited to instruction fine-tuning cohorts of 1,000 and 10,000 examples, as larger datasets were not available for instruction fine-tuning. The performance of each instruction fine-tuned language model was evaluated using 10,000 test instances of chess board states drawn from WSM-10M to assess the models ability to generate legal proposed move."
        },
        {
            "title": "4.3 Chess Game Play Improves with More Data",
            "content": "We evaluated the game play performance of our instruction fine-tuned models using check or checkmate in one white piece single-move chess problems (Polgár, 2013). The data for this evaluation process was our Check/Mate-in-1 dataset. Our instruction fine-tuned models were evaluated on: 1) their abilities to both propose legal moves and; 2) their abilities to propose legal moves that resulted in either check or checkmate to solve the chess problems. Our Check/Mate-in-1 dataset contained 1,836 examples. We sampled 1,000 test examples from this parent dataset for our evaluation. In our test instance containing 1,000 examples, the black king piece resided on 61 of the possible 64 squares of the board, representing diversity of locations and strategies required to solve the problems. The test set problems contained between 3 and 32 pieces on the board. On average, each test set problem included total of seven pawns, two knights, two bishops, three rooks, two queens, and two kings. Both the TinyStories-28M and OPT-125M instruction fine-tuned models demonstrated reasonable abilities to propose legal moves (Figure 4) and solve the chess problems with legal moves (Figure 5). Again, the instruction fine-tuned OPT-125M models outperformed the smaller TinyStories-28M models. As noted in our earlier studies, the ability to accurately perform both tasks improved as the number of instruction 7 Figure 4: Percentage of legal proposed moves versus count of the instruction fine-tuning examples for the TinyStories-28M (blue) and OPT-125M (orange) language models, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the models ability to generate legal proposed move. fine-tuning examples increased. Figure 5: Percentage of proposed moves which were legal and resulted in check or checkmate versus count of the instruction fine-tuning examples for the TinyStories-28M (blue) and OPT-125M (orange) language models, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the models ability to generate legal move that resulted in check or checkmate."
        },
        {
            "title": "4.4 Hallucinations Decrease with More Instruction Fine-Tuning Examples",
            "content": "While evaluating the performance of our instruction fine-tuned SLMs and their abilities to propose legal moves that resulted in check or checkmate on the Check/Mate-in-1 dataset, we also observed interesting trends regarding hallucinations. Generative language models are trained to accurately predict the next token in sequence, thus generating human like text that resembles their training data. Yet, it is well known that generative language models can create text that sometimes contain inaccuracies or nonsensical information known as \"hallucinations\" (Agrawal et al., 2024). We noted that increasing orders of magnitude of instruction fine-tuning data resulted in fine-tuned OPT-125M models which proposed fewer illegal moves and hallucinations of pieces on board which did not exist (Figure 6). Based on these results, we concluded that the instruction fine-tuned language models with fewer examples exhibited game play somewhat akin to novice child. Typically, when teaching children to play chess, they quickly grasp the objective and may suggest illegal moves to rapidly achieve check or checkmate. We observed similar outcome with the 1,000 and 10,000 example instruction fine-tuned OPT-125M models, where the models suggested majority of illegal moves to achieve check or checkmate. Yet, these tendencies diminished as the number of instruction fine-tuning examples increased. 8 Further, we observed that the tendency to hallucinate piece onto board to achieve check or checkmate was ablated as the number of instruction fine-tuning examples increased (Figure 6). The proposal of illegal moves to achieve check or checkmate decreased as the number of instruction fine-tuning examples were increased but was not totally eliminated with even 1 million instruction fine-tuning examples. Additionally, the percentage of illegal proposed moves to achieve check or checkmate (Figure 6) was much higher compared to legal moves proposed for the same goals (Figure 5). Therefore, we concluded that understanding the objective of the game was easier than learning its rules. Figure 6: Percentage of proposed moves which were either illegal (orange), or the proposed piece was not on the board (blue), to achieve either check or checkmate versus count of the instruction fine-tuning examples for the OPT-125M language model, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the models propensity to generate an illegal move that resulted in check or checkmate."
        },
        {
            "title": "4.5 Models Learn the Game Objective from Game Data",
            "content": "We also explored the role of the instruction fine-tuning text in achieving legal move which resulted in check or checkmate. For all instruction fine-tuning examples, we included the statement, \"You are chess Grandmaster and checkmate # is your goal.\" One could posit that this statement explicitly states the games objective, and the model was learning this objective from the instruction text, along with all moves marked with the hash sign \"#\" to indicate checkmate. To evaluate the role of this statement in the instruction fine-tuning text, we used the exact same 1,000 to 1,000,000 instruction fine-tuning cohorts for WSM-10M without the statement, \"You are chess Grandmaster and checkmate # is your goal.\" We referred to this revised instruction fine-tuning dataset as NoGoal-WSM-10M. Pretrained Foundational Language Model Instruction Fine-Tuning Dataset Instruction Fine-Tuning Example Count % Legal Moves % Legal and Check/Mate Moves OPT-125M OPT-125M OPT-125M OPT-125M NoGoal-WSM-10M NoGoal-WSM-10M NoGoal-WSM-10M NoGoal-WSM-10M 1,000 10,000 100,000 1,000,000 16% 33% 52% 60% 1% 6% 10% 12% Table 3: Percentage of proposed moves which were legal and resulted in check or checkmate versus count of the instruction fine-tuning examples for the OPT-125M language model, instruction finetuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The instruction fine-tuning examples for these instances were drawn exclusively from the NoGoal-WSM-10M dataset. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the models ability to generate legal move and legal move that resulted in check or checkmate. We found that usage of the WSM-10M cohort or the NoGoal-WSM-10M cohort for instruction finetuning the OPT-125M model resulted in essentially the same outcomes for the percentage of legal proposed moves and legal proposed moves which resulted in check or checkmate (Table 3, Figure 4, and Figure 5). 9 The language models fine-tuned with the NoGoal-WSM-10M datasets also showed similar results with regards to piece hallucinations and illegal moves proposed to achieve check or checkmate, as observed with the WSM-10M fine-tuned models (see Appendix for additional details). Therefore, we concluded that the instruction text was not teaching the model the game objective. Instead, the model was learning the game objective from the game play data."
        },
        {
            "title": "4.6 Revisiting the Same Data Provides Limited Benefits",
            "content": "We examined the models ability to learn from dataset by repeatedly revisiting the data and then evaluated its performance after multiple revisits. This process was analogous to Stefan Zweigs character Dr. B. obsessively studying his book of championship chess games. We discovered that increasing the number of instruction fine-tuning epochs for the OPT-125M language model initially improved the models ability to perform our tasks (Figure 7). This outcome was consistent for both the percentage of legal proposed moves and the legal moves that resulted in check or checkmate on the Check/Mate-in-1 dataset. These results also demonstrated the potential to improve an instruction fine-tuned models performance for task with constrained amount of instruction fine-tuning data. Yet, the successive improvements in results from increasing epochs do appear to plateau after 5-10 fine-tuning epochs. Figure 7: Percentage of proposed moves which were legal (orange) and resulted in check or checkmate (blue) versus count of the instruction fine-tuning epochs for the OPT-125M language model, instruction fine-tuned with 100,000 examples drawn from the WSM-10M dataset, learning rate = 2e-4, and batch size = 4. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the models ability to generate legal move that resulted in check or checkmate."
        },
        {
            "title": "4.7 Requiring Legal Moves with Increased Temperature Improves Outcomes",
            "content": "Throughout our studies, we used the default settings for the transformers.GenerationConfig() text generation function, unless otherwise specified (see Appendix). We also disabled the do_sample hyperparameter within that function to ensure consistent and reproducible text generation, as documented in our previous work (Fauber, 2024a;b). Conversely, enabling of the do_sample hyperparameter makes use of the temperature hyperparameter to allow for variation in text generation based on the temperature setting (default = 1.0). Thus, we explored the role of increasing generation variability, with the temperature hyperparameter 1.0, and requirement that the proposed moves be legal. In this analysis, the text generation process was allowed up to 100 iterations to obtain legal proposed move. If no legal move was proposed by the final iteration, the attempt was labeled as illegal, and the process moved on to the next test example. For this study, we used an OPT-125M language model that was instruction fine-tuned with 1,000,000 examples drawn from the WSM-10M dataset. The baseline text generation for this study, where the do_sample hyperparameter was disabled (temperature = 1.0 as the default setting) and only single text generation iteration was allowed, is shown in Figures 4 and 5. Conversely, enabling the do_sample hyperparameter and allowing up to 100 text generation iterations to achieve legal proposed move, while keeping the temperature at the default setting of 1.0, resulted in higher percentage of proposed legal moves and legal moves that led to check or checkmate (Figure 8). 10 Increasing the temperature hyperparameter under the same text generation conditions provided further improvements in proposed legal moves and legal moves that resulted in check or checkmate, until the process plateaued at temperature = 3.5 (Figure 8). At this point, > 99% of the proposed moves were legal, and 24% of the proposed moves were legal and resulted in check or checkmate. Further increasing the text generation temperature did not improve the percentage of legal moves, and only resulted in some small aberrations in the percentage of legal moves that resulted in check or checkmate. These results showed that increasing text generation variability to allow the model to be more creative, while ensuring the outputs remained legal, improved the results, but there were limits. This study also revealed that the instruction fine-tuned model contained some additional knowledge about proposing legal move for check or checkmate given board state, which was only disclosed when the model was forced to generate legal move. Figure 8: Percentage of proposed moves which were legal (orange) and resulted in check or checkmate (blue) versus the GenerationConfig() temperature parameter value when required to generate legal move based on the board state. The model was allowed up to 100 iterations to achieve this requirement before proceeding to the next example in the test dataset. All instances used an instruction fine-tuned OPT-125M language model, instruction fine-tuned with 1,000,000 examples drawn from the WSM-10M dataset, learning rate = 2e-4, and batch size = 4. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the models ability to generate legal move that resulted in check or checkmate."
        },
        {
            "title": "5 Discussion",
            "content": "We have provided framework for rigorous and systematic evaluation of our instruction fine-tuned language model outputs. We demonstrated that small pretrained foundational generative language models with millions of parameters can learn the latent rules of process from data associated with the process. We were inspired by Stefan Zweigs novella Schachnovelle, also known as The Royal Game in English, in which his character, Dr. B., learns how to play Grandmaster-level chess by compulsively studying book of 150 championship chess games. We showed that 28M and 125M parameter pretrained foundational language models can be instruction fine-tuned with 1,000-to-1,000,000 instruction examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also demonstrated that increasing the number of instruction fine-tuning examples leads to models learning the objective of the game before learning the rules of the game. We also showed that increasing the number of instruction fine-tuning examples minimizes hallucinations associated with illegal proposed moves and imagining pieces on the game board to enable check or checkmate. Finally, we showed that increasing the number of instruction fine-tuning epochs can lead to some improvements in legal moves and accurately solve chess problems, but the improvements quickly plateau after 5-10 epochs. At high level, these results further demonstrate that pretrained generative language models can serve as general learning frameworks for sequence-based tasks. Overall, our results suggest that instruction fine-tuning generative language models with chess game play data can replicate the learning experience of Stefan Zweigs character, Dr. B, in Schachnovelle. Specifically, increasing the amount of SAN-formatted game data used for instruction fine-tuning improves our models understanding of the game rules and game play. However, the amount of data Zweigs character encountered in his book of 150 championship chess games was quite small, with an estimated 40 white piece single moves per game, totaling around 6,000 white piece single moves in his book. In contrast, our systems required at least 1,000,000 white piece single-move examples to successfully learn the objectives and rules of chess. Even then, instruction fine-tuning models with 1,000,000 examples was insufficient to solve all the chess problems in our Check/Mate-in-1 dataset. This is certainly lesser outcome compared to the fictitious, yet plausible, character Dr. in Schachnovelle. Overall, these results suggest that while small generative language models can learn the latent rules of game from game play data alone, they need significantly more data than humans to do so."
        },
        {
            "title": "6 Conclusion",
            "content": "In conclusion, our results indicate that instruction fine-tuned generative language models can learn the rules of chess from only standard algebraic chess notation (SAN) game data. Further, model performance scales increasingly well with increasing orders of magnitude of instruction fine-tuning data. For example, instruction fine-tuning of the OPT-125M generative language model on 1 million examples resulted in nearly perfect proposals of legal moves when evaluated on 10,000 test examples drawn our WSM-10M dataset. Finally, we demonstrated that our instruction fine-tuned language models can propose winning strategies for chess problems."
        },
        {
            "title": "7 Acknowledgements",
            "content": "The author would like to thank Anas Bricha for supporting this project and Guy Laporte for providing access to the computational infrastructure to conduct these studies. The author declares no financial interests nor conflicts."
        },
        {
            "title": "References",
            "content": "Agrawal, A., Suzgun, M., Mackey, L., and Kalai, A. T. Do Language Models Know When Theyre Hallucinating References? arXiv, abs/2305.18248, 2024. URL https://arxiv.org/abs/2305.18248. Alekhine, A. My Best Games of Chess: 1908-1923. G. Bell and Sons, Ltd., London, 1927. Alekhine, A. My Best Games of Chess: 1924-1937. G. Bell and Sons, Ltd, London, 1939. Biderman, D., Ortiz, J. G., Portes, J., Paul, M., Greengard, P., Jennings, C., King, D., Havens, S., Chiley, V., Frankle, J., Blakeney, C., and Cunningham, J. P. LoRA Learns Less and Forgets Less. arXiv, abs/2405.09673, 2024. URL https://arxiv.org/abs/2405.09673. Bjelica, D. Alexander Alekhine. Zugarto Ediciones, S. A., Madrid, 1993. ISBN 9788488155160. Bjelica, D. José Raúl Capablanca. Zugarto Ediciones, S. A., Madrid, 1994. ISBN 9788488155269. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. Capablanca, J. R. Primer of Chess. Harcourt, Brace and Co., New York, 1935. Charushin, V. Laskers Combination: The Tacticians Handbook. Pickard and Son Publishers, 1998. ISBN 9781886846135. Chernev, I. 1000 Best Short Games of Chess. Touchstone Books, 1954. ISBN 9780671538019. Chernev, I. Practical Chess Endings: Basic Guide to Endgame Strategy for the Beginner and the More Advanced Chess Player. Dover Publications, New York, 1969. ISBN 9780486222080. Chernev, I. Capablancas Best Chess Endings: 60 Complete Games. Dover Publications, New York, 1982. ISBN 9780486242491. Chernev, I. The Most Instructive Games of Chess Ever Played: 62 Masterpieces of Chess Strategy. Dover Publications, New York, 1992. ISBN 9780486273020. Chernev, I. Twelve Great Chess Players and Their Best Games. Dover Publications, New York, 1995. ISBN 9780486286747. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Duchenne, O., Çelebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Tan, X. E., Xie, X., Jia, X., Wang, X., 13 Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Grattafiori, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Vaughan, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Franco, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Wyatt, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Ozgenel, F., Caggioni, F., Guzmán, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Thattai, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Damlaj, I., Molybog, I., Tufanov, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Prasad, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Huang, K., Chawla, K., Lakhotia, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Tsimpoukelli, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Laptev, N. P., Dong, N., Zhang, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Li, R., Hogan, R., Battey, R., Wang, R., Maheswari, R., Howes, R., Rinott, R., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Kohler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Albiero, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wang, X., Wu, X., Wang, X., Xia, X., Wu, X., Gao, X., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Hao, Y., Qian, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., and Zhao, Z. The Llama 3 Herd of Models. arXiv, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Edwards, S. J. Standard: Portable Game Notation Specification and Implementation Guide, March 1994. URL https://ia902908.us.archive.org/26/items/pgn-standard-1994-03-12/PGN_ standard_1994-03-12.txt. Internet Archive. 12 March 1994. Retrieved 25 July 2020. Eldan, R. and Li, Y.-F. TinyStories: How Small Can Language Models Be and Still Speak Coherent English? arXiv, abs/2305.07759, 2023. URL https://arxiv.org/abs/2305.07759. Fauber, B. Accurate Prediction of Ligand-Protein Interaction Affinities with Fine-Tuned Small Language Models. arXiv, abs/2407.00111, 2024a. URL https://arxiv.org/abs/2407.00111. Fauber, B. Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks. ArXiv, abs/2402.05616, 2024b. URL https://arxiv.org/abs/2402.05616. Golembek, H. Capablancas Best Games. B. T. Batsford, Ltd., London, 1996. ISBN 9780713480641. Harkness, K. The Official Blue Book and Encyclopedia of Chess. David McKay Company, Inc., New York, 1956. Hooper, D. and Whyld, K. The Oxford Companion to Chess. Oxford University Press, Oxford, England, 1992. ISBN 0198661649. Hsu, F. Behind Deep Blue: Building the Computer that Defeated the World Chess Champion. Princeton University Press, New Jersey, 2002. ISBN 9780691118185. 14 Hu, J. E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., and Chen, W. LoRA: Low-Rank Adaptation of Large Language Models. arXiv, abs/2106.09685, 2021. URL https://arxiv.org/abs/2106.09685. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7B. arXiv, abs/2310.06825, 2023. URL https: //arxiv.org/abs/2310.06825. Kmoch, H. Rubinsteins Chess Masterpieces: 100 Selected Games. Dover Publications, New York, 1960. ISBN 9780486206172. Kotov, A. Alexander Alekhine. R.H.M. Press, Delaware, 1975. Lasker, E. Laskers Manual of Chess. Dover Publications, New York, 1960. ISBN 9780486206400. Lasker, E. Common Sense in Chess. Dover Publications, New York, 1965. ISBN 9780486214405. Leininger, R. Middlegame Strategy. Pickard and Son Publishers, 1997. ISBN 9781886846074. Lombardy, W. Chess Panorama. Chilton Book Co., Pennsylvania, 1975. ISBN 9780801960789. Matanovic, A. (ed.). Encyclopaedia of Chess Openings. Batsford Limited, London, 1978. ISBN 9780713410440. Mednis, E. How to Play Good Opening Moves. Random House Publishing, New York, 1986. ISBN 9780679141099. Newell, A., Shaw, C., and Simon, H. Chess Playing Programs and the Problem of Complexity. IBM Journal of Research and Development, 4(2):320335, 1958. Nimzowitsch, A. Mein System (My System). G. Bell and Sons, Ltd., London, 1925. Nimzowitsch, A. Die Praxis Meines Systems (The Practice of My System). Siedentop and Co., Berlin, 1929. Panov, V. Capablanca. Editorial Spena Argentina S.A, Buenos Aires, 1973. Polgár, L. Chess: 5334 Problems, Combinations, and Games. Black Dog and Leventhal Publishers, New York, 2013. ISBN 9781579125547. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models are Unsupervised Multitask Learners, 2019. URL https://cdn.openai.com/better-language-models/language_ models_are_unsupervised_multitask_learners.pdf. Razuvaev, Y. S. and Murakhevri, V. I. Akiba Rubinshtein. Fizkultura Sport, Moscow, 1980. Reinfeld, F. The Immortal Games of Capablanca. Horowitz and Harkness, New York, 1942. Reinfeld, F. and Fine, R. Laskers Greatest Chess Games: 1889-1914. Dover Publications, New York, 1965. ISBN 9780486214504. Ruoss, A., Delétang, G., Medapati, S., Grau-Moya, J., Wenliang, L. K., Catt, E., Reid, J., and Genewein, T. Grandmaster-Level Chess Without Search. arXiv, abs/2402.04494, 2024. URL https://arxiv.org/ abs/2402.04494. Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P. O., Sanh, V., Laurenccon, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Etxabe, A. S., Aji, A. F., Alfassy, A., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C. C., Klamm, C., Leong, C., van Strien, D. A., Adelani, D. I., Radev, D. R., Ponferrada, E. G., Levkovizh, E., Kim, E., Natan, E., Toni, F. D., Dupont, G., Kruszewski, G., Pistilli, G., ElSahar, H., Benyamina, H., Tran, H. T., Yu, I., Abdulmumin, I., Johnson, I., Gonzalez-Dios, I., de la Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J., Frohberg, J., Tobing, J. L., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K., von Werra, L., Weber, L., Phan, L., Allal, L. B., Tanguy, L., Dey, M., Muñoz, M. R., Masoud, M., Grandury, M., vSavsko, M., Huang, M., Coavoux, M., Singh, M., Jiang, M. T.-J., Vu, M. C., Jauhar, M. A., Ghaleb, M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel, O., de Gibert, O., Villegas, P., Henderson, P., Colombo, P., Amuok, P., Lhoest, Q., Harliman, R., Bommasani, R., Lopez, R., Ribeiro, R., Osei, S., Pyysalo, S., Nagel, S., Bose, S., Muhammad, S. H., Sharma, S. S., Longpre, S., maieh Nikpoor, S., Silberberg, S., Pai, S., Zink, S., Torrent, T. T., Schick, T., Thrush, T., Danchev, V., Nikoulina, V., Laippala, V., Lepercq, V., Prabhu, V., Alyafeai, Z., Talat, Z., Raja, A., Heinzerling, B., Si, C., Salesky, E., Mielke, S. J., Lee, W. Y., Sharma, A., Santilli, A., Chaffin, A., Stiegler, A., Datta, D., Szczechla, E., Chhablani, G., Wang, H., Pandey, H., Strobelt, H., Fries, J. A., Rozen, J., Gao, L., Sutawika, L., Bari, M. S., Al-Shaibani, M. S., Manica, M., Nayak, N. V., Teehan, R., Albanie, S., Shen, S., Ben-David, S., Bach, S. H., Kim, T., Bers, T., Févry, T., Neeraj, T., Thakker, U., Raunak, V., Tang, X., Yong, Z.-X., Sun, Z., Brody, S., Uri, Y., Tojarieh, H., Roberts, A., Chung, H. W., Tae, J., Phang, J., Press, O., Li, C., Narayanan, D., Bourfoune, H., Casper, J., Rasley, J., Ryabinin, M., Mishra, M., Zhang, M., Shoeybi, M., Peyrounette, M., Patry, N., Tazi, N., Sanseviero, O., von Platen, P., Cornette, P., Lavallee, P. F., Lacroix, R., Rajbhandari, S., Gandhi, S., Smith, S., Requena, S., Patil, S., Dettmers, T., Baruwa, A., Singh, A., Cheveleva, A., Ligozat, A.-L., Subramonian, A., Neveol, A., Lovering, C., Garrette, D. H., Tunuguntla, D. R., Reiter, E., Taktasheva, E., Voloshina, E., Bogdanov, E., Winata, G. I., Schoelkopf, H., Kalo, J.-C., Novikova, J., Forde, J. Z., Tang, X., Kasai, J., Kawamura, K., Hazan, L., Carpuat, M., Clinciu, M., Kim, N., Cheng, N., Serikov, O., Antverg, O., van der Wal, O., Zhang, R., Zhang, R., Gehrmann, S., Mirkin, S., Pais, S. O., Shavrina, T., Scialom, T., Yun, T., Limisiewicz, T., Rieser, V., Protasov, V., Mikhailov, V., Pruksachatkun, Y., Belinkov, Y., Bamberger, Z., Kasner, Z., Kasner, Z., Pestana, A., Feizpour, A., Khan, A., Faranak, A., Santos, A. S. R., Hevia, A., Unldreaj, A., Aghagol, A., Abdollahi, A., Tammour, A., HajiHosseini, A., Behroozi, B., Ajibade, B. A., Saxena, B. K., Ferrandis, C. M., Contractor, D., Lansky, D. M., David, D., Kiela, D., Nguyen, D. A., Tan, E., Baylor, E., Ozoani, E., Mirza, F. T., Ononiwu, F., Rezanejad, H., Jones, H., Bhattacharya, I., Solaiman, I., Sedenko, I., Nejadgholi, I., Passmore, J., Seltzer, J., Sanz, J. B., Fort, K., Dutra, L., Samagaio, M., Elbadri, M., Mieskes, M., Gerchick, M., Akinlolu, M., McKenna, M., Qiu, M., Ghauri, M., Burynok, M., Abrar, N., Rajani, N., Elkott, N., Fahmy, N., Samuel, O., An, R., Kromann, R. P., Hao, R., Alizadeh, S., Shubber, S., Wang, S. L., Roy, S., Viguier, S., Le, T.-C., Oyebade, T., Le, T. N. H., Yang, Y., Nguyen, Z. K., Kashyap, A. R., Palasciano, A., Callahan, A., Shukla, A., Miranda-Escalada, A., Singh, A. K., Beilharz, B., Wang, B., de Brito, C. M. F., Zhou, C., Jain, C., Xu, C., Fourrier, C., Perinan, D. L., Molano, D., Yu, D., Manjavacas, E., Barth, F., Fuhrimann, F., Altay, G., Bayrak, G., Burns, G., Vrabec, H. U., Bello, I. I., Dash, I., Kang, J. S., Giorgi, J., Golde, J., Posada, J. D., Sivaraman, K., Bulchandani, L., Liu, L., Shinzato, L., de Bykhovetz, M. H., Takeuchi, M., Pàmies, M., Castillo, M. A., Nezhurina, M., Sanger, M., Samwald, M., Cullan, M., Weinberg, M., Wolf, M., Mihaljcic, M., Liu, M., Freidank, M., Kang, M., Seelam, N., Dahlberg, N., Broad, N. M., Muellner, N., Fung, P., Haller, P., Chandrasekhar, R., Eisenberg, R., Martin, R., Canalli, R. L., Su, R., Su, R., Cahyawijaya, S., Garda, S., Deshmukh, S. S., Mishra, S., Kiblawi, S., Ott, S., Sang-aroonsiri, S., Kumar, S., Schweter, S., Bharati, S. P., Laud, T., Gigant, T., Kainuma, T., Kusa, W., Labrak, Y., Bajaj, Y., Venkatraman, Y., Xu, Y., Xu, Y., Xu, Y., Tan, Z. X., Xie, Z., Ye, Z., Bras, M., Belkada, Y., and Wolf, T. BLOOM: 176B-Parameter Open-Access Multilingual Language Model. arXiv, abs/2211.05100, 2022. URL https://arxiv.org/abs/2211.05100. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., and Hassabis, D. General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go Through Self-play. Science, 362(6419):11401144, 2018. URL https://www.science.org/doi/abs/10.1126/science.aar6404. Tartakower, S. Die Hypermoderne Schachpartie (The Hypermodern Game of Chess). Verlag der Wiener Schachzeitung, Vienna, 1924. Tartakower, S. and du Mont, J. 500 Master Games of Chess. G. Bell and Sons, Ltd., London, 1952. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M. T., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open Pre-trained Transformer Language Models. arXiv, abs/2205.01068, 2022. URL https://arxiv.org/abs/2205.01068. Zweig, S. Schachnovelle (The Royal Game). Verlag Pigmalión, Buenos Aires, 1942."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Computational Infrastructure and Code The results described in this article were carried out using Dell Technologies PowerEdge C4140 server with 4 V100 NVIDIA SXM GPU cards with 32 GB VRAM each and NVLinkTM connectivity. There were 2 Intel Xeon processors on the server with 1.5 TB of CPU RAM. The server was configured with the Ubuntu v22.04 Linux operating system, Anaconda v23.1.0, NVIDIA CUDA v12.2, and NVIDIA drivers v535.54.03. Additional python dependencies included: python-chess v1.10.0, torch v2.1.1, and transformers v4.45.1. The Stanford ALPACA language model code was git cloned directly from https://github.com/ tatsu-lab/stanford_alpaca (accessed 30Dec2023). The train.py file in the GitHub repo, along with our corresponding instruction fine-tuning dataset, was used to instruction fine-tune the language models in our study. The language model fine-tuning code was executed via the command line interface (CLI). As an example, the following CLI command was used to instruction fine-tune pretrained foundational language model on 4 GPUs: r u o _ _ e =4 [ TRAINING_PY_FILE ] [OUTPUT_DIRECTORY ] e _ e _ _ h [HUGGINGFACE_MODEL_NAME] a _ h [ DATA_PATH_TO_FORMATTED_JSON_FILE ] 1 6 s p _ v r _ p _ T u _ i _ c 3 _ i _ i _ c _ e 4 _ i _ l _ c _ e 4 d t _ u a n _ p 8 e _ a y \" p \" e _ p 5000 e _ a _ i 1 r g _ e 2 4 g _ a 0 . m _ i 0 . 0 3 _ e e _ e \" i \" d 41 g _ p 1 3 2 s 17 A.2 Dataset Creation The dataset was focused on, but not exclusive to, games from prominent chess masters in the early 1900s to 1940s. The chess Grandmasters and International Chess Federation or FIDE (Fédération Internationale des Échecs) world champions leading up to the early 1940s included Alexander A. Alekhine (18921946) (Alekhine, 1927; 1939; Kotov, 1975; Bjelica, 1993), José Raúl Capablanca (18881942) (Capablanca, 1935; Reinfeld, 1942; Panov, 1973; Chernev, 1982; Bjelica, 1994; Golembek, 1996), Emanuel Lasker (1868 1941) (Lasker, 1960; 1965; Reinfeld & Fine, 1965; Charushin, 1998), Aron I. Nimzowitsch (18861935) (Nimzowitsch, 1925; 1929), Akiba K. Rubinstein (18801961) (Kmoch, 1960; Razuvaev & Murakhevri, 1980), and Savielly Tartakower (18871956) (Tartakower, 1924). In addition to the above-cited texts, we also included texts for short games (Chernev, 1954), openings (Mednis, 1986), middle games (Leininger, 1997), endings (Chernev, 1969), and esteemed games (Tartakower & du Mont, 1952; Lombardy, 1975; Chernev, 1992; 1995) to cover key tactics of game play. All chess game data was collected from the referenced texts as portable game notation (PGN) format files. All games were parsed into individual moves for both white and black pieces, along with the board status before each move. The FEN game boards were converted into SAN, ensuring that both the move and board status were in SAN. The conversion to SAN was conducted to align with the experience of Stefan Zweigs character, Dr. B., in Schachnovelle. Figure A1: Top 10 most prevalent white piece single-moves. Moves are shown by their standard algebraic chess notations (SAN). The x-axis of the plot displays the total single-move count for each move, with data labels indicating their percentage of the overall total. Parsing of the PGN files associated with all the above-cited publications resulted in total of 20M board and single-move combinations for white and black pieces. Filtering to only white piece single-moves resulted in 10M board and move combinations. The top 10 most prevalent white piece single-moves and proceeding board configurations are shown in Figures A1 and A2, respectively. Board diagrams of the top 8 (excluding the initial array) most common board configurations, and their associated Encyclopedia of Chess Openings (ECO) traditional names and codes (Matanovic, 1978), and paired-move sequences, are shown in Figure A3. The notable chess players associated with the single-move data are shown in Figure A4. Each game in the WSM-10M dataset contained mean of 39 white piece single-moves, with the longest game in the dataset containing 108 white piece single-moves. Further, there were 80k unique board and white piece single-move combinations in the dataset. Figure A2: Top 10 most prevalent chess boards preceding white piece single-moves (i.e., next move = white). Common move openings and sequences are described by their Encyclopedia of Chess Openings (ECO) traditional names. The x-axis of the plot displays the total board count for each board status, with data labels indicating their percentage of the overall total. Figure A3: Top 8 (excluding the initial array) most common board configurations in the white piece single-move dataset where white moves next. Their associated Encyclopedia of Chess Openings (ECO) traditional names and codes, and paired-move sequences in SAN, are included. 19 Figure A4: Notable chess players in the dataset were identified by the main player linked to each book or data source, covering all games/setups and the associated 20 million single piece moves. The x-axis of the plot displays the single-move count for each player, with data labels indicating their percentage of the overall total. 20 A.3 Language Model Text Generation Configuration The same language model fine-tuning and generation configurations were utilized throughout our studies, and only single-parameter changes were permitted, as annotated in the tables, when comparing methods. Language model text generation was conducted via the HuggingFace transformers library. Transformers GenerationConfig() was set to the default parameters, along with: num_beams = 2, repetition_penalty = 1.3, do_sample = False (for consistent output generation), early_stopping = True, max_time = 10, and length_penalty = 0.4. In prior studies, we found the above configuration parameters provided stable and reproducible text generation (Fauber, 2024b). The text generation prompt and the general prompt used in the language model fine-tuning process were identical: \"Below is an instruction that describes task. completes the request. ### Instruction: {instruction} ### Response: Write response that appropriately {output}\". Although not always necessary, we enforced truncation of the output text for all models to ensure consistency in outcomes. Truncation of the OPT model text output returned all text following the \"### Response:\" string. Similarly, the TinyStories families of models truncated the output to the text following the \"<endoftext>\" string. 21 A.4 Analysis of the Instruction Text We explored the role of the instruction fine-tuning text in achieving legal move which resulted in check or checkmate. For all instruction fine-tuning examples, we included the statement, \"You are chess Grandmaster and checkmate # is your goal.\" One could posit that this statement explicitly states the games objective, and the model was learning this objective from the instruction text, along with all moves marked with the hash sign \"#\" to indicate checkmate. To evaluate the role of this statement in the instruction fine-tuning text, we used the exact same 1,000 to 1,000,000 instruction fine-tuning cohorts for WSM-10M without the statement, \"You are chess Grandmaster and checkmate # is your goal.\" We referred to this revised instruction fine-tuning dataset as NoGoal-WSM10M. We found that usage of the WSM-10M cohort or the NoGoal-WSM-10M cohort for instruction finetuning the OPT-125M model resulted in essentially the same outcomes for the percentage of legal proposed moves and legal proposed moves which resulted in check or checkmate (Table 3, Figure 4, and Figure 5). The language models fine-tuned with the NoGoal-WSM-10M datasets also showed similar results with regards to piece hallucinations and illegal moves proposed to achieve check or checkmate, as observed with the WSM-10M fine-tuned models (Table A1 and Figure 6). Instruction Fine-Tuning Dataset Instruction Fine-Tuning Example Count % Illegal and Check/Mate Moves % Piece Not on Board and Check/Mate Moves NoGoal-WSM-10M NoGoal-WSM-10M NoGoal-WSM-10M NoGoal-WSM-10M 1,000 10,000 100,000 1,000,000 84% 67% 47% 40% 3% 0% 0% 0% Table A1: Percentage of proposed moves that were either illegal, or the proposed piece was not on the board, to achieve either check or checkmate versus count of the instruction fine-tuning examples for the OPT-125M language model, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The instruction fine-tuning examples for these instances were drawn exclusively from the NoGoal-WSM-10M dataset. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the models propensity to generate an illegal move that resulted in check or checkmate."
        }
    ],
    "affiliations": [
        "Dell Technologies"
    ]
}