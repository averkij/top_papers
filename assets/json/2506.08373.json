{
    "paper_title": "Draft-based Approximate Inference for LLMs",
    "authors": [
        "Kevin Galim",
        "Ethan Ewer",
        "Wonjun Kang",
        "Minjae Lee",
        "Hyung Il Koo",
        "Kangwook Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 7 3 8 0 . 6 0 5 2 : r Draft-based Approximate Inference for LLMs Kevin Galim1 Ethan Ewer2 Wonjun Kang1,3 Minjae Lee1 Hyung Il Koo1,4 Kangwook Lee2 1FuriosaAI 2UW-Madison 3Seoul National University 4Ajou University {kevin.galim, kangwj1995, minjae.lee, hikoo}@furiosa.ai {eewer, kangwook.lee}@wisc.edu"
        },
        {
            "title": "Abstract",
            "content": "Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft models attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm."
        },
        {
            "title": "Introduction",
            "content": "The demand for longer context lengths in large language models (LLMs) [1, 18] continues to grow [36], driven by applications such as dialogue systems [1, 18], document summarization [35], and code completion [15]. Modern models like GPT-4 [1] and Gemini-2.5-Pro [18] have pushed context windows to over million tokens. However, scaling Transformers [48] to these lengths remains difficult due to significant computational and memory constraints. Attention computation scales quadratically with context length, increasing inference latency, while key-value (KV) cache memory grows linearly, straining GPU resources. E.g., caching the KV states for 128K tokens in Llama-3.1-8B [19] can consume over 50GB of memory, limiting the practical scalability of LLMs. To address scalability challenges, recent work introduces approximate LLM inference techniques that reduce latency and memory usage at inference time. Techniques include sparse attention for prefilling [25] and decoding [44], which speed up inference by having each query attend to only subset of keys. Sparse prefilling shortens time to the first token, while sparse decoding boosts generation throughput. KV cache dropping [7, 32, 50, 54] reduces memory and increases throughput by shrinking the cache after prefilling or during decoding. Prompt compression [11, 26, 33] further Equal contribution. Preprint. Under review. Figure 1: Overview of our Draft-based Approximate Inference framework for input token importance estimation. Unlike prior methods that rely only on input tokens, our approach incorporates draft model predictions of future output tokens, yielding more accurate importance estimates. This better aligns with the hypothetical oracle setting, where the true output is known and influential tokens can be precisely identified. improves efficiency by removing unimportant tokens before inputting the prompt, reducing both attention and MLP computation, as well as decreasing KV cache size. Orthogonally, speculative decoding [6, 9, 22, 30] accelerates LLM inference by using small draft model to propose multiple tokens, which the target model verifies in parallel. This improves throughput without altering the output distribution and is particularly effective for autoregressive models, where sequential generation is bottleneck. However, unlike approximate inference, speculative decoding does not lower the total memory or computation requirements and struggles with increasing context length. In contrast, approximate LLM inference improves efficiency by reducing the amount of computation the model performs. This is often done by estimating the importance of each token or KV pair for future generation and discarding less important ones from attention or feedforward computations. Current methods [7, 16, 25, 32] use attention activations from input tokens to predict which tokens or KV pairs future tokens will attend to, as future tokens are not yet available. However, input attention activations alone do not reliably identify the tokens or KV pairs most relevant for future token generation. In this work, we argue that token importance estimation can be improved by incorporating information from future tokens. To enable this without incurring the full cost of generating them, we introduce Draft-based Approximate Inference, lookahead-based framework that uses smaller, more efficient draft model to approximate future outputs with minimal overhead  (Fig. 1)  . By leveraging the context provided by this draft output, Draft-based Approximate Inference improves token importance estimates, enabling more accurate inference approximations. Our main contributions are as follows: 1. We present Draft-based Approximate Inference, the first framework to use draft model lookahead for enhanced approximate inference. 2. Within the Draft-based Approximate Inference framework, we develop two concrete algorithms targeting three LLM inference optimizations: Speculative KV Dropping (SpecKV) for KV cache dropping with sparse prefill, and Speculative Prompt Compression (SpecPC) for prompt compression. 3. We present theoretical analyses that establish both the rigorous justification and the anticipated effectiveness of our proposed methods. 4. We perform comprehensive experiments on long-context benchmarks, demonstrating that our methods attain state-of-the-art accuracy under fixed KV cache or prompt size constraints. Our results consistently outperform prior baselines (by up to 25 points on RULER [21]), underscoring the potential of draft models for fast and accurate approximate inference in large language models."
        },
        {
            "title": "2 Related Work",
            "content": "Table 1: Summary of prior work. Complexity reported without auxiliary/draft model. nin and nout denote the number of input and output tokens, respectively. Cmax represents the maximum KV cache or prompt capacity. sprefill and sdecode indicate the number of keys each query attends to during the prefill and decoding phases, respectively. Highlighted cells indicate improved complexity. Type Dense Method Dense Sparse Attn KV Dropping Prefill Time O(nin 2) Decoding Time Decoding Space O(nout(nin + nout)) O(nin + nout) Sparse attention MInference [25], FlexPrefill [29] Quest [44], RetrievalAttention [34] KV dropping StreamingLLM [50] H2O [54] SnapKV [32], PyramidKV [7], AdaKV [16] SpecKV (Ours) Prompt compression LLMLingua-2 [38], CPC [33], R2C [11], SpecPC (Ours) Prefill Decode Prefill Prefill Decode Decode After prefill After prefill O(ninsprefill) O(nout(nin + nout)) O(nin + nout) O(nin 2) O(noutsdecode) O(nin + nout) O(ninCmax) O(nin O(nin 2) 2) O(noutCmax) O(noutCmax) O(Cmax) O(Cmax) O(nout(Cmax + nout)) O(Cmax + nout) O(ninsprefill) O(nout(Cmax + nout)) O(Cmax + nout) O(Cmax) O(nout(Cmax + nout)) O(Cmax + nout) Sparse Attention One way to improve inference efficiency is through sparse attention with static patterns. For example, sliding window attention [5], used in models like Mistral 7B [24] and Gemma 3 [45], restricts each query to attend only fixed-size window of recent keys, reducing computation and KV cache size during decoding. StreamingLLM [50] improves on sliding window by using initial tokenscalled attention sinksalong with the sliding window. MInference [25], adopted by Qwen2.5-1M [52], further boosts prefill efficiency by searching offline for adaptive sparse attention patternsA-shape, Vertical-Slash, and Block-Sparseassigned per head. FlexPrefill [29] extends this idea by determining sparsity rates for each input prompt. In contrast, Quest [44] and RetrievalAttention [34] target the decoding stage by only retrieving the most important KV pairs from the cache, reducing both memory bandwidth and computational demands during generation. KV Cache Dropping KV dropping reduces computation and memory during decoding. Sliding window attention [5] and StreamingLLM [50] are examples of KV dropping methods (as well as sparse attention) as they permanently evict KV pairs from cache. H2O [54] improves on this by dynamically selecting attention sinks, termed heavy-hitters, using attention scores at each decoding step, while also maintaining sliding window. SnapKV [32] compresses the KV cache at the end of the prefill stage by dropping unimportant KV pairs. Subsequent work extends this idea by allocating KV cache budgets non-uniformly across layers (PyramidKV [7]) and attention heads (AdaKV [16], HeadKV [17]). However, these approaches drop tokens based only on current information, making them less robust to changes in token importance over time [37]. In contrast, our method predicts future token importance using draft model for more accurate importance estimation. Prompt Compression Prompt compression removes tokens before reaching the model, reducing compute and memory usage during both prefill and decodingunlike KV dropping, which speeds up only decoding. It also surpasses sparse attention by saving both attention and MLP computation. Prompt compression works seamlessly with all types of inference setups, such as APIs or inference engines like vLLM [28], since it does not require any modifications to the model. However, KV dropping can achieve higher compression because it selects tokens per head, while prompt compression drops the same tokens across all layers and heads. In question-answer setup, prompt compression may be question-agnostic (compressing context without considering the question) or question-aware (factoring in the question). Selective context [31] and LLMLingua [26] are training-free, question-agnostic approaches using small LLM to keep only key tokens. LongLLMLingua [27] adapts this for longer contexts in question-aware manner. LLMLingua-2 [38] trains small model [12] to score token importance without using the question. CPC [33] uses trained encoder to compute sentence importance via cosine similarity with the question, while R2C [11] splits the prompt into chunks, processes each with the question using fine-tuned encoder-decoder Transformer (FiD [23]), and ranks them via cross-attention. Unlike CPC and R2C, our proposed SpecPC method imposes no constraints on prompt format and works seamlessly with any input or output accepted by the underlying draft modelincluding structured data, code, and visual modalities like image soft tokens. While prior methods are typically 3 Table 2: Comparison of speculative decoding and Draft-based Approximate Inference (ours). Method Memory Latency Compute Aim Speculative Decoding Draft-based Approximate Inference (Ours) Decode All Increase hardware utilization Save compute and memory limited to sentence-level structures, single modalities, or text-only/image-only formats, SpecPC offers unified and extensible approach that efficiently handles arbitrary and mixed-modal inputs. Speculative Decoding Speculative decoding [6, 9, 22, 30] emerged as an effective method for accelerating LLM inference. It leverages small draft model to propose multiple tokens, which the full target model then verifies in parallel, increasing decoding throughput without changing the output distribution. This technique is especially advantageous for autoregressive models, where sequential token generation is major bottleneck. Previous work further accelerates speculative decoding by enabling approximate inference in the draft model, using techniques such as sparse attention [42], KV dropping [43], or KV cache quantization [47], all while preserving exact inference. In contrast, our approach is the first to leverage draft models for fast, approximate inference directly in the target model. Table 1 summarizes prior work, highlighting their prefill, decoding time, and memory complexities."
        },
        {
            "title": "3 Proposed Framework: Draft-based Approximate Inference",
            "content": "Existing LLM approximation methods [7, 16, 25, 32] estimate the importance of current tokens on future generation by analyzing current attention patterns. While this can be effective, it provides only rough estimate of each tokens importance. In contrast, if future tokens were available, we could make substantially better importance estimates by directly identifying which input tokens contribute to generating those output tokens. However, these future tokens are inaccessible before generation. To overcome this, we propose novel lookahead-based framework that leverages approximate future information to improve token importance estimation. Inspired by speculative decoding, we use smaller draft model to generate an approximation of the future tokens. Our goal fundamentally differs from speculative decoding  (Table 2)  . Speculative decoding improves hardware utilization by having draft model propose tokens that the target model verifies, accelerating generation without changing the output distribution. However, it does not reduce total computation or memory usage. In contrast, we reduce computation and memory costs by directly approximating the target models behavior. When the draft and target models are trained similarly, or if the draft is distilled [20] from the target, their outputs and attention patterns typically align, making the draft output reliable proxy for future tokens. This approximation enables more accurate importance estimation by revealing which input tokens have the greatest influence on the draft output. This idea forms the basis of our Draft-based Approximate Inference framework  (Fig. 1)  , which we instantiate in two settings: SpecKV for KV cache dropping and sparse prefilling (Section 4) and SpecPC for prompt compression (Section 5)."
        },
        {
            "title": "4 SpecKV: Robust Importance Estimation for KV Cache Dropping",
            "content": "Existing sparse attention and KV cache dropping methods [25, 32] estimate token importance by analyzing attention activations from the last nwindow queries to all keys. While this offers rough approximation of future attention patterns, it can be inaccurate when the set of important KV pairs shifts over the course of generation. We argue that using attention activations from draft output queries to all keys yields more robust and accurate estimate of KV pair importance. 4 Figure 2: Overview of SpecKV: Instead of using only the last prompt tokens like SnapKV, SpecKV employs lightweight draft model to generate lookahead tokens, providing richer context for more accurate KV importance estimation. Figure 3: Importance scores estimated by SnapKV (top) and SpecKV (ours, bottom) on sample from RULER S-NIAH-3 [21], averaged across all heads. SpecKV exhibits stronger activations for important tokens relevant to the needle retrieval task. Lighter colors indicate higher attention activations and greater importance scores. 4.1 Motivation for SpecKV To guide both sparse prefilling and KV cache dropping, SpecKV estimates the importance of each KV pair in the input sequence. We define importance as the average attention activation from output queries to input keys. Specifically, the vector of importance scores and its approximation are given by sT = 1 nout nout i=1 (cid:88) Softmax x(o)T WqW T (cid:18) (cid:19) nout and ˆsT = 1 nout Softmax ˆx(o)T WqW d , (cid:19) (1) Rd is the ith output Rd is the ith approximate output embedding (from the draft model). si and Rnind is the matrix of input embeddings, x(o) i=1 (cid:88) (cid:18) where = [x1, . . . , xnin ]T embedding, and ˆx(o) ˆsi denote the importance of the ith KV pair. To understand when SpecKV provides reliable importance estimates, we analyze simplified setting where the draft and target models produce similar embeddings. Specifically, we consider single attention layer and assume that the output embeddings from the draft model are ϵ-close (in ℓ2 norm) to those from the target model. x(o) ˆx(o) ϵ for all and Theorem 1. If 2 2 This result shows that for single attention layer, the worst-case error in the approximate importance scores is proportional to the worst-case error in the approximate output embeddings, implying that SpecKV provides reliable estimates as long as the draft model remains reasonably accurate (see Appendix B.1 for proof). ˆs 2 ϵ WqW 2.2 for all j, then xj 4.2 SpecKV Algorithm SpecKV (Algorithm 1) begins by generating draft output of length nlookahead using small draft model, which acts as proxy for the target models future outputs. During prefilling, both the input tokens and the draft tokens are passed through the target model. For each attention head, we compute token importance scores by measuring the cross-attention between the queries from the last nwindow input tokens and the draft output tokens to the remaining input keys  (Fig. 2)  . Using draft outputs 2If the weight matrices are close to Kaiming uniform with gain of 5 5, then WqW 2 5. (a) Mean aggregation (b) Max aggregation Figure 4: Correlation of attention activations between Llama-3.2-1B (draft) and Llama-3.1-8B (target) on 45 RULER (16K) [21] examples (five per each of nine tasks). Attention maps are aggregated across layers, heads, and queries using mean or max per key. Each point plots keys activation in the draft versus target model. Both aggregation methods ((a) mean, (b) max) reveal strong correlations. allows SpecKV to better identify important tokens  (Fig. 3)  . We apply local pooling with kernel size to the attention scores to maintain continuity. These scores guide two optimizations: sparse prefilling and KV cache dropping. For sparse prefilling, we use variation of the Vertical-Slash kernel pattern nwindow KV pairs with the introduced in [25]. For KV cache dropping, we retain the top Cmax highest importance scores, along with the final nwindow KV pairs from the most recent tokens."
        },
        {
            "title": "5 SpecPC: Leveraging Draft Models for Efficient Prompt Compression",
            "content": "SpecKV leverages the output tokens of lightweight draft model to enable more effective KV cache dropping, building upon the core assumption of speculative decoding that the draft and target models have similar output distributions. In this section, we take this approach further by directly utilizing the draft models attention activations to accelerate inference. Specifically, we introduce SpecPC, which compresses the prompt to achieve improved latency and reduced memory usage during both prefilling and decoding stages, surpassing the efficiency gains provided by traditional KV cache dropping. 5.1 Motivation for SpecPC Assuming the draft and target models produce similar outputs, we analyze the similarity of attention activations in single attention layer. The target model attention layer uses weights Wq, Wk, and Wv, and the draft model attention layer uses ˆWq, ˆWk, and ˆWv. Let the input prompt be = [x1, . . . , xn]T Rnd. The outputs of the target attention layer are = Softmax XWqW T XWv = AXWv, (2) where = [a1, . . . , an]T is the attention matrix. Similarly, the outputs of the approximate (e.g., draft model) attention layer are ˆY = Softmax ˆWq ˆW T ˆWv = ˆAX ˆWv, (3) where ˆA = [ˆa1, . . . , ˆan]T is the approximate attention matrix. If the scaled inputs satisfy the Restricted Isometry Property (RIP)3 [8]a condition widely studied in compressed sensing to ensure the stable recovery of sparse signalswe can establish the following bound: 3The input embedding matrix may satisfy the RIP if its entries are approximately uniformly or normally distributed. RIP can also hold with positional embeddings constructed from Fourier basis. 6 (cid:16) (cid:16) (cid:17) (cid:17) ϵ ˆyi 2 ,2, then the attention error satisfies Theorem 2. If there exists constant such that cX satisfies the Restricted Isometry Property with parameters 2k, δwhere is the approximate sparsity of ai and ˆaiand the output error satisfies yi This result offers surprising and elegant connection: it reveals that mathematical tools developed for compressed sensing can also bound the error in attention approximations. Specifically, it shows that the worst-case error in the approximate attention activations is proportional to the worst-case error in the approximate outputs, with the constant depending on the conditioning of the weight matrices and the maximum input embedding norm. This implies that if the draft model provides reasonable approximation of the output, it also gives reasonable approximation of the attention activations (see Appendix B.2 for proof). Furthermore, even if the scaled inputs do not satisfy the RIP, we can still bound the attention approximation error by applying Theorem 3 (see Appendix B.3 for proof). 2cϵX,2 σmin(Wv)(1δ) .4 2 ˆai ai In addition to theoretical analysis, we perform small experiment to analyze the correlation of attention activations of Llama-3.2-1B (draft) and Llama-3.1-8B (target) in Fig. 4 where we plot each draft model activation versus target model activation (see Fig. 8 for additional models). The results show that their attention is highly correlated, further motivating our idea to use draft attention activations to approximate token importance. 5.2 SpecPC Algorithm Based on our analysis, we present SpecPC (Algorithm 2). SpecPC feeds an input prompt (length nin) Rnlayernhead(nin+nlookahead1)nin , to the draft model and directly extracts its attention activations where nlayer and nhead denote the number of layers and heads. These activations indicate token importance and are used to drop less relevant tokens from the prompt. 1) queries over each Specifically, we use attention activations from the final (nwindow + nlookahead key, excluding the last nwindow keys, which are always retained. We skip the first lskip layers, as later layers provide more focused importance scores, while early layers attend broadly [7]. To aggregate per-token attention, we reweight queries based on proximity to the prompts endlater tokens get higher weights. Aggregation is performed across layers, heads, and queries to produce single importance score per token (excluding the always-kept window). While mean aggregation shows superior attention correlation  (Fig. 4)  , max aggregation better prioritizes critical tokens and performs best in retrieval tasks. We smooth aggregated scores with average pooling, then apply max pooling so that included tokens also bring their neighbors. This maintains the local context LLMs require. Unlike other methods that select entire sentences, we avoid sentence-level pre-processing to support non-text inputs, such as images. We then select the top-Cmax tokens with the highest scoresalways including window tokensto form the compressed prompt, which is passed to the target model for generation."
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Setup We evaluate SpecKV and SpecPC on two benchmarks: RULER [21] and LongBench [4], comparing them against several baselines. For models, we use Qwen2.5-Instruct [51] (0.5B draft, 14B target) and Llama-3-Instruct [19] (3.2-1B draft, 3.1-8B target). RULER is synthetic benchmark with 13 tasks of varying complexity, including tasks such as key-value retrieval (NIAH), multi-hop tracing, and aggregation. It can be generated at any sequence length to assess models effective context window. We evaluate at 4K, 8K, 16K, 32K, and 64K (Qwen is excluded at 64K due to its 32K sequence limit). LongBench contains 14 English, five Chinese, and two code tasks across five categories. We exclude the Chinese tasks (unsupported by Llama) and synthetic tasks (already covered by RULER). For SpecKV, we compare against KV dropping methodsStreamingLLM [50], H2O [54], SnapKV [32], PyramidKV [7], and AdaKV [16] (Ada-SnapKV)using compression size (Cmax) of 256. Since layer-wise (PyramidKV) and head-wise (AdaKV) cache budgets can also be applied 4X,2 denotes the maximum ℓ2 norm of Xs rows; σmin(Wv) is the smallest singular value of Wv. (a) KV dropping (Cmax = 256) (b) Prompt compression (Cmax = 1024) Figure 5: Performance of SpecKV and SpecPC on RULER compared to baselines. Both methods consistently outperform all baselines across sequence lengths. Notably, SpecPC closely matches the target model, maintaining strong results at longer contexts. Low-performing methods are omitted. to SpecKV, we further evaluate AdaKV combined with SpecKV (Ada-SpecKV). For SpecPC, we benchmark against LLMLingua-v2 [38], CPC [33], and R2C [11] with Cmax = 1024. Based on our ablation studies  (Fig. 12)  , we set nlookahead to the maximum token limit for SpecKV and to one for SpecPC. See Appendix for datasets and metrics, Appendix for experimental details, and Appendix for additional results, including evaluations with different Cmax values, more models, and multimodal experiments using Qwen2.5-VL [3] on the MileBench [14] image-text benchmark. 6.2 Results Fig. 5 (RULER), Table 3 (LongBench Qwen), and Table 4 (LongBench Llama), compare our methods with baselines. Both SpecKV and SpecPC consistently outperform other methods, demonstrating superior KV cache and prompt compression. Their performance far exceeds the draft model, highlighting robustness even with weaker drafts. Performance improves further with better drafts  (Fig. 11)  . On RULER, SpecKV exceeds baselines by up to 25 points and SpecPC nearly matches the target model, maintaining strong results even at longer context lengths. On LongBench, our methods excel in few-shot learning and code generation, where the draft output is especially valuable. Notably, for Qwen code completion, SpecPC even outperforms the target, suggesting prompt compression can improve performance by filtering out irrelevant context. For larger Cmax, our methods remain superior (Fig. 9; Tables 9 and 10). Finally, multimodal (Appendix E.4) and additional model (Appendix E.6) evaluations further reaffirm the effectiveness and generality of our approach. Table 3: LongBench performance with Qwen2.5-0.5B (draft) and Qwen2.5-14B (target). Cmax Method Singledoc QA Multidoc QA Summary Few-shot Learning Code Completion Dense Draft Target 21.04 53.19 KV 256 PC 1024 StreamingLLM 38.66 46.98 H2O 49.07 SnapKV 47.07 PyramidKV 50.92 Ada-SnapKV 51.07 SpecKV 51.92 Ada-SpecKV LLMLingua-2 CPC R2C SpecPC 27.10 42.76 46.43 47. 21.23 24.99 19.10 19.82 19.49 18.32 19.63 23.20 24.90 22.57 22.58 22.64 23.30 54.07 65.34 49.75 50.88 54.49 54.81 55.07 59.68 61.05 35.85 48.38 47.21 59. 33.39 51.75 35.23 47.86 47.34 43.26 49.58 49.58 53.43 37.62 35.57 29.48 52.52 24.4 42.83 24.13 29.66 33.19 31.84 34.57 38.76 39.38 23.74 36.05 37.42 38. 8 All 30.64 47.33 33.24 38.41 40.25 38.76 41.41 44.09 45.61 28.79 37.17 37.15 43.73 Table 4: LongBench performance with Llama-3.2-1B (draft) and Llama-3.1-8B (target). Cmax Method Singledoc QA Multidoc QA Summary Few-shot Learning Code Completion Dense Draft Target 28.08 45.85 KV PC 1024 StreamingLLM 38.69 43.54 H2O 43.79 SnapKV 43.62 PyramidKV Ada-SnapKV 44.04 43.23 SpecKV 42.40 Ada-SpecKV LLMLingua-2 CPC R2C SpecPC 29.61 35.67 38.41 44.83 27.27 43. 27.12 36.81 37.31 37.79 37.86 39.73 40.73 24.83 36.61 39.07 39.94 25.65 28.68 21.64 22.62 21.96 21.75 22.34 24.43 25.74 23.43 25.26 25.28 25.85 60.16 66. 50.75 55.64 56.29 55.15 59.95 60.90 57.94 24.18 34.01 43.26 63.70 31.11 50.46 34.74 47.81 47.34 46.32 50.39 51.09 52.51 40.66 43.58 43.99 44.82 All 34.69 46.84 34.58 40.82 40.91 40.54 42.38 43.36 43.25 27.68 34.42 37.58 43.76 (a) KV dropping (Cmax = 256) (b) Prompt compression (Cmax = 1024) Figure 6: Time-to-First-Token (TTFT) latency with SpecKV and SpecPC compared to various baseline methods (using Qwen2.5). 6.3 Performance Fig. 6 compares the latency of our proposed methods and several baselines using Qwen2.5-14B as the target model and Qwen2.5-0.5B as the draft model, running on single NVIDIA H100 80GB GPU. We use nlookahead = 64 for SpecKV and nlookahead = 1 for SpecPC, matching our experimental setup in Section 6.2.5 We measure latency up to the generation of the first target token (TTFT), including both the draft stage and any auxiliary models. SpecKV surpasses SnapKV due to sparse prefilling. Without this step, SpecKV is only slightly slower than SnapKV, indicating that the draft model adds minimal overhead. Other KV dropping methods are omitted, as their performance is similar to SnapKV. For prompt compression, our SpecPC method outperforms all baselines despite processing the full prompt at once. By contrast, methods like CPC and R2C split the prompt into smaller units to reduce attention complexity but require CPU-based text preprocessing, which increases overhead for longer sequences. Overall, prompt compression methods are faster than KV dropping, as only Cmax tokens are passed to the target model. Memory-wise, SpecKV is similar to SnapKV (with negligible overhead from draft weights), and SpecPC is more memory-efficient than R2C (see Appendix E.1)."
        },
        {
            "title": "7 Discussion",
            "content": "In this paper, we introduced Draft-based Approximate Inference with Speculative KV Dropping (SpecKV) and Speculative Prompt Compression (SpecPC), the first methods to leverage draft models for accelerating long-context inference through KV cache dropping and prompt compression. Rooted in strong theoretical motivation, our techniques demonstrate clear advantages over existing baselines, achieving superior accuracy and reduced latency on standard long-context benchmarks. These results mark significant advancement in expanding the utility of draft models beyond speculative decoding, opening new avenues for accurate and efficient approximate inference. We believe this line of research will prove increasingly vital as the demand for efficient long-context inference continues to grow. 5Although some tasks can generate up to 128 tokens, the output typically remains under 64 tokens. 9 Limitations and Future Work For SpecKV, draft generation causes minimal latency. However, very long outputs or large nlookahead values may reduce performance. In these cases, lowering nlookahead could maintain speed with little loss in accuracy. For SpecPC, increasing nlookahead to generate more tokens led to only minor accuracy gains; better leveraging longer drafts remains future work. Currently, Draft-based Approximate Inference supports sparse prefill, KV dropping, and prompt compression. Extensions such as lookahead-based sparse decoding or iterative KV cache droppingwhere KV entries are periodically removed using draft lookaheadcould further improve support for reasoning models with long outputs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [5] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020. [6] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. In International Conference on Machine Learning, pages 52095235. PMLR, 2024. [7] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, and Xiao Wen. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. [8] E.J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51(12):42034215, 2005. [9] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. CoRR, abs/2302.01318, 2023. [10] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. [11] Eunseong Choi, Sunkyung Lee, Minjin Choi, Jun Park, and Jongwuk Lee. From reading to compressing: Exploring the multi-document reader for prompt compression. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1473414754, 2024. [12] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, 2020. 10 [13] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. [14] Song Dingjie, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. In First Conference on Language Modeling, 2024. [15] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE 24, New York, NY, USA, 2024. Association for Computing Machinery. [16] Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and S. Kevin Zhou. Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference, 2024. [17] Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level KV cache compression method with integrated retrieval and reasoning. In The Thirteenth International Conference on Learning Representations, 2025. [18] Google DeepMind. Gemini 2.5 Pro: Advanced Reasoning AI Model. https://deepmind. google/technologies/gemini/pro/, March 2025. [19] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. [21] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling, 2024. [22] Yunhai Hu, Zining Liu, Zhenyuan Dong, Tianfan Peng, Bradley McDanel, and Sai Qian Zhang. Speculative decoding and beyond: An in-depth survey of techniques. arXiv preprint arXiv:2502.19732, 2025. [23] Gautier Izacard and Édouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874880, 2021. [24] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [25] Huiqiang Jiang, YUCHENG LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [26] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [27] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16581677, 2024. [28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [29] Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: contextaware sparse attention mechanism for efficient long-sequence inference. In The Thirteenth International Conference on Learning Representations, 2025. [30] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [31] YUCHENG LI, BO DONG, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [32] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. SnapKV: LLM knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [33] Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. Prompt compression with context-aware sentence encoding for fast and improved llm inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2459524604, 2025. [34] Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, and Lili Qiu. Retrievalattention: Accelerating long-context llm inference via vector retrieval, 2024. [35] Fei Liu et al. Learning to summarize from human feedback. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 583592, 2020. [36] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. [37] Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, and Edoardo M. Ponti. The sparse frontier: Sparse attention trade-offs in transformer llms, 2025. [38] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Findings of the Association for Computational Linguistics ACL 2024, pages 963981, 2024. [39] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017. [40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [41] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, 2016. [42] Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, and Beidi Chen. Magicdec: Breaking the latencythroughput tradeoff for long context generation with speculative decoding. In The Thirteenth International Conference on Learning Representations, 2025. [43] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless In First acceleration of long sequence generation with hierarchical speculative decoding. Conference on Language Modeling, 2024. 12 [44] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. QUEST: Query-aware sparsity for efficient long-context LLM inference. In Forty-first International Conference on Machine Learning, 2024. [45] Gemma Team. Gemma 3. 2025. [46] Qwen Team. Qwen3, April 2025. [47] Rishabh Tiwari, Haocheng Xi, Aditya Tomar, Coleman Hooper, Sehoon Kim, Maxwell Horton, Mahyar Najibi, Michael W. Mahoney, Kurt Keutzer, and Amir Gholami. Quantspec: Selfspeculative decoding with hierarchical quantized kv cache, 2025. [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [49] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing, 2020. [50] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [51] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [52] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. [53] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. [54] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
        },
        {
            "title": "A Algorithms",
            "content": "Algorithm 1 SpecKV 1: Input: Input sequence with length nin Parameters: Number of lookahead tokens nlookahead, Maximum cache capacity Cmax, Compression window size nwindow, Kernel size k, Prefill window size nslash, Number of global tokens in prefill nvert 2: Generate draft output ydraft of length nlookahead using the draft model. 3: Forward and ydraft through the target model. 4: for each attention head in target model do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for Target model hidden states from prompt at current layer Target model hidden states from draft output at current layer nin nwindow CrossAttention (cid:0)Q from (cid:2) Xm: MaxReduce(A) AvgPool1D(s, k) ivert topk(s, nvert) islash {1, 2, . . . , nslash} icache topk(s, Cmax nwindow) {m + 1, + 2, . . . , nin} output VerticalSlash(X, ivert, islash) cache Kicache , Vicache (cid:3) , K/V from X:m (cid:1) Rnind Rnlookaheadd Compute attention Rn1n2 Rn2 Smooth attention Select global tokens (Sparse prefill) Sliding window (Sparse prefill) Select KVs Sparse prefill attention KV cache dropping Algorithm 2 SpecPC 1: Input: Draft attention tensor Rnlayernhead(nin+nlookahead1)nin Parameters: Window size nwindow, Kernel size k, Number of neighbors nneighbor, Number of selected tokens Cmax, Number of skipped layers lskip nwindow A...,j,: A...,j,: 2: nin nwindow 3: Alskip:,:,m:,:m 4: for {1, 2, . . . , nwindow} do 5: 6: end for 7: MaxReduce(A) 8: AvgPool1D(s, k) 9: MaxPool1D(s, nneighbor) 10: iselected topk(s, Cmax) {m + 1, + 2, . . . , nin} 11: return iselected Skip layers and only consider window queries and non-window keys Assign more weight to later tokens Rn1n2n3n4 Rn4 Smooth attention Keep neighbor tokens Keep most activated tokens and window tokens"
        },
        {
            "title": "B Deferred Proofs",
            "content": "Lemma 1. Softmax(x) Softmax(y) 2 . Proof. Let be the Jacobian matrix of Softmax. Then, J(v) = diag(p) 0 for all and Softmax(v). Note that is probability distribution, so pi ppT , where = pi = 1. For any (cid:80) vectors and z, J(v)z 2 2 = = (diag(p) (pizi 2 ppT )z 2 pipT z)2 p2 (zi pi(zi pT z)2 pT z)2 piz2 2pizipT + pi(pT z) (pT z)2 (cid:0) piz2 piz2 (cid:1) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) = = = pi 2 (cid:88) 2 . Thus, J(v)z 2 for all and z. From the fundamental theorem of line integrals, Softmax(x) Softmax(y) = J(y + t(x 1 0 (cid:90) y))(x y)dt. (4) Finally, Softmax(x) Softmax(y) 2 = = 1 J(y + t(x (cid:13) (cid:13) (cid:13) (cid:13) (cid:90) 0 (cid:90) 1 0 1 J(y + t(x 0 (cid:90) dt . y))(x y)dt 2 (cid:13) (cid:13) (cid:13) 2dt (cid:13) y))(x y) Lemma 2. Let = Softmax(x) and = Softmax(x). If such that p , where = mini(min(yi, i)) and ϵ c1 { 1, 2, . . . , ϵ, then there exists scalar . } (5) (6) Proof. From the mean value theorem, there exists ξ (yi, i) such that log yilog yiy = log dt = 1 ξ . yi 1 . (cid:12) t=ξ (cid:12) (cid:12) (cid:12) (cid:12) i Note that ξ > 0. Then, log yi log i = 1 ξ yi Let = log exj log ex , so (cid:80) exi exj (cid:80) log log (cid:12) (cid:12) (cid:12) (cid:12) (cid:80) ex x (cid:80) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) xi log 15 exj log ex (cid:88) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = xi (7) for all i. Thus, xi c yi 1 i = xi xi p i p c1 c1 x c1 = (cid:88) = = = x i 1 mp yi 1 mp yi i (cid:88) y 1 mp 1 ϵ . B.1 Proof of Theorem"
        },
        {
            "title": "We define the vector of importance scores as and its approximation as",
            "content": "nout sT = 1 nout Softmax x(o)T WqW d nout and ˆsT = 1 nout Softmax ˆx(o)T WqW T i=1 (cid:88) (cid:18) (cid:19) i=1 (cid:88) (cid:18) (8) Rd is the ith output where = [x1, . . . , xnin ]T embedding, and ˆx(o) Rd is the ith approximate output embedding (from the draft model). si and ˆsi denote the importance of the ith KV pair. In practice, SpecKV estimates importance using queries from recent input and draft output tokens. This is omitted from the theoretical analysis for clarity. Rnind is the matrix of input embeddings, x(o) , (cid:19) Proof. We assume ˆx(o) x(o) 2 x(o) ϵ, so x(o)T WqW xj (cid:12) (cid:12) (cid:12) (cid:12) Thus, ˆx(o) 2 ϵ for all and xj 2 for all j. ˆx(o)T WqW xj (cid:12) (cid:12) (cid:12) (cid:12) ˆx(o) )T WqW k xj = 1 = ϵ (x(o) WqW 1 (cid:12) (cid:12) (cid:12) WqW 2. 2ϵd (cid:12) (cid:12) (cid:12) Applying Lemma 1 and the triangle inequality, we get x(o)T WqW d ˆx(o)T WqW d WqW 2. ϵ (cid:13) (cid:13) (cid:13) (cid:13) 2 = ˆs Softmax Softmax x(o)T WqW d x(o)T WqW T (cid:18) (cid:18) (cid:19) (cid:19) 1 nout nout i=1 (cid:88) Softmax (cid:18) ˆx(o)T WqW d Softmax (cid:18) ˆx(o)T WqW d 2 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) x(o)T WqW T ˆx(o)T WqW d (cid:13) (cid:13) (cid:13) (cid:13) i=1 (cid:13) (cid:88) (cid:13) WqW (cid:13) (cid:13) ϵ 2, (cid:13) (cid:13) (cid:13) (cid:13) nout 1 nout (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:13) nout 1 nout i=1 (cid:88) nout i=1 (cid:13) (cid:88) (cid:13) nout (cid:13) (cid:13) B.2 Proof of Theorem = [x1, . . . , xnin ]T = [y1, . . . , ynin ]T = Softmax XWqW XWv (cid:16) (cid:17) 16 (9) (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (10) (11) ˆY = [ˆy1, . . . , ˆynin ]T = Softmax ˆWq ˆW T ˆWv (cid:16) = [a1, . . . , anin ]T = Softmax ˆA = [ˆa1, . . . , ˆanin ]T = Softmax (cid:17) XWqW T ˆWq ˆW T (cid:16) (cid:16) (cid:17) (cid:17) (12) (13) (14) ,2 is the maximum ℓ2 norm of the rows of Proof. We assume X. Additionally, we assume that there exists constant such that cX has the Restricted Isometry Property [8] with parameters 2k, δ, where is the approximate sparsity of ai and ˆai. ,2, where 2 ˆyi yi ϵ Recall that matrix satisfies the Restricted Isometry Property with constant δ k-sparse vector v, the following inequality holds: (0, 1) if for every Let Wv = Wv If nin = 1, then = ˆA ˆWv and ai = ai 2 = y1 xT 1 Wv ˆy1 2 ϵ x1 for all x1. (1 δ) 2 2 (1 + δ) 2 2. Bv 2 2 ˆai. (15) R11 with A1,1 = ˆA1,1 = 1, so AX = ˆAX = = xT 1 , which implies xT 1 WV ˆWv xT 1 ,2 = ϵ for all x1 is the definition of the matrix ℓ2 norm, so 2 = xT 1 Wv 2 ϵ x1 2 Wv (16) ϵ. 2 yi ˆyi 2 = = = = ˆWv ˆaT (ˆaT XWv (aT XWv XWv + ˆaT XWv ,2 aT XWv aT XWv aT XWv aT ϵ ,2 + 2. ˆaT XWv) aT XWv 2 ˆaT XWv) 2 2 ϵ 2 XWv aT Then, Since ˆaT is convex combination of the rows of X, aT ˆaT XWv ˆaT XWv ,2 + XWv Thus, 2 ϵ 2 ˆaT ϵ ,2 + ˆaT ,2. 2 Attention scores are approximately sparse [25], especially for long sequences. Therefore, we assume ai and ˆai are k-sparse. Then, ai is at most 2k-sparse. Since cX has the Restricted Isometry Property with parameters 2k, δ, 2 Wv 2ϵ ,2. (1 δ) ai 2 aT (cX) 2 (1 + δ) ai 2. Then, so B.3 Proof of Theorem 3 1 (1 ai δ) 2 aT 2ϵX,2 σmin(Wv) , 2 ai 2 2cϵX,2 σmin(Wv)(1δ) . (17) (18) (19) Theorem 3. If of the column space of Wv, then 2 ϵ ˆY 2 for all and the column space of Wq, Wk, ˆWq, ˆWk is subset ai 2 ˆai ϵδ, where σmin(Wv)2 , ˆWq2 ˆWk2 σmin( ˆWv)2 Wq2Wk2 2 ,2. (cid:17)(cid:17) (20) δ = 2d σmax(Wv)2 σmin(Wv) exp 2 max ˆY (cid:16) (cid:16) ϵ Proof. We assume Wq, Wk, ˆWq, ˆWk is subset of the column space of Wv. To get norm bound on ai = ai 2. Additionally, we assume that the column space of ˆai, 2 17 we will bound the norms of the error in approximate weight matrices. We will find these bounds by using specific inputs, taking advantage of the fact that ˆWv, by choosing an input that fixes and ˆA. If = 1, We will start by bounding Wv = Wv then = ˆA = [1], so AX = ˆAX = = xT 1 , which implies xT xT 1 WV 1 2 for all X. xT 1 Wv 2 = ϵ ˆAX ˆWv 2 = x1 ˆWv 2 = 2 2 ˆY ˆY ϵ ϵ 2 (21) 2 = AXWV for all x1. Thus, Wv ϵ. 2 Next, we will bound the norm of = . We will choose the so that the values are the identity matrix. Then = A. Let ΣV be the singular value decomposition of Wv. We set ˆB, where = WqW and ˆB = ˆWq ˆW = ΦV Σ1U , (22) where Φ is an arbitrary orthonormal basis spanning Rdd. Note that σmin(Φ) = σmax(Φ) = 1, so 2 = σmax(X) = σmin(Wv) and σmin(X) = 1 σmax(Wv) . Now, ˆY 2 = = = = ˆAXWv) AXWv 2 ˆAXWv) 2 2 2 + ˆAXWv ˆAX ˆWv 2 ( ˆAXWv (AXWv AXWV AXWV AXWV AXWv + ˆAXWv AXWv 2 + AΦV Σ1U ΣV ˆAXWv AΦ 2 + ˆAXWv 2 + 2 + ϵ ϵ 2ϵ σmin(Wv) . 2 2 2 2 ˆAXWv = = = Note that each row of ˆA is probability distribution (non-negative entries summing to 1), so leftmultiplying by ˆA forms convex combination of the rows of X. From Jensens inequality we get ˆAX 2 Let δ1 = max XBX satisfies 2, because σmin(Wv)2 , ˆWq2 ˆWk2 σmin( ˆWv)2 xT Bxj δ1 for all i, j. This implies that each attention weight 2 is convex function. δ1. Consequently, (cid:16) σmin(Wv) and Wq2Wk2 Wk . Since (cid:17) 2, 2 = Wq 2 2 2 1 The same argument applied to ˆa gives ai,j > eδ1 (cid:80)d j=1 eδ1 = 1 e2δ1. ˆai,j > eδ1 (cid:80)d j=1 eδ1 = 1 e2δ1. Applying Lemma 2 to each row, there exists Rd such that (23) (24) XBX ˆBX + c1T ˆBX + dc1T (cid:13) XBX (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Softmax ˆBX (cid:16) (cid:17) 2 (cid:17)(cid:13) (cid:13) (cid:13) (25) de2δ Softmax XBX (cid:13) d3/2e2δ1 (cid:13) (cid:13) 2ϵd3/2e2δ1 σmin(Wv) . (cid:16) 18 Minimizing over c, we obtain XBX min ˆBX + dc1T Substituting in the definition of X, we get = = 2 = min XBX XBX XBX (cid:13) (cid:13) (cid:13) (cid:13) (cid:0) dc1T 2 XBX 11T 1 1 11T (cid:1)(cid:13) (cid:13) ΦV Σ1U BU Σ1V ΦT (I d111T ) 2 ϵd3/2e2δ1 σmin(Wv) . 2 (cid:13) (cid:13) (26) (27) Each multiplication by Σ1 can decrease the norm by at most instances of Σ1 we scale the bound by σmax(Wv)2, giving us 1 σmax(Wv) , so when removing both ΦV BU ΦT (I d111T ) 2ϵd3/2e2δ1 σmin(Wv) σmax(Wv)2. (28) Then, since Φ and are orthonormal and preserve spectral norm under multiplication, we conclude BU ΦT (I d111T ) 2 2ϵd3/2e2δ1 σmin(Wv) σmax(Wv)2. (29) Finally, since the column space of Wq, Wk, ˆWq, ˆWk is subset of the column space of Wv, the column space of is subset of the column space of . Thus, left multiplication by does not impact the spectral norm, so BU ΦT (I d111T ) 2 2ϵd3/2e2δ1 σmin(Wv) σmax(Wv)2. (30) Note that the matrix singular values are [1, . . . , 1, 0], so its spectral norm is 1 11T is projection onto the subspace orthogonal to the all-ones vector. Its Moreover, since any fixed subspace orthogonal to 1. In this case, 2 = 1 and Φ is an arbitrary orthonormal basis of Rd, it follows that for Rdd, we can choose Φ such that the largest component of ΦT lies entirely in the (cid:13) (cid:13) (cid:13) (cid:13) 11T 1 1 11T (cid:13) (cid:13) 2 = 1. (cid:13) (cid:13) (31) (cid:13) δ2 where δ2 = 2ϵd3/2 σmax(Wv)2 (cid:13) ΦT (I 1 11T ) 2 = (cid:13) σmin(Wv) e2δ1. (cid:13) 2. Thus, 2 Now that we have bounded so BX xT From Lemma 1, δ2 2 ,2. 2, we will consider any input X. Then, ,2 is the maximum ℓ2 norm of the rows of X. xT Bxj (32) δ2 2 ,2, ai ˆai 2 = Softmax Softmax xT ˆBX 2 BX xT ˆBX xT BX xT (cid:16) (cid:13) (cid:13) (cid:13) = 2ϵd σmax(Wv)2 σmin(Wv) exp = 1 (cid:17) (cid:13) (cid:13) (cid:13) . (cid:13) (cid:13) δ2X2 ,2 (cid:16) BX xT (cid:17) (cid:13) (cid:13) σmin(Wv)2 , ˆWq2 ˆWk2 σmin( ˆWv)2 Wq2Wk2 ai ˆai 2 δ2X2 ,2 X 2 ,2. (33) (cid:17)(cid:17) 2 max (cid:16) (cid:16)"
        },
        {
            "title": "C Datasets",
            "content": "C.1 LongBench Task Dataset Source Avg. Words Metric Language Size Table 5: LongBench tasks. 1-1 1-2 1-3 1-4 2-1 2-2 2-3 2-4 3-1 3-2 3-3 3-4 4-1 4-2 4-3 45-1 5-2 5-3 6-1 6-2 NarrativeQA Qasper MultiFieldQA-en MultiFieldQA-zh HotpotQA 2WikiMultihopQA MuSiQue DuReader GovReport QMSum MultiNews VCSUM TREC TriviaQA SAMSum LSHT Single-Document QA Literature, Film Science Multi-field Multi-field 18,409 3,619 4,559 6,701 Multi-Document QA Wikipedia Wikipedia Wikipedia Baidu Search 9,151 4,887 11,214 15,768 Summarization Government report Meeting News Meeting 8,734 10,614 2,113 15,380 Few-shot Learning F1 F1 F1 F1 F1 F1 F1 Rouge-L Rouge-L Rouge-L Rouge-L Rouge-L Web question Wikipedia, Web Dialogue News 5,177 8,209 6,258 22,337 Synthetic Task 11,141 9,289 6,745 Accuracy (CLS) F1 Rouge-L Accuracy (CLS) Accuracy (EM) Accuracy (EM) Accuracy (EM) English English English Chinese English English English Chinese English English English Chinese English English English Chinese English English Chinese PassageCount PassageRetrieval-en PassageRetrieval-zh Wikipedia Wikipedia C4 Dataset LCC RepoBench-P Github Github repository 1,235 4,206 Edit Sim Edit Sim Python/C#/Java Python/Java Code Completion 200 200 150 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 500 500 LongBench6 [4] is benchmark suite designed for long-context evaluation, comprising 14 English tasks, five Chinese tasks, and two code tasks. As Llama does not support Chinese, we excluded the corresponding tasks. Furthermore, we removed the synthetic tasks, as these are already covered by the RULER benchmark. The remaining tasks are grouped into five categories: single-document question answering, multi-document question answering, summarization, few-shot learning, and code completion. For each category, the overall score is calculated as the average of all its subtasks. The final LongBench score is computed as the average across all included tasks. Table 5 provides an overview of all tasks, adapted from [4]. C.2 RULER RULER7 [21] is synthetic dataset designed to evaluate the true supported context length of LLMs. It comprises 13 tasks, including eight needle-in-a-haystack (NIAH) retrieval tasks, two aggregation tasks, two question answering (QA) tasks, and one multi-hop tracing task. The NIAH tasks involve hiding random key-value pairs within generated text and challenging the model to retrieve them. Aggregation tasks simulate summarization by asking the model to extract the most frequent or common words from given passage. The QA tasks require the model to answer question about randomly selected paragraph within the context, serving as real-world analog to NIAH tasks. In the multi-hop tracing task, the model must identify all variable names that reference the same value within chain of assignments. RULER is generated for range of sequence lengths using randomly generated texts drawn from Paul Graham essays, SQuAD [41], and HotPotQA [53] datasets. This approach enables comprehensive assessment of language models capability to process varying context lengths. Evaluation is 6https://huggingface.co/datasets/THUDM/LongBench (MIT License) 7https://github.com/NVIDIA/RULER (Apache License 2.0) conducted based on accuracy, considering response correct if it contains the requested value associated with the specified key. C.3 MileBench Table 6: Overview of the MileBench datasets. Average tokens are computed using Qwen2.5-VL [3]. Category Dataset Avg. Words Avg. Images Avg. Tokens Metric Size Temporal Semantic EgocentricNavigation MovingDirection SceneTransition SlideVQA TQA WebQA 85 62 66 66 50 45 5 20 2 8 2 3,079 1,042 5,125 2,053 5,536 1,706 Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy 200 200 200 200 200 200 MileBench8 [14] is long-context benchmark designed to evaluate Multimodal Large Language Models (MLLMs). It comprises 29 multi-image-text datasets, organized into 12 tasks, which are further grouped into four categories: Temporal Multi-Image, Semantic Multi-Image, and two diagnostic categoriesNIAH and Image Retrieval. For our additional experiments in Appendix E.4, we selected three datasets each from the Temporal Multi-Image and Semantic Multi-Image categories: EgocentricNavigation, MovingDirection, and SceneTransition for the Temporal Multi-Image category, and SlideVQA, TQA, and WebQA for the Semantic Multi-Image category. Table 6 provides an overview of the selected datasets."
        },
        {
            "title": "D Experimental Details",
            "content": "D.1 Hyperparameter Settings Table 7: Prompt compression backbones and parameter counts. Method Backbone Parameters LLMLingua-2 [38] CPC [33] R2C [11] xlm-roberta-large [12] Llama-3.2-1B [19] T5-base [40] 560M 1B 220M Table 8: Summary of hyperparameters for various methods. Hyperparameter Window size nwindow Pool Kernel size Reduction # lookahead tokens nlookahead Compression window size nslash # global tokens in prefill nvert # neighbors nneighbor # skipped layers lskip StreamingLLM H2O SnapKV PyramidKV Ada-SnapKV SpecKV SpecPC 32 32 32 Max 7 Mean 32 Max 7 Mean 32 Max 7 Mean 32 Max 7 Max All 2048 2048 64 Max 64/32 Max 1 64/32 Table 7 lists the backbone models employed by each prompt compression method, while Table 8 details the hyperparameters used in our experiments. Generally, we select hyperparameters for each method based on their respective codebases. We observe that using max aggregation improved performance compared to mean aggregation for SpecKV and SpecPC. For SpecKV, setting nslash and nvert to 2048 resulted in minimal accuracy loss but substantially reduced latency (Appendix E.5.3). For SpecKV, we always generate tokens until the draft model produces the EOS token, which yields the best performance. For latency measurements, we set nlookahead = 64 tokens, reflecting the average sequence length in our benchmarks. In SpecPC, prompt compression drops tokens uniformly across 8https://milebench.github.io (Apache License 2.0) 21 all layers and heads (unlike SpecKV, which prunes per head), so larger Cmax is needed to retain relevant information. While larger nlookahead can boost performance, in practice, generating only one token per prompt (nlookahead = 1) is usually sufficient. Strong alignment between the draft and target model attentions enables SpecPC to outperform methods like R2C and CPC. For an ablation on nlookahead, see Fig. 12. Retaining the local context for prompt compression proved essential. This observation aligns with the design of existing prompt compression methods, which typically aim to preserve entire sentences within the prompt. Consequently, we increase both the pooling kernel size (k) and the number of neighboring tokens (nneighbor) to 64. For Llama, slightly better results are achieved by reducing both and nneighbor to 32, though the performance difference was marginal. For all remaining methods not explicitly mentioned, we use the default configurations provided in their respective codebases. D.2 Setup and Environment For our main experimental results, we employ the following large language models: Llama-3.2-1BInstruct9, Llama-3.1-8B-Instruct10, Qwen2.5-0.5B-Instruct11, and Qwen2.5-14B-Instruct12. For MLLM evaluation on MileBench [14], we utilize Qwen2.5-VL-3B-Instruct-AWQ13 and Qwen2.5VL-32B-Instruct-AWQ14. Our implementation is based on PYTORCH [39] (BSD-3 License) and Huggingfaces TRANSFORMERS [49] (Apache License 2.0). All experiments leverage FLASHATTENTION-215 [13]. Latency measurements are performed using VLLM16 wherever possible (i.e., where attention map outputs are not required). For implementing the sparse prefill mechanism of SpecKV, we use kernels from MINFERENCE17. All methods are evaluated via greedy decoding. Experiments are conducted on NVIDIA H100 80GB GPUs, with runtimes varying by context length; for maximum context length of 64K tokens, experiments take up to 2 hours. For evaluating STREAMINGLLM[50], H2O[54], SNAPKV[32], and PYRAMIDKV[7], we use implementations from KVCACHE-FACTORY18. In this library, the STREAMINGLLM and H2O implementations drop KV once after prefill, rather than at each decoding step, differing from their original codebases. This adjustment enables fairer comparison to SNAPKV and others. We extend KVCACHE-FACTORY to support Grouped Query Attention [2] by repeating keys and values for each KV head, computing attention within the window, and averaging across KV heads. This approach avoids duplicating the KV cache. For other baselines, we use their official implementations."
        },
        {
            "title": "E Additional Experimental Results",
            "content": "E.1 Peak Memory Usage Fig. 7 compares the peak memory usage of KV dropping and prompt compression methods for Qwen2.5 (0.5B draft, 14B target). SpecKV consistently consumes more memory than SnapKV, primarily due to the need to load the draft model weights into memory. However, this additional overhead is constant and generally acceptable in practice. If further memory savings are required, it is possible to offload the draft model weights to the CPU. For prompt compression, we compare the peak memory usage of the draft or auxiliary models, as the target models peak memory usage remains the same for given Cmax. The results indicate that 9https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct (Llama 3.2 license) 10https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct (Llama 3.1 license) 11https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct (Apache License 2.0) 12https://huggingface.co/Qwen/Qwen2.5-14B-Instruct (Apache License 2.0) 13https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ (Apache License 2.0) 14https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct-AWQ (Apache License 2.0) 15https://github.com/Dao-AILab/flash-attention (BSD 3-Clause License) 16https://github.com/vllm-project/vllm (Apache License 2.0) 17https://github.com/microsoft/MInference (MIT License) 18https://github.com/Zefan-Cai/KVCache-Factory (MIT License) 22 (a) KV dropping (Cmax = 256), target model (b) Prompt compression (Cmax = 1024), draft model Figure 7: Peak memory usage. SpecPC requires substantially less memory than R2C, the second-best performing method in our experiments, highlighting the efficiency advantage of our approach. For the remaining methods, SpecPC exhibits similar peak memory consumption. E.2 Additional Attention Correlation Experiments We present additional attention correlation plots in Fig. 8 for Llama-3 [19], Qwen2.5 [51], Qwen3 [46], and Gemma-3 [45]. Across most model pairs, we observe strong correlations in attention patterns, especially when using mean aggregation for the attention activations. E.3 Additional RULER and LongBench Results Fig. 9 (RULER), Tables 9 and 10 (LongBench) present the performance of our proposed methods, SpecKV and SpecPC, alongside several baselines for additional values of Cmax. While increasing the budget Cmax improves performance across all methods and narrows the gap between our approaches and the baselines, our proposed methods consistently outperform the alternatives. E.4 MileBench (Multi-modal) Results We conduct additional experiments using Qwen2.5-VL-3B-Instruct-AWQ (draft) and Qwen2.5VL-32B-Instruct-AWQ (target) on six MileBench [14] datasets. We select three datasets each from the Temporal Multi-Image (EgocentricNavigation, MovingDirection, SceneTransition) and Semantic Multi-Image (SlideVQA, TQA, WebQA) categories. We focus on these datasets because, for Qwen2.5-VL, the performance gap between draft and target models is most significant; in other cases, the models perform too similarly or the draft even outperforms the target. For KV dropping, we evaluate H20, SnapKV, and PyramidKV from our prior experiments. We do not include AdaKV in our evaluation as it is dependent on an older Transformers [49] version incompatible with Qwen2.5-VL. For prompt compression, we compare with FastV [10]a method specialized for dropping image tokens inside LLMsas techniques such as C2C and R2C do not support image inputs. FastV uses hyperparameter k: it runs all tokens up to layer k, then drops less-attended image tokens based on the attention map, processing only the top tokens thereafter. This makes FastV less efficient than SpecPC, since all tokens must be processed up to with the full model, requiring considerable memory. Notably, FastV must compute the entire attention map at layer k, preventing the use of FlashAttention and leading to out-of-memory errors, even for moderate sequence lengths. As result, many MileBench datasets exceed 80GB VRAM, so we limit our analysis to these six datasets. Since the selected MileBench datasets have relatively short average context lengths, we conduct experiments using reduced Cmax values for both KV cache dropping (64, 96, and 128) and prompt compression (512, 768, and 1024). Fig. 10a presents results for various KV dropping methods. Our proposed method, SpecKV, demonstrates performance comparable to existing approaches, while significantly outperforming the others on the WebQA task. 23 Fig. 10b compares the performance of SpecPC and FastV under two configurations (k = 2 and = 5). Our method consistently outperforms FastV in most cases. E.5 Ablation Studies In this section, we conduct series of ablation experiments to further analyze the effectiveness of our two proposed methods: SpecKV and SpecPC. For consistency, we fix Cmax to 256 for KV dropping and 1024 for prompt compression across all experiments. We utilize Qwen2.5 (Instruct), employing the 0.5B model as the draft and the 14B model as the target. We sample 100 random examples per task from the LongBench and RULER benchmarks. E.5.1 SpecKV: Leveraging Enhanced Draft Models Fig. 11 illustrates the impact of using draft models of different sizes and versions on the RULER score. As anticipated, both newer [46] and larger draft models lead to improved performance of SpecKV. E.5.2 SpecKV and SpecPC: Number of Generated Draft Tokens Fig. 12 illustrates how varying the number of generated draft tokens, nlookahead, affects the performance of SpecKV and SpecPC. Overall, increasing nlookahead generally results in higher final accuracy for SpecKV, whereas it yields only marginal improvements for SpecPC. We attribute this to the larger Cmax budget of SpecPC, which allows it to capture all important tokens without needing to generate long drafts. E.5.3 SpecKV: Accuracy Impact of Sparse Prefill In this section, we experimentally evaluate how the sparsity of SpecKVs prefill procedure affects downstream task performance  (Fig. 13)  . Specifically, we set nvert equal to nslash and compare several values for these parameters. As anticipated, reducing sparsity (i.e., using higher nvert) generally results in higher accuracy; however, accuracy improvements plateau at nvert = 2048, which we therefore adopt for our main experiments. Interestingly, for certain LongBench categories, increased sparsity (i.e., lower nvert) can actually lead to improved performance. This counterintuitive result suggests that, for some tasks, sparser prefill may serve as form of regularization, preventing overfitting to irrelevant context. E.6 Additional Models In this section, we present results for RULER and LongBench using larger models. Specifically, we employ Qwen2.5-32B-Instruct, Qwen2.5-72B-Instruct-GPTQ-Int4, and Llama-3.1-70B-Instruct (4-bit quantized with bitsandbytes19) as target model. To explore the impact of draft model size in SpecKV, we extend our experiments to larger draftsQwen2.5-1.5B-Instruct, Qwen2.5-3B-Instruct, and Llama-3.2-3B-Instructin addition to the previously used Qwen2.5-0.5B-Instruct and Llama3.2-1B-Instruct. This enables us to systematically examine how increasing the draft size improves performance. For SpecPC, we find Qwen2.5-0.5B-Instruct and Llama-3.2-1B-Instruct already deliver sufficient performance. For each RULER and LongBench task, we randomly select 50 samples and compare SpecKV to SnapKV, as well as SpecPC to CPC and R2Cthe best-performing methods in our previous experiments. For Cmax, we use values of 256, 512, and 1024 for KV dropping, and 1024, 2048, and 3072 for prompt compression. Figs. 14 and 15 show the RULER results, and Tables 11 to 13 present the LongBench results. Overall, our method achieves higher accuracy than the baselines in most settings, especially with small Cmax. Specifically, SpecKV significantly outperforms SnapKV on RULER, with performance further improved by utilizing larger draft model. Similarly, SpecPC consistently achieves strong results, particularly at longer sequence lengths on RULER. On LongBench, both of our methods also surpass the baselines. 19https://github.com/bitsandbytes-foundation/bitsandbytes (MIT License) 24 (a) Llama-3 (1B / 8B) (b) Llama-3 (1B / 70B) (c) Qwen2.5 (0.5B / 14B) (d) Qwen2.5 (0.5B / 72B) (e) Qwen3 (0.6B / 14B) (f) Qwen3 (0.6B / 32B) (g) Gemma-3 (1B / 12B) (h) Gemma-3 (1B / 27B) Figure 8: Additional attention correlation plots for mean and max aggregation across different model combinations. Blue dots indicate attention activation pairs for draft and target; the black line represents the best linear fit. We observe strong correlations in most cases, especially with mean aggregationeven between models with large size gaps, such as Qwen2.5 (0.5B/72B). Correlation is notably weaker for Qwen2.5 (0.5B/14B), yet our methods still perform robustly on this model. 25 (a) KV dropping (Cmax = 256) (b) Prompt compression (Cmax = 1024) (c) KV dropping (Cmax = 512) (d) Prompt compression (Cmax = 2048) (e) KV dropping (Cmax = 1024) (f) Prompt compression (Cmax = 3072) Figure 9: Performance of the proposed SpecKV and SpecPC on RULER compared to various baselines (full results), using Qwen2.5-0.5B-Instruct and Llama-3.2-1B-Instruct as draft models, and Qwen2.5-14B-Instruct and Llama-3.1-8B-Instruct as target models. Our approaches consistently outperform all baselines across every sequence length. Notably, SpecPC performs nearly on par with the target model. Low-performing methods are omitted. 26 Table 9: Full results for LongBench performance with Qwen2.5-0.5B (draft) and Qwen2.5-14B (target). All 30.64 47.33 33.24 38.41 40.25 38.76 41.41 44.09 45.61 35.41 40.37 43.10 42.19 44.18 46.09 48.46 37.54 42.75 45.73 44.43 45.94 46.61 47.78 28.79 37.17 37.15 43. 34.40 41.92 43.27 46.76 38.67 43.83 45.31 47.14 Cmax Method Singledoc QA Multidoc QA Summary Few-shot Learning Code Completion Dense Draft Target 21.04 53. 256 KV 512 1024 1024 PC 2048 3072 StreamingLLM 38.66 46.98 H2O 49.07 SnapKV 47.07 PyramidKV 50.92 Ada-SnapKV 51.07 SpecKV 51.92 Ada-SpecKV StreamingLLM 38.97 47.56 H2O 50.74 SnapKV 50.19 PyramidKV 51.69 Ada-SnapKV 51.98 SpecKV 51.00 Ada-SpecKV StreamingLLM 40.08 49.73 H2O 51.56 SnapKV 51.42 PyramidKV 51.81 Ada-SnapKV 52.92 SpecKV 52.27 Ada-SpecKV LLMLingua-2 CPC R2C SpecPC LLMLingua-2 CPC R2C SpecPC LLMLingua-2 CPC R2C SpecPC 27.10 42.76 46.43 47.70 35.10 48.74 50.68 53.23 43.50 51.59 51.71 53.25 21.23 24. 19.10 19.82 19.49 18.32 19.63 23.20 24.90 21.14 21.02 20.97 20.25 21.48 24.11 25.74 22.12 22.38 22.65 21.76 22.83 24.30 26.39 22.57 22.58 22.64 23.30 23.51 23.39 23.47 23.68 24.10 23.84 23.89 24. 54.07 65.34 49.75 50.88 54.49 54.81 55.07 59.68 61.05 53.78 53.45 57.81 58.17 59.84 62.60 63.74 55.42 55.94 63.90 59.96 64.14 63.46 64.83 35.85 48.38 47.21 59.74 39.82 55.77 56.72 63. 45.66 58.14 62.09 64.51 33.39 51.75 35.23 47.86 47.34 43.26 49.58 49.58 53.43 36.52 49.78 49.94 47.88 51.85 52.33 55.28 38.36 51.26 51.48 49.92 51.97 52.50 54.40 37.62 35.57 29.48 52. 44.80 42.07 45.62 54.92 47.32 44.82 48.58 54.41 24.4 42.83 24.13 29.66 33.19 31.84 34.57 38.76 39.38 27.04 33.16 38.33 36.37 38.61 41.51 42.18 32.01 37.27 40.98 40.93 40.96 41.85 43. 23.74 36.05 37.42 38.30 32.22 39.70 40.65 41.43 35.67 41.08 41.39 41.48 27 Table 10: Full results for LongBench performance with Llama-3.2-1B (draft) and Llama-3.1-8B (target). All 34.69 46.84 34.58 40.82 40.91 40.54 42.38 43.36 43.25 36.33 42.79 43.83 43.51 44.30 44.59 44.56 38.54 44.13 45.22 45.11 45.43 45.30 45.70 27.68 34.42 37.58 43.76 31.64 37.80 41.97 44. 35.05 39.30 44.14 45.65 Cmax Method Singledoc QA Multidoc QA Summary Few-shot Learning Code Completion Dense Draft Target 28.08 45.85 KV 512 1024 1024 PC 3072 StreamingLLM 38.69 43.54 H2O 43.79 SnapKV 43.62 PyramidKV Ada-SnapKV 44.04 43.23 SpecKV 42.40 Ada-SpecKV StreamingLLM 38.80 44.82 H2O 45.00 SnapKV PyramidKV 45.26 45.09 Ada-SnapKV 43.48 SpecKV 43.80 Ada-SpecKV StreamingLLM 38.45 45.45 H2O 45.46 SnapKV PyramidKV 46.10 46.06 Ada-SnapKV 43.73 SpecKV 44.83 Ada-SpecKV LLMLingua-2 CPC R2C SpecPC LLMLingua-2 CPC R2C SpecPC LLMLingua-2 CPC R2C SpecPC 29.61 35.67 38.41 44.83 34.00 40.02 44.53 44.92 39.13 41.73 44.77 47.12 25.65 28.68 21.64 22.62 21.96 21.75 22.34 24.43 25. 24.16 23.95 23.61 23.36 24.19 26.17 26.59 25.03 25.42 25.42 25.17 25.48 26.95 27.50 23.43 25.26 25.28 25.85 24.90 26.83 26.63 27.30 25.98 27.27 27.35 28.02 60.16 66. 50.75 55.64 56.29 55.15 59.95 60.90 57.94 52.59 58.70 61.36 61.15 61.81 62.34 58.72 58.09 59.17 62.06 63.43 63.47 63.14 61.04 24.18 34.01 43.26 63.70 24.76 39.02 54.62 64.77 29.73 42.12 60.85 65. 31.11 50.46 34.74 47.81 47.34 46.32 50.39 51.09 52.51 37.25 49.30 49.92 48.06 51.74 51.36 54.41 38.60 50.10 52.37 49.55 50.50 51.28 54.23 40.66 43.58 43.99 44.82 47.27 46.66 46.67 46. 49.92 49.13 48.08 45.52 27.27 43.79 27.12 36.81 37.31 37.79 37.86 39.73 40.73 29.15 39.36 41.31 41.22 41.17 41.86 42.56 32.54 42.50 43.15 42.78 43.34 43.39 43.72 24.83 36.61 39.07 39. 32.51 39.41 38.97 40.71 35.44 39.52 40.97 41.95 28 (a) KV dropping (b) Prompt compression Figure 10: MileBench multi-modal results using Qwen2.5-VL-3B-Instruct-AWQ (draft) and Qwen2.5VL-32B-Instruct-AWQ (target). SpecKV demonstrates competitive performance across most tasks and achieves substantial improvement on WebQA. SpecPC consistently outperforms both FastV configurations on the majority of datasets. 29 Figure 11: Effect of draft model quality on RULER score. The target model is Qwen2.5-14B (Instruct). Consistent with expectations, employing more capable draft models boosts performance. For reference, we also evaluate an oracle setting where the draft model is identical to the target (Qwen2.5-14B), representing an empirical upper bound for SpecKV. (a) SpecKV on RULER (b) SpecKV on LongBench (c) SpecPC on RULER (d) SpecPC on LongBench Figure 12: Impact of the number of generated draft tokens, nlookahead, on the relative performance of SpecKV and SpecPC, using Qwen2.5-0.5B-Instruct as the draft model and Qwen2.5-14B-Instruct as the target model. The relative score is calculated as the score of each SpecKV configuration divided by the score of the full dense target model. Increasing nlookahead substantially boosts SpecKVs score, whereas SpecPC shows only minor improvement with higher nlookahead. 30 (a) RULER (b) LongBench Figure 13: Impact of different sparsity levels in SpecKVs sparse prefill on relative performance, using Qwen2.5-0.5B-Instruct as the draft model and Qwen2.5-14B-Instruct as the target model. We vary nvert (set equal to nslash) and observe the impact on accuracy. Accuracy improves as sparsity decreases (higher nvert) up to 2048, beyond which gains saturate, hence our choice of 2048 for main corresponds to fully dense prefill. Notably, for some LongBench tasks, higher sparsity results. actually benefits accuracy. The relative score is calculated as the score of each SpecKV configuration divided by the score of the full dense target model. 31 (a) Qwen2.5: Draft models (0.5B, 1.5B), target model (32B) (b) Qwen2.5: Draft models (0.5B, 3B), target model (72B, 4-bit) (c) Llama-3: Draft models (3.2-1B, 3.2-3B), target model (3.1-70B, 4-bit) Figure 14: RULER results of additional larger models on KV cache dropping. Our proposed SpecKV method consistently outperforms SnapKV across the majority of evaluated settings, often by substantial margin. Utilizing larger draft model leads to further performance improvements. (a) Qwen2.5: Draft model (0.5B), target model (32B) (b) Qwen2.5: Draft model (0.5B), target model (72B, 4-bit) (c) Llama-3: Draft model (3.2-1B), target model (3.1-70B, 4-bit) Figure 15: RULER results of additional larger models on prompt compression. Our proposed SpecPC method consistently outperforms CPC and R2C, maintaining strong performance even on long sequences. 33 Table 11: LongBench results for Qwen2.5, featuring 0.5B and 1.5B draft models and 32B target model. Cmax Method Singledoc QA Multidoc QA Summary Few-shot Learning Code Completion Dense 256 KV 512 1024 PC 2048 3072 Draft (0.5B) Draft (1.5B) Target (32B) SnapKV SpecKV (0.5B) SpecKV (1.5B) SnapKV SpecKV (0.5B) SpecKV (1.5B) SnapKV SpecKV (0.5B) SpecKV (1.5B) CPC R2C SpecPC (0.5B) CPC R2C SpecPC (0.5B) CPC R2C SpecPC (0.5B) 19.07 36.16 56. 52.54 52.45 53.48 55.24 53.70 52.78 55.32 57.56 55.72 45.60 50.49 51.23 51.01 50.32 55.40 56.80 51.67 56. 26.58 35.01 43.99 40.21 42.12 43.77 42.21 42.70 43.97 44.04 43.67 43.95 40.62 40.37 41.40 42.31 42.66 42. 42.05 42.88 42.42 20.90 22.79 25.90 19.89 23.10 24.02 21.47 24.16 24.80 23.08 24.62 25.20 23.09 23.26 23. 23.74 24.08 24.12 24.77 24.09 24.71 53.51 63.92 64.06 61.18 63.55 63.79 63.36 63.68 64.72 66.15 65.68 63. 60.08 53.45 62.26 60.92 59.11 61.46 62.79 62.45 62.66 32.48 36.62 44.74 40.12 45.72 44.80 39.69 43.14 46. 44.42 42.25 42.38 32.31 34.11 38.23 35.83 40.54 48.05 37.98 27.01 47.49 All 30.37 39.06 47. 42.98 45.36 46.06 44.73 45.64 46.60 46.76 47.08 46.38 40.91 39.88 43.66 43.26 44.19 46.20 45.37 42.66 46. Table 12: LongBench results for Qwen2.5, featuring 0.5B and 3B draft models and 72B (4-bit) target model. Cmax Method Singledoc QA Multidoc QA Summary Few-shot Learning Code Completion Dense 256 KV 1024 1024 PC 2048 3072 Draft (0.5B) Draft (3B) Target (72B) SnapKV SpecKV (0.5B) SpecKV (3B) SnapKV SpecKV (0.5B) SpecKV (3B) SnapKV SpecKV (0.5B) SpecKV (3B) CPC R2C SpecPC (0.5B) CPC R2C SpecPC (0.5B) CPC R2C SpecPC (0.5B) 19.07 39.26 58.70 55.85 54.11 53.05 59.08 54.00 56.99 59.56 56.58 58.14 46.37 55.78 48.52 55.14 54.61 58. 55.78 59.22 59.84 20.90 25.34 26.24 21.65 24.28 24.94 23.04 25.26 25.83 24.41 25.45 26.36 24.19 24.40 24. 24.71 24.99 25.32 25.47 25.69 25.91 53.51 63.43 64.83 55.82 64.72 63.09 61.33 64.40 61.66 62.85 61.95 61. 52.54 47.62 61.04 49.46 53.65 64.43 42.71 55.22 64.18 32.48 45.31 51.08 50.41 47.82 48.48 51.77 47.39 48. 52.87 49.59 50.53 28.30 33.17 48.30 33.21 42.43 51.11 39.16 44.04 46.88 26.58 40.06 45.89 40.12 42.13 44. 44.13 45.08 45.31 44.06 44.70 44.69 38.36 40.10 45.34 43.53 45.70 46.38 44.41 44.70 44.95 All 30.37 42.49 49.22 44.37 46.53 46.69 47.59 47.22 47.59 48.46 47.52 48.15 38.64 40.72 45. 41.78 44.41 48.98 41.68 45.90 48.46 Table 13: LongBench results for Llama-3, featuring 3.2-1B and 3.2-3B draft models and 3.1-70B (4-bit) target model. Cmax Method Singledoc QA Multidoc QA Summary Few-shot Learning Code Completion Dense KV 512 1024 1024 PC 3072 Draft (1B) Draft (3B) Target (70B) SnapKV SpecKV (1B) SpecKV (3B) SnapKV SpecKV (1B) SpecKV (3B) SnapKV SpecKV (1B) SpecKV (3B) CPC R2C SpecPC (1B) CPC R2C SpecPC (1B) CPC R2C SpecPC (1B) 28.37 45.53 55.02 55.88 52.03 51.80 56.08 52.06 53.31 54.94 51.32 53. 45.14 48.93 56.84 55.97 53.62 59.39 56.64 56.11 58.47 28.62 40.52 47.06 45.30 46.51 47.23 46.94 47.78 47. 47.40 48.48 46.02 39.41 42.01 44.48 46.00 46.70 46.25 46.69 45.15 48.07 26.12 27.46 28.61 22.49 25.35 25. 24.40 26.73 27.33 25.97 27.31 27.41 24.86 25.38 25.91 26.72 26.27 27.60 27.29 27.51 27.72 59.10 64.82 70. 62.15 62.00 64.02 63.34 65.40 67.49 67.56 67.12 66.40 61.40 58.91 67.37 64.78 62.41 68.42 64.92 61.76 65. 33.59 46.73 48.19 55.49 56.59 58.75 52.14 54.77 54.19 48.13 50.69 50.66 37.58 40.19 47.15 41.31 47.90 46. 44.75 47.17 41.28 All 35.27 44.89 49.99 47.75 47.91 48.80 48.32 48.96 49.66 48.85 48.86 48. 41.97 43.29 48.44 47.36 47.34 49.88 48.30 47.57 48."
        }
    ],
    "affiliations": [
        "Ajou University",
        "FuriosaAI",
        "Seoul National University",
        "UW-Madison"
    ]
}