{
    "paper_title": "$^R$FLAV: Rolling Flow matching for infinite Audio Video generation",
    "authors": [
        "Alex Ergasti",
        "Giuseppe Gabriele Tarollo",
        "Filippo Botti",
        "Tomaso Fontanini",
        "Claudio Ferrari",
        "Massimo Bertozzi",
        "Andrea Prati"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Joint audio-video (AV) generation is still a significant challenge in generative AI, primarily due to three critical requirements: quality of the generated samples, seamless multimodal synchronization and temporal coherence, with audio tracks that match the visual data and vice versa, and limitless video duration. In this paper, we present \\arch{}, a novel transformer-based architecture that addresses all the key challenges of AV generation. We explore three distinct cross modality interaction modules, with our lightweight temporal fusion module emerging as the most effective and computationally efficient approach for aligning audio and visual modalities. Our experimental results demonstrate that \\arch{} outperforms existing state-of-the-art models in multimodal AV generation tasks. Our code and checkpoints are available at https://github.com/ErgastiAlex/R-FLAV."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 0 3 8 0 . 3 0 5 2 : r RFLAV: Rolling Flow matching for infinite Audio Video generation Alex Ergasti1 Giuseppe Tarollo1 Filippo Botti1 Tomaso Fontanini1 Claudio Ferrari1, Massimo Bertozzi1 Andrea Prati1 1 University of Parma, Department of Engineering and Architecture. Parma, Italy 2 Univerisity of Siena, Department of Information engineering and mathematics. Siena, Italy {alex.ergasti, giuseppegabriele.tarollo, filippo botti, tomaso.fontanini}@unipr.it claudio.ferrari@unisi.it, {massimo.bertozzi, andrea.prati}@unipr.it"
        },
        {
            "title": "Abstract",
            "content": "Joint audio-video (AV) generation is still significant challenge in generative AI, primarily due to three critical requirements: quality of the generated samples, seamless multimodal synchronization and temporal coherence, with audio tracks that match the visual data and vice versa, and limitless video duration. In this paper, we present RFLAV, novel transformer-based architecture that addresses all the key challenges of AV generation. We explore three distinct cross modality interaction modules, with our lightweight temporal fusion module emerging as the most effective and computationally efficient approach for aligning audio and visual modalities. Our experimental results demonstrate that RFLAV outperforms existing state-of-theart models in multimodal AV generation tasks. Our code and checkpoints are available at https://github. com/ErgastiAlex/R-FLAV . 1. Introduction Despite extraordinary progress achieved in the field of generative AI, the development of effective joint audio-video (AV) generation approaches still represents significant challenge. In recent years, many works have been proposed, which yet deal with just single modality at time, such as, for example, video [24, 28, 29], or audio generation [5, 6, 11, 17, 31]. In addition, multimodal approaches that can shift from one modality to another have been developed as well [14, 17, 29, 31], yet the task of simultaneous AV generation remains quite unexplored due to the significant challenges it encompasses. Other than the clear technical difficulties, we identified three major critical requirements that an effective AV generation system should comply with: (i) audio and video quality, (ii) seamless multimodal synchronization and temporal coherence, (iii) infinite video length. In other words, the duration of the generated video should not be constrained by fixed length, while ensuring good quality without producing degenerate results and audio tracks that consistently match the dynamics of the visual data. Although there have been recent proposals in this field, they all struggle to meet all the above features. Among the few works that addressed this task, MMDiffusion (MMD) [23] pioneered novel fusion mechanism that allowed joint AV generation. However, such solution is restricted in terms of video duration, which is fixed in length during the diffusion process. Following the introduction of transformers in diffusion model architectures [21], Kim et al. [12] proposed novel method to generate longer videos. In order to do so, an auto-regressive approach is needed, where the generation of new sequence of frames is conditioned by the last generated frames of the previous sequence. Even though this approach enables the generation of videos with variable number of frames, in the long run the quality may deteriorate because of the error accumulation that affects auto-regressive kind of strategies. solution to this problem has been proposed by Ruhe et al. [24] by introducing Rolling Diffusion, novel technique for training diffusion models for video generation that It is based on uses sliding window denoising process. the idea that, in sequence, the prediction of frames that are distant in the future is more uncertain with respect to temporally-closer ones. Thus, the amount of noise of the diffusion process for each frame should be proportional to the temporal distance. This approach resulted preferable to the auto-regressive one for improving the quality and consistency of longer sequences, although it does not account for the generation of synchronized audio tracks. In light of the above, in this paper we propose novel architecture for joint AV generation, called RFLAV, specifically designed to address the key challenges discussed so far. In general, our method demonstrates better performance in comparison to previous approaches in terms of generation quality and AV consistency. Above all though, our model takes significant step forward by enabling infinite 1 AV generation. Our solution allows for the generation of paired AV sequences without any restriction on their duration, while maintaining high multimodal synchronization and quality. To achieve these results, we carefully designed several components in our architecture. In particular, we revisit the way audio and video are encoded so as to adapt the multimodal stream to be processed via rolling diffusion mechanism and we propose new lightweight crossmodality interaction module. Ultimately, our contributions are as follows: rolling flow matching model supporting infinite AV generation with sustained visual and acoustic consistency. An exploration of different alternatives for cross-modality interaction, resulting in lightweight AV fusion module which does not require attention mechanism. Our code and models will be fully open source. 2. Related Work Multimodal Generation. Multimodal models have the goal of learning more general and comprehensive representation of multiple data modalities, such as video, sound, and text. In particular, for these tasks several key challenges have to be faced, such as temporal coherence for time-dependent data and modality alignment, but also more practical ones like computational performance. Typically, previous studies focused on single-modality generation [9, 24, 28] or text-to-video, text-to-audio, and other conditional generation [5, 7, 8, 19, 29, 31], which are common applications for generative models. Among these, the most successful ones are based on diffusion models, but despite their versatility, the number of applications in multimodal tasks remains limited and lacks variety, due to the intrinsic complexity of such problems. Nevertheless, an application that has gained popularity is joint audio-video generation, which recently saw surge of different approaches based on diffusion models [12, 23, 25, 27, 30, 32]. Among these, MM-Diffusion (MMD) [23] proposed U-Net based architecture, featuring two separate branches for audio and video, merged by random-shift attention block, which allowed sound and visual data to influence and synchronize with each other. Upon MMD, AV-DiT [27] exploited shared DiT-XL/2 backbone [21], pre-trained on ImageNet, by introducing trainable adapter layers for audio and video, demonstrating better alignment and generation of modality. Finally, Kim et al. [12] introduced novel parametrization of the diffusion timestep for the forward process, applying different diffusion timestep across each modality and temporal dimension. This approach, combined with their model architecture, is capable of surpassing previous work. However, the output produced is limited to 2.125 seconds, and an autoregressive mechanism is still required to produce video longer than 2.125 seconds, with the risk of error accumulation and performance degradation. Our method improves upon such models by exploiting rolling approach to generate high-quality, coherent, longduration videos with aligned audio. Moreover, we propose novel strategy to fuse and align audio and video during training through lightweight temporal cross-modality interaction block. This design also allows our model to perform video-to-audio and audio-to-video tasks, along with joint audio-video generation. Finally, since our model does not employ any video or audio encoder, input and output lengths are not constrained to be multiples of the encoder length, allowing training with an arbitrary number of frames and the generation of videos with unconstrained duration. Diffusion Models. Diffusion models have achieved great success in various tasks due to their versatility and scalability [2, 13, 17, 23, 27] and have presented numerous backbones for different applications [1, 21, 22]. drawback of diffusion models though is that to generate new samples, the traditional diffusion process involves complex backward path that requires multiple integration steps. To address this, method called Rectified Flow, or Flow Matching, was proposed [16, 18] to learn straight path between data and noise by linearly interpolating them, demonstrating improved performance with appropriate weight adjustments compared to standard diffusion [4]. Furthermore, Ruhe et al. [24] proposed sliding window approach for video generation called Rolling Diffusion. In this method, model inputs consist of frames with progressively increasing diffusion steps, where frames present higher noise levels depending on their position in time, with the last frame consisting of full noise. During each diffusion step, frames are incrementally denoised until the first frame in the window is fully denoised, at which point it is removed from the window and replaced with new noise frame at the end. This approach effectively addresses the challenge of maintaining spatial and temporal consistency in video generation. In our work, we adapt this method to audio-video generation by pairing it with transformer architecture as backbone of our model, following [1, 4, 21], and using flow matching as model framework. 3. Methodology Our proposed architecture, called RFLAV, is rolling rectified-flow model designed for AV generation. More in detail, we modified the rolling diffusion methodology [24] to enable training using rectified flow matching [16, 18]. 3.1. Architecture is Our architecture (see Fig. 1), transformer-based model designed to allow AV generation of any length. This is possible since our model does not decalled RFLAV, 2 Figure 1. An overview of our RFLAV model architecture. pend on any audio or video encoder, which typically impose generated videos to be multiples of their output sizes. The model processes both video and audio in two separate branches. Fusion of different distributed modalities is avoided at early stages; they are instead combined later in the architecture. This allows for an early intra-modality interaction, performed with self-attentions, before the intermodality interaction. Video and Audio encoding. To generate videos of any length, we designed our system to synthesize samples frame-by-frame. Our approach uses an image encoder, the same employed in single image generation [22], instead of video encoder to reduce the spatial dimension of each frame of the video, avoiding any temporal compression. The output of the encoder is video RT chw, where is the temporal dimension, represents the number of encoder channels and = H/f , = W/f , with and being the original size of the video and being the downsampling factor. The audio RF Nm is instead represented by its mel spectrogram obtained from the raw waveform, where is the number of time frames and Nm the number of mel bins. Since each modality is encoded without performing any temporal compression (i.e., without any video or audio encoder), one-to-one reference between audio and video frames can be established. More in detail, each video frame can be associated with the corresponding part of the mel spectrogram, having size F/T , for better interaction between the two modalities. This is shown in Fig. 2. The processing occurs in parallel for both modalities. The video passes through patchify layer, producing feature embedding tensor RT LD, where is the number of spatial patches per frame (computed as = hw p2 , with frame dimensions h, and patch size p) and is the 3 Figure 2. Temporal alignment between video frames and mel spectrogram segments. Each video frame corresponds to fixed-size section (F/T) of the mel spectrogram, allowing for 1:1 mapping. hidden size. Simultaneously, the audio undergoes linear projection to obtain RF D, where is the number of audio segments and matches the hidden dimension of the video, resulting in video patches and audio patches. Thanks to its design, the proposed architecture enables the generation of videos of any length, whereas with the usage of video (or audio) encoder, the length must be multiple of the encoders output size. RFLAV block. The main block of our architecture is divided into two parallel branches, one for the video and one for the audio. The video branch sequentially applies spatial and temporal attention. The audio branch applies only temporal attention since the audio has no spatial structure. Both branches incorporate time-dependent feature modulation before and after each attention layer. This is performed with custom version of DiT adaptive layer normalization (AdaLN) [21], where each video frame and the corresponding part of the audio are modulated by different timestep embedding (see Sec. 3.3) and, optionally, by timestep-invariant class conditioning embedding y. To perform cross-modality fusion we propose and explore 3 different alternative blocks structure, as shown in Fig. 3. The first block (Fig. 3a) implements cross-modal interaction via self-attention. Audio embeddings are reshaped to RT F/T and concatenated with video embeddings, obtaining av RT (L+F/T )D. The feature map is then processed by causally masked self-attentions. Finally, the result is split again into the corresponding video and audio embeddings and summed to the respective branch. Although this approach offers sophisticated cross-modal interaction, it requires much higher computational and memory costs w.r.t. the following blocks. The second block (Fig. 3b) employs lightweight modulation mechanism that starts by computing temporal averages. More in detail, the video features RT LD are reduced to RT 1D, while audio features (a) (b) (c) Figure 3. a) Cross-modal interaction via self-attention, where and mean concatenation and split. b) Lightweight cross-modality interaction mechanism with temporal average modulation. c) Our final proposed RFLAV block, an enhanced lightweight mechanism incorporating timestep embedding and optional class conditioning embedding c. RF Nm are first reshaped to RT D and then reduced to RT 1D. Next, v, are used for cross-modal fusion. Before and after the feedforward layer in each branch, we apply the modulation operations used for the timesteps, but parameterized by the averaged features of the other branch. The final block (Fig. 3c) is built upon the second block. It sums the timestep embedding and the (optional) class embedding to the temporal average embedding. This enhances the propagation of timestep and class information. Furthermore, it is worth noting that in all the proposed blocks, cross-modality interaction is strategically placed after attention layers and before feed forward layers, allowing each modality branch to independently process information through its respective attention mechanisms before influencing the other modality branch. 3.2. Flow Matching Flow matching defines the forward path as linear trajectory between sample drawn from the data distribution and noise ϵ obtained from Gaussian distribution (0, 1), where the trajectory depends on time parameter, represented by t, with ranging from 0 to 1. xt = tx + (1 t)ϵ (1) The model is then trained to predict the velocity vector ϕ, which drives sample from noise to the data distribution. The velocity vector ϕ is defined as the derivative of the trajectory path with respect to t: Hence, the model predicts the velocity vector ˆϕ(xt, t) taking as input both xt and the timestep t. The final loss is: LFM(x) = EtU (0,1),ϵN (0,1)λt(x ϵ) ˆϕ(xt, t)2 (3) The loss is scaled by weight factor λt. In [4], the authors show that the best factor is obtained with λt = logit-normal(t; 0, 1). 3.3. Rolling Flow Matching Rolling Diffusion Model, introduced by Ruhe et al. [24], serves as foundational framework for generating infinite video sequences without necessitating an auto-regressive approach. More in detail, it employs sliding window denoising technique, in which each frame corresponds to distinct, but progressively higher, denoising timestep. Once the initial frame is fully denoised, the entire window shifts to allow new noisy frame at the end of the window. In our architecture, we propose novel rolling model which simultaneously generates both video and audio, without any pre-fixed length and with substained quality. Given RT chw and RT F/T Nm , we define the forward process to obtain the noisy video ˆv and the noisy audio embeddings ˆa as: ˆvk = tkvk + (1 tk)ϵv,k, ϵv,k (0, 1) ˆak = tkak + (1 tk)ϵa,k, ϵa,k (0, 1) (4) (5) ϕ = dxt dt = (x ϵ) (2) where tk is the timestep adjusted for each frame [0, 1] and its corresponding F/T mel spectrogram vector. 4 (a) (b) Figure 4. a) Rolling phase: at each step, new clean frame is produced (highlighted in red) and subsequently removed from the window. Then, new noisy frame, (highlighted in blue), is appended to the end of the window. b) Pre-rolling phase: the frames are gradually denoised starting from full noise configuration. The pre-rolling phase goes on for steps, until the window is ready for the rolling phase. Following [24], we introduce two distinct temporal scheduling mechanisms for tk depending on variable U(0, 1). The rolling phase scheduler, tr k(w), is designed for continuous generation in which the model produces content sequentially, while the pre-rolling phase scheduler, tp k(w), is specifically designed to handle the initial state where both modalities start as pure noise. During the rolling phase (see Fig. 4a) the timestep for each window frame is computed as: tr k(w) = 1 + k(w) [0, 1] having distance between each (6) with each tr consecutive timesteps of 1/T and tr k(w) < tr k+1(w). However, using only the rolling phase timesteps formulation would result in tr k(w) never being 0 for all the frames in the sequence, meaning that it would not be possible to sample new AV pairs from pure noise. Thus, pre-rolling phase (see Fig. 4b) is added during training, defining an alternative timestep formulation to allow the model to handle fully noisy inputs. The computation becomes: The final model loss is then formulated from Eq. 3 as: L(v, a) = wU (0,1) ϵvN (0,1) ϵaN (0,1) [Lv(v, t(w), ϵv) + La(a, t(w), ϵa)] (9) where Lv and La are defined as: Lv(v, t(w), ϵv) = La(a, t(w), ϵa) = (cid:34) (cid:34) 1 1 1 (cid:88) k=0 1 (cid:88) k=0 (vk ϵv,k) ˆϕ(ˆvk, tk(w))2(cid:17) (cid:16) λtk (cid:16) (ak ϵa,k) ˆϕ(ˆak, tk(w))2(cid:17) λtk (cid:35) (cid:35) (10) with λtk = logit-normal(tk; 0, 1). 4. Experiments 4.1. Model details Our model is composed of 12 RFLAV blocks and window size of 10 frames. To encode each video frame we employ the stable diffusion encoder [22], which has downsize factor = 8 and with = 4. For audio, we use the same vocoder as [31], with Nm = 256. θ is fixed to 0.2. tp k(w) = clamp (cid:18) 1 (cid:18) (cid:19) (cid:19) + ; 0, 1 (7) 4.2. Datasets k(w) tp This ensures that tp k(w) [0, 1], but with tp k+1(w). Specifically, = 1 represents the initial state when all frames are pure noise. Then, moving towards = 0, the noise is gradually removed from each frame in the sequence depending on its position in time. Finally, when = 0, the first frame will be noise-free and the last one will be pure noise and the rolling phase can begin. The final timestep vector t(w) = [t0(w), . . . , tT 1(w)] becomes: t(w) = (cid:40) tp(w), with probability θ tr(w), with probability 1 θ where θ [0, 1] is an hyperparameter controlling the balance between pre-rolling and rolling phase during training. 5 We follow previous work [23] to evaluate our model. Specifically, we train our model on two datasets, Landscape [23] and AIST++ [15]. Landscape dataset contains 9 different settings (i.e., explosion, fire crackling, raining, splashing water, squishing water, thunder, underwater bubbling, underwater burbling, and wind noise) providing 2.7 hours of high-quality audio-video pairs. AIST++ [15] is subset of AIST [26] that provides 5.2 hours of video featuring paired audio and dancer movements. (8) 4.3. Evaluation metrics We evaluated ours and other state-of-the-art models with 2048 generated samples at 6464 resolutions and 16 frames per video sample. Following previous work [23] we used Frechet Video Distance (FVD) and Kernel Video Distance (KVD) to measure video quality, employing the I3D video classifier pre-trained on Kinetics-400 [3]. To measure audio quality, we used Frechet Audio Distance (FAD) calculated from pre-trained AudioCLIP classifier [10]. 4.4. Ablation Study In Table 1.A, we present comparison of all our proposed blocks on the AIST++ dataset. All the metrics are calculated using 20 denoising steps. Indeed, the proposed block (c) outperforms blocks (a) and (b) in terms of overall performance. Specifically, compared to block (b), our architecture achieves more effective propagation of the timestep embedding, leading to improved performance in the diffusion process. Additionally, compared to block (a), our proposed block is more efficient and faster, requiring fewer computational resources and less memory. In addition, we also test window size of 5 frames and 20 frames. We observe, as reported in Table 1.B, that 10 frames is the optimal solution. The reason is that smaller window (e.g., 5 frames) may not capture enough temporal context, leading to incomplete motion patterns while larger window can introduce redundant information and increase noise. The 10-frame window provides the right balance, ensuring sufficient context while avoiding excessive complexity. AIST++ Model RFLAV-(a) RFLAV-(b) RFLAV-(c) Time (s) Memory Usage (GB) FVD KVD FAD 15.47 3.92 3. 7.9 2.7 2.7 53.38 51.31 50.92 7.33 9.76 8.73 8.70 8.30 8.40 Table 1.A. Comparison between inference time for sample of 16 frames, memory usage and quantitative metrics of all the 3 proposed blocks. Time was calculated on NVIDIA RTX 4090 gpu. AIST++ Model RFLAV-(c) RFLAV-(c) RFLAV-(c) Window Size FVD KVD FAD 5 10 20 65.42 50.92 77. 10.41 8.73 12.71 8.46 8.40 8.53 Table 1.B. Comparison of RFLAV-(c) with different window size. 4.5. Comparison with SOTA models We compare our model with the current SOTA models [12, 23, 27], performing both quantitative and qualitative analysis. For open source models, such as [23], we recompute the metrics on our hardware, and, vice versa, for closed source models ([12, 27]), we took the metrics directly from the original papers. The reason why, when possible, we choose to calculate the metrics from the SOTA models again is that, as pointed out in [20], the way images are resized and compressed can have large impact on the common evaluation metrics of generative models. Therefore, by reproducing the same settings for all the open-source generative models we compare with, we ensure fair comparison. Unfortunately, this cannot be done for closed-source models for obvious reasons. For the sake of fair comparison, re-implementing the methods is not viable solution since both (i) fine-grained technical details are not provided in the related papers and exactly reproducing the implementation is almost impossible and (ii) would likely lead to inconsistent results. Thus, we opted for copying the numbers. Model GT Params Steps FVD KVD FAD FVD KVD FAD AIST++ Landscape - - 3.67 MMD [23] Wang et al. [27] RFLAV RFLAV RFLAV 426 25 931 250 421 20 421 100 421 200 224.14 68.88 50.92 38.93 38. -0.28 50.52 21.01 8.73 6.58 6.15 8.25 11.41 10.17 8.40 8.35 8. 7.45 177.29 172.69 86.53 85.44 80.19 -0.15 7.63 15.41 3.36 3.73 3. 9.33 9.73 11.17 10.49 9.61 9.88 Table 2. comparison between our method and the current SOTA models, calculated with 2048 samples at 64 64 resolution. Metrics of the model are taken from the paper since neither code nor checkpoints are available. Quantitative analysis. The results of the SOTA models [12, 23, 27] are shown in Table. 2. Furthermore, the comparison with AVDiT by Kim et al. [12] is presented in Table 3. The reason for different table is that, for AIST++ and Landscape datasets, AVDiT by Kim et al. presented results only for the tasks of Audio2Video (A2V) and Video2Audio (V2A) generation, in which original audio is used to generate video and vice versa, and not for fully joint AV generation. Additionally, new results could not be produced since neither code nor checkpoints are currently available. Therefore, we took their FVD and KVD from A2V generation and their FAD from V2A generation and compared them with our results for these tasks. The results in Table 2 show that our proposed model equipped with the enhanced AdaLN block (block (c)) with 20 steps surpasses all the SOTA models in both datasets except for the FAD score in Landscape where MMD is slightly better. Notably, thanks to the lightweight image encoder, our model has less total parameters compared to others SOTA models. Additionally, we also evaluated our model with 100 and 200 steps, further improving the quality of the generated results. In particular, the improvements w.r.t. the metrics from 100 to 200 steps are negligible, which is consistent with the observations made in [16]. Regarding A2V and V2A comparisons  (Table 3)  , we chose to generate the samples using 200 steps. Results in terms of FVD and KVD for A2V generation are competitive w.r.t. Kim et al. [12], yet FAD value is far worse. This is likely because we depend on vocoder to convert the 6 mel spectogram back to WAV, while Kim et al. architecture uses closed-source WAV encoder-decoder architecture [33]. However, since the gap between our generated audio and the corresponding GT is very small (see Table 2), our model still generates high-quality audio. AIST++ Landscape A2V V2A A2V V2A Model Kim et al. [12] RFLAV Params Steps FVD KVD FAD FVD KVD FAD 731 250 421 38.04 46.81 5.27 8.72 1.10 8. 86.79 94.47 4.30 4.00 0.78 9. Table 3. Our model compared with Kim et al. [12] in A2V and V2A generation. Metrics of the model are taken from the paper since neither code nor checkpoints are available. Finally, we evaluated our model through user study on AIST++, with subjects randomly sampled from experts and non-experts in computer vision. Participants are asked to choose the best video (among pair) in terms of AV quality and alignment. We compared our model with MMD and asked the participants to evaluate 10 randomly sampled pairs of videos per form. We also compared with Kim et al. model. However, since their model is closed-source and we could not sample new videos, we compared our generated videos with videos downloaded from their project website1. The general results are shown in Table. 4 and prove that our model is considered the best in the majority of cases. Models Quality AV alignment Ours vs MMD [23] Ours vs Kim et al. [12] 81% 80% 78% 76% Table 4. User study to evaluate the overall quality and the AV align between our models and the current state-of-the-art models. Results on Long Video Generation. In Fig. 6 examples of longer videos (240 frames i.e., 24 seconds) are depicted, to show that the quality remains consistent throughout their duration, without visible signs of degradation. To provide quantitative evaluation of the video quality in the long run, we compute the metrics of Sect. 4.3 on the 240 frames-long sequences, using sliding window of 16 frames. The average results for 2048 videos are shown in Fig. 5. The figure unexpectedly reveals quite big jump in the metrics after the first evaluation window. We argue that this is caused by the rolling diffusion design and, particularly, when switching from the pre-rolling to the full rolling phase. Indeed, during the pre-rolling phase, each frame < is conditioned only on the previous 1 frames in the sequence, due to temporal attention. This makes the 1https://avdit2024.github.io/ Figure 5. AV metrics and feature drift calculated on long (i.e., 240 frames) generated videos using sliding window of 16 frames. generative process easier since the model needs to take into account less conditioning information. On the contrary, in the full rolling phase, at each diffusion step fully denoised frame exits the window, while new one (pure noise) gets in, meaning that each new frame will be conditioned by all the previous frames in the window, equal to 1. Overall, this seems to lead to slight shift in the generated data distribution, even though no perceivable changes are observed (see the supplementary to check videos). To validate this hypothesis, we calculated the distance between features extracted from the first window (16 frames) of the sequence, and all the other windows. To extract the features we used the same network used to calculate the FVD [3]. The idea is that, if the above conjecture holds, then we will observe strong initial drift in the encoded features. From the results (purple line in Fig. 5), we indeed observe large drift after the first chunk. After that though, the drift increment becomes minimal, suggesting that after the transitory phase from pre-rolling to full rolling, the features get stabilized. The same behavior is observed for all the other metrics. Nonetheless, despite the metrics do increase to some extent, they are still comparable or lower than previous state-of-the-art methods, suggesting that the proposed solution is viable tradeoff between generation quality and consistency throughout time. This analysis also suggests that the rolling technique can be improved to limit such drift, yet it is fundamental to ensure consistency in the long run. Detecting Looping Sequences in Videos. One may wonder whether the generated long videos are just looped repetition of shorter sequences which would hinder our claim of infinite AV generation. To clearly show this is not the case and our model effectively learned varying motion dynamics, we designed specific test. The idea is that if video contains looped sub-sequences, then similar frames will appear at periodic timesteps. To verify that, we com7 Figure 6. Long videos generated by our model on both AIST++ and landscape. Frames are sampled every 1.6s for visualization. people performing repetitive dance moves. The real video presents 3 peaks in r(k), denoting looping sequence with length of 23 frames. On the contrary, the analyzed generated video does not show such behavior. For the sake of completeness, we analyze the whole training split of AIST++, which contains 980 videos, with an average duration of 10 seconds. Among them, we identified 692 videos containing looping sequences. To do so, first we compute the Fourier transform of r(k). Then, in frequency domain, we can identify dominant frequency components and selecting the videos for which the largest component (indicating loop) is above fixed threshold. In contrast, when looking at 980 synthetic videos of the same length generated by our model, only 264 exhibited looping behavior. This demonstrates that our model can generate extended video sequences without repetitive loops. 5. Conclusion In this paper, we propose RFLAV, novel architecture based on flow matching that takes advantage of rolling diffusion approach to generate coherent and infinite video samples with synchronized audio tracks and unfixed duration. Moreover, we designed novel temporal fusion module that enables efficient AV generation, while only requiring limited amount of frames during training. Through series of experiments and ablation studies on two common datasets, we demonstrate, both qualitatively and quantitatively, our improvements over state-of-the-art methods. Despite the substantial contribution of the proposed architecture w.r.t. the quality and length of the generated AV sequences, when dealing with such task, series In particular, of technical challenges remain to be faced. when trained on AIST++, our model occasionally struggles to generate complex limb movements. Furthermore, when generating long videos, it may sometimes fail to preserve time consistency when some visual elements In are obscured beyond the model processing window. AIST++, this happens in scenarios with partial occlusion or unconventional movements, such as intricate dance Figure 7. Frame similarity matrix of real vs. generated videos from the AIST++ dataset, illustrating the presence of loops in real videos and their absence in generated content. pute pairwise frame-to-frame similarity using the Learned Perceptual Image Patch Similarity (LPIPS) metric [34] on both real and generated videos from the AIST++ dataset, which features complex motion dynamics. From these, we build pairwise similarity matrix where the (i, j) entry measures the LPIPS between the frames and j. To quantify looping behavior, we then compute the average of offdiagonal similarity values, denoted as r(k), where represents the frame offset or the time lag. Peaks in r(k) values denote looping sequence of frames i.e. high similarity peak in r(k) means that general frame closely resembles the frame (i + k), indicating repetitive pattern of period k. Fig. 7 illustrates the process for two videos, real and generated. On the left are the two similarity matrices and on the right the r(k) calculated from them. It is evident from the similarity matrix that the real video presents peculiar diagonal pattern, which is clear hint of loop. This can be verified by observing the AIST++ videos which depict 8 sequences. For example, dancers arm could cover the logo on their t-shirt for several frames and therefore the model will forget about it. On the other side, in Landscape, in sea scenarios, the water could obscure for an extended duration visual elements that will be then forgotten by the model. This reveals areas for future algorithmic refinement."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 2 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. 6, 7 [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 4 [5] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024. 1, 2 [6] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Flux that plays music, 2024. 1 [7] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In European Conference on Computer Vision, pages 102118. Springer, 2022. 2 [8] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Text-to-audio generation using arXiv and Soujanya Poria. instruction-tuned llm and latent diffusion model. preprint arXiv:2304.13731, 2023. 2 [9] Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. arXiv preprint arXiv:2104.01778, 2021. 2 [10] Andrey Guzhov, Federico Raue, Jorn Hees, and Andreas Dengel. Audioclip: Extending clip to image, text and audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 976980. IEEE, 2022. [11] Jiarui Hai, Yong Xu, Hao Zhang, Chenxing Li, Helin Wang, Mounya Elhilali, and Dong Yu. Ezaudio: Enhancing text-toaudio generation with efficient diffusion transformer. arXiv preprint arXiv:2409.10819, 2024. 1 Jansen, Jacob Walker, and Krishna Somandepalli. versatile diffusion transformer with mixture of noise levels for audiovisual generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 1, 2, 6, 7 [13] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. 2 [14] Seungwoo Lee, Chaerin Kong, Donghyeon Jeon, and Nojun Kwak. Aadiff: Audio-aligned video synthesis with text-toimage diffusion. arXiv preprint arXiv:2305.04001, 2023. 1 [15] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13401 13412, 2021. 5 [16] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 2, 6 [17] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 1, 2 [18] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2 [19] Zhijun Liu, Shuai Wang, Sho Inoue, Qibing Bai, and Haizhou Li. Autoregressive diffusion transformer for textto-speech synthesis. arXiv preprint arXiv:2406.05551, 2024. [20] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1141011420, 2022. 6 [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2, 3 [22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 5 [23] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. 1, 2, 5, 6, 7 [24] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel In Forty-first InHoogeboom. Rolling diffusion models. ternational Conference on Machine Learning, 2024. 1, 2, 4, 5 [12] Gwanghyun Kim, Alonso Martinez, Yu-Chuan Su, Brendan Jou, Jose Lezama, Agrim Gupta, Lijun Yu, Lu Jiang, Aren [25] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffu9 sion. Advances in Neural Information Processing Systems, 36:1608316099, 2023. 2 [26] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing. In ISMIR, page 6, 2019. 5 [27] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Efficient audio-visual diffusion transformer for joint audio and video generation. arXiv preprint arXiv:2406.07686, 2024. 2, 6 [28] Dirk Weissenborn, Oscar Tackstrom, and Jakob Uszkoreit. Scaling autoregressive video models. arXiv preprint arXiv:1906.02634, 2019. 1, 2 [29] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-tovideo generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73957405, 2024. 1, [30] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visualaudio generation with diffusion latent aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71517161, 2024. 2 [31] Jinlong Xue, Yayue Deng, Yingming Gao, and Ya Li. Auffusion: Leveraging the power of diffusion and large language models for text-to-audio generation. arXiv preprint arXiv:2401.01044, 2024. 1, 2, 5 [32] Ruihan Yang, Hannes Gamper, and Sebastian Braun. Cmmd: Contrastive multi-modal diffusion for video-audio conditional modeling. arXiv preprint arXiv:2312.05412, 2023. 2 [33] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An endto-end neural audio codec, 2021. 7 [34] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018."
        }
    ],
    "affiliations": [
        "University of Parma, Department of Engineering and Architecture. Parma, Italy",
        "University of Siena, Department of Information engineering and mathematics. Siena, Italy"
    ]
}