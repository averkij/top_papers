{
    "paper_title": "FinAudio: A Benchmark for Audio Large Language Models in Financial Applications",
    "authors": [
        "Yupeng Cao",
        "Haohang Li",
        "Yangyang Yu",
        "Shashidhar Reddy Javaji",
        "Yueru He",
        "Jimin Huang",
        "Zining Zhu",
        "Qianqian Xie",
        "Xiao-yang Liu",
        "Koduvayur Subbalakshmi",
        "Meikang Qiu",
        "Sophia Ananiadou",
        "Jian-Yun Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce \\textsc{FinAudio}, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the \\textsc{FinAudio} benchmark. Then, we evaluate seven prevalent AudioLLMs on \\textsc{FinAudio}. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released."
        },
        {
            "title": "Start",
            "content": "FINAUDIO: Benchmark for Audio Large Language Models in Financial Applications Yupeng Cao1, Haohang Li1, Yangyang Yu1, Shashidhar Reddy Javaji1 Yueru He2, Jimin Huang3, Zining Zhu1, Qianqian Xie3, Xiao-yang Liu2, Koduvayur Subbalakshmi1, Meikang Qiu4, Sophia Ananiadou5, Jian-Yun Nie6 1Stevens Institute of Technology 2Columbia University 3The Fin AI 4Augusta University 5The University of Manchester 6University of Montreal 5 2 0 2 6 2 ] . [ 1 0 9 9 0 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce FINAUDIO, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop novel dataset for financial audio summarization, comprising the FINAUDIO benchmark. Then, we evaluate seven prevalent AudioLLMs on FINAUDIO. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released."
        },
        {
            "title": "Introduction",
            "content": "Audio data plays critical role in the financial domain, encompassing earnings conference calls, investor presentations, and customer service interactions (see Figure 1). Timely and accurate interpretation of financial audio data underpins financial services, sentiment analysis, and investment decisions (Qin and Yang, 2019; Yang et al., 2020; Cao et al., 2024). Recent developments in financial AI have been propelled by breakthroughs in natural language processing (NLP), particularly large language models (LLMs), such as BloombergGPT (Wu et al., 2023b), PIXIU (Xie et al., 2023), FinGPT (Liu et al.). Financial evaluation suites like FinBen (Xie Figure 1: An overview of critical financial audio data types and applications. et al., 2025) further provide extensive assessments of LLMs in financial NLP tasks. Despite these advancements, there remains critical gap: there is no audio-focused financial LLM or corresponding evaluation benchmark. Although multimodal financial LLMs (e.g., FinLLaVA-8B (Xie et al., 2024)) can process text, images, and charts, they cannot yet handle audio data. In the general domain, AudioLLMs have made significant progress (Wu et al., 2023a; Zhang et al., 2024). AudioLLMs extend foundational LLMs with an audio encoder that aligns audio inputs to the LLMs input space (Radford et al., 2023; Hu et al., 2024; Chu et al., 2023, 2024; Tang et al.), enabling audio-centric tasks such as automatic speech recognition (ASR), speech question answering (SQA), audio scene understanding, and voice analysis. While their effectiveness in these tasks has been explored in variety of general scenarios (Yang et al., 2024; Wang et al., 2024), comprehensive benchmark for financial audio remains absent. This gap limits the research communitys ability to systematically evaluate performance, compare strategies, and identify biases in financial audio datasets, hindering downstream applications such as investment advisory and financial services. To address this critical need, we introduce FINAUDIO, the first AudioLLM benchmark tailored for the financial domain. Specifically, FINAUDIO includes three tasks representating real-world finanFigure 2: Overview of FinAudio benchmark. cial scenarios: (1) ASR for short financial audio clips, (2) ASR for long financial audio recordings, and (3) summarization of financial audio data. For this purpose, we have collected four open-source financial audio datasets. Two of these datasets comprise numerous short audio clips (each under one minute), while the other two feature much longer recordings, about 45 to 60 minutes, which is sufficient to include complete earnings conference call. Additionally, We collect new dataset tailored for financial audio summarization, named FinAudioSum. Altogether, these five datasets form the FINAUDIO benchmark, comprising over 400 hours of financial audio. FINAUDIOs scale is comparable to general-domain AudioLLM benchmarks. On FINAUDIO, we evaluate seven representative AudioLLMs and observed the following: (a) AudioLLMs perform better in ASR tasks on short financial audio clips compared to longer recordings; (b) the performance of long-audio ASR limits audio summarization quality; (c) AudioLLMs exhibit significant variability in their robustness to different instructions; (d) The open-source AudioLLM Whisper-v3 outperforms closed-source models across all ASR datasets, offering potential low-cost and privacy-preserving solution for both the financial industry and individual investors. Our main contributions are below: (1) We propose FINAUDIO, the first comprehensive open-source evaluation benchmark for AudioLLMs in the financial audio data. (2) We design three tasks that reflect realistic financial speech scenarios and compile five evaluation datasets, including newly developed dataset for financial audio summarization. (3) We conduct systematic evaluation of seven AudioLLMs, showcasing their advantages and limitations and highlighting challenges and future research directions."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Advancements in AudioLLM Among the modalities addressed by multimodal LLMs, audio stands out as particularly critical and challenging (Su et al., 2025). Several multimodal models, such as Gemini Pro (Team et al., 2023) and PandaGPT (Su et al., 2023), have Promibeen adapted to process audio data. nent AudioLLMs include AudioGPT (Huang et al., 2024), SpeechGPT (Zhang et al., 2023), AudioPaLM (Rubenstein et al., 2023), QwenAudio (Chu et al., 2023), WavLLM (Hu et al., 2024), Whisper (Radford et al., 2023), Salmonn (Tang et al.), and Qwen2-Audio (Chu et al., 2024). Correspondingly, general-domain benchmarks like AirBench (Yang et al., 2024), emphasizing conversational scenarios, and AudioBench (Wang et al., 2024), evaluating eight tasks across twenty datasets, have been developed to assess AudioLLMs. However, these benchmarks contain limited financial audio data, inadequate for comprehensive evaluation in financial contexts."
        },
        {
            "title": "And Benchmark",
            "content": "advances toward multimodal in general-domain LLMs Recent specialhave accelerated the emergence of such as FinGPT (Liu ized financial LLMs, InvestLM (Yang et al., 2023), and et al.), The BloombergGPT (Wu et al., 2023b). progression financial LLMsincluding Open-FinLLMs (Xie et al., 2024) and FinTral (Bhatia et al., 2024)further highlights efforts to address complex financial tasks involving charts and tabular data. These multimodal models overcome the limitations of traditional approaches (Ang and Lim, 2022; Bhatia et al., 2024; Yu and Suchow, 2022) by more flexibly adapting to financial contexts. As financial LLMs become increasingly specialized, Figure 3: Evaluation pipelines for the three FINAUDIOs major tasks. the demand for robust evaluation grows. In response, various benchmarks have been proposed. FLUE (Shah et al., 2022) introduced the first benchmark for financial NLP evaluation, covering tasks such as sentiment analysis, headline classification, named entity recognition, structure boundary detection, and question answering. PIXIU (Xie et al., 2023) extended these evalinto multimodal document analysis, uations while FinBen (Xie et al., 2025) significantly expanded coverage, featuring 36 datasets across 24 financial tasks. Despite these advances, existing benchmarks remain predominantly text-focused, overlooking audioan essential modality in financial applications."
        },
        {
            "title": "3 The FINAUDIO Benchmark",
            "content": "FINAUDIO aims to evaluate the performance of AudioLLMs on financial audio data. Our evaluation primarily focuses on the models ability to comprehend financial audio across various financial scenarios. Accordingly, our benchmarks are designed to evaluate three crucial aspects: 1) ASR for short financial audio clips, which are common in financial services and conversations; 2) ASR for long financial audio recordings, typically in major financial meetings; 3) Summarization for long financial audio recordings, aimed at testing the capacity of AudioLLMs to construct frameworks for understanding extensive financial audio content. For each task, we both compile existing datasets and develop new ones to ensure comprehensive understanding ability. Figure 1 is an overview of FINAUDIO . This section will detail the task definitions and the development of the evaluation datasets. 3.1 FINAUDIO Tasks Here we specify the FINAUDIO tasks, which are illustrated in Figure 3. 3.1.1 ASR for short financial audio clip Automatic Speech Recognition converts spoken language into text. Short audio clips from financial chatbot assistants or financial news often contain dense financial terms. This task evaluates AudioLLMs accuracy in transcribing financial audio clips, directly impacting applications such as financial voice assistants. In this task, each input is represented as (A, Q, R), where is an audio clip, is the prompt instruction, and is the reference transcript. The transcribed text is generated as = AudioLLM(A, Q). The Word Error Rate (WER) is computed as WER = S+D+I , where S, D, and represent the number of substitutions, deletions, and insertions, respectively, and is the total number of words in the reference (N = +D +C, with indicating correct words). lower WER indicates better ASR performance."
        },
        {
            "title": "3.1.2 ASR for long financial audio recordings",
            "content": "Long audio recordings are common in finance, capturing events like quarterly earnings calls, mergers and acquisitions discussions, and central bank meetings. In this scenario, ASR task is thus used to transcribe lengthy financial audio, evaluating AudioLLMs capability to handle extended audio inputs. Challenges include long durations and complex financial terminology. This task assesses the models ability to consistently maintain transcription accuracy over long recordings, which is essential for comprehensive financial analysis. To handle the varying window lengths and the inability of current AudioLLMs to process long audio Dataset Name Type #Samples # Hours Task MDRM-test SPGISpeech-test Earning-21 Earning-22 FinAudioSum Short Clips Short Clips Long Audio Long Audio Long Audio 22,208 39,341 44 125 64 87 130 39 120 55 short financial clip ASR short financial clip ASR long financial audio ASR long financial audio ASR Metrics WER WER WER WER long financial audio Summarization Rouge-L & BertScore Table 1: Statistics of the datasets in the FinAudio benchmark. files within single window, we standardize audio inputs to 30-second segments, dividing each raw audio into chunks {a1, a2, . . . , an}. We transcribe each chunk ai using AudioLLMs into ti, and concatenated the resulting transcriptions as = [t1; t2; . . . ; tn]. The transcription quality is then evaluated by computing the WER. 3.1.3 Financial audio summarization This task measures the ability of AudioLLMs to accurately summarize long financial audio content, scenario unique to financial applications. It evaluates models understanding of financial discussions, extraction of key points, and effectiveness in condensing lengthy audio into concise summaries. This capability is crucial for stakeholders needing quick insights from extensive financial dialogues, such as investor briefings or regulatory meetings. AudioLLMs can process audio with maximum length of 30 seconds. Therefore, AudioLLM cannot perform the summarization task independently, we developed processing pipeline. Given input data represented as (A, Q, L), where denotes the long audio recording, the prompt instruction, and the reference summary, the pipeline first segments the long audio into smaller chunks (see Section 3.1.2). Each chunk is transcribed by an AudioLLM into text segments ti, which are combined into complete transcription . This transcription is then input into an LLM to generate the summary S. Summarization performance is evaluated using two widely adopted summarization evaluation metrics: Rouge-L (Lin, 2004) and BertScore (Zhang et al.), computed between the generated summary (S) and the reference summary (L)."
        },
        {
            "title": "3.2 FINAUDIO Datasets",
            "content": "The evaluation dataset in FINAUDIO is developed from two primary data sources: 1) existing opensource financial audio data originally created for non-LLM evaluation purposes. 2) novel datasets introduced in this work. Table 1 summarizes the key statistics. This section outlines their key information, leaving the details to Appendix A. 3.2.1 Short financial audio clip datasets Two datasets provide financial audio clips: MDRM (Qin and Yang, 2019) and SPGISpeech (ONeill et al., 2021), both derived from earnings conference call recordings. The original audio data was segmented at the sentence level and aligned with corresponding transcripts. Each dataset is initially split into training and test subsets; we utilize only the test sets for evaluation. The MDRM-test set includes 22,208 audio clips totaling 87 hours, while the SPGISpeech-test set contains 39,341 clips totaling 130 hours."
        },
        {
            "title": "4 Model and Experiment Setup",
            "content": "We evaluate the performance of seven representative AudioLLMs on the FINAUDIO benchmark: five open-source models Whisper (Radford et al., 2023), Qwen2-Audio-7B, Qwen21https://earningscast.com/ Datasets Whisper Qwen2-Audio Qwen2-Audio -7B-Instruct -7B -v Models SALMONN SALMONN Gemini -1.5-flash -13B -7B MDRM-test SPGISpeech-test Earning-21 Earning-22 2.14 2.88 11.85 15.93 3.97 4.42 26.06 42. 4.68 5.74 29.58 33.65 51.52 39.51 83.20 88.50 49.17 41.17 80.54 86.23 4.850 5.802 18.58 27.13 Gemini -2.0-flash 4.321 5.143 19.17 28. Table 2: Comparison of model performance based on WER across various speech datasets on ASR task. Audio-7B-instruct (Chu et al., 2024), SALMONN7B, SALMONN-13B (Tang et al.); two closedsource models Gemini-1.5-flash and Gemini2.0-flash (Team et al., 2023). These models span various parameter sizes, openand closedsource implementations, and include both base and instruction-tuned versions, enabling comprehensive assessment of AudioLLMs for financial audio tasks. To ensure fairness, we standardized the prompt as: Convert the audio speech into text transcript,. For summarization, transcripts from each model were summarized using GPT-4o (temperature=0). For open-sourced AudioLLMs, these experiments are exclusively conducted on two NVIDIA RTX A6000 GPUs, each equipped with 48GB of DRAM. Each round of evaluation takes approximately 200 hours to complete. For experiments in Tasks 1 and 2, we repeated evaluations three times and report the average results."
        },
        {
            "title": "5 Results and Analysis",
            "content": "In this section, we first analyze the performance of seven models on two ASR tasks, then evaluate their summarization results, and finally examine their robustness to varying prompts."
        },
        {
            "title": "5.1 Results on ASR Tasks",
            "content": "Results on short financial audio clips ASR. Table 2 shows that Whisper-v3 achieves notably low WER scores (2%3%) on both datasets, highlighting its robust speech recognition without domainspecific tuning. The Qwen2-Audio series (base and instruct versions) and Gemini exhibit moderate performance with WER scores between 4% and 6%. Conversely, SALMONN (7B and 13B) demonstrates substantially lower effectiveness, with WER around or exceeding 40%50%. Despite SALMONNs limited performance, these findings highlight the potential of other mainstream AudioLLMs to support voice-based financial services, particularly conversational assistants handling iterative short audio communications. Figure 4: WER on models across different datasets. Whisper-v3 consistently achieved the lowest WER. Results on long financial audio recording ASR. In the long financial audio ASR task, Whisper-v3 remained the top performance. It achieves lower WER in the range of approximately 12% - 16%. The Gemini series emerged as the second-best performer, with WER spanning roughly 1828%. The error rate of Qwen2-Audio series models has also increased significantly. SALMONN once again reported the highest error rates across both financial datasets (8088%). These results reveal decline in performance across all AudioLLMs, indicating that AudioLLMs face significant challenges when processing long financial audio data. The SALMONN series models performed poorly on both ASR tasks. Upon reviewing the experimental logs, we observed that SALMONN frequently produced empty outputs or failed to understand prompt instructions. This suggests that SALMONN might be overfitting to specific audio data or tasks, lacks robust instruction-following capabilities, and exhibits limited understanding of financial audio content. In Figure 4, we find no significant difference in performance between Gemini-1.5 and Gemini-2.0. The open-source model Whisper-v3 achieves the best overall performance across all datasets. These results demonstrate robust capabilities even within Dataset Whisper Qwen2-Audio-7B-Instruct Gemini-1.5-flash Rouge-L BertScore Rouge-L BertScore Rouge-L BertScore FinAudioSum 0.053 0.514 0.048 0.467 0.072 0. Table 3: Summarization performance using Rouge-L and BertScore metrics. long financial audio recognition. The Qwen-2 series, another open-source model, delivers comparable results to the closed-source Gemini models on short-audio ASR tasks but performs slightly worse on long-audio ASR. These findings demonstrate that open-source models can surpass closed-source counterparts in financial audio scenarios. This suggests that open-sourced AudioLLMs provide an effective, low-cost, and privacy-preserving solution for the financial industry and individual investors. 5.2 Results on summarization task Given the high WER in the ASR tasks, effective summarization cannot be reliably performed by models with excessive transcription errors. Therefore, we excluded the SALMONN model from the summarization evaluation. Balancing model performance and computational cost, we selected three modelsWhisper-v3, Qwen2-Audio-7B-Instruct, and Gemini-1.5-flash for subsequent summarization experiments. As Table 3 shows, the summarization performance across all evaluated models is moderate. The overall summarization quality depends on the ASR performance of AudioLLMs. Gemini-1.5flash achieves the highest scores in both ROUGE-L (0.072) and BERTScore (0.610), suggesting that Geminis transcription accuracy and semantic fidelity during the ASR stage align more closely with the ground-truth labels. Whisper outperforms Qwen2-Audio-7B-Instruct (ROUGE-L: 0.053 vs. 0.048; BERTScore: 0.514 vs. 0.467), demonstrating its robust general summarization capability even without fine-tuning on financial domain data."
        },
        {
            "title": "5.3 Prompt Robustness Analysis",
            "content": "Figure 5: Prompt robustness analysis: comparison of WER between fixed-prompt and random-prompt trials. pendix B). For each audio input, we randomly selected one prompt from this set to observe how variations in prompts affected model performance, as presented in Figure 5. We compare the WER in Figure 5 using two prompt types: the random prompt described above and the fixed prompt from Section 4, used in the main results  (Table 2)  . Results for Qwen2-7BInstruct and Whisper-v3 remain consistent, indicating their robustness across prompts. However, the performance of Qwen2-7B notably declines under varied prompts, exhibiting higher WER. The substantial performance gap between Qwen2-7B and Qwen2-7B-Instruct indicates that this AudioLLM is sensitive to prompt variations, providing empirical support for the necessity of instructiontuning in AudioLLM training."
        },
        {
            "title": "5.4 Error Analysis",
            "content": "During the ASR tasks, we noticed that AudioLLMs sometimes misinterpret numerical information and specialized financial terms in speech. Such errors significantly impact the factual accuracy of AudioLLMs within financial contexts. Specific examples of these errors are provided in the Appendix C."
        },
        {
            "title": "6 Conclusion and Research Outlook",
            "content": "The previous experiments indicated that different models respond differently to the prompt. Therefore, we further conducted prompt robustness analysis on three AudioLLMs: Whisper-v3, Qwen2-Audio-7B, and Qwen2-Audio-7B-Instruct - across three ASR datasets. Following the approach from AudioBench (Wang et al., 2024), we constructed an instruction set comprising 10 prompts. These prompts convey similar instructions expressed in diverse formulations (see ApWe propose FINAUDIO, an AudioLLM Benchmark for financial audio data. We constructed total of 400h+ of audio data for three tasks and evaluated it on seven AudioLLMs. Future research directions for applying AudioLLMs in the financial industry include (1) extending the input context window length of AudioLLM, (2) enhancing the models comprehension of numerical transcription and specialized financial terminology, and (3) improving their reasoning capabilities in financial contexts."
        },
        {
            "title": "Limitations",
            "content": "First, due to the limited availability of financial audio data, FinAudio primarily focuses on ASRrelated tasks. Second, FinAudio does not address financial audio scenarios involving languages other than English."
        },
        {
            "title": "Ethics Statement",
            "content": "All data sources used in FINADUIO are public. The authors take full responsibility for the development of FINADUIO, ensuring that the publicly available part of the dataset does not contain personal information and conforms to established ethical guidelines."
        },
        {
            "title": "References",
            "content": "Gary Ang and Ee-Peng Lim. 2022. Guided attention multimodal multitask financial forecasting with intercompany relationships and global and local news. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 63136326. Gagan Bhatia, Hasan Cavusoglu, Muhammad AbdulMageed, et al. 2024. Fintral: family of GPT-4 level multimodal financial large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1306413087. Yupeng Cao, Zhi Chen, Qingyun Pei, Nathan Lee, KP Subbalakshmi, and Papa Momar Ndiaye. 2024. ECC Analyzer: Extracting trading signal from earnings conference calls using large language model for In Proceedings of the stock volatility prediction. 5th ACM International Conference on AI in Finance, pages 257265. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. 2024. Qwen2-Audio technical report. arXiv preprint arXiv:2407.10759. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-Audio: Advancing universal audio understanding via unified large-scale audiolanguage models. arXiv preprint arXiv:2311.07919. Miguel Del Rio, Natalie Delworth, Ryan Westerman, Michelle Huang, Nishchal Bhandari, Joseph Palakapilly, Quinten McNamara, Joshua Dong, Piotr Zelasko, and Miguel Jetté. 2021. Earnings-21: practical benchmark for ASR in the wild. arXiv preprint arXiv:2104.11348. Miguel Del Rio, Peter Ha, Quinten McNamara, Corey Miller, and Shipra Chandra. 2022. Earnings-22: practical benchmark for accents in the wild. arXiv preprint arXiv:2203.15591. Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, et al. 2024. WavLLM: Towards robust and adaptive speech large language model. arXiv preprint arXiv:2404.00656. Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2024. AudioGPT: Understanding and generating speech, music, sound, and talking head. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2380223804. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481. Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha. FinGPT: Democratizing internet-scale data for financial large language models. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, et al. 2022. ECTSum: new benchmark dataset for bullet point summarization of long earnings call transcripts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1089310906. Patrick ONeill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael Shulman, et al. 2021. SPGISpeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition. arXiv preprint arXiv:2104.02014. Yu Qin and Yi Yang. 2019. What you say and how you say it matters: Predicting stock volatility using verbal and vocal cues. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 390401. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. 2023. AudioPALM: large language model that can speak and listen. arXiv preprint arXiv:2306.12925. Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. 2022. When flue meets flang: Benchmarks and large pre-trained language model for financial domain. arXiv preprint arXiv:2211.00083. Yi Su, Jisheng Bai, Qisheng Xu, Kele Xu, and Yong Dou. 2025. Audio-language models for audio-centric tasks: survey. arXiv preprint arXiv:2501.15177. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. PandaGPT: One model to instruction-follow them all. In Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants!, pages 1123. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, MA Zejun, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy Chen. 2024. AudioBench: universal benchmark for audio large language models. arXiv preprint arXiv:2406.16020. Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Yu Philip. 2023a. Multimodal large In 2023 IEEE Interlanguage models: survey. national Conference on Big Data (BigData), pages 22472256. IEEE. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023b. BloombergGPT: large language model for finance. arXiv preprint arXiv:2303.17564. Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al. 2025. Finben: holistic financial benchmark for large language models. Advances in Neural Information Processing Systems, 37:9571695743. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. PIXIU: comprehensive benchmark, instruction dataset and large language model for finance. Advances in Neural Information Processing Systems, 36:3346933484. Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, et al. 2024. Open-finllms: Open multimodal large language models for financial applications. arXiv preprint arXiv:2408.11878. Linyi Yang, Tin Lok James Ng, Barry Smyth, and Riuhai Dong. 2020. Html: Hierarchical transformerbased multi-task learning for volatility prediction. In Proceedings of The Web Conference 2020, pages 441451. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, et al. 2024. AIR-Bench: Benchmarking large audio-language models via generative comprehension. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1979 1998. Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023. InvestLM: large language model for investment using financial domain instruction tuning. arXiv preprint arXiv:2309.13064. Yang Yu and Jordan W. Suchow. 2022. Deep tensor factorization models of first impressions. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1575715773. Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. 2024. MMLLMs: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with bert. In International Conference on Learning Representations."
        },
        {
            "title": "A Dataset Source",
            "content": "In section 3.2, we outline the statistical information for the datasets used in evaluation. Below, we provide detailed description of the original datasets. There are two datasets consisting of short financial audio clips: MDRM (Qin and Yang, 2019): This is the first dataset to use multimodal financial data for stock volatility and movement prediction. MDRM includes both audio recordings and text transcripts from the 2017 earnings calls of 500 major companies listed on the S&P 500 and traded on U.S. stock exchanges. There are total of 280 companies with 572 earnings conference calls. The author further segmented earnings conference calls into individual sentences based on the text and conducted corresponding audio segments. There are total of 88,829 audio clips. We split the dataset into training and test sets using 75%25% ratio, with the test portion utilized as the evaluation set for the benchmark. SPGISpecch (ONeill et al., 2021): The dataset comprises 5,000 hours of earnings conference call recordings. The original audio is segmented by sentence and professionally transcribed into structured format. Each clip is between 5s and 15s long. The SPGISpeech dataset is originally split into training and test subsets; we utilize only the test subset for evaluation. The combined test sets from the two datasets described above contain total of 61,549 clips. This scale is sufficient for the evaluation of model performance. The corresponding training sets can be utilized for further supervised fine-tuning of AudioLLMs. Additionally, there are two other publicly available earnings conference call datasets containing raw audio recordings, which can be leveraged for the long financial audio ASR task. Earnings-21 (Del Rio et al., 2021): The dataset comprises 44 complete earnings conference call recordings from 2021. distinguishing characteristic of Earnings-21 is its emphasis on named entities, financial jargon, and numerical data, which often pose difficulties for ASR models. The aligned transcripts are further enhanced with detailed formatting and entity tags, making it possible to conduct granular error analysis. Earnings-22 (Del Rio et al., 2022): This is updated version of Earnings-21. It has 125 audio samples with total of 120 hours. Both datasets were utilized for evaluating model performance."
        },
        {
            "title": "B Instruction Set",
            "content": "As described in Section 5.3, we established an instruction set comprising 10 distinct prompts to evaluate the robustness of AudioLLMs. These prompts convey the same meaning. The detailed prompts are listed as follows: (1) Convert the audio speech into text transcript. (2) Transcribe the spoken words into written form. (3) Listen to the audio and provide the text version. (4) Transform the speech into text document. (5) Capture the spoken language and convert it to text. (6) Decode the audio and give me the written transcription. (7) Recognize the verbal communication and transcribe it into text. (8) Turn the speech input into text transcription. (9) Process the audio speech and provide the text output. (10) Translate the spoken conversation into written text. To assess the robustness of AudioLLMs to variations in input instructions, we randomly select instruction from the above instruction set for each audio input. This approach closely simulates realworld scenarios, where user commands often differ in wording yet convey the same intent."
        },
        {
            "title": "C Error Analysis",
            "content": "The experimental results demonstrate that ASR performance significantly influences downstream tasks, such as summarization. Through detailed analysis of the outputs, we identified the following major error categories: Financial terminology error: Financial audio frequently contains specialized financial proper nouns, leading AudioLLMs to produce translation errors. For example, the company name NextEra Energy\" was incorrectly transcribed as Era Energy.\" Numerical information error : Financial audio typically includes extensive numerical information, such as monetary amounts. During transcription, AudioLLMs frequently encounter issues like digit inconsistencies or missing monetary units."
        }
    ],
    "affiliations": [
        "Augusta University",
        "Columbia University",
        "Stevens Institute of Technology",
        "The Fin AI",
        "The University of Manchester",
        "University of Montreal"
    ]
}