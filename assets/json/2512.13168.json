{
    "paper_title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "authors": [
        "Haoyu Dong",
        "Pengkun Zhang",
        "Yan Gao",
        "Xuanyu Dong",
        "Yilin Cheng",
        "Mingzhe Lu",
        "Adina Yakefu",
        "Shuxin Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management. We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work. We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 8 6 1 3 1 . 2 1 5 2 : r FINCH: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows FINCH members finworkbench@gmail.com Figure 1: Real-world F&A work is messy, spanning heterogeneous and large-scale artifacts such as spreadsheets and PDFs. Its also long-horizon and knowledge-intensive: workflows interleave multiple tasks and span diverse domains such as budgeting, trading, asset management, and operations."
        },
        {
            "title": "Abstract",
            "content": "We introduce finance & accounting benchmark (FINCH) for evaluating AI agents on real-world, enterprise-grade professional workflowsinterleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. FINCH is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management. We propose workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domainexpert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work. We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents. Dataset: https://huggingface.co/FinWorkBench Full author list: Appendix A. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Frontier AI systems are increasingly transforming professional workspaces. AI-assisted tools like ChatGPT [41], Claude [2], Gemini [21], and Copilot [39] are now embedded in daily enterprise workflowshelping professionals draft documents, explore data, manipulate spreadsheets, and generate reports. These tools are particularly impactful in finance and accounting (F&A), highstakes, knowledgeand labor-intensive domain critical to every organization. However, real-world F&A work is inherently messy with substantial contextual complexity: artifacts are interconnected across heterogeneous spreadsheets, PDFs, and more, evolving through multiple versions with collaborative edits [28]; spreadsheets contain large, complex structures [17] with cross-sheet references, intricate layouts, inconsistent formatting, cryptic terms, erroneous formulas, and multimodal artifacts such as charts, images, and code. It is also long-horizon [42]: workflows demand multi-step reasoning spanning data entry, editing, retrieval, calculation, modeling, validation, reporting, and more. This raises key question: Can todays frontier AI agents actually handle the messy, long-horizon, and knowledge-intensive workflows that professionals face daily? To answer this, we introduce FINCH, an enterprise-grade F&A benchmark sourced from authentic enterprise environments. FINCH captures the intrinsic complexity of professional work through: In-the-wild enterprise sourcing: FINCH is built around authentic enterprise spreadsheets, emails, and PDFs from real-world enterprise workspacesprimarily Enron [19] (about 15,000 spreadsheet files and 500,000 emails from 150 executives and employees) and EUSES [20] (about 450 financial spreadsheets from various sources), along with securities and asset management firms, global organizations such as World Bank [4], and Canadian and British governments [11, 24]. Documents are large, cross-referenced, and messy containing rich multimodal artifacts such as tables, formulas, charts, pivots, and images. Rigorous construction process: We propose novel workflow construction pipeline grounded in real collaborative context of emails and versioned artifacts. We induce workflows from enterprise email threads and attachments, where collaborators naturally describe, discuss, and track workflows as part of their daily work. And we propose an LLM-assisted, expert-verified method to derive workflows by analyzing changes across versioned spreadsheets, surfacing the underlying goals that drive professionals work. Annotators must reason over large multi-sheet workbooks, and subtle version deltas to infer what the original analyst was trying to achieve, making the annotation process substantially more difficult than curating QA pairs over isolated tables. We compile 172 meticulously annotated, enterprise-grade workflows built on 1,710 spreadsheets, along with PDFs and other artifacts, collectively capturing the compositional, messy, knowledgeintensive, and collaborative nature of real work. Each workflow spans one or more interdependent tasksdata entry, editing, retrieval, calculation, modeling, validation, translation, visualization, and reportingmirroring how professionals actually work on artifact manipulation and creation. Figure 2: Model pass-rate comparison on FINCH workflows. Bars show overall workflow success rates for product-side agents and API-based models. Detailed settings can be found in Section 3. We evaluate spectrum of frontier AI systemsincluding Claude Sonnet 4.5, GPT 5.1, Gemini 3, Grok 4, and Qwen 3using both expert evaluation and novel automated evaluation pipeline that closely aligns with expert judgments. Our experiments reveal that even the frontier agents pass only fewer than 40% of workflows (GPT 5.1 Pro spends 16.8 minutes per workflow on average), highlighting the substantial challenges that FINCH poses for AI agents. Comprehensive case analyses further surface concrete challenges that real-world enterprise workflows pose for AI agents. 2 Figure 3: Illustration of an end-to-end predictive modeling workflow commonly performed by financial analysts. The workflow involves multiple steps, including web search, data import, crosssheet and cross-file retrieval, calculation and financial modeling, visualization, and report generation. More illustrative examples in FINCH are presented in Appendix E."
        },
        {
            "title": "2 FINCH: A Real-world Finance & Accounting Workflow Benchmark",
            "content": "2.1 Dataset Construction We propose novel workflow construction pipeline grounded in the real collaborative context of emails and versioned artifacts, as illustrated in Figure 4 (ac). First, we induce workflows from enterprise email threads and versioned documents, where collaborators naturally describe, discuss, and track work as part of their daily routines. Second, we derive workflows by analyzing changes across versioned spreadsheets, surfacing the actual data transformations and analysis steps that professionals performed. Third, we leverage high-quality spreadsheets and reports: we design workflows, author task instructions, and revise these spreadsheets and reports so that they serve as the input files and reference solutions. Figure 4: Illustration of our workflow construction pipeline from real-world enterprise emails, versioned spreadsheets, and high-quality artifacts. 3 All annotated workflows from these different channels are consolidated into unified schema with consistent fields (NL instruction, input files, reference outputs), and each workflow is tagged with task types (e.g., data entry/import, structuring, validation) and business types (e.g., planning and budgeting, pricing and valuation, operations, asset management). Note that the reference outputs include both file-based reference answers (for most generation/editing cases) and textual reference answers (for small number of QA and summary/visualization cases). 2.1.1 Workflow from Enterprise Email Threads We first mine enterprise email threads to surface workflows. Starting from the Enron Email Corpus, we prompt GPT-5 to identify collaborative messages that (i) explicitly state business goal (e.g., update the RAC rankings or revise the 2002 allocations) and (ii) reference one or more attached spreadsheets. For each selected thread, the model summarizes the communicative intent and articulates workflow description. We distinguish two cases. In the strongly grounded case illustrated in Figure 4 (a), both the input and the final reference artifacts for the workflow are already present as attachments in the thread (e.g., an initial ranking file and corrected version). Strongly grounded cases well motivated this benchmark, but they are relatively rare. In the partially grounded case illustrated in Figure 4 (b), the email specifies clear goal, but only some of the required artifacts are attached (e.g., just the updated schedule, or an intermediate report). Across both cases, human experts normalize conversational email text into task instructions and abstract away idiosyncratic details while preserving the business intent. For strongly grounded threads, annotators primarily verify that the attached files exactly implement the requested change without extra edits. For partially grounded threads, they either identify the missing artifacts from versioned spreadsheets and revise them to align with the described workflowcarefully avoiding the introduction of new changesor create the missing artifacts themselves, which typically requires much more effort. 2.1.2 Workflow Derivation from Versioned Spreadsheets Beyond explicit messages in email threads, we propose to discover workflows that are implicitly captured in spreadsheet version histories, as illustrated in Figure 4(c). We collect families of versioned workbooks from the Enron and EUSES repositories and apply an LLM-based differencing procedure that recognizes consecutive versions and infers the underlying workflow. For each recognized pair (or chain) of versions, we prompt GPT-5 to propose (i) one or more workflow types (e.g., date-stamped versioning, assumption updates, and error correction, data entry, structuring, and visualization) and (ii) detailed NL description of all changes. Human experts then validate and refine these LLM-induced workflow candidates. They first determine whether the proposed diffs constitute coherent and meaningful workflow rather than incidental churn. For accepted cases, they (i) rewrite the draft description into precise task instruction that describes the transformation, and (ii) edit the corresponding workbook versions so that the designated input and reference files cleanly realize the described workflow without introducing out-of-scope changes beyond the instruction. The corresponding input and reference files are thus grounded in the actual versions used in the diff, yielding workflow instances that do not rely on email context but are anchored in real enterprise spreadsheet evolution. 2.1.3 Workflow Sourced from High-quality Artifacts Finally, we curate workflows from high-quality spreadsheets and reports drawn from the Enron and EUSES corpora, various investment and securities companies, international organizations, and national governments (e.g., the World Bank and the Canadian and British governments). Domain experts decompose these finalized financial artifacts into intermediate artifacts that serve as inputs, while treating the original documents as reference outputs, and then author realistic workflow instructions. For example, valuation model from an investment firm can be turned into financial modeling task; World Bank report can be used to define data-driven summarization and visualization task; and bilingual reports from the Canadian government can be used to construct translation and consistency-checking tasks. We additionally leverage labeled samples from existing datasets: we adapt 10 financial cases from WideSearch [47] into web-search-centric workflows and extend them into multi-step calculation and visualization pipelines, and we leverage 3 examples from DABStep [18] to construct multi-source question answering workflows. 4 2.1.4 Quality Control Given the high complexity of each workflow, rigorous quality control is essential. We perform inter-annotator validation on all workflows and use the ChatGPT 5.1 Pro and Claude-Sonnet-4.5 product-side agents as secondary checkers: given the instruction and the input and reference files, the model is asked to judge whether the reference output is consistent with the instruction and whether any obvious steps are missing. LLM-based judgments are used to flag potential defects for human review. Together, these quality-control procedures yield collection of workflows whose NL instructions, input files, and reference outputs are well aligned. 2.2 Dataset Analysis FINCH comprises 172 meticulously annotated, enterprise-grade workflows that collectively capture the compositional, messy, multimodal, and collaborative nature of real finance and accounting work. Across these workflows, the corpus contains 1,710 spreadsheets together with 13 PDFs, 7 images, 3 Word documents, and additional files such as JSON, CSV, and Markdown. This mixture reflects how real analysts coordinate over heterogeneous artifacts rather than clean, single-table inputs. Figure 5: Distribution of number of tasks per workflow and task types across business types. Figure 5 summarizes the coverage of task and business types. On the task side, categories are: Calculation (119 workflows): filling in formulas or computing figures (e.g., net value). Structuring / Formatting (86): reorganizing tables (e.g., adjusting hierarchies), formatting content (e.g., font size and cell fill), and inserting/deleting rows or columns. Data Entry / Import (44): transcribing or importing data from spreadsheets, PDFs, images, or external sources into spreadsheets. Validation / Review (37): checking consistency and reconciling calculations within sheet or across sheets/files. Cross-sheet/file Retrieval (36): pulling values from multiple sheets or files into target workbook. Summary / Visualization (33): producing summaries or charts that surface key financial insights. Financial Modeling (15): extending or calibrating valuation and timing models, often via scenario and sensitivity analysis. Web Search (11): collecting financial data from the web and integrating it into spreadsheets. Translation (3): translating spreadsheets or reports while preserving structure, formatting, and layout. On the business side, workflows span reporting (48 workflows), trading and risk management (35), predictive modeling (33), operational management (36), planning and budgeting (26), pricing and budgeting (15), accounts payable/receivable (10), as well as procurement and sales (7) and asset management (3); some workflows are tagged with multiple business types. Overall, the distribution indicates that FINCH targets core finance and accounting verticals rather than curated toy tasks. 2.2.1 Task Compositionality FINCH is explicitly designed around composite workflows rather than isolated tasks. As shown in Figure 5, only 37 workflows (21.5%) are single-task; the remaining 135 (78.5%) involve multiple tasks. Importantly, each task itself typically requires substantial multi-turn reasoning: for example, 5 web search often entails many rounds of LLM calls to discover, filter, and verify evidence; crosssheet retrieval requires repeated calls to read and locate key information across multiple sheets; and calculation usually spans many formulas distributed over different rows and columns. These tasks are not independent subtasks, but are interleaved around shared spreadsheets and files. typical workflow may begin with structuring or importing raw data, proceed to cross-sheet or cross-file retrieval, and then culminate in calculations, modeling, or reporting. The distribution in Figure 5 shows that most workflows weave multiple tasks. Figure 6: Distribution of the number of sheets and cells per workflow. 2.2.2 Messiness The source files in FINCH are intentionally large, multi-sheet, and structurally complex. At the file level, 86.6% of workflows involve more than one file when counting both input and reference artifacts, and some workflows touch up to 14 distinct files. At the spreadsheet level, 92.4% of workflows involve multiple sheets, with an average of 8 sheets and long tail reaching 91 sheets. As result, systems must navigate cross-sheet dependencies, hidden logic, and scattered intermediate calculations rather than operating on single analysis sheet. Moreover, most spreadsheets exhibit complex layouts that interleave text, tables, formulas, and charts, as well as intricate table structures with merged cells, nested headers, hierarchical data, blank rows and columns, and other irregularities. Cell-level statistics further highlight the scale of the data. The median workflow covers 15K cells (157K on average for all workflows), with the largest workbook scaling to 3.7 million cells. Formula density is similarly skewed: while workflows contain an average of 21.5K formulas (the median is 212), the most complex ones have hundreds of thousands of formulas, reflecting deeply nested calculations and long dependency chains. Taken together, these properties create challenging regime in which models must reason over large, noisy, and highly irregular spreadsheet layouts, rather than clean, rectangular tables. 2.2.3 Multimodality Although FINCH is spreadsheet-centric, the workflows are inherently multimodal. Around 10.5% of workflows link spreadsheets with additional non-spreadsheet artifacts such as PDFs, Word documents, and images, and 7.6% explicitly require reasoning over PDFs or images. Within the spreadsheets themselves, 20.3% of workflows include charts and 2.3% feature pivot tables, so models must understand not only raw cell values but also derived visual summaries and explicit aggregation structures (and most workflows involve implicit aggregation structures). This multimodal, crossartifact structure stands in contrast to prior benchmarks that operate purely on isolated tables, and better reflects the environments in which enterprise finance and accounting tasks actually occur. 2.3 Evaluation Method 2.3.1 Human Evaluation We conduct human evaluation on all workflows to directly assess model performance. For each workflow, annotators read the NL instruction, inspect the input, reference, and model output files side by side (typically by aligning spreadsheets or documents in adjacent tabs), and determine whether the model has faithfully completed the requested job. workflow is marked as successful only if 6 the model generates or revises the content and structure in accordance with the instruction and no critical errors, omissions, or unintended changes are introduced; otherwise, it is labeled as failure. Importantly, evaluation is based on whether the instruction has been satisfactorily fulfilled rather than on purely mechanical comparison between model and reference outputs, since there may be multiple acceptable solutions for summarization, visualization, formatting, formulas, and related aspects. To reduce subjectivity and ambiguity, annotators ultimately assign binary pass/fail label for each workflow. These human judgments serve as the gold standard for measuring model performance and for validating the reliability of our automatic evaluation method. Figure 7: Illustration of our automated evaluation pipeline. Here, diff_ref denotes the diff between the input file and the reference output, and diff_model denotes the diff between the input and the model output. We categorize all workflows into file modification, file generation, and file QA. This categorization is independent of the task types in Section 2.2; for example, calculation task may generate new file, modify an existing one, or simply return textual answer. 2.3.2 LLM-as-Judge Evaluation To scale evaluation, we employ an LLM-as-judge framework that supports the three high-level task types in FINCH: modify (editing input artifacts), generate (creating new files such as workbooks and documents), and QA (answering questions based on one or more artifacts). The framework accepts heterogeneous inputsincluding .xlsx, .txt, .docx, .md, .pdf, and imagesand normalizes them into sequence of textual inputs and screenshot images for the judge model. For modification tasks, especially on spreadsheets, the framework computes structured diffs between the input and the reference output (diff_ref) and between the input and the model output (diff_model), and then builds compact input snapshot (snapshot) that, for each modified sheet, retains only the first and last ten rows and the first five columns (which typically capture table headers and layout) together with rows and columns that contain edited cells. This preserves the crucial context for diff_ref and diff_model while dramatically reducing token length. In parallel, the framework renders screenshots (screenshot) of sheets containing changes from the input, reference, and model output, so that the judge perceives merged cells, conditional formatting, charts, and other layout-sensitive properties that are difficult to encode as text alone. For generation tasks involving spreadsheets, the framework extracts all cell values and formulas from both the reference and the model output, and captures screenshots of every sheet, since the entire generated artifact must be verified rather than just localized edits. For QA tasks, it feeds rich text-and-image sequence constructed from the reference answer and the models response, optionally augmented with relevant input files when the question requires grounding in external artifacts. We design three task-specific judge prompts for modify, generate, and QA, respectively, but they share common evaluation rubric. In all cases, the judge is instructed to focus on (i) completeness with respect to the NL instruction, (ii) numerical and logical correctness of derived values and formulas, (iii) the over-edit avoidance, penalizing unnecessary and unexpected changes of the workbook beyond instruction, and (iv) readability of the formatting and structure. Exact cell-by-cell equality with the reference is not required when multiple solutions are acceptable (e.g., alternative layouts, equivalent 7 formulas, or different but semantically equivalent summaries); instead, the judge decides whether the model has satisfactorily fulfilled the instruction. To reduce subjectivity, the judge outputs binary score (pass/fail) along with short NL rationale. In some web search tasks, the rubric permits small tolerance bands when comparing values, allowing for discrepancies in data from different sources. This LLM-as-judge framework not only automates large-scale evaluation but also surfaces subtle spreadsheet errors (such as formulas silently replaced with static values) that are difficult to catch with GUI-based human inspection alone. In Section 3.2, we report the consistency between human and automated evaluations and show that the LLM-as-judge scores closely track human judgments."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Agents and Models 3.1.1 Product-side Agents We evaluate two frontier product-side agents: (i) ChatGPT using the GPT 5.1 model in Pro mode, and (ii) Claude using the Sonnet 4.5 model in thinking mode. We focus on these two systems rather than alternatives such as Gemini or Grok because they natively support returning downloadable files (e.g., spreadsheets) as outputs, rather than emitting code or markdown-formatted tables that are not intuitive for human evaluation. For both agents, we enable their external web browsing, but disable using historical chats so that each workflow is evaluated independently and without cross-run leakage. Since model updates are frequent and manual evaluation is very time-consuming, we used the latest model from our final round of experiments and did not consider subsequent updates. 3.1.2 API-based Models We evaluate five frontier large language models (LLMs), shown in Table 1, through API interfaces. Our evaluation pipeline builds upon the open-source SpreadsheetBench framework [37]. Details of the execution paradigm and prompting strategies are deferred to the Appendix B.1. While SpreadsheetBench provides only baseline implementation, we introduce several practical enhancements to improve its applicability to real-world tasks. Model Provider Context Max Output Vision Native PDF GPT 5.1 [40] Claude Sonnet 4.5 [3] Grok 4 [49] Qwen 3 Max [52] Gemini 3 Pro Preview [22] 400K 1M 256K 256K 1.05M Table 1: API-based model configurations. Context and output limits are measured in tokens. Vision indicates native image input support, while Native PDF refers to direct PDF file ingestion via the providers API without explicit text extraction. Available via long-context beta API mode. OpenAI Anthropic xAI Alibaba Google 128K 64K 256K 32.8K 65.5K Spreadsheet Encoding. SpreadsheetBench only produces simple text tables without preserving cell addresses, data types, or formulas. However, these details are essential for tasks in FINCH. We extend SpreadsheetLLM [17] encoding and introduce semantic-rich tuple encoding that preserves full structural and semantic fidelity. Each sheet begins with its name and the corresponding data bounding box (e.g. ## Sheet: [name] (A1:Z100)). We then serialize the bounded region using Markdown-based format. Each cell is encoded as tuple (Address, Value, Type, Formula), where Address denotes the cell reference (e.g. A3), Type indicates the data type (T = Text, = Integer, = Float, = Date, = Boolean), and Formula records the cell formula (e.g., =SUM(A1:A10)->100). Multimodal Input Handling. We extend the framework to support multimodal inputs involving images and PDFs. For vision-capable models (GPT 5.1, Claude Sonnet 4.5, Grok 4, and Gemini 3 Pro), we use each providers official multimodal API to transmit visual inputs alongside text prompts. For PDF documents, we adopt tiered strategy. Models with native PDF supportGPT 5.1, Claude Sonnet 4.5, and Gemini 3 Pro Previewdirectly ingest PDF files via their file upload interfaces, enabling analysis of both textual and visual elements without pre-extraction. For Grok 4, which lacks 8 native PDF support, we extract text using PyMuPDF and include it in the pdf_content field. For Qwen 3 Max, which lacks multimodal support entirely, both image and PDF content are converted to textual descriptions. While this fallback retains semantic cues, it loses layout and visual context. Context Management. To handle large spreadsheets that may exceed model context limits, we implement automatic truncation. We reserve 32K tokens for model outputsufficient for comprehensive code generation and analysis while remaining within the output limits of all evaluated models. Truncation is triggered when input exceeds the remaining capacity, removing content from the end of spreadsheet data with an explicit notice appended to inform the model of data loss. 3.2 Experimental Results Web agents (ChatGPT 5.1 Pro vs. Claude Sonnet 4.5). As shown in Figure 2 and Table 3, ChatGPT 5.1 Pro achieves the best overall pass rates on FINCH. Their advantage largely comes from rich interactive affordances: they can iteratively inspect spreadsheets, revise intermediate states, and recover from partial errors over many tool calls. However, it still solves fewer than 40% of the FINCH workflows, suggesting that real-world finance and accounting work remains far from solved even for frontier agents. The detailed analysis in Figure 8 further highlights that long-horizon composition is key bottleneck: when workflow contains more than two tasks, the pass rate drops sharplyGPT 5.1 Pro decreases from 44.3% (workflows with 2 tasks) to 23.5% (workflows with > 2 tasks), and Claude Sonnet 4.5 decreases from 30.3% to 11.8%. This indicates that error accumulation across steps and missing intermediate affordances disproportionately hurt multi-step execution. Pass rate also varies substantially by task type (Figure 8). Data Entry / Import and Structuring / Formatting are consistently among the most challenging categories, which aligns with FINCH spreadsheets exhibiting messy layouts, irregular tables, and nontrivial structural constraints. Moreover, Data Entry / Import workflows are frequently entangled with web search or PDF parsing, introducing multimodal dependencies that amplify failure modes. Notably, Translationa task where modern LLMs typically excel in standard NLP settingsperforms surprisingly poorly in FINCH. In financeheavy tables, translation can easily distort or drop critical structure and layout cues (e.g., header hierarchies, row/column alignment), and large grids make omissions more likely, leading to systematic failures. Detailed error analysis can be found in Section 3.3. # Tasks per workflow # Workflows Pass Rate (%) Avg. time (min) 1 2 3 4 5 37 84 33 10 8 48.6 42.4 33.3 0 12.5 13.1 17.4 18.7 21.3 13.6 Table 2: Average GPT 5.1 Pro completion time across workflows with different numbers of tasks. We further analyze how GPT 5.1 web completion time scales with the number of tasks in workflow  (Table 2)  . The longest individual workflow run takes roughly 60 minutes to completebut fails, highlighting how challenging workflows can be for current agents. On average, single-task workflows take 13.1 minutes, while workflows with two, three, and four tasks require 17.4, 18.7, and 21.3 minutes, respectively, reflecting the increased compositional complexity of multi-task settings. Interestingly, five-task workflows show lower average time (13.6 minutes), most of which involve web search and almost all of which result in failure. 9 Figure 8: Pass rate comparison for GPT 5.1 Pro and Claude Sonnet 4.5 across different task combinations and task types. The left chart visualizes the aggregated pass rate based on task combinations, revealing the models capabilities in handling multi-step workflows commonly seen in professional finance and accounting tasks. The in the right chart represents the number of tasks included in workflow. For example, \"k=3\" workflow involves three distinct tasks, and its pass rate is calculated based on the collective performance of those tasks. The right chart shows the pass rates for individual tasks performed by both models in the FINCH benchmark. For workflows that involve multiple tasks (i.e., composite workflows), the correctness is counted for each individual task, as it is difficult to isolate which specific task caused the failure. API-based. Figure 2 further shows that our API-based method remains competitive despite using only single LLM call. In automated evaluation, for example, GPT 5.1 Pro achieves 41.9%, while GPT 5.1 with our API-based agent design reaches 32.0%a relatively small gap given the much weaker interaction budget. Our agent design narrows the gap by supplying more efficient spreadsheet representations and task-appropriate tool outputs within the single-call budget, thereby reducing brittle reasoning over complex layouts and mitigating cascading mistakes. 3.2.1 Consistency Between Human and Automated Evaluation We adopt lightweight multimodal model, GPT-5-mini, as the judge to validate the performance of our automated evaluation framework. As shown in Table 3, the automated evaluation largely aligns with human judgments: for GPT 5.1 Pro and Claude Sonnet 4.5, the judge agrees with human labels on 82.1% and 90.2% of workflows, respectively. The judge also achieves high recall (83.3% and 88.4%), meaning it recovers most human-labeled passes, and reasonably strong precision (73.6% and 76.0%), indicating that the majority of automatically predicted passes are also accepted by human evaluators. Overall, this suggests that automated evaluation may overestimate accuracy by several percentage points. Table 3: Comparison of human and automated evaluation on GPT and Claude product-side agents. Automated Eval shows pass counts/rates under the LLM-as-judge framework. Agreement w/ Human Eval reports how well the automated judgments match human labels: accuracy (Acc), recall (human-pass recall), and precision (human-pass precision). Human Eval Automated Eval Agreement w/ Human Eval Model (Product) Pass Pass Rate (%) Pass Pass Rate (%) Acc (%) Recall (%) Precision (%) GPT 5.1 Pro Claude Sonnet 4.5 72/172 50/172 41.9 29.1 66/172 43/172 38.4 25. 82.1 90.2 83.3 88.4 73.6 76.0 On the model side, the LLM judge can occasionally miss nuances in the rubriceither failing to catch subtle visual or numerical errors in large spreadsheets or, conversely, being overly literal about certain instructions (e.g., penalizing benign formula-to-constant conversions). However, we also observe cases where the LLM-based judge is correct but human raters are wrongfor example, when formulas are silently replaced with static values, which are difficult to detect through GUI-based inspection alone. On the system side, limitations of our spreadsheet tooling and data pipeline (e.g., incomplete support for corrupted but human-readable workbooks or uncommon file formats) can 10 cause valid outputs to be marked as failures. Taken together, these factors mean that our automated scores should be interpreted as approximate rather than exact, and that human review remains important for borderline or high-impact workflows. 3.3 Error Analysis To understand the sources of failure on FINCH, we conducted qualitative error analysis of GPT 5.1 Pro and Claude Sonnet 4.5 in both their web-agent and API configurations. For all failed workflows in our evaluation, we manually inspected the trajectories and annotated the primary cause of failure. From workflow-centric perspective, we identify five dominant categories of error.2 Take Claude Sonnet 4.5 web-agent as an example. Across all examined failures, 10% stem from task misunderstanding: enterprise tasks often rely on implicit context in enterprise artifacts (e.g., spreadsheets), which models frequently overlook, leading them to misinterpret what is being asked and the required deliverable. 25% are data retrieval errors, including selecting the wrong cross-sheet, cross-table, or intra-table row/column ranges. 35% arise from formula reasoning errors, such as failing to reconstruct the latent business logic encoded in formulas or deriving incorrect new formulas. 25% are due to code generation errors, where generated scripts (e.g., Python with spreadsheet APIs) are syntactically invalid or misaligned with the spreadsheet layout. The remaining 5% correspond to data rendering errors, including incorrect formatting, misconfigured charts, or flawed final reports that deviate from the requested layout or narrativefor example, creating brand-new spreadsheet instead of modifying the original one as requested. We also compare error patterns between web-based agents and API-based setups, with details provided in Appendix C. Notably, all of these error types correspond to generic capabilities that modern LLMs already appear to master on many existing benchmarks. The question, then, is why these ostensibly strong base abilities degrade so sharply on FINCH. Our analysis points to five intertwined properties of real-world enterprise Finance & Accounting workflows that make failures more likely and more catastrophic. First, FINCH workflows routinely involve large, fragmented spreadsheet ecosystems: dozens of interlinked workbooks and thousands of rows distributed across many sheets. Executing these workflows accurately requires long-range cross-sheet navigation and precise referencing, which substantially increases the likelihood of small retrieval errors. Second, the content is dense and semantically homogeneous: many cells contain domain-specific financial concepts that are subtly different yet lexically similar (e.g., variants of revenue/expense items, adjusted vs. unadjusted metrics), making entity disambiguation and cell grounding unusually difficult. Third, the table layouts and structures are complex and often irregular, including multi-level headers, merged cells, nested subtotals, and bespoke layouts that force the model to infer structure from noisy contents and ad hoc formatting. For example, at the code level, even tiny misinterpretations of these layouts (e.g., off-by-one errors when specifying ranges) can then propagate into globally incorrect outputs, especially when logic is applied in batch across many such sheets. Fourth, formulas encode latent structure and logic. In the FINCH dataset, each sheet contains large number of formulas that encode latent business logic, temporal assumptions, and fine-grained dependencies that are not visible from displayed values alone; yet models typically prioritize cell values and under-use formulas, leading to systematic misinterpretations. For example, in pricing sheet with the column header IF NGPL MidContinent index (@ Baker), the apparent semantics from the header alone suggest daily exposure metric. However, inspecting the associated formula (25 * V21 + C41 * C22) reveals that the column in fact encodes 55-day payment timing. Models that ignore or under-utilize formulas systematically misinterpret such columns roles in downstream calculations, and this misinterpretation then propagates through subsequent steps. Finally, many workflows involve multimodal artifacts and chat-centric tasks such as combining spreadsheets with PDFs, charts, and screenshots requiring the agent to jointly reason over heterogeneous formats. For example, tables embedded in PDFs are often only partially referenced, with key entries missing or truncated. Many of these factors have been examined in prior work (e.g., multi-spreadsheet settings, complex table structures, formula reasoning, and multi-step workflows), and state-of-the-art models can perform reasonably well on benchmarks that emphasize limited subset of these factors. In FINCH, however, these factors co-occur within the same workflow in real-world enterprise data, and our results suggest this COMPOSITION is what drives the sharp performance drop. FINCH does not 2For this study, once we identify the first clear error in failed workflow, we stop further analysis for that workflow. 11 demand fundamentally new abilities; rather, it probes these abilities under an enterprise extreme regime of high complexity, noise, and long-horizon dependenciesclosely mirroring real Finance & Accounting work. Progress on compositional capability, therefore, requires training and evaluation on long-running, computationand reasoning-intensive workflows over large, messy multimodal enterprise artifacts."
        },
        {
            "title": "4 Related Work",
            "content": "The integration of LLMs into enterprise productivity tools has accelerated dramatically recently. The recently launched ChatGPT Agent [41] extends these capabilities to autonomous task completion, enabling multi-step workflows across web browsing, code execution, and spreadsheet manipulation. Microsoft Copilot [39] embeds AI capabilities across the Microsoft 365 suite, enabling users to draft documents, analyze spreadsheets, and automate workflows through natural language interaction. Similarly, Google has integrated Gemini [21] into Google Workspace, providing AI-assisted features in Docs, Sheets, and Gmail. Anthropics Claude Excel has also entered the enterprise space with spreadsheet automation capabilities [2], while remarkable tools like Shortcut AI [1] focus specifically on AI-powered spreadsheet manipulation. The emergence of agentic AI systems marks significant shift from understanding and QA to autonomous task completion [30, 7, 37]. However, there are long-standing challenges such as messy inputs and multimodal processing. SpreadsheetLLM [17] introduces novel encoding and compression methods to help LLMs understand large and messy spreadsheet structures, and further addresses this challenge through spreadsheet post-training. Beyond structural understanding, multimodal processing remains challenging for spreadsheet AI systems. On the formatting front, early work explored neural approaches for table formatting [15]. Recent advances in formula generation have progressed from pretraining with numerical reasoning [9] to natural language-driven formula synthesis [54], contrastive learning-based recommendation [6], and interactive formula prediction through hierarchical expansion [23]. Recent years have seen significant progress in benchmarks for financial reasoning [8, 27, 5, 51, 35, 10, 44, 29, 36, 42, 25, 53], spreadsheet reasoning [30, 7, 37, 26, 33, 13, 56, 17, 48, 34, 16, 12, 31, 54, 43, 45], and multimodal document [38], table [32, 55], chart [46], table-chart [14], and spreadsheet [50, 13] reasoning, driving advances in LLM-based agents for enterprise tasks. However, FINCH proposes new benchmark for the messy artifacts and long-horizon workflows in wild F&A enterprise settings."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced FINCH, new benchmark for real-world F&A enterprise workflows. FINCH combines workflows induced from enterprise email threads, version histories of spreadsheets, and high-quality financial artifacts with rigorous expert annotation and calibrated LLM-as-judge framework, enabling systematic evaluation of agents on diverse workflows that operate over large, messy, and multimodal enterprise artifacts and require long-horizon, spreadsheet-centric reasoning. Our experiments show that even the strongest frontier systems pass fewer than 40% of workflows after spending 48 hours on the benchmark, revealing substantial gap between current AI capabilities and the demands of real enterprise practice. We hope FINCH will serve as foundation for developing agents to tackle real, messy and long-horizon professional work."
        },
        {
            "title": "References",
            "content": "[1] Shortcut AI. Shortcut ai for spreadsheets. https://www.tryshortcut.ai/, 2024. [2] Anthropic. Claude for excel. https://claude.com/claude-for-excel, 2025. [3] Anthropic. Introducing claude sonnet 4.5. https://www.anthropic.com/news/claude-s onnet-4-5, 2025. Accessed: 2025-12-14. [4] World Bank. International Debt Report 2024. World Bank, Washington, DC, 2024. World Banks annual publication on external debt statistics. [5] Antoine Bigeard, Langston Nashold, Rayan Krishnan, and Shirley Wu. Finance agent arXiv preprint benchmark: Benchmarking llms on real-world financial research tasks. arXiv:2508.00828, 2025. [6] Sibei Chen, Yeye He, Weiwei Cui, Ju Fan, Song Ge, Haidong Zhang, Dongmei Zhang, and Surajit Chaudhuri. Auto-formula: Recommend formulas in spreadsheets using contrastive learning for table representations. Proceedings of the ACM on Management of Data, 2(3):127, 2024. [7] Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni, Jianye Hao, Hangyu Mao, and Fuzheng Zhang. Sheetagent: towards generalist agent for spreadsheet reasoning and manipulation via large language models. In Proceedings of the ACM on Web Conference 2025, pages 158177, 2025. [8] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. Finqa: dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, 2021. [9] Zhoujun Cheng, Haoyu Dong, Ran Jia, Pengfei Wu, Shi Han, Fan Cheng, and Dongmei Zhang. Fortap: Using formulas for numerical-reasoning-aware table pretraining. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11501166, 2022. [10] Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, et al. Finagentbench: benchmark dataset for agentic retrieval in financial question answering. In Proceedings of the 6th ACM International Conference on AI in Finance, pages 632637, 2025. [11] Department of Finance Canada. Fiscal reference tables, november 2025. Technical report, Government of Canada, Ottawa, Canada, 2025. Provides annual data on the financial position of the federal, provincial-territorial and local governments. [12] Haoyu Dong, Yue Hu, and Yanan Cao. Reasoning and retrieval for complex semi-structured tables via reinforced relational data transformation. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1382 1391, 2025. [13] Haoyu Dong, Shijie Liu, Shi Han, Zhouyu Fu, and Dongmei Zhang. Tablesense: Spreadsheet table detection with convolutional neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6976, 2019. [14] Haoyu Dong, Haochen Wang, Anda Zhou, and Yue Hu. Ttc-quali: text-table-chart dataset for multimodal quantity alignment. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 181189, 2024. [15] Haoyu Dong, Jinyu Wang, Zhouyu Fu, Shi Han, and Dongmei Zhang. Neural formatting for spreadsheet tables. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 305314, 2020. [16] Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, and Guolin Ke. Machinelearninglm: Scaling many-shot in-context learning via continued pretraining. arXiv preprint arXiv:2509.06806, 2025. 13 [17] Haoyu Dong, Jianbo Zhao, Yuzhang Tian, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, Jos√© Cambronero, Yeye He, Shi Han, et al. Spreadsheetllm: encoding spreadsheets for large language models. arXiv preprint arXiv:2407.09025, 2024. [18] Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, and Thomas Wolf. Dabstep: Data agent benchmark for multi-step reasoning. arXiv preprint arXiv:2506.23719, 2025. [19] EnronData.org. Edo enron email pst dataset. https://enrondata.readthedocs.io/e n/latest/data/edo-enron-email-pst-dataset/. Creative Commons Attribution 3.0 United States License. To provide attribution, please cite to EnronData.org.. [20] Marc Fisher and Gregg Rothermel. The euses spreadsheet corpus: shared resource for supporting experimentation with spreadsheet dependability mechanisms. In Proceedings of the first workshop on End-user software engineering, pages 15, 2005. [21] Google. Gemini for google workspace. https://workspace.google.com/solutions/ai/, 2024. [22] Google DeepMind. Gemini 3 pro. https://deepmind.google/models/gemini/pro/, 2025. Accessed: 2025-12-14. [23] Wanrong He, Haoyu Dong, Yihuai Gao, Zhichao Fan, Xingzhuo Guo, Zhitao Hou, Xiao Lv, Ran Jia, Shi Han, and Dongmei Zhang. Hermes: Interactive spreadsheet formula prediction via hierarchical formulet expansion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 83568372, 2023. [24] HM Treasury. Public expenditure statistical analyses 2023. Technical report, HM Treasury, London, United Kingdom, 2023. UK public expenditure statistical release (PESA). [25] Liang Hu, Jianpeng Jiao, Jiashuo Liu, Yanle Ren, Zhoufutu Wen, Kaiyuan Zhang, Xuanliang Zhang, Xiang Gao, Tianci He, Fei Hu, et al. Finsearchcomp: Towards realistic, expert-level evaluation of financial search and reasoning. arXiv preprint arXiv:2509.13160, 2025. [26] Amila Indika and Igor Molybog. Sodbench: large language model approach to documenting spreadsheet operations. arXiv preprint arXiv:2510.19864, 2025. [27] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. [28] Bryan Klimt and Yiming Yang. The enron corpus: new dataset for email classification research. In European conference on machine learning, pages 217226. Springer, 2004. [29] Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Kp Subbalakshmi, Jimin Huang, et al. Investorbench: benchmark for financial decision-making tasks with llm-based agent. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25092525, 2025. [30] Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhao-Xiang Zhang. Sheetcopilot: Bringing software productivity to the next level through large language models. Advances in Neural Information Processing Systems, 36:49524984, 2023. [31] Jinyang Li, Nan Huo, Yan Gao, Jiayi Shi, Yingxiu Zhao, Ge Qu, Yurong Wu, Chenhao Ma, Jian-Guang Lou, and Reynold Cheng. Tapilot-crossing: Benchmarking and evolving llms towards interactive data analysis agents. arXiv preprint arXiv:2403.05307, 2024. [32] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. Tablebank: Table benchmark for image-based table detection and recognition. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 19181925, 2020. [33] Peng Li, Yeye He, Cong Yan, Yue Wang, and Surajit Chaudhuri. Auto-tables: Synthesizing multi-step transformations to relationalize tables without using examples. Proceedings of the VLDB Endowment, 16(11):33913403, 2023. [34] Zheng Li, Yang Du, Mao Zheng, and Mingyang Song. Mimotable: multi-scale spreadsheet benchmark with meta operations for table reasoning. In Proceedings of the 31st International Conference on Computational Linguistics, pages 25482560, 2025. [35] Shu Liu, Shangqing Zhao, Chenghao Jia, Xinlin Zhuang, Zhaoguang Long, Jie Zhou, Aimin Zhou, Man Lan, and Yang Chong. Findabench: Benchmarking financial data analysis ability of large language models. In Proceedings of the 31st International Conference on Computational Linguistics, pages 710725, 2025. [36] Zhaowei Liu, Xin Guo, Haotian Xia, Lingfeng Zeng, Fangqi Lou, Jinyi Niu, Mengping Li, Qi Qi, Jiahuan Li, Wei Zhang, et al. Visfineval: scenario-driven chinese multimodal benchmark for holistic financial understanding. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2409924157, 2025. [37] Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, and Jie Tang. Spreadsheetbench: Towards challenging real world spreadsheet manipulation. Advances in Neural Information Processing Systems, 37:9487194908, 2024. [38] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [39] Microsoft. Microsoft 365 copilot. https://www.microsoft.com/en-us/microsoft- /copilot, 2024. [40] OpenAI. Gpt-5. https://openai.com/index/introducing-gpt-5/, 2025. Accessed: 2025-12-14. [41] OpenAI. Introducing chatgpt agent. https://openai.com/index/introducing-chatgpt -agent/, 2025. [42] Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Sim√≥n Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, et al. Gdpval: Evaluating ai model performance on real-world economically valuable tasks. arXiv preprint arXiv:2510.04374, 2025. [43] Weixuan Wang, Dongge Han, Daniel Madrigal Diaz, Jin Xu, Victor R√ºhle, and Saravan Rajmohan. Odysseybench: Evaluating llm agents on long-horizon complex office application workflows. arXiv preprint arXiv:2508.09124, 2025. [44] Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, et al. Finauditing: financial taxonomy-structured multi-document benchmark for evaluating llms. arXiv preprint arXiv:2510.08886, 2025. [45] Zilong Wang, Yuedong Cui, Li Zhong, Zimin Zhang, Da Yin, Bill Yuchen Lin, and Jingbo Shang. Officebench: Benchmarking language agents across multiple applications for office automation. arXiv preprint arXiv:2407.19056, 2024. [46] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. [47] Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, et al. Widesearch: Benchmarking agentic broad info-seeking. arXiv preprint arXiv:2508.07999, 2025. [48] Pengzuo Wu, Yuhang Yang, Guangcheng Zhu, Chao Ye, Hong Gu, Xu Lu, Ruixuan Xiao, Bowen Bao, Yijing He, Liangyu Zha, et al. Realhitbench: comprehensive realistic hierarchical table benchmark for evaluating llm-based table analysis. arXiv preprint arXiv:2506.13405, 2025. [49] xAI. Grok 4. https://x.ai/news/grok-4, 2025. 15 [50] Shiyu Xia, Junyu Xiong, Haoyu Dong, Jianbo Zhao, Yuzhang Tian, Mengyu Zhou, Yeye He, Shi Han, and Dongmei Zhang. Vision language models for spreadsheet understanding: Challenges and opportunities. In Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR), pages 116128, 2024. [51] Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al. Finben: holistic financial benchmark for large language models. Advances in Neural Information Processing Systems, 37:9571695743, 2024. [52] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. [53] Zhihan Zhang, Yixin Cao, and Lizi Liao. Xfinbench: Benchmarking llms in complex financial problem solving and reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 87158758, 2025. [54] Wei Zhao, Zhitao Hou, Siyuan Wu, Yan Gao, Haoyu Dong, Yao Wan, Hongyu Zhang, Yulei Sui, and Haidong Zhang. Nl2formula: Generating spreadsheet formulas from natural language queries. In Findings of the Association for Computational Linguistics: EACL 2024, pages 23772388, 2024. [55] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. Multimodal table understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 91029124, 2024. [56] Ruiyan Zhu, Xi Cheng, Ke Liu, Brian Zhu, Daniel Jin, Neeraj Parihar, Zhoutian Xu, and Oliver Gao. Sheetmind: An end-to-end llm-powered multi-agent framework for spreadsheet automation. arXiv preprint arXiv:2506.12339, 2025."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 FINCH: Real-world Finance & Accounting Workflow Benchmark 2.1 Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 Workflow from Enterprise Email Threads . . . . . . . . . . . . . . . . . . 2.1.2 Workflow Derivation from Versioned Spreadsheets . . . . . . . . . . . . . 2.1.3 Workflow Sourced from High-quality Artifacts . . . . . . . . . . . . . . . 2.1.4 Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Dataset Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1 Task Compositionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2 Messiness . . . . 2.2.3 Multimodality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Evaluation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 LLM-as-Judge Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Experiments 3.1 Agents and Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Product-side Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 API-based Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Consistency Between Human and Automated Evaluation . . . . . . . . . . 3.3 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Related Work 5 Conclusion Author List Experiment Details B.1 API-based Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Analysis Ethics Statement Examples E.1 Example 1 . E.2 Example 2 . E.3 Example 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2 3 4 4 4 5 5 6 6 6 6 8 8 8 8 9 11 12 12 19 19 20 20 21 21 22 E.4 Example 4 . E.5 Example 5 . E.6 Example 6 . E.7 Example 7 . E.8 Example 8 . E.9 Example 9 . . . . . . . E.10 Example 10 . E.11 Example 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 24 24 25 25"
        },
        {
            "title": "A Author List",
            "content": "Haoyu Dong* (University of Chinese Academy of Sciences) donghaoyu82@gmail.com Pengkun Zhang (South China University of Technology) sezhangpengkun@mail.scut.edu.cn Yan Gao (Zhongguancun Academy) gaoyan@zgci.ac.cn Xuanyu Dong (Harvest Fund) qianmuxuanyu@126.com Yilin Cheng (Fudan University, Zhongguancun Academy) ylcheng@m.fudan.edu.cn Mingzhe Lu (University of Chinese Academy of Sciences) lumingzhe23@mails.ucas.edu.cn Adina Yakefu (Hugging Face) adina@huggingface.co Shuxin Zheng (Zhongguancun Academy) sz@bjzgca.edu.cn *Corresponding author."
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 API-based Models Execution Paradigm. We frame the API evaluation as code generation task. Models are instructed to solve spreadsheet manipulation and generation workflows by generating executable Python scripts, which are then executed in sandboxed environment to produce output artifacts. This paradigm aligns with SpreadsheetBenchs philosophy of treating model-written code as the primary action space, but is adapted here to accommodate long-horizon, multimodal enterprise tasks. Action Space: Models generate Python code using standard libraries including openpyxl (for Excel manipulation), pandas (for data processing), matplotlib (for visualization), and scikit-learn (for statistical analysis). Output Format: Models must produce complete, self-contained Python scripts wrapped in markdown code blocks (python ... ). Sandboxed Execution: Generated code is extracted via regex parsing and executed in isolated Docker containers running Jupyter Kernel Gateway. Each container mounts the dataset volume at /mnt/data/ with 10-minute session timeout. Single-shot Protocol: We employ strict one-shot generation protocol without iterative refinementeach model produces exactly one solution per workflow. If the generated code fails to execute (e.g., due to syntax errors or runtime exceptions), the workflow is marked as failed without retry. This strict setting is designed to evaluate the models raw code generation capability under realistic deployment constraints. This unified code-as-action setting ensures that the measured performance reflects the models inherent competence on complex workflows rather than benefits derived from interactive debugging. Prompting Strategy. We employ zero-shot setting with structured system prompt comprising: 1. role definition: You are an expert who can manipulate spreadsheets through Python code. 2. detailed description of the compact spreadsheet encoding format with illustrative examples. 3. The task instruction and explicit input/output file paths. 4. Library-specific best practices (e.g., openpyxl chart creation patterns) to mitigate common code errors. 5. An explicit directive to generate Python code as the final output. 19 This structured design explicitly guides models toward generating valid, context-aligned Python code, minimizing ambiguity in task interpretation. However, for models that support reasoning traces (GPT 5.1, Gemini 3 Pro), we request explicit reasoning via the include_reasoning API parameter, enabling us to capture the models internal deliberation process for subsequent qualitative error analysis. Temperature is set to 0.7 across all models."
        },
        {
            "title": "C Detailed Analysis",
            "content": "Web agents (ChatGPT 5.1 Pro vs. Claude Sonnet 4.5). ChatGPT 5.1 Pro tends to decompose workflows into more, smaller steps, with explicit reasoning, tool calls, execution, and self-checking at each step. This leads to longer traces and noticeably higher latency, but also more opportunities for intermediate validation (e.g., sanity-checking partial results). However, the code it generates is often hidden behind tool abstractions, so our error attribution is limited to observed behavior and natural language reasoning rather than the exact implementation details. Claude Sonnet 4.5 typically uses fewer steps and produces more direct solutions. In visualization-heavy workflows, its generated charts are often both more accurate and more aesthetically polished than those produced by ChatGPT 5.1 Pro, leading to relatively fewer failures in the data visualization sub-tasks. ChatGPT 5.1 Pro and Claude Sonnet 4.5 agents can explore Excel files through many API calls within single workflow, but their encoding methods are not well-suited to spreadsheets with complex layouts and structures. Thanks to efficient encoding and appropriate tool use, the following single-call API-based method achieves pass rate that is much closer to that of product-side agents. API-based Our API-based runs are single-call: they leverage the models underlying reasoning capabilities but lack two crucial affordances that web agents exploit. (i) interleaved code execution with feedback, and (ii) explicit reflection based on intermediate tool outputs. As result, the API agents must generate the entire plan, code, and outputs within single LLM call. When their initial structural assumptions about spreadsheet are slightly off, they have no mechanism to detect or correct the mistake, leading to significantly higher error rate, particularly in categories related to schema understanding and table manipulation. Its desirable for future work to explore agentic methods with multiple round of API calls."
        },
        {
            "title": "D Ethics Statement",
            "content": "The FINCH benchmark is constructed entirely from existing, publicly available data sources. Concretely, our workflows are derived from (1) the Enron email corpora, including the parsed Enron email dataset on Kaggle (released under the CC0 Public Domain dedication) and the Enron Email Dataset from EnronData.org (licensed under CC BY 3.0 US); (2) the EUSES spreadsheet corpus and its modified variants (CC BY 4.0); and (3) diverse collection of enterprise-like artifacts, including documents from investment and securities companies, the World Bank (CC BY 3.0), Canadian and British government websites (Open Government License), and public corpora such as WideSearch (MIT license) and DABStep (CC BY 4.0). We respect the original licenses of all upstream resources and only redistribute content within the terms they allow. On top of these sources, we apply additional filtering, normalization, and expert annotation to organize spreadsheets and related documents into coherent workflows with task instructions, input files, and reference outputs. We do not introduce any new personally identifiable information. During curation, we remove obviously sensitive fields when they are not necessary for the task (e.g., personal contact information or signatures) and avoid annotating workflows whose successful completion would depend on sensitive personal attributes rather than business logic. The resulting FINCH dataset is released under the Creative Commons Attribution 3.0 United States license (CC BY 3.0 US), which permits broad reuse while requiring appropriate attribution. The language in FINCH is primarily English, reflecting the dominant language of the underlying Enron and EUSES corpora and many of the public institutional sources. Because some artifacts originate from funds and securities institutions and from Canadian government materials, small fraction of workflows include Chinese or French content."
        },
        {
            "title": "E Examples",
            "content": "E.1 Example 1 Figure 9: For this task, the model must verify the department headcount summary by cross-checking each of the 39 departments against its detailed roster sheet. It should correct discrepancies such as miscounts and missing or duplicate entries. The summary must be updated by fixing incorrect totals, removing departments that no longer exist, and adding any omitted departments. Furthermore, the underlying schema varies slightly across departments, which challenges reliable code generation. 21 E.2 Example 2 Figure 10: An example of extracting data from tables and charts in PDFs and saving it to spreadsheet. AI agents must understand the layout, parse hierarchical structures, interpret how values map to specific cells, and reconstruct formulas from aggregated values. E.3 Example Figure 11: Cross-sheet reference validation. This example is relatively easy for frontier AI agents. 22 E.4 Example 4 Figure 12: This task requires deriving the XNPV5 of the contract under different combinations of assumptions. The analysis uses contract capacity rates, plant capacity, and the specified discount rate provided in the table. While key adjustment componentsnamely the Interest Rate Adjustment, Apache Savings, and O&M Adjustmentmust be retrieved from supporting documents and applied according to each scenario (included, excluded). For each assumption set, the analyst must then construct the annual capacity payment cash flows by deriving the adjusted capacity rate, converting it into monthly and annual capacity payments, and assembling the full month-by-month cash-flow schedule. Only after these intermediate steps are completed can the cash flows be discounted to the valuation date (e.g., December 31, 2000) to compute XNPV5. E.5 Example 5 Figure 13: The sum of A&B and the equity roll-forward test require cross-sheet retrieval and calculation. 23 E.6 Example 6 Figure 14: workflow that translates French report into English while preserving its format and structure. The report contains many tables to translate, along with text, notes, and even charts. E.7 Example 7 Figure 15: Transforming table from one structure to another requires reorganizing data and retrieving information across sheets (e.g., area_info and summary). This example poses additional challenges: (1) distinguishing value-driven operators from volume-driven operators, and (2) performing aggregation and validation over the reorganized data. E.8 Example 8 16: The apparent exposure metric. semantics Figure monthly/- daily However, formula (e.g., it actually enC5=$A4*Volumes!B6*Curves!G7+25*Volumes!B7*Curves!G8) reveals that codes 55-day payment timing schedule. Models that ignore or underutilize formula information, therefore systematically misattribute the columns role in downstream computations, and this misinterpretation then propagates through subsequent steps. from the inspecting underlying headers suggest the E.9 Example Figure 17: This workflow requires creating new spreadsheet with all values converted to USD. It also requires correct in-sheet and cross-sheet formula references while preserving the original spreadsheet layout. 25 E.10 Example 10 Figure 18: Generating reports from tabular data requires financial knowledge of data analysis, financial events, and visualization. For example, one may plot two series with different units on single chart (e.g., using secondary y-axis) to reveal their correlation. E.11 Example 11 Figure 19: This Excel sheet shows an assumption-update workflow, where mix of forward contracts is used to cover monthly peak-load short positions. It lists the contract allocations and MW volumes, along with monthly peak loads and the resulting short MW. table on the right computes blended prices and portfolio costs, and the stacked chart visualizes coverage by contract type over the year."
        }
    ],
    "affiliations": []
}