{
    "paper_title": "SurveyX: Academic Survey Automation via Large Language Models",
    "authors": [
        "Xun Liang",
        "Jiawei Yang",
        "Yezhaohui Wang",
        "Chen Tang",
        "Zifan Zheng",
        "Simin Niu",
        "Shichao Song",
        "Hanyu Wang",
        "Bo Tang",
        "Feiyu Xiong",
        "Keming Mao",
        "Zhiyu li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on www.surveyx.cn"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 7 7 4 1 . 2 0 5 2 : r SurveyX: Academic Survey Automation via Large Language Models Jiawei Yang j1aweiyang@ruc.edu.cn Renmin University of China Beijing, China Yezhaohui Wang yezhaohuiwang@gmail.com Northeastern University Shenyang, China Xun Liang xliang@ruc.edu.cn Renmin University of China Beijing, China Chen Tang tangc@iaar.ac.cn Institute for Advanced Algorithms Research Shanghai, China Zifan Zheng zzhe0348@uni.sydney.edu.au The University of Sydney Sydney, Australia Shichao Song songshichao@ruc.edu.cn Renmin University of China Beijing, China Simin Niu niusimin@ruc.edu.cn Renmin University of China Beijing, China Hanyu Wang hy.wang@ruc.edu.cn Renmin University of China Beijing, China Feiyu Xiong xiongfy@iaar.ac.cn Institute for Advanced Algorithms Research Shanghai, China"
        },
        {
            "title": "ABSTRACT",
            "content": "Keming Mao maokm@mail.neu.edu.cn Northeastern University Shenyang, China Bo Tang tangb@iaar.ac.cn Institute for Advanced Algorithms Research Shanghai, China Zhiyu Li lizy@iaar.ac.cn Institute for Advanced Algorithms Research Shanghai, China Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, pre-processing method called AttributeTree, and re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality These authors contributed equally to this research. Corresponding author: lizy@iaar.ac.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. KDD 2025, August 2025, Toronto, Canada 2025 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on our project website1."
        },
        {
            "title": "KEYWORDS",
            "content": "Automated Survey Generation, Literature Synthesis, Large Language Models, NLP ACM Reference Format: Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, and Zhiyu Li. 2025. SurveyX: Academic Survey Automation via Large Language Models. In Proceedings of Proceedings of the 2025 ACM Conference on Knowledge Discovery and Data Mining (KDD 2025). ACM, New York, NY, USA, 15 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "In recent years, computer science has advanced rapidly across various fields [2, 9, 24]. Statistics indicate that arXiv.org2 receives about thousand new papers daily. Figure 1 highlights remarkable trend: over the past three years (2022-2024), the number of papers published on arXiv has increased substantially, rising from 186,339 to 285,174, representing growth of over 50%. It is projected that this number will further climb to approximately 368,292 in 2025. However, the exponential growth of literature has made it increasingly challenging for researchers attempting to comprehend the technological evolution and developmental trajectories of specific subfields 1http://www.surveyx.cn 2https://arxiv.org/ KDD 2025, August 2025, Toronto, Canada Xun Liang et al. However, there is currently lack of effective tools that can efficiently procure large volumes of the latest and highly pertinent references, thus limiting the broader application of automated survey generation. Unlike traditional Natural Language Generation (NLG) tasks, the evaluation of surveys lacks unified metrics and standardized benchmarks [38]. The absence of evaluation frameworks hampers effective quality assessment of automatically generated surveys, thereby constraining their applicability in large-scale academic settings. Existing similar works have provided some solutions to the aforementioned challenges, yet notable deficiencies persist, particularly in the following areas: Exisiting retrieval methodologies contain inherent limitations. In existing works, Wang et al. [38] only supports offline retrieval and cannot access the latest references, leading to lack of timeliness. The methods for pre-processing references are often inadequate. In existing works, Wang et al. [38] only utilizes partial information from the references (titles and abstracts), overlooking significant amount of crucial content. The generated surveys lack diversity in their expression. Existing methodologies are restricted to generating textbased surveys and fall short in including visual elements such as figures and tables, diminishing the readability of the results. To address these deficiencies, we propose SurveyX, an efficient and well organized system for automated survey generation. SurveyX divides the composing process of surveys into two phases: the Preparation Phase and the Generation Phase. In the Preparation Phase, SurveyX employs retrieval algorithms to search and filter highly relevant references from the internet based on the given survey topic. It employs reference pre-processing method, termed AttributeTree, to distill key information from the references, constructing reference materials database for efficient retrieval through the Retrieval Augmented Generation (RAG) technique. In the Generation Phase, SurveyX utilizes the information obtained in the previous phase to sequentially generate the outline and main body of the survey, ensuring that the generated survey has clear structure and accurate content. Additionally, tables and figures are incorporated to enrich the surveys presentation. Beyond the generation process, SurveyX extends the evaluation framework proposed by Wang et al. [38] by incorporating additional evaluation metrics, thus aiding subsequent related research. To sum up, our main contributions are as follows: We propose an efficient reference retrieval algorithm capable of expanding keywords based on given topic, substantially broadening the retrieval scope. Additionally, 2-step filtering method is employed to eliminate papers with low relevance, leaving only high-quality references that comprehensively cover the topic. We design reference pre-processing method called AttributeTree, which efficiently extracts key information from documents. This method significantly enhances the information density of reference materials, improving LLMs comprehension and optimizing their context window usage. Figure 1: The number of papers received annually by the arXiv website from 2010 to 2025, with data sourced from our arXiv database. The projected number of submissions for 2025 is anticipated to be five times greater than that of 2010. from the ground up [47]. Surveys are instrumental in elucidating the current state of research and the historical progression within given topic [3, 45, 46]. However, the workload involved in manually writing these surveys is continually increasing, threatening the ability to maintain comprehensive coverage and high quality of these surveys. Against the backdrop of information overload, there is an urgent need to develop efficient systems for automated survey generation. The rise of Large Language Models (LLMs) has rendered automated survey generation viable approach [13, 16, 37]. Trained on large-scale textual corpora, LLMs possess the capability to produce text that is both fluent and logically coherent [8, 23, 26, 34]. Despite this potential, leveraging LLMs for automated survey generation poses several challenges, which can be categorized into two levels: 1) Technical Challenges: LLMs primarily rely on their internal knowledge bases for text generation. However, effective survey composition requires comprehensive and accurate support from up-to-date references, whereas knowledge stored within LLMs may become outdated and occasionally provide incorrect reference information [11, 18, 27, 43]. This limitation causes negative effects on the academic rigor and credibility of the generated surveys. Consequently, relying solely on LLMs makes it difficult to generate high-quality surveys. Current mainstream LLMs encounter limitations due to their context window sizes [14, 19, 20]. For instance, GPT-4o has context window of 128K tokens, while Claude 3.5 has 200K tokens. Crafting comprehensive survey typically entails citing hundreds of references, each averaging around 10K tokens. This scale far exceeds the context window capacity of existing LLMs, posing challenge to directly equipping them with all necessary references for generating high-quality surveys. 2) Application Challenges: The creation of surveys relies on substantial number of references, necessitating timely content retrieval via online methods. SurveyX: Academic Survey Automation via Large Language Models KDD 2025, August 2025, Toronto, Canada Figure 2: Pipeline of SurveyX. We introduce an outline generation method named Outline Optimization, which generates outlines based on hints and employs separate-then-reorganize step to eliminate redundancy. This method results in outlines with more rigorous logic and clearer structure. We expand the expressive format of the generated survey to include figures and tables alongside text, enriching the presentation and improving readability. We augment the evaluation framework by introducing additional metrics to assess the quality of the generated surveys and retrieved references. The evaluation results demonstrate that the surveys generated by SurveyX outperform existing works across multiple metrics, closely aligning with human expert performance."
        },
        {
            "title": "2 RELATED WORK\nLong-form Text Generation. Although LLMs excel in traditional\nNLG tasks, generating long-form, well-structured, coherent, and\nlogically organized text using LLMs still remains a persistent chal-\nlenge [5, 17, 25, 28, 32, 36, 40]. To address this problem, some studies\nhave sought to employ planning strategies. For instance, Tan et al.\n[31] proposed a method that first produces domain-specific con-\ntent keywords, and then progressively refines them into complete\npassages in multiple stages. Similarly, Liang et al. [21] used a se-\nries of auxiliary training tasks to endow LLMs with the skills to\nplan and structure long-form documents before generating the fi-\nnal full article. Some other researches focus on the generation of\nlong-form text in a specific format, such as Wikipedia articles, sur-\nveys, commentaries, etc [33, 35, 39, 42]. Shao et al. [30] developed\nSTORM, a system that models the pre-writing stage by discovering\ndiverse perspectives, simulating expert conversations, and curating\ninformation to create an outline, which is then used to generate a",
            "content": "complete Wikipedia article. Wang et al. [38] introduced AotuSurvey, framework for generating surveys consisting of initial retrieval, outline generation, parallel subsection drafting, integration, and rigorous evaluation. In comparison, our work addresses existing shortcomings by enhancing the retrieval scenarios, optimizing the reference pre-processing methods, expanding the expressive forms of the generated surveys, and improving overall quality. Retrieval Augmented Generation (RAG). RAG technique is used to help LLMs access external knowledge, thereby enhancing their text generation capabilities [6, 7, 10, 12]. This technique has proven especially useful in tasks that require up-to-date or domainspecific information, such as QA (Question Answering) [1, 15], Dialog Generation [22, 29], Reasoning [4, 44], etc. Beyond traditional NLG tasks, RAG is also used for long-form text generation, as this task often requires handling extensive external knowledge [30, 38, 44]. Compared to previous work that directly used the raw text of references as the retrieval data source, our work significantly improves retrieval efficiency and context window utilization by converting it into attribute trees."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we delineate the pipeline of SurveyX, which includes two main phases: Preparation Phase (section 3.1) and Generation Phase (section 3.2). The primary task of the Preparation Phase is to collect the related references needed for composing the survey and pre-processing them to construct reference materials database for retrieval using RAG technique. The Generation Phase undertakes the creation of both the outline and main body of the survey based on these materials. This phase also involves the post-processing refinement of the initial draft to enhance its overall quality and augment its expressive presentation. An overview of this system is depicted in Figure 2. KDD 2025, August 2025, Toronto, Canada"
        },
        {
            "title": "3.1 Preparation Phase",
            "content": "Preparation Phase consists of two stages: the references acquisition stage and the references pre-processing stage. 3.1.1 References Acquisition. Considering the current shortage of effective tools for efficiently acquiring large volume of highly relevant references, we independently develop module for acquiring references. This module comprises two separate components: retrieval data source and retrieval algorithms. The retrieval data source includes both offline and online data sources. The offline data source consists of 2,632,1893 available papers downloaded from the arXiv.org, with new papers added daily. The online data source is self-developed crawler system based on Google Scholar. Using the offline data source facilitates quick access to existing references, thereby saving time, while the online data source enables the acquisition of the latest and multisource references. The combination of both ensures efficiency and timeliness. The retrieval algorithm is divided into two steps: reference recall and reference filter (steps 1-1 1-6 and steps 1-7 1-8 in Figure 2). (1) The goal of the reference recall step is to avoid missing any references related to the topic as much as possible. (2) The paper filter step aims to filter out references unrelated to the topic as much as possible. To achieve (1), we design method called the Keyword Expansion Algorithm. The pseudo code can be found at Algorithm 1. The algorithm initializes the keyword pool with an initial keyword ùêæ0. After searching references by newly added keywords, it performs semantic clustering on the abstracts of all retrieved references ùê∑ùëúùëê and summarizes keywords for each cluster ùê∂ùëñ . The obtained keywords ùêæùëê are then semantically compared with the existing keyword pool ùêæùëùùëúùëúùëô and the topic ùëá , the most appropriate keyword ùëò based on semantic distance is added to the keyword pool. This process is repeated until the number of ùê∑ùëúùëê reaches the threshold ùúÉ (set as 1000). Algorithm 1 Keyword Expansion Algorithm 1: Input: initial keywords set ùêæ0, topic ùëá 2: Output: retrieved documents ùê∑ùëúùëê 3: keyword pool: ùêæpool ùêæ0 4: while LengthOf(ùê∑ùëúùëê) < ùúÉ do 5: 6: 7: 8: 9: 10: 11: 12: 13: end while 14: Return: ùê∑ùëúùëê ùëò newly added keywords in ùêæpool ùê∑ùëúùëê ùê∑ùëúùëê + Search(ùëò ) ùëõ LengthOf(ùêæpool ) ùê∂ Cluster(ùê∑ùëúùëê, ùëõ + 1) ùêæùëê ExtractKeywords(ùê∂ ) ùê∑ùëñùë†ùë° CalcDist(ùêæùëê, ùêæpool,ùëá , Weights) ùëò Select(ùê∑ùëñùë†ùë°, ùêæpool ) ùêæpool ùêæpool + ùëò Weights are the semantic distance weights. Considering that all keywords should more closely revolve around the topic, we double the semantic distance weight of the topic. The Select function, when choosing candidate words, assumes that the best candidate 3Data as of February 10, 2025. Xun Liang et al. word should have the smallest possible average distance to the existing keywords while also having the maximum furthest distance. Specifically: ùëò = arg min ùëòùëê ùêæùê∂ (cid:0)ùëÖ1 (ùëòùëê ) + ùëÖ2 (ùëòùëê )(cid:1) (1) ùëÖ1 and ùëÖ2 represent different ranking calculation methods. Their calculation methods are as follows: ùëÖ1 (ùëòùëê ) = rankùêæùëê (cid:32) 1 ùêæùëùùëúùëúùëô (cid:205) ùëòùëí ùêæùëùùëúùëúùëô cos_sim(ùê∏ (ùëòùëê ), ùê∏ (ùëòùëí )) (2) (cid:33) (cid:32) (cid:33) ùëÖ2 (ùëòùëê ) = rankùêæùëê max ùëòùëí ùêæùëùùëúùëúùëô cos_sim(ùê∏ (ùëòùëê ), ùê∏ (ùëòùëí )) (3) Here, rank() refers to the Rank Function, and ùê∏ () represents the embedding model. To achieve (2), we propose 2-step filtration algorithm. The first step uses an embedding model to calculate the semantic relevance between the topic and abstracts of references, selecting the Top-K references most relevant to the topic, which serves as coarse-grained filtration. The second stage uses LLMs for more precise semantic filtering, serving as fine-grained filtration. This algorithm ensures that the selected papers maintain high degree of relevance to the research topic, thereby enhancing the quality of the survey. 3.1.2 References Pre-processing. After obtaining the references, they need to be preprocessed for use in subsequent stages. naive and common pre-processing method is to directly provide the full text of the references to LLMs for generating the surveys. However, we believe this approach suffers from low context window utilization and inefficient extraction of key information. We have noticed that before composing survey, people often categorize the necessary references and organize all potentially useful information. Based on this observation, we design reference pre-processing method called AttributeTree. Specifically, we designed different attribute tree templates for different types of references in advance. Using these templates, we can efficiently extract key information from the references. The attribute trees of all references are combined into an attribute forest, which represents all the reference materials required for composing survey (steps 1-9 1-10 in Figure 2). Examples of attribute tree templates can be found in the Appendix B. This method significantly increases the information density of the reference materials and efficiently utilizes the context window of LLMs, thus laying solid foundation for composing high-quality surveys."
        },
        {
            "title": "3.2 Generation Phase",
            "content": "After obtaining all the reference materials needed for writing the survey, the generation phase begins. This phase is divided into three stages: outline generation, main body generation, and post refinement. 3.2.1 Outline Generation. Outline generation is the most crucial stage in the entire survey generation process. well-structured, logically organized, and focused outline ensures comprehensive, cohesive, and informative survey. Our preliminary experiments indicate that, when generating the primary outline, utilizing only SurveyX: Academic Survey Automation via Large Language Models KDD 2025, August 2025, Toronto, Canada stage to enhance the depth of the generated content (steps 2-4 2-5 in Figure 2). Unlike before, the hints used here are derived from the secondary outline, aiming to guide the generation of the main body content. LLMs will generate the main body content on subsection5 basis using these hints and the outline produced in the previous stage. Meanwhile, when the LLM is writing specific subsection, it can view the content of other subsections. This ensures that the content currently being generated remains consistent with the content already generated to some extent. Figure 3: An example of generating secondary outlines. LLMs first generate hints based on the attribute tree to guide the generating of the secondary outline. Then, by synthesizing all hints, LLMs identify the most suitable entry points to determine the segmentation strategy and generate the secondary outline. the internal knowledge of LLMs is sufficient to produce humanlevel primary outline. In contrast, generating the secondary outline is more challenging, and relying solely on the internal knowledge of LLMs is inadequate. We observed that humans typically categorize or summarize references to guide the writing of the survey. Inspired by this, we designed an outline generation method named Outline Optimization (steps 2-1 2-3 in Figure 2). This method consists of two steps. For the first step, this method initially lets LLMs generate hints corresponding to the secondary outline based on the attribute tree of each reference. hint can be considered as form of guiding content that assists LLMs in better understanding the key information particular reference can provide in constructing systematic framework4. Then, LLMs sequentially generate the secondary outline based on these hints. An example of this step is shown in Figure 3. Through this step, LLMs can more accurately identify the commonalities and differences among various references and more efficiently synthesize and organize these references, thereby improving the generation of secondary outlines. For the second step, this method begins by separating all secondary outlines from the primary outlines, retaining only their headings to facilitate deduplication by the LLMs. Then, use LLMs to reorganize the deduplicated secondary outlines into the primary outline. Through this step, the method effectively addresses the redundancy present in the generated secondary outlines. With the help of the outline optimization method, we can generate survey outlines with logical rigor and clear structure. 3.2.2 Content Generation. During the content generation stage, inspired by the human writing process for surveys, we continue to use the hint-based method employed during the outline generation 3.2.3 Post Refinement. In the human process of writing surveys, revising and polishing the draft to enhance its quality is common. Based on this, we design post refinement stage, which polishes the initial draft with two main goals: (1) to improve the quality of the content, including citation quality, textual fluency, and consistency of expression; (2) to add figures and tables to enrich the presentation of the survey. These two objectives are achieved by an RAG-based rewriting module and graph and table generation module, respectively. RAG-based Rewriting Module. Even though strategies were used in the content generation stage to ensure content consistency in the generated survey, these efforts are not completely sufficient. Additionally, the accuracy of citations in the generated survey has not been verified. To address these issues, we design RAG-based rewriting module to revise the survey content. This module first uses the paragraphs from the initial draft as queries to retrieve reference materials from the attribute forest. Subsequently, it constructs prompt based on these materials to rewrite the paragraph using LLMs (steps 2-6 2-7 in Figure 2). The rewriting process involves two aspects: first, removing irrelevant citations and adding highly relevant ones to the paragraph, and second, polishing the paragraph by considering its context. This module not only significantly enhances the citation quality but also ensures the content consistency of the generated survey. Figure and Table Generation Module. Surveys that consist solely of text often lack expression diversity, which can limit their effectiveness in conveying information. To address this issue, we designed figure and table generation module (steps 2-8 2-11 in Figure 2). Inspired by Napkin6, We construct several information extraction templates, each corresponding to specific figure or table generation template (the generation templates are scripts). Based on the information extraction templates, we use LLMs to extract the necessary information from the attribute tree of the references for generating figures or tables. Subsequently, the generation templates automatically construct the corresponding figures or tables based on the extracted information. Additionally, on subchapter basis, we leverage Multimodal Large Language Models (MLLMs) combined with context to retrieve figures from references. If figure effectively supports the content of subsection, we incorporate it to enhance clarity and expressiveness. Through the aforementioned modules, we have constructed comprehensive and systematic post refinement stage that significantly enhances the overall quality of the generated survey while expanding its expressive format. 4systematic framework refers to the framework proposed in survey for organizing and synthesizing all the references. 5One secondary outline corresponds to one subsection. 6https://app.napkin.ai/ KDD 2025, August 2025, Toronto, Canada"
        },
        {
            "title": "4 EXPERIMENTS\n4.1 Evaluation Metrics",
            "content": "Automatic evaluation. In terms of content quality evaluation, we expanded the evaluation dimensions presented in [38] by incorporating synthesis and critical analysis into the evaluation metrics. Synthesis evaluates the ability to interconnect disparate studies, identify overarching patterns or contradictions, and construct cohesive intellectual framework beyond individual summaries, while critical analysis examines the depth of critique applied to existing studies, including the identification of methodological limitations, theoretical inconsistencies, and research gaps. For the evaluation of citation quality, we adopted the Citation Recall and Citation Precision metrics proposed by [38]. The former evaluates whether each statement in the generated text is fully supported by the cited references, while the latter evaluates for the presence of irrelevant citations. In addition, we introduced the F1 Score metric to provide more comprehensive evaluation of citation quality. Regarding the evaluation of reference relevance, we identify an existing gap in the literature, as this aspect has not been addressed in previous studies. To fill this void, we propose novel set of metrics specifically designed to assess the relevance of retrieved references. The details of this proposed metric set are as follows: (1) IoU (Insertion over Union) This metric measures the similarity between machine-retrieved and human-retrieved references by calculating the degree of overlap between the two. The calculation method is as follows ùêºùëúùëà = ùê∑ùëúùëê‚Ñéùë¢ùëöùëéùëõ ùê∑ùëúùëêùëöùëéùëê‚Ñéùëñùëõùëí ùê∑ùëúùëê‚Ñéùë¢ùëöùëéùëõ ùê∑ùëúùëêùëöùëéùëê‚Ñéùëñùëõùëí (2) Relevancesemantic (semantic-based reference relevance). This metric is based on an embedding model, calculating the cosine similarity between the embeddings of the retrieved references and the topic to measure their relevance. The calculation method is as follows: ùëÖùëíùëôùëíùë£ùëéùëõùëêùëíùë†ùëíùëöùëéùëõùë°ùëñùëê = 1 ùê∑ùëúùëê ùëë ùê∑ùëúùëê cos_sim(ùê∏ (ùëë), ùê∏ (ùë°ùëúùëùùëñùëê)) (3) RelevanceLLM (LLM-based reference relevance). This metric is designed by constructing prompts to leverage LLMs to directly evaluate the relevance of the retrieved references to the topic. The calculation method is as follows: ùëÖùëíùëôùëíùë£ùëéùëõùëêùëíùêøùêøùëÄ = 1 ùê∑ùëúùëê ùëë ùê∑ùëúùëê Irelevant (ùêøùêøùëÄ (ùëÉùëüùëúùëöùëùùë° (ùëë, ùë°ùëúùëùùëñùëê))) The prompts used in the evaluation are presented in the AppenXun Liang et al. Human evaluation. In addition to automated evaluation, we also incorporate human evaluation. Human evaluation not only verifies the reliability of automated evaluation but also addresses complex factors such as contextual information and implicit logic, which are difficult for automated evaluation to capture. The evaluation criteria used for human evaluation are the same as that for automated evaluation. Considering the costs, we employ human evaluation only for content quality evaluation. The details of the human evaluation are presented in the Appendix. A."
        },
        {
            "title": "4.2 Experiment Settings",
            "content": "Implementation. During the retrieval stage and the evaluation stage, we selected bge-base-en-v1.5 [41] as the embedding model. Throughout the entire process, we use GPT4o as our LLM agent.7. Baselines. We employ the following methods as our baselines: Human. Human-written surveys collected from arxiv.org. Detailed information can be found at www.surveyx.cn. Navie RAG: We used the same references as SurveyX to provide abstracts for the LLM to guide the generation of the survey. The prompt used by Naive RAG is presented in the Appendix A.4. AutoSurvey: An automated survey generation system proposed by [38]. It divides the survey generation process into four stages: Initial Retrieval and Outline Generation, Subsection Drafting, Integration and Refinement, and Rigorous Evaluation and Iteration. In our experiments, we employed its 64k version. Test Cases. We adopted the 20 topics mentioned by [38] to generate the corresponding surveys for comparison. Details of these topics can be found in the Table 4. Ablation. To evaluate the impact of different modules or methods on the performance of SurveyX, we designed the following ablation setting: (1) For the retrieval algorithms (component) within the retrieval module of the references acquisition stage, we directly use the initial keywords for retrieval, bypassing the Keyword Expansion retrieval algorithm. (2) For the AttributeTree method in the references pre-processing stage, we replace the originally generated attribute trees with the full text of the references. (3) For the outline optimization method in the outline generation stage, we design prompts to let the LLM generate the entire outline in one step. (4) For the RAG-based rewriting module in the post refinment stage, we eliminate the entire RAG-based rewriting module. dix. A.3 7Specifically, we use gpt-4o-2024-08-06 Table 1: Content quality evaluation results of naive RAG, Autosurvey, SurveyX and Human writing. All LLM-Agent is GPT-4o. Model Coverage Structure Relevance Synthesis naive RAG AutoSurvey SurveyX Human 4.40 4.73 4.95 5.00 3.66 4.33 4.91 4.95 4.66 4.86 4.94 5.00 3.82 4.00 4.10 4. Critical Analysis 2.82 3.73 4.05 4.38 Avg Recall Precision F1 3.872 4.331 4.590 4. 68.79 82.25 85.23 86.33 61.97 77.41 78.12 77.78 65.20 79.76 81.52 81.83 SurveyX: Academic Survey Automation via Large Language Models KDD 2025, August 2025, Toronto, Canada"
        },
        {
            "title": "4.3 Experiment Results and Analysis",
            "content": "Table 3: Evaluation results of reference relevance. with comprehensive online reference retrieval capabilities, SurveyX demonstrates its potential and application prospects, laying the foundation for future research. Model Human SurveyX IoU 1 0.55 Relevancesemantic 0.4455 0.4226 RelevanceLLM 0.9485 0. Main results. The results for content quality evaluation, citation quality evaluation, and reference relevance evaluation are shown in Table 1 and Table 3. Key findings are as follows: (1) The experimental results for content quality evaluation indicate that SurveyX performs exceptionally well across all metrics, particularly in Coverage (4.95), Structure (4.91), and Relevance (4.94), closely approaching the performance of human experts. This demonstrates that the surveys generated by SurveyX not only comprehensively cover both the core and peripheral content of the given topic but also ensure logical and coherent outlines while maintaining high relevance. Compared to naive RAG and AutoSurvey, SurveyX shows clear advantage across all metrics, with significant improvements in Structure (4.91) and Critical Analysis (4.05). This indicates that the surveys generated by SurveyX have clearer and more coherent outlines and greater depth in content. Additionally, SurveyX leads existing automated survey generation systems with an average score of 4.590, proving its outstanding performance in survey generation. (2) The results of the citation quality experiments show that SurveyX outperforms existing automated survey generation systems and closely approaches the performance of human experts in Citation Recall (85.23), Citation Precision (78.12), and F1 Score (81.52). Notably, in the Precision metric (78.12), SurveyX even slightly surpasses human experts. This indicates that SurveyX can significantly enhance the comprehensiveness and accuracy of citations in its generated surveys, ensuring that all statements are well-supported by literature while effectively reducing irrelevant citations, thereby increasing the reliability and credibility of the generated surveys. These experimental results also indirectly demonstrate that the RAG-based rewriting component used in the post refinment stage of SurveyX effectively improves the citation quality of the generated surveys. (3) The results of the reference relevance experiments indicate that SurveyX approaches the performance of human experts in the Relevancesemantic metric. However, there is certain gap compared to human experts in the IoU and RelevanceLLM metrics. Nonetheless, as the first automated survey generation system in this field Figure 4: Human evaluation results. Human evaluation results. The results of the human evaluation are shown in Figure 4. The results show that SurveyX outperforms AutoSurvey across all metrics and is closer to the performance of human experts. This aligns with the results of the automated evaluation, which to some extent supports the validity of our automated evaluation method. Additionally, compared to the automated evaluation results, the scores from human evaluation are generally lower, especially in the Structure metric. This reflects the higher standards of human evaluators for the survey in areas such as structural coherence, logical consistency, content depth, etc. Ablation results. The results of the ablation experiments are shown in Table 2. The experimental results indicate that each module of SurveyX plays crucial role. Key findings are as follows: (1) After ablating the retrieval algorithm, the Coverage (4.954.74) and Relevance (4.944.79) metrics showed the most significant decreases. This indicates that the improvement in reference quality brought by the retrieval algorithm is crucial for ensuring comprehensive content coverage and high topic relevance in the generated surveys. (2) After ablating the AttributeTree method, the Structure (4.91 4.08), Synthesis (4.103.88), and Critical Analysis (4.053.93) metrics showed significant declines. Besides, the Recall (85.2360.09), Table 2: Ablation study results for SurveyX with different components removed. Data with significant declines are indicated by underlines. Ablation Object Coverage Structure Relevance Synthesis Retrieval Algorithm AttributeTree Method Outline Optimization Method RAG-based Rewriting Module No Ablation 4.74 4.84 4.90 4.92 4.95 4.88 4.08 3.80 4.89 4. 4.79 4.89 4.91 4.93 4.94 3.98 3.88 3.98 4.00 4.10 Critical Analysis 4.02 3.93 4.02 4.00 4.05 Avg Recall Precision F1 4.48 4.32 4.32 4.55 4.590 78.88 60.09 85.1 55.37 85.23 73.34 56.49 77.13 54.95 78.12 76.01 58.23 80.92 55.16 81.52 KDD 2025, August 2025, Toronto, Canada Xun Liang et al. Precision (78.1256.49), and F1 score (81.5258.23) metrics also decreased noticeably. This indicates that converting the content of the reference into an attribute tree enables the LLM to understand the core information better, thereby improving its ability to organize outlines, integrate information from different references, conduct in-depth analysis, and add references more accurately. (3) After ablating the outline optimization method, the Structure metric (4.913.80) showed the most significant decline. This indicates that this method effectively enhances the logical coherence and structural clarity of the generated survey outline. (4) After ablating the RAG-based rewriting module, the Recall (85.2355.37), Precision (78.1254.95), and F1 score (81.5255.16) metrics declined significantly. This indicates that the RAG-based rewriting module can considerably enhance the citation quality of the generated survey by more accurately adding relevant citations and removing irrelevant ones."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose SurveyX, an innovative automated survey generation system. SurveyX effectively addresses issues in LLM-based automated survey generation, such as context window limitations and internal knowledge constraints. Moreover, SurveyX also overcomes shortcomings of existing automatic generation systems, including lack of diversity in survey expression, inadequate reference pre-processing methods, and limitations in retrieval approaches. Experimental results demonstrate that SurveyX significantly outperforms existing systems in both content and citation quality, approaching the level of human experts. This achievement indicates that SurveyX can serve as reliable and efficient tool for automated survey generation, providing valuable assistance to researchers. In the future, we consider improving our system in the following areas: Optimizing the retrieval algorithm to achieve retrieval performance comparable to human levels. Expanding the methods for generating figures and tables. Enhancing the organization of surveys by further refining the composing approach based on the attribute tree. REFERENCES [1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through SelfReflection. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=hSyW5go0v8 [2] Alexander Bick, Adam Blandin, and David Deming. 2024. The rapid adoption of generative ai. Technical Report. National Bureau of Economic Research. [3] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2024. Survey on Evaluation of Large Language Models. ACM Trans. Intell. Syst. Technol. 15, 3, Article 39 (March 2024), 45 pages. https://doi.org/10.1145/3641289 [4] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang. 2023. UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 1231812337. https://doi.org/10.18653/v1/2023.emnlp-main.758 [5] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2024. BAMBOO: Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia, 20862099. https://aclanthology.org/2024.lrec-main.188 [6] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 64916501. [7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997 (2023). [8] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating large language models: comprehensive survey. arXiv preprint arXiv:2310.19736 (2023). [9] Vikram Gupta. 2023. Recent Advancements in Computer Science: Comprehensive Review of Emerging Technologies and Innovations. In International Journal for Research Publication and Seminar, Vol. 14. 329334. [10] Yucheng Hu and Yuxing Lu. 2024. Rag and rau: survey on retrieval-augmented language model in natural language processing. arXiv preprint arXiv:2404.19543 (2024). [11] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232 (2023). [12] Yizheng Huang and Jimmy Huang. 2024. Survey on Retrieval-Augmented Text Generation for Large Language Models. arXiv preprint arXiv:2404.10981 (2024). [13] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169 (2023). [15] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1043810451. https://doi.org/10.18653/v1/2024.acl-long.562 [16] Darioush Kevian, Usman Syed, Xingang Guo, Aaron Havens, Geir Dullerud, Peter Seiler, Lianhui Qin, and Bin Hu. 2024. Capabilities of large language models in control engineering: benchmark study on gpt-4, claude 3 opus, and gemini 1.0 ultra. arXiv preprint arXiv:2404.03647 (2024). [17] Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et al. 2024. LongLaMP: Benchmark for Personalized Long-form Text Generation. arXiv preprint arXiv:2407.11016 (2024). [18] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2023. Large Language Models with Controllable Working Memory. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 17741793. https://doi.org/10.18653/v1/2023.findings-acl.112 [19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How Long Can Context Length of Open-Source LLMs truly Promise?. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. https://openreview.net/forum?id=LywifFNXV5 [20] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024. arXiv preprint Long-context llms struggle with long in-context learning. arXiv:2404.02060 (2024). [21] Yi Liang, You Wu, Honglei Zhuang, Li Chen, Jiaming Shen, Yiling Jia, Zhen Qin, Sumit Sanghai, Xuanhui Wang, Carl Yang, et al. 2024. Integrating Planning into Single-Turn Long-Form Text Generation. arXiv preprint arXiv:2410.06203 (2024). [22] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. [n. d.]. RA-DIT: Retrieval-Augmented Dual Instruction Tuning. In The Twelfth International Conference on Learning Representations. [23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. 2023. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 2155821572. https://proceedings.neurips.cc/paper_files/paper/2023/file/ 43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf [24] Tyler Loakman, Chen Tang, and Chenghua Lin. 2024. Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases. arXiv preprint arXiv:2403.13901 (2024). [25] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Finegrained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language SurveyX: Academic Survey Automation via Large Language Models KDD 2025, August 2025, Toronto, Canada Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 1207612100. https://doi.org/10.18653/ v1/2023.emnlp-main.741 [26] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models: survey. arXiv preprint arXiv:2402.06196 (2024). [27] Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity Cloze By Date: What LMs Know About Unseen Entities. In Findings of the Association for Computational Linguistics: NAACL 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 693702. https: //doi.org/10.18653/v1/2022.findings-naacl.52 [28] Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. 2024. HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models. arXiv preprint arXiv:2409.16191 (2024). [29] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al. 2024. Recommender systems with generative retrieval. Advances in Neural Information Processing Systems 36 (2024). [30] Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. 2024. Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 62526278. https://doi.org/10.18653/v1/2024.naacl-long.347 [31] Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric Xing, and Zhiting Hu. 2021. Progressive Generation of Long Text with Pretrained Language Models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 43134324. https://doi.org/10.18653/v1/2021.naacl-main.341 [32] Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, et al. 2024. Proxyqa: An alternative framework for evaluating long-form text generation with large language models. arXiv preprint arXiv:2401.15042 (2024). [33] Chen Tang, Chenghua Lin, Henglin Huang, Frank Guerin, and Zhihao Zhang. 2022. EtriCA: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention. In Findings of the Association for Computational Linguistics: EMNLP 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 55045518. https://doi.org/10.18653/v1/2022.findings-emnlp.403 [34] Chen Tang, Tyler Loakman, and Chenghua Lin. 2024. cross-attention augmented model for event-triggered context-aware story generation. Computer Speech & Language 88 (2024), 101662. [35] Chen Tang, Shun Wang, Tomas Goldsack, and Chenghua Lin. 2023. Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 606618. https: //doi.org/10.18653/v1/2023.emnlp-main. [36] Chen Tang, Hongbo Zhang, Tyler Loakman, Chenghua Lin, and Frank Guerin. 2023. Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 46044616. https://doi.org/10.18653/v1/2023.acl-long.253 [37] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [38] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. 2024. AutoSurvey: Large Language Models Can Automatically Write Surveys. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [39] Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Zhi-Jian Zhao, et al. 2024. Automated review generation method based on large language models. arXiv preprint arXiv:2407.20906 (2024). [40] Yuhao Wu, Ming Shan Hee, Zhiqing Hu, and Roy Ka-Wei Lee. 2024. Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models. arXiv preprint arXiv:2409.02076 (2024). [41] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL] [42] Jiebin Zhang, Eugene Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, and Sujian Li. 2024. Retrievalbased Full-length Wikipedia Generation for Emergent Events. arXiv preprint arXiv:2402.18264 (2024). [43] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Sirens song in the AI ocean: survey on hallucination in large language models. arXiv preprint arXiv:2309.01219 (2023). [44] Zhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang Shi, Meng Han, Yongkang Wu, Ruofei Lai, and Zhao Cao. 2023. IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 114. https://doi.org/10.18653/v1/2023.emnlp-main.1 [45] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [46] Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, and Zhiyu Li. 2025. Attention heads of large language models. Patterns (2025), 101176. https://doi.org/10.1016/j.patter.2025.101176 [47] Shupeng Zhong, Xinger Li, Shushan Jin, and Yang Yang. 2024. The Solution for The PST-KDD-2024 OAG-Challenge. arXiv preprint arXiv:2407.12827 (2024). EVALUATION DETAILS A.1 Survey Topics Table 4 presents the 20 survey topics used in the evaluation experiment, along with the titles of the corresponding survey written by human experts. A.2 Details of human evaluation For the human evaluation, six PhD students with experience in writing LLM-related surveys were invited. The platform used for evaluation is Label Studio8. The final score was derived by averaging the scores given by all evaluators. A.3 Evaluation Prompt Prompts used for content evaluation are presented in figure 5, 6, 7, 8, 9. Prompts used for citation quality are presented in figure 10. Prompts used for reference papers are presented in figure 11. A.4 Naive RAG Prompt Prompts used for Naive RAG method are presented in figure"
        },
        {
            "title": "B ATTRIBUTE TREE TEMPLATES",
            "content": "Attribute tree templates are shown in figure 13, 14, 15, 16. Received 10 February 2025 8https://labelstud.io/ KDD 2025, August 2025, Toronto, Canada Xun Liang et al. Table 4: Survey Topics"
        },
        {
            "title": "Survey Title",
            "content": "In-context Learning LLMs for Recommendation LLM-Generated Texts Detection Explainability for LLMs Evaluation of LLMs LLMs-based Agents LLMs in Medicine Domain Specialization of LLMs Challenges of LLMs in Education Alignment of LLMs ChatGPT Instruction Tuning for LLMs LLMs for Information Retrieval Safety in LLMs Chain of Thought Hallucination in LLMs Bias and Fairness in LLMs Large Multi-Modal Language Models Acceleration for LLMs LLMs for Software Engineering survey for in-context learning Survey on Large Language Models for Recommendation Survey of Detecting LLM-Generated Texts Explainability for Large Language Models Survey on Evaluation of Large Language Models Survey on Large Language Model based Autonomous Agents Survey of Large Language Models in Medicine Domain Specialization as the Key to Make Large Language Models Disruptive Practical and Ethical Challenges of Large Language Models in Education Aligning Large Language Models with Human Survey on ChatGPT and Beyond Instruction Tuning for Large Language Models Large Language Models for Information Retrieval Towards Safer Generative Language Models: Safety Risks, Evaluations, and Improvements Survey of Chain of Thought Reasoning Survey on Hallucination in Large Language Models Bias and Fairness in Large Language Models Large-scale Multi-Modal Pre-trained Models Survey on Model Compression and Acceleration for Pretrained Language Models Large Language Models for Software Engineering Figure 5: Content coverage prompt for evaluation. SurveyX: Academic Survey Automation via Large Language Models KDD 2025, August 2025, Toronto, Canada Figure 6: Content structure prompt for evaluation. Figure 7: Content relevance prompt for evaluation. KDD 2025, August 2025, Toronto, Canada Xun Liang et al. Figure 8: Content synthesis prompt for evaluation. Figure 9: Content critical analysis prompt for evaluation. SurveyX: Academic Survey Automation via Large Language Models KDD 2025, August 2025, Toronto, Canada Figure 10: Citation prompt for evaluation. Figure 11: Judge relevance for reference paper. Figure 12: Prompts for Naive RAG. KDD 2025, August 2025, Toronto, Canada Xun Liang et al. Figure 13: Method paper attribute tree. Figure 14: Benchmark paper attribute tree. SurveyX: Academic Survey Automation via Large Language Models KDD 2025, August 2025, Toronto, Canada Figure 15: Theory paper attribute tree. Figure 16: Survey paper attribute tree."
        }
    ],
    "affiliations": [
        "Institute for Advanced Algorithms Research, Shanghai, China",
        "Northeastern University, Shenyang, China",
        "Renmin University of China, Beijing, China",
        "The University of Sydney, Sydney, Australia"
    ]
}