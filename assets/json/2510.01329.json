{
    "paper_title": "Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling",
    "authors": [
        "Huangjie Zheng",
        "Shansan Gong",
        "Ruixiang Zhang",
        "Tianrong Chen",
        "Jiatao Gu",
        "Mingyuan Zhou",
        "Navdeep Jaitly",
        "Yizhe Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Standard discrete diffusion models treat all unobserved states identically by mapping them to an absorbing [MASK] token. This creates an 'information void' where semantic information that could be inferred from unmasked tokens is lost between denoising steps. We introduce Continuously Augmented Discrete Diffusion (CADD), a framework that augments the discrete state space with a paired diffusion in a continuous latent space. This yields graded, gradually corrupted states in which masked tokens are represented by noisy yet informative latent vectors rather than collapsed 'information voids'. At each reverse step, CADD may leverage the continuous latent as a semantic hint to guide discrete denoising. The design is clean and compatible with existing discrete diffusion training. At sampling time, the strength and choice of estimator for the continuous latent vector enables a controlled trade-off between mode-coverage (generating diverse outputs) and mode-seeking (generating contextually precise outputs) behaviors. Empirically, we demonstrate CADD improves generative quality over mask-based diffusion across text generation, image synthesis, and code modeling, with consistent gains on both qualitative and quantitative metrics against strong discrete baselines."
        },
        {
            "title": "Start",
            "content": "Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling Huangjie Zheng, Shansan Gong, Ruixiang Zhang, Tianrong Chen, Jiatao Gu, Mingyuan Zhou, Navdeep Jaitly, Yizhe Zhang Apple Standard discrete diffusion models treat all unobserved states identically by mapping them to an absorbing [MASK] token. This creates an information void where semantic information that could be inferred from unmasked tokens is lost between denoising steps. We introduce Continuously Augmented Discrete Diffusion (CADD), framework that augments the discrete state space with paired diffusion in continuous latent space. This yields graded, gradually corrupted states in which masked tokens are represented by noisy yet informative latent vectors rather than collapsed information voids. At each reverse step, CADD may leverage the continuous latent as semantic hint to guide discrete denoising. The design is clean and compatible with existing discrete diffusion training. At sampling time, the strength and choice of estimator for the continuous latent vector enables controlled trade-off between mode-coverage (generating diverse outputs) and mode-seeking (generating contextually precise outputs) behaviors. Empirically, we demonstrate CADD improves generative quality over mask-based diffusion across text generation, image synthesis, and code modeling, with consistent gains on both qualitative and quantitative metrics against strong discrete baselines. Correspondence: Huangjie Zheng: huangjie_zheng@apple.com; Yizhe Zhang: yizhe_zhang@apple.com Date: October 3, 2025 5 2 0 O 1 ] . s [ 1 9 2 3 1 0 . 0 1 5 2 : r Figure 1 (Best view in color ) Comparison of diffusion models across modeling spaces. Masked diffusion uses [MASK] as noise and follows single mask-to-token path, jumping from an absorbing state to token predictions. Continuous (Gaussian) diffusion evolves in the full embedding space, but intermediate latents often do not decode to valid tokens until the final step because the search space is large. CADD combines the stability of masked diffusion with the flexibility of continuous diffusion: discrete tokens anchor context-consistent subspace, while the paired continuous latent allows smooth transitions among plausible token candidates, improving decoding at masked positions. Work done when Shansan Gong and Mingyuan Zhou were at Apple."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models have significantly advanced generative modeling tasks (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021; Dhariwal & Nichol, 2021; Karras et al., 2022), particularly in image synthesis (Saharia et al., 2022; Esser et al., 2024; Polyak et al., 2024; Zheng et al., 2024a; Brooks et al., 2024). Recently, with rapid progress in discrete diffusion models (Austin et al., 2021a; Hoogeboom et al., 2021; Lou et al., 2024), diffusion models have become powerful tool for discrete categorical data domains, such as text generation and code generation (Gat et al., 2024; Gong et al., 2023, 2025b). Early work on Continuous Diffusion Models (CDMs) for categorical data maps tokens into continuous space, applies Gaussian diffusion to the representations, and then rounds back to discrete symbols (Li et al., 2022; Dieleman et al., 2022; Han et al., 2022; Zhang et al., 2023; Gulrajani & Hashimoto, 2023). This route preserves smooth semantic signals and enables the use of established score-based methods. In parallel, Masked Diffusion Models (MDMs) have recently shown strong results for categorical data (Shi et al., 2024; instead of adding noise in an embedding space, MDMs progressively Sahoo et al., 2024; Nie et al., 2025): mask tokens over time and learn to unmask them, yielding clear training signals via token-level cross-entropy. Despite their respective successes, both approaches have limitations, which are illustrated in Figure 1. (i) MDMs suffer from information loss due to their use of absorbing [MASK] state (Chao et al., 2025; Wang et al., 2025). This design collapses all unobserved possibilities into one symbol, erasing information about how similar corrupted position is to the original token, thus creating an information void. This reduces the information available for resolving ambiguity and maintaining global semantic coherence. For example, as shown on the right of the figure, if masked token could plausibly be Language or Diffusion, the [MASK] representation offers no semantic clue to favor either option, forcing the model to make hard choice without graded guidance. (ii) While CDMs can represent semantic proximity, they face different challenge known as over-smoothing. Because the denoising process occurs entirely in continuous embedding space with discretization to tokens only at the end (Gao et al., 2022), their continuous denoising objective can over-smooth token identities, making it difficult to make precise predictions without localized contexta problem known as rounding error (Li et al., 2022). To address these challenges, we propose Continuously Augmented Discrete Diffusion (CADD), which combines the strengths of both CDMs and MDMs. CADD keeps the discrete masking process but augments parallel continuous diffusion in continuous semantic embedding space. This means masked positions retain semantic information through noisy but informative latent vectors instead of becoming collapsed information voids. In the reverse process, the model uses the continuous latent as soft semantic hint to guide token denoising at each step, while the discrete context constrains the latent dynamics locally. Returning to Figure 1, the continuous manifold offers graded path between candidates (Language and Diffusion, in this case), and the discrete neighborhood restricts the search space, allowing movement within the triangular region between hypotheses and enabling smooth transitions driven by the hints. In addressing the limits of both pure MDMs and CDMs, our contributions are: 1. Better token prediction with soft hints. For masked positions, the continuous latent representations are corrupted in smooth decay rather than an abrupt information loss, thus preserve graded proximity to the ground-truth token embedding, which reduces ambiguity and makes discrete prediction easier. 2. Diversity with multi-sample estimation. At inference, one can resample the continuous latent (e.g., multiple latent draws per discrete state) to explore alternative yet valid choices for token or span, which could lead to complete view of plausible tokens, enhancing the diversity of generation. 3. Training and sampling remain simple. CADD keeps standard cross-entropy for tokens and standard diffusion loss for the continuous head. The sampler can alternate or jointly update the discrete and continuous states. 4. Parameter efficiency and efficient fine-tuning. CADD requires no special architecture and can reuse the same backbone as an MDM. As result, the number of learnable parameters matches prior MDMs, and there is no significant increase in compute cost in training. Together with simple training loss described above, this enables efficient fine-tuning of existing MDM checkpoints to obtain the benefits of CADD."
        },
        {
            "title": "2 Related Work",
            "content": "Discrete Diffusion Models Discrete diffusion models (Hoogeboom et al., 2021; Zheng et al., 2024b; Austin et al., 2021a) operate by defining Markov chain over the discrete token space, gradually diffusing the data with either uniform or absorbing transitions. Later, the model was unified and simplified to continuous-time masked diffusion models (Campbell et al., 2022; Lou et al., 2024; Shi et al., 2024; Sahoo et al., 2024; Zhang et al., 2025b). Building on this, several recent works further scaled diffusion LMs to 7B parameters (Gong et al., 2025a; Ye et al., 2025; Nie et al., 2024), achieving performance on par with AR models. Parallel efforts explored unified multimodal variants that model text and images both in discrete token (Yang et al., 2025; Li et al., 2025). However, because masked diffusion models do not allow unmasked tokens to change, errors can accumulate during generation due to suboptimal unmasking in earlier steps. Several enhanced (re-)masking techniques have been proposed, using bits and simplex representation to enrich the binary choice of masking (Chao et al., 2025; Song et al., 2025a), remasking during the reverse process (Gat et al., 2024; Zhao et al., 2024; Wang et al., 2025), enabling edit operations (Havasi et al., 2025; Song et al., 2025b). Continuous Relaxations for Discrete Data Early continuous approaches either learn denoising in latent embedding without explicit statistical structure (Li et al., 2022; Dieleman et al., 2022; Chen et al., 2023; Zhang et al., 2023; Gulrajani & Hashimoto, 2023) or fully relax tokens into unconstrained Euclidean space as simplex (Han et al., 2022; Karimi Mahabadi et al., 2024; Tae et al., 2025). However, such unconstrained relaxations often fail to preserve the inherent discreteness and categorical semantics of language (Gulrajani & Hashimoto, 2023). More recent methods impose structure in the logit space (Hoogeboom et al., 2021; Graves et al., 2023) or directly on the probability simplex via Dirichlet priors (Avdeyev et al., 2023; Stärk et al., 2024), enforcing stronger statistical constraints on the noising process. Flow-matching techniques further treat the simplex as statistical manifold (Liu et al., 2023; Cheng et al., 2024; Davis et al., 2024), yet these approaches still lag behind discrete diffusion models in generation fidelity. Recently, Zhang et al. (2025a) leveraging density models with normalizing flow (Zhai et al., 2025; Gu et al., 2025) for flexible language modeling, and Sahoo et al. (2025) connect discrete diffusion language models and the underlying Gaussian diffusion. Bridging Through the Lens of Mode Balancing Our work is also motivated by balancing mode seeking and mode covering. Related efforts pursue this balance via guidance methods that tune the diversityprecision tradeoff (Dhariwal & Nichol, 2021; Ho & Salimans, 2022); score-distillation approaches that sharpen samples while retaining diffusion training for coverage (Poole et al., 2022; Song et al., 2023; Luo et al., 2023; Yin et al., 2024; Zhou et al., 2024; Zhang et al., 2025b); and techniques that improve GAN mode coverage using diffusion or augmentation (Zheng & Zhou, 2021; Zheng et al., 2023a; Wang et al., 2023; Karras et al., 2020; Zhao et al., 2020). Similar effects have been observed when distilling in paired continuous space (Sahoo et al., 2025). From this perspective, we consider the discrete path in CADD possesses the mode-seeking behavior, as the unmasked tokens anchor the modes in the embedding space. The augmented continuous space spreads probability mass to cover plausible alternatives for the next token to enhance the mode coverage."
        },
        {
            "title": "3 Preliminary",
            "content": "0, . . . , xn 0 ) represent sequence of discrete tokens from vocabulary = {1, 2, ..., } {m}, Let x0 = (x1 is one-hot vector in {0, 1}V +1. which contains tokens plus mask token ([MASK]). Each position xi 0 Let wθ : Rd be learnable token embedding matrix. The embedding representations are obtained as z0 := wθ(x0), where z0 Rnd. Discrete Diffusion Models The forward diffusion process is performed through an element-wise conditional sampler q(xtx0) = (cid:81)n 0), defined as (δ() denotes the dirac function): i=1 q(xi txi q(xi txi 0) αtδ(xi xi 0) + (1 αt)δ(xi m), (3.1) where αt [0, 1] is strictly decreasing scheduling function following αt = (cid:81)t s=1(1 βs). The reverse process aims to learn p(xsxt) for 0 < 1. This is typically achieved by training model pθ(x0xt) to 3 predict the original data from corrupted state, optimized by minimizing variational bound on the negative log-likelihood, denoting α the derivative of αt w.r.t. t: (cid:20) Lvb(x0; θ) Et,xtq(x0) α 1 αt (cid:21) log pθ(x0xt) . (3.2) Continuous Diffusion Models Continuous diffusion models corrupt real-valued data z0Rnd by adding Gaus- . The forward process q(ztz0) is Gaussian distribution with closed form: sian noise scheduled by {γt}T (3.3) t= q(ztz0) = (zt; γtz0, (1 γt)I) where γt is analogous to αt, with γt = (cid:81)t network fθ() with MSE objective reweighted by signal-to-noise ratio (SNR) function λ(γt, t): s=1 γs holding. The reverse process pθ(zt1zt) is trained by fitting Lvb(z0; θ) Et,xtq(z0) (cid:2)λ(γt, t)fθ(zt; t) z02(cid:3) . (3.4)"
        },
        {
            "title": "4 Continuously Augmented Discrete Diffusion (CADD)",
            "content": "Here we introduce Continuously Augmented Discrete Diffusion (CADD). The high-level intuition is to mitigate the sudden information loss that occurs when tokens are replaced by an absorbing state in discrete diffusion. Inspired by the smooth signal degradation in Gaussian diffusion, CADD augments the discrete state space with continuous latent variable, zt. This variable is paired with discrete tokens xt and is designed to retain semantics of tokens original signal even when tokens in xt are masked. Guided by set of latent vectors {z(k) , the model predicts next tokens by: }K k=1 pθ(xt1 xt) = Ezt[pθ(xt1 xt, zt)] 1 K (cid:88) k=1 pθ(xt1 xt, z(k) ). (4.1) Conditioning continuous view of the underlying content at step and traverse on the zt space, the expectation averages over plausible continuous states so the predictor could realize the distribution of the possible tokens more accurately. Noted that although we may use continuous-time notation and for diffusion steps, to improve readability, here we denote specific consecutive steps in the diffusion process by and 1, with total steps. Below we present the construction of CADD with main derivations. For more detailed ELBO derivations and proofs, please refer to Appendix A."
        },
        {
            "title": "4.1 Forward\nTo let zt retain semantic hints of tokens in xt when they are masked, we define the joint transition, which\ncan be factorized as the transitions between discrete tokens, as well as those in the paired continuous space:",
            "content": "q(xt, zt xt1, zt1, x0) := q(xt xt1) (cid:124) (cid:125) (cid:123)(cid:122) discrete part , q(zt zt1, xt1, xt, x0) (cid:123)(cid:122) (cid:125) continuous part (cid:124) Given fixed discrete schedule {βt}T transition of discrete and continuous part can be written as following with γt := (cid:81)t t=1 [0, 1)T and continuous diffusion schedule {γt}T t= s=1 γs: q(xt xt1) = (cid:89) i=1 Categorical(cid:0)xi t; t xi t1 (cid:1), Qt = (1 βt)I + βt 1 m. q(zt zt1, xt1, xt, x0) = (cid:89) i= δ(zi zi N(cid:0)zi t; N(cid:0)zi t; t1), γt zi γt zi t1, (1 γt)Id t1, (1 γt)Id xi (cid:1), xi (cid:1), xi = m, = m, xi = m, xi t1 = m, t1 = m. (4.2) , the forward (4.3) (4.4) The discrete path still follows the Markov chain and the status at only depends on the last time step. The continuous path is thus affected by the status at the last time 1 in the latent pace, as well as how the discrete token changes between these two steps, e.g., whether this token is masked for the first time or it is already masked/unmasked. 4 zi zi t1) = δ(zi As result, the discrete transition is the same as normal discrete diffusion like Austin et al. (2021a) and acts as trigger for the continuous embeddings evolution. The continuous trajectory for an embedding remains dormant as long as its token is unmasked, holding its value constant at its original 0) if xi state (δ(zi is never masked as the information is not changed). The moment token is masked, it triggers the continuous diffusion process for its embedding. The embedding then begins smooth degradation path determined by the Gaussian diffusion (Ho et al., 2020). If token stays masked, its embedding simply continues along this path, becoming progressively noisier. Figure 2 illustrates how our forward process differs from vanilla Mask Diffusion. When all tokens are visible, the SNR for both Mask Diffusion and CADD equals 1. Once token is masked, the SNR in Mask Diffusion drops to 0 because the absorbing [MASK] carries no token-specific signal. In CADD, the paired continuous latent at that position follows Gaussian diffusion, so its SNR decays smoothly over time, reflecting graded corruption rather than an abrupt loss. Figure 2 Example of Signal-to-Noise Ratio (SNR) change of one token in the forward of vanilla Mask Diffusion vs. CADD (Best view in color ). After the second token is masked at the first time, CADD gradually corrupt the information of this token with Gaussian diffusion in the latents, resulting in smooth decay. Now we extend the case to the marginals at timestep with the following proposition. Proposition 1 (Timestep-t joint marginal factorization). The marginal at timestep can be factorized: q(xt, zt x0) = q(xt x0) q(zt xt, x0) (4.5) Given αt := (cid:81)t the two terms factorized above represent the discrete and continuous part: s=1 Qs = αtI + (1 αt) 1 and γt := (cid:81)t s=1(1 βs) and Qt := (cid:81)t s=1 γs, with zi 0 = wθ(xi 0), q(xt x0) = (cid:89) i=1 q(zt xt, x0) = q(xi xi 0), q(xi xi 0) = Categorical(xi t; xi 0). (cid:89) i=1 q(zi xi t, xi 0) = (cid:89) i=1 (cid:40) zi δ(zi 0), N(cid:0)zi γt zi t; 0, (1 γt)Id xi (cid:1), xi = xi 0, = m, (4.6) (4.7) key property of the marginal distribution q(xt, zt x0) is that it conveniently factorizes into discrete and continuous components: q(xt x0) and q(zt xt, x0). This factorization is highly advantageous, as the distribution for each component is tractable and can be computed in closed form according to the predefined diffusion schedule."
        },
        {
            "title": "4.2 Reverse",
            "content": "Following Kingma et al. (2021); Xiao et al. (2022); Zhou et al. (2023), we choose the conditional distribution parameterized with neural network fθ() to define: pθ(xt1, zt1 xt, zt) := q(xt1, zt1 xt, zt, x0 = ˆx0), pθ( ˆx0 xt, zt) = Categorical(cid:0)logits = fθ(xt, zt)(cid:1) if xt = else δ( ˆx0 xt). (4.8) (4.9) The objective is to close the gap between the defined parametric distribution and the true posterior. Below we presenet the close form of the posterior. For notation simplicity, below we discuss on per position formulation and omit the notation i, since all distributions factorize across positions {1, . . . , n}. Proposition 2 (Factorization of the true posterior). By the forward construction, the posterior can be factorized in the following form q(xt1, zt1 xt, zt, x0) = q(xt1 xt, x0) (cid:125) (cid:124) (cid:123)(cid:122) discrete part q(zt1 xt, zt, xt1, x0) (cid:123)(cid:122) (cid:125) continuous part (cid:124) . (4.10) 5 Figure 3 (Best view in color ) Illustrative depiction of CADD model, combining both the discrete and continuous feature of the data. In training, the clean token at the masked position will be created by embedding matrix and used to form the noisy embedding according to the continuous forward. In sampling, the model is able to predict diverse distribution of possible tokens by sampling multiple zt. Then the predicted tokens will be recycled into the embedding matrix to form ˆz0,θ for the next iteration. Moreover, we can write the close form of each component: q(xt1xt, x0) = q(xtxt1)q(xt1x0) q(xtx0) = αt1αt 1αt 1αt1 1αt t1xt x t1x0 xt1 = m, xt = xt1 = m, xt = xt = m. q(zt1 xt, zt, xt1, x0) = with the following paramters: δ(zt1 z0), δ(zt1 z0), N(cid:0)zt1; µt, βtId xt = x0 (no mask at t), xt = m, xt1 = x0 (first unmask at t), (cid:1), xt = m, xt1 = m, βt = (1 γt1) (1 γt) 1 γt , µt = γt1 (1 γt) 1 γt z0 + γt (1 γt1) 1 γt zt. (4.11) (4.12) (4.13) Lemma 1. For the unmasked positions (xt = m), the KL is identically 0, and the masked positions splits exactly as DKL (cid:0)q( xt, zt, x0) (cid:13) (cid:13) pθ( xt, zt)(cid:1) = ρflip (cid:124) (cid:2) log pθ(x0xt, zt)(cid:3) (cid:125) (cid:123)(cid:122) discrete + ρkeep (cid:124) DKL (cid:123)(cid:122) continuous (cid:125) cont , (4.14) with the ratio that determines whether the position is going to be flipped to unmask or keep moving in the continuous space: ρkeep = 1 αt1 1 αt , ρflip = αt1 βt 1 αt = αt1 αt 1 αt . (4.15) The KL divergence in the continuous space has reweighted MSE form: Dcont KL = 1 2 βt (cid:13) (cid:13) µt(z0, zt) µt( ˆz0,θ, zi t)(cid:13) 2 (cid:13) = a2 2 βt z0 ˆz0,θ2; at = γt1(1 γt) 1 γt . (4.16)"
        },
        {
            "title": "4.3 Algorithm and Implementation",
            "content": "Given the results above, below comes the training and sampling algorithms. The model design is illustrated in Figure 3 regarding how the model is trained and how it handles one sampling step. 6 Algorithm 1 Training of CADD 1: Input: dataset , network fθ(), masking schedule {αt}T t=1, continuous schedule {γt}T t= 2: while not converged do 3: 4: draw data x0 , draw Uniform(1, ..., ) mask out each token position xi 0 with probability 1 αt to obtain xt 5: 6: 7: 8: 9: 10: form discrete embeddings zdiscwθ(xt), form continuous embeddings if xi = then ztwθ(x0) else zt 0 end if for position {1, ..., n}, if xi 1 γtϵ, ϵ (0, I) + end for zt zdisc + zt, compute logits fθ( zt) optimize with cross entropy loss in equation 4.17 = do, zi γtzi 11: 12: 13: 14: end while Algorithm 2 Sampling of CADD 1: Input: desired number of samples B, network fθ(), t=1, schedules {αt}T t=1, {γt}T 2: while not reach desired size do init: xT (m, ...m), zT 3: for = T, . . . , 1 do 4: 5: 6: 7: for = 1, . . . , n, if xi compute ρflip (equation 4.15) determine whether to unmask xi and ρkeep i.i.d. (0, I) = do Cat(ρflip fθ(xi 8: (cid:16) (cid:0) ˆzi µt t, zi if xi m) t) + ρkeep t1 then draw zi (cid:1), βtId with equation 4.13 (cid:17) t1 t1 t1 wθ(xi t1) 0,θ, zi else zi end if end for 9: 10: 11: end for 12: 13: end while Training Loss According to equation 4.14, the model aims to learn to maximize the likelihood of discrete path, and also minimize the reweighted MSE in equation 4.16. Inspired by continuous diffusion models that used for categorical modeling, e.g., CDCD (Dieleman et al., 2022) and Plaid (Gulrajani & Hashimoto, 2023), we may estimate ˆz0,θ := (cid:80) pθ( ˆx0 = xt, zt) wθ,v and just train the model to predict correct categorical output to minimize the KL divergence. Thus, we choose to train CADD by minimizing simple cross entropy loss as following and the training is summarized in Algorithm 1: LCADD = EtUniform(1,...,T )Eq(xt,ztx0) (cid:2) (cid:88) i: xi t=m log pθ(xi 0 xi t, zi t)(cid:3) (4.17) Note that we may add the MSE loss in equation 4.16 to the above objective to more accurately estimate the exact variational lower bound. Empirically we find the simplified loss is more computationally efficient, thus we choose to use this loss for most of our experiments unless otherwise specified. Sampling The sampling start from the last timestep of the diffusion chain. Under the absorbing forward, αT 0, hence p(xT ) = δxT =m, i.e., all tokens are masked. Since all positions are masked at , the continuous (cid:1), which matches the forward marginal at . For each timestep, given prior is p(zT xT ) = (cid:81)n (xt, zt), the network predicts i=1 (cid:0)zi ; 0, Id πθ,i(v) := 1 (cid:88) k= pθ( ˆxi 0 = xt, z(k) ) 1 for each position i. t1 = zi t1 = xi t). For masked position, with probability 1αt1 1αt For an unmasked position, the absorbing chain keeps xi is deterministic zi = wθ(xi πθ,i() to unmask it. If this masked position is unmasked in this step, the continuous latent zi If it remains masked, zi 0,θ, zi following equation 4.13. The full sampling process is shown in Algorithm 2. Note the choice of ˆzi options: almost surely and the continuous variable , it draws clean token t1 wθ,v. (cid:17) (cid:1), βtId has two moves along the continuous diffusion trajectory zi (cid:16) t1 (cid:0) ˆzi µt t1 0,θ hard: ˆz0,θ = wθ( ˆx0), ˆx0 = arg max πθ,i(v) soft: ˆz0,θ := πθ,i(v) wθ,v. (4.18) (cid:88) These two choices are both valid to use depending on whether we are looking for mode-covering or modeseeking behavior, i.e., better context localization or better diversity, respectively. In our main experiments we 7 Figure 4 Unconditional text generative evaluation of model trained on OpenWebText (OWT) data. All method are evaluated with 128, 256, 512 1024, and 4096 sampling steps. MAUVE (Left Panel, higher is better) and generative perplexity (Right Panel, measured using GPT2-Large, lower is better) are reported. keep the hard option, and our empirical exploration in Appendix C.3 justify these two choices could meet the demand of these two behavior. Moreover, although CADD may leverage multi-sample for the x0 distribution estimation, for fair comparison with baselines, we keep = 1 for most of our experiments. More detailed studies are also shown in the Appendix C.3. Implementation We follow the common-used design of the model architecture to let fθ() predict logits for categorical distribution. The discrete path follows earlier masked-diffusion setups: starting from x0, we mask subset of positions to obtain xt, embed the mixed sequence with the learnable table and form zdisc = wθ(xt). The only difference is the model needs to take an additional variable zt input for the continuous embeddings. To achieve this, we first form the clean embeddings z0 = wθ(x0), and then apply noise only at masked positions using the forward marginal equation 4.7 to obtain zt. We fuse zdisc and zt by element-wise addition zt := zdisc + zt, and feed zt to the backbone fθ to produce per-position logits."
        },
        {
            "title": "5 Experiments",
            "content": "In this section we present experiments to validate the proposed CADD model through experiments on text, image, and code generation benchmarks. The evaluations are designed to assess the models performance across diverse data modalities and scales."
        },
        {
            "title": "5.1 Text Generation\nExperiment setting For text generation, we strictly follow the experimental setup of the Masked Diffusion\nLanguage Model (MDLM) (Sahoo et al., 2024), a common configuration for this task. We train our CADD\nmodels on the OpenWebText (OWT) dataset (Gokaslan & Cohen, 2019). Data is tokenized using the GPT-2\ntokenizer with a vocabulary size of |V| = 50, 257 (Radford et al., 2019), and sequences are fixed to a length of\nn = 1, 024. To be consistent with the baselines, we use a Discrete DiT backbone (Peebles & Xie, 2023) with\napproximately 168M parameters, and train with same number of iterations. All training hyper-parameters\nare identical to those in MDLM.",
            "content": "Evaluation. We mainly compare the performance with discrete diffusion baselines in terms of the generative quality, and our evaluation protocol strictly follows that of Wang et al. (2025). We compare the performance against discrete diffusion baselines using two metrics: the MAUVE score (higher is better) (Liu et al., 2021; Pillutla et al., 2021) and generative perplexity (lower is better) (Lou et al., 2024). Further details on the evaluation setup are located in Appendix B. Main Results. Figure 4 presents the results for unconditional text generation on the OpenWebText (OWT) dataset, comparing CADD with SEDD (absorb) and MDLM across range of sampling steps {128, 256, 512, 1024, 4096}. Within the range 1024, all models show improvement as the number of sampling steps increases. We can notice CADD demonstrates stronger and consistent gains as steps increase compared to SEDD and MDLM in terms of both metrics. Plotting the x-axis on log2 scale reveals that the performance trend is approximately linear. 8 Table 1 FID and IS evaluation on CIFAR-10. The arrow symbols denote lower/higher is better respectively. Baseline results are quoted from Chao et al. (2025). Table 2 FID evaluation using model unconditionally trained on ImageNet (32 32 resolution). Method FID () IS () CADD (NFE=512) 2.88 10.04 Discrete MDM (NFE=512) MDM-Mixture (NFE=512) MDM-Prime (NFE=512) D3PM Absorb (NFE=1,000) D3PM Gauss. (NFE=1,000) CTDD-DG (NFE=1,000) Tau-LDR (NFE=1,000) Discrete FM (NFE=1,024) Continuous Continuous FM Bit Diffusion StyleGAN+ADA DDPM 4.66 4.80 3.26 30.97 7.34 7.86 3.74 3.63 6.35 3.48 3.26 3.17 9.09 9.22 9.67 6.78 8.56 8.91 9.49 - - - 9.74 9.46 Method CADD (NFE=1,024) FID () 3.74 Discrete MDM (NFE=1,024) MDM-Mixture (NFE=1,024) MDM-Prime (NFE=1,024) Continuous NDM DDPM MSGAN i-DODE (SP) i-DODE (VP) Stochastic Interp. Soft Trunc. DDPM ScoreFlow (subVP) ScoreFlow (VP) Continuous FM 7.91 8.08 6.98 17.02 16.18 12.30 10.31 9.09 8.49 8.42 8.87 8.34 5. Extending the sampling process to = 4096 further demonstrates CADDs scaling capabilities at inference time, as it continues to improve while the masked-only baselines stagnate or degrade. From = 1024 to 4096, CADDs MAUVE score still increases by 0.3, and its generative perplexity is scored from 44.6 to 35.3. MDLMs performance slightly worsens, which is consistent with the observation that mask-only diffusion models scale poorly with (Wang et al., 2025). Overall, CADD consistently show performance gain across all tested number of sampling steps over the mask-only discrete diffusion models, validating the effectiveness of the proposed continuous-augmented space. Computation With our design, the number of trainable parameters in the network is actually the same as MDMs, which is 168M for the used DiT architecture. We also measure the inference time for 5000 samples on 8 H100 GPUs, where both MDLM and CADD take 0.5h. When the number of samples used for ˆz0 is 1, i.e., = 1, the computation in the network is comparable since we only have extra computation in the forward and the fusion (add) operation. The computation cost increases linearly as goes greater than 1."
        },
        {
            "title": "5.2 Image Generation",
            "content": "We train and evaluate our models on the CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Krizhevsky et al., 2017) datasets (resolution 32 32). For both, input images are in RGB channels, thus dimensionality of = 32 32 3 with = 256 pixel values per channel. For fair comparison the MDM baselines, our model architecture follows the one used in Chao et al. (2025); Gat et al. (2024), which is based on the ADM (Dhariwal & Nichol, 2021) architecture. We choose MDM-Prime (Chao et al., 2025) and its variants as our main discrete diffusion baseline. We also include its discrete and continuous diffusion model baselines for comparison (Shih et al., 2022; Ho et al., 2020; Song et al., 2021; Austin et al., 2021a; Campbell et al., 2022; Gat et al., 2024; Nisonoff et al., 2025; Lipman et al., 2022; Chen et al., 2023; Bartosh et al., 2023; Tran et al., 2019; Zheng et al., 2023b; Albergo & Vanden-Eijnden, 2023; Kim et al., 2022). To assess sample quality, we report Fréchet Inception Distance (FID) and Inception Score (IS), computed with 50,000 random samples. We follow MDM variants to unconditionally sample images with same number of function evaluation (NFE) and report results on CIFAR-10 in Table 1. With the same NFE, we can observe CADD improves upon MDMs by significant margin. Attaining an FID of 2.88 and an Inception Score of 10.04 with 512 function evaluations (NFE), CADD surpasses the MDM variants by 0.38 in terms of FID and represents the best result among all compared method. On ImageNet-32, as shown in Table 2, the observation is constent, where CADD obtains FID of 3.74 and outperforms all reported baselines. The qualitative generated samples are provided in Appendix for visual justifications. 9 Table 3 Benchmark coding capacities of AR and Diffusion LLMs in 7/8B scale. We follow the evaluation settings in DiffuCoder (Gong et al., 2025b), where EvalPlus is computed as the average of HE+ and MBPP+. The best performance in AR and Diffusion LLMs are marked in bold. Model HumanEval MBPP EvalPlus BigCodeBench (C) - Plus - Plus Full Hard AR Qwen2.5-Coder 75.9 61.4 OpenCoder (Huang et al., 2024) 66.5 63.4 79.9 70.4 61.6 51.8 Diffusion LLaDA (Nie et al., 2025) Dream (Ye et al., 2025) DiffuCoder 35.4 30.5 56.7 50.0 67.1 60. 50.1 42.1 68.7 57.4 74.2 60.9 CADD (ours) CADD (ours, DiffuCoder init) 75.7 63.2 72.0 63.4 73.8 64.6 73.9 60.4 56.6 66.9 46.1 40.5 36.3 53.7 60. 63.3 62.5 18.9 23.6 40.2 42.1 41.5 16.2 9.5 4.1 4.1 12.8 17.6 15. Avg. 52.2 55.0 30.2 43.4 52.6 55.7 55."
        },
        {
            "title": "5.3 Code Generation",
            "content": "For large-scale setting, we conduct code generation experiments based on the DiffuCoder pipeline (Gong et al., 2025b). The DiffuCoder base model training process involves adapting pretrained autoregressive LLM (e.g., Qwen2.5-coder (Hui et al., 2024)) into discrete diffusion model by annealing its attention mechanism from causal to bidirectional (Gong et al., 2025a). The resulting model is then trained using masking diffusion loss (Shi et al., 2024). In this context, we evaluate our method using the following two distinct configurations. (i) Vanilla CADD: We follow the DiffuCoder procedure to adapt the Qwen2.5-coder model. Instead of using the MDM loss, we train the model from the beginning with our proposed CADD loss. (ii) CADD (fine-tuned): To demonstrate CADDs effectiveness as fine-tuning objective, we initialize our model from pretrained DiffuCoder checkpoint and then continue training it with the CADD loss. To ensure fair comparison, both CADD variants are trained on the same 65B total tokens and use the same training hyperparameters as the original DiffuCoder. In the evaluation, we follow their settings to test the model performance on three coding benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021b), and BigCodeBench (Zhuo et al., 2024). Table 3 reports the pass@1 performance, where the results of both autoregressive (AR) and diffusion-based LLMs are included, with an overall average score provided. Compared with Diffusion-based models, CADD emerges as the strongest diffusion model, outperforming competitors on nearly all metrics. Compared to the previous leading DM, DiffuCoder, CADD significantly improves performance on HumanEval, e.g., from 67.1 to 72.0; on the challenging BigCodeBench-Hard subset, we can also observe significant performance gain from 12.8 to 17.6. CADD is also highly competitive with leading AR code models. It surpasses Qwen2.5-Coder across all benchmarks and achieves higher overall average than OpenCoder (55.7 vs. 55.0). When using Diffucoders checkpoint as initialization for continuous space finetuning, we also find CADD improves the Diffucoders performance on HumanEval (73.8 vs. 67.1) and BigCodeBench (41.5 vs. 40.2)."
        },
        {
            "title": "6 Conclusion",
            "content": "In standard discrete diffusion, information is lost abruptly when tokens are replaced by an absorbing state. Inspired by Gaussian diffusion, where the data signal degrades smoothly, CADDs core idea is to introduce an auxiliary continuous space to guide the discrete process. This space is designed to retain semantic information, providing smooth continuous representation of token even after its discrete form has been absorbed. By conditioning on it, the model can better be aware of what was supposed to be in the masked position. This leads to more coherent and contextually accurate generations, as the model has stronger grasp of the underlying meaning. With extensive empirical justification on text, image and code generation, we justify that with the continuous augmented space proposed in CADD, the discrete diffusion models consistently generate higher quality samples across these different tasks and achieve strong performance."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors thank Josh Susskind, Irina Belousova, Miguel Angel Bautista, Richard Bai, Shuangfei Zhai, Tatiana Likhomanenko, Xiaoming Zhao, Yuyang Wang and Zijin Gu for insightful feedbacks and discussions. We also thank Marco Cuturi Cameto and Miguel Angel Bautista for helping setup the template of our arXiv version."
        },
        {
            "title": "References",
            "content": "Michael S. Albergo and Eric Vanden-Eijnden. Building Normalizing Flows with Stochastic Interpolants. In Proc. Int. Conf. on Learning Representations (ICLR), 2023. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, 2021a. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021b. URL https://arxiv.org/abs/2108.07732. Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In International Conference on Machine Learning, 2023. Grigory Bartosh, Dmitry Vetrov, and Christian Naesseth. Neural diffusion models. arXiv preprint arXiv:2310.08337, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators, 2024. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. In Advances in Neural Information Processing Systems, 2022. Chen-Hao Chao, Wei-Fang Sun, Hanwen Liang annd Chun-Yi Lee, and Rahul G. Krishnan. Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2025. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning. In ICLR, 2023. Chaoran Cheng, Jiahan Li, Jian Peng, and Ge Liu. Categorical flow matching on statistical manifolds. In Advances in Neural Information Processing Systems, 2024. Oscar Davis, Samuel Kessler, Mircea Petrache, İsmail İlkan Ceylan, Michael M. Bronstein, and Avishek Joey Bose. Fisher flow matching for generative modeling over discrete data. In Advances in Neural Information Processing Systems, 2024. Justin Deschenaux and Caglar Gulcehre. Beyond Autoregression: Fast LLMs via Self-Distillation Through Time. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, 2021. Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv:2211.15089, 2022. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. 11 Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, and Linli Xu. Empowering diffusion models on the embedding space for text generation. arXiv preprint arXiv:2212.09412, 2022. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. In Advances in Neural Information Processing Systems, 2024. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=jQj-_rLVXsj. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. Scaling diffusion language models via adaptation from autoregressive models. In The Thirteenth International Conference on Learning Representations, 2025a. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025b. URL https://arxiv.org/abs/2506.20639. Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow networks. arXiv:2308.07037, 2023. Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, and Shuangfei Zhai. Starflow: Scaling latent normalizing flows for highresolution image synthesis. arXiv preprint arXiv:2506.06276, 2025. Ishaan Gulrajani and Tatsunori B. Hashimoto. Likelihood-based diffusion language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 35b5c175e139bff5f22a5361270fce87-Abstract-Conference.html. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv:2210.17432, 2022. Marton Havasi, Brian Karrer, Itai Gat, and Ricky TQ Chen. Edit flows: Flow matching with edit operations. arXiv preprint arXiv:2506.09018, 2025. Zhengfu He, Tianxiang Sun, Qiong Tang, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: ImIn Annual Meeting of the Association for proving generative masked language models with diffusion models. Computational Linguistics, 2023. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Proc. of Int. Conf. on Neural Information Processing Systems (NeurIPS), 2020. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. In Advances in Neural Information Processing Systems, 2021. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, et al. Opencoder: The open cookbook for top-tier code large language models. ArXiv preprint, abs/2411.04905, 2024. URL https://arxiv.org/abs/2411.04905. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. ArXiv preprint, abs/2409.12186, 2024. URL https://arxiv.org/abs/ 2409.12186. Jaehyeong Jo and Sung Ju Hwang. Generative modeling on manifolds through mixture of riemannian diffusion processes. In International Conference on Machine Learning, 2024. Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson, Iz Beltagy, Matthew Peters, and Arman Cohan. TESS: Text-to-text self-conditioned simplex diffusion. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23472361, St. Julians, Malta, March 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.eacl-long.144. URL https://aclanthology.org/2024.eacl-long.144/. 12 Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft Truncation: Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation. In Proc. Int. Conf. on Machine Learning (ICML), 2022. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. arXiv preprint arXiv:2107.00630, 2021. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):8490, 2017. Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. ArXiv preprint, abs/2505.16839, 2025. URL https://arxiv.org/abs/2505.16839. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. In Advances in Neural Information Processing Systems, 2022. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Guan-Horng Liu, Tianrong Chen, Evangelos Theodorou, and Molei Tao. Mirror diffusion models for constrained and watermarked generation. Advances in Neural Information Processing Systems, 36:4289842917, 2023. Lang Liu, Krishna Pillutla, Sean Welleck, Sewoong Oh, Yejin Choi, and Zaid Harchaoui. Divergence frontiers for generative models: Sample complexity, quantization effects, and frontier integrals. Advances in Neural Information Processing Systems, 34:1293012942, 2021. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. In International Conference on Machine Learning, 2024. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-Instruct: universal approach for transferring knowledge from pre-trained diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=MLIs5iRq4w. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. ArXiv preprint, abs/2410.18514, 2024. URL https://arxiv.org/abs/2410.18514. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large Language Diffusion Models. arXiv:2502.09992 [cs.CL], 2025. Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete statespace diffusion and flow models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=XsgHl54yO7. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:48164828, 2021. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam S. Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, WeiNing Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali K. Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dmitry Vengertsev, Edgar Schönfeld, Elliot 13 Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. arXiv:2410.13720, 2024. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. Subham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander Rush, Yair Schiff, Justin Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In Advances in Neural Information Processing Systems, 2024. Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr In Forty-second International Conference on Machine Learning, 2025. URL Kuleshov. The diffusion duality. https://openreview.net/forum?id=9P9Y8FOSOk. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. In Advances in Neural Information Processing Systems, 2024. Andy Shih, Dorsa Sadigh, and Stefano Ermon. Training and inference on any-order autoregressive models the right way. In Advances in Neural Information Processing Systems, 2022. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In Proc. Int. Conf. on Machine Learning (ICML), 2015. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. ScoreIn International Conference on Learning based generative modeling through stochastic differential equations. Representations, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3221132252. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/song23a.html. Yuxuan Song, Zhe Zhang, Yu Pei, Jingjing Gong, Qiying Yu, Zheng Zhang, Mingxuan Wang, Hao Zhou, Jingjing Liu, and Wei-Ying Ma. Shortlisting model: streamlined simplexdiffusion for discrete variable generation. arXiv preprint arXiv:2508.17345, 2025a. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025b. Hannes Stärk, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi S. Jaakkola. Dirichlet flow matching with applications to DNA sequence design. In International Conference on Machine Learning, 2024. Jaesung Tae, Hamish Ivison, Sachin Kumar, and Arman Cohan. TESS 2: large-scale generalist diffusion language model. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 21171 21188, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1029. URL https://aclanthology.org/2025.acl-long.1029/. Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Linxiao Yang, and Ngai-Man Cheung. Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game. In Proc. of Int. Conf. on Neural Information Processing Systems (NeurIPS), 2019. Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking Discrete Diffusion Models with Inference-Time Scaling. arXiv:2503.00307 [cs.LG], 2025. 14 Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-GAN: Training GANs with diffusion. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=HZf7UbpWHuA. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= JprM0p-q0Co. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. ArXiv preprint, abs/2505.15809, 2025. URL https://arxiv.org/abs/2505.15809. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024. Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Ángel Bautista, Navdeep Jaitly, and Joshua Susskind. Normalizing flows are capable generative models. In Forty-second International Conference on Machine Learning, 2025. Ruixiang Zhang, Shuangfei Zhai, Jiatao Gu, Yizhe Zhang, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Josh Susskind, and Navdeep Jaitly. Flexible language modeling in continuous space with transformer-based autoregressive flows. arXiv preprint arXiv:2507.00425, 2025a. Ruixiang Zhang, Shuangfei Zhai, Yizhe Zhang, James Thornton, Zijing Ou, Joshua Susskind, and Navdeep Jaitly. Target concrete score matching: holistic framework for discrete diffusion. In Forty-second International Conference on Machine Learning, 2025b. Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Joshua Susskind, and Navdeep Jaitly. Planner: Generating diversified paragraph via latent language diffusion model. Advances in Neural Information Processing Systems, 36: 8017880190, 2023. Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. In Conference on Neural Information Processing Systems (NeurIPS), 2020. Yixiu Zhao, Jiaxin Shi, Feng Chen, Shaul Druckmann, Lester Mackey, and Scott Linderman. Informed Correctors for Discrete Diffusion Models. arXiv:2407.21243 [cs.LG], 2024. Huangjie Zheng and Mingyuan Zhou. Exploiting chain rule and Bayes theorem to compare probability distributions. Advances in Neural Information Processing Systems, 34:1499315006, 2021. Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=HDxgaKk956l. Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, and Mingyuan Zhou. Learning stackable and skippable LEGO bricks for efficient, reconfigurable, and variableresolution diffusion modeling. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=qmXedvwrT1. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. In Proc. Int. Conf. on Machine Learning (ICML), 2023b. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. reparameterized discrete diffusion model for text generation. In Conferenec on Language Modeling, COLM, October 7-9, 2024, Philadelphia, PA, 2024b. Mingyuan Zhou, Tianqi Chen, Zhendong Wang, and Huangjie Zheng. Beta diffusion. Advances in Neural Information Processing Systems, 36:3007030095, 2023. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=QhqQJqe0Wq. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. ArXiv preprint, abs/2406.15877, 2024. URL https://arxiv.org/abs/2406.15877."
        },
        {
            "title": "A Detailed Derivations and Proof",
            "content": "A.1 ELBO Derivation Forward chain. For any observation x0, the forward diffusion constructs as q(x1:T , z1:T x0) = (cid:89) t=1 (cid:0)xt, zt xt1, zt1, x0 qt (cid:1), (A.1) note we represent (cid:0)x0, z0 Reverse generative model. (cid:1) as x0 since the transform wθ is deterministic. pθ(x0, x1:T , z1:T ) = pT (xT , zT ) (cid:104) (cid:89) t= (cid:0)xt1, zt1 xt, zt pθ (cid:1)(cid:105) pθ(x0 x1, z1). (A.2) Proposition 3 (ELBO decomposition). Given the forward chain defined in equation A.1 and reverse model pθ in equation A.2, we have the decomposed ELBO as following: log pθ(x0) Eq(x1,z1x0) (cid:124) (cid:2) log pθ(x0 x1, z1)(cid:3) (cid:125) (cid:123)(cid:122) reconstruction term at t=1 (cid:104) DKL (cid:0)q(xt1, zt1 xt, zt, x0) (cid:13) (cid:13) pθ(xt1, zt1 xt, zt)(cid:1)(cid:105) Eq(xt,ztx0) (cid:88) t=2 (cid:124) (cid:123)(cid:122) denoising matches for t>1 DKL (cid:124) (cid:0)q(xT , zT x0) pT (xT , zT )(cid:1) (cid:125) (cid:123)(cid:122) prior match at . (cid:125) (A.3) If q(xT , zT x0) = pT (xT , zT ) for all x0, then the prior match term is zero. The bound is tight if and only if pθ(xt1, zt1 xt, zt) = q(xt1, zt1 xt, zt, x0) for all 2, and the prior match is zero, and the decoder pθ(x0 x1, z1) equals the true conditional induced by the joint. Recap the forward kernel defined in equation 4.3 and equation 4.4: q(xt xt1) = (cid:89) i= Categorical(cid:0)xi t; xi t1 (cid:1), Qt = (1 βt)I + βt 1 m. q(zt zt1, xt1, xt, x0) = (cid:89) i=1 zi δ(zi N(cid:0)zi t; N(cid:0)zi t; t1), γt zi γt zi t1, (1 γt)Id t1, (1 γt)Id xi (cid:1), xi (cid:1), xi = m, = m, xi = m, xi t1 = m, t1 = m. Proof of Proposition 3. The proof is mostly done in Sohl-Dickstein et al. (2015) and Ho et al. (2020). We include the following proof to show the generalized version with added variables. Start from the evidence identity and apply Jensen inequality: log pθ(x0) = log (cid:90) q(x1:T , z1:T x0) pθ(x0, x1:T , z1:T ) q(x1:T , z1:T x0) dx1:T dz1:T (cid:104) Eq(x1:T ,z1:T x0) =: L(θ; x0). log pθ(x0, x1:T , z1:T ) log q(x1:T , z1:T x0) (cid:105) (A.4) 16 Insert the model and forward factorizations equation A.2 and equation A.1: L(θ; x0) = Eq (cid:104) log pT (xT , zT ) + (cid:88) t=2 log pθ(xt1, zt1 xt, zt) + log pθ(x0 x1, z1) (cid:105) log q(xt, zt xt1, zt1, x0) . (cid:88) t=1 For each 2 use Bayes rule under q: q(xt, zt xt1, zt1, x0) = q(xt1, zt1 xt, zt, x0) q(xt, zt x0) q(xt1, zt1 x0) . Taking Eq[log()] of equation A.7 and rearranging gives, for 2, (cid:104) Eq log pθ(xt1, zt1 xt, zt) log q(xt, zt xt1, zt1, x0) (cid:105) (cid:104) = Eq(xt,ztx0) DKL (cid:2) log q(xt, zt x0)(cid:3) + Eq (cid:2) log q(xt1, zt1 x0)(cid:3). Sum equation A.8 over = 2, . . . , . The last two expectations telescope: Eq (cid:0)q(xt1, zt1 xt, zt, x0) pθ(xt1, zt1 xt, zt)(cid:1)(cid:105) (cid:88) Eq (cid:2) log q(xt, zt x0)(cid:3) + (cid:88) t= Eq (cid:2) log q(xt1, zt1 x0)(cid:3) t=2 = Eq (cid:2) log q(x1, z1 x0)(cid:3)Eq (cid:2) log q(xT , zT x0)(cid:3). Plug this back into equation A.6 and group the boundary terms with log pT : L(θ; x0) = Eq (cid:2) log pθ(x0 x1, z1)(cid:3) (cid:88) (cid:104) Eq(xt,ztx0) (cid:0)q(xt1, zt1 xt, zt, x0) pθ(xt1, zt1 xt, zt)(cid:1)(cid:105) DKL t=2 (cid:16) (cid:2) log q(xT , zT x0)(cid:3) Eq Eq (cid:2) log q(x1, z1 x0)(cid:3). Eq (cid:2) log pT (xT , zT )(cid:3)(cid:17) Now we recoginize the prior KL to obtain L(θ; x0) = Eq (cid:2) log pθ(x0 x1, z1)(cid:3) (cid:88) (cid:104) Eq(xt,ztx0) (cid:0)q(xt1, zt1 xt, zt, x0) pθ(xt1, zt1 xt, zt)(cid:1)(cid:105) DKL (A.5) (A.6) (A.7) (A.8) (A.9) (A.10) t=2 DKL (cid:0)q(xT , zT x0) pT (xT , zT )(cid:1) Eq (cid:124) (cid:2) log q(x1, z1 x0)(cid:3) (cid:123)(cid:122) (cid:125) =:C(x0) . (A.11) Note the last term C(x0) does not involve pθ and can be dropped, and we normally do not optimize the (cid:0)q(xT , zT x0) pT (xT , zT )(cid:1) as we let the schedule to make this statistical distance is last KL term DKL sufficiently small. A.2 Forward We can derive the following lemma for the marginal at time step t. Lemma 2 (Continuous marginal conditioned on (xt, x0)). Recap γt := (cid:81)t have continuous marginal conditioned on (xt, x0) as s=1 γs. For each position i, we q(zi xi t, xi 0) = (cid:40) δ(zi zi 0), N(cid:0)zi γt zi t; 0, (1 γt)Id xi (cid:1), xi = xi 0, = m, with zi 0 = wθ(xi 0). Hence We finally have q(zt xt, x0) = (cid:89) i= q(zi xi t, xi 0) = (cid:104) (cid:89) δ(zi zi 0) (cid:104) (cid:89) (cid:105) (zi t; i: xi t=m i: xi t=m γtzi 0, (1 γt)Id) (cid:105) . Then what follows proves Proposition 2. We first prove the conditional independency between zt and xt1 given (xt, x0) in the reverse context. Lemma 3 (Conditional independency between zt and xt1 given (xt, x0)). zt and xt1 are conditionally independent given (xt, x0) based on the forward kerned defined in equation 4.4. To prove Proposition 1, we first prove the following lemma: Proof of Lemma 2 and Lemma 3. If xi kernel gives zi = zi 0 almost surely, which is the first line of equation 4.4. = xi then the absorbing chain implies xi = for t, so the If xi = m, use the law of total probability over xi t1 {xi 0, m}. When xi When xi marginal zi 0, (1 γt)I). t1 = (first time masking at t), the second branch of the kernel gives zi t1 = (already masked), the third branch composes diffusion forward step with the previous ( γt zi t1 ( γt1 zi 0, (1 γt1)I), which yields N(cid:0) zi γtγt1 zi 0, (1 γtγt1)I(cid:1) = N(cid:0) γt zi 0, (1 γt)I(cid:1). This proves the masked line of equation 4.4. Then leveraging these results, we can easily prove Proposition 1. Proof of Proposition 1. Expand the path marginal, use equation 4.2 and Lemma 2, and factor over positions. The sum over discrete paths yields q(xt x0); conditioning on xt reduces the continuous part to Lemma 2. A.3 Reverse Proof of Proposition 2. We first prove the factorization shown in equation 4.10. To achieve this, we just need to show: q(xt1, zt1 xt, zt, x0) = q(xt1 xt, zt, x0) q(zt1 xt, zt, xt1, x0) = q(zt xt1, xt, x0)q(xt1 xt, x0) q(zt xt, x0) q(zt1 xt, zt, xt1, x0) = q(xt1 xt, x0) q(zt1 xt, zt, xt1, x0), (A.12) (A.13) (A.14) where q(zt xt1, xt, x0) = q(zt xt, x0) by the conditional independence according to Lemma 3. Then the discrete part is the same as discrete diffusion, we may leverage the results from Austin et al. (2021a); Sahoo et al. (2024); Shi et al. (2024) to complete the proof of equation 4.11. Next, we prove the closed form of the continuous part, q(zt1 xt, zt, xt1, x0), by case analysis based on the discrete states. We start with Bayes rule for the continuous variables: q(zt1 xt, zt, xt1, x0) q(zt zt1, xt) q(zt1 xt1, x0). (A.15) The forms of the two terms on the right-hand side are Gaussian distributions, but will change depending on the discrete states and it leads to the three cases. Case 1: No mask at (xt = x0). In this case, no noise has been applied to the embedding up to timestep t-1. Thus, both terms directly have Dirac delta function: q(zt1 xt1 = x0, x0) = δ(zt1 z0). The posterior is therefore also Dirac delta function, proving the first part of equation 4.12. 18 Case 2: First time unmask at (xt = m, xt1 = x0). In this case, the first term in equation A.15 is Gaussian while the second term becomes Dirac δ(zt1 z0). The multiplication yields Dirac posterior at the same point: q(zt1 xt1 = x0, x0) = δ(zt1 z0). Case 3: Remaining masked at (xt = m, xt1 = m). In this case, both terms remain in Gaussian distribution, and the parameters are same with normal Gaussian diffusion models. The product of these two Gaussians is new Gaussian, allowing us to use the standard derivation for DDPM (Ho et al., 2020), by completing the square on the exponent, we find that the resulting distribution is (zt1; µt(zt, z0), βtI), which proves the last part of equation 4.12. Proof of Lemma 1. Using the results from Proposition 2, for single position i, the exact onestep KL at timestep > 1 inside the ELBO is (cid:104) DKL(x0, t) := Eq(xt,ztx0)"
        },
        {
            "title": "DKL",
            "content": "(cid:0)q(xt1, zt1 xt, zt, x0) (cid:13) (cid:13) pθ(xt1, zt1 xt, zt)(cid:1)(cid:105) , (A.16) For the unmasked positions (xt = m), the KL is identically 0, and plug in equation 4.10, 4.11 and 4.12, we recover equation 4.14 exactly as DKL (cid:0)q( xt, zt, x0) (cid:13) (cid:13) pθ( xt, zt)(cid:1) = ρflip (cid:124) (cid:2) log pθ(x0xt, zt)(cid:3) (cid:125) (cid:123)(cid:122) discrete + ρkeep (cid:124) DKL (cid:123)(cid:122) continuous cont , (cid:125) with the ratio that determines whether the position is going to be flipped to unmask: ρkeep = 1 αt1 1 αt , ρflip = αt1 βt 1 αt = αt1 αt 1 αt . The discrete KL part exactly recovers the results from the absorbing discrete diffusion models (Austin et al., 2021a; Sahoo et al., 2024; Shi et al., 2024), and the continuous KL divergence: (cid:16) KL = DKL Dcont (µ, βtId) (cid:13) (cid:13) (µv, βtId) (cid:13) (cid:17) , µ = µt(z0, zt), µv = µt( ˆz0, zt), (A.17) where we recap µt(ζ, zt) = γt1(1 γt) 1 γt ζ + γt(1 γt1) 1 γt zt, βt = (1 γt1)(1 γt) 1 γt . This results in the comparison between z0 and ˆz0 and the KL divergence reduced to: Dcont KL = 1 2 βt (cid:13) (cid:13) µt(z0, zt) µt( ˆz0,θ, zi t)(cid:13) 2 (cid:13) = a2 2 βt z0 ˆz0,θ2; at = γt1(1 γt) 1 γt . Remark 1 (On the Alternative Factorization). One could also decompose the posterior using the alternative order from the chain rule: q(xt1, zt1 ) = q(zt1 xt, zt, x0) q(xt1 xt, zt, zt1, x0). While mathematically valid and could provide new properties in the sampling, this factorization is not fully tractable. The first term, q(zt1), is complex Gaussian Mixture Model. More critically, the second term, q(xt1), has no analytical closed form, as it would require inverting the continuous diffusion process and the embedding function to infer discrete state. The factorization in Prop. 2 is therefore adopted as tractable choice for more efficient algorithm implementation."
        },
        {
            "title": "B Detailed Experiment Settings",
            "content": "B.1 Diffusion Settings The CADD forward process has two coupled components, each with its own schedule. Discrete schedule: we adopt the MDLM log-linear masking schedule for the discrete process (Sahoo et al., 2024). The discrete forward corruption uses continuous-time α(t) = 1 t, with [0, 1]. Continuous schedule: to keep the meaning of time aligned, we set the continuous latent to follow linear flow-matching path to isotropic noise (Lipman et al., 2022), i.e., if the position is masked, we have zt = (1 t)z0 + tϵ, ϵ (0, I). Multi-sample estimation: we by default set = 1 for the estimation of ˆx0,θ for fair comparison with the baselines. We provide ablation studies to demonstrate the effect of > 1. B.2 Experiment-Specific Settings Text Generation In our main experiments, including ablation studies that used to explore the properties of CADD, we train the models on OpenWebText. Following the standard MDLM pre-processing (Sahoo et al., 2024), we use the GPT-2 tokenizer, resulting in vocabulary of 50,257 tokens. The sequence length is fixed at 1,024. Our text model is 12-layer DiT with 12 attention heads and an embedding dimension of 768, totaling approximately 168M parameters. During training, we keep the same training configuration, i.e., we train for about 2M steps with batch size of 256 to match the total 262B tokens seen in the training. We use the AdamW optimizer with learning rate warmed up from 0 to 3 104. The results in Table 4 and Table 5, are based on Text8 and LM1B dataset, where we strictly follow the training setting in Jo & Hwang (2024) and Sahoo et al. (2024). Please refer their experiment settings for more details. For evaluation, we follow ReMDM (Wang et al., 2025)s evaluation setting, where we randomly sample 5,000 text samples with length = 1, 024, using {128, 256, 512, 1024, 4096} sampling steps. The sampled token sequences are used to compute MAUVE score, generative perplexity with GPT2-Large model, and entropy. Image Generation We experiment on CIFAR-10 and ImageNet (with resolution 32 32), which consists of 50,000 and 1,281,149 natural images respectively. CIFAR-10 already has 32 32 resolution. For ImageNet images, we follow the preprocessing used in EDM (Karras et al., 2022), i.e., using center-crop to make it squared image and rescale to the desired 32 32 resolution. As the model is trained on pixel space, we treat each pixel as discrete token, resulting in vocabulary size 256 at each position. We follow the architecture design used in MDM-Prime (Chao et al., 2025), which is U-Net architecture based on ADM (Dhariwal & Nichol, 2021). For CIFAR-10, we leverage an augmentation pipeline proposed in Karras et al. (2020), but only keep the rotation and flip operation to avoid pixel value changes. We set the augmentation probability as 15% on CIFAR-10, and there is no augmentation used on ImageNet. For both experiments, we set learning rate as 1 104 using AdamW optimizer, and train the model until it has seen 200M and 4B images respectively. In sampling, we adopt cosine decay for temperature with τmax = 2.5, and applied the corrector following Gat et al. (2024). We use the standard Fréchet Inception Distance (FID) and Inception Score for evaluation, computed with 50,000 randomly generated images. Code Generation We use the OpenCoder dataset (Huang et al., 2024), selected by following the recipe in DiffuCoder (Gong et al., 2025b). We strictly follow their settings to initialize the 7B model with Qwen2.5Coder checkpoint, and adapt it to diffusion model using the techniques introduced in Gong et al. (2025a). Then we trained the model on 64 NVIDIA A100 GPUs in total. The training process utilized BF16 mixed precision and was scaled using Fully Sharded Data Parallelism (FSDP). For optimization, we employed the Adam optimizer with peak learning rate of 1 105, preceded by 2,000-step linear warmup. The model is trained with 65B tokens in total. For generation, both models were configured with maximum sequence length of 512 tokens and total of T=512 diffusion timesteps. During generation, we employed top negative entropy remasking sampler. The CADD from scratch variant uses temperature 0.2 and the DiffuCoder initialized variant uses temperature 0.01. 20 Figure 5 Analogous figure of Figure 4. We compare the finetuned checkpoint using CADD objective with CADD and the initialization checkpoint of MDLM. Table 4 Bits Per Character (BPC) results on Text8 test set. Results are taken from Jo & Hwang (2024). Bold denotes the best result in autoregressive or diffusion models. The best diffusion results are marked in bold. Table 5 Test perplexities (PPL; ) on LM1B. The baseline results are taken from Sahoo et al. (2025). For CADD, we report the bound on the discrete likelihood. Best diffusion value is bolded. the dataset for SEDD didnt incorporate sentence packing. Method Autoregressive AR Continuous Diffusion Plaid BFN RDLM Discrete Diffusion Multinomial Diffusion D3PM Uniform D3PM Absorb SEDD Absorb MDLM MD CADD (Ours) BPC () 1.23 1.48 1.41 1.32 1.72 1.61 1.45 1.39 1.40 1.37 1.35 Method Autoregressive Transformer LM1B OWT 22.8 17.5 Diffusion (Uniform-state / Gaussian) D3PM Uniform (Austin et al., 2021a) Diffusion-LM (Li et al., 2022) SEDD Uniform (Lou et al., 2024) UDLM (Deschenaux & Gulcehre, 2025) DUO (Sahoo et al., 2025) Diffusion (absorbing state) D3PM Absorb (Austin et al., 2021a) DiffusionBert (He et al., 2023) SEDD Absorb (Lou et al., 2024) MDLM (Sahoo et al., 2024) 137.9 118.6 40.3 36.7 33.7 76.9 63.8 32.7 31.8 - - 29.7 27.4 25.2 - - 24.1 23.2 CADD (Ours) 31. 23."
        },
        {
            "title": "C Additional Experiment Results",
            "content": "C.1 Training from mask diffusion model From the experiments on code generation, we have seen CADD could be used to finentune an existing discrete (masking) diffusion model to improve the performance. Here we provide complementary evidence that such observation is also valid on text generation. We finetune MDLM checkpoint with CADD objective for additional 50B tokens and evaluate the performance with same setting shown in the main experiments (Figure 4). The results are shown in Figure 5. The red curve shows close performance to the green one that represent CADDs performance, which indicates CADD could efficiently finetune an existing MDM model to enhance the generation capabilities. C.2 Perplexity Evaluation Since the objective of CADD involves the KL divergence of both discrete and continuous component as shown in equation 4.10, it is not fair to compare the tightness of the bound directly with other models, and we choose to focus more on the evaluation of the generated samples. However, our model is still able to compute the likelihood of the discrete part. Here we put the results for reference, aiming to provide more information to help the readers understand how the model helps the discrete diffusion side. Table 4 and Table 5 report the perplexity evaluation on character-level and token-level respectively. The model is trained on Text8 and LM1B, following the settings of Jo & Hwang (2024) and Sahoo et al. (2024). Table 6 Zero-shot perplexities (upper bounds) of models trained for 1M steps on OpenWebText. Baseline results are taken from Sahoo et al. (2025). Best diffusion model performance results are bolded and diffusion values better than AR are underlined. Plaid and D3PM are trained with 0.3M more steps. Method Autoregressive PTB Wikitext LM1B Lambada AG News Pubmed Arxiv Transformer 82.05 25.75 51.25 51. 52.09 49.01 41.73 Diffusion (Uniform-state / Gaussian) SEDD Unifor Plaid UDLM DUO 105.51 142.60 112.82 89. Diffusion (absorbing state) SEDD Absorb D3PM Absorb MDLM 100.09 200.82 95.26 41.10 50.86 39.42 33.57 34.28 50.86 32.83 82.62 91.12 77.59 73. 68.20 138.92 67.01 57.29 57.28 53.57 49.78 49.86 93.47 47.52 CADD (Ours) 93.33 31. 64.98 46.81 82.64 - 80.96 67.81 62.09 - 61.15 62.80 55.89 - 50.98 44. 50.86 - 44.08 40.39 44.53 - 41.89 38.48 - 37.37 42.62 37.52 (a) MAUVE (b) Generative perplexity. Figure 6 Analogous figure of Figure 4, comparing CADD variants using K=1-4 to estimate ˆx0. On Text8, we can see CADD achieves very competitive perplexity results, and is slightly worse than the SoTA RDLM (Jo & Hwang, 2024). On LM1B, we can see CADD achieves the best results among diffusion models when evaluating the discrete part perplexity on both LM1B data and OWT data. Table 6 reports the zero-shot evaluation results of the checkpoint trained on OWT data. We can observe CADD and MDLM both surpasses the perplexity of AR models on Lambada, Pubmed and Arxiv datasets. They have different dataset that they are good at in terms of perplexity, and CADD wins slightly more as it shows better zero-shot perplexity than MDLM on 4/7 tasks. These experiments result jointly indicate that CADD can not only provide strong generation quality, but also provide good discrete likelihood bound. C.3 Ablation studies Comparing the number of samples used for ˆx0 = fθ(xt, z(k) ) We first conduct ablation to study how the number of samples used to compute ˆx0 would affect CADDs performance. Similar to our main experiments in text generation, we compare CADD with {1, 2, 3, 4} in terms of MAUVE and generative perplexity. As shown in Figure 6, increasing both the number of sampling steps and the hyperparameter consistently improves CADDs performance. The value of K, which corresponds to the number of continuous samples It is interesting to see the used for soft hints, has consistent and positive effect on generation quality. largest performance gain, especially for generative perplexity, comes from increasing from 2 to 3. The subsequent gain from = 3 to = 4 is smaller. One possible reason is that when is not large enough, the predicted logits could vary and make the expected value smoothed to be flatten distribution. As gets bigger, the estimation of the correct x0 becomes more accurate, resulting in better generation quality while also increases the compute cost times larger, with trade-off between desired sample quality and inference-time latency. 22 Figure 7 Analogous figure of Figure 4: study of generation variance and diversity across all methods and across different K. We use entropy (higher indicates more stochasticity) are reported. Table 7 Performance vs. fusion method for zt Table 8 Performance vs. estimation method for ˆz0 Fusion Add Concate Reweight MAUVE () 0.24 0.21 0.24 Entropy () 5.31 5.37 5.30 Estimation MAUVE () Entropy () Hard Soft 0.24 0.18 5.31 5.42 We also use entropy as complementary metric to observe the models behavior, and the results are shown in Figure 7. We observe CADD, the highest-quality model in terms of MAUVE and generative perplexity (shown in Figure 4), has the lowest entropy. This indicates that CADD achieves its keeps lower variance in the generation process with concentrating its continuous conditions. The right plot, which analyzes different values of for CADD, shows that larger consistently leads to lower entropy. This reveals the role of as hint mechanism. larger provides stronger, more deterministic \"soft hint\" from the continuous space, preserving smaller variance during generation. However, this does not mean CADD lack of generation diversity, as it still hits strong MAUVE score, indicating it strikes good balance between mode-covering and mode-seeking. On the choice of fusion and ˆz0 estimation In most of our experiments, we choose to fuse the discrete mask token embedding and continuous embedding with addition operation, i.e., zt = zdisc + zt. We consider two extra manners to fuse these two domains: 1) concatenation [zdisc, zt]; 2) reweighted sum αtzdisc + (1 αt)zt, where αt decreases as the position is more likely to be clean (unmasked). The intuition is that when token is unlikely to be masked, the model should lean more on zt to carry semantic content, hence smaller αt. Observing the results in Table 7, MAUVE varies by only 0.03 absolute and Entropy varies by 0.07 absolute across the different choices. These three options do not show significant differences in performance, while concatenation involves an additional projection layer to match the embedding dimension. Morever, we compare the choice of ˆz0 estimation, as discussed in equation 4.18: hard: ˆx0 = arg max πθ,i(v), ˆz0 = wθ(x0) soft: ˆz0,θ := (cid:88) pθ( ˆx0 = xt, zt) wθ,v. From Table 8, hard estimation gives higher MAUVE (+0.06) and slightly lower Entropy (-0.11), indicating this choice is mode-seeking-oriented, where the context is localized faster. The soft estimation encounter shows higher entropy, meaning that the model reveals mode-covering behavior and it pursue better diversity for generation. The properties of these choices are justified. We consider both options are valid for the sampling, depending on which properties we are looking into in practical case. On model architecture Similar to the text generation, we also examine the performance of image generation. We conduct experiments to test the impacts of model architecture and number of function evaluations (NFEs) in the sampling stage. The results are reported in Table 8. As shown, ADM (Dhariwal & Nichol, 2021) shows stronger performance than DDPM++ (Song et al., 2021) across different NFEs. Especially when NFE is sufficiently large as 512, the performance of using ADM + NFE=512 configuration demonstrate significant performance gain. As qualitative justification, we can also observe the last row of Figure 9 has the best visual quality. 23 Model FID () 256 DDPM++ 31.24 30.41 ADM 4.72 4.29 512 4.70 2.88 Figure 8 Ablation results on image generation, trained with DDPM++ and ADM architecture. FID results measured using NFE=64, 256, 512. Figure 9 Qualitative results of CIFAR-10, generated by ADM, using NFE=64,256,512 (from top row to bottom)."
        },
        {
            "title": "D Additional Generated Samples",
            "content": "D.1 Text Samples Researchers conducted study from the Centre for Applied Biology Interface (IRAP) which appeared in unit of the journal Institale Konczakalye Medicine, gave the results: Sleep stimulation were involved in randomized setting compared. The results showed measurable difference when the abnormal disturbances involved in reducing working mood and reward were involved in the absence of serotonin. There was significant difference when serotonin was compared to aerobic stimuli that more positively affected aerobic intensity. These increased tactile disturbances were mediated by dopamine concentration, increased concentration, changes in peak pressure, reduced appetite and spin pressure intensity. The effects were important since aerobic activity was also involved in increased concentration and the brain was involved at the same level. The results were analyzed for physiological stimuli such as the EEG OxyRS. The results showed clear decrease for the subjective rhythm, concentration and reward and reward were involved. Changes also showed expression by changes in the total dopamine function and sleep frequencies were placed within stable pathway. In antidepressant stimulation, the heightened release of dopamine pressure and higher reward reward led to gradual differences in the frequency of dopamine stimulation... We have started recently introducing first parameter support. first command control is custom function that utilizes some combination of variable function to allow editing and transitions and transitions across the inputs. It causes filter support to activate. The extension utilizes the ability to set different inputs and outputs, allowing for different transitions between inputs and outputs, with option to set transitions and transitions around all possible transitions with switch. The extension depends on applying hierarchy of outputs like parameter function that links progress across inputs of different inputs. The workflow also improves inputs, inputs, balance and even random inputs. It is the common variable and function parameter for whatever input modification, variable control and outputs for common variables for possible play what regarding variable control. The basic parameter and many other useful possible explain the potential behind set functions as stack control and stack control. Linimental Changes to Use The parameter is given macro directly changing the linear parameter of filter control, instead, leading to possible read transitions and transitions to change around the inputs. It also supports based movable stack set and also based on inputs and gradient support resulting via the fixed inputs and inputs representing variable selection. It is only possible by binding in the inputs, first input control, first iteration control, variable control, stack control and guarantees that all effects fail to return performance. It can also be easily activated with continuous stack control, stack control and quick stack control. Increased prior warning and filter control are very important to filter control... 24 When it was only briefly used to experience psychic balance, after being removed at the optimal frequency, decreasing the chance for general performance, but when it changed at rest and only even moved at the same intensity, it did not you seriously control the transition from strength to strength. Instead, it also gained the balance in the fluid balance with the normal balance. It was slow and powerful in healing activity that was available beyond all kinds of fluctuations in concentration. So, when the movement was replaced with other possible such qualities with torque, psychic or psychic activity, it still had stronger sensitivity to performance, yet when it received even deeper part of the metabolism, it began becoming more energetic and efficient and therefore, it improves balance. When it was replaced with the meditation and then removed, it moved around rest and finally switched to random balance, and at that point with the max stimulation the amount of basic torque applied at the spell. It also returned to smooth, constant and consistent transition between internal and temporal control, therefore demonstrating that balance also decreases. But even after the activation of the trait, it experienced change in intensity. Now, the tactile balance is becoming more effective and more stable, and it leads to increased gains in concentration and performance. Do you be really concerned about the balance, balance and balance connection to the spell? The positive effect on the tactile balance now comes true to speed. The tactile balance is only determined by strength and balance, and it is still held at constant point at the critical frequency. In fact, the spirit is not moving in the same direction as spell, and it has not been able to experience balance because it moved to another true frequency. !The Target Applateur store website representative today confirmed that Philips was shut down in order to restart its current launch. While Target has not been asked for any explanation, confirmed major shutdown was found. Ït does no longer fully support operating systems, while its switch has been changed to replace the current system running the Double Storage, Fresh, Medium Storage and Hot Storage modules. Please Note that we are working on the matter is not there.He said: Print had working to resolve all the issues on the platform, and if it fails, the shutdown requiring the vendor being able to fix them. We do not know at the reason for the delay and therefore the reasons why we are continuing control will be determined by them and discussed today so we will not go on more comprehensive timetable. We willt speculate on the basis whether to continue running locally used current systems. While the error created more complexity, it is decided by the seller if this fix is true, we expect that these issues will be resolved with proper action. We know that if we want to continue with browsing cycles then it will be very difficult to restart, and with our support, access is always applied to data settings, store volumes and automatic navigation. Loading. D.2 Code Samples 1 from typing import List , Tuple 2 3 4 def rolling_max ( numbers : List [ int ]) -> List [ int ]: 5 6 8 9 10 11 12 14 15 16 \" \" \" From given list of integers , generate list of rolling maximum element found until given moment in the sequence . >>> rolling_max ([1 , 2 , 3 , 2 , 3 , 4 , 2]) [1 , 2 , 3 , 3 , 3 , 4 , 4] \" \" \" result = [] current_max = numbers [0] for num in numbers : if num > current_max : current_max = num result . append ( current_max ) return result Samples 1 \"Generation on HumanEval\" 25 1 def comb_sort ( arr ) : = len ( arr ) gap = swapped = True while (( gap > 1) or swapped ) : swapped = False gap = int (( gap / 1.3) ) if ( gap < 1) : gap = 1 for in range (( - gap ) ) : if ( arr [ ] > arr [( + gap ) ]) : ( arr [ ] , arr [( + gap ) ]) = ( arr [( + gap ) ] , arr [ ]) swapped = True return arr 2 3 4 5 6 8 9 10 11 12 14 15 16 17 assert comb_sort ([5 , 15 , 37 , 25 , 79]) == [5 , 15 , 25 , 37 , 79] Samples 2 \"Generation on MBPP\" 1 from random import randint , seed as random_seed 2 import time 3 import matplotlib . pyplot as plt 4 5 def task_func ( my_list , size =100 , seed =100) : 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 30 31 32 33 34 36 37 38 39 40 \" \" \" Enhances ' my_list ' by appending the number 12 , then generates list of random integers based on the sum of elements in ' my_list ', limited by ' size '. It measures the time taken for this process and plots histogram of the generated random numbers . The size of the random numbers list is determined by the sum of the numbers in ' my_list ', with an upper limit set by ' size '. The random integers are within the range 1 to 100 , inclusive . Parameters : - my_list ( list ) : The input list containing numeric elements . - size ( int ) : Maximum size limit for the generated list of random numbers . Default is 100. - seed ( int ) : Seed value for random number generator for reproducibility . Default is 100. Returns : - tuple : tuple containing the time taken to generate the list ( in seconds , as float ) and the matplotlib Axes object for the histogram . The histogram 's - axis is labeled ' Number ', representing the range of random integers , and the - axis is labeled ' Frequency ', representing the frequency of each integer in the generated list . Raises : - TypeError : If ' my_list ' is not list . - ValueError : If ' my_list ' contains elements that are not numeric ( int or float ) . The histogram plots the distribution of the random numbers generated , with the number range (1 -100) on the - axis and the count ( frequency ) of each number on the - axis . Requirements : - random - time - matplotlib . pyplot Example : >>> my_list = [2 , 3 , 5] >>> time_taken , ax = task_func ( my_list ) >>> print ( type ( time_taken ) ) # Example output : < class ' float '> 26 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 61 62 63 64 65 67 68 69 70 71 73 74 75 76 77 < class ' float '> >>> ax . get_title () ' Histogram of Random Numbers ' \" \" \" if not isinstance ( my_list , list ) : # Returns ' Histogram of Random Numbers ' raise TypeError ( \" ' my_list ' must be list . \" ) if not all ( isinstance (x , ( int , float ) ) for in my_list ) : raise ValueError ( \" ' my_list ' must contain numeric elements . \" ) # Append 12 to the list my_list . append (12) # Calculate the sum of the list total_sum = sum ( my_list ) # Determine the size of the random numbers list list_size = min ( total_sum , size ) # Set the seed for reproducibility random_seed ( seed ) # Generate the list of random numbers random_numbers = [ randint (1 , 100) for _ in range ( list_size ) ] # Measure the time taken start_time = time . time () # Generate the histogram plt . figure ( figsize =(10 , 6) ) plt . hist ( random_numbers , bins = range (1 , 102) , align = ' left ' , edgecolor = ' black ') plt . xlabel ( ' Number ') plt . ylabel ( ' Frequency ') plt . title ( ' Histogram of Random Numbers ') plt . show () end_time = time . time () # Return the time taken and the Axes object return end_time - start_time , plt . gca () Samples 3 \"Generation on BigcodeBench\" D.3 Image Samples 27 Figure 10 Unconditional image generation, generated by CADD trained on ImageNet-32 32."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}