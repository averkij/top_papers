{
    "paper_title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs",
    "authors": [
        "Nada Saadi",
        "Tathagata Raha",
        "Clément Christophe",
        "Marco AF Pimentel",
        "Ronnie Rajan",
        "Praveen K Kanithi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates the challenges of developing large language models (LLMs) proficient in both multilingual understanding and medical knowledge. We demonstrate that simply translating medical data does not guarantee strong performance on clinical tasks in the target language. Our experiments reveal that the optimal language mix in training data varies significantly across different medical tasks. We find that larger models with carefully calibrated language ratios achieve superior performance on native-language clinical tasks. Furthermore, our results suggest that relying solely on fine-tuning may not be the most effective approach for incorporating new language knowledge into LLMs. Instead, data and computationally intensive pretraining methods may still be necessary to achieve optimal performance in multilingual medical settings. These findings provide valuable guidance for building effective and inclusive medical AI systems for diverse linguistic communities."
        },
        {
            "title": "Start",
            "content": "Bridging Language Barriers in Healthcare: Study on Arabic LLMs. Nada Saadi, Tathagata Raha, Clement Christophe, Marco AF Pimentel, Ronnie Rajan, Praveen Kanithi M42 Health, Abu Dhabi, UAE 5 2 0 2 6 ] . [ 1 5 2 8 9 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper investigates the challenges of developing large language models (LLMs) proﬁcient in both multilingual understanding and medical knowledge. We demonstrate that simply translating medical data does not guarantee strong performance on clinical tasks in the target language. Our experiments reveal that the optimal language mix in training data varies signiﬁcantly across different medical tasks. We ﬁnd that larger models with carefully calibrated language ratios achieve superior performance on native-language clinical tasks. Furthermore, our results suggest that relying solely on ﬁne-tuning may not be the most effective approach for incorporating new language knowledge into LLMs. Instead, data and computationally intensive pretraining methods may still be necessary to achieve optimal performance in multilingual medical settings. These ﬁndings provide valuable guidance for building effective and inclusive medical AI systems for diverse linguistic communities. Introduction The evolution of multilingual language models like Llama3 and GPT-4 marks signiﬁcant advancement in natural language processing. However, most LLMs are primarily trained on English and other common European languages, often neglecting low-resource languages with different alphabets, such as Arabic. This limitation poses signiﬁcant challenge, particularly in specialized domains like healthcare, where accurate language understanding is crucial. One major obstacle is the scarcity of high-quality, domain-speciﬁc data for these languages. In this paper, we address this challenge by evaluating and improving the capabilities of LLMs for clinical tasks in Arabic. We ﬁrst conduct comprehensive evaluation of existing open-source LLMs to assess their performance on medical tasks in both English and Arabic. This analysis provides valuable insights into the current state of LLMs in handling clinical information across different languages. To further enhance the capabilities of these models, we investigate various techniques, including leveraging existCopyright 2025, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. ing LLMs for translation, paraphrasing, and generating synthetic data to augment Arabic medical datasets. Speciﬁcally, we explore the translation capabilities of both Llama and Qwen, highlighting their strengths and weaknesses in handling medical terminology and nuances in Arabic. Finally, we ﬁne-tune Llama 3.1 using different mixtures of original and synthetic Arabic medical data. This allows us to analyze the impact of different data sources and augmentation techniques on the models performance across various clinical tasks. Our ﬁndings reveal that the optimal data mixture varies depending on the speciﬁc task, emphasizing the importance of careful data curation and augmentation strategies for developing effective clinical LLMs. Our work focuses on the Llama 3.1 model, but the proposed methodology can be extended to other LLMs, domains, and low-resource languages. We believe this research contributes to developing more inclusive and robust LLMs that can effectively serve diverse linguistic communities and specialized domains. Related Work The emergence of large language models (LLMs) has marked signiﬁcant advancement in artiﬁcial intelligence, demonstrating impressive capabilities in natural language understanding and generation. Initially developed for general use-cases such as text summarization, translation, and dialogue generation, LLMs have quickly been adopted across diverse industries, including ﬁnance, law, and education. One domain where LLMs have shown considerable promise is healthcare (Zhang, Wang, and Chen 2023). Recent studies have explored the application of LLMs to variety of medical tasks, including clinical decision support, medical question answering, and diagnosis assistance. For instance, GPT-4 has demonstrated proﬁciency in medical knowledge evaluation, achieving scores comparable to human experts on standardized medical exams (Nori et al. 2023). Other models like Meditron (Chen et al. 2023), OpenBioLLM (Ankit Pal 2024) and Med42 (Christophe et al. 2024) have further advanced the ﬁeld, with many surpassing GPT-4s performance on speciﬁc medical tasks and releasing open-source models usable by the research community and facilitating further advancements in the ﬁeld. Evaluating the performance of clinical LLMs, however, presents unique challenges. Most current models are primarily evaluated on question answering tasks using datasets like USMLE, MedQA, and PubMedQA. New benchmarks such as MEDIC (Kanithi et al. 2024) have emerged to provide more comprehensive and standardized evaluation of medical LLMs, encompassing tasks like clinical diagnosis, treatment recommendation, and patient education. Despite the rapid progress, signiﬁcant limitation of most existing clinical LLMs is their reliance on English data for training and evaluation. This raises concerns about their applicability and fairness in multilingual healthcare settings, where signiﬁcant portion of the population may not be proﬁcient in English. While multilingual LLMs like Llama, Qwen or Mistral (Dubey et al. 2024; Yang et al. 2024; Jiang et al. 2023) have demonstrated strong performance across multiple languages, adapting these models for both new language and specialized domain like healthcare remains an active area of research. Although some efforts have been made to develop multilingual clinical LLMs (Lopez et al. 2023; Wang et al. 2024), broad studies on their cross-lingual performance and generalizability are limited. Developing LLMs for languages with unique linguistic characteristics and limited digital resources presents additional challenges. Arabic, with its complex morphology, dialectal variations, and relatively scarce medical corpora, exempliﬁes these challenges. While general-purpose Arabic LLMs like Jais (Sengupta et al. 2023) and Silma (Silma-AI 2024) have emerged, specialized medical models for Arabic remain scarce. To our knowledge, BiMedix (Pieri et al. 2024a) is the only model that speciﬁcally focuses on building an LLM with multilingual capabilities (Arabic + English) for the healthcare domain. Evaluating Large Language Models Capabilities in Arabic Medical Applications While many large language models claim to work well in multiple languages and medical tasks, most testing focuses on either general language skills or English medical knowledge separately. Few studies look at how well these models handle medical content in speciﬁc non-English languages (Jin et al. 2024). In this study, we test several popular models of different sizes on Arabic medical benchmarks. Our results show that these models still have long way to go to match their English performance levels. Arabic Evaluation Datasets We utilize set of Arabic-translated medical datasets for our zero-shot and ﬁne-tuning evaluations. These datasets, originally developed for training and evaluating questionanswering systems in the medical domain, include PubMedQA, MedMCQA, MedQA, and Medical MMLU. PubMedQA: This dataset is derived from biomedical research articles (Jin et al. 2019). MedMCQA (Pal, Umapathi, and Sankarasubbu 2022) and MedQA (Jin et al. 2021) consists of multiple-choice questions coming from Indian and United States medical license exams. Medical MMLU is derived from the MMLU benchmark, speciﬁcally focusing on biomedical subsets including Clinical Knowledge, College Biology, College Medicine, Medical Genetics, Professional Medicine, and Anatomy (Hendrycks et al. 2021). The translation of these datasets into Arabic was conducted using semi-automated iterative translation pipeline, as detailed in BiMediX (Pieri et al. 2024a). This process involves initial translations using language models, followed by human reﬁnement to ensure the accuracy and quality of the translations. The translated datasets maintain the original format of questions and answers, allowing for consistent evaluation across languages. Modiﬁcations to Harness Pipeline Our research utilizes the Harness evaluation framework (Gao et al. 2024), which calculates log-likelihood scores to evaluate predictive model performance. To accommodate the Arabic language, we made signiﬁcant modiﬁcations to handle its unique attributes, including its distinctive script, complex morphology, and syntactic structure, ensuring accurate processing of Arabic data. Arabic text runs right-to-left (RTL), unlike English (LTR). We updated the framework to display and process Arabic text correctly. This meant reformatting our dataset while keeping its structure intact. To conduct zero-shot evaluations in Arabic, we adapted the entire framework for zeroshot testing, converting all prompts, responses, and multiplechoice options to Arabic, ensuring an accurate display and functionality of the Arabic script. Arabics linguistic complexity makes context crucial for accurate understanding. single word can have multiple meanings depending on its grammatical form and context. For instance, the root word H. Qk. (j-r-b) can transform into various derivatives that range from to try to to test to other nuanced meanings, highlighting why machine learning models must carefully consider contextual cues when processing Arabic text. Additionally, rather than just calculating the probability of generating answer choice labels (e.g., a, b, c, or d), we calculate the probability of generating the full answer text. This modiﬁcation provides more detailed understanding of the models performance by taking into account the entire answer generation process. Results As shown in Table 1, large language models in all model families exhibit limited performance on Arabic medical benchmarks. While leading models like Llama3.1 achieve high accuracy in English (62.0 and 78.2 on MedQA), their performance signiﬁcantly degrades when applied to Arabic (29.5 and 56.6). Although Qwen2.5 models demonstrate relatively better performance in Arabic, accuracy remains suboptimal. We will focus on improving Arabic performance, using Llama3.1 as case study to explore strategies to achieve English-language proﬁciency on Arabic medical benchmarks."
        },
        {
            "title": "LLM Adaptation Through Translation Pipeline\nA straightforward approach to enhance large language\nmodel performance across languages is to implement a",
            "content": "Model/Dataset PubMedQA MedMCQA MedQA MMLU Qwen2.5-3B-Instruct (Yang et al. 2024) Qwen2.5-7B-Instruct (Yang et al. 2024) Pangea-7B (Yue et al. 2024) Mistral-7B-Instruct v0.3 (Jiang et al. 2023) Llama3.1-8B-Instruct (Dubey et al. 2024) Silma-9B-Instruct-v1.0 (Silma-AI 2024) Llama-3.1-70B-Instruct (Dubey et al. 2024) Qwen2.5-72B-Instruct (Yang et al. 2024) Med42-Llama3.1-70B (Christophe et al. 2024) Meditron3-70B (Chen et al. 2023) BiMedix(Bilingual) (Pieri et al. 2024b) En 29.2 45.2 57.0 45.8 76.2 75.6 73.6 63.2 77.6 80.6 77.2 Ar 61.2 74.4 61.0 46.6 73.2 64.0 79.4 76.6 75.0 75.8 78. En 49.2 56.8 50.2 46.3 58.4 54.9 71.8 68.4 72.4 70.9 61.6 Ar 35.5 39.5 37.5 28.0 35.8 38.9 52.2 56.9 49.3 51.2 49. En 48.8 60.2 53.0 49.3 62.0 61.6 78.2 76.1 80.4 79.3 65.2 Ar 41.7 53.9 49.6 33.8 29.5 54.7 56.6 76.1 53.5 72.0 47. En 68.0 76.7 68.3 65.1 73.4 76.1 87.6 87.4 86.8 87.0 73.2 Ar 28.0 34.9 32.4 21.6 46.4 31.5 70.0 76.1 67.7 56.6 56. Table 1: Accuracy of publicly available models on different Medical QA benchmarks. Even though Llama3.1 models are performing better in English, Qwen2.5 models show stronger performance in Arabic."
        },
        {
            "title": "PubMedQA MedMCQA MedQA MMLU",
            "content": "LlamaX (Lu et al. 2024) Helsinki (Helsinki-NLP 2024) Flores 101 (seyoungsong 2024) Llama3.1-70B-Instruct (Dubey et al. 2024) Qwen2.5-72B-Instruct (Yang et al. 2024) 74.6 72.0 72.0 75.8 75.9 53.1 48.9 36.6 54.8 55.2 55.8 40.8 31.2 70.5 71.3 59.5 56.6 34.0 70.7 71.5 Table 2: Performance comparison of various translation models on Arabic medical benchmarks, translated into English and evaluated using Llama3.1-70B-Instruct for accuracy (%). translation pipeline: convert the Arabic input to English, process it, and translate the output back to Arabic. This method leverages the models strong English capabilities. However, this approach introduces signiﬁcant computational overhead, which requires at least three separate model calls instead of one. We evaluated this pipelines effectiveness by testing various translation models and comparing Llama-3.1-70Bs performance on translated content against its native English capabilities. Using our established evaluation benchmarks, we translated both questions and multiple-choice options before processing them through Llama-3.1-70B, maintaining consistent evaluation methods. Our investigation included two categories of translation systems: specialized translation models designed for precise Arabic-English conversion, and general-purpose large language models trained on both languages. Though not specifically optimized for translation, these general-purpose models offer broader language understanding. Our results in Table 2 reveal that despite their reputation for accuracy, specialized translation models faced considerable challenges with medical content. The nuanced nature of medical terminology makes literal translations problematic, often resulting in technically accurate but contextually inappropriate translations. General-purpose models such as Llama and Qwen demonstrated superior performance in this domain, producing translations that better preserved both technical accuracy and medical context. Although translation pipelines can improve the performance of LLMs in Arabic medical content, the results still lag signiﬁcantly behind their native English capabilities, highlighting the imperfections of current translation methods. This discrepancy raises serious concerns in the healthcare domain, where accurate understanding and generation of medical information is crucial. Furthermore, the added computational overhead of translation limits the feasibility of deploying such models in resource-constrained environments, hindering their accessibility in regions where they are most needed. Language Speciﬁc Finetuning We aim to improve large language models performance by ﬁnetuning on bilingual domain-speciﬁc data not encountered during pretraining. In this section, we detail our ﬁnetuning pipeline, present the datasets utilized, and analyze the optimal balance between English and Arabic training data. Finetuning Datasets Due to the scarcity of high-quality Arabic clinical data, we developed comprehensive data preparation pipeline. This section details our methodology for cleaning existing datasets, generating new data, and performing translations to ensure robust data quality. The size of each dataset is described in Table 3. Arabic Health Questions & Answers Dataset (AHQAD): The AHQAD dataset, with its 90 richly diverse categories, offers comprehensive landscape of medical and healthcare-related themes tailored specifDataset 1. AHQAD Original Description Arabic 2. Translated MED42 Dataset English 3. CIDAR 4. Med42 Dataset 5. Synthetic Open-Ended Arabic English English Total # of Arabic Tokens Total # of English Tokens 100K sampled based on completeness and paraphrased with Qwen-72B-Instruct 500K sampled randomly, cleaned and translated with Qwen-72B-Instruct 10K Instruction-Output good quality dataset Full English FT dataset 200K sampled based on 1-5 rating and translated with Qwen-72B-Instruct Final # of Tokens Arabic 8.33 Arabic 230.69 Arabic 1.34 English Arabic 464.97 240 M"
        },
        {
            "title": "480.36 M\n469.97 M",
            "content": "Table 3: Dataset Overview with Language Distribution ically for the Arabic-speaking region. This collection spans an extensive array of topics, from general medicine to specialized ﬁelds such as cardiology, pediatrics, and oncology, as well as practical areas like pharmacology and patient care. It also includes emergent ﬁelds such as telemedicine and health informatics. For the AHQAD dataset, which comprises medical queries, we applied more selective ﬁltering process. Out of its total 298,000 entries, we chose 100,000 that featured the most complete questions. This was crucial as it ensured the datas clarity and relevance, enhancing the quality of the training material. Responses were standardized using Qwen2.5-72B model for paraphrasing. We instructed the model to rewrite responses while maintaining the original meaning, removing typos and complex abbreviations, and improving clarity. The model was explicitly prompted to preserve the original information without adding any new content. Translated Med42 Dataset: We leverage the ﬁnetuning dataset used for ﬁnetuning Med42-v2. The Med42 dataset is curated from various medical and biomedical resources and it also features chat interactions and chain-of-thought reasoning apart beyond simple question-answering. For the Med42 dataset, we randomly selected 500,000 records from the dataset used to train Med42 (Christophe et al. 2024). This dataset was then translated to Arabic using the Qwen2.5-72B-Instruct model, chosen for its strong performance on Arabic benchmarks, as demonstrated in Table 1. Our manual evaluation further conﬁrmed that its translations are of higher quality, with superior context preservation, both in medical and nonmedical scenarios. Following translation, we meticulously cleaned the data to ensure only Arabic samples were retained, discarding any residual English phrases, ultimately preserving approximately 90% of the dataset. Culturally Relevant Instruction Dataset For Arabic (CIDAR): The CIDAR dataset (Alyafeai et al. 2024) on Hugging Face is an instruction-output pair dataset explicitly crafted for Arabic NLP tasks, making it particularly valuable for training models in zero-shot and fewshot learning scenarios. Each record in the dataset features distinct instructiona prompt, question, or directive in Arabicand corresponding output that provides precise, contextually relevant response. This structure is intended to help models interpret and generate responses across various types of tasks, such as factual questions, conversational exchanges, and directive-based commands, thereby enhancing the models instructionfollowing capabilities. synthetic question-answer Synthetic Open-Ended QA: The preparation of healthcare-speciﬁc pairs employs systematic multi-stage approach. The process begins by randomly selecting seed questions from HealthSearchQA, ExpertQA, and MedicationQA datasets. These seed questions serve as the foundational examples for iteratively building synthetic instruction dataset. The iterative process involves using the seed instructions as few-shot examples to generate new synthetic instructions. With each iteration, the pool of instructions expands, ensuring diversity and coverage across healthcare topics. Importantly, to preserve data independence, the ﬁnal synthetic instruction dataset excludes the original seed instructions. The generated questions are then processed using Llama-3.1-70BInstruct to create comprehensive responses. To ensure quality, these responses undergo evaluation using the same model on scale of 1 to 5, with only pairs rated 5 being retained in the ﬁnal dataset to ensure high quality. The entire dataset is subsequently translated into Arabic using Qwen2.5-72B-Instruct. Fine-tuning Pipeline The bilingual medical ﬁne-tuning pipeline explores the optimal combination of Arabic and English medical datasets to enhance model performance on both English and Arabic medical tasks. The pipeline incorporates two distinct data streams: high-quality Arabic medical content obtained through rigorous cleaning and ﬁltering of native Arabic medical datasets, and carefully translated English medical datasets that maintain clinical accuracy in both languages. For different ratios, we maintain constant number of 469.97M tokens, randomly sampling from our dataset presented in Table 3. Model Conﬁguration Dataset Ratio (Arabic-English)"
        },
        {
            "title": "Accuracy",
            "content": "PubMedQA MedMCQA MedQA MMLU Llama 3.1 8B 1. Arabic Only 2. Strong Arabic Majority 3. Arabic Majority 4. Balanced Distribution 5. English Majority 6. Strong English Majority 7. English Only Llama 3.1 8B-Instruct 1. Arabic Only 2. Strong Arabic Majority 3. Arabic Majority 4. Balanced Distribution 5. English Majority 6. Strong English Majority 7. English Only Llama 3.1 70B 1. Arabic Only 2. Balanced Distribution 3. English Only Baseline 100%-0% 80%-20% 60%-40% 50%-50% 40%-60% 20%-80% 0%-100% Baseline 100%-0% 80%-20% 60%-40% 50%-50% 40%-60% 20%-80% 0%-100% Baseline 100%-0% 50%-50% 0%-100% En 38.0 68.6 73.6 69.2 70.0 59.0 67.0 72.8 76.2 72.0 76.6 76.8 71.2 74.8 73.4 68. 15.6 70.6 78.0 75.2 Ar 34.4 71.2 68.0 63.0 61.2 53.8 53.8 61.2 73.2 72.0 69.0 68.4 66.2 66.2 64.0 60.0 51.8 76.8 55.0 48.6 En 50.0 56.6 56.7 56.6 56.6 57.0 57.6 58.8 58.4 58.7 58.4 58.1 58.7 59.1 59.5 59.5 65.1 68.8 68.9 70.2 Ar 32.2 32.2 35.1 32.9 34.7 33.4 33.4 34.7 35.8 31.9 34.7 33.7 34.6 36.8 35.1 36. 41.6 51.7 50.5 47.5 En 55.1 55.8 57.4 57.9 58.7 57.7 57.3 58.5 62.0 57.9 59.6 60.3 60.2 61.9 60.8 60.6 75.9 75.3 76.9 76.1 78.2 Llama 3.1 70B-Instruct 77.1 1. Arabic Only 76.2 2. Balanced Distribution 3. English Only 77.8 Note: Results show performance on English (En) and Arabic (Ar) evaluations for each metric. Baseline 100%-0% 50%-50% 0%-100% 79.4 78.2 61.8 48.2 71.8 70.6 71.4 71.7 73.6 79.8 77.2 75.0 52.2 52.8 50.8 49.7 Ar 27.3 27.5 27.5 28.3 29.5 29.8 28.7 29.5 29.5 29.2 29.7 30.9 33.5 33.5 31.6 30.2 48.2 53.3 50.9 48.6 56.6 55.8 55.6 52.9 En 64.8 72.5 71.1 72.9 71.9 70.8 71.7 71. 73.4 73.6 72.8 73.1 73.7 72.3 73.3 73.6 82.4 84.3 84.0 85.7 87.6 86.3 86.9 87.5 Ar 39.8 40.4 40.2 40.9 42.3 42.0 42.0 42.4 46.4 30.0 42.1 43.0 42.5 42.1 42.8 46. 55.8 67.3 59.3 61.9 70.0 67.1 67.0 66.5 Table 4: Accuracy of Finetuned Llama3.1-8b and 70b models with Different Arabic-English Dataset Ratios on medical QA benchmarks. Fine-tuning Llama 3.1 models on varying Arabic-English dataset ratios yields inconsistent results across medical QA tasks. Even large instruct models show limited improvement on Arabic benchmarks after ﬁne-tuning. We ﬁnetuned both Llama3.1 models. We employ the classic auto-regressive loss for ﬁnetuning. Loss is backpropagated only on output tokens. This approach ensures that the model learns to generate appropriate responses and not learn to generate the prompts. Our training samples are concatenated into chunks of 8192 tokens. Each model was ﬁnetuned for two epochs over our curated dataset using cosine learning rate schedule between 1 105 and 1 106. All experiments are performed on cluster of 4 H100 nodes. Results Our results in Table 4 show that different ratio of Arabic-English data yield to different performance levels depending on the evaluation task. For PubMedQA, training with exclusively Arabic data produces the best accuracy (71.2). While for MedMCQA and MedQA, the models perform best with strong Arabic majority and English Majority, respectively (35.1 and 29.8). Surprisingly, for the MMLU datasets, which focuses on testing direct knowledge application, using only English data, achieves 42.4 compared to the 39.8 zero-shot accuracy."
        },
        {
            "title": "These patterns remain consistent across both the base and",
            "content": "instruct models. These results highlight the fact that the relationship between the language distribution used for ﬁnetuning and performance is fundamentally linked to the nature of each task. For instance, PubMedQA requires complex analytical reasoning within medical contexts, while MMLU focuses on structured knowledge assessment through multiplechoice questions. This difference suggests that tasks requiring deeper understanding of context need stronger language-speciﬁc training. Interestingly, our ﬁndings on the larger 70B parameter models show more consistent behavior across all Arabic tasks, with Arabic-only training data consistently achieving the best results. This suggests that larger models may handle language-speciﬁc tasks more uniformly than their smaller counterparts, given their greater capacity to abstract and generalize linguistic features across different training distributions. Llama3.1-70B base model exhibits poor performance on the PubMedQA test set due to its inability to follow the chat-template for highly context-based task. Intriguingly, our ﬁne-tuned version of Llama3.1-70B-Instruct rarely outperforms the original model, suggesting that it has reached its maximum capabilities after subsequent pretraining, supervised ﬁne-tuning, and alignment stages. Conclusion Our research into Arabic-English medical AI reveals critical insights for developing truly effective multilingual language models. First, we highlight the signiﬁcant performance gap between English and Arabic, especially pronounced in smaller models. This disparity underscores the need for models deeply trained in speciﬁc languages to achieve genuine language understanding and complex medical reasoning. Smaller models, with their limited capacity, struggle to capture the nuances of different languages and medical terminology, resulting in substantial performance gap between languages like English and Arabic. Second, while some general language models demonstrate superior translation capabilities compared to specialized translation models, they come with high computational costs and are not perfect. General language models, despite their broader training data, still have limitations in accurately translating medical terminology and complex linguistic structures, highlighting the need for further research in this area. Third, ﬁne-tuning models do not always guarantee improved performance compared to the baseline, and the results are highly dependent on the distribution of languages in the training data. The effectiveness of ﬁne-tuning can vary signiﬁcantly depending on the speciﬁc language mix used in the training data, suggesting that careful balance of languages is crucial for optimal performance. We acknowledge that our evaluation primarily focuses on close-ended question benchmarks, which, while valuable for assessing domain knowledge, do not fully capture the generation capabilities, safety, and bias aspects of model. These aspects are crucially important for any healthcare model. Therefore, we advocate for new benchmarks, such as MEDIC (Kanithi et al. 2024), to include multilingual capabilities tests to address these critical dimensions. It is important to note that the impact of language mixing is particularly signiﬁcant when dealing with languages that have vastly different alphabets, such as Arabic, Chinese, or Latin-based languages. The non-overlapping nature of their tokens can lead to unique challenges in training and optimization. Moreover, the performance of model in one domain should ideally transfer seamlessly across multiple languages. It should be easier for model to learn technical vocabularies in new language if it is already trained on that domain and possesses good understanding of the language. Therefore, the transfer capabilities of model for speciﬁc domain from one language to another should be high. Thus, we need to continue relying on extensive pretraining for models to learn new language effectively. At the same time, exploring the transfer capabilities of models for speciﬁc domains across languages is crucial. The ultimate goal extends beyond technical achievement: we aim to create AI systems that can break down language barriers, provide accurate medical insights, and expand healthcare access, especially in underserved and linguistically diverse communities. This means developing models that do not just translate words, but truly comprehend the intricate cultural and linguistic subtleties of medical communication. Achieving this goal will require models that can not only translate medical information accurately but also understand the cultural context and linguistic nuances associated with different languages, ensuring effective communication and healthcare access for diverse populations. References Alyafeai, Z.; Almubarak, K.; Ashraf, A.; Alnuhait, D.; Alshahrani, S.; Abdulrahman, G. A. Q.; Ahmed, G.; Gawah, Q.; Saleh, Z.; Ghaleb, M.; Ali, Y.; and Al-Shaibani, M. S. 2024. CIDAR: Culturally Relevant Instruction Dataset For Arabic. arXiv:2402.03177. Ankit Pal, M. S. 2024. OpenBioLLMs: Advancing OpenSource Large Language Models for Healthcare and Life Sciences. https://huggingface.co/aaditya/OpenBioLLMLlama3-70B. Chen, Z.; Cano, A. H.; Romanou, A.; Bonnet, A.; Matoba, K.; Salvi, F.; Pagliardini, M.; Fan, S.; Kopf, A.; Mohtashami, A.; Sallinen, A.; Sakhaeirad, A.; Swamy, V.; Krawczuk, I.; Bayazit, D.; Marmet, A.; Montariol, S.; Hartley, M.-A.; Jaggi, M.; and Bosselut, A. 2023. MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. arXiv preprint arXiv:2311.16079. Christophe, C.; Kanithi, P. K.; Raha, T.; Khan, S.; and Pimentel, M. A. 2024. Med42-v2: suite of clinical llms. arXiv preprint arXiv:2408.06142. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Gao, L.; Tow, J.; Abbasi, B.; Biderman, S.; Black, S.; DiPoﬁ, A.; Foster, C.; Golding, L.; Hsu, J.; Le Noach, A.; Li, H.; McDonell, K.; Muennighoff, N.; Ociepa, C.; Phang, J.; Reynolds, L.; Schoelkopf, H.; Skowron, A.; Sutawika, L.; Tang, E.; Thite, A.; Wang, B.; Wang, K.; and Zou, A. 2024. framework for few-shot language model evaluation. Helsinki-NLP. 2024. opus-mt-ar-en. https://huggingface.co/ Helsinki-NLP/opus-mt-ar-en. Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. Proceedings of the International Conference on Learning Representations (ICLR). Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; Casas, D. d. l.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825. Jin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and Szolovits, P. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14): 6421. Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.; and Lu, X. 2019. PubMedQA: Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 25672577. Jin, Y.; Chandra, M.; Verma, G.; Hu, Y.; De Choudhury, M.; and Kumar, S. 2024. Better to Ask in English: CrossLingual Evaluation of Large Language Models for Healthcare Queries. In Proceedings of the ACM Web Conference 2024, WWW 24, 26272638. New York, NY, USA: Association for Computing Machinery. ISBN 9798400701719. Kanithi, P. K.; Christophe, C.; Pimentel, M. A. F.; Raha, T.; Saadi, N.; Javed, H.; Maslenkova, S.; Hayat, N.; Rajan, R.; and Khan, S. 2024. MEDIC: Towards Comprehensive Framework for Evaluating LLMs in Clinical Applications. arXiv:arXiv:2409.07314. Lopez, M.; Parikh, A.; Yacouby, R.; and et al. 2023. GatorTron-M: Multilingual Clinical Language Models for Medical Natural Language Processing. Lu, Y.; Zhu, W.; Li, L.; Qiao, Y.; and Yuan, F. 2024. LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages. arXiv preprint arXiv:2407.05975. Nori, H.; King, N.; McKinney, S. M.; Carignan, D.; and Horvitz, E. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375. Pal, A.; Umapathi, L. K.; and Sankarasubbu, M. 2022. MedMCQA: Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering. In Flores, G.; Chen, G. H.; Pollard, T.; Ho, J. C.; and Naumann, T., eds., Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, 248260. PMLR. Pieri, S.; Mullappilly, S. S.; Khan, F. S.; Anwer, R. M.; Khan, S.; Baldwin, T.; and Cholakkal, H. 2024a. Bimedix: Bilingual medical mixture of experts llm. arXiv preprint arXiv:2402.13253. Pieri, S.; Mullappilly, S. S.; Khan, F. S.; Anwer, R. M.; Khan, S.; Baldwin, T.; and Cholakkal, H. 2024b. BiMediX: Bilingual Medical Mixture of Experts LLM. arXiv:2402.13253. Sengupta, N.; Sahu, S. K.; Jia, B.; Katipomu, S.; Li, H.; Koto, F.; Marshall, W.; Gosal, G.; Liu, C.; Chen, Z.; et al. 2023. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models. arXiv preprint arXiv:2308.16149. seyoungsong. 2024. huggingface.co/seyoungsong/ﬂores101mm100 175M. Silma-AI. 2024. SILMA 1.0. https://huggingface.co/silmaai/SILMA-9B-Instruct-v1.0. Wang, X.; Chen, N.; Chen, J.; Hu, Y.; Wang, Y.; Wu, X.; Gao, A.; Wan, X.; Li, H.; and Wang, B. 2024. Apollo: Lightweight multilingual medical llms towards dearXiv preprint mocratizing medical ai arXiv:2403.03640. Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; Li, C.; Li, C.; Liu, D.; Huang, F.; Dong, G.; Wei, H.; Lin, H.; ﬂores101mm100175M. to 6b people. https:// Tang, J.; Wang, J.; Yang, J.; Tu, J.; Zhang, J.; Ma, J.; Xu, J.; Zhou, J.; Bai, J.; He, J.; Lin, J.; Dang, K.; Lu, K.; Chen, K.; Yang, K.; Li, M.; Xue, M.; Ni, N.; Zhang, P.; Wang, P.; Peng, R.; Men, R.; Gao, R.; Lin, R.; Wang, S.; Bai, S.; Tan, S.; Zhu, T.; Li, T.; Liu, T.; Ge, W.; Deng, X.; Zhou, X.; Ren, X.; Zhang, X.; Wei, X.; Ren, X.; Fan, Y.; Yao, Y.; Zhang, Y.; Wan, Y.; Chu, Y.; Liu, Y.; Cui, Z.; Zhang, Z.; and Fan, Z. 2024. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671. Yue, X.; Song, Y.; Asai, A.; Kim, S.; de Dieu Nyandwi, J.; Khanuja, S.; Kantharuban, A.; Sutawika, L.; Ramamoorthy, S.; and Neubig, G. 2024. Pangea: Fully Open Multilingual Multimodal LLM for 39 Languages. arXiv preprint arXiv:2410.16153. Zhang, X.; Wang, Y.; and Chen, H. 2023. Large Language Models in Medicine: The Potentials and Pitfalls."
        },
        {
            "title": "Helsinki Translation",
            "content": "Original Translation: How big is the bottle youre gonna use in patient who needs quick blood transfusion (based on medical knowledge in 2020)? Options: 18 Gg. 20 Gs. 22 Gig. 24 Gg. Flores-101 Translation Original Translation: What size of cannula would you use in patient who needed rapid blood transfusion (as of 2020 medical knowledge)? Options: 18 gauge 20 gauge 22 gauge 24 gauge LlamaX Translation Original Translation: What is the volume of the cannula you will use for patient who needs rapid blood transfusion (according to medical knowledge in 2020)? Options: 18 qij. 20 qij. 22 qij. 24 qij."
        },
        {
            "title": "Analysis",
            "content": "Translation Quality Comparison Helsinki: Uses informal language (gonna) and confuses the medical terminology by referring to bottle instead of cannula. Unit notation is inconsistent (Gg., Gs., Gig.). Flores-101: Provides the most accurate medical terminology, using cannula and gauge correctly. Maintains consistent formatting and professional medical language. LlamaX: Uses correct medical term cannula but focuses on volume rather than size. Unit notation shows consistent but incorrect translation (qij.)."
        },
        {
            "title": "Qwen Translation Example",
            "content": "Input Context: Original English Text: Why do some people develop hypothyroidism after radioactive iodine treatment for thyroid cancer? Radioactive iodine treatment for thyroid cancer can sometimes lead to decreased thyroid function in some individuals. This occurs due to the damage caused by radiation to the thyroid gland cells. Qwen Translation: (cid:9)ªK. ú(cid:10) YªK. (cid:16)èY (cid:9)ªË@ (cid:17)(cid:9)(cid:29) (cid:16)(cid:9)K (cid:16)èY (cid:9)ªË@ (cid:9)àAQåË (cid:16)PYË@ Y(cid:16) (cid:16)èY (cid:9)ªË@ (cid:16)éJ(cid:10) (cid:16)èY (cid:9)ªË@ (cid:16)(cid:9)K Q(cid:9) úÍ@(cid:13) (cid:16)PYË@ (cid:16)éJ(cid:10) (cid:9)YË@ PQå(cid:9)Ë@ I. (cid:28).(cid:29). @ (cid:9)Yë ø(cid:10) (cid:16)PYË@ (cid:16)èY (cid:9)ªË@ ú(cid:10) . (cid:16)éJ(cid:10) (cid:9)áÓ (cid:144)A(cid:9)m(cid:25)(cid:17)(cid:13) (cid:9)GAªK(cid:10) @ (cid:9)XAÖÏ B@ (cid:16)éJ(cid:10) (cid:16)PYË@ ú(cid:10)Aª (cid:17)ƒB(cid:13) @ h. CªË@ (cid:9)àAQåË ú(cid:10)Aª (cid:17)ƒB(cid:13) @ h. CªË@? (cid:16)éJ(cid:10) (cid:16)PYË@ (cid:13) (cid:9) ø(cid:10) (cid:13)ñK(cid:10) (cid:9)ªK. ú(cid:10) (cid:9)àAJ(cid:10)k B@ (cid:13) (cid:9) .X@Q(cid:9) (cid:9)ªK. ú(cid:10) (cid:17)HYm(cid:26)'(cid:10) B@ (cid:9) AK(cid:10)C (cid:9)jÊË Aª (cid:17)ƒB(cid:13) @ éJ.(cid:28).(cid:29)(cid:10)"
        }
    ],
    "affiliations": [
        "M42 Health, Abu Dhabi, UAE"
    ]
}