{
    "paper_title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
    "authors": [
        "Disen Lan",
        "Weigao Sun",
        "Jiaxi Hu",
        "Jusen Du",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\\% of the Transformer-based LLM at 0.02\\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization."
        },
        {
            "title": "Start",
            "content": "Liger: Linearizing Large Language Models to Gated Recurrent Structures Disen Lan 1 2 * Weigao Sun 1 (cid:12) Jiaxi Hu 3 Jusen Du 1 4 * Yu Cheng 5 (cid:12) 5 2 0 2 3 ] . [ 1 6 9 4 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive finetuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight finetuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93% of the Transformerbased LLM at 0.02% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/ OpenSparseLLMs/Linearization. 1Shanghai AI Laboratory 2South China University of Technology 3The Hong Kong University of Science and Technology (Guangzhou) 4Nanjing University 5The Chinese University of Hong Kong. * Interns at Shanghai AI Laboratory. (cid:12) Correspondence to: Weigao Sun <sunweigao@outlook.com>, Yu Cheng <chengyu@cse.cuhk.edu.hk>. Copyright 2025 by the author(s). 1 Figure 1. Liger Performance and Efficiency. Our proposed Liger recovers nearly 93% performance of Llama-3.2-1B and outperforms pretrained gated recurrent models at only 0.02% of the pre-training tokens cost. 1. Introduction Large language models (LLMs) have demonstrated exceptional performance across various natural language processing tasks (Chintala, 2023; Team, 2023; Zhu et al., 2024; Qu et al., 2024). However, the Transformer-based architecture (Vaswani et al., 2017) used in modern LLMs, with its reliance on softmax attention, suffers from quadratic computational complexity. This inefficiency results in significant speed and memory challenges, particularly during pretraining on long sequences. During inference, the Key-Value (KV) cache (Kwon et al., 2023) grows linearly with the input sequence length, leading to reduced inference speed and high memory usage, which severely limits the capability of these models for handling long-sequence tasks (Sun et al., 2024b). In contrast, models based on linear recurrent modeling (Katharopoulos et al., 2020; Yang et al., 2023; Qin et al., 2024b; Sun et al., 2024a; Du et al., 2025) provide linear-time training and constant-memory inference, offering substantial efficiency benefits and positioning themselves as promising candidates for the next generation of foundational architectures (MiniMax et al., 2025). While pretraining LLMs using architectures based on linear resurrent modeling reduces costs due to their linear training complexity, the high expenses of pretraining from scratch associated with large model sizes and datasets still remain major obstacle to their adoption and practical use. This Liger: Linearizing Large Language Models to Gated Recurrent Structures challenge has hindered the advancement of linear recurrent models. Linearizing pretrained LLMs like SUPRA (Mercat et al., 2024), MambaInLlama (Wang et al., 2024) and LoLCATs (Zhang et al., 2024a), as an emerging new direction, allows the transfer of weights from an existing pretrained model to one with linear recurrent modeling architectures at small fraction of the original pretraining cost. The linearization approach is promising post-training technique to enable efficient pretrained model deployment while preserving their performance. Gating mechanisms (Qin et al., 2024a; Sun et al., 2023) play crucial role in linear recurrent models by controlling memory retention and forgetting, with their effectiveness widely demonstrated in such architectures. However, incorporating gate modules as additional components requires both transferring weights from pre-trained LLMs and training these gating modules from scratch. This process not only increases the cost of linearization but also creates larger architectural divergence from Transformer-based LLMs. This divergence may hinder the effective approximation of softmax attention, limiting the performance of gated linear recurrent models (Zhang et al., 2024d). Moreover, existing linearization methods often overlook the detailed design considerations of gated linear models, and the newly added modules fail to leverage the pre-trained weights of LLMs, further reducing the efficiency of linearization. In this paper, we present Liger, which stands for Linearizing large language models into gated recurrent structures, novel approach for linearizing LLMs. Liger repurposes the weights from pre-trained Transformer-based LLMs and introduces novel method for constructing crucial gating mechanisms in gated recurrent structures using the key projection. This approach avoids the complex attention transfer process found in existing linearization methods. After transforming the weights and constructing the gating mechanisms, Liger requires only lightweight fine-tuning of the linearized gated recurrent model parameters through LoRA autoregressive training. This efficient process restores the models performance with minimal linearization cost, achieving competitive results across range of language modeling and understanding benchmarks while benefiting from the linear-time inference efficiency of the recurrent architecture. Our contributions can be summarized as follows: We propose Liger Attention, an intra-layer hybrid attention mechanism that combines sliding window softmax attention with linear recurrent modeling. This simple yet effective design retains the essential softmax non-linearity, accelerating the linearization process while maintaining the capabilities of pre-trained LLMs and ensuring linear-time inference efficiency. We apply Liger to linearize the latest Llama3 series, ranging from 1B to 8B parameters. Experimental results show that Liger outperforms existing linearization methods (like SUPRA (Mercat et al., 2024), MambaInLlama (Wang et al., 2024) and LoLCATs (Zhang et al., 2024a)), in terms of both efficiency and its ability to preserve the original performance of pre-trained LLMs. 2. Preliminary Transformer with Softmax Attention. Given the input sequence = {x1, x2, . . . , xT } RT D, with sequence length and dimension D, vanilla transformer (Vaswani et al., 2017) adopts standard softmax attention: Q, K, = XWQ, XWK, XWV = Softmax(( QK ) M)V (1) where WQ, WK, WV RDD are learnable parameters for input sequence projection and RT is mask matrix for causal modeling by preventing future information leakage in autoregressive generation task. The above parallel form of softmax attention in Eq.1 is applied for efficient training and can be rewritten in the following recurrent form during inference stage: qt, kt, vt = xtWQ, xtWK, xtWV ot = (cid:80)t i=1 exp(qtk / (cid:80)t i=1 exp(qik / D)vi D) (2) The standard softmax attention is highly rely on the growing KV Cache (Chou et al., 2024; Wang et al., 2024) to recall the history \"memory\" for sequence modeling, which results in quadratic complexity and costly memory requirements especially in long context setting. We introduce Liger, novel method for adapting pretrained Transformer-based LLMs into gated recurrent structures. This approach efficiently repurposes redundant weights from pre-trained models to construct gating modules without introducing additional parameters, obtaining gated recurrent LLMs with the benefits of constant-memory inference. Linear Attention. Linear transformer (Katharopoulos et al., 2020; Qin et al., 2023) approximates softmax self-attention as the dot product of the kernel feature mapping and utilizes associative property of matrix products to calculate the selfattention weights, achieving efficient linear-time sequence modeling and constant memory consumption. Concretely, the linear attention can be formulated as follows: 2 Liger: Linearizing Large Language Models to Gated Recurrent Structures ot = = (cid:80)t i=1 ϕ(qt)ϕ(ki)vi (cid:80)t i=1 ϕ(qt)ϕ(ki) ϕ(qt) (cid:80)t i=1 ϕ(ki)vi ϕ(qt) (cid:80)t i=1 ϕ(ki) 3. Methodology (3) In this section, we will introduce our proposed Liger for linearizing large language models to gated recurrent structures. We also design simple yet effective hybrid attention form, namely Liger Attention, and build the Liger architecture based on it, including intraand inter-layer hybrid architectures. Let St = (cid:80)t i=1 ϕ(ki)vi and zt = (cid:80)t i=1 ϕ(ki), the above formulation in Eq. 3 can be rewritten in the recurrent form as an RNN (Katharopoulos et al., 2020): (cid:40)St = St1 + ϕ(kt)vt, zt = zt1 + ϕ(kt), ot = ϕ(qt)St ϕ(qt)zt (4) Although linear attention with causal mask matrix cannot use matrix associativity to reduce the parallel form training complexity from quadratic to linear, its chunk-wise parallel form allows hardware-efficient sub-quadratic and partially parallel training (Yang et al., 2023; Sun et al., 2024b;a; Qin et al., 2024a). Gating Mechanism. While linear attention (or linear recurrent structures) are widely recognized for their linear-time computational efficiency, they have historically exhibited notable performance gap compared to standard softmax attention. To address this limitation, recent advances in linear recurrent models have incorporated gating mechanism, which is critical architectural component enabling dynamic, context-aware information retention through input and forget gates. This mechanism allows models to selectively preserve or discard historical information, substantially enhancing their expressiveness through constant memorization capacity. The integration of gating mechanism has consequently become prevalent design paradigm in state-of-the-art linear attention variants (Yang et al., 2023; Peng et al., 2023; Qin et al., 2024c). representative implementation, Gated Linear Attention (GLA) (Yang et al., 2023), demonstrates this principle through its mathematical formulation: St = Gt St1 + vt (5) At its core, Gated Linear Attention (GLA) fundamentally augments conventional linear attention through the integration of gating mechanism. This modification serves as generalized framework that can be systematically extended to diverse linear recurrent architectures by reparameterizing the gating term Gt. Specifically, Gt governs the temporal decay dynamics, enabling broad and flexible adaptation across variant gated linear recurrent structures. 3 3.1. Gate Construction by Key Projection The parameter space of large language models (LLMs) exhibits intrinsic structural redundancy (Yu et al., 2024; Aghajanyan et al., 2020), phenomenon attributed to the overparameterization inherent in deep neural architectures. This redundancy motivates principled approach to reformulating LLMs as gated linear recurrent architectures: rather than introducing new parameters, we strategically repurpose subsets of pre-trained LLM weights to serve dual roles as gating modules. Building on the design principle of gated linear recurrent structures for optimal softmax attention approximation, we propose reallocating the key projection matrix WK as dual roles to concurrently perform its canonical linear transformation and gating mechanism. Formally, the gating mechanism is derived via transformation of Gt = (kt) = (xtWK), where () operates on the projected key embeddings. This parameter-sharing paradigm ensures compatibility with pre-trained weights while eliminating the need for auxiliary trainable gating parameters, thereby preserving computational and memory efficiency. In practical implementations, gating mechanisms can be instantiated through diverse transformation strategies. Our approach employs parameter-free Pooling() operation to derive gate values, circumventing the need for additional trainable parameters. This design preserves compatibility with pre-trained LLM weights, enabling direct reuse of existing parameters for gate construction without architectural modification. Empirical evaluations demonstrate that this parameter-efficient strategy achieves competitive performance compared to conventional trainable gating projections (e.g., linear or nonlinear parametric layers), while maintaining computational efficiency and reducing optimization complexity. 3.2. Liger: Linearizing LLMs to Gated Recurrent"
        },
        {
            "title": "Structures",
            "content": "Prior methodologies for linearizing transformer-based large language models (LLMs) typically rely on auxiliary components, such as feature mapping layers, to approximate softmax attention mechanisms (Zhang et al., 2024a). Notably, LoLCATs (Zhang et al., 2024a) propose two-stage fine-tuning paradigm to mitigate this limitation: First stage: Liger: Linearizing Large Language Models to Gated Recurrent Structures Figure 2. Overall Framework of Liger. We linearize the Transformer-based large language model (LLM) architecture into gated linear recurrent model by 1) Replacing Softmax Attention with Gated Recurrent Memory module, and 2) Employing LoRA to fine-tune the Liger architecture while frozen most original weight parameters. The Liger architecture enables efficient chunk-wise parallel training also enjoying cheap linear recurrent inference. Model Gate Parameterization Pooling for Gate Construction Gated Linear Attention (Yang et al., 2023) Gt = α Gt = αt11 Mamba2 (Dao & Gu, 2024) Gt = αt11 mLSTM (Beck et al., 2024) Gt = αt11 Gated Retention (Sun et al., 2024c) Gt = α HGRN2 (Qin et al., 2024c) Gt = α RWKV6 (Peng et al., 2024) Gated Slot Attention (Zhang et al., 2024c) Gt = α 1 1 1 1 αt = σ(Pooling(kt)) αt = exp( softplus(Pooling(kt))) αt = σ(Pooling(kt)) αt = σ(Pooling(kt)) αt = γ + (1 γ)σ(Pooling(kt)) αt = exp( exp(Pooling(kt))) αt = σ(Pooling(kt)) Table 1. Gated Linear Recurrent Structures with Variations of Gate Gt Parameterization. Gating mechanism can be constructed through pooling to reuse the key projection of pre-trained LLM. Attention Transfer phase trains newly introduced modules (e.g., kernel approximations) while freezing pre-existing parameters, followed by the Second stage: employing LowRank Adaptation (LoRA) to fine-tune attention layers. However, previous linearization approaches including LoLCATs incurs two critical constraints: ❶ Architectural Overhead: The dependency on supplementary feature mapping and gating modules to replicate softmax attention outputs precludes direct reuse of pre-trained LLM parameters, necessitating non-trivial architectural modifications. ❷ Optimization Fragility: The sequential training paradigm introduces brittleness, as end-to-end fine-tuning is infeasible due to the interdependency between the frozen base model and the auxiliary components. These limitations hinder extensibility to modern linear recurrent architectures incorporating gated mechanisms, which require seamless integration with pre-trained weights and end-to-end trainability. To advance the linearization of pre-trained large language models (LLMs) into gated recurrent neural architectures, we propose parameter-efficient strategy that employs the canonical Softmax operation for feature mapping. Unlike existing approaches that rely on trainable modules such as T2R (Mercat et al., 2024) or Hedgehog (Zhang et al., 2024b) to approximate attention dynamics, our method utilizes Softmax to inherently normalize query and key representations. This normalization ensures bounded magnitude in the query-key product space, critical property for faithfully replicating the numerical stability of conventional softmax attention within linearized frameworks. By eschewing auxiliary trainable components, our design eliminates architectural dependencies on attention transfer mechanisms. This divergence from sequential training paradigms (e.g., frozen base models with incremental module updates) enables fully end-to-end fine-tuning without Liger: Linearizing Large Language Models to Gated Recurrent Structures compromising compatibility with pre-trained LLM weights. The resultant gated recurrent architecture, formulated as: qt = xtWq, kt = xtWk, vt = xtWv Gt = Pooling(kt) (6) With the generated gate Gt, the recurrent update rule and the followed output computation will be: St = Gt St1 + ϕ(k ot = ϕ(qt)St )vt (7) Here all the trainable parameters WQ, WK, WV are inherited from the pre-trained LLM. This method of gating mechanism construction can be extended to various gated linear recurrent structures, as shown in Table 1. Prior linear recurrent models employ learnable feature mapping functions (denoted as ϕ()) to compute similarity representations between query (qt) and key (kt) vectors, which introduce superfluous trainable parameters while generating output distributions that deviate from the canonical Softmax attention distribution inherent in pre-trained LLM. Such distributional discrepancies consequently degrade the efficacy of linearization by impairing compatibility with the original attention mechanisms. We found that feature mapping ϕ() can be effectively approximated via simple normalization operation (Softmax() in our implementation), thereby eliminating the requirement for computationally intensive attention transfer or distillation procedures while maintaining fidelity to the target attention distribution. Eq. 7 preserves the expressivity of softmax attention while inheriting the computational efficiency of linear recurrent models. This unification of architectural simplicity and functional fidelity addresses key limitations in prior linearization methods. Following the initialization of the model with pre-trained LLM weights and its architectural reconfiguration into gated linear recurrent framework, we employ next-token prediction as the fine-tuning objective to recover performance in the transformed architecture. Formally, we parameterize the adapted weights as Θ = Θ0 + Θ, where Θ denotes the incremental adjustments required to align the original transformer-based parameters with the gated linear recurrent structure. The optimization objective minimizes the cross-entropy loss for autoregressive prediction over input sequences x1:t1: (cid:88) = log PΘ(xtx1:t1) (8) This approach circumvents the need for auxiliary training stages (e.g., attention transfer) by directly optimizing the gated linear recurrent architecture end-to-end, thereby preserving the computational efficiency and parameter efficiency inherent to the original LLM. Figure 3. Hybrid Architecture of Liger. Liger adopts intra-hybrid Liger Attention and inter-hybrid model architecture by stacking layer of standard attention Transformer blocks every few (e.g. 7) layers of Liger Transformer blocks. we apply Low-Rank Adaptation (LoRA) (Hu et al., 2021) specifically to the linear recurrent layers of large language models (LLMs), focusing on the fine-tuning of the weight matrices WQ, WK, WV. Instead of training all model parameters, LoRA decomposes the adaptation term Θ into two low-rank matrices and A, such that Θ = BA, where RDr, RrD, and with typically set to small value, such as 8. Our empirical results demonstrate that LoRA consistently outperforms full-rank fine-tuning, offering more efficient and effective approach for LLM linearization. 3.3. Liger Attention: Hybrid of Sliding Window Attention and Gated Recurrent Modeling Building upon previous works that combine softmax attention with linear attention (Katharopoulos et al., 2020; MiniMax et al., 2025), we propose intra-layer hybrid attention mechanism, termed Liger Attention. This method integrates hybrid form of Gated Recurrent Modeling (GRM) and Sliding Window Attention (SWA) (Beltagy et al., 2020) with narrow softmax attention window size, by blending their outputs in weighted manner. Specifically, the formulation is given by: ot = LigerAttn(qt, kt, vt) = α GRM(qt, kt, vt) + β SWA(qt, kt, vt) (9) where GRM denotes Gated Recurrent Modeling (and its variants) in Eq. 3 and SWA refers to Sliding Window Attention, which is variant of softmax attention as expressed in Eq. 1 formulated in Eq. 10. The parameters α and β control the relative contributions of each attention mechanism. ˆot = SWA(qt, kt, vt) (cid:80)t i=tw+1 exp(qtk / (cid:80)t i=tw+1 exp(qtk / D)vi D) = (10) Liger: Linearizing Large Language Models to Gated Recurrent Structures Model Mistral-7B SUPRA-Mistral-7B LoLCATs-Mistral-7B Attn. Trf. LoLCATs-Mistral-7B LoRA LoLCATs-Mistral-7B Liger-GLA-Mistral-7B (Ours) Llama-3-8B SUPRA-Llama-3-8B Mamba2-Llama-3-8B Mamba2-Llama-3-8B 50% Attn. LoLCATs-Llama-3-8B Attn. Trf. LoLCATs-Llama-3-8B LoRA LoLCATs-Llama-3-8B Liger-GLA-Llama-3-8B (Ours) Training Tokens (B) PiQA ARC-e ARC-c Hella. Wino. MMLU Avg. Avg. acc acc acc_norm acc_norm acc acc (5-shot) (no MMLU) 8000 100 0.02 0.02 0. 0.02 15000 20 20 20 0.02 0.02 0.04 0.02 80.6 80.4 79.8 77.3 79.7 80.1 79.4 78.9 76.8 81.5 78.4 72.4 80. 80.0 80.7 75.9 79.3 74.9 78.4 78.7 80.1 75.1 74.1 78.8 79.3 72.6 80.4 78.7 53.9 45.8 51.7 45.1 47. 49.3 53.2 46.5 48.0 58.2 51.9 44.3 53.5 51.9 81.1 77.1 48.3 40.9 58.4 76.3 79.2 71.7 70.8 79.5 51.6 34.6 63. 76.6 74.3 70.3 74.2 67.9 71.0 70.1 72.9 65.8 58.6 71.5 73.4 68.0 72.9 71.4 62.6 34.2 23.0 23.0 23. 36.3 65.3 40.9 43.2 56.7 23.5 23.0 23.0 43.4 72.2 64.0 59.4 54.8 59.8 65.1 71.7 63.2 61.9 71.0 59.7 52.5 62. 67.0 74.1 69.9 66.7 61.2 67.0 70.9 73.0 67.6 65.6 73.9 66.9 58.4 70.0 71.7 Table 2. Comparison with Linearized LLMs. Liger outperforms other linearization method on language modeling and understanding tasks with less training tokens across Mistral-7B and Llama-3-8B LLM architectures. where denotes the window size (set to 64 in our default implementation) for limiting the length of the lookback window for the input token. Liger Attention demonstrates strong performance in sequence modeling tasks while maintaining efficient linear complexity of O(T + D2). 3.4. Liger Architecture and Its Hybrid The overall architecture of our proposed Liger is presented in Fig. 2. Following the popular LLM architecture like Llama (Dubey et al., 2024), we retain the Pre-Norm layers and MLP layers with residual connection (He et al., 2016), only change the softmax attention layers with Liger attention without introduction of any new trainable modules like feature mapping. For the each Liger blocks including time mixing layer and token mixing layer, the forward process can be formulated as: = LigerAttn(Norm(X)) + = MLP(Norm(H)) + (11) As presented in Fig. 3, we also attempt to add one softmax attention block after stacking number of Liger (or gated linear recurrent) blocks to construct layer-wise hybrid model architecture. 4. Experiments RQ3: Does Liger genuinely achieves linear/subquadratic time complexity and constant memory inference? RQ4: How effective is Liger to its key components? 4.1. Experimental Setups Models and Datasets. We select two popular LLM architectures: Mistral-7B (Jiang et al., 2023) and Llama-3-8B (Dubey et al., 2024) as base model for linearization. We opt for GLA (Yang et al., 2023), general gated linear recurrent model structure, as the basis of the Liger and its hybrid architecture for linearization. We use 50,000 high quality instruction samples of cleaned Alpaca dataset (Taori et al., 2023) during linearization process to improve instructionfollowing ability and recover LLM performance in language modeling tasks. Implementation Configurations and Details. All experiments are implemented in PyTorch and conducted on single NVIDIA A800 80GB GPU. We opt for AdamW optimizer with learining rate of 1e3. By default, the LoRA rank is set to 8 and alpha is set to 8. The finetuning epochs is 2, which means we only use 100,000 cleaned Alpaca instruction samples (around 0.02B tokens) for gate reccurrent model linearization. We pad the input sequence to 1024 tokens with mini batch size of 1, and set the global batch size to 8 by gradient accumulaltion, following the settings in LoLCATs (Zhang et al., 2024a). In this section, we conduct extensive experiments to answer the following research questions (RQ): 4.2. Main Results: Liger can recover pre-trained LLMs performance more effectively (RQ1) RQ1: Can Liger linearize the pre-trained LLMs and recover performance more effectively compared with other linearization methods? RQ2: Can Liger serve as universal and scalable linearization method for different LLM architectures? To validate the effectiveness of our proposed method, we conducted experiments on series of language modeling and understanding tasks, including PiQA (Bisk et al., 2020), ARC-easy (ARC-e), ARC-challenge (ARC-c) (Clark et al., (Zellers et al., 2019), Wino2018), HellaSwag (Hella.) 6 Liger: Linearizing Large Language Models to Gated Recurrent Structures Model (Transformer) Mistral-7B Llama-3-8B (Linear/Subquadratic) Mamba-7B RWKV-6-World-7B TransNormerLLM-7B Hawk-7B Griffin-7B (Hybrid) StripedHyena-Nous-7B Zamba-7B Zamba2-7B (Linearized) Liger-GLA-Llama-3-8B (Ours) Liger-GLA-Llama-3-8B-H (Ours) Training Tokens (B) PiQA ARC-e ARC-c Hella. Wino. MMLU Avg. Avg. acc acc acc_norm acc_norm acc acc (5-shot) (no MMLU) 8000 15000 1200 1420 1400 300 300 - 1000 2100 0.02 0.02 80.6 79. 81.0 78.7 80.1 80.0 81.0 78.8 81.4 81.0 80.0 80.4 80.7 80.1 77.5 76.8 75.4 74.4 75.4 77.2 74.5 80. 78.7 80.1 53.9 53.2 46.7 46.3 44.4 45.9 47.9 40.0 46.6 56.4 51.9 52.6 81.1 79. 77.9 75.1 75.2 77.6 78.6 76.4 80.2 81.5 76.6 75.8 74.3 72.9 71.8 70.0 66.1 69.9 72.6 66.4 76.4 77. 71.4 71.5 62.6 65.3 33.3 - 43.1 35.0 39.3 26.0 57.7 64.8 43.4 44.4 72.2 71. 64.7 69.4 64.1 63.8 65.8 60.8 69.5 73.5 67.0 67.5 74.1 73.0 71.0 69.4 68.2 69.6 71.1 67.8 71.8 75. 71.7 72.1 Table 3. Performance Comparison of Pre-trained LLM Architectures on Commonsense Reasoning and Knowledge Benchmarks. Results span Transformer-based (Mistral-7B, Llama-3-8B), linear/subquadratic (Mamba, RWKV), hybrid (Zamba), and our linearized Liger-GLA variants on language modeling and understanding tasks. Our Linearized-Liger models achieve competitive performance with only 0.02B training tokens, demonstrating efficient adaptation to gated linear recurrent architectures. Figure 4. Decoding Latency Time and GPU Memory Usage of Each 8B Models. We variate the decoding length from 1K to 32K with fixed batch size of 16 on single A800 80GB GPU to evaluate the models efficiency. Liger enjoys linear-time inference with constant GPU memory usage. Grande (Wino.) (Sakaguchi et al., 2019) and MMLU (Li et al., 2023). The results are reported in Table 2 and all evaluations were done using lm-evaluation-harness (Gao et al., 2024). Liger, utilizing only 0.02B tokens, achieves linear recurrent model that recovers 90% of Mistrals performance and 93% of Llama-3s performance with only 0.085% model parameters LoRA finetuning. Our method significantly outperforms other linearization baselines, incluing SUPRA (Mercat et al., 2024) and Mamba In Llama (Wang et al., 2024), which still need billions of tokens for linear recurrent architecture conversion fine-tuning, and LoLCATs linearization approach, which requires two-stage process and twice the number of training tokens. We also compared Liger with other pre-trained models, including the Transformer models Mistral (Jiang et al., 2023) and Llama-3 (Touvron et al., 2023), linear/subquadratic models such as Mamba (Gu & Dao, 2023), RWKV-6 (Peng Model Avg. Avg. (no MMLU) Llama-3.2-1B GLA-1B LoLCATs-Llama-3.2-1B Liger-GLA-Llama-3.2-1B Llama-3.2-3B LoLCATs-Llama-3.2-3B Liger-GLA-Llama-3.2-3B Llama-3-8B LoLCATs-Llama-3-8B Liger-GLA-Llama-3-8B (Ours) 55.1 46.9 51.1 52.9 66.1 55.6 60.7 71.7 62.2 67.0 59.9 51.1 56.7 59.0 68.1 62.0 66. 73.0 70.0 71.7 Table 4. Scalability Analysis of Linearized Llama-3 Architectures across Model Sizes (1B to 8B). Liger demonstrates consistent scaling laws, outperforming LoLCATs by +6.811.5% absolute on average metrics while preserving 8398% of base model capabilities with only 0.02B adaptation tokens. et al., 2023), TransNormerLLM (Qin et al., 2023), Hawk and Griffin (De et al., 2024), as well as hybrid models like StripedHyena (Poli et al., 2023), Zamba and Zamba2 (Glorioso et al., 2024). As shown in Table 3, our proposed method outperformed nearly all of the pre-trained linear/subquadratic models and achieve competitive performance compared with transformer-based and hybrid LLMs. 4.3. Liger is Efficient and Scalable Hybrid Structure (RQ2 & RQ3)"
        },
        {
            "title": "We conducted experiments to compare efficiency in terms of\ndecoding latency speed and GPU memory consumption of",
            "content": "7 Liger: Linearizing Large Language Models to Gated Recurrent Structures Gated Linear Recurrent Variants Gated Memory Formulation Output Formulation Form of Gate Avg. MMLU 0-shot 5-shot Liger-GLA Liger-HGRN2 Liger-GSA St = Gt St1 + St = GtSt1 + (1 Gt)vt (cid:26) Kt = Gt Kt1 + (1 Gt)kt Vt = Gt Vt1 + (1 Gt)vt vt ot = qtSt ot = qtSt ot = Vt Softmax( Gt RD Gt RD qt) Gt RM 71.7 69.5 70.5 43.4 36. 41.2 Table 5. Gated Linear Recurrent Model Variants with Liger. Liger can be applied to the efficient linearization of various linear recurrent structures with gating mechanism and achive high quality performance recovery. Llama-3-8B without (w/o.) Flash-Attention-2 (FA2), Llama3-8B with (w/.) Flash-Attention-2 (FA2), Liger-GLA-8B and Liger-GLA-8B-Hybrid architecture (Liger-GLA-8B-H) on single A800 80GB GPU. We set fixed batch size of 16 and variate the decoding sequence length from 1K to 32K with the fixed prefix input length of 128. As presented in Fig. 4, we observe that Liger achieves linear-time decoding complexity while maintaining constant memory usage. We evaluate efficiency across three scales of the Llama-3 series (1B, 3B, 8B), comparing vanilla transformers, GLA, LoLCATs, and our Liger-GLA. As shown in Table 4, Liger consistently outperforms both GLA and LoLCATs while preserving 93% of the base Llama-3s performance on average. Notably, GLA-1B substantially underperforms all methods (46.9% average), highlighting the necessity of our parameter-efficient adaptation strategy. The performance gap between Liger and vanilla Llama-3 narrows with model size ( = 4.8% at 1B = 1.8% at 8B), indicating improved architectural compatibility at scale. We conduct experiments on gated linear recurrent structure variations including GLA (Yang et al., 2023), HGRN2 (Qin et al., 2024c) and GSA (Zhang et al., 2024d), with the results presented in Table 5, demonstrating the extensibility of Liger on various gated linear recurrent structures. 4.4. Liger Framework Analysis (RQ4) To verify the key components of Liger, we conducted ablation studies on the model structure. Specifically, we experimented with using gates generated from randomly initialized gate projections (Gate Proj.) instead of pooling. Additionally, we incorporated learnable feature map modules (Feat. Map.), similar to Zhang et al. (2024b). We also evaluated the effects of removing LoRA (w/o LoRA) and SWA (w/o SWA) individually. The results of these experiments are detailed in Table 6, demonstrating the effectiveness of proposed components in Liger. 5. Related Work Linear Recurrent Models and their Hybrid. To address the challenges of quadratic complexity computation cost"
        },
        {
            "title": "Model\nVariants",
            "content": "Liger-GLA - Gate Proj. - Feat. Map. - w/o LoRA - w/o SWA Validation PPL. Avg. Avg. 2.96 3.16 9.04 3.23 3.75 (no MMLU) 67.0 63.8 43.5 61.7 54.2 71.7 68.8 40.2 68.1 60.2 Table 6. Ablation Study of Liger on Gated Linear Attention. We linearize Llama-3-8B into Gated Linear Attention (GLA) to evaluate the key components of Liger. We report Validation perplexity (PPL.) on cleaned alpaca dataset after Liger linearization and the average performance on language modeling and understanding tasks. in standard softmax attention, many linear recurrent models are proposed to achieve efficient training and inference. Data-dependent gating/decay has been proved as an effective mechanism to control the memory changes and improve sequence modeling expressiveness (Yang et al., 2023; Peng et al., 2024; Qin et al., 2024c; Zhang et al., 2024c; Du et al., 2025). Recently, some layer-wise hybrid model architectures have been proposed to compensate for the lack of memory capacity in the linear recurrent models, such as StripedHyena-Nous (Poli et al., 2023), Jamba (Lieber et al., 2024) and Zamba (Glorioso et al., 2024). In addition, integration of softmax and linear attention shows great potential as new hybrid attention paradigm, such as Agent Attention (Han et al., 2024), GSA (Zhang et al., 2024c) and Minimax-01 (MiniMax et al., 2025). However, all of these hybrid attention architectures introduce extra modules (e.g. feature mapping and gate modules) that need to be trained from scratch, which increases the complexity of the model architecture design and may lead to suboptimal softmax attention approximation. Linearizing Large Language Models. Linearizing or finetuning (uptraining) transformers to linear-RNNs could significantly reduce the cost of training brand-new large-scale linear recurrent model architecture by distilling knowledge from pre-trained LLMs. Most linearization methods are proposed to uptrain transformer-based LLMs into linear-RNNs by introducing extra feature mapping modules (Kasai et al., 8 Liger: Linearizing Large Language Models to Gated Recurrent Structures 2021; Mercat et al., 2024; Chen et al., 2024) or adding loss (Zhang et al., 2024b; Bick et al., 2024; Zhang et al., 2024a) to approximate softmax attention. However, these methods have to introduce extra modules that cannot reuse the existing pre-trained LLM weights and need to be trained from scratch, which may not match the output of the original attention and increases the complexity of the model architecture, leading to suboptimal attention approximation. 6. Conclusion This paper introduces Liger, novel method for linearizing Transformer-based LLMs into gated linear recurrent structures. By leveraging the key matrix weights of pretrained standard-structure models and repurposing them to construct the gating mechanisms, Liger avoids the need for additional parameters and extensive fine-tuning, making it cost-effective and efficient linearization approach. The use of end-to-end LoRA fine-tuning restores model performance while minimizing linearization costs. Furthermore, the introduction of Liger Attention enhances nearly 93% performance recovery with only 0.02% pre-training tokens, making Liger competitive solution across range of language modeling and understanding tasks. Our results demonstrate the effectiveness of Liger on models ranging from 1B to 8B parameters, offering promising path toward more efficient deployment of large-scale LLMs with linear-time inference and constant memory usage."
        },
        {
            "title": "Impact Statement",
            "content": "This work represents notable advancement in artificial intelligence and machine learning, particularly in linearizing the pretrained Transformer-based models into gated recurrent structures. Liger enables the processing of much longer sequences compared to existing methods while significantly accelerating computation, making it highly beneficial for tasks like natural language understanding, genomic sequence analysis, and time-series forecasting. However, the enhanced capabilities and efficiency introduced by Liger also raise ethical and societal considerations, such as the potential for misuse in generating persuasive but misleading content or in surveillance applications. Nevertheless, the contributions of Liger to reducing computational overhead and energy consumption in training large models may also bring positive environmental impacts."
        },
        {
            "title": "References",
            "content": "Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. URL https://arxiv. org/abs/2012.13255. Beck, M., Pöppel, K., Spanring, M., Auer, A., Prudnikova, O., Kopp, M., Klambauer, G., Brandstetter, J., and Hochreiter, S. xlstm: Extended long short-term memory, 2024. URL https://arxiv.org/abs/2405. 04517. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer, 2020. Bick, A., Li, K. Y., Xing, E. P., Kolter, J. Z., and Gu, A. Transformers to ssms: Distilling quadratic knowledge to subquadratic models, 2024. URL https://arxiv. org/abs/2408.10189. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Chen, H., Liu, Z., Wang, X., Tian, Y., and Wang, Y. Dijiang: Efficient large language models through compact kernelization, 2024. URL https://arxiv.org/abs/ 2403.19928. Chintala, S. Gpt-4 moe, URL https://x.com/soumithchintala/status/ 1671267150101721090. June 2023. Chou, Y., Yao, M., Wang, K., Pan, Y., Zhu, R., Zhong, Y., Qiao, Y., Wu, J., Xu, B., and Li, G. Metala: Unified optimal linear approximation to softmax attention map, 2024. URL https://arxiv.org/abs/2411.10741. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. Dao, T. and Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. De, S., Smith, S. L., Fernando, A., Botev, A., CristianMuraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y., Srinivasan, S., Desjardins, G., Doucet, A., Budden, D., Teh, Y. W., Pascanu, R., Freitas, N. D., and Gulcehre, C. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. URL https://arxiv.org/abs/2402.19427. Du, J., Sun, W., Lan, D., Hu, J., and Cheng, Y. Mom: Linear sequence modeling with mixture-of-memories. arXiv preprint arXiv:2502.13685, 2025. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 9 Liger: Linearizing Large Language Models to Gated Recurrent Structures Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Li, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y., Duan, N., and Baldwin, T. Cmmlu: Measuring massive multitask language understanding in chinese, 2023. Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., ShalevShwartz, S., et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. Glorioso, P., Anthony, Q., Tokpanov, Y., Whittington, J., Pilault, J., Ibrahim, A., and Millidge, B. Zamba: compact 7b ssm hybrid model, 2024. URL https:// arxiv.org/abs/2405.16712. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Han, D., Ye, T., Han, Y., Xia, Z., Pan, S., Wan, P., Song, S., and Huang, G. Agent attention: On the integration of softmax and linear attention, 2024. URL https: //arxiv.org/abs/2312.08874. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. org/abs/2106.09685. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G., Pappas, N., Mao, Y., Chen, W., and Smith, N. A. Finetuning pretrained transformers into rnns, 2021. URL https://arxiv.org/abs/2103.13076. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers In International Conference on with linear attention. Machine Learning, pp. 51565165. PMLR, 2020. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023. Mercat, J., Vasiljevic, I., Keh, S., Arora, K., Dave, A., Gaidon, A., and Kollar, T. Linearizing large language models, 2024. URL https://arxiv.org/abs/ 2405.06640. MiniMax, Li, A., Gong, B., Yang, B., Shan, B., Liu, C., Zhu, C., Zhang, C., Guo, C., Chen, D., Li, D., Jiao, E., Li, G., Zhang, G., Sun, H., Dong, H., Zhu, J., Zhuang, J., Song, J., Zhu, J., Han, J., Li, J., Xie, J., Xu, J., Yan, J., Zhang, K., Xiao, K., Kang, K., Han, L., Wang, L., Yu, L., Feng, L., Zheng, L., Chai, L., Xing, L., Ju, M., Chi, M., Zhang, M., Huang, P., Niu, P., Li, P., Zhao, P., Yang, Q., Xu, Q., Wang, Q., Wang, Q., Li, Q., Leng, R., Shi, S., Yu, S., Li, S., Zhu, S., Huang, T., Liang, T., Sun, W., Sun, W., Cheng, W., Li, W., Song, X., Su, X., Han, X., Zhang, X., Hou, X., Min, X., Zou, X., Shen, X., Gong, Y., Zhu, Y., Zhou, Y., Zhong, Y., Hu, Y., Fan, Y., Yu, Y., Yang, Y., Li, Y., Huang, Y., Li, Y., Huang, Y., Xu, Y., Mao, Y., Li, Z., Li, Z., Tao, Z., Ying, Z., Cong, Z., Qin, Z., Fan, Z., Yu, Z., Jiang, Z., and Wu, Z. Minimax-01: Scaling foundation models with lightning attention, 2025. URL https://arxiv.org/abs/2501.08313. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Derczynski, L., Du, X., Grella, M., Gv, K., He, X., Hou, H., Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau, H., Lin, J., Mantri, K. S. I., Mom, F., Saito, A., Song, G., Tang, X., Wind, J., Wozniak, S., Zhang, Z., Zhou, Q., Zhu, J., and Zhu, R.-J. RWKV: Reinventing RNNs for the transformer era. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1404814077, Singapore, December 2023. Association for Computational doi: 10.18653/v1/2023.findings-emnlp. Linguistics. 936. URL https://aclanthology.org/2023. findings-emnlp.936. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Ferdinan, T., Hou, H., Kazienko, P., et al. Eagle and finch: Rwkv with matrixvalued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Poli, M., Wang, J., Massaroli, S., Quesnelle, J., Carlow, R., Nguyen, E., and Thomas, A. Stripedhyena: Moving beyond transformers with hybrid signal processing models. 10 Liger: Linearizing Large Language Models to Gated Recurrent Structures URL https://github. com/togethercomputer/stripedhyena, 2023. Qin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei, Y., Lv, B., Luo, X., Qiao, Y., et al. Transnormerllm: faster and better large language model with improved transnormer. arXiv preprint arXiv:2307.14995, 2023. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024a. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y. Various lengths, constant speed: Efficient language modeling with lightning attention. arXiv preprint arXiv:2405.17381, 2024b. Qin, Z., Yang, S., Sun, W., Shen, X., Li, D., Sun, W., and Zhong, Y. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024c. Qu, X., Dong, D., Hu, X., Zhu, T., Sun, W., and Cheng, Y. Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. arXiv preprint arXiv:2411.15708, 2024. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019. Sun, W., Lan, D., Qu, X., and Cheng, Y. Lasp-2: Rethinking sequence parallelism for linear attention and its hybrid. arXiv preprint, 2024a. Sun, W., Qin, Z., Li, D., Shen, X., Qiao, Y., and Zhong, Y. Linear attention sequence parallelism. arXiv preprint arXiv:2404.02882, 2024b. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Sun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma, S., Zhang, Q., Wang, J., and Wei, F. You only cache once: Decoder-decoder architectures for language models, 2024c. URL https://arxiv.org/abs/2405. 05254. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama https://github.com/tatsu-lab/ model. stanford_alpaca, 2023. Team, I. Internlm: multilingual language model with progressively enhanced capabilities, 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, J., Paliotta, D., May, A., Rush, A. M., and Dao, T. The mamba in the llama: Distilling and accelerating hybrid models, 2024. URL https://arxiv.org/ abs/2408.15237. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=fq0NaiU8Ex. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Zhang, M., Arora, S., Chalamala, R., Wu, A., Spector, B., Singhal, A., Ramesh, K., and Ré, C. Lolcats: On lowrank linearizing of large language models, 2024a. URL https://arxiv.org/abs/2410.10254. Zhang, M., Bhatia, K., Kumbong, H., and Ré, C. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry, 2024b. URL https://arxiv. org/abs/2402.04347. Zhang, Y., Yang, S., Zhu, R., Zhang, Y., Cui, L., Wang, Y., Wang, B., Freda Shi, Bailin Wang, W. B., Zhou, P., and Fu, G. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146, 2024c. Zhang, Y., Yang, S., Zhu, R., Zhang, Y., Cui, L., Wang, Y., Wang, B., Shi, F., Wang, B., Bi, W., et al. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146, 2024d. Zhu, T., Qu, X., Dong, D., Ruan, J., Tong, J., He, C., and Cheng, Y. Llama-moe: Building mixture-of-experts from llama with continual pre-training. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1591315923, 2024. Liger: Linearizing Large Language Models to Gated Recurrent Structures A. Datasets and Benchmarks We linearize Liger on Cleaned Alpaca dataset (Taori et al., 2023) and evaluate on downstream language tasks using lm-evaluation-harness (Gao et al., 2024). Cleaned Alpaca (Taori et al., 2023): The cleaned Alpaca dataset is structured dataset designed for instruction-tuning of language models, containing 52,000 unique instructions and corresponding outputs generated by OpenAIs textdavinci-003 engine. Each entry in the dataset includes an \"instruction\" field that specifies the task for the model, an optional \"input\" field providing context or additional information, and an \"output\" field with the models response. The dataset is formatted in JSON and is intended to enhance the ability of language models to follow instructions effectively. PiQA (Bisk et al., 2020): The PiQA (Physical Interaction: Question Answering) dataset is designed to assess physical commonsense reasoning, containing 3,084 samples for testing. Each instance includes \"goal\" field representing the question, two \"solution\" fields with potential answers, and \"label\" indicating the correct solution. The dataset focuses on everyday situations requiring physical commonsense. ARC-Easy & ARC-Challenge (Clark et al., 2018): The ARC (AI2 Reasoning Challenge) dataset is collection of 7,787 genuine grade-school level multiple-choice science questions. It is divided into two subsets: ARC-Easy and ARC-Challenge. The ARC-Easy subset contains relatively straightforward questions that test basic knowledge, while the ARC-Challenge subset includes more complex and difficult questions that require advanced reasoning abilities HellaSwag (Zellers et al., 2019): The HellaSwag dataset is comprehensive collection of narrative reasoning tasks designed to evaluate models ability to predict the next event in sequence. It consists of 10,125 examples, each containing context and four possible endings, with one correct and three incorrect options. The dataset is derived from the ActivityNet Captions corpus and is structured to test the models understanding of narrative coherence and common-sense reasoning based on the given context. WinoGrande (Sakaguchi et al., 2019): The WinoGrande dataset is large-scale collection of 44,000 problems designed to evaluate commonsense reasoning, inspired by the Winograd Schema Challenge but enhanced in scale and difficulty. Each problem is structured as fill-in-the-blank task with two options, requiring models to choose the correct answer based on contextual understanding. WinoGrande is significantly more challenging than the original Winograd Schema Challenge, with state-of-the-art models achieving lower accuracy, highlighting its effectiveness in testing true commonsense understanding. MMLU (Li et al., 2023): The MMLU (Massive Multitask Language Understanding) dataset is comprehensive benchmark designed to evaluate AI models general knowledge across wide range of subjects and languages. It comprises 57 distinct categories, spanning elementary-level knowledge to advanced professional topics such as law, physics, history, and computer science. The dataset has been translated into 14 languages using professional human translators, ensuring high-quality and accurate translations. This multilingual approach aims to improve the inclusivity and effectiveness of AI models across different linguistic communities. B. Experiment Details Our 8B model linearization experiments are conducted on single NVIDIA A800 80G GPU. With batch size 1 and gradient accumulation over 8 batches, Liger method takes around 4 hours and 27GB GPU memory usage for 2 epochs end-to-end linearization, instead of any multi-stage training. All our experiments were conducted and evaluated using fixed random seed of 0 to ensure reproducibility. C. Full Results 12 Liger: Linearizing Large Language Models to Gated Recurrent Structures Sequence Length Llama-3-8B w/o FA2 Llama-3-8B w/ FA2 Liger-8B Time Memory Time Memory Time Memory 1K 2K 4K 8K 16K 32K 37.92 102.54 312.98 1062.65 3882.36 - 17.50 19.75 24.25 33.26 51.26 OOM 29.36 62.52 151.51 436.04 1449.20 - 17.26 19.29 23.35 31.48 47.73 OOM 47.83 94.41 185.79 367.78 734.91 1465.52 16.37 16.37 16.37 16.37 16.37 16. Table 7. Detailed Results on Inference Efficiency in terms of Decoding Latency Time and GPU Memory Usage. We present the decoding latency time (seconds) and the GPU memory usage (GB) during inference stage compared with Llama-3-8B without (w/o), with (w/) Flash-Attention-2 (FA2) and Liger-8B. OOM denotes out of memory. Model PiQA ARC-e ARC-c Hella. Wino. MMLU Avg. Avg. acc acc acc_norm acc_norm acc acc (5-shot) (no MMLU) Llama-3.2-1B GLA-1B LoLCATs-Llama-3.2-1B Liger-GLA-Llama-3.2-1B Llama-3.2-3B LoLCATs-Llama-3.2-3B Liger-GLA-Llama-3.2-3B Llama-3-8B LoLCATs-Llama-3-8B Liger-GLA-Llama-3-8B (Ours) 74.1 69.9 74.1 75.0 76.4 76.7 77.9 79.4 80.1 80.0 65.4 55.2 63.7 65. 74.7 72.0 74.0 80.1 80.4 78.7 36.4 27.6 36.4 35.7 46.0 42.3 43.9 53.2 53.5 51.9 63.8 48.9 51.2 59. 73.6 51.91 70.3 79.2 63.4 76.6 60.0 53.9 58.2 59.1 69.9 66.9 66.3 72.9 72.9 71.4 31.0 25.9 23.1 22. 56.2 23.6 32.1 65.3 23.0 43.4 55.1 46.9 51.1 52.9 66.1 55.6 60.7 71.7 62.2 67.0 59.9 51.1 56.7 59. 68.1 62.0 66.5 73.0 70.0 71.7 Table 8. Full Results on Scalability Analysis of Linearized Llama-3 Architectures across Model Sizes (1B to 8B). Performance comparisons between vanilla Llama-3, GLA, LoLCATs, and our Liger-GLA variants on language modeling and reasoning tasks. Liger demonstrates consistent scaling laws, outperforming LoLCATs by +6.811.5% absolute on average metrics while preserving 8398% of base model capabilities with only 0.02B adaptation tokens. Model PiQA ARC-e ARC-c Hella. Wino. MMLU Avg. Avg. acc acc acc_norm acc_norm acc acc (5-shot) (no MMLU) Liger-GLA Liger-HGRN2 Liger-GSA 80.0 79.2 79.5 78.7 76.8 78.5 51.9 48.5 49.4 76.6 74.4 74. 71.4 68.8 70.5 43.4 36.2 41.2 67.0 64.0 65.6 71.7 69.5 70.5 Table 9. Full Results on Different Gated Linear Recurrent Model Variants with Liger. Liger can be applied to the efficient linearization of various linear recurrent structures with gating mechanism and achive high quality performance recovery. Model Liger-GLA - Gate Proj. - Feat. Map. - w/o LoRA - w/o SWA Validation PPL. PiQA ARC-e ARC-c Hella. Wino. MMLU Avg. Avg. 2.96 3.16 9.04 3.23 3.75 acc acc acc_norm acc_norm acc acc (5-shot) (no MMLU) 80.0 79.1 63.1 78.7 75.0 78.7 75.9 46.3 75.6 68.3 51.9 49.6 24.2 47.4 39.1 76.6 71.8 33.7 74.0 63.4 71.4 67.3 50.4 64.8 55. 43.4 39.2 23.8 29.5 26.4 67.0 63.8 40.2 61.7 54.2 71.7 68.8 43.5 68.1 60.2 Table 10. Full Results on Ablation Study. We linearize Llama-3-8B into Gated Linear Attention (GLA) to evaluate the key components of Liger. We report Validation perplexity (PPL.) on cleaned alpaca dataset after Liger linearization and the average performance on language modeling and under-standing tasks."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shanghai AI Laboratory",
        "South China University of Technology",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}