{
    "paper_title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
    "authors": [
        "Dongjie Cheng",
        "Yongqi Li",
        "Zhixin Ma",
        "Hongru Cai",
        "Yupeng Hu",
        "Wenjie Wang",
        "Liqiang Nie",
        "Wenjie Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 1 ] . [ 1 6 3 5 9 0 . 1 0 6 2 : r 2026-1-14 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Dongjie Cheng1 Yupeng Hu3 Wenjie Wang4 Yongqi Li1 Zhixin Ma2 Hongru Cai1 Liqiang Nie5 Wenjie Li1 1 The Hong Kong Polytechnic University 2 Singapore Management University 3 Shandong University 4 University of Science and Technology of China 5 Harbin Institute of Technology (Shenzhen) dong-jie.cheng@connect.polyu.hk, liyongqi0@gmail.com Abstract Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting promising direction for generative multimodal reasoning. Project Page: https://github.com/ModalityDance/Omni-R"
        },
        {
            "title": "1 Introduction",
            "content": "Multimodal Large Language Models (MLLMs) are evolving from basic perception to more advanced reasoning capabilities [1]. Early multimodal reasoning methods primarily focus on pure text-based reasoning, where MLLMs perform textual reasoning before generating the final answer. More recently, some studies [2] highlight that the intermediate reasoning steps also require the involvement of visual information. For example, as illustrated in Figure 1, spatial relation problem can be more easily solved by zooming in on the critical region during the intermediate reasoning process. There have been some interleaved-modal reasoning methods [3, 4, 5, 6] to incorporate multimodal information into the reasoning steps. For example, in DeepEyes [3], the method utilizes external tools to Corresponding author(s). Figure 1: An example illustrating the necessity of incorporating visual information into the intermediate reasoning steps for multimodal tasks. zoom in on specific region of an image, enabling the model to focus on particular areas of the input image for VQA tasks. In MVoT [4], the approach involves imagining possible paths by visualizing intermediate states to solve spatial reasoning tasks. Despite the Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Figure 2: The illustration of various multimodal tasks and the corresponding diverse multimodal reasoning skills, with multimodal tasks shown in the top row and the required reasoning skills summarized in the bottom row. effectiveness, current interleaved-modal reasoning methods still rely on single specific multimodal reasoning pattern to address particular multimodal task. To address the above issue, we propose to develop general and unified multimodal reasoning paradigm capable of handling broad range of multimodal tasks. 1) First, there are numerous multimodal tasks that require diverse reasoning skills in practice. As shown in Figure 2, skills such as zoom-in and grounding are essential for general VQA tasks, while visual prediction skills are clearly beneficial for visual-operational scenes. 2) Second, it is necessary to facilitate various multimodal reasoning skills within single MLLM in unified paradigm. Inspired by the recent development of Omni models [7, 8] that exhibit multimodal generation capabilities, we are excited to propose that all multimodal reasoning skills can be unified and internalized within generative paradigm, i.e., unified generative multimodal reasoning. For example, zoom-in can be realized by generating magnified view of specific image region, while grounding can be achieved by generating annotated bounding boxes over the objects in the original image. Despite this promising vision of the generative multimodal reasoning paradigm, putting it into practice remains challenging due to the following aspects. 1) Functional image generation. Unlike typical images, the images required for various reasoning skills are functional and often unnatural, such as those containing marked numbers over specific objects. This poses significant challenge for MLLMs. 2) Costly interleaved-modal reasoning annotations. The training of generative multimodal reasoning requires stepby-step annotations of the interleaved reasoning process. Such interleaved data is costly and rarely available at scale. In this work, we propose Omni-R1, framework that unifies various multimodal reasoning skills in generative manner without relying on extensive supervision. Specifically, we introduce standard OmniR1, which requires two-stage optimization procedure with supervised fine-tuning (SFT) and reinforcement learning (RL) with small amount of reasoning annotations. To address the issue of functional image generation, we introduce perception alignment loss for SFT and perception-calibrated reward for RL, which ensures that the models optimization is supervised by visual perception. Furthermore, we present Omni-R1-Zero, which eliminates the need for costly interleaved-modal reasoning annotations. Instead, 2 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning it introduces bootstrapping step-wise visualization method to synthesize interleaved-modal reasoning data. Empirical results suggest that Omni-R1 significantly achieves unified generative reasoning across various multimodal reasoning tasks, while Omni-R1Zero even surpasses Omni-R1 to some extent. The contributions are summarized as follows: We identify range of distinctive multimodal reasoning skills for multimodal tasks, and propose generative multimodal reasoning paradigm to unify them within single MLLM. We introduce Omni-R1, which facilitates the unified multimodal reasoning paradigm and stabilizes functional image generation via the designed perception alignment loss and perception-calibrated reward. We present Omni-R1-Zero, which eliminates the need for interleaved-modal reasoning annotations while achieving comparable performance to models trained with supervised annotations."
        },
        {
            "title": "2.1 Text-based Multimodal Reasoning",
            "content": "Prior work focuses on text-based multimodal reasoning, where images serve as context, and the model generates textual rationales and the final answer. KAM-CoT [9] augmented multimodal chain-ofthought with structured external knowledge. More recently, reinforcement learning has been proven to be effective in multimodal reasoning. MMEUREKA [1] trained MLLMs with rule-based, verifiable rewards, enhancing their reasoning abilities in math tasks. Visual-Thinker-R1-Zero [10] further applied such rule-based reinforcement learning directly to non-SFT MLLM on math data, achieving improved visual reasoning performance."
        },
        {
            "title": "2.2 Interleaved-modal Reasoning\nICOT [11] introduced the concept of interleaved-\nmodal reasoning, where the model augments its trajec-\ntory by interleaving additional multimodal evidence.\nDeepEyes [3] used tools to acquire and append new\nmultimodal observations during reasoning, enabling\nfine-grained improvements through reinforcement\nlearning. More recently, interleaving has been re-\nalized within a single omni-model as interleaved",
            "content": "generation. MVoT [4] trained an omni-MLLM to generate image-text interleaved rationale in spatial reasoning task. To support such behaviors with supervision, Zebra-CoT [12] released 182K interleaved text-image reasoning dataset for the exploration of interleaved-modal reasoning, especially in omni-models. There is another work with the same name, Omni-R1 [13], which uses RL to optimize the selection of informative keyframes and pixel-level grounding for video segmentation."
        },
        {
            "title": "3 Omni-Bench",
            "content": "To better evaluate MLLMs multimodal reasoning capabilities, we conduct Omni-Bench, which consists of various multimodal tasks that require diverse multimodal reasoning skills. Uni-Tasks. Uni-Tasks characterize the multimodal reasoning scenarios included in Omni-Bench. Natural-Scene Perception focuses on natural-scene images and requires evidence localization for answering. Diagrammatic Math involves diagramgrounded visual arithmetic and geometric reasoning. Structured-Image targets structured image inputs, such as figures and charts, that combine text and graphics. Vision-Operational Scenes, such as visual games and embodied planning, require complex visual operations or even predicting the transition of scene states. For each Uni-Task, we sample data from established datasets to form slice. Table 1 summarizes the source datasets and the number of prompts in each slice. Uni-Skills. Across these scenarios, generative multimodal reasoning should require set of recurring visual skills for functional image generation. We summarize them as four Uni-Skills: Grounding localizes task-relevant evidence and may zoom in for closer inspection when needed. Auxiliary line drawing draws helper lines to make geometric relations or alignment constraints explicit. Marking highlights or enumerates relevant instances so they can be referred to unambiguously. Visual prediction anticipates the next visual state by performing one-step transition. Evaluation. For evaluation, we extract the final answer span Ans and use an LLM judge [20, 21] with fixed evaluation template to compare it against 3 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Task Source #Samples Natural-Scene Perception [14] Structured-Image Diagrammatic Math ArxivQA [15] ChartQA [16] Geometry3k [17] MathVista [18] Vision-Operational Scenes ViC-Bench [19] 100 100 100 100 300 Table 1: The composition of Omni-Bench. total of 800 samples span four Uni-Tasks. the gold reference, producing binary correctness decision."
        },
        {
            "title": "4 Method",
            "content": "As shown in Figure 3, we present two training frameworks, Omni-R1 and Omni-R1-Zero. Both aim to learn generative multimodal reasoning while unifying diverse reasoning skills and remaining effective under limited or even zero interleaved supervision."
        },
        {
            "title": "4.1 Omni-R1",
            "content": "Omni-R1 optimizes the policy in two stages: 1) Perception-Aligned Supervised Fine-Tuning (PeSFT). This stage learns the interleaved reasoning format and Uni-Skills prediction using cross-entropy loss, and optimizes the image tokens representation with the perception loss. 2) Perception-Calibrated Relative Policy Optimization (PeRPO). It extends training to Uni-Tasks without multimodal annotations by refining the policy model with perceptioncalibrated reward."
        },
        {
            "title": "4.1.1 PeSFT\nWe denote the multimodal reasoning token trajectory\nfor input x as y1:T, where T indicates the token count.\nThe output y adheres to a unified template across\nvarious scenarios, as detailed in Appendix A. PeSFT\noptimizes a cross-entropy loss to enforce this unified\ngeneration format, while applying a perception loss\nto align the generation of visual tokens.",
            "content": "Cross-Entropy loss. We apply standard crossentropy loss over all tokens in the generative reasoning trajectory, [ LCE = t=1 log πθ(yt x, y<t) ] , (1) 4 which trains the model to follow or imitate the ideal reasoning process. Overall, LCE encourages the model to reproduce the interleaved generative reasoning format of the trajectory. For image-token segments, it also serves as local supervision for UniSkills, guiding the model to generate the desired functional images given the current context. Perception loss. We utilize visual codebook RKD as the reference for image tokens within the unified template, where denotes the codebook size and the embedding dimension. Let Ω be the index of an image token in y, and ct {1, . . . , K} be its corresponding ground-truth index in E. We define the perception loss as: LPe = 1 Ω tΩ Wht E[ct] 2 2. (2) where ht RH is the final-layer hidden state and RDH is linear projection. With frozen, this loss aligns hidden states with the codebooks geometry, acting as perceptual prior to stabilize autoregressive image-token generation. Overall training objective. Putting the two terms together, the overall objective is LPeSFT = LCE + λ LPe, λ = 1. (3) where we weight the perception term by coefficient λ and set λ = 1 in all experiments. 4.1.2 PeRPO PeRPO is built upon group-relative RL objective and aims to optimize long, interleaved multimodal sequences. It introduces perception-calibrated reward to more accurately align RL feedback with the quality of intermediate functional image generations. We formulate PeRPO as an optimization over the policy models (πθ ) autoregressive trajectory generation, integrating perception-calibrated reward with group-relative PPO objective. , and RPe Perception-calibrated reward. We define the reward as weighted sum of three components, , RFmt RAcc ). We compute an accuracy reward Accuracy (RAcc by parsing the final answer and comparing it to the ground truth with rule-based verifier that checks numeric correctness, symbolic/textual equivalence, . Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Figure 3: Training pipeline for Omni-R1 and Omni-R1-Zero. (Top) Data initialization uses either limited human annotations (orange) or synthetic bootstrapped trajectories (blue). (Middle) Stage 1 (PeSFT) performs supervised fine-tuning with joint cross-entropy and perception losses. (Bottom) Stage 2 (PeRPO) refines the policy using unified tasks without multimodal annotation (left) and composite reward (Accuracy, Format, Perception) for final alignment. and domain-specific patterns. The accuracy reward is set to value in [0, 1] based on the degree of match to the ground truth. Implementation details of the verifier are provided in Appendix B. Format (RFmt ). We set RFmt = 1 if the generated trajectory contains well-formed reasoning part and final answer part that can be reliably parsed by the verifier, and RFmt = 0 otherwise. Perception (RPe ). We measure perceptual coherence of intermediate image generations via 2D Total Variation (TV) on codebook embeddings. Let RKD be frozen visual codebook. Given visual token index ct {1, . . . , K}, where is the codebook size and is the codebook embedding dimension, we retrieve its embedding by et = E[ct] RD. (4) The 1-D image-token segment with = HqWq tokens, where Hq and Wq is the height/width of the quantized image grid in VQVAE of the model, is reshaped into Hq Wq grid. The embedding matrix of this image can be presented as RHqWqD. (5)"
        },
        {
            "title": "We define the index sets Nh",
            "content": "and Nv as Nh = {(i, j) 1 Hq, 1 < Wq}, Nv = {(i, j) 1 < Hq, 1 Wq}."
        },
        {
            "title": "Then",
            "content": "and Eh = 1 Nh (i,j)Nh Zi,j+1 Zi,j2 2, Ev = 1 Nv (i,j)Nv Zi+1,j Zi,j2 2. Finally, we compute the 2-D TV energy as E2D = Eh + Ev ) . ( 1 D"
        },
        {
            "title": "We map energy to a normalized score",
            "content": "sr = 1 1 + E2D/τ , (6) (7) (8) (9) (10) where τ > 0 controls sensitivity. Let Sseg be the set of image-token segments in trajectory. We aggregate segment scores by RPe = 1 Sseg sr. rSseg (11) Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning"
        },
        {
            "title": "We combine the three reward components as",
            "content": "R = α RAcc + β RFmt + γ RPe, (12) where α, β, γ balance answer accuracy, format compliance, and perceptual consistency. We use as the scalar reward ri for each candidate when computing group-relative advantages. Policy optimization. In each optimization step, we sample set of reasoning trajectory candidates = {y(i)} for each prompt using the behavioral policy πold. Each y(i) is an interleaved multimodal sequence of length Ti and is assigned scalar reward r(i) according to Eq. 12. To facilitate meaningful gradient updates, we filter the samples to retain only mixed-outcome (non-degenerate) groups (i.e., σG > 0) before updating the policy network to πθ . Subsequently, we compute the group-relative advantage based on the reward: A(i) = r(i) µG σG + δ , (13) where µG and σG denote the mean and standard deviation of the rewards {r(j)}jG, and δ > 0 is small constant for numerical stability. Let mi,t {0, 1} be the response mask for candidate y(i), where {1, . . . , Ti} indexes token positions and mi,t = 1 indicates response position to be optimized. We denote the number of optimized (response) positions by Li = Ti Using the computed advantages, we maximize PPO-clip objective with KL regularizer to fixed reference policy πref: t=1 mi,t . (θ) = x,{y(i)}πold [ 1 iG 1 Li Ti t=1 ( mi,t L(i,t) clip (θ) βKL (i,t) KL (πθ, πref) )] . sive prefix level likelihood ratio and KL term as (i) 1 , . . . , (i) <t = (y (i) t1), we define the tokenρ(i,t)(θ) = πθ (i) x, (i) <t ( ( ) ) , πold x, (i) <t (i) ( ( πθ (i) x, (i) <t πref (i) x, (i) <t ) . ) (16) (i,t) KL (πθ, πref) = log"
        },
        {
            "title": "4.2 Omni-R1-Zero",
            "content": "We consider the setting where human-annotated interleaved multimodal rationales are unavailable, and bootstrap interleaved supervision from text-only CoT to activate generative multimodal reasoning."
        },
        {
            "title": "4.2.1 Bootstrapping Step-wise Visualization\nAs illustrated in Figure 3, Omni-R1-Zero starts from\ntext-only Chain-of-Thought (CoT) seeds and con-\nstructs synthetic interleaved trajectories via step-wise\nvisualization, generating one image for each reason-\ning step (Text Step k → Image Step k) and assembling\nthem with the same control-token template as Omni-\nR1.",
            "content": "These synthetic traces are not intended to be perfect supervision, but rather to teach the interleaved reasoning format and expose intermediate multimodal states."
        },
        {
            "title": "5 Experiment",
            "content": "(14)"
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Here L(i,t) clip (θ) is the PPO clipped surrogate loss, L(i,t) clip (θ) = min { ρ(i,t)(θ)A(i), clip(ρ(i,t)(θ), l, u) A(i)} . (15) with = 1 εlow, = 1 + εhigh, βKL > 0, and clip(z, l, u) = min(max(z, l), u). For the autoregresBenchmarks. We evaluated Omni-R1 and Omni-R1Zero on Omni-Bench, which covers four Uni-Tasks, namely Natural-Scene Perception, Diagrammatic Math, Structured-Image, and Vision-Operational. We additionally reported results on standard general multimodal benchmarks, including MME [22], MMVet [23], V-Bench [14], POPE [24], MMVP [25], 6 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Model Natural Structured Diagrammatic Vision-Op. Average Avg (%) Anole (Base) [8] Zebra-CoT [12] 0.180 0. 0.140 0.130 0.055 0.073 0.027 0. 0.081 0.129 - (-37.2) +59.3 (-) Omni-R1-M Omni-R1-L 0.440 0.420 Omni-R1-Zero-T 0.450 Omni-R1-Zero-S 0. Omni-R1-Zero-M 0.410 0.135 0.175 0.165 0.145 0. Omni-R1 0.115 0.090 0.095 0.090 0. 0.093 0.090 0.087 0.097 0.093 0.152 +87.7 (+17.8) 0.152 +87.7 (+17.8) 0.154 +90.1 (+19.4) 0.138 +70.4 (+7.0) 0.159 +96.3 (+23.3) Table 2: Results on Omni-Bench. Column headers correspond to the four Uni-Tasks: Natural (Natural-Scene Perception), Structured (Structured-Image), Diagrammatic (Diagrammatic Math), and Vision-Op. (Vision-Operational). Bold numbers indicate the best result in each column, underlined numbers indicate the second-best result. Avg (%) is the relative change w.r.t. Anole (Base), and the value in parentheses is computed w.r.t. Zebra-CoT. Model MME-P MME-R MM-Vet POPE MMVP BLINK Average Avg (%) Anole (Base) [8] 224. 54.64 Zebra-CoT [12] 623.70 231.07 8.95 7.20 21.99 27.74 39.00 21. 22.25 - (-41.6) 24.61 48.88 37.00 29.04 38.12 +71.3 (-) Omni-R1-M 676.67 259.29 11.70 35.60 62.91 45.33 35.61 46.23 +107.8 (+21.3) Omni-R1-L 775.17 279. 11.97 30.90 61.84 50.67 35.25 48.29 +117.0 (+26.7) Omni-R1 Omni-R1-Zero Omni-R1-Zero-T 642.38 277. 17.16 35.08 66.19 36.67 31.35 45.73 +105.5 (+20.0) Omni-R1-Zero-S 699.84 295.71 22.02 37.61 66.06 48.67 33.04 50.19 +125.5 (+31.6) Omni-R1-Zero-M 631.27 232. 14.56 36.96 65.82 47.00 30.41 45.16 +102.9 (+18.4) Table 3: Results on general benchmarks. Bold numbers indicate the best result in each column, underlined numbers indicate the second-best result. Average is computed by first scaling MME-P and MME-R by 0.1 and 0.25, respectively, and then averaging over all seven metrics. Avg (%) is the relative change w.r.t. Anole (Base), and the value in parentheses is computed w.r.t. Zebra-CoT. and BLINK [26]. Detailed dataset descriptions are provided in the Appendix C. Baselines. We compared against two baselines built on the same backbone. Anole [8] is multimodal large language model that supports multimodal interleaved generation. Zebra-CoT [12] is derived from Anole by supervised fine-tuning and represents strong supervision baseline for Omni-R1. Evaluation. For Omni-Bench, we followed the evaluation protocol described in Section 3. For general benchmarks, we conducted evaluation using the widely adopted VLMEvalKit [27]. Unless otherwise specified, we reported accuracy, and for POPE [24], we reported the F1-score. Implementation. For Omni-R1, we report the results of Omni-R1-M and Omni-R1-L, which were trained with 30 and 60 PeRPO steps, respectively. As for Omni-R1-Zero, we report Omni-R1-Zero-T, OmniR1-Zero-S, and Omni-R1-Zero-M trained with 10, 20, and 30 PeRPO steps, respectively. Data usage is de7 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Model Natural Structured Diagrammatic Vision-Op. Avg. Omni-R1-Zero 0.410 w/o PeRPO 0.390 w/o PeReward 0. 0.170 0.130 0.150 0.120 0.075 0. 0.093 0.159 0.033 0.080 0.113 0. Table 4: Ablation study on Omni-Bench for Omni-R1Zero. Results are reported on four Uni-Task slices: Natural (Natural-Scene Perception), Structured (StructuredImage), Diagrammatic (Diagrammatic Math), and VisionOp. (Vision-Operational). Avg. denotes the average over all samples. scribed in Appendix C, and implementation details are provided in the Appendix D."
        },
        {
            "title": "5.2 Main Results",
            "content": "We presented and discussed results on Omni-Bench and general multimodal benchmarks separately. Analysis on Omni-Bench. Table 2 shows that OmniR1 and Omni-R1-Zero outperform the baselines across all four Uni-Task slices. The gains are most pronounced on the Vision-Op. slice, reflecting clear improvement in tasks that require complex visual operations. Within Omni-R1, the Omni-R1-L is particularly strong on the Structured slice and can outperform the corresponding Omni-R1-Zero variants. This indicates the effectiveness of trace supervision in eliciting effective generation. Omni-R1-Zero yields stronger overall gains than Omni-R1 and scales with the RL budget, indicating that PeRPO can successfully induce generative multimodal reasoning. Analysis on general benchmarks. Table 3 shows that both Omni-R1 and Omni-R1-Zero outperform the baselines on standard multimodal benchmarks, indicating the more general reasoning capability beyond Omni-Bench. On perception-oriented evaluations (MME-P, V, MMVP, and BLINK), Omni-R1 variants are generally stronger, reflecting improved fine-grained visual perception with multiple reasoning skills. By contrast, Omni-R1-Zero is more competitive on reasoning-oriented MME-R and also performs strongly on the comprehensive MM-Vet. It remains stronger on POPE, suggesting better evidence consistency and reduced hallucination."
        },
        {
            "title": "5.3 In-depth Analysis",
            "content": "While the main results establish consistent gains, they do not explain which components drive them or how Figure 4: The t-SNE visualization of generated images from Omni-R1 and Omni-R1-Zero, respectively. Filled markers indicate correct predictions and empty markers indicate incorrect predictions. the models behave. We therefore performed ablations and qualitative analyses to provide more insights. Ablation study. We conducted an ablation study on Omni-R1-Zero to disentangle the effects of PeRPO and the perception-calibrated reward, by either removing PeRPO entirely (w/o PeRPO) or disabling the perception reward while keeping PeRPO (w/o PeReward). As shown in Table 4, removing PeRPO results in the largest drop, with the regression concentrated on the Vision-Op. and Diagrammatic tasks. This suggests that PeRPO is crucial for developing multi-step and complex multimodal reasoning. Disabling the perception-calibrated reward yields smaller but consistent degradation, noticeably on Natural and VisionOp. tasks. This suggests that perception-calibrated reward improves visual evidence utilization and stabilizes policy optimization. Analysis on generated multimodal patterns. We studied how Omni-R1 and Omni-R1-Zero differently shape the distribution of generated multimodal patterns and how these patterns relate to reasoning. In Figure 4, Omni-R1 concentrates into few compact modes, whereas Omni-R1-Zero exhibits more dispersed and multi-modal structure. In both projections, correct instances cluster more tightly than incorrect ones, suggesting that trace supervision promotes canonical and stable generations, while reward guidance encourages broader exploration that can still support correct decisions. Case study. To better understand how our model behaves, we presented several examples of the reasoning trajectories produced by our models. Figure 5 shows that Omni-R1 produces task-conditioned in8 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Figure 5: Case studies of Omni-R1s multimodal reasoning skills, including (a) Grounding (Zoom-In) for attribute recognition, (b) Grounding (BBOX) for target localization, and (c) Grounding (BBOX) for parameter localization. (d) Marking for graph connectivity verification and (e) Visual prediction for robotic manipulation sequences. answer. More complete generative multimodal reasoning cases can be found in Appendix E."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "We revisit multimodal reasoning as generative multimodal reasoning, identifying two practical bottlenecks: functional image generation and costly interleaved-modal reasoning annotations. Omni-R1 targets the former, whereas Omni-R1-Zero targets the latter. Empirically, results on both unified and standard evaluations support these choices: Omni-R1 is particularly effective when reliable intermediate visual evidence is crucial, and Omni-R1-Zero still yields consistent gains while producing interpretable intermediate processes despite limited multimodal trace supervision. key direction is to develop scalable supervision signals for generative multimodal reasoning without annotation, thereby unlocking the full potential of zero-shot settings. Figure 6: Omni-R1-Zeros generative reasoning process on commonsense multimodal question. termediate visual evidence throughout generation. This supports our claim that Uni-Skills are flexibly composed within unified generative process. Figure 6 shows that Omni-R1-Zero, despite having no supervised multimodal traces, can still generate intermediate visual evidence before getting the final 9 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning"
        },
        {
            "title": "References",
            "content": "[1] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [2] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. [3] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [4] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [5] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought reasoning. arXiv preprint arXiv:2506.05331, 2025. [6] Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. [7] Chameleon Team. Chameleon: Mixed-modal arXiv preprint early-fusion foundation models. arXiv:2405.09818, 2024. [8] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. [9] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. Kamcot: Knowledge augmented multimodal chain-ofthoughts reasoning. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 1879818806, 2024. [10] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. [11] Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. 10 Interleaved-modal chain-of-thought. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1952019529, 2025. [12] Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. [13] Hao Zhong, Muzhi Zhu, Zongze Du, Zheng Huang, Canyu Zhao, Mingyu Liu, Wen Wang, Hao Chen, and Chunhua Shen. Omni-r1: Reinforcement learning for omnimodal reasoning via two-system collaboration. ArXiv, abs/2505.20256, 2025. URL https://api.semanticscholar. org/CorpusID:278912070. [14] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. URL https://openaccess.thecvf.com/ content/CVPR2024/papers/Wu_V_Guided_ Visual_Search_as_a_Core_Mechanism_in_ Multimodal_CVPR_2024_paper.pdf. [15] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal ArXiv: dataset for improving scientific comprehension of large vision-language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1436914387, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.775. URL https:// aclanthology.org/2024.acl-long.775. [16] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2263 2279, 2022. doi: 10.18653/v1/2022.findings-acl. 177. URL https://aclanthology.org/2022. findings-acl.177/. [17] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Intergps: Interpretable geometry problem solving with In The formal language and symbolic reasoning. 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [18] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning [27] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multimodality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 11198 11201, 2024. [28] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-ofthought. In Proc. of ACL, 2024. [29] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [30] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language In Proceedings of the 2020 Conferprocessing. ence on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/2020.emnlp-demos.6. [31] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. arXiv:2310.02255, 2024. URL https://arxiv. org/abs/2310.02255. ICLR 2024. [19] Xuecheng Wu, Jiaxing Liu, Danlei Huang, Xiaoyu Li, Yifan Wang, Chen Chen, Liya Ma, Xuezhi Cao, and Junxiao Xue. Vic-bench: Benchmarking visualinterleaved chain-of-thought capability in mllms with free-style intermediate state representations. arXiv preprint arXiv:2505.14404, 2025. URL https: //arxiv.org/abs/2505.14404. [20] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llmas-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. URL https:// arxiv.org/abs/2306.05685. [21] Vyas Raina, Adian Liusie, and Mark Gales. Is llm-asa-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7499 7517, 2024. doi: 10.18653/v1/2024.emnlp-main. 427. URL https://aclanthology.org/2024. emnlp-main.427/. [22] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [23] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-vet: Evaluating large multimodal models for integrated capabilities. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5773057754. PMLR, 2024. [24] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large visionlanguage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, 2023. Association for Computational Linguistics. [25] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal LLMs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [26] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, WeiChiu Ma, and Ranjay Krishna. BLINK: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 11 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning"
        },
        {
            "title": "Multimodal Reasoning",
            "content": "A.1 Trajectory Formulation Given multimodal input xM (e.g., an image) and textual input xT (e.g., the question), we model generative multimodal reasoning as an interleaved trajectory of textual rationales and executable visual actions. The trajectory is represented as ( = xM, xT, {(ratT ℓ , aℓ, ratM ℓ )}L ℓ=1, Ans ) , (17) where denotes the number of reasoning steps, ratT ℓ is textual rationale, aℓ is an atomic visual action, is the post-action visual rationale (an image), ratM ℓ and Ans is the final textual answer. Policy and execution. At each step ℓ, the policy πθ produces textual rationale and selects an action: (ratT ℓ , aℓ) πθ ( xT, ratM ℓ1, {(ratT , aj)}j<ℓ ) . (18) The selected action is then executed by renderer/executor that deterministically updates the visual state and produces exactly one post-action image: ratM ( ℓ = ratM ℓ1, aℓ ) , ℓ = 1, . . . , L, (19) with initialization ratM 0 := xM. Finally, the answer is generated conditioned on the full interaction history, e.g., (20) ( Ans πθ xT, ratM , {(ratT ℓ , aℓ)}L ℓ=1 ) . (21) Atomic action space. We use fixed set of atomic visual actions: = {ZOOM-in, BBOX, MARK, LINE, PRED}, (22) whose argument formats and semantics are specified next. A.2 Atomic Action Protocol Details This subsection specifies the atomic visual action vocabulary used in the interleaved trajectories (Eq. 1722). Each action aℓ is applied to the current visual state ratM and yields exactly one postℓ1 action image ratM ℓ via the executor (Eq. 19). 12 Coordinate convention. All spatial arguments are in normalized image coordinates w.r.t. ratM : (0, 0) ℓ1 is the top-left and (1, 1) is the bottom-right corner. For boxes, we use (x, y, w, h) for the top-left corner and width/height in [0, 1]. Out-of-range or degenerate arguments are treated as format errors. by overlaying at the specified region. by cropping the and resizing it to fixed ZOOM-in(x, y, w, h). Produces ratM ℓ specified region from ratM ℓ1 resolution (e.g., 512512). BBOX(x, y, w, h). Produces ratM ℓ bounding box on ratM ℓ1 by rendering viMARK(x, y, id). Produces ratM ℓ sual highlight on ratM to draw attention to target ℓ1 instance or region. The highlight may take various forms (e.g., colored semi-transparent overlay, halo, marker, or contour) anchored at (x, y). Optionally, lightweight identifier id may be included to facilitate references in ratT ℓ LINE(x1, y1, x2, y2). Produces ratM ℓ between two normalized endpoints on ratM ℓ by drawing line . . by applying one-step, PRED(). Produces ratM ℓ task-dependent state transition to ratM , where is ℓ1 serialized delta (text) specifying the update. Note: PRED updates the visual state (the post-action image) rather than directly outputting the final answer. Rule-based verifier for RAcc This appendix summarizes the rule-based verifier used to compute the accuracy reward RAcc . The verifier is deterministic and lightweight: it extracts final answer from the generated trajectory and compares it to the ground truth using small set of ordered checks. The design emphasizes reliability (avoiding false positives) and robustness to superficial formatting variation. Answer extraction and normalization. The verifier first locates dedicated final-answer segment using an explicit marker (e.g., Final Answer:). The extracted string is then lightly normalized to remove non-semantic variation such as whitespace and punctuation differences, and to strip common answerintroduction phrases (e.g., Answer is . . . ). This step improves the stability of the evaluation without relaxing the core matching criteria. Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Hyperparameter Settings. We summarize the hyperparameter settings used in our experiments in Table 6. The configurations are organized into four groups: (i) Objective, (ii) Batching, (iii) Optimization, and (iv) Rollout."
        },
        {
            "title": "E Additional Results",
            "content": "To qualitatively illustrate how Omni-R1 performs generative multimodal reasoning, we present additional full interleaved trajectories in Figures 79. These examples span diverse tasks and capability requirements, showing how the model generates intermediate visual evidence and composes Uni-Skills across different scenarios."
        },
        {
            "title": "F Prompt Templates",
            "content": "To reduce evaluation variance and ensure reproducibility on Omni-Bench, we adopt fixed LLMjudge prompt template, shown in Table 7. The template enforces binary decision using only the ground-truth answer and the model output, and standardizes the handling of common edge cases, including numeric formatting, unit conversion, and multiple-choice variants. Prioritized matching. The verifier applies checks in reliability-first order: (i) numeric matching when both answers can be interpreted as numbers (including common formats such as percentages); (ii) symbolic/mathematical equivalence when the answer contains explicit mathematical expressions; (iii) conservative textual matching for general free-form strings; and (iv) minimal domain-specific handling for small set of high-risk patterns (e.g., multiple-choice labels or chemistry-style strings) to reduce known falsepositive cases. When multiple checks are applicable, the verifier prefers the most reliable one. Scoring. The accuracy reward RAcc [0, 1] reflects the degree of match. Checks based on numeric or symbolic equivalence typically yield binary outcomes, while textual matching may assign partial credit only when the match is sufficiently close under conservais computed tive criteria. The format reward RFmt independently and only indicates whether the trajectory is well-formed and parsable."
        },
        {
            "title": "C Dataset Details",
            "content": "Training data. For Omni-R1, we sample ZebraCoT [12], an 182K image-text interleaved multimodal reasoning dataset, as the supervised data for the PeSFT stage. For Omni-R1-Zero, the PeSFT-stage data is bootstrapped from small text-only CoT subset of M3CoT [28] using the synthetic interleaved construction in Section 4.2, yielding 791 interleaved samples. For the PeRPO stage, we subsample approximately 10% of Zebra-CoT and additionally include 1K ArxivQA [15] and 250 Geometry3K [17] training samples, which do not provide interleaved multimodal reasoning annotations. General benchmarks. We report detailed information on the general benchmarks, including task formulations, dataset sizes, and evaluation metrics. Detailed description is shown in Table 5."
        },
        {
            "title": "D Training Details",
            "content": "Implementation Details. Our implementation is built upon the open-source libraries verl [29], Transformers [30], and trl [31], following their respective licenses. 13 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Benchmark MME-P [22] MME-R [22] MM-Vet [23] V-Bench [14] POPE [24] MMVP [25] BLINK [26] Details Task: Binary visual QA focusing on perception and recognition. Size: 1,536 questions. Metric: MME score for the perception split. Notes: Reported as MME-P in Table 3. Task: Binary visual QA emphasizing reasoning and cognition. Size: 280 questions. Metric: MME score for the cognition split. Notes: Reported as MME-R in Table 3. Task: Open-ended multimodal QA covering wide range of capabilities. Size: 218 examples. Metric: LLM-judge score on 0100 scale. Notes: We use the standard evaluation protocol in VLMEvalKit. Task: Detail-sensitive visual understanding and visual search. Size: 191 images. Metric: Accuracy. Notes: Reported as in Table 3. Task: Yes/No probing for object hallucination. Size: 3,000 questions. Metric: Overall F1 score. Notes: In Table 3, POPE corresponds to Overall F1 rather than accuracy. Task: Diagnostic visual patterns with challenging paired examples. Size: 300 questions. Metric: Accuracy. Notes: Evaluated with the default setup in VLMEvalKit. Task: Multi-choice evaluation of core visual perception skills across diverse sub-tasks. Size: 3,807 questions. Metric: Accuracy. Notes: Evaluated with the default setup in VLMEvalKit. Table 5: Details of general multimodal benchmarks. 14 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Hyperparameter Setting Description Objective KL Loss Coefficient PPO Clip Range (Low) PPO Clip Range (High) Batching Train Batch Size Gen Batch Size PPO Mini-batch Size Optimization Learning Rate Warmup Steps Weight Decay Entropy Coefficient Gradient Clipping Loss Aggregation Rollout Responses per Prompt Temperature Top-p 0.003 0.20 0.28 64 192 16 KL regularization weight lower clipping bound upper clipping bound prompts per update 3 train batch per PPO mini-batch 1 107 5 0.10 0.001 1.0 seq-mean-token-mean peak LR LR warmup L2 regularization entropy bonus weight global-norm clip loss reduction 16 1.0 0.95 samples per prompt sampling temperature nucleus threshold Table 6: Training Hyperparameters. Settings used in our experiments. 15 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Figure 7: Additional case results of Omni-R1. 16 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Figure 8: Additional case results of Omni-R1. 17 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning Figure 9: Additional case results of Omni-R1. 18 Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning System Prompt: You are strict QA judge. Decide correctness by comparing ONLY the ground-truth answer and the model answer. The question may be multiple-choice or open-ended. OUTPUT: Return exactly one token with NO quotes/punctuation/spaces/code fences: True or False. GENERAL RULES: 1) Judge factual/semantic equivalence; ignore phrasing, filler, or reasoning text. If the models final claim contradicts the ground truth or hedges without committing, return False. 2) Numbers: allow formatting differences (1,000 vs 1000), scientific notation, or rounding that preserves the stated value. If units are present, require the same value after conversion; missing/extra incompatible units => False. 3) Lists/sets: require the same items; order doesnt matter. Missing or extra items => False. 4) Spans/names: accept common synonyms and aliases that uniquely indicate the same entity. 5) If ambiguous, empty, multiple conflicting answers, or cannot be judged, return False. SPECIAL RULES FOR MULTIPLE-CHOICE (only when options are provided below): A) Treat option LETTERS and their NUMERIC ORDINALS as equivalent (A=1, B=2, C=3, ...), but ONLY within this questions options. B) Treat the CORRECT OPTIONS FULL TEXT as equivalent to its letter and numeric index. Your final output must be only the single token: True or False. User Prompt: ### Ground Truth {Ground Truth} ### Model Answer {Model Answer} ### Decision Return only one of: True, False. Table 7: Template for Omni-Benchs GPT Judge."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology (Shenzhen)",
        "Shandong University",
        "Singapore Management University",
        "The Hong Kong Polytechnic University",
        "University of Science and Technology of China"
    ]
}