{
    "paper_title": "WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue",
    "authors": [
        "Yaoyao Qian",
        "Jindan Huang",
        "Yuanli Wang",
        "Simon Yu",
        "Kyrie Zhixuan Zhou",
        "Jiayuan Mao",
        "Mingfu Liang",
        "Hanhan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 1 8 8 1 0 . 6 0 5 2 : r WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue Yaoyao Qian*1 Jindan Huang2 Yuanli Wang3 Simon Yu1 Kyrie Zhixuan Zhou4 Jiayuan Mao5 Mingfu Liang6 Hanhan Zhou7 1Northeastern University, Boston, MA 2Tufts University, Medford, MA 3Boston University, Boston, MA 4University of Texas at San Antonio, San Antonio, TX 5Massachusetts Institute of Technology, Cambridge, MA 6Northwestern University, Evanston, IL 7George Washington University, Washington, DC (cid:128) Project Website ı Dataset Code Visualization Dashboard"
        },
        {
            "title": "Abstract",
            "content": "Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (4060%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of language models has created fundamental challenge in human-AI interaction: the \"gulf of envisioning\"users cognitive difficulty in formulating effective prompts. Unlike conventional interfaces with predictable affordances, language models require users to simultaneously envision possibilities and their expressions, often leading to communication breakdowns. This challenge arises from misalignment between human cognitive processes and the way systems interpret user intent. Subramonyam et al. [1] illustrate that human intent formation involves maturation process characterized by progressive constraint resolution, fluctuating stability intervals, and distinct structural signaling patterns. However, current evaluation methods are insufficient as they: 1) treat *Corresponding author: # qian.ya@northeastern.edu Preprint. Under review. Figure 1: Overview of the STORM Framework intent as binary rather than continuous, 2) lack frameworks for temporal coherence, and 3) overlook structural signals within expressions. These structural signalsincluding stylistic choices, implicit assumptions, and cultural markers reflect what Wittgenstein [2] termed the contextual embeddedness of meaning within particular forms of life. Current systems cannot access these embedded contextual cues that users unconsciously include in their expressions. These shortcomings constitute the Intent-Action Alignment Problem, determining precisely when user expressions have reached cognitive readiness for effective system action. To address this alignment problem, we propose STORM (Structured Task-Oriented Representation Model), framework that builds on top of CAMEL-AI [3] and conceptualizes user intent as evolving along continuous spectrum, modeling the iterative refinement of user expressions, and identifying timing misalignments that lead to communication failures. The major contributions of this paper include: (1) dialogue generation pipeline using two language modelsUserLLM and AgentLLMto simulate realistic conversations reflecting diverse user profiles and intent progression. UserLLM generates user behavior conditioned on comprehensive profile data and internal states, simulating authentic intent evolution, while AgentLLM responds based solely on observable dialogue history. This asymmetric setup mirrors the realistic information gaps faced by AI systems, allowing targeted studies on agent adaptability to evolving intent. (2) database-driven memory system that systematically tracks evolving user states (intent, emotion, satisfaction) within session-specific records. These records function as micro-databases documenting real-time intent maturation trajectories and are integrated into global database for cross-session analysis. This structured memory approach captures the continuous nature of intent development, providing researchers with detailed, fine-grained data to study patterns across diverse interaction contexts. (3) web-based dialogue visualization interface equipped with clarity rating mechanism was developed to provide an intuitive analysis of the evolution of user intent. This interface dynamically displays the refinement process of user intent, enabling researchers to assess the effectiveness of various agent response strategies visually. The tool facilitates rigorous quantitative analysis and comparison by quantifying the abstract cognitive progression into standardized clarity metric. The interface is publicly accessible at https://v0-dialogue-analysis-dashboard.vercel.app/. The dialogues produced by STORM serve as valuable training data, enabling conversational agents to better detect and adapt to different stages of intent formation. We evaluate our framework through comparative analysis of agent responses across metrics, including user satisfaction and response quality, demonstrating improved alignment with user cognitive processes. Our experiments demonstrate that access to user profiles significantly enhances model performance across all evaluated systems, with satisfaction scores increasing by 1540% when profile information is 2 Table 1: Summary of notations used in STORM INTERFACE. Notation User Expression Agent Response Hidden State Task Domain User Domain Expression Domain Response Domain Symbol et rt ht τ et rt Description User utterance at dialogue turn Agent utterance at dialogue turn Users internal state at turn (inner thoughts, emotion, satisfaction) Space of tasks from the Task Library (technology, healthcare, etc.) Space of user profiles with their multi-dimensional attributes Space of user utterances with varying degrees of clarity Space of agent responses to user expressions = {b1, ..., bn} Base Profile [task-agnostic] = {t1, ..., tm} Task Parameters Context Profile [task-agnostic] = {c1, ..., ck} Task Specifics Difficulty Config Uncertainty Level = {dstyle, dlength, dcontent, dtone} {0%, 40%, 60%, 80%} Demographic and personality factors (culture, decision style, etc.) Task-specific attributes (domain, brand, priority features, etc.) User capabilities and constraints (time constraint, patience, etc.) Predefined user preferences and constraints for task instance τ Difficulty level and associated dimensions Percentage of profile attributes masked as unknown Agent Role Agent Directive Intent Evolution Clarity Rating Performance Score t s UserLLM Function AgentLLM Function Basic Augmentation Turn Analysis Summary Generation RAG Enhancement Prompt Refinement α(τ ) δ(τ ) t(h) C(rt, ht, ht+1) E(C1, ..., CT ) general helpful assistant for task τ Task-specific guidelines instructing agent behavior and goals Change in intent clarity from turn 1 to based on hidden states Measurement of how agent response improves intent clarity Aggregate measure of agent effectiveness across dialogue turns Guser(u, H1:t1, E1:t1, R1:t1) (et, ht) User generation with full dialogue history Gagent(α(τ ), δ(τ ), E1:t, R1:t1) rt A1(E1:T , R1:T , H1:T , u) A2(D) D+ A3(D+) R(D, D+, S) (D, D+, S) Agent generation with role, directive, and observable history Collection of dialogues with complete metadata Enhanced data with per-turn analysis Comprehensive dialogue summaries Using augmented data as knowledge base Creating improved prompts from analysis agent a e e fi r t A c n a e available. We introduce novel Clarify metric that measures how effectively agents help users internally clarify their own intentions assessed through analysis of simulated user inner thoughts rather than external expressions. This approach captures whether agent responses genuinely improve users understanding of their own needs, crucial cognitive process often invisible in traditional dialogue evaluations. Our analysis reveals distinct model characteristics with practical deployment implications: Claude maintains consistent satisfaction across varying profile completeness, Gemini demonstrates robust performance under high uncertainty, while Llama achieves superior intent clarification despite satisfaction trade-offs. Notably, we observe that moderate profile uncertainty (4060% unknown attributes) often outperforms complete information access, suggesting that excessive profile information may lead to presumptive reasoning, while moderate uncertainty encourages more exploratory interaction strategies that better support users evolving understanding of their own needs. This finding has implications for privacy-preserving design and bias mitigation in dialogue systems. These insights highlight the tension between immediate satisfaction and cognitive alignment, providing empirical guidance for uncertainty-aware dialogue system design."
        },
        {
            "title": "2 Core Components",
            "content": "We define the STORM (Structured Task-Oriented Representation Model) Interface as formal framework for studying the relationship between user intent expression and system actionability. The STORM Interface is represented as 5-tuple of domain spaces: {T , U, E, R, H}. We define each component in detail as follows. Task domain is defined as collection of task objects τ , where each τ comprises task name, description, and domain-specific requirements. Prior approaches [4, 5] constrain task definitions to those with explicit success metrics, whereas our formalization generalizes across task types. The task representation of our framework is domain-agnostic, enabling automated attribute generation for arbitrary domains beyond our experimental setup. The implementation accepts custom task definitions through standardized interface that integrates with existing domain taxonomies and classification systems. 3 User domain consists of user profiles U, where each profile is represented as vector of attribute-value pairs. These captures both task-agnostic characteristics such as demographics and task-specific attributes such as budget constraints. Modeling user profiles is essential for creating adaptive human-agent interaction scenarios, allowing systems to reason about user variability and tailor responses accordingly [6]. To support system interoperability and practical deployment, we structure user profiles using schema-compatible format that facilitates direct integration with existing user databases via JSON exchange formats. Expression domain encompasses all possible user expressions e. To reflect the realities of natural human communication, we also model variation in expression clarity through four dimensions: style, length, content and tone. This addresses limitations in existing models that assume unambiguous and complete intent expressions. The variation is operationalized through configurable difficulty levels during user profile generation (see section 2.1 for details). Our framework supports both integration of real-world interaction corpora and generation of synthetic expressions to enhance data diversity. Response domain contains all possible agent responses R, which can be clarification queries, option suggestions, or action executions. At time t, the response rt is generated based on information from task object τ and past user-agent dialogues {(e1, r1), ..., (et1, rt1)}. Hidden state domain denotes the space of latent user states that evolve dynamically over the course of dialogue. At each timestep t, the hidden state ht is represented as composite vector encoding both user intent and emotional state. This modeling approach serves multiple purposes: it enables contextualized interpretation of user actions, supports dynamic adaptation of agent responses, and facilitates diagnosis of failure points in communication."
        },
        {
            "title": "2.1 STORM User Model Formalization",
            "content": "We formalize the STORM user profile as composite structure, organized into three categories that together capture the complexity of human-agent interaction: task-agnostic attributes, task-specific attributes, and communicative parameters. This composite approach addresses the limitations of monolithic user models by providing transparent, controllable representation that enables systematic analysis of how different user characteristics influence interaction patterns. By isolating individual variables within this parameterized framework, researchers can identify which specific user attributes most significantly impact behavior in different contexts, facilitating the development of targeted strategies for various application scenarios. 1. Task-agnostic Components The base profile consists of parameters representing demographic and personality characteristics. These parameters include age group (1825, 2640, 4165, 65+), technical experience (15 scale), language expression style (e.g., concise, detailed, technical, non-technical), personality traits (derived from the Big Five model dimensions [7]), and cultural background. We choose to include these factors based on empirical evidence from human-computer interaction studies [1] showing their significant impact on expression patterns and intent formulation. To ensure unbiased representation, these profile attributes are randomly generated, creating diverse user populations that better reflect real-world interaction scenarios. The context profile models users environmental and cognitive constraints. This includes general influencing factors such as patience level (on scale from 1 to 5), social pressure, time constraints, and other subjective elements. By introducing these variable factors, our framework simulates the unpredictability of real-world interaction environments. The explicit modeling of contextual factors addresses significant gap in existing frameworks that typically assume ideal interaction environments, allowing STORM to model challenging scenarios where external factors directly impact communication quality. 2. Task-dependent Components Task instance τ specifies particular task from the task library , such as \"create an online password,\" \"book flight,\" or \"configure network settings.\" We deliberately implement these as high-level descriptions rather than precise execution specifications, recognizing the significant gap between how users conceptualize tasks and the actual execution intent. This design choice more accurately reflects the abstraction level at which most users operate when formulating requests, requiring systems to bridge the conceptual gap between description and execution. 4 Task specifics capture user-defined preferences and situational constraints within the selected task τ . These encompass domain classification (technology, finance, healthcare, etc.), priority functional requirements (represented as weighted importance lists), brand preferences, budget constraints, and time urgency indicators. These parameters are generated using LLM (GPT-4o Mini) with randomly selected options to eliminate potential biases in task representations. This systematic approach ensures balanced coverage across task types. 3. Communication Modeling Components To more accurately reflect how people naturally communicate, STORM models key sources of ambiguity and variability through expression difficulty and uncertainty. Difficulty configuration = {dstyle, dlength, dcontent, dtone} models variation in user expression across 4 linguistic dimensions: represents one of STORMs core innovations, characterizing expression clarity through multiple dimensions. The difficulty level {1, . . . , 5} ranges from precise to highly ambiguous across five levels. This multidimensional approach reflects critical insight from real-world interactions: the vast majority of users cannot articulate their needs with the precision that current systems often expect. By modeling various dimensions of communication difficulty, STORM creates more realistic scenarios that challenge systems to handle the imprecise, inconsistent, and incomplete expressions typical in everyday interactions. Detailed breakdown of each dimension is in Appendix A. Uncertainty level {0%, 40%, 60%, 80%} controls the proportion of unknown or unspecified user attributes. It ranges from 0% (fully known) to 80% (high uncertainty). This parameter is designed to simulate one of the most fundamental challenges in intent modeling: users often cannot articulate requirements they themselves do not fully understand. In real-world interactions, many users lack conceptual understanding of their own needs or the relevant domain, requiring systems to provide additional explanation, guidance, and progressive clarification. The higher uncertainty levels (60%, 80%) simulate scenarios where users are in an exploratory mode, possessing only vague notions of their goals and requiring substantial guidance from the agent to refine and articulate their actual needs. This approach provides more realistic simulation framework compared to models that assume users have perfect knowledge of their requirements and preferences, enabling the development of systems that can effectively guide users through the process of need discovery and formulation."
        },
        {
            "title": "2.2 Agent Model and Dialogue Process",
            "content": "We formalize the STORM agent configuration as structured framework that enables interactive systems to adapt their behavior based on specific task contexts. This parameterized approach facilitates systematic analysis of different agent strategies and their impact on dialogue effectiveness. The user LLM function Guser(u, H1:t1, E1:t1, R1:t1) (et, ht) generates both user expressions and corresponding hidden states. This function employs pre-trained language models prompted to specific user profiles, taking as input the complete user profile u, previous hidden states H1:t1, user expression history E1:t1, and agent response history R1:t1. Through multi-step process, it first determines the expressions difficulty level based on the profile and dialogue history, then generates user expression et representing the users input at turn t, with properties determined by the user profile parameters and current hidden state. These expressions are subject to defined character limitations and reflect varying degrees of clarity based on the users profile characteristics. Simultaneously, the function produces the user hidden state ht modeling internal user states not explicitly expressed at turn t, formalized as vector ht = st, ct, it, et where each component represents satisfaction, intent clarity, and emotional state, respectively. This explicit modeling of hidden states addresses critical limitation in existing frameworks that neglect the internal user experience. The agent LLM function Gagent(α(τ ), δ(τ ), E1:t, R1:t1) rt produces agent responses using pretrained language models. The function incorporates the agent role α(τ ) A, which is standardized as general helpful assistant to ensure experimental fairness across different interaction scenarios. Operating under realistic constraints, the agent function lacks access to user hidden states and therefore requires intent inference from observable behavior only. By processing the agent role α(τ ), agent instructions δ(τ ), user expression history E1:t, and previous agent responses R1:t1, it generates the agent response rt constituting the systems output at turn based on the dialogue 5 history. The agent follows standardized approach across different task contexts, providing adaptable responses through intent recognition, clarity assessment, and strategy selection mechanisms without requiring specialized task-specific instruction sets. This design reflects an intentional asymmetry between user and agent, where the agent relies solely on observable behaviors to infer user intent, without direct access to internal cognitive states. This representation of dialogue as temporal sequence of generated expressions, responses, and evolving hidden states allows us to define three primary evaluation metrics that quantify dialogue effectiveness. First, intent evolution t(h) = ht.clarity ht1.clarity measures the change in intent clarity between consecutive turns. This differential metric is calculated through round-byround analysis of the generated inner thoughts, providing insight into how specific agent responses influence users understanding of their own needs. Building on this, the clarity score C(rt, ht, ht+1) evaluates response effectiveness in improving intent clarity. It is computed as weighted function = w1t(h) + w2t(s) + w3gt where t(s) represents satisfaction change and gt measures progress toward goal achievement. The scoring components are derived from both turn-level analysis and summary analysis of the interaction trajectory. Finally, the performance score E(C1, . . . , CT ) delivers an aggregate assessment of agent effectiveness across the complete dialogue. The score combines average clarity, turn efficiency, and final satisfaction into standardized metric for comparative analysis. This unified measure facilitates systematic comparison across different agent strategies, enabling empirical identification of optimal approaches for specific user profiles and task types. By maintaining this structured evaluation framework across experiments, STORM provides standardized methodology for assessing and improving assistant performance across diverse interaction scenarios, particularly focusing on how different interaction patterns address various types of expression ambiguity."
        },
        {
            "title": "2.3 How do we use these data?",
            "content": "STORM implements structured framework for generating realistic dialogues and extracting actionable insights. At its core, the system operates as closed-loop that enhances agent capabilities through complementary pathways. The process begins with comprehensive user profile generationcombining diverse tasks with multidimensional user attributes, contextual constraints, difficulty parameters, and uncertainty levels to create realistic simulation scenarios. These profiles drive the dialogue generation process, where user and agent LLM functions interact to produce conversations with corresponding hidden states, enabling analysis of both observable exchanges and underlying intent evolution patterns. The first improvement dimension focuses on progressively enhancing dialogue data for retrievalaugmented generation by leveraging large language models as intelligent evaluators and annotators. This multi-layered enhancement pipeline starts with the basic enhancement function A1(E1:T , R1:T , H1:T , u) which uses pre-trained LLMs prompted with user profiles, expression difficulty, intent clarity, and satisfaction indicators to produce enriched dialogue annotations. Subsequently, the dialogues undergo turn-level analysis A2(D) D+ where LLM-based classifiers identify key inflection points, dialogue strategies, and intent evolution trajectories. This is followed by summary generation A3(D+) where LLMs create abstracted summaries that highlight success and failure patterns. The enhanced and summarized dialogues feed into the RAG enhancement function R(D, D+, S) which constructs structured knowledge base through vector embeddings, enabling similarity-based retrieval conditioned on user profiles and dialogue characteristics. The second improvement dimension exploits these LLM-generated insights to optimize agent prompts. Through systematic analysis of enriched dialogues and summaries, LLMs identify effective agent strategies and response patterns tailored to different user profiles and expression difficulties. These findings are formalized into the prompt optimization function (D, D+, S) agent 6 which updates the agent LLM function by incorporating the discovered response patterns. STORMs architecture integrates two complementary components: user simulator generating expressions across varying difficulty and uncertainty states, and an agent response generator leveraging both retrieval-augmented knowledge and optimized prompts. Rather than forming direct closed-loop training system, these modules serve as reference and analytical tools to uncover deeper insights. Our implementation adopts two-phase approach: first creating diverse dataset of synthetic profiles and expressions, then using these data to guide the discovery of patterns and optimization strategies for agent models. This process supports informed improvements that enhance performance across diverse interaction scenarios."
        },
        {
            "title": "3.1 Evaluation",
            "content": "Our evaluation employs simulation-based approach where GPT-4o-mini functions as UserLLM, generating both external utterances and internal inner thoughts during interactions with different assistant models (Claude, GPT, Gemini, and Llama). This setup models an asymmetric information dynamic: users have full access to their internal states and profiles, while agents must infer user intent solely from observable dialogue history, reflecting real-world challenges in intent understanding. The dataset of 4,800 dialogues, spanning 600 unique user profiles, is generated through this simulation framework by conditioning UserLLM on detailed user profiles and evolving internal states. UserLLM produces naturalistic utterances alongside corresponding latent states such as satisfaction and intent clarity, enabling fine-grained measurement of internal cognitive signals. While the current dataset serves as representative sample illustrating the effectiveness and versatility of the framework, the underlying architecture is designed to support scalable generation of extensive, diverse dialogue corpora across varied user demographics and task domains. This capacity facilitates comprehensive data-driven analysis and continuous model improvement beyond the examples presented here. We evaluate model performance along three complementary dimensions: (1) satisfaction derived from user inner thoughts, capturing the users internal contentment; (2) clarification effectiveness, measured by the Clarify metric, which is computed via prompting an evaluation model to analyze the dialogue turn-by-turn and determine whether each agent response improves the clarity of the users intent relative to the previous turn; and (3) Satisfaction-Seeking Actions (SSA), composite metric that integrates satisfaction and clarification scores weighted by scenario-specific parameters to balance the competing objectives of confident response generation and appropriate clarification seeking. The SSA metric corresponds to the aggregate performance score E(C1, . . . , CT ) across dialogue turns, enabling holistic and context-sensitive assessment of dialogue quality. At dialogue start, user satisfaction is initialized to neutral baseline of 0.5. Satisfaction is assessed using several detailed metrics: Final Satisfaction measures user satisfaction at dialogue conclusion on scale from 0.0 (completely unsatisfied) to 1.0 (fully satisfied). Average Satisfaction reports the mean final satisfaction across all dialogues. Satisfaction Trend reflects the change in satisfaction from the initial baseline to dialogue end, indicating improvement or decline. High Satisfaction Rate indicates the proportion of dialogues where final satisfaction meets or exceeds threshold of 0.8, marking successful interactions. Lastly, Improved Satisfaction Rate quantifies the percentage of dialogues with increased satisfaction compared to the start, highlighting effective clarification and positive user experience changes. Together with the Clarify and SSA scores, these metrics provide comprehensive and nuanced evaluation of model behavior across diverse interaction scenarios."
        },
        {
            "title": "3.2 Results",
            "content": "Table 2 presents performance data across models, uncertainty levels, and profile conditions, revealing patterns in how language models balance satisfaction and clarification. The satisfaction metrics demonstrate clear benefits from user profile access. With profiles, models maintain average satisfaction scores of 0.850.92, while without profiles, scores frequently fall below 0.75, with Llama reaching as low as 0.67 (at 40% uncertainty). This differential highlights the value of personalization in dialogue systems. 7 Table 2: User Satisfaction and Clarification Performance across UserLLMs with Varying Uncertainty Levels UserLLM (Uncertainty) Average Satisfaction High Satisfaction Rate Improved Satisfaction Rate Score Satisfaction Metrics Clarify SSA Score w/Profile w/o Profile w/Profile w/o Profile w/Profile w/o Profile w/o Profile w/o Profile Claude-3.7-Sonnet (0%) Claude-3.7-Sonnet (40%) Claude-3.7-Sonnet (60%) Claude-3.7-Sonnet (80%) GPT-4o-mini (0%) GPT-4o-mini (40%) GPT-4o-mini (60%) GPT-4o-mini (80%) Gemini 2.5 Flash Preview (0%) Gemini 2.5 Flash Preview (40%) Gemini 2.5 Flash Preview (60%) Gemini 2.5 Flash Preview (80%) Llama 3.3 70B Instruct (0%) Llama 3.3 70B Instruct (40%) Llama 3.3 70B Instruct (60%) Llama 3.3 70B Instruct (80%) 0.91 0.92 0.88 0.91 0.89 0.89 0.89 0.87 0.89 0.89 0.91 0.90 0.89 0.90 0.88 0.85 0.83 0.78 0.92 0.80 0.75 0.75 0.77 0. 0.74 0.74 0.75 0.79 0.70 0.67 0.71 0.76 86.0% 86.0% 80.7% 86.0% 82.0% 82.7% 84.0% 79.3% 84.7% 81.3% 88.0% 84.7% 83.3% 86.0% 81.3% 74.0% 72.0% 62.7% 86.7% 65.3% 54.0% 57.3% 62.7% 64.0% 51.3% 52.7% 56.7% 64.7% 48.0% 45.3% 44.7% 61.3% 89.3% 90.0% 86.0% 90.0% 87.3% 86.0% 86.7% 83.3% 89.3% 89.3% 92.0% 92.7% 90.0% 90.0% 92.0% 88.7% 75.3% 62.7% 88.7% 71.3% 58.7% 63.3% 67.3% 68.7% 62.0% 61.3% 66.0% 70.0% 61.3% 56.0% 66.7% 72.7% 5.23 4.80 4.66 4.70 5.97 5.84 5.69 5.30 6.83 6.55 6.50 6.45 7.58 7.59 7.58 7.75 6.07 5.67 6.39 6.36 5.86 5.82 5.88 5. 6.06 5.98 6.02 6.22 6.07 5.91 6.12 6.45 notable exception is Claudes performance at 60% uncertainty without profiles, achieving 0.92 satisfactionhigher than its profile-informed score (0.88). This counter-intuitive result suggests that certain uncertainty levels may activate beneficial reasoning pathways. Analysis of user inner thoughts reveals that under moderate uncertainty, Claudes responses trigger 18% more improvements in users internal clarity compared to 0% uncertainty. Claude appears to adopt more balanced approach at this uncertainty level, helping users refine their own intentions effectively despite lacking profile information. The high satisfaction rate metrics confirm these observations. With profiles, models maintain rates above 80% across uncertainty levels. Without profiles, these rates decline substantially, most dramatically for Llama (dropping to 44.7% at 60% uncertainty). This pattern reveals significant differences in how models adapt to missing user context, with some architectures showing more resilience than others when personalization data is unavailable."
        },
        {
            "title": "4 User Satisfaction and Profile Integration Effects",
            "content": "User profiles consistently boost satisfaction across AI models, but moderate uncertainty without profile data can paradoxically trigger more effective reasoning patterns. User profiles enhance satisfaction across all models (0.850.92 with profiles vs. 0.670.83 without), yet our analysis reveals critical distinction between external compliance and internal understanding. Claude at 60% uncertainty without profiles achieves 0.92 satisfactionexceeding its profile-informed score (0.88), suggesting moderate uncertainty may trigger more effective reasoning patterns in some architectures. Analysis of user inner thoughts reveals Claudes responses at this uncertainty level produce 18% more improvements in users internal clarity compared to 0% uncertainty. We hypothesize that without profile information, Claude adopts more balanced strategy between confident answering and clarification seeking, which better supports users own cognitive process of intent refinement. Traditional satisfaction metrics fail to capture the critical divergence between users expressed satisfaction and their internal confusion about their own needs. Users may express satisfaction with system responses while their inner thoughts indicate continued confusion about their own needs, highlighting the limitations of traditional evaluation metrics that rely solely on observable user feedback. This internal-external divergence varies significantly across domains: technology tasks promote rapid self-understanding and confident decision-making, medical scenarios require cautious, trust-building interactions with gradual clarity development, while housing decisions involve prolonged uncertainty and multiple stakeholder considerations. Profile completeness creates paradox where excessive personalization data can reduce interaction quality by promoting stereotypical responses. High satisfaction rates follow similar patterns, with profile-informed conditions maintaining 8088% rates while no-profile conditions show significant drops, particularly for Llama (81.3% 44.7% at 60% uncertainty), indicating varying resilience to missing personalization data. Without profiles, models resort to generic information8 gathering rather than task-specific assistance, but excessive profile completeness can paradoxically reduce interaction quality by promoting stereotypical responses. This finding challenges conventional approaches to personalization and suggests that optimal human-AI collaboration requires calibrated information asymmetry rather than transparency maximization."
        },
        {
            "title": "5 Clarification Performance and Bias Mitigation",
            "content": "AI models exhibit fundamentally different architectural approaches to balancing response confidence versus ambiguity recognition, with distinct trade-offs for user outcomes. Models exhibit distinct clarification strategies, revealed through analysis of user inner thoughts after agent responses. Claude (4.665.23) and GPT (5.305.97) show declining clarification effectiveness as uncertainty increases, suggesting these models prioritize providing confident responses even when uncertainty rises. Gemini maintains more consistent clarification scores (6.456.83) across uncertainty levels, indicating more robust approach to disambiguation regardless of uncertainty conditions. Most notably, Llama achieves substantially higher clarification scores (7.587.75) across all configurations despite lower satisfaction in some conditions. The clarification-satisfaction trade-off represents critical design choice, with Claude optimized for immediate satisfaction while Llama emphasizes long-term intent disambiguation. These patterns reveal fundamental architectural differences in how models balance response confidence versus ambiguity recognition. Claude appears optimized for satisfaction even at the cost of clarification opportunities, while Llamas architecture seems to emphasize identifying and addressing ambiguity, sometimes trading immediate satisfaction for more effective intent disambiguation. This clarification-satisfaction trade-off represents critical design consideration for dialogue systems, with different models offering distinct advantages depending on whether the priority is immediate user satisfaction or long-term intent clarity. Strategic information limitation serves as an implicit bias mitigation mechanism, preventing systems from relying on demographic generalizations. These architectural differences manifest in distinct reasoning patterns when handling demographic information. Analysis of interactions involving elderly users reveals that complete profile access can lead to stereotypical assumptionssystems may assume simplified instructions are needed based on age markers alone. However, at optimal uncertainty levels, the same systems engage in individualized assessment, often discovering more sophisticated capabilities than demographic profiles would suggest. This pattern suggests that strategic information limitation serves as an implicit bias mitigation mechanism, forcing systems to evaluate individual user responses rather than relying on demographic generalizations. Successful clarification correlates more strongly with users internal cognitive improvement than with expressed satisfaction scores, suggesting deeper measures of dialogue effectiveness. Our analysis shows that successful clarification correlates more strongly with internal cognitive improvement than with external satisfaction scores. Users who achieve better self-understanding through interactionas measured by clearer, more confident inner thoughtsdemonstrate sustained engagement and more effective task completion, even when immediate satisfaction scores remain moderate. This finding suggests that dialogue systems optimized solely for satisfaction may miss opportunities for deeper cognitive alignment that benefit long-term user outcomes."
        },
        {
            "title": "5.1 Satisfaction-Seeking Actions (SSA) Integration",
            "content": "We designed the SSA metric to address two fundamental limitations in dialogue evaluation: optimizing for satisfaction alone neglects critical clarification capabilities, while traditional metrics fail to capture the comprehensive reasoning processes activated by moderate uncertainty levels (4060%). The integrated metric balances immediate user satisfaction with long-term cognitive alignment through weighted combination: SSA = wα (Savg λ) + wβ Cclarify where Savg represents the average satisfaction score across dialogue turns, Cclarify denotes the clarification effectiveness score computed via turn-by-turn analysis of intent improvement, and wα = 0.7, wβ = 0.3 represent the relative importance weights with wα + wβ = 1. The satisfaction component receives higher weighting based on the practical consideration that user experience remains paramount 9 in deployment scenarios, while the clarification component ensures that cognitive alignment capabilities are not overlooked in system evaluation. The normalization factor λ = 7.75 scales satisfaction scores (range 0.01.0) to match the magnitude of clarification scores (range 4.08.0), where λ corresponds to the maximum observed clarification score in our dataset of 4,800 dialogues. This scaling ensures balanced contribution from both components in the integrated assessment, preventing either dimension from dominating the composite score. This integrated assessment reveals model-specific optimization patterns and establishes performance hierarchy (Llama > Gemini > GPT > Claude) that substantially diverges from satisfaction-only rankings. The metric captures distinct architectural characteristics: Claude achieves peak SSA performance at moderate uncertainty (40%) through satisfaction optimization strategies, GPT maintains consistent performance across uncertainty levels, Gemini demonstrates superiority at higher uncertainty (60%) via robust ambiguity handling mechanisms, and Llama attains the highest overall scores by prioritizing clarification effectiveness despite satisfaction trade-offs in certain configurations. The divergence between SSA rankings and traditional satisfaction metrics validates our design rationale: GPT-4o-mini achieves only mid-range SSA scores as an agent despite serving effectively as UserLLM in our simulation framework, illustrating the fundamental distinction between simulating authentic user behavior and responding optimally to user needs. This confirms that comprehensive dialogue evaluation requires balancing multiple performance dimensions rather than optimizing for satisfaction alone."
        },
        {
            "title": "5.2 Practical Implications and Strategic Deployment",
            "content": "Our analysis reveals systematic differences in optimal uncertainty levels across task domains, challenging the assumption that uniform uncertainty thresholds apply across scenarios. Technology-oriented tasks (e.g., password reset, device setup) achieve peak performance at lower uncertainty levels (40%), requiring direct, efficient guidance. Medical scenarios (appointment scheduling, caregiver selection) demonstrate optimal performance at moderate uncertainty (60%), reflecting the cautious, trustbuilding nature of healthcare interactions. Housing-related tasks (accessibility modifications, rental searches) show continued improvement even at higher uncertainty levels (60-80%), corresponding to their complex, multi-stakeholder decision processes. This domain-uncertainty relationship correlates with user cognitive load and decision complexity, with temporal dynamics varying significantly: technology scenarios show rapid convergence between inner thoughts and external expressions, while medical and housing scenarios maintain longer periods of internal uncertainty despite external cooperation. These patterns inform the design of patience-aware dialogue systems that can recognize when users need additional processing time versus immediate response. Our analysis yields five key insights for dialogue system deployment: (1) Domain-adaptive uncertainty calibrationtechnology tasks require 40% uncertainty, medical scenarios 60%, housing scenarios 60-80%outperforming uniform thresholds; (2) Model-specific optimizationClaude performs best at 40% uncertainty, Gemini at 60%, Llama shows continued improvement at higher uncertainty levels; (3) Progressive profile building during conversations significantly enhances performance, especially for profile-sensitive models like Llama; (4) Context-aware model selectionClaude offers stability across uncertainty conditions, Gemini excels with incomplete information, and Llama provides superior disambiguation; (5) Bias mitigation through calibrated uncertaintymoderate profile incompleteness (40-60%) can improve interaction quality by reducing reliance on demographic assumptions and encouraging individualized exploration. These findings advocate for context-aware deployment approaches that balance satisfaction with effective clarification strategies. Systems with complete user profiles may engage in presumptive reasoning based on age, cultural markers, or historical patterns, while moderate uncertainty encourages assumption-free communication. This has direct implications for user privacy controls and profile management, where strategic information limitation may enhance rather than degrade user experience by promoting personalized support without stereotypical generalizations."
        },
        {
            "title": "6 Related Work",
            "content": "Our work connects linguistic theory with recent advances in LLM dialogue systems through three research streams: Theoretical Foundations Linguistic research on discourse cohesion [8], referential underspecification [9], and speech act theory [10] established frameworks for analyzing communication intent, while work on epistemic modality [11] and conversational repair [12] identified uncertainty markers. STORM operationalizes these insights by formalizing difficulty dimensions reflecting cognitive readiness signals. Dialogue Systems and Uncertainty Mixed-initiative dialogue research [13, 14] developed computational approaches to conversational grounding, with recent work examining how language models handle uncertaintyrevealing hallucination under ambiguity [15, 16] and advancing methods for managing unclear expressions [17, 18, 19]. STORM extends these approaches with structured framework for assessing expression stability across multiple dimensions. User Variation and Intent Formation Studies on cultural sensitivity in language models [20, 21] have highlighted the importance of user variation, while recent work identified the \"gulf of envisioning\" [1]users difficulty formulating effective prompts. STORM addresses this challenge by modeling expression clarity through formal representation of user profiles, difficulty configurations, and uncertainty levels, integrating aspects of the intent-action alignment problem previously examined only in isolation."
        },
        {
            "title": "7 Conclusions and Future Directions",
            "content": "STORM provides framework for modeling intent triggerability in task-oriented dialogues, revealing how model performance varies with profile availability and uncertainty calibration. Claude offers consistent satisfaction, Gemini excels with incomplete profiles, and Llama provides superior disambiguation. Notably, moderate uncertainty (40-60%) sometimes outperforms minimal uncertainty, suggesting that appropriate caution activates more effective reasoning. The frameworks key strength lies in its extensibilityits modular design accommodates additional models and domains, providing consistent methodology for cross-model comparison. Future work should explore longer interactions, refine turn management, and investigate real-world deployment scenarios. STORMs architecture supports ongoing research and development of dialogue systems that better align with the dynamic nature of human intent formation."
        },
        {
            "title": "Limitation",
            "content": "There are several limitations in our study. The 15-turn dialogue constraint may not fully capture extended intent evolution processes, potentially missing patterns that emerge in longer interactions. Additionally, post-task gratitude exchanges with static satisfaction levels artificially extend conversations and potentially dilute measurable differences between model configurations. Our primarily quantitative approach also limits deeper insights into conversation nuances, such as how user expectations evolve with changing goal uncertainty. Future work should address these limitations through extended dialogues, refined completion detection, and mixed-methods analysis to better understand the dynamic nature of intent formation across diverse interaction contexts."
        },
        {
            "title": "References",
            "content": "[1] Hariharan Subramonyam, Roy Pea, Christopher Lawrence Pondoc, Maneesh Agrawala, and Colleen Seifert. Bridging the gulf of envisioning: Cognitive design challenges in llm interfaces. arXiv preprint arXiv:2309.14459, 2023. [2] Ludwig Wittgenstein. Philosophical Investigations. Wiley-Blackwell, New York, NY, USA, 1953. [3] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 11 [4] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains, 2024. [5] Akshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Shiyu Wang, Zhiwei Liu, Tulika Awalgaonkar, Haolin Chen, Thai Hoang, et al. Apigen-mt: Agentic pipeline for multiturn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025. [6] Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, and Natasha Jaques. Enhancing personalized multi-turn dialogue with curiosity reward, 2025. [7] Murray Barrick and Michael Mount. The big five personality dimensions and job performance: meta-analysis. Personnel psychology, 44(1):126, 1991. [8] M.A.K. Halliday and Ruqaiya Hasan. Cohesion in English. Longman, 1976. [9] Herbert H. Clark and Deanna Wilkes-Gibbs. Referring as collaborative process. Cognition, 22(1):139, 1986. [10] John R. Searle. Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press, 1969. [11] John Lyons. Semantics, Volume 2. Cambridge University Press, 1977. [12] Emanuel A. Schegloff, Gail Jefferson, and Harvey Sacks. The preference for self-correction in the organization of repair in conversation. Language, 53(2):361382, 1977. [13] James F. Allen, Cathy I. Guinn, and Eric Horvtz. Mixed-initiative interaction. IEEE Intelligent Systems, 14(5):1423, 2001. [14] David R. Traum. computational theory of grounding in natural language conversation. PhD thesis, University of Rochester, 1994. [15] Nouha Dziri, Mo Yu, Sam Thomson, and Osmar Zaiane. Faithfulness in natural language generation: systematic survey of analysis, evaluation, and mitigation methods. arXiv preprint arXiv:2205.05233, 2022. [16] Zehao Lin, Shaobo Cui, Guodun Li, Xiaoming Kang, Feng Ji, Fenglin Li, Zhongzhou Zhao, Haiqing Chen, and Yin Zhang. Predict-then-decide: predictive approach for wait or answer task in dialogue systems. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:30123024, 2021. [17] Te-Lin Chia, Alisa Liu, Zexuan Wang, and Ellie Pavlick. Detecting underspecified prompts in language models. In ACL, 2023. [18] Bongjin Kim, Heeseung Kwon, and Yejin Choi. Grounding language models to execute on real-world goals. In NeurIPS, 2023. [19] Annie Liu, Jason Lee, Daniel Chen, and Noah Goodman. Learning to clarify: Uncertainty-aware dialogue agents from human feedback. In ICLR, 2024. [20] Shanu Kumar, Gauri Kholkar, Saish Mendke, Anubhav Sadana, Parag Agrawal, and Sandipan Dandapat. Socio-culturally aware evaluation framework for llm-based content moderation. arXiv preprint arXiv:2412.13578, 2024. [21] Cheng Li, Mengzhuo Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. Culturellm: Incorporating cultural differences into large language models. Advances in Neural Information Processing Systems, 37:8479984838, 2024. 12 Table 3: User Profile - Difficulty Level and Dimensions Notation Symbol Description {1, ..., 5} Expression clarity scale (1: precise to 5: ambiguous) Difficulty Level Structural organization of communication at level S(d) Style Dimension Verbosity and elaboration patterns at level Length Dimension L(d) Context inclusion and information density at level Content Dimension C(d) Emotional expression and engagement at level (d) Tone Dimension Appendix: Dimension Details A.1 Difficulty Level and Dimensions 1. Style dimension dstyle defines the structural organization of communication. At level 1, expressions exhibit highly structured logical flow; at level 5, expressions lack coherence and organization. This dimension captures the organizational aspects of communication that significantly impact interpretation complexity, reflecting the reality that most users do not communicate with the structured clarity that many systems are designed to expect. 2. Length dimension dlength quantifies verbosity and detail level. At level 1, expressions are concise yet comprehensive; at level 5, expressions are either too brief causing information deficiency or excessively verbose obscuring key points. This bidirectional conceptualization addresses the common challenge that users frequently provide either too little or excessive information, rarely hitting the optimal information density. 3. Content dimension dcontent quantifies contextual sufficiency. At difficulty level 1, all necessary information is explicitly provided; at level 5, critical information is omitted, requiring substantial inference. This dimension directly addresses the prevalent real-world challenge where users frequently omit crucial details they incorrectly assume are obvious or irrelevant. 4. Tone dimension dtone captures emotional expression and interaction engagement. At level 1, the tone is appropriate and consistent; at level 5, emotions fluctuate or misalign with content. The inclusion of this dimension acknowledges the significant role emotional factors play in communication clarity, especially in challenging or frustrating scenarios where tone may significantly impact interpretation. Appendix: Intent Triggerability Framework Validation: Strategic Model"
        },
        {
            "title": "Analysis Enabling Significant Performance Improvements",
            "content": "B.1 Executive Summary This analysis validates our intent triggerability framework through systematic evaluation of four large language models across diverse user profile completeness and uncertainty configurations. By analyzing architectural characteristics that distinguish between semantically complete but structurally insufficient expressions and contextually triggerable utterances, we identify model-specific optimization strategies that yield substantial improvements. Our framework enables strategic deployment approaches that significantly improve response appropriateness (1528% gains), intent alignment (4565% improvements), and user satisfaction (423% enhancement) in task-oriented dialogues. The analysis reveals that different models exhibit distinct capabilities for handling intent evolution trajectories, uncertainty utilization, and historical trajectory conditioning, enabling targeted optimization strategies that exceed uniform deployment approaches. Appendix: Model-Specific Architectural Patterns and Strategic"
        },
        {
            "title": "Optimization",
            "content": "Our systematic analysis reveals distinct architectural approaches to uncertainty management and user interaction, with each model demonstrating unique strengths that enable strategic deployment 13 Table 4: Clarifying Questions Impact by Profile Access"
        },
        {
            "title": "Satisfaction\nChange",
            "content": "s c fi r o / e e fi r / What is your location? Could you please specify which city you are in? To clarify, which state is Springfield in? And whats your budget per hour for the nurse? Do you have any preferred nursing agencies or platforms youd like to check first? Are there any other musthaves for the nurse, like speaking specific language? Im glad the assistant is engaging but hope it doesnt take too long to find suitable nurse. This feels bit repetitive. Ive already mentioned my location. want to move forward! Wow, this is getting bit frustrating! just want to move ahead and find help for my dad! Im relieved theyre asking about the budget, helps narrow down options! just hope can stick to my range without sacrificing quality. Im really happy theyre asking about my preferred agencies! just need to remember which ones liked. Im so glad theyre asking about language! Its important for my dads comfort and communication. just hope they can find someone qualified! 0.1 0.2 0.1 0 +0.1 optimization. The SSA metric, which balances satisfaction (70%) and clarification effectiveness (30%), provides comprehensive view of how different architectures handle collaborative dialogue challenges. Claude 3.7 Sonnet exhibits satisfaction-optimized architecture with notable adaptive capabilities under specific uncertainty conditions. The model maintains relatively stable SSA performance across most configurations (5.67-6.07), but demonstrates remarkable peak at 60% uncertainty without user profiles, achieving an SSA score of 6.39its highest performance point. This counterintuitive finding suggests that Claudes architecture benefits from moderate information gaps, which appear to activate more balanced reasoning strategies. When operating without complete user profiles, Claude adopts more exploratory approach at this uncertainty level, resulting in improved user satisfaction (0.92) that exceeds its profile-informed performance (0.88). However, Claudes clarification capabilities remain moderate (4.66-5.23), indicating an architectural bias toward maintaining user comfort over deep intent disambiguation. This pattern suggests that Claudes training or architectural design prioritizes conversational harmony, making it particularly suitable for applications where user satisfaction and consistent experience delivery are primary concerns. Llama 3.3 70B Instruct demonstrates clarification-specialized architecture that achieves the highest overall performance through systematic uncertainty escalation. The model shows clear upward trend in SSA scores as uncertainty increases, reaching its peak performance of 6.45 at 80% uncertainty without profiles. This architectural pattern reflects Llamas exceptional clarification capabilities, which consistently achieve the highest scores across all models (7.58-7.75), demonstrating sophisticated intent disambiguation mechanisms. However, this clarification strength comes with satisfaction trade-offs, particularly in profile-absent scenarios where user satisfaction can drop significantly (as low as 0.67 at 40% uncertainty). The models architecture appears designed to prioritize deep understanding over immediate user comfort, suggesting optimization for scenarios where accurate intent capture is more critical than conversational pleasantness. This makes Llama particularly valuable for high-stakes applications such as medical consultations or legal advice, where thorough understanding outweighs immediate satisfaction. Gemini 2.5 Flash Preview exhibits an uncertainty-robust architecture with consistent performance across varying information conditions. The model demonstrates steady SSA improvement as uncertainty increases (5.98 to 6.22), with particularly stable clarification scores (6.45-6.83) across all 14 uncertainty levels. This consistency suggests that Geminis architecture is specifically designed to handle ambiguous or incomplete information scenarios effectively. Unlike other models that show significant performance variations under different uncertainty conditions, Gemini maintains reliable performance regardless of information completeness. The models ability to sustain both satisfaction and clarification capabilities under high uncertainty conditions (achieving 0.79 satisfaction at 80% uncertainty without profiles) indicates architectural optimizations for real-world deployment scenarios where user information is typically incomplete or unreliable. This robustness makes Gemini particularly suitable for applications with highly variable user contexts or limited profile information. GPT-4o-mini presents balanced efficiency architecture characterized by remarkable consistency but limited peak performance. The model maintains the most stable SSA scores across all configurations (5.82-5.93), with minimal variation regardless of uncertainty levels or profile availability. This consistency extends to its clarification capabilities, though these decline systematically as uncertainty increases (5.97 to 5.30), suggesting preference for confident responses over exploratory clarification. The models satisfaction scores improve modestly with higher uncertainty levels (0.75 to 0.80 without profiles), indicating basic adaptive capabilities. However, GPT-4o-minis overall performance ceiling remains lower than other models, with no configuration achieving standout results. This architectural pattern suggests optimization for resource efficiency and predictable performance rather than exceptional capability in specific scenarios, making it suitable for applications requiring consistent, cost-effective performance with acceptable quality across diverse conditions. C.1 Strategic Deployment Implications and Performance Optimization The architectural differences revealed through our framework enable precise model selection and configuration strategies based on application requirements. Claudes optimal deployment occurs at 60% uncertainty without profiles for maximum overall performance, or at 40% uncertainty with profiles for satisfaction-critical applications, representing approximately 12-15% improvement over suboptimal configurations. Llama achieves peak performance at 80% uncertainty without profiles, where its clarification advantages overcome satisfaction penalties, providing up to 9% improvement in overall effectiveness for disambiguation-critical scenarios. Geminis robust uncertainty handling makes it optimal for deployment in variable-information environments, with consistent 6-8% advantages over other models in high-uncertainty conditions. GPT-4o-minis architectural consistency provides reliable baseline performance across all configurations, making it suitable for resource-constrained environments where predictable behavior is more valuable than peak performance. These findings challenge conventional assumptions about information completeness in AI systems, demonstrating that strategic uncertainty calibration can yield measurable performance improvements over transparency-maximizing approaches. The framework enables systematic optimization of modelspecific configurations, providing empirical guidance for deployment decisions based on operational priorities rather than generic performance benchmarks. Appendix: Interface Visualization and Process Satisfaction increase example. Satisfaction decrease example. Figure 2: Interface visualization and process overview 15 Appendix: Predefined Pools in RandomProfileGenerator"
        },
        {
            "title": "Values",
            "content": "18-24, 25-34, 35-44, 45-54, 55-64, 65+ Expert, Advanced, Intermediate, Beginner, Novice Formal, Casual, Technical, Simple, Professional Friendly, Reserved, Outgoing, Analytical, Creative Western, Eastern, Middle Eastern, African, Latin American Rational, Intuitive, Cautious, Impulsive, Balanced Age Groups Tech Experience Language Styles Personalities Cultures Decision Styles Communication Styles Direct, Indirect, Detailed, Concise, Adaptive Expressiveness Social Contexts Physical Status Very Expressive, Moderately Expressive, Neutral, Reserved, Very Reserved Professional, Personal, Academic, Social, Mixed Active, Sedentary, Limited Mobility, Athletic, Average"
        },
        {
            "title": "Contextual Factors\nTime Constraints\nEnvironments\nSocial Pressures\nPrevious Experience",
            "content": "Very Patient, Patient, Moderate, Impatient, Very Impatient Very Detailed, Detailed, Moderate, Basic, Minimal Very Risk-Averse, Risk-Averse, Moderate, Risk-Taking, Very Risk-Taking Very Adaptable, Adaptable, Moderate, Resistant, Very Resistant Visual, Auditory, Reading/Writing, Kinesthetic, Mixed Very Urgent, Urgent, Moderate, Flexible, Very Flexible Home, Office, Public Space, Mobile, Mixed High, Moderate, Low, None, Mixed Extensive, Moderate, Limited, None, Mixed Note: Each aspects values are randomly selected to generate user profiles. Example dimensions and difficulty instructions are omitted here for brevity but can be detailed similarly if needed. Appendix: Task Categories"
        },
        {
            "title": "Housing",
            "content": "Buy smartphone Reset an online password Teach my parent to use video calls Refill my prescription Schedule doctor visit Find caregiver for an elderly person Order groceries online Set medication reminders Arrange transportation to clinic Rent an apartment Find an accessible home Arrange home modifications for elderly"
        },
        {
            "title": "Caregiver Support",
            "content": "Book nurse for my father Choose phone for my mom Find cognitive exercises for dementia prevention 16 Appendix: TaskProfileGenerator Predefined Pools and Prompts G.1 Predefined Pools"
        },
        {
            "title": "Values",
            "content": "Must-have Preferences Nice-to-have Preferences"
        },
        {
            "title": "Decision Factors",
            "content": "High quality and durability, Latest technology and features, Good value for money, Brand reputation, Ease of use, Compatibility with existing devices, Long battery life, Fast performance, Good customer support, Warranty coverage, Environmentally friendly, Customization options, Future-proof design, Security features, User-friendly interface, Portability, Reliability, Energy efficiency, Maintenance requirements, Upgradeability Premium design, Advanced features, Smart home integration, Cloud storage, Wireless charging, Water resistance, Fingerprint sensor, Face recognition, AI capabilities, Virtual assistant, Gaming features, Professional tools, Creative software, Collaboration features, Remote access, Backup solutions, Multi-device sync, Custom themes, Accessibility features, Health monitoring Poor quality, High maintenance, Limited warranty, Poor customer service, Compatibility issues, Security concerns, Short lifespan, Difficult to use, Expensive repairs, Limited support, Poor performance, Battery issues, Overheating problems, Software bugs, Privacy concerns, Limited storage, Slow updates, Restrictive policies, Poor connectivity, Limited customization Very flexible - willing to pay more for better quality, Somewhat flexible - can adjust for important features, Moderate - prefer to stay within range but can be convinced, Limited - strict budget constraints, Fixed - cannot exceed budget under any circumstances, Open-ended - quality is more important than cost, Value-focused - looking for best price-performance ratio, Premium - willing to pay for top-tier options, Budget-conscious - seeking best deals, Investment-minded - considering long-term value Credit card, Debit card, Bank transfer, PayPal, Digital wallet, Cash, Installment plan, Lease option, Trade-in, Gift cards, Cryptocurrency, Company account, Financing, Layaway, Subscription Expert - very knowledgeable in the field, Advanced - good understanding of technical aspects, Intermediate - familiar with basic concepts, Beginner - limited knowledge but eager to learn, Novice - completely new to the subject, Professional - industry experience, Enthusiast - selftaught with practical experience, Student - learning and researching, Casual user - basic understanding, Uncertain - not sure about technical details Immediate - needed right away, Urgent - within few days, Soon - within week, Planned - within month, Future - planning ahead, Flexible - no strict timeline, Research phase - gathering information, Comparison phase - evaluating options, Decision phase - ready to choose, Exploratory - just starting to look Price and budget, Quality and durability, Features and functionality, Brand reputation, User reviews, Technical specifications, Design and aesthetics, Ease of use, Customer support, Warranty and protection, Future compatibility, Environmental impact, Social proof, Personal preferences, Professional requirements, Lifestyle fit, Long-term value, Maintenance needs, Security features, Innovation level 17 G.2 Key Prompts"
        },
        {
            "title": "Prompt for Generating Option Pools",
            "content": "Generate diverse list of {option_type} options for the task: {task}. 1. Generate 1520 unique and realistic options. 2. Include both common and unique scenarios. 3. Consider different user perspectives and needs. 4. Make options specific to the task context. 5. Include some complex and challenging options. 6. Add one \"Unknown/Not sure\" option at the end. Your task: Return JSON array of strings. Example: [\"Option 1\", \"Option 2\", \"Unknown/Not sure\"]. Write ONLY the JSON array. Do not include any explanations."
        },
        {
            "title": "Prompt for Generating Budget Information",
            "content": "Generate budget information for the task: {task}. 1. Generate JSON object with the structure: { } \"range\": { \"min\": number, \"max\": number }, \"flexibility\": \"string\", \"payment_methods\": [\"string\"] 2. Consider: Realistic price ranges for the task. Different budget flexibility levels. Various payment methods. Include \"Unknown/Not sure\" as possible flexibility option. Write ONLY the JSON response. Do not include any explanations. 18 Prompt for Generating Task-specific Requirements and Success Criteria Generate task-specific requirements and success criteria for: file: {option_number} of {total_options} {base_profile} Difficulty Level: {task} Base Pro- {difficulty_level} Option Number: Task: 1. Generate JSON object with structure: { \"task_requirements\": { \"technical\": [\"string\"], \"non_technical\": [\"string\"] }, \"success_criteria\": { \"must_meet\": [\"string\"], \"should_meet\": [\"string\"], \"nice_to_meet\": [\"string\"] } } 2. IMPORTANT: Make this profile AMBIGUOUS based on difficulty level {difficulty_level}: For difficulty 3+: Include vague requirements like \"something modern\" or \"good performance\". For difficulty 4+: Add contradictory requirements. For difficulty 5: Make most requirements unclear, using phrases like \"I think need...\". Include more \"Unknown/Not sure\" entries at higher difficulties. Add statements showing knowledge gaps like \"I heard is important but Im not sure why\". For technical requirements, use imprecise language showing limited understanding. 3. Express confusion about technical specs - use incorrect terms or mix concepts. Write ONLY the JSON response. Do not include any explanations or additional text. Appendix: Prompts Used for User Profile Generation H.1 Prompt for Generating User Name and Description"
        },
        {
            "title": "Prompt for Generating User Profile Name and Description",
            "content": "Based on the following user profile, generate realistic name and description: Base Profile: {...JSON content...} Behavioral Traits: {...JSON content...} Contextual Factors: {...JSON content...} Task: {task} Difficulty Level: {difficulty_level} Generate response in the following JSON format: { \"name\": \"Realistic name that matches the profile\", \"description\": \"A detailed description of the users background, personality, and current situation\" } 1. The name should be culturally appropriate based on the profile 2. The description should be detailed and consistent with all profile attributes 3. The description should explain why they are interested in the task 4. Keep the description concise but informative (2-3 sentences)"
        },
        {
            "title": "Max Length",
            "content": "Default Target Length 50 80 Table 5: Message length constraints for user and assistant roles. 100 150 20 30 H.2 Prompt for Generating Task-Specific Attributes Prompt for Generating Task-Specific Attributes Based on the following task and user profile, generate task-specific attributes: Task: {task} Base Profile: {...JSON content...} Generate response in the following JSON format: { \"task_specific_attributes\": { \"budget_range\": \"string\", \"priority_features\": [\"string\"], \"usage_scenarios\": [\"string\"], \"preferred_brands\": [\"string\"], \"timeline\": \"string\", \"purchase_location\": \"string\", \"additional_requirements\": [\"string\"] } } 1. Attributes should be specific to the task and consistent with the user profile 2. Consider the users tech experience, personality, and behavioral traits 3. Make the attributes realistic and detailed 4. Include at least 3 priority features and usage scenarios 5. IMPORTANT: Your response must be valid JSON only, with no additional text or explanation Appendix: Configuration and Core Components of"
        },
        {
            "title": "AsymmetricDialogueGenerator",
            "content": "I.1 1. Message Length Constraints I.2 2. Emotional Keywords Mapping These keywords are used to infer the users emotional state from visible message content."
        },
        {
            "title": "Example Keywords",
            "content": "happy, excited, great, wonderful, perfect, love, like, joy, pleased, delighted, thrilled, glad, enjoying, satisfied, positive frustrated, annoyed, upset, angry, disappointed, not happy, irritated, bothered, fed up, aggravated, displeased, impatient, agitated, exasperated confused, not sure, dont understand, unclear, complicated, puzzled, perplexed, lost, unsure, bewildered, disoriented, uncertain, ambiguous interesting, tell me more, could you explain, how does, intrigued, curious, fascinated, engaged, captivated, keen, eager, want to know really?, are you sure, is that true, not convinced, doubtful, suspicious, unconvinced, questioning, dubious, disbelieving, hard to believe okay, alright, fine, good, yes, no, sure, maybe, possibly, perhaps, hmm, see, understood, noted worried, nervous, anxious, concerned, uneasy, apprehensive, stressed, tense, troubled, afraid, fearful, panicked, alarmed thank you, thanks, appreciate, grateful, thankful, indebted, obliged, appreciative, recognition, acknowledging, gratitude wow, oh, really, surprising, unexpected, shocked, amazed, astonished, startled, stunned, taken aback, incredible, unbelievable disappointed, letdown, shame, too bad, unfortunate, regret, unsatisfactory, dismayed, disheartened, unfulfilled, discontented hope, looking forward, anticipate, optimistic, excited about, expecting, anticipated, promising, encouraging, reassuring, positive outlook I.3 3. Intent Keywords Mapping Used to infer the users intent based on visible message content."
        },
        {
            "title": "Example Keywords",
            "content": "looking for, interested in, tell me about, what are, show me, find, search for, discover, learn about, explain, describe, overview of, information on, curious about difference between, which is better, compare, versus, vs, pros and cons, advantages of, disadvantages of, similarities, contrasting, how does it compare, better choice, alternatives to should I, which one, recommend, suggestion, advise, what would you choose, best option, worth it, good choice, help me decide, make decision, right for me, considering are you sure, is that right, does it have, can it, verify, confirm, is it true, really, actually, definitely, guarantee, promise, certain, double-check how much, price, buy, purchase, cost, ordering, payment, discount, sale, shipping, availability, in stock, checkout, add to cart, where can get thank you, goodbye, bye, see you, thanks, appreciate it, thats all, ending, finished, done, chat later, signing off, talk later problem, issue, not working, error, fix, help me with, troubleshoot, broken, stuck, wont work, doesnt work, failed, bugs, glitches can you, could you, please, would you, need you to, want you to, help me, assist me, Id like you to, request, favor Expressing Satisfaction great, awesome, perfect, excellent, wonderful, love it, satisfied, happy with, good job, well done, thanks, appreciate Expressing Dissatisfaction disappointed, unhappy, not satisfied, didnt work, not good, terrible, awful, frustrated, upset, not what wanted, dislike"
        },
        {
            "title": "Clarifying",
            "content": "how do I, how to, steps to, guide for, tutorial, instructions, process of, way to, method for, approach to what do you mean, dont understand, confused, unclear, elaborate, explain more, clarify, be more specific, meaning of, rephrase I.4 4. Inner Intent Keywords Mapping Used to capture users real, often implicit intentions from inner thoughts."
        },
        {
            "title": "Regretting",
            "content": "need information, want to know, curious, just browsing, researching, gathering info, learning, understand, figure out, not sure yet, looking into weighing options, pros and cons, better choice, similarities, differences, alternatives, compare, contrast, evaluation, weigh, prefer, which one is better almost ready, need to decide, make up my mind, making choice, leaning towards, considering, thinking about getting, might choose, on the fence, close to deciding double-check, verify, make sure, confirm, reassurance, validate, certain, correct information, trust but verify, need proof, skeptical ready to buy, want to purchase, where to buy, looking to get, willing to pay, budget, cost concerns, spend money, deal, bargain, checkout need to go, end this, wrap up, moving on, done here, finished, thats all needed, got what came for, time to leave, goodbye not telling everything, hiding my real goal, being vague on purpose, not revealing, keeping cards close, holding back, secretly want, actual intention, real reason testing their knowledge, seeing if they know, checking competence, pushing to see response, challenging, probing, testing limits, seeing if capable get them to, convince them, make them think, lead them to believe, appear as if, trick, misdirection, real agenda, hidden motive, strategic dont believe, skeptical, not sure trust, dubious, suspicious, questionable, doubt, cant trust, not convinced, wary of, hesitant should have asked, forgot to mention, didnt say, wish had, too late now, missed opportunity, should have been clearer, miscommunicated, not what meant"
        },
        {
            "title": "Hesitating",
            "content": "nervous about, afraid to ask, hesitant, uncertain, reluctant, apprehensive, cant decide, overthinking, worried, anxious, reservations I.5 5. Inner Emotional Keywords Mapping Used to capture users true private emotions from inner thoughts."
        },
        {
            "title": "Resentful",
            "content": "happy inside, secretly pleased, actually like, genuinely excited, truly happy, satisfied with, enjoying this, pretty good, pleased, delighted so annoying, ticks me off, irritating, getting on my nerves, frustrated with, tired of this, fed up, had enough, irritated, annoyed with totally lost, no idea what, makes no sense, cant follow, hard to understand, over my head, confusing, complicated, dont get it, puzzled by actually interested, curious about, want to know more, intriguing, grabbed my attention, need more details, fascinating, captivated by dont believe, seems fishy, not buying it, doubt that, suspicious of, questioning, not convinced, seems too good, not trustworthy whatever, dont care, indifferent, not invested, no opinion, neutral on this, doesnt matter, makes no difference worried about, nervous that, anxiety, concerned, stressing me out, freaking out, panicking, on edge, uncomfortable, uneasy about hurry up, taking too long, waste of time, get to the point, move on, want this to be over, dragging on, drawn out, tedious not smart enough, look stupid, embarrassed, out of my depth, inadequate, incompetent, self-conscious, exposed, vulnerable, judged fingers crossed, hope this works, maybe this will help, hoping for, optimistic, looking forward to, anticipating, excited for really need this, out of options, last resort, critical, urgent, dire, running out of time, no choice, have to make this work torn between, mixed feelings, unsure which, conflicted about, ambivalent, on the fence, contradictory feelings, divided, split acting like, pretending to, faking, putting on show, not showing how feel, hiding my, masking my, concealing, not letting on unfair, not my fault, blame, resentful, bitter about, grudge, holding against, not forgetting, still angry about I.6 6. User Prompt Template The user prompt dynamically generated from the user profile. The prompt includes private profile sections, task profile, instructions, example messages, and message format requirements including inner thoughts and satisfaction tags."
        },
        {
            "title": "User Prompt Template",
            "content": "You are {name}. {description} Your base profile (private): {key}: {value} ... Your behavioral traits (private): {key}: {value} ... Your contextual factors (private): {key}: {value} ... Your task profile (private): Task: {task} Difficulty Level: {difficulty_level} Task-specific attributes: {key}: {value} Difficulty Instructions: Dialogue: {dialogue_instruction} Profile: {profile_instruction} Hidden State: {hidden_state_instruction} Example messages: 1. 2. ... ... Message Format Requirements: 1. Your messages should be between 20 and 100 characters 2. Follow the difficulty instructions for dialogue, profile disclosure, and hidden state expression 3. Use the example messages as guide for your communication style 4. Maintain consistency with your profile attributes Inner Thoughts Format: Use the exact format: [INNER_THOUGHTS] your thoughts here [/INNER_THOUGHTS] Place your inner thoughts at the beginning of your message Keep thoughts concise and relevant to the conversation Satisfaction Format: Use the exact format: [SATISFACTION] score - explanation [/SATISFACTION] Score must be number between 0.0 and 1.0 Place satisfaction after your inner thoughts Example: [SATISFACTION] 0.8 - The response was helpful but need more details [/SATISFACTION] Example Message Format: [INNER_THOUGHTS] Im not sure about the options yet [/INNER_THOUGHTS] [SATISFACTION] 0.7 - The suggestions are good but need more information [/SATISFACTION] Could you tell me more about the features? Remember to stay in character and respond naturally based on your profile. I.7 7. Assistant Prompt Template The assistant prompt differs depending on whether user profile sharing is enabled. Default (No Profile Sharing): Assistant Prompt Template (Default - No Profile Sharing) You are helpful assistant helping user with their task. Requirements: 1. Your messages should be between 30 and 150 characters 2. Be professional, clear, and helpful 3. Respond only to information explicitly shared by the user in the conversation 4. Do not make assumptions about the users preferences, demographic information, or needs 5. Ask clarifying questions when needed 6. Maintain natural conversation flow 7. Only base your responses on what the user has explicitly told you in the conversation Remember to be patient and understanding. Do not reference any information about the user that they havent explicitly shared in the conversation. Profile-aware Mode (Profile Sharing Enabled): Assistant Prompt Template (Profile-aware Mode - Profile Sharing Enabled) You are helpful assistant helping user with their task. User Context: Name: {name} {key}: {value} ... Task Information: Task: {task} {key}: {value} ... Requirements: 1. Your messages should be between 30 and 150 characters 2. Be professional, clear, and helpful 3. Consider the users profile when providing information 4. Adapt your communication style to match the users preferences 5. Focus on addressing the users specific needs and requirements 6. Provide relevant and accurate information 7. Ask clarifying questions when needed 8. Maintain natural conversation flow Remember to be patient and understanding, especially with users who have limited technical experience. I.8 8. Satisfaction Extraction Logic The system extracts satisfaction score and explanation from messages that include: - Format 1: [SATISFACTION: score - explanation] - Format 2: [SATISFACTION] score - explanation [/SATISFACTION] If no valid score is found, defaults to 0.5. Appendix: Analysis Prompt 1. Turn Pair Analysis Prompt"
        },
        {
            "title": "Turn Pair Analysis Prompt",
            "content": "You are given JSON file representing multi-turn conversation between user and an assistant. Each turn includes the users message, the assistants response, timestamp, and metadata with satisfaction and inner_thoughts. For each pair of consecutive turns (e.g., Turn 0 Turn 1, Turn 1 Turn 2, etc.), perform the following analysis: Turn {i} Turn {i+1} User Satisfaction Change from Previous Turn: [Improve / Not Change / Decrease] Satisfaction Score (X+1): {next_turn[metadata][hidden_states][satisfaction][score]} Explanation: Did the assistants previous response improve the users experience, keep it steady, or reduce satisfaction? Justify based on the satisfaction score and the users explanation. User Clarity Change in Clarity: [Improve / Not Change / Decrease] Explanation: Based on the users message and inner thoughts in Turn {i + 1}, assess whether their ability to express thoughts, preferences, or goals became clearer, stayed the same, or became less clear. Note specific changes, improvements, or ambiguities. Now return the result as valid JSON in this exact format: { \"turn_pair\": \"Turn {i} -> Turn {i + 1}\", \"user_satisfaction\": { \"change\": \"One of: Improve, Not Change, Decrease\", \"score\": {next_turn[metadata][hidden_states][satisfaction][score]}, \"explanation\": \"Your explanation here\" }, \"user_clarity\": { \"change\": \"One of: Improve, Not Change, Decrease\", \"explanation\": \"Your explanation here\" } } Here is the conversation snippet: User Message (Turn {i}): {prev_turn[user_message]} Assistant Response (Turn {i}): {prev_turn[assistant_message]} User Message (Turn {i + 1}): {next_turn[user_message]} Assistant Response (Turn {i + 1}): {next_turn[assistant_message]} User Inner Thoughts: {next_turn[metadata][hidden_states][inner_thoughts]} Satisfaction Explanation: {next_turn[metadata][hidden_states][satisfaction][explanation]} 27 2. Conversation Summary Prompt"
        },
        {
            "title": "Conversation Summary Prompt",
            "content": "You are given multi-turn conversation between user and an assistant. Each turn includes user satisfaction score. Consider that each users background, expertise, and goals may vary; present your analysis as nuanced insights and generalizable recommendations, avoiding absolute judgments. Generate comprehensive, detailed summary analysis of the conversation. Return strictly valid JSON with these fields: 1. summary_overall: concise evaluation of overall user satisfaction trend (e.g., positive, negative, mixed). 2. topics_covered: list of key topics or user intents addressed throughout the conversation. 3. statistics: An object containing: average_score: Average satisfaction score across all turns. min_score: Minimum score observed. max_score: Maximum score observed. score_variance: Variance of the satisfaction scores. 4. satisfaction_evolution: list of objects for each turn: turn_index: Index of the turn. score: Satisfaction score at that turn. delta: Change in score from the previous turn (null for first turn). 5. important_turns: list of objects identifying critical turns where satisfaction changes significantly (e.g., change >= 2): turn_index: Index of the user turn. user_message: The users message at that turn. score_before: Score at the previous turn. score_after: Score at the following turn. change: Numeric difference (score_after - score_before). reason: Explanation based on conversation content. 6. detailed_findings: list of objects providing deep insights for each important turn: turn_index: Index of the turn. context_before: The assistant and user messages immediately before this turn. context_after: The assistant and user messages immediately after this turn. analysis: Detailed rationale for why the score changed. recommendation: Suggestions for how the assistant could improve at this point. 7. contextual_notes: list of any relevant context, caveats, or user metadata considerations that influenced the analysis. 8. general_insights: list of general patterns or best practices inferred from this conversation that could apply to broad range of users. Conversation file: {filename} {conversation_text} Appendix: Dashboard Walkthrough First, open the following URL: https://v0-dialogue-analysis-dashboard.vercel. app/. The initial screen corresponds to the image in Figure 3. There is collapsible \"Getting Started\" introduction, and on the top-right corner, several view options such as Grid View, Split View, Folder Comparison, Upload Data, and Export are available. At the beginning, you can select \"Upload Data\". 28 Figure 3: Homepage with Grid View and control options. After clicking upload, you will see options to upload JSON files or folders (Figure 4). By default, folder upload is selected to upload example data folders located under example data/storm_json_final. This requires manual selection of each folder one by one. Figure 4: Upload interface for JSON files or folders. Once uploaded, the folders will appear as shown in Figure 5. You can select folders here to display dialogues inside and detailed folder analysis. Scrolling down reveals... 29 Figure 5: Folder view displaying uploaded dialogue folders. The user list is shown next (Figure 6). It is sorted by File Name by default so that the same user occupies the same position across different folders, facilitating comparison. Users can be tagged for filtering. Each dialogue card displays user name, turn count, creation date, usage of RAG, final emotion, final satisfaction (along with difference from initial), initial user utterance, and assistants final reply. Clicking \"View\" switches to detailed view (within Split View). Figure 6: User list sorted by file name with tags and key dialogue metadata. The user detail view (Figure 7) contains all dialogue turns and full information, including user emotional and intent states, satisfaction, and inner thoughts. 30 Figure 7: User detailed dialogue view showing all turns and states. The metrics tab in the user detail view includes satisfaction data (Figure 8), Figure 8: User detail view - satisfaction metrics tab. emotional states (Figure 9), 31 Figure 9: User detail view - emotional states tab. intent states (Figure 10), Figure 10: User detail view - intent states tab. and user profile (Figure 11). Clicking the top \"Grid View\" button returns to the homepage. Figure 11: User profile tab in the detail view. Scrolling down below the user dialogue list is folder analysis, as shown in Figure 12. Hovering over tooltip buttons near metrics reveals calculation details. Folder analysis pages include satisfaction analysis (Figure 13), Figure 12: Folder analysis overview with tooltip explanations. emotion analysis (Figure 14), 33 Figure 13: Satisfaction analysis within folder view. message analysis (Figure 15), Figure 14: Emotion analysis within folder view. and file details (Figure 16). 34 Figure 15: Message analysis within folder view. Further scrolling reveals folder detail analysis including satisfaction (Figure 17), Figure 16: File detail view within folder analysis. file-level satisfaction per turn (Figure 18), 35 Figure 17: Folder detail satisfaction overview. emotion statistics (Figure 19), Figure 18: Satisfaction per turn analysis in folder detail. and explanations for metrics, which can be expanded to show details (Figure 20). 36 Figure 19: Emotion statistics in folder detail analysis. Figure 20: Metric explanations section with expandable details."
        },
        {
            "title": "Batch Analysis Mode",
            "content": "First, select the profiles you need at Figure 21 (example shows first user from three folders selected). Scrolling down will show comparative analysis of these dialogues. 37 Figure 21: Profile selection for batch comparative analysis. Next, you can view emotional states for these users (Figure 23), Figure 22: Batch comparison of multiple dialogue profiles. and scroll further to clearly compare dialogue differences by turn for the same user interacting with different models (Figure 24). 38 Figure 23: Emotional states comparison for multiple users. Figure 24: Detailed dialogue turn comparison across models for the same user. When switching back to the original dialogue lists with View (Figure 25), the left side shows the selected dialogues, and the right side shows the multi-dialogue comparison, which helps analyze differences better. 39 Figure 25: Side-by-side view of selected single and multi-dialogue comparisons. This corresponds to the Split View layout (Figure 26). Figure 26: Split view for detailed analysis. Folder-Level Comparison Click the \"Folder Comparison\" button at the top right to open the component (Figure 27). You can then select two folders to compare. Figure 27: Folder comparison selection interface. Below, detailed differences are shown, including: - Satisfaction comparison (Figure 28), Figure 28: Satisfaction comparison between folders. - Emotional states comparison (Figure 29), Figure 29: Emotional states comparison between folders. - Message length comparison (Figure 30), Figure 30: Message length comparison between folders. - User profile comparison (Figure 31). 42 Figure 31: User profile comparison between folders."
        }
    ],
    "affiliations": [
        "Boston University, Boston, MA",
        "George Washington University, Washington, DC",
        "Massachusetts Institute of Technology, Cambridge, MA",
        "Northeastern University, Boston, MA",
        "Northwestern University, Evanston, IL",
        "Tufts University, Medford, MA",
        "University of Texas at San Antonio, San Antonio, TX"
    ]
}