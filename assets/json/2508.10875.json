{
    "paper_title": "A Survey on Diffusion Language Models",
    "authors": [
        "Tianyi Li",
        "Mingda Chen",
        "Bowei Guo",
        "Zhiqiang Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs."
        },
        {
            "title": "Start",
            "content": ""
        },
        {
            "title": "A Survey on Diffusion Language Models",
            "content": "Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen AbstractDiffusion Language Models (DLMs) are rapidly emerging as powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them compelling choice for various natural language processing tasks. Despite their growing prevalence, DLMs present challenges and opportunities that warrant further exploration, requiring detailed and systematic understanding of their principles, techniques, and limitations. In this survey, we provide holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs. Index TermsDiffusion Language Model, Large Language Model, Diffusion Model, Diffusion Large Language Model, Language Modeling, Multimodal Language Model"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "R ECENT advancements towards artificial general intelligence (AGI) have been largely driven by the emergence of autoregressive large language models (LLMs) [1][7] and diffusion models for image and video generation [8][12], These models exhibit remarkable capabilities in both understanding and generation across diverse modalities, achieving levels of performance that were previously unimaginable. The unprecedented scale of these models, reflected in massive parameter counts, vast datasets, substantial efforts in training, and significant computational demands during inference, has pushed AI to new heights, equipping these models with broad general knowledge and deep understanding of language and the real world. The rise of the GPT series [1], [13], [14], particularly with the public release of ChatGPT [2], has propelled autoregressive (AR) language models to dominant position in natural language processing. By training to predict the next token using causal attention and teacher forcing, AR models [4], [15], [16] can effectively scale to large datasets and model sizes. Generating text in sequential, tokenby-token fashion, AR models excel at supporting wide range of tasks, from simple question answering to complex reasoning and creative writing. However, this sequential nature imposes major bottleneck on inference speed. The autoregressive generation process, which produces one toTianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen are with VILA Lab, Mohamed bin Zayed University of Artificial Intelligence. Mingda Chen is also with Department of Automation, Tsinghua University. E-mail: {Tianyi.Li, Bowei.Guo, Zhiqiang.Shen}@mbzuai.ac.ae, cmd22@mails.tsinghua.edu.cn ken at time, inherently limits parallelism and significantly constrains computational efficiency and throughput. Diffusion models are another highly promising generative paradigm. They are trained to recover data from progressively noised versions through denoising process, and generate new samples by reversing this stochastic corruption step by step. Excelling at modeling complex data distributions, diffusion models have achieved stateof-the-art results in image and video synthesis [17]. Academic breakthroughs in diffusion modeling [18][21] have established solid theoretical foundation for training and inference. Concurrently, large-scale practical models like Stable Diffusion [8], [10], [11], Imagen [9], and Sora [12] demonstrate the remarkable scalability and generalization of diffusion paradigm, enabling generation of high-fidelity, art-level images and videos from simple text prompts often with just few words. Beyond their strong capacity for modeling complex data distributions, diffusion models offer an inherent advantage in parallelism. Through an iterative denoising process, they can generate multiple tokens or an entire sequence simultaneously, potentially leading to superior inference throughput and better utilization of modern parallel computing hardware. While challenges remain, particularly in modeling discrete data and handling dynamic sequence lengths, Diffusion Language Models (DLMs) have emerged as compelling alternative to address the trade-off between generation quality and speed. To adapt diffusion for discrete language data, several key approaches have been proposed. In the early times, the development of DLMs was primarily driven by diffusion models success in continuous domains like image synthesis. Continuous DLMs map tokens into embeddings and perform denoising in continuous space, as in pioneer 5 2 0 2 4 1 ] . [ 1 5 7 8 0 1 . 8 0 5 2 : r Fig. 1. Timeline of Diffusion Language Models. This figure highlights key milestones in the development of DLMs, categorized into three groups: continuous DLMs, discrete DLMs, and recent multimodal DLMs. We observe that while early research predominantly focused on continuous DLMs, discrete DLMs have gained increasing popularity in more recent years. works Diffusion-LM [22] and SED [23]. Discrete DLMs, on the other hand, define the diffusion process directly in token space. Early efforts such as D3PM [24] introduced structured transition matrices with absorbing states, allowing tokenlevel corruption and iterative denoising. Subsequent work like DiffusionBERT [25] integrated pre-trained masked language models (e.g., BERT) to enhance denoising quality, and proposed tailored noise schedules (e.g., the spindle schedule) to better align token corruption with token frequency. These early models demonstrated the feasibility of applying iterative denoising to non-autoregressive text generation, offering controllability and parallelism, though their performance still lagged behind strong autoregressive baselines. As core challenges in DLMs are gradually addressed and the paradigm matures, larger-scale DLMs have been developed. By initializing from autoregressive models, 7B-level models like Dream [26] and DiffuLLaMA [27] have shown that DLMs can be effectively adapted from existing models while achieving competitive performance. LLaDA-8B [28] further demonstrates the potential of training DLMs from scratch, achieving performance comparable to similarly sized LLaMA3-8B models. Multimodal DLMs, also known as diffusion multimodal large language models (dMLLMs), have also shown promise in modeling hybrid data such as text and images. Built upon open-source DLMs, models like LLaDA-V [29], Dimple [30], and MMaDA [31] integrate cross-modal reasoning and generation into the diffusion framework. Meanwhile, industry efforts have also shown growing interest in DLMs. The Mercury series [32] and Gemini Diffusion [33] report strong performance while achieving inference speeds of thousands of tokens per second. These developments highlight the growing practicality and commercial potential of DLMs. We provide timeline of DLMs development in Fig. 1, ranging from representative models to recent advancements [34][40], followed by visualization of DLM trends in Fig. 2. Diffusion language models also present unique challenges and opportunities in both training and inference. Pretraining typically follows strategies similar to those used in autoregressive language models or image diffusion models [26], [30], [31]. To accelerate training and reuse previous training efforts, many DLMs are initialized from pretrained autoregressive model weights [26], [27]. Supervised finetuning (SFT) in DLMs also mirrors that of autoregressive models: clean prompt data is provided, and the model learns to generate the target completion. Reinforcement learning (RL) is also adopted to DLMs post-training to improve performance on complex tasks. Variants of GRPO [41] algorithm such as diffu-GRPO [42] and UniGRPO [31] have been proposed to enhance the reasoning capabilities 3 generation. Additionally, guidance techniques (e.g., classifier-free guidance) enable better control over style and semantic relevance. Unified Modeling Across Modalities: By applying shared denoising-based modeling framework, DLMs naturally support unified text and vision generation tasks. This makes them particularly promising for multimodal applications that require both generation and understanding within single model. Despite the recent rise in popularity of DLMs, there remains lack of comprehensive survey that systematically covers the entire DLM ecosystem. We structured our survey as follows: Section 2 provides comprehensive overview of modern language modeling paradigms, including autoregressive, masked, and diffusion-based approaches. Section 3 delves into the training methodologies for diffusion language models, covering both pre-training and subsequent fine-tuning techniques such as SFT and RL alignment. Section 4 details various inference strategies and optimizations, focusing on techniques tailored for continuous and discrete space models. Section 5 explores the extension of diffusion models to multimodal contexts, surveying state-of-the-art models and architectures like LLaDA-V [29], MMaDA [31], and Dimple [30]. Section 6 presents and visualizes performance comparisons of DLMs. Section 7 showcases the diverse applications of DLMs in tasks ranging from text and code generation to computational biology. Section 8 highlights the challenges and limitations of diffusion language models, including issues of efficiency, reasoning, agent capability, and infrastructure, also outlines promising directions for future research. To provide consolidated overview, taxonomy of DLMs is presented in Fig. 3."
        },
        {
            "title": "2 PARADIGMS OF DIFFUSION LANGUAGE MODELS",
            "content": "Diffusion Language Models have emerged as powerful non-autoregressive paradigm that balances generative quality with inference parallelism. Inspired by principles from non-equilibrium thermodynamics [129], DLMs learn to reverse gradual noising process. This iterative refinement approach allows for parallel generation of the entire sequence, offering potential solution to the inference bottleneck of AR models. DLMs can be broadly categorized based on the space in which the diffusion process operates: either continuous or discrete. Additionally, there are hybrid ARDiffusion models that combine autoregressive and diffusion in various forms, aiming to leverage the complementary strengths of both paradigms. We present model information from several works in Table 1 and provide comparison of different paradigms in Fig. 4."
        },
        {
            "title": "2.1 Preliminary of Modern Language Modeling",
            "content": "The field of language modeling has evolved through several distinct paradigms, each characterized by unique architectural choices, training objectives, and associated trade-offs. In this subsection, we provide brief overview of recent transformer-based paradigms at scale, highlighting their core principles, mathematical formulations, and representative models. Earlier approaches are not included, as we focus on modern, large-scale designs here. This review serves Fig. 2. Trend of diffusion language model papers. For discrete DLM, the statistics are drawn from papers citing D3PM [24], with further selection of those whose titles or abstracts include the keyword language. For continuous DLM, the statistics are based on the number of related studies documented in the repository associated with this paper. The results reflect growing research interest in this domain. The statistics are for reference only. and alignment of DLMs at scale. During inference, various strategies and optimizations have been developed to fully utilize the capabilities of DLMs. Continuous DLMs can leverage ODE/SDE solvers or other few-step generation techniques to accelerate the iterative denoising process [43]. As discrete DLMs face more challenges in parallel generation, specialized parallel decoding strategies [30], [44], [45] have been proposed to enable acceptance of multiple tokens at single step and overcome parallel curse. Unmasking and remasking strategies [28], [46] further improve generation quality by selectively revealing low-confidence tokens, while caching techniques [47], [48] can significantly reduce computation and enhance inference speed for both paradigms. Compared to autoregressive models, diffusion language models are widely believed to offer several distinct advantages as follows: Parallel Generation: DLMs can generate multiple tokens in parallel through an iterative denoising process, significantly improving inference speed and throughput over autoregressive models. Bidirectional Context: DLMs naturally incorporate bidirectional context, enabling more nuanced language understanding and generation. They also produce richer contextual embeddings, which are beneficial for cross-modal generation tasks. This enables fine-grained control over the generation process as well. Iterative Refinement: The iterative denoising process allows DLMs to update their perceptions over multiple steps. By accepting high-confidence tokens early and retaining low-confidence regions as masked, Masked DLMs can progressively improve uncertain areas, often resulting in more coherent and higherquality text generation. Controllability: DLMs can be conditioned on specific token positions or structures, making them well-suited for tasks like infilling and structured 4 d g n o f D y o Continuous Space Models Diffusion-LM [22], SED [23], LATENTOPS [49], Diffuseq [50], CDCD [51], Difformer [52], LD4LG [53], GENIE [54], InfoDiffusion [55], EDDPMs [56], SMOOTHIE [57], TESS [58], TESS 2 [59], LDEBM [60] Paradigms ( 2) Discrete Space Models D3PM [24], DiffusionBERT [25], LLaDA [28], RDMs [61], MD4 [62], MDLM [63], Diffusion-LLM [64], Diffusion-NAT [65], Plaid [66], SEDD [67], RADD [68], DFM [69], DDPD [70], MGDM [71], Diffu-LLaMA [27], Dream-7B [26], GIDD [72], LongLLaDA [73] Hybrid DLMs SSD-LM [74], AR-DIFFUSION [75], BD3-LM [76], CtrlDiff [77], SpecDiff [78] Training Strategies ( 3) Pre-training Post-training From scratch: LLaDA-8B [28] Adapting from AR models: Dream [26], DiffuLLaMA [27] Adapting from image diffusion models: D-DiT [79], Muddit [80] DoT [81], DCoLT [82] Policy Gradient: diffu-GRPO [42], UniGRPO [31], SEPO [83], Coupled-GRPO [84] Preference Optimization: VRPO [85] Parallel Decoding Fast-dLLM [44], APD [45], SlowFast Sampling [86], SpecDiff [78], Dimple [30] Inference & Optimization ( 4) Multimodal & Applications ( 5, 7) Unmasking/Remasking LLaDA [28], Dream [26], Masked DLM [63], Fast-dLLM [44], ReMDM [46] Guidance A-CFG [87], Freecache [88], DINGO [89] Efficiency Techniques Key-Value Cache [44], [47], [48], [76]; Feature Cache [47], [88], [90][93]; Step Distillation [43], [94], [95]; Multimodal DLMs LLaDA-V [29], Dimple [30], MMaDA [31], D-DiT [79], LaViDa [96] Fudoki [97], Muddit [80], UniDisc [98], Conventional NLP Tasks ROIC-DM [99], DiffusionNER [100], IPAD [101], DiffusionABSA [102], DiffuSum [103] TermDiffuSum [104], Diff-KPE [105], IPED [106], EdiText [107], DIFFUSEMP [108] DiffuDetox [109], ParaGuide [110], PLANNER [111], DiffuCom [112], DiffusionDialog [113] LDP [114], PoetryDiffusion [115], XDLM [116], DiffusionRet [117], DIFND [118] Code Generation DUS [119], DiffuCoder [84], DCoLT [82], Mercury Coder [32] Computational Biology Molecular : TransDLM [120], TGM-DLM [121] Protein Design: MeMDLM [122], DPLM [123], CFP-GEN [124] DRAKES [125], ForceGen [126], DSM [127], DPLM2 [128] Fig. 3. taxonomy of Diffusion Language Models, covering foundations, training and inference strategies, and key applications. The section numbers () correspond to the sections in this survey. to establish the conceptual foundation for understanding the emergence of diffusion language models as novel and promising alternative that addresses key limitations of prior methods."
        },
        {
            "title": "2.1.1 Masked Language Models",
            "content": "Masked Language Models (MLMs), popularized by represent foundational paradigm that BERT [130], scales pretrained language models using transformer-based encoder-only architectures. Conceptually simple yet empirically powerful, MLMs learn bidirectional contextual representations by predicting randomly masked tokens within an input sequence, leveraging both preceding and succeeding context. This approach follows denoising autoencoding framework, where subset of input tokens is masked, and the model is trained to reconstruct them: LMLM = ExD EMM ask(x) (cid:34) (cid:35) log Pθ(xi xM) (1) (cid:88) iM Here, denotes the input sequence, is the set of masked positions, and xM represents the visible (unmasked) context. BERT also introduces next sentence prediction (NSP) objective to model inter-sentence relationships: LNSP = E(A,B,y)D [ log Pθ(y A, B)] (2) where (A, B) is pair of text segments, and {0, 1} indicates whether follows in the original text. BERTs effectiveness in language understanding tasks such as sentiment analysis, named entity recognition, and question answering has inspired numerous improved variants. For instance, RoBERTa [131] removes the NSP objective and adopts more aggressive training strategies, while ALBERT [132] introduces parameter sharing and matrix factorization for efficiency. DeBERTa [133] further enhances contextual encoding with disentangled attention and improved decoding mechanisms for masked token prediction. Despite their strengths in understanding tasks, MLMs are not inherently designed for generative tasks, generating text requires specialized fine-tuning strategies or decoding schemes, making them unsuitable for open-ended generation without significant architectural modifications."
        },
        {
            "title": "2.1.2 Autoregressive Language Models",
            "content": "Illustrated by GPT series [1], [2], [13], [14] and TransformerXL [134], further advanced by subsequent LLMs [3][5], [135], autoregressive language models have become the backbone of modern generative AI, characterized by their unidirectional, left-to-right token generation process. Unlike bidirectional models, Autoregressive LMs factorize the joint probability of text sequence into product of conditional probabilities: (x) = (cid:89) i=1 Pθ(xi x1, x2, . . . , xi1) (3) Given token sequence = (x1, x2, . . . , xn), the training objective is to maximize the log-likelihood of the sequence under this factorization: 5 fixed left-to-right order. The objective is to maximize the expected log-likelihood over all possible permutations of the factorization order: (cid:34) LPLM = EzZT (cid:35) log Pθ(xztxz<t ) (cid:88) t=1 (5) where ZT denotes the set of all possible permutations of sequence of length , and zt, z<t refer to the t-th and first 1 elements of given permutation ZT . This formulation allows the model to capture bidirectional context for each token, combining the advantages of bidirectional context (like MLMs) with coherent autoregressive generation process. This contrasts with DLMs, which achieve bidirectionality through parallel iterative refinement process. (cid:35)"
        },
        {
            "title": "2.2 Continuous Diffusion Language Models",
            "content": "(cid:34) LAR = EXD (cid:88) i=1 log Pθ(xi x1, . . . , xi1) (4) This is typically implemented using decoder-only Transformer architecture with causal attention masking and teacher forcing during training, ensuring that each token prediction is conditioned only on preceding tokens while enabling parallel computation of the loss. The sequential generation formulation is both strength and limitation. On one hand, it aligns with text generation tasks and facilitates straightforward sampling, naturally suits for various applications. On the other hand, it imposes fundamental bottleneck on inference speed, as token generation is inherently sequential and cannot be parallelized. This trade-off between generation quality and latency has become central challenge in advancing AR models. Beyond the standard next-token prediction (NTP), recent research has explored multi-token prediction (MTP) [16], [136] to accelerate inference by generating multiple tokens per step. These efforts share conceptual similarities with parallel decoding strategies employed in DLMs."
        },
        {
            "title": "2.1.3 Other Paradigms\nSequence-to-Sequence Models.\nSequence-to-sequence\n(Seq2Seq) models [137], an early yet powerful paradigm,\nare built on an encoder-decoder architecture and serve\nas a versatile framework for conditional text generation\ntasks such as machine translation and summarization.\nModern models like T5 [138] and BART [139] are prominent\nexamples.",
            "content": "In this architecture, the encoder processes the source sequence to produce an intermediate representation, which the decoder then uses to generate the target sequence, typically in an autoregressive manner. While standard Seq2Seq decoders are autoregressive, the framework itself is highly flexible. Many DLMs, such as DiffuSeq [50] and SeqDiffuSeq [140], adapt this architecture by replacing the autoregressive decoder with non-autoregressive diffusion decoder, leveraging the encoders strong conditioning ability to guide the denoising process in generation. Permutation Language Models. Permutation Language Models (PLM), exemplified by XLNet [141], offer an alternative approach to incorporating bidirectional context within generative framework. PLMs are trained to predict tokens in sequence, but in random, permuted order rather than Continuous-space DLMs model language by first mapping discrete tokens into continuous embedding space. diffusion process then models the data distribution in this continuous space [22], [23]. Typically, diffusion models define generative process by learning to reverse predefined corruption process that gradually transforms data into noise. This process consists of forward (noising) process and reverse (denoising) process. The forward process gradually transforms data sample x0 into noise over timesteps via fixed Markov chain: q(x1:T x0) = (cid:89) t=1 q(xt xt1) q(xt xt1) = (xt; µt(xt1), Σt), (6) (7) where µt and Σt define the noise schedule. In many implementations, such as DDPM [18] and Rectified Flow [21], the marginal distribution at each timestep has closed-form expression: xt = αtx0 + btϵ, ϵ (0, I), (8) where αt and bt are deterministic functions of time t. The reverse process learns to invert the corruption, starting from noise xT (0, I) and gradually denoising to recover sample close to x0. This is parameterized by neural network fθ(xt, t), typically implemented as Transformer, which predicts target quantity associated with the forward process (e.g., clean data, noise, or velocity). common training objective takes the form: Lsimple = Et,x0,z (cid:104) fθ(xt, t) z2(cid:105) , (9) where xt is sampled via the forward process given x0, and is the corresponding regression target derived from x0 and t. After training, generation proceeds by sampling from the learned reverse process, starting from noise xT (0, I). At each timestep = T, 1, . . . , 1, the model defines conditional distribution pθ(xt1 xt) which aims to approximate the true reverse transition q(xt1 xt). Sampling iteratively from these learned conditionals produces progressively less noisy latent states until an estimate of the original data x0 is recovered. After generating denoised embedding ˆx0, rounding step maps it back to discrete token. This is typically done by nearest-neighbor search in the embedding space or using decoder head. diffusion-specific pretraining recipe and instruction tuning, enabling strong instruction-following capabilities. 6 Diffusion-LM [22] firstly introduces diffusion process in the embedding space to create non-autoregressive language generation model. By using classifier-guidance mechanism similar to those in image diffusion models, it achieves highly controllable text generation and infilling. LDEBM [60] presents novel symbiosis of latent space EBMs and diffusion models in variational learning framework to address the learning issues of energy-based priors, with focus on interpretable text modeling. LATENTOPS [49] proposes an efficient framework for composable text operations by working within compact latent space. It introduces an efficient sampler based on ordinary differential equation (ODE) to generate latent vectors guided by arbitrary plug-in control operators, which are then decoded into the desired text. Later, Diffuseq [50], classifier-free DLM for sequence-to-sequence tasks is proposed, which corrupts only the target sequence embeddings in the forward process to achieve strong and diverse conditional text generation. The Self-conditioned Embedding Diffusion (SED) [23] framework conducts diffusion directly on fixed, continuous token embedding space. By incorporating self-conditioning mechanism, it achieves strong performance in both conditional and unconditional text generation, rivaling standard autoregressive models. CDCD [51] applies continuous diffusion to categorical data by embedding tokens into continuous space. It proposes score interpolation, which uniquely allows the model to be trained with cross-entropy loss, and time warping, an adaptive strategy to efficiently schedule noise levels during training. To address optimization challenges in the embedding space, Difformer [52] introduces an anchor loss to prevent embedding collapse and noise rescaling framework to mitigate model degeneration. LD4LG [53] leverages pretrained language model as powerful autoencoder to create compact latent space, where continuous diffusion model is then trained for high-quality text generation. GENIE [54] proposes large-scale pre-training framework for diffusion language models, introducing novel continuous paragraph denoise objective to effectively learn from large corpora by reconstructing corrupted text paragraphs. InfoDiffusion [55] introduces an information entropy-aware noise schedule to guide the model toward more humanlike keyinfo-first process that prioritizes generating core content. EDDPMs [56] unify generation, reconstruction, and representation by generalizing the diffusion process with parameterized encoder-decoder, enabling stable, joint training of all components within single framework. SMOOTHIE [57] proposes novel diffusion process that progressively smooths token embeddings based on semantic similarity, combining the advantages of continuous latent spaces and discrete token handling. Continuous diffusion processes can also be formulated in the logit space rather than the embedding space. TESS [58] introduces fully non-autoregressive framework that diffuses over k-logit simplex representation of tokens and employs novel self-conditioning mechanism tailored to this setting. Extending this, TESS 2 [59] scales the approach by adapting pretrained large autoregressive models into general-purpose diffusion language models through a"
        },
        {
            "title": "2.3 Discrete Diffusion Language Models",
            "content": "Discrete space DLMs define the diffusion process directly on the vocabulary of tokens, avoiding the need for continuous embedding space during the diffusion itself. D3PM [24] firstly illustrates this by introducing structured diffusion process over discrete tokens. The forward process corrupts sequence by applying transition matrix Qt at each step. This matrix defines the probability of token transitioning to any other token in the vocabulary. The probability of state xt given an initial state x0 is given by categorical distribution: q(xtx0) = Cat(xt; = x0 Qt), where Qt = (cid:89) i=1 Qi common choice for Qt is an absorbing state transition, where each token has probability of either remaining unchanged or transitioning to special [MASK] token. The reverse process learns to reverse these transitions, predicting the probability distribution of the original tokens given the corrupted sequence. Over time, masked DLMs have emerged as modern and highly effective evolution of discrete diffusion language models, forming the foundation for several recent largescale efforts [27], [28]. We take LLaDA [28], the most representative model of this kind as an example. Inspired by earlier work on reparameterized and simplified training objectives [61], [62], [68], LLaDA is trained from scratch using cross-entropy loss that is computed only over masked tokens: L(θ) Et,x0,xt (cid:34) 1 (cid:88) i=1 1[xi = M] log pθ(xi 0xt) (cid:35) , (10) where x0 is sampled from the training corpus, is sampled uniformly from [0, 1], and xt is obtained by corrupting x0 through the forward process. The indicator function 1[] ensures that the loss is applied only to positions that have been masked. During inference, the generation process starts with fully masked sequence of desired length. In each iterative step, the model takes the current sequence (containing mix of generated tokens and [MASK] tokens) and predicts complete sequence of tokens. Based on the models prediction confidence and noise schedule, certain number of the highest-confidence predictions are unmasked and fixed, while the remaining positions are remasked. This refinement process continues iteratively until all [MASK] tokens are resolved. This approach elegantly combines the bidirectional context of MLMs with controllable, parallel generation process. LLaDA-8B, in particular, exhibits strong scalability and instruction-following ability, achieving performance on par with powerful autoregressive models such as LLaMA3-8B. This challenges the longstanding dominance of autoregressive models in large-scale language generation. DiffusionBERT [25] combines pre-trained BERT with discrete diffusion process, leveraging its powerful denoising capabilities to learn the reverse process from masked state. The model is further enhanced by novel spindle noise TABLE 1 Summary of diffusion language models, configurations, and their design choices. 7 Model Parameters Diffusion type Noise schedule Task Training data D3PM [24] Diffusion-LM [22] Diffuseq [50] SSD-LM [74] DiffusionBERT [25] CDCD [51] LD4LG [53] SeqDiffuSeq [140] TESS [58] MDLM [63] DFM [69] TESS-2 [59] LLaDA [28] Mecury [32] LLaDA-1.5 [85] MMaDA [31] Dream [26] LLaDA-V [29] LaViDa [96] Dimple [30] LongLLaDA [73] DiffuCoder [84] 70M 100M & 300M 91M 400M 110M 1.3B 188M 65M & 110M 125M & 355M 110M 1.7B 7B 1B & 8B 8B 8B 7B 8.4B 8.4B 7B 8B 7B Discrete Continuous Continuous Continuous Discrete Continuous Continuous Continuous Continuous Discrete Discrete Continuous Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Discrete Mutual information Square-root Square-root Cosine Spindle Cosine Adaptive Linear Log-linear Linear & Cubic Log-linear Linear Log-linear Linear Log-linear Log-linear Linear Convex Log-linear Log-linear Log-linear Language Language Language Language Language Language Language Language Language Language Language & Code Language Language & Code Code Language Language Language & Code Multimodal Multimodal Multimodal Language & Code Code 65B tokens 565K sentence pairs 123B tokens 16B tokens 315B tokens 5.2M sentence pairs 45B tokens 622B tokens 2.5T tokens 360B tokens 2.3T tokens Trillions tokens 2.3T tokens 900B image-text tokens 580B tokens 3M image-text samples 1.6M image-text samples 0.8B tokens 2.3T tokens 130B tokens schedule that considers token informativeness, achieving significant improvements in generation quality compared with previous DLMs. different approach, Reparameterized Discrete diffusion Models (RDMs) [61], establishes an alternative formulation for the reverse process, which simplifies the training objective to weighted cross-entropy loss. This enables more flexible and adaptive decoding strategies, leading to significant performance gains over previous discrete diffusion models. Similarly, MD4 [62] derives simple weighted integral of cross-entropy losses as the continuous-time variational objective of masked diffusion models, providing simple and generalized framework for training DLMs. Another analogous approach is MDLM [63], which introduces simplified, Rao-Blackwellized objective that takes the form of weighted average of masked language modeling losses. Diffusion-LLM [64] demonstrates the scalability of DLMs by adapting pre-trained masked language models to diffusion paradigm and further taskspecific finetuning and instruction finetuning, unlocking their versatility in solving general language tasks. DiffusionNAT [65] unifies discrete diffusion model with PLM by reformulating the denoising process as non-autoregressive masked token recovery task, allowing BART to act as an effective denoiser. Plaid [66] is the first diffusion language model trained to maximize data likelihood, demonstrating through scaling laws that it can outperform autoregressive models like GPT-2 on standard benchmarks. To improve the training objective, SEDD [67] introduces score entropy loss to directly learn the ratios of the data distribution, which serves as discrete extension of score matching. Reparameterized Absorbing Discrete Diffusion (RADD) [68] reveals that the concrete score in absorbing diffusion can be expressed as time-independent conditional probability of the clean data, multiplied by an analytic, time-dependent scalar. It also formally unifies the training objectives of absorbing discrete diffusion and any-order autoregressive models. Discrete Flow Matching (DFM) [69] introduces novel generative paradigm for discrete data that is analogous to continuous Flow Matching. The method learns generating probability velocity to transform samples along general family of probability paths from source to target distribution. By scaling the model architecture, DFM significantly closes the performance gap with autoregressive models on various benchmarks. DDPD [70] presents framework that decouples the generation process into two specialized models: planner and denoiser. At each step, the planner identifies the most corrupted token positions needing refinement, after which the denoiser predicts their values. To improve performance on complex reasoning tasks, MGDM [71] is introduced to address the problem of subgoal imbalance. This approach enhances discrete diffusion by prioritizing more difficult subgoals during the learning process through token-level reweighting mechanism. To address the challenge of scaling, continual pre-training approach [27] is proposed to adapt existing autoregressive models, such as LLaMA, into diffusion language models. The resulting models, named DiffuGPT and DiffuLLaMA, are competitive with their AR counterparts while gaining diffusion-native capabilities like flexible infilling. Build on this observation, Dream-7B [26] is initialized from Qwen2.5 7B [142] and further trained with 580B tokens, largely outperforming existing DLMs and matching the performance of top-tier AR models. GIDD [72] is introduced to overcome the limitation that masked diffusion models cannot revise generated tokens. This framework generalizes the noising process by combining masking with uniform noise, which unlocks the models ability to self-correct mistakes and improves sample quality. Recently, to address long-context capabilities, LongLLaDA [73] provides the first systematic analysis of DLMs in this domain. It reveals that DLMs can maintain stable perplexity during direct context extrapolation and have better retrieval capabilities. LongLLaDA also introduces training-free NTK-based RoPE extrapolation method, which significantly improves the extrapolation per8 Fig. 4. An overview of training and inference procedures across different paradigms of Diffusion Language Models, with autoregressive (AR) models included for comparison. AR models are trained using teacher forcing and causal attention, whereas both discrete and continuous DLMs employ fully bidirectional attention mechanisms. Block-wise diffusion models, exemplified by BD3-LM [76], integrate autoregressive and diffusion strategies, and are trained using specially designed block-causal attention mask. formance of DLMs, validating that established extrapolation scaling laws remain effective for DLMs."
        },
        {
            "title": "2.4 Hybrid AR-Diffusion Language Models",
            "content": "Hybrid AR-Diffusion models aim to strike balance between the full parallelism of non-autoregressive models and the strong causal dependency modeling of autoregressive models. prominent strategy for hybrid AR-diffusion modeling adopts block-wise semi-autoregressive generation process. In this setting, the model generates blocks of tokens autoregressively, while the tokens within each block are generated in parallel using diffusion-like iterative process. Early efforts such as SSD-LM [74] pioneered hybrid approaches by block-wise continuous diffusion process on simplex representations, AR-DIFFUSION [75] illustrates multi-level diffusion process and achieves semiautoregressive by adjusting timestep according token position. Recent Representative model BD3-LM [76] further advances this direction on discrete models, demonstrating strong performance compared to pure AR and diffusion models. CtrlDiff [77] improves this paradigm by introducing dynamic block prediction techniques to enhance block-level efficiency and control. The generation process in these models usually consists of two nested loops. In the outer loop, blocks of tokens are generated autoregressively, with each block conditioned on previously generated blocks. Within each block, the inner loop performs parallel token-wise generation through diffusion-style iterative denoising process. In BD3-LM, the training objective is formalized as: LBD(x, θ) := (cid:88) b= Et[0,1]Eq 1 log pθ(xbxb t, x<b) (11) This hybrid strategy enables the model to capture longrange dependencies across blocks via autoregression, while simultaneously accelerating generation within each block through parallel diffusion. The design also supports flexible output lengths and KV-Cache which is widely used in AR models [76]. Notably, recent masked diffusion language models [28], [31] also adopt similar semi-autoregressive block-based decoding strategies, which can be seen as instances of hybrid AR-diffusion modeling. Beyond block-based approaches that combine AR and diffusion at sequence level, hybridization can also occur at the architectural level, where some part of the neural network, typically the encoder, diffuses the entire sequence altogether to an intermediate representation, then an autoregressive decoder generates the final sequence [143]. LADIDA [144] is slightly different approach that diffuses at document level but decodes sentences by an AR decoder. SpecDiff [78] proposed collaborative speculative decoding framework, where lightweight diffusion model drafts candidate outputs, which are then validated and finalized by large AR model."
        },
        {
            "title": "3.1 Pre-training and Supervised Fine-tuning",
            "content": "The pretraining process of DLMs largely follows procedures similar to those used in autoregressive language models (for discrete DLMs) or image diffusion models (for continuous DLMs), with relatively fewer design spaces. This section briefly summarizes existing approaches for DLM pretraining, aiming to bridge the methodological gap between DLMs and AR models. To accelerate training, particularly for large-scale models, it is common practice to initialize DLMs from pretrained 9 AR language models or image diffusion models. DiffuGPT and DiffuLLaMA [27] try to initialize masked DLMs with open-sourced LLMs from 127M to 7B parameters, found that DLMs can be efficiently adapted from AR models, significantly reducing training time and cost while achieving comparable or even superior performance to their AR counterparts. Building on this insight, Dream-7B is initialized from Qwen 2.5 7B [142], and is reported to outperform both LLaDA-8B and LLaMA3-8B on various benchmarks. Some multimodal DLMs, on the other hand, are initialized from pretrained image diffusion models. D-DiT [79] and Muddit [80] are initialized from pretrained MM-DiT backbones from SD3 [11] and Meissonic [145] respectively. Although these models are not originally designed for text generation, their latent representations contain intrinsic languagealigned knowledge, which can effectively facilitate the training of language modeling while retaining strong visual generation capabilities. the overall Supervised fine-tuning in DLMs generally mirrors that of AR models. For masked DLMs like LLaDA [28], prompt tokens are left unmasked while response tokens are selectively masked, enabling the model to learn conditional response generation in manner compatible with pretraining. In continuous DLMs, SFT can also be performed by corrupting only the response segment, as demonstrated in TESS2 [59]. Despite similarity with AR training paradigms, DLMs face several unique challenges due to their diffusion-based formulation. major issue lies in loss computation efficiency of masked DLMs. In typical masked DLM training, only 50% of tokens (on average) are involved in the loss computation, if uniformly sampling timesteps. This reduces data utilization and may lead to suboptimal gradients, particularly if critical answer tokens are excluded from the loss. To address this, LaViDa [96] proposes complementary masking strategy: each training sample is duplicated with two disjoint masking patterns, ensuring that all tokens are included in the loss computation at least once. Furthermore, due to the train-inference discrepancy, as illustrated in [146], the model performs significantly better during training than at inference time. The authors propose two-step diffusion process and an improved scheduling technique to mitigate this issue."
        },
        {
            "title": "3.2 Post-training for Reasoning Capabilities",
            "content": "Exploration of reasoning capabilities is becoming increasingly popular in DLMs as their performance on language tasks improves. Typically, reasoning capabilities are gained through fine-tuning on reasoning datasets. For DLMs, this presents unique and formidable challenge. Traditional Chain-of-Thought (CoT) methods are based on the sequential nature of AR models to reason step-by-step, but DLMs generate tokens in parallel. The most successful posttraining techniques in the AR domain, particularly those based on reinforcement learning (RL) and policy gradient methods, are built upon the ability to efficiently compute the log-probability of generated sequence. This is straightforward in AR models due to their factorizable, sequential nature. In DLM, where generation is an iterative, nonsequential process, the log-likelihood is intractable, creating significant technical barrier to applying the mature suite of RL algorithms to AR models. Intuitively, we categorized these works into three main streams, which form the structure of this subsection: (1) Parallelizing the reasoning chain, where CoT in AR models is adapted to DLMs in parallel generation. (2) Adapting policy gradient methods, where variants of popular algorithms like GRPO are introduced to DLMs. (3) Adapting preference optimization methods such as DPO to DLMs."
        },
        {
            "title": "3.2.1 DoT and DCoLT: Parallelizing the Reasoning Chain",
            "content": "One of the pioneering works to elicit complex reasoning in DLMs is Diffusion-of-Thought (DoT) [81], which adapts the popular Chain-of-Thought paradigm to the diffusion framework. Instead of generating reasoning steps sequentially like autoregressive models, DoT formulates them as intermediate thoughts that are refined in parallel throughout the diffusion denoising process. The approach is implemented by fine-tuning pre-trained DLMs such as Plaid [66] and SEDD [67] on datasets containing problems and their corresponding step-by-step rationales. To enhance the models ability to recover from its own mistakes, DoT introduces specialized training techniques like scheduled sampling and coupled sampling, which exposes the model to its own generated errors during training to improve its self-correction capabilities. This post-training methodology enables smaller DLMs to achieve impressive reasoning performance, even outperforming significantly larger autoregressive models on certain mathematical and logical reasoning benchmarks. more recent approach, Diffusion Chain of Lateral Thought (DCoLT) [82], introduces distinct RL-based reasoning framework inspired by the cognitive concept of lateral thinking, which contrasts with the step-by-step vertical thinking of traditional CoT methods. Instead of supervising intermediate steps, DCoLT treats each step of reverse diffusion process as latent thinking action, but optimizes the entire multi-step denoising trajectory with outcomebased RL to maximize reward on the final answer. When applied to masked DLMs like LLaDA, DCoLT innovatively introduces an Unmasking Policy Module (UPM), which learns the optimal order for revealing tokens as part of the RL action space. This approach significantly boosts the reasoning capabilities of DLMs, with the DCoLT-reinforced LLaDA model achieves gains of +9.8% on GSM8K and +19.5% on HumanEval."
        },
        {
            "title": "3.2.2 Adapting Policy Gradient Methods to DLMs",
            "content": "Score Entropy Policy Optimization (SEPO) [83] introduce RLHF to discrete DLMs, proposing theoretically grounded framework to fine-tune discrete diffusion models using policy gradient methods and non-differentiable rewards. Operating within the score entropy framework, SEPO adapts modern policy gradient methods like PPO and GRPO by using importance sampling to derive stable and lowvariance gradient estimate. This allows the models policy to be iteratively updated to maximize reward function, making it general framework for both conditional and 10 unconditional generation. The objective function of SEPO is defined as follows: lA(θ) = Exπθold (cid:88) yX y=x wx,y log sθ(x, T0)y (12) x,y where the model parameters θ are optimized to maximize the expected log-likelihood of the score entropy sθ weighted by wx,y = πθ(y)f (rT T0 ). The expectation is taken over samples from the previous policy πθold . The function can be selected to recover different policy gradient variants; for example, clipped function yields PPO, while group-standardized rewards yield GRPO. This formulation enables stable and low-variance gradient estimation, even with non-differentiable rewards, and provides flexible objective for fine-tuning discrete diffusion models. Numerical experiments across several discrete generative tasks showcase scalability and efficiency of SEPO, demonstrating that policy gradient RL can be soundly applied to discrete diffusion models. d1 [42] provides two-stage post-training framework for masked DLMs that combine supervised finetuning (SFT) with novel policy gradient algorithm, diffu-GRPO. To adapt GRPO for DLMs, which lack factorized likelihood, it introduces novel methods for both sequence log-probability and per-token log-probability estimation. d1 uses simple mean-field decomposition to approximate sequence logprobability by product of independent per-token probabilities, while per-token log-probability is computed by performing single forward pass on fully masked completion, conditioned on randomly masked prompt during each policy gradient update. Using different random masks for the prompt in each inner gradient update step serves as form of regularization, improving training efficiency and stability. The full d1 pipeline, leveraging SFT followed by diffu-GRPO, demonstrates significant performance improvements on mathematical and planning reasoning tasks for the LLaDA model. MMaDA [31], unified multimodal diffusion model, presents three-stage training pipeline. After first-stage pre-training, MMaDA employs mixed Long chain-ofthought fine-tuning strategy, where reasoning trajectories from diverse tasks are curated into unified format to align reasoning processes across modalities. This facilitates code start training of the third stage, which introduces UniGRPO, policy-gradient reinforcement learning algorithm tailored for diffusion language models. UniGRPO overcomes the limitations of baseline methods like d1, by leveraging structured noising strategy which samples masking ratio pi [0, 1] uniformly rather than masking all response tokens. This ensures the model is exposed to various stages of multi-step diffusion denoising process, from almost fully masked to nearly unmasked, which is consistent with conventional diffusion training and boosts the utilization of the models multi-step denoising capabilities. Additionally, the sequence-level log-likelihood is approximated by averaging over masked tokens. DiffuCoder [84] is 7B-parameter DLM specifically developed and analyzed for code generation. This work introduces an RL algorithm named coupled-GRPO, which TABLE 2 brief summary of current post-training methods for DLMs reasoning capabilities, focusing on their algorithm type, major goal, key technical innovations, and applicable model types. Notably, most of these methods are based on policy gradient, and are designed for discrete DLMs. 11 Method DoT [81] Algorithm Type Core Goal Key Technical Innovation Model Type Non-RL Fine-tuning Enable parallel Chain-ofThought reasoning Converts serial CoT into parallel diffusion process; training-time self-correction Continuous/ Discrete DCoLT [82] Outcome-based RL Enable non-linear latent reasoning Lateral thought; outcome-based RL; Unmask Policy Module SEPO [83] Policy Gradient Framework(PPO/GRPO) Finetune discrete DLMs with non-differentiable rewards Low-variance gradient estimator via score entropy & importance sampling diffu-GRPO [42] Policy Gradient (GRPO) Introduce policy gradient method to DLMs Efficient one-step log-probability estimator for applying GRPO to masked DLMs coupled-GRPO [84] Policy Gradient (GRPO) UniGRPO [31] Policy Gradient (GRPO) Reduce variance and maintain training efficiency Coupled-sampling with complementary masks Unified reinforcement learning Structured noising strategy; diversified reward modeling VRPO [85] Preference Optimization (DPO) Align with human preferences Sample budget allocation; antithetic sampling Continuous/ Discrete Discrete Discrete Discrete Multimodal Discrete Discrete is designed to be diffusion-native by leveraging the unique properties of the DLM generation process. The central innovation of coupled-GRPO is its coupled-sampling scheme for log-likelihood estimation. To obtain more robust and lower-variance estimate, it constructs paired, complementary masks for each completion sequence in training batch. For given sequence, two masks are generated such that every token position is masked in exactly one of the two masks. The log-probability estimate is then derived by averaging the losses from these two complementary forward passes. This ensures that every token is evaluated in partial-masking context during training, providing full token coverage and more stable gradient signal compared to methods that use single random mask or full mask. Coupled-GRPO is shown to substantially improve DiffuCoders performance on code generation tasks, while also encouraging more parallel, less autoregressive generation patterns."
        },
        {
            "title": "3.2.3 Adapting Preference Optimization to DLMs",
            "content": "LLaDA 1.5 [85] proposes novel framework called VarianceReduced Preference Optimization (VRPO) to adapt preference optimization methods to discrete DLMs. The work identifies that applying Direct Preference Optimization (DPO) to discrete DLMs is challenging due to the high variance of the Evidence Lower Bound (ELBO) used to approximate log-likelihoods. VRPO addresses this by introducing two key unbiased variance reduction techniques: (1) Optimal allocation of the Monte Carlo sampling budget by sampling more diffusion timesteps rather than multiple masked versions per timestep, i.e. nt = and nyt = 1 (2) Antithetic sampling, where the same timesteps and masked data are shared between the ELBO estimates of the current policy πθ and the reference policy πref for the same input yw or yl. By applying VRPO to LLaDA, the resulting LLaDA 1.5 model shows significant and consistent improvements across mathematics, code, and alignment benchmarks."
        },
        {
            "title": "4.1 Parallel Decoding",
            "content": "Parallel decoding naturally aligns with DLMs, leveraging their inherent mask-predict capability to generate multiple tokens simultaneously rather than sequentially. However, naıve parallelization can degrade coherence, motivating series of adaptive strategies that balance efficiency and quality. Fast-dLLM [44] employs confidence-aware decoding, selectively unmasking tokens whose predicted probabilities exceed threshold, and realizes up to 27.6 speed-ups without compromising quality. Adaptive Parallel Decoding (APD) [45] modulates the degree of parallelism on-the-fly by consulting lightweight autoregressive auxiliary model, thus trading throughput for fidelity when necessary. SlowFast Sampling [86] introduces two-stage schedule. Firstly, cautious slow phase to locate stable tokens, then an aggressive fast phase to finalise them in bulk, achieving up to 34 acceleration when combined with caching. SpecDiff [78] further pushes throughput by using discrete diffusion model as fully parallel drafter whose output is quickly verified (and corrected if needed) by larger autoregressive model, yielding up to 7.2 speed-ups over vanilla AR generation. Finally, the visionlanguage model Dimple [30] employs confident parallel decoding, dynamically adapting the number of tokens revealed per step and cutting generation iterations by 1.5 7. Collectively, these parallel decoding approaches substantially narrow the latency gap between diffusion and autoregressive models while preserving, and in some cases improving, generation quality."
        },
        {
            "title": "4.2 Unmasking/Remasking",
            "content": "State-of-the-art open-source discrete DLMs such as LLaDA [28] and Dream [26] adopt mask-predict paradigm: at 12 Fig. 5. Inference Techniques of Diffusion Language Models. We illustrate six different strategies here, including: (a) Parallel Decoding; (b) Unmasking & Remasking; (c) Classifier-free Guidance; (d) Key-Value Cache; (e) Feature Cache; and (f) Step Distillation. each diffusion step they unmask high-confidence tokens and remask uncertain positions, iteratively refining the sequence. The choice of unmasking/remasking policy, i.e., low-confidence sampling, random selection, or adaptive temperature, therefore dominates both generation quality and convergence speed, making it one of the most critical inference levers. Early work Masked DLM [63] formalised two baselines: random remasking and confidence-ranked remasking, showing that prioritising low-confidence positions yields better quality at no extra cost. Building on this insight, Fast-dLLM [44] introduces confidence-aware parallel decoding: every step it unmasks all positions whose predicted probabilities exceed global threshold, realizing up to 13 speed-ups while maintaining accuracy. Most recently, ReMDM [46] proposes principled inference-time remasking sampler that can remask already decoded tokens for further refinement; by scaling the remasking budget, it offers smooth computequality trade-off and closes the quality gap with autoregressive models under fixed compute. Collectively, these adaptive unmasking/remasking strategies substantially boost the efficiency and quality of diffusion language models, and they integrate cleanly with orthogonal accelerators that will be discussed later such as caching and step distillation."
        },
        {
            "title": "4.3 Guidance",
            "content": "Guidance is pivotal inference technique in diffusion models, steering the generative trajectory toward desired attributes and thereby enhancing output quality. In diffusion models, guidance refers to any technique that modifies the models denoising trajectory so that samples conform to desired condition, such as text prompt, class label, or stylistic attribute. The idea was popularized by classifier guidance [17], where gradients from an external classifier are added to the score estimate to nudge the sample toward target class. Soon after, classifier-free guidance [87] removed the need for an extra classifier: the model is trained once with and without conditioning, and at inference the two score estimates are combined: sguided = suncond + λ(scond suncond), (13) where λ is the guidance scale that balances fidelity to the condition against sample diversity. This simple formulation now underpins most text-to-image systems (e.g., Stable Diffusion [8]) and has been adopted by DLMs for promptcontrolled generation. Subsequent work refines CFG along several axes: dropout-augmented CFG smooths the qualitydiversity curve; particle-based guidance blends multiple conditions; and p2-weighting rescales the noise term to stabilise high-λ sampling. In the text domain, newer schemes extend guidance to structural and semantic constraints. FreeCache [88] couples lightweight autoregressive verifier with discrete DLM: the verifier approves (or vetoes) draft tokens before they are committed, simultaneously enforcing coherence and enabling aggressive feature caching. DINGO [89] formulates regular-expression control as dynamic-programming search over DFA, guaranteeing constraint satisfaction without altering the model distribution. In other discrete DLMs, guidance can also be applied at each diffusion step, optionally combined with masking/remasking or caching, to steer content (e.g., topic, sentiment) while preserving efficiency. Overall, guidance has become cornerstone of diffusion inference, offering lightweight, tunable handle for aligning model outputs with user intent."
        },
        {
            "title": "4.4 Efficient Inference",
            "content": "Recent state-of-the-art diffusion language models [26], [32], [85] integrate the canonical Transformer architectures [147] with the step-wise stochastic inference procedures of diffusion processes. Consequently, efforts to accelerate inference in DLMs have converged on two complementary strategies: (1) lowering the per-step computational overhead of the Transformer backbone, e.g., through KeyValue (KV) Cache or Feature Cache. (2) reducing the total number of diffusion sampling steps, e.g., via Step Distillation. KeyValue Cache. The conventional KV cache leverages the strictly autoregressive decoding pattern of LLMs and is therefore ill-suited to the bidirectional, multi-step generation paradigm of DLMs [48]. Recent work, however, shows that carefully redesigning the decoding schedule can recover much of its benefit. Block Diffusion [76] introduces Block Discrete Denoising Diffusion Language Models (BD3LMs), which decode text autoregressively across coarse blocks while running diffusion within each block; once block is finished, its keys and values are frozen and reused, enabling variable-length generation and measurable speedups. Fast-dLLM [44] keeps the blockwise view but adds training-free, approximate DualCache that exploits the nearidentity of KV activations across successive diffusion steps for both prefix and suffix tokens, delivering up to 27 endto-end throughput gains on LLaDA and Dream with < 1% accuracy loss. Complementing these block-based schemes, dKV-Cache [48] observes that token representations stabilise only after position is decoded and therefore deploys delayed, conditional cache that stores KVs one step later; this design achieves 210 speed-ups on the same models with negligible quality drop. Together, these results show that semi-autoregressive scheduling and delayed caching provide practical bridges between diffusions bidirectional conditioning and Transformer tricks originally devised for autoregression. Feature Cache. Feature caching was first introduced by DeepCache [90], which leverages the strong similarity of intermediate U-Net activations across consecutive diffusion steps to avoid redundant computation. Follow-up work -DiT [91], Learning-to-Cache [92], and FasterCache [93] demonstrate that the same principle transfers cleanly to Transformer-based diffusion models, yielding comparable speed-ups without retraining. With the rise of diffusion language models, dLLM-Cache [47] extends feature caching to text by distinguishing two redundancies: prompt tokens remain almost static throughout denoising, whereas response tokens evolve only sparsely. It therefore pairs long-interval prompt cache with an adaptive short-interval response cache refreshed only when lightweight value-similarity test (V-verify) detects substantial change, achieving up to 9 end-to-end speed-ups on LLaDA-8B and Dream7B. Most recently, FreeCache [88] caches the KV/feature projections of already clean tokens and refreshes only dynamic positions, pushing acceleration further to 34 while preserving fidelity. Collectively, these advances illustrate that feature caching can bring diffusion language models within striking distance of autoregressive LLMs in inference latency without sacrificing output quality. Step Distillation. Step distillation is widely adopted 13 acceleration technique for diffusion models, collapsing the typical thousand-step denoising process into only few and sometimes even single sampling steps, thereby drastically reducing inference time. Unlike the training-free methods discussed earlier, it imposes an offline cost: compact student network must first be trained to mimic the teacher. Early work such as Progressive Distillation [95], followed by ADD [148] and LADD [149], progressively halves the step count or aligns intermediate distributions to preserve fidelity. Di4C [94] extends the framework to discrete diffusion by explicitly distilling inter-token correlations, enabling four to ten steps students that match teacher quality while providing 2 speed-ups. Most recently, DLM-One [43] employs score-based distillation with adversarial regularisation to train continuous diffusion language model that generates an entire sequence in single forward pass, realising up to 500 acceleration with near-teacher quality. Collectively, these works establish step distillation as the principal route toward closing the latency gap between diffusion and autoregressive language models."
        },
        {
            "title": "5 MULTIMODAL AND UNIFIED APPROACHES",
            "content": "This section explores recent developments in extending DLMs to multimodal and unified architectures. Similar to autoregressive LLMs, DLMs can be naturally adapted to handle multimodal inputs and outputs. straightforward approach is to accept vision inputs through pretrained vision encoder. Following the success of LLaVA [150] in the AR domain, models such as LLaDA-V [29], LaViDa [96], and Dimple [30] employ vision encoders to extract image features, which are then projected into the same embedding space as text tokens. Beyond simple visual understanding, DLMs offer promising pathway toward unified multimodal generation and understanding. Thanks to their shared denoising diffusion framework, DLMs naturally support joint modeling of different modalities. Visual inputs can be discretized using VQ-VAE, enabling training on multimodal inputs and outputs in unified token space. Representative models such as MMaDA [31], Fudoki [97], and Muddit [80] exemplify this direction. LLaDA and LLaDAs Derivatives. We begin by introducing the LLaDA [28] family and its derivatives, which are built on the architecture and pretrained weights of the base LLaDA model. LLaDA-V [29] integrates vision encoder with an MLP-based projector that maps visual features into the language token embedding space, enabling effective visual instruction tuning. Following LLaVA-NeXT [151], LLaDA-V adopts three-stage tuning strategies. In the first stage, they only train the MLP projector to align visual representations with text embeddings using LLaVAs training data. In the second stage, the model is further tuned by large-scale visual instruction data [152] using DLM objective. The third stage is to enhance multimodal reasoning capabilities by training on QA pairs with reasoning chains. Although the LLaDA backbone is slightly weaker than LLaMA3-8B [153] on pure text tasks, LLaDA-V achieves strong performance and better scalability across various benchmarks compared with LLaMA3-V trained on the same data. It narrows the performance gap with Qwen2-VL [154] and outperforms both hybrid and pure DLM-based models [79], [155], [156], demonstrating the effectiveness of diffusion architectures in multimodal understanding. LaViDa [96] introduces family of VLM based on LLaDA and Dream-7B [26]. Also utilizing pretrained vision encoder, LaViDa uses two-stage training strategy to train the projector and finetune the model respectively. LaViDa makes notable contributions to address training and inference challenges of multimodal DLMs. Typically, in masked DLMs, only about 50% of the tokens are masked for loss computation on average, which reduces efficiency and may omit critical answer tokens during VLM training, thereby causing gradient misalignment. LaViDa introduces complementary masking for effective training: For each sample, two masked versions with disjoint corrupted spans are generated, ensuring all tokens are eventually used in training and improving sample efficiency and gradient flow. During inference, LaViDa employs Prefix KV-Cache to cache the keys and values of visual and prompt tokens, significantly reducing latency and achieving maximum speedup of 3.9 with marginal performance drop. Additionally, timestep shifting is used to unmask tokens earlier, further boosting generation quality. Empirical results show that LaViDa achieves competitive or superior performance to AR-based VLMs, while enjoying significant inference speedup. Building upon LLaDA, MMaDA [31] further generalizes the architecture to support both multimodal understanding and generation. Unlike prior models, MMaDA eliminates the need for an explicit vision encoder by tokenizing images into discrete codes using VQ-VAE, and modeling all modalities jointly with modality-agnostic diffusion transformer. This design allows seamless integration across text and image modalities without modality-specific components. MMaDA also implements mixed long CoT fine-tuning strategy that aligns CoT reasoning format across modalities. Moreover, UniGRPO, unified policy-gradient based RL algorithm, is tailored specially for diffusion language models, making it possible to reason across modalities. Not only surpass similar-sized models like LLaMA3 for textual reasoning and Show-o [155] for multimodal understanding, MMaDA even excels professional image generation models like SDXL [10] in image generation. Dimple. Dimple [30] introduces large multimodal DLM, combining vision encoder with discrete DLM backbone. The authors identify that pure discrete diffusion training approach suffers from significant instability, poor performance, and severe length bias. To overcome these challenges, Dimple proposes novel two-phase training paradigm called Autoregressive-then-Diffusion. In the first phase, the model undergoes standard autoregressive training to effectively align the vision and language modalities. In the second phase, it switches to diffusion-based training to restore its parallel decoding capabilities. This hybrid strategy ensures stable and efficient training while achieving performance comparable to or even better than contemporary autoregressive models like LLaVA-NEXT. For inference, Dimple introduces several techniques to improve efficiency and controllability. Confident Decoding dynamically adjusts the number of tokens generated in each step based on confidence threshold, which reduces the total number of generation iterations. The model also successfully re-implements the prefilling technique, common in autoregressive models, to cache prompt tokens and achieve speedup of up to 7 with minimal performance loss. Furthermore, Dimple explores the use of Structure Priors, allowing for precise, fine-grained control over the response format and length, feature that is difficult to achieve in autoregressive models. D-DiT. Dual Diffusion Transformer (D-DiT) [79] is largescale fully end-to-end unified multimodal diffusion model that supports both text-to-image (T2I) and image-to-text (I2T) tasks. It directly addresses the challenges previous diffusion models faced in visual understanding tasks, which have been largely dominated by autoregressive models. The architecture is inspired by the Multimodal Diffusion Transformer (MM-DiT), featuring dual-branch transformer that processes image and text tokens, with attention mechanisms allowing interaction between modalities in every layer. The model uses frozen VAE for image processing and frozen T5 encoder for text, and the major backbone MM-DiT is initialized from pretrained SD3 [11] weight. One core innovation of D-DiT is its joint training objective, which combines continuous latent-space diffusion for images and discrete masked-token diffusion for text by jointly optimizing the sum of both modalities losses. Unlike prior multimodal diffusion models that required an autoregressive component to decode text latents, D-DiT is fully diffusion-based and demonstrates competitive performance against other unified models. UniDisc. Unified Multimodal Discrete Diffusion (UniDisc) [98] is proposed as unified generative model for the joint text and image modeling, building upon discrete diffusion as an alternative to dominant AR approaches. Different from previously discussed D-DiT, UniDisc employs an entire masked diffusion process jointly on text and image tokens with full attention, learning to map sequence of masked tokens back to clean sequence from shared vocabulary. Training is performed using unified discrete diffusion objective from scratch, where tokens from both modalities are randomly masked and the model is supervised with re-weighted cross-entropy loss. key advantage of UniDisc is its superior performance in conditional generation tasks, which is largely attributed to the effective use of classifier-free guidance. One of the most notable capabilities of UniDisc is its ability to perform joint image and text inpainting in zero-shot manner, feature not possible with previous AR or unified generative models. The author performs scaling analysis by scaling up the model up to 1.4B, demonstrating UniDisc outperforms AR models in terms of both performance and inferencetime compute, with enhanced controllability and editability. However, UniDisc is found to be less training-efficient than comparable AR model in terms of achieving the same validation loss. Fudoki. Fudoki [97] is introduced as the first generalpurpose unified multimodal model built entirely on the discrete flow matching framework, challenging the dominance of autoregressive (AR) and masking-based diffusion models. Instead of relying on simple masking corruption process, Fudoki leverages more general metric-induced probability path with kinetic optimal velocities, which allows for more semantically meaningful corruption process and enables the model to continuously self-correct its predictions during iterative refinement. This self-correction capability is key distinction from masked DLMs, where unmasked tokens are typically fixed and cannot be revised. To reduce the high cost of training from scratch, Fudoki is initialized from pre-trained AR-based MLLM, Janus1.5B [157], and is then adapted to the discrete flow matching paradigm in two-stage process. Its architecture is based on Janus-1.5B but uses full attention mask to better capture global context and removes time embedding layers, as the model can implicitly infer the timestep from the corrupted input. Fudoki achieves performance comparable to state-ofthe-art AR models in both visual understanding and image generation tasks, demonstrating flexible trade-off between inference speed and quality. The model shows significant performance gains when test-time inference scaling techniques are applied, suggesting the potential of this architecture to be further explored for next-generation unified models. Muddit. Muddit [80] is pure unified discrete diffusion transformer that integrates strong text-to-image backbone with lightweight text decoder, enabling flexible and highquality multimodal generation under truly unified architecture. Initialized from pretrained MM-DiT from Meissonoic [145], the model is trained using unified discrete diffusion objective, where text and image tokens are stochastically masked according to cosine schedule and the model learns to predict the original tokens via re-weighted crossentropy loss. By combination of the strength from semantically rich visual prior and parallel discrete diffusion, Muddit achieves competitive or superior performance compared to significantly larger AR models across generation and understanding benchmarks. It also demonstrates several times speedup over AR baseline, highlighting the efficiency and scalability of discrete diffusion approach when properly initialized."
        },
        {
            "title": "6 PERFORMANCE STUDY",
            "content": "In this section, we briefly compare the performance of various DLMs with AR models. We present visualizations based on several widely used benchmarks for evaluating DLMs, including PIQA [158] and HellaSwag [159] for general language understanding, HumanEval [160] for code generation, and GenEval [161], MME [162], MMMU [163] and GQA [164] for multimodal generation and comprehension. We also include GSM8K [165], popular benchmark in DLM literature for assessing mathematical reasoning capabilities. The corresponding performance visualizations are shown in Fig. 6. The DLMs surveyed range in size from under 1B to 8B parameters. For comparison, we also report the performance of representative AR models of similar scale. Performance data are primarily taken from original publications. If results were not available in the source papers, we consulted subsequent works that reported comparable evaluations. Our findings suggest that DLMs generally perform competitively with AR models of comparable size. On general language understanding benchmarks such as PIQA and HellaSwag, models like LLaDA achieve performance that is slightly below or on par with AR models such as 15 LLaMA2 [4] and Qwen2.5 [166]. However, DLMs exhibit stronger performance in math and science-related benchmarks, including GSM8K, GPQA [167], and MATH [168], where models such as LLaDA and Dream consistently outperform similarly sized AR counterparts. In multimodal tasks, models like MMaDA [31] and LLaDA-V [29] often surpass AR-based multimodal models, highlighting the potential of DLMs in unified and cross-modal reasoning. On code generation tasks, DLMs also demonstrate competitive capabilities. Notably, DiffuCoder [84] achieves competitive HumanEval performance among open-source models, illustrating the potential of DLMs in structured, logic-heavy domains. Furthermore, closed-source DLMs such as Gemini Diffusion [33] and Mercury [32] achieve state-of-the-art results among all DLMs, rivalling top-tier AR models like GPT-4o. Given the relatively limited training data and computational resources used to train most current DLMs, these results suggest that DLMs hold strong potential as viable alternatives to AR models in many real-world applications."
        },
        {
            "title": "7 APPLICATIONS ON DOWNSTREAM TASKS\n7.1 Conventional NLP Tasks",
            "content": "Before the emergence of large-scale DLMs for generalpurpose language generation, DLMs have already been applied to various conventional NLP tasks, such as text classification [99], named entity/scene recognition [100], [101], sentiment analysis [102], document summarization [103], [104], style transfer [110], [169], constrained generation [111][115], and machine translation [116], [170], etc. ROIC-DM [99] is the first work to adapt diffusion models for robust text classification and inference. It applies the diffusion process directly to the class labels and conditions the denoising process on the input text, which can be further enhanced by incorporating traditional language models as advisors. DiffusionNER [100] formulates Named Entity Recognition as boundary-denoising task. It applies diffusion process to the start and end boundaries of entities, generating entity spans from random noise through an iterative refinement process. For scene text recognition, IPAD [101] introduces parallel, iterative network that frames the task as conditional text generation, employing discrete diffusion and an easy-first decoding method to effectively balance recognition accuracy and inference speed. For aspect-based sentiment analysis, DiffusionABSA [102] employs diffusion model to progressively extract the aspects step-by-step. DiffuSum [103] proposes novel paradigm for extractive summarization by using diffusion model to directly generate desired summary sentence representations. The final summary is then formed by extracting document sentences that best match these generated representations. For legal document summarization, TermDiffuSum [104] proposes term-guided diffusion model that prioritizes sentences with legal terminology via multifactor fusion noise weighting schedule. For keyphrase extraction, Diff-KPE [105] enhances phrase representations by guiding text diffusion process with Variational Information Bottleneck to generate and inject keyphrase information. IPED [106] treats relational triple extraction as an implicit block diffusion task. EdiText [107] introduces controllable coarse-to-fine text edit16 Fig. 6. Performance comparison on eight benchmarks: Overall-GenEval, MME, CQA, Hellaswag, PIQA, HumanEval, GSM8K, and MMMU. The horizontal axis in each subplot represents the model size, measured in the number of parameters. The vertical axis indicates the score under the corresponding benchmark, with higher scores reflecting better performance. Model types are distinguished by color: blue represents AR language models, while orange represents DLMs. ing framework by integrating an SDEdit-based technique with novel self-conditioning method for precise editing control. To generate more specific empathetic responses, DIFFUSEMP [108] utilizes conditional diffusion model guided by multi-grained control signals (e.g., intent and semantic frames) that are integrated via special masking strategy. DiffuDetox [109] utilizes mixed diffusion approach for text detoxification, combining conditional model to reduce toxicity with an unconditional model to ensure the fluency of the output text. finetuned DiffuSeq model is shown to achieve state-of-the-art performance on fine-grained text style transfer tasks [169], while ParaGuide [110] introduces more flexible plug-and-play framework that guides paraphrase-conditioned diffusion model with off-the-shelf classifiers and style embedders at inference time. To generate fluent and diverse paragraphs while avoiding repetition, PLANNER [111] combines latent diffusion planning module to generate semantic paragraph embeddings with an autoregressive decoding module to render the final text. DiffuCom [112] presents an efficient diffusion model for comment generation that uses context-aware attention mechanism and self-conditioning technology. DiffusionDialog [113] tackles the one-to-many problem in dialogue generation by performing diffusion process with continuous latent variables, improving response diversity and inference speed. For paraphrase generation, LDP [114] models diffusion in pretrained models latent space, avoiding the typical rounding step to achieve greater efficiency. For the highly constrained task of poetry generation, PoetryDiffusion [115] uniquely separates the task by using the diffusion model to generate semantics while novel, independently trained metrical controller enforces structural rules like format and rhyme. In machine translation, XDLM [116] pioneers cross-lingual pre-training objective for diffusion models, enabling them to effectively learn the mapping between languages in the pretraining stage. DiffusionRet [117] proposes two-stage generative retrieval method that first utilizes diffusion model to generate pseudo-document from query, which then serves as input for an n-gram-based model to retrieve the final document. DIFND [118] employs diffusion model to generate debunking evidence and multi-agent MLLM system for chain-of-debunk reasoning to improve accuracy and interpretability for multimodal fake news detection."
        },
        {
            "title": "7.2 Code Generation",
            "content": "Although DLMs are rarely explicitly designed for code generation, the global planning and iterative refinement capabilities of them are particularly well-suited for the nonsequential nature of code generation. Foundational models like DiffuCoder [84], 7B open-source model, have been developed specifically for this domain. DiffuCoders analysis reveals unique decoding behaviors, such as generation order becoming more flexible at higher temperatures. It also proposes coupled-GRPO, novel sampling scheme that constructs complementary mask noise for completions used in training, which significantly improves the models performance on code generation tasks. Building on the reasoning aspect, DCoLT [82] treats the entire reverse diffusion process as form of non-linear, lateral thinking. With outcome-based RL and unmasking policy module, it achieves strong results on complex coding tasks. Dilated Unmasking Scheduler (DUS) [119] offers an inference-only, planner-free method that unmasks tokens in non-adjacent pattern to minimize an upper bound on joint entropy gain at each denoising step, achieving promising results on code generation while improving speed-quality trade-off. Demonstrating the real-world potential of DLMs speed, Mercury Coder [32] is commercial-scale diffusion model that achieves state-of-the-art throughput, outperforming speed-optimized autoregressive models by up to 10 times while maintaining comparable quality on major code benchmarks."
        },
        {
            "title": "7.3 Biological and Scientific Applications",
            "content": "TransDLM [120] performs molecular optimization guided by textual description of target properties to avoid the error propagation. Another text-guided approach, TGMDLM [121], focuses on molecular generation by collectively and iteratively updating token embeddings of SMILES strings. Without relying on additional data resources, TGMDLM surpasses MolT5-Base in generation performance. DRAKES [125] introduces an RL-based fine-tuning method for discrete diffusion models that backpropagate rewards using the Gumbel-Softmax trick for DNA and protein design. For protein modeling, ForceGen [126] enables de novo protein design by using protein language diffusion model to generate sequences that meet complex, nonlinear mechanical property-design objectives. MeMDLM [122] introduces masked diffusion language model for de novo membrane protein design by fine-tuning the ESM-2 protein language model to generate novel and realistic transmembrane sequences. Inspired by LLaDA, DSM [127] introduces enabling both high-quality representation learning and effective generative protein design. DPLM [123] offers versatile protein language model that exhibits strong generative and predictive capabilities for protein sequences, and demonstrates superior performance in representation learning. DPLM2 [128] further extends the model into multimodal protein foundation model that can simultaneously process both sequences and structures. By converting 3D structural coordinates into discrete tokens, DPLM-2 learns the joint distribution of these two modalities. This enables the simultaneous co-generation of compatible protein sequences and their 3D structures, in addition to supporting conditional tasks such as protein folding and inverse folding. CFP-GEN [124] is novel diffusion language model designed for Combinatorial Functional Protein Generation. It 17 facilitates de novo protein design by integrating multimodal constraints, including functional, sequence, and structural information. CFP-GEN supports high-throughput generation of novel proteins with functionality comparable to that of natural proteins and achieves high success rate in the design of multifunctional proteins."
        },
        {
            "title": "8 CHALLENGES AND FUTURE DIRECTIONS",
            "content": "While diffusion language models have shown considerable promise across wide range of tasks, several key challenges still remain and limit their practical deployment and broader application. In this section, we outline and discuss critical areas that require further research and innovation."
        },
        {
            "title": "8.1 Major Challenges",
            "content": "1) ParallelismPerformance Trade-off. Diffusion language models are designed to generate multiple tokens in parallel. However, this parallelism often comes at the expense of generation quality and consistency. In discrete DLMs, unmasking multiple tokens simultaneously in single step increases the denoising burden, which can lead to error accumulation. central issue is the interdependence between tokens, known as the Parallel Decoding Curse [44]. When predicting multiple tokens at once, the model produces distribution for each position and samples from them independently, failing to account for dependencies among positions. Consider simple example where the training data consists only of two sequences: ABABAB and BABABA. Statistically, and appear with equal frequency at each position in the training data, leading DLMs to assign them similar probabilities during prediction. In autoregressive models, once the first is generated, the model is likely to predict next, preserving consistency. In contrast, DLM generating tokens in parallel may independently sample for both the first and second positions, producing sequence like AAABBA, which deviates from valid training patterns. Empirical studies show that this issue significantly affects DLM performance, particularly when the number of denoising steps is reduced [27]. This phenomenon is illustrated in Fig. 7. Future work may focus on mitigating this trade-off. Potential directions include introducing structured constraints, modeling inter-token dependencies more explicitly, or refining sampling strategies to improve coherence during parallel generation. 2) Infrastructure. While the training, fine-tuning, and inference of AR models have been significantly simplified and accelerated by open-source, highly optimized libraries and frameworks (e.g., Hugging Face Transformers [171]), DLMs still lag behind in this regard. Currently, major machine learning ecosystems offer little to no native support for DLMs, posing practical challenges for researchers and developers. Furthermore, during inference, DLMs lack mature, open-source deployment infrastructure akin to vLLM [172], making efficient serving of DLMs difficult. 3) Long Sequence and Dynamic-Length Generation. DLMs are typically trained to denoise fixed-length sequences under diffusion-based objective, which makes it challenging to generalize to longer or dynamically sized sequences at inference time. Most existing DLMs are limited to maximum 18 Fig. 7. Generation results of LLaDA [28] and MMaDA [31] under different denoising step settings. Note that the generation length is set to 128 tokens and 256 tokens for LLaDA and MMaDA respectively. Both models generate correct and coherent response only when 1 or 2 tokens are unmasked at each step. With fewer steps and more parallelism, the responses are either incorrect or lack fluency and consistency. This illustrates the trade-off between parallelism and output quality in DLMs. We omit part of the thinking process of MMaDA with 256 steps for simplicity. context length of 4,096 tokens, and widely used extrapolation techniques in AR models for longer sequences remain underexplored in the DLM setting. This limitation hinders the applicability of DLMs in tasks requiring long-context understanding or complex reasoning. In addition, DLMs generally require the generation length to be predetermined during inference, making them ill-suited for dynamic-length generation. Although DLMs can predict an [EOS] token and omit displaying tokens generated afterward, the entire sequence is still fully updated throughout the denoising process, regardless of whether the generation has logically ended, which leads to unnecessary computational overhead. In addition, masked DLMs utilize full bidirectional attention at every denoising step, which incurs computational cost of O(N 2) per step, where is the sequence length. Assuming fixed number of tokens are unmasked at each step, the total number of denoising steps scales linearly with , leading to an overall inference complexity of O(N 3). Without architectural optimizations such as KV-Cache, this cubic time complexity severely limits the scalability of DLMs for long-sequence generation in real-world applications. 4) Scalability. Scalability remains an underexplored challenge for diffusion language models, particularly in comparison to autoregressive models. Although DLMs have shown promising results on certain metrics and benchmarks, they have yet to be scaled to the same extent as AR counterparts. The largest publicly available DLM currently contains only around 8B parameters, significantly smaller than leading AR models that have been scaled to hundreds of billions or even trillions, such as Llama-3.1-405B [153], DeepSeek-V3671B-A37B MoE [16], Qwen3-235B-A22B MoE [173], KimiK2-1T-A32B MoE [174], etc. Closed-source DLMs, such as Mercury and Gemini Diffusion, also fall short of state-ofthe-art AR models across wide range of benchmarks. Furthermore, many existing DLMs are trained either from previously pretrained AR models or built upon baseline DLMs (e.g., LLaDA) using limited datasets, which further constrains their scalability and performance. Therefore, the ability to further scale up DLMs still needs to be validated or explored."
        },
        {
            "title": "8.2 Future Directions",
            "content": "Despite the challenges discussed above, DLMs present many promising directions for future exploration. Below, we briefly outline several under-explored directions and opportunities that could significantly advance the field: Training Efficiency: Current DLMs generally exhibit lower training efficiency compared to AR models, due to factors such as limited token usage during loss computation. Future research could explore hybrid DLM architectures or improved training schemes that match or exceed AR models in efficiency. Quantization and Binarization (Low-bit DLMs): While extensively studied in AR models, low-bit quantization and binarization remain largely unexplored in DLMs. Adapting these techniques to the diffusion paradigm could yield faster inference and reduced memory consumption, benefiting deployment in real-world systems. Pruning and Distillation: Model compression techniques such as pruning and knowledge distillation have been successful in reducing model size and inference cost for AR models. Applying these techniques to DLMs could enhance their deployability, especially in resource-constrained or latency-critical environments. Multimodal Unified Reasoning: Although recent multimodal DLMs demonstrate impressive capabilities in cross-modal understanding and generation, most models are still limited to reasoning within single modality at time. Future efforts can focus on building unified DLMs capable of performing complex reasoning across multiple modalities in truly integrated manner. DLM-based Agents: The potential of DLMs in powering intelligent agents remains largely underexplored. Leveraging their bidirectional context modeling, parallel decoding, and iterative refinement capabilities, DLM-based agents could offer greater flexibility and adaptability in dynamic environments, making them promising alternative to traditional AR-based agent approaches."
        },
        {
            "title": "REFERENCES",
            "content": "[1] [3] [2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language models are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 18771901, 2020. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research, vol. 24, no. 240, pp. 1113, 2023. [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [5] [8] [7] [6] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., survey of large language models, arXiv preprint arXiv:2303.18223, vol. 1, no. 2, 2023. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 68410 695. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans et al., Photorealistic text-to-image diffusion models with deep language understanding, Advances in neural information processing systems, vol. 35, pp. 36 47936 494, 2022. [9] [10] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. uller, J. Penna, and R. Rombach, Sdxl: Improving latent diffusion models for high-resolution image synthesis, in The Twelfth International Conference on Learning Representations. [11] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. uller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first international conference on machine learning, 2024. [12] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman et al., Video generation models as world simulators, OpenAI Blog, vol. 1, p. 8, 2024. [13] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., Improving language understanding by generative pre-training, 2018. [14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [15] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [16] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. [18] [17] P. Dhariwal and A. Nichol, Diffusion models beat gans on image synthesis, Advances in neural information processing systems, vol. 34, pp. 87808794, 2021. J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 68406851, 2020. J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [19] [20] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, in International Conference on Learning Representations. [21] X. Liu, C. Gong et al., Flow straight and fast: Learning to generate and transfer data with rectified flow, in The Eleventh International Conference on Learning Representations. [22] X. Li, J. Thickstun, I. Gulrajani, P. S. Liang, and T. B. Hashimoto, Diffusion-lm improves controllable text generation, Advances in neural information processing systems, vol. 35, pp. 43284343, 2022. [23] R. Strudel, C. Tallec, F. Altche, Y. Du, Y. Ganin, A. Mensch, W. Grathwohl, N. Savinov, S. Dieleman, L. Sifre et al., Selfconditioned embedding diffusion for text generation, arXiv preprint arXiv:2211.04236, 2022. J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg, Structured denoising diffusion models in discrete state-spaces, Advances in neural information processing systems, vol. 34, pp. 17 98117 993, 2021. [24] [25] Z. He, T. Sun, Q. Tang, K. Wang, X. Huang, and X. Qiu, Diffusionbert: Improving generative masked language models with diffusion models, in The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. J. Ye, Z. Xie, L. Zheng, Jiang, Z. Li, and L. Kong, Dream 7b, 2025. [Online]. Available: https: //hkunlp.github.io/blog/2025/dream S. Gong, S. Agarwal, Y. Zhang, J. Ye, L. Zheng, M. Li, C. An, P. Zhao, W. Bi, J. Han et al., Scaling diffusion language models J. Gao, Z. Wu, X. [27] [26] [28] via adaptation from autoregressive models, in The Thirteenth International Conference on Learning Representations. S. Nie, F. Zhu, Z. You, X. Zhang, J. Ou, J. Hu, J. Zhou, Y. Lin, J.-R. Wen, and C. Li, Large language diffusion models, arXiv preprint arXiv:2502.09992, 2025. [29] Z. You, S. Nie, X. Zhang, J. Hu, J. Zhou, Z. Lu, J.-R. Wen, and C. Li, Llada-v: Large language diffusion models with visual instruction tuning, arXiv preprint arXiv:2505.16933, 2025. [30] R. Yu, X. Ma, and X. Wang, Dimple: Discrete diffusion multimodal large language model with parallel decoding, arXiv preprint arXiv:2505.16990, 2025. [31] L. Yang, Y. Tian, B. Li, X. Zhang, K. Shen, Y. Tong, and M. Wang, Mmada: Multimodal large diffusion language models, arXiv preprint arXiv:2505.15809, 2025. I. Labs, S. Khanna, S. Kharbanda, S. Li, H. Varma, E. Wang, S. Birnbaum, Z. Luo, Y. Miraoui, A. Palrecha et al., Mercury: Ultra-fast language models based on diffusion, arXiv preprint arXiv:2506.17298, 2025. [32] [33] DeepMind, Gemini diffusion, https://deepmind.google/ technologies/gemini, 2024, accessed: 2025-07-09. [34] M. Xu, T. Geffner, K. Kreis, W. Nie, Y. Xu, J. Leskovec, S. Ermon, and A. Vahdat, Energy-based diffusion language models for text generation, arXiv preprint arXiv:2410.21357, 2024. J. Deschenaux and C. Gulcehre, Beyond autoregression: Fast llms via self-distillation through time, in The Thirteenth International Conference on Learning Representations. [35] [36] K. Han, K. Kenealy, A. Barua, N. Fiedel, and N. Constant, Transfer learning for text diffusion models, arXiv preprint arXiv:2401.17181, 2024. S. S. Sahoo, J. Deschenaux, A. Gokaslan, G. Wang, J. Chiu, and V. Kuleshov, The diffusion duality, arXiv preprint arXiv:2506.10892, 2025. [37] [38] Y. Zhang, S. He, D. Levine, L. Zhao, D. Zhang, S. A. Rizvi, E. Zappala, R. Ying, and D. van Dijk, Non-markovian dislanguage models, arXiv preprint crete diffusion with causal arXiv:2502.09767, 2025. [39] M. Dang, J. Han, M. Xu, K. Xu, A. Srivastava, and S. Ermon, Inference-time scaling of diffusion language models with particle gibbs sampling, arXiv preprint arXiv:2507.08390, 2025. [40] L. Rout, C. Caramanis, and S. Shakkottai, Anchored diffusion language model, arXiv preprint arXiv:2505.18456, 2025. [41] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. S. Zhao, D. Gupta, Q. Zheng, and A. Grover, d1: Scaling reasoning in diffusion large language models via reinforcement learning, arXiv preprint arXiv:2504.12216, 2025. [42] [43] T. Chen, S. Zhang, and M. Zhou, Dlm-one: Diffusion language models for one-step sequence generation, arXiv e-prints, pp. arXiv2506, 2025. [44] C. Wu, H. Zhang, S. Xue, Z. Liu, S. Diao, L. Zhu, P. Luo, S. Han, and E. Xie, Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, arXiv preprint arXiv:2505.22618, 2025. [45] D. Israel, G. V. d. Broeck, and A. Grover, Accelerating diffusion llms via adaptive parallel decoding, arXiv preprint arXiv:2506.00413, 2025. [46] G. Wang, Y. Schiff, S. S. Sahoo, and V. Kuleshov, Remasking discrete diffusion models with inference-time scaling, in ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy. [47] Z. Liu, Y. Yang, Y. Zhang, J. Chen, C. Zou, Q. Wei, S. Wang, and L. Zhang, dllm-cache: Accelerating diffusion large language models with adaptive caching, arXiv preprint arXiv:2506.06295, 2025. [48] X. Ma, R. Yu, G. Fang, and X. Wang, dkv-cache: The cache for diffusion language models, arXiv preprint arXiv:2505.15781, 2025. [50] [49] G. Liu, Z. Feng, Y. Gao, Z. Yang, X. Liang, J. Bao, X. He, S. Cui, Z. Li, and Z. Hu, Composable text controls in latent space with odes, arXiv preprint arXiv:2208.00638, 2022. S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong, Diffuseq: Sequence to sequence text generation with diffusion models, in The Eleventh International Conference on Learning Representations. S. Dieleman, L. Sartran, A. Roshannai, N. Savinov, Y. Ganin, P. H. Richemond, A. Doucet, R. Strudel, C. Dyer, C. Durkan [51] et al., Continuous diffusion for categorical data, arXiv preprint arXiv:2211.15089, 2022. [52] Z. Gao, J. Guo, X. Tan, Y. Zhu, F. Zhang, J. Bian, and L. Xu, Empowering diffusion models on the embedding space for text generation, arXiv preprint arXiv:2212.09412, 2022. J. Lovelace, V. Kishore, C. Wan, E. Shekhtman, and K. Q. Weinberger, Latent diffusion for language generation, Advances in Neural Information Processing Systems, vol. 36, pp. 56 99857 025, 2023. [53] [54] Z. Lin, Y. Gong, Y. Shen, T. Wu, Z. Fan, C. Lin, N. Duan, and W. Chen, Text generation with diffusion language models: pre-training approach with continuous paragraph denoise, in International Conference on Machine Learning. PMLR, 2023, pp. 21 05121 064. [55] R. Wang, J. Li, and P. Li, Infodiffusion: Information entropy aware diffusion process for non-autoregressive text generation, in Findings of the Association for Computational Linguistics: EMNLP 2023, 2023, pp. 13 75713 770. [56] G. Liu, Y. Wang, Z. Feng, Q. Wu, L. Tang, Y. Gao, Z. Li, S. Cui, J. Mcauley, Z. Yang et al., Unified generation, reconstruction, and representation: Generalized diffusion with adaptive latent encoding-decoding, in International Conference on Machine Learning. PMLR, 2024, pp. 31 96431 993. [57] A. Shabalin, V. Meshchaninov, and D. Vetrov, Smoothie: Smoothing diffusion on token embeddings for text generation, arXiv preprint arXiv:2505.18853, 2025. [58] R. K. Mahabadi, H. Ivison, J. Tae, J. Henderson, I. Beltagy, M. E. Peters, and A. Cohan, Tess: Text-to-text self-conditioned simplex diffusion, in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 23472361. Ivison, S. Kumar, and A. Cohan, Tess 2: J. Tae, H. large-scale generalist diffusion language model, arXiv preprint arXiv:2502.13917, 2025. [59] [60] P. Yu, S. Xie, X. Ma, B. Jia, B. Pang, R. Gao, Y. Zhu, S.-C. Zhu, and Y. N. Wu, Latent diffusion energy-based model for interpretable text modelling, in International Conference on Machine Learning. PMLR, 2022, pp. 25 70225 720. [62] [61] L. Zheng, J. Yuan, L. Yu, and L. Kong, reparameterized discrete diffusion model for text generation, in First Conference on Language Modeling. J. Shi, K. Han, Z. Wang, A. Doucet, and M. Titsias, Simplified and generalized masked diffusion for discrete data, Advances in neural information processing systems, vol. 37, pp. 103 131103 167, 2024. S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. Chiu, A. Rush, and V. Kuleshov, Simple and effective masked diffusion language models, Advances in Neural Information Processing Systems, vol. 37, pp. 130 136130 184, 2024. J. Ye, Z. Zheng, Y. Bao, L. Qian, and Q. Gu, Diffusion language models can perform many tasks with scaling and instructionfinetuning, arXiv preprint arXiv:2308.12219, 2023. [64] [63] [65] K. Zhou, Y. Li, W. X. Zhao, and J.-R. Wen, Diffusion-nat: Selfprompting discrete diffusion for non-autoregressive text generation, in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 14381451. I. Gulrajani and T. B. Hashimoto, Likelihood-based diffusion language models, Advances in Neural Information Processing Systems, vol. 36, pp. 16 69316 715, 2023. [66] [69] [68] [67] A. Lou, C. Meng, and S. Ermon, Discrete diffusion modeling by estimating the ratios of the data distribution, in International Conference on Machine Learning. PMLR, 2024, pp. 32 81932 848. J. Ou, S. Nie, K. Xue, F. Zhu, J. Sun, Z. Li, and C. Li, Your absorbing discrete diffusion secretly models the conditional distributions of clean data, in The Thirteenth International Conference on Learning Representations, 2024. I. Gat, T. Remez, N. Shaul, F. Kreuk, R. T. Chen, G. Synnaeve, Y. Adi, and Y. Lipman, Discrete flow matching, Advances in Neural Information Processing Systems, vol. 37, pp. 133 345133 385, 2024. S. Liu, J. Nam, A. Campbell, H. Stark, Y. Xu, T. Jaakkola, and R. Gomez-Bombarelli, Think while you generate: Discrete diffusion with planned denoising, in The Thirteenth International Conference on Learning Representations, 2024. J. Ye, J. Gao, S. Gong, L. Zheng, X. Jiang, Z. Li, and L. Kong, Be- [70] [71] yond autoregression: Discrete diffusion for complex reasoning and planning, arXiv preprint arXiv:2410.14157, 2024. in Neural Information Processing Systems, vol. 37, pp. 133 282 133 304, 2024. [72] D. von utte, J. Fluri, Y. Ding, A. Orvieto, B. Sch olkopf, and T. Hofmann, Generalized interpolating discrete diffusion, in Forty-second International Conference on Machine Learning, 2025. [73] X. Liu, Z. Liu, Z. Huang, Q. Guo, Z. He, and X. Qiu, Longllada: Unlocking long context capabilities in diffusion llms, arXiv preprint arXiv:2506.14429, 2025. [74] X. Han, S. Kumar, and Y. Tsvetkov, Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 11 57511 596. [75] T. Wu, Z. Fan, X. Liu, H.-T. Zheng, Y. Gong, J. Jiao, J. Li, J. Guo, N. Duan, W. Chen et al., Ar-diffusion: Auto-regressive diffusion model for text generation, Advances in Neural Information Processing Systems, vol. 36, pp. 39 95739 974, 2023. [76] M. Arriola, A. Gokaslan, J. T. Chiu, Z. Yang, Z. Qi, J. Han, S. S. Sahoo, and V. Kuleshov, Block diffusion: Interpolating between autoregressive and diffusion language models, in The Thirteenth International Conference on Learning Representations. [78] [77] C. Huang and H. Tang, Ctrldiff: Boosting large diffusion language models with dynamic block prediction and controllable generation, arXiv preprint arXiv:2505.14455, 2025. J. K. Christopher, B. R. Bartoldson, T. Ben-Nun, M. Cardei, B. Kailkhura, and F. Fioretto, Speculative diffusion decoding: Accelerating language generation through diffusion, in Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025, pp. 12 04212 059. [79] Z. Li, H. Li, Y. Shi, A. B. Farimani, Y. Kluger, L. Yang, and P. Wang, Dual diffusion for unified image generation and understanding, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 27792790. [80] Q. Shi, J. Bai, Z. Zhao, W. Chai, K. Yu, J. Wu, S. Song, Y. Tong, X. Li, X. Li et al., Muddit: Liberating generation beyond textto-image with unified discrete diffusion model, arXiv preprint arXiv:2505.23606, 2025. J. Ye, S. Gong, L. Chen, L. Zheng, J. Gao, H. Shi, C. Wu, X. Jiang, Z. Li, W. Bi et al., Diffusion of thought: Chain-of-thought reasoning in diffusion language models, Advances in Neural Information Processing Systems, vol. 37, pp. 105 345105 374, 2024. [81] [82] Z. Huang, Z. Chen, Z. Wang, T. Li, and G.-J. Qi, Reinforcing the diffusion chain of lateral thought with diffusion language models, arXiv preprint arXiv:2505.10446, 2025. [83] O. Zekri and N. Boulle, Fine-tuning discrete diffusion models with policy gradient methods, arXiv preprint arXiv:2502.01384, 2025. Jaitly, L. Kong, S. Gong, R. Zhang, H. Zheng, and Y. Zhang, Diffucoder: Understanding and improving masked diffusion models for code generation, arXiv preprint arXiv:2506.20639, 2025. J. Gu, N. [84] [85] F. Zhu, R. Wang, S. Nie, X. Zhang, C. Wu, J. Hu, J. Zhou, J. Chen, Y. Lin, J.-R. Wen et al., Llada 1.5: Variance-reduced preference optimization for large language diffusion models, arXiv preprint arXiv:2505.19223, 2025. [86] Q. Wei, Y. Zhang, Z. Liu, D. Liu, and L. Zhang, Accelerating diffusion large language models with slowfast: The three golden principles, arXiv preprint arXiv:2506.10848, 2025. [87] P. Li, S. Yan, J. Tsai, R. Zhang, R. An, Z. Guo, and X. Gao, Adaptive classifier-free guidance via dynamic low-confidence masking, arXiv preprint arXiv:2505.20199, 2025. [88] Z. Hu, J. Meng, Y. Akhauri, M. S. Abdelfattah, J.-s. Seo, Z. Zhang, and U. Gupta, Accelerating diffusion language model inference via efficient kv caching and guided diffusion, arXiv preprint arXiv:2505.21467, 2025. [89] T. Suresh, D. Banerjee, S. Ugare, S. Misailovic, and G. Singh, Dingo: Constrained inference for diffusion llms, in ICML 2025 Workshop on Reliable and Responsible Foundation Models. [90] X. Ma, G. Fang, and X. Wang, Deepcache: Accelerating diffusion models for free, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 15 76215 772. [91] P. Chen, M. Shen, P. Ye, J. Cao, C. Tu, C.-S. Bouganis, Y. Zhao, and T. Chen, -dit: training-free acceleration method tailored for diffusion transformers, arXiv preprint arXiv:2406.01125, 2024. [92] X. Ma, G. Fang, M. Bi Mi, and X. Wang, Learning-to-cache: Accelerating diffusion transformer via layer caching, Advances [93] Z. Lv, C. Si, J. Song, Z. Yang, Y. Qiao, Z. Liu, and K.-Y. K. Wong, Fastercache: Training-free video diffusion model acceleration with high quality, in The Thirteenth International Conference on Learning Representations. S. Hayakawa, Y. Takida, M. Imaizumi, H. Wakaki, and Y. Mitsufuji, Distillation of discrete diffusion through dimensional correlations, in Forty-second International Conference on Machine Learning. [94] [96] [95] T. Salimans and J. Ho, Progressive distillation for fast sampling of diffusion models, in International Conference on Learning Representations. S. Li, K. Kallidromitis, H. Bansal, A. Gokul, Y. Kato, K. Kozuka, J. Kuen, Z. Lin, K.-W. Chang, and A. Grover, Lavida: large diffusion language model for multimodal understanding, arXiv preprint arXiv:2505.16839, 2025. J. Wang, Y. Lai, A. Li, S. Zhang, J. Sun, N. Kang, C. Wu, Z. Li, and P. Luo, Fudoki: Discrete flow-based unified understanding and generation via kinetic-optimal velocities, arXiv preprint arXiv:2505.20147, 2025. [97] [98] A. Swerdlow, M. Prabhudesai, S. Gandhi, D. Pathak, and K. Fragkiadaki, Unified multimodal discrete diffusion, arXiv preprint arXiv:2503.20853, 2025. S. Yuan, W. Yuan, H. Yin, and T. He, Roic-dm: Robust text inference and classification via diffusion model, arXiv preprint arXiv:2401.03514, 2024. [99] [100] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, Diffusionner: Boundary diffusion for named entity recognition, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 3875 3890. [101] X. Yang, Z. Qiao, and Y. Zhou, Ipad: Iterative, parallel, and diffusion-based network for scene text recognition, International Journal of Computer Vision, pp. 121, 2025. [102] S. Liu, J. Zhou, Q. Zhu, Q. Chen, Q. Bai, J. Xiao, and L. He, Lets rectify step by step: Improving aspect-based sentiment analysis with diffusion models, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 10 32410 335. [103] H. Zhang, X. Liu, and J. Zhang, Diffusum: Generation enhanced extractive summarization with diffusion, in Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 13 089 13 100. [104] X. Dong, W. Li, Y. Le, Z. Jiang, J. Zhong, and Z. Wang, Termdiffusum: term-guided diffusion model for extractive summarization of legal documents, in Proceedings of the 31st international conference on computational linguistics, 2025, pp. 32223235. [105] Y. Luo, Q. Zhou, and F. Zhou, Enhancing phrase representation by information bottleneck guided text diffusion process for keyphrase extraction, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 60366047. [106] J. Zhao, C. Xu, and B. Jiang, Iped: An implicit perspective for relational triple extraction based on diffusion model, in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2024, pp. 20802092. [107] C. H. Lee, H. Kim, J. Yeom, and S. Yoon, Editext: Controllable coarse-to-fine text editing with diffusion language models, arXiv preprint arXiv:2502.19765, 2025. [108] G. Bi, L. Shen, Y. Cao, M. Chen, Y. Xie, Z. Lin, and X. He, Diffusemp: diffusion model-based framework with multi-grained control for empathetic response generation, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 28122831. [109] G. Floto, M. M. A. Pour, P. Farinneya, Z. Tang, A. Pesaranghader, M. Bharadwaj, and S. Sanner, Diffudetox: mixed diffusion model for text detoxification, in Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 75667574. [110] Z. Horvitz, A. Patel, C. Callison-Burch, Z. Yu, and K. McKeown, Paraguide: Guided diffusion paraphrasers for plug-and-play textual style transfer, in Proceedings of the AAAI conference on artificial intelligence, vol. 38, no. 16, 2024, pp. 18 21618 224. [111] Y. Zhang, J. Gu, Z. Wu, S. Zhai, J. Susskind, and N. Jaitly, Planner: Generating diversified paragraph via latent language diffusion model, Advances in Neural Information Processing Systems, vol. 36, pp. 80 17880 190, 2023. bustly optimized bert pretraining approach, arXiv preprint arXiv:1907.11692, 2019. 22 [112] J. Liu, P. Cheng, J. Dai, and J. Liu, Diffucom: novel diffusion model for comment generation, Knowledge-Based Systems, vol. 281, p. 111069, 2023. [113] J. Xiang, Z. Liu, H. Liu, Y. Bai, J. Cheng, and W. Chen, Diffusiondialog: diffusion model for diverse dialog generation with latent space, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 49124921. [114] W. Zou, Z. Zhuang, X. Geng, S. Huang, J. Liu, and J. Chen, Improved paraphrase generation via controllable latent diffusion, arXiv preprint arXiv:2404.08938, 2024. [115] Z. Hu, C. Liu, Y. Feng, A. T. Luu, and B. Hooi, Poetrydiffusion: Towards joint semantic and metrical manipulation in poetry generation, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 18 27918 288. [116] L. Chen, A. Feng, B. Yang, and Z. Li, Xdlm: Cross-lingual diffusion language model for machine translation, arXiv preprint arXiv:2307.13560, 2023. [117] S. Qiao, X. Liu, and S.-H. Na, Diffusionret: Diffusion-enhanced generative retriever using constrained decoding, in Findings of the Association for Computational Linguistics: EMNLP 2023, 2023, pp. 95159529. [118] K. Yan, M. Liu, Y. Liu, R. Fu, Z. Wen, J. Tao, and X. Liu, fake news detection via Debunk and infer: Multimodal diffusion-generated evidence and llm reasoning, arXiv preprint arXiv:2506.21557, 2025. [119] O. Luxembourg, H. Permuter, and E. Nachmani, Plan for speed dilated scheduling for masked diffusion language models, arXiv preprint arXiv:2506.19037, 2025. [120] Y. Xiong, K. Li, J. Chen, H. Zhang, D. Lin, Y. Che, and W. Hu, Text-guided multi-property molecular optimization with diffusion language model, arXiv preprint arXiv:2410.13597, 2024. [121] H. Gong, Q. Liu, S. Wu, and L. Wang, Text-guided molecule generation with diffusion language model, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 1, 2024, pp. 109117. [122] S. Goel, V. Thoutam, E. M. Marroquin, A. Gokaslan, A. Firouzbakht, S. Vincoff, V. Kuleshov, H. T. Kratochvil, and P. Chatterjee, Memdlm: De novo membrane protein design with masked discrete diffusion protein language models, in NeurIPS 2024 Workshop on AI for New Drug Modalities. [123] X. Wang, Z. Zheng, D. Xue, S. Huang, Q. Gu et al., Diffusion language models are versatile protein learners, in Forty-first International Conference on Machine Learning. [124] J. Yin, C. Zha, W. He, C. Xu, and X. Gao, Cfp-gen: Combinatorial functional protein generation via diffusion language models, in Forty-second International Conference on Machine Learning. [125] C. Wang, M. Uehara, Y. He, A. Wang, A. Lal, T. Jaakkola, S. Levine, A. Regev, T. Biancalani et al., Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design, in The Thirteenth International Conference on Learning Representations. [126] B. Ni, D. L. Kaplan, and M. J. Buehler, Forcegen: End-toend de novo protein generation based on nonlinear mechanical unfolding responses using language diffusion model, Science Advances, vol. 10, no. 6, p. eadl4000, 2024. [127] L. Hallee, N. Rafailidis, D. B. Bichara, and J. P. Gleghorn, Diffusion sequence models for enhanced protein representation and generation, arXiv preprint arXiv:2506.08293, 2025. [128] X. Wang, Z. Zheng, F. Ye, D. Xue, S. Huang, and Q. Gu, Dplm-2: multimodal diffusion protein language model, arXiv preprint arXiv:2410.13782, 2024. [129] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, Deep unsupervised learning using nonequilibrium thermodynamics, in International conference on machine learning. pmlr, 2015, pp. 22562265. [130] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pretraining of deep bidirectional transformers for language understanding, in Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 2019, pp. 41714186. [131] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Roberta: ro- [132] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, Albert: lite bert for self-supervised learning of language representations, in International Conference on Learning Representations. [133] P. He, X. Liu, J. Gao, and W. Chen, Deberta: Decoding-enhanced bert with disentangled attention, in International Conference on Learning Representations. [134] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, Transformer-xl: Attentive language models beyond fixedlength context, arXiv preprint arXiv:1901.02860, 2019. [135] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., Opt: Open pre-trained transformer language models, arXiv preprint arXiv:2205.01068, 2022. [136] F. Gloeckle, B. Y. Idrissi, B. Roziere, D. Lopez-Paz, and G. Synnaeve, Better & faster large language models via multi-token prediction, in Forty-first International Conference on Machine Learning. [137] I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence learning with neural networks, Advances in neural information processing systems, vol. 27, 2014. [138] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of machine learning research, vol. 21, no. 140, pp. 167, 2020. [139] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 78717880. [140] H. Yuan, Z. Yuan, C. Tan, F. Huang, and S. Huang, Seqdiffuseq: Text diffusion with encoder-decoder transformers, arXiv preprint arXiv:2212.10325, 2022. [141] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, Xlnet: Generalized autoregressive pretraining for language understanding, Advances in neural information processing systems, vol. 32, 2019. [142] Q. Team, Qwen2.5: party of foundation models, [Online]. Available: https://qwenlm.github. September 2024. io/blog/qwen2.5/ [143] X. Zhu, G. Karadzhov, C. Whitehouse, and A. Vlachos, Segment-level diffusion: framework for controllable longform generation with diffusion language models, arXiv preprint arXiv:2412.11333, 2024. [144] Y. Zihuiwen, Y. Elle Michelle, and B. Phil, Latent diffusion for document generation with sequential decoding, in NeurIPS 2023 Workshop on Diffusion Models, 2023. [Online]. Available: https://neurips.cc/virtual/2023/74876 [145] J. Bai, T. Ye, W. Chow, E. Song, Q.-G. Chen, X. Li, Z. Dong, L. Zhu, and S. Yan, Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis, in The Thirteenth International Conference on Learning Representations, 2024. [146] M. Asada and M. Miwa, Addressing the training-inference discrepancy in discrete diffusion for text generation, in Proceedings of the 31st International Conference on Computational Linguistics, 2025, pp. 71567164. [147] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [148] A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach, Adversarial diffusion distillation, in European Conference on Computer Vision. Springer, 2024, pp. 87103. [149] A. Sauer, F. Boesel, T. Dockhorn, A. Blattmann, P. Esser, and R. Rombach, Fast high-resolution image synthesis with latent adversarial diffusion distillation, in SIGGRAPH Asia 2024 Conference Papers, 2024, pp. 111. [150] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, information processing systems, vol. 36, pp. Advances in neural 34 89234 916, 2023. [151] F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. Ma, and C. Li, Llava-interleave: Tackling multi-image, video, and 3d in large multimodal models, in The Thirteenth International Conference on Learning Representations. 23 State-of-the-art natural language processing, in Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 2020, pp. 3845. [172] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica, Efficient memory management for large language model serving with pagedattention, in Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [173] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. [174] K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen et al., Kimi k2: Open agentic intelligence, arXiv preprint arXiv:2507.20534, 2025. [152] J. Guo, T. Zheng, Y. Bai, B. Li, Y. Wang, K. Zhu, Y. Li, G. Neubig, W. Chen, and X. Yue, Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale, arXiv preprint arXiv:2412.05237, 2024. [153] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. AlDahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [154] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [155] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z. Shou, Show-o: One single transformer to unify multimodal understanding and generation, in The Thirteenth International Conference on Learning Representations. [156] S. Kou, J. Jin, Z. Liu, C. Liu, Y. Ma, J. Jia, Q. Chen, P. Jiang, and Z. Deng, Orthus: Autoregressive interleaved imagetext generation with modality-specific heads, arXiv preprint arXiv:2412.00127, 2024. [157] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan et al., Janus: Decoupling visual encoding for unified multimodal understanding and generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 12 96612 977. [158] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al., Piqa: Reasoning about physical commonsense in natural language, in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 74327439. [159] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, Hellaswag: Can machine really finish your sentence? in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 47914800. [160] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374, 2021. [161] D. Ghosh, H. Hajishirzi, and L. Schmidt, Geneval: An objectfocused framework for evaluating text-to-image alignment, Advances in Neural Information Processing Systems, vol. 36, pp. 52 132 52 152, 2023. [162] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun et al., Mme: comprehensive evaluation benchmark for multimodal large language models, arXiv preprint arXiv:2306.13394, 2023. [163] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al., Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 95569567. [164] D. A. Hudson and C. D. Manning, Gqa: new dataset for realworld visual reasoning and compositional question answering, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 67006709. [165] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. Qwen2 arXiv:2407.10671, 2024. technical [166] Q. report, preprint Team, arXiv [167] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, Gpqa: graduate-level googleproof q&a benchmark, in First Conference on Language Modeling, 2024. [168] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, Measuring mathematical problem solving with the math dataset, in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). [169] Y. Lyu, T. Luo, J. Shi, T. C. Hollon, and H. Lee, Fine-grained text style transfer with diffusion-based language models, arXiv preprint arXiv:2305.19512, 2023. [170] Y. Demirag, D. Liu, and J. Niehues, Benchmarking diffusion models for machine translation, in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, 2024, pp. 313324. [171] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., Transformers:"
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University",
        "VILA Lab, Mohamed bin Zayed University of Artificial Intelligence"
    ]
}