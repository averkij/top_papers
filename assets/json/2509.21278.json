{
    "paper_title": "Does FLUX Already Know How to Perform Physically Plausible Image Composition?",
    "authors": [
        "Shilin Lu",
        "Zhuming Lian",
        "Zihan Zhou",
        "Shaocong Zhang",
        "Chen Zhao",
        "Adams Wai-Kin Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 8 7 2 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DOES FLUX ALREADY KNOW HOW TO PERFORM PHYSICALLY PLAUSIBLE IMAGE COMPOSITION? Shilin Lu1,, Zhuming Lian1,, Zihan Zhou1, Shaocong Zhang1, Chen Zhao2, Adams Wai-Kin Kong1 1Nanyang Technological University, 2Nanjing University {shilin002, zhuming001, zihan010, shaocong001}@e.ntu.edu.sg 602024710020@smail.nju.edu.cn, adamskong@ntu.edu.sg Figure 1: Showcase of our training-free image composition method, SHINE. This gallery highlights SHINEs ability to seamlessly integrate subjects into complex scenes, including low-light conditions, intricate shadows, and water reflections."
        },
        {
            "title": "ABSTRACT",
            "content": "Image composition aims to seamlessly insert user-specified object into new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background Equal Contribution."
        },
        {
            "title": "Preprint",
            "content": "blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication."
        },
        {
            "title": "INTRODUCTION",
            "content": "Image composition, which places userspecified object into new scene, is demanding image editing task. Despite the breathtaking progress of multimodal foundation models (e.g., GPT-5 (OpenAI, 2025), Gemini-2.5 (Gemini2.5, 2025), SeedEdit/Doubao (Shi et al., 2024b), and Grok-4 (gro, 2025)), these generic models still struggle with image composition. Typical failures include imprecise object placement, inconsistent lighting, and the subjects identity drift (see Fig. 2). These limitations indicate that, as of now, massive multimodal pre-training alone has not yet endowed them with sufficient compositional ability for this task. natural response has been to train specialized models. Yet building large-scale, high-quality, multi-resolution triplet datasets (object, scene, composite) is prohibitively costly. As result, most composition models are fine-tuned from base models (e.g., FLUX.1-dev (Black Forest Labs, 2024a), FLUX.1-Fill (Black Forest Labs, 2024c), SDXL (Podell et al., 2024)) on synthetic data generated via inpainting or augmentations (Chen et al., 2024c; Yang et al., 2023; Song et al., 2023; Wang et al., 2025; He et al., 2024). These models, however, face two main limitations (see Fig. 6): (i) Lighting realism. They struggle to achieve natural composition under complex lighting conditions, such as accurate shadow generation or water reflections for the inserted subject. (ii) Resolution rigidity. They are tied to fixed resolution, necessitating downsampling or cropping when applied to varied, high-resolution background images, which degrades generation quality. Notably, such issues are absent in the base models, implying that the underlying physical priors are present but are not effectively exploited by fine-tuned variants. The degradation largely stems from low-quality synthetic datasets, which inherit flaws from inpainting models that often mis-handle shadows and reflections, producing implausible edits, hallucinated content, or incomplete object removal (Yu et al., 2025b; Winter et al., 2024). There have been prior training-free attempts to exploit the priors of text-to-image (T2I) models for advancing image composition, but they fall short for two main reasons. (i) Inversion bottlenecks. Most methods (Lu et al., 2023d; Pham et al., 2024; Yan et al., 2025; Li et al., 2024b) depend on accurate image inversion (Song et al., 2020; Lu et al., 2022; Mokady et al., 2023). In practice, inversion constrains the inserted object to the pose of its reference image, often resulting in contextually inappropriate orientations. Moreover, inversion is less effective for classifier-free guidance (CFG) distilled models (e.g., FLUX), where elevated inversion errors degrade identity preservation. (ii) Fragile attention surgery. Many training-free approaches rely on attention manipulation (Lu et al., 2023d; Yan et al., 2025; Li et al., 2024b). While compatible with the joint self-attention in Multimodal Diffusion Transformers (MMDiT) (Peebles & Xie, 2023), these methods inherit the instability and hyperparameter sensitivity (Lu et al., 2023d), limiting their robustness. To bridge these gaps we present SHINE, training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors (see Fig. 1). SHINE comprises three innovations: (i) ManifoldSteered Anchor (MSA) loss, which leverages pretrained open-domain customization adapters (e.g., IP-Adapter (Ye et al., 2023b)) to steer noisy latents toward faithfully representing the reference subject while preserving the structural integrity of the background. (ii) Degradation-Suppression Guidance (DSG) that steers sampling away from low-quality distributions. (iii) Adaptive Background Blending (ABB) that eliminates visible seams along mask boundaries. Existing benchmarks primarily comprise background images with fixed resolution of 512 512 pixels. To evaluate performance across diverse, high-resolution, and demanding scenarios, we introduce ComplexCompo, benchmark that includes varied resolutions, both landscape and portrait orientations, and complex conditions such as low lighting, intense illumination, intricate shadows, and water reflections. Extensive experiments on ComplexCompo and DreamEditBench (Li et al., 2023b) demonstrate that SHINE achieves state-of-the-art (SOTA) performance, surpassing base-"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Image composition from advanced multimodal models under three challenging conditions: backlighting, shadows, and water surfaces. Refer to Appendix for prompt details. lines on standard metrics (e.g., DINOv2 (Oquab et al., 2023)) and human-aligned metrics (e.g., DreamSim (Fu et al., 2023), ImageReward (Xu et al., 2023), VisionReward (Xu et al., 2024))."
        },
        {
            "title": "2 RELATED WORK",
            "content": "This section reviews prior work on image composition. more comprehensive discussion, covering image composition, general image editing, and subject-driven generation, is offered in Appendix A. Classical image composition splits into sub-tasks (Niu et al., 2021) such as object placement (Azadi et al., 2020; Zhang et al., 2020a), blending (Wu et al., 2019; Zhang et al., 2020b), harmonization (Cao et al., 2023; Lu et al., 2023b), and shadow generation (Hong et al., 2022; Sheng et al., 2021), typically handled by separate models. Diffusion models have shifted the field toward unified frameworks, either training-based or training-free. Training-based approaches fine-tune diffusion models with curated datasets, adding grounding layers, controllability signals, or identity-preserving supervision from image or video sets (Wang et al., 2025; Chen et al., 2024c; Yang et al., 2023; Song et al., 2023; Lu et al., 2023c). However, they often bias model priors and struggle with complex lighting due to the lack of large-scale real-world triplets. Training-free approaches avoid retraining by manipulating inversion and attention during inference, enabling flexible test-time adaptation (Yan et al., 2025; Li et al., 2024b; 2023b; Lu et al., 2023d; Pham et al., 2024). Yet these methods remain fragile: strong injections preserve identity but fix unnatural poses, while weaker ones improve realism at the cost of fidelity, reflecting core trade-off between identity preservation and natural composition."
        },
        {
            "title": "3 METHOD",
            "content": "Image composition seeks to integrate subject into designated area of background image while preserving the integrity of the surrounding scene. This process typically requires three inputs: (1) one or more reference images of the subject {xsubj , xsubj }, (2) background image xbg, and 2 1 (3) user-provided mask user specifying the insertion region within the background. , . . . , xsubj Our framework is built on three core components: Manifold-Steered Anchor (MSA) loss, Degradation-Suppression Guidance (DSG), and Adaptive Background Blending (ABB). Importantly, the design is model-agnostic and requires only standard features of modern generative models: MSA loss assumes that the base model supports either personalization finetuning or provides access to pretrained personalization adapter, DSG uses self-attention maps, and ABB relies on textimage cross-attention. These mild assumptions enable seamless integration into existing pipelines without architectural changes. We present main results with FLUX, while additional experiments on SDXL (Podell et al., 2024), SD3.5 (Esser et al., 2024), and PixArt (Chen et al., 2023) are provided in Appendix E. The complete algorithm is shown in Algorithm 1. 3.1 NON-INVERSION LATENT PREPARATION In training-free diffusion-based image composition (Lu et al., 2023d; Pham et al., 2024; Yan et al., 2025; Li et al., 2024b), it is common to start from noisy latent. Existing training-free frameworks"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Overview of the proposed framework. (a) The noisy latent is created by inpainting the background with VLM-derived object description, then adding Gaussian noise. (b) ManifoldSteered Anchor (MSA) loss guides noisy latents toward faithfully capturing the reference subject (red arrow), while preserving the structural integrity of the background. Concretely, it enforces that the prediction of the optimized latent on the adapter-augmented models manifold remains close to the prediction of the original latent zt on the base models manifold. (c) Degradation-Suppression Guidance (DSG) constructs negative velocity pointing toward low-quality regions by blurring Qimg and, in CFG-like manner, steers the trajectory away from this low-quality distribution. typically rely on image inversion, where the initial noisy latent is constructed by copying the inverted latent of the subject image into designated region of the background images inverted latent. However, this copy-paste strategy constrains the inserted object to the exact pose of its reference image, often leading to contextually inappropriate orientations in the composed result. Moreover, inversion is suboptimal for CFG-distilled models (e.g., FLUX), as it introduces higher inversion errors that compromise subject identity preservation. To address these limitations, we abandon inversion and instead perform one-step forward diffusion to obtain the noisy latent. As illustrated in Fig. 3(a), we use vision-language model (VLM) (Xue et al., 2024; Chen et al., 2024d; Liu et al., 2024) to caption the subject image and leverage this caption, along with an image inpainting model (Li et al., 2024c; Ju et al., 2024; Zhuang et al., 2024; Black Forest Labs, 2024b), to generate the inpainted background image, denoted as xinit. The noisy latent is encoded in the VAE space as zinit and perturbed to timestep via one-step forward diffusion, following the flow matching formulation: zt = (1 σt)zinit + σtϵ, where ϵ (0, 1). 3.2 MANIFOLD-STEERED ANCHOR LOSS The Manifold-Steered Anchor (MSA) loss is designed to optimize the noisy latent zt (from Sec. 3.1) during the denoising process, steering it toward reference subject while preserving the structural integrity of the original image. The key intuition is to leverage the prior knowledge embedded in pretrained open-domain customization adapters (or alternatively, personalized LoRAs) such as IPAdapter (Ye et al., 2023b), PuLID (Guo et al., 2024), and InstantCharacter (Tao et al., 2025), to intervene directly in the diffusion trajectory. Specifically, the MSA loss is defined as: min zt LMSA(zt) = (cid:13) (cid:13) 2 (cid:13)vθ+θ(zt, t, c, zsubj) sg[vt] (cid:13) (cid:13) (cid:13) 2 , (1) where vt vθ( zt, t, c) serves as fixed anchor, preserving the structure of the background image at given noise level t, with zt held constant as the original noisy latent. vθ() denotes the velocity predicted by the frozen T2I model θ, while vθ+θ() represents the velocity predicted by the T2I"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Left: Robustness of FLUX. Right: Impacts of blurring different features in FLUX. model augmented with an adapter θ. zsubj is the latent of the subject image. The text prompt is from the VLMs description of xinit, and sg[] indicates the stop-gradient operation. The MSA loss is motivated by the observation that optimizing latent representation against frozen generative model implicitly projects the latent onto the models learned data manifold (Meng et al., 2021; Kim et al., 2022; Graikos et al., 2022; Feng et al., 2023). The generator serves as an implicit prior, guiding gradient descent toward the manifolds basin of attraction. For instance, when generative model G(w) is trained solely on cat images, its outputs are confined to the cat-image manifold. Thus, approximating dog image xdog by solving minw G(w)xdog2 2 yields G(w) that remains cat image, but with structural features (e.g., pose or outline) aligned to xdog. The result is the projection of the dog image onto the cat manifold, not genuine dog image. Analogously, MSA loss is designed to achieve two goals simultaneously. (1) It seeks an optimized noisy latent that remains within the manifold of the adapter-augmented model when conditioned on the subject zsubj. (2) It encourages the adapters prediction on this latent to align with the base models prediction on the original latent zt, i.e., vθ+θ(z , t, c, zsubj) vθ(zt, t, c) (see Fig. 3(b)). Since the velocity prediction of T2I model on noisy latent zt can also be interpreted as coarse estimate of the clean image that encodes essential structural information (Zheng et al., 2023), this alignment preserves the spatial layout and background details inherited from the original image. The gradient of LMSA with respect to zt is: ztLMSA(zt) = 2 (cid:16) vθ+θ(zt, t, c, zsubj) sg[vt] (cid:17) vθ+θ(zt, t, c) zt . (2) The Jacobian term necessitates backpropagation through the MMDiT, which is computationally expensive. However, this scenario is analogous to Score Distillation Sampling (SDS) (Poole et al., 2022), where research shows that omitting the Jacobian term yields an effective gradient for optimization with diffusion models. Thus, we adopt the same strategy for optimization. 3.3 DEGRADATION-SUPPRESSION GUIDANCE MSA loss effectively facilitates the insertion of reference objects. However, due to the inherent stochasticity of the denoising and optimization process, the results sometimes suffer from degraded visual quality, manifesting as oversaturated colors and reduced identity consistency (see Fig. 5). To address this, we introduce Degradation-Suppression Guidance (DSG), inspired by negative prompting (Schramowski et al., 2023), defined as: θ+θ(zt, t, c, zsubj)(cid:1), = vθ+θ(zt, t, c, zsubj) + η(cid:0)vθ+θ(zt, t, c, zsubj) vneg vdsg where vneg θ+θ denotes negative velocity prediction that guides the generation toward low-quality regions. key challenge is the design of meaningful negative velocity prediction vneg θ+θ within MMDiT-based architectures. In our experiments with FLUX, we observed that using nonsensical text prompts or explicit negative prompts fails to introduce degradation. The generated images remain high-fidelity (Fig. 4(a)), suggesting that text-based negative prompting is ineffective for FLUX. In our setting, the ideal negative velocity vneg θ+θ should target directions that preserve the semantic content and spatial layout while lowering perceptual quality. To achieve this, we investigate whether we can manipulate FLUXs internal representations to construct such targeted degradation signal. (3)"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Comparison of rectangular-mask blending and Adaptive Background Blending (ABB). Boundary regions (pink dashed boxes) are enlarged for clarity. Zoom in for details. In FLUX, both multi-stream and single-stream blocks compute joint self-attention over concatenated text and image tokens as follows: (cid:16) (cid:17) = softmax [Qtxt, Qimg][Ktxt, Kimg]T/ dk [Vtxt, Vimg], (4) (cid:112) where [Qtxt, Qimg] represents the concatenation of text and image queries, and similarly for keys and values. To identify an effective manipulation strategy, we systematically perturb different components in the attention mechanism (i.e., Qtxt, Ktxt, Vtxt, Qimg, Kimg and Vimg) and observe their impact on generation quality. As shown in Fig. 4(b), our findings are as follows: 1. Blurring Qtxt, Ktxt, or Vtxt has negligible impact on semantic fidelity and visual quality. 2. Blurring Vimg severely disrupts the output distribution, leading to unintelligible images. 3. Blurring Kimg moderately impacts quality, while the image remains visually acceptable. 4. Blurring Qimg yields pronounced degradations while preserving structural integrity, making it the most effective lever for constructing negative velocity. Based on these insights, we construct the negative velocity prediction vneg θ+θ in Eqn. 3 by blurring Qimg within FLUX (see Fig. 3(c)). Moreover, we show that blurring Qimg is mathematically equivalent to blurring the self-attention weights, whereas blurring Kimg or Vimg is not (see Appendix for the proof). This equivalence is consistent with the fact that attenuating self-attention activations suppresses informative interactions and thus degrades image quality (Lu et al., 2024a). 3.4 ADAPTIVE BACKGROUND BLENDING Previous methods typically rely on the user-provided mask user to preserve the background during each denoising step, blending as zt = user zt + (1 user) zbg , but this often introduces visible seams along mask boundaries (see the first row of Fig. 5). To address this limitation, we propose Adaptive Background Blending (ABB), defined as = ˆM zt + (cid:16) 1 ˆM (cid:17) zbg , ˆM = 1{t > τ } D(M attn) + 1{t τ } user, (5) where user is the user mask, while attn is derived by binarizing the cross-attention maps corresponding to subject tokens. These maps can be obtained by either averaging across layers or selecting the most informative layer via lightweight analysis (details in Appendix D). The operator D() performs dilation and extracts the largest connected component, ensuring robustness to noise. Compared to user, attn is more spatially precise, particularly for elongated or irregularly shaped objects that do not fully occupy rectangular region. As illustrated in the second row of Fig. 5, our method produces smoother transitions by replacing the rigid user mask with the semantically guided mask. This refinement better preserves the surrounding scene, enabling seamless integration between generated content and the original background. However, applying this method throughout"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 The Image Composition Process of SHINE. Input: background latent zbg, inpainted latent zinit, subject latent zsubj, user mask user. Output: The composition latent z0. (cid:13)vθ+θ(zt, t, c, zsubj) sg[vt](cid:13) (cid:13) 2 (cid:13) 2 end for // Manifold-Steered Anchor (MSA) Optimization if > τ then 1: zt1 (1 σt1 )zinit + σt1ϵ, where ϵ (0, 1) 2: for = t1, . . . , 0 do 3: 4: 5: 6: 7: 8: 9: 10: 11: vt vθ(zt, t, c) for = 1, . . . , do zt zt α user zt end if // Degradation-Suppression Guidance (DSG) vt, At vθ+θ(zt, t, c, zsubj) vt + η (cid:0)vt vneg vdsg zt1 zt + (σt1 σt)vdsg zbg t1 (1 σt1)zbg + σt1ϵ, where ϵ (0, 1) // Adaptive Background Blending (ABB) 13: 14: 15: 16: attn MaxConnectedComponent(cid:0)Dilate(1(At γ))(cid:1) 17: θ+θ(zt, t, c, zsubj)(cid:1) 12: ˆM 1{t > τ } attn + 1{t τ } user zt1 ˆM zt1 + (cid:16) 1 ˆM zbg t1 (cid:17) 18: 19: end for 20: return z0 the denoising process may truncate object shadows or reflections. Through empirical evaluation, we find that leveraging attn during the initial denoising steps (t > τ ) sufficiently mitigates visible seams along mask boundaries, ensuring high-fidelity scene coherence."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Benchmark. Current benchmarks primarily consist of background images with fixed resolution of 512 512 pixels. To assess performance across diverse, high-resolution, and complex scenarios, we introduce ComplexCompo, benchmark built upon DreamEditBench (Li et al., 2023b). DreamEditBench includes 220 (subject, background, bounding box) pairs designed for 512 512 resolution. In contrast, ComplexCompo features 300 composition pairs with varying resolutions, encompassing both landscape and portrait orientations, and incorporates challenging conditions such as low lighting, intense illumination, intricate shadows, and water reflections. The background images are sourced from OpenImage (Kuznetsova et al., 2020). Further details are provided in Appendix G. Metrics. Previous methods primarily adapt CLIP-I (Radford et al., 2021) and DINOv2 (Oquab et al., 2023) to assess subject identity consistency. However, these features capture high-level semantic information that may not fully align with human perception of finer-grained attributes. Thus, we further incorporate instance retrieval features (IRF) from (Shao & Cui, 2022) and DreamSim (Fu et al., 2023), which better align with human judgments. An analysis of identity consistency metrics is provided in Appendix I. To evaluate overall image quality, we use ImageReward (IR) (Xu et al., 2023) and VisionReward (VR) (Xu et al., 2024), which are fine-grained, multi-dimensional reward models that more accurately reflect human preferences. For background consistency, LPIPS (Zhang et al., 2018) and SSIM (Wang et al., 2004) are employed. Implementation Details. In our experiment, we used FLUX.1-dev, 12B-parameter flow matching model, as the base model, combined with InstantCharacter (Tao et al., 2025) as the adapter (Additional results on SDXL, SD3.5, and PixArt are presented in Appendix E). Our approach also supports per-concept LoRA (Hu et al., 2021a), which requires test-time tuning (Ruiz et al., 2023b)"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Qualitative comparison of our method with multiple baselines in challenging scenarios, drawn from our benchmark dataset. More qualitative comparisons are available in Appendix K. but delivers superior identity consistency compared to an open-domain adapter, making it ideal for scenarios demanding precise identity preservation. The denoising process consists of 20 steps, with the inpainted image perturbed to timestep 15 and denoising initiated from that point. Additional details and hyperparameters are provided in Appendix J. 4.2 EXPERIMENTAL RESULTS We compare our method with two categories of baselines: (1) Training-based methods (6 in total): UniCombine (Wang et al., 2025), AnyDoor (Chen et al., 2024c), Paint by Example (PBE) (Yang et al., 2023), ObjectStitch (Song et al., 2023), MADD (He et al., 2024), and DreamCom (Lu et al., 2023c); (2) Training-free methods (5 in total): EEdit (Yan et al., 2025), TIGIC (Li et al., 2024b), DreamEdit (Li et al., 2023b), TF-ICON (Lu et al., 2023d), and TALE (Pham et al., 2024). As shown in Tab. 1, both variants of our method surpass all baselines on DreamEditBench across human preference aligned metrics (i.e., DreamSim, IR, VR), which are the most critical indicators of quality. For background-related metrics, all methods achieve comparable results, with differences so small they are imperceptible to the human eye. On the more challenging ComplexCompo dataset, which includes non-square resolutions and intricate scenes, most methods experience notable performance drop, yet our approach consistently remains the top performer. From Fig. 6, it is evident that while AnyDoor achieves high scores on many identity metrics, the model tends to copy and paste the subject into the scene, resulting in unnatural compositions and lower image quality scores. In contrast, our method excels at naturally composing objects in challenging conditions (e.g., lowlight settings, water surfaces, and scenes with complex shadows). Appendix provides user study."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison of composition performance across two benchmarks. The best result in each column is highlighted in bold, while the second-best is underlined. Metrics shown in pink are those specifically trained to better align with human preferences. Abbreviations: IRF: Instance Retrieval Features; IR = ImageReward; VR = VisionReward. Bench Method Training -Free DreamEditBench (220) ComplexCompo (300) MADD (He et al., 2024) ObjectStitch (Song et al., 2023) DreamCom (Lu et al., 2023c) AnyDoor (Chen et al., 2024c) UniCombine (Wang et al., 2025) PBE (Yang et al., 2023) TIGIC (Li et al., 2024b) TALE (Pham et al., 2024) TF-ICON (Lu et al., 2023d) DreamEdit (Li et al., 2023b) EEdit (Yan et al., 2025) Ours-Adapter Ours-LoRA MADD (He et al., 2024) ObjectStitch (Song et al., 2023) DreamCom (Lu et al., 2023c) AnyDoor (Chen et al., 2024c) UniCombine (Wang et al., 2025) PBE (Yang et al., 2023) TIGIC (Li et al., 2024b) TALE (Pham et al., 2024) TF-ICON (Lu et al., 2023d) DreamEdit (Li et al., 2023b) EEdit (Yan et al., 2025) Ours-Adapter Ours-LoRA Base Model SD SD SD SD FLUX SD SD SD SD SD FLUX FLUX FLUX SD SD SD SD FLUX SD SD SD SD SD FLUX FLUX FLUX External Model DINO VIT LoRA DINO LoRA - - - - LoRA, VIT - Adapter LoRA DINO VIT LoRA DINO LoRA - - - - LoRA, VIT - Adapter LoRA Subject Identity Consistency Background Image Quality CLIP-I DINOv2 IRF DreamSim LPIPS SSIM IR VR 0.7118 0.7567 0.7414 0.8183 0.8058 0.7742 0.7226 0.7329 0.7479 0.7703 0.6998 0.8086 0.8125 0.6780 0.7608 0.648 0.7982 0.7361 0.7537 0.6913 0.6816 0.6987 0.7314 0.6713 0.7721 0.7999 0.6279 0.6930 0.6749 0.7283 0.7332 0.7040 0.6718 0.6604 0.6865 0.7151 0.6590 0.7415 0.7452 0.5993 0.7077 0.5692 0.7052 0.6552 0.6802 0.6329 0.6151 0.6435 0.6722 0.6153 0.7107 0.7384 0.4333 0.5525 0.5597 0.7714 0.7579 0.5845 0.4711 0.5007 0.5179 0.6147 0.4438 0.7702 0.7900 0.3638 0.5513 0.2788 0.7319 0.5380 0.5189 0.3848 0.3799 0.4167 0.5069 0.3797 0.6764 0. 0.5810 0.5093 0.5626 0.3764 0.3984 0.4985 0.6108 0.6176 0.5441 0.5047 0.6160 0.3730 0.3577 0.5979 0.4717 0.8192 0.4493 0.5682 0.5187 0.6549 0.6773 0.6030 0.5670 0.6821 0.4294 0.3542 0.0604 0.0190 0.0200 0.0251 0.0050 0.0197 0.0584 0.0392 0.0582 0.0140 0.0039 0.0236 0.0271 0.0781 0.0388 0.0389 0.0299 0.0237 0.0397 0.0929 0.059 0.0815 0.0468 0.0226 0.0404 0.0430 0.8182 0.8316 0.8283 0.8894 0.9397 0.8287 0.8153 0.8251 0.8111 0.9775 0.9475 0.8959 0.8847 0.5658 0.6357 0.6342 0.7262 0.7077 0.6321 0.6228 0.6334 0.6216 0.7201 0.7107 0.7789 0. -0.2545 0.0791 0.1873 0.4511 0.4565 0.2083 -0.1332 -0.1502 0.0816 0.1744 0.0216 0.5709 0.5906 -0.0088 0.2482 -0.0778 0.3804 0.2470 0.2139 -0.131 0.0783 0.1798 0.1212 0.1433 0.4090 0.4246 2.7011 3.2416 3.5053 3.3946 3.6108 3.3482 2.9873 3.1349 3.2823 3.1775 3.3606 3.6234 3.6161 2.6582 3.4411 3.4409 3.3787 3.5454 3.4310 2.8898 3.4498 3.4323 3.2531 3.5009 3.6020 3.5951 Figure 7: Qualitative ablation study comparing different variants of our framework. 4.3 ABLATION STUDY We validate our design choices through ablation (see Tab. 2 and Fig. 7). The results highlight three key insights. First, MSA loss notably improves subject identity consistency. Second, DSG boosts IR and VR scores by steering denoising away from low-quality regions. Finally, ABB effectively suppresses visible seams along mask boundaries. While this improvement is readily apparent in visual comparisons (Figs. 5, 7), it is less well captured by quantitative metrics, since LPIPS and SSIM primarily assess structural similarity rather than perceptual smoothness. Table 2: Ablation study examining the impact of key components on DreamEditBench. Method MSA DSG ABB Subject Identity Consistency Background Image Quality Config Config Config Config Config Config Config Ours-Adapter CLIP-I DINOv2 IRF DreamSim LPIPS SSIM IR VR 0.7328 0.7814 0.7528 0.7421 0.7481 0.8084 0.8077 0.8086 0.6745 0.7204 0.6941 0.6814 0.6987 0.7429 0.7375 0. 0.5754 0.7414 0.6533 0.6158 0.6647 0.7609 0.7589 0.7702 0.5233 0.3951 0.4436 0.5127 0.4317 0.3756 0.3762 0.3730 0.0166 0.0172 0.0178 0.0210 0.0218 0.0231 0.0182 0.0236 0.9076 0.9075 0.9038 0.9010 0.8971 0.8991 0.9037 0.8959 0.5577 0.5455 0.5633 0.5595 0.5850 0.5459 0.5745 0.5709 3.5997 3.5952 3.6130 3.6109 3.6277 3.6023 3.6191 3."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Composition inherit erroneous colors if the inpainting prompt specifies an incorrect color."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced SHINE, training-free framework for seamless and high-fidelity image composition with pretrained T2I models. SHINE integrates Manifold-Steered Anchor Loss, DegradationSuppression Guidance, and Adaptive Background Blending to ensure precise subject placement and artifact-free synthesis across diverse resolutions and lighting conditions. To enable rigorous evaluation, we proposed ComplexCompo, benchmark for challenging composition scenarios. SHINE achieves state-of-the-art results on both ComplexCompo and DreamEditBench. Limitations. Our method reliably converges to the correct subject identity through MSA optimization even when the inpainted subjects appearance deviates substantially from the reference (see Fig. 8(a)). However, when the inpainting prompt specifies an incorrect color, the final inpainted result tends to inherit and preserve this erroneous color (see Fig. 8(b)). On the other hand, the similarity between the inserted object and the user-provided object depends on the quality of the customization adapter used. As shown in Tab. 1, because LoRA performs test-time tuning for individual concepts, it generates subjects that are more similar to the target than those produced by pretrained open-domain customization adapters, resulting in higher subject identity consistency metrics in the composition. While current customization adapters already perform well, the potential of our method will continue to improve as advancements are made in the field of open-domain customization adapters."
        },
        {
            "title": "REFERENCES",
            "content": "Grok 4, 2025. URL https://x.ai/news/grok-4. Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, and Trevor Darrell. Compositional gan: Learning image-conditional binary composition. International Journal of Computer Vision, 128(10):2570 2585, 2020. Black Forest Labs. Flux.1 [dev]. https://huggingface.co/black-forest-labs/ FLUX.1-dev, 2024a. Black Forest Labs. Flux.1 fill [dev]. https://huggingface.co/black-forest-labs/ FLUX.1-Fill-dev, 2024b. Black Forest Labs. Flux.1 [schnell]. https://huggingface.co/black-forest-labs/ FLUX.1-schnell, 2024c. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1839218402, 2022. URL https://api.semanticscholar.org/ CorpusID:253581213."
        },
        {
            "title": "Preprint",
            "content": "Junyan Cao, Yan Hong, and Li Niu. Painterly image harmonization in dual domains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 268276, 2023. Bor-Chun Chen and Andrew Kae. Toward realistic image compositing with adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84158424, 2019. Jiaxuan Chen, Bo Zhang, Qingdong He, Jinlong Peng, and Li Niu. Mureobjectstitch: Multi-reference image composition. arXiv preprint arXiv:2411.07462, 2024a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, and Baobao Chang. Multimodal representation alignment for image generation: Text-image interleaved control is easier than you think, 2025. URL https://arxiv.org/abs/2502. 20172. Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation. Advances in Neural Information Processing Systems, 37:8401084032, 2024b. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 65936602, 2024c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF conference on computer for generic visual-linguistic tasks. vision and pattern recognition, pp. 2418524198, 2024d. Zhekai Chen, Wen Wang, Zhen Yang, Zeqing Yuan, Hao Chen, and Chunhua Shen. Freecompose: Generic zero-shot image composition with diffusion prior. In European Conference on Computer Vision, pp. 7087. Springer, 2024e. Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, and Liqing Zhang. Dovenet: Deep image harmonization via domain verification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 83948403, 2020. Xiaodong Cun and Chi-Man Pun. Improving the harmony of the composite image by spatialseparated attention module. IEEE Transactions on Image Processing, 29:47594771, 2020. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Berthy Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine Bouman, and William Freeman. Score-based diffusion models as principled priors for inverse imaging. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1052010531, 2023. Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022a. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2022b."
        },
        {
            "title": "Preprint",
            "content": "Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, et al. Eraseanything: Enabling concept erasure in rectified flow transformers. arXiv preprint arXiv:2412.20413, 2024. Google Gemini2.5. age model, introducing-gemini-2-5-flash-image/. Introducing gemini 2.5 flash image, our imhttps://developers.googleblog.com/en/ state-of-the-art 2025."
        },
        {
            "title": "URL",
            "content": "Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. Advances in Neural Information Processing Systems, 35:1471514728, 2022. Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in neural information processing systems, 37: 3677736804, 2024. Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: In Proceedings of the IEEE/CVF InternaCompact parameter space for diffusion fine-tuning. tional Conference on Computer Vision, pp. 73237334, 2023. Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. Ace: All-round creator and editor following instructions via diffusion transformer. arXiv preprint arXiv:2410.00086, 2024. Jixuan He, Wanhua Li, Ye Liu, Junsik Kim, Donglai Wei, and Hanspeter Pfister. Affordance-aware object insertion via mask-aware dual diffusion. arXiv preprint arXiv:2412.14462, 2024. HiDream-ai. Hidream-e1. https://github.com/HiDream-ai/HiDream-E1, 2025. Yan Hong, Li Niu, and Jianfu Zhang. Shadow generation for composite image in real-world scenes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 914922, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021a. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021b. Junjia Huang, Pengxiang Yan, Jiyang Liu, Jie Wu, Zhao Wang, Yitong Wang, Liang Lin, and Guanbin Li. Dreamfuse: Adaptive image fusion with diffusion transformer. arXiv preprint arXiv:2504.08291, 2025. Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, and Ying Shan. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 83628371, 2024. URL https://api.semanticscholar.org/CorpusID:266174392. Jun Ahn Hyung, Jaeyo Shin, and Jaegul Choo. Magicapture: High-resolution multi-concept portrait customization. In AAAI Conference on Artificial Intelligence, 2023. Yifan Jiang, He Zhang, Jianming Zhang, Yilin Wang, Zhe Lin, Kalyan Sunkavalli, Simon Chen, Sohrab Amirghodsi, Sarah Kong, and Zhangyang Wang. Ssh: self-supervised framework for image harmonization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 48324841, 2021. Jian Jin, Yang Shen, Xinyang Zhao, Zhenyong Fu, and Jian Yang. Unicanvas: Affordance-aware unified real image editing via customized text-to-image generation. International Journal of Computer Vision, pp. 125, 2025."
        },
        {
            "title": "Preprint",
            "content": "Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-andplay image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pp. 150168. Springer, 2024. Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 24262435, 2022. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19311941, 2023. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023a. Leyang Li, Shilin Lu, Yan Ren, and Adams Wai-Kin Kong. Set you straight: Auto-steering denoising trajectories to sidestep unwanted concepts. arXiv preprint arXiv:2504.12782, 2025. Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu, Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, and Feng Zheng. Tuning-free image customization with image and text guidance. In European Conference on Computer Vision, pp. 233250. Springer, 2024b. Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. Dreamedit: Subject-driven image editing. arXiv preprint arXiv:2306.12624, 2023b. Yaowei Li, Yuxuan Bian, Xu Ju, Zhaoyang Zhang, Ying Shan, and Qiang Xu. Brushedit: Allin-one image inpainting and editing. ArXiv, abs/2412.10316, 2024c. URL https://api. semanticscholar.org/CorpusID:274763155. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. 2023c. Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman, and Simon Lucey. St-gan: Spatial transformer generative adversarial networks for image compositing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 94559464, 2018. Daquan Liu, Chengjiang Long, Hongpan Zhang, Hanning Yu, Xinzhi Dong, and Chunxia Xiao. Arshadowgan: Shadow generative adversarial network for augmented reality in single light scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81398148, 2020. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. Haoming Lu, Hazarapet Tunanyan, Kai Wang, Shant Navasardyan, Zhangyang Wang, and Humphrey Shi. Specialist diffusion: Plug-and-play sample-efficient fine-tuning of text-to-image In Proceedings of the IEEE/CVF Conference on diffusion models to learn any unseen style. Computer Vision and Pattern Recognition, pp. 1426714276, 2023a."
        },
        {
            "title": "Preprint",
            "content": "Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, and Liqing Zhang. Painterly image harmonization using diffusion model. arXiv preprint arXiv:2308.02228, 2023b. Lingxiao Lu, Jiangtong Li, Bo Zhang, and Li Niu. Dreamcom: Finetuning text-guided inpainting model for image composition. arXiv preprint arXiv:2309.15508, 2023c. Pengqi Lu. Qwen2vl-flux: Unifying image and text guidance for controllable image generation, 2024. URL https://github.com/erwold/qwen2vl-flux. Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free crossdomain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22942305, 2023d. Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 64306440, 2024a. Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. Robust watermarking using generative priors against image editing: From benchmarking to advances. arXiv preprint arXiv:2410.18775, 2024b. Jiancang Ma, Qirong Peng, Xu Guo, Chen Chen, H. Lu, and Zhenyu Yang. X2i: Seamless integration of multimodal understanding into diffusion transformer via attention distillation. ArXiv, abs/2503.06134, 2025. URL https://api.semanticscholar.org/ CorpusID:276903455. Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Li Ming, Gu Xin, Chen Fan, Xing Xiaoying, Wen Longyin, Chen Chen, and Zhu Sijie. Superedit: Rectifying and facilitating supervision for instruction-based image editing. arXiv preprint arXiv:2505.02370, 2025. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60386047, 2023. Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, and Liqing Zhang. Making images real again: comprehensive survey on deep image composition. arXiv preprint arXiv:2106.14490, 2021. OpenAI. Introducing gpt-5, 2025. URL https://openai.com/index/ introducing-gpt-5/. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Kien Pham, Jingye Chen, and Qifeng Chen. Tale: Training-free cross-domain image composition In Proceedings of the 32nd via adaptive latent manipulation and energy-guided optimization. ACM International Conference on Multimedia, pp. 31603169, 2024."
        },
        {
            "title": "Preprint",
            "content": "Trung Pham, Zhang Kang, Ji Woo Hong, Xuran Zheng, and Chang Yoo. E-md3c: Taming masked diffusion transformers for efficient zero-shot object customization. arXiv preprint arXiv:2502.09164, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image In The Twelfth International Conference on Learning Representations, 2024. URL synthesis. https://openreview.net/forum?id=di52zR8xgf. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Yuandong Pu, Le Zhuo, Kaiwen Zhu, Liangbin Xie, Wenlong Zhang, Xiangyu Chen, Pneg Gao, Yu Qiao, Chao Dong, and Yihao Liu. Lumina-omnilv: unified multimodal framework for general low-level vision. arXiv preprint arXiv:2504.04903, 2025. Pengchong Qiao, Lei Shang, Chang Liu, Baigui Sun, Xiangyang Ji, and Jie Chen. Facechain-sude: Building derived class to inherit category attributes for one-shot subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7215 7224, 2024. Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Scholkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:7932079362, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= Ha6RTeWMd0. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023a. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023b. Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David Jacobs, and Shlomi Fruchter. Magic insert: Style-aware drag-and-drop. arXiv preprint arXiv:2407.02489, 2024. Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe latent diffusion: In Proceedings of the IEEE/CVF Mitigating inappropriate degeneration in diffusion models. Conference on Computer Vision and Pattern Recognition, pp. 2252222531, 2023. Shihao Shao and Qinghua Cui. 1st place solution in google universal images embedding. arXiv preprint arXiv:2210.08473, 2022. Yichen Sheng, Jianming Zhang, and Bedrich Benes. Ssn: Soft shadow network for image compositing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 43804390, 2021."
        },
        {
            "title": "Preprint",
            "content": "Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 85438552, 2024a. Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024b. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Wensong Song, Hong Jiang, Zongxing Yang, Ruijie Quan, and Yi Yang. Insert anything: Image insertion via in-context editing in dit. arXiv preprint arXiv:2504.15009, 2025. Yeji Song, Jimyeong Kim, Wonhark Park, Wonsik Shin, Wonjong Rhee, and Nojun Kwak. Harmonizing visual and textual embeddings for zero-shot text-to-image customization. arXiv preprint arXiv:2403.14155, 2024a. Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Generative object compositing. arXiv preprint arXiv:2212.00932, 2022. Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and In Proceedings of the Daniel Aliaga. Objectstitch: Object compositing with diffusion model. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1831018319, 2023. Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, and Daniel Aliaga. Imprint: Generative object compositing by learning In Proceedings of the IEEE/CVF Conference on Computer identity-preserving representation. Vision and Pattern Recognition, pp. 80488058, 2024b. Jiale Tao, Yanbing Zhang, Qixun Wang, Yiji Cheng, Haofan Wang, Xu Bai, Zhengguang Zhou, Ruihuang Li, Linqing Wang, Chunyu Wang, et al. Instantcharacter: Personalize any characters with scalable diffusion transformer framework. arXiv preprint arXiv:2504.12395, 2025. Gemma Canet Tarres, Zhe Lin, Zhifei Zhang, Jianming Zhang, Yizhi Song, Dan Ruta, Andrew Gilbert, John Collomosse, and Soo Ye Kim. Thinking outside the bbox: Unconstrained generative object compositing. arXiv preprint arXiv:2409.04559, 2024. Gemma Canet Tarres, Zhe Lin, Zhifei Zhang, He Zhang, Andrew Gilbert, John Collomosse, and Soo Ye Kim. Multitwine: Multi-object compositing with text and layout control. arXiv preprint arXiv:2502.05165, 2025. Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Addit: Training-free object insertion in images with pretrained diffusion models. arXiv preprint arXiv:2411.07232, 2024. Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James Rehg, and Visesh Chari. Learning to generate synthetic data via compositing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 461470, 2019. Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. stantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv:2404.02733, 2024a. InarXiv preprint Haoxuan Wang, Jinlong Peng, Qingdong He, Hao Yang, Ying Jin, Jiafu Wu, Xiaobin Hu, Yanjie Pan, Zhenye Gan, Mingmin Chi, et al. Unicombine: Unified multi-conditional combination with diffusion transformer. arXiv preprint arXiv:2503.09277, 2025. Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Instructedit: Improving automatic masks for diffusion-based image editing with user instructions. ArXiv, abs/2305.18047, 2023a. URL https://api.semanticscholar.org/CorpusID:258959425."
        },
        {
            "title": "Preprint",
            "content": "Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024b. Yanghao Wang and Long Chen. Inversion circle interpolation: Diffusion-based image augmentation for data-scarce classification. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2556025569, 2025a. Yanghao Wang and Long Chen. Noise matters: Optimizing matching noise for diffusion classifiers. arXiv preprint arXiv:2508.11330, 2025b. Yanghao Wang, Zhongqi Yue, Xian-Sheng Hua, and Hanwang Zhang. Random boxes are openworld object detectors. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 62336243, 2023b. Yibin Wang, Weizhong Zhang, Jianwei Zheng, and Cheng Jin. Primecomposer: Faster progressively combined diffusion for image composition with attention steering. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1082410832, 2024c. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1594315953, 2023. Daniel Winter, Asaf Shul, Matan Cohen, Dana Berman, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectmate: recurrence prior for object insertion and subject-driven generation. arXiv preprint arXiv:2412.08645, 2024. Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. Gp-gan: Towards realistic high-resolution image blending. In Proceedings of the 27th ACM international conference on multimedia, pp. 24872495, 2019. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Rui Xie, Chen Zhao, Kai Zhang, Zhenyu Zhang, Jun Zhou, Jian Yang, and Ying Tai. Addsr: Accelerating diffusion-based blind super-resolution with adversarial diffusion distillation. arXiv preprint arXiv:2404.01717, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, and Xing Tang. Dccf: Deep comprehensible color filter learning framework for high-resolution image harmonization. In European Conference on Computer Vision, pp. 300316. Springer, 2022. Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, and Linfeng Zhang. Eedit: Rethinking the spatial and temporal redundancy for efficient image editing. arXiv preprint arXiv:2503.10270, 2025."
        },
        {
            "title": "Preprint",
            "content": "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227, 2022. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1838118391, 2023. Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. ArXiv, URL https://api.semanticscholar.org/CorpusID: abs/2401.11708, 2024. 267068823. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023a. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023b. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. Xinlei Yu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Ruolin Shen, Jiangning Zhang, Xiaobin Hu, Yanwei Fu, and Shuicheng Yan. Visual document understanding and question answering: multi-agent collaboration framework with test-time scaling. arXiv preprint arXiv:2508.03404, 2025a. Yongsheng Yu, Ziyun Zeng, Haitian Zheng, and Jiebo Luo. Omnipaint: Mastering object-oriented editing via disentangled insertion-removal inpainting. arXiv preprint arXiv:2503.08677, 2025b. Bo Zhang, Yuxuan Duan, Jun Lan, Yan Hong, Huijia Zhu, Weiqiang Wang, and Li Niu. Controlcom: Controllable image composition using diffusion model. arXiv preprint arXiv:2308.10040, 2023a. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023b. Lingzhi Zhang, Tarmily Wen, Jie Min, Jiancong Wang, David Han, and Jianbo Shi. Learning object placement by inpainting for compositional data augmentation. In European Conference on Computer Vision, pp. 566581. Springer, 2020a. Lingzhi Zhang, Tarmily Wen, and Jianbo Shi. Deep image blending. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 231240, 2020b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shuyang Zhang, Runze Liang, and Miao Wang. Shadowgan: Shadow synthesis for virtual objects with conditional adversarial networks. Computational Visual Media, 5(1):105115, 2019. Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv, 2025. URL https://arxiv.org/abs/2504.20690. Chen Zhao, Weiling Cai, Chenyu Dong, and Chengwei Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82818291, 2024a. Chen Zhao, Weiling Cai, Chenyu Dong, and Ziqi Zeng. Toward sufficient spatial-frequency interaction for gradient-aware underwater image enhancement. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 32203224. IEEE, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Chen Zhao, Weiling Cai, Chengwei Hu, and Zheng Yuan. Cycle contrastive adversarial learning with structural consistency for unsupervised high-quality image deraining transformer. Neural Networks, 178:106428, 2024c. Chen Zhao, Chenyu Dong, and Weiling Cai. Learning physical-aware diffusion model based on transformer for underwater image enhancement. arXiv preprint arXiv:2403.01497, 2024d. Chen Zhao, Wei-Ling Cai, and Zheng Yuan. Spectral normalization and dual contrastive regularization for image-to-image translation. The Visual Computer, pp. 112, 2025a. Chen Zhao, Zhizhou Chen, Yunzhe Xu, Enxuan Gu, Jian Li, Zili Yi, Qian Wang, Jian Yang, and Ying Tai. From zero to detail: Deconstructing ultra-high-definition image restoration from progressive spectral perspective. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1793517946, 2025b. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024e. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood In International Conference on Machine Learning, pp. 42363 estimation for diffusion odes. 42389. PMLR, 2023. Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. arXiv preprint arXiv:2305.10028, 2023. Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. arXiv preprint arXiv:2402.05408, 2024a. Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc++: Advanced multi-instance generation controller for image synthesis. arXiv preprint arXiv:2407.02329, 2024b. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. arXiv preprint arXiv:2410.12669, 2024c. Dewei Zhou, Mingwei Li, Zongxin Yang, and Yi Yang. Dreamrenderer: Taming multi-instance attribute control in large-scale text-to-image models. arXiv preprint arXiv:2503.12885, 2025a. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis-flux: simple and efficient multi-instance generation with dit rendering. arXiv preprint arXiv:2501.05131, 2025b. Yuanzhi Zhu, Ruiqing Wang, Shilin Lu, Junnan Li, Hanshu Yan, and Kai Zhang. Oftsr: Onestep flow for image super-resolution with tunable fidelity-realism trade-offs. arXiv preprint arXiv:2412.09465, 2024. Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pp. 195211. Springer, 2024. 19 21 21 22 23 24 24 24 24 26 26 27 27 29"
        },
        {
            "title": "Table of Contents",
            "content": "A Related Work . A.1 Image Composition . . A.2 General Image Editing . . A.3 Subject-Driven Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Visualizing the Impact of Adaptive Background Blending Equivalence of Query Blurring and Attention Weight Blurring . . C.1 Blurring the Query Matrix . . C.2 Blurring the Key and Value Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluating Cross-Attention Map Accuracy via IoU Experiments with SDXL, SD3.5, and PixArt User Study Benchmark Details Prompts for Proprietary Foundation Models Subject Identity Metrics Analysis Implementation Details Additional Qualitative Results LLM Usage Statement"
        },
        {
            "title": "A RELATED WORK",
            "content": "A."
        },
        {
            "title": "IMAGE COMPOSITION",
            "content": "Image composition involves integrating specific objects and scenarios from user-provided photos, often guided by text prompts. Traditionally, this process is divided into sub-tasks (Niu et al., 2021), including object placement (Azadi et al., 2020; Chen & Kae, 2019; Lin et al., 2018; Tripathi et al., 2019; Zhang et al., 2020a), image blending (Wu et al., 2019; Zhang et al., 2020b), image harmonization (Cao et al., 2023; Lu et al., 2023b; Zhang et al., 2020b; Cong et al., 2020; Cun & Pun, 2020; Jiang et al., 2021; Xue et al., 2022), and shadow generation (Hong et al., 2022; Liu et al., 2020; Sheng et al., 2021; Zhang et al., 2019), each typically handled by distinct models and pipelines. However, with the rise of diffusion-based generative models, recent approaches have shifted toward unified frameworks that address all sub-tasks simultaneously. These methods are broadly classified into training-based and training-free approaches. Training-based methods fine-tune foundational models using datasets tailored for image composition. Early methods like Paint by Example (Yang et al., 2022) and ObjectStitch (Song et al., 2022) use CLIP to encode subject features, ensuring high semantic similarity between inserted objects and reference images. These approaches use image augmentation to create training datasets, enabling effective training. GLIGEN (Li et al., 2023c) incorporates grounding information into new trainable layers of pre-trained diffusion model via gated mechanism. ControlCom (Zhang et al., 2023a) integrates 2-dim indicator vector to improve controllability. DreamCom (Lu et al., 2023c) and MureObjectStitch (Chen et al., 2024a) fine-tune models with small sets of reference images to preserve subject identity. AnyDoor (Chen et al., 2024c), IMPRINT (Song et al., 2024b), and E-MD3C (Pham et al., 2025) leverage DINOv2 to enhance identity fidelity and control over shape and pose, drawing supervision from video data. MimicBrush (Chen et al., 2024b) similarly uses video-derived supervision for imitative editing. In contrast, MADD (He et al., 2024), ObjectMate (Winter et al., 2024), and OmniPaint (Yu et al., 2025b) employ image inpainting models to generate higher-quality training datasets compared to those based on image or video augmentation. Multitwine (Tarres et al., 2025) enables the integration of multiple objects, capturing interactions from simple positional relationships to complex actions requiring reposing. DreamFuse (Huang et al., 2025) uses Positional Affine mechanism to embed foreground size and position into the background, fostering effective foreground-background interaction through shared attention. Insert Anything (Song et al., 2025) and UniCombine (Wang et al., 2025) introduces FLUX-based, multi-conditional generative framework that handles diverse condition combinations. However, these methods often bias the generative priors of base models toward curated datasets, resulting in unnatural compositions, such as implausible object-environment interactions (e.g., missing or unrealistic shadows and reflections). This stems from the absence of large-scale, high-quality, multi-resolution, real-world triplet dataset comprising an object, scene, and the object seamlessly integrated into the scene, which is expensive to produce. Training-free methods, on the other hand, modify the inference process of pre-trained models to achieve composition without additional training. Early approaches like TF-ICON (Lu et al., 2023d) leverage accurate image inversion to lay the groundwork for composition, achieved through composite self-attention map injection. TALE (Pham et al., 2024) and PrimeComposer (Wang et al., 2024c) build on TF-ICON to enhance identity preservation and background-object style adaptation. TIGIC (Li et al., 2024b) focuses on preserving non-target areas during composition. Thinking Outside the BBox (Tarres et al., 2024) enables unconstrained image compositing, unbound by input masks. FreeCompose (Chen et al., 2024e) employs pipeline of object removal, image harmonization, and semantic composition. DreamEdit (Li et al., 2023b), UniCanvas (Jin et al., 2025), and Magic Insert (Ruiz et al., 2024) use test-time tuning to fine-tune models during inference. Addit (Tewel et al., 2024) enables text-guided object insertion on FLUX, where users describe objects via text prompts instead of reference images. EEdit (Yan et al., 2025) recently improves TF-ICON on FLUX, introducing step-skipping to reduce time costs and spatial locality caching to minimize redundancy. However, training-free methods rely on precise image inversion and fragile attention surgery, which can lock inserted objects into the exact pose of the reference image, leading to awkward or contextually inappropriate orientations. Attention manipulation often causes instability and hyperparameter sensitivity, as feature or attention map injection does not always preserve subject identity. This creates trade-off: stronger injection preserves identity but results in unnatural poses, while lighter injection yields more natural poses but compromises identity."
        },
        {
            "title": "Preprint",
            "content": "A.2 GENERAL IMAGE EDITING Recent and significant advancements in text-to-image generative models have enhanced numerous applications (Zhou et al., 2025a;b; 2023; 2024a;b;c; Zhao et al., 2024a;d;c; 2025b;a; 2024b; Xie et al., 2024; Wang et al., 2023b; Wang & Chen, 2025a;b; Lu et al., 2024b; Li et al., 2025; Gao et al., 2024; Zhu et al., 2024; Yu et al., 2025a). Instruction-based image editing has evolved rapidly. Early systems relied on modular, two-stage pipelines: multimodal language model first produced textual prompts, spatial guidance, or synthetic instructionimage pairs, and separate diffusion model then executed the editas in InstructEdit (Wang et al., 2023a), InstructPix2Pix (Brooks et al., 2022), MagicBrush (Zhang et al., 2023b), and BrushEdit (Li et al., 2024c). Recent work has shifted toward tightly integrated, instruction-centric architectures. Models such as SmartEdit (Huang et al., 2024), X2I (Ma et al., 2025), RPG (Yang et al., 2024), AnyEdit (Yu et al., 2024), and UltraEdit (Zhao et al., 2024e) embed routing, task-aware objectives, and fine-grained controls directly into the network, yielding higher fidelity and more precise manipulation. Unified generation-and-editing frameworks (e.g., OmniGen (Xiao et al., 2024), ACE (Han et al., 2024), ACE++ (Mao et al., 2025), Lumina-OmniLV (Pu et al., 2025), Qwen2VL-Flux (Lu, 2024), DreamEngine (Chen et al., 2025), MetaQueries (Pan et al., 2025), Hidream-E1 (HiDream-ai, 2025)) treat editing as one capability of an end-to-end vision-language model, often fusing language embeddings with diffusion latents to provide context-aware, pixel-level control. Efficiency has advanced in parallel: ICEdit (Zhang et al., 2025) couples LoRA with mixture-of-experts tuning and optimized noise initialization, while SuperEdit (Ming et al., 2025) relies on higher-quality data and contrastive supervision to sustain performance at lower cost. Looking ahead, large foundation models such as Gemini (Gemini2.5, 2025) and GPT-5 (OpenAI, 2025) already show strong visual reasoning and coherent, instruction-guided image generation. Yet, despite extensive multimodal pre-training, they still fall short on image composition: object placement remains hard to control, lighting is often inconsistent, and subjects can drift in identity. A.3 SUBJECT-DRIVEN GENERATION Extensive research has explored subject-driven image generation, in which the output must not only portray the contexts described by the text prompt but also faithfully include the specific subject supplied by reference images. Methods in this area are divided into two categoriestest-time finetuning customization and zero-shot customizationaccording to whether extra training is needed for each new subject. Our framework accommodates both categories, so we provide two corresponding variants in the main paper. Test-time fine-tuning methods (Gal et al., 2022a; Ruiz et al., 2023a) adapt pre-trained T2I model to small set of reference images (typically 3 to 5 images). Although this step adds computational cost and latency, it offers the greatest flexibility for diverse customization requirements. Such methods are commonly grouped into three subclasses: data regularization, weight regularization, and loss regularization. In the data-regularization family, DreamBooth (Ruiz et al., 2023a) limits overfitting by generating superclass images with the base T2I model and training on both reference and regularization images; Custom Diffusion (Kumari et al., 2023) improves regularization quality by retrieving real images; and Specialist Diffusion (Lu et al., 2023a) applies extensive data augmentation. Weight-regularization approaches (Gal et al., 2022b; Hu et al., 2021b; Han et al., 2023; Qiu et al., 2023) confine updates to carefully chosen parameters, such as the subject-specific text embeddings or the singular values of weight matrices. Loss-regularization approaches, including Specialist Diffusion (Lu et al., 2023a), MagiCapture (Hyung et al., 2023), and FaceChain-SuDe (Qiao et al., 2024), introduce objective terms that respectively maximize CLIP-space similarity between generated and reference images, disentangle identity and style via masked facial reconstruction, or encourage correct superclass classification. Zero-shot image customization methods avoid subject-specific fine-tuning at inference time but rely on extensive pre-training. For general subject customization, InstantBooth (Shi et al., 2024a) adds visual encoder that captures coarse-to-fine image features from the references; BLIPDiffusion (Li et al., 2024a) fine-tunes BLIP-2 (Li et al., 2023a) to extract multimodal subject representations; ELITE (Wei et al., 2023) maps reference images into hierarchical textual tokens through global and local networks; and Song et al. (Song et al., 2024a) enhance textual control by removing the projection of visual embeddings onto textual embeddings. For facial customization, Instan-"
        },
        {
            "title": "Preprint",
            "content": "tID (Wang et al., 2024b) isolates facial regions from reference images to extract appearance and structural cues. For style customization, InstantStyle (Wang et al., 2024a) identifies style-controlling layers and injects IP-Adapter features (Ye et al., 2023a) into those layers to achieve style transfer. InstantCharacter (Tao et al., 2025), IP-Adapter (Ye et al., 2023b), and PuLID (Guo et al., 2024) have each released versions compatible with the FLUX model."
        },
        {
            "title": "B VISUALIZING THE IMPACT OF ADAPTIVE BACKGROUND BLENDING",
            "content": "Although our loss function aims to find new latent within the manifold learned by the adapter, encouraging the adapter-augmented T2I models predictions to closely match those of the base model on the original noisy latent, this early-stage optimization primarily preserves structural elements rather than fine details. Consequently, discrepancies in fine-grained features often arise between the masked and unmasked regions. As illustrated in Fig. 9, we compare the composite images generated using our Adaptive Background Blending (ABB) method with those produced via direct background blending using the rectangular user mask. For clarity, we enlarge the boundary areas of each image (highlighted in pink dashed boxes) to better reveal differences. Figure 9: Comparison of composites from our Adaptive Background Blending (ABB) method and direct blending with rectangular mask. Boundary regions within pink dashed boxes are enlarged for clarity. Please zoom in to see details."
        },
        {
            "title": "BLURRING",
            "content": "Consider 2D Gaussian filter applied as convolution operation, denoted by . The self-attention weights are computed as QKT, where Q, Rnd, with being the sequence length and the embedding dimension. We explore the effect of applying Gaussian blur to the attention weights and its equivalence to blurring the query matrix. C.1 BLURRING THE QUERY MATRIX Blurring the self-attention weights QKT with 2D Gaussian filter can be expressed as: (QK T), (6) where denotes 2D convolution. Due to the linearity of convolution, there exists Toeplitz matrix Rnn such that the convolution operation can be represented as matrix multiplication: Using the properties of matrix multiplication, we can rewrite this as: B(QK T) = (BQ)KT. (QK T) = B(QKT). Since the convolution operation is linear, applying the Gaussian filter to the rows of yields: BQ = Q. (7) (8) (9) Thus, we obtain: (QK T) = (G Q)KT. (10) This establishes that blurring the query matrix with is mathematically equivalent to applying the same blur to the self-attention weights QKT. This equivalence suggests that query blurring can be used as computationally efficient proxy for smoothing attention weights, potentially reducing the need for direct manipulation of the attention matrix. C.2 BLURRING THE KEY AND VALUE MATRICES In contrast, applying the Gaussian blur to the key matrix does not yield similar equivalence. Consider the convolution applied to K. The resulting attention weights become: Q(G K)T = Q(BK)T = QKTBT. (11) Since BT = for general Toeplitz matrix derived from Gaussian filter, we have: QKTBT = B(QKT). (12) Thus, blurring the key matrix does not produce an equivalent effect to blurring the attention weights QKT. similar argument applies to the value matrix , as the output of self-attention, (QKT)V , involves in post-multiplication step, and convolution on does not commute with the attention weight computation in the same manner. EVALUATING CROSS-ATTENTION MAP ACCURACY VIA IOU To identify the most accurate cross-attention maps that reflect the location of the generated object, we first create 100 prompts containing main subject (e.g.,a dog is sleeping on couch) using GPT-5. These prompts are then used to generate 100 images with FLUX.1-dev, employing 20 denoising steps. Cross-attention maps are extracted from 19 multi-stream (or double-stream) blocks and 38 single-stream blocks across all denoising steps. The maps are averaged over the 20 steps and subsequently normalized and binarized, resulting in total of 57 binary masks. To determine which of these 57 masks is the most accurate, we compute the Intersection over Union (IoU) between each mask and ground-truth mask obtained by segmenting the final generated images using SAM (Ravi et al., 2025). The IoU for each block is averaged over the 100 generated images. The results are presented in Fig. 10, showing that the cross-attention maps from the last multi-stream (or double-stream) block achieve the highest segmentation accuracy. visualization of the cross-attention maps from all blocks is provided in Fig. 11."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: The IoU is calculated between the mask produced from each block and the ground-truth mask, which is obtained by segmenting the final generated images using SAM. The IoU for each block is averaged over 100 images. Figure 11: Visualization of cross-attention maps from different MMDiT blocks of FLUX.1-dev."
        },
        {
            "title": "Preprint",
            "content": "E EXPERIMENTS WITH SDXL, SD3.5, AND PIXART Our framework introduces novel, model-agnostic approach for enhancing generative architectures, anchored by three synergistic components: Manifold-Steered Anchor (MSA) loss, DegradationSuppression Guidance (DSG), and Adaptive Background Blending (ABB). These components are meticulously designed to leverage ubiquitous features of modern generative models, ensuring seamless integration without requiring architectural modifications. Specifically, MSA utilizes either LoRA-based personalization or pretrained personalization adapter, DSG capitalizes on widely available self-attention maps, and ABB harnesses textimage cross-attention maps, staple in most text-to-image pipelines. This design ensures broad applicability across diverse generative models. In the main text, we showcase the effectiveness of our approach on Flux, leading open-source model. To establish its generalizability, we conduct experiments on SDXL (Podell et al., 2024), SD3.5 (Esser et al., 2024), and PixArt (Chen et al., 2023) across DreamEditBench and ComplexCompo. The results are presented in Tab. 3. On DreamEditBench, our LoRA-based variant with Flux achieves state-of-the-art performance in subject identity preservation, while delivering superior image quality. Similarly, on ComplexCompo, the Flux-LoRA configuration excels in identity consistency and image fidelity. Notably, the frameworks benefits extend beyond single model family: both SDXL and PixArt-Σ exhibit substantial performance gains, affirming the approachs generality and adaptability across diverse generative architectures. Table 3: Comparison of compositional performance across two benchmarks with different base models. The best result in each column is highlighted in bold, while the second-best is underlined. Metrics shown in pink are those specifically trained to better align with human preferences. Abbreviations: IRF: Instance Retrieval Features; IR = ImageReward; VR = VisionReward. Bench Method DreamEditBench (220) ComplexCompo (300) Ours-Adapter Ours-Adapter Ours-LoRA Ours-Adapter Ours-LoRA Ours-Adapter Ours-Adapter Ours-LoRA Ours-Adapter Ours-LoRA Base Model SDXL SD3.5 PixArt-Σ FLUX FLUX SDXL SD3.5 PixArt-Σ FLUX FLUX Subject Identity Consistency Background Image Quality CLIP-I DINOv2 IRF DreamSim LPIPS SSIM IR VR 0.7944 0.8054 0.8098 0.8086 0.8125 0.7657 0.7701 0.7924 0.7721 0.7999 0.7334 0.7407 0.7445 0.7415 0. 0.7084 0.7091 0.7287 0.7107 0.7384 0.7659 0.7699 0.7798 0.7702 0.7900 0.6862 0.6977 0.7311 0.6764 0.7659 0.3761 0.3745 0.3612 0.3730 0.3577 0.4457 0.4173 0.3603 0.4294 0.3542 0.0238 0.0234 0.0251 0.0236 0. 0.0457 0.0401 0.0424 0.0404 0.0430 0.8922 0.8937 0.8875 0.8959 0.8847 0.7612 0.7784 0.7698 0.7789 0.7634 0.5621 0.5701 0.5842 0.5709 0.5906 0.3894 0.4091 0.4277 0.4090 0.4246 3.6158 3.6187 3.6198 3.6234 3. 3.5987 3.6021 3.5988 3.6020 3."
        },
        {
            "title": "F USER STUDY",
            "content": "We conduct user study involving 50 participants. Each participant was asked to complete 50 ranking tasks. In each task, they were shown 13 composition results generated by different methods, along with reference subject image. To ensure balanced evaluation, 25 of the tasks were randomly sampled from DreamEditBench and the remaining 25 from ComplexCompo. Participants were asked to rank the results based on two key criteria: (1) subject identity consistency and (2) composition realism. lower rank (e.g., 1st) indicates better composition result, while higher rank (e.g., 13th) reflects less favorable outcome. We summarize the average ranking scores for each method in Tab. 4. Our method received the most favorable rankings from the majority of participants, demonstrating its effectiveness in producing high-quality compositions."
        },
        {
            "title": "G BENCHMARK DETAILS",
            "content": "Our benchmark consists of 300 triplets, each comprising subject image, background image, and bounding box. The subject images are identical to those used in DreamEditBench (Li et al., 2023b; Ruiz et al., 2023a), while the background images are sampled from the OpenImages dataset (Kuznetsova et al., 2020). These backgrounds exhibit variety of aspect ratios and"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Average ranking scores from the user study on image composition methods. Lower is better. Method Training-Free Average Ranking (Lower is Better) MADD ObjectStitch DreamCom AnyDoor UniCombine PBE TIGIC TALE TF-ICON DreamEdit EEdit Ours-Adapter Ours-LoRA 12.44 11.80 6.66 4.12 2.94 4.94 9.74 9.06 8.36 6.36 10.76 2.30 1. resolutions, including landscape and portrait formats, such as 768 1088, 768 1072, 768 1024, 768 1152, 1024 768, 1152 768, 1200 768, 848 768, and 1360 768. The bounding boxes are manually designed to ensure that the size and placement of the inserted subjects are contextually appropriate and visually plausible. The benchmark will be released publicly."
        },
        {
            "title": "H PROMPTS FOR PROPRIETARY FOUNDATION MODELS",
            "content": "To perform image composition with proprietary, general-purpose multimodal foundation models (e.g., GPT-5 (OpenAI, 2025), Gemini 2.5 Pro (Gemini2.5, 2025), SeedEdit/Doubao (Shi et al., 2024b), and Grok 4 (gro, 2025)), we upload three images: (1) Subject image; (2) Background image; and (3) Mask image defining the insertion region. We then issue prompt of the following form (with the resolution and coordinates adjusted for each case): Please insert the object from the first uploaded image into the second image. The target region for insertion is defined by the mask in the third image. For reference, the resolution of the second image is 1152 768, and the bounding box for placement is specified by the top-left and bottom-right coordinates: (x1 = 550, y1 = 600, x2 = 700, y2 = 750). The inserted object should retain the same identity and appearance as in the first image. The final composite should appear realistic, natural, and physically plausible."
        },
        {
            "title": "I SUBJECT IDENTITY METRICS ANALYSIS",
            "content": "In our experiments we found that widely-used subject-identity metrics such as CLIP-I (Radford et al., 2021) and DINOv2 (Oquab et al., 2023) correlate poorly with human preferences. Because they focus almost exclusively on semantic similarity, they ignore appearance changes introduced by lighting, shadows, reflections, and surrounding context. Fig. 12(b) presents several image pairs produced by AnyDoor (left) and by our method (right); the corresponding CLIP-I () (Radford et al., 2021), DINOv2 () (Oquab et al., 2023), IRF () (Shao & Cui, 2022), and DreamSim () (Fu et al., 2023) scores are shown beneath each image, with the better score highlighted in red. Although the AnyDoor results are visibly less realistic and less consistent, they nevertheless receive higher CLIP-I and DINOv2 scores, and in most cases higher IRF scores, demonstrating that these measures do not faithfully capture compositional quality. reliable metric should recognise the same object whether it is underwater (see Fig. 12(b)[(8), (17), (20)]), in shadow (see Fig. 12(b)[(9), (13), (19)]), partially occluded (see Fig. 12(b)[(2)]), or situated in low-light or back-lit scene (see Fig. 12(b)[(5), (14), (15), (16)]). Among the metrics we evaluated, only DreamSim, which was designed to align more closely with human perception, consistently exhibits this desired behaviour."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Comparison of Subject Identity Metrics. (a) Reference subject images used for metric calculations. (b) Image pairs generated by AnyDoor (left) and our method (right), with corresponding CLIP-I (), DINOv2 (), IRF (), and DreamSim () scores displayed below each image; the better score is highlighted in red. Despite AnyDoors results appearing less realistic and consistent, they often achieve higher CLIP-I, DINOv2, and IRF scores, indicating that these metrics may not reliably reflect compositional quality. In contrast, DreamSim provides more reliable assessment."
        },
        {
            "title": "Preprint",
            "content": "J"
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "The hyperparameters used in our frameworks are summarized in Tab. 5. Each baseline is implemented according to the configuration settings recommended in its original publication. The repositories utilized for each baseline are listed below: Table 5: Hyperparameters of Our Frameworks. Bin Thresh = Binarization Threshold; #iter = Number of Iterations. Denoising Setup Manifold-Steered Anchor Loss Degradation-Suppression Guidance Adaptive Background Blending Variant Total steps (T 1) 0 Ours-Adapter Ours-LoRA 19 0 19 0 Start step t1 14 13 Step Range t1 τ 14 12 13 Learning Rate α #iters 500, 750, 1000 50, 300 10 2 Step Range t1 0 14 0 13 0 Scale η 0.5 0.7 Blur σ 10 10 Step Range (t1 1) 1 13 1 12 1 Bin Thresh Dilation Kernel Size 0.2 0.4 3 3 1. MADD: https://github.com/KaKituken/affordance-aware-any 2. ObjectStitch: https://github.com/bcmi/ObjectStitch-Image-Composition 3. DreamCom: https://github.com/bcmi/DreamCom-Image-Composition 4. AnyDoor: https://github.com/ali-vilab/AnyDoor 5. UniCombine: https://github.com/Xuan-World/UniCombine 6. PBE: https://github.com/Fantasy-Studio/Paint-by-Example 7. TIGIC: https://github.com/zrealli/TIGIC 8. TALE: https://github.com/tkpham3105/TALE 9. TF-ICON: https://github.com/Shilin-LU/TF-ICON 10. DreamEdit: https://github.com/DreamEditBenchTeam/DreamEdit 11. EEdit: https://github.com/yuriYanZeXuan/EEdit"
        },
        {
            "title": "K ADDITIONAL QUALITATIVE RESULTS",
            "content": "We offer more qualitative assessment results, including visualizations of all baselines, presented in Figs. 13 to 18."
        },
        {
            "title": "L LLM USAGE STATEMENT",
            "content": "We used large language models for text polishing and grammar correction during manuscript preparation. No LLMs were involved in the design of the method, experiments, or analysis. All content has been carefully verified and validated by the authors."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: (Part 1 of 2) Qualitative comparison of our method against baselines in challenging scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: (Part 2 of 2) Qualitative comparison of our method against baselines in challenging scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: (Part 1 of 2) Qualitative comparison of our method against baselines in challenging scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: (Part 2 of 2) Qualitative comparison of our method against baselines in challenging scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: (Part 1 of 2) Qualitative comparison of our method against baselines in challenging scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 18: (Part 2 of 2) Qualitative comparison of our method against baselines in challenging scenarios."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Nanyang Technological University"
    ]
}