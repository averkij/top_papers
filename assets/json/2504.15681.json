{
    "paper_title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
    "authors": [
        "Vidi Team",
        "Celong Liu",
        "Chia-Wen Kuo",
        "Dawei Du",
        "Fan Chen",
        "Guang Chen",
        "Jiamin Yuan",
        "Lingxi Zhang",
        "Lu Guo",
        "Lusha Li",
        "Longyin Wen",
        "Qingyu Chen",
        "Rachel Deng",
        "Sijie Zhu",
        "Stuart Siew",
        "Tong Jin",
        "Wei Lu",
        "Wen Zhong",
        "Xiaohui Shen",
        "Xin Gu",
        "Xing Mei",
        "Xueqiong Qu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans naturally share information with those they are connected to, and video has become one of the dominant mediums for communication and expression on the Internet. To support the creation of high-quality large-scale video content, a modern pipeline requires a comprehensive understanding of both the raw input materials (e.g., the unedited footage captured by cameras) and the editing components (e.g., visual effects). In video editing scenarios, models must process multiple modalities (e.g., vision, audio, text) with strong background knowledge and handle flexible input lengths (e.g., hour-long raw videos), which poses significant challenges for traditional models. In this report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a wide range of video understand editing scenarios. The first release focuses on temporal retrieval, i.e., identifying the time ranges within the input videos corresponding to a given text query, which plays a critical role in intelligent editing. The model is capable of processing hour-long videos with strong temporal understanding capability, e.g., retrieve time ranges for certain queries. To support a comprehensive evaluation in real-world scenarios, we also present the VUE-TR benchmark, which introduces five key advancements. 1) Video duration: significantly longer than existing temporal retrival datasets, 2) Audio support: includes audio-based queries, 3) Query format: diverse query lengths/formats, 4) Annotation quality: ground-truth time ranges are manually annotated. 5) Evaluation metric: a refined IoU metric to support evaluation over multiple time ranges. Remarkably, Vidi significantly outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task, indicating its superiority in video editing scenarios."
        },
        {
            "title": "Start",
            "content": "Vidi: Large Multimodal Models for Video"
        },
        {
            "title": "Understanding and Editing",
            "content": "Intelligent Editing Team, Intelligent Creation, ByteDance Inc. San Jose/Seattle, US https://bytedance.github.io/vidi-website/"
        },
        {
            "title": "Abstract",
            "content": "Humans naturally share information with those they are connected to, and video has become one of the dominant mediums for communication and expression on the Internet. To support the creation of highquality large-scale video content, modern pipeline requires comprehensive understanding of both the raw input materials (e.g., the unedited footage captured by cameras) and the editing components (e.g., visual effects). In video editing scenarios, models must process multiple modalities (e.g., vision, audio, text) with strong background knowledge and handle flexible input lengths (e.g., hour-long raw videos), which poses significant challenges for traditional models. In this report, we introduce Vidi, family of Large Multimodal Models (LMMs) for wide range of video understand editing scenarios. The first release focuses on temporal retrieval, i.e., identifying the time ranges within the input videos corresponding to given text query, which plays critical role in intelligent editing. The model is capable of processing hour-long videos with strong temporal understanding capability, e.g., retrieve time ranges for certain queries. To support comprehensive evaluation in real-world scenarios, we also present the VUE-TR benchmark, which introduces five key advancements: 1) Video duration: spans from 20 seconds to over an hour, which is significantly longer than existing temporal/moment retrieval datasets. 2) Audio support: includes audio-based queries for temporal retrieval. 3) Query format: accommodates three different query lengths/formats, i.e., keyword, phrase, and sentence. 4) Annotation quality: all ground-truth time ranges are manually annotated with high accuracy. 5) Evaluation metric: refined IoU metric designed to support evaluation over multiple time ranges. Remarkably, Vidi significantly outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task, indicating its superiority in video editing scenarios. 5 2 0 2 2 2 ] . [ 1 1 8 6 5 1 . 4 0 5 2 : r Figure 1: Temporal retrieval accuracy of different models on the proposed VUE-TR benchmark. detailed contributor list can be found in Section 10."
        },
        {
            "title": "1 Introduction",
            "content": "Video has become one of the most dominant mediums for sharing information on the Internet. However, for most users, video creation remains complex and time-consuming process, especially on mobile devices where precise editing is challenging. Among all editing stages, the most labor-intensive step is often identifying the desired segments with long, unedited footage. Beyond trimming, users frequently struggle with video composition tasks such as selecting appropriate music, transitions, effects, animations, filters, stickers, and fonts, which require both technical skill and artistic judgment. Moreover, creative editing actions beyond composition are increasingly important in modern video production but are even harder to achieve without expert tools or AI assistance, e.g., generating thumbnails, cover art or stylized scenes. We aim to build the next generation of video creation systems to be powered by advanced video understanding and editing capabilities, enabling users to complete complex editing workflows automatically and effortlessly. In this release, we present Vidi, large multimodal model for video understanding and editing (VUE) with particular focus on the temporal retrieval (TR) task. Vidi takes text, vision, and audio as input modalities and retrieves the most relevant time ranges corresponding to natural language query. key strength of our model lies in its ability to handle hour-long input videos, significantly surpassing the duration constraints of existing academic temporal/moment retrieval datasets [6, 12, 9, 14, 7], which typically cap at around 150 seconds. To support comprehensive evaluation in realistic settings, we introduce manually annotated benchmark (abbreviated as VUE-TR) for temporal retrieval. VUE-TR consists of videos ranging from approximately 20 seconds to over 1 hour, categorized into five groups: ultra-short (< 1 min), short (110 mins), medium (1030 mins), long (30 60 mins) and ultra-long (> 1 hour). Each video is paired with queries of varying formats and lengths (i.e., keyword, phrase, and sentence) to reflect the diversity of user search intent. Critically, the queries may require visual, audio, or both modalities for accurate localization. This aligns closely with realworld scenarios, where audio plays an essential role in video comprehension, particularly in domains such as TV shows, broadcast, and musical performances. As illustrated in Figure 2, we showcase an example from an hour-long video with three representative queries, spanning different formats and modalities. Remarkably, Vidi-7B significantly outperforms leading LMMs (Large Multimodal Models), e.g., Gemini and GPT-4o, highlighting its effectiveness in complex, multimodal temporal retrieval tasks. Figure 2: Examples of temporal retrieval queries and their corresponding time ranges from the proposed VUE-TR benchmark. The duration of the example video is 3, 871 seconds (i.e., 01:04:31). Each query is presented in one of three formats (i.e., keyword, phrase, sentence) with visual or audio information. Facial regions in the video frames are blurred to preserve privacy."
        },
        {
            "title": "2 General Overview",
            "content": "Figure 3 illustrates the model architecture of Vidi. For LLM training, we adopt the decomposed attention mechanism proposed by Kuo et al. [13], which reduces the computation complexity over multimodal tokens from O(N 2) to O(N ) without sacrificing performance on downstream multimodal tasks. This efficient attention design enables both training and inference on extremely long videos, which are otherwise infeasible for standard transformer-based models [26]. All videos are uniformly sampled at 1 frames per second (fps) for visual input and 16, 000 Hz for audio, ensuring that the model can localize and understand content at second-level precision. Figure 3: An overview of the Vidi architecture. Raw visual and audio inputs are first process by pretrained modality-specific encoders to extract token sequences. multimodal adapter is applied to both the visual and audio branches to compress the token sequences and project them into the input space of the pretrained LLM [11, 24]. Notably, the LLM operates using the decomposed attention [13] to enable efficient and scalable modeling over long, densely sampled multimodal sequences. The training of Vidi is conducted in two main stages to ensure strong multimodal temporal retrieval: Multimodal Alignment: This stage focuses on aligning vision and audio data with the corresponding text descriptions and timestamps. We first train the vision and audio adapters using visual and audio captioning data, while keeping the rest of the model frozen. Then the LLM and adapters are jointly trained on synthetic video/audio data with ground-truth time ranges, allowing the model to learn multimodal-to-temporal grounding. Finally, the model is fine-tuned using real videos, with supervision from paired (i.e., timestamp and caption/ASR) data to narrow the domain gap between synthetic and in-the-wild content. Application Post-training: To support diverse application scenarios, we further fine-tune the model on task-specific data, including temporal retrieval and video/image VQA. The temporal retrieval training data mirrors the structure of the VUE-TR benchmark, with queries in three formats (i.e., keyword, phrase, sentence), requiring information from vision, audio, or both. This stage enhances the models ability to handle real-word usage patterns where query types vary significantly. Depending on the intended use case, we train different versions of Vidi, with the tailored training recipes, e.g., temporal retrieval expert model, and more generic VQA model (to be included in future releases). During inference, Vidi is capable of running on single 80G GPU without quantization, and efficiently process videos exceeding 2 hours in length, making it practical for real-world deployment in long-form video editing and retrieval tasks."
        },
        {
            "title": "3 Model Architecture",
            "content": "Unlike generic video understanding tasks that rely on sparse or uniform sampling of fixed number of frames, temporal retrieval requires models to localize relevant segments with second-level precision, necessitating dense sampling, typically at 1 fps. Under this fixed sampling rate, the multimodal inputs (e.g., vision, audio, and text) can vary significantly in length, ranging from ultra-short of just few seconds to ultra-long exceeding an hour. Furthermore, our proposed VUE-TR benchmark emphasizes realistic retrieval conditions by requiring models to process both visual and audio modalities when localizing queries. To address challenges in handling videos with varying length, multimodal inputs at scale, we adopt the Decomposed Attention (DAttn) [13] architecture in Vidi. D-Attn offers exceptional computational efficiency while maintaining strong multimodal capabilities. It also facilitates seamless integration of visual, audio, and textual streams, making it well-suited for temporal retrieval in long-form, real-world videos. As shown in Figure 3, the visual and audio inputs are first encoded into token sequences using the pretrained visual and audio encoders, respectively. The resulting multimodal embeddings are projected with corresponding adapter layers. Together with the text query embeddings, they are fed into D-Attn LLM, which localizes the text query within the input video. As illustrated in Figure 4, D-Attn is an architectural adaptation of modern pretrained LLMs, where causal self-attention within an LLM is equivalently decomposed into visual-to-visual selfattention (V2V Self-Attn), textual-to-textual selfattention (T2T Self-Attn), and textual-to-visual cross-attention (T2V Cross-Attn). With this decomposition strategy, Kuo et al. [13] introduce novel modifications to both V2V Self-Attn and T2V Cross-Attn to enhance multimodal performance and computational efficiency, while preserving the capabilities of the pretrained LLM. From computational standpoint, diagonalized variant of V2V Self-Attn is proposed to retain performance while reducing complexity from O(N 2) to O(N ) for visual tokens, shown in Figure 4. This lightweight design is particularly well-suited for densely sampled, ultra-long videos, where might be extremely large. For example, if each frame consists of 400 tokens, one-hour video would have 1.44M tokens. Such input length is challenging for conventional fully self-attention mechanisms. Figure 4: Decomposed Attention [13] equivalently decomposes causal self-attention in an LLM into three components: visual-to-visual (V2V), textual-to-textual (T2T), and textual-to-visual (T2V) attentions. In terms of multimodal alignment, Kuo et al. [13] identify critical issue: positional bias between text and visual tokens can hinder the models ability to establish comprehensive understanding of multimodal contents. To address this, debiased positional encoding strategy is proposed to remove positional encodings from T2V Cross-Attn, eliminating the undesirable bias. This approach yields consistent improvements across wide range of downstream tasks. By integrating both diagonal V2V Self-Attn and debiased positional encodings in T2V Cross-Attn, the resulting D-Attn LLM can process multimodal tokens longer than the pretrained LLMs original context length. Please refer to Kuo et al. [13] for more details. To effectively address the challenge of varying-length video input, we modify the α-weighting strategy used in the D-Attn framework. This adjustment is designed to balance the contributions of textual and multimodal information. In particular, video lengths (and thus token counts) can vary significantly in the context of In D-Attn, Kuo et al. [13] analytically derive how self-attention can be equivalently temporal retrieval. decomposed into weighted sum of T2T Self-Attn and T2V Cross-Attn. Given text token and sequence of concatenated visual and textual tokens [V, ], where = [v1, v2, , vN ] represents sequence of visual tokens and = [t1, t2, , tM ] represents sequence of text tokens, the attention from to [V, ] can be 4 formulated as where αV = Sigmoid(SV ST ), αT = Sigmoid(ST SV ) = 1 αV . SV and ST are defined as the log sum of exponential of the dot product between and , and between and , respectively. Attn(t, [V, ]) = αV XA(t, ) + αT SA(t, ), (1) SV = log (cid:32) (cid:88) (cid:33) eqtkvn , and ST = log (cid:33) eqtktm , (cid:32) (cid:88) (2) where is the number of visual tokens and is the number of text tokens. In the temporal retrieval task, this formulation leads to an imbalance: while the text query length typically stays within small range, the visual token count can vary dramatically due to differences in video duration. As result, SV tends to be significantly larger for longer videos, which in turn biases αV towards 1 and αT towards 0. This causes the model to overemphasize visual inputs while neglecting the textual query, potentially degrading performance. To mitigate this issue, we simplify the formulation in Vidi by fixing the weighting coefficients, i.e., αV = αT = 1. This ensures balanced contribution from both modalities, regardless of the input video length. The D-Attn framework can be trivially generalized to accommodate audio inputs. Given audio tokens = [a1, a2, , aP ], the attention from text token to the combined multimodal sequence [V, A, ] can be similarly derived as Attn(t, [V, A, ]) = αV XA(t, ) + αA XA(t, A) + αT SA(t, ) XA(t, ) + XA(t, A) + SA(t, ), (3) (4) where we set α-weightings αV = αA = αT = 1. In practice, we observe that this fixed-weight decomposition significantly improves training stability, accelerates convergence, and yields better performance than both the original α-adaptive formulation and fully self-attention."
        },
        {
            "title": "4 Multimodal Alignment",
            "content": "To align the multimodal inputs (vision and audio) with the corresponding timestamps and text in either the inputs or outputs, we adopt three-stage training strategy: 1) adapter training, 2) synthetic data training, and 3) real video training."
        },
        {
            "title": "4.1 Adapter Training",
            "content": "In this stage, we train the adapters from scratch while keeping the weight of the vision/audio encoders and the LLM fixed. Each adapter contains convolution layer for compressing the raw visual or audio tokens, followed by an MLP that maps the compressed representations into the LLM-compatible input space. We leverage the strong publicly available pretrained models for all other components: SigLIP [33] for vision, Whisper [21] for audio, and Mistral [11] for the language model backbone. The adapters are trained on approximately 1 million image and audio caption data, allowing them to learn semantic grounding across modalities while maintaining alignment with the LLM."
        },
        {
            "title": "4.2 Synthetic Data Training",
            "content": "To support large-scale training for temporal alignment, we propose synthetic video generation pipeline based on captioned video and audio datasets with ground-truth timestamps. As shown in Figure 5, we begin by randomly sampling group of candidate samples from large-scale caption corpora, i.e., approximately 25 million images and 400 thousand audio clips. Each image is expanded into synthetic video segment using sliding window approach, with randomized parameters such as window size, starting corner, sliding direction, and sliding speed. This mimics camera movement or visual variation over time. Similarly, audio clips are segmented and spliced in random order to create synthetic audio tracks. The resulting synthetic inputs are arbitrarily long and composed of diverse multimodal segments. Since each segment originates from known captioned sample, we can automatically generate large-scale (timestamp, caption) pairs as supervision. Since 5 Figure 5: An overview of the synthetic training data generation pipeline. Visual and audio segments can be duplicated at multiple timestamps to simulate real videos. images do not match audio, the training input contains only one active modality (vision or audio), while the other is padded with empty tokens. We train the model on two complementary objectives: 1) caption prediction: given timestange range, predict the corresponding caption; 2) timestamp localization: given caption, predict the matching timestamp range. These dual tasks help the model develop deep understanding of both visual/audio content and the temporal axis. After training on synthetic data, the model achieves over 80% accuracy on synthetic evaluation set with multi-minute sequences, indicating strong alignment capability and robust temporal grounding."
        },
        {
            "title": "4.3 Real Video Training",
            "content": "To bridge the gap between synthetic videos and real-world videos, we further train the model on large corpus of 1 million long videos, each annotated with dense supervision in the form of (timestamp, caption) and (timestamp, subtitle) pairs. As shown in Figure 6, each long video is first segmented into short clips (typically 5 30s) using combination of scene boundary detection [16] and subtitle punctuation cues [8]. This process yields approximately 3 500 segments per video, depending on content density and duration. After that, dense captions are generated with state-of-the-art LMMs [3, 35], resulting in fine-grained semantic coverage. Original subtitles are reprocessed into sentence-level units using punctuation-based splitting, enhancing their suitability for timestamped supervision. This pipeline produces over 30 million paired (timestamp, caption) and (timestamp, subtitle) training samples. The training input consists of raw video files containing both visual and audio tracks. We design four training objectives to fully leverage the aligned supervision: Caption prediction: given timestamp range, predict the corresponding dense caption. Subtitle prediction: given timestamp range, predict the associated sentence-level subtitle. Caption-based localization: given caption, predict the matching timestamp range. Subtitle-based localization: given sentence-level subtitle, predict the corresponding timestamp range. These tasks not only reinforce the models ability to align textual descriptions with multimodal signals but also help it adapt to complex video/audio in real-world distributions. 6 Figure 6: An overview of the proposed real video training data generation pipeline. Long videos are first segmented into short clips using scene boundary detection and subtitle punctuation. Then, existing LMMs are used to generate dense, timestamp-aligned captions for each clip, resulting in high-quality supervision for real-world temporal grounding tasks."
        },
        {
            "title": "5 Application Post-training",
            "content": "To support the temporal retrieval task, we build an annotation pipeline to generate user-like search queries and ground-truth timestamps to guide the training phase. As shown in Figure 7, we take advantage of the video clip split in Section 4.3 with dense captions to generate user-style queries and timestamp ranges. Dense Captions. Similar to [28], we generate structural captions to maintain the consistency of text-clip and cover the details in each video clip. Specifically, each caption contains six aspects: 1) subjects of the video, 2) actions of the subjects, 3) scene environment where the action takes place and all visual text overlay on-screen, including logos, subtitles, signs, or other writings, 4) visual style or special effects including video effects, animation, style, composition, and lighting, 5) camera parameters including camera motion, angles, and focal length, 6) background knowledge for common sense reasoning such as famous landmarks, celebrities, or historical events. Query and Timestamp Pairs. To generate high-quality querytimestamp pairs for long videos, we use chain-of-thought (CoT) [30] prompting of LLM. We first combine detailed dense captions and subtitles with timestamps from all video segments to provide comprehensive textual representation of the videos content. The aggregated text is sent to pretrained LLM. The LLM is used to generate concise summary to capture the key events and themes with the guidance from the CoT prompting to comprehend the context of the video. In this way, CoT prompting enables the model to perform intermediate reasoning, leading to more accurate and coherent understanding of video content. To improve format diversity and better simulate real-word scenarios, we generate queries in three levels of granularity: keyword: concise terms representing objects, concepts or entities, such as love, coffee making process, and washing machine. phrase: short descriptions capturing actions or states, like man riding bike, person in deep thought, and enjoying swim in the pool. 7 sentence: complete sentence(s) describing detailed events or scenes, e.g., The majestic presence of volcano surrounded by lush vegetation and shrouded by clouds. Post-processing and Filtering. We design rule-based post-processing step to enhance the quality of generated queries and the corresponding time ranges, formed by the following stages. 1. Merging adjacent time ranges: for each query, we combine consecutive time segments or those separated by small gaps (e.g., 0.5 seconds), provided the query appearing in captions or subtitles. 2. Filtering out low-confidence queries: queries with confidence scores below 0.9 are discarded to ensure reliability. 3. Excluding overly general queries: queries associated with more than 10 timestamps are considered too general. We remove such cases that frequently appear throughout the video. 4. Eliminating machine-style queries: queries exhibiting templated or unnatural phrasing, such as The video concludes... or In the closing moments, will be filtered out to ensure quality. After that, the generated queries and the corresponding time ranges are sent to human annotators to conduct the final stage verification and modification. Figure 7: An illustration of post-training data generation pipeline for temporal retrieval. Queries of varying formats (keywords, phrases, and sentences) are constructed and paired with timestamp annotations from real videos. Two-Round Human Annotation. Although involving filtering process, generated queries and the corresponding time ranges still contain ambiguity of phrasing and misaligned timestamps. To obtain highquality annotations, we further design comprehensive human-in-the-loop refinement process. Annotators begin by examining each query for clarity and relevance. If query is ambiguous or unrealistic, it is required to be rewritten to reflect the video content accurately. After that, annotators review the entire video to identify precise time segments corresponding to the refined queries. Besides, another annotator is required to review annotations independently. Discrepancies identified during this phase will be resolved through discussion or by consulting senior annotator. It is worth mentioning that each query is annotated based on its reliance on visual and/or auditory information. Annotators assign one of the following modality tags to each query: vision, audio, or vision+audio. Vision. The query can be accurately identified purely based on the visual content. Audio information is not necessary for retrieving the segments. Audio. The query depends solely on auditory information. Visual cues are not necessary to identify the video segment. Vision+Audio. The query relies on both visual and auditory information for accurate interpretation and localization within the video. This categorization strategy helps researchers to evaluate the capabilities of the models with more detailed analysis."
        },
        {
            "title": "6 Evaluation Benchmark",
            "content": "We introduce VUE-TR, new evaluation benchmark specifically designed to advance temporal retrieval in real-world scenarios. VUE-TR addresses five critical aspects often overlooked in prior academic benchmarks [6, 12, 9, 14, 7]: video duration, audio support, query format, annotation quality, and metric design. To ensure high-quality supervision, all annotations are manually curated using robust annotation pipeline described in Section 5, which yields significantly more accurate and consistent labels than existing benchmarks."
        },
        {
            "title": "Duration Category",
            "content": "# Videos # Queries Video Hours Ultra-short < 1 min 63 183 0.81 Short 1 10 mins 150 439 11.71 Medium 10 30 mins 150 427 43."
        },
        {
            "title": "Long",
            "content": "Ultra-long 30 60 mins > 60 mins 50 396 34.17 15 153 17."
        },
        {
            "title": "Total",
            "content": "428 1,598 107.87 Table 1: Duration distribution of videos in the proposed VUE-TR evaluation benchmark. The dataset covers wide range of video lengths, from ultra-short clips (< 1 minute) to ultra-long videos (> 1 hour), enabling comprehensive evaluation of temporal retrieval models across diverse real-world scenarios. Figure 8: The distribution of query modality and format in the VUE-TR benchmark. This diverse distribution reflects real-world retrieval scenarios and enables fine-grained analysis of model performance across input types."
        },
        {
            "title": "6.1 Data Statistics",
            "content": "VUE-TR is built using publicly available videos, and the annotations are made accessible to foster open research. As presented in Table 1 and Figure 8, the benchmark consists of 1, 598 queries across 428 videos, spanning over 107 hours. It supports attribute-based slicing for fine-grained performance analysis as follows. 9 Video Duration. Unlike prior datasets limited to short clips, the video length varies from 20 seconds to over 1 hour, covering the full spectrum of durations encountered in real-world scenarios. To facilitate duration-wise evaluation, we categorize videos into five balanced buckets: ultra-short (< 1 min), short (1 10 mins), medium (10 30 mins), long (30 60 mins) and ultra-long (> 60 mins). It enables systematic analysis of model performance as video length increases, which is key challenge for temporal retrieval that prior works fail to capture. Query Modality. While most large vision-language models (LVLMs) operate on vision and text alone, VUE-TR explicitly integrates audio as core input modality. As shown in Figure 8 (left), 47% of the queries involve both visual and audio signals, 35% are vision-only, and 18% are audio-only. This breakdown enables comprehensive evaluation of multimodal capabilities. Query Format. VUE-TR is the first temporal retrieval benchmark to incorporate multiple query formats, reflecting the diversity of user intent in real-word search scenarios. The three formats, ranging from keywords to free-form sentences, are carefully balanced, as shown in Figure 8 (right). Then the model can be evaluated to handle queries of varying linguistic complexity."
        },
        {
            "title": "6.2 Evaluation Metric",
            "content": "To support evaluation of generic temporal retrieval involving multiple timestamp ranges, we re-define IoU (Intersection over Union), precision, and recall along the time axis. As illustrated in Figure 9, the IoU is computed based on the intersection and the union between the predicted time intervals and ground-truth. Figure 9: Definition of intersection and union for temporal retrieval. Both prediction and groundtruth annotations may contain multiple timestamp ranges. The IoU for each sample is computed as (cid:80) I(TP , TG)/ (cid:80) U(TP , TG). For each sample, we compute precision , recall and IoU as the core evaluation metrics. According to the definition in Equation (5), perfect prediction would be exactly the same as the ground-truth timestamp ranges, i.e., = = IoU = 1. To capture performance across varying levels of overlap, we sweep thresholds in the range [0, 1] to generate accuracy-threshold curves. We then compute the area under curve (AUC) for each metric, denoted as , R, IoU, which serve as the final evaluation scores. While AUC provides comprehensive summary, user experience in real-world applications often depends on performance as specific thresholds, e.g., IoU@0.5. Therefore, we also report accuracy-threshold curves (see Figure 10) to facilitate more detailed analysis, while treating AUC as the primary metric for evaluation. = = IoU = (cid:88) (cid:88) (cid:88) I(TP , TG)/ I(TP , TG)/ I(TP , TG)/ (cid:88) (cid:88) (cid:88) TP , TG, U(TP , TG), (5) where I(, ) and U(, ) denote the interaction and union function between predicted timestamps TP and ground-truth TG, respectively. The summation is taken over all overlapping time intervals between prediction and ground-truth."
        },
        {
            "title": "7 Experiment",
            "content": "7."
        },
        {
            "title": "Adapter",
            "content": "Synthetic & Real Videos Post Training lr adapter lr llm mm encoders video duration weight decay optimizer optimizer β1 optimizer β2 optimizer ϵ training steps lr scheduler total batch size dtype deepspeed gradient ckpt 1e-3 frozen frozen 10-600 secs 0.0 AdamW [20] default (0.9) default (0.999) default (1e-8) 2k cosine 128 bfloat16 stage 2 off 5e-6 2e-6 frozen 10-1800 secs 0.0 AdamW [20] default default default 10k cosine 128 bfloat16 stage 3 on 5e-6 2e-6 frozen 10-1800 secs 0.0 AdamW [20] default default default 1 epoch cosine 128 bfloat16 stage 3 on Table 2: Training configurations and hyper-parameters used across different training stages for Vidi. We implement the released Vidi model using three core components: the visual encoder google/siglip -so400m-patch14-384 [33], the audio encoder openai/whisper-large-v3 [21], and the language model mistralai/Mistral-7B-Instruct-v0.3 [11]. Unlike existing video LLMs that use fixed number of uniformly sampled frames, Vidi adopts modality-specific fixed sampling rates: 1.0 fps for video frames and 16, 000 Hz for audio signals. It ensures sufficient coverage of fine-grained visual and auditory details across varying video durations. After sampling, each image frame is independently encoded by SigLip to convert it into sequence of visual tokens. In parallel, the audio waveform is segmented into batches, each independently encoded by Whisper, and then concatenated back into sequence of audio tokens. The complete set of training configurations and hyper-parameters for various stages is provided in Table 2."
        },
        {
            "title": "7.2 Temporal Retrieval Results",
            "content": "We compare our model with three state-of-the-art proprietary models including GPT-4o [10], Gemini-2.0Flash [1] and Gemini-2.5-Pro [1]. There models are chosen for their strong performance and broad modality support, making them competitive baselines for real-world temporal retrieval. Since GPT-4o API does not support audio, we extract frames at 1 fps and feed them as input images. To adapt GPT-4o for temporal retrieval, we design simple custom prompt with in-context examples, ensuring that the output only contains frame index ranges. An example instruction is: The input images are frames from video. to the text query: \"query\". Output the frame indexes that correspond Only output the index range, for example, 2-4, 6-8. We observe that GPT-4o follows this prompt reliably, typically producing clean and parseable frame ranges, which are then converted into time intervals for evaluation."
        },
        {
            "title": "Category",
            "content": "Ultra-Short (< 1m) Short (1 10m) Medium (10 30m) Long (30 60m) Ultra-Long (> 60m) Keyword Phrase Sentence Audio Vision Vision+Audio Overall Metric R IoU R IoU R IoU R IoU R IoU R IoU R IoU R IoU R IoU R IoU R IoU R IoU Vidi 64.5 79.6 54.6 57.4 55.8 40.5 47.4 46.6 32.1 38.9 44.9 27.6 36.7 46.7 27.5 54.4 57.4 39.4 45.3 48.1 32.4 47.5 52.3 34.7 34.2 43.8 24.2 53.6 61.6 43.9 51.2 49.0 33.4 49.0 52.5 35.4 Gemini-2.0-Flash Gemini-2.5-Pro GPT-4o 72.3 65.4 49.2 51.7 50.6 31.6 35.1 34.4 18.2 26.3 12.9 7.1 17.1 8.3 2.9 50.3 33.0 21.9 41.4 36.6 22.3 31.1 34.2 19.7 32.5 35.8 20.3 48.6 38.9 25.4 37.2 30.9 18.4 40.3 34.6 21.2 58.8 69.2 42.6 43.3 41.7 22.7 31.1 22.6 9.9 29.9 11.9 4.9 25.3 5.8 2.4 47.9 33.4 17.4 41.7 31.1 16.0 30.2 23.3 12.6 30.9 25.5 10.2 48.2 37.9 21.8 33.9 23.4 12.1 39.3 28.9 15.2 53.6 42.1 32.5 30.0 26.5 15.6 18.2 21.1 9.1 18.2 17.7 9.2 20.4 19.6 9.5 37.4 26.4 18.3 25.1 23.7 13.2 17.5 22.2 10.0 7.3 16.5 3.7 41.3 30.5 22.0 21.2 22.0 11.1 25.9 24.0 13. Table 3: Performance of different models on the VUE-TR benchmark across various evaluation attributes. and represent the AUC (Area Under Curve) values for precision and recall, respectively; while IoU denotes the AUC of intersection-over-union between prediction and ground-truth timestamp ranges, as defined in Section 6.2. All Gemini and GPT models are accessed via the Azure API. Gemini models are evaluated by directly uploading videos. To comply with the 100 MB upload limit on Azure, long videos are resized to resolution of 256 pixels. Compared to Gemini-2.0-Flash, Gemini-2.5-Pro incurs higher latency and token cost for reasoning and exhibits higher content filtering rate, often resulting in empty outputs. GPT-4o is constrained by the Azure APIs 120-frame limit. For videos longer than 120 seconds, we uniformly sample 120 frames. 12 Unlike GPT-4o, Gemini models [1] naturally support text, vision and audio with extremely long context length, making them ideal for long video understanding. Gemini can also take raw video files as input. We evaluate two versions: Gemini-2.5-Pro (exp/preview): Offers strong reasoning and often outputs explicit Chain-of-Thought (CoT) explanations. However, when not constrained by output formatting instructions, the responses can be highly inconsistent and challenging to parse. Conversely, adding strict format requirements improves consistency but leads to degraded performance. Therefore, we choose the simple prompt without format requirement and parse the major possible formats for evaluation. Gemini-2.0-Flash (stable): Provides better output structure and lower rejection rates, making it more reliable for batch evaluations. Although slightly less capable in reasoning than 2.5-Pro, it balances performance and robustness well. In Table 3, we report the performance across multiple evaluation dimensions: video duration, query format, and query modality. Remarkably, Vidi outperforms all baseline models across all categories for the primary metric IoU. On ultra-short videos, Gemini-2.0-Flash achieves the best precision, but its recall is significantly lower, leading to lower IoU. Performance across different query formats is relatively consistent, though models generally struggle more with longer and more descriptive queries. As expected, GPT-4o performs poorly on audio-only queries due to lack of audio input support. Other models perform comparably across modalities, but accuracy on audio-based queries is consistently lower than that on vision-based queries, highlighting the challenge of audio-conditioned understanding. As shown in Figure 10, we can still observe that Vidi consistently outperforms all competitors by significant margin across the entire range of thresholds. This performance gap even widens at higher thresholds (e.g., 0.5), where other models degrade much more rapidly. It shows Vidis strength in fine-grained temporal precision, critical capability for real-world video editing and retrieval tasks. Figure 10: Overall performance curves for temporal retrieval on the VUE-TR benchmark. We report accuracy across varying thresholds for different models."
        },
        {
            "title": "8 Related Work",
            "content": "Benchmarks. Recent advancements in video question answering (VideoQA) have been sharped by benchmarks such as CinePile [22], Video-MME [5], MovieChat-1K [25], LVBench [29], and LongVideoBench [31]. To evaluate models on long-form videos, many of these benchmarks demonstrate strong performance with only sparse frame sampling. They may not fully reflect models video understanding capabilities. For example, LLaVA-OneVision [15] only employs as few as 32 uniformly sampled frames to achieve 66.3% accuracy on Video-MME [5]. In contrast, temporal retrieval requires fine-grained understanding across the full temporal span of video instead of few scattered glimpses. As such, temporal retrieval serves as more rigorous benchmark for evaluating models true capability in video understanding. Charades-STA [6] and DiDeMo [9] primarily focus on action grounding within short clips ( 30s). QVHighlights [14] and ActivityNet Captions [12] extend to several minutes videos, covering event descriptions and subjectively defined highlights. Recently, LongVALE [7] introduces multimodal audio-visual queries, but remains constrained to relatively short durations. 13 To address above limitations, our benchmark is designed to evaluate model performance in realistic, large-scale video environments. In particular, we extend the video length to over one hour, and introduce video duration categorization into ultra-short, short, medium, long, and ultra-long segments. Furthermore, we diversify the retrieval challenge by varying query format (keyword, phrase, sentence) and modality (visiononly, audio-only, vision+audio), enabling more comprehensive understanding of model capabilities. Video LMMs. Video understanding has experienced significant progress with the emergence of LMMs that integrate visual and textual information. Models such as InternLM-XComposer [34, 4, 35], QwenVL [2, 27, 3], and LLaVA [19, 31] series have introduced innovative architectures and training strategies. InternLM-XComposer adopts modular architecture that integrates interleaved text-image composition and comprehension with rich multilingual knowledge. Qwen-VL and its successors introduce flexible multi-image and box-conditioned input capabilities, supporting multilingual and multi-turn interaction for diverse visionlanguage tasks. The LLaVA series bridge visual perception with LLMs through unified representation space, and its recent extensions like LLaVA-NeXT further support video and 3D input via interleaved multimodal pretraining [17]. While these models demonstrate impressive general-purpose reasoning, they typically operate on limited number of frames (e.g., 64), which constrains their effectiveness in long-form temporal retrieval. Temporal Retrieval. Early methods [14, 6, 9, 12] usually employ proposal-based strategies, where models first generate candidate segments and then rank them based on their semantic relevance to the query. With the emergence of video LMMs, recent research has shifted toward enhancing video-text alignment through unified frameworks. For instance, TimeChat [23] connects time-aware frame encoder and sliding video Q-Former [18] to deal with temporal retrieval based on an instruction-tuning dataset. TimeSearch [? ] proposes temporal spotlight grounding strategy to find key events and temporal reflection mechanism to verify time range predictions and guide the search direction. Ye et al. [32] revisit temporal search in long-form videos by proposing lightweight framework that reformulates temporal retrieval as spatial search problem. Although efficient, the above methods rely on sampling only 8 96 frames, which may be insufficient to capture fine-grained temporal cues. Despite these advancements, most existing methods still focus on short videos or rely on sparse frame sampling, and primarily limited to vision-only inputs. These constraints hinder the evaluation of models full multimodal reasoning abilities, particularly in understanding and retrieving moments from long, complex videos involving both visual and auditory modalities. In this work, we propose Vidi, new approach that jointly incorporates text, vision and audio input from hour-long videos to retrieve the most relevant temporal segments based on free-form natural language queries. Our setting pushes beyond existing benchmarks by requiring fine-grained temporal understanding across extended multimodal content."
        },
        {
            "title": "9 Conclusion",
            "content": "We introduce Vidi, large multi-modal model for real-world video understanding and editing (VUE). The first release focuses on temporal retrieval (TR), which is foundational step in video trimming and editing, by localizing relevant segments from long videos given natural language queries. To support robust evaluation under realistic conditions, we propose VUE-TR, new benchmark for practical video editing use cases. VUE-TR introduces key improvements over prior datasets in terms of video duration, audio support, query format, annotation quality, and metric design for multi-span timestamp evaluation. Vidi leverages unified vision-audio-language architecture with modality-aware sampling and precise token alignment. Besides, the Decomposed Attention mechanism allows the model to capture fine-grained temporal cues across diverse modalities and scales seamlessly to long videos. As demonstrated on the VUE-TR benchmark, Vidi consistently outperforms leading proprietary models such as GPT-4o and Gemini. For future work, we plan to extend Vidi to support broader spectrum of video understanding tasks, including temporal VQA, and high-level video understanding. Additionally, we aim to explore interactive editing capabilities to further bridge the gap between large models and real-world VUE applications."
        },
        {
            "title": "10 Contributors",
            "content": "Core Contributors - Research (alphabetical order) Chia-Wen Kuo, Dawei Du, Fan Chen, Guang Chen, Sijie Zhu. Core Contributors - Infrastructure (alphabetical order) Celong Liu, Tong Jin. Research Leads Longyin Wen, Xiaohui Shen. Contributors (alphabetical order) Jiamin Yuan, Lingxi Zhang, Lu Guo, Lusha Li, Qingyu Chen, Rachel Deng, Stuart Siew, Wei Lu, Wen Zhong, Xin Gu, Xing Mei, Xueqiong Qu."
        },
        {
            "title": "References",
            "content": "[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaıs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. [4] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. CoRR, abs/2401.16420, 2024. [5] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CoRR, abs/2405.21075, 2024. [6] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. TALL: temporal activity localization via In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, language query. October 22-29, 2017, pages 52775285. IEEE Computer Society, 2017. [7] Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng. Longvale: Vision-audio-language-event benchmark towards time-aware omni-modal perception of long videos. CoRR, abs/2411.19772, 2024. 15 [8] Oliver Guhr, Anne-Kathrin Schumann, Frank Bahrmann, and Hans-Joachim Bohme. Fullstop: Multilingual deep models for punctuation prediction. 2957, 2021. [9] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan C. Russell. Localizing moments in video with natural language. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 58045813. IEEE Computer Society, 2017. [10] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, and et al. Gpt-4o system card. CoRR, abs/2410.21276, 2024. [11] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. [12] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 706715. IEEE Computer Society, 2017. [13] Chia-Wen Kuo, Sijie Zhu, Fan Chen, Xiaohui Shen, and Longyin Wen. Rethinking homogeneity of vision and text tokens in large vision-and-language models. CoRR, abs/2502.01906, 2025. [14] Jie Lei, Tamara L. Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. pages 1184611858, 2021. [15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. [16] Congcong Li, Xinyao Wang, Dexiang Hong, Yufei Wang, Libo Zhang, Tiejian Luo, and Longyin Wen. Structured context transformer for generic event boundary detection. CoRR, abs/2206.02985, 2022. [17] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llavanext-interleave: Tackling multi-image, video, and 3d in large multimodal models. CoRR, abs/2407.07895, 2024. [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image In International Conference on pre-training with frozen image encoders and large language models. Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2023. [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [21] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR, 2023. [22] Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. CoRR, abs/2405.08813, 2024. 16 [23] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal In IEEE/CVF Conference on Computer Vision large language model for long video understanding. and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1431314323. IEEE, 2024. [24] Morgane Rivi`ere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, and et al. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118, 2024. [25] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang. Moviechat: From dense token to sparse memory for long video understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1822118232. IEEE, 2024. [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. [27] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. [28] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, Fei Yang, Pengfei Wan, and Di Zhang. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. CoRR, abs/2410.08260, 2024. [29] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark. CoRR, abs/2406.08035, 2024. [30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [31] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [32] Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, and Manling Li. Re-thinking temporal search for long-form video understanding. CoRR, abs/2504.02259, 2025. [33] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1194111952. IEEE, 2023. [34] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlmxcomposer: vision-language large model for advanced text-image comprehension and composition. CoRR, abs/2309.15112, 2023. 17 [35] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. CoRR, abs/2407.03320, 2024."
        }
    ],
    "affiliations": [
        "Intelligent Creation, ByteDance Inc. San Jose/Seattle, US"
    ]
}