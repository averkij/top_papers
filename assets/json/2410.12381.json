{
    "paper_title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks",
    "authors": [
        "Fengji Zhang",
        "Linquan Wu",
        "Huiyu Bai",
        "Guancheng Lin",
        "Xiao Li",
        "Xiao Yu",
        "Yue Wang",
        "Bei Chen",
        "Jacky Keung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs -- core capabilities for advancing Artificial General Intelligence. Despite the progress in Large Multimodal Models (LMMs), which extend LLMs with visual perception and understanding capabilities, there remains a notable lack of coding benchmarks that rigorously assess these models, particularly in tasks that emphasize visual reasoning. To address this gap, we introduce HumanEval-V, a novel and lightweight benchmark specifically designed to evaluate LMMs' visual understanding and reasoning capabilities through code generation. HumanEval-V includes 108 carefully crafted, entry-level Python coding tasks derived from platforms like CodeForces and Stack Overflow. Each task is adapted by modifying the context and algorithmic patterns of the original problems, with visual elements redrawn to ensure distinction from the source, preventing potential data leakage. LMMs are required to complete the code solution based on the provided visual context and a predefined Python function signature outlining the task requirements. Every task is equipped with meticulously handcrafted test cases to ensure a thorough and reliable evaluation of model-generated solutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1 and 36.4% pass@10, while open-weight models with 70B parameters score below 4% pass@1. Ablation studies further reveal the limitations of current LMMs in vision reasoning and coding capabilities. These results underscore key areas for future research to enhance LMMs' capabilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 2 1 8 3 2 1 . 0 1 4 2 : r HUMANEVAL-V: EVALUATING VISUAL UNDERSTANDING AND REASONING ABILITIES OF LARGE MULTIMODAL MODELS THROUGH CODING TASKS Fengji Zhang* 1 Linquan Wu* 1 Huiyu Bai* 1 Guancheng Lin* 2 Xiao Li3 Xiao Yu4 Yue Wang5 Bei Chen 5 Jacky Keung 1 1City University of Hong Kong 2Wuhan University 3Tsinghua University 4Zhejiang University 5Rhymes AI"
        },
        {
            "title": "ABSTRACT",
            "content": "Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs core capabilities for advancing Artificial General Intelligence. Despite the progress in Large Multimodal Models (LMMs), which extend LLMs with visual perception and understanding capabilities, there remains notable lack of coding benchmarks that rigorously assess these models, particularly in tasks that emphasize visual reasoning. To address this gap, we introduce HumanEval-V, novel and lightweight benchmark specifically designed to evaluate LMMs visual understanding and reasoning capabilities through code generation. HumanEval-V includes 108 carefully crafted, entry-level Python coding tasks derived from platforms like CodeForces and Stack Overflow. Each task is adapted by modifying the context and algorithmic patterns of the original problems, with visual elements redrawn to ensure distinction from the source, preventing potential data leakage. LMMs are required to complete the code solution based on the provided visual context and predefined Python function signature outlining the task requirements. Every task is equipped with meticulously handcrafted test cases to ensure thorough and reliable evaluation of model-generated solutions. We evaluate 19 state-ofthe-art LMMs using HumanEval-V, uncovering significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1 and 36.4% pass@10, while open-weight models with 70B parameters score below 4% pass@1. Ablation studies further reveal the limitations of current LMMs in vision reasoning and coding capabilities. These results underscore key areas for future research to enhance LMMs capabilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark."
        },
        {
            "title": "INTRODUCTION",
            "content": "Coding ability is essential for both the development and evaluation of Large Language Models (LLMs) (Sun et al., 2024). By enabling LLMs to solve complex tasks by decomposing them into modular sub-tasks, coding fosters more autonomous and efficient interactions with the world (Patil et al., 2023; Liu et al., 2023b; Schick et al., 2024). As result, coding tasks serve as valuable testbed for advancing research in Artificial General Intelligence (Bubeck et al., 2023). Recently, Large Multimodal Models (LMMs) composed of billions of parameters have emerged, with notable examples such as GPT-4o (OpenAI, 2024) and Claude 3.5 Sonnet (Anthropic, 2024), demonstrating remarkable capabilities in understanding and reasoning within visual contexts. While several recent multimodal benchmarks offer evaluations across wide range of vision-related tasks (Goyal et al., 2017; Singh et al., 2019; Lu et al., 2022; Liu et al., 2023c; Yue et al., 2024), there remains significant gap in benchmarks specifically designed for coding scenarios. These Equal contribution. Correspondence to fengji.zhang@my.cityu.edu.hk 1 Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-4o and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A. conventional benchmarks typically involve multiple-choice or open-ended questions based on commonsense reasoning, neglecting more complex reasoning scenarios like coding. Notably, coding is valuable form to assess LLMs complex reasoning abilities and has been exploited in various reasoning tasks such as mathematical reasoning (Madaan et al., 2022; Gao et al., 2023). It demands the ability to understand high-level instructions, apply complex logic, and implement functional programs. Another appealing benefit of coding is that it allows for more reliable evaluation of reasoning correctness through deterministically program execution and unit testing. However, research on multimodal coding evaluation remains largely underexplored. To address this gap, we introduce HumanEval-V, novel and lightweight benchmark tailored to evaluate LMMs in multimodal coding scenarios. HumanEval-V consists of 108 manually crafted code generation tasks sourced from platforms such as CodeForces and Stack Overflow. Each task is adapted from the source by carefully modifying the original problems context and algorithmic patterns as well as redrawing the visual elements. As an example task shown in Figure 1, each task involves completing Python function based on single image, function signature, and problem descriptions provided in the comment block. These tasks require reasoning over both visual and textual contexts to complete function, with the correctness of the predicted solution assessed using reliable set of human-annotated test cases. HumanEval-V is novel in that it is the first benchmark where visual information plays an essential role in solving coding tasks. For instance, the diagram in Figure 1 not only indicates the available position options for the function inputs, but also offers important clues for determining whether two lines intersect, significantly complementing the function signature and problem descriptions. To solve these tasks, models have to accurately understand the nuances of the image, such as the position of two lines on the circle and tick labels. Moreover, they need the ability to perform cross-modal reasoning, integrating visual elements with the structured function signature and textual problem descriptions cohesively. In contrast to other benchmarks (Li et al., 2024b), which suggest that visual information has limited impact on coding performance, HumanEval-V ensures that all coding tasks are unsolvable without the visual context. Textual descriptions in the coding tasks are minimized to prevent models from relying solely on textual information to infer solutions. The visual contexts in HumanEval-V encompass diverse range of visual elements, each providing essential information for understanding and solving the coding tasks. curated selection of representative images is provided in Appendix B. Another compelling characteristic of HumanEval-V is its lightweight nature and ease of testing. It mirrors the difficulty of well-established code generation benchmarks like HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) that target entry-level programmers. The simplicity of evaluation has been one of the key reasons for the wide adoption of these benchmarks. In HumanEval-V, each task is formulated in Python code completion setting like HumanEval and annotated with comprehensive suite of test cases in format of assertion statements, making it easy to execute and efficient to measure the correctness of the completion. Additionally, the tasks are restricted to using only common Python libraries, promoting the accessibility without requiring domain-specific knowledge and avoiding compatibility issues with different library versions. We perform cross-validation among several annotators to ensure the data integrity. 2 Figure 2: The HumanEval-V construction pipeline, with representative examples for each stage. Through extensive experiments with 19 state-of-the-art LMMs, we have the following key findings: (1) Even leading proprietary models like GPT-4o achieve only 13% pass@1 on HumanEval-V, while open-weight models perform much worse, with none of them exceeding 4% pass@1. HumanEval-V reveals limitations of current LMMs. (2) Proprietary models significantly outperform open-weight LMMs, highlighting the challenges in developing more advanced open-weight models. (3) Current LMMs remain limited in their visual reasoning abilities, as evidenced by the significant performance gains when provided with human-annotated textual descriptions of the images. (4) Open-weight LMMs suffer from deteriorated coding performance after integrating the vision encoder. These findings emphasize the need for future research to enhance LLMs visual reasoning and coding abilities."
        },
        {
            "title": "2 BENCHMARK CONSTRUCTION",
            "content": "As shown in Figure 1, each coding task in HumanEval-V consists of three main components. The first component is single image input, denoted as I, which provides the essential visual context necessary to solve the coding problem. The second component is Python function signature, denoted as σ, which specifies the function name, input parameters, and return type, accompanied by brief problem description in the comment block. Both the image and the function signature σ are formatted into predefined prompt template, which is then provided to the LMM. The models output, denoted as O, represents the complete Python function generated by the LMM based on σ and I. The third component is set of test cases = {t1, t2, . . . , tn}, which are used to validate the functional correctness of through execution. solution is considered correct if passes all test cases, meaning it produces the expected outputs for each ti . Before constructing HumanEval-V, we establish rigorous standards to ensure the quality of the coding task annotations: (1) the visual context provided must be essential for solving the task, with all relevant information contained within single image; (2) the coding task should be largely selfexplanatory through its visual context, requiring minimal textual descriptions; and (3) the coding task should target entry-level programmers and be solvable using only common Python libraries. As shown in Figure 2, the construction of HumanEval-V follows collect-adapt-mutate pipeline. First, we collect coding problems with visual contexts from platforms such as CodeForces and Stack Overflow, identifying those suitable for adaptation based on the aforementioned standards (Section 2.1). Next, we modify the selected problems by adapting their task descriptions and redrawing the visual elements to ensure they meet our quality requirements. During this stage, we annotate each task with function signature (σ), set of test cases (T ), and ground truth solution. To further expand the benchmark, some tasks undergo mutations, generating similar yet distinct versions by introducing changes to the coding tasks visual patterns while preserving the core context. This iterative process results in final set of 108 code generation tasks (Section 2.2). After constructing the benchmark, we perform rigorous validation to ensure that each coding task aligns with the standards (Section 2.3). Finally, we provide detailed benchmark statistics for reference (Section 2.4)."
        },
        {
            "title": "2.1 DATA COLLECTION AND SCREENING",
            "content": "The coding tasks in HumanEval-V are sourced from prominent Q&A and coding challenge platforms such as Stack Overflow and CodeForces. These platforms offer diverse range of coding problems and are also commonly used in the development of well-established benchmarks for code generation (Yin et al., 2018; Lai et al., 2023; Wang et al., 2023; Li et al., 2023b; Jain et al., 2024; Wu et al., 2024b). From these sources, we collect large set of coding problems that incorporate visual elements in their problem descriptions. However, the collected problems are unsuitable for direct inclusion in HumanEval-V. In most cases, the visual context is non-essential for solving the task, with the problem primarily solvable through rich textual descriptions alone. This makes it challenging to adapt such problems into our benchmark, which emphasizes visual reasoning abilities. Therefore, we focus on identifying tasks that already feature high-quality visual elements and present moderate level of difficulty. After thorough screening process, we retain 40 candidate coding tasks out of the thousands reviewed for further adaptation. detailed discussion of the challenges encountered during data collection and screening, along with demonstrating examples, is provided in Appendix D.1."
        },
        {
            "title": "2.2 CODING TASK ANNOTATION",
            "content": "The annotation process begins by adapting the screened coding problems. For each of the 40 selected coding tasks, we first identify and summarize the essential context and algorithmic patterns required to solve the problem. We then create new coding problem by modifying the context and patterns of the original problem and redrawing the corresponding images. This is to prevent data leakage and ensure consistency with the standards of HumanEval-V. Detailed examples of the problem adaptation can be found in Appendix D.2. During adaptation, we ensure that all critical visual information for each coding task is encapsulated within single image. The coding tasks in HumanEval-V span variety of visual elements, including trees, graphs, matrices, maps, grids, flowcharts, and other abstract representations. This diversity allows for comprehensive testing of the models visual reasoning abilities. Next, we define Python function signature for each task, beginning with the input and output specifications. To simplify the Input/Output (I/O) formats, we prioritize basic data structures such as numbers, strings, lists, and dictionaries. After finalizing the image and I/O definitions, we craft concise problem description that directs models to rely primarily on the visual information to complete the Python function. Once the task definition is completed, we manually construct test cases and implement ground truth solution for each coding problem to ensure its validity. To further verify the comprehensiveness of the test cases, we perform statement and branch coverage analysis on the ground truth solution, ensuring that all logical branches and execution paths are thoroughly tested. Following the initial annotation of the 40 coding tasks, we conduct an additional round of mutationbased extensions. This process expands the number of coding tasks based on the initial annotations, by creating similar yet distinct coding tasks. The mutated tasks retain most of the original visual elements but incorporate different algorithms to solve. For example, we can change the rule of the coding task in Figure 1 by just considering the situation where the line segments intersect within the circle, regardless of outside the circle. It is important to note that not all of the 40 tasks are suitable for mutation. For each suitable task, we create one or two mutations, resulting in total of 108 coding tasks in HumanEval-V. Examples of the mutation process are provided in Appendix D."
        },
        {
            "title": "2.3 QUALITY ASSURANCE",
            "content": "We implement rigorous quality assurance process to ensure the quality of HumanEval-V. The annotation team consists of three programmers, each with over four years of Python programming experience. During each of the data collection, adaptation, and mutation stages, annotators independently perform annotations based on pre-defined guidelines. After that, all annotators conduct cross-validation process to review and resolve any identified issues. coding task is only finalized when consensus is reached among all annotators. Additionally, one annotator maintains consistent formatting and style across all visual representations and coding tasks. Each annotator dedicates over 200 hours to the overall benchmark construction process. To validate the reliance on visual context, we ensure that GPT-4o cannot solve any of the coding tasks without access to the images, 4 confirming the essential role of visual information. Finally, to facilitate continuous improvement, we will publish an online data viewer for HumanEval-V after the review period, where the community can review the dataset and report issues."
        },
        {
            "title": "2.4 DATASET STATISTICS",
            "content": "Med Attributes Avg Min Max 1024 709 106 14 10 998.2 729.0 111.3 16.3 9.8 Image Width (px) Image Height (px) Textual Token Count GT Code Statements Test Cases Count To provide clearer understanding of our benchmark, Table 1 presents key statistics for several dataset attributes. Each coding task includes single image input, with the image dimensions constrained to maximum of 1024 pixels in height or width, to prevent overly long or complex visual contexts. The average image width and height are 998.2 and 729 pixels, respectively. We also analyze the length of function signatures using the OpenAI tiktoken1 tokenizer. The longest function signature consists of 230 tokens, while the average token count is 111.3, demonstrating high succinctness. We also quantify the complexity of the ground truth (GT) code solutions annotated by human experts. On average, GT solutions contain 16.3 code statements, encompassing import statements, function definitions, and the function body, reflecting the relative simplicity of the tasks. Finally, we provide statistics on the number of test cases used for evaluation, with an average of 9.8 test cases per task. We ensure the test cases achieve full statement and branch coverage on the GT solutions, guaranteeing rigorous testing of the generated code. We also include detailed list of the I/O types and module dependencies in Appendix D.4. Table 1: The descriptive statistics for the key attributes of HumanEval-V, showcasing the Median, Average, Minimum, and Maximum values. 1024 1024 230 44 16 596 216 59"
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "Models: We conduct comprehensive evaluation of 19 state-of-the-art LMMs to assess the current progress in visual reasoning and coding capabilities. Our selection includes representative set of the most advanced proprietary and open-weight models. Specifically, we evaluate five of the latest proprietary models: GPT-4o (0513), GPT-4o-mini (0718) (OpenAI, 2024), Gemini 1.5 Pro (001), Gemini 1.5 Flash (001) (Google, 2024), and Claude 3.5 Sonnet (0620) (Anthropic, 2024). In addition, we test 14 top-performing open-weight models, selected based on their rankings on the OpenVLM Leaderboard (Duan et al., 2024). These models span various parameter sizes to explore the impact of scale on performance in the HumanEval-V benchmark. The open-weight models include Phi-3-Vision (4.2B) (Microsoft, 2024a), Phi-3.5-Vision (4.2B) (Microsoft, 2024b), LLaVA-OneVision (8.0B, 73.2B) (Li et al., 2024a), MiniCPM-V 2.5 (8.5B) and 2.6 (8.1B) (Yao et al., 2024b), InternVL-Chat-V1.5 (26.0B) (Chen et al., 2023), InternVL-2 (4.2B, 8.1B, 25.5B, 40.1B) (OpenGVLab, 2024), and Qwen2-VL (8.3B, 73.4B) (Wang et al., 2024). We deliberately include different versions within the same model series, such as Phi-3-Vision and Phi-3.5-Vision, MiniCPM-V 2.5 and 2.6, as well as InternVL-Chat-V1.5 and InternVL-2, to investigate whether iterative improvements in model development result in enhanced performance on HumanEval-V. More details of the models can be found in Appendix G. Prompting, Hyper-parameters, and Post-processing: All the LMMs evaluated in our experiments have been trained on instruction-following or conversational data. To align with this, we employ conversational prompt template, formatted in Markdown, as illustrated in Figure 3, to prompt the LMMs to generate code solutions for the tasks in HumanEval-V. For hyper-parameters, we follow the established practices in code generation benchmarking (Chen et al., 2021; Austin et al., 2021; Chen et al., 2022), using two distinct settings. First, we employ greedy search to generate single code solution from each LMM, allowing us to assess the models performance in deterministic setting. Additionally, we sample 20 code solutions using Top-p sampling method with = 0.95 and relatively high temperature of 0.8. This setting is designed to explore the likelihood of the models generating correct solutions when given more opportunities. 1https://github.com/openai/tiktoken 5 Given the moderate complexity of the benchmark, we set the maximum number of new tokens for code generation to 1024. Early stopping is triggered by n```n, since the LMMs are instructed to enclose the generated code within Markdown code block. We also develop postprocessing pipeline to extract valid code solutions from the model outputs. This pipeline identifies and extracts the content within the Markdown code block and uses an abstract syntax tree parser to detect any generated import statements, along with class and function definitions. These components are then concatenated to form the final predicted solution for test-execution-based evaluation. Evaluation Metrics Following established practices in code generation (Chen et al., 2021; Austin et al., 2021; Chen et al., 2022), we use the pass@k metric to evaluate the functional correctness of the generated code solutions. For each coding task, code samples are generated, and solutions are randomly selected from these samples to be tested against ground truth test cases. task is considered solved if at least one of the selected solutions passes all test cases. The pass@k score is then calculated as the percentage of successfully solved tasks. We report pass rate results for = 1, 10. For greedy search, we set = 1 to compute pass@1. For sampling-based evaluation, we set = 20 to calculate pass@10. Figure 3: The prompting template used for LMMs to generate code solutions. The {code context} placeholder is for the corresponding function signature. We incorporate second evaluation metric: Parsing Success Rate. This metric measures the syntactic correctness of the generated code, independent of its functional accuracy. solution is considered compliant if it passes Pylint (Wikipedia, 2024) checks without triggering critical errors or fatal issues, such as undefined variables, attribute access errors, or import failures, regardless of passing the test cases. The Parsing Success Rate is calculated as the proportion of code samples that pass these checks out of all generated samples."
        },
        {
            "title": "4.1 MAIN EXPERIMENTS",
            "content": "We evaluate 19 state-of-the-art LMMs on HumanEval-V, with results presented in Table 2. Based on the results, we have the following key findings: Current LMMs performance is underwhelming on our benchmark: While proprietary models like GPT-4o and Claude 3.5 Sonnet show the best results, even their highest pass@1 scores (13% and 18.5% respectively) fall short of expectations. Moreover, there remains substantial performance gap between proprietary and open-weight models. Open-weight models spanning 4B to 76B parameters exhibit particularly weak performance, with none exceeding 4% pass@1. This is surprising given that the coding tasks in our benchmark are designed for entry-level programmers with simplified problem context. None of the open-weight models with fewer than 70B parameters achieve more than 4% pass@10. Even the best-performing model, GPT-4o, achieves only 36.4% pass@10, showing there is much room for improvement. In terms of parsing success rate, we observe rough correlation with the pass rate. Most LMMs exhibit high parsing success rate, while smaller-scale open-weight models show lower success rates. Most failed cases are due to common syntax errors, such as unclosed parentheses, generating code repeatedly without termination, or encountering list index out-of-range issues. To further investigate, we perform another experiment increasing the number of samples to evaluate model performance, as detailed in Appendix C.1. Overfitting leads to hallucination errors in LMMs generated solutions: Upon examining many incorrect solutions produced by the LMMs, we identify recurring issue: the models tend to generate solutions based on the context of the original problems rather than the new versions of coding tasks in our benchmark. For instance, both GPT-4o and Claude 3.5 Sonnet fail to produce correct solutions for the coding task shown in Figure 1, as they mistakenly assume that the numbers in the image are arranged in clockwise order. Furthermore, their solutions rely on the assumption that the two line segments can only intersect within the circle, which reflects the context of the original"
        },
        {
            "title": "Params",
            "content": "Pars. Rate pass@k k=1 k=10 k=1 k= GPT-4o GPT-4o-mini Claude 3.5 Sonnet Gemini 1.5 Pro Gemini 1.5 Flash InternVL-2 Qwen2-VL LLaVA-OneVision InternVL-V1.5 MiniCPM-V 2.6 MiniCPM-V 2.5 Phi-3.5-Vision Phi-3-Vision Proprietary 97.2 100 99.1 97.2 99. Open-Weight 81.5 91.7 74.1 85.2 91.7 76.3B 40.1B 25.5B 8.1B 4.2B 73.4B 8.3B 73.2B 8.0B 25.5B 8.1B 8.5B 4.2B 4.2B 100 95. 99.1 83.3 70.4 85.2 94.4 95.4 87.0 98.0 99.8 99.0 97.5 97.5 77.8 87.6 69.8 78.8 92.1 97.2 73.8 97.1 85. 71.5 79.9 85.1 91.6 82.2 13.0 6.5 18.5 10.2 8.3 3.7 0.0 0.0 0.9 0.0 3.7 0.0 1.9 0.9 0.0 0.9 0.0 0.9 0. 36.4 15.4 25.9 22.2 13.2 12.8 1.6 3.2 2.6 2.3 16.0 1.6 12.4 1.9 2.1 2.2 2.3 1.6 2.6 Table 2: Performance of 19 LMMs on HumanEval-V. Scores are shown as percentages, with the highest values highlighted in bold. We also include model size (Params) and code Parsing Success Rate (Pars. Rate). Figure 4: Correlation analysis between HumanEval-V pass@10 results and three popular multimodal benchmarks spanning multidisciplinary abilities. problem on the CodeForces platform, rather than our adapted version. We attribute these hallucination errors to that LMMs overfit on the previously seen data. This observation underscores the necessity of our adaptation process, which aims to minimize data leakage and prevent models from relying on memorized patterns. Larger parameter size does not guarantee better performance in open-weight models: While open-weight LMMs with over 70B parameters show superior results, smaller models (ranging from 4B to 40B parameters) exhibit highly variable performance. For example, Phi-3-Vision (4.2B) and InternVL-2 (4.2B) achieve pass@10 scores of 2.6% and 2.3%, outperforming larger models like QwenVL2 (8.3B) and InternVL-2 (40.1B). Notably, iterations of the Phi-Vision (33.5) and MiniCPM-V(2.52.6) series do not lead to consistent performance improvements. This inconsistency may be attributed to several factors. One possibility is the varying quality and scale of the training data used for each model, which can impact their generalization ability. Our benchmark reveals unique weaknesses in LMMs: Open-weight LMMs, such as Qwen2VL (Wang et al., 2024) and InternVL-2 (OpenGVLab, 2024), have demonstrated comparable or even superior performance to proprietary LMMs on popular multimodal benchmarks like MMMU (Yue et al., 2024), MathVista (Lu et al., 2023), and MMVet (Yu et al., 2023). However, these models perform significantly worse on HumanEval-V, suggesting that our benchmark exposes previously undetected limitations in current LMMs. The three mentioned benchmarks evaluate broad range of multidisciplinary abilities, focusing on visual understanding, reasoning, and general knowledge through formats such as question-answering or multiple-choice questions, using accuracy as the primary evaluation metric. By contrast, HumanEval-V adopts unique evaluation approach based on coding tasks, where visual contexts are tightly integrated with algorithmic patterns, presenting distinct challenge that differs from existing benchmarks. To further investigate this discrepancy, we perform correlation analysis between HumanEval-V and the three mentioned benchmarks. We collect the performance results of the 19 evaluated LMMs from the OpenVLM Leaderboard (Duan et al., 2024) as well as from corresponding papers and reports, and compare them to"
        },
        {
            "title": "Image Only",
            "content": "Desc. Only Image & Desc. pass@1 pass@10 pass@1 pass@ pass@1 pass@10 GPT-4o GPT-4o-mini InternVL-2 Qwen2-VL MiniCPM-V 2.6 MiniCPM-V 2.5 Phi-3.5-Vision Phi-3-Vision CodeStral DSCoder-V2-Lite Yi-Coder-Chat DSCoder-V1.5 76.3B 25.5B 8.1B 4.2B 73.4B 8.3B 8.1B 8.5B 4.2B 4.2B 22.2B 15.7B 8.8B 6.9B Large Multimodal Models 45.432.4 33.326. 36.4 15.4 13.0 6.5 3.7 0.0 0.9 0.0 3.7 0.0 0.9 0.0 0.9 0.0 12.8 3.2 2.6 2. 16.0 1.6 2.2 2.3 1.6 2.6 12.08.3 2.82.8 3.72.8 5.65.6 20.416.7 5.65.6 3.72.8 0.90.9 0.00.9 3.73."
        },
        {
            "title": "Large Code Language Models",
            "content": "67.931.6 46.130.7 41.128.3 15.712.5 10.37.8 16.213.9 38.922.9 13.511.9 7.14.8 14.612.2 9.88.2 10.07.5 44.431.5 35.228.7 23.219.5 4.64.6 5.64.6 2.82. 23.219.5 3.73.7 2.81.9 2.82.8 2.81.9 2.82.8 71.034.6 50.635.2 47.935.1 15.212.0 12.39.7 13.010.7 48.232.2 16.915.2 6.94.6 14.211.9 10.08.3 6.84. 18.5 13.0 25.0 13.0 36.8 37.4 40.2 21.5 Table 3: The performance of LMMs and Code LLMs on HumanEval-V using different input settings. Image Only refers to the setting used in the main experiments. Desc. Only evaluates models using annotated descriptions of images instead of the images themselves. Image & Desc. provides both inputs to the models. Scores are presented as percentages. The and indicate performance improvement and degradation over the Image Only setting. pass@10 scores on HumanEval-V in regression plot, shown in Figure 4. For proprietary models, we observe rough positive correlation between HumanEval-V and the other benchmarks. Many scatter points for our benchmark are concentrated around zero, even though they show competitive results on the other three benchmarks, highlighting the distinct challenge posed by our benchmark. comprehensive analysis of correlations between HumanEval-V and 5 other benchmarks can be found in Appendix C.2."
        },
        {
            "title": "4.2 ANALYSING EXPERIMENTS",
            "content": "To investigate the reasons behind the suboptimal performance of current LMMs on HumanEval-V, we perform analyzing experiments by answering two key research questions. Q1. Are LMMs Limited by Their Vision Capabilities? We conduct an ablation study to evaluate whether the limitations in visual understanding contribute to the underperformance of LMMs. In this study, we manually annotate detailed descriptions for each image in the coding tasks, ensuring that these descriptions are descriptive rather than instructive, without revealing any specific algorithms. We design new prompt template incorporating the image description to provide LMMs with better-grounded visual context, thereby reducing issues such as ambiguity and hallucination. Details of the new prompt template and examples of annotations are provided in Appendix C.3. To further assess the quality of our annotations, we also test setting where LMMs receive only the image descriptions, without access to the images themselves. Additionally, we evaluate several top-performing Code LLMs using image descriptions to explore their potential in HumanEval-V. We present the results in Table 3. Below are the key findings: (1) The inclusion of image descriptions leads to notable performance gains across all LMMs, with higher-capability models showing the most significant improvements. For example, GPT-4o exhibits 31.5% absolute increase in pass@1. Similarly, large open-weight LMMs demonstrate substantial improvement, indicating that current models still require enhanced visual understanding capabilities. However, the limited improvement observed in smaller open-weight models suggests that merely perceiving visual elements is insufficient for solving tasks that require deeper reasoning. We 8 Figure 5: coding task that InternVL-2-26B fails to solve with grounded image description. LMMs LLM Decoders Params HumanEval+ MBPP+ LLM LMM LLM LMM LLM LMM Nous-Hermes-2-Yi InternVL-2 InternLM2-Chat InternVL-2 InternLM2.5-Chat InternVL-2 Phi-3-Mini-Instruct InternVL-2 Phi-3.5-Mini-Instruct Phi-3.5-Vision Qwen2-VL Qwen2 LLaVA-OneVision Qwen2 Qwen2 MiniCPM-V 2.6 Llama-3-Instruct MiniCPM-V 2.5 34.4B 40.1B 19.9B 25.5B 8.1B 7.7B 4.2B 3.8B 4.2B 3.8B 8.3B 7.6B 8.0B 7.6B 8.1B 7.6B 8.5B 8.0B 66.5 65.2 63.4 64.0 65.9 58.5 58.5 58.5 55. GPT-4o GPT-4o-mini 38.428.1 54.910.3 50.013.4 57.36.7 51.814.1 65.26.7 59.10.6 39.618.9 46.39.2 86.0 84.8 57.9 55.4 53.9 57.1 52.6 53.1 53.1 53.1 51.9 47.110.8 51.93.5 52.41.5 57.1 0.0 50.42.2 43.69.5 51.61.5 37.615.5 47.14.8 68.7 65. Table 4: The performance comparison of open-weight LMMs and their corresponding LLM decoders on HumanEval+ and MBPP+ benchmarks. Scores are shown as percentages, with and indicating performance improvement and degradation of LMMs compared to their LLM decoders. illustrate this limitation with an example from InternVL-2 (25.5B) shown in Figure 5. The task requires determining the number of illuminated red segments based on an OR operation depicted in the image. While the models solution correctly implements the algorithm, it fails to identify the segment mappings for each number, as this information is not explicitly provided in the image description. This example underscores the challenge of integrating visual and textual reasoning in coding tasks. (2) The Desc. Only setting performs comparably to the Image & Desc. setting, underscoring that the annotated image descriptions can effectively capture the key visual information to solving the task. (3) The Code LLMs with small-scale parameter sizes perform well on the tasks when provided with image descriptions alone (i.e., without access to the images). For instance, Yi-Coder-Chat (8.8B) achieves 25% pass@1 and 40.2% pass@10. This highlights the great potential for current open-weight LMMs to further develop their reasoning and coding abilities. 9 Q2. Are LMMs Limited by Their Coding Abilities? Open-weight LMMs with parameter sizes ranging from 4B to 40B exhibit surprisingly low performance on HumanEval-V, even when utilizing grounded visual elements through image descriptions. This suggests that open-weight LMMs may suffer from degradation of relevant coding abilities. So we evaluate the models on well-established code generation benchmark, EvalPlus Liu et al. (2023a), to investigate their coding abilities. This benchmark includes two sub-datasets refined from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), both consisting of Python function completion tasks with problem descriptions and test-execution-based evaluation. Different from HumanEval-V, these datasets depend exclusively on textual context. Given that open-weight LMMs typically employ vision-encoder and language-decoder architecture, we also evaluate their LLM decoders separately to determine whether their coding performance deteriorates after integrating the vision abilities. The results presented in Table 4 lead to the following findings: (1) Open-weight LMMs consistently experience performance degradation on coding benchmarks compared to their LLM decoders, despite having similar parameter sizes. Among these, InternVL-2 (40.1B) and MiniCPM-V 2.6 show the most degradation, while InternVL-2 (4.2B) and LLaVA-OneVision (8B) show the least. (2) Despite this degradation, open-weight LMMs still exhibit relatively strong coding capabilities. Although their performance on EvalPlus does not match GPT-4o, many of these models produce competitive results, indicating they retain substantial degree of code generation ability. These results highlight the need for further improvement in the coding abilities of current open-weight LMMs."
        },
        {
            "title": "5 RELATED WORK",
            "content": "While numerous benchmarks have been developed to evaluate various capabilities of LMMs, ranging from optical character recognition (OCR) to multidisciplinary knowledge reasoning, few specifically focus on the intersection of visual reasoning and code generation. This section reviews the current progress of LMM benchmarking and demonstrates how HumanEval-V fills this gap. OCR and Multidisciplinary Knowledge Abilities: variety of benchmarks have been developed to evaluate multidisciplinary capabilities of LMMs. There are popular benchmarks like DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022), TextVQA(Singh et al., 2019), OCRBench (Liu et al., 2023d), and OCRVQA (Mishra et al., 2019) assess models ability to recognize and interpret text embedded in visual formats, including documents, charts, and images, often combining these with multiple-choice questions (MCQ) and visual question answering (VQA) tasks. Meanwhile, benchmarks such as MMMU (Yue et al., 2024), MME (Fu et al., 2023), MMBench (Liu et al., 2023c), MMVet (Yu et al., 2023), SEEDBench (Li et al., 2023a), MMT-Bench (Ying et al., 2024), and MMStar (Chen et al., 2024) test models on their general knowledge and reasoning abilities across diverse domains, such as scientific concepts, cultural knowledge, and logical reasoning. In contrast, HumanEval-V distinguishes itself by expanding the evaluation format beyond traditional MCQ and VQA. HumanEval-V requires models to interpret visual elements and apply that understanding to generate correct and executable code, which introduces more complex challenge. Specialized Abilities: There are also benchmarks focusing on specific capabilities of LMMs. MathVista (Lu et al., 2023) evaluates mathematical problem-solving skills. Safety-related benchmarks (Gu et al., 2024) assess models on their ability to recognize and mitigate potential risks or harmful content. ConvBench (Liu et al., 2024) evaluates conversational abilities, testing models on their proficiency in maintaining coherent and contextually relevant dialogues. Benchmarks for instruction-following ability (Qian et al., 2024) assess how well models can execute tasks based on given instructions. Long-context reasoning benchmarks (Ma et al., 2024) assess the ability of models to maintain coherence and logical reasoning over extended dialogues or documents. HallusionBench (Guan et al., 2024) focuses on hallucination detection abilities to differentiate between factual information and generated content. There are also benchmarks (Zhang et al., 2024) evaluating mobile app navigation, testing models on their ability to interpret and interact with user interfaces. In contrast, HumanEval-V mainly focuses on integrating visual reasoning and coding. Coding Abilities: Despite the wide range of benchmarks available, the coding ability of LMMs remains under-explored. Coding capabilities are crucial for leveraging LMMs in autonomous and agentic applications (Xie et al., 2024). Current efforts focus primarily on the derendering of web 10 pages (Si et al., 2024; Laurencon et al., 2024) and scientific figures (Shi et al., 2024; Wu et al., 2024a), where models translate visual representations into code. The other related area is Programbased VQA, where models are provided with set of pre-defined modules (e.g., for OCR, object detection, and segmentation) and tasked with invoking them to answer visual questions like counting or identifying spatial relationships (Surıs et al., 2023; Subramanian et al., 2023). These methods show how models can use existing tools to perform vision tasks, while they complicate evaluation In contrast, HumanEval-V utilizes simple Python due to reliance on multiple heavy modules. coding tasks to streamline evaluation and focuses on visual understanding in coding tasks. Another closely related work is MMCode (Li et al., 2024b), which evaluates the coding ability of LMMs on visually rich competition-level coding problems. utilizing existing coding challenges from competitive programming websites. However, MMCode overlooks two critical issues: the potential for data leakage when relying on scraped data, and the use of text-rich problem contexts, which makes visual information non-essential for solving tasks. By contrast, our approach addresses both concerns with rigorous data screening and annotation. We list detailed discussion on MMCode in Appendix E."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present novel and lightweight benchmark HumanEval-V designed to evaluate the visual reasoning capabilities of LMMs through 108 high-quality, entry-level Python coding tasks that rely on visual context to solve. We ensure rigorous evaluation of generated code solutions using comprehensive test cases. Our benchmark effectively uncovers weaknesses in current LMMs that are overlooked by existing benchmarks. Through our analysis, we identify three critical limitations in the current generation of LMMs. First, their visual perception abilities remain inadequate. We observe significant performance gains when we provide textual descriptions of visual elements, indicating that models still struggle to understand visual context independently. Second, open-weight LMMs exhibit consistent decline in their coding proficiency compared to their LLM decoders, suggesting that the current multimodal training strategy still needs improvement. Finally, hallucination due to overfitting is major issue, causing models to incorrectly apply memorized patterns rather than adapt to the new visual context in the coding tasks. We hope these findings will inform and guide future research on enhancing the visual reasoning and coding capabilities of LMMs. We also provide discussion on our works limitations in Appendix F."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude 3.5 sonnet, June 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 11 Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024. URL https://arxiv.org/abs/ 2407.11691. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 1076410799. PMLR, 2023. Google. gemini URL February google-gemini-next-generation-model-february-2024/. model, next-generation https://blog.google/technology/ai/"
        },
        {
            "title": "Introducing",
            "content": "googles 2024. 1.5, ai Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Tianle Gu, Zeyang Zhou, Kexin Huang, Dandan Liang, Yixu Wang, Haiquan Zhao, Yuanqi Yao, Xingge Qiao, Keqing Wang, Yujiu Yang, et al. Mllmguard: multi-dimensional safety evaluation suite for multimodal large language models. arXiv preprint arXiv:2406.07594, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for In Proentangled language hallucination and visual illusion in large vision-language models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14375 14385, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 1831918345. PMLR, 2023. Hugo Laurencon, Leo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma. Mmcode: Evaluating multimodal code large language models with visually rich programming problems. arXiv preprint arXiv:2404.09486, 2024b. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023b. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https: //openreview.net/forum?id=1qvx610Cu7. 12 Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, et al. Convbench: multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models. arXiv preprint arXiv:2403.20194, 2024. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023d. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523, 2024. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. New models added to the phi-3 family, Microsoft. May new-models-added-to-the-phi-3-family-available-on-microsoft-azure/. available on microsoft azure, https://azure.microsoft.com/en-us/blog/ 2024a."
        },
        {
            "title": "URL",
            "content": "Microsoft. Discover the new multi-lingual, high-quality phi-3.5 slms, Aug 2024b. URL https: //techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/ discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/ 4225280. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual In 2019 international conference on document question answering by reading text in images. analysis and recognition (ICDAR), pp. 947952. IEEE, 2019. Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-ofthought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1442014431, 2024. OpenAI. Hello gpt-4o, May 2024. URL https://openai.com/index/hello-gpt-4o/. OpenGVLab. Internvl2: Better than the bestexpanding performance boundaries of opensource multimodal models with the progressive scaling strategy, July 2024. URL https: //internvl.github.io/blog/2024-07-02-InternVL-2.0/. 13 Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024. Chufan Shi, Cheng Yang, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, et al. Chartmimic: Evaluating lmms cross-modal reasoning capability via chart-to-code generation. arXiv preprint arXiv:2406.09961, 2024. Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: How far are we from automating front-end engineering? arXiv preprint arXiv:2403.03163, 2024. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein. Modular visual question answering via code generation. arXiv preprint arXiv:2306.05392, 2023. Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, et al. survey of neural code intelligence: Paradigms, advances and beyond. arXiv preprint arXiv:2403.14734, 2024. Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1188811898, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for In Findings of the Association for Computational Linguistics: open-domain code generation. EMNLP 2023, pp. 12711290, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wikipedia. Pylint Wikipedia, the free encyclopedia. http://en.wikipedia.org/w/ index.php?title=Pylint&oldid=1191495734, 2024. [Online; accessed 24-October2024]. Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, and Ping Luo. Plot2code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots. arXiv preprint arXiv:2405.07990, 2024a. Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, and Gholamreza Haffari. Versicode: Towards version-controllable code generation. arXiv preprint arXiv:2406.07411, 2024b. xAI. Grok-1.5 vision preview, April 2024. URL https://x.ai/blog/grok-1.5v. Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: survey. arXiv preprint arXiv:2402.15116, 2024. 14 Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024a. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024b. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to mine aligned code and natural language pairs from stack overflow. In Proceedings of the 15th international conference on mining software repositories, pp. 476486, 2018. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and arXiv preprint Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv:2403.02713, 2024."
        },
        {
            "title": "A ERROR ANALYSIS ON THE EXAMPLE TASK",
            "content": "Figure 6: Examples of incorrect solutions generated by GPT-4o and Claude 3.5 Sonnet for the coding task shown in Figure 1. Figure 1 illustrates simple coding task in HumanEval-V. The task requires determining whether two line segments, defined by pairs of numbers on clock-like circle, will ultimately intersect if allowed to extend outside the circle. The numbers on the circle are arranged in non-standard order. Despite the problems simplicity, all evaluated LMMs failed to solve it correctly even when generating 20 samples. We present representative solutions generated by GPT-4o and Claude 3.5 Sonnet in Figure 6. Both models implement sorting-based algorithms that compare the numbers at the endpoints of the line segments. However, they fail to account for the critical scenario where the segments intersect outside the circle, and fail to recognize the unordered arrangement of the numbers. This oversight indicates that the models are not effectively capturing the essential visual details of the problem. Notably, this issue appears to stem from data leakage, as the original coding task is derived from CodeForces problem (https://codeforces.com/contest/1971/problem/C), and the generated solutions in Figure 6 reflect patterns more suitable for the original context. This phenomenon is not isolated to this task; we observe similar issues across many coding tasks in HumanEval-V. This highlights that the models rely on memorized patterns instead of genuinely understanding the visual context. Such failures emphasize the importance of preventing data leakage and validate the rationale behind our careful adaptation and mutation processes during data annotation. 16 REPRESENTATIVE IMAGES IN HU NEV L-V Figure 7: curated selection of representative images in HumanEval-V, covering visual elements like trees, graphs, matrices, maps, grids, flowcharts, and more. As shown in Figure 7, the coding tasks in HumanEval-V incorporate wide range of visual elements. These elements go beyond basic representations like trees, graphs, and maps that are typically used to illustrate fundamental data structures in computer science. Instead, the visual contexts in HumanEval-V are carefully designed to be self-explanatory, embedding rich contextual information and algorithmic patterns directly into the images. Achieving this standard is particularly challenging because encoding complex rules, conditions, and problem contexts into single image requires significant effort. Each image must convey the necessary information for understanding the task without relying heavily on textual descriptions. For instance, in tasks where geometric transformations, recursive structures, or logical operations are involved, the image alone should provide enough visual cues for the model to infer the underlying patterns and expected outputs. To meet these high standards, we dedicate extensive resources and effort to the data annotation process. Each image is meticulously crafted to ensure that the visual context is indispensable for solving the task, thus minimizing the reliance on textual explanations. This involves transforming the critical problem-solving elements into concise visual representations while preserving clarity and interpretability. Through this rigorous approach, HumanEval-V sets new bar for integrating visual and algorithmic reasoning, ensuring that the coding tasks genuinely assess the visual understanding and reasoning capabilities of LMMs."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "C.1 PERFORMANCE WITH MORE SAMPLES"
        },
        {
            "title": "Params",
            "content": "pass@1 pass@10 pass@k (n = 100) k=10 k=20 k= k=100 GPT-4o GPT-4o-mini InternVL-2 InternVL-2 InternVL-2 InternVL-2 Qwen2-VL LLaVA-OneVision InternVL-Chat-V1.5 MiniCPM-V 2.6 MiniCPM-V 2.5 Phi-3.5-Vision Phi-3-Vision 40.1B 25.5B 8.1B 4.2B 8.3B 8.0B 25.5B 8.1B 8.5B 4.2B 4.2B"
        },
        {
            "title": "Proprietary",
            "content": "13.0 6.5 36.4 15.4 39.0 15.3 44.1 20.1 49.9 26.7 Open-Weight 1.6 0.0 3.2 0.0 0.9 2.6 2.3 0.0 1.6 0.0 0.9 1.9 2.1 0.0 0.9 2.2 2.3 0.0 0.9 1.6 2.6 0. 3.0 3.2 3.0 2.3 3.1 1.9 3.1 1.7 1.3 2.1 1.8 4.9 4.9 5.0 4.4 5.2 3.4 5.3 2.8 2.4 3.3 3.3 8.0 7.7 8.4 9.4 8.7 6.7 9.3 4.8 5.5 5.0 6.6 53.7 31.5 10.2 10.2 10.2 14.8 11.1 10.2 13.0 7.4 9.3 6.5 9.3 Table 5: The performance of 13 LMMs on HumanEval-V with more generated code solution samples. The pass@1 and pass@10 columns are the results from Table 2. Scores are shown as percentages, with the highest values highlighted in bold. LMMs pass@1 pass@10 k=100 k=200 k= k=600 k=800 k=1000 GPT-4o GPT-4o-mini 13.0 6.5 36.4 15. 55.3 31.3 59.9 36.0 64.3 40.5 66.4 43.0 67.7 44.9 68.5 46. pass@k (n = 1, 000) Table 6: The impact of scaling the number of samples on HumanEval-V. Scores are reported as percentages. The pass@1 and pass@10 columns correspond to results from Table 2. The results in Section 4.1 indicate that increasing the number of samples can significantly enhance model performance on HumanEval-V, so we conduct an ablation study to examine the effect of scaling up sample sizes. Due to budgetary constraints, we primarily test open-weight LMMs ranging from 4B to 40B parameters. For proprietary models, we evaluate GPT-4o and GPT-4o-mini. For all selected models, we increase the number of generated samples to 100 to observe their performance. The results are presented in Table 5. From the results, we observe that increasing the sample size consistently improves performance across most models. For example, GPT-4o achieves substantial improvement, rising from 36.4% pass@10 to 53.7% pass@100. Smaller-scale open-weight LMMs also show notable gains; for instance, InternVL-2 (4.2B) improves from pass@10 of 2.3% to pass@100 of 14.8%. However, not all models benefit equally from scaling the sample size. For instance, Phi-3.5-Vision, which has the same 4B-level parameter size, achieves only pass@100 score of 6.5%. These findings underscore both the potential and the limitations of scaling sample numbers to improve current LMMs performance on HumanEval-V. To further evaluate the performance of current LMMs, we increase the sample size for GPT-4o to 1,000. The results, presented in Table 6, show promising results with GPT-4o achieving pass@1000 of 68.5%, compared to the 36.4% pass@10. Similarly, GPT-4o-mini demonstrates strong performance, achieving pass@1000 score of 46.3%, surpassing the pass@10 score of GPT4o. These findings suggest that significant proportion of the coding tasks in HumanEval-V are solvable with current LMM capabilities, highlighting the need for further research on strategies to better motivate the abilities of LMMs. It is important to note that there may be some variance between the pass@10 scores reported with n=20 and those with n=100 or n=1,000. Increasing typically improves the accuracy of the estimated pass@k, making comparisons between different values less straightforward. Moreover, the pass@100 and pass@1000 values reported in Table 5 and Table 6 may exhibit bias due to using the same and values for calculating pass@k, potentially affecting reproducing the results. C.2 COMPARISON WITH OTHER MULTIMODAL BENCHMARKS"
        },
        {
            "title": "Multidisciplinary Multimodal Benchmarks",
            "content": "HumanEval-V MMMU MathVista MMVet MME RealWorldQA pass@1 pass@ GPT-4o GPT-4o-mini Claude 3.5 Sonnet Gemini 1.5 Pro Gemini 1.5 Flash InternVL-2 Qwen2-VL LLaVA-OneVision InternVL-Chat-V1.5 MiniCPM-V 2.6 MiniCPM-V 2.5 Phi-3.5-Vision Phi-3-Vision 76.3B 40.1B 25.5B 8.1B 4.2B 73.4B 8.3B 73.2B 8.0B 25.5B 8.1B 8.5B 4.2B 4.2B 69.2 60.0 65.9 60.6 58.2 58.3 55.2 50.7 51.2 48.3 64.5 54. 56.8 48.8 46.8 49.8 45.8 44.6 46.1 Proprietary 61.3 52.4 61.6 57.7 51.2 69.1 66.9 66.0 64.0 63.2 Open-Weight 65.6 64.0 59.4 58.3 58.1 64.4 61.8 60.0 54.3 50. 70.5 58.2 67.5 63.2 54.7 60.6 54.3 43.2 44.6 74.0 62.0 63.7 57.5 55.4 60.0 52.8 43.2 44. 2310.3 2003.4 1920.0 2110.6 2077.9 2397.6 2293.1 2259.8 2215.1 2064.6 2482.7 2326.8 2261.0 1998.0 2189.6 2268.7 2024.6 1838.1 1508.0 75.4 67.1 60.1 64.1 69. 72.7 70.1 68.1 64.2 60.5 77.8 70.1 71.9 66.3 65.6 65.0 63.5 53.6 58.8 13.0 6.5 18.5 10.2 8.3 3.7 0.0 0.0 0.9 0. 3.7 0.0 1.9 0.9 0.0 0.9 0.0 0.9 0.0 36.4 15.4 25.9 22.2 13.2 12.8 1.6 3.2 2.6 2.3 16.0 1. 12.4 1.9 2.1 2.2 2.3 1.6 2.6 Table 7: performance comparison of 19 LMMs on HumanEval-V and five other popular multimodal benchmarks. The pass@1 and pass@10 columns correspond to results from Table 2. Values are highlighted using blue color scale, where darker shades indicate higher scores. MMMU MathVista MMVet MME RealWorldQA HumanEval-V MMMU MathVista MMVet MME RealWorldQA HumanEval-V Average - 0.51 0.88 0.42 0.61 0.90 0.66 0.51 - 0.72 0.77 0.73 0.28 0.60 0.88 0.72 - 0.68 0.81 0. 0.75 0.42 0.77 0.68 - 0.80 0.17 0.57 0.61 0.73 0.81 0.80 - 0.38 0.67 0.90 0.28 0.67 0.17 0.38 - 0.48 Table 8: The Pearson correlation coefficients between pairs of six multimodal benchmarks. Lower correlation values highlight benchmarks that capture distinct aspects of model performance. To analyze whether HumanEval-V identifies specific weaknesses that are not captured by existing benchmarks, we select five widely used multimodal benchmarks that cover multidisciplinary abilities. The selected benchmarks include MMMU (Yue et al., 2024), MathVista (Lu et al., 2023), MMVet (Yu et al., 2023), MME (Fu et al., 2023), and RealWorldQA (xAI, 2024). We collect the performance results of the 19 LMMs evaluated in this paper from the OpenVLM Leaderboard (Duan et al., 2024) and the corresponding papers and reports. These results are presented alongside the pass@1 and pass@10 scores on HumanEval-V in Table 7. From the results, we observe that openweight LMMs with over 70B parameters generally perform well on the selected benchmarks, with models such as InternVL-2 (76.3B) and Qwen2-VL (73.4B) even surpassing proprietary models 19 Figure 8: The correlations between six multimodal benchmarks, including HumanEval-V. Each subplot, except on the diagonal, visualizes the relationship between two benchmarks. like GPT-4o and Claude 3.5 Sonnet in some cases. However, these open-weight LMMs show significantly lower performance on HumanEval-V, indicating that our benchmark can uncover model weaknesses that are not apparent in other evaluations. To quantify the relationship between HumanEval-V and the five selected benchmarks, we calculate the Pearson correlation coefficient using the data in Table 7. The results, shown in Table 8, reveal that HumanEval-V has the lowest average correlation coefficient across all benchmarks, suggesting that it captures aspects of model performance that are overlooked by existing benchmarks. Among the benchmarks, HumanEval-V shows the highest correlation with MMMU, which primarily evaluates advanced perception and reasoning abilitieskey focuses of our benchmark as well. We also visualize these relationships using regression plots for each benchmark pair in Figure 8, providing an intuitive view of the correlations. From the plots, we observe that many of the scatter points for HumanEval-V are concentrated around zero, contributing to the low correlation with other benchmarks and highlighting the distinct challenges posed by our benchmark. 20 Figure 9: An example of image description annotation. C.3 EXPERIMENTING WITH IMAGE DESCRIPTIONS We provide two examples in Figure 9 and Figure 10 to illustrate our annotation process and demonstrate how we construct image descriptions. When creating these descriptions, we ensure they are purely descriptive rather than instructive, refraining from disclosing any specific algorithms or problem-solving strategies. This approach allows us to evaluate whether current LMMs possess genuine visual understanding capabilities and whether they can perform well when the visual elements are grounded through detailed textual descriptions. This process poses unique challenge. While humans can intuitively identify patterns in images and summarize them succinctly, we require our annotators to use precise descriptive language that details every visual aspect without inferring the specific steps to solve the problem. This increases the complexity of annotation and often results in verbose image descriptions. Despite this verbosity, maintaining purely descriptive approach is crucial for our benchmark, as it ensures that solving the task requires the model to interpret and reason about the visual content, rather than simply interpreting the description into code. Once the image descriptions are finalized, we employ the prompt template shown in Figure 11 to guide the LMMs in generating code solutions for the tasks in HumanEval-V."
        },
        {
            "title": "D BENCHMARK CONSTRUCTION DETAILS",
            "content": "D.1 ADDITIONAL DETAILS OF DATA COLLECTION Our data collection process involves two primary sources: Stack Overflow (SO) and coding challenge platforms. Each coding problem undergoes strict screening process to ensure that it aligns with the standards of HumanEval-V. Annotators are instructed to identify suitable problems by 21 Figure 10: An example of image description annotation. Figure 11: The template used for prompting LMMs to solve code generation tasks with image descriptions. The {image description} placeholder is replaced with the annotated image description. The {code context} placeholder is replaced with the corresponding function signature. assessing whether they can be adapted with minimal effort to meet the predefined standards, which include the following criteria: (1) the visual context must be essential to solving the task, with all relevant visual information able to fit within single image; (2) the problem should be largely selfexplanatory through its visual context, requiring minimal textual description; and (3) the problem should target entry-level programmers and be solvable using only common Python libraries. We select SO due to its extensive repository of real-world programming problems. To identify relevant posts, we filter for questions from 2020 that have non-negative votes and accepted answers. Next, we focus on posts with images in the question body and code blocks in the corresponding answers, narrowing down to those tagged with python. After this automated filtering, we manually 22 Figure 12: negative example in our data screening process, sourced from CodeForces (https: //codeforces.com/problemset/problem/294/B), where the image is non-essential for solving the problem. Figure 13: negative example in our data screening process, sourced from GeeksforGeeks (https://www.geeksforgeeks.org/problems/last-cell-in-a-matrix/1), where the visual elements require extensive textual descriptions to interpret. review the remaining posts, excluding topics such as front-end, mobile, or UI development, as these often require high-level API usage and do not align with the goals of our benchmark. We also filter out many posts containing images that only provide supplementary details (e.g., code snippets, error messages, or execution outputs) rather than being essential to problem-solving. Ultimately, Figure 14: positive example in our data screening process, sourced from Stack Overflow (https: //stackoverflow.com/questions/69163515). Figure 15: positive example in our data screening process, sourced from CodeForces (https: //codeforces.com/problemset/problem/1381/E). we identify 8 posts satisfying our standards, covering topics like geometry, plotting, and image processing. The final screened SO posts account for less than 1% of the total viewed posts, and even the selected problems often require significant adaptation to fit our benchmarks requirements. Regarding the coding challenge platforms, we utilize the open-source MMCode dataset Li et al. (2024b), which already scraped coding problems from various coding challenge platforms that in24 Figure 16: positive example in our data screening process, sourced from CodeForces (https: //codeforces.com/problemset/problem/1996/B). corporate visual elements in problem descriptions. However, we find that most of these problems are unsuitable for HumanEval-V. Many images merely display simple mathematical equations, which are essentially textual in nature and do not require visual reasoning. In other cases, the visual content is redundant, as it can be easily inferred from the text alone, rendering the images nonessential. Some problems, although containing relevant visual information, are overly complex and require extensive textual descriptions to interpret, violating our requirement for self-explanatory visual contexts. After careful screening, we identify 32 problems suitable for our benchmark: 23 from CodeForces, 5 from LeetCode, and 1 each from GeeksforGeeks, AtCoder, Open Kattis, and Project Euler. These selected problems account for less than 5% of the total viewed problems. To further illustrate our screening process, we present two negative examples that do not meet our standards in Figure 12 and Figure 13, along with three positive examples that are selected for our benchmark in Figure 14, Figure 15, and Figure 16. Below are the detailed explanations: In Figures 12 and 13, we present two negative examples that do not meet the standards for inclusion in our benchmark. Figure 12 is coding problem sourced from CodeForces that requires determining an optimal stacking method for set of books with identical heights, given their respective thickness and width, to minimize the total thickness. Although the provided image illustrates possible stacking configuration, it lacks essential information, such as constraints on the stacking method and precise book dimensions. Furthermore, the core problem-solving information is conveyed predominantly through text, making the image non-essential for understanding the solution. Figure 13 depicts coding problem from GeeksForGeeks that involves traversing 2D matrix according to specified pattern, starting from the top-left corner and identifying the traversal endpoint. Although the image provides basic representation of the matrix, the traversal pattern is too intricate to be effectively captured visually and requires substantial textual explanation. As result, the textual description contains more problem-solving information than the image itself, violating our requirement that the visual context be self-explanatory and the primary source of information. 25 Figure 17: The adapted coding task from Figure 14 as incorporated into HumanEval-V. In Figure 14, Figure 15, and Figure 16, we present three examples that are well-suited for inclusion in our benchmark. Figure 14 illustrates practical problem from Stack Overflow, where developer seeks to draw parallelogram on coordinate plane using four specified points. The image visually demonstrates how these points are connected to form the parallelogram, serving as the critical piece of information needed to solve the task. Additionally, the text merely reiterates the geometric properties shown in the image, making it possible to reduce the textual content significantly without loss of essential details. This ensures that the image itself is indispensable for solving the problem while relying on the text alone would be insufficient. Figure 15 features problem from CodeForces involving the folding of polygon, where the goal is to compute the area of the resulting shape after series of folds. The image clearly depicts the folding process along the designated dashed lines, showing both the original shape and its transformation after folding. These visual details are integral to solving the problem, as understanding the fold pattern and resulting shape is necessary. Figure 16, also sourced from CodeForces, involves reducing grid according to specified pattern. The image effectively conveys the grid reduction process, showing the transformation step-by-step. Any redundant textual description of the pattern can be omitted, ensuring that the problem can be solved primarily by interpreting the visual information, with minimal reliance on the accompanying text. These three examples are relatively straightforward yet require precise visual understanding, making them ideal candidates for adaptation into coding tasks within HumanEval-V. D.2 EXAMPLES OF ADAPTING CODING PROBLEMS We present three adapted examples in Figure 17, Figure 18, and Figure 19, derived from the original coding tasks shown in Figure 14, Figure 15, and Figure 16. For each problem, we redesign the questions, redraw the accompanying images to include the critical problem-solving context, and simplify the textual descriptions. Furthermore, we adjust the difficulty to ensure that entry-level programmers can interpret the visual information accurately and implement the solution using basic coding skills. In Figure 17, we transform the original parallelogram problem into the coding task involving five-pointed star, incorporating richer visual information. To enhance the visual cues, we include four examples in the image demonstrating different ways to connect five points to form star. In the adapted function signature, we specify the implementation requirements for the model, clearly 26 Figure 18: The adapted coding task from Figure 15 as incorporated into HumanEval-V. defining the functions objectives, input parameters, and constraints on the return value. Unlike the original problem, which requires generating an image of parallelogram, the adapted task simply asks whether two specified points should be connected. This adaptation reduces the complexity while maintaining strong focus on assessing the models visual reasoning abilities. Additionally, the structured I/O format allows us to evaluate the generated solutions through test cases. In Figure 18, we simplify the original polygon folding problem into matrix folding task. After folding, overlapping sections of the matrix result in color changes, and the model is required to determine the resulting color distribution. We restrict the input matrix to two initial colors: white and light blue, such that after folding, the matrix can display three distinct color outcomes: white, light blue, and dark blue. This adaptation preserves the visual reasoning involved in understanding the folding process while reducing the programming difficulty. We also provide three illustrative examples within the image to ensure clarity. In Figure 19, we slightly increase the difficulty of the original problem. We remove redundant textual details that can be inferred from the image. We omit the reduction factor from the function parameters, setting as fixed value instead. The model is expected to deduce that = 2 based on the three provided examples. Moreover, instead of performing simple scaling operations with 0 and 1 values as in the original problem, we adapt it into pooling operation based on statistical features (e.g., determining the minimum value), which requires not only OCR capabilities but also deeper visual reasoning. D.3 EXAMPLES OF MUTATING CODING TASKS We apply mutations to some of the 40 screened coding tasks to expand the volume of our benchmark. The objective is to generate new tasks that retain the essence of the original tasks but introduce distinct patterns with minimal modification. As illustrated in Figures 20, Figure 21, and Figure 22, these mutated tasks are derived from the coding problems in Figures 17, 18, and 19, respectively. 27 Figure 19: The adapted coding task from Figure 16 as incorporated into HumanEval-V. In Figure 20, we maintain the same function signature as in the original task but modify the image pattern from five-pointed star to six-pointed star, altering the visual configuration while preserving the overall task settings. In Figure 21, we transform the color addition rule in the folded matrix into numeric addition rule, requiring the model to recognize and infer the numerical changes before and after folding. This mutation introduces additional complexity, further evaluating the models OCR capabilities. For Figure 22, we increase the pooling stride from 2 to 3, requiring the model to observe larger matrix to deduce the pattern, thereby raising the demands on both visual reasoning and OCR proficiency. In each case, we adjust the test cases to align with the modified patterns introduced through the mutations, ensuring that the new tasks remain consistent with the requirements of our benchmark. D.4 ADDITIONAL DATASET STATISTICS dict float Input Output 8 - 3 3 int 34 5 1D list 2D list np.ndarray str tuple pd.DataFrame bool 35 34 24 2 6 4 3 12 3 - 3 - 45 Table 9: The distribution of Input/Output types for the coding tasks in HumanEval-V. The input and output (I/O) types used in the coding tasks in HumanEval-V are designed to maintain low level of complexity. distribution of their frequencies is shown in Table 9. We focus on using simple and commonly used data structures, such as integers, lists, dictionaries, and tuples, which are frequently encountered in standard programming tasks. Most of the tasks utilize basic types like integers, 1D and 2D lists, or simple boolean outputs, ensuring that solving them does not require specialized fine-tuning on domain-specific data. These I/O types are prevalent in open-source code 28 Figure 20: mutated version of the coding task from Figure 17. used for model pretraining, making our benchmark compatible with general-purpose LMMs without requiring additional adaptation or targeted training on specified datasets. In terms of module dependencies, HumanEval-V utilizes minimal set of common Python libraries, including typing, pandas, numpy, math, heapq, and collections. These libraries are well-supported and widely used in both general programming and scientific computing contexts. This ensures that our benchmark can comprehensively assess the visual reasoning capabilities of models using common and accessible libraries, without introducing dependencies that are rarely present in the training data. Notably, the coding tasks in HumanEval-V use only the stable APIs from these libraries, ensuring consistent and reliable testing."
        },
        {
            "title": "E DISCUSSION ON THE MMCODE DATASET",
            "content": "MMCode (Li et al., 2024b) introduces multimodal coding dataset aimed at evaluating LMMs algorithmic problem-solving skills in visually rich contexts. The dataset includes 3,548 questions scraped from various competitive programming websites. However, as discussed in Appendix A, the issue of data leakage poses significant challenge, as many of these coding tasks may have been previously encountered and memorized by the models, making them unsuitable for direct use as test data. Additionally, as demonstrated in Appendix D.1, majority of the coding challenges in MMCode contain visual content that is redundant; the information conveyed through images can often be inferred from the textual descriptions alone, rendering the visuals non-essential. The reported results from MMCode further confirm this issue, as the performance using language-only inputs is similar to that with vision + language inputs. In contrast, HumanEval-V is specifically designed to focus on visual understanding and reasoning abilities, rather than general coding proficiency, ensuring an irreplaceable dependency on visual context. During the annotation phase, we verify that language-only inputs achieve 0% pass rate for GPT-4o, demonstrating the necessity of visual context in HumanEval-V. Moreover, our careful adaptation and mutation processes prevent data leakage, ensuring that evaluations accurately measure visual reasoning and coding abilities, rather than memorization of previously seen tasks. 29 Figure 21: mutated version of the coding task from Figure 18."
        },
        {
            "title": "F LIMITATIONS",
            "content": "Despite the contributions of our benchmark, several limitations remain that we aim to address in future work: (1) Limited Number of Coding Tasks: The size of our benchmark is currently restricted due to the difficulty of identifying suitable coding problems and the challenges associated with adapting these problems to meet our standards. Each annotator has dedicated over 200 hours to constructing the benchmark, ensuring that every task is meticulously curated and aligns with our goals of testing visual reasoning. Our priority has been to maintain high quality, which we believe is crucial for deriving meaningful insights. Fortunately, the current version of HumanEval-V has already enabled us to identify several unique findings about the limitations of current LMMs. Moving forward, we plan to expand HumanEval-V by continuing to annotate additional tasks using our established pipeline and guidelines. To benefit the community, we will open-source our annotation process and release all details of our work. (2) Limited Model Coverage: While our experiments evaluate diverse set of representative topperforming LMMs, the rapid pace of development in this area means that new models are frequently released, which may not be covered in our evaluation. We acknowledge that broader model coverage could provide more comprehensive understanding of current capabilities. To address this, we will publicly release the evaluation toolkit and dataset, along with an up-to-date leaderboard to track ongoing advancements and benchmark new models as they become available. This will help keep our benchmark relevant and provide platform for continuous assessment. (3) Limited Scope of Experimental Analysis: Due to budget constraints, our in-depth analysis is limited to subset of the evaluated models and hyper-parameter settings. While we have included as many models as possible to ensure that our findings are not biased toward specific LMMs, there are areas that remain unexplored, such as evaluating the impact of different prompting templates and experimenting with alternative sampling strategies, including varying temperature settings. Never30 Figure 22: mutated version of the coding task from Figure 19. theless, we have carefully chosen hyper-parameters that are widely used and deemed fair for crossmodel comparisons. We believe that the settings used in our experiments provide reliable insights and lead to trustworthy conclusions. Additionally, our investigation into advanced reasoning methods is limited. In preliminary experiments, we applied the zero-shot Chain-of-Thoughts (CoT) (Wei et al., 2022) approach, which prompts the model to perform step-by-step reasoning before generating code. However, this method showed limited improvement in our coding tasks. Given that zeroshot CoT is relatively weak baseline for reasoning research, fully exploring more sophisticated reasoning-enhancement techniques (Yao et al., 2024a; Mitra et al., 2024) would require significant resources. We leave this comprehensive study to future work."
        },
        {
            "title": "G DETAILS OF THE EVALUATED MODELS",
            "content": "To facilitate the reproducibility of our results, we provide detailed information on all the evaluated models in Table 10. The open-weight models are sourced from Hugging Face2, while the proprietary models are accessed via their respective APIs. For model inference, we utilize 8 NVIDIA A800 GPUs and maintain the original tensor data types specified by each model to ensure consistent evaluation. To further optimize inference efficiency, we leverage the open-source framework vLLM3. Additionally, the Code LLMs used in Section 4.2 are also listed in Table 10. These models are finetuned versions of foundational LLMs, specifically trained on large-scale, multilingual programming datasets to enhance their performance across diverse coding scenarios. 2https://huggingface.co 3https://docs.vllm.ai/en/latest/ 31 Models Params Links GPT-4o-0513 GPT-4o-mini-0718 Claude 3.5 Sonnet Gemini 1.5 Pro (001) Gemini 1.5 Flash (001) Qwen2-VL Qwen2-VL MiniCPM-V 2.6 MiniCPM-V 2.5 InternVL-Chat-V1.5 InternVL2 InternVL2 InternVL2 InternVL2 InternVL2 LLaVA-OneVision LLaVA-OneVision Phi-3.5-Vision Phi-3-Vision Nous-Hermes-2-Yi InternLM2-Chat InternLM2.5-Chat Phi-3-Mini-Instruct Phi-3.5-Mini-Instruct Qwen2 Llama-3-Instruct CodeStral DSCoder-V2-Lite Yi-Coder-Chat DSCoder-V1.5 73.4B 8.3B 8.1B 8.5B 25.5B 76.3B 40.1B 25.5B 8.1B 4.2B 73.2B 8.0B 4.2B 4.2B 34.4B 19.9B 7.7B 3.8B 3.8B 7.6B 8.0B 22.2B 15.7B 8.8B 6.9B Proprietary https://platform.openai.com/docs/models/gpt-4o https://platform.openai.com/docs/models/gpt-4o-mini https://docs.anthropic.com/en/docs/about-claude/models https://ai.google.dev/gemini-api/docs/models/gemini https://ai.google.dev/gemini-api/docs/models/gemini Open-Weight LMM https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct https://huggingface.co/openbmb/MiniCPM-V-2_ https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5 https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5 https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B https://huggingface.co/OpenGVLab/InternVL2-40B https://huggingface.co/OpenGVLab/InternVL2-26B https://huggingface.co/OpenGVLab/InternVL2-8B https://huggingface.co/OpenGVLab/InternVL2-4B https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov https://huggingface.co/microsoft/Phi-3.5-vision-instruct https://huggingface.co/microsoft/Phi-3-vision-128k-instruct Open-Weight LLM https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B https://huggingface.co/internlm/internlm2-chat-20b https://huggingface.co/internlm/internlm2_5-7b-chat https://huggingface.co/microsoft/Phi-3-mini-128k-instruct https://huggingface.co/microsoft/Phi-3.5-mini-instruct https://huggingface.co/Qwen/Qwen2-7B https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct Open-Weight Code LLM https://huggingface.co/mistralai/Codestral-22B-v0.1 https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct https://huggingface.co/01-ai/Yi-Coder-9B-Chat https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1. Table 10: The model identification links."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "Rhymes AI",
        "Tsinghua University",
        "Wuhan University",
        "Zhejiang University"
    ]
}