{
    "paper_title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
    "authors": [
        "Meiqi Wu",
        "Jiashu Zhu",
        "Xiaokun Feng",
        "Chubin Chen",
        "Chen Zhu",
        "Bingze Song",
        "Fangyuan Mao",
        "Jiahong Wu",
        "Xiangxiang Chu",
        "Kaiqi Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation."
        },
        {
            "title": "Start",
            "content": "Preprint ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints Meiqi Wu1,3 Bingze Song2 1 UCAS 2 AMAP, Alibaba Group 3 CRISE 4 THU 5 SEU Jiashu Zhu2 Xiaokun Feng1,3 Fangyuan Mao2 Chubin Chen4 Chen Zhu Jiahong Wu2 Xiangxiang Chu2 Kaiqi Huang1,3 5 2 0 2 6 1 ] . [ 1 7 4 8 4 1 . 0 1 5 2 : r Figure 1: The motivation of ImagerySearch. The figure illustrates two semantic dependency scenarios related to camels. Left: The distance depicts the corresponding strength of prompt tokens during the denoising process. LDT-Bench consists of imaginative scenarios with long-distance semantics, whose semantic dependencies are typically weak. Right: Wan2.1 performs well on short-distance semantics but fails under long-distance. Test time scaling methods (e.g., Video T1 (Liu et al., 2025a), Evosearch (He et al., 2025a)) also struggle. However, ImagerySearch generates coherent, context-aware motions (orange box)."
        },
        {
            "title": "Abstract",
            "content": "Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with longdistance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, prompt-guided adaptive test-time Work done during the internship at AMAP, Alibaba Group. Project leader. Corresponding author. 1 Preprint search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation."
        },
        {
            "title": "Introduction",
            "content": "Imagine describing surreal scenea panda playing violin on Mars during sandstormand instantly seeing it come to life as video. Text-to-video generation promises just that: the ability to turn language into vivid, dynamic worlds. Recent video generation models have made significant progress in generating realistic scenes (Wang et al., 2023; Yang et al., 2024; Peng et al., 2025; OpenAI, 2025; Wan Team et al., 2025); however, their performance drops sharply when handling subjectively imaginative scenarios, hindering the advancement of truly creative video generation. Why is imagination so hard to generate? This limitation arises from two primary factors. (1) The models semantic dependency: Generative models exhibit strong semantic dependency constraints on long-distance semantic prompts, making it difficult to generalize to imaginative scenarios beyond the training distribution  (Fig. 1)  . (2) The scarcity of imaginative training data: Mainstream video datasets (Huang et al., 2024b; Liu et al., 2024b; Sun et al., 2024; Liu et al., 2023; Liao et al., 2025; Ling et al., 2025) predominantly contain realistic scenarios, offering limited imaginative combinations characterized by long-distance semantic relationships (Fig. 3(d)). Recent test-time scaling approaches (Liu et al., 2025a; He et al., 2025a) alleviate data scarcity by sampling multiple candidates and selecting the most promising one. However, their predefined sampling spaces and static reward functions constrain adaptability to the open-ended nature of creative generation. The Imagery Construction theory (Thomas, 1999; Pylyshyn, 2002) posits that humans create mental scenes for imaginative scenarios by iteratively refining visual imagery in response to language. Motivated by this principle, we introduce ImagerySearch, test-time search strategy that enhances prompt-based visual generation. ImagerySearch comprises two core components: (i) Semanticdistance-aware Dynamic Search Space (SaDSS), which adaptively modulates sampling granularity according to the semantic span of the prompt; and (ii) Adaptive Imagery Reward (AIR), which incentivizes outputs that align more closely with the intended semantics. To assess generative models in imaginative settings, we propose LDT-Bench, the first benchmark designed specifically for long-distance semantic prompts. It comprises 2,839 challenging concept pairs, constructed by maximizing semantic distance across objectaction and actionaction dimensions from diverse recognition datasets (e.g., ImageNet-1K (Deng et al., 2009), Kinects-600 (Carreira et al., 2018)). In addition, LDT-Bench includes an automatic evaluation protocol, ImageryQA, which quantifies creative generation with respect to element coverage, semantic alignment, and anomaly detection. Extensive experiments reveal that general models (e.g., Wan14B (Wan Team et al., 2025), Hunyuan13B (Kong et al., 2024), CogVideoX (Yang et al., 2024)) and TTS-based models (e.g., VideoT1 (Liu et al., 2025a), EvoSearch (He et al., 2025a)) suffer from significant degradation in video quality and semantic alignment when conditioned on long-distance semantics. In contrast, our framework consistently improves generation fidelity and alignment, demonstrating superior capability in handling long-distance semantic prompts. Our contributions can be summarized as follows: We propose ImagerySearch, dynamic test-time scaling law strategy inspired by mental imagery that adaptively adjusts the inference search space and reward according to prompt semantics. 2 Preprint We present LDT-Bench, the first benchmark specifically designed for video generation from long-distance semantic prompts. It comprises 2,839 promptsspanning 1,938 subjects and 901 actionsand offers an automatic evaluation framework for assessing model creativity in imaginative scenarios. Extensive experiments on LDT-Bench and VBench reveal that our approach consistently improves imaging quality and semantic alignment under long-distance semantic prompts."
        },
        {
            "title": "2 Related Work",
            "content": "Text-to-Video Generation Models. With increased training resources, large-scale T2V models (OpenAI, 2025; Kwai, 2025; Runway, 2025; Bao et al., 2024; Zheng et al., 2024a; Peng et al., 2025; Genmo Team, 2024; Kong et al., 2024; Wan Team et al., 2025) have emerged, capable of generating coherent videos, understanding physics, and generalizing to complex scenarios. But they require massive data, and collecting enough long-range semantic prompts is impractical. Although fine-tuning (Fan and Lee, 2023; Lee et al., 2023; Black et al., 2023; Wallace et al., 2024; Clark et al., 2023; Domingo-Enrich et al., 2024; Mao et al., 2025) and post-training (Yuan et al., 2024a; Prabhudesai et al., 2024; Luo et al., 2023; Li et al., 2024a;b) methods mitigate data requirements to some extent, the extreme scarcity of long-distance semantic videos still hinders effective training. In contrast, the Test-Time Scaling (TTS) methods (Oshima et al., 2025; Xie et al., 2025; Yang et al., 2025; Liu et al., 2025a; He et al., 2025a) used in ImageSearch require no additional training and achieve strong performance through highly general approach. Test-Time Scaling in T2V Models. TTS improves performance by using rewards to select better outputs (Jaech et al., 2024; Guo et al., 2025). In T2V generation, TTS are primarily explored in two aspects: selection strategies and reward strategies. Selection strategies mainly include Best-of-N, particle sampling, and beam search. The Best-of-N (Ma et al., 2025; Liu et al., 2025a) selects the top outputs from multiple generations. Particle sampling (Singhal et al., 2025; Li et al., 2024c; 2025; Singh et al., 2025; Sunwoo Kim, 2025) improves upon this by performing importance-based sampling across the denoising process. Beam search (Liu et al., 2025a; Yang et al., 2025; Xie et al., 2025; Oshima et al., 2025; Liu et al., 2025a; He et al., 2025b) keeps multiple candidates at each step, expanding the sequence set over time. Reward strategies are based on various evaluation metrics, such as VisionReward (Xu et al., 2024), ImageReward (Xu et al., 2023), Aesthetic score (Schuhmann et al., 2022), which guide the selection process by quantifying the quality of generated output. These reward functions are crucial for aligning outputs with desired visual and semantic characteristics. Current TTS methods optimize search and reward strategies for general T2V generation to enhance overall performance. In this work, we investigate this specific challenge and explore how TTS can be leveraged to improve model performance in long-distance semantic prompts. Evaluation of Video Generative Models. Early video-generation metrics are simplistic: some diverged from human judgment (Unterthiner et al., 2018; Radford et al., 2021b), while others reused real-video tests unsuited to synthetic clips (Soomro et al., 2012; Xu et al., 2016). Later, studies (Szeto and Corso, 2022; Liu et al., 2023; 2024b; Huang et al., 2024c; Sun et al., 2025; Zheng et al., 2025) such as VBench (Huang et al., 2024b) evaluated AI-generated videos from comprehensive, multi-dimensional perspective. Several studies (Liu et al., 2024a; Yuan et al., 2024b; 2025; Ling et al., 2025) refine evaluation along single dimensions such as frame realism or temporal coherence. Although existing methods focus on video quality and human perception, semantic content assessment remains underexplored. Current benchmarks struggle to effectively evaluate long-distance semantic prompts, which are key to advancing video generation capabilities. To address this, LDT-Bench was introduced as the first benchmark for evaluating long-distance semantic understanding in video generation."
        },
        {
            "title": "ImagerySearch",
            "content": "Text-to-video generation aims to synthesize coherent videos conditioned on prompts. Diffusion models inherently possess the flexibility to adjust test-time computation via the number of denoising steps. To further improve generation quality, we formulate search problem that identifies better noise inputs for the diffusion sampling process. We organize the design space along two axes: the 3 Preprint Figure 2: Overview of our ImagerySearch. The prompt is scored by the Constrained Semantic Scorer (producing Dsem) and simultaneously fed to the T2V backbone (Wan2.1). At every step specified by the imagery scheduler, we sample set of candidate clips, rank them with reward function conditioned on Dsem, and retain only Dsem-controlled subset. The loop repeats until generation completes. reward functions that evaluate video quality, and the search algorithms that explore and select optimal noise candidates. 3.1 Preliminaries In standard diffusion frameworks, sampling starts from Gaussian noise xT (0, I), and the model iteratively denoises the latent through learned network fθ. As widely used sampling paradigm, DDIM performs the following step-wise denoising update: xt1 = ζt1( xt σtfθ(xt, t, c) ζt ) + σt1fθ(xt, t, c), (1) Where ζt1, ζt, σt1 denote predefined schedules. Prior test-time scaling approaches (Liu et al., 2025a; He et al., 2025a; Yang et al., 2025) operate within fixed noise search space and use static reward functionssuch as VideoScore (He et al., 2024), VideoAlign (Liu et al., 2025b), or their combinationsto rank candidates. By contrast, our framework supports flexible reward design and adaptive noise selection, substantially improving both sample efficiency and generation quality. 3.2 Dynamic Search Space Inspired by imagery cognitive theory (Thomas, 1999; Pylyshyn, 2002; Feng et al., 2023)which posits that humans expend more effort and time to construct mental imagery for semantically distant conceptswe likewise adapt the candidate-video search space to prompts semantic distance: shrinking it for short-distance prompts to boost test-time efficiency, and enlarging it for long-distance prompts to explore broader range of possibilities. Therefore, we propose Semantic-distance-aware Dynamic Search Space (SaDSS). As shown in Fig. 2, this adaptive resizing is driven by Constrained Semantic Scorer, which dynamically modulates the search space. Specifically, we define semantic distance as the average embedding distance between key entities (objects and actions) extracted from the prompt. Given prompt p, we extract its compositional units {pi}n and compute: i=1 Dsem(p) = 1 (cid:88) (i,j)E ϕ(pi) ϕ(pj)2 , (2) 4 Preprint Figure 3: Overview of our LDT-Bench. Upper: (a) LDT-Bench is built by first extracting metainformation from existing recognition datasets; (b) GPT-4o is then used to generate candidate prompts, which are filtered jointly by DeepSeek and humans to obtain the final prompt set; (c) Additionally, we design set of three MLLM-based QA tasks that serve as the creativity metric. Lower: (d) Compared with other benchmarks, LDT-Bench covers much richer variety of categories; (e) its prompts also exhibit semantic-distance distribution that is shifted toward substantially longer ranges. Note that ASD denotes the average semantic distance of prompts. where ϕ() denotes the embedding function (e.g., T5 encoder), and is the set of key entity pairs in the prompt. At inference time, we adapt the sampling procedure based on Dsem. Specifically, the search space dynamically adapts based on semantic distance. Formally, the number of candidates Nt at timestep is dynamically adjusted as: Nt = Nbase (cid:0)1 + λ Dsem(p)(cid:1) , (3) where Nbase is the base number of samples, and λ is scaling factor that controls the sensitivity to semantic distance. In this work, we set λ = 1. By tailoring the search scope to the inherent difficulty of the prompt, SaDSS encourages the model to explore more diverse visual hypotheses when needed, improving visual plausibility under challenging conditions, without incurring unnecessary computational costs for simple prompts. 3.3 Adaptive Imagery Reward Based on our observations, adjacent denoising steps alter the latent video only marginally, so we invoke ImagerySearch at few key noise levels = {5, 10, 20, 45}, termed the Imagery Schedule (see Appendix A). As shown in Fig. 2, starting from partially denoised latent xt, we produce ˆx0 by completing the denoising trajectory and compute the reward on ˆx0 to assess the influence of different denoising stages on the final video quality. To enhance semantic alignment between generated videos and prompts with long-distance semantics, we introduce an Adaptive Imagery Reward (AIR) that modulates evaluation feedback based on the prompts semantic difficulty. Specifically, we incorporate the semantic distance as soft re-weighting factor into the reward formulation. The reward RAIR(ˆx0) for each candidate video x0 is defined as: RAIR(ˆx0) = (α MQ + β TA + γ VQ + ω Rany) Dsem(ˆx0), (4) where α, β, γ, and ω are scaling factors that adaptively adjust the reward based on the prompt semantic distance Dsem. MQ, TA, and VQ are from VideoAlign (Liu et al., 2025b), and Rany denotes an extensible reward (e.g., VideoScore (He et al., 2024), VMBench (Ling et al., 2025), UnifiedReward (Wang et al., 2025), VisionReward (Xu et al., 2024)). 5 Preprint"
        },
        {
            "title": "4 LDT-Bench",
            "content": "The rapid progress of video generation models is closely tied to the development of targeted evaluation benchmarks. Existing benchmarks primarily assess models using text prompts designed to depict realistic scenarios. However, as video generation models have achieved impressive performance in realistic scenarios, it is timely to shift the focus towards imaginative scenarios. Generally, such complex settings involve prompts in which entitiessuch as objects and actionsexhibit long semantic distances, meaning these entities rarely co-occur (e.g., panda piloting helicopter). These corner cases reveal the robustness limits of generative models. Nonetheless, most existing works remain limited to qualitative analysis on few cases, and there is lack of unified benchmark specifically designed for this task. To fill this gap, we propose novel benchmark LDT-Bench, designed to systematically analyze the generalization ability of video generation models in complex scenarios induced by prompts with Long-Distance semantic Texts. In the following sections, LDT-Bench is introduced from two perspectives: the construction of the prompt suite and the design of evaluation metrics. The core components of LDT-Bench are illustrated in Fig. 3. 4.1 Prompt Suite Meta-information Extraction. Considering that objects and actions are the main entities in text prompts, we construct our prompts using the following two structural types. (1) ObjectAction: An object combined with an uncommon or incompatible action. (2) ActionAction: Two semantically distant or even contradictory actions. To cover wide range of objects and actions, we build our object and action sets from representative large-scale datasets. Specifically, the object set is derived from ImageNet-1K (Deng et al., 2009) and COCO (Lin et al., 2014) (covering 1,938 objects), while the action set is collected from ActivityNet (Caba Heilbron et al., 2015), UCF101 (Soomro et al., 2012), and Kinetics-600 (Carreira et al., 2018) (covering 901 actions). These collections serve as the foundation for subsequent prompt generation. We first encode each object and action element texti using pretrained T5 text encoder (Raffel et al., 2020), obtaining high-dimensional textual feature hi Rd. These embeddings are then projected into shared 2D semantic space via Principal Component Analysis (PCA): zi = PCA(hi) = PCA(T5(texti)), zi R2, (5) where zi represents the semantic position of the i-th element in the 2D space. T5 can be replaced with other encoders, such as CLIP (Radford et al., 2021b); see Appendix B.1 for details. To measure semantic divergence, we compute the Euclidean distance between each pair of elements as criterion for selecting long-distance semantic prompts. We then construct two candidate sets: one by pairing each object with its most distant action (1,938 objectaction pairs), and the other by matching each action with its most distant counterpart (901 actionaction pairs). From each set, we select the 160 most distant pairs, resulting in 320 high-distance prompts that challenge the model with long-distance semantic combinations. For more analysis of the prompt suite, please refer to Appendix B.2. Long-distance Prompt Generation. Based on the obtained text element pairs, we employ large language model, i.e., GPT-4o (Hurst et al., 2024), to generate fluent and complete text prompts by filling in necessary sentence components. Subsequently, each prompt is double-checked by both DeepSeekR1 (Guo et al., 2025) and human annotators to ensure quality, resulting in our final prompt suite. The detailed generation process and several illustrative cases are presented in Fig. 3 (b). 4.2 Imagery Evaluation Metrics To quantitatively evaluate the performance of video generation models under long-distance semantic settings, we develop targeted evaluation metrics. Inspired by recent MLLMs-based evaluation methods (Cho et al., 2023; Feng et al., 2025), we generate questions based on the text prompts. Subsequently, MLLMs with strong semantic comprehension capabilities analyze the generated videos in response to these questions, yielding quantitative evaluation results. Specifically, our assessment framework encompasses three primary dimensions. 6 Preprint ElementQA. Because our prompts focus on objects and actions, ElementQA primarily consists of targeted questions revolving around these elements. For example, given the prompt The traffic light is dancing., we can generate two questions: Does the traffic light appear in the video? and Is the traffic light performing dancing action? AlignQA. In addition to the basic semantic information covered by ElementQA, we also evaluate the generated videos in terms of visual quality and aesthetics (Murray et al., 2012). Given the challenging and inherently subjective nature of this assessment, we employ recently developed MLLMs that have been specifically optimized for alignment with human perception to perform the evaluation (Huang et al., 2024a; Wu et al., 2023). AnomalyQA. We have observed that current video generation models frequently produce anomalous outputs. Consequently, we also leverage MLLMs to analyze the generated frames and answer targeted questions aimed at identifying these anomalies. Implementation Details. For ElementQA, we employ Qwen2.5-VL-72B-Instruct (Bai et al., 2025) as the underlying MLLM, whereas for AlignQA we adopt Q-Align (Wu et al., 2023), model specifically optimized for rating visual quality and aesthetics. Given the broader generalization required by AnomalyQA, we utilize the more powerful GPT-4o (OpenAI, 2024) for evaluation. We collectively refer to these three components as ImageryQA. Further implementation details are provided in Appendix B.3."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Datasets & Metrics. To assess the imaginative capacity of video-generation models, we evaluate them on both LDT-Bench and VBench (Huang et al., 2024b), using each benchmarks full prompt suite and associated metrics. Compared Models. We compare two categories of models: (1) General models: Hunyuan (Kong et al., 2024), Wan2.1 (Wan Team et al., 2025), Open-Sora (Zheng et al., 2024b), CogVideoX (Yang et al., 2024); (2) TTS methods: Video-T1 (Liu et al., 2025a) and EvoSearch (He et al., 2025a). We use Wan2.1 as the base model and generate 33-frame clips with the default settings (see Appendix for details). Experimental Environment. All experiments are run on server equipped with 8 NVIDIA H20 GPUs (96 GB each), an Intel Xeon Gold 6348 CPU (32 cores, 2.6 GHz), and 512 GB of RAM, under Ubuntu 20.04 LTS (kernel 5.15). We used Python 3.9 with PyTorch 2.5.1 (CUDA 12.4, cuDNN 9.1), torchvision 0.20.1, and Transformers 4.50.3. 5.2 Comparison with Other Generation Models Performance on LDT-Bench. As shown in Tab. 1, we adopt Wan2.1 as the base model. Our method achieves significant improvement of 8.83%, demonstrating clear advantage. Furthermore, compared to other test-time scaling approaches, ImagerySearch also delivers consistently superior performance. These results highlight the effectiveness of our method in handling long-distance semantic prompts and its robustness in imagination-driven scenarios. Model LDT-Bench (%) ElementQA AlignQA AnomalyQA ImageryQA (All) Wan2.1 (Wan Team et al., 2025) Video-T1 (Liu et al., 2025a) Evosearch (He et al., 2025a) ImagerySearch (Ours) 1. 1.91 1.92 2.01 31.62 38.16 36.10 36.82 15. 14.68 16.46 18.28 48.28 54.75 54.48 57.11 Table 1: Quantitative comparison on LDT-Bench. performance. ImagerySearch achieves the best average Performance on VBench. For balanced evaluation, we compare two classes of methods on VBench. The upper rows of Tab. 2 report general generators, while the lower rows list test-time 7 Preprint Figure 4: Visualization of examples. Upper: Results from general models. Lower: ImagerySearch versus other test-time scaling methods. Ours produces more vivid actions under long-distance semantic prompts. Model Aesthetic Quality Background Consistency Dynamic Degree VBench (%) Imaging Quality Motion Smoothness Subject Consistency Average General Wan2.1 (Wan Team et al., 2025) Opensora (Peng et al., 2025) CogvideoX (Yang et al., 2024) Hunyuan (Kong et al., 2024) TTS Video-T1 (Liu et al., 2025a) Evosearch (He et al., 2025a) ImagerySearch (Ours) 50.50 48.80 48.80 50.45 57.20 55.55 57.70 91.80 95.25 95.30 92.65 95.65 94.80 96. 82.85 73.15 47.20 85.00 54.05 80.95 84.05 58.25 61.35 65.05 59.55 60.25 68.90 69. 97.50 99.05 98.55 95.75 99.30 97.70 98.00 90.25 92.95 94.65 90.55 94.80 94.55 95. 78.53 78.43 74.93 78.99 76.88 82.08 83.48 Table 2: Quantitative comparison of video generation models on VBench. ImagerySearch achieves the best average performance across multiple metrics, indicating better alignment and generation quality. Figure 5: (a) Effect of semantic distance across different models. As semantic distance increases, our method remains the most stable. (b-e) Our AIR consistently delivers superior performance. Scaling behavior of ImagerySeach and baselines as inference-time computation increases. From left to right, the y-axes represent the score changes for MQ, TA, VQ, and Overall (VideoAlign (Liu et al., 2025b)). (f) Effect of reward weight. scaling approachesVideo-T1 (Liu et al., 2025a), EvoSearch (He et al., 2025a), and our proposed ImagerySearch. All models are evaluated on long-distance prompts from LDT-Bench using the VBench metrics. ImagerySearch achieves the best overall score and ranks highest on the fine-grained Dynamic Degree, Subject Consistency metrics and so on, indicating its strong ability to preserve prompt fidelity under wide semantic gaps. Fig. 4 illustrates this strength: ImagerySearch accurately reproduces both the specified subjects (e.g., bear, controls) and their associated actions (e.g., 8 Preprint VBench (%) Model Aesthetic Quality Background Consistency Dynamic Degree Imaging Quality Motion Smoothness Subject consistency Average Baseline Wan2.1 (Wan Team et al., 2025) Modules w/o AIR w/o SaDSS SaDSS-static weight 0.5 0.9 Search BON (Ma et al., 2025) Particle Sampling (Ma et al., 2025) ImagerySearch (Ours) 50.50 56.25 55.35 57.25 57.40 57.40 56. 57.70 91.80 94.60 95.10 96.15 96.05 95.00 93.52 96.00 82. 81.85 77.20 70.00 70.00 83.01 81.72 84.05 58.25 68.05 68. 70.75 70.80 68.10 67.04 68.50 97.50 97.50 97.60 97.45 97. 97.70 96.18 97.65 90.25 94.40 94.55 95.45 95.50 94.63 93.38 94. 78.53 82.11 81.30 81.18 81.22 82.64 81.39 83.10 Table 3: Ablation Study. Baseline is the plain backbone; Modules successively add our two novel modules; SaDSS-static weight denotes the performance obtained when the selection space is kept at fixed size; Search swaps in alternative search strategies. The full configuration (ImagerySearch) yields the best performance. uses). Additional examples in Appendix further demonstrate its robustness in handling complex long-distance prompts. Robustness Analysis Across Semantic Distances. As illustrated in Fig. 5(a), our approach maintains nearly constant VBench scores as semantic distance increases, whereas competing methods exhibit pronounced fluctuations. This stability highlights the superior robustness of our model across wide range of semantic distances. Additional error analysis is provided in the Appendix E. 5.3 Test-time Scaling Law Analysis We measure the inference-time computation by the number of function evaluations (NFEs). As shown in Fig. 5(bd), where performance is assessed with the MQ, TA, and VQ metrics from VideoAlign (Liu et al., 2025b), ImagerySearch exhibits monotonic performance improvements as inference-time computation increases. Notably, on Wan2.1 (Wan Team et al., 2025), ImagerySearch continues to gain as NFEs grow, whereas baseline methods plateau at roughly 1 103 NFEs (corresponding to the 30th timestep). Computation details are provided in the Appendix F. Moreover, our method shows an even more pronounced advantage in the overall VideoAlign score, as illustrated in Fig. 5(e). 5.4 Ablation Study Effect of SaDSS and AIR. As shown in the first three rows of Tab. 3, adding either the SaDSS or the AIR module individually already surpasses the baseline, while combining SaDSS with AIR achieves the best performance, confirming the complementary nature of semantic guidance and adaptive selection. Effect of Search Space Size. The SaDSSstatic weight rows in Tab. 3 compare fixed and dynamic search-space configurations. With static weights of 0.5, and 0.9, performance improves gradually, reaching VBench score of 81.22%. In contrast, the dynamic approach attains markedly higher score of 83.48%, demonstrating its superior ability to optimize the search space and thus boost model performance. Effect of Search Strategy. The Search rows in Tab. 3 compare different search strategies (e.g., BON, Particle Sampling (Ma et al., 2025)). The experimental results demonstrate that our search strategy delivers the best performance. Effect of Reward Dynamic Adjustment Mechanism. Fig. 5(f) demonstrates the impact of varying reward weights on VBench scores across different models (MQ, TA, VQ). As weights change from 0.2 to 1.2, TA shows notable improvement while MQ and VQ maintain relatively stable performance. The consistent superiority of the Ours approach, represented by the dashed line, underscores the effectiveness of dynamic reward adjustment, achieving optimal performance irrespective of weight changes. 9 Preprint"
        },
        {
            "title": "6 Conclusion",
            "content": "In this study, we propose ImagerySearch, an adaptive test-time search method that improves video-generation quality for long-distance semantic prompts drawn from imaginative scenarios. Additionally, we present LDT-Bench, the first benchmark designed to evaluate such challenging prompts. ImagerySearch attains state-of-the-art results on both VBench and LDT-Bench, with especially strong gains on LDT-Bench, demonstrating its effectiveness for text-to-video generation under long-range semantic conditions. In future, we will explore more flexible reward mechanisms to further enhance video-generation performance."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018. Chubin Chen, Jiashu Zhu, Xiaokun Feng, Nisha Huang, Meiqi Wu, Fangyuan Mao, Jiahong Wu, Xiangxiang Chu, and Xiu Li. S2-guidance: Stochastic self guidance for training-free enhancement of diffusion models. arXiv preprint arXiv:2508.12880, 2025. Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. arXiv preprint arXiv:2310.18235, 2023. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. Ying Fan and Kangwook Lee. Optimizing ddpm sampling with shortcut fine-tuning. arXiv preprint arXiv:2301.13362, 2023. Xiaokun Feng, Shiyu Hu, Xiaotang Chen, and Kaiqi Huang. hierarchical theme recognition model for sandplay therapy. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pages 241252. Springer, 2023. Xiaokun Feng, Haiming Yu, Meiqi Wu, Shiyu Hu, Jintao Chen, Chen Zhu, Jiahong Wu, Xiangxiang Chu, and Kaiqi Huang. Narrlv: Towards comprehensive narrative-centric evaluation for long video generation models. arXiv preprint arXiv:2507.11245, 2025. 10 Preprint Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Ling Pan. Scaling image and video generation via test-time evolutionary search. arXiv preprint arXiv:2505.17618, 2025a. Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Ling Pan. Scaling image and video generation via test-time evolutionary search. arXiv preprint arXiv:2505.17618, 2025b. Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. Yipo Huang, Xiangfei Sheng, Zhichao Yang, Quan Yuan, Zhichao Duan, Pengfei Chen, Leida Li, Weisi Lin, and Guangming Shi. Aesexpert: Towards multi-modality foundation model for image aesthetics perception. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 59115920, 2024a. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024b. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024c. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Kwai. Kling. Accessed February 25, 2025 [Online] https://klingai.com/, 2025. URL https://klingai.com/. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, and Xiangxiang Chu. Advancing end-to-end pixel space generative modeling via self-supervised pre-training. arXiv preprint arXiv:2510.12586, 2025. Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. Advances in neural information processing systems, 37:7569275726, 2024a. Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang Wang. T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. arXiv preprint arXiv:2410.05677, 2024b. 11 Preprint Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, et al. Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252, 2024c. Xiner Li, Masatoshi Uehara, Xingyu Su, Gabriele Scalia, Tommaso Biancalani, Aviv Regev, Sergey Levine, and Shuiwang Ji. Dynamic search for inference-time alignment in diffusion models. ArXiv, abs/2503.02039, 2025. URL https://api.semanticscholar.org/CorpusID: 276768450. Mingxiang Liao, Qixiang Ye, Wangmeng Zuo, Fang Wan, Tianyu Wang, Yuzhong Zhao, Jingdong Wang, Xinyu Zhang, et al. Evaluation of text-to-video generation models: dynamics perspective. Advances in Neural Information Processing Systems, 37:109790109816, 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. Xinran Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, and Xiangxiang Chu. Vmbench: benchmark for perception-aligned video motion generation. arXiv preprint arXiv:2503.10076, 2025. Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. Video-t1: Test-time scaling for video generation. arXiv preprint arXiv:2503.18942, 2025a. Jiahe Liu, Youran Qu, Qi Yan, Xiaohui Zeng, Lele Wang, and Renjie Liao. Frechet video motion distance: metric for evaluating motion consistency in videos. arXiv preprint arXiv:2407.16124, 2024a. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025b. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024b. Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36:6235262387, 2023. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, and Xiangxiang Chu. Omni-effects: Unified and spatially-controllable visual effects generation. arXiv preprint arXiv:2508.07981, 2025. Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: large-scale database for aesthetic visual analysis. In 2012 IEEE conference on computer vision and pattern recognition, pages 24082415. IEEE, 2012. OpenAI. Gpt-4o: Openais new flagship model. https://openai.com/index/ gpt-4o-and-gpt-4-api-updates/, 2024. Accessed: 2024-06-05. OpenAI. Sora. Accessed February 25, 2025 [Online] https://openai.com/index/sora/, 2025. URL https://openai.com/index/sora/. 12 Preprint Yuta Oshima, Masahiro Suzuki, Yutaka Matsuo, and Hiroki Furuta. Inference-time text-to-video alignment with diffusion latent beam search. arXiv preprint arXiv:2501.19252, 2025. Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, and et al. Open-sora 2.0: Training commercial-level video generation model in $200k. arXiv preprint arXiv:2503.09642, 2025. Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. Zenon Pylyshyn. Mental imagery: In search of theory. Behavioral and brain sciences, 25(2): 157182, 2002. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021a. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021b. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Runway. Runway gen3. Accessed February 25, 2025 [Online] https://app.runwayml.com/, 2025. URL https://app.runwayml.com/. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. Anuj Singh, Sayak Mukherjee, Ahmad Beirami, and Hadi Jamali Rad. Code: Blockwise control for denoising diffusion models. ArXiv, abs/2502.00968, 2025. URL https://api. semanticscholar.org/CorpusID:276094284. Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. John Stam. Stable diffusion: High-resolution image synthesis with latent diffusion models, 2023. Placeholder entry. Please update with correct details. Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2vcompbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84068416, 2025. Dongmin Park Sunwoo Kim, Minkyu Kim. Test-time alignment of diffusion models without reward over-optimization. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=vi3DjUhFVm. Ryan Szeto and Jason Corso. The devil is in the details: diagnostic evaluation benchmark for video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2105421063, 2022. 13 Preprint Nigel JT Thomas. Are theories of imagery theories of imagination? an active perception approach to conscious mental content. Cognitive science, 23(2):207245, 1999. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. Wan Team, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. In International Conference on Machine Learning, pages 5401554029. PMLR, 2024a. Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, and Weiqiang Wang. Finger in camera speaks everything: Unconstrained air-writing for real-world. IEEE Transactions on Circuits and Systems for Video Technology, 34(9):86028613, 2024b. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1590315935, 2023. Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Yexin Liu, Zelin Peng, Junjun He, Zongyuan Ge, and Imran Razzak. Scalingnoise: Scaling inference-time search for generating infinite videos. arXiv preprint arXiv:2503.16400, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. Instructvideo: Instructing video diffusion models with human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64636474, 2024a. 14 Preprint Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Rui-Jie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. Advances in Neural Information Processing Systems, 37:2123621270, 2024b. Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, and Li Yuan. Opens2v-nexus: detailed benchmark and million-scale dataset for subject-to-video generation. arXiv preprint arXiv:2505.20292, 2025. Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024a. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024b. Preprint"
        },
        {
            "title": "A The Selection of Imagery Schedule",
            "content": "As illustrated in Fig. S1, we observe that adjacent denoising steps modify the latent video only marginally; substantial deviations from earlier stages emerge only at several pivotal steps. To improve generation efficiency, we therefore trigger ImagerySearch at limited set of noise levels, = {5, 20, 30, 45}, which we term the Imagery Schedule. This schedule specifies the exact timesteps at which ImagerySearch is invoked."
        },
        {
            "title": "B More Details About Imagery Evaluation Metrics",
            "content": "B.1 More Text Encoders. In our current implementation, T5 serves three purposes: it encodes the key entities in each prompt, measures their semantic distances, and then uses those distances to adjust the search space and reward weights during generation. The same pipeline can be run with CLIP text encoder (Radford et al., 2021a; Blattmann et al., 2023). Trained on large-scale imagetext pairs, CLIP yields text embeddings whose cosine distances correlate well with visual concepts, so these distances can play exactly the same role in deciding when to expand or shrink the search space. In addition, CLIP similarities are widely used as measure of textimage or textvideo alignment, which makes them natural choice for the alignment term in our reward function (Stam, 2023). Because CLIP, like T5, produces fixed-length vector in single forward pass, it can be swapped in as drop-in replacement without changing any downstream components while fully preserving the effectiveness of our adaptive search and reward mechanisms. B.2 More Analysis about Prompt Suite. As shown in Fig. S2, we provide multi-faceted overview of the LDT-Bench prompt suite and underscore its advantages for long-distance semantic evaluation. (a) Examining the distribution of actions, pronounced long-tail pattern emerges: of the five super-categories, Sports & Wellness and Daily Services each supply 300 prompts, ensuring ample coverage of everyday yet highly diverse actions. (b) For objects, treemap of 14 super-categoriesscaled by instance countreveals that Animal and Artifact jointly exceed half of all samples, while still leaving room for rarer classes; this balance of head and tail categories is largely missing in prior benchmarks. (c) The object word cloud (after stop-word filtering) highlights high-frequency nouns such as cricket, person, and remote, evidencing fine-grained lexical diversity across domains. (d) The action word cloud reveals wide semantic spanverbs like play, join, use, and handlethat challenges models to cope with imaginative, long-distance dependencies. Taken together, these statistics show that LDT-Bench not only covers richer mix of objects and actions than existing datasets but also accentuates long-distance semantic relationships that current models find most difficult, making it uniquely effective testbed for stress-testing creative video generation systems. B.3 ImageryQA Implementation Details. As described in Sec. 4.2 of the paper, our metric is primarily composed of three components: ElementQA, AlignQA, and AnomalyQA (Fig. S3 (a)). In this subsection, we provide further clarification using specific examples and illustrating the metric computation process. As shown in Fig. S3 (b), given the evaluation prompt, person polishes furniture attentively at home, then packs cleaning products for organization., two videos generated by different video generation models. First, ElementQA formulates questions based on the objects and actions within the prompt, i.e., person, polishes furniture, and packs cleaning products for organization, resulting in the questions Q1, Q2, and Q3 in Fig. S3. Next, AlignQA assesses the first frame of each video in terms of image quality and aesthetics. Finally, AnomalyQA evaluates abnormal events in both videos, as illustrated by Q5 in Fig. S3. Based on these questions, we employ different MLLMs and answer strategies. Recent studies (Feng et al., 2025; Liu et al., 2025b; Wu et al., 2024a; Zheng et al., 2025; Wu et al., 2024b) suggest that for 1 Preprint Figure S1: Imagery schedule. The heatmaps visualize 13th-layer attention projected onto the first video frame at successive denoising steps. Adjacent steps show nearly identical focus regions, whereas only few key steps exhibit pronounced changes. Concentrating analysis and search on these pivotal steps therefore captures the prompt-to-frame semantic correspondence more efficiently. Figure S2: LDT-Bench prompt suite analysis: (a) Action super-category distribution shown as horizontal bar chart. (b) Object super-category distribution displayed as treemap, with area proportional to class count. (c) Word cloud highlighting the most frequent object-action prompts. (d) Word cloud highlighting the most frequent action-action prompts. questions with inherent uncertainty, having general-purpose MLLM (Bai et al., 2025; OpenAI, 2024) answer the same question multiple times and averaging the results yields more reliable evaluations. Therefore, for ElementQA, we prompt Qwen2.5-VL-72B-Instruct (Bai et al., 2025) to answer each question five times. For AnomalyQA, considering the higher cost of GPT-4o (OpenAI, 2024), we collect three responses per question. For Q-Align (Wu et al., 2023) in AlignQA, since it is dedicated model trained for aesthetic quality assessment and directly outputs quantitative score, we use single response. Experimental SetupModel details Parameter settings. In our implementation, the baseline model is Wan2.1-1.3B (Wan Team et al., 2025). And we set the imagery schedule to {5, 20, 30, 45} and set the imagery size schedule to {10, 5, 5, 5, 5}. As shown in Fig. S4, and exhibit the same selection trend, whereas shows the opposite. Therefore, regarding the parameters in Equation (5), we set β = γ = 1.0, and α are dynamically adjusted. 2 Preprint Figure S3: Evaluation with ImageryQA. (a) We design structured question set ImageryQA, consisting of ElementQA, AlignQA, and AnomalyQA.(b) Comparison between Wan2.1 and ImagerySearch on the same prompt. Wan2.1 fails to depict person and the actions described, resulting in low aesthetic quality (Q4) and visual anomalies (Q5). In contrast, ImagerySearch successfully captures both actionspolishing furniture and packing cleaning productsscoring higher in both Q4 and Q5."
        },
        {
            "title": "D More Examples",
            "content": "Additional qualitative examples are provided in Fig. S5, Fig. S6, and Fig. S7. Specifically, Fig. S5 reports results on LDT-Bench, where the first five rows correspond to actionaction prompts and the last three to objectaction prompts. Fig. S6 and Fig. S7 show further actionaction cases drawn from VBench. Across all examples, our method produces vivid and coherent videos, even under long-distance semantic prompts, illustrating its capacity to handle challenging imaginative scenarios."
        },
        {
            "title": "E Error Analysis",
            "content": "In the VBench (Huang et al., 2024b) error analysis (Fig. S8), ImagerySearch shows higher mean score with tighter interquartile range, indicating more stable performance across prompts. Evosearch (He et al., 2025a) attains comparable median but displays greater dispersion, whereas wan2.1 (Wan Team et al., 2025) and Video-T1 (Liu et al., 2025a) exhibit lower central scores and wider quartile spans. Overall, dynamically adjusting the search space and rewarding by semantic distance helps maintain generation quality while reducing sensitivity to prompt difficulty. 3 Preprint Figure S4: Reward-Weight Analysis. The left of figure shows an actionaction example and the right of figure is an objectaction one, visualizing the videos under different weight settings. and follow almost identical trends, whereas moves in the opposite direction. Accordingly, we fix the and coefficients to 1 and vary the coefficient with the prompt, selecting videos that better fit imaginative scenarios. 4 Preprint Figure S5: More examples on LDT-Bench. The images below the prompt show the result of frame sampling, where 16 frames are uniformly extracted from 33-frame video. 5 Preprint Figure S6: More examples on VBench (Part I). The images below the prompt show the result of frame sampling, where 16 frames are uniformly extracted from 33-frame video. 6 Preprint Figure S7: More examples on VBench (Part II). The images below the prompt show the result of frame sampling, where 16 frames are uniformly extracted from 33-frame video. 7 Preprint Figure S8: Error analysis about VBench scores on long-distance semantic prompts. Each box shows the score distribution for one model (mean marked by white diamond); individual data points are overlaid in matching colors. ImagerySearch (orange) attains the highest mean with the tightest spread, while the other methods exhibit lower central tendencies and larger variances."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "CRISE",
        "SEU",
        "THU",
        "UCAS"
    ]
}