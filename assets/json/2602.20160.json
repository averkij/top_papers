{
    "paper_title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
    "authors": [
        "Chen Wang",
        "Hao Tan",
        "Wang Yifan",
        "Zhiqin Chen",
        "Yuheng Liu",
        "Kalyan Sunkavalli",
        "Sai Bi",
        "Lingjie Liu",
        "Yiwei Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes."
        },
        {
            "title": "Start",
            "content": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction Chen Wang1 Hao Tan2 Wang Yifan2 Zhiqin Chen2 Yuheng Liu3 Kalyan Sunkavalli2 1University of Pennsylvania Sai Bi2 2Adobe Research Lingjie Liu1 Yiwei Hu2 3UCI https://cwchenwang.github.io/tttLRM 6 2 0 2 3 2 ] . [ 1 0 6 1 0 2 . 2 0 6 2 : r Figure 1. We propose tttLRM, Large Reconstruction Model based on Test-Time Training, enabling high-resolution, long-context, autoregressive 3D reconstruction. Our model achieves 1) high-resolution (1024px) single-image-to-3D reconstruction via multi-view generator 2) long-context (64 input views) and feedforward 3DGS reconstruction, and supports 3) autoregressive streaming reconstruction."
        },
        {
            "title": "Abstract",
            "content": "reconstruction compared to state-of-the-art approaches on both objects and scenes. We propose tttLRM, novel large 3D reconstruction model that leverages Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the models capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian *Work done as interns at Adobe Research. Equal advising. 1. Introduction Reconstructing explicit 3D representations for photorealistic rendering from streaming visual input is central goal of 3D reconstruction. This process is similar to how humans perceive the physical world: we observe continuous visual stream, build an abstract internal representation of the world, and decode this abstraction into explicit 3D only when needed for fine-grained tasks or to recall detailed 3D structure. In light of this human-like process, we aim to enable long-context, autoregressive reconstruction of explicit 3D from streaming visual input. However, existing 3D reconstruction methods are not designed for long-context scenarios with memory mechanism. Traditional approaches to generating 3D representations and synthesizing novel views, including Neural Radiance Fields (NeRF) [36, 65] and 3D Gaussian Splat- [22] have achieved substantial progress for ting (3DGS) high-quality rendering, but they either require slow scenespecific optimization or rely on feedforward reconstruction models with limited input-view scalability. For example, Large Reconstruction Models (LRMs) have been proposed to rapidly reconstruct various 3D representations such as NeRFs [17], meshes [56], and 3DGS [69] from input images. However, these models are typically restricted to only few input views (e.g., four), which limits their ability to reconstruct large-scale scenes. While Long-LRM [72] extends the number of input views to 32, its use of bidirectional attention layers hinders further scalability and prevents efficient processing of inputs with longer and streamed context, limiting its applicability in real-world scenarios. On the other hand, recent research [13, 19, 39, 71] on implicit latent-space 3D representations has demonstrated superior novel view synthesis quality using purely neural networks. However, despite being feedforward models for reconstruction, their rendering speed is significantly slower than that of explicit representations such as 3DGS due to repetitive network inference, and they lack controllability and interpretability, making them less suitable for many downstream applications. In this paper, we propose tttLRM, novel reconstruction model that leverages neural architectures and the knowledge distilled from pretrained implicit latent-space 3D models, decoding them into explicit 3D representations. This design ensures high-quality novel view synthesis with long-context and autoregressive modeling, while maintaining real-time rendering capability via explicit 3D outputs. Our model builds upon Test-Time Training (TTT) [48, 71] and introduces an architecture composed of LaCT [71] blocks that has only linear computational complexity. We interpret the fast weights of TTT models, which are updated according to inputs during inference, as implicit latentspace 3D representations that can be decoded into various explicit formats such as 3DGS or NeRFs. We demonstrate that, with minimal architectural modification, our framework effectively leverages the pretrained knowledge of large novel view synthesis models [19, 71] for explicit 3D reconstruction. Specifically, our model is trained to query the fast weights for different 3D representations, such as set of virtual view planes for 3DGS, or triplane feature grid for NeRF-based reconstruction. This design unlocks greater flexibility in the final 3D representation. Also, by redesigning the fast-weight update and query mechanism, tttLRM enables autoregressive 3D reconstruction and refinement with streaming inputs. We further introduce sequence parallelism to enhance scalability. We validate our model on both objectand scene-level datasets. Across both datasets, our model achieves superior reconstruction quality compared to baseline methods, while also being highly efficient. We also show that our model supports autoregressive reconstruction, enabling practical real-world applications. The contributions of our paper can be summarized as following: We propose tttLRM, the first large reconstruction model that leverages TTT for both feedforward long-context and autoregressive 3D modeling with linear complexity. We design scalable, unified 3D modeling framework that interprets TTT fast weights into observable and controllable explicit 3D representations. We achieve state-of-the-art results on both objectand scene-level datasets, delivering superior quality and efficiency in 3D reconstruction and novel view synthesis. 2. Related Work Multi-view 3D Reconstruction 3D reconstruction from images has been extensively studied in computer vision. Traditional methods such as structure-from-motion [41] or multi-view stereo (MVS) [15] focus on recovering 3D geometry. Deep learning has enabled feed-forward 3D reconstruction [25, 61, 62], which builds cost volumes using plane sweep for per-view depth estimation. Recently, learning-based MVS approaches [8, 26, 27, 53, 55, 59] directly estimate point clouds from input images and have been applied to camera pose estimation. Test3R [67] optimizes the network at test time in self-supervised manner to improve 3D reconstruction. Concurrent work TTT3R [8] defines gradient to update states for point cloud reconstruction. However, none of these methods can produce photo-realistic novel view synthesis. Neural representations then have emerged as promising way for both geometry reconstruction and NVS. NeRF [36] represents the scene as continuous field and leverages coordinate-based MLP to predict per-point color and density, enabling differential volumetric rendering with rendering-based supervision. Original NeRF takes hours to optimize single scene and following works improved its training and rendering efficiency using advanced representations, including voxels [29, 46], points [58], hash grids [37], and triplanes [4, 6, 14]. Recently, 3D Gaussian Splatting [18, 22] has become the state-of-the-art neural scene representation. It uses volume rendering and rendering loss for optimization similar to NeRF but represents the scene with simple Gaussian primitives which enables real-time rendering and large-scale scene reconstruction [23, 30]. However, 3DGS still requires optimizing 3D Gaussians from scratch, taking several minutes per scene, whereas our model performs 3D reconstruction within seconds in feed-forward way. Learning-based Feedforward 3D Reconstruction The development on learning-based methods enables 3D reconstruction and novel view synthesis by training neural networks on large-scale datasets to directly infer 3D structures without per-scene optimization. Early work utilizes Convolutional Neural Networks (CNN) to predict multi-plane images [12, 35], points [2, 64] or voxels [45]. Large Reconstruction Models (LRM) [17] propose transformerbased architecture without 3D inductive bias for 3D object reconstruction from multi-view images, with triplane as the 3D representation. GS-LRM [69] further extends LRM to predict pixel-aligned 3DGS, but the model can only take very few images as input due to the quadratic complexity of attention layers. Similarly, the subsequence approach [5, 9, 49, 57] also apply feedforward framework with different neural architectures and 3D inductive bias for Gaussian prediction. Mamba-based models [42, 63] has attempted to reduce the complexity of attention layers, but are still limited to very few input views. Long-LRM [72] represents the state of the art in long-sequence Gaussian reconstruction, but it remains limited to 32 input views and relies on additional attention layers. By leveraging TTT, our model achieves longer-context and autoregressive reconstruction with improved NVS quality. Linear Attention and State Space Models To circumvent the quadratic complexity of attention [50], recent research has explored efficient alternatives that retain contextual expressivity while reducing computational cost. Linear attention models [21, 40, 43] approximate the softmax kernel with linearized feature maps to achieve linear complexity, but uniform compression of past keyvalue pairs often degrades the upper bound of long sequence modeling. State Space Models (SSMs) introduce state variable to represent historical information, similar to classical Recurrent Neural Networks (RNNs). Recent works [10, 16, 31, 47] incorporate attenuation factors into the state updates, allowing the model to retain more recent information while gradually forgetting the distant past. Among them, Mamba [10, 16, 31] proposes date-dependent decay to model sequences as continuous-time dynamical systems governed by state transition, but it still cannot compete with transformers in long-context reasoning [52]. Jamba [1] implements hybrid mamba attention model to improve the performance. Test Time Training (TTT), on the other hand, [3, 48, 71] transforms the problem into an online learning problem and applies modern optimizers to learn the states. DeltaNet [40, 60] and MesaNet [51] share the same idea but use different update rules when updating. Inspired by its success, we introduce Test-Time Training into 3D reconstruction tasks for high-quality long-context novel view synthesis, but with only linear complexity. 3. Method 3.1. Preliminary: TTT and LaCT Layer We first briefly introduce the fundamentals of TTT and Large Chunk Test-Time Training (LaCT) layer, which form the core building blocks of our model. In sequence modeling, the input is typically represented as sequence of tokens of length L, denoted by [x1, x2, ..., xL], where each token has dimension d: xi Rd. In standard attention, each input token will be projected into query, key and value vectors, denoted as qi, ki, vi. Each token attends to all others via dot-product operation, leading to quadratic complexity in sequence length. TTT [48] learns set of fast weights that are updated at inference time according to the input to capture the relationship between input tokens. Specifically, it treats the key-value pairs (ki, vi) of input tokens as training data to update fast weights using mean-square error: ηLMSE(fW (k), v), which can be further applied to queries to obtain the final input = fW (q). In this way, the fast weights effectively encode the keyvalue (KV) cache of the input sequence into fixed-size neural memory. Originally, the TTT model [48] updates the fast weights using only small minibatch (e.g. 16 tokens), which results in very low GPU FLOP utilization and difficulty in handling long sequences. Large Chunk Test-Time Training (LaCT) [71] instead updates fast weights with large chunk size (up to 1M tokens). Its chunk-wise update computes the gradient of the summed loss over all keys and values within the chunk. More details can be found in [71]. 3.2. Model Architecture We illustrate our model architecture in Figure 2, using 3DGS reconstruction as an example, though the same framework can be applied to other 3D representations as well. Given set of posed images, denoted as {Ii RHW 3i = 1, 2, .., }, we concatenate them channelwise with their ray embeddings {Ri RHW 9i = 1, 2, .., } as the positional embedding. After dividing each image into non-overlapping patches of size p, we tokenize these image patches using lightweight linear layer into sequence of tokens T: HW/p2 j=1 = Tokenize(cid:0)Patchify([{Ii}N i=1, {R}N {Ti,j}N i= i=1])(cid:1), These visual tokens then iteratively update the fast weights of set of LaCT blocks using Muon [20] optimizer: Ti = Ti + WinAttn(Ti), = Update({Ti}N i=1), Ti = Apply(W, Ti) (1) (2) (3) Each LaCT layer includes window attention module that captures local relationships within each view. We omit the feedforward layers in the block in the equation for simplicity. The update and apply operations are in linear complexity with respect to the sequence length. To retrieve information from the fast weights, we introduce set of virtual tokens that serve as queries to our In 3DGS reconstruction, these virtual tokens are model. RHW 3i = 1, 2, .., } for GS virtual views {Iv Figure 2. Given set of posed input images, tttLRM encodes them into tokens (green boxes) after patchifying. The input tokens are fed into the LaCT block (shown in the blue frame) where fast weights are updated accordingly. Another set of virtual tokens (blue boxes) are used to query the updated fast weights, and decoded into 3D representations like 3DGS for high-quality novel view synthesis. i,j}N i=1 HW/p2 j=1 prediction, which will also be patchified and tokenized to {Tv . In other 3D representations, such as triplane NeRFs, these virtual tokens are learnable triplane features. The virtual tokens are only used in the apply operation without updating the fast weights: Tv = Apply(W, Tv ) (4)"
        },
        {
            "title": "Given the updated query tokens Tv",
            "content": "i , linear token decoder transforms them into explicit 3D representations, such as per-patch Gaussian parameters in 3DGS reconstruction. The RGB color, scale, rotation, and opacity of each Gaussian are predicted directly. For Gaussian positions, we first decode the depth of each pixel and use range function (object-centric for object data and linear for scene data) to convert it to real depth. After that, we convert depth to Gaussian position with known ray locations and directions. 3.3. Autoregressive Reconstruction Algorithm 1 Autoregressive 3DGS Reconstruction Input: Reconstructor with initial fast weights W0; input/query view batches {(I(b), (b))}B b=1 Output: Reconstructed GS 1: W0 2: for = 1 to do , F(W, I(b)) 3: 4: G(b), F(W, 5: end for 6: return G(B) (b)) An important feature of our architecture is its support for autoregressive modeling with streamed input images. To enable this, we modify the update and apply steps to incorporate causal dependencies among tokens. Unlike the standard setting where all input views are jointly processed to update the fast weights before decoding, the streaming variant performs incremental updates in causal manner. As illustrated in Algorithm 1, providing our model F, for each incoming mini-batch of views I(b) (e.g., four images at time), the model updates the fast weights and immediately predicts the corresponding 3D Gaussian parameters for the new query views (b), returning the current reconstructed Gaussian splat results G(b). This design effectively transforms the model into an RNN-like inference process, where the internal state (fast weights) evolves as new observations arrive, enabling online 3D Gaussian reconstruction. The fast weight update can also consider historical gradients and fast weights to mitigate drifting (See Supplemental). 3.4. Distributed Feedforward Reconstruction large number of input views and high-resolution images introduce substantial number of tokens, leading to significant increase in both computation and memory cost. key limitation of prior works lies in their inability to handle long input sequences efficiently, largely due to the lack of parallelism at the sequence level and most methods process all input views within single device. To address this limitation, we introduce sequence parallelism for training feedforward reconstruction models, exemplified by 3DGS reconstruction as shown in Figure 3. Specifically, we partition the tokenized input views along the sequence dimension and assign each shard to separate device. During training: Since Gaussians can be predicted independently for each virtual view once the fast weights are synchronized, each GPU predicts pixel-aligned Gaussian primitives for its assigned views (first row). The predicted Gaussians from all devices are gathered to form the complete scene representation (second row). Each GPU subsequently renders its own set of novel views and computes photometric reconstruction losses against the ground truth, and gradients are all reduced to (z axis) with ground truth depth for that Gaussian. We opt for using the monocular depth estimator [54] for pseudo ground truth since we found that feedforward MVS methods like VGGT [53] provide less detailed depth prediction, albeit being multi-view consistent. Similar to LongLRM [72], we also use opacity regularization to reduce the number of Gaussians. Our final loss function can be written as follows: = LRGB + λdepthLdepth + λopacityLopacity (6) 4. Experiments 4.1. Model and Training Model Details Our model consists of 24 LaCT blocks with the hidden dimension of 768. The window attention layers have 64-dimension for each head with QK-normalization for stability. For the feedforward layer, we use two-layer MLP with 4 intermediate expansion ratios for the intermediate dimension. We use patch size of 8 8 for the image tokenizer. Our architecture shares the same parameterization as TTT-LVSM [71] except for the decoding module, allowing us to effectively leverage its pretrained weights as strong initialization for our model. 4.2. Datasets Object-level Dataset We train our object-level reconstruction model on the Objaverse dataset [11]. Following prior works [17, 69], each 3D object is centered and normalized to fit within bounding box of [1, 1]. We render 32 views per object, where cameras are randomly distributed around the object at distances uniformly sampled from [1.5, 2.8]. All images are rendered at resolution of 512 512 under uniform lighting conditions. In total, we use 730K objects for training. We evaluate our model on 100 objects sampled from the Google Scanned Objects (GSO) dataset. For evaluation, we select few views as input and the same random 8 views for testing. Scene-level Dataset We train our model on the challenging DL3DV-10K [28] dataset, which consists of 10,510 highresolution videos, each containing up to 500 keyframes with camera pose annotation obtained from COLMAP [38]. The testing set of DL3DV-140 contains 140 test scenes. We use the same input and target split from that provided by LongLRM [72]: the testing views are evenly selected from every 8 views (around 40 images each scene) and input views are selected based on K-means clustering based on camera positions and view directions. We also tested our model on Tanks&Temples [24] dataset. 4.3. Baselines and Metrics Object-level We compare our method with GS-LRM [69], an attention-based method. We train the model under 8 input views setting with the same iterations of our method. Figure 3. Illustration of distributed feedforward reconstruction training. First, image tokens are sharded across GPUs, and each GPU predicts Gaussians for its assigned virtual views after the fast weights are synchronized. The predicted Gaussians are then gathered to construct the full scene, after which each GPU renders subset of novel views and computes its respective losses. Gradients are finally all reduced and backpropagated across all devices. enable sequence-level backpropagation (third row). Thanks to the linearity of our LaCT fast-weight updates, gradients of the fast weights across devices can be easily synchronized through PyTorch Distributed Data Parallel (DDP), ensuring consistent global optimization. During inference, the distributed reconstruction also allows us to accelerate the reconstruction with more GPUs. 3.5. Training Objective Our training does not require explicit 3D supervision. We render the reconstructed GS on the target views for supervised training, and minimize the rendering loss that is combination of Mean Squared Error (MSE) and perceptual loss based on VGG-19 features [44]: LRGB = MSE(Ipred, Igt) + λ Perceptual(Ipred, Igt) (5) For non-autoregressive training, we randomly sample unordered inputtarget image pairs from the dataset. For the autoregressive model, we instead sample ordered input sequences to better simulate streaming use case. Apart from rendering loss, for scene-level data, we use depth regularization with the scale-invariant depth loss [72] by aligning the Gaussian position along the depth direction Figure 4. Qualitative comparison between our method and baseline approaches. Our model reconstructs the 3DGS scene with higher fidelity than both optimization-based and feedforward baselines, as also reflected in the PSNR metrics. Please zoom in for better comparison. Figure 5. We demonstrate that our high-resolution 1024 1024 3DGS tttLRM can be effectively used for image-to-3D generation when combined with multi-view generator. Our model enables the reconstruction of fine-grained, photorealistic details e.g., hair, fur, and text, from the input images. Video results are provided in the supplemental material. Scene-level Previous feedforward reconstruction methods like GS-LRM [69] cannot be directly extended to long sequence due to the high complexity of attention. Long-LRM [72] is the only available feedforward method that can handle more than 16 input views. We also include three optimization-based methods: 3DGS [22], MipSplatting [66] and Scaffold-GS [32]. Metrics For all baselines, in addition to visual comparisons, we report three metrics to evaluate novel view synthesis quality: PSNR, SSIM, and LPIPS [70]. 4.4. Results Object-level We present quantitative comparison results under varying resolutions and numbers of input views in Table 1. Across all settings, our method consistently outperforms the baselines. At lower resolutions and shorter sequences, our inference speed is comparable to full attention models, as it is primarily determined by MLP operations. Thanks to the linear complexity of our architecture with respect to sequence length, at resolution of 512 512, our model runs twice as fast as attention-based models while achieving over 1 dB PSNR improvement. Our model also demonstrates strong generalization abilitywhen trained with 8 input views, it can be directly applied to 16 or 24 views (last two rows in the table). With longer sequences, inference becomes substantially faster, and rendering quality further improves through test-time training. Moreover, our model scales seamlessly to 1024 1024 resolution, whereas GS-LRM encounters out-of-memory issues under high-resolution training. Results in Figure 5 show that our model can achieve high-quality 3D reconstruction of humans, animals, and texts from single image when combined with multi-view diffusion model. Scene-level We further evaluate our model on scene Figure 6. We show that tttLRM, as general framework, can also interpret the latent 3D memory into formats besides 3DGS. In this experiment, we use set of triplane tokens to query the fast weights and then fine-tune the model for triplane-based NeRF reconstruction. We visualize the resulting triplanes and present the corresponding renderings and depth maps for 4 views at resolution of 512 512. Table 1. Comparison between our method and GS-LRM [69] on the GSO dataset under different resolutions and numbers of input views. Our method consistently outperforms GS-LRM in both inference speed and reconstruction quality, and also shows strong generalization ability. V. denotes the number of virtual views used to query the fast weight, which equals input views unless noted. Method Resolution Views Time (s) PSNR SSIM LPIPS 256 256 512 512 GS-LRM [69] Ours GS-LRM [69] Ours GS-LRM [69] Ours GS-LRM [69] Ours 8 8 8 8 16 16 (10 V.) 24 24 (10 V.) 0.1 0.1 0.7 0.3 2.5 0.8 5.5 1.1 31.55 33.14 32.83 34. 33.55 34.67 33.26 34.80 0.964 0.024 0.969 0.974 0.976 0.978 0.976 0. 0.028 0.972 0.029 0.025 0.023 0.022 0.022 0.022 Table 2. Quantitative comparison on both DL3DV-140 and Tanks&Temples datasets under different numbers of input views. Our method surpasses previous feedforward methods and is comparable with optimization-based methods. Note that Long-LRM trains separate model for each input view, while we are single model across all input views. Our model can be linearly accelerated with multiple GPUs, here we report time on 1 A100. Views Method Time DL3DV-140 Tanks&Temples PSNR SSIM LPIPS PSNR SSIM LPIPS 32 64 3D GS30k Mip-Splatting30k Scaffold-GS30k Long-LRM (16v model) Ours (single model) 3D GS30k Mip-Splatting30k Scaffold-GS30k Long-LRM (32v model) Long-LRM (32v model w/ optim) Ours (single model, AR) Ours (single model) 3D GS30k Mip-Splatting30k Scaffold-GS30k Long-LRM (64v model) Ours (single model, AR) Ours (single model) 13m 13m 16m 0.4s 3.6s 13m 13m 16m 1s 12s 7.5s 7.2s 13m 13m 16m 3.7s 15.2s 14.8s 21.20 20.88 22.13 22.66 23.60 23.60 23.32 24.77 24.10 24.99 24.31 25. 26.55 26.29 27.07 24.63 24.81 25.95 0.708 0.712 0.738 0.740 0.784 0.779 0.784 0.805 0.783 0.809 0.803 0. 0.852 0.850 0.857 0.799 0.814 0.844 0.264 0.274 0.250 0.292 0.255 0.213 0.217 0.205 0.254 0.243 0.237 0. 0.164 0.166 0.173 0.243 0.225 0.195 16.76 16.82 17.02 17.51 18.15 18.10 18.39 18.41 18.38 18.69 18.96 19. 20.78 20.08 20.96 19.11 19.80 20.31 0.598 0.616 0.634 0.555 0.613 0.688 0.700 0.691 0.601 0.623 0.653 0. 0.778 0.759 0.768 0.627 0.675 0.700 0.334 0.332 0.321 0.408 0.360 0.269 0.262 0.290 0.363 0.360 0.322 0. 0.205 0.220 0.240 0.346 0.308 0.274 reconstruction, as shown in Table 2. Compared with optimization-based methods that tend to overfit to input views, our method achieves better results on 16 and 32 input views. With more input views, it remains competitive in reconstruction quality, while being hundreds times faster. Moreover, one single tttLRM model can be applied to different sequence lengths and effectively generalizes to new datasets like Tanks & Temples. to the"
        },
        {
            "title": "Compared",
            "content": "feedforward baseline LongLRM [72], tttLRM achieves substantially better performanceapproximately 1 dB PSNR improvementacross different numbers of input views. On the other hand, we show our model constantly outperforms Long-LRM even when its combined with additional post-optimization. Furthermore, our method can be linearly accelerated by distributing the input across multiple GPUs, as described in Section 3.4. Figure 4 shows visual comparisons between our method tttLRM achieves better visual quality with and baselines. fewer artifacts than optimization-based methods, thanks to the learned priors across diverse scenes. Our model also outperforms Long-LRM by reconstructing sharper and more detailed geometry (as shown in the red boxes). Autoregressive Reconstruction We demonstrate the autoregressive reconstruction capability of our model in the second row of Figure 1. With only 4 initial input views, the model already produces reasonable 3D Gaussian reconstructions; as additional views arrive (8 and 32 views), both the rendering quality and scene coverage progressively improve. Additional examples of autoregressive reconstruction are provided in the supplemental materials. Decoding into Other 3D Formats Beyond using 3DGS as the output representation, our architecture can also decode the latent 3D representation into other formats, such as triplane-based NeRFs. As described in Section 3.2, replacing the virtual tokens with triplane tokens enables the fast weights to be queried as triplane representation for NeRF reconstruction. We finetune the model with rendering loss to enable this capability. We show the NeRF renderings and the corresponding queried triplanes in Figure 6. This demonstrates that our architecture is flexible and can generalize to different 3D output formats. 4.5. Ablation Study We conduct ablation studies to analyze our design choices in LVSM pretraining and the autoregressive reconstruction strategy. Pretraining from TTT-LVSM We investigate the effectiveness of leveraging pretrained knowledge for both Gaussian Splatting and triplane training at resolution of 256 256. The GS reconstruction has 8 input views with patch size 8 8, while the triplane version has 4 input views with patch size 16 16. As shown in Figure 7, Using GS model as an example, initialization with pretrained checkpoints substantially accelerates convergence, especially in the early training stage, where models quickly reach high PSNR compared to the one trained from scratch. Moreover, as reported in Table 3, pretrained initialization not only improves convergence speed but also leads to higher final quality after full training. The gains persist even when trying to adapt the pretrained weights to different 3D representations. The results suggest that pretrained knowledge of novel view synthesis serves as an effective inductive bias for 3D reconstruction, improving both training efficiency and final rendering fidelity. Table 3. Leveraging pretrained knowledge from novel view synthesis tasks improves the final 3D reconstruction quality across different 3D representations. Table 4. Although progressive GS prediction with merging provides more efficient computation, the reconstruction quality is degraded due to accumulated errors (compared on 32 views under 1K iterations finetuning). PSNR SSIM LPIPS Predict & Merge Ours 21.50 23.63 0.891 0.904 0.318 0.259 Optimizer and Losses We use Muon optimizer for its stabilty and robustness. Table 5 shows that use Muon as opitmizer can bring better results even on low resolution setting. It will bring better results on longer sequence (e.g. high resolution and more input views). Also, incorporating depth and opacity as regularization can help reduce opaque Gaussians. Table 5. Ablation on 32 view 256 144 input with the same iterations across settings. Muon Opacity+Depth SSIM LPIPS Opacity> 0. PSNR 20.44 20.68 20.76 0.649 0.661 0.666 0.295 0.290 0. 96% 97% 47% 3D Rep. Type PSNR SSIM LPIPS 4.6. Discussions and Limitations GS Triplane w/o Pretrain Pretrain w/o Pretrain Pretrain 32.77 33. 26.40 27.87 0.026 0.024 0.903 0.925 0.969 0.972 0.093 0.075 Figure 7. Our 3DGS reconstruction model leverages pretraining with LVSM on novel view synthesis tasks, which significantly accelerates learning and leads to better performance, compared to training from scratch. Autoregressive strategy In Section 3.3, we introduce our autoregressive reconstruction strategy. Here we consider more straightforward way called Predict & Merge: instead of generating new 3DGS G(b) F(W, v(b)) for each step, we reuse the previously predicted Gaussians G(b1) and merge them with the newly predicted subset G(b) F(W, (b)), forming G(b) = G(b1) G(b). Here, (b) is subset of (b) containing only new virtual views not covered in (b1). However, we found that though this approach is computationally more efficient, it cannot correct the accumulated errors in G(b1), leading to worse results than our proposed full reconstruction method, as shown in Table 4. Our fast-weight memory has fixed size, which may limit its ability to handle highly complex scenarios with extremely large numbers of input views. More discussions can be found in the supplemental. Also, we observe that, compared with the pretrained LVSM model from which we finetune, our quality slightly degraded but we have much faster rendering speed and explicit 3D representations for flexible downstream tasks. This might reflect the inherent trade-off between implicit and explicit representations. Future works might design better memory mechanism, further improve the quality, and speed up the inference to enable real-time high-quality reconstruction for streaming inputs. 5. Conclusion In this paper, we present tttLRM, large reconstruction model that supports both feedforward long-context and autoregressive 3D modeling. Under the Test-Time Training framework, it produces implicit fast-weight representations and converts them into explicit 3D representations such as Gaussian splats and triplanes for efficient, high-quality novel view synthesis. Experiments on objectand scenelevel datasets show that tttLRM outperforms prior feedforward methods in quality and scalability while approaching the speed of explicit representations. Our framework helps close the gap between neural network rendering and realtime explicit 3D systems."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to thank Ziwen Chen for the evaluation of the baselines and Tianyuan Zhang for helpful discussions on LaCT."
        },
        {
            "title": "References",
            "content": "[1] Joshua Ainslie, Santiago Ontanon, Yi Tay, et al. Jamba: Hybrid transformermamba models for efficient long-context processing. arXiv preprint arXiv:2403.19887, 2024. 3 [2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural point-based graphics. In European conference on computer vision, pages 696 712. Springer, 2020. 3 [3] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. 3 [4] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1612316133, 2022. 2 [5] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image In Propairs for scalable generalizable 3d reconstruction. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. 3 [6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European conference on computer vision, pages 333350. Springer, 2022. 2 [7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. arXiv Training deep nets with sublinear memory cost. preprint arXiv:1604.06174, 2016. [8] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Ttt3r: 3d reconstruction as test-time training. arXiv preprint arXiv:2509.26645, 2025. 2 [9] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. 3 [10] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. 3 [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 5 [12] John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2367 2376, 2019. 3 [13] John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clement Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, and Ryan Overbeck. Quark: Real-time, high-resolution, and general neural view synthesis. ACM Trans. Graph., 43(6), 2024. [14] Quankai Gao, Qiangeng Xu, Hao Su, Ulrich Neumann, and Zexiang Xu. Strivec: Sparse tri-vector radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1756917579, 2023. 2 [15] Michael Goesele, Brian Curless, and Steven Seitz. MultiIn 2006 IEEE Computer Society view stereo revisited. Conference on Computer Vision and Pattern Recognition (CVPR06), pages 24022409. IEEE, 2006. 2 [16] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. 3 [17] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2, 3, 5 [18] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 2 [19] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.17242, 2024. [20] Keller Jordan, Yuchen Jin, Vlado Boza, You Jiacheng, Franz Cecista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan.github.io/posts/muon, 6. 3 [21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. 3 [22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 6 [23] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM Transactions on Graphics (TOG), 43(4):115, 2024. 2 [24] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36 (4):113, 2017. 5 [25] Haowen Lai, Zitong Lan, and Mingmin Zhao. Non-line-ofsight 3d reconstruction with radar. In Annual Conference on Neural Information Processing Systems (NeurIPS), 2025. 2 [26] Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, and Xingang Pan. Stream3r: Scalable sequential 3d reconstruction with causal transformer. arXiv preprint arXiv:2508.10893, 2025. [27] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. 2 [28] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 5 [29] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:1565115663, 2020. 2 [30] Yang Liu, Chuanchen Luo, Lue Fan, Naiyan Wang, Junran Peng, and Zhaoxiang Zhang. Citygaussian: Real-time high-quality large-scale scene rendering with gaussians. In European Conference on Computer Vision, pages 265282. Springer, 2024. 2 [31] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Jianbin Jiao, and Yunfan Liu. Vmamba: Visual state space model. Advances in neural information processing systems, 37:103031103063, 2024. 3 [32] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 6 [33] Ziqiao Ma, Xueyang Yu, Haoyu Zhen, Yuncong Yang, Joyce Chai, and Chuang Gan. Fast spatial memory with scalable elastic test-time training. Blog Post, 2025. [34] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. 13 [35] Ben Mildenhall, Pratul Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (ToG), 38(4):114, 2019. 3 [36] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1, 2 [37] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 2 [38] Linfei Pan, Daniel Barath, Marc Pollefeys, and Johannes Schonberger. Global structure-from-motion revisited. In European Conference on Computer Vision, pages 5877. Springer, 2024. 5 [39] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luˇcic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62296238, 2022. [40] Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR, 2021. 3 [41] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 2 [42] Qiuhong Shen, Zike Wu, Xuanyu Yi, Pan Zhou, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Gamba: Marry gaussian splatting with mamba for single-view 3d reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 3 [43] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linIn Proceedings of the IEEE/CVF winter ear complexities. conference on applications of computer vision, pages 3531 3539, 2021. 3 [44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 5 [45] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24372446, 2019. [46] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5459 5469, 2022. 2 [47] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. 3 [48] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. 2, 3 [49] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 3 [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [51] Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, et al. Mesanet: Sequence modeling by locally optimal test-time training. arXiv preprint arXiv:2506.05233, 2025. 3 [52] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. 3 point-based geometry processing. ACM Transactions On Graphics (TOG), 38(6):114, 2019. 3 [65] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. 1 [66] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1944719456, 2024. 6 [67] Yuheng Yuan, Qiuhong Shen, Shizun Wang, Xingyi Yang, and Xinchao Wang. Test3r: Learning to reconstruct 3d at test time. arXiv preprint arXiv:2506.13750, 5, 2025. 2 [68] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic radiance fields. In European Conference on Computer Vision, pages 717733. Springer, 2022. 13 [69] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024. 2, 3, 5, 6, 7 [70] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [71] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. 2, 3, 5, 12 [72] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Longsequence large reconstruction model for wide-coverage In Proceedings of the IEEE/CVF Internagaussian splats. tional Conference on Computer Vision, pages 43494359, 2025. 2, 3, 5, 6, 7 [53] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 2, 5 [54] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain In Proceedings images with optimal training supervision. of the Computer Vision and Pattern Recognition Conference, pages 52615271, 2025. 5 [55] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 2 [56] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for highquality meshes. arXiv preprint arXiv:2404.12385, 2024. [57] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1645316463, 2025. 3 [58] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Pointnerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54385448, 2022. 2 [59] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2192421935, 2025. 2 [60] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. Advances in neural information processing systems, 37:115491115522, 2024. 3 [61] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767783, 2018. 2 [62] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution In Proceedings of multi-view stereo depth inference. the IEEE/CVF conference on computer vision and pattern recognition, pages 55255534, 2019. [63] Xuanyu Yi, Zike Wu, Qiuhong Shen, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Shuicheng Yan, Xinchao Wang, and Hanwang Zhang. Mvgamba: Unify 3d content generation as state space sequence modeling. Advances in Neural Information Processing Systems, 37:75807607, 2024. 3 [64] Wang Yifan, Felice Serena, Shihao Wu, Cengiz Oztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction"
        },
        {
            "title": "Contents",
            "content": "A. Further Discussions B. Experiment Details B.1. Scene-level Training . B.2. Object-level training . . . . . . . . . . . . . . . . . . . . . . . . . C. More results and Comparison A. Further Discussions 12 12 12 13 13 Effect of Scene Complexity on Fast Weights The memory of fast-weights has fixed capacity and is bounded, especially in the autoregressive setting. Our empirical analysis on DL3DV scene labels indicates that higher scene complexity leads to degraded performance, as observed in outdoor vs. indoor scenes (PSNR: 24.45 vs. 24.96) and highvs. low-frequency scenes (PSNR: 24.20 vs. 25.97). The memory capacity is also influenced by sequence length, where earlier inputs may be gradually forgotten as more tokens are processed. Selective Update of Fast Weights in AR Setting Instead of updating the fast weights only according to current inputs, we can further use history states for selective update to mitigate drifiting. Inspired by [33], we explore mechanism to prevent weight drift. Specifically, we approximate the diagonal of the Fisher information using an exponential moving average of squared gradients, as an estimate of parameter importance. Meanwhile, we maintain sliding anchor via EMA to track the historical trajectory of the fast weights. After each gradient update, we apply elastic regularization based on parameter importance. Specifically, we leverage Fisher information for selective update, where parameters with high Fisher values that are important for the current input, are left parameters with high Fisher values unconstrained, while parameters with low Fisher values are pulled back toward the anchor. This encourages adaptation to the current input and suppresses drift in unimportant parameters. This training-free strategy can further improve our autoregressive model, and we envision it to be more effective by incorporating it into training for future work. Table 6. Training-free selective update considering history fast weights can further enhance our AR model. PSNR SSIM LPIPS w/o selective selective 24.81 24. 0.814 0.818 0.225 0.223 Scaling to More Input Views With distributed training, tttLRM can be further scaled to hundreds of views given enough compute. For example, by finetuning our full model with more iterations on 128 input views (more than 1M tokens), it can achieve 26.80 PSNR. Possible Usage of Attention Layers We deliberately avoid attention blocks in our model since it has quadratic complexity O(N 2d) compared to our linear FLOPS O(N d2) of LaCT blocks (N is the number of tokens and is hidden dimension). Therefore, attention will bottleneck the computation with growing number of tokens and be very slow in our million-level token setting. As shown in Figure 8, even 3-layer attention only will be slower than our 24layer LaCT blocks from 2M tokens (256 views). With more compute, our model can easily scale to longer sequence and remain linear complexity. Figure 8. Time comparison of 3 Attention layers vs 24 layers of LaCT blocks under different numbers of tokens. B. Experiment Details B.1. Scene-level Training We adopt curriculum training strategy that progresses from low to high resolution, motivated by two main reasons. First, fast low-res pretraining enables the model to train with large batch size with faster iteration time. Second, we found that even with pretrained TTT-LVSM [71] checkpoints at high-resolution (i.e. 960 640), the model cannot predict reasonable Gaussians at the beginning iterations, leading to excessive GPU memory usage due to the rendering of large number of Gaussians. For scene-level training, We train our model on three stages with 144 256, 288 512 and 540 960 resolution. For each stage, we resize the images to the target resolution, which all have the same aspect ratio as the original dataset. For all stages, we first determine continuous range based on the start and end frames from the entire video sequence and another 4 views as supervision and use patch size of 1616. We train on the resolution of 256256 with batch size of 256 for 60K iterations and finetune on 512512 with batch size of 64 for another 20K iterations. C. More results and Comparison In Table 7, we provide more detailed comparison including our models performance in autoregressive reconstruction mode, which also constantly outperforms Long-LRM and remains competitive with, or superior to, optimizationbased baselines. We also show results where we combine our method with few additional optimization steps. It demonstrates that the reconstructed model can be further improved with minimal optimization cost, surpassing both purely optimization-based methods and the previous state-of-theart feed-forward method, Long-LRM, under the same postoptimization setup. Notably, the quality of Long-LRM with 3-step postoptimization is still lower than our model without postoptimization, even though it requires more time to perform the optimization than our feedforward inference. Table 7. More quantitative comparison on both DL3DV-140 and Tanks&Temples datasets under 32/64 input views. Our method surpasses previous feedforward methods and can further surpass optimization-based methods with few steps post-optimization. Note that Long-LRM trains separate model for each input view, while we are single model across all input views. Our model can be linearly accelerated with multiple GPUs, here we report time on single Nvidia A100 80GB GPU. Views Method Time DL3DVTanks&Temples PSNR SSIM LPIPS PSNR SSIM LPIPS 32 64 3D GS30k Mip-Splatting30k Scaffold-GS30k Long-LRM (32v model) Long-LRM (w/ 3-step optim) Long-LRM (w/ 10-step optim) Ours Ours (w/ 3-step optim) Ours (w/ 10-step optim) 3D GS30k Mip-Splatting30k Scaffold-GS30k Long-LRM (64v model) Long-LRM (w/ 3-step optim) Long-LRM (w/ 10-step optim) Ours Ours (w/ 3-step optim) Ours (w/ 10-step optim) 13m 13m 16m 1s 12s 37s 7.2s 18s 42s 13m 13m 16m 3.7s 38.9s 114s 14.8s 47s 124s 23.60 23.32 24.77 24.10 24.99 25.60 25.07 25.86 26.37 26.55 26.29 27.07 24.63 25.74 26.72 25.95 26.97 27.65 0.779 0.784 0.805 0.783 0.809 0.826 0.822 0.842 0. 0.852 0.850 0.857 0.799 0.833 0.852 0.844 0.866 0.880 0.213 0.217 0.205 0.254 0.243 0.233 0.215 0.208 0.201 0.164 0.166 0.173 0.243 0.225 0.212 0.195 0.185 0. 18.10 18.39 18.41 18.38 18.69 18.90 19.22 19.57 19.78 20.78 20.08 20.96 19.11 19.69 20.03 20.31 20.76 21.07 0.688 0.700 0.691 0.601 0.623 0.642 0.662 0.687 0. 0.778 0.759 0.768 0.627 0.659 0.681 0.700 0.724 0.743 0.269 0.262 0.290 0.363 0.360 0.350 0.305 0.300 0.291 0.205 0.220 0.240 0.346 0.333 0.320 0.274 0.269 0. from the dataset. The range is randomly sampled from 128 to 512 for each sample to ensure enough coverage of the scene. Then, we randomly sample 124 frames from this range, from which both input and target views will be further sampled. They are ensured to have overlap frames for stable training. We train the model across 16 to 64 input views. For training, we use the input views as the virtual views and found that provides the best results. For the first stage, we train the model with peak learning rate of 3e4 with 2K warmup steps and cosine decay. We use AdamW optimizer with betas (0.9, 0.95) and weight decay 0.05. We train the model using batch size of 128 for 80K steps, which is around 0.3T tokens. For the second stage, we finetune the model at the resolution of 288 512 with peak learning rate of 5e5. We use batch size of 64 to train 6K steps. For the final stage, we enable depth loss and opacity loss, training the model with 32 input views with 5K steps with peak learning rate 1e5 and batch size of 64. Finally, we train the model with 16 to 64 input views for another 1K steps. We prune 70% Gaussians with the smallest opacity for 64 views and 60% otherwise."
        },
        {
            "title": "For autoregressive model",
            "content": "training, we finetune our model on the final stage checkpoints for around another 3K iterations with peak learning rate 1e4 and batch size of 64. We train the model on input views from 8 to 64. Our models are trained on 64 Nvidia A100 80GB GPUs. Besides, we use gsplat Python library for efficient Gaussian training. We enable torch.compile to accelerate computation, achieving roughly 30% per-iteration speedup. To further optimize memory and stability, we implement gradient checkpointing [7] and mixed-precision training [34] with the BFloat16 format. For Gaussian rendering, we utilize deferred backpropagation [68] to reIn addition, iterations duce GPU memory consumption. with gradient norm that exceeds 5.0 are skipped to improve training stability. B.2. Object-level training For the GS-based model, we use 8 views as input and another 8 views as supervision and use patch size of 16 16. We firstly sample set of 15 images (from 32 renderings) as data point, from which we randomly select 8 input views and 8 supervision views independently. This sampling strategy encourages more overlap between input views and rendering views than directly sampling from 32 rendering views. We train on the resolution of 256 256 with batch size of 512 for 80K iterations with peak learning rate of 4e4. We then finetune on 512512 with batch size of 128 for another 10K iterations with peak learning rate of 5e5. We further finetune on 1024 1024 with batch size of 64 for another 4K iterations with peak learning rate of 5e5. For the triplane-based model, we use 4 views as input"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "UCI",
        "University of Pennsylvania"
    ]
}