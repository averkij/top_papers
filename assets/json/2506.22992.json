{
    "paper_title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
    "authors": [
        "Yulun Jiang",
        "Yekun Chai",
        "Maria Brbić",
        "Michael Moor"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 2 9 9 2 2 . 6 0 5 2 : r MARBLE: Hard Benchmark for Multimodal Spatial Reasoning and Planning Yulun Jiang1,2 Yekun Chai2 Maria Brbić1 Michael Moor2 1 EPFL 2 ETH Zurich https://marble-benchmark.github.io July 1, 2025 Abstract The ability to process information from multiple modalities and to reason through it step-by-step remains critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLEall the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still challenge for existing MLLMs. Moreover, we show that perception remains bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps."
        },
        {
            "title": "Introduction",
            "content": "Human reasoning is inherently multimodal and sequentialintegrating modalities such as language or vision as context to draw conclusions through structured, step-by-step thought. While LLMs have made significant strides in step-by-step reasoning [27, 11, 8, 18], the multimodal reasoning abilities of Multimodal LLMs (MLLMs) are still in their infancy and not yet well understood. Achieving complex, multi-step, multimodally grounded reasoning is critical for building intelligent systems that can generalize across domains and interact adaptively with complex environments. Recent benchmarks such as ScienceQA [14], MathVista [16], and MMMU [30] have shown that MLLMs can solve tasks involving both visual and linguistic understanding. However, these benchmarks often emphasize relatively shallow forms of reasoning, such as single-step question answering or factual retrieval. They frequently conflate perception (e.g., interpreting an image or diagram) with reasoning (e.g., drawing logical inferences, comparing evidence, or crafting multi-step plan), reducing complex reasoning to pattern matching and multimodal integration. As result, current evaluations underexplore and undermeasure an MLLMs capacity for deep, structured reasoning. Moreover, the recent literature has focused heavily on abstract reasoning in domains such as advanced mathematics or code generation, where multimodal embodiment plays limited role. In contrast, interacting with and planning in spatially and physically constrained environments 1 Table 1: Conceptual overview of the MARBLE benchmark. Dataset Description Modality Subtasks # Samples Metrics M-Portal Solving complex multimodal spatial reasoning and planning problems. Text, Image Plan correctness, Fill-the-blanks 512 512 M-Cube Assembling 3D Cube from six jigsaw pieces. Text, Image CUBE, CUBE-easy 1,000 1,000 F1-Score, Accuracy Accuracy is fundamental dimension of human intelligence but it is largely missing from todays MLLM evaluations. While recent effort introduced an escape room-inspired benchmark [26], frontier models were not sufficiently challenged by its task complexity, achieving up to 100% escape rate. Thus, hard benchmarks that stress multi-step planning and spatial reasoning under physical constraints remain an open need. Analogous to how difficult challenges have historically driven progress, we believe that an ARC-like test [6] for multimodal reasoning could spark foundational advances in MLLM capabilities. In this work, we present MARBLE (MultimodAl Reasoning Benchmark for Language modEls), highly challenging multimodal reasoning benchmark specifically designed to evaluate step-bystep, multimodally grounded reasoning in MLLMs. Our benchmark introduces tasks that are cognitively demanding, requiring models to decompose complex multimodal prompts into interpretable intermediate steps, align information across inputs, and to carefully craft multi-step plan to solve complex problems under diverse spatial and physical constraints. Unlike prior datasets that overemphasize final-answer accuracy, our benchmark emphasizes reasoning trajectories and plans, providing both gold-standard rationales and mechanisms for evaluating intermediate step fidelity. MARBLE consists of two main tasks, M-Portal which tests complex spatial reasoning and planning abilities inspired by the puzzle video game Portal 2, and M-Cube, which tests the ability to understand and assemble 3D jigsaw pieces into target cube shape, inspired by the Happy Cube puzzle. Each dataset also contains two subtasks at different difficulty levels, as shown in Table 1. We conduct an extensive evaluation of MARBLE across 12 state-of-the-art MLLMs and reasoning models. Intriguingly, all the prominent models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Even in simplified configurations, only about half of the models are able to outperform the random baseline. Notably, GPT-o3 [18] is the only model demonstrating reasonable performance on easier tasks, achieving 17.6% on the simpler M-Portal subtask and 72.0% accuracy on CUBE-easy. These results indicate that complex multimodal reasoning remains significant challenge for current MLLMs. Our further analysis shows that perception is still bottleneck for multimodal reasoning: all the advanced MLLMs completely fail to understand and extract structured information from the visual inputs. Additionally, we present an interactive setup for M-Cube where the model iteratively refines its response based on the feedback from solution validator tool, reflecting the real-world problem-solving processes. We hope that MARBLE will serve as probing benchmark to reveal the limitations of current MLLMs and drive the development of next-generation models with stronger capabilities in multi-step multimodal reasoning and planning."
        },
        {
            "title": "2 Related work",
            "content": "Chain-of-Thought and multimodal reasoning paradigms. The Chain-of-Thought (CoT) prompting paradigm has significantly advanced reasoning in language models by enabling stepwise decomposition of complex problems [27]. The Multimodal Chain-of-Thought (MCoT), its extension to the multimodal domain, represents natural progression, encouraging models to articulate intermediate reasoning steps while integrating multiple modalities such as images, text, and diagrams. Recent works like [25] highlight prompt-based, plan-based, and learning-based MCoT strategies, yet 2 also underscore the lack of robust, diagnostic benchmarks tailored to multimodal reasoning. Recent multimodal instruction tuning approaches fine-tune LLMs augmented with visual encoders to follow multimodal prompts [13, 33]. While these models can generate fluent outputs, their reasoning often lacks depth or consistency, particularly on tasks involving spatial, numerical, or abstract visual patterns [30, 5]. Multimodal reasoning benchmarks. Several datasets have been proposed to evaluate multimodal reasoning, such as ScienceQA [14], MMMU [30], MathVista [15], EMMA [9] and MEGABench [4]. These benchmarks span academic knowledge domains and require integrating visual and textual information. However, they often prioritize answer accuracy over the evaluation of the full reasoning trace, making it difficult to diagnose model errors. Others, like PuzzleVQA [5] and NLVR [28], introduce abstract reasoning challenges but are limited in modality diversity and stepwise supervision. Recent works like Critic-V [31] and MMIR [29] introduced frameworks for multimodal inconsistency detection or critic-guided refinement, which improved performance but was limited to rather shallow reasoning paths. There are few previous benchmarking approaches that leveraged multimodal tasks inspired by video game puzzle environments [32, 19, 24]. Most recently and closely related, [26] proposed MM-Escape, an escape-room like environment where MLLMs have to navigate and leverage the surroundings (e.g., retrieving hidden key) in order to escape room. While this benchmark shares some similarity with the M-Portal task in MARBLE, M-Portal introduces novel and much harder, multi-step problem solving challenge. To illustrate this, consider GPT-4o model which solved 70 100% of the maps in MM-Escape, but performed very poorly on M-Portal (e.g., 4.1% accuracy on fill-the-blanks)."
        },
        {
            "title": "3 MARBLE: a benchmark for multimodal spatial reasoning",
            "content": "and planning We present MARBLE, challenging game-inspired multimodal reasoning benchmark designed to evaluate the complex reasoning abilities of multimodal LLMs (MLLMs). In contrast to prior reasoning benchmarks that evaluate only the final answer independent of the reasoning trace, MARBLE focuses on assessing the correctness of the reasoning process itself. MARBLE consists of two tasks, MPortal and M-Cube, both of which require complex, multi-step and multimodal reasoning skills to forge an appropriate plan that accounts for complex spatial and physical problem constraints. The M-Portal task challenges MLLMs to solve problems derived from Portal 2 videogame with multi-step reasoning and planning. The M-Cube evaluates MLLMs in their ability to solve Happy Cube puzzles, i.e., rotate complex shapes to arrange them into 3D cubes under physical constraints."
        },
        {
            "title": "3.1 M-Portal\nThe M-Portal task is a multimodal reasoning task that involves planning, spatial reasoning, as well\nas multimodal integration. M-Portal is inspired by the game Portal 2, a first-person perspective\npuzzle videogame released by Valve in 2011. Portal challenges players to overcome obstacles and\nto pass through rooms by means of placing two portals through which players can teleport. A key\nmechanic in Portal is the conservation of momentum: when a player enters one portal with a given\nvelocity, they exit the second portal with the same relative momentum. This enables creative traversal\nstrategies, such as jumping across large gaps or over obstacles, by combining gravity-driven falls\nwith portal placement. Various additional features (e.g., buttons, lasers, tractor beams, liquids) add\nfurther complexity to the puzzle environments. The ultimate trial will be for MLLMs to interactively\nnavigate and solve the game. However, to enable broad accessibility and usability of this benchmark,\nwe abstract a given map into a set of visual question-answering tasks that require the MLLM to",
            "content": "3 Figure 1: Overview of the M-Portal Dataset of the MARBLE Benchmark. Illustrated is rather basic level Portal 2 problem, which only requires seven steps to solve. For comparison, the advanced problems introduced in this benchmark may involve several dozens of steps. Also, steps are not always decomposed into their most atomic form to keep enough complexity within step to make mistaken steps harder to detect. Appendix provides more examples. integrate several depictions of the map, textual instruction to the map, in order to examine partial or complete chain of thought (CoT) solution plans that may consist of dozens of steps. Figure 2 gives an introductory overview of how basic portal map could look like, displaying scene overview (top left), the step-by-step solution, and few in-game screenshots. M-Portal consists of 1 024 problems that comprise two types of evaluations, plan correctness and fill-the-blanks, each contributing to 512 problems. Problem statement. Given an input = (I, ), where is set of multimodal inputs (e.g., screenshots of Portal map or textual contextualization of the environment) and is task instruction, the objective is to generate Chain-of-Thought (CoT) plan = (s1, s2, . . . , sn) consisting of interpretable, physically sound reasoning steps that, if executed, would successfully solve the problem. The reward of plan R(P ) is 1 if the exit door is passed, and 0 otherwise. Then the objective is to evaluate the ability of models to implement the mapping that maximizes the reward, i.e., = arg max EXD [R(F (X))] , where : (cid:55) = (s1, s2, . . . , sn). (1) (2) Figure 2: Data generation and evaluation pipeline for the M-Portal task. The top row illustrates how given Portal 2 map (sourced from the community test chambers) was analyzed with human annotation to produce set of illustrative screenshots that fully depict the map, textual map instructions, ground-truth solution chain of thought (CoT), as well as set of five mistaken steps. The steps are designed to operate independently so that mistakes and correct steps can be easily combined. The bottom row indicates two evaluation types of M-Portal: first, plan correctness, binary evaluation where candidate solutions have to be rated as correct or wrong. Second, fill-the-blanks evaluation, where multiple steps of the ground truth CoT solution are masked, and multiple options are available to fill in at the right place. Data collection. For data collection, human annotator with advanced Portal 2 experience browsed through top-rated maps from the Portal 2 community test chambers. We focused on the community test chambers, as they were often self-contained, well-defined problems in single room. The annotator selected 16 high-quality maps that received top user-rating, while being compactly shaped such that they would be amenable to capture within few screenshots. Figure 2 gives an overview of how the M-Portal dataset was created in the top row, whereas the bottom row indicates the evaluation strategies employed in the M-Portal task. Evaluation subtasks. Since direct execution and success validation in the Portal environment would depend on closed-source game environment and could involve brittle interfacing and limited accessibility, we focus on evaluating the ability of model to reason about the correctness of candidate plans or the missing steps in incomplete plans. For this, we consider two types of closed-ended evaluations: plan correctness and fill-the-blanks tasks. i) Plan correctness: Is the provided candidate plan correct? Plan correctness is the binary classification task and requires answering yes/no questions. It is harder task compared to fill-the-blanks because models have to carefully review lengthy candidate plans that may be dozens of steps long and involve various spatial and physical constraints and dependencies. These candidates may contain no mistake at all up until five mistaken steps. This task has significant class imbalance, as one Portal map with five available mistaken steps allows the creation of 25 = 32 candidates that leverage individual mistakes, whereas only one out of 32 candidates is correct. ii) Fill-the-blanks: Can the model accurately identify several missing steps given surrounding context and few candidate options? On the easier fill-the-blanks task, models receive partial plan to solve the Portal map whereas several steps are masked. To fill the missing steps, the model needs to choose five correct options from five mistake or distracting options in correct order. Even though this task is hard for naive random baseline, for model that is able to interpret the multimodal inputs as well as the partial solution, it should be easier to identify the correct missing steps especially since mistaken steps also appear in their correct version as highly similar options. Furthermore, fill-the-blanks can also be seen as simplification as it helps the model focus its attention on few relevant steps out of large sequence, whereas in the binary evaluation any step could be potentially mistaken."
        },
        {
            "title": "3.2 M-Cube\nProblem statement. The M-Cube task is a 3D spatial puzzle inspired by the Happy Cube, a\nmechanical puzzle originally invented by Dirk Laureyssens in 1986. In this task, one is presented with\n6 jigsaw-style pieces taken from the faces of a 5 × 5 × 5 Cube. Each piece is featured by the bump\nand gap pattern on its edges. The goal is to assemble the pieces into a valid cube where the edges\nare aligned seamlessly without gap or overlap. To solve the M-Cube task, an MLLM needs to assign\neach piece into a cube face with proper orientation, i.e., to rotate and/or flip the piece accordingly\nto align with other pieces. For each problem, an MLLM must account for 6! possible piece-to-face\nassignments (modulo rotational symmetries), and for each piece, 8 discrete states of rotations and\nflips, resulting in a combinatorial explosion of candidate solutions. Among the vast search space,\nonly very few solutions are valid given the geometric constrains imposed by the interlocking bump\nand gap patterns. András et al. [1] reported that most commercially available cubes have only one\nsolution (up to rotational equivalences), making this a challenging reasoning problem.",
            "content": "Data generation. While the M-Cube tasks are inspired by the Happy Cube puzzle, we generate all samples synthetically. Figure 3 gives an overview of the workflow. Specifically, the data generation pipeline starts with 5 5 5 cube and disassembles the surface into 6 interlocking pieces. Each piece can be regarded as 5 5 grid, where the center 3 3 region is always preserved. For remaining cells located on the edges, we randomly assign each cell to one of the adjacent faces of the big 5 5 5 cube, to create the bump and gap patterns along the boundary. After that, the obtained pieces are shuffled and rendered from random 3D viewpoint as the input to an MLLM. We interactively selected viewpoint ranges such that the shape was clearly discernible. Concretely, we render the objects by sampling camera elevation in the range of 155 to 115 and an azimuth in the range of 150 to 90, relative to the canonical front view. The base view corresponds to an elevation of 135 and an azimuth of 120, with uniformly random perturbations of 20 and 30, respectively. Solution validator. The model is required to find the correct piece-to-face mapping and the orientation of 6 pieces. However, for each problem, there is no unique solution since cube contains 24 rotational symmetries. Therefore, instead of directly comparing the answer to ground-truth, we provide solution validator by testing whether the solution from MLLM could successfully assemble the pieces into perfect cube. Beside binary evaluation, the solution validator could also identify the conflicts in given configuration, such as mismatched edges. This diagnostic feedback can be used by an MLLM to iteratively refine its solution. See Appendix for example. 6 Figure 3: Overview of the M-Cube workflow including data generation, problem rendering, as well as solution validation. Appendix provides more dataset examples. Evaluation subtasks. To measure the performance of MLLMs with controlled difficulty level, we create two subtasks called CUBE and CUBE-easy. Each subtask contains 1000 examples. CUBE-easy is simplified version of CUBE along three axes: i) the input pieces are represented as 2D arrays instead of the rendered image to reduce the perception error of MLLM (see the discussion in Section 3.5 for more details); ii) each puzzle is specially designed such that the solution does not require flipping of any pieces; iii) partial solution with the arrangement of 4 pieces is provided in the prompt, leaving only 2 missing pieces to be placed. Consequently, ii) and iii) significantly reduce the size of search space. In comparison, CUBE retains the full complexity of the task, where the MLLM needs to understand the input images, and explore over all the possible arrangements of the 6 pieces."
        },
        {
            "title": "3.3 Evaluated models",
            "content": "We evaluate performance on the MARBLE benchmark using eight state-of-the-art MLLMs, including both open-source and closed-source models with advanced multimodal reasoning capabilities. Specifically, we assess three representative open-source MLLMs: Qwen2.5-VL-72B [3], InternVL3-78B [34] and Llama-4-Scout [17], alongside six closed-weight models: GPT-4o [10], GPT-o3 [18], GPT-o4mini [18], Claude-3.7-Sonnet [2] Gemini-2.5-pro [7], and Seed1.5-VL [22]. In addition, we also include three text-only models DeepSeek-R1 [8], DeepSeek-R1-0528 and Qwen3 [3] in the evaluation. We remove or manually convert the input images into textual descriptions to evaluate the models that only takes text inputs. All the experiment configurations, prompts and hyperparameters are detailed in the Appendix B. Experiments are conducted on single node server with 8 Nvidia H200 GPUs."
        },
        {
            "title": "3.4 Results on M-Portal\nOverall performance. We evaluate state-of-the-art MLLMs on the plan correctness and\nfill-the-blanks tasks of the M-Portal, as reported in Table 2. On the plan correctness task,\nall investigated models (MLLMs as well as text-only LLMs) performed very poorly with a minority\nclass F1 score of around 6%, similar to the random baseline. In the easier fill-the-blanks task, 8\nout of 12 models outperform the random baseline. In particular, the performance gap compared to\nthe random baseline is substantial (≥ 5%) for DeepSeek-R1, Claude-3.7-Sonnet, DeepSeek-R1-0528,\nGemini-2.5-pro and GPT-o3 that significantly outperforms all other models. Still, even the best\nperforming model, GPT-o3, manages to correctly solve only 17.6% of the problems. Note that\nalthough the fill-the-blanks task results in random baseline scores, it is expected to be easier than\nthe plan correctness task for models capable of interpreting the multimodal inputs and leveraging\nthe partial solution.",
            "content": "Token usage. On both tasks, the number of output tokens of reasoning models is significantly larger than those of open-source models, e.g., Gemini-2.5-pro spends on average 9.2 thousand tokens 7 Table 2: Performance of state-of-the-art MLLMs on the M-Portal tasks. Models are evaluated on two types of closed-ended tasks: the plan correctness and the fill-the-blanks tasks. We report F1-score for plan correctness and accuracy for fill-the-blanks. We also report the average output token for each model. Standard deviation is reported in Appendix B. *All the visual inputs are removed for text-only LLMs. Models F1 (%) Tokens (k) Acc (%) Tokens (k) Plan correctness Fill-the-blanks Random Qwen3-235B-A22B* InternVL3-78B Qwen2.5-VL-72B Llama-4-Scout GPT-4o GPT-o4-mini Seed1.5-VL DeepSeek-R1* Claude-3.7-Sonnet DeepSeek-R1-0528* Gemini-2.5-pro GPT-o3 6.1 0.0 6.4 6.6 6.5 6.5 0.0 7.6 6.1 6.3 0.0 4.7 6.6 - 11.7 0.1 0.2 0.3 0.2 0.2 0.6 2.5 1.1 4.1 5.3 0.8 3e-3 0.0 0.0 0.2 0.2 0.4 3.1 3.5 5.5 6.8 8.4 16.1 17.6 - 9.5 0.1 0.1 0.5 0.1 1.5 1.6 7.6 1.9 11.3 9.2 3. for one question of fill-the-blanks. Moreover, the model tends to think more on fill-the-blanks tasks compared to the plan correctness task. We hypothesize that this is related to the question format. In the fill-the-blanks task on Influence of blanks. M-Portal, each question contains multiple steps in the complete solution, and part of them are masked. To systematically understand the impact of missing information, we construct series of questions where the model is asked to fill blanks from 2n candidate options. We evaluate the performance of Qwen2.5-VL-72B and the result is shown in Figure 4. Notably, the model obtains around 70% accuracy when only single blank is present. However, the performance declines rapidly as the number of blanks increases, dropping to less than 1% when 4, which indicates the challenges of the subtask under the conditions of extensive missing information. Figure 4: The influence of number of blanks to M-Portal. Results with hint images. To better understand the M-Portal benchmark, we also collect one or more hint images for 14 out of 16 maps in the dataset. These images illustrate key insight to solve each map (see Appendix for example). We then evaluate the MLLMs performance when provided with the hint images. For the experiment, we use Seed1.5-VL, as it obtains the highest F1 score on plan-correctness. The result shows slight improvement on the plan-correctness, increasing the F1 score from 7.6% to 8.6%, while the performance of fill-the-blanks remains unchanged. These results suggest that even with an additional visual context, M-Portal continues to pose significant challenge for multimodal reasoning models."
        },
        {
            "title": "3.5 Results on M-Cube\nIn this section, we first evaluate state-of-the-art MLLMs on the CUBE and CUBE-easy tasks of the\nM-Cube. After that, we disentangle M-Cube into two factors, perception and reasoning, and conduct\ncomprehensive experiments to understand the challenges of M-Cube. Perception denotes the process\nof understanding visual inputs while reasoning refers to the process of searching the valid solution\nfrom the huge search space. Our results show that both are bottleneck of the current MLLMs. Finally,\nwe show that MLLMs could also use the solution validator as a tool to gather feedback and refine its\nresponse for solving the complex reasoning problems.",
            "content": "Overall performance. The results on the CUBE and CUBE-easy tasks of M-Cube are shown in Table 3. Intriguingly, all the advanced MLLMs completely fail on the harder subtask CUBE and obtain 0% accuracy despite more than 10, 000 tokens spent on thinking the problems. The results highlight the complex multimodal reasoning process involved in CUBE, where the model has to iterate over verification and backtracking through long reasoning chain to make final answer. In comparison, on the simplified CUBE-easy task, 6 out 12 frontier models are able to perform better than random guess. Among them, GPT-o3 achieves remarkable performance of 72.0% accuracy, substantially outperforming the second best models GPT-o4-mini, which only reaches 16%. Despite being simplified, the number of reasoning tokens spent on CUBE-easy is still the same or bit higher than that of CUBE, suggesting that CUBE-easy is already challenging task for most existing MLLMs. Interestingly, for some models (GPT-4o, GPT-o3 and GPT-o4-mini), the token usage of CUBE is significantly lower than CUBE-easy. We hypothesize this might due to the visual inputs of CUBE resulting in less in-depth reasoning for these models. Table 3: Model performance on the M-Cube. The tasks are evaluated in an open-ended fashion: given problem may have multiple valid solutions, thus we create solution validator to test the validity of assembly plans that are proposed by the model. We report accuracy in percentage points for both tasks. Random guess is estimated as the ratio of valid solutions to the total search space. Standard deviation is reported in Appendix B. *Result is obtained by converting visual input into 2D arrays in text. Models Acc (%) Tokens (k) Acc (%) Tokens (k) CUBE CUBE-easy Random Qwen3-235B-A22B* Llama-4-Scout Qwen2.5-VL-72B GPT-4o Seed1.5-VL InternVL3-78B Claude-3.7-Sonnet DeepSeek-R1-0528* Gemini-2.5-pro Deepseek-R1* GPT-o4-mini GPT-o3 1e-5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 - 7.2 0.7 0.3 0.2 3.9 0.1 13.2 21.3 27.8 16.3 1.6 1.9 9 3.1 0.3 1.6 2.0 2.0 2.0 2.8 7.4 8.0 11.0 14.0 16.0 72.0 - 16.1 1.2 0.8 0.6 16.6 1.0 13.2 21.3 28.4 17.3 11.0 21. Figure 5: Perception remains bottleneck for M-Cube. Left: perception task designed to test MLLMs ability on retrieve structured information from visual input (full prompt in Appendix A) and example response of an MLLM. Right: Performance of 8 MLLMs on this perception task based on 200 test examples. Accuracy is measured both at individual cells and for the entire 5 5 piece. All the MLLMs perform poorly and completely fail on the full-piece accuracy. Error on perception. To solve the M-Cube puzzle, the first step is to understand the visual input and retrieve the relevant information, which serves as the basis of the reasoning steps afterwards. Thus, we design perception task to measure whether the MLLMs could correctly extract information from the input image: given jigsaw-style piece in 3D viewpoint, the model is asked to convert the piece into 5 5 array, as shown in Figure 5. We evaluate all the 8 MLLMs on this perception task with 200 test examples, and report the accuracy on cells and accuracy of the whole piece also on Figure 5. Surprisingly, we found all the models could only achieve around 70% accuracy per cell. The best perception performance, is 76% accuracy from Gemini-2.5-pro, meaning that the model could still occasionally make mistakes. As result, all the models achieve 0% accuracy on the whole piece. These results highlight that even advanced MLLMs struggle with this seemingly simple perception task, posing potential bottleneck for multimodal reasoning in complex scenarios like CUBE. Though there have been few works discussing the shortfalls of visual capabilities of MLLMs, such as [20] and [23], its the first time that MLLMs have been reported to perform poorly on such simple structured perception tasks, to the best of our knowledge. Error on reasoning. Apart from the perception errors, M-Cube still remains highly challenging problem due to the vast search space from the combination of all possible arrangements and orientations of 6 pieces. Figure 6 illustrates the size of search space of M-Cube as function of both the number of missing pieces and whether solution requires flipping the pieces. In particular, CUBE comprises 6! 86 = 188, 743, 680 possible solutions. In comparison, CUBE-easy only contains 32 possible solutions, 5, 000, 000 fold reduction of the hypothesis space. To isolate the reasoning challenge from perceptual limitation, we manually convert the visual inputs into corresponding text arrays. We then compare the performance of DeepSeek-R1 in different search space configurations, as shown in Figure 7. The model obtains 57% accuracy in the simplest setting with only one missing piece. However, the performance drops drastically as the search space expands, falling to 0% when more than 3 pieces are missing. The substantial decline underscores the difficulty of reasoning among expanding combinatorial search space, major bottleneck for existing reasoning models. In summary, besides perception error, reasoning among the vast search space is also challenge, making M-Cube an especially difficult task for state-of-the-art MLLMs. 10 Figure 6: Search space of the M-Cube dataset under different configurations. Figure 7: Performance of DeepSeek-R1 across varying levels of task difficulty of the M-Cube dataset. Results with solution validator. The ability to use tools or perform function calls has emerged as crucial feature in latest MLLMs [21]. In case of M-Cube, the solution validator could serve In each round, the as an auxiliary tool to assist MLLMs in tackling complex reasoning tasks. model proposes candidate solution and evaluates it with the solution validator. Based on the validators feedback, the model could iteratively refine its response towards better solution in the next round. Specifically, we design two types of feedback: (i) Binary feedback, which simply indicates whether solution is correct or not in black box manner, (ii) Detailed feedback, which not only verifies the correctness of the solution but also provides diagnostic information such as which edges of the cube are in conflict. Figure 8 shows the performance of GPT-o4mini under both types of feedback. On CUBE-easy, the performance increases significantly for both binary and detailed feedback and detailed feedback consistently outperforms binary feedback, increasing the performance from 10% to up to 28% accuracy after 5 rounds of interactions, which indicates the value of diagnostic information. However, on more challenging CUBE dataset, the performance using the solution validator tool remains 0% regardless of the feedback type, highlighting the limitation of current MLLMs in solving harder multimodal reasoning problems. In summary, we introduce multi-step setup within M-Cube that enables iterative refinement through the feedback from solution validator. This setup closely mirrors how humans tackles real-world problem-by making initial attempts, gathering feedback from the environment, and refining their strategies accordingly. However, many current reasoning models would not retain and build upon previous reasoning steps, often discarding the reasoning in earlier context1, resulting in less effective reasoning in multi-round setup. Therefore, future models capable of interleaved thinking and tool use would benefit more from such validator-assisted setup. 1Check this OpenAI API document for example. 11 Figure 8: Performance of GPT-o4-mini on CUBE-easy with binary or detailed feedback from solution validator. On CUBE, the performance will remain 0%."
        },
        {
            "title": "4 Discussion",
            "content": "This paper introduces MARBLE, hard multimodal reasoning benchmark for MLLMs. MARBLE provides focused testbed for evaluating MLLMs on complex spatial reasoning and planning tasks that are underlying heterogenous physical constraints. Our tasks are designed such that an MLLM must first understand the physical constraints imposed by the multimodal input, and then formulate coherent, multi-step plan that draws from vast search space in order to solve the problem. MARBLE fills the gap of multimodal reasoning evaluation by shifting the focus from outcome accuracy to process-oriented, multi-steps reasoning that requires coherent multimodal understanding. By contributing challenging benchmark for multi-step, multimodal reasoning amidst spatial and physical constraints, MARBLE aspires to elicit more progress and innovation in MLLM development that will unlock unprecedented abilities in reasoning and planning amidst complex and multimodal environmentscapabilities that are essential for real-world, embodied, and general-purpose intelligence. Our empirical evaluation reveals that state-of-the-art MLLMs struggle significantly with MARBLE. They can only outperform random baselines in simplified ablations and fail even on structured perception tasks, underscoring limitations in both reasoning and visual understanding. Limitations and future work. For ease of use, we do not explore real-time interactive settings, nor do we fine-tune or adapt models at test time. Future work should investigate interactive and adaptive approaches, enabling models to reason with and through different modalitiessuch as thinking with imagesin more compositional way. Broader impact. As with any benchmark, there is risk of overfitting to dataset-specific patterns. However, our setting involves abstract puzzle domains, which do not raise direct societal risks. Advancing multimodal reasoning has strong potential for positive impact in domains like healthcare, accessibility, and education. Rigorous benchmarks like MARBLE can help ensure that future systems are robust and beneficial ahead of deployment."
        },
        {
            "title": "References",
            "content": "[1] Szilárd András, Kinga Sipos, and Anna Soós. Which is harder?-Classification of Happy Cube puzzles. 2013. [2] Anthropic. Anthropic 3.7 Sonnet and Claude Code, February 2025. URL https://www. anthropic.com/news/claude-3-7-sonnet. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. MEGA-Bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024. [5] Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. PuzzleVQA: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024. [6] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. 12 [7] Google DeepMind. Gemini 2.5: Our most intelligent ai model, March 2025. URL https://blog. google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ #gemini-2-5-thinking. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can MLLMs reason in multimodality? EMMA: an enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. [10] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [11] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. doi: 10.48550/ARXIV.2412.16720. URL https://doi.org/10.48550/arXiv.2412.16720. [12] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [13] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [14] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 2022. [15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 13 [16] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations, 2023. [17] Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, April 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. [18] OpenAI. Introducing OpenAI o3 and o4-mini, April 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. [19] Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. [20] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. In Proceedings of the Asian Conference on Computer Vision, 2024. [21] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36: 6853968551, 2023. [22] ByteDance Seed Team. Seed1.5-VL technical report. arXiv preprint arXiv:2505.07062, 2025. [23] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? Exploring the visual shortcomings of multimodal LLMs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [24] Oguzhan Topsakal, Colby Jacob Edell, and Jackson Bailey Harper. Evaluating large language models with grid-based game competitions: an extensible llm benchmark and leaderboard. arXiv preprint arXiv:2407.07796, 2024. [25] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. [26] Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, and Yang Liu. How do multimodal large language models handle complex multimodal reasoning? placing them in an extensible escape game. arXiv preprint arXiv:2503.10042, 2025. [27] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 2022. [28] Anne Wu, Kianté Brantley, and Yoav Artzi. surprising failure? multimodal llms and the NLVR challenge. arXiv preprint arXiv:2402.17793, 2024. [29] Qianqi Yan, Yue Fan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, and Xin Eric Wang. Multimodal inconsistency reasoning (mmir): new benchmark for multimodal reasoning models. arXiv preprint arXiv:2502.16033, 2025. [30] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, 2024. 14 [31] Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, et al. Critic-V: VLM critics help catch vlm errors in multimodal reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 90509061, 2025. [32] Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, and Lijuan Wang. V-mage: game evaluation framework for assessing visual-centric capabilities in multimodal large language models. arXiv preprint arXiv:2504.06148, 2025. [33] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations. [34] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. InternVL3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Illustration of Example Problems",
            "content": "A.1 M-Portal Figure 9 gives an extended overview of the M-Portal problem. It introduces simple example problem, created for illustrative purposes and does not cover the full complexity the benchmark. Each map in M-Portal requires sequence of actions to solve, making it complex multimodal reasoning problem. Figure 10 shows challenging example problem of the M-Portal task of MARBLE. Figure 10 shows input images and instruction text that describe the problem. manually curated solution is shown on the right side, together with five mistaken steps, below. hint image depicts the crucial insight that allows to solve the map. 16 Figure 9: Overview of the Portal-2 Dataset of the MARBLE-Benchmark. Illustrated is rather basic level Portal 2 problem, which only requires seven steps to solve. For comparison, the advanced problems introduced in this benchmark may involve several dozens of steps. Also, steps are not always decomposed into their most atomic form to keep enough complexity within step to make mistaken steps harder to detect. 17 Figure 10: Illustration of an example problem of the M-Portal dataset (problem 5), composed of problem description, images, solution steps, mistakes, and optional hint images. 18 A.2 M-Cube Figure 11 presents complete example question of M-Cube task, and the solution to the instance with the corresponding 2D and 3D visualization. Figure 12 shows the prompt of the perception task. Figure 11: Illustration of M-Cube Problem. Top: Example input image and prompt of the problem. Bottom: Example solution to the problem (left) and corresponding 2D and 3D visualization (right). The visualization is not part of the inputs or outputs of the benchmark. 19 Figure 12: Prompt for evaluating the perception ability of MLLMs on M-Cube. The solution validator of M-Cube can serve as an auxiliary tool to assist MLLM in solving the reasoning problems. Given candidate solution, the solution validator could determine whether the solution is correct or not (binary feedback). In addition, it can also provide diagnostic information such as edge conflicts (detailed feedback). Figure 13 illustrates an example where the MLLM leverages feedback from the validator to iteratively refine its solution. Figure 13: Example of MLLM using solution validator as tool to gather feedback and iteratively refine its response on the M-Cube dataset. 20 Experiment Details. Table 4 provides comprehensive list of all the models evaluated oin this paper, along with the hyperparameters. We use the same hyperparameters for evaluating both the M-Portal and M-Cube tasks. For open-source models such as Qwen2.5-VL-72B, InternVL3-78B and Llama-4-Scout, we use vLLM [12] for efficient inference, with setting of temperature of 0 and maximum output token length of 16, 000 for all the models. The open-source models are evaluated on the whole evaluation suite of M-Cube and M-Portal. In contrast, close-source models such as GPT-4o, Claude-3.7-Sonnet, Gemini-2.5-pro, GPT-o3 and GPT-4o-mini are evaluated with their respective APIs. The \"reasoning effort\" parameter, which controls the allowed length of reasoning chain, is set to \"medium\" for GPT-4o-mini and Gemini-2.5Pro, and 12,000 for Claude-3.7 Sonnet. Due to the limit of budget, we choose 200 representative examples on M-Cube and the whole set of M-Portal for evaluating close-source models. The prediction of reasoning model can vary significantly on different random seed. Due to the budget constraints, we do not re-run each experiment multiple times to directly measure the variance. Instead, we report standard deviation estimated by bootstrapping, as shown in Table 5. Table 4: MLLMs and corresponding hyperparameters for evaluating MARBLE benchmark. Reasoning effort represents the budget of reasoning tokens to generate before the final response. * For reasoning models, max tokens denote the sum of tokens generated for reasoning and final response. Model Date Temperature Reasoning Effort Max Tokens* 2025.02.19 Qwen2.5-VL-72B 2025.04.11 InternVL3-78B Llama-4-Scout 2025.04.05 Qwen3-235B-A22B 2025.04.29 2024.08.06 GPT-4o 2025.01.22 DeepSeek-R1 2025.05.28 DeepSeek-R1-0528 2025.04.28 Seed-1.5-VL 2025.02.19 Claude-3.7-Sonnet 2025.05.06 Gemini-2.5-pro 2025.04.16 GPT-o4-mini 2025.04.16 GPT-o3 - - - - - - - - 12,000 medium medium medium 16,000 16,000 16,000 16,000 16,000 16,000 16,000 16,000 16,000 25,000 25,000 40,000 0.0 0.0 0.0 0.6 0.0 - - - - - - - 21 Table 5: Results of M-Portal and M-Cube datasets, reported with standard deviation ( STD) estimated via bootstrapping. Models Qwen3-235B-A22B Llama-4-Scout Qwen2.5-VL-72B GPT-4o Seed1.5-VL InternVL3-78B Claude-3.7-Sonnet DeepSeek-R1-0528 Gemini-2.5-pro Deepseek-R1 GPT-o4-mini GPT-o Plan-correctness F1(%) STD Fill-the-blanks Acc(%) STD CUBE CUBE-easy Acc(%) STD Acc(%) STD 0.0 0.0 6.5 1.7 6.6 1.6 6.5 1.5 7.6 5.4 6.4 1.7 6.3 1.6 0.0 0.0 4.7 4.4 6.1 4.1 0.0 0.0 6.6 3. 0.0 0.0 0.2 0.2 0.2 0.2 0.4 0.3 3.5 0.8 0.0 0.0 6.8 1.1 8.4 1.2 16.1 1.7 5.5 1.0 3.1 0.8 17.6 1.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.2 1.6 0.4 2.0 0.4 2.0 1.4 2.0 1.4 2.8 0.5 7.4 2.7 8.0 2.7 11.0 3.1 14.0 3.4 16.0 3.7 72.0 4."
        }
    ],
    "affiliations": [
        "EPFL",
        "ETH Zurich"
    ]
}