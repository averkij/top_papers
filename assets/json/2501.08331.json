{
    "paper_title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise",
    "authors": [
        "Ryan Burgert",
        "Yuancheng Xu",
        "Wenqi Xian",
        "Oliver Pilarski",
        "Pascal Clausen",
        "Mingming He",
        "Li Ma",
        "Yitong Deng",
        "Lingxiao Li",
        "Mohsen Mousavi",
        "Michael Ryoo",
        "Paul Debevec",
        "Ning Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage: https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code and model checkpoints are available on GitHub: https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 2 1 3 3 8 0 . 1 0 5 2 : r Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise Ryan Burgert1,3 Yuancheng Xu1,4 Wenqi Xian1 Oliver Pilarski1 Pascal Clausen1 Mingming He Li Ma1 Yitong Deng2,5 Lingxiao Li2 Mohsen Mousavi1 Michael Ryoo3 Paul Debevec1 Ning Yu1 1Netflix Eyeline Studios 2Netflix 3Stony Brook University 4University of Maryland 5Stanford University {ryan.burgert,yuancheng.xu,wenqi.xian,oliver.pilarski,pascal.clausen, mingming.he,li.ma,mohsen.mousavi,debevec,ning.yu}@scanlinevfx.com lingxiaol@netflix.com {rburgert,mryoo}@cs.stonybrook.edu ycxu@umd.edu yitongd@stanford.edu https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow/ Figure 1. Go-with-the-Flow presents simple, robust, and easy-to-implement method for motion-controllable video diffusion models based on optical flow and noise warping. It only requires fine-tuning video diffusion models as black box using warped noise patterns. Leveraging our models, we can (1) control the motion of individual objects or parts of those objects, (2) direct the camera movement by providing global flow fields corresponding to the desired movements, and (3) transfer the motion from input videos to target contexts."
        },
        {
            "title": "Abstract",
            "content": "Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just change in data: we pre-process training videos to yield strucProject lead tured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, 1 and provide one-stop solution for wide range of userfriendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage; source code and model checkpoints are available on GitHub. 1. Introduction We adore chaos because we love to produce order. M. C. Escher, Dutch artist The essence of generative modeling lies in producing order from chaos, learning to transform random noise from the latent space into structured outputs that align with the In this paper, we propose distribution of training data. novel approach to enhance generative model learning by proactively introducing partial order into the chaos of latent space sampling. Our work is motivated by the remarkable progress in video diffusion generative models [3, 4, 7, 15, 16, 62] and the equally significant challenges they face in terms of controllability beyond text and image guidance. Finegrained interactive control over motion dynamics remains an under-explored area due to the intricate spatiotemporal correlations among video frames. The complexity of modern video diffusion architectures [7, 62], which leverage 3D autoencoders [66] and spatiotemporal tokenizers [34], further complicates efforts to adapt models for effective motion control. The optimal format for defining and disentangling motion control from other guidance remains an open question. Within the domain of motion-controllable video diffusion models, current applications typically fall into three categories: (1) local object motion control, represented by object bounding boxes or masks with motion trajectories [13, 25, 37, 43, 47, 56, 58, 61]; (2) global camera movement control, parameterized by camera poses and trajectories [17, 30, 56, 57, 60, 61] or categorized by common directional patterns such as panning and tilting [16, 61]; and (3) motion transfer from reference videos to target contexts specified by prompts or initial frames [1, 14, 29, 33, 36, 55, 63, 64]. However, these approaches share three key limitations: (1) they often necessitate complex modifications to the base model design such as guidance attention [43], limiting compatibility with modern full-attention architectures involving spatiotemporal tokens [62]; (2) they are constrained to specific applications, requiring detailed parameterized motion signals, such as camera parameters, which are challenging to acquire or estimate accurately [70], thus restricting generalizability across diverse scenarios; and (3) they are over-rigid to motion control at the cost of spatiotemporal visual quality. To address these limitations, we propose novel and straightforward method to incorporate motion control as structured component within the chaos of video diffusions latent space. We achieve this by correlating the temporal distribution of latent noise. Specifically, starting with 2D Gaussian noise slice, we temporally concatenate it with warped noise slices, given the optical flow field [51] extracted from training video sample. Fig. 2 illustrates the diagram of our method. Our approach requires only change in data: we pre-process training videos to yield warped noise and then fine-tune video diffusion model. As it occurs solely during noise sampling, our method is agnostic to diffusion model design, requiring no modifications to model architectures or training pipelines. Surprisingly, removing temporal Gaussianity from the noise distribution does not deteriorate model fine-tuning. Instead, it can be quickly adapted after fine-tuning because temporal structure in the chaos of latent space facilitates generative learning and enables motion correspondence. Temporal coherence occurring in the latent space also harmonizes motion control with per-frame pixel quality by inheriting the high-quality prior from the base model. It is worth noting that video diffusion fine-tuning relies on efficient noise warping algorithms that introduce minimal overhead during data pre-processing and noise sampling. The existing noise warping algorithm, How Warped Your Noise (HIWYN) [8], that maintains spatial Gaussianity and enables temporal flow warping, however, suffers frame count, from the quadratic computation costs w.r.t. making it much slower than in real time and therefore impractical for large-scale video diffusion model training. To address this, we propose novel noise warping algorithm that runs fast enough in real time. Rather than warping each frame through chain of operations from the initial frame, our algorithm iteratively warps noise between consecutive frames. This is achieved by carefully tracking the noise and the flow density along forward and backward flow at the pixel level, accounting for both expansion and contraction dynamics, supplemented with conditional white noise sampling from HIWYN Chang et al. [8] to preserve Gaussianity. Algorithm 1 provides further details. We validate the spatial Gaussianity and time complexity of our noisewarping algorithm and apply it to training-free image diffusion models for quantitative and qualitative assessments of controllability and temporal consistency. During video diffusion inference, our method offers one-stop solution for diverse motion control applications by adapting noise warping based on motion type. (1) For local 2 Figure 2. Our method consists of three components: flow field extraction, real-time noise warping, and diffusion model finetuning/inference. During fine-tuning, we use the original captions of video samples. At inference, our method enables adaptation of reference motion to various prompts and/or initial frames, offering creativity and diversity in generation. object motion, we interactively transform noise elements within object masks given users dragging signals. (2) For global camera movement control, we reuse the optical flows from reference videos to warp input noise, and regenerate videos conditioned on different texts or initial frames. (3) For arbitrary motion transfer, the motion representations are not limited to optical flows [51], but also include flows from 3D rendering engines [53], depth warping [65], etc. We validate the effectiveness of our solution across various video generation tasks, demonstrating its ability to preserve consistent motion across different contexts or render distinct motions for the same context. Extensive experiments and user studies indicate the advantages of our solution in pixel quality, motion control, text alignment, temporal consistency, and user preference. In summary, our contributions include: (1) novel and simple one-stop solution for motioncontrollable video diffusion models, integrating motion control as flow field for noise warping in latent space sampling, plug-and-play for any video diffusion base models as black box, and compatible with any other types of controls. (2) An efficient noise warping algorithm that maintains spatial Gaussianity and follows temporal motion flows across frames, facilitating motion-controllable video diffusion model fine-tuning with minimal overhead. (3) Comprehensive experiments and user studies demonstrating the overall advantageous pixel quality, controllability, temporal consistency, and subjective preference of our method on diverse motion control applications, including but not limited to: local object motion control, motion transfer to new contexts, and reference-based global camera movement control. 2. Related work 2.1. Image and video diffusion models With the theoretical establishments of diffusion models [20, 27, 48, 49] and their practical advancements [19, 38], and when sophisticated text encoders [44] and language models [45] meet diffusion models, great breakthroughs in textto-image generation [40, 46, 50] have revolutionized how we digitize and create visual worlds. Building upon these, image-to-image diffusion models [6, 28, 71] enable image editing applications like stylization [35], relighting [18], and super-resolution [50, 68], expanding creativity in recreating or enhancing visual worlds. natural extension of image generation use cases is to cover the temporal dimension for video generation. The most cost-efficient way is to reuse the well-trained image diffusion model weights. Directly querying the above image diffusion models using random noise to generate videos frame-by-frame often struggles with temporal inconsistency, flickering, or semantic drifting. Noise warping, HIWYN [8], as method for creating sequence of temporally-correlated latent noise from optical flow while claiming spatial Gaussianity preservation, yields temporally consistent motion patterns after querying image diffusion models without further fine-tuning. To overcome its defective spatial Gaussianity preservation and undesired time complexity, we propose novel warped noise sampling algorithm that guarantees spatial Gaussianity and runs fast enough in real time. We validate its efficacy by applying it to the training-free image diffusion models like DifFRelight [18] for video relighting and DeepFloyd IF [50] for video super-resolution. Video diffusion model training is more costly yet more effective way for video generation [3, 4, 7, 9, 16, 42, 59, 62]. AnimateDiff [16] upgrades pre-trained image diffusion models by fine-tuning temporal attention layers on large-scale video datasets. CogVideoX [62], state-of-the-art open-source video diffusion model, combines spatial and temporal dimensions by encoding/decoding videos via 3D causal VAE [66] and diffusing/denoising spatiotemporal tokens via diffusion transformers [39]. We use CogVideoX [62] as base model and incorporate our warped noise sampling for motion-controllable fine-tuning. We also fine-tune on AnimateDiff [16] to show our method is model-agnostic. 3 2.2. Motion controllable video generation Beyond text [16, 62] and image controls [15, 59, 73] for video diffusion models, motion control makes video generation more interactive, dynamically targeted, and spatiotemporally fine-grained. Current approaches to motion control follow three main paradigms: Firstly, local object motion control is represented by object bounding boxes or masks with motion trajectories [13, 25, 37, 43, 47, 56, 58, 61]. DragAnything [58] allows precise object motion manipulation in images without retraining, while SG-I2V [37] generates realistic, continuous video from single images using self-guided motion trajectories. These serve as recent baselines for local object motion control. Our method is plug-and-play, treating diffusion models as black box while using synthetic flows to mimic and densify object trajectories at the pixel level. Secondly, global camera movement control is parameterized by camera poses and trajectories [17, 30, 56, 57, 60, 61] or categorized by common directional patterns like panning and tilting [16, 61]. These methods introduce additional modules that accept camera parameters, trained in supervised manner. Other approaches [22, 67] leverage rendering priors as input for camera control. Approaches like ReCapture [69] enable reconfiguration of camera trajectories in given videos. Our method bypasses the need for extensive camera parameter collection, and directly generalizes new camera movements from reference videos at inference. Lastly, motion transfer happens from reference videos to target contexts [1, 14, 29, 33, 36, 55, 63, 64]. DiffusionMotionTransfer [63] introduces loss that maintains scene layout and motion fidelity in target videos, while MotionClone [33] uses temporal attention as motion representation, streamlining motion transfer. Using them as motion transfer baselines, we demonstrate our models flexibility in combining reference geometries with target text guidance. 3. Method Go-with-the-Flow is comprised of two separate parts: our noise warping algorithm and video diffusion fine-tuning. The noise warping algorithm operates independently from the diffusion model training process: we use the noise patterns it produces to train the diffusion model. Our motion control is based entirely on noise initializations, introducing no extra parameters to the video diffusion model. Inspired by the existing noise warping algorithm HIWYN [8], which introduced noise warping for image diffusion models, we introduce new use case for the warped noise: we use it as form of motion conditioning for video generation models. After fine-tuning video diffusion model on large corpus of videos paired with warped noise, we can control the motion of videos at inference time. Algorithm 1 Go-with-the-Flow next-frame warping 1: Input: previous-frame noise RD, previous-frame density RD, forward flow : N2, backward flow : N2. 2: Let = (V, , E) be bipartite graph with = D, = and edge set = {} to be constructed. (v, + (v)) if + (v) Contraction 3: for in do 4: 5: end for 6: for in do 7: Expansion if degG(v) = 0 then degG(v) denote the degree of in 8: (v + (v), v) if + (v) end if 9: 10: end for 11: for in do 12: Conditional white noise sampling 13: 14: degG(v) Sample (0, Id), and set (cid:80)d Xi q(v) (Zi R(v) {Xi}i[d] ) for [d] + 1 i=1 Zi 15: 16: end for 17: for (v) in do Compute next-frame noise and density q(v) 0, p(v) 0, 0 for in such that (v, v) do degG(v), α p(v) q(v) q(v) + αR(v).pop() p(v) p(v) + α + α2 1 end for if = 0 then Sample q(v) (0, 1) else q(v) q(v) Renormalize to unit variance 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end if 29: 30: end for 31: return next-frame noise and density q, p. 3.1. Go-with-the-Flow noise warping 3.1.1 Algorithm To facilitate the large-scale noise warping required by this new use case, we introduce fast noise warping algorithm (Algorithm 1) that warps noise frame-by-frame, storing just the previous frames noise (with dimensions C, where is height, is width, and is the number of channels) and matrix of per-pixel flow density values (with dimensions ). The density values indicate how much noise has been compressed into given region. Unlike HIWYN [8] which requires time-consuming polygon rasterization and upsampling of each pixel, our algo4 rithm directly tracks the necessary expansion and contraction between frames according to the optical flow and uses only pixel-level operations that are easily parallelizable. We show that our algorithm retains the same Gaussianity guarantee as HIWYN [8] (Proposition 1). Next-frame noise warping. Our noise warping algorithm calculates noise iteratively, where the noise for given frame depends only on the state of the previous frame. Let be the dimensions of each video frame. Let = [H] [W ] denote 2D matrix with height and width , where we use the notation [n] := 1, . . . , n. Given the previous frames noise1 RD and the flow density RD together with forward and backward flows2 f, : N2, our algorithm computes the next-frame noise and density q, RD such that (resp. p) is temporally correlated with (resp. p) via the flows. At high level, our algorithm (in Algorithm 1) combines two types of dynamics: expansion and contraction. In the case of expansion, such as when region of the video zooms in or an object moves towards the camera, one noise pixel is mapped to one or more noise pixels in the next frame (hence it expands). In the case of contraction, we adopt the Lagrangian fluid dynamics viewpoint of treating noise pixels as particles moving along the forward flow . This often leaves gaps that need to be filled. Hence, for regions not reached when flowing along , we use the backward flow to pull back noise pixel. That gap is filled with noise calculated with the expansion case. Additionally, to preserve the distribution correctly over long time periods, we use density values to keep track of how many noise pixels were aggregated into given region, so that when mixed with other nearby particles in the contraction case, these higher density particles have larger weight. This is illustrated in Fig. 3. We unify both expansion and contraction cases by building bipartite graph where edges represent how noise and density should be transferred from the previous frame to the next. When aggregating the influence from graph edges to form the next-frame noise q, we scale the noise in accordance with the flow density to ensure the preservation of the original frames distribution, as detailed in Algorithm 1. The expansion and contraction cases are calculated in tandem to prevent any cross-correlation, guaranteeing the output will be perfectly Gaussian. 3.1.2 Theoretical analysis Proposition 1 (Preservation of Gaussian white noise). If the pixels of the previous-frame noise in Algorithm 1 are i.i.d. standard Gaussians, then the output next-frame noise Figure 3. Diagram of our noise warping algorithm. case example of our algorithm illustrates both the expansion and contraction cases, along with example density values. Each node represents some noise pixel q. Noise values q0..3 are transferred from frame 0 to frame 1 using forward optical flow, and the remaining pixels in frame 1 that did not receive any values obtain their values from frame 0 using reverse optical flow (the expansion case). In the contraction cases such as 2, their densities become the sum of their sources. And in the expansion case, where one source pixel spreads out into multiple target pixels, such as q2 spreading out into 3, its density is dispersed. 1 and q also has i.i.d. standard Gaussian pixels. Please check the appendix for formal mathematical proof. Proposition 2 (Time Complexity). For given frame, the time complexity of this algorithm is O(D), linear time with respect to the number of noise pixels processed. Proof: There are only two cases - contraction and expansion. Because each previous-frame pixel can only be contracted to one current-frame pixel, and during expansion each current-frame pixel can only be mapped to one previousframe pixel, the total number of edges will never exceed 2D. 3.2. Training-free image diffusion models with warped noise As shown by Chang et al. [8] and Deng et al. [11], noise warping can be combined with image diffusion models to yield temporally consistent video edits without training. To do this, we first take an input video and calculate its optical flows using RAFT [51]. Then, with Algorithm 1, we use the flow fields to create sequences of Gaussian noise for each frame in the input video, ensuring that the noise moves along the flow fields. These noises are used during the perframe diffusion processes in place of what would normally be temporally independently sampled Gaussian noise. This enables temporally consistent inference for video tasks, such as relighting [18] and super-resolution [50], using image-based diffusion models. 3.3. Fine-tuning video diffusion models with warped noise 1Since different channels are treated independently, we will assume single channel in images. 2We allow flows to go out of bounds, i.e., and can land in N2 D. We use warped noise to condition video diffusion model on optical flow. In particular, we fine-tune two variants of latent video diffusion model CogVideoX [62], both the 5 3.4. Video diffusion inference with warped noise At inference, we generate warped noise from an input video to guide the motion of the output video. Then, using deterministic sampling process such as DDIM [48], we use that warped noise to initialize the diffusion process of our fine-tuned video diffusion model. This method of control is much simpler than other motion control methods, as it does not require any changes to the diffusion pipeline or architecture - using exactly the same amount of memory and runtime as the base model. In the case of local object motion control, we allow the user to specify object movements through simple user interface as shown in Fig. 5. It is used to generate synthetic optical flows, where multiple layers of polygons are overlaid on an image. Then, these polygons are translated, rotated and scaled with paths defined by the user. We warp the noise accordingly, and use that noise to initialize the diffusion process, along with text prompt, and in the case of the image-to-video model, given first frame image. By controlling the extent to which the output video follows these polygons, users can simulate camera movement by shifting the background, or even 3D motion effects by overlaying two polygons in parallax and moving them at different speeds. We find that this motion control representation is quite robust to user error, where even if the polygon only roughly matches the object or area of interest it will still produce high quality results. For synthetic object motion control, we typically use degradation value γ between 0.5 and 0.7, depending on the level of motion precision the user desires, which is higher level than we would normally use for motion transfer. The case of motion transfer and camera motion control are very similar the only difference is the source of the In the case of flows used to generate the warped noise. motion transfer, we calculate the optical flow of driving video, get warped noises that match the motion. Like in local object motion control, we use that warped noise to initialize diffusion process. In the case of motion transfer, we typically use lower degradation value γ between 0.2 and 0.5, as we usually want the output videos motion to match the driving videos motion as closely as possible. 3.5. Implementation details We fine-tune the recent state-of-the-art open-source video diffusion model, CogVideoX-5B [62], on both its T2V and I2V variants. We use large general-purpose video dataset composed of 4M videos with resolution 720480 ranging from approximately 10 to 120 seconds in length, with paired texts captioned by CogVLM2 [54]. We used 8 NVIDIA A100 80GB GPUs over the course of 40 GPU days, for 30,000 iterations using rank-2048 LoRA [23] with learning rate of 105 and batch size of 8. Our method is data agnostic and model agnostic. It Figure 4. Showcasing the effect of noise degradation level γ on generated videos. few frames from the driving video are shown in the leftmost column. Our model outputs are in the next 3 columns. As degradation decreases (γ from 0.7 to 0.5), the video more strictly adheres to the input flow. This allows us to control video movement with user-specified level of precision. text-to-video (T2V) and image-to-video (I2V) variants. We regard CogVideoX as black box without changing its architecture. We use the same training objective as in normal finetuning, i.e., the mean squared loss between denoised samples and samples with noise added. In fact, we use the exact same training pipeline as the original CogVideoX repository, with exactly one difference: during training, we use warped noise instead of regular Gaussian noise. For each training video, we calculate its optical flow for each frame, and create warped noise tensor RF CHW , where F, C, H, are the number of frames, the number of channels, the height and width of encoded video samples respectively by applying our algorithm iteratively. We also introduce the concept of noise degradation, which lets us control the strength of our motion conditioning at inference time. After calculating the clean warped noise, we then degrade it by random degradation level γ [0, 1], by first sampling uncorrelated gaussian noise ζ (0, 1) and modifying the warped noise (1γ)Q+ζγ (1γ)2+γ2 . As degradation level γ 1, approaches an uncorrelated Gaussian, and as γ 0, approaches clean warped noise. At inference, the user can control how strictly the resulting video should adhere to the input flow. Please see Fig. 4 for qualitative depiction of the effect of γ. In practice, because the diffusion model works on latent embeddings, we calculate the optical flow and warped noise in image space and then downsample that noise into latent space, which in the case of CogVideoX means downscaling by factor of 8 8 spatially and 4 temporally. We use nearest-neighbor interpolation along the temporal axis and mean-pooling along the two spatial axes, which are then multiplied by 8 to preserve unit variance. 6 can be used to add motion control to arbitrary video diffusion models, while only processing the noise sampling during fine-tuning. For example, it also works with AnimateDiff [16] fine-tuned with the WebVid dataset [2], trained on 840GB A100 GPUs over period of 2 days with 12 frames and 256 320 resolution. See its qualitative results in Fig. 16 in the supplementary material. 4. Experiments 4.1. Gaussianity Evaluation metrics. To validate the preservation of spatial i.i.d. Gaussianity, we follow the evaluation protocol outlined by InfRes [11]. Specifically, we use Morans to measure the spatial correlation of warped noise and the Kolmogorov-Smirnov (K-S) test to assess normality. Baselines. Following HIWYN [8], we choose the per-frame fixed and independently-sampled noise as oracle baselines for perfect spatial Gaussianity but zero temporal correlation. We choose bilinear, bicubic, and nearest neighbor temporal interpolation as oracle baselines for sufficient temporal correlation but no spatial Gaussianity. We also compare with the recent noise warping algorithms including HIWYN [8] and InfRes [11]. In line with these papers, we also include baselines Preserve Your Own Correlation (PYoCo) [12] and Control-A-Video (CaV) [10], which have perfect Gaussianity but zero and insufficient temporal correlation, respectively. Results. According to Tab. 1 1st section, we observe: (1) For Morans I, value close to 0 indicates no spatial cross-correlation, which is desirable for i.i.d. noise. Our method achieves Morans index of 0.00014 and high p-value of 0.84, indicating strong evidence for no spatial autocorrelation. Similarly low Morans values and high pvalues are observed for PYoCo, CaV, HIWYN and InfRes, because they also aim to generate spatially gaussian outputs. (2) The K-S test compares the empirical distribution of the warped noise to standard normal distribution. small K-S statistic and high p-value indicate the two distributions are similar. Our method obtains K-S statistic of 0.060 and p-value of 0.44, suggesting the warped noise follows normal distribution. Comparable results are seen for the other Gaussianity-preserving methods. (3) In contrast, the bilinear, bicubic, and nearest neighbor warping methods fail to maintain Gaussianity, exhibiting Morans values an order of magnitude higher (0.24 to 0.30) with p-values of 0.0, and K-S statistics 3-6 times larger (0.17 to 0.37) with very low p-values (<0.05). These results provide strong evidence for the presence of spatial autocorrelation and deviation from normality in the warped noise from these interpolation-based methods. 4.2. Efficiency Noise generation efficiency is measured by wall time profiling on an NVIDIA A100 40GB GPU, generating noise at resolution of 10241024 pixels. We compare with the same baselines as above. According to Tab. 1 2nd section, our method runs faster than the concurrent InfRes and significantly outperforms the most recent published baseline HIWYN by 26, due to our algorithms linear time complexity. The efficiency is one order of magnitude faster than real time, validating our feasibility to apply noise warping on the fly during video diffusion model fine-tuning. 4.3. Video editing via image diffusion To further validate the effectiveness of our noise warping algorithm, we repurpose off-the-shelf image-to-image diffusion models to perform video-to-video editing tasks in frame-by-frame manner, without training. Noise is warped using our algorithm and the above baselines based on the RAFT optical flow [51] from input video and fed to two image pre-trained diffusion models: DeepFloyd IF [50] for super-resolution and DifFRelight [18] for portrait relighting. By measuring the quality and temporal consistency of the output video, we can effectively evaluate the spatial Gaussianity and temporal consistency of different noise warping algorithms. Evaluation metrics. We use LPIPS [72], SSIM [21], and PSNR [21] to measure the quality of the output frames w.r.t. ground truth frames. We use warping error [31] to measure temporal consistency (mean square error) between two adjacent generated frames after flow warping. 4.3.1 DeepFloyd IF video super-resolution We evaluate noise warping on DeepFloyd IF [50] superresolution using 43 videos from the DAVIS dataset [41]. The videos were downsampled to the 6464 and superresolved to 256256. Results. According to Tab. 1 3rd section, our algorithm outperforms all the baselines in terms of temporal consistency (warping error). Our supplementary video also shows that our algorithm is more stable for the foreground, background, and edges, in contrast to InfRes which is often unstable in the background and HIWNY which is much less stable around moving edges. Our algorithm is comparable to other methods in PSNR, SSIM, and LPIPS image quality metrics, apart from the bilinear, bicubic, and nearest methods which result in low quality generation due to spatial non-Gaussianity. See Fig. 12 in the supplementary material for more details. 7 Table 1. Noise warping algorithm benchmarking in terms of Gaussianity, efficiency, and spatial quality and temporal consistency for two image diffusion based applications. / indicates higher/lower value is better. Noise w/o warping Noise warping method Fixed Random Bilinear Bicubic Nearest PYoCo CaV HIWYN InfRes Ours Morans (index) Morans (p-value) K-S Test (index) K-S Test (p-value) -0.00027 0.29 0.089 0.12 0.00019 0.36 0.075 0.19 0.30 0.0 0.34 0.0005 0.24 0.0 0.37 0.0004 0.26 0.0 0.17 0.04 0.00023 0.73 0.13 0. -0.00079 0.25 0.073 0.27 0.0011 0.11 0.062 0.42 0.00036 0.60 0.055 0.50 0.00014 0.84 0.060 0.44 Gaussianity GPU time (ms) < 1 < 1 4.41 Efficiency at 10241024 resolution 2.31 4.33 6.82 3. 55.2 2.61 2.14 LPIPS SSIM PSNR Warping error LPIPS SSIM PSNR Warping error Super-resolution - DeepFloyd IF 0.29 0.88 29.36 163.84 0.33 0.69 28.91 86.65 0.29 0.88 29.41 233.65 0.31 0.77 29.02 128.11 0.60 0.72 28.68 165.90 0.62 0.70 28.55 167. 0.55 0.65 28.59 244.72 0.28 0.88 29.40 186.63 Relighting - DifFRelight 0.40 0.73 28.87 47.53 0.41 0.70 28.82 43.57 0.73 0.38 28.21 164. 0.35 0.58 28.83 95.24 0.28 0.88 29.39 220.28 0.35 0.67 28.87 106.77 0.29 0.87 29.31 164.35 0.36 0.64 28.82 87.72 0.28 0.88 29.38 190. 0.35 0.60 28.81 87.97 0.29 0.88 29.39 152.04 0.33 0.70 28.92 85.82 4.3.2 DifFRelight video relighting We evaluate noise warping on DifFRelight [18] portrait video relighting using their own dataset, which includes 4 subjects in 4 scenarios: 180-degree view animation, 720-degree view animation, zigzag camera movement sequence, and an interpolating camera path through several fixed stage capture positions, all with fixed lighting conditions. During inference, we center crop 10241024 region out of 10801920 Gaussian splat rendering and infer with various noises using conditioned lighting. Results. According to Tab. 1 4th section, throughout all baseline comparisons, our algorithm shows consistently advantageous scores in both image and temporal metrics, validating its fundamental benefits to the image diffusion model. Although our visual results at first glance are comparable to HIWYN and InfRes in the supplementary Fig. 13 and our webpage, its visual improvements can be seen in the beard regions and skin reflections. We also notice quite low warping error values on the bilinear and bicubic noise inferences, likely coming from the long blurry streaks generated along the flow, while at the same time image quality deteriorates significantly. 4.4. Video diffusion with motion control 4.4.1 Local object motion control We introduce novel method for controlling object motion, by leveraging the flows of input templates. These templates include user-defined local region masks and cut-and-drag trajectories that allow users to specify the motion of one or more objects built with simple, intuitive UI  (Fig. 5)  , and synthetic flows of camera rotating around 3D objects Figure 5. Qualitative comparisons of local object motion control. Zoom in for details. The user selects any number of polygons, then scales, rotates, or translates them along arbitrary paths, which are then used to create the warped noise flow.  (Fig. 6)  . During inference, we use the precise flow computed from the input template frames to guide noise warping for video generation. This enables our I2V model to apply accurate, localized movements and adjustments to the input image while preserving object structure and faithfully following the intended motion trajectory. Baselines. We evaluate our video generation model against three state-of-the-art baselines, SG-I2V [37], MotionClone [33], and DragAnything [58], to benchmark its ability to accurately control object and camera movements derived from given input template. One of the most recent alistic splashes when moving duck within tub. Extensive user studies and quantitative evaluations validate our superior performance in motion consistency, visual fidelity, and overall realism. User study. We conducted comprehensive user study with 40 participants, asking them to evaluate and rate different methods based on their effectiveness in object motion control and maintaining 3D and temporal consistency. Our method stands out significantly, achieving win percentage of 82% for cut-and-draglocal object motion control like Fig. 5 and 90% for the turnable camera movement control like Fig. 6. The three baselines have substantially lower performance levels. More user study details are included in the supplementary material Sec. 10 and Fig. 15. 4.4.2 Motion transfer and camera movement control Our method also supports motion transfer and camera movement control, working with both T2V and I2V video diffusion models. By using reference videos and applying noise warping based on their optical flows, it can effectively capture and transfer complex motions. Datasets. We choose the DAVIS video dataset [41] containing 43 videos of general object motion with ground truth object segmentation annotations, random subset of 100 videos from the DL3DV dataset [32], and 19 videos generated with WonderJourney [65] that predominantly feature camera movements  (Fig. 8)  , which itself uses depthwarping. Evaluation metrics. For pixel quality, we calculate Frechet Inception Distance (FID) between set of real and generated frames. For motion controllability, we calculate (1) the mean Interaction over Union (mIoU) of CoTrackers tracking bounding boxes [26] between ground-truth and generated videos, and (2) the pixel MSE between ground-truth and generated videos, considering an I2V diffusion model is conditioned on ground-truth prompts, ground-truth initial frames, and ground-truth motion trajectories/flows. For text controllability, we calculate the cosine similarity between the prompts CLIP [44] text embedding and the generated frames CLIP image embeddings, and average over frames of generated video. For temporal consistency, we calculate (1) the cosine similarity of the CLIP image embeddings between two consecutive generated frames and average over all pairs in generated video, and (2) the Frechet Video Distance (FVD) [52] between set of real and generate videos. In addition, we also benchmark on four metrics of VBench [24], specifically for the temporal consistency/smoothness dimension. Baselines. For the motion transfer T2V scenario, we compare with the recent state-of-the-art methods Diffusion Motion Transfer (DMT) [63] and MotionClone [33]. For the motion transfer I2V, we compare only with MotionClone as Figure 6. Qualitative comparisons of camera movement video generation of our method (b) and MotionClone (c) using turning source video (a). works, SG-I2V, is an I2V model for object motion transfer guided by bounding box trajectories. We adapt our userdefined polygons to bounding boxes as its input. Qualitative results. From Fig. 5, Fig. 6, and our supplementary webpage, we observe: (1) Existing methods struggle to handle complex, localized object motions. Specifically, when specifying local adjustments, such as rotating dogs head while keeping the rest of the body static, these methods often fail, applying unnatural translational or global transformations to the entire object. (2) We found that SG-I2V frequently misinterprets object-specific movements as global camera shifts, resulting in scene-wide translations rather than accurate object manipulations. (3) DragAnything, which employs single-line trajectory control, lacks temporal and 3D consistency, leading to significant distortions and reduced fidelity in complex motion scenarios. (4) MotionClone also fails to capture subtle object dynamics, as it relies on sparse temporal attention for motion guidance and is likely limited by the low spatial resolution of its diffusion features. (5) Our model consistently outperforms these baselines by maintaining high object fidelity and 3D consistency, even in scenarios with intricate or overlapping motions. Notably, our approach preserves object integrity and introduces plausible physical interactions, such as generating re9 Figure 7. Qualitative comparisons of motion transfer T2V on the DAVIS dataset. Zoom-in needed. Figure 9. We explore 3D scene, flying into given image. Similar to Fig. 8, we take an image as an input and use monocular depth estimator DepthPro [5] to get depth map. Then, we use that depth to generate crudely warped video (note the pixelation on the rough depth warp column when zoomed in) - and from the movement in that video get warped noise. From there, we run our motion-conditioned I2V model. Figure 8. We apply our method to sequence of frames warped using monocular depth estimation, enabling consistent 3D scene generation from single image. In this example, we use results from WonderJourney. Zoom-in needed. DMT does not take an image as input. In addition, we demonstrate video first-frame editing, challenge where user starts with an original video and an edited version of its initial frame. The goal is to seamlessly propagate the edits made to the first frame throughout the entire video while preserving the original motion. We qualitatively compare with MotionClone [33] and the state-ofthe-art video editing method AnyV2V [29] on real videos with photoshopped first frames. We also source few images for image-based depth warping, where we take an image, use monocular depth estimator, DepthPro [5], to get depth map, and crudely warp it to simulate desired camera trajectory. Results. We present both qualitative and quantitative comparisons with baselines in Tab. 2, Fig. 7, Fig. 8, Fig. 9, Fig. 10, and our supplementary webpage. We observe: (1) Our superior object motion transfer: On the DAVIS dataset, which includes object motion along with some degree of camera movement, our method demonstrates improved motion fidelity and overall video quality, as meaFigure 10. Comparison of initial frame video editing results across different methods. All methods start with the same edited initial frame derived from the original video. sured by Vbench. In particular, in the I2V setting, where both the initial frame and the source video are provided, our method achieves significantly better scores in FID, FVD, and motion metrics, indicating much closer reconstructions of the ground truth videos. (2) Our superior camera movement control: On the DL3DV and WonderJourney datasets, which involve substantial camera movement, our method notably outperforms MotionClone in both motion fidelity and general video quality. This highlights our methods ability to effectively replicate intricate camera movements while maintaining visual coherence. For our depth-warping example Fig. 9, our results are far better than simply warping an image from its depth map, resulting in smooth, realistic camera trajectory. See the supplementary website for videos. (3) Our superior video first-frame editing: In Fig. 10, our method seamlessly integrates the added object into the 10 Table 2. Quantitative comparisons of motion transfer. / indicates higher/lower value is better. Bold indicates the best results. FID CoTracker Pixel mIoU MSE CLIPtext CLIPimage FVD Subject consistency Background consistency Motion smoothness Temporal flickering VBench DMT MotionClone Ours MotionClone Ours Ours (prompt only) MotionClone Ours Ours (lower LoRA rank) Ours (w/o noise degradation) MotionClone Ours - - - 99.41 78.62 76.62 82.67 48.41 110.42 55. 177.87 128.27 0.85 0.75 0.70 0.72 0.74 0.52 0.71 0.83 0.79 0.84 0.81 0.85 - - - 0.068 0.053 0.088 0.10 0.046 0.061 0.047 0.10 0.07 Motion transfer T2V on DAVIS 0.95 0.93 0.98 - - - 0.86 0.78 0.88 Motion transfer I2V on DAVIS 0.94 0.97 0.96 1839.13 1211.37 1357.46 0.75 0.88 0.85 0.31 0.32 0. 0.31 0.31 0.31 Camera movement transfer I2V on DL3DV 0.33 0.32 0.31 0.32 0.94 0.97 0.96 0.96 1111.92 340.06 1200.35 453.24 0.74 0.88 0.84 0. 0.92 0.89 0.93 0.85 0.92 0.91 0.85 0.92 0.91 0.91 Camera movement transfer I2V on WonderJourney 0.87 0.91 1928.12 1548.53 0.96 0. 0.75 0.82 0.32 0.31 0.94 0.86 0.97 0.92 0.98 0.96 0.91 0.97 0.96 0.96 0.93 0. 0.91 0.81 0.89 0.87 0.93 0.92 0.86 0.93 0.93 0.93 0.87 0.94 scene while accurately preserving the camera movement from the original video. In contrast, both baselines exhibit significant identity loss: MotionClone generates additional, unintended objects, and in AnyV2V, the foreground object gradually disappears. This demonstrates the superiority of our method in maintaining the original videos motion while faithfully preserving the identity of the object added to the first frame. 4.4.3 Ablation studies In Tab. 2 for the DAVIS I2V task, we compare our method with variant that excludes motion conditioning using warped noise (Ours (prompt only)), relying solely on textual prompts describing the objects. We observe: (1) Better video reconstruction: Our method, which incorporates motion conditioning, achieves superior FID, FVD, and CoTracker mIoU scores, indicating more accurate reconstruction of the source video. This is because textual prompts and the initial frame alone are insufficient to capture videos future dynamics, whereas incorporating real videoderived motion guidance enables the generation of more realistic sequences. (2) Improved video quality: By utilizing warped noise for motion conditioning, our approach not only maintains but also enhances overall video quality, as measured by Vbench, demonstrating that integrating realistic motion cues improves the plausibility of the generated videos without compromising quality. We perform additional ablation studies for the DL3DV I2V task of camera movement transfer application. Our final noise-warping-controlled model is trained using LoRA with rank 2048. For one ablation (Ours (lower LoRA rank)) in Tab. 2, we fine-tune on lower LoRA rank (256) for reduced memory consumption and improved efficiency, finding that performance deteriorates across all metrics. Especially, with FID and FVD significantly affected. This is because lighter-weight LoRA fine-tuning is unable to effectively adapt to the new warped noise distribution. Additionally, we find in Tab. 2 that, while fine-tuning and testing without noise degradation (Ours w/o (noise degradation)) improves the motion controllability of driving videos (increasing CoTracker mIoU), the other metrics tend to worsen. Sometimes objects fade in and out of existence at different points in the generated videos. This is because, with lower degradation value, the input signal of the warped noise is more strongly preserved. However, because the optical flow is not always accurate, this overreliance on warped noise can cause quality deterioration at test time if the given flow does not match the first frame. Noise degradation during fine-tuning serves as augmentation for improved robustness, while at inference, it allows the diffusion model to draw from its priors, decreasing the strength of our motion conditioning and allowing for looser flow control. See Fig. 4 for an example. 5. Conclusion In this work, we introduce novel and faster-than-real-time noise warping algorithm that seamlessly incorporates motion control into video diffusion noise sampling, bridging the gap between chaos and order in generative modeling. By leveraging this noise warping technique to preprocess video data for video diffusion fine-tuning, we provide unified paradigm for wide range of user-friendly, motioncontrollable video generation applications. Extensive experiments and user studies demonstrate the superiority of our method in terms of visual quality, motion controllability, and temporal consistency, making it robust and versatile solution for motion control in video diffusion models."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to express our gratitude to Stephan Trojansky and Jeffrey Shapiro for their initial and ongoing executive support; Sebastian Sylwan, Daniel Heckenberg, Jitendra Agarwal, Matheus Leao, and Sungmin Lee for their IT support; Xueming Yu and David George for their hardware support; Jennifer Lao and Lianette Alnaber for their operational support; and Winnie Lin, Ahmet Tasel, Yiqun Mei, Lukas Lepicovsky, Rahul Garg, Ashish Rastogi, Ritwik Kumar, Cornelia Carapcea, and Girish Balakrishnan for their insightful technical discussions."
        },
        {
            "title": "Social impact statement",
            "content": "Our work contributes to the growing field of video generative models by advancing motion-controllable video generation, which has the potential to revolutionize creative industries such as filmmaking and animation. By introducing computationally efficient and accessible framework, our method democratizes high-quality video generation, enabling creators, developers, and artists to produce dynamic content with minimal resources or specialized training. However, we acknowledge the potential misuse of such technology, including the creation of deepfakes or misleading media. To mitigate these risks, we advocate for responsible use, proper content labeling, and the integration of detection mechanisms to ensure ethical deployment. Our approach also emphasizes compatibility with diverse models, encouraging transparency and collaboration within the research community to address societal concerns effectively while maximizing the positive impact of this technology."
        },
        {
            "title": "References",
            "content": "[1] Luca Savant Aira, Antonio Montanaro, Emanuele Aiello, Diego Valsesia, and Enrico Magli. Motioncraft: Physicsbased zero-shot video generation. In NeurIPS, 2024. 2, 4 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 7, 17 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv, 2023. 2, 3 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 2, 3 [5] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second, 2024. 10 [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 3 [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. URL https://openai. com/research/videogeneration-models-as-world-simulators, 2024. 2, [8] Pascal Chang, Jingwei Tang, Markus Gross, and Vinicius Azevedo. How warped your noise: temporally-correlated noise prior for diffusion models. In ICLR, 2024. 2, 3, 4, 5, 7, 16, 17 [9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv, 2023. 3 [10] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv, 2023. 7 [11] Yitong Deng, Winnie Lin, Lingxiao Li, Dmitriy Smirnov, Ryan Burgert, Ning Yu, Vincent Dedun, and Mohammad Taghavi. Infinite-resolution integral noise warping for diffusion models. arXiv, 2024. 5, 7 [12] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In ICCV, 2023. 7 [13] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv, 2024. 2, 4 [14] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In ICLR, 2024. 2, [15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In ECCV, 2024. 2, 4 [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toIn ICLR, image diffusion models without specific tuning. 2024. 2, 3, 4, 7, 17 [17] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv, 2024. 2, 4 [18] Mingming He, Pascal Clausen, Ahmet Levent Tasel, Li Ma, Oliver Pilarski, Wenqi Xian, Laszlo Rikker, Xueming Yu, Ryan Burgert, Ning Yu, et al. Diffrelight: Diffusion-based facial performance relighting. In SIGGRAPH Asia, 2024. 3, 5, 7, 8 [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv, 2022. 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3 [21] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, 2010. 7 [22] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv, 2024. 4 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. 6 [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. [25] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via maskeddiffusion. In CVPR, 2024. 2, 4 [26] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv, 2023. 9 [27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. 3 [28] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 3 [29] Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: plug-and-play framework for any videoto-video editing tasks. arXiv, 2024. 2, 4, 10 [30] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. arXiv, 2024. 2, [31] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In ECCV, 2018. 7 13 [32] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR, 2024. 9 [33] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv, 2024. 2, 4, 8, 9, 10 [34] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv, 2024. 2 [35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. [36] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. ArXiv, abs/2405.13865, 2024. 2, 4 [37] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David Lindell. Sg-i2v: Self-guided trajectory control in image-to-video generation. arXiv, 2024. 2, 4, 8 [38] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv, 2021. 3 [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv, 2023. [41] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv, 2017. 7, 9 [42] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video generation. In CVPR, 2024. 3 [43] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv, 2024. 2, 4 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, 9 [45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 2020. 3 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3 [47] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In SIGGRAPH, 2024. 2, [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3, 6 [49] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 3 [50] StabilityAI. Deepfloyd if. URL https://github.com/deepfloyd/IF?tab=readme-ov-file, 2023. 3, 5, 7 [51] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. 2, 3, 5, 7 [52] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv, 2018. [53] Oliver Villar. Learning Blender. Addison-Wesley Professional, 2021. 3 [54] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. arXiv, 2024. 6 [55] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. In NeurIPS, 2024. 2, 4 [56] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, 2024. 2, 4 [57] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv, 2024. 2, [58] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In ECCV, 2024. 2, 4, 8 [59] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV, 2024. 3, 4 [60] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv, 2024. 2, 4 [61] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn SIGdirected camera movement and object motion. GRAPH, 2024. 2, 4 14 [62] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv, 2024. 2, 3, 4, 5, 6 [63] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In CVPR, 2024. 2, 4, [64] Wenjie Yin, Yi Yu, Hang Yin, Danica Kragic, and Marten Bjorkman. Scalable motion style transfer with constrained diffusion generation. In AAAI, 2024. 2, 4 [65] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In CVPR, 2024. 3, 9 [66] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model In beats diffusiontokenizer is key to visual generation. ICLR, 2024. 2, 3 [67] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv, 2024. 4 [68] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. image superResshift: resolution by residual shifting. NeurIPS, 2024. Efficient diffusion model for [69] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. arXiv, 2024. 4 [70] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv, 2024. 2 [71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 3 [72] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [73] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. arXiv, 2024. 4 15 Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Gaussianity preservation of our noise warping algorithm In this section, we discuss our noise warping algorithm, providing formal proof of its Gaussianity preservation properties. We also present an illustrative example that demonstrates how noise that undergoes expansion and subsequent contraction returns to its original state, showcasing how our noise warping algorithm maintains the underlying Gaussian distribution throughout the warping process. Proof. For each (x, y) , R(x, y) is collection of upsampled noise Xi, where E[Xi] = E[ Var(Xi) = Var( q(x, y) q(x, y) 1 Var( 1 d2 + ] + E[ 1 (Zi )] = 0 ) + Var( 1 (Zi 1 (cid:88) Zi j=i (d 1)2 + (d 1) d2 Zj = )) ) 1 , 1 d2 + 1 = = where we used the fact that q(x, y) and Zis are i.i.d. standard Gaussians. Since Xi is constructed as weighted sum of Gaussians, itself is also Gaussian. Moreover, for = j, we compute q(x, y) + 1 (Zj )) Cov(Xi, Xj) q(x, y) =Cov( E[(Zi )(Zj + (Zi 1 E[ZiS] ), d E[S2] )] (0 2 + ) ( 2 + 1 ) = 0. = = = 1 d2 + 1 d2 + 1 d2 + 1 1 1 Hence all Xis are independent. Otherwise, For each (x, y) , if degG((x, y)) = 0, then q(x, y) is sampled as an independent standard Gausthe output noise pixel q(x, y) is sian. built as weighted sum of R(x, y).pop() for each edge ((x, y), (x, y)) E, where R(x, y).pop() is an independegG((x,y)) . Hence dent Gaussian of mean 0 and variance q(x, y) is also Gaussian with mean 0. The variable after executing the inner for loop thus represents the variance of q(x, y), so the renormalization at the end brings q(x, y) back to standard Gaussian. Since the composing Xis are independent, the resulting noise should also have an independent Gaussian in each pixel. Example 1 (Exact recovery of expansion-contraction). Consider the following evolution of noise across three frames with forward flows fij going from frame to frame with + 1 = (and backward flow if 1 = j). Suppose at frame 1, pixel with density 1 has noise q. Suppose further that is pixel at frame 2 such that 1 a) = {v}, and 12(v is the only pixel at frame b) = and f21(v 2 such that 1 12(v b) = v. This represents the scenario where is expanded into two pixels a, v b. Then Algorithm 1 with forward flow f12 and backward flow f21 will result in having density 1/2 and noise ( ZaZb having density 1/2 and noise 2 2 + 1 ), where Za and Zb are i.i.d. standard Gaussians. Now, from frame 2 to frame 3, suppose there exists pixel such that 1 b}, i.e., they both 23(v) = {v b} = . a, contract to v, and that f32(D) {v and Then Algorithm 1 with forward flow f23 and backward flow f32 will result in having density 1 and noise q, hence deterministically recovering the noise and density of in frame 0. 2 + 1 ( ZbZa 2 ), and a, 2 7. Qualitative results of training-free image diffusion based video editing Noise warping methods that do not preserve Gaussianity degrade per-frame performance, as originally pointed out in [8]. For example, using nearest neighbor and bilinear interpolation destroys the Gaussianity (see Fig. 11) and consequently deteriorates the per-frame performance on pretrained image-to-image diffusion models (see Fig. 12 and Fig. 13). 8. The advantage of noise warping By using noise warping as condition for motion, we effectively discard all structural information from our input video that cannot be inferred from motion alone. This can be advantageous, as demonstrated in Fig. 14. MotionClone does not use optical flow to guide the video trajectory, instead relying on manipulating activations within the diffusion model. As result, the windmill gains an extra set of arms, whereas our method, which relies solely on motion information from optical flow via warped noise, does not introduce such artifacts. 16 Figure 11. direct visualization of the noise produced by our noise warping algorithm, HIWYN [8], bilinear, and nearest neighbor interpolations. The forward movement in this long roller-coaster video forces the noise to expand significantly. Early in the video, the HIWYN baseline produces visibly non-Gaussian results. See the full video on our supplementary website. Figure 12. Using different noise warping algorithms on DeepFloyd IF for video super-resolution on the DAVIS dataset. Figure 13. Using different noise warping algorithms on DifFRelight for portrait video relighting. 9. Comparison to the video diffusion base model without finetuning 10. User study settings and statistics Fig. 15 presents our user study questionnaires and statistics for two applications: (1) local object motion control, and (2) turnable camera movement video generation. Our questions focus on users overall subjective preference, controllability, and temporal consistency. Interestingly, video diffusion models respond to noise warping even without training. In Fig. 14 the rightmost column, even though the per-frame quality suffers, the flow of the output video still roughly follows the flow of the warped noise. However, because warped noise is statisically distinct from the pure Gaussian noise CogVideoX was trained on, without fine-tuning it can result in visual artifacts. 11. Model Agnostic Our method is dataand model-agnostic. It can be used to add motion control to arbitrary video diffusion models by only processing the noise sampling during fine-tuning. For example, it also works with AnimateDiff [16] fine-tuned on the WebVid dataset [2]. See its qualitative results in Fig. 16. Therefore, our method will generalize to future more advanced video diffusion base model. 12. Pseudo code See Fig. 17 for our noise warping pseudo code. See our source code and model checkpoints on GitHub. 18 Figure 14. We show cut-and-drag animation of windmill rotating clockwise, next to the derived optical flow, our outputs, baseline and an ablation. Note that the input video column appears to have two sets of panels because its being cut and dragged over itself to create rotational motion. When using noise warping is better: Per-frame structural information can poison the result of MotionClone, giving the windmill an extra set of arms - whereas ours only receives motion information from optical flow alone via warped noise (there are no double-windmills in the optical flow patterns). Ablation in rightmost column: warped noise with γ = .5 on the CogVideoX base model before we fine-tune it. Because warped noise is statisically distinct from the pure Gaussian noise CogVideoX was trained on, without fine-tuning it can result in visual artifacts. Note how although the per-frame quality suffers here, it still picks up on motion queues from the warped noise (the camera zooms into the windmill). (a) User study interface and questions for local object motion control, corresponding to Fig. 5 in the main paper. (b) User study interface and questions for turnable camera movement video generation, corresponding to Fig. 6 in the main paper. (c) User study statistics for local object motion control on the first question Which video is the best overall? (d) User study statistics for local object motion control on the second question Which video best aligns with the user intent for controlling the object movement based on the input? (e) User study statistics for local object motion control on the third question Which video best preserves the intended camera movement from the input? (f) User study statistics for local object motion control on the fourth question Which video maintains the most consistent and stable motion throughout? (g) User study statistics for motion transfer on the first question Which video has better overall quality? Figure 15. User study questionnaires screenshots and statistics. For all the questions of both applications, our method (the rightmost bar plot) significantly wins the most user preferences. 20 Figure 16. Fine-tuning AnimateDiff with our warped noise flow. We used Go-with-the-Flow to fine-tune AnimateDiff T2V, and display the results above. The input video is on the left, and from that video we derive warped noise which is used to initialize AnimateDiff on the columns to its right with different text prompts. 21 1 def warp_noise(prev_frame, cur_frame, prev_noise, prev_weight): 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 height, width, _ = prev_frame.shape flow = optical_flow(prev_frame, cur_frame) # Agnostic to the optical flow algorithm backwards_flow = -flow # cheap approximation of optical_flow(cur_frame, prev_frame) expansion_noise contraction_noise = zeros(height, width) = prev_noise.copy() expansion_mask contraction_mask = ones (height, width, type=bool) = zeros(height, width, type=bool) for in range(width): for in range(height): dx, dy = flow[x,y] if 0 <= x+dx <= width-1 and 0 <= y+dy <= height-1: # This particle stays in bounds expansion_mask contraction_mask[x [x+dx, y+dx] = False ] = True , # Contraction mask is True where for in range(width): for in range(height): if expansion_mask[x, y]: dx, dy = backwards_flow[x,y] expansion_noise [x, y] = prev_noise[x+dx, y+dy] # Weve decided which source pixels are involved in contraction and expansion now contraction_noise &= contraction_mask expansion_noise, contraction_noise, cur_weight = jointly_regaussianize_and_rebalance_weights( expansion_noise, contraction_noise, prev_weight ) # Regaussianize all noise values here, and divide the weights by the number of pixels in each bin contraction_weight = zeros(height, width) for in range(width): for in range(height): if contraction_mask[x, y]: # Contraction treats the noise pixels as particles, each moving from the source to the # destination with this flow dx, dy = flow[x,y] # Contraction is weighted sum of source pixels to destination pixel pixel_weight = cur_weight[x, y] # Sum all the source noise pixels that contract to the same destination contraction_noise [x+dx, y+dy] += prev_noise[x, y] * pixel_weight # When we multiply noise pixel by weight, the variance changes by that weight squared contraction_weight[x+dx, y+dy] += pixel_weight ** 2 contraction_noise /= sqrt(contraction_weight) # Adjust the variance of the summed contracted noise # Mixing contraction and expansion noises with their respective masks cur_noise = contraction_noise & contraction_mask + expansion_noise & expansion_mask return cur_noise, cur_weight Figure 17. Our noise warping pseudo code."
        }
    ],
    "affiliations": [
        "Eyeline Studios",
        "Netflix",
        "Stanford University",
        "Stony Brook University",
        "University of Maryland"
    ]
}