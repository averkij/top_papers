{
    "paper_title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
    "authors": [
        "Zhixuan Liang",
        "Yizhuo Li",
        "Tianshuo Yang",
        "Chengyue Wu",
        "Sitong Mao",
        "Liuao Pei",
        "Xiaokang Yang",
        "Jiangmiao Pang",
        "Yao Mu",
        "Ping Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 2 7 0 0 2 . 8 0 5 2 : r Preprint. Under Review DISCRETE DIFFUSION VLA: BRINGING DISCRETE DIFFUSION TO ACTION DECODING IN VISIONLANGUAGE-ACTION POLICIES Zhixuan Liang1,2, Yizhuo Li1, Tianshuo Yang1,2, Chengyue Wu1, Sitong Mao4, Liuao Pei1,2, Xiaokang Yang3, 1The University of Hong Kong 3Shanghai Jiao Tong University {zxliang, yzli, tsyang, cywu, pluo}@cs.hku.hk maositong1@huawei.com, pangjiangmiao@gmail.com, muyao@sjtu.edu.cn Jiangmiao Pang2, Yao Mu3,2,, Ping Luo1, 2Shanghai AI Laboratory 4Huawei Cloud Computing Technologies Co., Ltd."
        },
        {
            "title": "ABSTRACT",
            "content": "VisionLanguageAction (VLA) models adapt large visionlanguage backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder unified, scalable architecture. We present Discrete Diffusion VLA, single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusions progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained visionlanguage priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnvFractal and 49.3% overall on SimplerEnvBridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets."
        },
        {
            "title": "INTRODUCTION",
            "content": "VisionLanguageAction (VLA) models enable robots to interpret visual and linguistic inputs and execute corresponding action sequences. Modern VLA frameworks typically adapt large pretrained visionlanguage model (VLM) by adding an action-generation head that outputs motor commands (either continuous trajectories or discrete tokens). Current approaches fall into two paradigms: (1) an autoregressive (AR) approach, inspired by GPT-style transformers, that predicts action tokens sequentially (e.g., OpenVLA (Kim et al., 2024), π0-FAST (Pertsch et al., 2025)), and (2) continuous diffusion approach that treats an entire action trajectory as continuous signal to iteratively denoise (e.g., π0 (Black et al., 2024) and SmolVLA (Shukor et al., 2025)). Continuous diffusion can model sophisticated multimodal actions better than AR but remains decoupled from the VLM backbone. Even partial integration efforts (e.g., the Transfusion (Zhou et al., 2024) design in π0) still rely on diffusion-specific training and iterative sampling, lacking truly unified structure which is consistent with VLM part. Drawing on recent advances of discrete diffusion and discrete flow-matching for language and multimodal generation (Nie et al., 2025a; Shi et al., 2024b; Gat et al., 2024; Kim et al., 2025a; Yang et al., Corresponding authors: muyao@sjtu.edu.cn, pluo@cs.hku.hk; 1 Preprint. Under Review Figure 1: Paradigm comparison. Continuous diffusion over action chunks (left) versus discrete token decoders: AR (sequential), BERT-style (parallel), and our discrete diffusion with re-masking. 2025), we introduce Discrete Diffusion VLA, the first VLA framework to unify vision, language, and action in single transformer, maintaining strong action-modeling capability. In Discrete Diffusion VLA, each action dimension is discretized into tokens via binning scheme (as in RT-1 (Brohan et al., 2022) and OpenVLA (Kim et al., 2024)) and grouped into fixed-length chunks. The fixedlength action tokens are exactly suitable for discrete diffusion models. During training, we mask subset of these action tokens and train the transformer to predict them from the context of the unmasked tokens across all modalities. The noise scheduling is governed by intuitive mask ratios. At inference, all action tokens start masked and are iteratively decoded by repeatedly predicting tokens and re-masking low-confidence ones until convergence, following the first-easy, then-hard philosophy. Moreover, we employ second re-masking technique to guarantee the consistency across different denoising steps. This yields flexible parallel decoding and robust error correction. For robotics manipulation tasks, in contrast to continuous diffusion decoders, our formulation keeps action generation inside unified transformer with the same training objective (i.e., cross-entropy loss) used by VLMs. This preserves much of the backbones pretrained vision and language capability, analogous to extending vocabulary to new languages, while offering potential path to inherit from unified transformers scaling behavior, paving the way for large-scale VLA research. Moreover, Discrete Diffusion VLA breaks the AR decoders left-to-right bottleneck. Action chunks are adaptively decoded in parallel over small, fixed number of steps, and unconfident tokens can be revisited via iterative re-masking, leveraging full cross-modal context (including inter-action dependencies) for refinement. We evaluate Discrete Diffusion VLA across three different robot settings: Franka Panda arm on LIBERO (Liu et al., 2023), Google Robot in SimplerEnvFractal (Li et al., 2025), and WidowX Robot in SimplerEnvBridge (Li et al., 2025). Using only RGB inputs, language and end-effector positions (no depth or affordances), Discrete Diffusion VLA achieves 96.3% average success rate on LIBERO ( +0.9% vs. OpenVLA-OFT (Discrete) ), 71.2% visual matching and 64.1% overall on SimplerEnvFractal, and 49.3% overall on SimplerEnvBridge ( +9.8% vs. π0andπ0+FAST). Our model outperforms both diffusion and AR baselines on most tasks while using fewer Number of Function Evaluations (NFEs) than autoregressive paradigm. Ablations confirm the benefits of our adaptive decoding strategy. In summary, our contributions are threefold: 1) We introduce the first discrete diffusion action head for VLA, unifying action generation with visionlanguage in one transformer while maintaining strong performance. 2) We develop first-easy, then-hard adaptive decoding strategy with iterative re-masking, enabling parallel action-token decoding and error correction, improving accuracy with the unified architecture. 3) We validate Discrete Diffusion VLA on Franka Panda in LIBERO, Google Robot and WidowX Arm in SimplerEnv, obtaining 96.3% avg. SR on LIBERO, 64.1% and 49.3% overall on SimplerEnvFractal and Bridge, consistently outperforming AR and continuous diffusion baselines (e.g., π0)."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 AUTOREGRESSIVE VISION-LANGUAGE-ACTION MODELS When the visionlanguageaction model was first proposed in RT-1 (Brohan et al., 2022) and RT2 (Zitkovich et al., 2023), it followed an autoregressive (AR) paradigm that discretizes robot actions into token sequences and generates them via next-token prediction. OpenVLA (Kim et al., 2024) 2 Preprint. Under Review advanced this direction by adopting 7B-parameter Llama-2 (Touvron et al., 2023b) backbone and fusing DINOv2 (Oquab et al., 2024a) and SigLIP (Zhai et al., 2023) visual features, demonstrating strong general-purpose manipulation. LAPA (Ye et al., 2024) reduces reliance on precise action labels by learning VQ-VAEbased latent action space (Van Den Oord et al., 2017) from Internet videos, pretraining in this space and finetuning with small action-labeled set, leading to improved language-conditioned control. π0-FAST (Pertsch et al., 2025) introduces the FAST tokenizer, combining the Discrete Cosine Transform (DCT) (Khayam, 2003) with Byte-Pair Encoding (BPE) (Shibata et al., 1999) to compactly encode 50Hz control signals for efficient high-frequency AR training. More recently, OpenVLA-OFT (Kim et al., 2025b) proposed parallel decoding and action chunking to rapidly adapt OpenVLA (Kim et al., 2024), while SpatialVLA augments VLA with depth cues to strengthen spatial reasoning. In contrast to ARs left-to-right factorization, our approach decodes action chunks in parallel and iteratively re-masks low-confidence tokens, leveraging full multi-modal and inter-action context for robust refinement across small, fixed number of inference steps."
        },
        {
            "title": "2.2 DIFFUSION-BASED VISION-LANGUAGE-ACTION MODELS",
            "content": "To capture the multi-modal structure of manipulation trajectories and enable fine-grained control, recent VLA systems (Black et al., 2024; Intelligence et al., 2025; Bu et al., 2025a;b; Wen et al., 2025; Li et al., 2024a; Liu et al., 2025; 2024) augment visionlanguage backbones with diffusion or flow-matching action generators (Janner et al., 2022; Chi et al., 2023; Liang et al., 2023; Ni et al., 2023; Liang et al., 2025; Chen et al., 2025a) while retaining autoregressive conditioning over visionlanguage tokens. These generators natively produce actions in continuous spaces. CogACT (Li et al., 2024a) introduces lightweight diffusion-based action transformer, while RDT-1B (Liu et al., 2024) scales to 1.2B parameters and pretraining over 46 multi-robot datasets, achieving state-of-theart bimanual dexterity. AgiBot World Colosseo (Bu et al., 2025a), UniVLA Bu et al. (2025b) and related works (Wen et al., 2025; Zhong et al., 2025; Liang et al., 2024) explore hierarchical latent actions and hybrid ARdiffusion schemes. π0 (Black et al., 2024) employs flow matching to learn dense continuous trajectories and is the state-of-the-art scheme, while π0.5 (Intelligence et al., 2025) adds two-stage regime with high-level thinking phase for semantic sub-task planning, enabling robust real-world generalization. Unlike these designs that attach diffusion/flow module alongside the VLM and rely on iterative denoising loops, our method performs discrete diffusion entirely within single transformer over tokenized action chunks, preserving the VLMs emergent language and multimodal capabilities and offering potential path for VLA to scale. 2.3 DISCRETE DIFFUSION MODELS Discrete diffusion has recently achieved strong results on discrete data, notably tokenized images and natural language. Foundational work D3PM (Austin et al., 2021) formalizes discrete diffusion as Markov chain. The forward process progressively corrupts an input x0 over timesteps, where the transition to noised state xt is defined by product of transition matrices Qt = Qt . . . Q1. Different from continuous diffusion models, it factorizes across positions, so each token xi (represented as one-hot vector) is independently transformed into categorical distribution that, (cid:89) q(xtx0) = Categorical(xt,i Qtx0,i). (1) i=1 The reverse process then learns to approximate the denoising conditionals and invert this corruption. Building on this, VQ-Diffusion (Gu et al., 2022) and MaskGIT (Chang et al., 2022a) achieve highfidelity image generation with transformers that iteratively predict masked/corrupted image tokens in non-autoregressive manner. In language, Diffusion-BERT (He et al., 2022) demonstrates the viability of this approach and subsequent Masked Diffusion Models unify parameterizations and show equivalences (Shi et al., 2024c; Zheng et al., 2024). More recently, LLaDA (Nie et al., 2025b) and DiffuLLaMA (Gong et al., 2024) scale discrete diffusion to 7 LMs competitive with AR baselines. With Fast-dLLM (Wu et al., 2025), the throughput can accelerate up to 10. MMaDA (Yang et al., 2025) demonstrates unified discrete diffusion model that jointly generates text and images. Our work extends this line to the action modality: we perform discrete diffusion within single transformer over tokenized action chunks, preserving language capabilities and VLM synergy. This 3 Preprint. Under Review yields competitive state-of-the-art VLA performance in our evaluations and lays groundwork toward unified, scalable, multi-modal foundation models spanning vision, language, and action."
        },
        {
            "title": "3.1 OVERVIEW",
            "content": "Discrete Diffusion VLA is single-transformer visionlanguageaction (VLA) policy that predicts fixed-length future action chunk from multimodal context. Given image observations (single or multi-view) and language instruction, we discretize each continuous control dimension into tokens and assemble them into fixed-length action sequence. single transformer processes all modalities: vision tokens from frozen visual encoder, language tokens from pretrained LM, and the action tokens. We pose action generation as masked-token denoising inside the same transformer that encodes vision and language. During training, at each step, we sample mask ratio over action positions, replace the selected tokens with mask token [MASK], and optimize cross-entropy objective to recover the masked tokens, aligning action learning with discrete-diffusion style corruption and denoising. During inference, we initialize all action positions as [MASK] and run small number of parallel refinement iterations. An adaptive re-masking policy is employed to perform multi-step denoising and refinement, while secondary re-masking is used to further enforce consistency across iterations. Section 3.2 formalizes discrete diffusion over action tokens; Section 3.3 presents the unified transformer architecture; Section 3.4 outlines the overall algorithmic pipeline; and Section 3.5 details inference, including the adaptive decoding mechanism and secondary re-masking for consistency. 3.2 DISCRETE DIFFUSION OVER ACTION TOKENS Let single action chunk be length-L token sequence a0 = (a0,1, . . . , a0,L), where each a0,i {1, . . . , K} is obtained by binning continuous controls (position, orientation, gripper) following prior VLA discretizations (Brohan et al., 2022; Kim et al., 2024). We augment the action vocabulary with special mask token (i.e., [MASK]), yielding = K+1 symbols and one-hot basis {e1, . . . , eK, eM}. The forward (noising) process of discrete diffusion is Markov chain {at}T t=0 with per-step transition matrices Qt RV that independently map each token to with probability βt and keep it unchanged with probability 1βt. Formally, for any one-hot vector eat,i of token at,i, Qt eat,i = (1βt) eat,i + βt eM. (2) Composing transition matrices yields Qt = Qt Q1, and the corrupted distribution at time factorizes across positions: q(at a0) = Categorical(cid:0)at,i (cid:12) (cid:12) Qt ea0,i (cid:1), (3) (cid:89) where is the length of action chunk tokens. i=1 The reverse (denoising) process defines conditionals pθ(at1 at, c) under multimodal (i.e., vision and language) context c. By Bayes rule, for each position i, which, under the masking corruption in Eq. 3, reduces to pθ(at1,i at,i, c) q(at,i at1,i) pθ(at1,i c), pθ(at1,i at,i, c) = (cid:40)δ(cid:0)at1,i = at,i at,i = M, Categorical(cid:0)at1,i πθ(i c)(cid:1), at,i = M, (cid:1), (4) (5) where πθ(i c) is the models predictive distribution (in practice it conditions on the current masked sequence at and c). Thus, at each step, Discrete Diffusion VLA recovers only subset of masked positions and leaves the rest masked, moving from higher to lower mask ratios until reaching a0. In implementation, we follow masked-diffusion / infilling formulations and collapse the multi-step chain into single masked-token prediction objective. We draw mask ratio γt (0, 1] that emulates time t, replace the selected action positions by special token [MASK] to obtain at, and train 4 Preprint. Under Review Figure 2: Discrete Diffusion VLA architecture. single-transformer VLM backbone encodes multi-view RGB (SigLIP+DINOv2 ViTs) and tokenized instruction, and decodes discrete action chunks via diffusion-style iterative refinement. Adaptive Decoding (bottom left) keeps highconfidence tokens each round and anneals the keep ratio with cosine schedule for easy-first parallel refinement. Secondary Re-Masking (bottom right) uses threshold and residual-drop checks to re-mask uncertain tokens, enforcing cross-step consistency and robust error correction. transformer fθ to predict the original tokens with cross-entropy on masked indices: pθ() = softmax(cid:0)W fθ(at, c)(cid:1), (cid:0)a0,i at, c(cid:1), LCE(θ) = log pθ (cid:88) (6) iMγt where Mγt is the masked set and projects hidden states of action positions to the K-way action vocabulary. This objective preserves diffusions corruptiondenoising spirit while using simple maximum-likelihood surrogate. As shown in recent analyses of masked diffusion (Shi et al., 2024a; Kim et al., 2025a), such losses upper-bound the negative log-likelihood under appropriate schedules. At inference time, we instantiate the reverse process via parallel decoding with adaptive re-masking: (i) initialize all action positions as [MASK]; (ii) predict token distributions for all positions in parallel; (iii) keep high-confidence predictions and re-mask the uncertain ones; (iv) repeat for small number of iterations. Discrete Diffusion VLA accepts increased training-time complexity (i.e., solving exponentially many infilling tasks) to gain arbitrary-order decoding at test time, selecting the inference order adaptively (e.g., by confidence or confidence gap), different from BERT-style masked LM which uses fixed, small mask ratio in single pass and lacks principled generative reverse chain. 3.3 UNIFIED VLA ARCHITECTURE The architecture of our Discrete Diffusion VLA is shown in Fig. 2. Base architecture. We build on OpenVLA (Kim et al., 2024) as the representative backbone (Prismatic-7B VLM (Karamcheti et al., 2024) with SigLIP+DINOv2 (Zhai et al., 2023; Oquab et al., 2024b) visual encoders, projector, and Llama 2 language model (LM) backbone (Touvron et al., 2023a)). Unlike the original autoregressive action head, Discrete Diffusion VLA embeds discrete diffusion over actions inside the same transformer that processes vision and language, yielding single, unified transformer as the VLA. Tokenization and action chunking. Following Brohan et al. (2022); Kim et al. (2024), we discretize each control dimension into vocabulary of action tokens. As in OpenVLA, we adopt 256-bin quantile-based scheme (1st99th percentiles) per dimension to avoid outliers, and treat the gripper as binary token. single-timestep action therefore consists of Dact=7 tokens: 3 for translation, 3 for rotation, and 1 for gripper. For action chunking, we arrange tokens from future 5 Preprint. Under Review timesteps into fixed layout, yielding total of L=H Dact action positions. We additionally append special [MASK] token to the action vocabulary for discrete diffusion. Visual inputs comprise mandatory third-person camera and optional left/right wrist cameras  (Fig. 2)  . Each view is encoded by SigLIP and DINOv2 ViTs, whose features are projected into the LM embedding space; the language instruction is tokenized directly by the LM. Optional proprioceptive state (e.g., end effector positions and joint angles) is embedded via small MLP and concatenated. The final input sequence is [ vision tokens ; language tokens ; action tokens ], where, during training, randomly selected subset of the action positions is replaced by [MASK]. Unified transformer and heads. All tokens pass through single transformer, and we only predict logits at the action positions. For action tokens, we use bidirectional attention mask which means no causal constraint and allow each action position to attend to all visionlanguage tokens. This design enables naturally full cross-modal fusion without bespoke adapters. Hidden states at action positions are projected to 256-way logits via shared classification head. Vision and language follow standard VLM masking and heads if used. Compared with prior two action head designs, this unified backbone retains the language models representation power, scales seamlessly with model size, and allows parallel decoding. At inference time, our adaptive re-masking (detailed in Sec. 3.5) further refines uncertain tokens, combining the global-context strength of transformers with the iterative denoising spirit of diffusion. 3.4 ALGORITHMIC PIPELINE Training pipeline. During training, for each minibatch, firstly we sample mask ratio γ (0, 1] from schedule (e.g., linear or cosine) that emulates diffusion time. Then we replace γL action positions with special token [MASK], and minimize the masked cross-entropy on those action indices following Eq. 6. We adopt hard-label supervision, representing ground truth as one-hot vectors at the masked indices. Vision and language tokens are left unchanged (standard LM masking/packing may be applied). This preserves the pretrained VLM capability since the objective is compatible with LM training, while aligning action learning with discrete diffusion via mask schedules. Notably, because the model predicts fixed-length action chunk with L=H tokens, all sequences have uniform length and require no [EOS] padding. Joint optimization over the positions naturally trains the action chunk in single pass. Inference pipeline. At test time, we initialize all action positions as [MASK] and perform small number of parallel refinement rounds. At each round t, our model predicts token posteriors for every currently masked position. We then draw candidate token at each position by multinomial sampling from the predicted logits. Next, we set the mask ratio to γt according to preset schedule and use it to determine how many positions remain masked. We rank the masked positions by data-dependent scores (e.g., maximum confidence or confidence gap), commit the sampled tokens at the top (1 γt) fraction, and keep the remaining γt fraction masked for the next round. This schedule makes the decoding order adaptive to each instance rather than fixed. The reverse-step conditionals follow the masking Bayesian formula in Eq. 5. lightweight secondary re-masking mechanism further applies threshold and consistency checks to previously demasked tokens to prevent error propagation. Full criteria are detailed in Section 3.5. 3.5 ADAPTIVE DECODING MECHANISM AND SECONDARY RE-MASKING Adaptive Decoding Mechanism. As illustrated above, the inference pipeline starts from fully masked action chunk a1 = ML with mask ratio γ1=1, and then performs refinement steps with monotone schedule γt+1 < γt. Here, we use cosine schedule following Chang et al. (2022b) as, π 2 t), [0, 1) γt = cos( (7) At step t, the model yields per-position posteriors pθ(at1 at, c) instantiating the reverse conditionals in Eq. 5. We score each position using one of the adaptive metrics: st,i = max pθ(k at, c) gt,i = pθ(k(1) ) pθ(k(2) ) (Max Confidence), (Confidence Gap), (8) (9) 6 Preprint. Under Review Figure 3: Benchmarks and tasks. We evaluate Discrete Diffusion VLA across three robot settings: LIBERO with Franka Panda arm, SimplerEnvFractal with Google Robot, and SimplerEnvBridge with WidowX Arm. with k(1), k(2) the labels corresponding to the highest and second-highest probabilities. Let mt,i {st,i, gt,i}. We keep the top (1 γt+1)L positions Kt = arg top (1γt+1)L mt,i, (10) and update these positions tokens via tempered Gumbel sampling to encourage exploration: at+1,i Categorical (cid:18) softmax (cid:18) log pθ( at, c) + ε τt (cid:19)(cid:19) , Kt, (11) where ε has i.i.d. Gumbel(0, 1) components (equivalently, GumbelMax), and τt is temperature that decays with γt. Positions not in Kt are set to (mask), and the process iterates until γT =0 or convergence. This instance-wise ranking makes the decoding order adaptive rather than fixed. Secondary Re-Masking. Beyond meeting the target ratio γt+1, we run two lightweight checks on previously committed tokens to prevent early errors from persisting. Tokens that fail either checks are re-masked, step we refer to as secondary re-masking. (i) Threshold check. If the current confidence falls below monotonically-increasing step-dependent threshold ηabs , the token is re-masked: Rabs = (cid:8) Kt : st,i < ηabs (cid:9). (12) (ii) Residual-drop check. Let reference confidence score sref Tokens with large degradation are re-masked either by threshold or top-Q rule: denote the first step at which position was kept, and cache the ,i. We compute confidence residual t,i = sref := st st,i. Rdrop = (cid:8) Kt : t,i > ηdrop (cid:9) or Rdrop = arg top t,i. (13) Rdrop The secondary re-masked set is Rt = Rabs . For Rt we set at+1,i = before proceeding to step + 1. These operations maintain alignment with the Bayes reverse kernel (Eq. 5) while improving cross-iteration consistency. t"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 BENCHMARKS AND BASELINES Benchmarks. We evaluate our Discrete Diffusion VLA on three different robot settings as shown in Fig. 3: (i) Franka Panda arm in LIBERO (Liu et al., 2023), using the four suites LIBEROSpatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long (10 tasks per suite; 500 expert demos per suite). (ii) Google Robot in SimplerEnvFractal (Li et al., 2025), which reports Visual Matching and Variant Aggregation scores across diverse scene variations. (iii) WidowX Robot in SimplerEnvBridge, real-to-sim evaluation aligned with BridgeData-V2 Walke et al. (2023) tasks. Policies receive only RGB images, that is one third-person view and one wrist view for LIBERO, 7 Preprint. Under Review Table 1: LIBERO task performance results (%). Each column is LIBERO task suite; values are averaged over 500 rollouts per suite (10 tasks 50 episodes). Methods above the horizontal rule (Diffusion Policy, Seer (scratch), MDT) are trained from scratch; those below are fine-tuned from pretrained bases. Best and second-best are bold and underlined, respectively. Model Diffusion Policy (Chi et al., 2023) MDT (Reuss et al., 2024) Seer (scratch) (Tian et al., 2025) OpenVLA (Kim et al., 2024) Octo-Base (Ghosh et al., 2024) Seer (fine-tuned) (Tian et al., 2025) Dita / DiT Policy (Hou et al., 2025) TraceVLA (Zheng et al., 2025) SpatialVLA (Qu et al., 2025) π0 + FAST (Pertsch et al., 2025) π0 (Black et al., 2024) OpenVLA-OFT (Cont-Diffusion) (Kim et al., 2025b) OpenVLA-OFT (Discrete) (Kim et al., 2025b) GR00T-N1 (Bjorck et al., 2025) Discrete Diffusion VLA Libero-Spatial Libero-Object Libero-Goal Libero-Long Average SR (%) SR (%) SR (%) SR (%) SR (%) 78.3 78.5 84.7 78.9 84.2 84.6 88.2 96.4 96.8 96.9 96.2 94.4 97.2 92.5 87.5 88.4 85.7 96.3 85.2 89.9 96.8 98.8 98.1 98.2 97.6 98.6 68.3 73.5 79.2 84.6 85.4 75.1 78.6 88.6 95.8 95.5 95.6 93.0 97.4 50.5 64.8 78.7 53.7 51.1 87.7 63.8 54.1 55.5 60.2 85.2 91.1 92.0 90.6 92.0 72.4 76.1 76.5 75.1 82.4 74.8 78.1 85.5 94.2 95.4 95.5 93.9 96.3 and single third-person view for SimplerEnv, together with language instruction and optional end-effector positions; no depth, affordances, or other auxiliary information are used. Baselines. We compare against representative policies spanning the two dominant paradigms, autoregressive (AR) token decoders and continuous diffusion / flow-matching policies, covering both models trained from scratch and models fine-tuned from large pretrained bases. AR action decoders. RT-1-X / RT-2-X (ONeill et al., 2024), OpenVLA (Kim et al., 2024), OctoSmall / Octo-Base (Ghosh et al., 2024), HPT (Wang et al., 2024), TraceVLA (Zheng et al., 2025), SpatialVLA (Qu et al., 2025), OpenVLA-OFT (Discrete) (Kim et al., 2025b) and π0+FAST (Pertsch et al., 2025) represent AR-style generation of discrete action tokens with unified VLM backbone. Continuous diffusion / flow-matching. Diffusion Policy (Chi et al., 2023), MDT (Reuss et al., 2024), DiT Policy (Dita) (Hou et al., 2025), RoboVLM (Li et al., 2024b), π0 (Black et al., 2024), OpenVLA-OFT (Cont.-Diffusion) (Kim et al., 2025b) and GR00T-N1 (Bjorck et al., 2025) instantiate denoising or flow-matching heads over continuous action trajectories. Seer is reported in both scratch and fine-tuned forms (Tian et al., 2025). All baselines are evaluated on the three benchmarks using official metrics: LIBERO success rates; SimplerEnvFractal visual matching and variant aggregation success rates; and SimplerEnvBridge partial / full success rates. Unless otherwise noted, numbers are taken from the original papers or reproduced from open-source implementations under the same input modality described above. 4.2 OVERALL PERFORMANCE COMPARISONS Training details. We fine-tune Discrete Diffusion VLA on each benchmark from the same VLM backbone as OpenVLA (Prismatic7B), following the respective official protocols. All input images are resized to 224px 224px. For LIBERO, we train separate policy per suite using the provided demonstrations, filtering unsuccessful episodes as in Kim et al. (2025b), and report success rates over the official test episodes. For SimplerEnvFractal and SimplerEnvBridge, we fine-tune on Fractal (Brohan et al., 2022) and BridgeData V2 (Walke et al., 2023), respectively. Across all settings, Discrete Diffusion VLA uses our unified transformer with discrete action tokens and fixed action chunk. Chunk sizes are chosen to match the widely used settings for fair comparison: 8 for LIBERO and SimplerEnvFractal but 5 for SimplerEnvBridge. At inference, the adaptive decoding runs small, fixed number of refinement rounds (12 by default) with cosine mask schedule, which has been shown effective for discrete diffusion decoding (Chang et al., 2022b). Full hyperparameters and implementation details are provided in Appendix A. LIBERO results. Table 1 reports success rates (SR) on the four LIBERO suites. Discrete Diffusion VLA attains the best average SR of 96.3%, with per-suite scores of 97.2% (Spatial), 98.6% (Object), 97.4% (Goal), and 92.0% (Long). Our most comparable baseline is OpenVLA-OFT (Dis8 Preprint. Under Review Table 2: SimplerEnv evaluation across different policies on Google Robot tasks. We report the results of all models pretrained with OXE dataset (ONeill et al., 2024) and then fine-tuned with Fractal dataset (Brohan et al., 2022). Model Visual Matching Variant Aggregation Pick Coke Mv Near Drawer Avg. Pick Coke Mv Near Drawer Avg. Overall Average RT-1-X (ONeill et al., 2024) RT-2-X (ONeill et al., 2024) Octo-Base (Ghosh et al., 2024) OpenVLA (Kim et al., 2024) HPT (Wang et al., 2024) Moto (Chen et al., 2025b) RoboVLM (Li et al., 2024b) TraceVLA (Zheng et al., 2025) π0 (Black et al., 2024) π0+FAST (Pertsch et al., 2025) OpenVLA-OFT (Kim et al., 2025b) GR00T-N1 (Bjorck et al., 2025) Discrete Diffusion VLA 56.7% 78.7% 17.0% 16.3% 56.0% 74.0% 77.3% 28.0% 72.7% 75.3% 72.3% 47.0% 85.4% 49.0% 31.7% 59.7% 53.4% 77.9% 25.0% 60.7% 82.3% 0.6% 4.2% 22.7% 16.8% 54.5% 46.2% 35.6% 27.7% 60.0% 24.0% 46.0% 60.4% 43.1% 59.2% 75.6% 61.7% 43.5% 63.4% 60.0% 53.7% 57.0% 42.0% 75.2% 65.3% 38.3% 58.8% 77.6% 67.5% 42.9% 61.9% 65.3% 69.6% 47.2% 63.0% 70.0% 18.1% 45.0% 78.8% 67.5% 60.6% 71.2% 82.5% 32.3% 29.4% 39.6% 46.5% 79.2% 35.3% 64.3% 62.5% 3.1% 9.0% 47.7% 17.7% 39.8% 33.8% 1.1% 1.1% 60.0% 10.6% 51.3% 57.4% 56.4% 31.0% 45.0% 43.5% 63.7% 25.6% 54.8% 56.8% 68.2% 31.3% 59.0% 60.5% 59.0% 12.2% 45.5% 54.3% 62.5% 13.2% 51.5% 48.4% 64.6% 23.6% 56.9% 64.1% Table 3: SimplerEnv evaluation across different policies on WidowX Robot tasks. We report the results of all models pretrained with OXE dataset (ONeill et al., 2024) and then fine-tuned with BridgeData V2 (Walke et al., 2023). Method Put Spoon on Towel Grasp Spoon Success Grasp Carrot Success Grasp Green Block Stack Green Block on Yellow #Overall Success Average Put Carrot on Plate RT-1-X (ONeill et al., 2024) Octo-Base (Ghosh et al., 2024) Octo-Small (Ghosh et al., 2024) OpenVLA (Kim et al., 2024) RoboVLM (Li et al., 2024b) π0 (Black et al., 2024) π0-FAST (Pertsch et al., 2025) OpenVLA-OFT (Kim et al., 2025b) Discrete Diffusion VLA 16.7% 34.7% 77.8% 4.1% 54.2% 45.8% 62.5% 50.0% 70.8% 0.0% 12.5% 47.2% 0.0% 29.2% 29.1% 29.1% 12.5% 37.5% 20.8% 52.8% 27.8% 33.0% 25.0% 25.0% 58.5% 41.7% 58.3% 4.2% 8.3% 9.7% 0.0% 25.0% 0.0% 21.9% 4.2% 29.2% 8.3% 31.9% 40.3% 12.5% 45.8% 50.0% 54.0% 70.8% 79.2% 8.3% 0.0% 23.4% 0.0% 34.5% 4.2% 8.3% 0.0% 32.0% 12.5% 27.8% 16.6% 39.5% 10.8% 8.3% 31.3% 20.8% 49.3% crete), which uses the same action discretization as ours but decodes via parallel decoding rather than discrete diffusion. Discrete Diffusion VLA reaches 96.3% average SR vs 95.4% for OpenVLA-OFT (Discrete), +0.9 point gain. These gains at matched tokenization indicate that discrete diffusion decoding provides consistent advantage over parallel decoding. For reference, the pure AR baseline (OpenVLA) averages 76.5%, underscoring the benefit of moving beyond left-to-right decoding. Against methods trained from scratch, Discrete Diffusion VLA surpasses Diffusion Policy and MDT by +23.9 and +20.2 points on average, respectively. We observe consistent advantages across suites. Google Robot results. As shown in Tab. 2, Discrete Diffusion VLA achieves the best Visual Matching average of 71.2%, clearly surpassing widely used baselines including π0 (58.8%) and π0+FAST (61.9%), and markedly outperforming OpenVLA-OFT (Discrete) (63.0% VM avg.). On Variant Aggregation, Discrete Diffusion VLA attains 56.9%, competitive with the top RT-2-X (64.3%) and π0+FAST (59.0%). Aggregating both metrics, Discrete Diffusion VLA yields the highest overall average of 64.1%, reflecting strong robustness across appearance changes and task variants. WidowX Robot results. On the WidowX evaluation in Tab. 3, Discrete Diffusion VLA attains the best overall average of 49.3%, outperforming diffusion / flow-matching policies (π0: 39.5%, π0+FAST: 39.5%) and exceeding AR-style baselines (e.g., Octo-Small at 34.5%). Per-task breakdown shows consistent gains in both grasp and success metrics (e.g., Put Spoon on Towel: 70.8% grasp / 37.5% success), indicating that discrete diffusion decoding improves reliability in these visually diverse manipulation settings. Across all three settings, the best overall score supports our central claim that casting action generation as discrete diffusion inside single transformer trained with masked cross-entropy preserves VLM priors while enabling parallel, adaptive, revisitable decoding. This unified, non-autoregressive design consistently outperforms both AR and continuous diffusion / flow-matching baselines under identical action tokenization way. 9 Preprint. Under Review Table 4: Ablation study. Success rates on LIBERO-Goal when varying the decoding strategy and the choice temperature. Ablation Item Decoding Strategy Choice Temperature Settings Parallel Decoding Random Order Confidence Gap Max Confidence (Ours) Hard Sample (Temp=0) Fixed Temp (Temp=1) Decay Temp (Temp=1-t) Success Rates 95.6% 96.0% 96.6% 97.4% 96.2% 96.4% 97.4%"
        },
        {
            "title": "4.3 ABLATION STUDY",
            "content": "Decoding strategy. We compare one-shot parallel decoding, random order, confidence-gap selection, and our max-confidence selection. On LIBERO-Goal the success rates are 95.6%, 96.0%, 96.6%, and 97.4% respectively. The adaptive easy-first schedule guided by per-token confidence gives +1.8 points over one-shot parallel and +0.8 over confidence-gap, while random order lags behind. These results indicate that ranking tokens by instance-wise confidence and resolving the easy elements first improves refinement efficiency and final accuracy. Choice temperature. We study the temperature used to turn posteriors into discrete choices during refinement. Hard argmax at all steps reaches 96.2%, fixed temperature of 1.0 gives 96.4%, and simple linear decay from 1.0 to 0.0 across steps attains 97.4% on LIBERO-Goal. The decayed schedule encourages mild exploration early and sharper commitment later, and it works well with adaptive decoding to correct early mistakes and consolidate consistent actions. 4.4 ANALYSIS ON INFERENCE EFFICIENCY key advantage of our discrete diffusion approach lies in its inference efficiency, particularly regarding NFEs compared to AR models. For an action chunk of length = Dact, an AR decoder without mechanisms like Multi-Token Prediction (Gloeckle et al., 2024) must perform sequential forward passes, as each token prediction is conditioned on all previously generated ones. This results in an NFE count of exactly L. For instance, in our LIBERO experiments where the horizon = 8 and action dimension Dact = 7, the total chunk length is = 56. An AR model would thus require 56 sequential function evaluations, creating computational bottleneck where latency scales linearly with the prediction horizon. Its critical constraint for real-time robotics applications. In contrast, our Discrete Diffusion VLA denoises the entire action chunk over small, fixed number of refinement steps, . At each step t, the model performs single forward pass to predict posteriors for all currently masked tokens simultaneously, making the total NFE equal to . In our experiments, we use default of = 12 steps. This reduces the required NFEs from = 56 to constant = 12, 4.7x reduction in function evaluations for the LIBERO setup. Discrete diffusion decouples the inference cost from the sequence length. The adaptive decoding and secondary re-masking mechanisms are lightweight computations performed on the output logits and do not require additional forward passes, preserving the core efficiency of the parallel refinement process."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Discrete Diffusion VLA unifies vision, language, and action inside single transformer by marrying diffusions progressive-refinement paradigm with discrete-token action interface, enabling an adaptive easy-first, hard-later decoding order and secondary re-masking for reliable error correction. Our architecture preserves pretrained VLM priors, the left-to-right bottleneck common in autoregressive VLA, and delivers state-of-the-art performance across LIBERO and two SimplerEnv suites while using fewer function evaluations than continuous-diffusion and autoregressive baselines. By aligning action decoding with the VLM transformer, Discrete Diffusion VLA offers path to inherit unified-transformer scaling behavior, paving the way for large-scale VLA with larger models and broader datasets. Limitations: Our fixed-bin action tokenization remains coarse relative to continuous control, leading to sub-bin precision loss. Developing more expressive action encoding, or integrating mechanism that imposes the continuous-space loss with discrete diffusion VLA, is promising direction. 10 Preprint. Under Review"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We sincerely thank Wanqi Zhong from Harbin Institute of Technology for assistance with figure illustration and layout."
        },
        {
            "title": "REFERENCES",
            "content": "Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025a. Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. arXiv preprint arXiv:2505.06111, 2025b. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022a. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022b. Tianxing Chen, Yao Mu, Zhixuan Liang, Zanxin Chen, Shijia Peng, Qiangyu Chen, Mingkun Xu, Ruizhen Hu, Hongyuan Zhang, Xuelong Li, et al. G3flow: Generative 3d semantic flow for poseaware and generalizable object manipulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17351744, 2025a. Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, and Xihui Liu. Moto: Latent motion token as the bridging language for robot manipulation. In Proceedings of the IEEE/CVF international conference on computer vision, 2025b. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, pp. 02783649241273668, 2023. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. Advances in Neural Information Processing Systems, 37: 133345133385, 2024. Dibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, et al. Octo: An open-source generalist robot policy. In Robotics: Science and Systems, 2024. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi`ere, David Lopez-Paz, and Gabriel SynarXiv preprint naeve. Better & faster large language models via multi-token prediction. arXiv:2404.19737, 2024. 11 Preprint. Under Review Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1069610706, 2022. Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. DiffusionImproving generative masked language models with diffusion models. arXiv preprint bert: arXiv:2211.15029, 2022. Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, et al. Dita: Scaling diffusion transformer for generalist visionlanguage-action policy. arXiv preprint arXiv:2503.19757, 2025. Intelligence, Black, Brown, Darpinian, Dhabalia, Driess, Esmail, Equi, Finn, Fusai, et al. π0.5: vision-language-action model with open-world generalization, 2025. URL https://arxiv. org/abs/2504.16054, 2025. Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, pp. 99029915. PMLR, 2022. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. Syed Ali Khayam. The discrete cosine transform (dct): theory and application. 2003. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. In Forty-second International Conference on Machine Learning, 2025a. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, et al. Openvla: An open-source vision-language-action model. In 8th Annual Conference on Robot Learning, 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025b. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024a. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024b. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. In Conference on Robot Learning, pp. 37053728. PMLR, 2025. Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser: In International Conference on Machine Diffusion models as adaptive self-evolving planners. Learning, pp. 2072520745. PMLR, 2023. Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding, and Ping Luo. Skilldiffuser: Interpretable hierarchical planning via skill abstractions in diffusion-based task execution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1646716476, 2024. 12 Preprint. Under Review Zhixuan Liang, Yao Mu, Yixiao Wang, Tianxing Chen, Wenqi Shao, Wei Zhan, Masayoshi Tomizuka, Ping Luo, and Mingyu Ding. Dexhanddiff: Interaction-aware diffusion planning for adaptive dexterous manipulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17451755, 2025. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, et al. Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model. arXiv preprint arXiv:2503.10631, 2025. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, and Zhixuan Liang. Metadiffuser: Diffusion model as conditional planner for offline meta-rl. In International Conference on Machine Learning, pp. 2608726105. PMLR, 2023. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, JUN ZHOU, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, 2025a. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025b. URL https: //arxiv.org/abs/2502.09992. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pp. 131, 2024a. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024b. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visuallanguage-action model. arXiv preprint arXiv:2501.15830, 2025. Moritz Reuss, Omer Erdinc Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. In First Workshop on VisionLanguage Models for Navigation and Manipulation at ICRA 2024, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024a. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024b. Preprint. Under Review Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024c. Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: text compression scheme that accelerates pattern matching. 1999. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In The Thirteenth International Conference on Learning Representations, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pp. 17231736. PMLR, 2023. Lirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. Advances in neural information processing systems, 37:124420124450, 2024. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Se June Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. In CoRL 2024 Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. In The Thirteenth International Conference on Learning Representations, 2025. 14 Preprint. Under Review Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, and Yuanpei Chen. Dexgraspvla: vision-language-action framework towards general dexterous grasping. arXiv preprint arXiv:2502.20900, 2025. Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In The Thirteenth International Conference on Learning Representations, 2024. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 21652183. PMLR, 2023."
        }
    ],
    "affiliations": [
        "Huawei Cloud Computing Technologies Co., Ltd.",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The University of Hong Kong"
    ]
}