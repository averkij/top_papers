{
    "paper_title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
    "authors": [
        "Jian Hu",
        "Mingjie Liu",
        "Ximing Lu",
        "Fang Wu",
        "Zaid Harchaoui",
        "Shizhe Diao",
        "Yejin Choi",
        "Pavlo Molchanov",
        "Jun Yang",
        "Jan Kautz",
        "Yi Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks."
        },
        {
            "title": "Start",
            "content": "BRORL: SCALING REINFORCEMENT LEARNING VIA BROADENED EXPLORATION Jian Hu1 Mingjie Liu1 Ximing Lu1 Yejin Choi1 Pavlo Molchanov1 1NVIDIA 2Stanford University {jianhu, mingjiel, ximingl}@nvidia.com fangwu97@stanford.edu, zaid@uw.edu {sdiao, yejinc, pmolchanov, joyang, jkautz, yidong}@nvidia.com Fang Wu2 Zaid Harchaoui3 Jan Kautz1 Yi Dong1 Jun Yang1 3University of Washington Shizhe Diao1 5 2 0 O 1 ] . [ 1 0 8 1 1 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL (Liu et al., 2025a) has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate complementary paradigm for scaling RL: BroRLincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under onestep RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that sufficiently large rollout size corresponding to ample explorationguarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks. Notably, under the same training time, BroRL is both more dataand compute-efficient: large-N rollouts reduce the number of filtered samples during dynamic sampling at the algorithmic level and shift generation from memory-bound to compute-bound at the hardware level, nearly doubling throughput compared to ProRL in our hardware setup, highlighting BroRLs practicality for real-world deployment."
        },
        {
            "title": "INTRODUCTION",
            "content": "One of the central drivers behind the rapid advances in Large Language Models (LLMs) over the past few years has been the discovery and application of Scaling Laws. Kaplan et al. (2020) showed that model performance follows predictable power-law improvements with respect to parameters, data, and compute. Building on this, Hoffmann et al. (2022) demonstrated that training is computeoptimal when model size and training tokens are scaled proportionally. These insights powered breakthroughs from GPT-3 to Claude/GPT-4 era, where scaling laws guided compute-optimal training of larger and more capable models. More recently, Reinforcement Learning with Verifiable Rewards (RLVR) has brought new excitement to the field, unlocking complex reasoning in LLMs and fueling the rise of large reasoning models such as DeepSeek-R1 (Guo et al., 2025) and OpenAI-o3 (Jaech et al., 2024). Yet, how to effectively scale the RLVR paradigm remains an open question. Recent work ProRL (Liu et al., 1 2025a; Hu et al., 2025b) has demonstrated the potential of scaling RL by increasing the number of training steps. While this approach yields steady initial gains, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate complementary dimension of the RL scaling law: BroRLincreasing the number of rollouts per example to the order of hundreds or thousands to exhaustively Broaden exploration. Intuitively, our approach mirrors how humans tackle hard problems (e.g., four color theorem), making countless attempts over decades until breakthrough emerges. Theoretically, our approach is motivated by mass balance equation analysis. As shown in Figure 2, under the one-step RL assumption, the change in correct-token probability mass Qpos consists of two parts. (1) The sampled portion always contributes non-negative gain by promoting sampledcorrect tokens and demoting sampled-incorrect tokens, thus ensuring mass expansion. (2) The unsampled portion is conditional, potentially adding or removing mass depending on the batch distribution. Importantly, as the number of rollouts per prompt increases, the influence of the unsampled terms diminishes, driving the overall effect toward Qpos 0. Figure 1: Empirical results demonstrate that BroRL (N = 512) continues to improve math performance, whereas ProRL (N = 16) reaches plateau at the 3k-steps checkpoint and further degrades with prolonged training. To verify our theoretical analysis, we conduct simulations with TRPO-style linear surrogate objective. The results show that sufficiently large rollout size corresponding to ample explorationguarantees an increase in the probability mass of all correct tokens and eliminates knowledge shrinkage (Wu et al., 2025), i.e., worst-case probability reductions among correct tokens, which implies that with enough exploration RLVR can reliably acquire new knowledge without forgetting the old. Building on this foundation, we apply the BroRL recipe to scale RL training on real-world reasoning models. In particular, we continue training the ProRL model that plateaues after 3K steps and find that BroRL yields robust, continuous performance improvements, ultimately achieving new state-of-the-art results for the 1.5B model across diverse benchmarks. Figure 2: This illustration shows how single RLVR update step alters the total probability mass Qpos for correct tokens, where the dashed guide lines labeled Qpos (green) and Qneg (red) connect the pooled probability assigned to the correct and incorrect token sets across sampled and unsampled regions. The change is composed of two parts: the Sampled portion (left) always produces nonnegative gain by promoting sampled-correct tokens (concentration measured by A2) and demoting sampled-incorrect tokens (concentration measured by B2), thereby shifting probability from the Qneg pool to the Qpos pool. The unsampled part (right) is conditional: it can add or remove mass depending on the batch mood SR and whether unsampled incorrect probability is more concentrated than unsampled correct probability. As the number of samples per prompt grows, the unsampled concentration terms Upos,2 and Uneg,2 shrink, so the net effect tends toward Qpos 0; the amount of mass moved scales with the pool sizes Qpos and Qneg. 2 Notably, under the same training time, BroRL is both more dataand compute-efficient: large-N rollouts reduce the number of filtered samples during dynamic sampling at the algorithmic level and shift generation from memory-bound to compute-bound at the hardware level, nearly doubling throughput compared to ProRL in our hardware setup, underscoring BroRLs practicality for realworld deployment. BroRL highlights the central role of exploration in RL, revealing that the perceived limits of RLVR are sometimes artifacts of algorithmic design (e.g., insufficient rollouts) rather than the fundamental limits of RL itself, underscoring the necessity and promise of future algorithmic advances in RL."
        },
        {
            "title": "2 THEORETICAL ANALYSIS",
            "content": "We develop theoretical analysis based on mass balance argument, common in physics for mass and transfer analysis. Our analysis is performed in the logit domain, focusing on the partial mass of correct tokens (negative tokens, respectively). By common abuse of language, we shall regularly use probability to refer to logit 1. Notation. We consider vocabulary of size , with logits RV and probabilities = softmax(z). Let and denote the sets of correct and incorrect tokens in vocabulary , respectively. rollout tokens is sampled, where each sampled token receives binary reward Ri {Rc, Rw} depending on whether it is correct or incorrect, while unsampled tokens are assigned Ri = 0. In the standard setting, the rewards satisfy Rc 0 Rw. Let be the set of sampled correct tokens, the set of sampled incorrect tokens, and the set of unsampled tokens. Let the partial mass Ppos denote the total probability mass of the sampled correct tokens, and Pneg the total probability mass of the sampled incorrect tokens. Similarly, let Qpos be the total probability mass of all correct tokens, and Qneg the total probability mass of all incorrect tokens. Ppos = (cid:88) iA pi, Pneg = (cid:88) iB pi, Qpos = (cid:88) iP pi, Qneg = 1 Qpos. The corresponding second moments, which measure how each partial mass is concentrated, are given by: (cid:88) A2 = p2 , B2 = p2 , Upos,2 = (cid:88) (cid:88) p2 , Uneg,2 = (cid:88) p2 . iA iB iU iU Finally, define SR = (cid:80) contribution of sampled tokens, balancing the rewards from correct and incorrect tokens. kB Rw pk = Rc Ppos + Rw Pneg which represents the net kA Rc pk + (cid:80) Connection between pass@k and Qpos The quantity Qpos denotes the total probability mass assigned to correct tokens. For single task, let Qpos(x) [0, 1] denote the total probability mass assigned to correct tokens for input x. When drawing i.i.d. samples, the per-task success probability for input is pass@k(x) = 1 (cid:0)1 Qpos(x)(cid:1)k This expression is strictly increasing in Qpos(x); thus, RLVR updates that increase the positive probability mass directly improve pass@k, and at geometric rate. Taking the expectation over the task distribution yields . Ex[pass@k(x)] = 1 Ex (cid:2)(1 Qpos(x))k(cid:3) . Moreover, if Qpos(x) increases pointwise (i.e., on set of positive measure), then both pass@k(x) and its expectation increase strictly. pos(x) Qpos(x) for all x, with strict inequality One-step RLVR update. We perform our analysis under the simplifying assumption of single step of RLVR, which allows us to obtain convenient analytical formulas. We model single RLVR step as adjusting logits RV via gradient update with rewards {Rc, Rw} on sampled tokens. 1This language is unrelated to the confidence we may assign to logit and whether the model is statistically calibrated or not (Geng et al., 2023; Liu et al., 2025b). 3 The update induces first-order change in token probabilities p, which aggregates into total correct-mass change Qpos = (cid:88) iP pi, where is the set of correct tokens. Then we show that the one-step change decomposes into an sampled positive term (always nonnegative) and an unsampled coupling term (which can be negative but vanishes as the rollout size grows). This decomposition allows us to uncover distinct dynamics in each term. In particular, the scaling of each of these terms with respect to , leads us to identify the roll-out size as key quantity to strike trade-off for superior performance in experiments. Formally: Theorem 1 (Sign of Correct-Mass Change). Qpos = (cid:104) η (Rc SR)QnegA2 + (SR Rw)QposB2 + SR (cid:0)QposUneg,2 QnegUpos, (cid:1)(cid:105) , where A2, B2 0, and SR [Rw, Rc], which implies Rc SR 0 and SR Rw 0. Therefore, the first two terms nonnegative, While the last term represents the coupling of unsampled masses. Interpretation. Three terms account for the change in the probability mass of correct predictions, as illustrated in Figure 2. The first term, (Rc SR)QnegA2, arises from sampled-correct tokens. Each correct token has an advantage of (Rc SR), meaning it is explicitly rewarded. Normalization redistributes this reward by taking mass from the incorrect pool (the Qneg share), and the size of the effect grows when those correct tokens are highly concentrated (large A2). This term is always nonnegative: pushing up correct tokens can never reduce correct probability. This is key feature of the sampled-correct tokens component of the reinforcement dynamics. The second term, (SR Rw)QposB2, arises from sampled-incorrect tokens. These have an effective (negative) advantage of (Rw SR) 0, so their probabilities are pushed down. Normalization then routes the freed-up mass to the correct pool in proportion to its size (Qpos), and the effect is stronger when the incorrect samples were concentrated (large B2). Again, this is always nonnegative: pushing down incorrect tokens leaves more probability for correct ones. (cid:0)QposUneg,2 QnegUpos,2 (cid:1), comes from unsampled tokens, and unlike the first The third term, SR two, it can be positive or negative. Here the batch mood SR sets the direction: If SR > 0 (a reward-positive batch), unsampled logits are nudged downward. This helps if unsampled incorrect mass is more concentrated (large Uneg,2), but hurts if unsampled correct mass is more concentrated (large Upos,2). If SR < 0 (a reward-negative batch), the signs flip: unsampled logits are nudged upward. This helps if unsampled correct tokens are more concentrated, but hurts if unsampled incorrect mass dominates. Thus, the first two terms always contribute positively, while the third can either reinforce or oppose them depending on batch balance and how probability is distributed among unsampled tokens. We draw several implications: (i) As the per-prompt sampling size grows, the unsampled terms Upos,2, Uneg,2 shrink, ensuring Qpos 0. (ii) Even for small , positivity holds under balanced batches (SR 0) or when unsampled mass is sufficiently small. (iii) Increasing per-prompt sampling size directly improves pass@k by enlarging the positive margin of Qpos. Since pass@k(x) is monotone in Qpos(x), any step with Qpos > 0 improves success probability. Larger strengthens this effect by reducing the contribution of the third (unsampled) term, which can be negative under certain conditions. Full derivations and proofs are in Appendix C.1. Expected decay of unsampled mass. The coupling term in Theorem 1 depends on the unsampled second moments Upos,2, Uneg,2. These shrink as the rollout size grows: Lemma 2. Let token with probability be sampled independently in each of draws. The expected unsampled second-moment contribution of this token is E[U2(p)] = p2(1 p)N . Corollary 3. For collection of tokens with probabilities {pi}, the expected total unsampled second moment after draws is (1 pi)N . p2 (cid:88) By linearity, this ensures Upos,2 and Uneg,2 decrease monotonically with , driving Qpos toward nonnegativity as increases. full proof is provided in Appendix C.2."
        },
        {
            "title": "3.1 BACKGROUND: PROLONGED REINFORCEMENT LEARNING",
            "content": "We adopt the prolonged reinforcement learning (RL) framework from ProRLv2 (Hu et al., 2025b). This approach is centered around clipped Proximal Policy Optimization (PPO) algorithm, with the objective function: LPPO(θ) = Eτ (cid:16) min rθ(τ )A(τ ), clip(cid:0)rθ(τ ), 1 εlow, 1 + εhigh (cid:1)A(τ ) (cid:20) (cid:17)(cid:21) , where rθ(τ ) is the probability ratio and A(τ ) is the advantage. key feature is its REINFORCE++- style decoupled advantage normalization (Hu et al., 2025a). First, the advantage Aτ for trajectory τ with return Rτ is computed by subtracting the mean return of its corresponding group for each prompt. This value is then normalized across the entire global sample batch: Aτ = Rτ meangroup (cid:0)Rτ (cid:0)Aτ Aτ meanbatch (cid:1) (cid:0)Aτ stdbatch (cid:1), (cid:1) . Anorm τ = To further improve performance and exploration, the framework integrates several key techniques. core component is Dynamic Sampling (Yu et al., 2025), which filters out trivial trajectories that are either entirely correct or entirely incorrect to focus training on the most informative samples. For batch of trajectories τ , the filtered batch is: (cid:40) = τ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) I(Mi = Mcorrect) < , (cid:41) 0 < (cid:88) i=1 where is the number of rollout samples per prompt, Mi is the prediction and I() is the indicator function. Other methods include periodic resets of the reference policy, exploration enhancements via Clip-Higher (εhigh > εlow) (Yu et al., 2025), and truncated importance sampling (Yao et al., 2025) to correct off-policy mismatch between the inference engine and the training engine. 3.2 SCALING REINFORCEMENT LEARNING VIA NUMBER OF ROLLOUTS BroRL is predicated on the principled scaling of the rollout size per prompt , which directly operationalizes the theoretical insights established in Section 2. The decomposition in Theorem 1 reveals that the policy update, as measured by the change in correct probability mass (Qpos), is (cid:1), which subject to potentially negative unsampled coupling term, SR can introduce instability and counteract policy improvement. Our theoretical framework rigorously establishes that the detrimental influence of this term on Qpos is inversely related to the rollout size . Consequently, in contrast to conventional approaches, BroRL employs significantly large to substantially increase the rollout diversity for each prompt. This rollout size scaling robustifies the learning signal by minimizing the variance and potential negativity arising from unsampled portions of the action space. This ensures more consistent and stable policy optimization process, directly translating our theoretical guarantees into more effective training regime for complex reasoning tasks. (cid:0)QposUneg,2 QnegUpos, 5 Figure 3: Training dynamics of the simulator under varying rollout size . We track (i) the total probability mass assigned to correct actions, (ii) the fraction of correct actions whose probability increased relative to step 0, and (iii) the worst negative change in probability among correct actions. Larger produces more stable updates, faster accumulation of probability mass, and crucially it eliminates knowledge shrinkage by removing negative probability drops altogether."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We first conduct token-level simulations to verify our theoretical insight (Section 4.1), and then apply BroRL in real-world scenarios by continuing RL training on ProRL models that plateau after 3K steps (Section 4.2). 4.1 SIMULATION OF THE THEORETICAL ANALYSIS Simulation Setup. We build token-level simulator reflecting the per-token update analysis in Theorem 1, using TRPO-style linear surrogate objective (Schulman et al., 2015; Zhu et al., 2025). The vocabulary has size = 128,000, with subset [d] of 10,000 correct tokens assigned reward Ri = +1 and the remainder Ri = 1. Logits Rd are initialized as zi = 0, with optional seeding by setting zi = 3 for and fixing one anchor token z0 = 5. Probabilities are pi = softmax(z/τ )i with τ = 1. At each step t, we draw i.i.d. samples center rewards with the batch baseline = 1 j=1 Rj, yielding rj = Rj to reduce variance. (cid:80)N We optimize the RLVR surrogate Lsur = 1 N (cid:88) j=1 rj pj and update with AdamW (learning rate η = 103) for = 1000 steps. Rollout size {4, 8, 16, 512, 51200} are varied while all other hyperparameters are fixed. After each update, we record: (i) the total correct probability mass Qpos = (cid:80) iP pi, (ii) the percent of correct tokens whose probabilities increased relative to step 0, and (iii) the worst probability drop among correct tokens. Results. The simulation results align with our key insight: increasing rollout size dampens the influence of the unsampled coupling term in Theorem 1, yielding more reliably positive mass expansion and stable policy updates. As shown in Figure 3, larger rollout size accelerates the growth of positive mass Qpos and increase the proportion of correct tokens whose probabilities improve at each step, whereas small-N updates exhibit slower progress, higher variance, and occasional regressions. Importantly, the worst-case probability drops among correct tokensknown as knowledge shrinkage (Wu et al., 2025) and common with small disappear at large . In the extreme, when is very large, RLVR eliminates knowledge shrinkage entirely, ensuring that all correct tokens gain probability mass. This matches the theoretical prediction that unsampled second-moment terms shrink with width (Lemma 2), thereby suppressing potential harmful contributions from unsampled tokens. Taken together, these findings confirm that allocating compute to rollout size, rather than step depth, yields consistently positive updates and provides the principled basis for BroRL."
        },
        {
            "title": "4.2.1 EXPERIMENTAL SETUP",
            "content": "Base Model. We build upon the publicly available ProRLv2 checkpoint and five task families: math, code, science, IFEval (Zhou et al., 2023) and reasoning gym (Stojanovski et al., 2025). This model, having already undergone 3,000 RL training steps with context length of 8,192 tokens, provides strong starting point. To further enhance its capabilities, especially for tasks requiring long-context reasoning, we expanded its context window to 16,384 tokens for all subsequent training phases with 64 NVIDIA H100 GPUs and the veRL famework (Sheng et al., 2025). BroRL Implementation. We continue RL training on top of ProRLv2 checkpoint with the BroRL recipe. We increased the number of generated samples per prompt from the baseline of 16 to = 512. This large value of is central to our hypothesis that broader exploration of the solution space during each update step leads to more robust and generalizable reasoning abilities. For baseline comparison, we also extend RL training on top of ProRLv2 checkpoint using the original ProRL recipe under the same compute budget. Learning Rate Scaling. To maintain training stability while accommodating the significantly larger effective batch size resulting from the increased rollout size (N ), we adjusted the learning rate while keeping the number of PPO mini-batches per step unchanged. Specifically, the learning rate was scaled proportionally to the square root of the increase in the batch size (Krizhevsky, 2014). Let η0 be the base learning rate for reference batch size B0. Our new learning rate ηnew for new, larger batch size Bnew is determined by the formula: ηnew = η0 . This principled adjustment ensures that the magnitude of parameter updates remains well-controlled. (cid:113) Bnew B0 4.2.2 ANALYSIS OF PASS@1 SUCCESS RATE Figure 4: Pass@1 comparison of BroRL vs. ProRL, normalized by training compute. Rows show representative trajectories: (1) both improve but BroRL consistently outperforms ProRL; (2) ProRL degrades while BroRL continues to improve; (3) both methods fail to yield consistent gains. To better understand the practical impact of our methods, we compare BroRL and ProRL across benchmark tasks under equalized training compute. Figure 4 summarizes these results by tracking performance at intermediate checkpoints. We observe three characteristic types of training trajectories. In the first, both methods improve but BroRL consistently outperforms ProRL, aligning with 7 theoretical expectations and highlighting stronger learning dynamics. In the second, ProRL degrades over time while BroRL continues to improve, underscoring its robustness. In the third, both methods fail to achieve consistent gains, In the third scenario, both methods fail to achieve consistent gains, suggesting that = 512 might not be large enough for some of the harder problems. Most benchmarks fall into the first two patterns, while the third is less common. Collectively, these trajectories show that BroRL not only matches theoretical predictions but also demonstrates clear practical advantages in training efficiency and stability. Importantly, all results are measured on the test dataset, highlighting that BroRLs improvements reflect not only better learning dynamics during training but also stronger generalization to unseen instances. To complement the trajectory analysis, we perform statistical evaluation to test whether BroRL provides measurable improvement over ProRL. We collect results from all individual problem instances across benchmarks, yielding over 10,000 data points, and measure Pass@1 at the final checkpoint under equal training compute ( 140 hours). paired t-test reveals small but statistically significant advantage for BroRL ( = 0.0033, = 4.84, one-tailed = 6.5 107). One-tailed and t-tests reject the null hypothesis, confirming that BroRL outperforms ProRL with extremely strong statistical confidence. Although the mean difference is small, this is expected since we build on strong baseline already fine-tuned for 3000 steps and evaluate after only 100 additional steps, where gains scale roughly log-linearly with training time (Liu et al., 2025a). In this regime, even modest but statistically significant improvement is meaningful, confirming that BroRL yields more reliable progress and better generalization to unseen test instances. 4.2.3 PUSHING REASONING BOUNDARIES BEYOND STEPS SCALING common challenge in longterm RLVR training is performance saturation, where simply training longer steps yields diminishing returns. The initial ProRLv2 checkpoint trained 3000 RL steps had reached such plateau. This section details controlled experiment to demonstrate that BroRLs rollout-scaling approach is not only more effective but also more time-efficient at breaking through this performance ceiling. We compare the performance of two continued training strategies. The ProRL approach uses conventional small rollout size (N = 16), while our BroRL approach scales the size significantly (N = 512). Table 1 and Figure 1 summarize the trade-offs in terms of computational cost and performance outcome at different checkpoints. Table 1: Efficiency and Performance Comparison. BroRL shows steady improvement and achieves higher score in less total time, while the ProRL stagnates and degrades. The number of samples refers to the amount before dynamic sampling filtering. Method Baseline ProRL ProRL BroRL BroRL BroRL 16 16 16 512 512 512 Prompts / Step RL Steps Samples (k) Time (h) Math Score Code Score Reasoning Gym Score 512 512 512 128 128 128 3000 +225 +535 +107 +134 + - +4390 +10439 +11226 +14059 +20039 - 56.3 133.8 98.1 122.8 173. 61.69 62.08 62.02 62.62 62.85 63.03 52.00 52.26 52.74 53.31 53.48 54. 61.29 62.10 61.45 62.71 62.82 63.09 The result reveals two divergent outcomes. The ProRL method shows marginal initial gains across all tasks, peaking at 62.08 on Math and 62.10 on Reasoning Gym. However, continued training leads to performance stagnation and degradation. While the Code Score sees minor increase to 52.74, the Math Score drops to 62.02, and the Reasoning Gym Score falls significantly to 61.45. This pattern, observed after nearly 134 hours, clearly illustrates the diminishing and ultimately negative returns of simply scaling training steps for this saturated model. In stark contrast, the BroRL approach demonstrates robust and continuous improvement across all three benchmarks, ultimately achieving the highest scores: 63.03 in Math, 54.20 in Code, and 63.09 in Reasoning Gym. The efficiency of this rollout-size-scaling approach is particularly striking. After just 98.1 hours, BroRL had already decisively surpassed the final performance of the ProRL method across all metrics, doing so in approximately 35 fewer hours. This result confirms that scaling the rollout size is more effective and computationally efficient strategy for pushing the performance boundaries of saturated model. This superior performance stem not from performing more gradient updates, but from executing fewer, yet higher-quality updates, as we maintain the same number of PPO mini-batches per RL step. More evaluation details and results are in Appendix D. The following section investigates the core reasons for this enhanced efficiency at both the algorithmic and hardware levels."
        },
        {
            "title": "4.2.4 ROLLOUT SIZE SCALING’S IMPACT ON GPU COMPUTE EFFICIENCY",
            "content": "The primary performance bottleneck in training models for long Chain-of-Thought (CoT) reasoning via RLVR is the sample generation phase (Hu et al., 2024). Our BroRL framework address this challenge through two-pronged approach: one at the algorithmic level and another at the hardware level. To isolate these variables, all experiments were conducted on an identical hardware setup (GPU and node count). Table 2 quantifies these two factors. Table 2: Algorithmic and Hardware Efficiency Metrics. BroRL improves both the diversity of samples (Pass Rate) and the speed of generation (Throughput). Method (N) Dynamic Sampling Pass Rate Generation Throughput (samples/s) ProRL (16) BroRL (Ours, 512) 41% 62% 36.5 72.4 First, at the algorithmic level, larger rollout size leads to more diverse set of candidate samples. The Dynamic Sampling Pass Rate in Table 2 shows that with = 512, 62% of the generated samples are deemed useful for training, compared to only 41% for = 16. This minimizes wasted computation and ensures each training step is based on more effective data. Second, at the hardware level, our approach achieves significantly higher generation throughputnearly 100% faster (72.4 vs 36.5 samples/s). This improvement comes from addressing common bottleneck in GPU computing: being memory-bound (Recasens et al., 2025). With small batches (N = 16), the generation process is often memory-bound; the GPUs compute cores idle while waiting to fetch data from memory. By generating large number of samples (N = 512) at once, the operation becomes more compute-bound and also leads to higher prefix cache hit rate (Zheng et al., 2024). This allows the GPU to leverage its parallel processing cores to their full potential, increasing arithmetic intensity and leading to higher sustained computing utilization. Therefore, BroRL is not only more effective RL training recipe but also utilizes the underlying hardware more efficiently."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Reinforcement Learning for Reasoning Reasoning models represent specialized category of AI systems that engage in long chain-of-thought to generate answers. This approach is central to models like DeepSeek-R1, which use RLVR as core training methodology. The RLVR paradigm, which adapts RLHF techniques (Christiano et al., 2017; Ouyang et al., 2022), has popularized algorithms such as GRPO (Shao et al., 2024), RLOO (Ahmadian et al., 2024), REINFORCE++ (Hu et al., 2025a) and DAPO (Yu et al., 2025). However, RL training is notoriously sensitive to hyperparameters, making stable, long-term optimization significant challenge. While many open-source efforts exist, most focus on narrow domains or test-time compute scaling. Few have addressed the challenge of prolonged RL training or investigated the underlying training-time scaling laws, leaving critical gap in understanding how to robustly enhance model reasoning. Scaling Axes in Reinforcement Learning The scaling laws of the RL process itself are underexplored. Prior work has focused on the axis of the total number of training steps. For example, ProRL demonstrates that prolonged RL training can expand the reasoning boundaries of LLMs (Liu et al., 2025a). In contrast, we investigate complementary axis: rollout size , the number of rollouts (N ) sampled per prompt in each update step. Our work, BroRL, is the first to formalize rollout size as principled scaling dimension in RLVR. We provide formal analysis proving that increasing dampens negative unsampled coupling term in the policy update, ensuring more reliable learning signal. This mechanism directly addresses the training instabilities that can limit RLs effectiveness for reasoning."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This work establishes rollout size , not just longer steps, as critical and efficient axis for scaling reinforcement learning in large language models. We demonstrated that the performance plateaus encountered by steps-scaling methods like ProRL are not fundamental limits but artifacts of an unstable learning signal caused by insufficient exploration. Our theoretical analysis pinpointed the unsampled coupling term as the primary source of this instability and proved that increasing rollout size systematically mitigates it. Empirically, our BroRL framework validated this theory by transforming stagnated model into one capable of continuous learning, achieving state-of-theart 1.5B model in complex reasoning tasks. Critically, these gains were achieved with superior computational efficiency, doubling hardware throughput by shifting the bottleneck from memory to compute in some cases, underscoring BroRLs practicality for real-world deployment."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Pete Bauman, Kevin Swersky, and et al. Back to basics: Revisiting reinforce style optimization for learning from human feedback. arXiv preprint arXiv:2402.14740, 2024. URL https://arxiv.org/abs/2402.14740. Søren Asmussen and Peter Glynn. Stochastic simulation: algorithms and analysis, volume 57. Springer, 2007. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. Weihua Du, Yiming Yang, and Sean Welleck. Optimizing temperature for language models with multi-sample inference. arXiv preprint arXiv:2502.05234, 2025. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. arXiv preprint survey of confidence estimation and calibration in large language models. arXiv:2311.08298, 2023. Daya Guo, Dejian Yang, Haowei Zhang, and Junxiao Song. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.07570, 2025. URL https://arxiv.org/abs/2501.07570. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. arXiv preprint arXiv:2105.09938, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021b. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Anna Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. 10 Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025a. URL https://arxiv.org/abs/2501.03262. Jian Hu, Mingjie Liu, Shizhe Diao, Ximing Lu, Xin Dong, Pavlo Molchanov, Yejin Choi, Jan Kautz, and Yi Dong. Prorl v2: Prolonged training validates rl scaling laws. August 2025b. URL https://hijkzzz.notion.site/prorl-v2?pvs=74. First published on Notion. Aaron Jaech et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks, 2014. URL https://arxiv.org/abs/1404.5997. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. doi: 10.1126/science.abq1158. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025a. Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, and Hua Wei. Uncertainty quantification and confidence calibration in large language models: survey. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pp. 61076117, 2025b. Mathematical Association of America. American invitational mathematics examination (aime), 2024. Official competition overview. Mathematical Association of America. American mathematics competitions (amc), 2025. Program overview. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. Pol Recasens, Ferran Agullo, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Jordi Torres, and Josep Ll Berral. Mind the memory gap: Unveiling gpu bottlenecks in large-batch llm inference. arXiv preprint arXiv:2503.08311, 2025. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015. 11 Zheng Shao, Xiaonan Li, Boqi Chen, Yuhui Zhang, Yongqi Li, Xu Han, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. URL https://arxiv.org/abs/2402. 03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Kopf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards, 2025. URL https://arxiv.org/abs/2505.24760. Fang Wu, Weihao Xuan, Ximing Lu, zaid. harchaoui, and Yejin Choi. The invisible leash: Why rlvr may not escape its origin. ArXiv, abs/2507.14843, 2025. URL https://api. semanticscholar.org/CorpusID:280271476. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https: //fengyao.notion.site/off-policy-rl. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. URL https://arxiv.org/abs/2503.14476. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37: 6255762583, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning, 2025. URL https://arxiv.org/ abs/2506.01347."
        },
        {
            "title": "A LIMITATIONS",
            "content": "A primary limitation of our current study is the scope of our investigation into the hyperparameter , the rollout size. Our experiments focus on demonstrating the significant performance and efficiency gains achieved by moving from small-rollout-size regime = 16) to large-rollout-size one (N = 512). While these results strongly support our central thesis, they do not provide complete picture of the relationship between rollout size and model improvement. comprehensive analysis sweeping across wider range of intermediate values (e.g., 64, 256, 1024) would be necessary to fully characterize this relationship. Such an analysis could reveal the precise shape of the performance curve, identify potential points of diminishing returns, and establish more formal cost-benefit trade-off. Our simulation results (Figure 3) suggest that the gains are monotonic but concave, yet validating this trend on large-scale language models is computationally demanding task that we leave for future work. more granular understanding of this scaling behavior would provide invaluable practical guidance for researchers and practitioners aiming to select an optimal for their specific computational budget and performance targets."
        },
        {
            "title": "B BROADER IMPACT",
            "content": "The development of more capable and efficient methods for training AI models, such as BroRL, has the potential for significant positive and negative societal impacts. Potential Positive Impacts. Our work demonstrates path toward more computationally efficient scaling of reinforcement learning for LLMs. By improving sample efficiency and hardware utilization, BroRL could lower the barrier to entry for training highly capable reasoning models. This could democratize access to state-of-the-art AI, enabling academic institutions and smaller organizations to contribute to cutting-edge research. Furthermore, enhancing the mathematical, logical, and coding abilities of LLMs can accelerate scientific discovery, create more effective educational tools, and augment human expertise in complex technical domains. Potential Negative Impacts and Societal Risks. Like any advancement that increases the capabilities of AI systems, this work warrants thoughtful consideration of potential risks. Enhanced reasoning and coding capabilities are powerful tools that could be applied in sensitive domains. For instance, the application of highly autonomous systems in areas such as cybersecurity requires careful oversight to prevent unintended consequences. Additionally, the ability to generate highly plausible and complex content at scale has implications for the information ecosystem that merit ongoing study. As with any powerful automation technology, the long-term economic and labor market impacts also warrant careful consideration by the broader community. It is crucial that the advancement of AI capabilities, spurred by research like ours, is accompanied by parallel and robust effort in safety and ethics. We advocate for the responsible development and deployment of these models within strong ethical framework."
        },
        {
            "title": "C PROOF DETAILS",
            "content": "C.1 THEOREM 1 Notation. For clarity, we repeat key quantities: (i) A, B, : sampled correct, sampled incorrect, and unsampled token sets. (ii) Qpos, Qneg: global correct/incorrect probability masses. (iii) A2, B2, Upos,2, Uneg,2: second moments. (iv) . SR = Rc Ppos + Rw Pneg which represents the net contribution of sampled tokens, balancing the rewards from correct and incorrect tokens. Define the reward Rj for sampled correct, sampled incorrect and unsampled tokens as: Rj = Rc, Rw, 0, A, B, U. 13 Logit update and Jacobian expansion. We start from the TRPO-style (Schulman et al., 2015) linear surrogate LRLVR(θ) = ExD (cid:104) (cid:88) r(x, y) πθ(y x) (cid:105)"
        },
        {
            "title": "1\nN",
            "content": "(cid:88) Ripi, iABU where Ri {Rw, 0, Rc}. This linear surrogate furnishes convenient Monte-Carlo estimate - sample average approximation when using relative entropy - Kullback-Leibler regularizer. This estimator is unbiased, hence all derivation and integration operations carry through to be interchanged with the expectation sign (Asmussen & Glynn, 2007). Denote zj as the logit for the j-th token. Then we differentiating w.r.t. zj using pi zj gives = pi(δij pj) zj = pj(Rj SR), SR = Rc Ppos + Rw Pneg η First-order change in probabilities. By first-order expansion, pi = (cid:88) j= pi zj zj = pi (cid:16) zi pjzj (cid:17) . (cid:88) j=1 Summing over any index set S, (cid:88) iS pi = (cid:88) iS pizi (cid:16) (cid:88) pi (cid:17)(cid:16) (cid:88) pjzj (cid:17) . iS j= We will need (cid:88) j=1 pjzj = η (cid:104) (Rc SR)A2 + (Rw SR)B2 SR (cid:105) , and, restricted to correct tokens, pizi = η (cid:88) iP (cid:104) (Rc SR)A2 SR Upos,2 (cid:105) . Total change of correct mass. The total change of correct-token probability mass is Pcorrect (cid:88) iP pi = (cid:88) iP pizi Qpos (cid:88) j= pjzj. Substituting the identities above and simplifying with Qpos = Ppos + Ppos,out, Qneg = 1 Qpos, and U2 = Upos,2 + Uneg,2, we obtain the compact form Pcorrect = (cid:104) η (Rc SR) Qneg A2 + (SR Rw) Qpos B2 + SR (cid:0)Qpos Uneg,2 Qneg Upos,2 (cid:1)(cid:105) , (1) with SR = RcPpos + RwPneg. C.2 LEMMA 2 We seek to obtain the scaling of the the unsampled second-moment with respect to . For this, we work under the simple assumption of token drawn independently and identically distributed as Bernoulli random variables. This is popular assumption (see e.g. Du et al. (2025)), which allows us to obtain convenient analytical formula capturing the scaling we are interested in. This scaling is further corroborated by the extensive experimental results from Section 4. Let Bin(N, p) be the number of times token is drawn in independent Bernoulli trials, each with success probability p. By the binomial distribution, the probability of never drawing the token is Pr[X = 0] = (1 p)N . 14 Equivalently, by independence across draws, the probability that the token is not selected in any of the trials is also (1 p)N . Define the indicator variable = 1{X = 0}, which is 1 if the token is never sampled and 0 otherwise. The tokens unsampled second-moment contribution is then the random variable = p2I. Taking expectations, we obtain E[S] = p2 E[I] = p2 Pr[I = 1] = p2 Pr[X = 0] = p2(1 p)N ."
        },
        {
            "title": "D EMPIRICAL EVALUATION",
            "content": "To rigorously test whether rollout size scaling breaks the trainingdepth plateau observed at 3,000 RL steps in the baseline (Liu et al., 2025a), we compare ProRL (small rollout size = 16 and longer steps) against BroRL (large rollout size = 512) under an identical evaluation protocol across three task families: math competitions (AIME/AMC, MATH, Minerva, OlympiadBench (of America, 2024; 2025; Hendrycks et al., 2021b; Lewkowycz et al., 2022; He et al., 2024)), code generation (APPS, CodeContests/Codeforces, TACO (Hendrycks et al., 2021a; Li et al., 2022; 2023)), and multi-domain reasoning (Reasoning Gym (Stojanovski et al., 2025)). Importantly, the table columns capture training controls: is the number of samples per prompt, is the number of prompts per RL step, and Steps is the count of continued RL steps. For details on sample generation and GPU compute consumption, please refer to Table 1.For evaluation, we report pass@1 with 32k context length, averaged over 16 independent samples per instance to ensure stable estimates, using nucleus sampling (top p=0.95) with temperature of 0.6. The experimental results presented in the tables 3,4,5 unequivocally support the superiority of rollout size scaling. In the critical domain of mathematical reasoning, the ProRL approach confirms the performance plateau; after an initial small gain (from 61.69 baseline to 62.08), its average score slightly degrades to 62.02. In stark contrast, BroRL not only avoids this degradation but also consistently improves, reaching superior score of 62.85 after 134 steps. This advantage is even more pronounced in other domains. For code generation, BroRLs score jump (+1.48 points) far exceeds the marginal gains from ProRL (+0.74 points). Similarly, on the Reasoning Gym benchmark, BroRL achieves substantial improvement of over 1.5 points, while ProRL provides almost no meaningful gain. In conclusion, across all three demanding domains, widening the generation search space per RL step proves to be significantly more effective and efficient strategy than merely continuing training with narrow search. Crucially, as detailed in Table 1, BroRL achieves these superior results with comparable number of total generated samples while consuming fewer wall-clock GPU hours. The BroRL method successfully overcomes the performance limitations observed in the baseline, leading to stronger and more stable reasoning capabilities. This highlights that for complex problemsolving, the diversity of experience in each training step is more crucial than the sheer length of the training process. 15 Table 3: Math scores. Method Baseline ProRL ProRL BroRL BroRL BroRL 16 16 512 512 512 Steps AIME24 AIME25 AMC Math Minerva Olympiad Bench Math Avg. 512 512 512 128 128 3000 +225 +535 +107 +134 +191 49.58 54.58 54.38 56.10 57.71 57. 36.04 36.25 35.83 35.30 35.63 36.88 82.53 92.49 80.95 80. 81.76 80.12 81.02 91.93 92.15 92.18 92.06 92.14 49.03 48.25 48.55 48.92 49.72 49. 60.44 60.52 60.77 61.41 61.87 61.54 61.69 62.08 62.02 62.62 62.85 63. Table 4: Code generation scores. Method Baseline ProRL ProRL BroRL BroRL BroRL 16 16 16 512 512 512 Steps apps codecontests codeforces taco Code Avg. 512 512 128 128 128 3000 58.52 +225 +535 +107 +134 +191 58.83 59. 60.28 60.19 61.59 54.99 54.58 55.09 55.84 56.52 56.62 58.64 59.27 59. 59.80 60.04 60.86 35.87 36.36 37.06 37.31 37.15 37.74 52.00 52.26 52. 53.31 53.48 54.20 Table 5: Reasoning Gym scores. Method Baseline ProRL ProRL BroRL BroRL BroRL 16 16 16 512 512 512 Steps algebra algorithmic arc arithmetic code cognition games geometry graphs induction logic Avg. 512 512 512 128 128 128 3000 +225 +535 +107 +134 + 97.19 97.01 97.46 97.55 97.70 97.59 55.32 58.22 55.56 59.11 59.28 59. 4.98 5.33 4.79 5.10 5.31 6.27 85.74 85.74 85.70 85.97 85.95 86. 48.20 47.96 48.43 49.22 49.30 49.45 45.91 46.01 46.33 44.05 44.53 45. 25.68 25.55 25.71 25.99 25.88 25.77 91.62 91.59 92.56 92.16 92.88 93. 70.25 69.83 70.40 71.51 72.01 72.03 80.25 80.31 80.31 80.40 80.38 80. 82.25 61.29 85.26 85.29 85.41 85.29 85.56 62.10 61.45 62.71 62.82 63."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Stanford University",
        "University of Washington"
    ]
}