{
    "paper_title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning",
    "authors": [
        "Fucai Ke",
        "Joy Hsu",
        "Zhixi Cai",
        "Zixian Ma",
        "Xin Zheng",
        "Xindi Wu",
        "Sukai Huang",
        "Weiqing Wang",
        "Pari Delir Haghighi",
        "Gholamreza Haffari",
        "Ranjay Krishna",
        "Jiajun Wu",
        "Hamid Rezatofighi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 9 2 7 1 . 8 0 5 2 : r Explain Before You Answer: Survey on Compositional Visual Reasoning Fucai Ke1 Sukai Huang1 Weiqing Wang1 Joy Hsu2 Zhixi Cai1 Zixian Ma3 Xin Zheng4 Xindi Wu5 Pari Delir Haghighi1 Gholamreza Haffari1 Ranjay Krishna3,6 Jiajun Wu2 Hamid Rezatofighi 1Monash University 2Stanford University 4Griffith University 5Princeton University 3University of Washington 6Allen Institute for Artificial Intelligence"
        },
        {
            "title": "Abstract",
            "content": "Compositional visual reasoning has emerged as key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including worldmodel integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as foundational reference and inspire the next generation of compositional visual reasoning research."
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Background . . 2.1. Visual Reasoning . 2.2. Monolithic Visual Reasoning . . 2.3. Compositional Visual Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3. Why Compositional Visual Reasoning? 3.1. Cognitive Alignment with Human Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2. Semantic and Relational Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3. Generalization and Robustness . . 3.4. Transparency, Interpretability, and Modular Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5. Reducing Language Biases and Hallucinations . 3.6. Lower Data Requirements and Improved Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 5 5 5 6 6 6 6 7 7 7 7 . . . . . . . . . . . . . . . . . . 1 4. Key Stages of Compositional VR Paradigms . . . . . . . . . . . . . . 4.1. Stage I: Prompt-Enhanced Language-Centric Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 . Task Decomposition Followed by Visual Perception . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 . Perceptual Grounding Prior to Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.3 . Synthesis and Outlook . 4.2. Stage II: Tool-Enhanced Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 . Single-Turn Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 . Adaptive Tool Use via Training, Feedback, and Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.3 . Synthesis and Outlook . 4.3. Stage III: Tool-Enhanced Vision-Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 . Language-Mediated Grounding Action Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.2 . Embedding-Mediated Grounding Action Control . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.3 . Image as Feedback in Tool-Enhanced VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.4 . Synthesis and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 . Prompt-Enhanced CoT Reasoning VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.2 . RL-Enhanced CoT Reasoning VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.3 . Visually Grounded CoT Reasoning VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.4 . Synthesis and Outlook . 4.5. Stage V: Unified Agentic Vision-Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.1 . Automatic Discovery of Informative Regions and Goal-Driven Exploration . . . . . . . . . . . . . 4.5.2 . Imagination-Enhanced Unified Agentic VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.3 . Synthesis and Outlook . 4.4. Stage IV: Chain-of-Thought Reasoning VLMs . . . . . . . . . . . . . . . 5. Benchmark and Evaluation 5.1. Types of Benchmarks and Datasets . 5.2. Evaluation Metrics . . . 5.3. Synthesis and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6. Insights, Challenges and Directions . . . . . . . . . . . . . . . 6.1. Insights . 6.2. Key Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 . Limitations of LLM-Based Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2 . Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.3 . Bias Toward Deductive Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.4 . Data Limitations and Scalability Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.5 . Tool Integration and Architectural Bottlenecks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.6 . Benchmark Limitations and Evaluation Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7. Conclusion 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 . 10 . 10 . 10 . 10 . 11 . 11 . 11 . 11 . 11 . 12 . 12 . 13 . 13 . 13 . 13 . 14 . 14 . 14 . 15 . 15 . 15 . 16 16 . 16 . 18 . 18 19 . 19 . 19 . 20 . 20 . 20 . 21 . 21 . 21 21 Humans possess remarkable ability to interpret high-dimensional, uncompressed visual input and abstract its underlying structure, enabling the manipulation of underlying concepts as symbolic representations with notable efficiency [1, 2]. As result, humans can effortlessly locate target object in cluttered room, determine whether cup will fit inside drawer, or predict the direction in which stack of objects might fall. This cognitive capability, known as visual reasoning, is widely regarded as concentrated embodiment of human intelligence, forming the basis for concept formation, world understanding, and interaction with the environment [25]. In pursuit of human-level intelligence, growing body of research has sought to replicate visual reasoning capabilities in machines [1, 68]. Early approaches, especially driven by large language models (LLMs), have led to the emergence of monolithic black-box architectures that directly map visual and textual inputs to answers [913]. These systems have shown impressive results in general-purpose multimodal understanding [1416] and have been increasingly adopted across real-world applications. For example, in robotics, visual reasoning enables collision-free object manipulation by assessing 2 Figure 1. Illustration of visual reasoning tasks, highlighting the differences between monolithic and compositional approaches. Monolithic models map input to output directly, which often leads to hallucinations or incorrect outputs due to the lack of intermediate reasoning. In contrast, compositional methods explicitly break down the task into sequence of interpretable reasoning steps. Each step is grounded in visual evidence, allowing the model to progressively infer the answer with greater transparency, accuracy, and robustness. affordances and spatial relations; in medical imaging, it assists in detecting anatomical structures or abnormalities from complex scans; and in autonomous driving or UAVs, it helps interpret dynamic scenes, such as predicting pedestrian intent or navigating cluttered environments [9, 1722]. Despite the promising performance of monolithic approaches in general multimodal understanding, significant challenges remain, particularly when viewed from the perspective of the inherently compositional and multi-step nature of human visual reasoning [13, 2328]. Concretely, we identify the following key limitations of monolithic visual reasoning: Challenge 1: Heavy reliance on dataset biases. Instead of performing grounded inference, monolithic models often exploit spurious correlations and linguistic priors to produce plausible but incorrect answers [23, 24], undermining their generalization to challenging or novel scenarios. Challenge 2: Diminishing returns with increasing reasoning complexity. Scaling up data and compute does not yield proportional gains when tasks demand multi-hop reasoning, spatial understanding, or precise grounding [2933]. Challenge 3: Incompatibility with human-like compositional reasoning. Humans interpret visual scenes as structured compositions of objects, attributes, and relationships [5, 34, 35], and flexibly derive new inferences by recombining known visual elements and concepts [4, 3639]. Monolithic models, by contrast, treat the input holistically and lack mechanisms for explicit decomposition and relational reasoning. These limitations highlight the need for modular, interpretable, and compositionally aligned approaches to visual reasoning. Motivated by these cognitive insights and the limitations of monolithic models, researchers have begun to explore new paradigm: Compositional Visual Reasoning (CVR), which explicitly incorporates structured reasoning steps guided by visual perception. At the core of this paradigm, and the central focus of this survey, is the development of methods that construct explicit scene representations and perform reasoning through step-by-step inference. These approaches make intermediate thought processes visible by decomposing complex tasks into compositional reasoning steps guided by perception, rather than relying on monolithic, end-to-end mapping [4, 29], as illustrated in Figure 1. Given its rapid progress since 2023, compositional visual reasoning has emerged as central paradigm in the study of visual intelligence. However, its development trajectory, methodological foundations, and future directions remain insufficiently examined. Existing surveys in visual reasoning have primarily focused on conventional or monolithic approaches [40 42], and thus fail to capture the emerging trend of compositional visual reasoning, which has quickly become core research direction. Meanwhile, surveys focused on multimodal large language models and reasoning tend to emphasize generalpurpose reasoning [31, 43], neurosymbolic frameworks [44], abstract pattern recognition [45], or agent-based architectures [31, 46]. While these works are related to compositional visual reasoning or cover relevant componentssuch as planning, perception, or symbolic abstractionthey do not provide an in-depth or systematic analysis of compositional visual reasoning as distinct and rapidly evolving paradigm. As shown in Table 1, targeted and comprehensive review of this area is still lacking. This gap is particularly pressing given the growing momentum in compositional visual reasoning, which is increasingly 3 Table 1. Overview of survey papers on visual reasoning and multimodal AI from 2024. While informative, these surveys largely overlook the recent surge of compositional visual reasoning methodshighlighting the need for dedicated review as pursued in this work. Survey Ishmam et al. [47], Kim et al. [40] Year 2024 Ma et al. [48] 2024 Wang et al. [26] 2024 Topic Contribution Feature extraction and vision language pre-training for VQA Critically analyzes feature extraction and vision language pretraining techniques of VQA, introduces taxonomy, discusses dataset/method trends, and highlights emerging challenges and application opportunities. Robustness in VQA under distribution shift Focuses on robustness in VQA across inand out-of-distribution settings. Proposes typology for debiasing techniques and assesses model reliability and dataset quality. Reasoning capabilities in MLLMs Categorizes multimodal large language models and their reasoning types (deductive, abductive, analogical). Summarizes evaluation protocols and trends in reasoning-focused applications. Xie et al. [46] 2025 LLM-driven multimodal agent Systematically reviews LLM-driven agents, proposes taxonomies for perception and action, and introduces unified evaluation frameworks and application landscapes. Lin et al. [43] 2025 Reasoning capabilities in MLLMs Categorizes reasoning into language-centric and collaborative modes. Discusses technical challenges like semantic alignment and dynamic interaction. Wang et al. [31] 2025 Multimodal CoT reasoning Khan et al. [44] 2025 Neurosymbolic reasoning with scene graphs and commonsense knowledge Li et al. [49] 2025 Large multimodal reasoning models Kuang et al. [42] 2025 Song et al. [50] Natural language understanding and inference within VQA Modality alignment methods in MLLMs Qu et al. [51] 2025 Tool learning with LLM Malkinski et al. [45] 2025 Abstract visual reasoning First survey dedicated to Multimodal CoT reasoning. Provides taxonomy, benchmarks, challenges, and resources to support research in multimodal AGI. First to survey neurosymbolic visual reasoning combining deep learning, scene graphs, and common sense knowledge. Categorizes methods by architecture and knowledge type, and outlines key challenges. Provides comprehensive survey of large multimodal reasoning models. Proposes three-stage road-map (modular, multimodal CoT, System-2), introduces native large multimodal reasoning models, and reorganizes benchmarks and datasets for multimodal reasoning. Reviews VQA models focusing on perceptual and cognitive reasoning. Highlights recent progress in few-shot VQA and knowledge-guided inference in MLLMs. Categorizes four alignment methods in MLLMs (converter, perceiver, tool, data-driven). Traces evolution and highlights key challenges in multimodal semantic bridging. Surveys tool learning from motivation to implementation. Defines four tool-use stages and discusses benchmarks, evaluation, and design challenges. Reviews deep learning methods for solving Ravens Progressive Matrices. Provides benchmarks, comparisons, and insights. supported by LLM-guided inference mechanisms, tool-integrated workflows, and vision-language models capable of multistep reasoning with explicit intermediate steps. These techniques are becoming foundational for tasks such as visual question answering (VQA) and visual grounding, where reasoning is no longer treated as single-step black-box prediction. To address this need, our systematic survey provides comprehensive review of compositional visual reasoning methods that emphasize explicit intermediate thought steps prior to answer generation. We specifically focus on large-modelenhanced methods applied to 2D image-based modalities, excluding video-based or 3D reasoning frameworks. The survey spans research published between January 2023 and May 2025, covering papers from top AI venues such as NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, CVPR, ICCV, and ECCV, along with highly cited arXiv preprints. In this survey, we examine the recent evolution of compositional visual reasoning, focusing on four central research questions that reflect the key developments and outstanding challenges in this field: 1. Why is compositional visual reasoning necessary? 2. What are the major reasoning architectures and paradigms of compositional visual reasoning? 3. What are the benchmarks and evaluation metrics? 4. What are the current limitations and bottlenecks? In the following sections, we begin by introducing core definitions in Sec. 2, covering visual reasoning, monolithic visual reasoning, and compositional visual reasoning. In Sec. 3, we analyze the advantages of compositional visual reasoning from multiple perspectives, including cognitive alignment with human reasoning, semantic and relational understanding, generalization and robustness, transparency and interpretability, modular reuse, mitigation of language biases and hallucinations, and improved efficiency with reduced data requirements. In Sec. 4, we outline multi-stage roadmap that captures the recent evolution of compositional visual reasoning, highlighting progression from prompt-enhanced, LLM-centric methods to unified agentic VLM architectures. The development trajectory begins with early prompting-based pipelines that loosely integrate frozen LLMs and VLMs, followed by toolaugmented systems that coordinate external visual modules through structured workflows. Building on these foundations, recent advances have led to the emergence of chain-of-thought reasoning VLMs and unified agentic VLM architectures, which unify perception and reasoning within cohesive frameworks. This evolution marks paradigm shiftfrom static, templatebased prompting toward dynamic, feedback-driven inference, enabling more flexible, interpretable, and autonomous visual reasoning. We then examine existing benchmarks and evaluation methodologies of compositional visual reasoning in Sec. 5, identify key challenges, and conclude with discussion of future research directions for advancing CVR systems in Sec. 6. 2. Background In this section, we provide an overview of the key concepts and terminology related to compositional visual reasoning, offering clearer understanding of the fundamental aspects of this field. 2.1. Visual Reasoning Visual reasoning is cognitive process that enables the interpretation and analysis of relationships among entities within visual scene to support decision-making and problem-solving [14, 26, 45, 52]. Rather than being limited to low-level perception, visual reasoning involves the integration of high-level information from multiple modalities (e.g., vision and language) to facilitate deeper semantic understanding [42, 47]. This multimodal reasoning underlies tasks such as visual question answering, entailment, and grounding [53, 54], where accurate comprehension and logical analysis of visual content are critical. In this survey, we focus on visual reasoning tasks that involve interpreting visual inputs (i.e., images) paired with textual queries or prompts to produce structured outputs. Formally, we denote the visual input as v, the query or prompt as q, and the answer as y. visual reasoning system is function : (v, q) (cid:55) y, which varies depending on the reasoning approach, monolithic or compositional. The form of also varies depending on the task and may include natural language responses, discrete answer selections (e.g., multiple choice), bounding boxes, or segmentation masks. Representative tasks include, but are not limited to, general visual question answering (e.g., What color is the car?), visual grounding (e.g., Find the chair next to the window), relational scene graph reasoning (Is the cat under the table?), visual commonsense reasoning (e.g., What might happen next?). At its core, visual reasoning comprises two essential components: visual perception, which involves extracting visual information such as objects, attributes, and relations [55, 56], and logical reasoning, which applies structured reasoning to derive conclusions based on the perceived information [57, 58]. While intuitive for humans, this capability remains major challenge for artificial intelligence systems due to the requirement for high-level abstraction and generalization. 2.2. Monolithic Visual Reasoning Monolithic visual reasoning methods are class of neural network models that directly map visual input and textual query to an output answer a, where commonly directly encodes both modalities into joint representation and predicts the answer without exposing intermediate steps. These architectures do not explicitly represent reasoning steps, performing reasoning implicitly within end-to-end pipelines [1, 37]. These models typically extract visual features from the entire image and combine them with language embeddings using co-attention or joint encoding to perform implicit multimodal reasoning [9, 49, 5961]. 5 Early models such as MFB [59], BAN [60], and MCAN [62] used convolutional features and cross-modal attention for direct answer prediction. Recent extensions include dual-encoder contrastive models (e.g., ViLBERT [63], CLIP [64], BLIP2 [11]), single-Transformer backbones (e.g., UNITER [65], Flamingo [66]), and vision-encoder-to-LLM pipelines (e.g., LLaVA [9], MiniGPT-4 [67], Otter [68], InstructBLIP [12], Qwen-VL [10]). 2.3. Compositional Visual Reasoning Compositional reasoning approaches map an input pair (v, q) to answer via an intermediate structured representation, for example, sequence of steps = {s1, s2, . . . , sn}, where the decomposition is either inferred dynamically or predefined based on the query structure. Each step si is optionally grounded to the visual input v, and may involve finer-grained operations such as object identification (oi), attribute inference (ai), and relation resolution (r(oi, oj)). These grounding processes may encompass visual perception capabilities such as object detection, attribute recognition, and depth estimation accordingly. The steps can then be executed sequentially or composed hierarchically into symbolic program, ultimately enabling the model to produce the final answer y. Compositional visual reasoning is specialized form of visual reasoning that emphasizes such structured decomposition and recombination of visual elements to solve complex tasks [69]. It is grounded in the principle of compositionality, which posits that the meaning of the whole is function of the meanings of its parts [70]. Rather than treating scene as monolithic input, compositional reasoning explicitly identifies and manipulates objects, attributes, and their interrelations to construct flexible reasoning pathways [1, 8, 37]. hallmark of compositional visual reasoning is its capacity for systematic generalization: the ability to infer novel combinations of familiar elements without direct exposure to those configurations during training [4, 37]. Concretely, this generalization refers to executing reasoning over the combinatorial product space of primitive visual elements, such as objects o, attributes a, and relations r(oi, oj), as well as adapting to different orderings of steps S. This mirrors human-like cognitive flexibility and is central to building AI systems that are interpretable, modular, and capable of robust generalization across diverse multimodal tasks [29, 30]. 3. Why Compositional Visual Reasoning? In this section, we outline the core motivations for adopting compositional visual reasoning, emphasizing its advantages over traditional monolithic models. Specifically, we examine seven key perspectives: cognitive alignment with human reasoning; semantic and relational understanding; generalization and robustness; transparency, interpretability, and modular reuse; reducing language biases and hallucinations; and lower data requirements and improved efficiency. 3.1. Cognitive Alignment with Human Reasoning Compositional visual reasoning (CVR) is inspired by the systematic and modular way humans approach visual problems [36, 69, 7174]. Humans naturally break down complex scenes into interpretable components (e.g., objects, attributes, and relationships) and apply sequential reasoning and contextual adaptation [1, 29, 75, 76]. This process reflects our ability to manipulate abstract symbols and relational concepts extracted from visual inputs. Moreover, the human capacity for compositional reasoning, rapid concept formation, and relational generalization is particularly remarkable [8, 37, 74, 77]. These abilities, often associated with fluid intelligence and non-verbal reasoning, enable flexible understanding across novel contexts and support exceptional sample efficiency when learning new visual concepts. Therefore, key desideratum for CVR models is to mirror core cognitive strategies, such as task decomposition, focused attention, iterative refinement, and the use of external tools to isolate critical visual details [7880]. In summary, aligning CVR models reasoning processes with human cognitive strategies enable more efficient learning in complex visual environments. 3.2. Semantic and Relational Understanding Traditional monolithic models often rely on superficial correlations and pattern matching, which limits their ability to capture deep semantic structures or precise object-centric relationships. In contrast, compositional visual reasoning enables explicit modeling of such relationships through structured representations like scene graphs or deductive reasoning within the language space [26, 44, 8184]. This allows systems to reason about spatial configurations, semantic relations, and attribute interactions in principled and interpretable manner [44, 48]. Such capabilities are essential for abstract scene understanding tasks, including counting, identifying intersections, and comparing object properties [43]. Moreover, compositional reasoning facilitates multimodal understanding by bridging vision and language, enabling accurate interpretation of complex visual elements in diagrams, charts, and tables [31]. 6 3.3. Generalization and Robustness Monolithic visual reasoning models are often brittle and prone to bias, with poor performance on out-of-distribution or few-shot tasks due to shortcut learning [5, 85, 86]. Compositional approaches, by contrast, promote systematic generalizationthe ability to solve novel tasks by recombining known elements, such as objects, attributes, or relations [4, 87]. For example, when structured models learn to ground visual concepts in scene (e.g. monitor and desk) and resolve relations (e.g. on top of), they can generalize to identify any object-relation pair (e.g. monitor on desk), without having seen that specific combination during training. This ability reduces dependence on dataset-specific biases and enhances robustness across domains. Benchmarks like CLEVR [5], GQA [73], and VCR [71] have demonstrated the necessity of compositional reasoning to prevent models from exploiting statistical priors and encourage faithful reasoning [1, 75]. 3.4. Transparency, Interpretability, and Modular Reuse Compositional visual reasoning enables the generation of intermediate outputs (e.g., bounding boxes, scene graphs, textual rationales) that expose the models decision path, improving interpretability and allowing targeted debugging [72, 88]. This ability is essential for building trustworthy and reliable systems, especially in domains where high precision is required. In contrast to black-box architectures, compositional systems are modular: components can be reused across tasks, reducing the need to retrain models from scratch and enabling plug-and-play integration of vision experts [8991]. As individual components improve, so does the full compositional framework. This flexibility also leads to efficiency gains in inference and model adaptation. 3.5. Reducing Language Biases and Hallucinations While generalization focuses on performance across diverse input distributions, another critical challenge for VLMs lies in the generation of inaccurate or unsupported content. Monolithic methods, especially those trained to directly map visual inputs to answers or rationales, are highly susceptible to hallucinations. These models might generate responses that appear plausible but are actually incorrect or ungrounded, particularly when they depend too heavily on linguistic priors [9294]. For instance, when asked about the color of green banana, monolithic model might incorrectly answer yellow. This issue stems from shortcut learning, where models exploit statistical biases in the training data rather than grounding their reasoning in visual evidence [5, 85, 95, 96]. In contrast, compositional reasoning mitigates this by enforcing explicit grounding: intermediate reasoning steps, such as object detection, relation extraction, or visual token selection, compel the system to anchor its inferences in actual visual content. This not only improves factual consistency but also reduces reliance on spurious correlations and enhances robustness in multimodal reasoning tasks [47, 88, 97]. Furthermore, compositional methods can incorporate external knowledge or common sense when needed, but do so in controlled and interpretable manner, avoiding the conflation of visual and linguistic content. 3.6. Lower Data Requirements and Improved Efficiency Traditional monolithic models often require massive datasets and computational resources to achieve acceptable performance, particularly as training must account for the combinatorial explosion of possible visual-linguistic combinations [29, 30]. This approach is not only costly but also inefficient, especially in dynamic or specialized domains. In contrast, compositional visual reasoning promotes modularity and reusability: once basic visual skills (e.g., object detection, OCR, segmentation) are learned, they can be reused across tasks without retraining [98]. This modular structure reduces the demand for extensive annotated datasets and enables few-shot or zero-shot generalization to new tasks by recombining known components [1, 75, 99]. Additionally, techniques such as visual program distillation or intrinsic tool orchestration (e.g., VPD, Griffon-R) allow compositional systems to approximate multi-step reasoning with reduced runtime and compute overhead in deployment [100, 101]. Together, these advantages position compositional visual reasoning as promising paradigm for building more capable, trustworthy, and efficient multimodal systems that align more closely with human reasoning. 4. Key Stages of Compositional VR Paradigms In this section, we trace the evolution of compositional visual reasoning models in recent years, highlighting how each stage has progressively enhanced their ability to decompose complex queries, reason over structured visual elements, and generalize across novel compositions. These transitions, illustrated in Figure 2, capture the key methodological shifts that have shaped modern compositional visual reasoning systems. Early monolithic large vision-language models like [9, 11, 12, 6668, 102108] adopt end-to-end architectures that directly map visual and textual inputs to answers. These models aim to learn human-like visual reasoning capabilities directly 7 Figure 2. Key shift from monolithic reasoning to compositional reasoning. from data and perform well on perceptual and shallow reasoning tasks. However, they struggle with more complex visual reasoning challenges, such as spatial understanding and fine-grained grounding [109111]. These limitations have driven the development of compositional visual reasoning approaches, which decompose the reasoning process into structured, interpretable steps. Figure 2 illustrates the key architectural transition from monolithic to compositional paradigms since 2023, and Figure 3 highlights representative models at each stage of this evolution. The recent surge of interest in compositional visual reasoning is largely rooted in the growing recognition of LLMs powerful reasoning capabilities in domains such as natural language, mathematics, and code [112114]. These advances have inspired researchers to explore whether such symbolic and structured reasoning abilities can be extended to vision-language settings. Recognizing monolithic approaches limitations, researchers began exploring whether the structured reasoning strengths of LLMs could be leveraged for visual domains. Given LLMs ability to decompose and solve complex problems in language, math, and code, natural question arose: Question I: Can complex visual reasoning tasks be decomposed into sequence of simpler sub-questions, each solvable by VLM, followed by answer synthesis via an LLM? Stage I: Prompt-Enhanced Language-Centric Methods provides an effective solution to Question I. These approaches leverage LLMs strong language-based reasoning ability to break down complex visual questions into series of simpler subquestions, each of which can be independently answered by VLM. After obtaining the sub-answers, the LLM performs final reasoning and synthesis entirely in the language space to produce the overall answer. This idea has given rise to compositional visual reasoning as prominent research direction. Rather than treating reasoning as single-step mapping from input to output, compositional visual reasoning paradigms frame it as multi-stage process involving perception, task decomposition, intermediate reasoning, and final synthesis, as illustrated in Figure 2. Although Stage prompt-enhanced language-centric methods have achieved some success, they remain limited in visual understanding due to weak grounding in the underlying visual content and their relatively fixed prompting logic. This naturally raises an important question: Question II: Can complex visual reasoning tasks be addressed more effectively by enabling models to actively invoke and coordinate multiple external tools through central decision-making module? Stage II: Tool-Enhanced LLMs have emerged as new paradigm to solve Question II. In this paradigm, the LLM issues grounding actions to external tools, and an external environment executes these actions to extract relevant information from the visual input. This interaction enables the system to progressively gather visual evidence and address complex visual reasoning tasks more effectively. These methods offer greater reasoning flexibility, but often struggle with effective tool coordination, frequently resulting in noisy or suboptimal workflows. In most cases, the tool action plannerusually an LLMhas access to visual information only in the form of text descriptions produced by upstream vision models. If these textual descriptions are incomplete or inaccurate, the reasoning quality degrades and cannot be fully recovered downstream. This raises natural question: 8 Figure 3. Roadmap of compositional visual reasoning models. Question III: Can VLMs, which directly perceive visual inputs rather than relying on textual proxies, provide improved performance and greater structural advantages in tool-enhanced frameworks for visual reasoning? Stage III: Tool-Enhanced VLMs have emerged consequently as an answer to Question III, allowing the planner to access visual input directly rather than relying solely on textual descriptions. This direct perception reduces semantic loss, enables more accurate and informed action planning, and opens new possibilities for designing both the format of grounding actions and the structure of visual feedback. In some cases, this also enables models to simulate visual imagination and perform visual verification by generating or manipulating visual content via tools during the reasoning process. While prompt-enhanced and tool-enhanced methods rely on explicit problem decomposition and inter-module interaction to support compositional reasoning, this motivates the following question: Question IV: Can VLMs directly perform perception and generate reasoning steps to derive an answer? Stage IV: Chain-of-Thought VLMs are promising solution to the challenges posed in Question IV, adopting unified architectures that tightly integrate perception and inference. These models perform multi-step reasoning without relying on external tools, and explicitly reveal intermediate thought processes and perception information before the final answer. While chain-of-thought (CoT) VLMs integrate perception and reasoning, they remain constrained by relatively rigid pipelines, single-pass forward architectures, and reliance on textual representations of thought. Rather than directly interacting with visual content (e.g., observing specific image regions), these models typically convert visual information into text and reason over it, limiting their ability to dynamically perceive and reflect. Based on these trends, critical research question arises: Question V: Can VLMs behave more like humans by autonomously exploring, inspecting, and manipulating visual content while reasoning iteratively and adaptively, all without relying on external tools? This question lies at the heart of Stage V: Unified Agentic VLMs, which represent the latest evolution in the field. These models incorporate higher-order cognitive mechanisms such as planning, memory, operation, imagination, textual feedback and visual evidence. Such capabilities enable iterative perception, step-by-step reasoning, and adaptive decision-making for complex visual tasks. 9 Figure 4. Overview of prompt-enhanced language-centric (Stage I) pipeline. 4.1. Stage I: Prompt-Enhanced Language-Centric Methods We introduce prompt-enhanced language-centric approaches in this subsection, with an overview illustrated in Figure 4. These methods capitalize on the reasoning capabilities of LLMs, which typically serve two complementary roles. First, as task decomposition module, the LLM decomposes complex visual question into sequence of simpler sub-questions that can be independently addressed by VLMs given visual input v. Second, as consolidator or reasoner, it synthesizes final answer based on the perceptual outputs or intermediate answers provided by the VLMs. In this paradigm, the LLM performs the core reasoning process entirely within the language space, using information obtained from the VLM and textual query. The VLM is tasked with visual perception, either by directly answering decomposed sub-questions or by extracting relevant visual features from the input image to text descriptions. This modular design promotes interpretability, adaptability, and prompt-based control, and often functions effectively without requiring fine-tuning or task-specific supervision. 4.1.1. Task Decomposition Followed by Visual Perception The typical architecture follows structured pipeline in which high-level visual question is first decomposed into sequence of low-level sub-questions. Each sub-question is then independently addressed, and the final answer is synthesized based on the intermediate responses. Early methods like DDCoT [115], ChatGPT-Ask-BLIP2 [116], IdealGPT [117] and Modeling Collaborator [118] adopt straightforward strategy in which frozen LLM decomposes complex visual question into sub-questions or instructions, which are then processed by pretrained VLMs such as BLIP-2 [11]. These approaches are lightweight and task-agnostic, enabling rapid prototyping and broad applicability. However, their reliance on fixed prompts and lack of coordination training often results in suboptimal reasoning paths, redundant queries, and brittle performance. 4.1.2. Perceptual Grounding Prior to Reasoning To address the limitations mentioned in Section 4.1.1, more recent work introduces varying degrees of structured interaction. CoLa [119] improves coordination by training the LLM to aggregate and reason over outputs from multiple VLMs, yielding better coherence and adaptability. PromptCap [120], MM-CoT [121] and AdGPT [122] shift focus toward enriching image understanding through question-aware or dialogue-driven captioning. Here, the VLM acts primarily as visual-to-text converter, and the LLM handles downstream reasoning. Similarly, VDLM [123] extracts mid-level features that align low-level visual details with high-level semantic abstractions for LLM-based reasoning, and Finedefics [124] first extracts visual information through perception modules and then employs trained LLM to derive the final answer. This separation enables compatibility with black-box LLMs and performs well in knowledge-oriented VQA, but the quality of reasoning is highly dependent on the captioning fidelity and lacks adaptive feedback. VCTP [125] further strengthens integration by training the LLM to operate in structured see-think-confirm pipeline. It adaptively grounds visual concepts, reasons over intermediate representations, and verifies outputs in closed loop, improving transparency, consistency, and efficiency. Unlike earlier systems, VCTP [125] enables tighter coupling between perception and inference, bridging the gap between modularity and end-to-end reasoning. 4.1.3. Synthesis and Outlook Overall, prompt-enhanced language-centric methods represent clear progression from simple prompting strategies toward more coordinated and structured reasoning. However, these approaches remain limited in their visual grounding, as they 10 rely solely on VLMs for perception without deeper integration of visual context. Additionally, their relatively fixed pipeline architecture constrains flexibility, making it difficult to adapt to more complex or dynamic visual reasoning tasks. 4.2. Stage II: Tool-Enhanced Large Language Models In contrast to prompt-enhanced approaches that rely solely on textual reasoning, tool-enhanced LLMs equip language models with the ability to invoke external modules for perception, analysis, and computation. This paradigm typically involves two key components: generating actions from the current state, and transitioning between states by executing those actions. The LLM acts as central planner or action generator, responsible for decomposing tasks, selecting appropriate tools, and synthesizing final answers [29, 117, 126]. Specifically, it maps the current state to grounding actions by producing structured tool calls, such as code snippets or formatted prompts. The environment then interprets and executes these actions, triggering state transitions on the visual input using corresponding tools. These tools may include vision models (e.g., detectors, captioners), knowledge bases, image editors, or code interpreters. Therefore, LLMs role is to understand tool descriptions, generate calls to those tools, and interpret returned results in multi-turn or sequential manner. 4.2.1. Single-Turn Framework Representative systems such as GPT4Tools [127], Visual ChatGPT [128], Chameleon [74], VisProg [72], ViperGPT [129], HuggingGPT [130], CRAFT [131], MM-REACT [132], Contextual Coder [133] and InternGPT [134] follow this structure, as illustrated in the tool-enhanced LLM framework overview in Figure 5. They assume access to library of vision-language tools and use frozen LLM to plan tool usage via prompt engineering, code generation, or natural language APIs. This setup enables training-free or zero-shot compositional reasoning across modalities, and provides flexible interface to integrate new tools. However, these frozen LLMs often lack true tool awareness. They struggle to reason about tool capabilities, generate incorrect tool calls, and are unable to adapt when tool outputs are noisy or invalid [135]. 4.2.2. Adaptive Tool Use via Training, Feedback, and Verification To improve LLMs tool awareness, recent methods introduce enhanced LLM training, learning from tool feedback, or adopting agentic reasoning. For example, CLOVA [136] introduces closed-loop learning mechanism that combines reflection with continual tool updates. This enables the system to adapt its tools and strategies over time. VisRep [89] improves LLM tool usage through self-training (i.e., supervised fine-tuning) based on program execution feedback. HYDRA [29] introduces multi-turn agent architecture with an LLM planner, reinforcement learning controller, and reasoning engine that dynamically refines tool usage through iteration. Building on these foundations, DWIM [76] advances the paradigm by explicitly training LLMs through instruct-masking fine-tuning, making them tool-aware. This allows the model to better understand tool functionalities, detect potential errors, and revise the decisions accordingly. In parallel, NAVER [137] and SYNAPSE [138] adopt neuro-symbolic strategies that integrate LLM-guided logic with structured tool invocation and verification, achieving robust and reliable reasoning under complex constraints. ViuniT [139] proposes complementary approach that generates visual unit tests (i.e., image-answer pairs) to validate the logical correctness of visual grounding actions (i.e., visual programs) used during inference. Within this paradigm, LEFT [39] introduces trainable neuro-symbolic framework that reasons over LLM-proposed visual concepts with differentiable logic executor, enabling end-to-end updates to tools (i.e., grounding modules) via backpropagation. 4.2.3. Synthesis and Outlook Overall, this class of methods reflects shift from static prompting to interactive, interpretable, and dynamically evolving tool use, where LLMs are not just passive planners, but increasingly learn to reason with and through tools [76]. These advances lay the groundwork for future multimodal agents with stronger compositionality, reliability, and generalization. However, these methods have critical bottleneck in that there is an unbounded number of interpretations from finite set of visual building blocks. When LLMs operate solely on textual descriptions converted from visual inputs, their abstraction capacity becomes major limitation. If the abstraction is inaccurate, misaligned, or fails to capture essential visual cues, crucial information may be lost during the conversion. This impairs the systems ability to solve the task correctly, as important visual semantics are never incorporated into the reasoning process. 4.3. Stage III: Tool-Enhanced Vision-Language Models Tool-enhanced VLMs extend tool-enhanced LLMs by replacing the underlying LLM with VLM, enabling more direct and flexible visual interaction, as illustrated in Figure 5. Unlike tool-enhanced LLMs where planners rely on preprocessed visual information from tools, this framework allows planners to directly receive raw images as input, reducing information loss and improving efficiency. Large-scale VLMs are dynamically augmented with external tools, enabling selective 11 Figure 5. Tool-enhanced LLMs (Stage II) and VLMs (Stage III) pipeline. invocation of specialized systems (e.g., such as object detectors, OCR engines, segmenters, and image generators) via natural language interfaces or latent control signals. This architecture enhances performance on complex tasks and mitigates issues such as hallucination and weak visual grounding. Tool-enhanced VLMs can be broadly classified according to the modality and explicitness of their tool interaction. The first category, referred to as language-mediated control, includes models that issue interpretable instructions, such as natural language prompts or symbolic commands, to activate tool functionalities. The second category, termed embedding-mediated integration, encompasses models that coordinate tool use through learned latent representations, embedding control logic directly into the architecture without relying on explicit symbolic mediation. 4.3.1. Language-Mediated Grounding Action Control Models such as Image-of-Thought [92], P2G [85], LLaVA-Plus [140], Syn [141], and VIREO [90] exemplify VLM-centered planning paradigms. These systems rely on the VLM to generate interpretable prompts (e.g., natural language or code), which are then used to invoke relevant external tools for perception and reasoning. For example, Image-of-Thought [92] simulates human cognitive processes by planning sequences of visual operations like segmentation or cropping and combining the resulting visual rationales with textual reasoning. P2G [85] facilitates finegrained visual understanding by allowing the VLM to dynamically request help from OCR or grounding agents, particularly for high-resolution and text-rich images. LLaVA-Plus [140] employs modular design in which the VLM acts as planner that generates Thought-Action-Value prompts to control repository of pre-trained visual tools. VIREO [90] improves multi-step reasoning by automatically decomposing questions and invoking tools step-by-step to resolve sub-questions. Furthermore, with the emergence of GPT-o1 and DeepSeek-R1, reinforcement learning-based approaches for tool selection have gained traction, as seen in models like OpenThinkIMG [142] and VToolR1 [143]. 4.3.2. Embedding-Mediated Grounding Action Control By contrast, VITRON [144] and VisionLLM v2 [145] exemplify models in the second category, where interaction with external modules occurs through learned embeddings rather than textual prompts. These systems embed communication pathways directly into their architecture. VITRON [144] employs hybrid message-passing framework that combines discrete routing tokens with continuous embeddings to activate various back-end modules for tasks like image editing, segmentation, 12 Figure 6. Monolithic versus Chain-of-Thought (CoT) reasoning VLMs (Stage IV) pipeline. and video generation. VisionLLM v2 [145] introduces super link mechanism that routes task-specific information using learnable query embeddings bound to routing tokens, allowing seamless integration of decoders for diverse tasks such as object detection, pose estimation, and image synthesis. 4.3.3. Image as Feedback in Tool-Enhanced VLMs more general class of tool-enhanced approaches for compositional visual reasoning aims to incorporate feedback during multi-turn, iterative processes. The most common form of feedback is textual, often delivered through natural language messages or structured programming outputs. However, when VLMs are used for action grounding and receive raw image inputs directly, this enables the system to obtain visual feedback in the form of updated or modified images. This gives rise to distinct framework in which the model interacts with external tools to operate on images and then processes the resulting visual input for further reasoning. Models like Visual Sketchpad [146], VToolR1 [143], Self-Imagine [147], LATTE [148], CoT-VLA [149], Creative Agents [150] extend this paradigm by using external tools to synthesize imagined or modified images during reasoning. These imagined visuals validate intermediate actions and enhance the planning process by providing grounded visual feedback. 4.3.4. Synthesis and Outlook While tool-enhanced VLMs demonstrate strong generalization, grounded reasoning, and modular interpretability, they also present distinct challenges. These include high computational demands, increased input complexity, reliance on external tool accuracy, and architectural overhead [66, 85, 127, 145]. Future work should focus on improving integration robustness and efficiencymoving from prompt-based orchestration toward seamless architectural unificationto enable scalable, interpretable, and general-purpose multimodal agents [149]. 4.4. Stage IV: Chain-of-Thought Reasoning VLMs Recent developments in vision-language models (VLMs) have led to the emergence of new generation of chain-of-thought (CoT) reasoning-centric, end-to-end architectures. Unlike prompt-enhanced or tool-enhanced systems that rely on external modules and prompt chaining, these models aim to integrate perception and reasoning into single forward pass, leveraging pretraining and fine-tuning strategies inspired by large-scale LLM developments, such as GPT-o1 [151] and DeepSeekR1 [152]. This section categorizes these models into three evolving trends: 1) Prompt-enhanced CoT reasoning VLMs; 2) RL-enhanced CoT reasoning VLMs; 3) Visually grounded CoT reasoning VLMs. 4.4.1. Prompt-Enhanced CoT Reasoning VLMs Prompt-enhanced CoT reasoning VLMs simulate multi-step reasoning behavior within standard vision-language pipeline by leveraging in-context prompting or explicit stage-wise instruction formats. These models often frame the problem through intermediate representations or structured sub-tasks that guide reasoning while avoiding external modules or extra learning. representative example is LLaVA-CoT [153], which breaks its output into structured reasoning stages, including summarization, captioning, intermediate deduction, and final conclusion. Similarly, CCoT [35] enhances compositionality by instructing the model to generate scene graphs from the input, which then guide the reasoning process through object-centric abstractions. GPT-o1-style prompting strategies extend this approach by defining explicit intermediate stages that structure the reasoning process in predetermined manner. While these methods demonstrate improved performance and explainability through fixed structures, their reasoning capacity remains constrained by limited adaptability and generalization across diverse tasks. 13 4.4.2. RL-Enhanced CoT Reasoning VLMs Inspired by proprietary reasoning models such as GPT-o1 [151] and DeepSeek-R1 [152], RL-enhanced CoT reasoning VLMs internalize systematic reasoning through combined supervised fine-tuning and reinforcement learning. These models typically adopt two-phase learning framework: an initial supervised fine-tuning stage to encode reasoning patterns, followed by reinforcement learning or curriculum-based refinement to improve generalization and robustness. For example, ReasonRFT [154] and Vision-R1 [155] leverage Group Relative Policy Optimization (GRPO) [156] to balance logical coherence with perceptual grounding during reasoning. DeepPerception [157] introduces cognition-perception synergy where reasoning accuracy and visual comprehension mutually enhance each other. G1 [158] adapts these strategies for visually interactive settings, enabling perception-grounded decision-making across complex tasks. Ground-R1 [159] introduces interpretable reasoning by decoupling visual evidence grounding from answer generation, trained end-to-end using only question-answer pairs without external annotations. The two-phase learning framework aligns with findings that RL can encourage VLMs to actively search for correct answers [152, 159]. From an optimization standpoint, the performance improvements from RL (e.g., RLHF) stem from how it reshapes the models output distribution. In distributional terms, the SFT objective corresponds to trajectory-level distribution matching using forward KL divergence (KL(PdataQθ)), encouraging the model θ to broadly match the empirical distribution of the training data. While maximizing this distributional coverage ensures diversity, it also assigns probability mass to plausible but suboptimal or irrelevant outputs. In contrast, RL shifts the objective toward mode-seeking via reverse KL divergence (KL(QθPdata)), training the model to concentrate specifically on generating outputs that are consistently correct or optimal. Although this reduces behavioral diversity, it yields more coherent and accurate responses, which is particularly beneficial in multi-step reasoning tasks where correctness and logical consistency outweigh broad coverage [160162]. Although these models represent significant shift toward more autonomous and cognitively aligned reasoning architectures, they often require substantial computational resources and careful curriculum design to achieve robust performance. While both RL-enhanced and prompt-enhanced CoT reasoning VLMs provide interpretable solutions by exposing structured reasoning steps, they still depend on implicit reasoning paths and typically lack explicit intermediate supervision or visual grounding. 4.4.3. Visually Grounded CoT Reasoning VLMs Visually grounded CoT reasoning VLMs explicitly couple reasoning steps with visual grounding information, producing interpretable intermediate outputs such as bounding boxes, object references, or spatial annotations alongside final answers. This class of models benefits from training data that aligns structured reasoning with grounded visual supervision, either synthesized via tool-based pipelines or adapted from existing multimodal datasets [73, 163, 164]. representative example is PaLI-X-VPD [100], which introduces visual program distillation to inject coherent, toolgenerated chain-of-thought reasoning traces into the training process. Building on the idea of explicit object grounding, VoCoT [88], CoF [78], LLaVA-Aurora [165] formulates each reasoning step as tuple combining visual region, its spatial coordinates, and semantic description, enabling object-centric, multi-step inference. MM-GCoT [94] and VISTAR [166] contribute dedicated dataset designed to enforce answer-grounding consistency, encouraging VLMs to align each reasoning step with grounded visual evidence. CoReS [167] advances this line of research by combining segmentation and logical inference through interleaved dual chains, showing superior performance in compositional and fine-grained visual reasoning tasks. Meanwhile, Griffon-R [101] unifies the stages of understanding, thinking, and answering within single forward pass. By leveraging high-resolution visual encoder and fully end-to-end architecture, the model learns to internalize both finegrained visual understanding and stepwise inference through supervised training. Despite their interpretability and improved alignment with human-understandable reasoning, these models often require complex training pipelines and rely on highquality annotations or synthetic visual traces, which can limit scalability or generalization in less structured environments. 4.4.4. Synthesis and Outlook CoT reasoning VLMs represent critical advancement in bridging the cognitive gap between perception and inference. Prompt-enhanced models enable lightweight mechanisms for incorporating structured reasoning, RL-enhanced architectures embed systematic thought processes through iterative optimization, and visual grounded CoT models enrich reasoning with spatial alignment and interpretability. The next frontier lies in unifying these strengths, developing hybrid models that can flexibly switch between internal reasoning, spatial grounding, and external tool interaction as needed. Such integration holds the promise for truly general, robust, and trustworthy visual reasoning agents. However, the single-pass, CoT architecture may hinder performance as task complexity increases, due to its limited capacity for iterative reasoning and refinement. 14 Figure 7. Illustration of unified agentic VLM (Stage V) inference. The model iteratively refines its understanding by appending intermediate outputs to the input and dynamically invoking internal capabilities to resolve complex visual reasoning tasks. 4.5. Stage V: Unified Agentic Vision-Language Models Unified agentic VLMs represent recent shift in multimodal reasoning, wherein the model actively plans, adapts, imagines and executes sequence of decisions to solve complex visual tasks [168, 169]. Different from prompt-enhanced or toolaugmented systems, where language models issue static queries or deterministic tool calls, unified agentic VLMs are designed to autonomously decompose tasks, selectively gather visual evidence, and iteratively refine their reasoning process. These models embody form of learned deliberation: they identify uncertainties, determine which visual information is required, and justify conclusions through traceable decision-making trajectories. These models [78, 111, 170] often operate within multi-step feedback loop that integrates perception, decision-making, and execution, effectively simulating high-level planning. The typical architecture consists of controller (usually an LLM or policy model), one or more visual engines (e.g., captioners [9, 104], segmenters [171], detectors [172]), and an intermediate memory or context accumulation module. The reasoning process is conducted either explicitly, through structured chains of operations, or implicitly, using learned state representations. In contrast to static tool-use frameworks, unified agentic VLMs learn to adapt tool usage, focus visual attention, and revise intermediate beliefs when faced with ambiguity, occlusion, or fine-grained queries. 4.5.1. Automatic Discovery of Informative Regions and Goal-Driven Exploration subset of unified agentic VLMs demonstrates the ability to automatically identify, search, and discover informative regions within an image to iteratively solve subtasks and ultimately complete the overall task. These models exemplify active visual reasoning by dynamically allocating attention and modifying inputs based on task demands. Representative systems illustrate this paradigm in diverse ways. SEAL [169] incorporates visual working memory that stores the question, global image, target regions, and retrieved patches. When the initial image context proves insufficient, the model identifies missing visual entities, performs targeted search and cropping, and revisits the question with enriched inputs. ZoomEye [173] treats the image as hierarchical patch tree and uses confidence scores from an MLLM to guide zoom-in actions and determine when to stop based on answer certainty. FAST [168] adopts dual-system architecture that switches between fast and slow reasoning modes. When greater precision is needed, the model activates segmentation and proposal tools and aggregates evidence through chain-of-evidence structure. Other methods, such as CogCoM [111], VisCoT [174], Insight-V [175], DeepEyes [176], GeReA [177] and Argus [27], guide an LLM to generate sequence of visual manipulation stepssuch as grounding, zooming, OCR, and countingto incrementally acquire task-relevant information. CoF [78] introduces zoom-in tokens to support interleaved visual reasoning, progressively appending fine-grained features during generation. DC2 [170] proposes an alternative strategy by dividing high-resolution images into patches, summarizing them individually, and using visual memory mechanism to selectively retrieve relevant regions at inference time. 4.5.2. Imagination-Enhanced Unified Agentic VLMs Another class of unified agentic VLMs incorporates artificial visual thinking, enabling models to simulate mental imagery as part of the reasoning process. These systems generate intermediate visual states, modified image representations, or hypothetical scenes to support problem-solvingmirroring the way humans often visualize outcomes when reasoning. Crucially, this imagination is performed internally, without invoking external tools. Some models focus on textual imagination within the latent space. For example, Mirage [178] introduces machine mental imagery framework that augments VLM decoding with latent visual tokens alongside textual outputs. Rather than generating pixel-level images, the model performs visual thinking by extending hidden state trajectories with multimodal tokens, maintaining multimodal reasoning flow entirely in latent space. Another notable approach, FOREWARN [179], adapts the LLaMA-3.2-11B-Vision-Instruct model by replacing its image tokenization module with world model encoder and latent dynamics component. This design allows the model to internally simulate visual states during reasoning, enabling it to perform latent imagination without relying on external rendering. 4.5.3. Synthesis and Outlook Unified agentic VLMs bring several benefits. They enhance compositionality, adapt to ambiguous inputs, and support interpretability through intermediate visual artifacts [111, 168, 170]. Many show strong performance on high-resolution and compositional benchmarks such as V*Bench [169], GQA [73], ReasonSeg [180], and MM-Vet [181]. They also demonstrate robustness when paired with relatively small LLMs, offering an efficient alternative to large proprietary models [78, 101]. Nonetheless, these systems face challenges. Many rely on handcrafted prompts, special tokens, or rule-based triggers, which limit scalability [140, 167]. Multi-step pipelines often incur higher computational costs and may propagate errors through sequential stages. Visual operations such as cropping or segmentation are sometimes imprecise, especially for small or occluded objects [76, 136, 173]. Moreover, the lack of unified supervision across reasoning steps and final outputs hinders end-to-end optimization [100, 159]. In summary, agentic vision-language models represent promising direction that combines planning, perception, and reasoning within an integrated framework. They bridge the gap between passive captioning and interactive visual understanding. Future work may focus on improving their decision policies, unifying learning objectives, and integrating them with broader multimodal toolchains to support general-purpose visual intelligence. 5. Benchmark and Evaluation Evaluation and benchmarking play pivotal role in assessing the progress and diagnosing the limitations of visual reasoning systems. wide range of datasets and metrics have been developed to probe different aspects of reasoning, minimize dataset bias, and analyze model capabilities with fine-grained annotations. 5.1. Types of Benchmarks and Datasets General Visual Question Answering. General VQA benchmarks aim to evaluate models ability to answer naturallanguage questions based on real-world images. These datasets focus on broad coverage of visual concepts and diverse question types (e.g. object presence, color, location, and actions), usually with human-generated questions. Prominent examples include VQA-v1 [182], VQA-v2 [183], COCO-QA [184], Visual Genome [185], and Visual7W [186]. These datasets focus broadly on object presence, attributes, relationships, and basic visual comprehension. Variants such as VQA-CP [187] specifically address biases through carefully restructured training and testing splits. In short, general VQA datasets are mostly well-suited for testing perception-based understanding grounded in image content alone. Relational Scene Graph Reasoning and Systematic Generalization Relational scene graph reasoning and systematic generalization datasets are designed to evaluate models ability to reason over novel combinations of familiar visual concepts, or interpret high-level semantic concepts across unseen environments. These tasks require models to perform multi-step inference, manipulate symbolic representations, and handle compositional semantics such as nested attributes, spatial relations, and logical structures. Often constructed using synthetic or structured environments, these benchmarks allow precise control over data generation and enable evaluation of reasoning fidelity and generalization capabilities under distribution shifts. Systematic generalization is rigorously tested by datasets such as GQA [73], ReasonSeg [180], TallyQA [188], NaturalBench [95], Cola [189], the Visual Abstractions Benchmark [190], CREPE [69] and SugarCrepe [191]. These datasets explicitly target the models compositional generalization, relational learning, and reasoning under uncertainty. In short, this category is essential for assessing structured reasoning, generalization to novel combinations, and step-wise inference fidelity. Synthetic and Diagnostic Evaluation. Synthetic and diagnostic benchmarks provide controlled environments to test specific reasoning sub-skills, such as attribute comparison, spatial relation tracking, and logical consistency. These datasets are often built with artificial scenes, minimizing irrelevant variability and allowing fine-grained analysis of reasoning behaviors. 16 Figure 8. Examples of benchmarks and datasets Diagnostic variants further introduce linguistic perturbations, attention supervision, or adversarial rephrasings to evaluate robustness and interpretability [40, 192]. Representative datasets include CLEVR [5], SHAPES [193], TaskMeAnything [194], SVRT [195], CLOSURE [196], CURI [197] and CLEVR variants (e.g., CLEVR-Ref+ [198], CLEVR-XAI [199], QLEVR [200], CLEVR-X [201], CLEVRDialog [202]). Diagnostic benchmarks like VQA-X [203], VQA-HAT [204], and VQA-Rephrasings [205] specifically target reasoning quality, explanation generation, human attention alignment, and linguistic robustness. Additionally, synthetic tests for abstract reasoning also fall under this category, notably, Ravens Progressive Matrices datasets (e.g. PGM [206], RAVEN [6]). Other examples include KANDINSKYPatterns [207] and Bongard-LOGO [208], which focus on structural pattern recognition and analogy-based concept learning, respectively. Knowledge-Based and Commonsense Reasoning. Knowledge-based and commonsense benchmarks are developed to assess models capacity to incorporate external knowledge (e.g., factual, cultural and commonsense) into the visual reasoning process. Unlike general VQA, these tasks require inference beyond observable pixels, leveraging external sources or prior training on world knowledge. To this end, number of datasets have been proposed to evaluate such capabilities. Examples include OK-VQA [163], AOKVQA [164], VCR [71], FVQA [209], and KB-VQA [210], which require models to reason with factual or commonsense knowledge not present in the image. In addition, CRIC [211] further emphasizes compositional and grounded commonsense reasoning. These benchmarks are therefore crucial for evaluating whether model can move beyond surface-level perception and engage in truly knowledge-intensive visual inference. Image-Based Text and Document Understanding. These benchmarks focus on evaluating models ability to extract, read, and reason over textual information embedded in visual formats such as photographs, charts, and scanned documents. Tasks typically involve OCR-based understanding, document layout parsing, or diagram interpretation, requiring alignment between textual and spatial structures. Representative datasets include TextVQA [212], ST-VQA [213], DocVQA [214], and AI2D [215]. High-Resolution and Fine-Grained Perception. To explore vision-language models capabilities in fine-grained perception, high-resolution benchmarks such as V*Bench [169], HR-Bench (4K/8K) [170], and LISA-Grounding [180] evaluate models performance on detailed visual scenes and precise object recognition. Multidisciplinary and Specialized Domains. Benchmarks under this category extend visual reasoning into specialized domains or underrepresented user populations. They include medical diagnostics, accessibility for the visually impaired, 17 cultural diversity, mathematical and chart reasoning, and embodied agents [216222]. Many also incorporate multimodal challenges across domains. Specialized datasets are developed for specific domains and applications, including medical imaging (e.g. VQA-RAD [216], PathVQA [217]), visually impaired assistance (e.g. VizWiz [218]), multilingual and cultural diversity (e.g. WorldCuisines [219]), mathematical reasoning (e.g. MathVista [223], ChartQA [220], Geoclidean [222]), and embodied environments (e.g. VQA 360 [221]). Additionally, comprehensive benchmarks such as MM-Vet [181], MME [224], MMMU [225], and SEEDBench [226] systematically evaluate vision-language capabilities across multiple reasoning domains. Benchmarks including MMBench [227], LLaVA-Bench [228], NovPhy [229], M3CoT [230], and VisIT-Bench [231] further probe fine-grained skills such as instruction-following, physical reasoning, and visual conversational interactions. Hallucination and Robustness Evaluation. This category includes benchmarks designed to detect hallucinated outputs, breakdowns in robustness, and model inconsistency, particularly in cases where the alignment between vision and language inputs is unreliable or weak. These tasks typically involve verifying whether model-generated responses are faithfully grounded in the input image and whether the model maintains consistent behavior under various perturbations. Representative datasets such as POPE [232], HallusionBench [233], YESBUT [234], and CasualVQA [235] are designed to ensure that model outputs remain factually supported by visual evidence. 5.2. Evaluation Metrics Evaluating visual reasoning involves diverse set of metrics designed to capture not only the correctness of answers but also the interpretability, grounding consistency, and efficiency of the reasoning process. These metrics are typically selected based on task characteristics, dataset design, and the specific reasoning abilities being assessed. Accuracy-Based Evaluation. Accuracy and exact match remain the dominant metrics for classification-based tasks, including general VQA, compositional reasoning (e.g. GQA [73]), and knowledge-intensive VQA (e.g. OK-VQA [163], AOKVQA [164]). For tasks with variable answers, such as TextVQA [212] and ST-VQA [213], metrics like VQA Score are used. common evaluation paradigm in these benchmarks is option matching, closed-set method where models select from predefined candidates. This reduces ambiguity in open-ended tasks and ensures standardized evaluation. Datasets such as CLEVR [5], ScienceQA [236], MME [224], and MMBench [227] rely on this approach to improve fairness and comparability. Grounding and Segmentation Metrics. For tasks requiring visual localization, metrics such as Intersection over Union (IoU), and generalized IoU [237] are widely used to assess the alignment between predicted and ground-truth regions. Metrics like AP@50 and AP@75 evaluate precision at different IoU thresholds. Tasks involving visual reasoning with segmentation leverage grounding accuracy and salient object detection metrics, including S-measure and mean absolute error. Rationale and Text Quality. To evaluate explanation quality, metrics such as BLEU [238], ROUGE [239], and CLIP sentence similarity [240] are used to assess the coherence and informativeness of generated rationales. F1, precision, and recall are employed for sequential tasks or when answers involve structured text. Metrics like answer-grounding consistency and average output token length further probe whether generated explanations align with visual evidence and maintain concise reasoning. Task-Specific and Behavioral Metrics. Visual reasoning systems are further evaluated using specialized metrics such as search length (number of steps in visual search) [169], action counts (frequency of operations like zoom or segmentation) [92], hallucination rate (frequency of unsupported content) [241], and confidence improvement (sensitivity to question variations) [40]. Keypoint-aware evaluation and format adherence metrics are employed in manipulation and program-based reasoning settings. Efficiency and Cost. Finally, metrics related to resource usage, such as runtime, inference latency, LLM token consumption, and financial cost, are reported to assess the computational efficiency and scalability of vision-language models [137]. These evaluation metrics collectively provide comprehensive view of model performance, capturing both end-task accuracy and the underlying reasoning behavior, interpretability, and resource efficiency necessary for reliable multimodal intelligence. 5.3. Synthesis and Outlook Current benchmarks and evaluation protocols provide broad landscape for assessing visual reasoning, offering evaluations across final answer accuracy, visual grounding, textual explanation quality, and domain coverage. They support tasks ranging from general VQA to fine-grained perception, knowledge-based reasoning, and multimodal interactions, spanning diverse modalities, task types, and reasoning skills. Despite this breadth, several critical limitations remain. An important limitation in current evaluation practices is that existing benchmarks rarely evaluate explicit intermediate reasoning steps, also referred to as step-level or thought-step outputs, which have become increasingly central in recent compositional visual reasoning methods. Traditional evaluation metrics like BLEU [238], ROUGE [239], and grounding scores provide only indirect assessments by evaluating final rationales or explanations. Consequently, they fail to directly measure the accuracy, coherence, or causal consistency of individual reasoning steps. For instance, model might produce correct final answers but rely on incorrect intermediate deductions or superficial correlations. Second, current benchmarks lack explicit quantification and analysis of reasoning difficulty levels, making it challenging to understand the distribution of question complexity. Questions involving single object and its attributes are significantly easier compared to those involving multiple objects with intricate attribute references and complex inter-object relationships. For example, question like What color is the ball? demands far simpler reasoning than Which object next to the small blue cube is the largest red sphere? Without explicitly defined difficulty scales, researchers cannot accurately evaluate models capacity to handle varying complexity levels. To address these issues, future benchmarks should: 1) incorporate detailed annotations and corresponding metrics to evaluate intermediate reasoning steps, with particular focus on assessing step-level consistency, causal coherence, and grounding accuracy; 2) explicitly quantify and categorize reasoning difficulty levels, enabling systematic evaluation of model performance across spectrum of question complexities; and 3) provide comprehensive diagnostic insights by clearly differentiating reasoning capabilities, thereby helping researchers identify specific limitations in models and informing targeted improvements. Advancing these directions will enable deeper insights into model behaviors and support the robust development of compositional visual reasoning frameworks. 6. Insights, Challenges and Directions Compositional visual reasoning (CVR) has evolved quickly in recent years, with numerous papers emerging across different classes of models and evaluated on wide range of benchmarks. In this section, we synthesize key insights from recent research, identifying trends and promising principles. We then investigate core challenges that continue to limit scalability, generalization, and robustness, as well as examine future directions aimed at overcoming these challenges to advance generalpurpose CVR systems. 6.1. Insights Recent studies in compositional visual reasoning indicate notable shift from static, perception-based approaches toward structured, multi-step reasoning frameworks inspired by cognitive science. Models such as Chain-of-Focus [78], Chainof-Manipulations [111], and Visual CoT Prompting [125] introduce modular reasoning procedures that enable intermediate decision making and enhance interpretability. Dual-system frameworks, exemplified by FAST [168], emulate both intuitive and deliberative cognitive processes, while reinforcement learningbased methods like Ground-R1 [159] and Vision-R1 [155] leverage feedback signals to promote self-corrective and generalizable reasoning without heavy reliance on supervised annotations. This trend is further exemplified by the emergence of agentic VLMs that integrate control flow, memory, and visual grounding into unified reasoning loop. Architectures such as SEAL [169] and ZoomEye [173] enable iterative, goal-directed visual exploration guided by questions, while Griffon-R [101] fuses perception and reasoning within single forward pass. These models demonstrate the potential of embedding planning, action selection, and evidence integration within an agentic framework for compositional tasks. To address data scarcity, researchers have turned to automated pipelines that synthesize supervision using LLMs and vision models. These pipelines generate CoT rationales, visual masks, and object-level annotations, providing rich multimodal supervision. Benchmarks such as MM-GCoT [94] and VoCoT [88] emphasize step-wise reasoning and grounded verification, contributing to more interpretable and robust evaluation protocols. At the architectural level, ongoing efforts aim to unify static scene understanding with dynamic interaction and planning capabilities. This includes the development of tool-aware LLMs for adaptive tool invocation and coordination, as well as models with visual working memory mechanisms that manage long-horizon reasoning over fine-grained visual inputs. Approaches like DC2 [170] and SEAL [169] illustrate the effectiveness of such memory-based designs in maintaining contextual coherence while minimizing computational overhead. 6.2. Key Challenges and Future Directions Despite the rapid progress in compositional visual reasoning, significant challenges remain in enhancing reasoning capability, improving data quality, integrating architectures, and refining evaluation methodology. Addressing these limitations is 19 essential for developing scalable, trustworthy, and general-purpose compositional visual reasoning systems. 6.2.1. Limitations of LLM-Based Reasoning central limitation across all five stages of compositional visual reasoning systems discussed is their heavy reliance on LLMs as the core reasoning module. While LLMs excel at processing natural language and generating coherent text, they often lack the grounded, structured reasoning necessary for complex visual tasks. In particular, LLMs lack an internal world model, which limits their ability to simulate visual or physical dynamics, perform spatial transformations, or reason about hypothetical or counterfactual outcomes. These capabilities are essential for tasks such as object reconfiguration, spatial prediction, and planning tool usage [242]. For example, when tasked with predicting whether one object can fit into another or reasoning about the consequence of moving an obstacle, LLMs tend to rely on linguistic priors rather than perceptual evidence, often producing plausible but incorrect answers. This limitation is rooted in their autoregressive architecture, which is optimized for next-token prediction rather than iterative self-correction or causal inference [243]. From cognitive standpoint, this behavior resembles System 1 reasoningfast, heuristic, and shallowrather than System 2 reasoning, which involves deliberate, multi-step analysis grounded in perceptual input and symbolic manipulation [244246]. This gap is especially evident in visual reasoning benchmarks that require multi-hop spatial inference, counterfactual exploration, or tracking of visual state changes, where LLMs often fail to reason beyond surface-level correlations [247, 248]. Potential Directions. To address these challenges, one promising solution is to introduce an explicit internal world model into compositional visual reasoning systems. Such models would enable agents to simulate hypothetical visual scenarios, reason over spatial and temporal dynamics, and plan multi-step actions grounded in perception. This capability supports forward simulation, counterfactual inference, and long-horizon planning. It also aligns more closely with System 2 reasoning. Incorporating world models takes inspiration from model-based reinforcement learning and offers pathway to bridging the gap between language-driven reasoning and perceptually grounded intelligence [159]. 6.2.2. Hallucinations Although LLMs are highly effective at language-based inference, their reasoning often lacks strong connection to visual content, particularly when visual inputs are projected into language space without sufficient grounding. This misalignment can lead to hallucinated conclusions, superficial associations, and inaccurate spatial or relational reasoning [85, 92]. For instance, when presented with an image of green banana, an LLM may incorrectly respond that the banana is yellow, reflecting linguistic bias rather than perceptual evidence. Compositional approaches, while modular in structure, are not fully immune to hallucination and unfaithfulness. Models can produce plausible yet unsupported intermediate steps or final answers, especially when reasoning components operate on incomplete or poorly grounded visual information [76, 101, 111]. These issues may arise from semantic mismatches between visual inputs and symbolic reasoning, limited supervision during grounding, or an overdependence on prior knowledge encoded in the LLM [42, 44]. Furthermore, shortcut learning persists even in multi-step pipelines, where models exploit spurious correlations within decomposed tasks rather than reason from visual evidence. These issues limit generalization, decrease interpretability, and reduce confidence in system outputs, highlighting the need for improved alignment, step-level supervision, and stronger grounding mechanisms in compositional reasoning frameworks [94, 168]. Potential Directions. Given the ambiguity and context-dependence of real-world CVR tasks, incorporating interactive frameworks with human-in-the-loop supervision holds promise. Allowing users to guide, verify, or revise intermediate reasoning steps can enhance both system reliability and user trust. Such interaction also supports adaptive learning by providing feedback signals that reinforce faithful reasoning. 6.2.3. Bias Toward Deductive Reasoning Most current compositional visual reasoning systems predominantly rely on deductive reasoning, where inference steps follow formal logic. However, this approach assumes the correctness of initial premises and can produce erroneous conclusions when the inputs are noisy or biased [26, 249]. For example, system might conclude that an object is chair because it has four legs and backrest, even if the object is actually sculpture, due to flawed visual grounding. Potential Directions. To improve robustness and adaptability, alternative reasoning paradigms offer promising directions. Inductive reasoning enables models to generalize patterns from visual observationssuch as learning affordances from multiple object instances [250, 251]. Abductive reasoning supports generating plausible explanations when visual input is ambiguous, for instance, inferring that spilled drink on the floor suggests tipped-over glass nearby [252, 253]. Analogical 20 reasoning facilitates relational transfer, such as applying knowledge about assembling furniture from diagrams to configuring unfamiliar mechanical parts in robotics task, where similar spatial relations and assembly logic are required [254, 255]. 6.2.4. Data Limitations and Scalability Challenges Progress in compositional visual reasoning is significantly hindered by data-related constraints. Existing datasets lack sufficient compositional diversity, multi-step supervision, and grounded visual programs, limiting the development of robust reasoning capabilities [256, 257]. High-quality annotations for step-by-step inference are costly and difficult to scale [85, 155, 258], while synthetic pipelines introduce noise due to unreliable perception tools [76, 136]. Supervised fine-tuning on curated CoT data can cause overfitting and cognitive rigidity, restricting generalization [76, 100, 154]. Automatically generated instruction data often lacks reasoning depth and diversity, and even advanced models like GPT-4V can yield unfaithful or inconsistent outputs [111, 168]. Benchmarks such as CLEVR [5] and VQA [182] still emphasize shallow tasks, overlooking multi-step reasoning [94]. Potential Directions. Addressing data scarcity and scalability challenges requires the development of more efficient and grounded data construction strategies. One promising direction is to build hybrid data engines that combine synthetic scene generation with real-world imagery, augmented by perception-enhanced simulation environments [259, 260]. These platforms can generate richly annotated multi-step reasoning traces with minimal manual effort. Additionally, leveraging weak supervision, self-training, and feedback-driven refinement can mitigate annotation costs while improving fidelity and reasoning diversity. Instruction tuning with human-in-the-loop filtering or discrepancy-aware self-verification can further enhance data quality [76, 89, 261]. 6.2.5. Tool Integration and Architectural Bottlenecks Compositional visual reasoning faces architectural challenges in the integration of LLMs with vision modules and external tools. Current systems often lack tool awareness, incur high computational costs during tool use, and struggle with replanning based on intermediate results [29, 74, 76, 89]. Vision encoders introduce information bottlenecks by projecting low-resolution inputs into limited token spaces [64, 76, 169], while fragmented pipelines that treat static and dynamic content separately weaken cross-modal grounding [100, 136, 140, 144]. Tool learning is hindered by retrieval constraints, inconsistent tool formats, limited generalization to unseen tools, and underdeveloped multimodal capabilities [74, 262]. Balancing reasoning depth with efficiency remains an open issue [263]. Potential Directions. Future research should explore more integrated architectures that support tighter coupling between perception, reasoning, and tool execution. Promising directions include improving tool-awareness, designing efficient planning and re-planning strategies, enhancing cross-modal grounding, and supporting scalable generalization to unseen tools. 6.2.6. Benchmark Limitations and Evaluation Contamination Current benchmarks often overestimate model capabilities by relying on in-distribution data and biased annotations [191, 264]. Models frequently exploit linguistic shortcuts instead of engaging in faithful visual reasoning [40, 261]. Evaluation contamination arises from synthetic queries, artificial distribution shifts, and browsing-enabled LLMs, which obscure reasoning failures [132, 241]. particularly critical gap lies in the evaluation of compositional visual reasoning models. Although these methods emphasize explicit intermediate reasoning steps, current benchmarks typically assess performance based solely on final answers, without evaluating the quality or faithfulness of the intermediate reasoning process. This evaluation limitation undermines interpretability and hampers progress toward robust, generalizable visual reasoning [265]. Potential Directions. Future progress relies on the development of more nuanced evaluation protocols that move beyond final answer accuracy. Promising directions include difficulty-aware scoring, assessment of intermediate reasoning steps, and multimodal explanation evaluation. Moreover, extending benchmarks to include richer visual compositions, multi-hop reasoning trajectories, and step-level annotations will be crucial for advancing robust and generalizable compositional visual reasoning [48]. 7. Conclusion In this survey, we focus on recent advances in compositional visual reasoning (CVR), rapidly emerging research direction that emphasizes structured, multi-step reasoning over visual inputs. Unlike monolithic vision-language models that directly map input to output in single pass, CVR frameworks aim to explicitly bridge perception and reasoning through intermediate, interpretable steps. We highlight the key distinctions between these two paradigms, illustrating why compositional reasoning is essential for achieving robust and cognitively aligned intelligence. 21 This survey is organized around four core questions: Why is compositional visual reasoning necessary? What are the major architectural shifts and reasoning paradigms in this field? What benchmarks and evaluation protocols are currently used? What are the prevailing limitations and open challenges? To answer these questions, we reviewed over 260 papers and distilled the field into five major stages of development: prompt-enhanced language-centric methods, tool-enhanced LLMs, tool-enhanced VLMs, chain-of-thought VLMs, and unified agentic VLM-based frameworks. For each stage, we introduce representative works, analyze their design principles, and summarize their strengths and limitations. We also provide roadmap that charts the evolution of CVR systems and offers insights for future research. In addition, we discuss benchmark datasets and evaluation metrics, and identify key challenges including visual grounding, hallucination, data scalability, system integration bottlenecks, benchmark limitations and evaluation contamination. We outline potential future directions to address these bottlenecks, including the integration of world models, more faithful reasoning supervision, and cognitively inspired reasoning strategies."
        },
        {
            "title": "We hope this survey provides a comprehensive and structured foundation for researchers and practitioners working toward",
            "content": "the next generation of interpretable, generalizable, and human-aligned visual reasoning systems."
        },
        {
            "title": "References",
            "content": "[1] Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, and Thomas Serre. Benchmark for Compositional Visual Reasoning. Advances in neural information processing systems, 35:2977629788, 2022. 2, 5, 6, 7 [2] Mingyu Zhang, Jiting Cai, Mingyu Liu, Yue Xu, Cewu Lu, and Yong-Lu Li. Take Step Back: Rethinking the Two Stages in Visual Reasoning. In European Conference on Computer Vision, pages 124141. Springer, 2024. 2 [3] Melanie Frappier. The Book of Why: The New Science of Cause and Effect. Science, 361:855 855, 2018. [4] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Inferring and Executing Programs for Visual Reasoning. In Proceedings of the IEEE international conference on computer vision, pages 29892998, 2017. 3, 6, 7 [5] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. CLEVR: In Proceedings of the IEEE conference on Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. computer vision and pattern recognition, pages 29012910, 2017. 2, 3, 7, 17, 18, 21 [6] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. RAVEN: Dataset for Relational and Analogical Visual REasoNing. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 53125322, 2019. 2, 17 [7] Jakob Suchan, Mehul Bhatt, Przemyslaw Andrzej Walega, and Carl P. L. Schultz. Visual Explanation by High-Level Abduction: On Answer-Set Programming Driven Reasoning about Moving Objects. ArXiv, abs/1712.00840, 2017. [8] Chenchen Jing, Yunde Jia, Yuwei Wu, Xinyu Liu, and Qi Wu. Maintaining Reasoning Consistency in Compositional Visual Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50995108, 2022. 2, 6 [9] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2, 3, 5, 6, 7, [10] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966, 2023. 6 [11] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 6, 7, 10 [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: towards general-purpose vision-language models with instruction tuning. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. 6, 7 [13] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. Survey of Multimodel Large Language Models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. 2, 3 [14] Feijuan He, Yaxian Wang, Xianglin Miao, and Xia Sun. Interpretable Visual Reasoning: Survey. Image and Vision Computing, 112:104194, 2021. 2, [15] Dibyanayan Bandyopadhyay, Soham Bhattacharjee, and Asif Ekbal. Thinking Machines: Survey of LLM based Reasoning Strategies. arXiv preprint arXiv:2503.10814, 2025. [16] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards Reasoning Era: Survey of Long Chain-of-Thought for Reasoning Large Language Models. arXiv preprint arXiv:2503.09567, 2025. 2 [17] Yunsheng Ma, Wenqian Ye, Can Cui, Haiming Zhang, Shuo Xing, Fucai Ke, Jinhong Wang, Chenglin Miao, Jintai Chen, Hamid Rezatofighi, et al. Position: Prospective of Autonomous Driving - Multimodal LLMs World Models Embodied Intelligence AI Alignment and Mamba. In Proceedings of the Winter Conference on Applications of Computer Vision, pages 10101026, 2025. 3 [18] Zhixi Cai, Cristian Rojas Cardenas, Kevin Leo, Chenyuan Zhang, Kal Backman, Hanbing Li, Boying Li, Mahsa Ghorbanali, Stavya Datta, Lizhen Qu, et al. NEUSIS: Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions. arXiv preprint arXiv:2409.10196, 2024. [19] Jorge Fernandez Galarreta, Norman Kerle, and Markus Gerke. UAV-based Urban Structural Damage Assessment Using Objectbased Image Analysis and Semantic Reasoning. Natural hazards and earth system sciences, 15(6):10871101, 2015. [20] Erika Rogers. Study of Visual Reasoning in Medical Diagnosis. In Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society, pages 213218. Routledge, 2019. [21] Li-Ming Zhan, Bo Liu, Lu Fan, Jiaxin Chen, and Xiao-Ming Wu. Medical Visual Question Answering via Conditional Reasoning. In Proceedings of the 28th ACM International Conference on Multimedia, pages 23452354, 2020. [22] Renhao Wang, Jiayuan Mao, Joy Hsu, Hang Zhao, Jiajun Wu, and Yang Gao. Programmatically Grounded, Compositionally Generalizable Robotic Manipulation. arXiv preprint arXiv:2304.13826, 2023. 3 23 [23] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: In Proceedings of the IEEE/CVF Conference on Probing Vision and Language Models for Visio-Linguistic Compositionality. Computer Vision and Pattern Recognition, pages 52385248, 2022. 3 [24] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It? In The Eleventh International Conference on Learning Representations, 2023. 3 [25] Kushal Kafle, Robik Shrestha, and Christopher Kanan. Challenges and Prospects in Vision and Language Research. Frontiers in Artificial Intelligence, 2, 2019. [26] Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, and Hongxia Yang. Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): Comprehensive Survey on Emerging Trends in Multimodal Reasoning. arXiv preprint arXiv:2401.06805, 2024. 4, 5, 6, 20 [27] Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, and Zhiding Yu. Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1426814280, 2025. 15 [28] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 3 [29] Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, and Hamid Rezatofighi. HYDRA: Hyper Agent for Dynamic Compositional Visual Reasoning. In European Conference on Computer Vision, pages 132149. Springer, 2024. 3, 6, 7, 11, 21 [30] Aleksandar Stanic, Sergi Caelles, and Michael Tschannen. Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. Transactions on Machine Learning Research, 2024. 6, 7 [31] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal Chain-ofThought Reasoning: Comprehensive Survey. arXiv preprint arXiv:2503.12605, 2025. 3, 4, 6 [32] Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and Volker Tresp. Enhancing Multimodal Compositional Reasoning of Visual In Proceedings of the IEEE/CVF Winter Conference on Applications of Language Models With Generative Negative Mining. Computer Vision, pages 55635573, 2024. [33] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 91104, 2025. [34] Yiqun Yao, Jiaming Xu, Feng Wang, and Bo Xu. Cascaded Mutual Modulation for Visual Reasoning. In Conference on Empirical Methods in Natural Language Processing, 2018. 3 [35] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional Chain-of-Thought Prompting for Large Multimodal Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431, 2024. 3, 13 [36] Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building Machines That Learn and Think Like People. Behavioral and brain sciences, 40:e253, 2017. 3, 6 [37] Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta Module Network for Compositional Visual Reasoning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 655664, 2021. 5, 6 [38] Wenfeng Zheng, Xiangjun Liu, Xubin Ni, Lirong Yin, and Bo Yang. Improving Visual Reasoning Through Semantic Representation. IEEE access, 9:9147691486, 2021. [39] Joy Hsu, Jiayuan Mao, Josh Tenenbaum, and Jiajun Wu. Whats Left? Concept Grounding with Logic-Enhanced Foundation Models. Advances in Neural Information Processing Systems, 36:3879838814, 2023. 3, 11 [40] Byeong Su Kim, Jieun Kim, Deokwoo Lee, and Beakcheol Jang. Visual Question Answering: Survey of Methods, Datasets, Evaluation, and Challenges. ACM Computing Surveys, 57(10):135, 2025. 3, 4, 17, 18, 21 [41] Sruthy Manmadhan and Binsu Kovoor. Visual Question Answering: State-of-the-Art Review. Artificial Intelligence Review, 53(8):57055745, 2020. [42] Jiayi Kuang, Ying Shen, Jingyou Xie, Haohao Luo, Zhe Xu, Ronghao Li, Yinghui Li, Xianfeng Cheng, Xika Lin, and Yu Han. Natural Language Understanding and Inference with MLLM in Visual Question Answering: Survey. ACM Computing Surveys, 57(8):136, 2025. 3, 4, 5, 20 [43] Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, and Jitao Sang. Mind with Eyes: from Language Reasoning to Multimodal Reasoning. arXiv preprint arXiv:2503.18071, 2025. 3, 4, [44] Jaleed Khan, Filip Ilievski, John Breslin, and Edward Curry. survey of neurosymbolic visual reasoning with scene graphs and common sense knowledge. Neurosymbolic Artificial Intelligence, 1:NAI240719, 2025. 3, 4, 6, 20 [45] Mikołaj Małkinski and Jacek Mandziuk. Deep Learning Methods for Abstract Visual Reasoning: Survey on Ravens Progressive Matrices. ACM Computing Surveys, 57(7):136, 2025. 3, 4, 5 24 [46] Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large Multimodal Agents: Survey. arXiv preprint arXiv:2402.15116, 2024. 3, 4 [47] Md Farhan Ishmam, Md Sakib Hossain Shovon, Muhammad Firoz Mridha, and Nilanjan Dey. From image to language: critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities. Information Fusion, 106:102270, 2024. 4, 5, 7 [48] Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, and Junzhou Zhao. Robust Visual Question Answering: Datasets, Methods, and Future Challenges. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 4, 6, 21 [49] Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, et al. Perception, reason, think, and plan: survey on large multimodal reasoning models. arXiv preprint arXiv:2505.04921, 2025. 4, 5 [50] Shezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, Weimin Zhang, and Meng Wang. How to Bridge the Gap Between Modalities: Survey on Multimodal Large Language Model. IEEE Transactions on Knowledge and Data Engineering, 2025. 4 [51] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8):198343, 2025. [52] Federico Cocchi, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Augmenting Multimodal LLMs with In Proceedings of the Computer Vision and Pattern Self-Reflective Tokens for Knowledge-based Visual Question Answering. Recognition Conference, pages 91999209, 2025. 5 [53] Chao Zhang, Zichao Yang, Xiaodong He, and Li Deng. Multimodal Intelligence: Representation Learning, Information Fusion, and Applications. IEEE Journal of Selected Topics in Signal Processing, 14(3):478493, 2020. 5 [54] Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, and Xueqi Cheng. Transformation Driven Visual Reasoning. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 69036912, 2021. [55] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 5 [56] Wentao He, Jianfeng Ren, Ruibin Bai, and Xudong Jiang. Two-stage Rule-induction visual reasoning on RPMs with an application to video prediction. Pattern Recognition, 160:111151, 2025. 5 [57] Wentao He, Jialu Zhang, Jianfeng Ren, Ruibin Bai, and Xudong Jiang. Hierarchical ConViT with Attention-Based Relational Reasoner for Visual Analogical Reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2230, 2023. 5 [58] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. 5 [59] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-Modal Factorized Bilinear Pooling With Co-Attention Learning for Visual Question Answering. In Proceedings of the IEEE international conference on computer vision, pages 18211830, 2017. 5, 6 [60] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear Attention Networks. Advances in neural information processing systems, 31, 2018. 6 [61] Duy-Kien Nguyen and Takayuki Okatani. Attention for Visual Question Answering. pages 60876096, 2018. 5 Improved Fusion of Visual and Language Representations by Dense Symmetric CoIn Proceedings of the IEEE conference on computer vision and pattern recognition, [62] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep Modular Co-Attention Networks for Visual Question Answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 62816290, 2019. 6 [63] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. Advances in neural information processing systems, 32, 2019. 6 [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6, 21 [65] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In European conference on computer vision, pages 104120. Springer, 2020. [66] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: Visual Language Model for Few-Shot Learning. Advances in neural information processing systems, 35:2371623736, 2022. 6, 7, 13 [67] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. In The Twelfth International Conference on Learning Representations, 2024. 6 [68] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Joshua Adrian Cahyono, Jingkang Yang, Chunyuan Li, and Ziwei IEEE Transactions on Pattern Analysis and Machine Liu. Otter: Multi-Modal Model With In-Context Instruction Tuning. Intelligence, 2025. 6, 7 [69] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. CREPE: Can Vision-Language Foundation Models Reason Compositionally? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1091010921, 2023. 6, 16 [70] Max Cresswell. Logics and Languages. Routledge, 2016. 6 [71] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From Recognition to Cognition: Visual Commonsense Reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67206731, 2019. 6, 7, 17 [72] Tanmay Gupta and Aniruddha Kembhavi. Visual Programming: Compositional Visual Reasoning without Training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1495314962, 2023. 7, 11 [73] Drew Hudson and Christopher Manning. GQA: New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 7, 14, 16, [74] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. Advances in Neural Information Processing Systems, 36, 2024. 6, 11, 21 [75] Huaizu Jiang, Xiaojian Ma, Weili Nie, Zhiding Yu, Yuke Zhu, and Anima Anandkumar. Bongard-HOI: Benchmarking Few-Shot In Proceedings of the IEEE/CVF conference on computer vision and pattern Visual Reasoning for Human-Object Interactions. recognition, pages 1905619065, 2022. 6, 7 [76] Fucai Ke, Xingjian Leng, Zhixi Cai, Zaid Khan, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi, Manmohan Chandraker, et al. DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow Generation & Instruct-Masking Tuning. arXiv preprint arXiv:2503.19263, 2025. 6, 11, 16, 20, 21 [77] Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and Ranjay Krishna. Iterated Learning Improves Compositionality in Large Vision-Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1378513795, June 2024. 6 [78] Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL. arXiv preprint arXiv:2505.15436, 2025. 6, 14, 15, 16, [79] Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, et al. Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 90509061, 2025. [80] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Can Large Vision-Language Models Correct Semantic Grounding Errors By Themselves? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1466714678, 2025. 6 [81] Zhi Hou, Xiaojiang Peng, Yu Qiao, and Dacheng Tao. Visual Compositional Learning for Human-Object Interaction Detection. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XV 16, pages 584600. Springer, 2020. 6 [82] Austin Stone, Hagen Soltau, Robert Geirhos, Xi Yi, Ye Xia, Bingyi Cao, Kaifeng Chen, Abhijit Ogale, and Jonathon Shlens. Learning Visual Composition through Improved Semantic Guidance. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 37403750, 2025. [83] Andy Zeng, Maria Attarian, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, et al. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. In The Eleventh International Conference on Learning Representations, 2023. [84] Jae Sung Park, Zixian Ma, Linjie Li, Chenhao Zheng, Cheng-Yu Hsieh, Ximing Lu, Khyathi Chandu, Quan Kong, Norimasa Kobori, Ali Farhadi, Yejin Choi, and Ranjay Krishna. Synthetic Visual Genome: Dense Scene Graphs at Scale with Multimodal Language Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [85] Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Weimo Deng, Ziyong Feng, Yongle Zhao, and Yin Xie. Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models. arXiv preprint arXiv:2403.19322, 2024. 7, 12, 13, 20, 21 [86] Joy Hsu, Jiayuan Mao, and Jiajun Wu. DisCo: Improving Compositional Generalization in Visual Reasoning through Distribution Coverage. Transactions on Machine Learning Research, 2023. 7 [87] Shi Chen and Qi Zhao. Divide and Conquer: Answering Questions With Object Factorization and Compositional Reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67366745, 2023. [88] Zejun Li, Ruipu Luo, Jiwen Zhang, Minghui Qiu, Xuanjing Huang, and Zhongyu Wei. VoCoT: Unleashing Visually Grounded In Proceedings of the 2025 Conference of the Nations of the Americas Multi-Step Reasoning in Large Multi-Modal Models. Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3769 3798, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. 7, 14, 19 [89] Zaid Khan, Vijay Kumar BG, Samuel Schulter, Yun Fu, and Manmohan Chandraker. Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1434414353, 2024. 7, 11, 21 26 [90] Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. From the Least to the Most: Building Plug-and-Play Visual Reasoner via Data Synthesis. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 49414957, 2024. 12 [91] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers. arXiv preprint arXiv:2506.23918, 2025. 7 [92] Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models. arXiv preprint arXiv:2405.13872, 2024. 7, 12, 18, 20 [93] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. [94] Qiong Wu, Xiangcong Yang, Yiyi Zhou, Chenxin Fang, Baiyang Song, Xiaoshuai Sun, and Rongrong Ji. Grounded Chain-ofThought for Multimodal Large Language Models. arXiv preprint arXiv:2503.12799, 2025. 7, 14, 19, 20, 21 [95] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1704417068. Curran Associates, Inc., 2024. 7, [96] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. BLINK: Multimodal Large Language Models Can See but Not Perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. 7 [97] Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. ConceptMix: Compositional Image Generation Benchmark with Controllable Difficulty. Advances in Neural Information Processing Systems, 37:8600486047, 2024. 7 [98] Seung Wook Kim, Makarand Tapaswi, and Sanja Fidler. Visual Reasoning by Progressive Module Networks. In International Conference on Learning Representations, 2018. [99] Emily Jin, Joy Hsu, and Jiajun Wu. Predicate Hierarchies Improve Few-Shot State Classification. arXiv preprint arXiv:2502.12481, 2025. 7 [100] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95909601, 2024. 7, 14, 16, 21 [101] Yufei Zhan, Hongyin Zhao, Yousong Zhu, Shurong Zheng, Fan Yang, Ming Tang, and Jinqiao Wang. Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models. arXiv preprint arXiv:2505.20753, 2025. 7, 14, 16, 19, 20 [102] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191, 2024. [103] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing Unified Representation for Variety of Vision Tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. [104] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge, January 2024. 15 [105] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [106] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. DeepSeek-VL: Towards Real-World Vision-Language Understanding. arXiv preprint arXiv:2403.05525, 2024. [107] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. [108] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. arXiv preprint arXiv:2304.14178, 2023. 7 [109] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. SpatialVLM: Endowing VisionIn Proceedings of the IEEE/CVF Conference on Computer Vision and Language Models with Spatial Reasoning Capabilities. Pattern Recognition, pages 1445514465, 2024. 8 [110] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual Spatial Reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. [111] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations. arXiv preprint arXiv:2402.04236, 2024. 8, 15, 16, 19, 20, 21 27 [112] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program Synthesis with Large Language Models. arXiv preprint arXiv:2108.07732, 2021. 8 [113] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [114] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021. 8 [115] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. Advances in Neural Information Processing Systems, 36:51685191, 2023. 10 [116] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny. ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions. Transactions on Machine Learning Research, 2024. 10 [117] Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1128911303, 2023. 10, [118] Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou, et al. Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1755317563, 2024. 10 [119] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Large Language Models are Visual Reasoning Coordinators. Advances in Neural Information Processing Systems, 36:7011570140, 2023. 10 [120] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah Smith, and Jiebo Luo. PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29632975, 2023. 10 [121] Zhuosheng Zhang, Aston Zhang, Mu Li, hai zhao, George Karypis, and Alex Smola. Multimodal Chain-of-Thought Reasoning in Language Models. Transactions on Machine Learning Research, 2024. 10 [122] Jiannan Huang, Mengxue Qu, Longfei Li, and Yunchao Wei. AdGPT: Explore Meaningful Advertising with ChatGPT. ACM Trans. Multimedia Comput. Commun. Appl., 21(4), April 2025. 10 [123] Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, and Heng Ji. Visually Descriptive Language Model for Vector Graphics Reasoning. arXiv preprint arXiv:2404.06479, 2024. 10 [124] Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, and Yuxin Peng. Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models. In The Thirteenth International Conference on Learning Representations, 2025. 10 [125] Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Zhiqing Sun, Dan Gutfreund, and Chuang Gan. Visual Chain-ofIn Proceedings of the AAAI Conference on Artificial Intelligence, Thought Prompting for Knowledge-Based Visual Reasoning. volume 38, pages 12541262, 2024. 10, 19 [126] Yaoyao Zhong, Mengshi Qi, Rui Wang, Yuhan Qiu, Yang Zhang, and Huadong Ma. VIoTGPT: Learning to Schedule Vision Tools Towards Intelligent Video Internet of Things. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1068010688, 2025. 11 [127] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction. Advances in Neural Information Processing Systems, 36:7199572007, 2023. 11, 13 [128] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. arXiv preprint arXiv:2303.04671, 2023. 11 [129] Didac Suris, Sachit Menon, and Carl Vondrick. ViperGPT: Visual Inference via Python Execution for Reasoning. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1185411864, 2023. 11 [130] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. 11 [131] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets. In The Twelfth International Conference on Learning Representations, 2024. 11 [132] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. arXiv preprint arXiv:2303.11381, 2023. 11, 21 [133] Ruoyue Shen, Nakamasa Inoue, Dayan Guan, Rizhao Cai, Alex Kot, and Koichi Shinoda. ContextualCoder: Adaptive In-context Prompting for Programmatic Visual Question Answering. IEEE Transactions on Multimedia, 2025. 11 [134] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language. arXiv preprint Qingyun Li, et al. arXiv:2305.05662, 2023. [135] Joy Hsu, Gabriel Poesia, Jiajun Wu, and Noah Goodman. Can Visual Scratchpads With Diagrammatic Abstractions Augment LLM Reasoning? In Proceedings on, pages 2128. PMLR, 2023. 11 28 [136] Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. CLOVA: Closed-LOop Visual Assistant with Tool Usage and Update. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1325813268, 2024. 11, 16, 21 [137] Zhixi Cai, Fucai Ke, Simindokht Jahangard, Maria Garcia de la Banda, Reza Haffari, Peter Stuckey, and Hamid Rezatofighi. NAVER: Neuro-Symbolic Compositional Automaton for Visual Grounding with Explicit Logic Reasoning. arXiv preprint arXiv:2502.00372, 2025. 11, 18 [138] Sadanand Modak, Noah Tobias Patton, Isil Dillig, and Joydeep Biswas. SYNAPSE: SYmbolic Neural-Aided Preference Synthesis Engine. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2752927537, 2025. 11 [139] Artemis Panagopoulou, Honglu Zhou, Silvio Savarese, Caiming Xiong, Chris Callison-Burch, Mark Yatskar, and Juan Carlos Niebles. ViUniT: Visual Unit Tests for More Robust Visual Programming. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2464624656, 2025. 11 [140] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents. In European Conference on Computer Vision, pages 126 142. Springer, 2024. 12, 16, 21 [141] Zhuowan Li, Bhavan Jasani, Peng Tang, and Shabnam Ghadar. Synthesize Step-by-Step: Tools Templates and LLMs as Data Generators for Reasoning-Based Chart VQA. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1361313623, 2024. 12 [142] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning. arXiv preprint arXiv:2505.08617, 2025. 12 [143] Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use. arXiv preprint arXiv:2505.19255, 2025. 12, [144] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing. arXiv preprint arXiv:2412.19806, 2024. 12, 21 [145] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal Foundation Models: From Specialists to General-Purpose Assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1 214, 2024. 12, 13 [146] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual Sketchpad: Sketching as Visual Chain of Thought for Multimodal Language Models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 13 [147] Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, and Eric Nyberg. Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination. ArXiv, abs/2401.08025, 2024. [148] Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, In The 2025 Conference on Empirical Methods in Caiming Xiong, et al. LATTE: Learning to Reason with Vision Specialists. Natural Language Processing, 2025. 13 [149] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17021713, 2025. 13 [150] Penglin Cai, Chi Zhang, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. Creative Agents: Empowering Agents with Imagination for Creative Tasks. In The 41st Conference on Uncertainty in Artificial Intelligence, 2025. 13 [151] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. OpenAI o1 System Card. arXiv preprint arXiv:2412.16720, 2024. 13, 14 [152] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. 13, 14 [153] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. LLaVA-CoT: Let Vision Language Models Reason Step-byStep, 2025. 13 [154] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning. arXiv preprint arXiv:2503.20752, 2025. 14, [155] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models. arXiv preprint arXiv:2503.06749, 2025. 14, 19, 21 [156] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv:2402.03300, 2024. 14 [157] Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek Wong, Xiaoyi Feng, and Maosong Sun. DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding. arXiv preprint arXiv:2503.12797, 2025. 14 29 [158] Liang Chen, Hongcheng Gao, Tianyu Liu, Zhiqi Huang, Flood Sung, Xinyu Zhou, Yuxin Wu, and Baobao Chang. G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning. arXiv preprint arXiv:2505.13426, 2025. [159] Meng Cao, Haoze Zhao, Can Zhang, Xiaojun Chang, Ian Reid, and Xiaodan Liang. Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning. arXiv preprint arXiv:2505.20272, 2025. 14, 16, 19, 20 [160] Teng Xiao, Yige Yuan, Mingxiao Li, Zhengyu Chen, and Vasant Honavar. On Connection Between Imitation Learning and RLHF. arXiv preprint arXiv:2503.05079, 2025. 14 [161] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. In The Twelfth International Conference on Learning Representations, 2024. [162] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training. arXiv preprint arXiv:2501.17161, 2025. 14 [163] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: Visual Question Answering Benchmark In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages Requiring External Knowledge. 31953204, 2019. 14, 17, 18 [164] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: Benchmark for Visual Question Answering Using World Knowledge. In European conference on computer vision, pages 146162. Springer, 2022. 14, 17, 18 [165] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception Tokens Enhance Visual Reasoning in Multimodal Language Models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 38363845, 2025. 14 [166] Yu Cheng, Arushi Goel, and Hakan Bilen. Visually Interpretable Subtask Reasoning for Visual Question Answering. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 27602780, 2025. 14 [167] Xiaoyi Bao, Siyang Sun, Shuailei Ma, Kecheng Zheng, Yuxin Guo, Guosheng Zhao, Yun Zheng, and Xingang Wang. CoReS: Orchestrating the Dance of Reasoning and Segmentation. In European Conference on Computer Vision, pages 187204. Springer, 2024. 14, [168] Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. Visual Agents as Fast and Slow Thinkers. In The Thirteenth International Conference on Learning Representations, 2018. 15, 16, 19, 20, 21 [169] Penghao Wu and Saining Xie. V?: Guided Visual Search as Core Mechanism in Multimodal LLMs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 15, 16, 17, 18, 19, 21 [170] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, Conquer and Combine: Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 79077915, 2025. 15, 16, 17, 19 [171] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment Anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [172] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 15 [173] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. ZoomEye: EnarXiv preprint hancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration. arXiv:2411.16044, 2024. 15, 16, 19 [174] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual CoT: Advancing Multi-Modal Language Models with Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. 15 [175] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-V: Exploring LongChain Visual Reasoning with Multimodal Large Language Models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 90629072, 2025. 15 [176] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. DeepEyes: Incentivizing Thinking with Images via Reinforcement Learning. ArXiv, abs/2505.14362, 2025. 15 [177] Ziyu Ma, Shutao Li, Bin Sun, Jianfei Cai, Zuxiang Long, and Fuyan Ma. GeReA: Question-Aware Prompt Captions for Knowledgebased Visual Question Answering. arXiv preprint arXiv:2402.02503, 2024. 15 [178] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens. ArXiv, abs/2506.17218, 2025. 16 [179] Yilin Wu, Ran Tian, Gokul Swamy, and Andrea Bajcsy. From Foresight to Forethought: VLM-in-the-loop policy steering via latent alignment. In ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2025. 16 30 [180] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA: Reasoning Segmentation via Large Language Model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 16, 17 [181] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5773057754. PMLR, 2127 Jul 2024. 16, 18 [182] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. 16, 21 [183] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [184] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 16 [185] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International journal of computer vision, 123(1):3273, 2017. 16 [186] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7W: Grounded Question Answering in Images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49955004, 2016. 16 [187] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Dont Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49714980, 2018. 16 [188] Manoj Acharya, Kushal Kafle, and Christopher Kanan. TallyQA: Answering Complex Counting Questions. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 80768084, 2019. 16 [189] Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan Plummer, Ranjay Krishna, and Kate Saenko. Cola: Benchmark for Compositional Text-to-image Retrieval. Advances in Neural Information Processing Systems, 36:4643346445, 2023. 16 [190] Joy Hsu, Jiayuan Mao, Joshua Tenenbaum, Noah Goodman, and Jiajun Wu. What Makes Maze Look Like Maze? In International Conference on Learning Representations (ICLR), 2025. 16 [191] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality. Advances in neural information processing systems, 36:3109631116, 2023. 16, 21 [192] Raihan Kabir, Naznin Haque, Md Saiful Islam, et al. Comprehensive Survey on Visual Question Answering Datasets and Algorithms. arXiv preprint arXiv:2411.11150, 2024. 17 [193] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural Module Networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3948, 2016. 17 [194] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task Me Anything. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 17 [195] Francois Fleuret, Ting Li, Charles Dubout, Emma Wampler, Steven Yantis, and Donald Geman. Comparing Machines and Humans on Visual Categorization Test. Proceedings of the National Academy of Sciences, 108:17621 17625, 2011. 17 [196] Dzmitry Bahdanau, Harm de Vries, Timothy J. ODonnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron C. Courville. CLOSURE: Assessing Systematic Generalization of CLEVR Models. ArXiv, abs/1912.05783, 2019. [197] Ramakrishna Vedantam, Arthur Szlam, Maximillian Nickel, Ari Morcos, and Brenden Lake. CURI: Benchmark for Productive Concept Learning Under Uncertainty. In International Conference on Machine Learning, pages 1051910529. PMLR, 2021. 17 [198] Runtao Liu, Chenxi Liu, Yutong Bai, and Alan Yuille. CLEVR-Ref+: Diagnosing Visual Reasoning With Referring Expressions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41854194, 2019. 17 [199] Leila Arras, Ahmed Osman, and Wojciech Samek. CLEVR-XAI: Benchmark Dataset for the Ground Truth Evaluation of Neural Network Explanations. Information Fusion, 81:1440, 2022. [200] Zechen Li and Anders Søgaard. QLEVR: Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning. arXiv preprint arXiv:2205.03075, 2022. 17 [201] Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, and Zeynep Akata. CLEVR-X: Visual Reasoning Dataset for Natural Language Explanations. ArXiv, abs/2204.02380, 2022. 17 [202] Satwik Kottur, Jose M. F. Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. CLEVR-Dialog: Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog. In North American Chapter of the Association for Computational Linguistics, 2019. 17 [203] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Multimodal Explanations: Justifying Decisions and Pointing to the Evidence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 87798788, 2018. 17 [204] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? Computer Vision and Image Understanding, 163:90100, 2017. 17 [205] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-Consistency for Robust Visual Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66496658, 2019. 17 [206] Adam Santoro, Felix Hill, David G. T. Barrett, Ari S. Morcos, and Timothy P. Lillicrap. Measuring Abstract Reasoning in Neural Networks. ArXiv, abs/1807.04225, 2018. 17 [207] Andreas Holzinger, Anna Saranti, and Heimo Mueller. KANDINSKYPatternsAn experimental exploration environment for Pattern Analysis and Machine Intelligence. arXiv preprint arXiv:2103.00519, 2021. 17 [208] Weili Nie, Zhiding Yu, Lei Mao, Ankit Patel, Yuke Zhu, and Anima Anandkumar. Bongard-LOGO: New Benchmark for Human-Level Concept Learning and Reasoning. Advances in Neural Information Processing Systems, 33:1646816480, 2020. 17 [209] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. FVQA: Fact-Based Visual Question Answering. IEEE transactions on Pattern Analysis and Machine Intelligence, 40(10):24132427, 2017. 17 [210] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. Explicit Knowledge-based Reasoning for Visual Question Answering. arXiv preprint arXiv:1511.02570, 2015. 17 [211] Difei Gao, Ruiping Wang, S. Shan, and Xilin Chen. CRIC: VQA Dataset for Compositional Reasoning on Vision and Commonsense. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45:55615578, 2019. 17 [212] Olga Kovaleva, Chaitanya Shivade, Satyananda Kashyap, Karina Kanjaria, Joy Wu, Deddeh Ballah, Adam Coy, Alexandros Karargyris, Yufan Guo, David Beymer Beymer, et al. Towards Visual Dialog for Radiology. In Proceedings of the 19th SIGBioMed workshop on biomedical language processing, pages 6069, 2020. 17, 18 [213] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene Text Visual Question Answering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4291 4301, 2019. 17, 18 [214] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. DocVQA: Dataset for VQA on Document Images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 17 [215] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. Diagram is Worth Dozen Images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. 17 [216] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. Dataset of Clinically Generated Visual Questions and Answers About Radiology Images. Scientific data, 5(1):110, 2018. 18 [217] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. PathVQA: 30000+ Questions for Medical Visual Question Answering. arXiv preprint arXiv:2003.10286, 2020. [218] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. VizWiz Grand Challenge: Answering Visual Questions From Blind People. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. 18 [219] Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, et al. WorldCuisines: Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines. arXiv preprint arXiv:2410.12705, 2024. 18 [220] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. 18 [221] Shih-Han Chou, Wei-Lun Chao, Wei-Sheng Lai, Min Sun, and Ming-Hsuan Yang. Visual Question Answering on 360deg Images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 16071616, 2020. 18 [222] Joy Hsu, Jiajun Wu, and Noah Goodman. Geoclidean: Few-Shot Generalization in Euclidean Geometry. Advances in Neural Information Processing Systems, 35:3900739019, 2022. 18 [223] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, In The Twelfth and Jianfeng Gao. MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. International Conference on Learning Representations, 2024. 18 [224] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. MME: Comprehensive Evaluation Benchmark for Multimodal Large Language Models, 2024. 18 [225] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 18 [226] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. SEED-Bench: Benchmarking Multimodal Large Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329913308, June 2024. 18 [227] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is Your Multi-modal Model an All-Around Player? In European conference on computer vision, pages 216233. Springer, 2024. 18 [228] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning, 2023. 18 [229] Vimukthini Pinto, Chathura Gamage, Cheng Xue, Peng Zhang, Ekaterina Nikonova, Matthew Stephenson, and Jochen Renz. NovPhy: Physical Reasoning Benchmark for Open-World AI Systems. Artificial Intelligence, 336:104198, 2024. 18 [230] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3CoT: Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought. arXiv preprint arXiv:2405.16473, 2024. [231] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. VisIT-Bench: Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. 18 [232] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating Object Hallucination in Large Vision-Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. 18 [233] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14375 14385, 2024. 18 [234] Tuo Liang, Zhe Hu, Jing Li, Hao Zhang, Yiren Lu, Yunlai Zhou, Yiran Qiao, Disheng Liu, Jeirui Peng, Jing Ma, et al. WhenYESMeets BUT: Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning? arXiv preprint arXiv:2503.23137, 2025. 18 [235] Vedika Agarwal, Rakshith Shetty, and Mario Fritz. Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 96879695, 2019. 18 [236] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [237] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized Intersection Over Union: Metric and Loss for Bounding Box Regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 658666, 2019. 18 [238] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: Method for Automatic Evaluation of Machine Translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. 18, 19 [239] Chin-Yew Lin. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. 18, 19 [240] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. 18 [241] OpenAI. OpenAI o3 and o4-mini System Cards, 2025. System Cards for OpenAIs o3 and o4-mini models. 18, 21 [242] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with Language Model is Planning with World Model. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 20 [243] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, and Anil Murthy. Position: LLMs cant plan, but can help planning in LLM-modulo frameworks. In Forty-first International Conference on Machine Learning, 2024. [244] Sukai Huang, Nir Lipovetzky, and Trevor Cohn. Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2654226550, 2025. 20 [245] Steven Sloman. The empirical case for two systems of reasoning. Psychological bulletin, 119(1):3, 1996. [246] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and Zhiting Hu. PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. In The Twelfth International Conference on Learning Representations, 2024. 20 33 [247] Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. Survey on Evaluation of LLM-based Agents. arXiv preprint arXiv:2503.16416, 2025. 20 [248] Haoming Li, Zhaoliang Chen, Jonathan Zhang, and Fei Liu. LASP: Surveying the State-of-the-Art in Large Language ModelAssisted AI Planning. arXiv preprint arXiv:2409.01806, 2024. 20 [249] Philip Johnson-Laird. Deductive Reasoning. Annual review of psychology, 50(1):109135, 1999. 20 [250] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander Rush, Bart Van Merrienboer, Armand Joulin, and Tomas Mikolov. Towards AI-Complete Question Answering: Set of Prerequisite Toy Tasks. arXiv preprint arXiv:1502.05698, 2015. 20 [251] Fei Yu, Hongbo Zhang, Prayag Tiwari, and Benyou Wang. Natural Language Reasoning, Survey. ACM Computing Surveys, 56(12):139, 2024. 20 [252] Mohit Prabhushankar and Ghassan AlRegib. Introspective Learning : Two-Stage approach for Inference in Neural Networks. Advances in Neural Information Processing Systems, 35:1212612140, 2022. 20 [253] Dongran Yu, Bo Yang, Da Liu, Hui Wang, and Shirui Pan. Survey on Neural-Symbolic Learning Systems. Neural networks : the official journal of the International Neural Network Society, 166:105126, 2021. 20 [254] Maxwell Crouse, Constantine Nakos, Ibrahim Abdelaziz, and Ken Forbus. Neural Analogical Matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 809817, 2021. 21 [255] Mark Blokpoel, Todd Wareham, W.F.G Pim Haselager, Ivan Toni, and I.J.E.I. van Rooij. Deep Analogical Inference as the Origin of Hypotheses. J. Probl. Solving, 11, 2019. 21 [256] Maitreya Patel, Naga Sai Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, et al. TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives. Advances in neural information processing systems, 37:3273132760, 2024. 21 [257] Xindi Wu, Hee Seung Hwang, Polina Kirichenko, and Olga Russakovsky. COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning. In Synthetic Data for Computer Vision Workshop@ CVPR 2025, 2025. [258] Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, Silvio Savarese, et al. ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models. arXiv preprint arXiv:2412.07012, 2024. 21 [259] Duy Tho Le, Chenhui Gou, Stavya Datta, Hengcan Shi, Ian Reid, Jianfei Cai, and Hamid Rezatofighi. JRDB-PanoTrack: An Openworld Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2232522334, 2024. 21 [260] Mahsa Ehsanpour, Fatemeh Saleh, Silvio Savarese, Ian Reid, and Hamid Rezatofighi. JRDB-Act: Large-Scale Dataset for SpatioIn Proceedings of the IEEE/CVF Conference on Computer Vision and Temporal Action, Social Group and Activity Detection. Pattern Recognition, pages 2098320992, 2022. 21 [261] Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, and Ranjay Krishna. & ms: Benchmark to Evaluate Tool-Use for ulti-step ulti-modal Tasks. In European Conference on Computer Vision, pages 1834. Springer, 2024. [262] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. AssistGPT: General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn. arXiv preprint arXiv:2306.08640, 2023. 21 [263] Xinyu Xi, Hua Yang, Shentai Zhang, Yijie Liu, Sijin Sun, and Xiuju Fu. Lightweight Multimodal Artificial Intelligence Framework for Maritime Multi-Scene Recognition. arXiv preprint arXiv:2503.06978, 2025. 21 [264] Reza Abbasi, Mohammad Hossein Rohban, and Mahdieh Soleymani Baghshah. Deciphering the Role of Representation Disentanglement: Investigating Compositional Generalization in CLIP Models. In European Conference on Computer Vision, pages 3550. Springer, 2024. 21 [265] Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang, and Nanyun Peng. VISCO: Benchmarking FineIn Proceedings of the Computer Vision and Grained Critique and Correction Towards Self-Improvement in Visual Reasoning. Pattern Recognition Conference, pages 95279537, 2025."
        }
    ],
    "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Griffith University",
        "Monash University",
        "Princeton University",
        "Stanford University",
        "University of Washington"
    ]
}