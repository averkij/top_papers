{
    "paper_title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "authors": [
        "Zhixuan Lin",
        "Evgenii Nikishin",
        "Xu Owen He",
        "Aaron Courville"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 0 3 1 2 0 . 3 0 5 2 : r Published as conference paper at ICLR 2025 FORGETTING TRANSFORMER: SOFTMAX ATTENTION WITH FORGET GATE Zhixuan Lin Mila & Universite de Montreal zxlin.cs@gmail.com Xu Owen He MakerMaker AI owen.hexu@gmail.com Evgenii Nikishin Mila & Universite de Montreal evgenii.nikishin@mila.quebec Aaron Courville Mila & Universite de Montreal courvila@mila.quebec"
        },
        {
            "title": "ABSTRACT",
            "content": "An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on longcontext language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformers superior long-context capabilities over recurrent sequence models such as Mamba2, HGRN2, and DeltaNet. We also introduce Pro block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/ forgetting-transformer."
        },
        {
            "title": "INTRODUCTION",
            "content": "Despite the growing interest in reviving recurrent sequence models (Gu et al., 2021; Peng et al., 2021; Yang et al., 2023; Gu & Dao, 2023; Sun et al., 2023; De et al., 2024; Qin et al., 2024b; Dao & Gu, 2024; Peng et al., 2024; Beck et al., 2024; Zhang et al., 2024), these models still underperform the Transformer (Vaswani et al., 2017) in terms of long-context capabilities (Hsieh et al., 2024; Waleffe et al., 2024; Shen et al., 2024; Qin et al., 2024a), likely due to their relatively small fixedsized hidden states (Jelassi et al., 2024). While the Transformer excels in handling long-context information, it lacks an explicit mechanism for forgetting past information in data-dependent way. Such mechanism often implemented as some form of the forget gate (Gers et al., 2000) is ubiquitous in recurrent sequence models and has proven critical in their success in short-context tasks (Greff et al., 2016; Van Der Westhuizen & Lasenby, 2018; Peng et al., 2021; Yang et al., 2023; Gu & Dao, 2023). natural question to ask is then: can we have forget gate in Transformers? To address this question, we leverage an important fact: many recurrent sequence models with forget gate can be written in parallel linear attention form (Katharopoulos et al., 2020) analogous to softmax attention (Yang et al., 2023; Dao & Gu, 2024). In this parallel form, the forget gate mechanism translates into down-weighing the unnormalized attention scores in data-dependent way. Our key insight is that this exact mechanism is also applicable to softmax attention. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). Correspondence to Zhixuan Lin. Work done while at Google DeepMind. 1 Published as conference paper at ICLR 2025 We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Notably, it does not require any positional embeddings. It also retains Transformers long-context retrieval abilities and achieves near-perfect accuracy in the needlein-the-haystack test (Kamradt, 2023) within the training context length. In contrast, all the tested recurrent sequence models fail. We also introduce Pro block design that integrates several architectural components commonly used in recurrent sequence models, which significantly improves the performance of FoX and the baseline Transformer. Finally, we show that FoX can be implemented in hardware-aware way with simple modification to the FlashAttention (Dao, 2023) algorithm."
        },
        {
            "title": "2 BACKGROUND: LINEAR ATTENTION WITH A FORGET GATE",
            "content": "This section introduces the notation used in this work and gives brief background on linear attention. We also introduce gated variant of linear attention and discuss its parallel form, which naturally leads to FoX. Throughout this work, we only consider causal sequence modeling. 2.1 LINEAR ATTENTION Standard causal softmax attention takes sequence of input vectors (xi)L of output vectors (oi)L i=1, where xi, oi Rd, {1, . . . , L}. Each oi is computed as follows: i=1 and produces sequence (cid:80)i qi, ki, vi = Wqxi,Wkxi, Wvxi Rd, j=1 kexp(qi, kj)vj (cid:80)i j=1 kexp(qi, kj) j=1 exp(q (cid:80)i j=1 exp(q kj)vj kj) (cid:80)i = oi = , (1) (2) where Wq, Wk, Wv Rdd are projection matrices and kexp(q, k) = exp(qk) is the exponential dot product kernel.1 Linear attention (Katharopoulos et al., 2020) replaces the exponential dot product kernel kexp(q, k) = exp(qk) with kernel kϕ(q, k) with some feature representation ϕ : Rd (R+)d : oi = (cid:80)i j=1 kϕ(qi, kj)vj (cid:80)i j=1 kϕ(qi, kj) = (cid:80)i j=1(ϕ(qi)ϕ(kj))vj (cid:80)i j=1 ϕ(qi)ϕ(kj) (3) Following Yang et al. (2023), we call this the parallel form of linear attention as it can be computed with matrix multiplications. Alternatively, linear attention can be computed in recurrent form: St = St1 + vtϕ(kt) zt = zt1 + ϕ(kt) Stϕ(qt) ϕ(qt) ot = , (4) (5) (6) where St Rdd , zt Rd , {0, . . . , L} are computed recurrently, with S0 = 0 and zt = 0. 2.2 LINEAR ATTENTION WITH FORGET GATE The recurrent form of linear attention makes it natural to introduce forget gate. Specifically, we xt + bf ) at each timestep, where σ is the sigmoid can compute scalar forget gate ft = σ(w function and wf Rd, bf are learnable parameters. The recurrent computation is then: St = ftSt1 + vtϕ(kt) zt = ftzt1 + ϕ(kt) Stϕ(qt) ϕ(qt) ot = . 1Note we omit the 1 scaling factor to reduce visual clutter. In practice we always scale kj by 1 (7) (8) (9) . Published as conference paper at ICLR 2025 Note that this gated variant of linear attention differs from most models in the literature. In particular, most gated variants of linear attention models, such as GLA (Yang et al., 2023) and Mamba-2 (Dao & Gu, 2024), do not have the normalization term (i.e., there is no zt, and the output is just ot = Stϕ(qt)). We keep the normalization term to maintain similarity with softmax attention. Crucially, similar to the normalization-free version derived in GLA and Mamba-2, we can show that this gated variant of linear attention also has parallel form: oi = (cid:80)i j=1 Fijϕ(qi)ϕ(kj)vj (cid:80)i j=1 Fijϕ(qi)ϕ(kj) = (cid:80)i j=1 Fijkϕ(qi, kj)vj (cid:80)i j=1 Fijkϕ(qi, kj) , (10) where Fij = (cid:81)i l=j+1 fl, with the convention that Fij = 1 if = j. Our key observation is that Equation 10 and the softmax attention in Equation 2 are very similar in form. In fact, if we change the kernel kϕ in Equation 10 back to the exponential dot product kernel kexp, we obtain softmax attention with forget gate. We introduce this formally in the next section."
        },
        {
            "title": "3 FORGETTING TRANSFORMER",
            "content": "Our proposed model, the Forgetting Transformer (FoX), features modified softmax attention mechanism with forget gate. We name this attention mechanism the Forgetting Attention. Similar to the gated variant of linear attention introduced in the previous section, we first compute scalar forget gate ft = σ(w xt + bf ) for each timestep t. The output of the attention is then (cid:80)i (cid:80)i oi = j=1 Fij exp(q (cid:80)i j=1 Fij exp(q l=j+1 fl and dij = (cid:80)i j=1 exp(q (cid:80)i j=1 exp(q l=j+1 log fl. This can be written in matrix form: kj + dij)vj kj + dij) kj)vj kj) = , where Fij = (cid:81)i (11) = log RLL, = softmax(QK + D)V RLd, (13) where RLL is lower triangular matrix whose non-zero entries are Fij, i.e., Fij = Fij if and 0 otherwise. We adopt the convention that log 0 = . Q, K, , RLd are matrices containing qi, ki, vi, oi, {1, . . . , L} as the rows. The softmax operation is applied rowwise. For multi-head attention with heads, we maintain instances of forget gate parameters {w(h) h=1 separately for each head. h=1 and compute the forget gate values {f (h) h=1 and {b(h) }H }H }H (12) Hardware-aware implementation The logit bias form on the rightmost side of Equation 11 can be computed with simple modification to the FlashAttention (Dao, 2023) algorithm. Here we briefly describe the forward pass. The backward pass follows similar idea. First, we compute the cumulative sum ci = (cid:80)i l=1 log fl for {1, . . . , L} and store it in the high-bandwidth memory (HBM) of the GPU. This allows us to compute dij = ci cj easily later. Whenever we compute the attention logit kj in the GPUs fast shared memory (SRAM) (as in FlashAttention), we also load ci and cj to SRAM, compute dij, and add it to the attention logit. The rest of the forward pass remains the same as FlashAttention. This algorithm avoids instantiating the matrix in the HBM. We provide detailed algorithm description in Appendix E. Moreover, since the forget gates are scalars instead of vectors, the additional computation and parameters introduced are negligible. Connection to ALiBi Besides its natural connection to gated linear attention, the Forgetting Attention can also be seen as data-dependent and learnable version of ALiBi (Press et al., 2021). ALiBi applies data-independent bias bij = (i j)mh to the attention logits, where mh is fixed slope specific to each head h. It is easy to show that ALiBi is equivalent to using fixed, head-specific, and data-independent forget gate (h) = exp(mh). In Section 4.5, we verify the superiority of data-dependent forget gates over ALiBi. Positional embeddings Though we find that using Rotary Position Embeddings (RoPE) (Su et al., 2024) sometimes slightly improves the performance of FoX, it is not necessary as it is for the standard Transformer (see ablations in Section 4.5). For simplicity, we do not use RoPE or any other positional embeddings for FoX by default. 3 Published as conference paper at ICLR Figure 1: Default architecture of FoX. (left) single FoX block. (right) single FoX (Pro) layer. All RMSNorms on the right are applied independently to each head. σ is the sigmoid function. is element-wise multiplication. ShiftLinear implements the computation in Equation 14. Architecture design We test FoX with two different architectures. First, we replace RoPE in the LLaMA architecture (Touvron et al., 2023) with forget gates and refer to this model as FoX (LLaMA). Second, we test an improved Pro architecture with output gates2 and output normalization (also used in GLA and Mamba-2). We also use QK-norm (Dehghani et al., 2023) and apply simplified variant of data-dependent token shift (Peng et al., 2024) to the keys and values (KV-shift). Concretely, the keys (ki)L i=1 are computed as follows with additional parameters wk Rd: = σ(w kt1 + (1 αkey kt = Wkxt Rd, αkey kt = RMSNorm(αkey xt) )kt) (14) The values (vi)L is shown in Figure 1 and detailed in Appendix A. We refer to the resulting model as FoX (Pro). i=1 are computed in the same way, but without RMSNorm. The overall architecture"
        },
        {
            "title": "4 EMPIRICAL STUDY",
            "content": "The advantages of Transformers in long-context abilities over recurrent sequence models have been verified multiple times (Hsieh et al., 2024; Waleffe et al., 2024; Shen et al., 2024; Qin et al., 2024a). However, forget gates introduce recency bias. It is thus natural to ask whether FoX still maintains this advantage. Therefore, our empirical study places special focus on long-context capabilities. 4.1 EXPERIMENTAL SETUP Dataset and baselines We focus on long-context language modeling and train all models on LongCrawl64 (Buckman, 2024), long-sequence subset of RedPajama-v2 (Together Computer, 2023) pre-tokenized with the TikToken tokenizer (OpenAI, 2022) for GPT-2 (Radford et al., 2019). For baselines, we focus on two types of comparisons. First, we compare FoX with the Transformer. For the Transformer, we also test both the LLaMA and the Pro architecture (referred to as Transformer (LLaMA) and Transformer (Pro), respectively). Similar to Xiong et al. (2023), we find it crucial to use large RoPE angle θ for the Transformer for long-context training. Following Xiong et al. (2023) we use θ = 500000. Second, to show the advantage of FoX over recurrent sequence models in long-context capabilities, we compare it with Mamba-2 (Dao & Gu, 2024), HGRN2 (Qin et al., 2024a), and DeltaNet (Yang et al., 2024). The implementation of all models is based on the Flash Linear Attention repository (Yang & Zhang, 2024). Training setup For our main experiments, we train models with 760M (non-embedding) parameters on 45 230-token (roughly 48B tokens) subset of LongCrawl64 with training context length 2When output gates are used, we reduce the number of parameters in the MLPs so the total number of parameters remains the same. 4 Published as conference paper at ICLR 2025 Figure 2: (left) Per-token loss L(i) at different token position i. (right) Validation perplexity (l) over different validation context length l. The vertical dashed line indicates the training context length. The per-token loss is smoothed using moving average sliding window of 101 tokens. of 16384 tokens. For the validation set, we use 2 230-token subset of the LongCrawl64 held-out set with sequences of 65536 tokens. We choose longer validation context length than the training context length to test the length extrapolation abilities of the models. All models are trained with AdamW (Loshchilov, 2017) with (β1, β2) = (0.9, 0.95). We use linear learning rate warmup from 0 to the peak learning rate for the first 256 220 tokens and then cosine decay to 0. Each training batch contains 0.5 220 tokens. All models use weight decay of 0.1 and gradient clipping of 1.0. We search the learning rate for each model within {1 10i, 2 10i, 5 10i} for different is until we identify locally optimal value. We tune the head dimensions for FoX and the Transformer in {64, 128}. We find that FoX often prefers higher learning rates and more heads/smaller head dimensions than the Transformer, and the Pro models often prefer higher learning rates than the LLaMA models. Details of the hyperparameters and experimental setup can be found in Appendix B. 4.2 LONG-CONTEXT LANGUAGE MODELING Metrics For our main metric, we use per-token loss on the validation set at different token positions. To be precise, let be the vocabulary size, y(j) {0, 1}V be the one-hot vector encoding the language modeling target for the i-th token in the j-th validation sequence, and p(j) RV be the corresponding output probabilities of the model, then the per-token loss L(i) at token position is defined as L(i) = 1 ], where is the number of validation sequences. j=1 log[(p(j) )y(j) (cid:80)M The per-token loss is particularly meaningful for understanding the long-context capabilities of model. Informally, monotonically decreasing L(i) with steep slope indicates the model is using the full context well. On the other hand, if L(i) plateaus after some token position k, it indicates the model struggles to use tokens that are tokens away from the current token position for its prediction. This correspondence between the slope of L(i) and the models context utilization is explained in more detail in Appendix C. Besides per-token loss, we also report perplexity over different context lengths. Concretely, perplexity (l) over context length is defined as (l) = exp( 1 i=1 L(i)). We warn the readers that the slope of (l) is less meaningful. Since (l) is the exponential of the cumulative average of L(i), even if L(i) plateaus after some token position k, (l) may still keep decreasing after k, giving the wrong impression that the model can make use of the part of the context that is tokens away. (cid:80)l In Figure 2, we show the per-token loss L(i) at different token indices and perplexity Results (l) over different validation context lengths l. As shown in Figure 2, with either architecture, FoX outperforms the standard Transformer both within and beyond (i.e., length extrapolation) the 5 Published as conference paper at ICLR 2025 Figure 3: Visualization of the forget gate weight matrix and the attention score matrix from two heads in different layers. Since is very sparse, we only show entries with scores larger than 0.5. These results use FoX (Pro). More examples can be found in Appendix F.10. training context length. Similar to the Transformer, it maintains monotonically decreasing pertoken loss within the training context length, indicating that it utilizes the entire training context for its prediction. In contrast, the per-token loss curves of all recurrent sequence models start flattening at around 5k tokens and plateau after 10k tokens. This indicates that these recurrent models struggle to use the full context effectively for their prediction. In terms of the absolute values of the loss and perplexity, FoX (Pro) also clearly outperforms HGRN2, DeltaNet and Mamba-2. Visualization of forget gate values and attention map In Figure 3, we visualize the forget gate weight matrix and the attention scores = softmax(QK + log ) from two heads in different layers. The head on the left-hand side exhibits strong decay, and most entries of are close to zero; accordingly, the attention focuses on local entries. The head on the right-hand side has much weaker decay, and the attention is distributed across the entire context. This shows that FoX can learn to retain information across long contexts when necessary. 4.3 NEEDLE IN THE HAYSTACK The needle-in-the-haystack test (Kamradt, 2023) is popular test for the long-context retrieval abilities of language models. Besides the standard mode where the needle only includes the answer to be retrieved, we also use an easy mode (Qin et al., 2024a) where the needle placed in the context includes both the question and the answer. This easy mode is particularly suitable for base models that have not been instruction-tuned. Full details, including the prompts used, are in Appendix B.3. In Figure 4, we show the results for different models. HGRN2 performs even worse than Mamba-2 and we leave it to Appendix F.5. As shown in Figure 4, FoX achieves near-perfect needle retrieval within the training context length in all cases. Transformer (Pro) and Transformer (LLaMA) also have perfect accuracy within the training context length in the easy mode, though they sometimes fail in the standard mode.3 In contrast, Mamba-2 and DeltaNet (and also HGRN2 in Appendix F.5) perform poorly even within the training context length. FoX (Pro), FoX (LLaMA), and Transformer (Pro) also partially extrapolate beyond the training context length. This is expected given their per-token loss pattern beyond the training context length (see Figure 2). However, we find that the extrapolation behaviors of these models could be hyperparameter-dependent. For example, in Figure 5, we show that for FoX (Pro), the needle retrieval results and the per-token loss slope beyond the training context length vary depending on the number of training tokens and learning rates. In particular, we find that more training tokens often leads to worse extrapolation, indicating that the models may be gradually overfitting to their training context length during training. 4.4 DOWNSTREAM TASKS We use two sets of downstream tasks: set of short-context tasks from LM-evaluation-harness (Gao et al., 2024) and set of long-context tasks from LongBench (Bai et al., 2023). 3Note these are small models without instruction-tuning. We expect that with more parameters/training tokens or instruction-tuning Transformers should also have perfect accuracy within the training context length. 6 Published as conference paper at ICLR 2025 Figure 4: Needle-in-the-haystack analysis for different models. We show results for the easy mode on the left and the standard mode on the right. The results are scored on scale of 1 to 10 by GPT4o-2024-08-06. The vertical dashed line indicates the training context length. Figure 5: FoX (Pro) easy mode needle-in-the-haystack results and per-token loss for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length. More results can be found in Appendix F.7. Table 1: Evaluation results on LM-eval-harness. All models have roughly 760M non-embedding parameters and are trained on roughly 48B tokens on LongCrawl64. acc-n means length-normalized accuracy. Bold and underlined numbers indicate the best and the second best results, respectively. Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c COPA OBQA SciQA BoolQ Avg acc ppl acc-n acc-n acc-n acc acc acc acc acc acc ppl FoX (Pro) Transformer (Pro) FoX (LLaMA) Transformer (LLaMA) Mamba-2 HGRN2 DeltaNet 23.04 24.12 26.45 28.14 28.20 30.57 29.17 14.91 16.16 18.27 22.34 21.05 20.14 29.14 42.75 41.47 40.17 38.27 36.50 38.60 34.27 64.09 64.04 63.44 63.22 63.17 63.49 62. 38.39 36.60 35.17 34.20 35.86 34.94 33.28 52.33 49.72 51.78 49.49 50.59 51.78 50.28 52.23 51.98 49.66 47.98 49.96 50.13 47.39 26.54 25.26 25.09 24.49 25.60 25.51 24.32 71.00 62.00 69.00 66.00 71.00 66.00 70.00 29.80 29.20 28.00 29.40 31.00 30.00 29. 85.10 82.80 81.90 78.90 80.90 75.60 74.30 46.57 60.86 54.04 58.93 57.49 58.41 54.37 50.88 50.39 49.82 49.09 50.21 49.45 47.99 7 Published as conference paper at ICLR 2025 Table 2: Evalution results on LongBench. All models have roughly 760M non-embedding parameters and are trained on roughly 48B tokens on LongCrawl64. Bold and underlined numbers indicate the best and the second-best results, respectively. Model FoX (Pro) Transformer (Pro) FoX (LLaMA) Transformer (LLaMA) Mamba-2 HGRN2 DeltaNet Single-Document QA Multi-Document QA Summarization Few-shot Learning Code arrativeQ 13.38 11.42 10.47 11.11 10.65 8.78 9.36 asper 18.88 21.54 14.81 13.5 11.26 10.94 9.76 A 28.73 22.89 24.71 21.52 16.98 18.66 16.49 otpotQ 2 ikiM usique ov Report 15.27 19.58 13.03 9.42 11.59 7.78 6.57 25.39 22.65 21.58 21.33 16.69 15.29 15.09 6.49 6.09 5.25 4.32 5.0 4.32 2.76 22.71 21.92 20.05 18.53 9.31 6.13 8.19 Su 13.51 10.7 10.97 8.43 11.22 12.19 12.3 ultiN ews 12.27 8.11 4.86 10.99 10.89 7.83 7.62 C 63.5 55.0 61.5 51.5 28.5 16.5 35.5 TriviaQ Sa Su 37.36 40.67 34.48 28.41 15.6 14.46 17.57 22.74 30.66 19.13 19.17 16.19 6.37 18.42 10.9 10.79 7.69 8.21 12.07 18.17 12.24 Repo Bench-P 9.1 14.25 8.12 14.06 15.17 16.62 3.94 Figure 6: Per-token loss given different model sizes, numbers of training tokens, and training context lengths. At each token index i, we report the averaged loss over window of 101 centered at i. We only show results within the training context length to reduce visual clutter. See Appendix F.6 for additional results, including length extrapolation and 125M-parameter model results. Short-context tasks We use Wikitext (Merity et al., 2016), LAMBADA (Paperno et al., 2016), PiQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Zellers et al., 2019), ARCeasy, ARC-challenge (Clark et al., 2018), Copa (Roemmele et al., 2011), SciQA (Auer et al., 2023), OpenbookQA (Mihaylov et al., 2018), and BoolQA (Clark et al., 2019). Following Yang et al. (2023), we report perplexity for Wikitext and LAMBADA, length-normalized accuracy for HellaSwag, ARC-challenge, and OpenbookQA, and accuracy for all other tasks (we also report accuracy for LAMBADA). All results are zero-shot. As shown in Table 1, FoX outperforms the Transformer with either architecture. FoX (Pro) performs the best among all models. Long-context tasks We use 14 tasks from LongBench: HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), MultiFieldQA-en, NarrativeQA (Koˇcisk`y et al., 2018), Qasper (Dasigi et al., 2021), GovReport (Huang et al., 2021), QMSum (Zhong et al., 2021), MultiNews (Fabbri et al., 2019), TriviaQA (Joshi et al., 2017), SAMSum (Gliwa et al., 2019), TREC (Li & Roth, 2002), LCC (Guo et al., 2023), and RepoBench-P (Liu et al., 2023). We use the default metrics of LongBench for different tasks, which are either F1, Rough-L, accuracy, or edit similarity. As shown in Table 2, with either architecture, FoX performs on par with the Transformer and better than the recurrent sequence models. 4.5 ANALYSES We present three sets of analyses. First, we study how the advantages of FoX over the Transformer vary with model size and training context length. Second, we investigate the contribution of each component in FoX and how RoPE affects performance. Finally, we study the importance of using forget gate that is data-dependent. For these experiments, we use either 760M-parameter models trained on roughly 16B tokens or 360M-parameter models trained on roughly 7.5B tokens. Experimental details can be found in Appendix B. 8 Published as conference paper at ICLR 2025 Table 3: Ablation experiments for FoX. We use 360M-parameter models trained on 7.5B tokens on LongCrawl64. The perplexity is measured over validation context length of 16384 tokens. For the bottom half, all addition (+) or removal (-) of components are relative to FoX (Pro). Model RoPE Forget gate QK-norm Output gate Output norm KV-shift Perplexity Transformer (LLaMA) w/o RoPE Transformer (LLaMA) FoX (LLaMA) FoX (Pro) - QK-norm - output gate - output norm - KV-shift + RoPE - forget gate + RoPE (i.e. Transformer (Pro)) - forget gate 29.30 7.49 7.19 7.25 7.08 6.88 6.80 6.62 6.79 6.86 6.69 6.80 6.63 6.82 7.40 Figure 7: Data-dependent forget gate (data-dep) vs. data-independent (data-indep) and fixed forget gate. (left and middle-left) Comparison using the LLaMA architecture. (middle-right and right) Comparison using the Pro architecture. We use 360M-parameter models trained on roughly 7.5B tokens on LongCrawl64. All per-token loss curves are smoothed with moving average sliding window of 1001 tokens. The vertical dashed line indicates the training context length. Model size and training context length In Figure 6, we show the per-token loss for two different model sizes (trained on different numbers of tokens) and several training context lengths for FoX (Pro) and Transformer (Pro). As shown in Figure 6, the advantages of FoX over Transformer (1) increase as we increase the training context length and (2) decrease as we increase the model size (and training tokens). This indicates that the advantages of having forget gate might depend on the ratio between the model size and the training context length, as larger models can better model long contexts, and thus forgetting may be less important. We also note that long-context training damages short-context performance, which is known effect (Ren et al., 2024; Sun et al., 2024) likely due to reduced document diversity within training batches. Component analysis We present both (1) an incremental style analysis where we incrementally add/remove components from Transformer (LLaMA) to obtain the complete FoX (Pro) model and (2) perturbation style analysis where we add/remove components from FoX (Pro). The results are shown in Table 3. First, as mentioned previously, adding RoPE to FoX (LLaMA) and FoX (Pro) results in minor and no improvement, respectively. Second, both types of analyses show that all components in FoX contribute positively. Also note that models that use neither forget gates nor RoPE perform poorly (the first and the last row of the table). Data-independent and fixed forget gates To show the importance of using forget gate that is data-dependent, we test data-independent forget gate (h) = σ(b(h)), where the superscript (h) means for the h-th head. We also test forget gate that has fixed values (i.e., (h) = σ(b(h)), but we do not update b(h)). As mentioned in Section 3, using fixed forget gate is equivalent to ALiBi. For these data-independent forget gate designs, we find it crucial to initialize b(h) properly. In particular, we intialize {b(h)}H h=1 for the heads with two hyperparameter Tmin and Tmax such 9 Published as conference paper at ICLR 2025 that using fixed forget gates with (Tmin, Tmax) is equivalent to ALiBi with minimum slope and maximum slope . This initialization method is detailed in Appendix D. 1 Tmin 1 Tmax In Figure 7, we show the per-token loss of different forget gate designs with the LLaMA and the Pro architecture. For the data-independent and the fixed forget gate designs, we set Tmin = 2 and test different values of Tmax. As shown in Figure 7, data-dependent forget gate always has the best performance."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Recurrent sequence models There has been growing interest in reviving recurrent sequence models (Katharopoulos et al., 2020; Peng et al., 2021; Gu et al., 2021; Orvieto et al., 2023; Yang et al., 2023; Gu & Dao, 2023; Katsch, 2023; De et al., 2024; Sun et al., 2024; Peng et al., 2024; Qin et al., 2024a; Dao & Gu, 2024; Beck et al., 2024; Zhang et al., 2024; Buckman et al., 2024). Many recent recurrent sequence models feature some form of the forget gate, which has been shown to be essential in these architectures (Qin et al., 2024b; Gu & Dao, 2023; Yang et al., 2023). Notably, GLA (Yang et al., 2023) and Mamba-2 (Dao & Gu, 2024) show that gated variants of linear attention could be written in form similar to softmax attention, which directly inspired our work. Several works (Ma et al., 2022; 2024; Ren et al., 2024) combine recurrent layers with quadratic attention. However, unlike our method which embeds the forget gate into the attention mechanism in these hybrid architectures, the recurrent layers and the quadratic attention layers are independent. Related improvements and alternatives to softmax attention Several positional embedding methods (Press et al., 2021; Raffel et al., 2020; Chi et al., 2022a;b) add bias terms to the attention logits based on the distances between the keys and queries, which can implement data-independent decay. LAS-attention (Zimerman & Wolf) applies multiplicative exponential decay to the attention logits. RoPE (Su et al., 2024) also has similar decay effect that becomes stronger with increasing relative query/key distances. However, all these methods can only achieve data-independent decay based on the relative distances between the queries and keys. CoPE (Olsson et al., 2022) and Selective Attention (Leviathan et al., 2024) modify the current timesteps attention logit based on the sum of transformed logits from some previous timesteps. Our method differs from these in various aspects. For example, in our approach, there is no need to compute sums of transformed logits, which may come with several issues such as potential incompatibility with FlashAttention. Geometric attention (Csordas et al., 2021) and stick-breaking attention (Tan et al., 2024) use stickbreaking process to compute the attention scores, which has similar data-dependent decay effect to our method. These methods explore in different direction from ours, as they seek to develop alternatives to softmax attention while our approach is only an improvement on softmax attention."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We propose the Forgetting Transformer (FoX), Transformer variant with forget gate. Our experiments show that FoX outperforms the Transformer and several recurrent sequence models on long-context language modeling and various downstream tasks. We also show that our Pro block design greatly outperforms the basic LLaMA architecture, with or without forget gate. We therefore recommend that future work adopt FoX (Pro) and Transformer (Pro) as baselines in addition to the commonly used LLaMA architecture. We discuss several limitations of our work and potential future work. First, due to our limited computing resources, our main experiments only use models up to 760M parameters, 48B tokens, and training context length of 16384 tokens. Thus, an important direction for future work is to test FoX at larger scales. Second, we only consider causal sequence modeling in this work. It would be interesting to extend the Forgetting Attention to the non-causal case. Finally, we could potentially prune computation (e.g., KV-cache eviction) adaptively based on the forget gate values, which may greatly reduce training and inference costs. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "ZL thanks Songlin Yang, Jacob Buckman, Zhen Qin, Artem Zholus, Benjamin Therien, Jonathan Pilault, and Mahan Fathi for their helpful discussion. AC acknowledges funding from Microsoft research. This research was enabled in part by the compute resources, software and technical help provided by Mila (mila.quebec) and the Digital Research Alliance of Canada (alliance. can.ca)."
        },
        {
            "title": "REFERENCES",
            "content": "Soren Auer, Dante AC Barone, Cassiano Bartz, Eduardo Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, et al. The sciqa scientific question answering benchmark for scholarly knowledge. Scientific Reports, 13 (1):7240, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058, 2024. Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Jacob Buckman. Longcrawl64: Long-Context Natural-Language Dataset, 2024. URL https: //manifestai.com/articles/longcrawl64. Jacob Buckman, Carles Gelada, and Sean Zhang. Symmetric Power Transformers, 2024. Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:83868399, 2022a. Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. arXiv preprint arXiv:2212.10356, 2022b. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Robert Csordas, Kazuki Irie, and Jurgen Schmidhuber. The neural data router: Adaptive control flow in transformers improves systematic generalization. arXiv preprint arXiv:2110.07732, 2021. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah Smith, and Matt Gardner. dataset arXiv preprint of information-seeking questions and answers anchored in research papers. arXiv:2105.03011, 2021. 11 Published as conference paper at ICLR 2025 Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pp. 74807512. PMLR, 2023. Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: largescale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749, 2019. FlagOpen, 2023. URL https://github.com/FlagOpen/FlagAttention. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Felix Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. Neural computation, 12(10):24512471, 2000. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: humanannotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237, 2019. Klaus Greff, Rupesh Srivastava, Jan Koutnık, Bas Steunebrink, and Jurgen Schmidhuber. Lstm: search space odyssey. IEEE transactions on neural networks and learning systems, 28(10): 22222232, 2016. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: long-range pretrained language model for code completion. In International Conference on Machine Learning, pp. 1209812107. PMLR, 2023. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021. Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Gregory Kamradt, 2023. URL https://github.com/gkamradt/LLMTest_ NeedleInAHaystack/blob/main/README.md. 12 Published as conference paper at ICLR 2025 Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint arXiv:2311.01927, 2023. Tomaˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Selective attention improves transformer. arXiv preprint arXiv:2410.02703, 2024. Xin Li and Dan Roth. Learning question classifiers. Conference on Computational Linguistics, 2002. In COLING 2002: The 19th International Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022. Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. arXiv preprint arXiv:2404.08801, 2024. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html. OpenAI, 2021. URL https://github.com/triton-lang/triton. OpenAI, 2022. URL https://github.com/openai/tiktoken. Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 2667026698. PMLR, 2023. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. 13 Published as conference paper at ICLR 2025 Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI spring symposium series, 2011. Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for linear complexity language models. arXiv preprint arXiv:2406.16690, 2024. Daria Soboleva, and Nolan Dey. Faisal Al-Khateeb, Robert Myers, tness, plicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Joel Hesdeduand https://www.cerebras.net/blog/ Jacob Steeves, cleaned 627B token SlimPajama: RedPajama. version of Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Shawn Tan, Yikang Shen, Songlin Yang, Aaron Courville, and Rameswar Panda. Stick-breaking attention. arXiv preprint arXiv:2410.17980, 2024. Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Jos Van Der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. arXiv preprint arXiv:1804.04849, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In I. Guyon, U. Von Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. 14 Published as conference paper at ICLR 2025 Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models. arXiv preprint arXiv:2406.07887, 2024. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. Songlin Yang and Yu Zhang. Fla: triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, et al. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146, 2024. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: new benchmark for query-based multi-domain meeting summarization. arXiv preprint arXiv:2104.05938, 2021. Itamar Zimerman and Lior Wolf. Viewing transformers through the lens of long convolutions layers. In Forty-first International Conference on Machine Learning. 15 Published as conference paper at ICLR"
        },
        {
            "title": "Table of Contents",
            "content": "A Detailed FoX (Pro) layer computation Experimental details B.1 Model and training hyperparameters . B.2 Model parameters, estimated FLOPs, and throughput . . B.3 Needle in the haystack details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 17 18 18 Explanation on the relationship between per-token-loss slope and context utilization 19 Data-independent forget gate initialization Hardware-aware implementation of Forgetting Attention Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.1 Per-token loss for the ablation studies . . . . F.2 Transformer (Pro) ablation . . F.3 Short-context training on SlimPajama . . . F.4 Additional comparison with sliding window attention and Samba F.5 Additional needle-in-the-haystack results . . . F.6 Additional results with 125M-param/2.7B-token, 360M-param/7.5B-token, and . . . . . . . . F.7 Sensitivity of length extrapolation behaviors to hyperparameters . . . F.8 Training curves . . . . F.9 Stability across random seeds . F.10 Additional visualization of forget gate and attention score matrices . 760M-param/16B-token training configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 20 22 22 22 22 24 26 30 30 33 33 16 Published as conference paper at ICLR 2025 DETAILED FOX (PRO) LAYER COMPUTATION We describe the computation of FoX (Pro) layer (depicted in Figure 1 right) in detail. Each FoX i=1 Rd. We layer takes an input sequence (xi)L first describe the computation for the single head case with head dimension dhead. For each time step t, we first compute the key kt Rdhead , query qt Rdhead , value vt Rdhead , forget gate ft R, and output gate gt Rdhead as follows: i=1 Rd and produces an output sequence (yi)L = σ(w qt = RMSNorm(Wqxt) kt = Wkxt, αkey kt = RMSNorm(αkey vt = Wvxt, αvalue vt = αvalue ft = σ(w gt = σ(Wgxt). = σ(w vt1 + (1 αvalue xt + bf ) xt) kt1 + (1 αkey xt) )vt )kt) This is followed by the computation of the attention output: oi = (cid:80)i j=1 exp(q (cid:80)i j=1 exp(q kj + dij)vj kj + dij) Rdhead (15) (16) (17) (18) (19) (20) (21) (22) where dij = (cid:80)i output projection: l=j+1 log fl. We then apply the output normalization, output gate, and the final yi = Wo(RMSNorm(oi) gi) Rd. (23) For the multi-head case, each head maintains an independent copy of the parameters and computes its output sequence (y(h) from different heads are then summed together to obtain the final output yi. Note that similar to the standard Transformer, even though the computation and parameters of different heads are conceptually independent, the computation can be implemented equivalently by properly splitting/concatenating the intermediate vectors/weight matrices of different heads. i=1 independently. y(h) )L i"
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 MODEL AND TRAINING HYPERPARAMETERS Configuration 760M params / 48B tokens 760M params / 16B tokens 360M params / 7.5B tokens 125M params / 2.7B tokens nlayers 24 24 24 12 dmodel 1536 1536 1024 768 dhead 64 for FoX, 128 for Transformer 64 for FoX, 128 for Transformer 64 Peak learning rate See Table 5 1 103 2 103 2 103 Table 4: Hyperparameters for different configurations. The head dimension dhead is only applicable to FoX and the Transformer. We tune dhead for the 760M-parameter FoX and Transformer models in {64, 128}. nlayer counts the number of blocks. For example, for the Transformer each block contains an attention layer and an MLP layer. We list the hyperparameters for different training configurations used in this work in Table 4. For FoX and Transformer, we follow the HuggingFace LLaMA initialization and initialize all linear layer weights and embedding parameters with (0, 0.022). Other hyperparameters are either mentioned in the main text (Section 4.1) or follow the default values in the Flash Lienar Attention repository (Yang & Zhang, 2024)4. Note that our main experiments use the 760M-parameter/48B-token 4Commit 1c5937eeeb8b0aa17bed5ee6dae345b353196bd4. Published as conference paper at ICLR"
        },
        {
            "title": "Model",
            "content": "FoX (Pro) Transformer (Pro) FoX (LLaMA) Transformer (LLaMA) Mamba-2 HGRN2 DeltaNet Learning rate 2 103 1 103 1 103 5 104 2 103 2 103 1 103 Table 5: Peak learning rates for different models for the 760M-parameter/48B-token main experiments. These are tuned using grid {1 10i, 2 10i, 5 10i} with different values of i. configuration. The other three configurations are for ablation studies and additional analyses. For the 760M-parameter/48B-token experiments, we tune the learning rate for each model. We also tune the head dimension for each Transformer and FoX model, along with learning rates. For the other three configurations used for ablation studies and additional analyses, the learning rates are tuned for Transformer (LLaMA) for the 16k training context length setting and transferred to other models and training context lengths. The head dimensions for these three settings are tuned for Transformer (LLaMA) and FoX (LLaMA) for the 16k training context length setting and transferred to Transformer (Pro) and FoX (Pro) and other training context lengths. The optimal hyperparameters are chosen based on the average training loss over the last 512 220 (or 512M) tokens. Note that each token is only visited once by the models during training, so there is no fundamental difference between the training and validation loss. We do not share the parameters between the embedding layer and the output layer. Following the original LLaMA architecture, no bias terms are used in linear layers, except for forget gates.5 Weight decay is not applied to RMSNorm parameters and bias terms in linear layers (again, only used for forget gates) or convolution layers. We use bfloat16 mixed-precision training for all models. B.2 MODEL PARAMETERS, ESTIMATED FLOPS, AND THROUGHPUT We report the number of (non-embedding) parameters, estimated FLOPs, and training throughput in Table 6. For the recurrent models, we estimate FLOPs using their recurrent form for simplicity. Note that the FlashAttention kernels for FoX (Pro), Transformer (Pro), and FoX (LLaMA) are implemented in Triton by us on top of Flag Attention (FlagOpen, 2023) without significant optimization, while Transformer (LLaMA) uses the official FlashAttention implementation (Dao, 2023)) implemented in CUDA. Also, we did not focus on optimizing the efficiency of components such as QK-norm, KV-shift, and so on. We expect these four models to have similar throughput if they are properly optimized. Since we use long training context length compared to model size, recurrent models have significant advantage in theoretical FLOPs due to their linear complexity. Though an exact FLOPsmatched comparison would be interesting, it will require recalibrating the scaling law for the longcontext setting and is beyond the scope of this work. B.3 NEEDLE IN THE HAYSTACK DETAILS Our needle-in-the-haystack test is based on LongAlign (Bai et al., 2024), which is adapted from the original needle test repositoty (Kamradt, 2023) for HuggingFace6 models. The prompt for the standard mode has the following structure: [irrelevant context...] The best thing to do in San Francisco is eat sandwich and sit in Dolores Park on sunny day. 5In preliminary small-scale experiments, we do not find bias terms in forget gates to matter for performance in statistically significant way. We keep it as it might be useful in some cases. 6https://huggingface.co/ 18 Published as conference paper at ICLR 2025 Model Params Forward FLOPs/token Formula for FLOPs/token Throughput (tokens/sec) FoX (Pro) Transformer (Pro) FoX (LLaMA) Transformer (LLaMA) Mamba-2 HGRN2 DeltaNet 759M 757M 757M 756M 780M 756M 757M 2.72 109 2.72 109 2.72 109 2.72 109 1.65 109 1.54 109 1.54 109 2N + 2nlayerdmodelL 2N + 2nlayerdmodelL 2N + 2nlayerdmodelL 2N + 2nlayerdmodelL 2N + 20nlayerdmodeldstate 2N + 5nlayerdmodeldhead 2N + 6nlayerdmodeldhead 27k 30k 30k 38k 44k 46k 48k Table 6: Number of parameters, estimated forward pass FLOPs per token, formulas for FLOPs estimation, and training throughput in tokens per second for different models. Throughput is measured on 4 NVIDIA L40S GPUs. is the number of parameters and is the training context length. [irrelevant context...] There is an important piece of information hidden inside the above document. Now that youve read the document, will quiz you about it. Answer the following question: What is the best thing to do in San Francisco? Answer: The best thing to do in San Francisco is The easy mode is the same, except the needle placed within the context also includes the question: [irrelevant context...] What is the best thing to do in San Francisco? Answer: The best thing to do in San Francisco is eat sandwich and sit in Dolores Park on sunny day. [irrelevant context...] There is an important piece of information hidden inside the above document. Now that youve read the document, will quiz you about it. Answer the following question: What is the best thing to do in San Francisco? Answer: The best thing to do in San Francisco is The results are scored by GPT-4o-2024-08-06 on scale from 1 to 10. EXPLANATION ON THE RELATIONSHIP BETWEEN PER-TOKEN-LOSS SLOPE AND CONTEXT UTILIZATION To understand the relationship between the slope of the per-token loss and context utilization of the model, we first point out that LongCrawl64 applies the preprocessing of randomly rolling the sequences7 to remove any position bias. This means that when given contexts of the same length, the difficulty of predicting tokens at different positions is roughly the same in expectation.8 For example, in expectation, predicting the 100-th token in sequence given only the previous 90 tokens as the context is roughly as difficult as predicting the 90-th token given the full previous 90-token context. Therefore, if L(100) < L(90), it indicates that the first 10 tokens in the context contribute to the models predictions for the 100-th token; and larger the difference L(90) L(100) is, the more these distant tokens contribute. On the other hand, if L(100) is roughly the same L(90) (i.e., the graph of L(i) plateaus after = 100), it means the first 10 tokens do not contribute to the models prediction for the 100-th token, either because they are inherently not useful for this prediction or the model are unable to utilize them. In summary, the slope of L(i) at token position reflects how much tokens from roughly steps earlier contribute to the models prediction at the current token position. 7Concretely, this can be implemented with np.roll with random shift value. 8Note that even without random rolling, given the same number of previous tokens as the context, the difficulty of token prediction at different positions may still remain relatively uniform. 19 Published as conference paper at ICLR 2025 DATA-INDEPENDENT FORGET GATE INITIALIZATION 1 (init)) = exp(log Tmin + (log Tmax log Tmin) h1 To understand our initialization for the fixed forget gate and the data-independent forget gate, we first log σ(b) . This function is defined such that σ(b)T (b) = 1/e is always true define function (b) = (i.e., (b) is the timesteps needed to achieve decay of 1/e). We then initialize b(h) = b(h) init such that (b(h) H1 ), where Tmin and Tmax are hyperparameters and is the number of heads. For example, if = 4 and (Tmin, Tmax) = (2, 128), then we have (T (b(1) init )) = (2, 8, 32, 128). As mentioned in the main text, fixed forget gate with (Tmin, Tmax) is equivalent to ALiBi with minimum slope and maximum slope 256 (the default values in 1 Tmin Press et al. (2021)) is equivalent to using fixed forget gate with (Tmin, Tmax) = (2, 256). 1 Tmax 2 and minimum slope 1 . For example, ALiBi with maximum slope 1 init ), (b(4) init ), (b(2) init ), (b(3) We also tested this initialization for the data-dependent forget gate but did not find it useful, so we simply initialize {b(h)}H h=1 to zero for the data-dependent forget gate. For the data-independent and fixed forget gate, zero initialization performs extremely poorly. HARDWARE-AWARE IMPLEMENTATION OF FORGETTING ATTENTION Algorithm 1 Forgetting Attention Forward Pass Require: Matrices Q, K, RN d, vector RN in HBM, block sizes Bc, Br. 1: Divide into Tr = (cid:108) Bc blocks K1, . . . , KTc and V1, . . . , VTc, of size Bc each. (cid:108) Br Tc = (cid:109) (cid:109) blocks Q1, . . . , QTr of size Br each, and divide K, in to 2: Divide the output RN into Tr blocks O1, . . . , OTr of size Br each, and divide the logsumexp into Tr blocks L1, . . . , LTr of size Br each. = (0)Brd RBrd, ℓ(0) = (0)Br RBr , m(0) = ()Br 1, . . . , cq Tr 1, . . . , ck Tc from HBM to on-chip SRAM. 3: Let cq = c. Devide cq into Tr blocks cq 7: 4: Let ck = c. Devide ck into Tc blocks ck 5: for 1 Tr do Load Qi, cq 6: On chip, initialize O(0) RBr . for 1 Tc do Load Kj, Vj, ck On chip, compute S(j) On chip, compute D(j) 8: 9: from HBM to on-chip SRAM. ) RBrBc . RBrBc . 1 1(ck + D(j) = QiKT = cq = S(j) = mask(S(j) = max(m(j1) RBrBc , i, j) RBrBc. . On chip, compute S(j) On chip, compute S(j) On chip, compute m(j) m(j) On chip, compute O(j) ) RBrBc (pointwise), ℓ(j) 10: 11: 12: 13: 14: 15: 16: 17: end for On chip, compute Oi = diag(ℓ(Tc) On chip, compute Li = m(Tc) + log(ℓ(Tc) 18: 19: Write Oi to HBM as the i-th block of O. 20: Write Li to HBM as the i-th block of L. 21: end for 22: Return the output and the logsumexp L. )1O(Tc) ). . 20 = em(j1) m(j) , rowmax(S(j) m(j) ℓ(j1) )1O(j1) )) RBr , (j) + rowsum( (j) + (j) Vj. = exp(S(j) ) RBr . = diag(em(j1) Published as conference paper at ICLR 2025 Algorithm 2 Forgetting Attention Backward Pass Require: Matrices Q, K, , O, dO RN in HBM, vector c, dc RN , vector RN in HBM, block sizes Bc, Br. (cid:109) (cid:108) Br 1: Divide into Tr = (cid:108) Bc Tc = (cid:109) blocks Q1, . . . , QTr of size Br each, and divide K, in to blocks K1, . . . , KTc and V1, . . . , VTc, of size Bc each. 2: Divide into Tr blocks O1, . . . , OTr of size Br each, divide dO into Tr blocks dO1, . . . , dOTr of size Br each, and divide into Tr blocks Li, . . . , LTr of size Br each. 3: Initialize dQ = (0)N in HBM and divide it into Tr blocks dQ1, . . . , dQTr of size Br each. Divide dK, dV RN in to Tc blocks dK1, . . . , dKTc and dV 1, . . . , dV Tc, of size Bc each. 4: Let cq = ck = c. Devide cq into Tr blocks cq 1, . . . , cq Tr . Devide ck into Tc blocks ck 1, . . . , ck Tc . 5: Let dcq = dck = (0)N . Devide dcq into Tr blocks dcq dck 1, . . . , dck Tc. 1, . . . , dcq Tr . Devide dck into Tc blocks 6: Compute = rowsum(dO O) Rd (pointwise multiply), write to HBM and divide it into Tr blocks D1, . . . , DTr of size Br each. 7: for 1 Tc do Load Kj, Vj, ck 8: Initialize dKj = (0)Bcd, dV = (0)Bcd on SRAM. for 1 Tr do from HBM to on-chip SRAM. 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: . . from HBM to on-chip SRAM. Load Qi, Oi, dOi, dQi, Li, Di, cq On chip, compute S(j) On chip, compute D(j) RBrBc. ) RBrBc 1 1(ck + D(j) = QiKT = cq = S(j) RBrBc = mask(S(j) , i, j) RBrBc. = exp(Sij Li) RBrBc. )dOi RBcd. On chip, compute S(j) On chip, compute S(j) On chip, compute (j) On chip, compute dV dV + (P (j) On chip, compute dP (j) RBrBc. On chip, compute dS(j) (dP (j) Load dQi from HBM to SRAM, then on chip, update dQi dQi + dS(j) and write back to HBM. Load dcq write back to HBM. On chip, compute dKj dKj + dS(j) dS(j) On chip, compute dck from HBM to SRAM, then on chip, update dcq Qi RBcd. 1 RBc. = dOiV = (j) Di) RBrBc. dcq dck i Kj RBrd, + dS(j) 1 RBr , and end for 24: 25: Write dKj, dV j, dck 26: end for 27: Compute dc = dcq + dck. 28: Return dQ, dK, dV , dc . to HBM. In Algorithm 1 and 2, we provide the algorithms for computing the forward pass and backward pass of Forgetting Attention in hardware-aware way. The algorithm is reproduced from the FlashAttention-2 paper (Dao, 2023), with the changes needed to implement Forgetting Attention added and highlighted. In this algorithm, we assume that we pre-computed the cumulative sums 21 Published as conference paper at ICLR Figure 8: Per-token loss for the incremental-style ablation studies presented in Section 4.5. All models are roughly 360M parameters and are trained on roughly 7.5B tokens on LongCrawl64. The vertical line indicates the training context length. = cumsum(log ). The mask operation properly sets some entries of its first operand to to satisfy the causality requirement. Note for the backward pass for ease of presentation we combine the computation of dK, dV , dck and the computation of dQ, dcq in single algorithm, but in practice these are computed in two different kernels for implementation simplicity. In practice, we implement Forgetting Attention based on the Triton (OpenAI, 2021) FlashAttention implementation in Flag Attention (FlagOpen, 2023)."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "F.1 PER-TOKEN LOSS FOR THE ABLATION STUDIES In Figure 8 and Figure 9 we show the per-token loss for the ablation studies presented in Table 3 in Section 4.5. Transformer (LLaMA) without RoPE performs extremely poorly and we show it separately in Figure 10. F.2 TRANSFORMER (PRO) ABLATION In Figure 11, we present small-scale ablation study using Transformer (Pro) in the 125Mparameter/2.7B-token setting. We start with Transformer (LLaMA) and add one component at time. Notably, we find that QK-norm seems to be helpful for length extrapolation. F.3 SHORT-CONTEXT TRAINING ON SLIMPAJAMA To complement our main results in which we perform long-context training on LongCrawl64, we have also run short-context training on the more commonly used SlimPajama dataset (Soboleva et al., 2023). We follow the 340M-parameter/15B-token/2k-context-length setting in Yang et al. (2024). We also use the same hyperparameters and tokenizer as Yang et al. (2024). We train FoX and Transformer with both the LLaMA and the Pro architecture. We also test Mamba-2, the strongest recurrent sequence model in our main results. We show the per-token loss of tested models in Figure 12 and downstream task evaluation results in Table 7. We use the same set of tasks as Yang et al. (2024) so our results can be directly compared to those of Yang et al. (2024). As shown in the results, in this short-context training setting FoX 22 Published as conference paper at ICLR 2025 Figure 9: Per-token loss for the perturbation-style ablation studies presented in Section 4.5. All models are roughly 360M parameters and are trained on roughly 7.5B tokens on LongCrawl64. The vertical line indicates the training context length. Figure 10: Removing RoPE from Transformer (LLaMA) results in poor performance. All models are roughly 360M parameters and are trained on roughly 7.5B tokens on LongCrawl64. The vertical line indicates the training context length. Figure 11: Incremental style ablation study for Transformer (Pro). All models are roughly 125M parameters and are trained on roughly 2.7B tokens on LongCrawl64. The vertical line indicates the training context length. 23 Published as conference paper at ICLR 2025 Figure 12: Results on SlimPajama with training context length of 2048 tokens. All models have roughly 340M non-embedding parameters and are trained on roughly 15B tokens on SlimPajama. The vertical line indicates the training context length. Table 7: Evaluation results on LM-eval-harness for models trained on SlimPajama with training context length of 2048 tokens. All models have roughly 340M non-embedding parameters and are trained on roughly 15B tokens on SlimPajama. acc-n means length-normalized accuracy. Bold and underlined numbers indicate the best and the second best results, respectively. Note the results for Transformer++ and DeltaNet are from Yang et al. (2024). Note that Transformer++ from Yang et al. (2024) and Transformer (LLaMA) in our work have exactly the same architecture. Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c acc-n ppl acc-n acc acc acc acc ppl Transformer++ (Yang et al., 2024) DeltaNet (Yang et al., 2024) FoX (Pro) Transformer (Pro) FoX (LLaMA) Transformer (LLaMA) Mamba-2 28.39 28.24 25.69 25.92 27.86 27.98 27.51 42.69 37.37 31.98 31.93 43.26 35.25 41. 31.00 32.10 35.82 35.01 32.56 32.31 29.83 63.30 64.80 65.61 65.02 64.80 63.71 65.94 34.00 34.30 36.39 36.09 34.59 34.89 35. 50.40 52.20 51.07 50.51 50.12 48.07 50.20 44.50 45.80 45.79 46.42 45.12 45.33 45.45 24.20 23.20 25.09 23.38 23.38 23.72 23. Avg 41.23 42.07 43.29 42.74 41.76 41.34 41.85 (LLaMA) does not have an advantage over the Transformer (LLaMA) except for length extrapolation, while FoX (Pro) still outperforms Transformer (Pro) in language modeling loss and downstream tasks. Note that these are small-scale experiments without extensive hyperparameter tuning (e.g., learning rate), so the results might not transfer to larger scales with proper hyperparameter tuning for each model. F.4 ADDITIONAL COMPARISON WITH SLIDING WINDOW ATTENTION AND SAMBA In this section, we compare the standard Transformer, FoX, and Mamba-2 with sliding-windowattention-based Transformer (Transformer-SWA). We also compare with Samba (Ren et al., 2024), hybrid architecture combining sliding window attention and Mamba. Both Transformer-SWA and Samba use window size of 2048. For these experiments, we use the 760M-parameter/16B-token configuration in Table 4. Note that as mentioned in Section B, all models in this configuration use the same learning rate that is tuned for Transformer (LLaMA), so the results might not be optimal for other models. We show the per-token loss, easy-mode needle-in-the-haystack experiment, shortcontext downstream task results, and long-context task results in Figure 13, Figure 14, Table 8, and Table 9, respectively. Though both Transformer-SWA and Samba perform well on short-context tasks, they show an early plateau in the per-token loss, which indicates that they struggle to utilize the long context. Accordingly, they perform poorly in the needle-retrieval task. 24 Published as conference paper at ICLR 2025 Figure 13: Additional comparison with Samba and Transformer-SWA. (left) Per-token loss L(i) at different token position i. (right) Validation perplexity (l) over different validation context length l. All models have 760M parameters and are trained on roughly 16B tokens. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using moving average sliding window of 101 tokens. In this plot 1k = 1024. Table 8: Evaluation results on LM-eval-harness. All models have roughly 760M non-embedding parameters and are trained on roughly 16B tokens on LongCrawl64. acc-n means length-normalized accuracy. Bold and underlined numbers indicate the best and the second best results, respectively. Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c COPA OBQA SciQA BoolQ Avg acc ppl acc-n acc-n acc-n acc acc acc acc acc acc ppl FoX (Pro) Transformer (Pro) FoX (LLaMA) Transformer (LLaMA) Samba Transformer-SWA (LLaMA) Mamba28.10 28.17 31.03 32.33 31.71 33.63 33.26 23.67 24.63 28.41 34.41 27.78 33.04 42.38 36.93 36.17 34.89 32.41 34.25 33.15 27.29 61.64 61.53 61.21 60.94 60.45 60.01 60.83 33.44 33.46 32.27 31.68 32.88 31.83 32.03 49.72 50.28 50.51 49.96 51.70 51.14 50. 47.94 47.81 46.68 45.62 49.03 46.93 46.21 23.98 24.15 24.06 23.63 24.32 23.38 23.55 65.00 67.00 67.00 64.00 61.00 62.00 64.00 26.80 28.40 29.60 28.60 28.20 27.40 28.40 80.40 77.90 77.30 74.00 78.80 76.70 76.70 57.49 55.72 61.07 60.06 60.58 54.62 57. 48.33 48.24 48.46 47.09 48.12 46.72 46.73 Table 9: Evalution results on LongBench. All models have roughly 760M non-embedding parameters and are trained on roughly 16B tokens on LongCrawl64. Bold and underlined numbers indicate the best and the second best results, respectively. Model FoX (Pro) Transformer (Pro) FoX (LLaMA) Transformer (LLaMA) Samba Transformer-SWA (LLaMA) Single-Document QA Multi-Document QA Summarization Few-shot Learning Code arrativeQ 10.48 8.67 9.48 8.44 6.33 8.46 asper 12.98 13.92 15.55 10.08 10.89 8.59 A 20.62 22.45 17.13 18.77 15.86 16.65 otpotQ 2 ikiM usique ov Report 6.87 9.36 5.26 6.09 5.1 6.9 16.2 14.21 15.78 14.47 11.28 13.84 5.48 5.16 3.78 3.98 2.79 4.03 27.51 19.88 21.95 11.83 9.42 7. Su 10.15 10.66 10.59 11.52 11.39 12.87 ultiN ews 9.27 12.23 8.63 12.94 10.88 10.0 C 63.5 52.0 29.0 23.5 28.5 12.0 TriviaQ Sa Su 26.97 30.18 19.16 18.46 16.07 14.92 18.02 25.53 10.07 16.04 2.8 5.1 C 6.34 8.37 6.93 8.27 11.65 16.16 Repo Bench-P 3.4 10.72 9.89 13.5 14.26 14.22 25 Published as conference paper at ICLR 2025 Figure 14: Easy mode needle-in-the-haystack analysis for FoX, the Transformer, Mamba-2, Samba, and the Transformer with sliding window attention. These are 760M-parameter models trained on 16B tokens on LongCrawl64. The results are scored on scale of 1 (red) to 10 (green) by GPT-4o. The vertical dashed line indicates the training context length. Figure 15: Needle-in-the-haystack analysis for HGRN2. The results are scored on scale of 1 (red) to 10 (green) by GPT-4o. The vertical dashed line indicates the training context length. F.5 ADDITIONAL NEEDLE-IN-THE-HAYSTACK RESULTS In Figure 15, we show the results of the needle test for HGRN2 in the 760M-parameter/48B-token setting. F.6 ADDITIONAL RESULTS WITH 125M-PARAM/2.7B-TOKEN, 360M-PARAM/7.5B-TOKEN, AND 760M-PARAM/16B-TOKEN TRAINING CONFIGURATIONS Besides our main results with 760M-parameter model trained on 48B tokens, we also report per-token loss results with 125M-parameter/2.7B-token, 360M-parameter/7.5B-token, and 760Mparameter/16B-token training configurations in this section. The hyperparameters used are given in Appendix and Table 4. Note that, as mentioned in Appendix B, the learning rates for these experiments are tuned for Transformer (LLaMA) for the 16k training context length setting and transferred to other models and training context lengths, so the reported results may not be optimal for some models (e.g., FoX typically prefers higher learning rates than the Transformer). Per-token loss for different models in the main experiment In Figure 16, Figure 17, and Figure 18, we show the per-token loss for different models given training context length of 16k tokens for the 125M-parameter/2.7B-token, 360M-parameter/7.5B-token, and 760M-parameter/16B-token 26 Published as conference paper at ICLR 2025 Figure 16: Results with 125M-parameter models trained on roughly 2.7B tokens. (left) Per-token loss L(i) at different token position i. (right) Validation perplexity (l) over different validation context length l. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using moving average sliding window of 101 tokens. In this plot 1k = 1024. Figure 17: Results with 360M-parameter models trained on roughly 7.5B tokens. (left) Per-token loss L(i) at different token position i. (right) Validation perplexity (l) over different validation context length l. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using moving average sliding window of 101 tokens. In this plot 1k = 1024. training configurations, respectively. These results are consistent with the 760M-parameter/48Btoken results in Figure 2. Per-token loss for different training context lengths In Figure 19, Figure 20, and Figure 21, we show the per-token loss for the FoX and Transformer models given different training context lengths for the 125M-parameter/2.7B-token, 360M-parameter/7.5B-token, and 760M-parameter/16B-token training configurations, respectively. Consistent with the results in Figure 4.5, we see that the advantages of FoX over the Transformer (1) reduce for larger models and (2) increase for longer training context lengths. 27 Published as conference paper at ICLR 2025 Figure 18: Results with 760M-parameter models trained on roughly 16B tokens. (left) Per-token loss L(i) at different token position i. (right) Validation perplexity (l) over different validation context length l. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using moving average sliding window of 101 tokens. In this plot 1k = 1024. Figure 19: Per-token loss given different training context lengths for the 125M-parameter/2.7Btoken setting. (left) Results for the LLaMA models. (right) Results for the Pro models. At each token index i, we report the averaged loss over window of 101 centered at i. Published as conference paper at ICLR 2025 Figure 20: Per-token loss given different training context lengths for the 350M-parameter/7.5Btoken setting. (left) Results for the LLaMA models. (right) Results for the Pro models. At each token index i, we report the averaged loss over window of 101 centered at i. Figure 21: Per-token loss given different training context lengths for the 760M-parameter/16B token setting. (left) Results for the LLaMA models. (right) Results for the Pro models. At each token index i, we report the averaged loss over window of 101 centered at i. 29 Published as conference paper at ICLR 2025 Figure 22: FoX (Pro) and Transformer (Pro) easy mode needle-in-the-haystack results for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length. F.7 SENSITIVITY OF LENGTH EXTRAPOLATION BEHAVIORS TO HYPERPARAMETERS This section presents more results on the sensitivity of length extrapolation behaviors to hyperparameters, in addition to our results in Section 4.3 and Figure 5. Figure 22 and Figure 23 show the easy-mode and standard-mode needle retrieval results for FoX (Pro) and Transformer (Pro) with different numbers of training tokens and learning rates. Figure 24 shows the corresponding per-token loss curves. As shown in these results, length extrapolation is sensitive to hyperparameters. F.8 TRAINING CURVES In Figure 25, we show the training curves for all models presented in Figure 2. Note that different models use different peak learning rates, so their learning curves have different shapes. 30 Published as conference paper at ICLR Figure 23: FoX (Pro) and Transformer (Pro) standard mode needle-in-the-haystack results for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length. 31 Published as conference paper at ICLR 2025 Figure 24: FoX (Pro) and Transformer (Pro) per-token loss for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length. Figure 25: Training curves of different models presented in Figure 2. These curves show the training loss averaged every 512 220 tokens. All models have 760M parameters. Published as conference paper at ICLR 2025 Figure 26: Result stability across seeds with 360M-parameter FoX (LLaMA). All models are trained on roughly 7.5B tokens. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using moving average sliding window of 101 tokens. In this plot 1k = 1024. F.9 STABILITY ACROSS RANDOM SEEDS Although it is computationally impractical for us to run multiple seeds for all our results, we have run three seeds for our 360M-parameter FoX (LLaMA) model to show that the variance across seeds is small. As shown in Figure 26, the variance across seeds is small. F.10 ADDITIONAL VISUALIZATION OF FORGET GATE AND ATTENTION SCORE MATRICES In Figure 27 and Figure 28, we show the forget gate matrices and the attention score matrices from 16 heads distributed in 4 layers. Note that since these matrices are large (16384 16384), if only near-diagonal entries of are non-zero the visualization will look almost all black. 33 Published as conference paper at ICLR 2025 Figure 27: Visualization of the forget gate weight matrix from 16 heads in 4 different layers. These results use FoX (Pro). Figure 28: Visualization of the attention score matrix from 16 heads in 4 different layers. These results use FoX (Pro)."
        }
    ],
    "affiliations": [
        "MakerMaker AI",
        "Mila & Universite de Montreal"
    ]
}