{
    "paper_title": "Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression",
    "authors": [
        "Lirui Wang",
        "Kevin Zhao",
        "Chaoqi Liu",
        "Xinlei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \\ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information."
        },
        {
            "title": "Start",
            "content": "Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression Lirui Wang1 Kevin Zhao1 Chaoqi Liu2 Xinlei Chen3 1MIT 2UIUC 3Meta, FAIR https://liruiw.github.io/hma 5 2 0 F 6 ] . [ 1 6 9 2 4 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. HMA achieves better visual fidelity and controllability than the previous robotic video generation models with 15 faster speed in the real world. After post-training, this model can be used as video simulator from low-level action inputs for evaluating policies and generating synthetic data. 1. Introduction Scaling robot learning is bottlenecked by i) large amounts of high-quality data for training; ii) real-time and high-fidelity evaluation. This is in stark contrast with other domains such as natural language processing [36, 38] and computer vision [5, 25] where abundant training data are available on the Internet and evaluation can be simply performed online with held-out data. One compelling solution to these bottlenecks is generative modeling [1, 5], which aims to learn the full dynamics and simulate the world. For training, it can produce an infinite amount of in-distribution data; for evaluation, it can potentially generate reasonable physical interactions without real-world deployment. However, building useful generative models is nontrivial. It has to be general to handle various setups across embodiments, domains, and tasks rather than specific settings [2, 6, 48]; its also desirable to be efficient, so as to deal with the real-time interactions from the policies [52, 60] necessary for robotic applications. Building general solution across embodiments, domains, Figure 1. Action-Video Dynamics Model from Heterogeneous Robot Interactions. HMA utilizes heterogeneous datasets comprising over 3 million trajectories (videos) from 40 distinct embodiments to pre-train full dynamics model with next-set-of-token predictions using masked autoregression. After pre-training, the resulting action-video dynamics model is versatile, supporting applications such as video simulation, policy evaluation, synthetic data generation, and direct adoption as an imitation policy. and tasks at scale requires handling the action heterogeneity in robotics. In particular, different robots use different action spaces, action frequencies, and action horizons for their specific tasks. To this end, we build on the recent idea that aligns such heterogeneity into the shared latent space [14, 44, 49] of action-video dynamics generation. To maximize the generality of the framework  (Fig. 1)  , the network is modularized such that after pre-training, any new embodiment only requires training small action encoder (stem) and action decoder (head) from scratch. Beyond pure policy learning [49], our full-dynamics setting also requires modeling of observations, which are typically formatted as sequences of images, or videos. While state-of-the-art, diffusion-based video modeling [5, 40, 60] has achieved impressive visual quality, such frameworks are inefficient for real-time applications like robotics due to their need for extensive iterations over the entire sequence at each generation step. In contrast, we build on recent advances 1 in autoregressive modeling for vision [27, 29, 45], which offer more efficient alternative while maintaining high generation quality. In particular, we leverage masked autoregression [8, 27] for action-video dynamics and explore two variants to trade off speed vs. fidelity for videos: the discrete variant which generates vector-quantized (VQ) tokens at high speed, and the continuous one which better preserves visual fidelity. With these insights on Heterogeneity (H) on actions and Masked Autoregression (MA) on video dynamics, we introduce HMA, masked autoregression framework for actionvideo dynamics (AVD) over heterogeneous robotic data. The versatility of this unmasking architecture handles several core problems in robotics, including world models and endto-end policies  (Fig. 2)  . These problems can be framed as dynamic models using sequence modeling, enabling the joint generation of observations and actions [9, 39]. HMA is shown to scale across embodiments, trajectories, and model sizes in the heterogeneous pre-training mixtures. In particular, we study the scaling behaviors measured by visual fidelity and action controllability. The model is pretrained with over 3 million trajectories (videos) with action labels and single HMA can generate video across wide range of 40 embodiment datasets from 2-DoF action space to 28-DoF action space. The main use case of HMA is the first high-fidelity and real-time video robotic simulator (example). Our generation achieves better visual fidelity and controllability than the previous state-of-the-art [60] with 15 faster inference latency, enabling real-time interactions [31]. Because HMA autoregressively models the video and action sequences in interaction data, it can also be used to generate trajectories over 100 frames (over 30 seconds) stably in various robotic applications. On simulation benchmarks [32], HMA is used to evaluate policies as high-fidelity simulator and to generate 90% synthetic trajectory data for improving policy performance to match ground truth data. The learned dynamic models can also be used as an imitation policy to predict actions. We hope that HMA can shed some light on learning action-video dynamic models in unified framework from heterogeneous data. 2. Related Works World Models. World models [18], or dynamic models [3], are computer programs that evolve based on agents behaviors. Different from simulator software, learned dynamic models predict future states or reward functions based on past observations and then apply to model-based reinforcement settings [9, 19, 56] or robotics [7, 28, 41]. Notably, since ground truth states are often unavailable in decision-making, dynamic models need to handle highdimensional video data and low-dimensional grounded physical actions. Full-sequence diffusion and autoregressive models are two primary approaches for generative tasks across languages, images, and videos [8, 22, 26, 27, 53]. In particular, 1xGPT [1] uses masked autoregression for video generation and MAR [27] applies diffusion losses with masked autoregression for image generation, and Diffuser [24] uses diffusion models to jointly model the full state and action sequences in planning. Our work focuses on masked autoregression over full-sequence diffusion [2, 6, 48], aiming to create efficient and interactive world models. Steering Video Models with Actions. Video models [5] can be applied to wide range of video data including curated videos, human videos, synthetic videos, and robot videos. Interactive video models usually rely on language instructions [52], latent actions [6], or sketches to guide the video or image models. However, video controllability is still an issue when fine-grained details and low-level motion controls are used as prompts [2, 40, 60]. In particular, IRASim [60] applies DiT [37] to several robotic datasets and demonstrates high-quality video simulation qualities. Unlike previous works that apply such models to single task or embodiment or use natural languages as actions, our work investigates action-conditioned video models across heterogeneous embodied settings and their scaling behaviors. Visual Generative Models for Robotics. Visual generative models have been explored extensively for robotic applications such as policy learning, planning, and synthetic data generation. Synthesized goals and subgoals [15, 34] have been used to improve end-to-end manipulation policies. Video predictions such as visual foresight [16] have been used to guide policy executions. Video language planning [15] achieves long-horizon planning tasks through modelpredictive control and search. In the context of robotics, diffusion methods have been used to augment images [10, 55] and 3D generative methods have been used to generate synthetic data from novel views [59]. In this work, we use learned dynamic models for policy evaluation and synthetic data generation in policy learning. Moreover, the dynamics model can act as policies for action predictions. 3. Heterogeneous Masked Autoregression 3.1. Dynamic Models Our objective is to learn the dynamics model on visual observation history Ohistory = {otNpast, ..., ot1} and action history Ahistory = {atNpast, ..., at1} to predict the future observations Ofuture = {ot, ..., ot+Nfuture} and actions Afuture = {at, ..., at+Nfuture } where Npast and Nfuture are hyper-parameters. Using the masking objective in the next section, the model is trained to predict masked component in this function given other components. In Fig. 2, multiple core problems in decision-making and robotics [3, 33] can be viewed as subset of this problem: full dynamics: 2 the conditional distribution based on previous generations. It uses the masked autoencoding objective with random order of tokens at training time. Then it predicts the nextset-of-tokens and gradually unmasks at inference time [27], which is natural in robotic interactions. The joint dynamics probability distribution can be decomposed as: p(Ohistory, Ofuture, Ahistory, Afuture) = p(X1, ..., XK) = ΠK k=1p(XkX1, ..., Xk1), (1) where Xk can be any (causally) valid masked set of observations and actions. Therefore, it can unify the different dynamic settings mentioned in Sec. 3.1. We use neural network fθ parameterized by θ to learn such distribution. Notably, both images and actions are continuous signals, and multiple loss functions can be applied to this. One simple choice is to tokenize the images into discrete tokens and use the raw actions. We then train the network with cross-entropy (CE) loss on image tokens while applying mean-squarederror (MSE) regression losses to the actions. For simplicity, let = (ˆo, ˆa) generated by θ and (o, a) denote the ground truth, the discrete loss on the video vector-quantized (VQ) tokens can be written as: LVQ(X; θ) = SE(a, ˆa) + CE(o, ˆo). (2) Alternatively, we can use separate denoising diffusion objective that learns to reconstruct based on continuous latent z, which we also call soft tokens [27]: Lsoft(X; θ) = ϵa (at, z)2 + ϵo (ot, z)2, (3) where ϵa, ϵo are noise vectors sampled from (0, I) and is the timestep of the noise schedule. Note that z, are separate for action and video in practice. Different from previous video models that use either diffusion [2, 60] or autoregression [6, 29], this method combines the efficiency and expressiveness of both approaches to model the joint dynamic models. 3.4. Model Architecture The overall architecture, denoted as Heterogeneous Masked Autoregression (HMA), builds on the generality across embodiments with action heterogeneity, and the efficiency across dynamic settings and discrete/continuous observation tokens with masked autoregression. In Fig. 3, the network architecture of HMA follows the heterogeneous pre-training in HPT [49], in which we create multiple modules of action inputs (stem) and action outputs (head) and share the core spatial-temporal transformer (trunk) as the dynamic models for pre-training and transferring. The spatial attention runs bi-directionally with the masked video and action tokens and the unmasked tokens, Figure 2. Dynamics Model. Masked autoregression in the dynamics model generalizes multiple problem settings including policy learning, forward and passive dynamics, and full dynamics. ffull-dynamics(Ohistory, Ahistory) (cid:55) (Ofuture, Afuture), passive video predictions fpassive(Ohistory) (cid:55) Ofuture, forward dynamics: fforward-dynamics(Ohistory, Ahistory) (cid:55) Ofuture, and policy models: fpolicy(Ohistory, Ahistory) (cid:55) Afuture are all subsets of such sequential and generative models. The objective can be applied to datasets with missing labels such as pure video datasets and policy datasets, and can even be extended to inverse dynamics finverse-dynamics(Ohistory, Ofuture) (cid:55) Afuture if non-causal models are used. 3.2. Action Heterogeneity world model that understands the physical world should be able to simulate various forms of embodiments and actions. For instance, humans can intuitively understand how other creatures physical actions would impact the world. Action heterogeneity in robotics includes different action spaces, action frequencies, and action dimensions. For example, dynamics model with 30Hz frequencies and 50-DoF joint space for humanoid is different from one with 5Hz frequencies and 6-DoF end effector space for Franka Arm. Since the videos can be unified as sequence of images of fixed resolutions, we decouple observation and actions to handle the heterogeneity in actions. One particular approach that handles such action heterogeneity without explicit tokenization processing step is HPT [49]. Specifically, for each domain, we consider 2Hz dynamics frequencies and 12 frame context (6 seconds wallclock time) to balance generation length and compute efficiency, but technically any action frequencies can be used. We chunk the action sequences into fixed 2Hz to ensure consistency across datasets. For action prediction objectives, we will use different decoder heads to predict actions for different embodiments. 3.3. Masked Autoregression Following the success of language models and image/video models, we use masked autoregression (MAR) [1, 8] to generate video predictions and action predictions based on previous video observations and action sequences. Specifically, masked autoregression models the joint distribution as time dimension, unmasking steps in the image patch dimension, and diffusion steps for the continuous token generation. Since each step in the nested iterations is fast, the generation process is both high-quality and efficient. For diffusion head, we use DDIM with per-step clipping [43] to train with Ntrain = 1000 timesteps and test with Ntest = 100 steps. As in standard diffusion works [37], we use patch size 2 to reduce context length. This means 2 2 patch of tokens is replaced with single token, We find = 2 unmasking [8, 27] iterations with random unmasking order to be sufficient to generate high-quality videos at inference time and iterations across are not necessary. 4. Pre-Training Experiments We first discuss the implementation details below for training HMA, including datasets, models, and evaluation metrics. Datasets. We use multiple domains of embodiment data related to robotic manipulations. In the largest dataset mixture, the 35 real robot datasets contain primarily the Open-X embodiment dataset [35], and 3 human video datasets contain ego-centric hand motions [12, 13, 17] and the 2 simulation datasets contain standard benchmark [32, 54]. We follow the same dataset processing as in previous works [49] for using 2D hand detections as action label proxies. The full dataset has over 2.5 billion frames and 3 million trajectories. Note that these datasets with action labels usually have different action dimensions and action frequencies. During training, we apply inverse exponential probability for sampling the dataset of each batch sample. We preprocess videos in these datasets into tokens with fixed resolution (256 256) using the pre-trained Stable Video Diffusion VAE [4] with 8 8 spatial downsampling for implicit diffusion models and the 1XGPT tokenizer [1], which is fine-tuned OPENMAGVIT2 [30] tokenizer, with 16 16 downsampling for explicit cross-entropy losses. We unify all datasets into 2Hz by adaptively choosing action strides. For example, 10Hz dataset will have an action stride of 5 and shrinking into 2Hz action chunks. Under 2Hz frequencies, the context length is in total 12 frames (6 seconds) with 4 frames (2 seconds) as prompts and 8 frames (4 seconds) as predictions, which are sufficient for most closed-loop interactive applications. We choose these hyperparameters for prototyping, but technically any image resolution or video frequencies can be used. The continuous objective also does not require tokenizer and can directly operate on pixel space. Model. Under the standard setup for ablations, we use the discrete model and VQ tokens with forward dynamics objective (with cross-entropy loss) for the simplicity of training. We use 32-layer transformer model with dimension = 256. The model is trained with 8 V-100 GPUs and batch size 64 for around 60,000 iterations and 2 epochs. We train larger models with up to 64 GPUs. Figure 3. Network Architecture. The HMA model architecture maps low-level video and action sequences across different embodiments into shared latent space. For actions, embodiment projectors are activated based on the training sample. The spatialtemporal Transformer produces the output video and action tokens for future frames. and the temporal attention is causal in predicting tokens in future steps. This model architecture explicitly handles the heterogeneity in action spaces in robotics [42, 49]. Specifically, each domain has its own action encoder which has an MLP layer that maps normalized actions of certain horizons into features. decoder can be 3-layer diffusion MLP that is trained to regress on the continuous features and/or actions. Notably, the ability to predict controllable and highfidelity future given past observations requires dedicated information streams from the actions. We use modulation [37] layers for each domain in every Transformer block [42]. To generalize the video model to predict both video and action in the full dynamics task, we also use the token concatenation method for fusing video and action tokens. 3.5. Training and Inference In practice, each frame in the video has 256 tokens and 64 repeated action tokens. The masking has minimum ratio and follows cosine schedule to mask more tokens at later steps in the training horizon. We use Maximal Update Parametrization [51] for scaling with bigger models. We find Xavier initialization with gain 0.1 for the action projectors to be useful for training stability. We use different mask tokens and different decoders for action and video tokens. We ablate with other variants such as cross-attention layers in the Section 4. At inference time, we append masked tokens to the video sequence and masked tokens to the action sequence up to the maximum horizon whenever needed. The full inference process for the diffusion-based HMA contains three nested autoregression procedures across timesteps in the video Figure 4. Pre-trained Video Model Generation. We show that single unified HMA model can generate realistic (left 3 columns) and diverse (right 3 columns) videos across multiple embodiment datasets with heterogeneous action spaces. Each group shows three generated frames from single sequence. 4.1. Action-Video Dynamics under Heterogeneity In this experiment section, we investigate how well heterogeneous pre-training works for world models. We choose the discrete loss objective in HMA for these experiments. In Fig. 4, we showcase the generation of our single unified pre-train model HMA to handle heterogeneous datasets. Dynamic Model Settings. In this section, we measure the performance of different dynamics model settings, including passive video generation, forward dynamics (actionconditioned video generation), and full dynamics which have an auxiliary task of action predictions. Shown in Fig. 5 (a), low-level action conditioning in the forward dynamics model can improve video generation results. Intuitively, passive video sequences without actions contain less information on the causal physical relationships of these videos and are sometimes hard (or even ambiguous) to capture fine-grained details related to motions. We find full dynamics training outperforms passive dynamics but does not outperform forward dynamics generation. We hypothesize that these results stem from the limitations of VQ tokens, which were originally designed for visual generation tasks rather than for action prediction. Action Architecture Ablation. We compare several architectural designs in conditioning action information for video generation. Specifically, we ablate architecture variants including i) modulation, ii) token concatenation, iii) feature addition, and iv) token cross-attention following [37]. In Fig. 5 (b), we show that the modulation method can outperform other methods. In particular, token concatenation along the sequence dimension for each frame does not have enough expressiveness and compute on action-conditioning, compared to per-layer modulation. 4.2. Scaling Behaviors of HMA We experiment with the scaling performance of HMA along multiple axis. In Fig. 6, we scale across the number of videos per dataset, scale across the number of datasets, and scale Figure 5. Ablation on Pre-training Settings and Architecture. Under the pre-training setting with VQ tokens, we ablate the video generation performance (visual fidelity measured by perplexity and controllability measured by controllability). (a) We find actionconditioned models outperform passive video models. (b) We compare different action conditioning architectures in the masked autoregression framework. The purple color denotes the best model that we use by default. Metrics. We measure the performance of the video models via several metrics on the held-out validation datasets with maximum of 500 examples per dataset and present the average statistics across datasets. When we use auto-regressive models with explicit cross-entropy loss on tokenized outputs, we measure the training objectives for fidelity via validation perplexity, which is directly correlated with validation loss and visual fidelity metrics, such as PSNR, SSIM. We use PSNR [6] to measure controllability, which is the average difference of PSNR on the last frame computed from ground truth action sequence with PSNR computed from 5 random action sequences. Intuitively, if PSNR is small, then the model predictions are less affected by actions and therefore the video model is not controllable. 5 Figure 6. Experiments on Scaling Behaviors of HMA. We observe positive trends in the scaling performance of heterogeneous video models across axes including the number of datasets, number of trajectories, and model sizes. The evaluation metrics on fidelity (perplexity) and controllability (PSNR) are averaged across validation datasets. across model sizes. Under the same validation datasets, we observe consistent gains from scaling measured by perplexity and PSNR. Scaling Embodiments. Scaling the number of datasets (Fig. 6 Left) from 5 to 40 datasets strictly increases the dataset heterogeneity and creates more model parameters to handle such action space differences among embodiments. Yet we see that the video prediction performance does not degrade or become unstable. This gives positive signals for joint heterogeneous training with diverse embodiment video datasets. For this experiment, we evaluate the models on the first random subset of 5 datasets to be consistent. Scaling Data. Using 40 datasets, we scale the number of trajectories (videos) per dataset from 10 to 106, equivalent to training on 8 thousand to 3 million trajectories. In Fig. 6 middle, the slight plateau of the trajectory scaling performance after 105 total trajectories is likely due to the dataset quantity imbalance, as the additional trajectories all come from the largest few datasets. Scaling Model. Under the largest dataset with the maximum number of datasets (40) and maximum trajectories (3 million), we study model scaling of HMA. In Fig. 6 right, we scale the hidden dimension of the Transformer while keeping other parameters constant. This changes the model parameters from 3 million to 400 million (sparse) parameters. We qualitatively observe improved visual qualities and controllability when we increase the model capacity. 5. Post-Training Applications After pre-training, we finetune HMA to evaluate its performance with limited data and ground truth controllability metrics. In the real-robot action simulator setting, we demonstrate its speed advantages over the previous state-of-the-art interactive simulator by 15. We also post-train the model to various downstream robotics use cases including policy evaluation, policy data generation, and dynamic models as policies. Datasets. We focus on Robomimic [32], simulation benchmark with 200 trajectories per task, and Language Table [31], real-world benchmark with 442k trajectories, to study the applications of HMA model in simulation and policy learning. Simulation benchmarks are suitable because we can evaluate the ground truth controllability and benchmark against policy methods and policy evaluation. Language Table is suitable for its sheer amounts of trajectory data and focused domain. We use held-out videos to evaluate HMA on these two datasets. Since we need to apply interactions in post-training, we use an action stride of 1 and maintain the original action frequencies in the dataset. Model. Following the training protocol of Sec. 4, we finetune the pre-trained HMA with layer dimension 256 on each dataset for 10 epochs. The interactive simulation setting requires the forward dynamics formulation of our model. In Tab. 1, we compare different model sizes of HMA with the previous state-of-the-art open-source model IRASim [60] which uses the DiT [37] model and action modulation architecture and outperforms VDM and LVDM [20, 22] in robotics. Notably, IRASim is not optimized for single-frame interactions, as the default setup predicts video sequences based on action sequences. On Tab. 1, we observe 15 model speed improvement (4.44 v.s. 0.28 FPS) even compared to their amortized result, thanks to masked autoregression. Using the diffusion heads, the model can still be faster while maintaining high visual fidelity. Metrics. For real-world data, we use model-based metrics such as PSNR [23] and SSIM [50], learning-based metrics such as LPIPS [57], FID [21], and FVD [47] for visual fidelity measurement; and PSNR [6] for controllability. For simulation tasks, since we can access ground truth observations in any state through rendering, we directly compare ground truth against the learned model to measure control6 Model IRASim-XL IRASim-XL, amortized HMA-Base HMA-XL HMA-Base HMA-XL Method DiT DiT MaskGIT MaskGIT MAR MAR Parameters (M) 679 679 44 679 96 741 FPS 0.28 0.58 22.72 4.38 4.44 2.01 Table 1. Inference Speed. We measure the per-frame inference speed across 16 frames for various model sizes. The Base model has model size of around 30M and the XL model has similar model size as IRASim-XL. The models all use 32-block transformers where the base model has dimensions 256 and the XL models have dimensions 768. Our fastest model of the same size is more than 15 faster than [60] because HMA does not pass through the full Transformer multiple times (with diffusion modeling) to generate each frame. MAR incurs more parameters than MaskGIT [8] because of the diffusion heads [27]. The amortized result for [60] comes from averaging over multiple frames. The speeds are all measured on the same hardware setup with RTX-4080 GPU. IRASim HMA PSNR SSIM PSNR LPIPS FID FVD 152.20 111.52 23.22 33. 25.41 28.19 0.82 0.83 5.78 6.06 0.08 0.07 Table 2. Comparison with IRASim. In Language Table Benchmark [31], we show that pre-trained HMA-based model (diffusion) is able to achieve better visual qualities and controllability than IRASim while maintaining faster speed and requiring less compute. The results are computed over 200 held-out trajectories. lability rather than using proxy PSNR. In particular, we apply perturbations to the action sequences and denote the metrics as PSNR and perplexity which measure the average sensitivity of the dynamics model to small action perturbations. 5.1. Toward Real-Time Simulation with HMA In this section, we illustrate how to use HMA as real-time learned simulation model in robotics and compare it to IRASim [60] for controllability and visual fidelity. First, in Tab. 3, we observe better visual fidelity and controllability by using pre-trained HMA model with discrete image tokens. In Fig. 7, we qualitatively compare different tokenizers along with training objectives. We find soft tokens to contain richer visual information in general, but take more space and compute to store and genereate. In Tab. 2, our MAR model achieves better visual fidelity and controllability compared to IRASim [60] despite using less than 1/8th of the parameters (Tab. 1). When using similar number of parameters, HMA is significantly faster than the amortized version of [60]. Our fastest discrete model can run up to 22Hz, which allows for real-time interactions. The speedup compared to full-sequence diffusion comes from the autoregression process and the architecture, which only 7 Figure 7. Qualitative Comparisons Between Tokenizers and Models. Despite longer convergence time, diffusion-based methods (Eq. (3)) on soft tokens generate better visual quality than on VQ tokens (Eq. (2)), qualitatively and measured by PSNR. Figure 8. Video Controllability. HMA can follow user action inputs to generate physically plausible object permanence (top row) and block pushing interactions (bottom row). These video predictions are both at out-of-distribution settings and at much longer horizon than training (over 100 frames). requires computation on the diffusion heads rather than the full Transformer. Compared to full-sequence diffusion models, autoregressive models learn to predict one frame/action at time, which is natural for robotic interactions. Qualitatively, we also find interacting with our learned simulators to be more reactive and consistent than [60], with only small compounded errors in generation quality when running long autoregressive inferences (e.g. Fig. 8) Given the scaling performance (e.g. Fig. 6) in Sec. 4, we expect training longer with more compute resources to further improve the generation quality. In Fig. 8, we build simple user interface to test the performance of HMA under an out-of-distribution setting and find stable simulation quality. Under real-world compute and inference speed constraints for robotic policy and simulation use cases, HMA with masked autoregression is more suitable than full-sequence diffusion, which can be of order faster to reach real-time speed. This showcases the potential of HMA for policy evaluation and synthetic data generation in the real world, which can save effort in evaluation and collecting data when scaling robot learning. 5.2. Evaluating HMA with Simulator In this section, we evaluate the controllability and visual fidelity of HMA against the ground truth in simulation. In Tab. 4, we observe that pre-trained models can help with the performance compared to training from scratch. Moreover, HMA HMA + PSNR 21.01 22. Perplexity PSNR 305.87 189.83 0.01 0.06 LPIPS 0.19 0.17 Policy Evaluator Ground Truth Simulator HMA Simulator 1 0.38 0. 2 0.52 0.56 3 0.70 0.66 4 1.00 0.73 Table 3. Real World Finetuning. HMA + denotes finetuned model based on pre-trained checkpoints while HMA trains from scratch on the finetuning data. This experiment uses the discrete loss baseline. Table 5. Policy Evaluation Results Across 4 Different Policies. We observed positive correlations of the evaluation results for 4 different policies bewteen the ground truth and learned simulators. The Pearson ratio between evaluations is 0.95. the evaluation results of four baseline policies, each trained with the same data but differing in training iterations before full convergence. We train HMA on the perturbed datasets generated earlier to evaluate groundtruth PSNR. We evaluate each policy using 50 runs and observe correlation of evaluation statistics between policies evaluated in the Mujoco Simulator [46] and policies evaluated in the learned simulators. Note that since rewards and success criteria rely on the ground truth states, we use manual human annotation to determine the evaluation results using learned simulators. This can be done in batches and extending to using foundation models for reward labeling can be future works. There are still failure cases in capturing the physical details of dropping and catching the boxes. But we also see many highly realistic interactions with only limited amounts of data. Notably, even though the training horizon is only 12 steps, the autoregressive horizon of HMA at inference time can generalize to over 100 steps. We also report the time taken to evaluate policies in both ground truth and learned simulators. Each episode in the simulation has maximum of 100 timesteps and the average simulation time is 32 seconds. This is still slower compared to 9 seconds in the Mujoco simulator, but the potential of simulating deformable, granular objects in cluttered realistic scenes for policy evaluation is promising. 5.4. Synthetic Data Generation We investigate generating synthetic data for policy training using HMA. We leverage dataset of 100 action trajectories to generate synthetic data (video, action) pairs based on trained HMA. We evaluate the quality of the generated data by adding different amounts of them to 10 original trajectories. We then train baseline DP policies for 10k iterations and evaluate for 50 trials. In Tab. 6, we show policy improvements by training on 90% of synthetic data in the low-data regime, to the extent that matches the full original data with 100 trajectories. This also leverages HMAs ability to generate long-horizon videos without much degradation. We conduct similar experiments in the Language Table [31] benchmark. Due to no access to real robots, we measure the data quality by validation learning losses. We use 10 out of 100 trajectories from the original benchmark. We then replay and render-augment it with HMA from 10 to 90 traFigure 9. Policy Evaluation with HMA. By learning the actionvideo dynamics over both successful and failed examples, HMA can be used to evaluate policies, similar to traditional simulator [46]. The autoregressive horizon at inference time is 10 times more than the training time horizon. HMA HMA + PSNR 24.17 25.11 Perplexity 20.69 11.82 PSNR 19.19 20.20 Perplexity 1193.70 103. Table 4. Simulation Transfer Learning. We show that pre-trained HMA can help with fine-tuning using cross-entropy losses and diffusion losses jointly. where HMA + denotes the finetuned model based on pre-trained checkpoints. through the PSNR and Perplexity metrics with ground truth, we observe better robustness and less video divergence with pre-training on large amounts of heterogeneous data. In Fig. 9, we show some interaction examples with the learned video models compared to the ground truth simulation. The experiments show that the learned simulator is reactive despite being finetuned on only 200 trajectories. 5.3. HMA for Policy Evaluation In this section, we show that well-trained video model can be used as simulator for evaluating policy performance. Note that since policy can be flawed, simulator must simulate both succeeded and failed examples, which might come from broader sets of interaction data than high-quality demonstrations, as shown in Fig. 9. Note that learned models for policy evaluation can be used with any particular policy. Specifically, Diffusion Policy (DP) [11] is trained on Robomimic Lift [32] task with 200 trajectories for 10k iterations with an action horizon of 16. In Tab. 5, we present 8 Success in [32] Validation Loss in [31] +10 +50 original +0 82% 90% 96% 100% 100% 0.87 1.72 1.16 1.09 0. +90 Table 6. Synthetic Data for Policy Learning. We evaluate the quality of generated synthetic data by adding different numbers of generated video trajectories in [32] and [31], from 10 to 100, to fixed subset (10 trajectories) of the original data (100 trajectories). We then conduct policy training and evaluation and report the Robomimic success rates (top row) and Language Table validation losses (bottom row). jectories with image observations generated by our systems. The training pipeline has language and image inputs and we measure the MSE losses. Using small diffusion policies (17M), we show an improved BC policy performance trained with generated data from HMA. We plan to explore removing the dependence on action trajectories and initial states. We can also scale up training and generating more unseen trajectories in complex realworld settings. We explored using dynamics models for policies but did not achieve surprising results. The hypothesis is that the perception used to predict actions is still not unified with the image generation objectives with VQ codes. One underexplored capability of autoregression in robotic policies is that single frame can allow autoregressive generation of longer action sequences beyond the training context, in contrast to full sequence generation policies such as ACT [58] and DP [11]. 6. Conclusion In this work, we present HMA, masked autoregression approach for training action-video dynamics models from heterogeneous data. Towards this direction, we investigate the scaling behavior of video models in heterogeneous pretraining. We explore multiple downstream applications in robotics and demonstrate the generality and efficiency of the framework. We demonstrate that real-time interactive video simulation, much faster than previous works, can be learned. HMA is also used to evaluate policy training, generate synthetic data, and act as policies. Limitations include the imperfect controllability of the dynamic models with limited data as well as the limited policy performance for action generation. Future works include studying autoregressive policy performance with real robots and complex setups, generating synthetic data at large scale, and investigating world models for longhorizon planning and model predictive control. We hope this work will shed some light on building interactive world models for embodied intelligence. 9 7. Acknowledgements We would like to thank Kaiming He, Tianhong Li, Shuang Li, Yilun Du, and Xiaolong Wang for their early discussions and suggestions. We also thank 1X for open-sourcing the 1XGPT starter code and tokenizer."
        },
        {
            "title": "References",
            "content": "[1] 1X Technologies. 1X World Model Challenge, June 2024. 1, 2, 3, 4 [2] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and Francois Fleuret. Diffusion for world modeling: Visual details matter in atari. arXiv preprint arXiv:2405.12399, 2024. 1, 2, 3 [3] Dimitri Bertsekas and John Tsitsiklis. Neuro-dynamic programming: an overview. In Proceedings of 1995 34th IEEE conference on decision and control, volume 1, pages 560564. IEEE, 1995. 2 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 4 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 2 [6] Jake Bruce, Michael Dennis, Ashley Edwards, Jack ParkerHolder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 3, 5, 6 [7] Arunkumar Byravan and Dieter Fox. Se3-nets: Learning rigid body motion using deep neural networks. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 173180. IEEE, 2017. [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 2, 3, 4, 7 [9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097, 2021. 2 [10] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash Kumar. Genaug: Retargeting behaviors to unseen situations via generative augmentation. arXiv preprint arXiv:2302.06671, 2023. 2 [11] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. 8, 9 [12] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. The epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):41254141, 2020. [13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, pages 123, 2022. 4 [14] Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, and Sergey Levine. Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. arXiv preprint arXiv:2408.11812, 2024. 1 [15] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, et al. Video language planning. arXiv preprint arXiv:2310.10625, 2023. 2 [16] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 27862793. IEEE, 2017. 2 [17] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. 4 [18] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 2 [19] Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828, 2023. 2 [20] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 6 [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2, 6 [23] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. [24] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022. 2 [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 1 [26] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 2 [27] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 2, 3, 4, 7 [28] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv preprint arXiv:1810.01566, 2018. [29] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Perez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. 2, 3 [30] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 4 [31] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. 2, 6, 7, 8, 9 [32] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martın-Martın. What matters in learning from offline human demonstrations for robot manipulation. In arXiv preprint arXiv:2108.03298, 2021. 2, 4, 6, 8, 9 [33] Richard Murray, Zexiang Li, and Shankar Sastry. mathematical introduction to robotic manipulation. CRC press, 2017. 2 [34] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. Advances in neural information processing systems, 31, 2018. [35] Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. 4 [36] OpenAI. Gpt-4 technical report, 2023. 1 [37] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2, 4, 5, 6 [38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1 [39] Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, and Jitendra Malik. Humanoid locomotion as next token prediction. arXiv preprint arXiv:2402.19469, 2024. 2 [40] Marc Rigter, Tarun Gupta, Agrin Hilmkil, and Chao Ma. Avid: Adapting video diffusion models to world models. arXiv preprint arXiv:2410.12822, 2024. 1, 2 [41] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked world 10 [56] Michael Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang, and Mohammad Norouzi. Autoregressive dynamics models for offline policy evaluation and optimization. arXiv preprint arXiv:2104.13877, 2021. 2 [57] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [58] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with lowcost hardware. arXiv preprint arXiv:2304.13705, 2023. 9 [59] Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, and Chelsea Finn. Nerf in the palm of your hand: Corrective augmentation for robotics via novel-view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1790717917, 2023. 2 [60] Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong. Irasim: Learning interactive realrobot action simulators. arXiv preprint arXiv:2406.14540, 2024. 1, 2, 3, 6, 7 models for visual control. In Conference on Robot Learning, pages 13321344. PMLR, 2023. 2 [42] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv preprint arXiv:1701.06538, 2017. 4 [43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 4 [44] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 1 [45] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. 2 [46] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 50265033. IEEE, 2012. [47] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [48] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines, 2024. 1, 2 [49] Lirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. arXiv preprint arXiv:2409.20537, 2024. 1, 3, 4 [50] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [51] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. 4 [52] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. LearnarXiv preprint ing interactive real-world simulators. arXiv:2310.06114, 2023. 1, 2 [53] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. 2 [54] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020. [55] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imagined experience. arXiv preprint arXiv:2302.11550, 2023."
        }
    ],
    "affiliations": [
        "MIT",
        "Meta, FAIR",
        "UIUC"
    ]
}