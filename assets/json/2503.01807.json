{
    "paper_title": "Large-Scale Data Selection for Instruction Tuning",
    "authors": [
        "Hamish Ivison",
        "Muru Zhang",
        "Faeze Brahman",
        "Pang Wei Koh",
        "Pradeep Dasigi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection."
        },
        {
            "title": "Start",
            "content": "Large-Scale Data Selection for Instruction Tuning Hamish Ivison1,2, Muru Zhang1,3, Faeze Brahman2, Pang Wei Koh1,2, Pradeep Dasigi2 1University of Washington, 2Allen Institute for AI, 3University of Southern California hamishiv@cs.washington.edu 5 2 0 2 3 ] . [ 1 7 0 8 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Selecting high-quality training data from larger pool is crucial step when instructiontuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/ automated-instruction-selection."
        },
        {
            "title": "Introduction",
            "content": "Instruction tuning has quickly become crucial element of building modern language model (LM) systems, and step for which curating high-quality data is especially important (Zhou et al., 2023; Wang et al., 2023). Finetuning on as few as 1,000 carefully chosen samples can yield models more preferred by human users than models trained on noisier datasets 10 times larger (Zhou et al., 2023). 1 Figure 1: Performance against estimated compute cost of varied data selection methods when selecting 10k points from data pools consisting of 200k (left points) and 5.8M (right points) data points in the single-task setup described in 4.1. We do not run LESS with 5.8M samples due to its high compute cost. Most data selection methods do not improve in performance with larger pool, with the exception of RDS+ and Embed (GTR). We shade the Pareto frontier of efficiency and performance in red. The importance of data quality has led to surge of papers proposing and investigating methods for selecting small (110k) datasets that outperform larger (50-200k) ones (Chen et al., 2024; Xia et al., 2024; Li et al., 2024b; Liu et al., 2024). However, the sizes of the selected datasets are often significantly smaller than those of commonly used open post-training datasets, which typically contain hundreds of thousands to millions of examples(Ivison et al., 2023b; Lambert et al., 2024; Llama Team, 2024; DeepSeek-AI, 2025). As such, it is unclear how well these data selection approaches scale as both the number of the selected data points and size of the data pool grow. Additionally, prior work suggests that common data selection approaches underperform random selection when considering the additional compute cost incurred by running the data selection techniques themselves even in smallerscale settings (Yin and Rush, 2024). It is unclear how performance and compute costs further change as we further scale beyond smaller-scale (selecting 10k data points from 200k samples) settings. In this work, we aim to answer these questions by investigating how well data selection methods work for selecting instruction-tuning data when used to select larger datasets (100s of thousands of samples) from even larger data pools (millions of samples). We construct data pools consisting of all data considered for TÜLU 2 (Ivison et al., 2023b) and TÜLU 3 (Lambert et al., 2024), state-of-the-art instruction-tuned models with publicly released instruction tuning data. These data pools are large, diverse, unbalanced, and of mixed quality, requiring automated data selection techniques that can scale to millions of samples, filter out noise, and maintain enough diversity to ensure effective generalization. This allows us to examine how well data selection techniques scale up to selecting millions of samples from pools up to 6M data points in size. We select for and evaluate on seven diverse evaluations, covering diverse range of skills from code generation (HumanEval) to general chat (AlpacaEval). We additionally investigate selecting single dataset for multiple tasks, reflecting how instruction-tuned models are often used to perform multiple tasks at once. We test nine different data selection approaches in our setting, covering variety of different approaches to data selection: gradient-based influence methods (Xia et al., 2024), embedding-based methods (Zhang et al., 2018; Hanawa et al., 2021; Ivison et al., 2023a), and loss/perplexity-based methods (Yin and Rush, 2024; Antonello et al., 2021; Marion et al., 2023; Ankner et al., 2024; Li et al., 2024b). Surprisingly (and contrary to prior work), we find that simple embedding method using the hidden states of pretrained LM works best overall across all settings tested, consistently beating all other approaches. In particular, we find that: 1. Many dataset selection methods struggle with increased dataset size. 4 of the 7 methods we examine drop in performance as we increase the data pool they select over, even when selecting datasets of the same size. In contrast, RDS+ not only improves as the data pool size grows, but also outperforms all other methods by at least 2 points on average. This highlights that examining the scaling properties of data selection is important for identifying strong methods (4.1). 2. Similarly, when selecting for multiple tasks at once, using RDS+ outperforms baselines at varied data sizes, selecting datasets from 10k to 2.5M samples in size. Additionally, we find that RDS+ even outperforms TÜLU 2, which was trained on human-curated mixture (4.2). 3. Controlling for the compute used during selection, our RDS variant outperforms random selection by an average of two points when selecting hundreds of thousands to millions of samples, whilst using less compute. This stands in contrast to prior work (Xia et al., 2024), which does not scale up to large enough sizes to observe the point at which RDS becomes more compute-efficient. This highlights that data selection techniques may only become particularly useful for larger-scale settings than those typically used in prior work. (4.3) Overall, by examining performance when selecting over much larger data pools that are tied to real instances of popular instruction-tuned models, we believe that our findings better reflect how data selection approaches can aid LM training when working at the data scales used to train deployed models."
        },
        {
            "title": "2 Related Work",
            "content": "Data Selection For Instruction Tuning Earlier approaches to data selection for instruction tuning relied primarily on careful human curation (Zhou et al., 2023) or extensive ablation studies to determine optimal data mixtures (Wang et al., 2023). Recently, many automated data selection approaches have emerged, ranging from using GPT-based quality assessment (Chen et al., 2024; Wettig et al., 2024; Lu et al., 2023; Liu et al., 2024), embedding or perplexity features from model inference (Marion et al., 2023; Ivison et al., 2023a; Li et al., 2024b; Zhang et al., 2024c; Sachdeva et al., 2024; Li et al., 2024a; Du et al., 2023; Zhang et al., 2024b), gradient-based methods (Xia et al., 2024; Han et al., 2020; Zhang et al., 2024a; Yu et al., 2024), or shallower heuristics (Zhao et al., 2024a; Cao et al., 2024; Li et al., 2024d). These works typically focus on finding small subsets of larger datasets that can match or outperform using the entire dataset, often selecting subsets of roughly 10k samples in size (Chen et al., 2024; Xia et al., 2024; Li et al., 2 2024b) from datasets with 100s of thousands of samples. This does not match the known sizes of various instruction-tuning datasets used in practice, which range from 300k to millions of samples in size (Touvron et al., 2023; Llama Team, 2024; Ivison et al., 2023b; Lambert et al., 2024). In this work, we aim specifically to study how these methods perform when selecting large datasets from large pools, and find that many proposed methods fall short of random selection and an RDS baseline in this setting. Surveying Data Selection Methods Some prior studies have similarly studied how well automated data selection techniques work for instruction tuning: Yin and Rush (2024) examine how well various data selection approaches scale in computeconstrained scenarios. However, they use relatively small data pool, only examine single task setting, and assume only single epoch of training (not common in instruction tuning). Diddee and Ippolito (2024) also examine how well data selection strategies generalize across different pools at various scales, but similarly only examine selecting relatively small datasets (up to 10k samples) and small pools (up to 200k in size). In contrast to both these works, we further scale the setting to use millions of samples, more evaluations, and more diverse datasets, and find that this changes our view of what method works best, with RDS outperforming random selection even in FLOPscontrolled scenarios. Dai et al. (2024) examine how to balance influence scores when selecting multi-task data, and uses selection algorithm similar to our proposed round-robin approach. However, we also examine how well multi-task selection works when selecting from significantly larger data pools, across multiple different pools and base models."
        },
        {
            "title": "3 Data Selection Methods",
            "content": "We now describe set of popular data selection methods commonly used and tested in prior work alongside the experimental setup we use to investigate them. We assume that one has pool of data and wishes to select up to instances from the pool. Additionally, we assume that the user has some (10s to 100s) of query samples from query set . This query set is small dataset that is in the same distribution as the evaluation set (e.g., we can use existing validation sets as query sets if available). We assume both and contain prompts and responses (i.e., both are labelled). In practice, this query split can either be task-specific (i.e., some query split of the downstream task we want to test on), or contain samples from variety of tasks that do not overlap with downstream test tasks, but are reflective of our desired test distribution in some way. Each method either (a) takes in dataset as input and produces score for each point or (b) takes in dataset and query points and produces score for each v, V, pair, which we then aggregate to compute score for each (described further in 3.1). Given this, we explore the following methods. We pick set of methods that represent simple but varied ways to select data: using model embeddings, using loss-based features (perplexity, IFD), using model gradients, and random selection. Our choices match common baselines used in prior work (Xia et al., 2024; Yin and Rush, 2024). Random Selection. We explore random selection, common strong baseline for data selection. We report both taking random samples of given data pool, and taking balanced random samples, where we uniformly sample from data sources. If we run out of data from given source, we divide its remaining budget equally among the dataset sources we have not yet exhausted. Perplexity. We compute the loss of each on our original base model and use it as its score, following prior work (Yin and Rush, 2024; Antonello et al., 2021; Marion et al., 2023; Ankner et al., 2024). We use the same setup as Yin and Rush (2024) and examine both mid-ppl (taking points in the middle of the score distribution) and top-ppl (taking only the highest loss points). IFD. We follow the procedure used in Li et al. (2024b), which involves first training model on representative samples from the dataset, and then scoring data points using the ratio of the answer loss given the question to the loss of the answer on its own (called the IFD score). We compute the IFD score for all points D. We use the codebase provided by the authors.1 When selecting data from the 5.8M TÜLU 2 unfiltered pool, we use the same model trained on the 200k-size pool, as the smaller pool is simply subsampled version of the larger one. 1https://github.com/tianyi-lab/Cherry_LLM 3 LESS. We follow the procedure outlined in Xia et al. (2024), training LoRAs on random subset of the data, and then selecting data by computing the gradient-based influence of each on each to obtain the selection score for the pair v, d. We use the codebase provided by Xia et al. (2024).2 Embedding. We use an embedding model to score each pair v, V, based on the cosine similarity of and when processed by the embedding model. We test using two different embedding models: NV-Embed-v2 (Lee et al., 2024), which was at the top of the MTEB leaderboard (Muennighoff et al., 2023b) at time of experimentation, and GTR-base (Ni et al., 2022), following Yin and Rush (2024). RDS+. Finally, similar to embedding-based models, we explore using the hidden representations of the model we plan to train, as opposed to trained embedding model (Zhang et al., 2018; Hanawa et al., 2021; Ivison et al., 2023a; Xia et al., 2024) representation-based data similarity (RDS). We use custom variant of RDS in which we take position-weighted mean pool of the last hidden layer states (see G.1 for details). We compute this for each v, in and and compute the cosine similarity for each pair v, V, D. We ablate alternate variants of RDS in App. G.1, and denote our tuned version of RDS as RDS+. 3.1 Selection & Aggregation For methods that provide score for each pair v, V, D, we end up with scores for each data point D. As such, we need to determine how to aggregate these scores to determine the most valuable points D. In pilot experiments, we found using round-robin approach worked best, iteratively adding to the selected pool the highestscoring point for each until we reach desired size. This is what we use to construct taskspecific datasets. We illustrate the algorithm in Alg 1. For multi-task scenarios, we also need to aggregate on task-level when combining scores from different tasks. We say that Vt is the query set for task t, and S[vt, d] is the score computed between dataset point and query point vt. We then construct dictionary of scores where S[t, d] represents the score of for task by 2https://github.com/princeton-nlp/LESS Algorithm 1 Single-task selection method Require: dictionary of scores such that S[v, d] is the score given by selection method between points and D. the desired size of our selected Require: n, dataset. [] while < do for do argmaxdDS[v, d] S[v, d] inf Set score such that this point will not get chosen again. if then break end if end for end while return setting S[t, d] = maxvtVt S[vt, d]. We then run round-robin procedure where we iterative over tasks, pop out the highest-scoring data point, add to our dataset, and repeat until we have dataset of the desired size (after deduplication). This is essentially running Alg. 1 again, but replacing with and with . We also experimented with averaging task-level scores together, but found this performed worse overall (See App. for details). 3.2 Data Pool In this work, we wish specifically to explore how well automated data selection methods work at scale. As such, we test the above methods in three different settings, across two different dataset pools: TÜLU 2 unfiltered and TÜLU 3 unfiltered. Both pools comprise of millions of samples with diverse data sources, and are constructed by examining the original datasets chosen for TÜLU 2 and 3 (state-ofthe-art post-trained models with openly available instruction data) and retrieving the same datasets but performing no downsampling. We perform exact-match deduplication of samples over the pool to ensure all samples are unique. Note that the unfiltered mix is extremely unbalanced, comprising mostly of data from FLAN and Open Orca. We compare the makeup and size of our data pools to pools in prior work (Xia et al., 2024; Li et al., 2024b) in Fig. 2. Notably, our pools are significantly larger and more diverse than prior work, and are based on real datasets used for open instructiontuned models. 4 Figure 2: Size and makeup of data pools considered in this work (unfiltered Tulu 2, 3) and in past work (Xia et al., 2024; Chen et al., 2024; Li et al., 2024b). We provide the size of each pool on top of each bar. Each color represents different dataset. See App. for more details on data pool composition. 3.3 Experimental Design We extend our experimental design off TÜLU 2 (Ivison et al., 2023b), an open-source state-of-the-art dataset and model family (at time of release). As TÜLU 2 is finetuned starting from Llama 2 base models (Touvron et al., 2023), we primarily experiment with the Llama 2 7B model. We additionally report results using the TÜLU 3 mixture (Lambert et al., 2024), used to train state-of-the-art open instruction-tuned model, and Llama 3.1 (Llama Team, 2024) in 4.2, and 8 varied models in 5. Training. For finetuning, we fully finetune for two epochs with batch size of 1, 128 gradient accumulation steps, learning rate of 2e5 (1e5 for 70B size models), linear learning rate warmup for the first 3% of training steps, and linear cooldown for the rest of training. This follows the settings used in prior work (Wang et al., 2023; Ivison et al., 2023b). We report the mean across three random runs (including reselecting data) for random baselines and single-run scores for other settings. Evaluation. For evaluation, we largely follow the same evaluation procedures as described in Wang et al. (2023); Ivison et al. (2023b). We evaluate on (and select for) MMLU (Hendrycks et al., 2020), GSM8k (Cobbe et al., 2021), BBH (Suzgun et al., 2022), TydiQA (Clark et al., 2020), HumanEvalCodex (Chen et al., 2021a), Squad (Rajpurkar et al., 2016), and AlpacaEval (Li et al., 2023). We either use the existing few-shot examples or create custom development splits from the evaluations for data selection. We also experiment with using outof-domain query sets in 4.2. We provide precise details on each evaluation and query set in Appendix A. FLOPs Estimation. For the FLOPs estimates used throughout the paper, we follow Kaplan et al. (2020) in estimating the compute cost of training step as roughly 6N FLOPs per token processed and an inference step as 2N per token processed, where is the parameter count of the model. Based on this, we estimate the FLOPs of each method using the total number of inference and training steps that take place across selection and model training. We provide further details in App. E."
        },
        {
            "title": "4 Results",
            "content": "4.1 Single-Task Data Selection We start by testing the different data selection methods over TÜLU 2 unfiltered both with smaller pool (TÜLU 2 unfiltered downsampled to 200k samples) and the entire data pool. We select 10k samples, train models, and then evaluate for each task separately. We report the results in Table 1. We do not report LESS performance using the 5.8M pool as it requires computing gradients three times over all 5.8M samples in the pool, which is beyond our computational budget. RDS+ performs best. RDS+ is the highestperforming method on average in both settings, despite being similar or cheaper in cost than most other baselines  (Fig. 1)  . Additionally, models trained with RDS-selected data perform the best in each task individually (with the exception of SQuAD, where they are close second) when selecting from the full pool. PPL, Random, IFD, and Embed (NV) perform worse with larger pool. As seen in Fig. 1, multiple methods we test actually perform worse when we select over the larger data pool, suggesting that In contrast, both they cannot effectively scale. 5 Method MMLU GSM8k BBH TydiQA Codex Squad AlpacaEval Average Random (unbal.) Random (bal.) LESS Embed (NV) Embed (GTR) Top-PPL Mid-PPL IFD RDS+ Random (unbal.) Random (bal.) Top-PPL Mid-PPL Embed (GTR) Embed (NV) IFD RDS+ Selecting 10k datapoints for single tasks from 200k data points 45.8 45.4 46.7 46.9 47.1 46.4 46.9 42.6 47.0 17.2 16.6 21.5 19.9 20.2 9.9 15.5 17.5 20. 41.5 40.7 41.8 42.4 42.4 33.8 39.7 39.1 42.7 52.7 50.7 57.5 52.3 52.4 46.9 53.6 38.7 52.3 27.0 27.9 19.6 25.0 27.7 20.3 25.7 29.1 31.8 83.5 80.6 84.7 88.3 88.3 77.6 81.6 57.9 88.2 18.4 33.2 37.9 24.0 16.7 4.3 12.9 45.9 42.3 Selecting 10k datapoints for single tasks from all 5.8M data points 46.9 45.3 44.6 46.1 45.0 47.0 41.9 47.5 17.4 16.5 6.1 14.9 29.9 23.1 13.0 33.9 42.1 40.2 23.9 40.7 42.8 41.1 37.9 42.9 53.2 50.9 40.4 52.7 45.5 51.2 40.0 54.9 26.8 28.2 20.9 23.0 29.7 29.1 26.4 32.4 83.6 78.0 74.4 82.3 88.3 88.8 46.6 88. 15.1 29.0 2.5 12.5 43.8 10.9 44.6 53.5 40.9 42.2 44.2 42.7 42.1 34.2 39.4 38.7 46.4 40.7 41.1 30.4 38.9 46.4 41.6 35.8 50.5 Table 1: Single-task performance of different data selection techniques over the TÜLU 2 unfiltered set. Each cell reports the performance of model trained with 10k samples chosen for that particular target task. We show results selecting from downsampled form or full set of the TÜLU 2 unfiltered set. We find RDS performs best overall, even beating more computationally expensive methods like LESS. RDS+ and Embed (GTR) improve with larger pool. In later areas of the paper, we focus on RDS+ as we wish to achieve the strongest possible performance with larger compute budget. 4.2 Multi-task Selection We next examine how well data selection methods work when selecting single datasets while targeting multiple tasks. We use the round-robin method described in 3.1 to balance selection across tasks for task-specific methods3, or just use the existing scores for task-agnostic methods. We select 326k samples from the TÜLU 2 unfiltered set, matching the number of samples in the TÜLU 2 SFT mixture, and so also compare to TÜLU 2. We show the results in Table 2 and find that: RDS+ still performs best overall. RDS+ outperforms all other methods on average, including TÜLU 2 itself, showing that RDS+ can humancurated mixtures. In fact, we observe that all methods except RDS+ and Embed (GTR) still underperform random selection, suggesting that embedding-based methods overall are best for data 3We found this worked best across multiple data selection methods. selection. RDS+ selection performs well even when the evaluations are out of distribution. We explore using WildChat (Zhao et al., 2024b) and Arena Hard (Li et al., 2024c) as out of distribution query sets for selecting data. Crucially, this means we do not assume access to any query samples from our test tasks. We find that using Arena Hard samples performs close to RDS+, showing we can select high-quality samples without assuming any data from the evaluations in our test suite. This suggests that using high-quality selection set results in data mixture that generalizes well to unseen tasks. RDS+ performs strongly with other data pools. In order to test how well our findings generalize, we make use of the recently released state-of-theart TÜLU 3 data and model (Lambert et al., 2024), comparing the model trained with RDS-selected data the officially released TÜLU 3 SFT model. We then select 939k instances, equal to the size of TÜLU 3 SFT Mix, from the pool and finetune Llama 3.1 models on it using the hyperparameters from Lambert et al. (2024). We show our results in Table 3. We find that RDS+ round-robin se6 Method MMLU GSM8k BBH TydiQA Codex Squad AlpacaEval Average Rand. (unbal) Rand. (bal) Top-PPL Mid-PPL Embed (GTR) Embed (NV) IFD TÜLU 2 RDS+ RDS+ - Wildchat RDS+ - Arena Hard 52.2 51.5 49.1 53.1 49.9 50.6 45.7 50.0 50.2 50.9 48.1 18.0 21.8 10.5 13.3 32.8 28.7 21.8 22.7 35.2 24.8 36.2 44.5 45.1 39.4 42.8 44.6 44.4 41.2 45.1 44.7 43.6 43. 55.3 50.7 49.4 52.4 54.4 56.0 39.5 54.0 56.3 57.3 51.8 25.7 32.2 21.6 20.3 30.4 30.4 27.7 33.1 35.1 31.1 31.8 81.5 87.9 80.3 86.2 88.4 89.1 17.0 86.9 89.0 87.3 81. 33.9 43.2 5.6 20.7 35.7 17.9 57.4 54.4 45.6 39.3 59.4 44.5 47.5 36.6 41.3 48.0 45.3 35.7 49.5 50.9 47.8 50.4 Table 2: Multi-task performance of dataset selection methods when selecting 326k samples from the full TÜLU 2 unfiltered pool. Each row reflects the performance of single model trained on single dataset chosen to perform well across tasks. For WildChat and Arena Hard we use samples from WildChat and Arena Hard for selection. Figure 3: Average multi-task performance against FLOPs cost (including selection) for balanced random and RDS+. We label points with the % of the total data pool used. RDS+ outperforms random selection significantly when selecting less data, and is more FLOPs efficient at larger selection sizes. See App. for details on FLOPs estimates. lected data outperforms the official TÜLU 3 SFT checkpoint and random baselines, showing that RDS+ remains an effective data selection method even on different base model and with different selection data pool. 4.3 Scaling Multi-task Selection Finally, we examine how the performance of our selection method changes as we scale the amount of data being selected. Due to the high cost of these experiments (fully finetuning on millions of samples), we only look at RDS+ (the bestperforming method overall) and random selection Figure 4: Average multi-task performance against number of samples selected. RDS+ consistently beats balanced random at all data sizes tested, up to using the entire data pool. (a strong, cheap baseline). We examine RDS+ as the strongest performing data selection method, and compare to random selection as cheap but often effective baseline (that also outperforms all but two of the tested data selection approaches). We select for multiple tasks at once from the full TÜLU 2 unfiltered pool, selecting varying numbers of samples with RDS+. We plot average performance against the number of samples selected in Figure 4. We also plot performance against total FLOPs used for selection and training in Figure 3. We observe that: RDS+ consistently beats balanced random selection across different data selection sizes  (Fig. 4)  . This shows that targeted selection is even useful when selecting datasets 100s of thousands to 7 Method MMLU GSM8k BBH TydiQA Codex Squad AlpacaEval Average Random (unbal.) Random (bal.) TÜLU 3 SFT RDS+ RDS+ - Arena Hard 61.6 62.1 62.2 62.5 57.0 81.2 76.0 74.3 77.6 78.7 66.8 68.6 68.2 66.6 59. 71.1 68.8 67.4 72.1 49.4 76.4 87.2 83.8 83.8 75.7 89.7 87.4 85.5 90.2 66. 75.6 72.4 71.9 80.2 84.5 74.6 74.7 73.3 76.1 67.3 Table 3: Multi-task performance of RDS against baselines when finetuning from Llama 3.1 8B base and selecting 939k samples from the TÜLU 3 unfiltered mixture following the multitask setup in 4.2. For Arena Hard we use samples from Arena Hard as the query set. RDS+ outperforms the official TÜLU 3 SFT model. millions of data points in size. Furthermore, RDS+ performs similarly to training on all data (50.8 vs 50.9) even when selecting only 326,000 samples (roughly 6% of the overall pool), and outperforms training on all data when selecting over 1M samples. RDS+ can achieve better performance with less compute when selecting more data points  (Fig. 3)  . When taking into account the compute used for selection and training, we find that the extra compute incurred by RDS does not pay off until we select relatively large datasets (roughly 326k), after which it becomes significantly better than random selection. This highlights once again that examining data selection performance at scale is important to show the potential of some methods. Additionally, we note that the cost of RDS+ could potentially be further reduced by reusing existing embeddings or using smaller models for selection."
        },
        {
            "title": "5 Analysis",
            "content": "What gets selected? We examine qualitatively what samples get selected by RDS+, IFD, and PPL (see App. for visualization). For RDS+, We find that the sources selected vary reasonable amount depending on downstream evaluation, often with intuitive explanations: e.g., GSM8k requires strong chain-of-thought reasoning abilities, and so the chain-of-thought data is upweighted relative to the normal distribution. Similarly, for HumanEval-Codex the Code Alpaca dataset is upweighted. Additionally, we observe that PPL and IFD appear to select more ShareGPT and FLAN data respectively than RDS+ or random selection, suggesting these methods have biases to particular types of data. This also explains why IFD performs relatively well in AlpacaEval (which improves when training on GPT-derived data), but drops in other aspects, while Top-PPL achieves very low AlpacaEval scores in Table 1. Can we reduce the cost of RDS+? While we have used the same model for selection and training for RDS+, we also wished to investigate to what extent changing the model used for selection impacts data selection performance, potentially allowing RDS+ to use less or more compute for selection. Encouragingly, we find that using smaller models for selection (even from different model families) can still results in strong performance, suggesting that RDS+ can indeed be made computationally cheaper by using smaller models for selection. However, we do not observe gains from using larger models compared to Llama 2 7B, suggesting that using larger models for selection does not yield improved performance. We provide further details in App. F."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we have explored how well variety of data selection techniques work when selecting datasets of up to 2.5M samples from pools comprising of close to 6M data points. By examining data selection techniques in these settings, we find that many methods proposed in the past not only underperform random selection baselines, but also perform worse with larger pools of data. Embeddingbased methods, and in particular RDS+, are the primary exception, outperforming all other data selection methods across various selection sizes. Finally, we find that using smaller models to select data can also work well, albeit not as well as larger ones. Exploring how to use smaller proxy models for RDS+ selection at scale is promising future direction to further reduce the cost of automated data selection. We believe that our results overall highlight the importance of testing data selection 8 methods over large, diverse pools of data, and testing how well data selection approaches scale with respect to both data and compute. We will publicly release our data and code to aid in future work."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Hannaneh Hajishirzi, as well as members of UW NLP and Ai2, for useful advice and feedback on earlier versions of this project. We thank Mengzhou Xia for useful advice around running the LESS codebase. This project is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Information under the AI Visiting Professorship Programme (award number AIVP-2024-001), and by the AI2050 program at Schmidt Sciences."
        },
        {
            "title": "Limitations",
            "content": "While we cover reasonable number of base models in this work, we ultimately only use two data pools, TÜLU 2 and TÜLU 3, due to the computational cost of running experiments over millions of instances. We hope to further examine how well our findings transfer to data pools with different characteristics in future work. Additionally, we note that any data selection method that requires model passes over data will scale in cost proportionally with the data pool used to select, which limits the use of this method in extremely large-scale settings where even doing model forward passes over all data may be too computationally expensive. While this is less common in instruction-tuning settings (as there is less instruction-tuning data available generally), it may happen for pretraining datasets containing trillions of tokens (which is not focus in this work). Finally, while we do not explicitly analyze the safety and potential toxicity of our models, we hope that our findings could be used to improve data selection for selecting safety-relevant data, which often does make up some portion of popular instruction data mixes (Lambert et al., 2024). Furthermore, we hope that reducing the total FLOPs cost of training strong instruction models can help aid the overall environmental cost of training LMs (Strubell et al., 2020)."
        },
        {
            "title": "References",
            "content": "Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403. Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew L. Leavitt, and Mansheej Paul. 2024. Perplexed by perplexity: Perplexity-based data pruning with small reference models. Preprint, arXiv:2405.20541. Richard Antonello, Nicole Beckage, Javier Turek, and Alexander Huth. 2021. Selecting informative contexts improves language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 10721085, Online. Association for Computational Linguistics. BIG-bench authors. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, Yulia Tsvetkov, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024. The Art of Saying No: Contextual Noncompliance in Language Models. Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction mining: Instruction data selec2024. tion for tuning large language models. Preprint, arXiv:2307.06290. Sahil Chaudhary. 2023. Code alpaca: An instructionfollowing llama model for code generation. GitHub repository. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2024. Alpagasus: Training better alpaca with fewer data. In The Twelfth International Conference on Learning Representations. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, 9 Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code. Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, and Kentaro Inui. 2021. Evaluation of similarity-based explanations. Preprint, arXiv:2006.04528. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large lanarXiv preprint guage models trained on code. arXiv:2107.03374. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454470. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168. Qirun Dai, Dylan Zhang, Jiaqi W. Ma, and Hao Peng. 2024. Improving influence-based instruction tuning data selection for balanced learning of diverse capabilities. Databricks. 2023. Free dolly: Introducing the worlds first truly open instruction-tuned llm. Blog post. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Harshita Diddee and Daphne Ippolito. 2024. Chasing random: Instruction selection strategies fail to generalize. Preprint, arXiv:2410.15225. Qianlong Du, Chengqing Zong, and Jiajun Zhang. 2023. Mods: Model-oriented data selection for instruction tuning. Preprint, arXiv:2311.15653. Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. 2024. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. Preprint, arXiv:2406.18495. Xiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov. 2020. Explaining black box predictions and unveiling data artifacts through influence functions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5553 5563, Online. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring Massive Multitask Language Understanding. Hamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. 2023a. Data-efficient finetuning using cross-task nearest neighbors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 90369061, Toronto, Canada. Association for Computational Linguistics. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023b. Camels in changing climate: Enhancing lm adaptation with tulu 2. Preprint, arXiv:2311.10702. Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, and Nouha Dziri. 2024. Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. Preprint, arXiv:2001.08361. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Openassistant conversationsdemocratizing large language model alignment. arXiv preprint arXiv:2304.07327. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2024. Tülu 3: Pushing frontiers in open language model post-training. Preprint, arXiv:2411.15124. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. Preprint, arXiv:2405.17428. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. 10 [https://huggingface.co/ Numinamath tir. AI-MO/NuminaMath-TIR](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024a. Superfiltering: Weak-to-strong data filtering In Proceedings of the for fast instruction-tuning. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1425514273, Bangkok, Thailand. Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024b. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 76027635, Mexico City, Mexico. Association for Computational Linguistics. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024c. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Ling-Hao Chen, Junhao Liu, Tongliang Liu, Fei Huang, and Yongbin Li. 2024d. One-shot learning as instruction data prospector for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 45864601, Bangkok, Thailand. Association for Computational Linguistics. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". 2023. Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://huggingface. co/Open-Orca/OpenOrca. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations. Llama Team. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2024. Wizardcoder: Empowering code large language models with evolinstruct. In The Twelfth International Conference on Learning Representations. Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023. When less is more: Investigating data pruning for pretraining llms at scale. Preprint, arXiv:2309.04564. Niklas Muennighoff. 2022. sentence embeddings for semantic search. Preprint, arXiv:2202.08904. Sgpt: Gpt Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. 2023a. OctoPack: Instruction Tuning Code Large Language Models. arXiv preprint arXiv:2308.07124. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023b. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, Dubrovnik, Croatia. Association for Computational Linguistics. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 98449855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. OLMo Team. 2024. OLMo 2: The best fully open language model to date. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Qwen Team. 2024. Qwen2.5: Party of Foundation Models. Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander M. Rush, and Thomas Wolf. 2023. No robots. https://huggingface.co/ datasets/HuggingFaceH4/no_robots. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models. Preprint, arXiv:2308.07074. Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed H. Chi, James Caverlee, Julian McAuley, and Derek Zhiyuan Cheng. 2024. How to train data-efficient llms. Preprint, arXiv:2402.09668. 11 Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Chien, Sebastian Ruder, Surya Guthikonda, Emad Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. 2024. Aya dataset: An open-access collection for multilingual instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1152111567, Bangkok, Thailand. Association for Computational Linguistics. Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2020. Energy and policy considerations for modern deep learning research. Proceedings of the AAAI Conference on Artificial Intelligence, 34(09):1369313696. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. 2024. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, and Arman Cohan. 2024. Sciriff: resource to enhance language model instruction-following over scientific literature. Preprint, arXiv:2406.07835. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. How far can camels go? exploring the state of instruction tuning on open resources. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting high-quality data for training language models. In International Conference on Machine Learning (ICML). Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. LESS: Selecting influential data for targeted instruction tuning. In International Conference on Machine Learning (ICML). Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Junjie Oscar Yin and Alexander M. Rush. 2024. Preprint, Compute-constrained data selection. arXiv:2410.16208. Zichun Yu, Spandan Das, and Chenyan Xiong. 2024. Mates: Model-aware data selection for efficient pretraining with data influence models. In NeurIPS. Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao. 2023. Tablegpt: Towards unifying tables, nature language and commands into one gpt. Preprint, arXiv:2307.08674. Jipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong Zhang, Rui Pan, and Tong Zhang. 2024a. Tagcos: Taskagnostic gradient clustered coreset selection for instruction tuning data. Preprint, arXiv:2407.15235. Qi Zhang, Yiming Zhang, Haobo Wang, and Junbo Zhao. 2024b. RECOST: External knowledge guided data-efficient instruction tuning. In Findings of the Association for Computational Linguistics ACL 2024, pages 1091110921, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. 12 Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In CVPR. Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. 2024c. Autonomous data selection with language models for mathematical texts. arXiv preprint arXiv:2402.07625. Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. 2024a. Long is more for alignment: simple but tough-to-beat baseline for instruction fine-tuning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024b. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems."
        },
        {
            "title": "A Evaluation Details",
            "content": "We provide more details on each evaluation setting and how we constructed the split used for data selection below. In all cases we use an instruction chat template following the TÜLU format, matching the format used during SFT training. 1. MMLU (Hendrycks et al., 2020): We use the official MMLU evaluation script and prompts available at https://github.com/ hendrycks/test. We evaluate using 0 fewshot examples, following the original setup of MMLU. We report average accuracy across test examples. For the query set, we use the dev examples (commonly used as few-shot samples for evaluation). 2. GSM8k (Cobbe et al., 2021): We evaluate models on the test set of GSM. Following Wei et al. (2022), we evaluate with chain-ofthought. We use 8 few-shot in-context examples. Because all answers in GSM are numbers, we extract the last number in the model response as the final answer. We report average accuracy across test examples. For the query set, we use the 8 few-shot examples individually (without the other shots included in the prompt). 13 3. Big Bench Hard (BBH) (BIG-bench authors, 2023; Suzgun et al., 2022): We follow the setup described in the original paper and evaluate with chain-of-thought. The officially provided prompts, which have 3 few-shot incontext examples are used. For the CoT setup, we extract the first word after the phrase So the answer is, or the entire response if there is no such substring present. We report average accuracy over sub-tasks (all of which use accuracy as the primary metric). For the query set, we use the 3 few-shot examples from each task individually (without the other shots included in the prompt). 4. TydiQA (Clark et al., 2020): We follow the setup described in the PaLM 2 technical report (Anil et al., 2023) to evaluate models performance in answering multilingual questions under two settings when the gold passage that contains the answer is given (i.e., gold passage setting). One in-context example is used to familiarize the model with the answering format. For the query set, we use the 9 few-shot examples from each language individually (without any shots included in the prompt). 5. HumanEval Codex (Chen et al., 2021b): We use the HumanEval dataset in the codex paper for evaluating models coding ability. The dataset contains 164 programming problems, where models are prompted to complete the Python function given its docstring. Following the original paper, we compute unbiased estimates of pass@k to measure the functional correctness of models outputs. We report pass@10, sampling with temperature of 0.8. We additionally use the instructions provided by HumanEvalPack (Muennighoff et al., 2023a), as this better suites instruction-tuned models. We create custom test split of 148 samples, and evaluate on those. We use the remaining 16 samples as the query split. 6. SQuAD (Rajpurkar et al., 2016): We use the validation split of SQuAD as test set, comprising of 10,570 questions about Wikipedia articles. We include the article containing the answer in the prompt, and include 3 in-context examples (randomly selected from the train set) in order to ensure the model outputs in Dataset # Query # Test MMLU GSM8k BBH TydiQA Codex Squad AlpacaEval 285 8 81 9 16 500 50 14,042 1,319 6,511 5,077 148 10,570 755 Table 4: Query and test split counts for evaluation datasets. the desired format.4 We report text-based F1. We use 500 random samples from the SQuAD train set as query samples. Figure 5: Histogram of RDS scores for the top 10,000 samples picked for GSM8k and AlpacaEval from the TÜLU 2 unfiltered pool. We find that AlpacaEval instances have lower average similarity than GSM8k. 7. AlpacaEval (Li et al., 2023): We use the package provided by Li et al. (2023), following the default setup for both AlpacaEval 1. We allow the evaluated model to generate up to 8192 tokens, without specifying special stop sequences. We create custom split of 50 samples for the query set, and use the remaining samples for evaluation. We provide summary of the number of query and test samples in Table 4."
        },
        {
            "title": "B Data Pool Sources Breakdown",
            "content": "We provide breakdown of the splits of the TÜLU 2 SFT mixture and our unfiltered version in Table 5. We provide similar comparison between the TÜLU 3 SFT mixture and our unfiltered version in Table 6."
        },
        {
            "title": "C Compute Details",
            "content": "We run all experiments on cluster containing H100s and A100s with up to 8 GPUs per node. We find that training on 10,000 examples takes roughly 30 minutes on 1 H100 GPU, while training on 2.5 million samples (our largest selected set) takes 62 hours on node of 8 H100s. Running the selection itself varies according to method, with RDS itself taking 87 H100 GPU-hours for indexing the TÜLU 3 unfiltered set (although this can be efficiently parallelized). Multi-task Selection Algorithm We experimented with two methods for selecting multi-task datasets: round-robin (the method explained in 3.1) and mean-max. For mean-max, we compute the per-task scores for in the same way as before, but then simply average all task scores for datapoint together. Using the notation , where S[d] from 3.1, we do: S[d] = is the score for d. We then can simply take the top-scoring points as our dataset. tT S[t,d] (cid:80) We compare using round-robin and mean-max for RDS+ and Embed in Table 7. We find that round-robin consistently outperforms mean-max across methods on average, and so we use it in our core experiments. We further investigate why this is the case by visualizing the distribution of RDS scores over the TÜLU 2 unfiltered pool in Figure 5 for GSM8k and AlpacaEval. We observe that scores for GSM8k are consistently higher than AlpacaEval, suggesting that GSM8k scores dominate when averaging task scores together. This means that the final selected dataset is likely to be mostly comprised of samples that are useful for GSM8k, but not AlpacaEval. We find similar trends comparing AlpacaEval to other datasets, suggesting that merely averaging tasks scores leads to some targeted evaluations being under-served. This agrees with concurrent work suggesting that round-robin algorithms 4In pilot experiments, we found that models without incontext samples would often provide the right answer in verbose manner, harming the string-matching-based metrics used for SQuAD. 14 Source # Samples in TÜLU 2 # Samples in Unfil. FLAN V2 (Chung et al., 2022) FLAN CoT (Chung et al., 2022) Open Assist. (Köpf et al., 2023) Dolly (Databricks, 2023) GPT-4 Alpaca (Peng et al., 2023) Code Alpaca (Chaudhary, 2023) ShareGPT LIMA (Zhou et al., 2023) Wizard Evol-Instruct V2 (Xu et al., 2023) Open Orca (Lian et al., 2023) SciRIFF (Wadden et al., 2024) Hardcoded Total 49,123 49,747 7,331 0 19,906 20,016 111,912 1,018 29,810 29,683 7,468 140 326,153 961,322 398,439 7,707 15,007 52,002 20,022 100,054 1,030 142,802 4,111,858 7,535 14 5,817,792 Table 5: Number of samples per dataset in the original TÜLU 2 dataset and our unfiltered version. Note that we deduplicate samples in the unfiltered mixture. that carefully balance selected samples across tasks outperform more naive methods (Dai et al., 2024)."
        },
        {
            "title": "E Further Details on FLOPs Estimates",
            "content": "For the FLOPs estimates used throughout the paper, we follow Kaplan et al. (2020) in estimating the compute cost of training step as roughly 6N FLOPs per token processed, where is the parameter count of the model (roughly 7B). Kaplan et al. (2020) notes that the forward pass is roughly half the cost of the backward pass, giving us an estimate of 2N FLOPs per token when processing samples. We use rough estimate of 2,048 tokens per sample, since during training and selection we truncate all samples to be at most this length. Note we fullyfinetune models for two epochs in all setups. Let be the model size, be the size of the data pool (in number of samples), and the number of samples selected to train on. Based on this, the cost for each method is estimated as follows: 1. Random Selection: 2 2048 6N 2. Perplexity: 220482N +220486N 3. IFD: 200000 2049 2N + 1000 2048 6N + 2 2048 2N + 2 2048 6N (We train the initial model used to compute IFD scores on 1000 samples selected from the 200k data pool.) 4. LESS: 320486N +220486N (LESS computes gradients for three checkpoints over the entire pool.) 5. Embedding: 220482N +220486N 6. RDS+: 2 2048 2N + 2 2048 6N Note that for embedding and RDS+, we can select using smaller model than we train (as done for Embed (GTR) or the experiments in App. F). In this case, we adjust the inference cost computation accordingly. We assume the cost of processing scores is negligible compared to the rest of the procedure, since in practice this runs on CPU in under an hour for methods we test. These formulations provide some intuition for why testing large selection sets (large ) is important: if >> D, methods like RDS+ use significantly more compute than random selection. As approaches , the added cost of doing inference over the data pool becomes less significant."
        },
        {
            "title": "F Varying Model Size and Family for",
            "content": "RDS+ Since RDS+ relies on the hidden states of the model, we examine to what extent changing the model used for selection impacts data selection performance. We construct pool of 100k samples from TÜLU 2 unfiltered, balanced via uniform random downsampling across the different sources. We select from the pool using RDS+ with range of models varying both in size (from 1.5B to 70B) and in family (Qwen (Qwen Team, 2024), Llama 2 (Touvron et al., 2023), Llama 3.1 (Llama Team, 2024), OLMo 2 (OLMo Team, 2024)), and show the results in Table 8. Surprisingly, we find that us15 Source # Samples in TÜLU 3 # Samples in Unfil. TÜLU 3 Hardcoded Open Assist. (Köpf et al., 2023) No Robots (Rajani et al., 2023) WildChat (GPT-4 subset) (Zhao et al., 2024b) FLAN V2 (Chung et al., 2022) SciRIFF (Wadden et al., 2024) TableGPT (Zha et al., 2023) TÜLU 3 Persona MATH TÜLU 3 Persona GSM TÜLU 3 Persona Algebra OpenMathInstruct 2 (Toshniwal et al., 2024) NuminaMath-TIR (LI et al., 2024) TÜLU 3 Persona Python Evol CodeAlpaca (Luo et al., 2024) TÜLU 3 CoCoNot (Brahman et al., 2024) TÜLU 3 WildJailbreak (Jiang et al., 2024) TÜLU 3 WildGuardMix (Han et al., 2024) Aya (Singh et al., 2024) TÜLU 3 Persona IF Total 240 7,132 9,500 100,000 89,982 10,000 5,000 149,960 49,980 20,000 50,000 64,312 34,999 107,276 10,983 50,000 50,000 100,000 29,980 939,344 24 7,131 9,500 235,028 961,322 35,149 13,159 149,960 49,980 50,000 2,570,505 64,312 34,998 107,276 10,983 178,344 85,090 190,320 29,962 4,783, Table 6: Number of samples per dataset in the original TÜLU 3 SFT mixture and our unfiltered version. See Lambert et al. (2024) for further details on all splits. Sel. Meth. Aggr. Meth. Avg. Perf. RDS+ RDS+ round-robin mean-max Embed (GTR) Embed (GTR) mean-max round-robin 50.9 47.9 48.0 44.9 Table 7: Average performance of different methods when selecting 326k samples from TÜLU 2 unfiltered using the multitask setting, using either round-robin or mean-max methods for aggregating samples across tasks. Round-robin beats mean-max performance in both cases. See App. for details. ing selection models from different families to the ones being trained can still result in strong performance, suggesting that RDS strong performance does not come from matching the selection and downstream model, but from good general selection of samples. While we do observe some outliers (Llama 2 7B and 13B being particularly good and bad respectively), this suggests that using larger selection models or matching the selection and training model is not crucial for RDS+. This promising finding means that we can potentially select data for much larger models with much smaller ones, significantly reducing the compute cost of RDS+. Further Details on RDS+ G.1 Ablations In this work, we use custom RDS variant (RDS+) that we chose based on series of ablations testing (a) using different sets of hidden states and (b) using different parts of the input data itself. We perform the ablations in the 200k single-task setting (matching the setting used for Table 4.1). For constructing the embeddings, we considered using just the hidden state corresponding to the final EOS token (EOS token only) (Xia et al., 2024), mean-pooling the final hidden states across the sequence (uniform mean pool), or using weighted mean-pooling approach that takes into account the causal attention mask used with decoderonly models, inspired by past work on converting decoder-only models to generic text embedding models (Muennighoff, 2022). See below for further detail on the weighting. As shown in Table 9, using the EOS token alone or uniform mean-pooling underperforms our chosen weighted mean pooling approach. Second, we explore using just the ini16 Train Model Sel. Model Qwen 2.5 1.5B Llama 2 7B Llama 2 13B Llama 2 70B Llama 3.1 8B Llama 3.1 70B Olmo 2 7B Olmo 2 13B Q2.5 L2 7B L2 13B L2 70B L3.1 8B L3.1 70B O2 8B O2 13B Avg. 56.5 52.0 52.0 53.7 53.0 53.2 53.2 51.0 43.5 45.1 42.4 45.4 45.0 45.1 43.5 43.2 54.4 57.0 54.4 54.3 55.9 57.3 54.6 54.9 70.6 72.4 70.3 72.3 72.5 71.7 71.4 71. 65.8 70.4 62.9 68.4 67.8 68.0 66.9 65.8 82.7 83.1 82.3 83.2 82.1 82.1 82.2 83.0 57.0 57.5 56.3 57.1 57.2 57.6 57.4 57.1 64.6 66.7 63.4 64.9 65.7 65.3 64.8 65.1 61.9 63.0 60.5 62.4 62.4 62.5 61.8 61.4 Table 8: Average multi-task performance of RDS round-robin when when varying the model doing the selecting and the model being trained. We find that using different model to the one being trained does not hurt performance, with Llama 2 7B being the best selector overall. Where hi is the last layer hidden state of the ith token."
        },
        {
            "title": "H Visualization of Selected Samples",
            "content": "We visualize what samples get selected when selecting 326,000 samples using RDS+ from the TÜLU 2 unfiltered pool in Figure 6, and what samples get selected when selecting with IFD, Top-PPL, random (unbalanced), and RDS+ from the TÜLU 2 unfiltered pool in Figure 7. Method RDS (ours) - EOS token only - Uniform mean-pool - Prompt-only - Label-only Avg. Perf. 46.4 45.4 45.8 43.2 45.1 Table 9: Overall performance of RDS variations when selecting 10k samples for single target tasks (matching Tab. 1). Using only labels is surprisingly effective. tial user turn (prompt-only) or the only the first response (label-only) instead of the entire input. Surprisingly, using only the label outperforms using only the prompt, although both underperform using the entire sample, despite the label often containing relatively little task information (e.g., just the letter answer for multiple-choice questions). G.2 RDS+ Weighted Pooling Details For the weighted mean-pooling strategy, we follow Muennighoff (2022) in using position weighting to average hidden states across the model inputs while taking into account the causal mask. Specifically, token has weight wi as follows: wi = (cid:80)L i=1 Where is the total length in tokens of the given sample. Given these weights, we then simply perform weighted averaging to compute the RDS embedding: embedding = (cid:88) i=1 wihi 17 Figure 6: Breakdown of what data gets selected when selecting 326,000 samples using RDS from the TÜLU 2 unfiltered pool. Random represents the samples chosen when randomly downsampling to 326,000 samples, and round-robin refers to the samples selected by the multi-task round-robin selection. Figure 7: Breakdown of what data gets selected when selecting 10,000 or 326,000 samples using RDS from the TÜLU 2 unfiltered pool using various selection methods. Sample counts normalized to add to 1. Random represents the samples chosen when randomly downsampling to 326,000 samples. IFD has clear bias to ShareGPT data at both sizes, while PPL has clear bias to FLAN data."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "University of Southern California",
        "University of Washington"
    ]
}