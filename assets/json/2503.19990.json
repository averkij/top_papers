{
    "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
    "authors": [
        "Kexian Tang",
        "Junyao Gao",
        "Yanhong Zeng",
        "Haodong Duan",
        "Yanan Sun",
        "Zhening Xing",
        "Wenran Liu",
        "Kaifeng Lyu",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce \\textbf{LEGO-Puzzles}, a scalable benchmark designed to evaluate both \\textbf{spatial understanding} and \\textbf{sequential reasoning} in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90\\% accuracy. In addition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 9 9 9 1 . 3 0 5 2 : r LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? Kexian Tang1,2* Junyao Gao1,2* Yanhong Zeng1 Haodong Duan1 Yanan Sun Zhening Xing1 Wenran Liu1 Kaifeng Lyu3 Kai Chen1 Shanghai AI Laboratory1 Tongji University Simons Institute, UC Berkeley3 {tangkexian, gaojunyao, zengyanhong, duanhaodong, sunyanan}@pjlab.org.cn {xingzhening, liuwenran, chenkai}@pjlab.org.cn, kaifenglyu@berkeley.edu"
        },
        {
            "title": "Abstract",
            "content": "Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex realworld applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGOPuzzles consists of 1,100 carefully curated visual questionanswering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. In addition to VQA tasks, we evaluate MLLMs abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning. 1. Introduction Spatial intelligence [5] has attracted growing attention due to its significance in various applications, including robotics *Equal contribution; work done during internships in Shanghai AI Laboratory. Project Leads. Corresponding Authors. control [22, 28], autonomous driving [18, 52], and automated assembly [12]. These complex real-world applications inherently require advanced multi-step spatial reasoning capabilities, which involve perceiving 3D-aware spatial relationships and reasoning about them across multiple sequential steps [5, 44, 58]. With the rapid advancement of Large Language Models (LLMs) [3, 17, 41, 49], Multimodal Large Language Models (MLLMs) [8, 34, 42, 48, 50] have also witnessed significant progress in perceiving visual information and interacting with humans through natural language. While MLLMs have made remarkable strides in fundamental tasks such as object recognition [14, 29] and optical character recognition [16, 36, 39, 46], existing evaluations [31, 37] suggest that their spatial reasoning abilities are still limited. Research on evaluating MLLMs multi-step spatial reasoning capabilities remains largely unexplored. Existing studies primarily focus on assessing the spatial understanding capability, which pertains to the comprehension of static scene. Some works [21, 30, 53] employ synthetic environments to render multiple simple 3D objects and then query the spatial relationships between them. However, such question-answering (QA) tasks tend to be overly simplistic for MLLMs to handle, lacking the diversity and complexity of real-world scenarios. Other studies [31, 38] construct spatial understanding tasks based on natural images, but this approach often involves manual annotations, which may limit scalability. Moreover, most existing evaluations rarely evaluate reasoning over sequences of spatial transformations or actions, leaving the multi-step aspect of spatial reasoning largely unaddressed. In this work, we take inspiration from common recreational activity, LEGO construction, to design comprehensive evaluation framework for assessing the multi-step spatial reasoning capabilities of MLLMs. The assembly process of complete LEGO model typically encompasses dozens or even hundreds of discrete construction steps, providing an ideal foundation for testing sequential reasoning abilities. Each step requires accurate comprehension 1 of geometry, orientation, and connection mechanisms of LEGO pieces to successfully follow the provided illustrations. Based on publicly available LEGO projects with detailed step-by-step assembly instructions, we introduce LEGO-Puzzles, novel benchmark specifically engineered to evaluate MLLMs multi-step spatial reasoning capabilities. In total, LEGO-Puzzles encompasses diverse collection of over 1,100 carefully curated visual questionanswering (VQA) pairs spanning 11 distinct tasks, which fall into three major categories. First, we develop set of fundamental tests to assess MLLMs basic spatial understanding capabilities, including recognition of height relationships, rotational transformations, adjacency patterns, and viewpoints within 3D space. Building upon this foundation, we construct both single-step and multi-step sequential reasoning evaluations based on LEGO assembly sequences to examine models sequential reasoning ability. These advanced tests include identifying the configuration of intermediate assembly states (single-step) or determining the correct order of multiple intermediate LEGO states (multi-step). LEGO-Puzzles offers several distinctive advantages compared to existing spatial understanding benchmarks: 1) Enhanced visual richness. Unlike synthetic datasets such as CLEVR [21, 30], which utilize rendered primitive shapes, LEGO-based questions present significantly greater visual complexity and diversity. 2) Superior scalability. single LEGO assembly instruction manual can generate hundreds of unique evaluation questions, which enables efficient benchmark expansion with minimal additional resource investment. Leveraging LEGO-Puzzles, we conduct comprehensive evaluations of 20 state-of-the-art MLLMs, including proprietary models such as GPT-4o and Gemini-2.0-Flash, as well as leading open-source alternatives [8, 43, 50, 56]. Our experimental results reveal substantial gap between current MLLMs and human-level proficiency. Even the strongest models struggle with basic spatial understanding tasks, such as accurately identifying the height of LEGO pieces and determining adjacency relationships in 3D space. Among open-source models, only few achieve performance notably above random guessing across different tasks. Beyond VQA tasks, LEGO-Puzzles also enables the assessment of spatially grounded image generation. For instance, given an assembly illustration, an MLLM is tasked with generating an image of the intermediate state following the specified assembly operation. In these generation tests, most of the evaluated models fail completely, either disregarding the provided instructions or generating images that are entirely irrelevant to the intended LEGO configuration. In summary, our novel benchmark LEGO-Puzzles provides comprehensive evaluation of the spatial understanding and sequential reasoning capabilities of MLLMs. Our main contributions are as follows: novel benchmark for spatial understanding. Based on LEGO constructions, our benchmark LEGO-Puzzles offers natural and diverse test cases for evaluating the spatial understanding capabilities of MLLMs, with improved visual richness and scalability over existing datasets. Evaluation of multi-step spatial reasoning. Built upon LEGOs step-by-step building process, LEGO-Puzzles is the first benchmark explicitly designed to assess multistep spatial reasoning, where each task requires reasoning over up to 7 LEGO construction steps. Comprehensive assessment on visual question answering and image generation. LEGO-Puzzles assesses the spatial reasoning capability of LLMs across both VQA and image generation tasks, providing comprehensive assessment of their ability to comprehend and process spatial information in human-like manner. 2. Related Work General Multi-Modal Evaluation Benchmarks. Recent years have seen significant advancements in multimodal large language models (MLLMs), accompanied by surge in benchmark datasets evaluating their visual understanding. Several comprehensive benchmarks have been introduced to assess various multimodal capabilities. MME [14] provides systematic evaluation of 14 image-centric tasks, revealing persistent challenges such as object hallucination and spatial reasoning failures. MMBench [37] introduces bilingual multiple-choice format for fine-grained multimodal assessment. Moving beyond static images, SEEDBench [25] evaluates generative comprehension across 19K Q&A pairs spanning both image and video reasoning, showing that temporal understanding remains major limitation. For expert-level reasoning, MMMU [60] presents discipline-specific benchmark across 183 subtopics, revealing substantial knowledge gaps even in leading MLLMs even in leading MLLMs, such as GPT-4o and Gemini. Overall, these benchmarks reveal that while MLLMs have made progress, they still struggle with spatial understanding, temporal coherence, multimodal integration, and highlevel reasoning, presenting clear directions for future research. Visual-Spatial Understanding in MLLMs. Multimodal large language models (MLLMs) have made significant strides in vision-and-language tasks, yet they still struggle with 3D spatial understanding. Benchmarks such as 3DSRBench [38] show that even the most advanced models achieve only 4550% accuracy on 3D spatial tasks and experience substantial performance drops under unusual camera angles. To enhance spatial reasoning, several studies have explored Chain-of-Thought (CoT) prompting. For example, Park et al. [44] demonstrate that combining CoT with explicit image-to-text conversion can improve gener2 alization from simple to hard visual reasoning tasks. However, beyond such tailored interventions, traditional CoT prompting alone has generally failed to improve spatial reasoning performance [58]. In response, alternative approaches have emerged. Spatially enriched datasets, such as Spatial Aptitude Training (SAT) [45], significantly boost zero-shot performance across real-image benchmarks. Architectural innovations like CAD-GPT [51], which embeds 3D coordinates into language representations, and MVoT [27], which introduces visual sketching during inference, further expand the solution space. Additionally, lightweight strategies like Coarse Correspondences [33] improve spatial understanding without requiring model fine-tuning. Despite these advances, achieving human-level 3D spatial reasoning in MLLMs remains an open challenge. 3. LEGO-Puzzles In this section, we introduce LEGO-Puzzles, diverse and comprehensive benchmark designed to evaluate the multi-step spatial reasoning capability of MLLMs in detail. Specifically, we first introduce the motivation and definition of each task in Section 3.1. Then, we introduce our dataset curation process, including data collection, question-answer generation, and quality control, in Section 3.2. 3.1. Task Definition To enhance the evaluation of multi-step spatial reasoning for Multimodal Large Language Models (MLLMs), we define three primary categories of tasks based on insights from cognitive psychology and human experiences in developing relevant skills [5, 40, 55]. Using LEGO building as concrete example of how humans develop spatial intelligence, we find that individuals typically engage in the following processes: First, they must understand the spatial relationships between each LEGO piece and how these pieces relate from different perspectives in 3D space. Next, they need to reason through the dependencies and assembly logic of each block at every step of the building process. Finally, they extend their reasoning to multi-step reasoning across the entire assembly sequence. To achieve this, our tasks range from fundamental spatial understanding (36.4%) to single-step sequential reasoning (36.4%) and, ultimately, to multi-step sequential reasoning (27.3%), as illustrated in Figure 1. Below, we provide further details on each task. Task 1: Spatial Understanding. (1) Height: Distinguish the relative heights of LEGO objects. (2) Adjacency: Determine whether LEGO objects are adjacent or separated. (3) Rotation: Calculate the angle of rotation between LEGO object and its corresponding rotated version. (4) Multiview: Predict the current LEGO status from different viewpoints. Task 2: Single-step Sequential Reasoning. (5) Rotation Status: Assess the rotation status of the next LEGO pieces during assembly. (6) Position: Identify the Figure 1. Problem Statistics in LEGO-Puzzles. correct assembly position for the next LEGO pieces. (7) Next-Step: Determine the next LEGO status based on the current status and the upcoming pieces. (8) Dependency: Identify the necessary LEGO pieces required to transition from the current to the next status. Task 3: Multi-Step Sequential Reasoning. (9) Backwards: Identify the correct LEGO status in the assembly pipeline of the LEGO object. (10) Ordering: Predict the correct assembly order of the provided final LEGO images. (11) Outlier: Detect the LEGO status that does not belong to the provided assembly sequence. In conclusion, LEGO-Puzzles consists of over 1,100 visual question-answering (VQA) pairs derived from 407 LEGO building instructions, encompassing 11 tasks across spatial understanding, single-step and multi-step sequenIn addition to VQA tasks, We further extial reasoning. tend several sub-tasks of spatial understanding (Rotation* and Multiview*) and single-step sequential reasoning (Position*, Dependency* and Next-Step*) to include image generation following [57], as part of the visual Generation evaluation of MLLMs. 3.2. Dataset Curation As illustrated in Figure 3, our pipeline consists of three key steps: data collection, question-answer generation, and quality control. This design ensures the scalability, accuracy, and reliability of our data. Data Collection. Data collection consists of three stages. First, we collect diverse set of open-source LEGO source files from the Internet, which include comprehensive stepby-step LEGO building instructions, visualizations, and the required LEGO pieces for each step. Notably, the camera perspective remains consistent across all steps within specific instruction, ensuring temporal, spatial, and logical coherence throughout the building process. Second, 3 Figure 2. Task examples of LEGO-Puzzles. From left to right, the columns represent tasks in Spatial Understanding, Single-Step Sequential Reasoning, and Multi-Step Sequential Reasoning. Note: The questions above are slightly simplified for clarity and brevity. Figure 3. Data curation pipeline. Our pipeline first collects diverse set of LEGO building instructions to render and extract LEGO images in unified format. Next, we generate question-answer pairs by using combination of human annotation and predefined question templates. Finally, we implement three quality control strategies to ensure the accuracy, consistency, and reliability of the data. we render LEGO building instruction files as PDF files using publicly available rendering software, Studio1. This tool enables us to adjust default rendering settings to construct tasks that evaluate spatial relationships at varying levels of complexity. Specifically, for the Rotation and Multiview tasks, we utilize the Persistence of Vision Raytracer 1https://www.bricklink.com/v3/studio/download. page (POV-Ray) style and modify the lighting strength to generate realistic LEGO images from different angles. For the Backward task, we also edit attributes such as color, quantity, and assembly positions of pieces to create erroneous images. Finally, we use PDF-Extract2 to extract all LEGO pieces and objects of interest from the rendered PDF files. All images are systematically organized according to uni2https://github.com/opendatalab/PDF-Extract-Kit 4 [Tiny/Small] [56], Pixtral-12B [1], LLaVA-OneVision7B [26], and EMU3 [54]. For proprietary models, we evaluate Claude-3.5-Sonnet [2], Gemini-1.5-Flash, Gemini-1.5Pro, Gemini-2.0-Flash [48], GPT-4o (20241120), and GPT4o-mini [42]. For the additional image Generation evaluation, we evaluate the open-source models Emu2 [47], GILL [23], and Anole [9], as well as the proprietary models GPT-4o and Gemini-2.0-Flash, all of which support longrange sequence input and image output. Moreover, all evaluations are conducted in zero-shot setting for fair comparison. Baselines. We provide two baselines for comparison: Random indicates the accuracy of random selection for each question, assuming equal probability for all options. p-value-based critical value indicates the minimum accuracy required to statistically surpass random guessing at given significance level (p = 0.05). Evaluation Metrics. We calculate the accuracy (%) of verbal answers to multiple-choice questions using exact matching as our primary metric, following Duan et al. [11]. When models fail to generate an answer in the required format, we utilize the ChatGPT-0125 [13] method from VLMEvalKit [11] as fallback option. Additionally, we randomly select 20 questions from each task to create LEGO-PuzzlesLite, resulting in total of 220 question-answer pairs. This dataset is designed to investigate the performance gap between human intelligence, as evaluated by additional human annotators, and current models in patial Understanding and Sequential Reasoning. In the Generation evaluation, traditional metrics such as FID [20], CLIPScore [15, 19], and X-IQE [6] are inadequate for assessing interleaved outputs in visual answers. Therefore, we enlist human experts to evaluate performance based on appearance similarity and instruction following, using scoring scale from 0 to 3. This approach is necessary due to biases present in VLM-based scoring [35]. 4.2. Main Results We include evaluation results for spatial understanding, sequential reasoning, and generation in Table 1, Table 2, and Table 3. We summarize key findings as below. Challenges of LEGO-Puzzles. Our findings indicate that human experts consistently achieve significantly higher overall performance (93.6%), as shown in Table 2. In contrast, current MLLMs fall short, with even the most advanced models, Gemini-2.0-Flash and GPT-4o, trailing over 30% behind human performance across all tasks. This persistent gap highlights the need for comprehensive and substantial improvements in our LEGO-Puzzles. Gap between Open-source and Proprietary Models. There is significant gap between open-source and proprietary MLLMs in both spatial understanding and sequential reasoning abilities. Most open-source MLLMs perform 5 Figure 4. Task-specific template. Our question-answer template includes instructions, questions, and answers. Here, we provide an example from the Position task for reference. fied naming standard and prepared for question-answer generation across different tasks. Question-Answer Generation. To ensure the scalability of our pipeline, we design several task-specific templates for question-answer generation. See Figure 4 for an example. Each data example includes an instruction, question, and an answer. To meet the requirements of different tasks, we create LEGO sequences of varying lengths. Note that the image token <image x> serves as placeholder for the corresponding image input here. Quality Control. We implement rigorous human review process to maintain high quality and minimize errors. Specifically, we carefully examine the consistency between LEGO objects in source files and rendered PDF files, conducting checks for duplication, adherence to standards, and formatting in the generated images. Additionally, we apply cross-validation to ensure that each question-answer pair aligns with its task-specific template and that the image annotations are accurate, as verified by multiple annotators. This rigorous process ensures high-quality and reliable evaluation. 4. Evaluation on LEGO-Puzzles 4.1. Experimental Setting Benchmark Models. We extensively evaluate 20 models, covering diverse range of architectures, sizes, and training processes for Spatial Understanding and Sequential Reasoning tasks. For open-source models, we evaluate MiniCPM-V2.6 [59], Qwen2-VL-[7B/72B] [50], Qwen2.5-VL-[7B/72B] [7], Idefics3-8B [24], DeepSeek-VL2VILA1.5-13B [32], InternVL2.5-[8B/78B] [4], Models Proprietary Claude-3.5-Sonnet Gemini-1.5-Flash Gemini-1.5-Pro Gemini-2.0-Flash GPT-4o GPT-4o-mini Open-source MiniCPM-V2.6 Qwen2-VL-7B Qwen2.5-VL-7B InternVL2.5-8B VILA1.5-13B Idefics3-8B InternVL2.5-78B Qwen2-VL-72B Qwen2.5-VL-72B DeepSeek-VL2-Small DeepSeek-VL2-Tiny Pixtral-12B 39.0 29.0 35.0 35.0 49. 31.0 26.0 31.0 35.0 35.0 26. 29.0 41.0 40.0 30.0 31.0 32. 31.0 LLaVA-OneVision-7B 42.0 EMU3 Baseline"
        },
        {
            "title": "Random Guessing",
            "content": "Random (p < 0.05) 31.0 33.0 42.0 Spatial Understanding Single-Step Reasoning Multi-Step Reasoning Height Adjacency Rotation Multiview Next-Step Dependency Rotation Stat. Position Backwards Ordering Outlier Overall 60.0 58.0 58. 70.0 66.0 53.0 56.0 57.0 60. 53.0 55.0 51.0 62.0 62.0 61. 52.0 52.0 68.0 59.0 52.0 50. 59.0 42.0 28.0 38.0 49.0 41. 26.0 22.0 30.0 22.0 23.0 26. 23.0 32.0 37.0 27.0 36.0 36. 24.0 21.0 24.0 25.0 33.0 48. 45.0 56.0 45.0 51.0 51.0 44. 40.0 27.0 37.0 35.0 23.0 47. 51.0 27.0 41.0 24.0 24.0 41. 25.0 25.0 33.0 61.0 57.0 59. 69.0 65.0 27.0 34.0 44.0 26. 38.0 17.0 18.0 60.0 57.0 55. 38.0 27.0 21.0 30.0 17.0 20. 28.0 78.0 77.0 84.0 81.0 87. 71.0 50.0 70.0 60.0 48.0 34. 20.0 79.0 79.0 72.0 57.0 25. 38.0 50.0 25.0 25.0 33.0 58. 57.0 61.0 54.0 51.0 57.0 51. 48.0 49.0 64.0 48.0 47.0 58. 49.0 58.0 59.0 47.0 53.0 59. 47.0 50.0 59.0 37.0 32.0 39. 46.0 51.0 32.0 29.0 26.0 25. 25.0 26.0 30.0 32.0 43.0 47. 28.0 27.0 21.0 26.0 25.0 25. 33.0 49.0 28.0 35.0 56.0 53. 50.0 23.0 13.0 24.0 35.0 12. 24.0 40.0 34.0 60.0 41.0 26. 24.0 20.0 24.0 25.0 33.0 54. 20.0 44.0 46.0 72.0 7.0 0. 9.0 5.0 0.0 4.0 4.0 15. 26.0 33.0 3.0 4.0 3.0 0. 0.0 4.2 9.0 64.0 51.0 59. 43.0 49.0 27.0 19.0 28.0 13. 29.0 22.0 24.0 37.0 31.0 43. 26.0 16.0 37.0 22.0 20.0 20. 28.0 53.6 43.8 51.6 54.0 57. 39.3 32.2 36.0 31.5 35.2 27. 26.6 45.7 46.3 46.6 37.5 28. 31.3 33.6 26.4 27.5 35.5 Table 1. Full Evaluation Results of 18 MLLMs on LEGO-Puzzles. Dark Gray indicates the best performance for each task among all models and Light Gray indicates the best result among open-source model. We also highlight the top three models based on their overall performance, using Dark Green , Medium Green , and Light Green , respectively."
        },
        {
            "title": "Models",
            "content": "LEGO-Puzzles-Lite"
        },
        {
            "title": "Human proficiency",
            "content": "Claude-3.5-Sonnet Gemini-2.0-Flash GPT-4o InternVL2.5-78B Qwen2-VL-72B Qwen2.5-VL-72B"
        },
        {
            "title": "Spatial Understanding",
            "content": "Single-Step Reasoning Multi-Step Reasoning Height Adjacency Rotation Multiview Next-Step Dependency Rotation Stat. Position Backwards Ordering Outlier 70.0 40.0 30. 35.0 40.0 30.0 25.0 95.0 55. 65.0 75.0 55.0 65.0 70.0 95. 50.0 55.0 45.0 30.0 45.0 25. 100.0 50.0 40.0 50.0 45.0 50. 35.0 90.0 60.0 80.0 60.0 60. 55.0 65.0 100.0 100.0 75.0 85. 85.0 85.0 80.0 70.0 55.0 60. 60.0 55.0 45.0 65.0 95.0 35. 40.0 60.0 30.0 35.0 45.0 95. 60.0 60.0 55.0 25.0 30.0 55. 95.0 55.0 50.0 60.0 20.0 15. 20.0 95.0 60.0 45.0 65.0 50. 35.0 55."
        },
        {
            "title": "Overall",
            "content": "93.6 54.1 55.5 59.1 45.0 44. 48.2 Table 2. Comparing Top-Performing MLLMs with Human Proficiency on LEGO-Puzzles-Lite. The best results are marked in bold. The top three overall performances are highlighted in Dark Green , Medium Green , and Light Green , respectively. only marginally better than Random, while leading proprietary models, such as Gemini-2.0-Flash and GPT-4o, exhibit strong spatial reasoning capabilities, achieving overall accuracies of 54.0% and 57.7%, respectively. In the Height Model Performance in Different Tasks. task, where height relationships are complicated by the interplay between 2D and 3D perspectives, most models (11/20) perform worse than Random, with even human experts achieving significantly lower scores than in other tasks. In the Rotation and Rotation Status tasks, our findings indicate that models exhibit limited sensitivity to rotationrelated recognition, achieving low scores, with 7 out of Task MLLM Rotation* Multiview* Position* Dependency* Next-Step* Overall Gemini-2.0-Flash GPT-4o Emu2 GILL Anole App 2.30 1.80 3.00 1.85 1. 2.15 IF 1.65 1.35 1.40 1. 0.20 1.17 App 0.95 2.25 3. 0.55 0.55 1.46 IF 0.80 0. 1.10 0.25 0.20 0.56 App 2. 2.10 0.65 0.65 2.10 1.52 IF 0.00 0.00 0.00 0.00 0.00 0. App 0.00 0.00 0.00 0.00 0. 0.00 IF 0.00 0.00 0.00 0. 0.00 0.00 App 0.10 0.05 0. 0.00 0.00 0.03 IF 0.00 0. 0.00 0.00 0.00 0.00 Setting = = 2 = 3 = 4 = 5 GPT-4o Gemini-2.0-Flash Qwen-2.5-72B Internvl-2.5-78B w/o CoT w. CoT w/o CoT w. CoT w/o CoT w. CoT w/o CoT w. CoT 45.0 15.0 5.0 5.0 5. 75.0 25.0 5.0 0.0 0.0 85. 45.0 35.0 35.0 20.0 60.0 50. 40.0 50.0 25.0 65.0 60.0 75. 65.0 65.0 65.0 55.0 75.0 65. 65.0 35.0 30.0 10.0 20.0 25. 55.0 20.0 20.0 5.0 10.0 Table 3. Evaluation on Generation. We conduct human-based evaluation to assess the Appearance (App) and Instruction Following (IF) scores of Gemini-2.0-Flash, GPT-4o, Emu2, GILL, and Anole, using scoring scale from 0 to 3 for both dimensions. 20 models performing below Random in both tasks. Conversely, most models achieve accuracy above the critical threshold in the Multiview task, indicating that existing MLLMs possess basic spatial modeling capabilities. However, MLLMs performance is notably weaker in multi-step sequential reasoning tasks such as Ordering and Outlier compared to single-step spatial reasoning tasks like Dependency and Next-Step. This disparity highlights the models limitations in capturing long-range dependencies and executing effective sequential reasoning. In conclusion, our LEGO-Puzzles highlights both the spatial understanding and sequential reasoning abilities of MLLMs. Results in Table 1 demonstrate that GPT-4o achieves the highest performance. However, the overall results suggest significant room for improvement, particularly in domains involving relative relationships, rotation perception, and long-range sequential reasoning. 4.3. Image Generation Evaluation As mentioned in Section 3.1, we evaluate image generation ability across several tasks related to spatial understanding (Rotation* and Multiview*) and single-step sequential reasoning (Position*, Dependency* and Next-Step*) as part of the Generation assessment. As shown in Table 3, all MLLMs struggle to simultaneously maintain appearance identity and strictly adhere to user instructions when generating image answers across all tasks. This poor performance indicates that existing MLLMs are ineffective at visualizing spatial understanding and sequential reasoning capabilities, underscoring the challenge of integrating multimodal information effectively. 4.4. Exploring Multi-Step Sequential Reasoning Experiments in Section 4.2 show that current MLLMs perform poorly when extending single-step sequential reasoning QA to multi-step tasks. To further investigate the underlying reasons for these performance variations in sequential reasoning tasks, we design fine-grained sequential reasoning task called Next-k-Step, which explicitly controls the number of steps required to complete the task. Experimental Setup. Next-k-Step builds upon our singlestep sequential reasoning task, Next-Step, and requires Table 4. Evaluation on Next-k-Step. represents the number of steps, and CoT refers to adding Think step by step before answering instruction in QA pairs, similar to those in LLMs. MLLMs to identify the correct LEGO object by sequentially adding additional LEGO pieces to the current LEGO object. We set = 1, 2, 3, 4, 5 and construct 20 test cases for each value. Specifically, we input current LEGO object (x1), next LEGO pieces (x2, x3, . . . , xk+1) and the target LEGO object (xk+2), along with the corresponding text instructions into MLLMs, excepting them to generate the correct answer from four options (A, B, C, D). Additionally, to investigate the effectiveness of the widely adopted Chain of Thought (CoT) approach from the LLM community in enhancing multi-step sequential reasoning, we designed experiments comparing model performance under two conditions: standard prompting (without CoT) and explicit step-by-step reasoning (with CoT). We conduct experiments using the four top-performing models on the NextStep task, GPT-4o, Gemini-2.0-Flash, Qwen-2.5-72B, and Internvl-2.5-78B. Performance Degradation when Increases. As shown in Table 4, the relationship between accuracy and the number of reasoning steps varies across models. GPT-4o and Gemini-2.0-Flash exhibit clear performance decline as increases. These results align with the performance discrepancy between single-step and multi-step sequential reasoning in Table 1, further demonstrating that current MLLMs struggle to handle multi-step sequential relationships requiring iterative reasoning. key challenge lies in the accumulation of errors as reasoning steps increase. Each intermediate inference introduces potential deviations that can compound over multiple steps, leading to significant inconsistencies in the final predictions. Additionally, MLLMs might lack an explicit visual memory mechanism compared to LLMs in language memory, making it difficult to coherently track and integrate positional changes throughout the reasoning process. Surprisingly, we observe that Qwen2.5-72B achieves stable and relatively consistent accuracy scores (around 65.0%) across all values of k, even when is increased to seven. And for Internvl-2.5-78B, accuracy scores are close to random guessing. Limited Effectiveness of Chain-of-Thought (CoT). By applying CoT prompting, we observe significant improvements when = 1 for GPT-4o and InternVL-2.5-78B. However, this effect diminishes for 2, where accuracy 7 even declines dramatically. This is because these MLLMs perform worse than random guessing (25%) when dealing with longer steps, with accuracies of 5% for GPT-4o and 10% for InternVL-2.5-78B. For other MLLMs like Gemini2.0-Flash and Qwen-2.5-72B, CoT prompting does not provide obvious benefits, as they fail to perform genuine stepby-step reasoning in their CoT responses."
        },
        {
            "title": "Task\nHeight\nAdjacency",
            "content": "PCC P-value 0.00723 0.93 0.00046 0.98 Table 5. Pearson Correlation Coefficients (PCC) and P-values for Height and Adjacency Tasks 4.5. Consistency Compared with Natural Dataset Besides its high scalability as virtual framework, LEGOPuzzles also demonstrates strong consistency with the natural environment. To verify this, we compare LEGO-Puzzles with 3DSRBench [38], which includes several similar tasks (Height in LEGO-Puzzles and 3DSRBench, Adjacency in LEGO-Puzzles and Location in 3DSRBench) but focuses on real-world domain images. Specifically, we evaluate all proprietary models tested on LEGO-Puzzles within the 3DSRBench dataset and compute the Pearson correlation coefficient [10] to measure the accuracy correlation between these two benchmarks. The results in Table 5 indicate that model performance on LEGO-Puzzles reliably reflects trends observed in natural data. 4.6. Task Similarity In this subsection, we analyze task similarity in our benchmark by calculating the average rank correlation between each task and all others, as proposed by Zhang et al. [61]. The similarity score for each task is derived from the average correlation between its ranking and those of all other tasks, using three metrics: Spearman Rank Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC), and R² Score (R-squared Coefficient of Determination). The results in Figure 5 show that only few task pairs have strong correlations, such as Next Step to Dependency, Multi-step to Single-step Sequential Reasoning, and Spatial Understanding. This is because they either share similar image inputs or integrate step-wise logical progression and spatial comprehension. However, most tasks exhibit moderate to low correlations, ensuring benchmark diversity. More than half of the task pairs have an SRCC between 0.3 and 0.6, indicating limited dependency among tasks. Overall, our benchmark offers balanced assessment of MLLMs across various reasoning skills. While some tasks show strong correlations due to conceptual overlap, the majority remain sufficiently independent, providing comprehensive and distinctive evaluation framework. 5. Error Analysis In this section, we conduct detailed error analysis during the evaluation process of our benchmark, providing insights into model behaviors and shortcomings. Failure in Ordering Task. As shown in Table 1, several open-source models (4/14) completely fail in the Ordering task, with scoring zero. Ordering task requires MLLMs to enable multi-step reasoning ability. Despite explicitly specifying the required answer format in the prompt (e.g., sequence such as BACD), some models were unable to generate valid response and instead produced arbitrary outputs. For instance, several models (InternVL2.58B, Emu3, MiniCPM-V-2-6, and LLava-OneVision-7B) exhibited strong biases, frequently defaulting to single letter rather than complete sequence. In extreme cases, models provided nearly identical responses across all test instances, such as Emu3, which answered for 98 out of 100 test cases, demonstrating lack of genuine reasoning ability. These results indicate that many open-source models struggle with sequence generation and constrained output formatting, suggesting potential issues in their ability to follow structured prompts for reasoning tasks. These biases in responses further suggest that models may be overly reliant on spurious correlations in training data rather than understanding the stepwise dependencies of given sequence. Challenges in Height Perception: 2D vs. 3D Understanding. In the Height task, we observe that most models (11/20) achieve scores lower than Random. We provide some failure cases in Figure 6, which exhibit noticeable 2D and 3D optical illusions. Since MLLMs are primarily trained on images with predominantly 2D viewpoint, the discrepancy between 2D and 3D spatial understanding in our Height task often causes MLLMs to answer questions based on 2D projection rather than true 3D perspective. Even when we construct instruction prompts that explicitly require MLLMs to comprehend 3D spatial relationships, GPT-4o, the top-performing model, still fails to achieve human-level performance. This observation highlights the tendency of MLLMs to rely on 2D spatial priors during inference, suggesting the need for further research in 3D understanding training. Weak Appearance Consistency and Instruction Following in Image Generation. Our evaluation of MLLMgenerated images reveals substantial differences in instruction following and reasoning-based image synthesis when processing sequential visual inputs. As shown in Table 7, open-source models struggle significantly in both appearance consistency (App) and instruction-following (IF), while proprietary models demonstrate varying degrees of success. Among the proprietary models, Gemini-2.0Flash exhibits the strongest performance in both appearance and instruction adherence. It effectively follows input constraints and maintains high appearance fidelity, often editing 8 Figure 5. Task Similarity Heatmap. The heatmap illustrates the pairwise correlation between tasks in our benchmark, measured using SRCC, PLCC, and R² scores. Figure 6. Visualization of sample failure cases in Height and Ordering. The Ground Truth answer is marked in blue, while the MLLMs answer is marked in red. Note: The questions above are slightly simplified for clarity and brevity. the given image rather than generating completely new one. This suggests that Gemini-2.0-Flash has stronger spatial consistency mechanism, enabling it to make precise modifications while preserving structural coherence. For GPT-4o, the results suggests that GPT-4o may not directly edit the input image but instead interpret its semantic content and generate new image based on textual understanding. The differences in appearance fidelity and instruction following indicate that GPT-4os generation process might involve reconstructing the scene conceptually rather than modifying the original image step by step. While this allows it to maintain conceptual relevance, its output often deviates in style and structure from the original input, leading to lower appearance fidelity compared to Gemini-2.0-Flash. Moreover, its instruction following ability remains incon9 Figure 7. Qualitative image generation results for Rotation* and Multiview* tasks. Note: The questions above are slightly simplified for clarity and brevity. sistent, particularly for tasks requiring fine-grained reasoning. For open-source models, Emu2 exhibits some capability in preserving visual appearance but fails entirely in instruction-following, treating the task as mere image reconstruction rather than reasoning-based generation. It struggles with spatial dependencies and sequential modifications, making it ineffective for reasoning-intensive tasks. GILL and Anole perform the worst, failing to generate relevant outputs in nearly all cases. Their instruction-following scores are close to zero, and their generated images are often completely unrelated to the expected result. This highlights fundamental limitation in their ability to process sequential visual transformations, making them unsuitable for complex, instruction-driven image generation. We provide some failure cases in Figure 7. These findings emphasize the fundamental challenges in spatial and sequential reasoning within open-source MLLMs. While Gemini-2.0-Flash shows the most precise adherence to instructions and image editing capabilities, GPT-4o tends to generate semantically relevant but visually divergent outputs. Open-source models, by contrast, still lack robust mechanisms for sequential reasoning, underscoring the need for advancements in instruction-following and reasoning-aware image generation techniques. 6. Conclusion We introduce LEGO-Puzzles, novel benchmark specifically designed to evaluate spatial understanding, as well as single-step and multi-step sequential reasoning in MLLMs. Inspired by human cognitive patterns in LEGO construction, we create dataset that includes over 1,100 carefully curated visual question-answering (VQA) samples across 11 distinct tasks, providing diverse scenarios to assess multimodal visual reasoning. We conduct comprehensive experiments with 20 advanced MLLMs, revealing substantial performance gaps compared to humans, particularly in extended sequential reasoning and the generation of spatially coherent visual outputs. These findings underscore the urgent need to enhance the spatial understanding and sequential reasoning capabilities of multimodal AI."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, arXiv preprint Theophile Gervet, et al. arXiv:2410.07073, 2024. 5 Pixtral 12b. [2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. 5 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 1 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Marc Bornstein. Frames of mind: The theory of multiple intelligences. Journal of Aesthetic Education, 20(2), 1986. 1, 3 [6] Yixiong Chen, Li Liu, and Chris Ding. X-iqe: explainable image quality evaluation for text-to-image generaarXiv preprint tion with visual large language models. arXiv:2305.10843, 2023. 5 [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 5 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites, 2024. 1, 2 [9] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. 5 [10] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, pages 14, 2009. 8 [11] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. [12] Jianguo Duan, Liwen Zhuang, Qinglei Zhang, Ying Zhou, and Jiyun Qin. Multimodal perception-fusion-control and humanrobot collaboration in manufacturing: review. The International Journal of Advanced Manufacturing Technology, 132(3):10711093, 2024. 1 [13] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30: 681694, 2020. 5 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. 1, 2 [15] Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, and Cairong Zhao. Styleshot: snapshot on any style. arXiv preprint arXiv:2407.01414, 2024. 5 [16] Junyao Gao, Yanan Sun, Fei Shen, Xin Jiang, Zhening Xing, Kai Chen, and Cairong Zhao. Faceshot: Bring any character into life. arXiv preprint arXiv:2503.00740, 2025. 1 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1 [18] Xianda Guo, Ruijun Zhang, Yiqun Duan, Yuhang He, Chenming Zhang, Shuai Liu, and Long Chen. Drivemllm: benchmark for spatial understanding with multimodal large 10 language models in autonomous driving. arXiv:2411.13112, 2024. 1 arXiv preprint the IEEE/CVF conference on computer vision and pattern recognition, pages 1496314973, 2023. 1, 2 [19] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 5 [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [21] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. 1, 2 [22] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1 [23] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36:2148721506, 2023. [24] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Building and better understanding visionTronchon. In Worklanguage models: insights and future directions. shop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024. 5 [25] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 2 [26] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 5 [27] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv preprint arXiv:2501.07542, 2025. 3 [28] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1806118070, 2024. 1 [29] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 1 [30] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose doIn Proceedings of main robustness in visual reasoning. 11 [31] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. arXiv preprint arXiv:2409.09788, 2024. 1 [32] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. 5 [33] Benlin Liu, Yuhao Dong, Yiqin Wang, Zixian Ma, Yansong Tang, Luming Tang, Yongming Rao, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondences boost spatial-temporal reasoning in multimodal language models. arXiv preprint arXiv:2408.00754, 2024. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1 [35] Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, and Lifu Huang. Holistic evaluation for interleaved text-and-image generation. arXiv preprint arXiv:2406.14643, 2024. 5 [36] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 1 [37] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 1, 2 [38] Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Alan Yuille, and Jieneng Chen. 3DSRBench: comprearXiv preprint hensive 3d spatial reasoning benchmark. arXiv:2412.07825, 2024. 1, 2, 8 [39] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947 952. IEEE, 2019. [40] Nora Newcombe and Andrea Frick. Early education for spatial intelligence: Why, what, and how. Mind, Brain, and Education, 4(3):102111, 2010. 3 [41] OpenAI. Chatgpt. https://openai.com/blog/ chatgpt, 2023. 1 [42] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 1, 5 [43] OpenBMB. Minicpm: Unveiling the potential of end-side large language models, 2024. 2 [44] Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Generalizing from simple to hard visual reasoning: Can we mitigate modality imbalance in vlms? arXiv preprint arXiv:2501.02669, 2025. 1, 2 Mingyu Ding, Linjie Li, et al. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models. arXiv preprint arXiv:2410.10139, 2024. 3 [58] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. 1, 3 [59] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 5 [60] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 2 [61] Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan, Kai Chen, and Guangtao Zhai. Redundancy principles for mllms benchmarks. arXiv preprint arXiv:2501.13953, 2025. [45] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko. SAT: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. 3 [46] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 1 [47] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023. 5 [48] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 5 [49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [50] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, [51] Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, and Jie Yang. CAD-GPT: Synthesising cad construction sequence with spatial reasoning-enhanced mulIn Proceedings of the AAAI Conference on timodal llms. Artificial Intelligence (AAAI), 2025. 3 [52] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, et al. Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving. arXiv preprint arXiv:2312.09245, 2023. 1 [53] Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, and Alan Yuille. 3d-aware visual question answering about parts, poses and occlusions. Advances in Neural Information Processing Systems, 36:5871758735, 2023. 1 [54] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 5 [55] Marcy Willard. What is sequential reasoning in childhood?, 2022. 3 [56] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 2, 5 [57] Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui,"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Simons Institute, UC Berkeley",
        "Tongji University"
    ]
}