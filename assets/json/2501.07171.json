{
    "paper_title": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature",
    "authors": [
        "Alejandro Lozano",
        "Min Woo Sun",
        "James Burgess",
        "Liangyu Chen",
        "Jeffrey J Nirschl",
        "Jeffrey Gu",
        "Ivan Lopez",
        "Josiah Aklilu",
        "Austin Wolfgang Katzer",
        "Collin Chiu",
        "Anita Rau",
        "Xiaohan Wang",
        "Yuhui Zhang",
        "Alfred Seunghoon Song",
        "Robert Tibshirani",
        "Serena Yeung-Levy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of vision-language models (VLMs) is driven by large-scale and diverse multimodal datasets. However, progress toward generalist biomedical VLMs is limited by the lack of annotated, publicly accessible datasets across biology and medicine. Existing efforts are restricted to narrow domains, missing the full diversity of biomedical knowledge encoded in scientific literature. To address this gap, we introduce BIOMEDICA, a scalable, open-source framework to extract, annotate, and serialize the entirety of the PubMed Central Open Access subset into an easy-to-use, publicly accessible dataset.Our framework produces a comprehensive archive with over 24 million unique image-text pairs from over 6 million articles. Metadata and expert-guided annotations are also provided. We demonstrate the utility and accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style models continuously pre-trained on the BIOMEDICA dataset via streaming, eliminating the need to download 27 TB of data locally.On average, our models achieve state-of-the-art performance across 40 tasks - spanning pathology, radiology, ophthalmology, dermatology, surgery, molecular biology, parasitology, and cell biology - excelling in zero-shot classification with a 6.56% average improvement (as high as 29.8% and 17.5% in dermatology and ophthalmology, respectively), and stronger image-text retrieval, all while using 10x less compute. To foster reproducibility and collaboration, we release our codebase and dataset for the broader research community."
        },
        {
            "title": "Start",
            "content": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature Alejandro Lozano* Department of Biomedical Data Science Min Woo Sun* Department of Biomedical Data Science Stanford University lozanoe@stanford.edu Stanford University minwoos@stanford.edu James Burgess* ICME Stanford University jmhb@stanford.edu"
        },
        {
            "title": "Liangyu Chen\nDepartment of Computer Science",
            "content": "Stanford University liangyuc@stanford.edu Jeffrey J. Nirschl Department of Pathology"
        },
        {
            "title": "Jeffrey Gu\nICME",
            "content": "Stanford University jnirschl@stanford.edu Stanford University sanketg@stanford.edu"
        },
        {
            "title": "Josiah Aklilu\nDepartment of Biomedical Data Science",
            "content": "Stanford University ivlopez@stanford.edu Stanford University josaklil@stanford.edu"
        },
        {
            "title": "Yuhui Zhang\nDepartment of Computer Science",
            "content": "Stanford University arau@stanford.edu Stanford University awkatzer@stanford.edu Stanford University yuhuiz@stanford.edu"
        },
        {
            "title": "Collin Chiu\nDepartment of Biochemistry",
            "content": "Stanford University cochiu9@stanford.edu"
        },
        {
            "title": "Alfred Seunghoon Song\nDepartment of Biomedical Data Science",
            "content": "Stanford University xhanwang@stanford.edu Stanford University alfredss@stanford.edu"
        },
        {
            "title": "Robert Tibshirani\nDepartment of Biomedical Data Science",
            "content": "Department of Statistics Stanford University tibs@stanford.edu Serena Yeung-Levy Department of Biomedical Data Science Department of Electrical Engineering Department of Computer Science Stanford University syyeung@stanford.edu 5 2 0 2 3 1 ] . [ 1 1 7 1 7 0 . 1 0 5 2 : r"
        },
        {
            "title": "Abstract",
            "content": "The development of vision-language models (VLMs) is driven by large-scale and diverse multi-modal datasets. However, progress toward generalist biomedical VLMs is limited by the lack of annotated, publicly accessible datasets across biology and medicine. Existing efforts are limited to narrow domains, missing the full diversity of biomedical knowledge encoded in scientific literature. To address this gap, we introduce BIOMEDICA: scalable, open-source framework to extract, annotate, and serialize the entirety of the PubMed Central Open Access subset into an easy-to-use, publicly accessible dataset. Our framework produces comprehensive archive with over 24 million unique image-text pairs from over 6 million articles. Metadata and expert-guided annotations are additionally provided. We demonstrate the utility and accessibility of our resource by releasing BMCA-CLIP, suite of CLIP-style models continuously pre-trained on BIOMEDICA dataset via streaming (eliminating the need to download 27 TB of data locally). On average, our models achieve state-of-the-art performance across 40 tasks spanning pathology, radiology, ophthalmology, dermatology, surgery, molecular biology, parasitology, and cell biology excelling in zero-shot classification with 6.56% average improvement (as high as 29.8% and 17.5% in dermatology and ophthalmology, respectively) and stronger image-text retrieval while using 10x less compute. To foster reproducibility and collaboration, we release our codebase1,2 and dataset3 for the broader research community. 1. Introduction Progress in vision-language foundation models (FMs) is driven by readily available large-scale and diverse datasets [16, 49]. Providing the basis for pretaining and adaptation, leading to strong visual representations, and achieving expert-level zero-shot performance across wide range of downstream tasks [3, 53, 62] . This success in the general domain has sparked interest in biomedical multimodal FMs as integrating knowledge captured throughout different modalities across medical specialties, molecular biology, genetics, and related fields has the potential to revolutionize precision health. For instance, in daily practice, clinicians typically have one care-related question for every two patients seen [15], often turning to sources like UpToDate for summarized information to address these inquiries [14]. Questions that cannot be answered within three minutes are often aban1https://github.com/minwoosun/biomedica-etl 2https://github.com/Ale9806/open_clip_with_biomedica 3https://huggingface.co/BIOMEDICA Figure 1. Overlap of BIOMEDICA dataset with the Landscape of Biomedical Research [22] Each color and labeled region reflects thematic concentrations, capturing the diversity of topics within our dataset. Gray points represent articles not present in our dataset. doned, negatively impacting patient care [15, 37]. Biomedical FMs could bridge this gap by retrieving relevant information from visual and textual findings linking observations to emerging therapeutics, providing additional recommendations (e.g. linking patient to promising clinical trial [29, 58]), flagging missed diagnoses [47], and identifying biomarkers correlated with patient prognosis [23] ultimately offering essential support for patient care and informed medical decision-making. However, despite the growing interest in generalist biomedical FMs [39], the pursuit of this goal is hindered by the limited availability of annotated and publicly accessible multimodal datasets across wide array of biomedical domains. Furthermore, privacy concerns associated with sharing patient information [21] and the logistical complexities of expert-level annotation exacerbates data collection, annotation, and redistribution at scale [61]. Scientific biomedical literature, however, provides an ever-expanding, highly curated multimodal resource encompassing the knowledge of specialized professionals, reflecting rigorously supported medical and biological evidence. Naturally, open-source biomedical literature offers an unparalleled resource to construct comprehensive and diverse datasets at scale. PubMed [46], free archive of biomedical and life sci2 Figure 2. BIOMEDICA curation pipeline: In the Extract phase, metadata, text (caption, figure reference, full-text), and images are sourced and processed from PMC-OA. In the Transform phase, DINO v2 features are generated for each image, followed by clustering using PCA and k-means. Clinicians and scientists annotate these clusters, identifying 12 global concepts and 170 local concepts, which are then propagated across all images. Finally, in the Load phase, the dataset is made available on Hugging Face with the listed features. ences literature, indexes approximately 1.5 million new publications annuallyequivalent to over two papers per minutewith 9% yearly growth rate. While prior efforts have leveraged this resource [34, 43, 51], current opensource literature-based biomedical datasets are often prefiltered to narrow diagnostics imaging modalities within radiology and pathology, overlooking the vast breadth of complementary information available in other fields such as cell and molecular biology, proteomics, omics, and pharmacogenomics [38]. These adjacent domains provide insights into the very biological mechanisms that dictate health outcomes. Often, advancements in these areas lead to scientific discoveries that not only expand our understanding of complex biological processes but also influence medical practice (e.g. codeine super metabolizers: patients who metabolise codeine very rapidly are at increased risk of developing adverse effects [7] ). Therefore, integrating knowledge from these domains is essential for effective clinical reasoning, yet it is often underappreciated in the development of biomedical vision-language training paradigms and datasets. With the aim to democratize access to open-source scientific data across the vast landscape of biomedical research, we developed the Biomedical Image-Caption Archive (BIOMEDICA), an open-source framework including: an ETL pipeline to efficiently extract and serialize the entirety of PubMed Central Open Access (PMC-OA) repository into standardized and dense archive, as well as tools to annotate, filter, and retrieve the archive on demand. Leveraging BIOMEDICA, we present the following contributions: BIOMEDICA Dataset: large-scale, deep-learningready biomedical dataset containing over 24M imagecaption pairs and 30M image-references from 6M unique open-source articles. Each data point is highly annotated with over 27 unique metadata fields, including articlelevel information (e.g., license, publication title, date, PMID, keywords, MeSH terms) and coarse-grained image metadata (e.g., primary and secondary content labels and panel type) assigned via an unsupervised algorithm and human curation by seven experts. The dataset is optimized for model development and is available in hybrid format: Parquet [56] for fast querying and filtering, and WebDataset for high-throughput streaming (providing 3x-10x higher I/O rates when compared to random access memory). Table 1 summarizes the BIOMEDICA dataset, highlighting its distinctions from existing literature-based datasets. BMCA-LIP: We continually pretrain CLIP using the BIOMEDICA dataset. We leverage our features and tools, such as streaming (enabling remote training) and fast filtering, to explore modern training strategies, in3 Dataset ROCO [43] MEDICAT [51] PMC-OA [34] PMC-15M [67] Scientific Papers 1.8M 1.8M 2.4M 3M BIOMEDICA(Ours) 6M Image-Captions Metadata Fields 0.08M 0.21M 1.65M 15M 24M 3 3 0 3 27 Source PMC-OA PMC-OA PMC-OA PMC-OA PMC-OA Streaming Expert Curated Concepts Clinicians & Scientists Table 1. Comparison of our work to existing literature-based biomedical datasets. BIOMEDICA stands out by offering 2x more scientific papers and 1.6x more image-caption pairs, metadata fields, support for streaming, and expert-curated concepts from clinicians and scientists. See Table 2 for full list of provided metadata. cluding concept filtering [20] and balancing [5]. Large-scale evaluation: We standardized 40 established biomedical datasets across cell and molecular biology, radiology, pathology, ophthalmology, dermatology, and surgery to evaluate our models and compare them to prior work. Our assessments encompass zero-shot classification, image-to-text retrieval, and text-to-image retrieval. Leveraging this benchmark, we observe that our best model surpasses previous state-of-the-art in two-thirds of the cases, achieving an average improvement of 6.56% in general biomedical imaging classification tasks and 6.7% in microscopy composition identification. Furthermore, our high-throughput dataset enables our models to outperform the current state-of-the-art with 10x less compute. Our dataset and models are hosted on Hugging Face and will be updated annually to keep pace with the growing availability of biomedical data. 2. Related Work Literature-based Biomedical Image-caption Pretraining Datasets Biomedical image-caption pretraining mixtures leverage scientific literature to create pre-filtered datasets. Pelka et al. [43] leveraged PMC-OA subset [46] to develop the Radiology Objects in Context (ROCO) dataset. It filters PMC-OA images using supervised CNN to classify and select radiology-specifc images. This results in collection of 81,000 image-caption pairs spanning seven radiology modalities, with additional annotations including keywords, UMLS identifiers, and semantic types. MEDICAT [51] extends ROCO by further filtering more medical images through two-step process. First, keyword filter searches for modality words (e.g., MRI, ultrasound) to identify medical images across captions and reference text. Next, supervised ResNet-50 discards non-medical images. This results in 217,000 caption-figure pairs from 131,000 open-access biomedical papers. PMC-OA dataset [34] follows the same two-step filtering strategy as MEDICAT but replaces the ResNet-50 with ResNet-101 model trained on the DocFigure dataset [30]. This approach yields 381,096 unique medical figures from 2.4 million articles, which are further refined into 1.65 million image-caption pairs. PMC15M [67] scales literature-based biomedical datasets to million articles, extracting 15 million image-caption pairs using PubMed Parser [2]. In contrast to prior work, (1) we adopt domain-agnostic approach. Rather than filtering to specific domains, we provide 9x more metadata and expert-derived annotations at various granularities. Subsequently, we offer parallelized pipeline to use these metadata and filter subsets on demand, accommodating to different interests in the biomedical community. (2) Prior efforts rely on supervised models to classify images. An approach confounded by the diversity of the training datasets. Instead, we employ multistep, data-driven strategy guided by clinicians and scientists to (2.a) develop comprehensive hierarchical taxonomy derived from biomedical ontologies and PMC-OA content, (2.b) annotate clusters of images following this taxonomy, and (2.c) propagate these annotations to each individual image. This data-centric approach ensures scalability, allowing newly added images to inherit labels from the nearest annotated data point. Literature-based Biomedical Contrastive Pretraining Large-scale, literature-based datasets enable selfsupervised learning (SSL). PMC-CLIP [18] leverages one million image-caption pairs from PMC-OA to train an image-captioning model with dual contrastive learning and masked language modeling objective. BiomedCLIP [67] expands on PMC-CLIP by pertaining on 15 million image-caption pairs from PMC-15M and upgrading the vision encoder to vision transformer (ViT-B-16). Unlike the original CLIP model [45], prior work in the biomedical literature aligns vision-text representations from scratch with at least 8x smaller batch sizes and 26x smaller datasets factors linked to suboptimal model performance [13, 33, 66]. Recognizing that even at 24M image-caption pairs, biomedical datasets are orders of magnitude smaller than general vision-language datasets (e.g., LAION-5B [49]) and the scarcity of large compute clusters in academia, we instead investigate continual pretraining. This setup, combined with our framework, enables the exploration of modern training techniques such as data filtering, data balancing, and robust fine-tuning [59]. Evaluation of Biomedical Image-caption Models While biomedical CLIP-style models may be trained on diverse 4 datasets, their evaluation is typically limited to tasks within radiology and pathology, overlooking the broader biomedical domain. In general, CLIP-style models are evaluated on retrieval, classification, and open vision-question answering (VQA) tasks. BiomedCLIP evaluates retrieval performance on 726k held-out set from PMC-15M, with classification capabilities tested on LC25000 [9], PathCamelyon [55], TCGA-TIL, and the RSNA 2018 Challenge [50]. PMC-CLIP assesses retrieval performance on ROCO [43] and classification performance on MedMNIST [64] (including PneumoniaMNIST, BreastMNIST, and DermaMNIST). Both BiomedCLIP and PMC-CLIP also evaluate open-VQA performance on the VQA-RAD and Slake datasets. However, to support open-ended evaluation, additional models are included, requiring full training. BiomedCLIP introduces transformer-based co-attention multimodal module that generates cross-modal representations from image and text encodings, which are then passed to classifier to predict the final answer. In contrast, PMCCLIP uses Model-Agnostic Meta-Learning (MAML) networks for its approach. Existing evaluation protocols have several limitations. First, evaluations are narrow in scope as performance is quantified on limited range of datasets, mainly radiology and pathology. Second, open-VQA evaluation relies on training at least one additional decoder using nonstandardized frameworks, leading to incomparable results. Moreover, the introduction of extra learnable parameters on top of CLIP-style models compromises the reliability of the assessment. For example, while BiomedCLIP and PMCCLIP show at least 40% performance gap across all classification and retrieval tasks, this gap narrows to just 7% in Open-VQA, suggesting that (all things equal) the added decoders are capturing much of the information needed to solve the proposed tasks, as supposed to the evaluated encoder models. As way to evaluate our models and as an initial step in addressing these issues, we conduct systematic assessment of zero-shot performance across 40 datasets, incorporating existing evaluations as thoroughly as possible, given data availability and reproducibility constraints. We include additional datasets to encompass previously overlooked biomedical domains. Lastly, all models are evaluated using the same framework, ensuring fair and consistent comparison. 3. BIOMEDICA Data Curation Process Our dataset curation workflow is illustrated in Figure 2, and consists of three stages: (1) dataset extraction, (2) concept labeling, and (3) dataset serialization. Provenance Generated File List nXML File Generated Generated nXML File nXML File nXML File Concept Image Data Image Key Image File Caption Image Metadata Image Cluster ID Image Hash Image File Name Image Set Image Context Image Annotations Image Panel Type Generated Image Panel Subtype Generated Generated Image Content Primary Label Image Content Secondary Label Generated Article Metadata Article Keywords Article Category Article Title Article Abstract Article full text Article Publication Date Article MeSH Terms Article Journal Article PMID Article Citation Article License List of PMIDs citing article Count of PMIDs citing article Image Embeddings Image DINO-V2 Features Image PMC-CLIP Features nXML File nXML File nXML File nXML File nXML File Entrez API Entrez API File List File List File List File List Entrez API Entrez API Generated Generated Table 2. List of image data (n=3), image metadata (n=5), image annotations (n=4), article metadata (n=13), and image embeddings (n=2) provided in BIOMEDICA dataset, alongside concept provenance. 3.1. Data Extraction Collected article data and metadata is stored in JSONL files. Table 2 lists all the extracted data along with its provenance. Media file download Compressed files containing articles (stored as nXML files) and media files are downloaded from the National Center for Biotechnology Information (NCBI) via the File Transfer Protocol service 4[41, 44] from July 2 to September 20, 2024. Article retrieval Article data and metadata is aggregated from the retrieved file list, nXML files and the Entrez API. File List: Contains metadata fields to link articles to their corresponding media files. We iterate over this index, collecting PMID (accession ID), media file paths, publication date, references, journal and license. nXML file: Articles are provided in structured nXML file. By parsing the nXML, we extract article data, including: title, abstract, keywords, category, full text, figure mentions and captions. Entrez API: The Entrez API provides access to NCBIs full collection of biomedical literature metadata, allowing the retrieval of additional information. Through this API, we collect MeSH terms and PMIDs of citing articles. 4https : / / ftp . ncbi . nlm . nih . gov / pub / pmc / oa _ package/ 5 Figure 3. Left: Examples of images included in the BIOMEDICA dataset, ranging from clinical imaging to maps and bar plots. Right: Visualization of the concept breakdown in the BIOMEDICA taxonomy. The inner level of the pie chart reflects the panel type (light green indicates multi panel, dark green indicates single panel) and the outer level reflects the global concept of individual taxonomies, and the word cloud reflects the fine-grained local concept proportions for the most frequent concepts. Figure Caption Each figure is matched with its corresponding caption by finding the graphic element in the nXML tree with an @xlink:href attribute matching the image id. Figure Mentions In addition to the captions, we extract figure mentionsparagraphs in the article text that reference given figure. To this end, we search for all XML tags with figure attribute <xref ref-type=\"fig\" ...> and store all matched paragraphs. We leave the identifier tag within each paragraph to indicate the exact location where the figure is referenced. License: Each image-caption pair entry includes the associated license information and the article citation. To the best of our knowledge, we adhere to all license terms provided by NCBI. To maintain strict compliance, we followed the three primary license groupscommercial, noncommercial, and other. 3.2. Concept Labeling Feature Clustering All images are embedded using DINOv2 [11] (ViT-L-14 distilled) and further reduced to 25 principal components (refer to Figure 7) using PCA. Subsequently, the compacted features are over-clustered using Kmeans with = 2000. Concept taxonomy team of two licensed clinicians (pathology and surgery) and bioinformatician were tasked with developing hierarchical taxonomy for concepts within PMC-OA. First, an initial taxonomy is derived from biomedical ontologies including Uberon [40], NCBITaxon [17], and the Cell Line Ontology [48]. Subsequently, 30 samples are randomly sampled from each cluster. Annotators are then tasked to review the image content of these clusters to expand the taxonomy, refining it over three iterative rounds. The taxonomy is presented in figure 18. Cluster Annotation group of seven individuals with expertise in genetics, pathology, surgery, developmental biology, and biomedical informatics were tasked to annotate each cluster using the generated taxonomy. Using the majority of observations within cluster, annotators were instructed to provide either one or multiple global and local concept labels, identify whether the images were multipanel, and, if so, specify the types of sub-panels included (see Section 14). Label Resolution and Propagation majority voting system is employed to determine concept labels (see Section 14). In cases of ties, labels are re-evaluated. Once the final labels are assigned, hash map linking cluster IDs to label metadata is used to propagate the annotations to individual images. 3.3. Data Serialization Table 8 shows total compute time for parallelized data serialization. We convert the annotated paired media-JSON files into WebDataset format (following OpenClips naming convention for images and captions). This structure ensures that the dataset can be efficiently processed and distributed on demand via streaming, improving scalability. Dataset Access To facilitate efficient user access, we make the BIOMEDICA dataset available on Hugging Face. This enables users to stream our large dataset directly from the Hub without downloading the entire artifact and provides seamless access even with limited local random access memory through memory mapping. 4. BIOMEDICA Dataset Description Downloaded Articles total of 6,042,494 articles are downloaded from the NCBI server through FTP. Within this collection, 5,050,473 articles have at least one image, while 6 992,021 articles are text-only. All articles have corresponding nXML that contains the full text. Example images are shown in Figure 3. Image-caption pairs collected From the full-text articles and associated image files, we collected total of 24,076,288 unique image-caption pairs and extracted 30,711,542 figure references. On average, each article contains 4.9 images and each image is accompanied by 1.6 figure references. Table 7 presents detailed text and image statistics. Caption token lengths range from single token to 12,389 tokens, with median length of 64 tokens, indicating that In most captions are concise but can vary substantially. contrast, figure reference paragraphswhich describe or contextualize images within the articletend to be longer, with token counts reaching up to 699,117 tokens and median of 338 tokens, showing the level of detail often required for clinical or scientific context. Overall, the text content includes total of 2.84 109 tokens for captions, and 6.65 1010 tokens for the full text, illustrating the extensive scale of language data within the dataset. For images, the median image width and height are 709 and 476 pixels, respectively. However, the wide range in both dimensions, from pixel up to tens of thousands (with maximum width of 52,490 pixels and height of 65,081 pixels). This variability is due to the presence of both thumbnails or low-resolution images and high-resolution images, such as full-page figures or detailed illustrations. Image taxonomy The image taxonomy comprises 12 global concepts and 170 local concepts as shown in Figure 3. total of 23,271,916 images are covered by this taxonomy. Table 16 shows image counts by global concept. Biomedical (microscopy and clinical imaging) images represent 17% of the total dataset. Notably, biomedical images are more common in noncommercial publications, where they make up 13% of the images, compared to 5% and 6% in commercial and other license categories, respectively. The Plots and Charts category holds the largest count across all source types, with total of over 13 million images, which is 57% of the images, demonstrating that graphical data representations are common across scientific literature. Each global concept is further divided into local concepts to capture the specific image types within broader categories. Plots and Charts has the most detailed taxonomy, with 96 local concepts, representing data visualizations like bar charts, line graphs, and heatmaps. Clinical Imaging follows with 34 local concepts, covering range of imaging techniques including MRI, CT, and ultrasound. Figure 4. Average model performance of best BMCA-CLIP models compared to prior work. 5. Evaluation Benchmark To evaluate the effectiveness of continual pretraining on the BIOMEDICA dataset, we repurposed 39 established biomedical classification tasks and leveraged new retrieval dataset based on Flicker, for total of 40 datasets. Image classification To construct comprehensive classification benchmark, we take the union of evaluations from prior work (BioMedCLIP and PMC-CLIP) and supplement it with underrepresented domains. For each individual task, classes are converted into captions (see Supplements), providing two variations per class. This evaluation set spans multiple fields: pathology (11 tasks), radiology (3 tasks), ophthalmology (1 task), dermatology (1 task), surgery (10 tasks), biology (9 tasks) and general microscopy (4 tasks). The biology and pathology tasks are sourced from Micro-Bench [38]; ophthalmology and dermatology tasks from MedMnist [63]; radiology tasks from RSNA [50] and CheXpert [26]; and minimal invasive (M.I.) surgery tasks are collected from the Dresden surgical anatomy dataset [12]. Table 12 provides short description, citation, and domain. The image classification benchmark is subdivided into two splits, similar to Micro-Bench [38]. The first category, general bioimaging classification, encompasses tasks such as diagnosis, object identification, and cellular profiling (e.g. determining cell state). This subset covers 35 tasks. The second split, microscopy composition identification, is derived from the Micro-Bench Perception coarse-grained 7 Model Num tasks Avg. Options Random OpenCLIP [25] CoCa [65] PMC-CLIP [34] BioMedCLIP [67] BMCA-CLIPCF BMCA-CLIPCB BMCA-CLIP BMCA-CLIPCF/WiSE-FT BMCA-CLIPCB/WiSE-FT BMCA-CLIP/WiSE-FT Biology Cell Profiling 6 (5) Structure Profiling 3 (7) 20.00 24.63 23.58 6.01 26.54 24.36 23.54 23.56 26.27 24.66 27.70 14. 44.84 32.92 11.23 49.14 53.51 50.35 48.66 51.68 50.44 47.67 Cytology 3 (6) 16. 20.24 36.27 12.48 17.03 19.48 16.39 16.61 16.67 17.19 16.82 Pathology NeoHistopath 4 (5) Non NeoHistopath 4 (3) 20.00 43.88 36.50 29.86 48.83 55.81 52.33 51.97 49.08 49.09 44.19 33.33 36.81 38. 7.82 42.39 38.60 33.62 34.55 39.07 38.50 38.62 Radiology Ophthamology Dermatology Surgery Average Chest X-ray 2 (3) 33.33 60.88 24.69 28.47 56.04 56.68 50.34 53.06 71.50 64.59 66.32 Breast ultrasound 1 (2) Fundus Camera 1 (4) Skin diagnosis 1 (7) M.I. surgery 10* (2) 50.00 68.27 30.45 58.97 56. 64.10 66.35 56.41 67.31 70.19 68.27 25.00 23.75 43.50 20.12 26.12 43.62 43.50 43.00 37.88 36.75 27.62 28. 20.37 10.15 12.59 36.01 65.81 56.01 23.07 28.45 24.06 15.84 50.00 67.51 36.78 42.96 53. 55.27 56.51 62.64 62.18 62.39 67.68 Total 35 (4) 29.06 41.18 31.12 23.05 41. 47.72 44.89 41.35 45.01 43.79 42.07 Table 3. General Biomedical Imaging Classification Average zero-shot performance across 35 classification tasks (from 21 unique datasets) stratified by domain and task. We show results for three variants of our model, including continual pretraining on all of BIOMEDICA, subset after concept-filtering (CF), and subset after concept-balancing (CB). Models with indication WiSE-FT are merged counterparts as described in [59]. Bold indicates best performance, underline indicates second best performance. benchmark. It spans both pathology and biology across light, fluorescence, and electron microscopy. These tasks involve identifying basic micrograph properties, including: 1. The domain (DM) or field of the image (e.g. pathology) 2. The microscopy modality (MD) and submodality (SM) used for image acquisition (e.g. light microscopy) 3. The staining technique (ST) applied to the specimen. Retrieval task selection Since PMC-15M is not publicly available at the time of this manuscript, we cannot split BIOMEDICA dataset into the similar train and test sets as prior work while simultaneously ensuring balanced number of concepts within each split. Therefore, we assess retrieval performance using new collection of 7K highquality, open-source biomedical image-caption pairs from Flickr. This benchmark spans concepts across pathology, radiology, biology, dermatology, and surgery (see Supplements Section 17 for dataset description and samples). Metrics. Classification tasks are evaluated using average accuracy across two caption variations, while retrieval tasks are measured using retrieval at 1, 10, and 100. Despite variations in the number of classes and samples across tasks, summary statistics are reported as unweighted averages, ensuring each task is treated with equal importance. 6. Experiments We examine the effects of (1) continual pretraining on the full set of 24M image-caption pairs and compare these insights against (2) topic balancing, (3) dataset filtering, (4) robust fine-tuning. Additionally, we (5) include baseline approach involving training model from scratch using random weight initialization. This topic exploration is nonexhaustive and it is designed to utilize the supplementary annotations, metadata, and features made available in our dataset. For all experiments, we use batch size of 1024 per GPU, distributed across four H100 80GB GPUs with batch accumulation frequency of two steps, yielding an effective batch size of 8192 samples per step. We use learning rate of 1e-6 with 1K warmup, and 32-bit floating-point precision. All of our experiments are trained via streaming, eliminating the need for local storage of the 27TB dataset. Additional training details are shared in supplements section 15. With these configurations, we run the following experiments: 1. Continual pretaining on full dataset (24M) In this experiment, we continually pretrain OpenCLIP (ViT-L-14) on the complete dataset of 24,076,288 pairs. We train each model for 9 epochs. This experiment serves as baseline to compare additional data mixture strategies. 2. Concept-Balancing (8M) For this experiment, we continually pretrain OpenCLIP (ViT-L-14) model on 8,404,992 pairs, balancing all topics for 27 epochs. To this end, over represented topics (e.g. plots) are dropped. This experiment targets potential biases introduced by data imbalance by restricting category overrepresentation. 3. Concept-Filtering (6M) In this experiment, we continually pretrain OpenCLIP (ViT-L-14) using filtered dataset of 6,602,752 image-caption pairs. This dataset includes only concepts within clinical and scientific imaging, immunoassays, illustrative diagrams, chemical structures, maps, tools and materials, and handdrawn/screen-based visuals (excluding tables, figures, and scientific equations). We train the model for 36 epochs. 4. Robust Fine-tuning This experiment explores model merging [59] by using convex combination of the weights from base model and its adapted counterpart. Model Image Text Text Image Recall@1 Recall@10 Recall@100 Recall@1 Recall@10 Recall@100 OpenCLIP [25] CoCA [65] PMC-CLIP [34] BiomedCLIP [67] BMCA-CLIPCF BMCA-CLIPCB BMCA-CLIP BMCA-CLIPCF/WiSE-FT BMCA-CLIPCB/WiSE-FT BMCA-CLIP/WiSE-FT 2.78 1.68 0.03 3.70 4.13 3.90 3.67 3.90 3.97 3.83 9.78 5.47 0.13 12. 15.13 13.24 12.72 13.24 13.57 12.68 26.75 15.96 1.39 36.27 38.30 33.43 31.93 33.43 33.32 30.94 2.91 1.50 0.00 3. 4.15 3.66 3.41 3.66 3.58 3.26 9.51 5.51 0.13 13.63 13.75 12.22 11.20 12.22 12.43 11.57 24.59 15.24 1.50 35. 36.10 31.80 30.03 31.80 31.82 29.19 Table 4. Top-K retrieval performance on BioMed-Flickr.Bold indicates best performance, underline indicates second best performance. Model OpenCLIP [25] CoCa [65] PMC-CLIP [34] BioMedCLIP [67] BMCA-CLIPCF BMCA-CLIPCB BMCA-CLIP BMCA-CLIPCF/WiSE-FT BMCA-CLIPCB/WiSE-FT BMCA-CLIP/WiSE-FT DM 49.67 47. 5.56 41.54 49.07 40.07 44.56 58.78 52.12 56.26 MD 85.66 71.89 14.68 74.72 78.71 83.4 85.70 86.88 87.21 83.97 ST 46.77 39. 10.92 49.74 59.17 57.28 58.40 60.84 61.84 54.81 SMD 29.23 48.00 12.49 32.89 14.54 25.48 26.72 27.65 25.04 27.89 Avg. 53.03 47. 10.91 49.72 50.37 51.55 53.84 58.53 56.55 55.73 Table 5. Microscopy Composition Identification Performance in 4 course-grained classification tasks from Âµ-Bench Perception across pathology and biology. Bold indicates best performance, underline indicates second best performance. 7. Results Concept Filtering leads to better performance across zero-shot classification and retrieval tasks Intuitively, when compared to other continual pretraining strategies within BMCA-CLIP models, filtering the dataset (e.g., dropping over-represented topics like plots and tables) yields the best average performance across general biomedical imaging classification tasks with respect to concept balancing (better 80% of the time) and full-dataset pretraining (better 90% of the time). Additionally, concept filtering leads to superior performance compared to concept balancing or full-dataset pretraining in image-to-text and text-toimage retrieval. Indeed, within this training strategy, 48% of the data mixture corresponds to clinical imaging and microscopy. Models Trained on the BIOMEDICA Dataset Lead to State-of-the-Art Zero-Shot Performance Compared to prior work, models trained on the BIOMEDICA dataset yield better average performance in classification and reBMCA-CLIPCF outperforms PMC-CLIP trieval tasks. 9 in all tasks, achieving +24.67% improvement in general biomedical imaging classification tasks, with minimum gap of +5.13% in ultrasound (radiology) and maximum gap of +53.22% in dermatology. Similarly, +39.46% improvement is observed in microscopy composition tasks. Additionally, recall@100 gap of +36.91% and +34.6% is observed in image-to-text and text-to-image retrieval, respectively. Similarly, BMCA-CLIPCF outperforms BioMedCLIP in 8/10 general biomedical imaging classification subsets. yielding an average improvement of 6.56%. For individual tasks, BMCA-CLIPCF achieves the highest differential performance w.r.t BioMedCLIP in dermatology (+29.8%), ophthalmology (+17.5%), breast ultrasound (+8.01%) and non-neo histopathology (+6.98 %) and marginal better performance in microscopy composition identification and all retrial evaluations. It is noteworthy to highlight that BMCA-CLIPCF achieves these results while using 10 less compute and 2.5 less data. Robust Fine-tuning complements model performance in subset of tasks Another advantage of our setup (continual pretraining) is its capability to improve individual task performance without further training, as shown in previous work [38, 59]. For example, in microscopy composition identification tasks, WiSE-FT improves BMCACLIPCF by 8.16%, further increasing the performance gap with respect to BioMedCLIP. Similarly, WiSE-FT enhances BMCA-CLIPCFs performance in 5/10 general biomedical imaging classification tasks (see Figure 4). Notably, BMCA-CLIPCF/WiSE-FT increases the performance gap w.r.t BioMedCLIP in X-ray radiology (+15.46%), breast ultrasound (+11.22%), and surgery (+8.78%), complementing weakness in the original BMCA-CLIPCF model. However, this gain in performance comes at the cost of lower performance in other subtasks, such as -7.56% in dermatology and marginally worse retrieval performance. 8. Limitations While our models offer state-of-the-art performance, all evaluations indicate that there is still significant room for improvement. We highlight two promising directions: Short context length: CLIP (ViT-L-14) has maximum context length of 77 tokens, which restricts our ability to fully utilize captions exceeding this length. While the median caption length in our dataset is 64 tokens, there remains portion of longer captions that cannot be leveraged in their entirety during continual pre-training. This limitation may lead to information loss potentially impacting model performance on tasks requiring more comprehensive textual understanding. Varied image size: The BIOMEDICA dataset contains images of diverse sizes and resolutions, reflective of the original formats in the PMC Open Access Subset. For model training, all images are uniformly resized to meet the input requirements of the image encoder, without any preprocessing to address the variation in image quality or aspect ratios. High-resolution images with fine-grained details may lose important visual information when downscaled, while low-resolution images may become blurry when upscaled, distorting features and potentially introducing misleading information. 9. Conclusion In this work, we present BIOMEDICA, framework for converting PMC-OA into the largest deep-learning-ready dataset, comprising 24 million image-caption pairs with 27 metadata fields derived from scientific literature. We demonstrate the utility of the BIOMEDICA dataset by continually pretraining CLIP-style models, fully leveraging its expert-guided annotations, metadata, and streaming capabilities. Our results showcase the effectiveness of this resource, even in low-memory and GPU-constrained scenarios. Our models achieve state-of-the-art zero-shot classification performance using prior open-source tools and models, while utilizing 10x less compute and 2.5x less dataunderscoring the importance of large-scale annotated open datasets. The BIOMEDICA framework, dataset, models, and large-scale evaluation serve as foundation for advancing vision-language research and applications in scientific and biomedical domains. We release all our contributions under permissive license to facilitate broader use and further development. 9.1. Acknowledgments This research was (NIH#P30AG066515 to JJN), supported by NIH grants the Chan Zuckerberg Initiative Neurodegeneration Challenge Pairs Pilot Project to SYL (2020-221724, 5022), the Wu Tsai Knight Initiative Innovation grant (#KIG 102) to SYL, Hoffman-Yee Research Grant to SYL, the Arc Institute Graduate Fellowship the Stanford Data Science Graduate Research to AL, Fellowship MW, and the Quad Fellowship to JB, and. SYL is Chan Zuckerberg Biohub San Francisco Investigator. We also thank Daniel van Strien, Matthew Carrigan, and Omar Sanseviero from Hugging Face for their invaluable assistance with data upload and design planning on the Hugging Face platform."
        },
        {
            "title": "References",
            "content": "[1] Andrea Acevedo, Anna Merino, Santiago Alferez, Angel Molina, Laura Boldu, and Jose Rodellar. dataset of microscopic peripheral blood cell images for development of automatic recognition systems. Data Brief, 30(105474):105474, 2020. 8 [2] Titipat Achakulvisut, Daniel Acuna, and Konrad Kording. Pubmed parser: python parser for pubmed open-access xml subset and medline xml dataset xml dataset. Journal of Open Source Software, 5(46):1979, 2020. 4 [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [4] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in brief, 28:104863, 2020. 8 [5] Ibrahim Alabdulmohsin, Xiao Wang, Andreas Steiner, Priya Goyal, Alexander DAmour, and Xiaohua Zhai. Clip the bias: How useful is balancing data in multimodal learning? arXiv preprint arXiv:2403.04547, 2024. [6] Sebastiano Battiato, Alessandro Ortis, Francesca Trenta, Lorenzo Ascari, Mara Politi, and Consolata Siniscalco. Pollen13k: large scale microscope pollen grain image dataset. 2020 IEEE International Conference on Image Processing (ICIP), pages 24562460, 2020. 8 [7] Leif Bertilsson, Marja-Liisa Dahl, Per Dalen, and Ayman AlShurbaji. Molecular genetics of cyp2d6: clinical relevance with focus on psychotropic drugs. British journal of clinical pharmacology, 53(2):111122, 2002. 3 [8] AA Borkowski, MM Bui, LB Thomas, CP Wilson, LA DeLand, and SM Mastorides. Lung and colon cancer histopathological image dataset (lc25000). arxiv 2019. arXiv preprint arXiv:1912.12142. 8 [9] Andrew Borkowski, Marilyn Bui, Brannon Thomas, Catherine Wilson, Lauren DeLand, and Stephen Mastorides. Lung and colon cancer histopathological image dataset (lc25000). arXiv preprint arXiv:1912.12142, 2019. 5 [10] James Burgess, Jeffrey Nirschl, Maria-Clara Zanellati, Alejandro Lozano, Sarah Cohen, and Serena Yeung-Levy. Orientation-invariant autoencoders learn robust representations for shape profiling of cells and organelles. Nat. Commun., 15(1), 2024. 8 [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 6 [12] Matthias Carstens, Franziska Rinner, Sebastian Bodenstedt, Alexander Jenke, Jurgen Weitz, Marius Distler, Stefanie Speidel, and Fiona Kolbinger. The dresden surgical anatomy dataset for abdominal organ segmentation in surgical data science. Scientific Data, 10(1):18, 2023. 7, 8 [13] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scalIn Proing laws for contrastive language-image learning. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. 4 [14] Azra Daei, Mohammad Reza Soleymani, Hasan AshrafiRizi, Ali Zargham-Boroujeni, and Roya Kelishadi. Clinical information seeking behavior of physicians: systematic International journal of medical informatics, 139: review. 104144, 2020. 2 [15] Guilherme Del Fiol, Elizabeth Workman, and Paul Gorman. Clinical questions raised by clinicians at the point of care: systematic review. JAMA internal medicine, 174(5): 710718, 2014. 2 [16] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021. [17] Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. Ncbi disease corpus: resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:110, 2014. 6 [18] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visual question In Findings of the Asanswering in the medical domain? sociation for Computational Linguistics: EACL 2023, pages 11811193, 2023. 4 [19] Philipp Eulenberg, Niklas Kohler, Thomas Blasi, Andrew Filby, Anne Carpenter, Paul Rees, Fabian Theis, and Alexander Wolf. Reconstructing cell cycle and disease progression using deep learning. Nature communications, 8(1): 463, 2017. 8 [20] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. 4 [21] Scott Fleming, Alejandro Lozano, William Haberkorn, Jenelle Jindal, Eduardo Reis, Rahul Thapa, Louis Blankemeier, Julian Genkins, Ethan Steinberg, Ashwin Nayak, et al. Medalign: clinician-generated dataset for instruction following with electronic medical records. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 22021 22030, 2024. 2 [22] Rita Gonzalez-Marquez, Luca Schmidt, Benjamin Schmidt, Philipp Berens, and Dmitry Kobak. The landscape of biomedical research. Patterns, 2024. 2 [23] Zepeng Huo, Jason Alan Fries, Alejandro Lozano, Jeya Maria Jose Valanarasu, Ethan Steinberg, Louis Blankemeier, Akshay Chaudhari, Curtis Langlotz, and Nigam Shah. Time-to-event pretraining for 3d medical imaging. arXiv preprint arXiv:2411.09361, 2024. [24] Elima Hussain, Lipi Mahanta, Himakshi Borah, and Chandana Ray Das. Liquid based-cytology pap smear dataset for automated multi-class diagnosis of pre-cancerous and cervical cancer lesions. Data Brief, 30(105589):105589, 2020. 8 [25] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. If you use this software, please cite it as below. 8, 9 [26] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A. Mong, Safwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N. Patel, Matthew P. Lungren, and Andrew Y. Ng. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison, 2019. 7 [27] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, pages 590597, 2019. 8 [28] Andrii Iudin, Paul Korir, Sriram Somasundharam, Simone Weyand, Cesare Cattavitello, Neli Fonseca, Osman Salih, Gerard Kleywegt, and Ardan Patwardhan. Empiar: the electron microscopy public image archive. Nucleic Acids Research, 51(D1):D1503D1511, 2023. 8 [29] Qiao Jin, Zifeng Wang, Charalampos Floudas, Fangyuan Chen, Changlin Gong, Dara Bracken-Clarke, Elisabetta Xue, Yifan Yang, Jimeng Sun, and Zhiyong Lu. Matching patients to clinical trials with large language models. Nature Communications, 15(1):9074, 2024. [30] KV Jobin, Ajoy Mondal, and CV Jawahar. Docfigure: dataset for scientific document figure classification. In 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), pages 7479. IEEE, 2019. 4 [31] Changhun Jung, Mohammed Abuhamad, David Mohaisen, Kyungja Han, and DaeHun Nyang. Wbc image classification and generative models based on convolutional neural network. BMC Medical Imaging, 22(1):94, 2022. 8 [32] Jakob Nikolas Kather, Cleo-Aron Weis, Francesco Bianconi, Susanne Melchers, Lothar Schad, Timo Gaiser, Alexander Marx, and Frank Gerrit Zollner. Multi-class texture analysis in colorectal cancer histology. Sci. Rep., 6:27988, 2016. 8 [33] Zichao Li, Cihang Xie, and Ekin Dogus Cubuk. Scaling (down) clip: comprehensive analysis of data, architecture, and training strategies. arXiv preprint arXiv:2404.08197, 2024. 4 11 [34] Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 525536. Springer, 2023. 3, 4, 8, [35] Geert Litjens, Peter Bandi, Babak Ehteshami Bejnordi, Oscar Geessink, Maschenka Balkenhol, Peter Bult, Altuna Halilovic, Meyke Hermsen, Rob Van de Loo, Rob Vogels, et al. 1399 h&e-stained sentinel lymph node sections of breast cancer patients: the camelyon dataset. GigaScience, 7 (6):giy065, 2018. 8 [36] Ruhan Liu, Xiangning Wang, Qiang Wu, Ling Dai, Xi Fang, Tao Yan, Jaemin Son, Shiqi Tang, Jiang Li, Zijian Gao, et al. Deepdrid: Diabetic retinopathygrading and image quality estimation challenge. Patterns, 3(6), 2022. 8 [37] Alejandro Lozano, Scott Fleming, Chia-Chun Chiang, and Nigam Shah. Clinfo. ai: An open-source retrievalaugmented large language model system for answering medical questions using scientific literature. In PACIFIC SYMPOSIUM ON BIOCOMPUTING 2024, pages 823. World Scientific, 2023. 2 [38] Alejandro Lozano, Jeffrey Nirschl, James Burgess, Sanket Rajan Gupte, Yuhui Zhang, Alyssa Unell, and Serena Yeung-Levy. {mu}-bench: vision-language bencharXiv preprint mark for microscopy understanding. arXiv:2407.01791, 2024. 3, 7, 9, 8 [39] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan Krumholz, Jure Leskovec, Eric Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259265, 2023. 2 [40] Christopher Mungall, Carlo Torniai, Georgios Gkoutos, Suzanna Lewis, and Melissa Haendel. Uberon, an integrative multi-species anatomy ontology. Genome biology, 13:120, 2012. [Internet], 2003. [41] National Library of Medicine. PMC Open Access Subset Bethesda (MD): National Library of Medicine. [cited 2024 Jul 2]. Available from: https://www.ncbi.nlm.nih.gov/pmc/tools/ openftlist/. 5 [42] Jeffrey Nirschl, Andrew Janowczyk, Eliot Peyster, Renee Frank, Kenneth Margulies, Michael Feldman, and Anant Madabhushi. deep-learning classifier identifies patients with clinical heart failure using whole-slide images of h&e tissue. PLoS One, 13(4):e0192726, 2018. 8 [43] Obioma Pelka, Sven Koitka, Johannes Ruckert, Felix Nensa, and Christoph Friedrich. Radiology objects in context (roco): multimodal image dataset. In Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3, pages 180189. Springer, 2018. 3, 4, 5 [44] Jon Postel and Joyce Reynolds. File transfer protocol. Technical report, 1985. 5 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [46] Richard Roberts. Pubmed central: The genbank of the published literature, 2001. 2, 4 [47] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416, 2024. 2 [48] Sirarat Sarntivijai, Yu Lin, Zuoshuang Xiang, Terrence Meehan, Alexander Diehl, Uma Vempati, Stephan Schurer, Chao Pang, James Malone, Helen Parkinson, et al. Clo: the cell line ontology. Journal of biomedical semantics, 5:110, 2014. 6 [49] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 2, 4 [50] George Shih, Carol Wu, Safwan Halabi, Marc Kohli, Luciano Prevedello, Tessa Cook, Arjun Sharma, Judith Amorosa, Veronica Arteaga, Maya GalperinAizenberg, et al. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial Intelligence, 1(1): e180041, 2019. 5, [51] Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, and Hannaneh Hajishirzi. Medicat: dataset of medical images, captions, and textual references. arXiv preprint arXiv:2010.06000, 2020. 3, 4 [52] Ziqi Tang, Kangway Chuang, Charles DeCarli, LeeWay Jin, Laurel Beckett, Michael Keiser, and Brittany Dugger. Interpretable classification of alzheimers disease pathologies with convolutional neural network pipeline. Nat. Commun., 10(1):2173, 2019. 8 [53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [54] Philipp Tschandl. The HAM10000 dataset, large collection of multi-source dermatoscopic images of common pigmented skin lesions, 2018. 8 [55] Bastiaan Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. 2018. 5 [56] Deepak Vohra and Deepak Vohra. Apache parquet. Practical Hadoop Ecosystem: Definitive Guide to Hadoop-Related Frameworks and Tools, pages 325335, 2016. [57] Daniel Wong, Ziqi Tang, Nicholas Mew, Sakshi Das, Justin Athey, Kirsty McAleese, Julia Kofler, Margaret 12 Flanagan, Ewa Borys, Charles White, 3rd, Atul Butte, Brittany Dugger, and Michael Keiser. Deep learning from multiple experts improves identification of amyloid neuropathologies. Acta Neuropathol. Commun., 10(1):66, 2022. 8 [58] Michael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth Mahaffey, and Nigam Shah. Zero-shot arXiv preprint clinical trial patient matching with llms. arXiv:2402.05125, 2024. 2 [59] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 79597971, 2022. 4, 8, 9 [60] Gong-Her Wu, Charlene Smith-Geater, Jesus GalazMontoya, Yingli Gu, Sanket Gupte, Ranen Aviner, Patrick Mitchell, Joy Hsu, Ricardo Miramontes, Keona Wang, Nicolette Geller, Cathy Hou, Cristina Danita, Lydia-Marie Joubert, Michael Schmid, Serena Yeung, Judith Frydman, William Mobley, Chengbiao Wu, Leslie Thompson, and Wah Chiu. CryoET reveals organelle phenotypes in huntington disease patient iPSC-derived and mouse primary neurons. Nat. Commun., 14(1):692, 2023. 8 [61] Yan Yan, Romer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer Dy. Learning from multiple annotators with varying expertise. Machine learning, 95:291327, 2014. [62] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 2 [63] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2 - large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023. 7 [64] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023. 5 [65] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 8, 9 [66] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 4 [67] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. 4, 8, 13 BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature"
        },
        {
            "title": "Supplementary Material",
            "content": "10. Supplementary Material Table of Content Table 6. BIOMEDICA dataset statistics: Counts of articles, image-caption pairs, metadata, human annotators and specialties. 1. Dataset Description"
        },
        {
            "title": "Dataset Statistics\nCohort Diagram\nEntrez Statistics",
            "content": "2. Dataset Statistics"
        },
        {
            "title": "Context length",
            "content": "3. Dataset Curation Process PMC OA Subseet Description Serialization at the Image-caption Level 4. Labeling Dimensionality Reduction Over Clustering Taxonomy Annotation Form Correlation Analysis Cluster Examples 5. Model Training Configuration"
        },
        {
            "title": "Base Model selection\nHyperparameters",
            "content": "6. Evaluation Classification to VQA Conversion Conversion Prompts Evaluation Examples Dataset Provenance Table. Task Explanation Performance by Dataset. Bio-med Flicker and Statistics Evaluate Retrieval Performance by Caption Length. 7. Compute Environment 11. Dataset Description"
        },
        {
            "title": "Aspect\nArticles\nArticles with Images\nImages\nCaptions\nFigure Reference\nMetadata\nGlobal Concepts\nLocal Concepts\nClinical Annotators\nScientist Annotators",
            "content": "Count 6,042,494 5,050,473 24,076,288 24,076,288 30,711,542 22 13 170 2 6 Figure 5. BIOMEDICA cohort diagram: selection criteria for the construction of relevant image-caption pairs. BIOMEDICA dataset contains total of 24,076,288 imageEach caption pairs from 5,050,473 scientific articles. image-caption pairs is assigned article metadata and additional annotations. Table 6 shows descriptive statistics for BIOMEDICA dataset. Table 19 shows example imagecaption pairs. Figure 5 shows cohort diagram, summarizing the materialization of BIOMEDICA dataset. 12. Dataset Statistics We provide additional statistics for our dataset. The reference count, representing the number of articles citing given article, ranges from 0 to 3346, with median of 37 (IQR: 35). Articles with no references account for 120,870 entries. total of 263,836,608 references is found across the entire dataset. For MeSH terms, the dataset includes 29,859 unique terms. MeSH term counts per article range from 0 to 44, with median of 0 (IQR: 10) and total of 32,896,861 terms. Notably, 2,991,141 articles are annotated with MeSH terms, emphasizing the datasets depth in biomedical categorization. Table 14 summarizes the top 50 most frequent MeSH terms in the dataset. Text Statistics Caption Token Length Caption Character Length Figure Reference Token Length Figure Reference Character Length Full Text Token Length Full Text Character Length Image Statistics Image Width (pixels) Image Height (pixels) Image Area (pixelsÂ²) Min 1 1 17 39 20 70 Max 12389 25539 699117 1395195 2449711 8701762 Median 64 246 338 1323 10306 39072 IQR 134 498 299 1162 8831 32080 Total 2.84 109 1.05 1010 1.28 1010 4.82 1010 6.65 1010 2.47 1 1 1 52490 65081 1.95 109 709 476 334400 150 406 307272 - - - Table 7. Overview of dataset statistics, detailing text token and character lengths, and image dimensions. For text statistics, tokens are generated using the BPE tokenizer from the tiktoken library 13. Data Curation Process Besides releasing the code to make BIOMEDICA dataset fully reproducible, we provide additional descriptions and design choices for the dataset curation process. To increase efficiency and enable scaling, everything step in the data curation process is parallelized, unless specified otherwise. 13.1. PMC OA Dataset Description The PubMed Central (PMC) Open Access (OA) Subset is publicly accessible collection of full-text scholarly articles hosted by the National Center for Biotechnology Information (NCBI). This subset contains articles that have been made available under various open-access licenses. It covers wide range of disciplines within biomedical and life sciences, providing rich content that includes research articles, reviews, case reports, and clinical trials. As of 2024, over six million articles are available, with tens of thousands of new articles added annually, reflecting the continuous contributions of researchers worldwide. 13.2. Data Extraction The remote paths to the compressed files (tar.gz) containing each articles media files are listed in CSV file, referred to as the file list. The file list provides the structure for locating and accessing the content, ensuring that all relevant files can be traced and downloaded accurately. The file list includes the following columns: File, Citation, Accession ID, Date, PMID, and License. The server stores files with randomized paths to optimize storage; therefore, the absolute file path provided in the file list is required to retrieve the media files for specific article. We connect to the server using the Python package ftplib: ftp = FTP(\"ftp.ncbi.nlm.nih.gov\") ftp.retrbinary(\"RETR <remote_file_path>\", open(\"<local_file_path>\", \"wb\").write) Bulk downloads are available only for text files, necessitating individual retrieval of associated media files. The server enforces rate limit of three requests per second per IP address, with varying download speeds requiring precise scheduling to prevent disruptions. FTP connections may disconnect intermittently, requiring robust retry mechanism with short delays to maintain data integrity. To conserve storage, only necessary files (nxml and jpg) are kept, while other files (e.g., pdf, docx, xlsx, mp4) are discarded if present. After downloading and uncompressing all files, we create JSON files to store data extracted from the raw nxml and image files. These JSON files contain list of dictionaries, where each dictionary holds the data for single unique article. The figure set is list of dictionaries, where each dictionary contains the figures PMID, volume number, image file, caption, and context. This structure is shown in Figure 6. Entrez is search and retrieval system from NCBI that we use to collect additional metadata, including publication details and MeSH terms, which are not available in the file list or raw nxml files. The Entrez API supports batch queries with up to 200 PMIDs per request: from Bio import Entrez Entrez.email = \"<your_email>\" handle = Entrez.efetch( db = \"pubmed\", id = \"<comma_separated_PMIDs>\", retmode = \"xml\") Each JSON file is limited to maximum of 200 articles to comply with the Entrez API batch limit and to keep file sizes manageable for processing. 13.3. Dataset Serialization As shown in Figure 6 A, the retrieved data is collection of articles with full metadata and figure set (containing multiple images and captions). This structure can natively be serialized by article; however, it requires an extra iteration within the figure set. Instead, we decide to serialize 2 Figure 6. A) Diagram illustrating the structure of JSON file containing list of dictionaries representing PMC articles. Each article dictionary includes metadata fields such as PMID, nXML path, abstract, title, keywords, and nested figure set. The figure set is list of dictionaries, where each dictionary contains the figures PMID, volume number, image file, caption, and context. B) Diagram illustrating the WebDataset format, where data is stored across multiple .tar archives (e.g., data-000.tar, data-001.tar). Each archive contains paired text and image files representing individual records. the dataset by figure, such that row (figure within figure set) becomes unique image-caption pair rather than an article. This implies that each image-caption includes all the corresponding metadata. Note that this comes with the disadvantage of repeated entries for images belonging to the same figure set (e.g. within the same publication). In other words, if two different images come from the same article figure set, then all the metadata and nxml will be repeated twice. Figure 6 shows the data structure before (A) and after (B) serialization. PMC-OA Subsets were serialized in parallel. Table 8 shows the total serialization run time. Notably, we can serialize the entirety of PMC-OA within single day. 13.4. Tokenized Caption Distribution Note that the histogram in Figure 9 shows long right tail in the distribution of caption token lengths, with many captions exceeding the CLIP models context length limit. Need to provide numbers, mean min, max, median etc This issue is even more pronounced for figure reference."
        },
        {
            "title": "Subset\nCommercial\nNonCommercial\nOther",
            "content": "Serialization Time (Hrs) 23:50:57 7:36:35 1:36:"
        },
        {
            "title": "Total",
            "content": "33:04:14 Table 8. Total Serialization time by PMC-OA subset 14. Concept Labeling: Additional Details We developed an AI-assisted pipeline to categorize similar concepts within PMC-OA, as described in section Section 3.2. In summary, this process involves four stages: First, we define and organize similar images in clusters applying unsupervised clustering on image content, second, group of 2 clinicians and 1 scientist use these clusters and taxonomies to create hierarchical taxonomy for PMC-OA (see Figure 10), then group of 2 clinicians and 6 scientists use this taxonomy to annotate each cluster. majority vote approach is used to select the final cluster annotations. Lastly, this metadata is propagated to each cluster instance. In this section, we provide additional details and experiments for each design choice. 14.1. Dimensionality Reduction We used DINOv2 (ViT-L/14 distilled) to generate 1024dimensional vector for each image in PMC-OA . However, directly clustering such high-dimensional data can lead to poor performance due to the curse of dimensionality, where the increased sparsity in high-dimensional spaces reduces the reliability of distance-based measures like those used in K-means clustering. To address this, we applied PCA (Principal Component Analysis) to reduce the dimensionality of the embeddings. scree plot analysis was performed to determine the minimum number of principal components required to retain 99% of the data variance. As shown in Table 7, 25 principal components were sufficient to achieve this threshold. Consequently, we selected PCA (n=25) to transform the data before applying K-means clustering. 14.2. Over-Clustering We opt to over-cluster using KMeans with (K=2000), as this approach allows us to thoroughly explore the PMCOA dataset. By focusing on fine-grained patterns and subgroups, this strategy ensures detailed analysis, particularly when the optimal number of clusters remains uncertain. This number was also selected to achieve an effective annotation time of at most 72 hours per annotator. To do this at scale, we extract DINO-v2 features in parallel with NVIDIA Triton Inference. PCA and K-means are Figure 7. DinoV2 features Scree plot not parallelized. However, due to the simplicity of these approaches, they scaled to 24M pairs. Finally, cluster labels are stored along with image-uid. This is later used to propagate metadata to individual instances. 14.3. Online Cluster Annotations Form Via form hosted on Google Forms we asked two practicing clinicians and 5 scientists to annotate each image cluster by answering the following questions: 1. Are the majority images within this cluster single panel or multiple panel? Options: Single Panels Multiple panels with non-biomedical imaging Multiple panels with biomedical imaging and plots Multiple panels with biomedical imaging and assays 2. Think of the main characteristics the MAJORITY of images in this cluster share in common. Given these characteristics, what is the most likely global class for this group? Options:"
        },
        {
            "title": "Clinical Imaging\nMicroscopy\nImmuno Assays\nIllustrative Diagrams\nHand Drawn and Screen Based Visuals\nTables\nPlots and Charts\nPCR\nGraph and Network\nScientific Formulae and Equations\nChemical Structures\nMaps\nTools and Materials",
            "content": "3. Given these characteristics, what is the most likely local class for this group? (remember to use the hierarchical taxonomy provided) 4 Due to the quantity of high-resolution images, is not possible to compile all clusters together, thus 20 Google Forms, each containing 100 clusters were automatically created using Google Apps Script. Within each form, we provide the cluster image on top of each question to facilitate annotations. Annotators are calibrated by doing practice run on 20 examples (these annotations are not added to the analysis). Annotators were given two weeks 336 hours) to fill all assigned forms. Annotators could only be assigned up to 1000/2000 clusters (10 forms). Lastly, forms were overlapped with maximum of three annotators, meaning that annotations could have at most three labelers. 14.4. Taxonomy Curators Statistics Table 15 shows statistics for taxonomy curators (denoted with ). Curators have minimum of 3 years of experience, with median of 11 years and maximum of 14 years. Both clinical curators have an additional PhD. All curators have wet-lab experience. 14.5. Cluster Annotator Statistics Table 15 shows statistics for cluster annotators. Annotators have minimum of 3 years of experience, with median of 4 years and maximum of 14 years. On average, annotators spent 17.66 hours to finish all assigned forms. Collectively, annotators spent 103 hours annotating all 2000 clusters. For our statistics, experience in the biomedical domain is measured from the time person starts their graduate studies (such as masters or Ph.D.) and continues to accumulate as they work in the field. For this work, undergraduate studies do not contribute to the experience count, meaning that time spent pursuing Bachelors degree or any other undergraduate-level studies is excluded from the total experience calculation, even when its biomedical related. 14.6. Dataset Taxonomy The full delivered taxonomy is shown in Figure 10. Examples of topics with they corresponding image cluster are shown in Table 18. In total the taxonomy spans 13 global topics enumerated below: Clinical Imaging Microscopy Plots and Charts Immuno Assays Illustrative Diagrams Scientific Formulae and Equations Tables Hand Drawn and Screen Based Visuals Graph and Network Chemical Structures Maps Tools and Materials 5 PCR (Polymerase Chain Reaction) Notice that in contrast to the derived taxonomy, Clinical and Scientific Image Data is not included as topic. Instead we itemize this concept by its children (clinical imaging and microscopy). This design choice facilitates labeling. For the rest of this work, clinical imaging and microscopy do not share any parent. 14.7. Label Assignment and Propagation After labeling, each cluster has at most three expert-level annotations pertaining to panel type, global taxonomy, and local taxonomy. To resolve annotations, we follow threestep pipeline: 1. Text Prepocessing Since panel-type and global taxonomy are multiple-choice questions, no further reprocessing is necessary. Local taxonomy, however, consists of open questions (users are asked to follow the taxonomy as shown in Figure 10). To this end, answers in this section are preprocessed by being lowercased, and white spaces and dashes are deleted. 2. Majorty vote After preprocessing the final label for each field is resolved by taking the consensus of annotations by using the majority vote. 3. Label Propagation The metadata for the labeled cluster is then propagated to each image within that cluster. This is accomplished by retrieving the cached cluster label and image-uid (see Over-Clustering section) along with the corresponding resolved clustered metadata and adding this information to each item in within the serliaized data. Concept Min Max Mean Median 66.66 Panel 66.66 Global 75.0 Local 13.54 8.39 19.91 0.0 0.0 0. 0.0 0.0 0.0 IQR 33.33 0.0 50.00 Table 9. (lower is better). Statistical overview of inter-annotator disagreement 14.8. Inter-annotator Disagreement Table 9 shows statistical overview of inter-annotator disagreement. The median disagreement for all concepts is 0.0, while the maximum disagreement (present in local taxonomy) is 19.91%, highlighting high inter-annotator agreement greater than 80% for all concepts. Easier concepts, such as global taxonomy, show low mean disagreement of 8.39%. Figure 8 shows histogram summarizing these findings. 14.9. Data Upload is Data upload large folder with 40 workers. This upload mechato HuggingFace uploaded using Figure 8. Inter-annotator disagreement (lower is better). with stable training curve. To this end , We use learning rate of 1e-6 and warmup phase of 1000 steps. Optimizer: We use the Adam optimizer with parameters set to Î²1 = 0.9 and Î²2 = 0.95, and weight decay of 0.2. Floating-Point Precision: All computations are performed with 32-bit floating-point precision. Table 11 summarizes all parameters used during training. Model ViT-L-14 ViT-B-32 ViT-SO400M-14-SigLIP webli ViT-L-14 convnext-larged-320 ViT-L-14 EVA02-B-16 RN50-quickgelu ViT-B-16-SigLIP-384 ViT-B-16-SigLIP ViT-B-16-SigLIP-256 EVA02-L-14 ViT-B-16-SigLIP-512 ViT-B-32 coca-ViT-B-32 ViT-B-32 convnext-B ViT-B-16 coca-ViT-L-14 Pretraining Dataset Mean 38.465 commonpool 37.892 laion2b 35.010 33.951 33.773 33.148 32.995 32.736 32.482 32.399 31.762 31.680 31.319 30.614 29.133 29.077 28.701 25.786 25. laion2b laion2b datacomp merged2b s8b b131k openai webli webli webli merged2b s4b b131k webli commonpool laion2b datacomp laion2b laion2b laion2b Table 10. Average Accuracy of Base Models (with Corresponding Pretraining Datasets) on 5 biomedical classification tasks nism enables parallelism but relies on an already structured dataset. from huggingface_hub import HfApi api = HfApi(token=HF_TOKEN) api.upload_large_folder( repo_type repo_id folder_path = LOCAL_PATH, num_workers = 40) = \"dataset\", = REPO_ID, 15. Model training 15.1. Base Model selection In biomedical model development, it is common to skip ablations when selecting strong base model for continual pretraining, often defaulting to models that yield state-ofthe-art results in general domains. To this end, we use the validation sets of four random datasets to identify robust base model for further fine-tuning (see Table 10). Our analysis reveals that, generally, ViT models trained on datasets such as CommonPool, Liabo2B, and WebLi achieve the strongest performance in the selected biomedical domains, while previously favored models like CoCa show the weakest results. Based on these findings, we select CLIP, ViT-L14 Base as our model for subsequent experiments. 15.2. Modeling Hyperparameters We detail the key configurations for training our model as follows: Batch Size and Accumulation: We use batch size of 1024 per GPU on 4 GPUs with batch accumulation frequency of 2, yielding an effective batch size of 8192. Learning Rate: We perform sweep of learning rates from 1e 6 to 1e 8. We select the biggest learning rate"
        },
        {
            "title": "Values",
            "content": "Full Data (24M) CLIP: ViT-L-14 (commonpool) Concept Balanced Data (8M) Concept Filtered Data (6M) batch size (per GPU) GPUs accumulation frequency effective batch size learning rate beta1 beta2 warmup epochs precision gradient clipping norm dataset type batch size (per GPU) GPUs accumulation frequency effective batch size learning rate beta1 beta2 warmup epochs precision gradient clipping norm dataset type batch size (per GPU) GPUs accumulation frequency effective batch size learning rate beta1 beta2 warmup epochs precision gradient clipping norm dataset type 1024 4xH100 2 8192 1e6 0.9 0.95 1000 9 FP32 1.0 WebDataset 1024 4xH100 2 8192 1e6 0.9 0.95 1000 27 FP32 1.0 WebDataset 1024 4xH100 2 8192 1e6 0.9 0.95 1000 36 FP32 1.0 WebDataset Table 11. Hyper-parameters used for continual pretraining. . 7 Table 12. Provenance of evaluation benchmark. Each row lists dataset name (with its corresponding citation), task description, image modality, and number of classes in each corresponding tasks."
        },
        {
            "title": "Modality",
            "content": "N. Classes Cell Biology Cell Cycle & Stage Identification BBBC048 (BF) [19] BBBC048 (DF) [19] BBBC048 (EF) [19] Cell Profiling PCST Contour [10] PCST-Texture [10] PCST Eccentricity [10] Cell & Structure Identification Fluorescence Cells [38] EMPIAR SBF-SEM [28] ICPR2020 Pollen [6] Pathology Cytology Acevedo et al 2020 [1] Jung et al 2022 [31] Pap Smear 2019 [24] Neoplastic Histopathology Kather et al 2016 [32] LC25000 (Lung) [8] PCAM [35] LC25000 (Colon) [8] Non-neoplastic Histopathology Tang et al 2019 [52] Wong et al 2022 [57] Nirschl et al 2018 [42] Wu et al 2023 [60]"
        },
        {
            "title": "White blood cell\nWhite blood cell\nPap smear grading",
            "content": "Light Microscopy (Giemsa) Light Microscopy (Giemsa) Light Microscopy (Pap Smear)"
        },
        {
            "title": "Colorectal tissue\nLung tissue classification\nLymph node classification\nColon tissue classification",
            "content": "Light Microscopy (H&E) Light Microscopy (H&E) Light Microscopy (H&E) Light Microscopy (H&E) Amyloid morphology Light Microscopy (IHC) Light Microscopy (IHC) Amyloid morphology Clinical chronic heart failure Light Microscopy (H&E) Light Microscopy (IHC) Mitochondrial morphology General Microscopy Micro-Bench Submodality [38] Microscopy Submodality Micro-Bench Stain [38] Micro-Bench Domain [38] Micro-Bench Modality [38]"
        },
        {
            "title": "Microscopy\nMicroscopy\nMicroscopy\nMicroscopy",
            "content": "Radiology Diagnostics Chexpert [27] RSNA 2018 BreastMNIST [4] Dermatology HAM1000 [54] Chest X-Rays Findings Chest X-Rays Findings Breast cancer diagnosis Chest X-Ray Chest X-Ray Breast Ultrasound"
        },
        {
            "title": "Dermatoscope",
            "content": "Surgery Dresden Anatomy Dataset [12] Ophthalmology DeepDRiD [36]"
        },
        {
            "title": "Retina Fundus Images",
            "content": "7 7 5 3 3 3 13 5 4 8 5 4 8 3 2 2 4 4 2 6 6 6 3 5 2 2 7 7x2 4 Text Statistics Caption Token Length Caption Character Length Image Statistics Image Width (pixels) Image Height (pixels) Image Area (pixelsÂ²) Min 4 10 70 70 4900 Max 1324 3287 Median 23 78 IQR 63 Total 4.5 105 1.10 106 1024 1024 1048576 1024 768 786432 1 87 109206 Table 13. Overview of Biomedical Flickr statistics, detailing text token and character lengths, and image dimensions. Tokens are generated using the BPE tokenizer from the tiktoken library 16. Evaluation 16.1. Closed VQA Benchmark 16.1.1. Closed VQA Formulation total of 39 existing classification tasks are formulated as multiple-choice visual question answering. The following subsection provides additional details for evaluation reformulation. We first collect the test set of each dataset, yielding image-label pairs. Di = {(x , l1 ), . . . , (xNi , lNi )} where Ni corresponds to the total number of samples in the test subset of the i-th dataset. For each image-label pair, the label lj corresponds to one of Mi possible classes defined for the i-th dataset.. lj Ci, where Ci = {c1, c2, . . . , cMi } , Ci = Mi To convert each classification task into closed VQA task, we define mapping function fi for each dataset. This function maps given label lj to human written textual description: fi : lj is the textual descriptor of label lj tj , where tj applied to the entire dataset: . This process is Di = {(x1 , t1 ), . . . , (xNi , tNi )} Then the reminder of (incorrect) classes textual descriptions are added to each data point to create multiple-choice list: Ai = {tj }. Lastly, random permutation is applied, storing the position of the correct label after this operation kj . , . . . , aM i1 , a2 , i These operations convert the initial dataset to collection of image-text pairs, where each image xi is associated with: 1. list of multiple-choice answers 2. The correct index of the label within this list, denoted as ki: 9 Di = {(x1 , A1 , i ), . . . , (xNi , ANi , kNi )}. Where Ï denotes the random permutation function applied to the answers in AÏ(j) . 16.1.2. Closed VQA Evaluation All evaluated contrastive models have vision encoder Eimage and text encoder Etext. We first compute the image embedding, zxi = Eimage(xj ), along with each candii (j) = Etext(aÏ date answer: zaÏ (j)) for [1, Mi]. Then we compute the cosine similarity score for each caption, xi for [1, Mi]. The option with the largest sij = zaÏ sij is then assigned as the final prediction. If argmax(sij) has the same index as the corresponding correct answer kj the question is marked as correct, incorrect otherwise. (j) zT 16.1.3. Closed VQA conversion prompts If dataset did not explicitly contain Closed-VQA form, then group consisting of biomedical informatics, pathologist converted each class to unique its corresponding caption. 16.2. Retrieval Benchmark Evaluation"
        },
        {
            "title": "Given a dataset of images and captions",
            "content": "Dc = {(x1, c1), . . . , (xNi, cNi )}. We evaluate retrieval performance using Recall@k, using the following protocol: All evaluated contrastive models have vision encoder Eimage and text encoder Etext. We first compute the image embedding, zxi = Eimage(xi), along with each caption: zci = Etext(ci) for [1, Mi]. Then we compute the cosine similarity score for each caption, sij = zci zT xi for [1, Ni]. Captions are arranged from the largest to smallest similarity (sij). If the correct caption is within the first k-th arranged items, then the option is considered relevant, irrelevant otherwise. lastly, we calculate Recall@k using the following equation: Recall@k ="
        },
        {
            "title": "Number of relevant items in the top k results\nTotal number of relevant items in the dataset",
            "content": "16.3. Computing confidence intervals Error bars represent 95% confidence intervals (CI) computed via nonparametric bootstrapping using the SciPy stats.bootstrap function with 1000 resampling and default settings. 17. Flickr Dataset Description The Biomedical Flickr dataset consists of 7k image-caption pairs retrieved from flicker channels with permissive licenses. It mostly spans microscopy. Table 13 shows statistics for the dataset. Table 20 shows 10 random samples from the dataset. 18. Compute Environment Experiments are performed in local on-prem university compute environment using 24 Intel Xeon 2.70GHz CPU cores, 8 Nvidia H100 GPUs, 16 Nvidia A6000 GPUs, and 40 TB of Storage. 10 MeSH Term Humans Female Male Animals Adult Middle Aged None Aged Mice Young Adult Adolescent Retrospective Studies Child COVID-19 Cross-Sectional Studies Risk Factors Treatment Outcome Aged, 80 and over Signal Transduction SARS-CoV-2 Cell Line, Tumor Surveys and Questionnaires Prospective Studies Prognosis Rats Pregnancy Child, Preschool Mutation Biomarkers Disease Models, Animal Cell Proliferation Time Factors Mice, Inbred C57BL Infant Pandemics China Algorithms Neoplasms Cohort Studies Reproducibility of Results Phylogeny Prevalence Apoptosis Cells, Cultured Cell Line Gene Expression Profiling Brain Case-Control Studies Quality of Life Infant, Newborn Frequency 2,189,713 990,873 897,332 775,314 521,585 483,806 375,532 371,170 288,320 205,580 200,700 178,740 166,187 149,167 135,551 135,111 131,638 125,351 109,685 100,287 100,154 98,150 96,729 89,765 89,433 88,439 79,751 79,073 77,093 76,046 75,080 75,034 72,500 71,594 70,915 68,963 64,904 64,674 64,563 62,995 62,412 61,952 61,057 58,811 57,933 57,680 57,472 57,044 56,170 55,529 Table 14. Top 50 most common MeSH Terms and their frequencies 11 Annotator 1 Annotator 2 Annotator 3 Annotator 4 Annotator 5 Annotator 6 Annotator 7 Annotator 8 Years of experience 3 3 3 5 3 9 11 14 Field of study Developmental biology Microbiology Biomedical Informatics Biomedical Informatics Genetics Biomedical Informatics Biomedical Engineering, Molecular Biology, Surgical Data Science, ML Pathology, Cell biology, Neuroscience Role PhD Student PhD Student PhD Student PhD Student PhD Student MD-PhD Student Industry Director, Post-doc Attending Pathologist, Post-doc Table 15. Description of cluster annotators. Years of experience include years of research or laboratory experience in biology/biomedical or microscopy related discipline. Annotator developed taxonomy."
        },
        {
            "title": "Category\nScientific Formulae and Equations\nPCR\nTools and Materials\nMaps\nHand Drawn and Screen Based Visuals\nGraph and Network\nTables\nImmuno Assays\nChemical Structures\nClinical Imaging\nMicroscopy\nIllustrative Diagrams\nPlots and Charts",
            "content": "Other 2,322 2,307 10,320 18,700 24,690 26,453 26,190 38,055 92,881 82,349 101,617 140,925 542,"
        },
        {
            "title": "Commercial Noncommercial",
            "content": "20,384 25,353 210,740 264,092 356,047 415,737 269,384 651,339 839,082 1,078,901 1,818,302 2,227,690 9,426,147 6,182 7,693 51,339 41,473 76,404 83,470 343,455 215,238 196,119 766,908 597,413 551,380 2,394,358 Total 28,888 35,353 272,399 324,265 457,141 525,660 639,029 904,632 1,128,082 1,928,158 2,517,332 2,919,995 12,389,066 Table 16. Number of images by global taxonomy concepts. License Type CC0 CC BY CC BY-SA CC BY-ND CC BY-NC CC BY-NC-SA CC BY-NC-ND Other Number of Articles 132592 3795419 1287 7900 771755 275638 642982 Table 17. Number of articles by license type. As described in the PMC website: Commercial use allowed: CC0, CC BY, CC BY-SA, CC BY-ND. Non-commercial use only: CC BY-NC, CC BY-NC-SA, CC BY-NC-ND. Other: no machine-readable Creative Commons license, no license tagged, or custom license. 12 Cluster Example Global Taxonomy Local Taxonomy Multi-panel Clinical Imaging Data x-ray radiography Clinical Imaging Data computerized tomography Clinical Imaging Data electrocardiogram Microscopy fluorescence croscopy miMicroscopy electron microscopy Microscopy light microscopy Immuno Assays gel electrophoresis Plots and Charts bar plot Plots and Charts pie chart Chemical Structures 2D chemical reaction Chemical Structures 3D chemical structure Illustrative Diagrams signaling pathway Tables Maps table map Drawings drawing Tools and Materials lab equipment Natural Images natural image Table 18. Taxonomy of clusters with example images. Images resized to uniform width of 10cm and height of 1cm. Column widths adjusted to fit the page. Image Caption Four weeks after the accident: the radiograph shows good alignment (lateral view). CT-images belong to brain and maxillary sinus. Ct-images taken before any procedure applied illustrating two separate tumors in the brain (2a) and left-maxillary sinus (2b). Note the enhancing heterogeneous tumor at the left-temporoparietal lobe shifting the midline to the right (2a); and invasion of the tumor (T4) in the left-maxillary sinus into the adjacent tissues (2b). Detection of HSV-1 antigen. An impression cytology smear obtained from patient with HSK showing the presence of rounded up corneal epithelial cells positive for viral antigen. Infected cells show brilliant apple green fluorescence. Note the absence of background staining. Indirect immunofluorescence assay, 500. Detection of presence of CotC-LTB and CotC-TTFC by immunofluorescence microscopy. Sporulation of B. subtilis strains was induced by the resuspension method, and samples were taken 6 after the onset of sporulation and analysed by immunofluorescence microscopy as described previously [46]. Samples were labelled with mouse anti-LTB antibody followed by anti-mouse IgG-TRITC conjugate (red fluorescein, Panels & B), or rabbit anti-TTFC antibody followed by anti-rabbit IgG-FITC conjugate (green fluorescein, Panels & D). Panels & C, wild type spores; Panel B, isogenic spores expressing CotC-LTB); Panel D, isogenic spores expressing CotC-TTFC. Clinical photograph of the abdomen (close-up view): The surgical scar of implantation of baclofen pump is seen. The pigtail catheter emerges close to the scar. The skin around the pigtail catheter is red and angry-looking. Approximate position of baclofen pump is marked on the skin with pen. Comparison of apoptotic cells numbers Photomicrographs of the outer nuclear layer of 8 mo uninjected Rpe65-/- (A), rAAV.RPE65 injected Rpe65- /- (B) and C57BL/6J mice (C) stained for apoptotic nuclei (arrows). (D) Graphical presentation of the percentage of photoreceptor nuclei that are apoptotic in uninjected Rpe65-/-, rAAV.RPE65-injected Rpe65-/- and uninjected C57BL/6J mice. Apoptotic and total photoreceptor nuclei were counted along 60 Âµm lengths of the outer nuclear layer of mice at 7 mo post-injection (8 mo of age). Average total photoreceptor counts: uninjected Rpe65-/- = 106.8 22.9, rAAV.RPE65 injected Rpe65-/- = 134 30.3, uninjected C57 = 213.5 3.3. All data are mean S.D. Flow diagram showing randomisation and response rates of the survey. (a) simplified model of Cdk4 kinase molecule illustrates how basic BioD icons and action arrows can concisely represent intraand intermolecular actions. The Cdk4 molecule includes kinase site (K) that, when active, phosphorylates phosphorylation site (P) on the RB protein. The kinase site on Cdk4 is activated (filled arrow) by occupancy of its phosphorylation site and inhibited (open-squared arrow) by occupancy of the binding site (dotted circle) that binds the Cdk4 inhibitor p15. (b) An event model derived from the model above. Events are defined as changes of state of one or more functional properties of icons in state model. Here, for instance, the event model displays chain of events triggered by an increase of p15 concentration (see text). Renal contribution to endogenous glucose release from lactate during the postabsorptive phase. Data from [1]. Strategy for detection and treatment of adrenal failure during sepsis. ACTH, adrenocorticotrophic hormone. Mammographic changes in treated and control individuals. Values are expressed as median change from baseline and interquartile range. Table 19. Bomedica dataset Image-caption examples Benchmark Example: 10 examples from Biomedica dataset grouped by concept 14 Image Caption Colorized scanning electron micrograph of cell (red) heavily infected with SARS-CoV-2 virus particles (green), isolated from patient sample. HBE human derived cell lines cultured as micro-tissues (DAPI staining in blue; fixed with PFA). Infection with human Adenovirus type 2 expressing GFP. Immunofluorescence image of actin bundles in muscle precursor cells called myoblasts. The actin is labeled with fluorescently-tagged phalloidin, which is toxin from the Amanita phalloides mushroom. Nuclei are shown in blue. Cut surface of large apical bulla. Involved hilar lymph nodes also present. In this pleural biopsy, chronic inflammatory reaction with giant cells is seen, reacting to the presence of food material originating from an esophageal fistula. In the two bottom photographs, the structure of this material, although deteriorated, is still preserved. These are particles of vegetable material which, due to their size and shape, surely are seed -derived storage cells. Typical carcinoid tumor with organoid/insular growth and oncocytic tumor cells. There are many morphologic variants of carcinoid tumors. Metastatic calcifiication of alveolar walls and blood vessels in an area of acute pneumonitis. Dicrofilarium Pleomorphic lung carcinoma is composed of 10% or more of spindle cells and/or giant cells admixed with variable amounts of adenocarcinoma, squamous carcinoma, or large cell carcinoma. Some are composed solely of spindle cells and/or tumor giant cells. The diagnosis can only be made in surgical specimen, not in biopsy specimen. The type(s) of nonspindle cell carcinoma that are present should be mentioned in the pathology report. The term sarcomatoid carcinoma should be avoided because it is an umbrella term encompassing pleomorphic carcinoma, carcinosarcoma, and pulmonary blastoma.These images of pleomorphic carcinoma show malignant spindle cells admixed with adenocarcinoma Normal PA chest x-ray Table 20. BiomedFlickr Benchmark Example: 10 Random examples from biomedflcikr 15 Figure 9. Distributions of token counts and image dimensions in the dataset. Histograms are shown for token counts in captions, figure references, and full text, as well as for image widths and heights. Outliers have been excluded to highlight the central tendencies and areas of higher data density. TAXONOMY = {Ambiguous: [ambiguous], Chemical Structures: [2D chemical reaction,3D chemical reaction,2D chemical structure, 3D protein structure,3D chemical structure], Clinical Imaging: [x-ray radiography,optical coherence tomography,endoscopy, intraoral imaging,angiography,procedural image,skull,patient photo, functional magnetic resonance, magnetic resonance,eye,mammography, electrocardiography, clinical imaging, skin lesion,ultrasound, specimen,computerized tomography,laryngoscopy,teeth, intraoperative image,surgical procedure,brain], Graphs and Networks: [graph,neural network,network], Illustrative Diagrams: [sankey diagram,metabolic pathway,scientific illustration,diagram, signaling pathway,illustrative diagram,flow diagram, cohort selection flowchart,illustration,drawing,system diagram, flowchart], Immuno Assays: [immunocytochemistry,karyotype,gel electrophoresis,immunoassay, immunoblot,assay,immunohistochemistry], Laboratory Specimens and Cultures: [reagents,laboratory specimen,bacterial culture], Maps: [map], Microscopy: [scanning electron microscopy,electron microscopy,flowcytometry, transmission electron microscopy,light microscopy,fluorescence microscopy, phase contrast microscopy,confocal microscopy,epifluorescence microscopy, microscopy], Natural Images: [face,aerial photography,natural image,human head,humans and devices, human,insects,nature], PCR: [qPCR,RT PCR], Plots and Charts: [violin plot,bar plot,roc curve,sequence plot,radial plot,plot, matrix plot,phylogenetic tree,process chart,dot plot,pyramid chart, forest plot,box plot,survival curve,circos plot,venn diagram, heatmap plot,circular plot,scatter plot,word cloud,list,tree, density plot,funnel plot,plot and chart,2D mesh,3D plot, radial diagram,pie chart,manuscript,histogram, differential gene expression matrix,line plot,signal plot], Screen Based Visuals: [screenshot,user interface], Scientific Formulae and Equations: [algorithm], Tables: [table,checklist table], Tools and Materials: [medical equipment,microscope,electronic circuit,lab equipment, tool]} Figure 10. Hierarchical Taxonomy"
        }
    ],
    "affiliations": [
        "Department of Biomedical Data Science, Stanford University",
        "Department of Computer Science, Stanford University",
        "Department of Electrical Engineering, Stanford University",
        "Department of Pathology, Stanford University",
        "Department of Statistics, Stanford University"
    ]
}