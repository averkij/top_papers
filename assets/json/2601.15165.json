{
    "paper_title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "authors": [
        "Zanlin Ni",
        "Shenzhi Wang",
        "Yang Yue",
        "Tianyu Yu",
        "Weilin Zhao",
        "Yeguo Hua",
        "Tianyi Chen",
        "Jun Song",
        "Cheng Yu",
        "Bo Zheng",
        "Gao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap"
        },
        {
            "title": "Start",
            "content": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models January 22, 2026 Zanlin Ni 1 , Shenzhi Wang 1, Yang Yue 1, Tianyu Yu 2, Weilin Zhao 2, Yeguo Hua 3, Tianyi Chen 3, Jun Song 4, Cheng Yu 4, Bo Zheng 4, Gao Huang 1 (cid:66) 1 LeapLab, Tsinghua University 3 Tsinghua University 2 NLPLab, Tsinghua University 4 Alibaba Group nzl22@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL)1 to elicit the reasoning capability of dLLMs. In this paper, we reveal counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap 6 2 0 2 1 2 ] . [ 1 5 6 1 5 1 . 1 0 6 2 : r Figure 1: Less flexibility unlocks better reasoning potential. Left: We observe counter-intuitive phenomenon where restricting dLLMs to standard Autoregressive (AR) order expands the reasoning solution space. Right: Motivated by this, we propose JustGRPO. By foregoing complex arbitraryorder adaptations and adopting standard GRPO, we effectively elicit the reasoning capability of dLLMs. 1In this paper, unless otherwise specified, we use the term RL to refer specifically to Reinforcement Learning with Verifiable Rewards (RLVR), as it is the predominant paradigm for enhancing the reasoning capabilities of dLLMs. 1. Introduction The Flexibility Trap Recent research has witnessed surge in Diffusion Large Language Models (dLLMs) (Nie et al., 2025; Ye et al., 2025; Zhu et al., 2025; Zhao et al., 2025), which challenge the dominant autoregressive (AR) paradigm (Brown et al., 2020; Achiam et al., 2023) by treating sequence generation as discrete denoising process. Central to the appeal of dLLMs is their theoretical flexibility, which offers two distinct advantages over the strict left-to-right causal chain of AR models: efficient parallel decoding and the capability for arbitrary-order generation. While the efficiency gains of parallel decoding have been well-established (Wu et al., 2025b; Labs et al., 2025; DeepMind, 2025; Song et al., 2025; Wu et al., 2025a), the implications of arbitrary-order generation remain less explored (Ye et al., 2024; Kim et al., 2025). Theoretically, the unconstrained generation order constitutes superset of the fixed autoregressive trajectory. This flexibility naturally suggests potential for superior reasoning: in general reasoning tasks like mathematics and coding, such freedom could unlock non-sequential problem-solving paths inaccessible to standard left-to-right models. As result, recent works have increasingly adopted RL to elicit reasoning capabilities of dLLMs (Zhao et al., 2025; Gong et al., 2025; Wang et al., 2025a; Ou et al., 2025). this In this paper, we present counter-intuitive observation: arbitrary-order generation, in its current form, narrows rather than expands the reasoning potential elicitable by RL. To rigorously assess this, we employ Pass@k (Chen, 2021), which measures the coverage of solution space. Recent studies suggest that RL primarily acts to sharpen the base models distribution; consequently, the Pass@k performance of the base model effectively sets the upper bound for the reasoning capability of the model after RL training (Yue et al., 2025; Liu et al., 2025; Zhang et al., 2025). Under this metric, we compare the reasoning potential of LLaDA (Nie et al., 2025) with arbitrary-order generation against standard AR decoding. As shown in Figure 1 (Left), restricting dLLM to standard AR order in fact yields higher Pass@k, and consequently higher reasoning boundary, than its flexible counterpart. counter-intuitive pheWe attribute nomenon to the way model handles uncertainty. Reasoning is inherently non-uniform: it typically hinges on sparse forking tokens, i.e., connectives like Therefore or Since which do not merely continue sentence but fundamentally steer the logical trajectory into distinct branches (Wang et al., 2025c; Cheng et al., 2025; Huang et al., 2025a). At these forks, the reasoning path diverges, naturally manifesting as localized spikes in entropy (Wang et al., 2025c). Standard AR decoding compels the model to confront this uncertainty (Figure 2a). By sampling exactly at the fork, the model is able to explore different reasoning paths, thereby preserving the diversity of the generated rationales. Arbitrary-order generation, however, allows the model to bypass these hard decisions (Figure 2b). It prioritizes low-entropy completions first. By the time the model returns to infill the bypassed forks, the established bidirectional context has already severely constrained the potential branches. The ambiguity is prematurely resolved, and the original high entropy is suppressed. We term this phenomenon entropy degradation. Effectively, the model trades the exploration of diverse reasoning paths for the greedy optimization of local consistency. Figure 2: Confronting vs. bypassing uncertainty. (a) AR order preserves reasoning space by forcing decisions at uncertain tokens. (b) Arbitrary order bypasses uncertainty and resolves easier tokens first. Once future context is established, the original forks collapse, prematurely narrowing the solution space. The above observations motivate rethink of RL for dLLMs. Current methods operate under the assumption that preserving arbitrary-order flexibility is essential. This commitment incurs heavy tax: algorithms must grapple with combinatorial explosion of denoising trajectories (Zhao et al., 2025; Yang et al., 2025; Gong et al., 2025) and intractable marginal likelihoods (Ou et al., 2025), forcing reliance on 2 The Flexibility Trap unstable approximations (Wang et al., 2025a; Ou et al., 2025; Rojas et al., 2025). However, if arbitrary order is non-essential, or even detrimental for eliciting reasoning potential, this complexity is unjustified. To this end, we propose return to simplicity with Just GRPO. We demonstrate that eliciting reasoning potential for general reasoning tasks does not require complex, diffusion-specific RL adaptations. Instead, it is best achieved by simply treating the dLLM as an AR model during RL training. This allows us to apply standard GRPO (Shao et al., 2024) without bells and whistles, turning an otherwise intractable optimization with unstable approximations into well-defined task. JustGRPO is surprisingly effective. On complex reasoning benchmarks, it achieves competitive results (e.g., 89.1% accuracy on GSM8K, 45.1% on MATH), surpassing methods that rely on complex diffusionspecific RL. Crucially, while we train with AR constraints to maximize reasoning potential, the model retains dLLMs ability of efficient parallel decoding at inference time. By returning to basic left-to-right ordering, this work encourages re-evaluation of arbitrary versus AR order in the development of next-generation language models. 2. Preliminaries 2.1. Diffusion Large Language Models Diffusion Large Language Models (dLLMs), particularly Masked Diffusion Models (MDMs), generate sequences by iteratively denoising masked state xt initialized from fully masked tokens. The process is indexed by continuous time variable [0, 1], representing the masking ratio. Given clean sequence x0, the forward process independently masks each token with probability t: q(xk xk 0) = (cid:26)[MASK], with prob t, xk 0, with prob 1 t. Unlike Gaussian diffusion, MDMs directly predict the clean token. neural network pθ(x0 xt) estimates the original token distribution at masked positions. During inference, generation starts from x1 (all [MASK]) and iteratively unmasks subset of tokens based on heuristics such as confidence scores, updating xt xtt until completion. As special case, autoregressive generation can be recovered by always unmasking the leftmost token. The model is trained by minimizing the Negative Evidence Lower Bound, which reduces to weighted cross entropy loss over masked tokens: LMDM(θ) = EtU [0,1], xtq(xtx0) (cid:34) 1 L (cid:88) k=1 2.2. Group Relative Policy Optimization 1[xk = [MASK]] log pθ(xk (cid:35) 0 xt) . Group Relative Policy Optimization (GRPO) is reinforcement learning algorithm that avoids value function estimation by using group level reward normalization. It is typically applied to autoregressive policies πθ(ok o<k, q). For each query q, GRPO samples group of outputs {o1, . . . , oG} from the old policy πθold . An advantage Ai is computed by standardizing the reward r(oi) against the group statistics: Ai = (r(oi) µG)/σG, where µG and σG are the group mean and standard deviation. The GRPO objective maximizes clipped surrogate function with KL regularization term: Li(cid:88) (cid:35) (min (ρi,kAi, clip(ρi,k, 1 ϵ, 1 + ϵ)Ai) βDKL(πθπref)) JGRPO(θ) = (cid:88) (cid:34) qD,{oi}G i=1πθold 1 1 Li i=1 k= (1) with the token level importance ratio ρi,k = πθ(oi,koi,<k,q) (oi,koi,<k,q) πθold . 2.3. Pass@k as Proxy for Reasoning Potential To rigorously quantify the reasoning capability boundaries of dLLMs, we adopt the Pass@k metric (Chen, 2021; Yue et al., 2025). In the context of Reinforcement Learning with Verifiable Rewards (RLVR), which The Flexibility Trap Figure 3: Reasoning potential measured by Pass@k. While arbitrary order is competitive in single-shot settings (k = 1), it exhibits notably flatter scaling curves compared to AR Order. is currently the dominant paradigm for enhancing reasoning capabilities, exploration is prerequisite for improvement. An RL agent can only reinforce correct reasoning paths if it is capable of sampling them during the exploration phase to obtain positive reward signal. Accordingly, Pass@k has been widely established as standard measure of models reasoning potential (Yue et al., 2025; Liu et al., 2025; Zhang et al., 2025). It measures the probability that at least one correct solution is generated within independent samples, effectively delineating the upper bound of the solution space accessible to the RL optimizer. Formally, following the unbiased estimator formulation (Chen, 2021; Yue et al., 2025), given samples where are correct, Pass@k is calculated as: (cid:34) Pass@k = 1 (cid:35) (cid:1) (cid:0)nc (cid:1) (cid:0)n (2) high Pass@k indicates that the correct reasoning trajectory lies within the models sampling distribution and is thus learnable via RL optimization. Conversely, if model consistently fails to yield solution despite vast sampling budget, it suggests the problem lies beyond its intrinsic reasoning boundary. In such scenarios, standard RLVR methodologies are fundamentally limited by the absence of positive exploration signals (Yue et al., 2025). 3. The Flexibility Trap In this section, we rigorously test whether the theoretical flexibility of arbitrary-order generation translates into higher reasoning potential. We compare two decoding modes: Arbitrary Order, which 4 The Flexibility Trap follows standard diffusion decoding with low-confidence remasking (Nie et al., 2025; Zhao et al., 2025; Wu et al., 2025b), and AR Order, where arbitrary-order flexibility is disabled and generation is strictly constrained to left-to-right decoding. We adopt the commonly used experimental setup in prior diffusion LLM work: maximum of 256 tokens decoded over 256 steps, semi-autoregressive block size of 32. Sampling temperature is set to 0.6 as in (Yue et al., 2025). The prompt template follows (Zhao et al., 2025). Results under alternative temperatures and sampling configurations are deferred to Appendix B. 3.1. Arbitrary Order Limits Reasoning Potential Pass@k analysis. We first evaluate the reasoning potential using the Pass@k metric on three representative dLLMs: LLaDA-Instruct (Nie et al., 2025), Dream-Instruct (Ye et al., 2025), and LLaDA 1.5 (Zhu et al., 2025) on four reasoning benchmarks: GSM8K, MATH500, HumanEval, and MBPP. As shown in Figure 3, while arbitrary order often achieves competitive performance at = 1, it exhibits notably flatter scaling curve compared to the AR mode. As increases, the AR mode demonstrates stronger capacity to uncover correct solutions. Solution space coverage. One might hypothesize that arbitrary order explores different solution space, albeit less efficiently, which could account for its lower reasoning potential. We test this by analyzing solution coverage at = 1024 using LLaDA-Instruct. Figure 4 presents stark reality: the reasoning traces generated by arbitrary order are largely strict subset of those generated by AR. On HumanEval, AR solves 21.3% of the problems that arbitrary order misses, whereas the reverse is negligible (0.6%). This indicates that the flexible decoding process rarely unlocks genuinely new solutions. Instead, it appears to retreat into more conservative subset of the AR solution space. Figure 4: Solution space coverage measured by Pass@1024. The reasoning traces generated by arbitrary order are largely strict subset of those generated by AR Order. 3.2. Mechanism: The Entropy Degradation Adaptive decoding bypasses logical forks. To understand why the theoretically superior solution space of dLLMs collapses in practice, we examine more closely how the two decoding modes handle uncertainty. In AR order, the model is constrained to strictly resolve the left-most unknown token at each step, forcing the model to confront uncertainty as it arises. In contrast, arbitrary order adaptively selects tokens to update based on model confidence, preferentially generating easy tokens with high certainty while bypassing hard ones. Inspecting the frequently bypassed tokens reveals clear pattern: As shown in Figure 5, the diffusion sampler disproportionately defers logical connectives and transition markers such as Therefore, Thus, and Since. Prior work has shown that such tokens often have high entropy (which also holds true in dLLMs; see Figure 6), and act as reasoning sparks or logical forks, functioning as branching points that determine subsequent reasoning directions (Wang et al., 2025c; Cheng et al., 2025; Wang et al., 2025b; Huang et al., 2025a). In conventional language models, keeping these tokens in high-entropy state is critical for effective exploration of the reasoning space (Wang et al., 2025c). Figure 5: Frequently bypassed tokens in arbitrary order, measured on MATH-500, are typically logical connectors and transition words. 5 The Flexibility Trap The entropy degradation phenomenon. This adaptive behavior comes at cost: the premature collapse of reasoning possibilities. We measure the entropy of these pivotal connectors at decoding. In AR order, these tokens maintain high entropy, reflecting genuine branching point where multiple logical paths remain viable. In contrast, arbitrary order exhibits sharp decrease in entropy (Figure 6, with more results in Appendix B). By deferring the logical connector, the model commits to specific future outcome that is generated based on its inherent inductive biases before deciding the logic that leads there. When the model eventually returns to fill in the bypassed connector, it is no longer making navigational decision at fork; it is more like selecting the connector that grammatically bridges the gap to its pre-generated conclusion. The decoding process thus implicitly shifts from reasoning exploration into semantic pattern matching. We term this phenomenon entropy degradation. Conclusion. In summary, the flexibility of arbitrary order serves as mechanism for inference-time exploitation rather than reasoning exploration. By bypassing high-uncertainty tokens, the model effectively collapses the solution space to safe, low-entropy path, squeezing out slightly better singleshot coherence at the expense of reasoning potential. Autoregressive models, by contrast, lack this bypassing capability and are therefore forced to sample directly from high-entropy distributions at logical forks. It is precisely this inability to circumvent critical decision points that prevents the premature narrowing of the search space and preserves the reasoning potential. Figure 6: Entropy degradation. While the global average entropy of Arbitrary Order remains comparable to AR (dashed lines), the entropy at logical forks drops significantly (blue bars). 4. Just GRPO for Diffusion Language Models The findings in Section 3 suggest that arbitrary order actually limits the reasoning potential accessible to RL. Despite this, current RL methods for dLLMs remain heavily burdened by the need to preserve this specific flexibility. In this section, we uncover the heavy tax imposed by this flexibility (Section 4.1) and show that discarding it enables minimalist yet surprisingly effective solution: JustGRPO (Section 4.2). 4.1. The Flexibility Tax in dLLMs RL Existing diffusion RL methods operate under the premise that the policy must optimize over the full combinatorial space of denoising trajectories to preserve the flexibility of arbitrary order generation. This design choice, while conceptually general, introduces several fundamental challenges. Ambiguity in token-level decomposition. In dLLMs, generation state st is noisy sequence conditioned on stochastic unmasking trajectory τ . Unlike autoregressive models, dLLMs do not admit unique, index-aligned conditional probability of the form π(ot st), making token-level credit assignment ambiguous and rendering the standard importance ratio ρt = πθ(otst) πold(otst) Intractable sequence likelihood. Autoregressive models factorize the sequence likelihood as log π(o) = (cid:80) o<t, q), whereas dLLMs require marginalization over all valid denoising trajectories, log π(ot πθ(o q) = (cid:80) τ πθ(o, τ q). For sequence of length , the trajectory space grows as = O(N !), rendering exact likelihood computation infeasible and forcing existing methods to rely on ELBO-based surrogates rather than the true objective (Wang et al., 2025a; Ou et al., 2025). hard to define. Sampler-learner mismatch. Even with an accurate likelihood approximation, more subtle issue persists. In practice, rollout samples are produced by heuristic-guided policies πheur (o q) to explore the combinatorial space. However, the ELBO objective still targets the likelihood of the original model distribution πθ(o q), rather than that of the heuristic-guided sampler πheur (o q), leading to critical mismatch between sampling and optimization that can degrade performance (Schulman et al., 2015). θ θ 6 4.2. JustGRPO The Flexibility Trap We propose return to simplicity. Since pure autoregressive order yields better reasoning potential (Section 3), we explicitly forgo arbitrary-order generation during the RL stage. This constraint transforms the dLLM from chaotic sequence denoiser into well-defined autoregressive policy πAR Formulation. Standard GRPO assumes policy π(oto<t, q) accepting partial sequence o<t with all tokens observed and predicts one token ot at time, where is the query. Diffusion language models, however, are architected as sequence-level denoisers that accept full sequence with mixed observed and masked tokens and predict the original values for all masked positions simultaneously. By forgoing arbitrary-order generation, we are able to bridge the above gap and rigorously define an AR policy πAR for dLLMs. To obtain the probability of the next token ot given history o<t, we construct an input state xt where the past is observed and the future is masked: . θ θ ]. , [MASK], . . . , [MASK] xt = [o1, . . . , ot1 (cid:125) (cid:123)(cid:122) (cid:124) (cid:125) Masked (cid:123)(cid:122) Observed (cid:124) (3) Although the diffusion language model outputs predictions for all masked positions, the autoregressive (oto<t, q) as the probability distribution of policy concerns only the next token ot. We thus define πAR ot: (4) where fθ denotes the model logits. Consequently, the likelihood of complete reasoning chain is exactly computable as: (oto<t, q) Softmax(fθ(xt))t, πAR θ θ πAR θ (oq) = (cid:89) t= πAR θ (oto<t, q). (5) Optimization. The above formulation enables the direct application of standard GRPO to diffusion language models. For each query q, we sample group of outputs {oi}G . The objective is: using the old policy πAR θold i= (θ) = qP (Q),{oi}G i=1πAR θold 1 G (cid:88) i=1 1 oi oi (cid:88) (cid:16) (cid:16) min t=1 ρi,t ˆAi,t, clip(ρi,t, 1 ε, 1 + ε) ˆAi,t (cid:17) βDKL (cid:17) , (6) where ρi,t = πAR (oi θ (oi πAR θold the group-standardized advantage. <t,q) <t,q) toi toi is the probability ratio between the current and old policies and ˆAi,t denotes Remarks. One might worry whether training in AR mode turns the diffusion language model into standard autoregressive model. This is not the case. The AR constraint is applied only during training to correctly assign credit. It refines the models joint distribution p(o) without altering the underlying architecture. At inference time, the model retains its conditional independence properties, still allowing us to employ parallel samplers (Ben-Hamu et al., 2025) to accelerate decoding. JustGRPO thus achieves the reasoning depth of autoregressive models while preserving the inference speed of dLLMs (see Section 5.2). 5. Experiments We evaluate JustGRPO on standard mathematical reasoning and coding benchmarks. Our experimental design aims to verify two hypotheses: (i) that enforcing autoregressive (AR) order during RL training elicits superior reasoning capabilities compared to complex arbitrary-order approximations, and (ii) that this constraint applies only to the optimization objective, leaving the diffusion models parallel decoding capabilities intact at inference. Experimental Setups. We apply JustGRPO on LLaDA-Instruct (Nie et al., 2025) and evaluate its effectiveness on four standard reasoning and coding benchmarks: GSM8K, MATH, HumanEval, and 7 The Flexibility Trap Table 1: System-level comparison. RL post-training approaches on LLaDA-Instruct. JustGRPO consistently achieves state-of-the-art performance across all tasks and sequence lengths. LLaDA-1.5 and LLADOU are excluded from the comparison as LLaDA-1.5 is trained on privately collected dataset at significantly larger scale, while LLADOU modifies the base architecture with an auxiliary module. GSM8K MATH-500 HumanEval MBPP Model / Seq Len LLaDA-1.5 LLaDOU D1 WD1 d-TreeRPO ESPO GDPO SPG 128 - - 73.2 77.2 - 80.0 78.4 78.5 256 83.3 88.1 81.1 80.8 81.2 82.3 82.8 86.1 512 - - 82.1 82.3 82.6 83.7 84.5 84. 128 - - 33.8 33.3 - 36.0 33.2 33.4 256 - 41.1 38.6 34.4 37.7 39.0 39.6 40.0 512 - - 40.2 39.0 38.9 43.4 41.4 41. 128 29.3 - - - - 28.1 26.2 - 256 39.6 59.1 - - - 42.1 39.6 - JustGRPO (Ours) 83.8 89.1 89.8 39.0 45.1 45.2 37.8 49.4 51.9 - - - - 50.0 39.0 - 48.7 128 39.6 - - - - 47.4 43.6 - 39.9 51.6 - - - 44.6 50.6 - 512 38.8 - - - - 44.2 47.1 - 50.6 52.4 49.0 MBPP. For mathematical tasks, we train on the official training split of each dataset following (Zhao et al., 2025; Ou et al., 2025). For coding tasks, we train on subset of AceCoder-87K (Zeng et al., 2025) following (Gong et al., 2025; Ou et al., 2025). We apply JustGRPO directly to the models without additional task-specific SFT. Our training recipe largely follows (Huang et al., 2025b). To evaluate the trained models, we follow the standard LLaDA evaluation protocol (Nie et al., 2025), which applies low-confidence remasking together with semi-autoregressive decoding in blocks of 32 tokens, using sampling temperature of 0. We evaluate all benchmarks at generation lengths of 128, 256, and 512 following (Zhao et al., 2025; Ou et al., 2025). More details on training are provided in Appendix A. 5.1. Main Results Table 1 reports the system-level comparison. We observe that simplifying the training objective to standard autoregressive formulation yields consistent improvements over methods specifically designed for dLLMs. Performance on reasoning tasks. On GSM8K, JustGRPO achieves 89.1% accuracy (seq len 256), improving upon the previous best method, SPG, by non-trivial margin (3.0%). This trend generalizes to the more challenging MATH-500 benchmark, where our approach outperforms ESPO by 6.1%. These results challenge the prevailing assumption that RL for diffusion models requires optimizing over the full combinatorial space of denoising trajectories. Instead, treating the dLLM as sequential generator during training appears more effective for credit assignment in logic-heavy tasks. Robustness across generation budgets. potential concern is that AR constraints might overfit the model to specific trajectory lengths. However, we observe robust performance gains across varying sequence lengths (128, 256, 512). This stability suggests that the policy has improved its fundamental reasoning capability, i.e., the ability to navigate logical branches, rather than merely memorizing fixed-length patterns. 5.2. JustGRPO Preserves Parallel Decoding We further investigate whether the AR training constraint compromises the models inherent diffusion capabilities. We employ the training-free Entropy Bounded (EB) Sampler (Ben-Hamu et al., 2025) to evaluate inference performance under varying degrees of parallelism (tokens per step). Figure 7 reveals that our model not only retains full compatibility with parallel decoding but exhibits strictly superior trade-off between speed and accuracy. Surprisingly, the performance gain becomes more 8 The Flexibility Trap Figure 7: JustGRPO preserves the parallel decoding capability of dLLMs. Interestingly, when compared to original instruct model, accuracy gains are larger with more parallel tokens, likely due to more robust reasoning capabilities after JustGRPO training. We adopt training-free EB-sampler (BenHamu et al., 2025) for parallel decoding. pronounced as parallelism increases. As shown in the MBPP and HumanEval results, while the baselines performance degrades sharply with more aggressive parallel steps, our model maintains stability. For instance, on MBPP, the accuracy gap expands from +10.6% at conservative settings (1 token/step) to +25.5% at aggressive settings (5 tokens/step). This observation suggests that JustGRPO does not merely fit specific decoding path; rather, it learns more robust reasoning manifold that is resilient to the approximation errors inherent in parallel sampling. The AR formulation thus acts as an effective training scaffold that refines the joint distribution p(o) during optimization, providing more stable foundation for parallel samplers to exploit at inference. 6. Related Work Diffusion language models. Inspired by the success of diffusion models in continuous image domains (Ho et al., 2020; Rombach et al., 2022), recent work has extended diffusion to discrete text generation. Early approaches operating in continuous embedding spaces (Li et al., 2022; Gong et al., 2022; Han et al., 2022) suffered from optimization and discretization issues. In contrast, masked diffusion models (Lou et al., 2023; Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024), which define the diffusion process directly in the discrete token space via random masking, have become the dominant paradigm. Notably, recent large-scale models such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025) demonstrate scalability and performance competitive with autoregressive (AR) models. The emergence of such large-scale diffusion language models has spurred growing body of follow-up studies exploring their distinctive modeling and inference properties. Among various advantages discussed in the literature, two aspects have received particular attention. First, dLLMs naturally support parallel decoding, enabling significant inference acceleration (Wu et al., 2025b; Ben-Hamu et al., 2025; Labs et al., 2025; DeepMind, 2025; Song et al., 2025). Second, their non-autoregressive formulation allows for arbitrary-order token generation, which has been hypothesized to benefit complex reasoning by relaxing strict left-to-right constraints (Ye et al., 2024; Kim et al., 2025). The value of order arbitrariness. While early studies validated the utility of arbitrary order generation in constrained tasks like Sudoku and Zebra Puzzles (Ye et al., 2024; Nie et al., 2025; Kim et al., 2025), recent research has begun to investigate its value in standard reasoning domains. One line of research, exemplified by Dream-Coder (Xie et al., 2025) and DiffuCoder (Gong et al., 2025), shows that diffusion models can naturally exhibit non-standard, human-like decoding behaviors (e.g., sketch-first reasoning) without any explicit supervision on generation order. DiffuCoder (Gong et al., 2025) further quantifies this phenomenon by demonstrating that higher sampling temperatures reduce AR-ness, with increased order randomness correlating with improved output diversity. Another line of research, exemplified by P2 (Peng et al., 2025) and LLaDOU (Huang et al., 2025b), explicitly optimizes 9 The Flexibility Trap the decoding strategy for better generation order, thus better performance. Despite these advances, it remains unclear whether the observed gains primarily arise from better exploitation of existing solution patterns encoded in the data and model, or whether order arbitrariness itself enables qualitatively new reasoning strategies that are unattainable under purely autoregressive decoding regime. Reinforcement learning for diffusion language models. Reinforcement learning for dLLMs faces structural optimization hurdles distinct from the autoregressive paradigm, primarily stemming from the combinatorial explosion of denoising trajectories. While early attempts (Zhao et al., 2025; Yang et al., 2025; Gong et al., 2025) sought to adapt token-level formulations directly, they were fundamentally limited by ill-defined state transitions, necessitating reliance on unstable mean-field approximations. Consequently, the field has shifted toward sequence-level perspectives (Zhu et al., 2025; Wang et al., 2025a; Rojas et al., 2025; Ou et al., 2025), employing various surrogates to approximate the intractable marginal likelihood. Yet, critical off-policy misalignment persists across these methods: the heuristicguided sampling required for efficient exploration diverges from the underlying diffusion prior, rendering gradients biased without principled correction (Schulman et al., 2015). Two notable exceptions partially address these issues. LLaDOU (Huang et al., 2025b) explicitly models token position selection via an auxiliary policy, enabling direct estimation of trajectory likelihoods, while TraceRL (Wang et al., 2025d) aligns optimization with inference traces through shrinkage-based step-wise MDP formulation. Nevertheless, these approaches remain committed to preserving the full arbitrary-order generation mechanism, implicitly treating its structural complexity as indispensable, whereas we question whether effective RL training can be achieved more simply by reexamining the necessity of arbitrary order itself. 7. Conclusion The intuitive appeal of diffusion language models (dLLMs) lies in their order arbitrariness, often perceived as superior mechanism for navigating complex reasoning paths. Our study reveals counter-intuitive reality: this unrestricted flexibility in fact narrows the reasoning potential. By allowing the model to bypass high-entropy tokens, effectively skipping the most demanding logical branches, arbitraryorder generation acts as an exploitation mechanism that prioritizes greedy optimization of individual trajectories at the expense of broader solution coverage. Therefore, eliciting the reasoning capability of dLLMs can be simpler. By operating dLLMs in standard autoregressive manner, we enable the direct application of Group Relative Policy Optimization (GRPO) without any complex adaptations tailored for order arbitrariness. This intentional constraint paradoxically yields significant upgrade in reasoning performance, while fully preserving the parallel decoding capabilities of dLLMs. By returning to the basic, natural left-to-right order of language modeling, we hope to encourage re-examination of its real value in the training of next-generation diffusion models. 10 The Flexibility Trap References Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 Ben-Hamu, H., Gat, I., Severo, D., Nolte, N., and Karrer, B. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. 7, 8, Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 Chen, M. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 2, 3, 4 Cheng, D., Huang, S., Zhu, X., Dai, B., Zhao, W. X., Zhang, Z., and Wei, F. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. 2, 5 DeepMind. Gemini diffusion, 2025. URL https://deepmind.google/models/gemini-diffusion/. 2, 9 Gong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. 9 Gong, S., Zhang, R., Zheng, H., Gu, J., Jaitly, N., Kong, L., and Zhang, Y. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. 2, 8, 9, 10, 14 Han, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In NeurIPS, 2020. 9 Huang, G., Xu, T., Wang, M., Yi, Q., Gong, X., Li, S., Xiong, R., Li, K., Jiang, Y., and Zhou, B. Low-probability tokens sustain exploration in reinforcement learning with verifiable reward. arXiv preprint arXiv:2510.03222, 2025a. 2, 5 Huang, Z., Chen, Z., Wang, Z., Li, T., and Qi, G.-J. Reinforcing the diffusion chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025b. 8, 9, 10, 14 Kim, J., Shah, K., Kontonis, V., Kakade, S., and Chen, S. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. 2, 9, 15 Labs, I., Khanna, S., Kharbanda, S., Li, S., Varma, H., Wang, E., Birnbaum, S., Luo, Z., Miraoui, Y., Palrecha, A., et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. 2, Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. 9 Liu, M., Diao, S., Lu, X., Hu, J., Dong, X., Choi, Y., Kautz, J., and Dong, Y. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. 2, 4 Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 9 Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. 2, 5, 7, 8, 9, 15 11 The Flexibility Trap Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. 9 Ou, J., Han, J., Xu, M., Xu, S., Xie, J., Ermon, S., Wu, Y., and Li, C. Principled rl for diffusion llms emerges from sequence-level perspective. arXiv preprint arXiv:2512.03759, 2025. 2, 3, 6, 8, 10, 14, Peng, Z., Bezemek, Z., Patel, S., Rector-Brooks, J., Yao, S., Tong, A., and Chatterjee, P. Path planning for masked diffusion model sampling. ArXiv, abs/2502.03540, 2025. URL https://api. semanticscholar.org/CorpusID:276161145. 9 Rojas, K., Lin, J., Rasul, K., Schneider, A., Nevmyvaka, Y., Tao, M., and Deng, W. Improving reasoning for diffusion language models via group diffusion policy optimization. arXiv preprint arXiv:2510.08554, 2025. 3, 10 Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 9 Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. 9 Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015. 6, 10 Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Xiao, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. 3 Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. 9 Song, Y., Zhang, Z., Luo, C., Gao, P., Xia, F., Luo, H., Li, Z., Yang, Y., Yu, H., Qu, X., et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. 2, Wang, C., Rashidinejad, P., Su, D., Jiang, S., Wang, S., Zhao, S., Zhou, C., Shen, S. Z., Chen, F., Jaakkola, T., et al. Spg: Sandwiched policy gradient for masked diffusion language models. arXiv preprint arXiv:2510.09541, 2025a. 2, 3, 6, 10 Wang, J., Liu, R., Zhang, F., Li, X., and Zhou, G. Stabilizing knowledge, promoting reasoning: Dual-token constraints for rlvr. arXiv preprint arXiv:2507.15778, 2025b. 5 Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025c. 2, 5, 15, 18 Wang, Y., Yang, L., Li, B., Tian, Y., Shen, K., and Wang, M. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025d. Wu, C., Zhang, H., Xue, S., Diao, S., Fu, Y., Liu, Z., Molchanov, P., Luo, P., Han, S., and Xie, E. Fast-dllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328, 2025a. 2 Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., and Xie, E. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025b. 2, 5, 9 Xie, Z., Ye, J., Zheng, L., Gao, J., Dong, J., Wu, Z., Zhao, X., Gong, S., Jiang, X., Li, Z., et al. Dream-coder 7b: An open diffusion language model for code. arXiv preprint arXiv:2509.01142, 2025. 9 12 The Flexibility Trap Yang, L., Tian, Y., Li, B., Zhang, X., Shen, K., Tong, Y., and Wang, M. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. 2, 10 Ye, J., Gao, J., Gong, S., Zheng, L., Jiang, X., Li, Z., and Kong, L. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157, 2024. 2, 9 Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. 2, 5, 9, Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 2, 3, 4, 5 Zeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. Acecoder: Acing coder rl via automated test-case synthesis. arXiv preprint arXiv:2502.01718, 2025. 8, 14 Zhang, C., Neubig, G., and Yue, X. On the interplay of pre-training, mid-training, and rl on reasoning language models. arXiv preprint arXiv:2512.07783, 2025. 2, 4 Zhao, S., Gupta, D., Zheng, Q., and Grover, A. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. 2, 5, 8, 10, 14, 17 Zhu, F., Wang, R., Nie, S., Zhang, X., Wu, C., Hu, J., Zhou, J., Chen, J., Lin, Y., Wen, J.-R., et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. 2, 5, 10 13 The Flexibility Trap"
        },
        {
            "title": "Appendix",
            "content": "A. Experimental Details A.1. Data Preparation For mathematical reasoning tasks, we train on the official training split of each dataset, following the standard protocol in (Zhao et al., 2025; Ou et al., 2025). For code generation tasks, we adopt the AceCoder-87K dataset (Zeng et al., 2025). Following the data processing pipeline of DiffuCoder (Gong et al., 2025), we further select 21K challenging samples from AceCoder-87K that are equipped with verifiable unit tests. A.2. Training Configuration Our training setup largely follows (Huang et al., 2025b). Unlike (Huang et al., 2025b), we perform reinforcement learning directly on the dLLM without introducing training modules. During the rollout phase, we adopt exact autoregressive sampling, which allows direct log-probability computation under the standard GRPO formulation. We also reduce the total number of training steps, as JustGRPO exhibits fast and stable convergence. All experiments are conducted on 16 NVIDIA H100 GPUs. Training on GSM8K takes approximately three days. Detailed hyperparameters are reported in Table 2. Table 2: Training hyperparameters for JustGRPO. Hyperparameter Base Model RL Algorithm Optimizer Learning Rate LR Scheduler Weight Decay Optimizer Betas (β1, β2) Global Batch Size Group Size (G) Policy Update Steps Training Steps Max Completion Length Sampling Temperature KL Penalty Coefficient Value LLaDA 8B Instruct GRPO AdamW 5 106 Constant 0.0 (0.9, 0.999) 64 16 1 125 256 1.0 0.0 A.3. Reward Function Mathematical Reasoning Tasks. We employ binary reward scheme. Each completion receives reward of 1 if and only if the final answer is mathematically equivalent to the ground-truth solution, and 0 otherwise, following (Huang et al., 2025b). Code Generation Tasks. For code generation, the total reward is defined as weighted sum of correctness reward rcode and format reward rformat: = rcode + rformat. Correctness reward (rcode): Defined as the pass rate (ranging from 0 to 1) of the generated code on the provided unit tests. This term is only evaluated when rformat = 1. Format reward (rformat): heuristic reward designed to encourage syntactically valid outputs. 1.0: valid Markdown code block with correct Python syntax. 0.5: valid Markdown code block that contains syntax errors. 0.0: Failure to generate valid Markdown code block. 14 (7) B. More Analysis Results The Flexibility Trap To further validate the robustness of our findings in Section 3, we conduct extended analyses on the HumanEval benchmark using the LLaDA-Instruct model (Nie et al., 2025). We investigate the impact of temperature, sampling strategies, and decoding block sizes on reasoning performance. B.1. Temperature analysis. Figure 8: Pass@K comparison with different temperatures. As shown in Figure 8, the AR mode exhibits standard pattern: performance peaks at moderate temperatures (T 0.6) and degrades when > 1.0, whereas the Arbitrary Order mode attains its peak performance at higher temperatures. This observation aligns with the entropy degradation mechanism discussed in Section 3: the diffusion sampler inherently suppresses uncertainty at critical branching points, thereby requiring higher temperatures to induce sufficient exploration. Crucially, even under these optimized settings, the arbitrary order mode fails to match the reasoning potential of the AR mode. As illustrated in the Optimal comparison setting (bottom right of Figure 8), the best-performing AR configuration still outperforms the optimal arbitrary order baseline in scaling behavior. plausible explanation is that excessively high temperatures in arbitrary order decoding inject noise into tokens that require high determinism (e.g., code syntax or mathematical suffixes), leading to nonsensical outputs and degraded results (Wang et al., 2025c). B.2. Different sampling algorithms. We also experiment with different sampling algorithms, such as negative entropy sampling (Neg-Entropy) (Ye et al., 2025) and top-k margin sampling (Margin) (Kim et al., 2025) in Figure 9(a). The results show that although more sophisticated sampling algorithms can achieve better pass@k performance than default confidence-based sampling, they still cannot catch up with AR order. Meanwhile, these better pass@k algorithms also show slightly worse pass@1 performance compared to confidence-based sampling, making the overall pass@k curve closer to AR orders pass@k curve. To investigate this similarity, we calculate the per-problem accuracy correlation between different sampling algorithms and the AR mode (Figure 9(b)). We observe that algorithms with higher scaling potential (higher pass@128) consistently show stronger correlation with AR, with the most effective method (Neg-Entropy) achieving 15 The Flexibility Trap Figure 9: (a) Sampling algorithm comparison. (b) Correlation between different sampling algorithms and AR in per-problem accuracy. We compare different sampling algorithms in pass@k performance and correlation with AR in per-problem accuracy. correlation of 0.970. This suggests that sampling algorithms with better pass@k tend to behave more like AR in terms of task-level performance characteristics. B.3. Block size analysis. Figure 10: Pass@k comparison with different semi-autoregressive block sizes of arbitrary order (AO) generation. Smaller block sizes explicitly restrict the models order flexibility, leading to more AR-like behavior and better pass@k performance. We analyze the effect of the semi-autoregressive block size in Figure 10. The AR Order (which is effectively block size of 1) maintains clear advantage across all tested block sizes. Furthermore, we observe trend of improvement in pass@k as the block size decreases (from 128 to 8). Since smaller block sizes explicitly restrict the models order flexibility, this trend is aligned with our finding that less order flexibility leads to better reasoning potential. B.4. Entropy Comparison Results on More Forking Tokens To validate the robustness of the entropy degradation phenomenon observed in Section 3, we extended our analysis to wider range of logical connectors. We conducted experiments on comprehensive set of common logical connectors that typically serve as pivotal decision points in reasoning chains. The results, shown in Figure 11, demonstrate that the phenomenon is consistent across these diverse tokens. Similar to the primary findings, the AR order maintains higher average entropy at these forks, The Flexibility Trap Figure 11: Entropy comparison results on more forking tokens. indicating active reasoning and decision-making. Conversely, the Arbitrary Order consistently results in lower entropy, confirming the premature collapse of branching possibilities. The specific forking tokens evaluated in this extended experiment include: Therefore, Thus, So, Since, When, Given, However, Let, First, Then, Next, Finally, Now, Similarly, Calculate, Solving, Notice, Specifically, Follows, Because, But, Or, Consider, Also, Express, and Write. B.5. Training Efficiency Analysis Applying GRPO to dLLMs presents structural trade-off. Unlike autoregressive models that benefit from causal masking, dLLMs require independent likelihood evaluation at each position, inherently incurring additional computational overhead. While approximation-based methods (Zhao et al., 2025; Ou et al., 2025) bypass this cost by forgoing exact estimation, we demonstrate that adhering to exact likelihoods yields superior trade-off between accuracy and wall-clock time. Figure 12 compares JustGRPO against ESPO (Ou et al., 2025), representative approximation-based baseline. It can be observed that ESPO saturates early, effectively plateauing around 10 hours. In contrast, JustGRPO exhibits superior efficiency frontier: it surpasses the baselines peak accuracy in less time and maintains continuous scaling trend. Furthermore, motivated by the observation 17 The Flexibility Trap Figure 12: Training efficiency on GSM8K (Wall-clock Time vs. Accuracy). The approximationbased baseline (ESPO) suffers from early saturation. In contrast, JustGRPO (Ours) exhibits better accuracy-wall time trade-off despite the theoretical overhead of exact likelihood estimation. Moreover, Ours-Fast (gradient update on top-25% entropy tokens) illustrates the potential for further acceleration via simple engineering optimizations. Wall-clock time is measured on 16H100 GPUs. that reasoning is primarily steered by subset of high-entropy forking tokens (Wang et al., 2025c), we demonstrate that JustGRPO allows for further acceleration via simple heuristic: restricting gradient computation to the top-25% high-entropy tokens (Ours-Fast). This modification yields even faster convergence without compromising the final performance. We prioritize simplicity in the current implementation and leave further efficiency optimizations for future work."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "LeapLab, Tsinghua University",
        "NLPLab, Tsinghua University",
        "Tsinghua University"
    ]
}