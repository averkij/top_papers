{
    "paper_title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing",
    "authors": [
        "Quan Dao",
        "Xiaoxiao He",
        "Ligong Han",
        "Ngan Hoai Nguyen",
        "Amin Heyrani Nobar",
        "Faez Ahmed",
        "Han Zhang",
        "Viet Anh Nguyen",
        "Dimitris Metaxas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 4 8 9 1 0 . 9 0 5 2 : r Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing Quan Dao1 Ngan Hoai Nguyen3 Han Zhang5 1Rutgers University Xiaoxiao He1 Amin Heyrani Nobari4 Ligong Han2 Faez Ahmed Dimitris Metaxas1 3QualcommAI Research 5CUHK Viet Anh Nguyen6 2Red Hat AI Innovation 5ReveAI 4MIT"
        },
        {
            "title": "Abstract",
            "content": "Visual autoregressive models (VAR) have recently emerged as promising class of generative models, achieving performance comparable to diffusion models in textto-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as practical editing approach."
        },
        {
            "title": "Introduction",
            "content": "In the era of generative AI, diffusion models [19, 46] have emerged as dominant methods in image synthesis, outperforming GANs [14] in both unconditional and conditional generation tasks [9], notably in text-to-image generation [41]. This success has enabled diverse practical applications, including personalization [42, 54, 26], 3D generation [36, 58], and prompt-based image editing [3, 22, 33, 61, 18, 17], underscoring their increasing popularity and utility. 1 In contrast, autoregressive models, traditionally dominant in natural language processing, have only recently gained traction in visual synthesis. Recent models such as LlamaGen [47] and MagVit-v2 [62] have optimized image tokenization and transformer architectures, achieving performance competitive with diffusion models. Furthermore, MARS [16] integrates mixtures of experts to train large-scale text-to-image autoregressive models, whereas MAR [28] removes vector quantization by leveraging diffusioninspired training in continuous space. Despite achieving diffusion-level quality, these methods still incur significant inference costs due to dependence on output size, limiting scalability for highresolution generation. Addressing this issue, [50] proposed next-scale prediction to substantially reduce inference time without sacrificing performance, highlighting potential future advantages of autoregressive approaches. Additionally, [48] introduced the Hybrid VAR Transformer (HART) diffusion framework, combining visual autoregressive models (VAR) with lightweight diffusion refinement, achieving comparable results to pure diffusion methods but with reduced inference time. 1: Equally Contribution, : Corresponding Author Preprint. Under review. Figure 1: Qualitative performance of VARIN given diverse prompts. This model raises further research opportunities in downstream tasks, including personalization, prompt-based image editing, and text-to-3D synthesis within VAR-based frameworks. In this paper, we focus specifically on prompt-guided text-to-image editing within visual autoregressive models (VAR). Given source image and text prompt describing desired edits, the task is to modify the image according to the prompt while preserving unrelated details from the original. We observe that VAR generates the overall structure and layout at initial scales, progressively refining finer details at higher scales. As illustrated in Figure 2, editing primarily becomes necessary at middle scales, around levels 6 or 7. straightforward baseline is Regeneration, which preserves tokens at initial scales and regenerates remaining scales. However, this approach inadequately retains non-target image details. To address these limitations, we propose Visual Autoregressive Inverse Noise (VARIN), the first training-free editing method designed explicitly for VAR. Inspired conceptually by noise inversion techniques in diffusion models, VARIN identifies editable noise sets capable of perfectly reconstructing the source image, enabling precise editing. While continuous autoregressive models (e.g., Gaussian-based) allow straightforward inversion through inverse transformation flows [24], discrete VAR models like [50, 48] complicate inversion due to reliance on the non-invertible argmax sampling (Gumbel-max trick). The simplest pseudo-inverse, the one-hot argmax inversion [17], produces uncontrollable noise sets, limiting precise editing capabilities. Therefore, we propose Location-aware Argmax Inversion (LAI), novel pseudo-inverse function enhancing controllability and alignment with target prompts. LAI extracts inverse noises to reconstruct source images precisely and allows adjustable bias towards preserving source details, significantly outperforming basic regeneration. Our method demonstrates editing quality on par with more complex, optimization-based test-time tuning approaches such as Null-Text Inversion, without requiring additional retraining or intricate crossattention manipulations. These advanced methods remain complementary and could be combined in future work for further improvements. We summarize our contributions as follows: We introduce first editing technique for VAR named VARIN. Our VARIN is based on noise inversion technique to obtain set of editable noises and control these noises for editing image. We propose Location-aware Argmax Inversion (LAI) as pseudo-inverse of argmax inversion. This allows us to extract inverse noises that perfectly reconstruct the source image. Furthermore, we can control the bias information of source image in extracted inverse noises, leading to better results. We validate the effectiveness of VARIN through experiments on text-to-image editing tasks, demonstrating their ability to align with target prompts while preserving source image details. 2 Figure 2: Visualizations of each scale of the generation process of HART [48]. The features of cat (top) and the landscape (bottom) are only distinguishable above 6 scales."
        },
        {
            "title": "2 Related Work",
            "content": "Visual autoregressive models. Autoregressive models have significantly shaped NLP, particularly through their next-token prediction paradigm [55, 1, 51, 8, 59, 2, 49]. This same paradigm has also proven effective in visual generation: models such as VQVAE [53, 40], VQGAN [12, 27], DALL-E [38], LlamaGen [47], and MARS [16] tokenize images for next-token prediction. While these token-based methods can match diffusion models in image quality, their inference speed often suffers because the number of tokens grows with image resolution. Recent work shifts toward next-scale prediction, where models predict multiple size scales in parallel via residual quantization [27]. VAR [50] and HART [48] exemplify this approach by generating high-quality images in only few steps. Furthermore, VAR is closely related to ImageBART [11], which applies transformers to each denoising step in multinomial diffusion [20]; one can interpret VAR similarly, but using blurring-to-deblurring viewpoint. Diffusion editing. Diffusion-based text-to-image editing has gained popularity for its flexibility and controllability [31, 22, 35, 21]. Many works rely on large-scale pretraining [63, 4, 13, 43, 65] or fine-tuning approaches [15, 66, 10, 44], where either model weights or embeddings are optimized at test time to achieve editing. Null-text Inversion [33] further refines editing control by adjusting null embeddings to align the reconstruction path with the source image. Diffusion inversion. key challenge in diffusion-based editing is extracting an internal representation from the source image so that edits can be made without losing fidelity. Continuous models often invert via deterministic or stochastic processes [7, 45, 29, 30, 60, 22], while recent research addresses discrete diffusion [17] and masked generative models [6]. These methods confirm that diffusion inversion can guide image editing by reconstructing the original data from learned representation. Inspired by these, our work focuses on inverting visual autoregressive model to enable text-driven editing while preserving source image content."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this section, we firstly review about the visual autoregressive model in Section 3.1. Motivating from method SDEdit [31] for diffusion, latter Section 3.2 introduces simple editing method for VAR called Regeneration, where we simply initialize the VAR generative process by beginning token maps r1, r2, . . . , rt extracted from source image using VAR encoder and generate the rest of token maps rt+1, rt+2, . . . , rK following target text prompt ctgt. Although this technique produces fairly good final image that has the same structure as the source image and follows the target text prompt ctgt, they lose background details as the final token maps are completely generated without any constraint from the source image. 3 3.1 Visual Autoregressive Model VAR has Variational AutoEncoder (VAE) based on VQ-VAE with [C] = {1, . . . , C} of vocal size C. In training process, given an image R3HW , the VAR-VAE encoder EVAR will output the image into token maps, (r1, r2, . . . , rK) = EVAR(I), where each token maps rk has different resolution hk wk and these resolution increases with the scale k. The VAR-AutoRegressive model θ can now be considered as the next scale predictor, which models the following likelihood: pθ(r1, r2, . . . , rK) = (cid:89) k=1 pθ(rkr1, r2, . . . , rk1), (1) where rK [C]hwwk is the token map at scale k, and the sequence (r1, r2, . . . , rk1) is the prefix of rk. In inference time, we use VAR-AutoRegressive to predict the sequence r1, r2, . . . , rK iteratively. We then pass the sequence to the VAR-VAE decoder DVAR to construct the generative RGB images Igen = DVAR(r1, r2, . . . , rK). Unlike traditional autoregressive methods, which flatten the image following predefined scanning order then perform the next token prediction, the next scale prediction of VAR strictly follows the autoregressives behaviour. In VAR, each token map rk depends only on the previous token maps r<k. As consequence, VAR costs shorter inference time than diffusion and traditional next-token prediction autoregressive models since they only need to run model for only few scale prediction (K 14) instead of diffusion and vanilla autoregressive with thousands of model iterations. 3.2 Text-based Image Editing & Baseline Regeneration Text-based Image Editing: We consider the text-based editing problem for the text-to-image VAR model. Given source image Isrc R3HW and target prompt ctgt, we want to edit the source image Isrc to comply with the prompt ctgt. In our proposed VARIN in Section 4, we use inversion algorithm for editing. Therefore, we need to define corresponding source text csrc which algins with source image for noise extraction. The inverse noises are then used for editing process with target prompt ctgt as shown in Section 4.3. The image editing is evaluated based on two main criteria, which could be conflicting: the edited image should be aligned with the target prompt while still preserve the unedited part from the source image. Baseline Regeneration: Using the VAR encoder, we could obtain the following token maps (r1, r2, . . . , rK) = EVAR(Isrc). As illustrated in Figure 2, while the token map of the beginning scales builds up the structure and layout of images, the later token map adds more detailed information. For editing with diffusion model, SDEdit [31] starts denoising generative process from middle noise level with some guidance to perform editing, while keeping the overall structure of the source image. Motivating from SDEdit, we could pick scale index between 1 and K, then we fix the token maps for the beginning scales from r1, . . . , rs, and start to generate rs+1, . . . , rK conditioning on the target prompt ctgt. We name this intuitive editing method Regeneration, and outline it in Algorithm 4. By keeping the beginning scale token map of the source image, the editing output image Itgt could keep the overall structure and layout of the source image. However, it may fail miserably to preserve the fine-grained details in background that should not be edited, examples of failures are shown in Figure 4. To search for more fine-grained editing control, we approach the noise inversion technique and investigate how to control the inverse noise for editing in next section."
        },
        {
            "title": "Inverse Autoregressive Transformation and Editable Inverse Noise",
            "content": "We introduce our technique Visual AutoRegressive Inverse Noise (VARIN) that could preserve better the unedited part of the source image. Towards this goal, we discuss the process of noise inversion for an autoregressive model in Section 4.1. For discrete space, there is no closed-form solution for inversion due to argmax operator, we therefore propose pseudo-inverse function for the argmax operator to obtain inverse noises from source image in Section 4.2. Finally, we propose the inverse noise editing algorithm for text-based image editing. The overal pipeline is in Figure 3. 4 Figure 3: VARIN pipeline: the prefix token csrc + r<k is fed to transformer to get log probability pk. We then use pseudo inverse-argmax to find the inverse noise nk from ground truth label rk and logit pk. These noise set n1, n2, . . . , nK is later used for editing control. 4.1 Discrete Inverse Autoregressive Transformations Given continuous Gaussian autoregressive model pθ(xtx<t) and sequence of tokens {x1, x2, . . . , xT }, we could apply an inverse transformation in parallel to find the sequence of inverse noises that perfectly reconstruct the sequence [24]. To see this, note that under the Gaussian assumption of pθ(xtx<t), we have xt = µt + σt ϵt, where µt and σt > 0 are the conditional mean and standard deviation of xt given the history x<t. We can obtain ϵt by ϵt = xtµt . Computing the inverse noise under the Gaussian case could be implemented in parallel due to the independence of ϵt and ϵt over different time stamps = t. The above inversion only holds when the noise admits continuous density. Unfortunately, most of autoregressive generative models for vision including VAR are based on discrete tokens and model the sequence of token with multinominal distribution. Consequentially, the inversion could no longer be straightforwardly integrated into the VAR pipeline. σt We now propose pseudo-inversion for VAR models that extends the continuous noise inversion to the case of discrete tokens. The VAR models use multinomial distribution, where samples are drawn using the Gumbel-max trick. Gumbel distribution with mean 0 and scale 1 has continuous density exp (cid:0) + exp(z)(cid:1) for any value z. We consider the discrete inverse autoregressive transformation problem using the Gumbel-max trick as follows: pt pθ(r<t) Find Gumbel noise nt s.t. argmax(pt + nt) = rt, with (unnormalized) log probability pt RlC, nt is the Gumbel noise from Gumbel-max trick, and rt Rl is the ground truth label with is number of tokens. As shown in Equation (2), there is no perfect way to obtain the reverse Gumbel noise from label and predicted probability, as the Gumbel-max trick involves an argmax operator, which is apparently non-invertible. Therefore, in the next section, we propose two pseudoinverse functions for the argmax operator in Section 4.2. (2) 4.2 Pseudo-inverse Argmax Since (pt + nt) represents the Gumbel-perturbed logits, we need to choose the unormalized log probability qt such that qt = pt + nt and arg max(qt) = rt. Notably, to ensure the editing ability, we better need the nt to follow the below properties: Prop 1: nt needs to be likely sampled from standard Gumbel noise since in line 6 of Algorithm 3, we interpolate nt with standard Gumbel noise for preserving randomness of generative process. Prop 2: nt needs to preserve some bias information from source image. This allows the editing algorithm to preserve the unedited part from source image. Now, we discuss about how to choose pseudoinverse function for argmax operator. The easiest way is setting qt = LogOnehot(rt), and we call this Onehot Argmax Inversion (OAI). With OAI, we can 5 Algorithm 1 Location-aware Argmax Inversion Function (LAI) Input: tokens [C]l, log probability RlC, and information preserving term τ 1: rmask Onehot(r, num_classes = C) RlC Rl1 2: lmax Sum(rmask p, dim = 1) Rl1 3: qmax Gumbel(µ = lmax, β = 1) 4: GumbelTrunc(µ = p, β = 1, trunc = RlC. See Algorithm 5 5: (1 rmask) + qmax rmask RlC Output: qmax τ ) Algorithm 2 Visual Autoregressive Inverse Noise (VARIN) Input: Source image Isrc, text prompt ctgt Parameters: τ Parallel Autoregressive Inversion: 1: (r1, r2, . . . , rK) EVAR(Isrc) 2: for from 1 to do qt LAI(rt, pt) 3: argmax Algorithm 1 logits Pseudo-inverse nt qt pt Gumbel noise inversion 4: 5: end for Return: (n1, n2, . . . , nK) find the list of inverse noise for perfect reconstruction. However, when using inverse noise OAI for editing, it usually fails to control the editing process. The reason is that the qt is 0 at the label index and significantly negative at other indices. This results in nt = qt pt not likely being sampled from standard Gumbel noise distribution violating Prop 1 and nt will be highly biased by source image since qt is extremely biased by label of token. Therefore, in editing, it is hard to control the editing process with OAI. Furthermore, we notice that the OAI process relies only on ground-truth labels and omits the information of predicted logits pt from models. To remedy the above issues, we propose novel argmax inversion function called Location-aware Argmax Inversion (LAI). Since qt = pt + nt and nt is sampled from standard Gumbel noise, qt should be close to pt. LAI exploits this property, and it takes pt as the location of Gumbelmax sampling. This makes qt closer to pt, and nt is more likely sampled from standard Gumbel distribution which is then satisfied Prop 1. For the remaining discussion, we will omit subscript to avoid clutter notation. To ensure the argmax condition, for each token from 1 to l, the ground truth label is r[i], we sample q[i] so that argmax(q[i]) = r[i]. Firstly, we sample the value for q[i]r[i] using the Gumbel-max trick with the predicted location p[i]r[i]. Later, we sample other position q[i]=r[i] by truncated Gumbel-max trick with corresponding predicted location and truncated value q[i]r[i] τ . Please noted that τ is very important parameter to control the unedited part preservation and we shall discuss details of τ below. Hyperparameter τ in LAI: When τ = 0, the nt from LAI satisfies Prop 1 but it still fails to edit while preserving the background. We could explain this effect: For simplicity, we consider RC is vector. When λ = 0, the sampled from Algorithm 5 (Gumbel Trunc) is highly uncertain cause there exist some indices such that qj qi, where = argmaxkqk. Therefore, during editing, qedit = + (1 λ) + λ = + + (1 λ) (g n) = + (1 λ) (g n) = + ϵ. Since is highly sensitive, even small ϵ can cause change in the maximum index of qedit, such that argmaxkqedit = j. As result, preserving the unedited part becomes exceedingly difficult. Therefore, by setting τ , we could let qt retains bias from source image which provides useful information to nt and control the sensitivity of Algorithm 3 better. This is also satisfied Prop 2. The efficient implementation is shown in Algorithm 1. LAI always guarantees perfect reconstruction, the same as OAI, but qt is closer to pt, and nt is more like standard Gumbel, which satisfied both above noise properties. We use LAI in Line 3 of Algorithm 2. 4.3 Editing with VARIN Algorithm 3 Editing by VARIN Input: 1: (r1, r2, . . . , rK) EVAR(Isrc) 2: n1, n2, . . . , nK VARIN((r1, r2, . . . , rK), csrc) is staring scale 3: for from to do pt is log probability 4: 5: 6: 7: 8: end for 9: Itgt DVAR(r1, . . . , rs1, rs, . . . , rK) Output: Itgt pt pθ(r<t, ctgt) Gumbel(0, I) qt = pt + (1 λ) + λnt rt = argmax(qt) For editing using inversion, similar to Regeneration Editing, we first obtain the token maps for each scale using the VAR encoder. We then use Algorithm 2 to collect the list of inverse noise n1, n2, . . . , nK. For each t, we get the predicted log probability pt from autoregressive model pθ. Instead of sampling using the Gumbel-max trick given pt as in Algorithm 4, which uses new Gumbel noise g, we interpolate new 6 Table 1: We evaluate our method VARIN against discrete generative model such as DICE and EditAR. We can see that our method outperforms DICE in terms of editing part with better CLIP Similarity but slightly underperforms in terms of background preservation due to the T2I reconstruction ability. With EditAR, we are better in terms of background reconstruction. Compared to recent continuous diffusion, we also achieve on par performance. Method Structure Background Preservation CLIP Similarity Name Prompt-to-Prompt Negative Prompt PnP Inversion Null-text Inversion Pix2pix-zero MasaCtrl InfEdit InstructPix2Pix MGIE DDPM-Inversion T2I SD1.4 SD1.4 SD1.4 SD1.4 SD1.4 SD1.4 SD1.4 SD1.5 SD1.5 SD1.4 u n e DICE c EditAR Regeneration VARIN Paella LlamaGen HART HART Distance103 PSNR LPIPS103 MSE104 SSIM102 Whole Edited 69.43 16.17 11.65 13.44 61.68 28.38 14.22 107.43 67.41 22.12 11.34 39.43 25.56 11.46 17.87 26.21 27.22 27.03 20.44 22.17 27.52 16.69 21.20 22.66 27.29 21.32 20.45 26.54 208.80 69.01 54.55 60.67 172.22 106.62 47.98 271.33 142.25 67.66 52.90 117.15 106.50 54. 219.88 39.73 32.86 35.86 144.12 86.97 34.17 392.22 295.11 53.33 43.76 130.27 95.90 38.33 71.14 83.40 84.76 84.11 74.67 79.67 85.05 68.39 77.52 78.95 89.79 75.13 73.31 85.39 25.01 24.61 25.02 24.75 22.80 23.96 24.89 23.49 24.28 26.22 23.79 24.87 24.65 25. 22.44 21.87 22.10 21.86 20.54 21.16 22.03 22.20 21.79 23.02 21.23 21.87 21.13 21.49 noise with inverse noise nt by interpolation coefficients λ. The λ is hyperparameter for tuning the editing process."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we first provide details of the evaluation dataset for text-based image editing task in Section 5. Section 5.1 presents the reconstruction performance of our proposed method, VARIN + HART. Later, in Section 5.2, we conduct editing experiments to demonstrate that VARIN can effectively do text-based image editing. Due to limited space, ablation is put in appendix. Dataset: To conduct the experiment, we use the Prompt-based Image Editing Benchmark (PIEBench) [23] which is dataset created to evaluate text-to-image (T2I) editing methods. It includes 700 images in 9 different editing scenarios, making it useful evaluation protocol for testing how well these methods handle text-guided image edits. The dataset contains detailed annotations and variety of tasks for us to thoroughly test and fairly compare our method with other approaches. 5.1 Inversion Reconstruction Here we evaluate the reconstruction performance of our inversion method, VARIN. First, we use Algorithm 2 to generate set of inverse noises. These inverse noises are then used to reconstruct token maps at each scale. Finally, the final token maps are decoded back to images. Table 2: Inversion reconstruction performance among discrete generative model. This indicates the underperformance of HART compared to Paelle in terms of reconstruction. Method Inversion DICE VARIN Model Paella HART Metric PSNR LPIPS103 MSE104 SSIM102 30.91 26.00 39.81 70. 11.07 33.20 90.22 79.83 Evaluation Metrics. To measure the reconstruction, we adopt image similarity metric such as Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [64], Mean Squared Error (MSE), and Structural Similarity Index Measure (SSIM) [57] to compute the difference between source images and the reconstructed images. Results. In Table 2, we compares our inversion VARIN with DICE [17], as both operate on discrete distributions. While DICE is based on discrete diffusion model, our approach utilizes HART, discrete visual autoregressive model. Mathematically, discrete noise inversion for both DICE and VARIN should produce token maps identical to the input token maps. However, the recorded metrics reveal gap between the reconstructed images and the original source images. This discrepancy 7 Figure 4: Qualitative result of editing results between VARIN and baseline Regeneration. We should better editing capability and background preservation. arises because both Paella [39] and HART rely on vector quantization autoencoders, which inherently involve reconstruction loss due to compression. From Table 2, we observe that the VAR-VAE of HART underperforms compared to the VQ-VAE used in Paella. 5.2 Text-based Image Editing Performance This section demonstrates the effectiveness of our editing algorithm VARIN. For both Regeneration and VARIN, by default, we set the start scale of editing = 6 based on observation from Figure 2 and set the τ to be 18 in LAI. Furthermore, we propose linear scheduler for λ with respect to scale, where λ = 1 on scale and start reducing linearly to 0 at final scale 14. Evaluation Metrics. We evaluate the performance of our proposed editing methods on three main aspects: structural similarity, background preservation, and alignment between the edit prompt and the generated image. To assess structural similarity between the original and generated images, we apply the structure distance metric from [52]. For evaluating background preservation outside the editing mask, we use PSNR, LPIPS, MSE, and SSIM. Consistency between the edit prompt and the generated image is measured using the CLIP Similarity Score [37], computed for the entire image and specifically for the regions within the editing mask. These metrics offer holistic assessment of our inversion method, addressing methods ability to maintain structure, preserve background details, and ensure prompt-image consistency, as in [23]. Results. Looking at Table 1, we begin by fairly comparing our editing method, VARIN, with trainingand optimization-free approaches including Regeneration, DICE, and DDPM-Inversion, as VARIN similarly relies solely on an instant inversion technique. While VARIN outperforms DICE in editing effectiveness, it exhibits marginally lower preservation performance, likely due to HARTs weaker Figure 5: Fine-grained editing examples with VARIN using Switti [56]. VARIN successfully performs localized modifications such as changing facial expressions, object replacement, and hand gestures while preserving original image details. reconstruction capability compared to Paella (see Section 5.1). Methodologically, VARIN is based on next-scale autoregressive framework, whereas DICE adopts discrete diffusion approach. Since the editing quality in both methods is closely tied to the sampling process, it is worth noting that VARIN requires only 10 sampling steps, while diffusion-based methods like DICE demand more, resulting in slower inference. Empirically, VARIN achieves real-time editing at approximately 1 second per image, compared to 2 seconds for DICE. Against DDPM-Inversion and Regeneration (see Figure 4 and Figure 10 in Supplementary Materials for qualitative comparisons with Regeneration), VARIN provides better reconstruction fidelity and background retention, while also adhering to the target prompt more effectively. In terms of efficiency, VARIN is approximately 10 faster than DDPM-Inversion. Overall, among purely training-free editing methods, VARIN achieves competitive quantitative performance while substantially outperforming other diffusion based-approaches in editing speed. Secondly, within the category of autoregressive editing, we compare VARIN to the recently proposed EditAR model [34]. VARIN achieves superior performance across most evaluation metrics, with the exception of CLIP Edited Similarity. Unlike EditAR, which requires additional training akin to InstructPix2Pix [4], VARIN relies solely on an inversion-based approach using next-scale autoregressive prediction, and does not require any task-specific fine-tuning. In comparison to continuous diffusion-based editing techniques such as MasaCtrl [5], MGIE [13], InstructPix2Pix [4], Prompt-to-Prompt [18], and Pix2Pix-Zero [38], VARIN demonstrates superior background and structural preservation. It achieves better balance between editing precision and content fidelity. This balance is also comparable to that of more advanced editing methods including InfEdit, PnP Inversion, Null-text Inversion, and Negative Prompting [32]. Unlike these diffusionbased methods, VARIN enables real-time editing with an average inference time of approximately 1 second per image, leveraging instant inversion and next-scale autoregressive. In contrast, approaches such as Negative Prompting and Null-text Inversion often require time-consuming optimization during both inversion and editing stages. Other techniques like PnP and InfEdit depend on pretrained editing models (e.g., P2P) or attention manipulation, whereas VARIN is entirely training-free. While attention control and pretrained editing models offer promising directions, they are orthogonal to our approach and can be integrated with VARIN. We leave such extensions for future work. As demonstrated in Figure 1, VARIN effectively handles diverse prompts to add objects, change backgrounds, and alter image styles. These edits maintain alignment with the target prompts while preserving essential background details. Additionally, fine-grained editing results using the base model Switti [56] are presented in Figure 5, illustrating moderate local edits like closing eyes, turning heads, changing objects (e.g., book to iPad), and adjusting hand gestures or leg positions, though substantial pose or structural modifications remain challenging. In addition to the automated evaluations, we conducted user study to assess and compare the visualization quality and editing prompt agreement of DDPM-Inv, DICE, and VARIN methods (detailed in Table 6 within the supplementary materials)."
        },
        {
            "title": "6 Conclusion",
            "content": "We proposed VARIN, that enable text-guided image editing capabilities for the text-to-image VAR (HART). Through extensive experiments, we have shown that this method successfully perform 9 text-guided image editing while maintaining background preservation. These editing advancements extend HARTs capabilities beyond mere text-to-image generation, making it more versatile tool for real-world applications. Moving forward, promising directions for future research include exploring the application of VARIN to other traditional next-token autoregressive models or investigating attention control like [18] to further improve editing quality."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [3] M. Brack, F. Friedrich, K. Kornmeier, L. Tsaban, P. Schramowski, K. Kersting, and A. Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [4] T. Brooks, A. Holynski, and A. A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [5] M. Cao, X. Wang, Z. Qi, Y. Shan, X. Qie, and Y. Zheng. Masactrl: Tuning-free mutual selfattention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2256022570, October 2023. [6] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [7] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. [8] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [9] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [10] W. Dong, S. Xue, X. Duan, and S. Han. Prompt tuning inversion for text-driven image editing using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74307440, 2023. [11] P. Esser, R. Rombach, A. Blattmann, and B. Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. Advances in neural information processing systems, 34:35183532, 2021. [12] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [13] T.-J. Fu, W. Hu, X. Du, W. Y. Wang, Y. Yang, and Z. Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. [14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [15] L. Han, Y. Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73237334, 2023. [16] W. He, S. Fu, M. Liu, X. Wang, W. Xiao, F. Shu, Y. Wang, L. Zhang, Z. Yu, H. Li, et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. arXiv preprint arXiv:2407.07614, 2024. [17] X. He, L. Han, Q. Dao, S. Wen, M. Bai, D. Liu, H. Zhang, M. R. Min, F. Juefei-Xu, C. Tan, et al. Dice: Discrete inversion enabling controllable editing for multinomial diffusion and masked generative models. arXiv preprint arXiv:2410.08207, 2024. 11 [18] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-Or. Prompt-toprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [19] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [20] E. Hoogeboom, D. Nielsen, P. Jaini, P. Forré, and M. Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:1245412465, 2021. [21] Y. Huang, J. Huang, Y. Liu, M. Yan, J. Lv, J. Liu, W. Xiong, H. Zhang, S. Chen, and L. Cao. Diffusion model-based image editing: survey. arXiv preprint arXiv:2402.17525, 2024. [22] I. Huberman-Spiegelglas, V. Kulikov, and T. Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1246912478, 2024. [23] X. Ju, A. Zeng, Y. Bian, S. Liu, and Q. Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. [24] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved variational inference with inverse autoregressive flow. Advances in Neural Information Processing Systems, 29, 2016. [25] W. Kool, H. Van Hoof, and M. Welling. Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement. In International Conference on Machine Learning, pages 34993508. PMLR, 2019. [26] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y. Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023. [27] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [28] T. Li, Y. Tian, H. Li, M. Deng, and K. He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [29] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [30] X. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [31] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [32] D. Miyake, A. Iohara, Y. Saito, and T. Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023. [33] R. Mokady, A. Hertz, K. Aberman, Y. Pritch, and D. Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. [34] J. Mu, N. Vasconcelos, and X. Wang. Editar: Unified conditional generation with autoregressive models. arXiv preprint arXiv:2501.04699, 2025. [35] T.-T. Nguyen, D.-A. Nguyen, A. Tran, and C. Pham. Flexedit: Flexible and controllable diffusion-based object-centric image editing. arXiv preprint arXiv:2403.18605, 2024. [36] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. 12 [37] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [38] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [39] D. Rampas, P. Pernias, and M. Aubreville. novel sampling scheme for text-and imageconditional image synthesis in quantized latent spaces. arXiv preprint arXiv:2211.07292, 2022. [40] A. Razavi, A. Van den Oord, and O. Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [41] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [42] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [43] S. Sheynin, A. Polyak, U. Singer, Y. Kirstain, A. Zohar, O. Ashual, D. Parikh, and Y. Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [44] Y. Shi, C. Xue, J. H. Liew, J. Pan, H. Yan, W. Zhang, V. Y. Tan, and S. Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. [45] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [46] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [47] P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [48] H. Tang, Y. Wu, S. Yang, E. Xie, J. Chen, J. Chen, Z. Zhang, H. Cai, Y. Lu, and S. Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. [49] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [50] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [51] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [52] N. Tumanyan, M. Geyer, S. Bagon, and T. Dekel. Plug-and-play diffusion features for textdriven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. [53] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 13 [54] T. Van Le, H. Phung, T. H. Nguyen, Q. Dao, N. N. Tran, and A. Tran. Anti-dreambooth: Protecting users from personalized text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21162127, 2023. [55] A. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [56] A. Voronov, D. Kuznedelev, M. Khoroshikh, V. Khrulkov, and D. Baranchuk. Switti: Designing scale-wise transformers for text-to-image synthesis. 2024. [57] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [58] Z. Wang, C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. [59] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, et al. Bloom: 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. [60] C. H. Wu and F. De la Torre. Unifying diffusion models latent space, with applications to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559, 2022. [61] C. H. Wu and F. D. la Torre. latent space of stochastic diffusion models for zero-shot image editing and guidance. In ICCV, 2023. [62] L. Yu, J. Lezama, N. B. Gundavarapu, L. Versari, K. Sohn, D. Minnen, Y. Cheng, V. Birodkar, A. Gupta, X. Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [63] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [64] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [65] S. Zhang, X. Yang, Y. Feng, C. Qin, C.-C. Chen, N. Yu, Z. Chen, H. Wang, S. Savarese, S. Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036, 2024. [66] Z. Zhang, L. Han, A. Ghosh, D. N. Metaxas, and J. Ren. Sine: Single image editing with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60276037, 2023."
        },
        {
            "title": "A Appendix",
            "content": "In this supplementary material, we first present an ablation study of our proposed methods, Regeneration and VARIN. Subsequently, we discuss variation of VARIN editing, referred to as only target editing."
        },
        {
            "title": "B Regeneration Algorithm",
            "content": "Algorithm 4 Editing by Regeneration Input: Source image Isrc, text prompt ctgt 1: r1, r2, . . . , rK EVAR(Isrc) 2: for from to do 3: 4: end for 5: Itgt DVAR(r1, . . . , rs, rs, . . . , rK) Output: Itgt rt pθ(r<t, ctgt) is starting regeneration scale Sampling using target prompt and token maps of previous scale"
        },
        {
            "title": "C Ablation",
            "content": "In this section, we provide ablation details for our method VARIN and Regeneration. First, we examine the ablation study on the initial step of editing for the baseline Regeneration method. Finally, we do ablation on VARIN hyperparameter τ to indicate the importance of this hyperparameter in controlling how much information retains from the source images. C.1 Regeneration For the Regeneration method, we preserve the first few scales (from = 0 to = 6 or = 7) and generate the remaining scales using the target prompt. As shown in Table 3, increasing the starting editing scale leads to poorer performance on editing alignment metrics, such as structure distance and CLIP similarity, while improving performance on background preservation metrics. For qualitative results, refer to Figure 6. When = 0, Regeneration behaves as standard text-to-image generation process. However, when 9, the output image closely resembles the source image and, in some cases, fails to align with the provided target prompt (as illustrated in the first and second rows of Figure 6). On the other hand, with smaller scales 5, preserving the background from the original image becomes significantly challenging. Therefore, for editing tasks, the most effective scale to begin editing is within the range of = 6 to = 8. Regeneration Structure CLIP Similarity Background Preservation Begin Step = 6 = 7 = 8 Distance103 Whole Edited 23.11 22.57 21.13 26.83 26.08 24.65 42.85 33.70 25. PSNR LPIPS103 MSE104 SSIM102 19.21 20.49 20.45 166.03 137.00 106.50 171.18 133.17 95.90 68.93 70.79 73. Table 3: Ablation on beginning step to edit for method Regeneration. Similar to observation from Figure 6, as increases, the target alignment becomes worse while the background preservation is better. The recommended scale is 6, based on qualitative result. C.2 VARIN Observing the qualitative and quantitative results of the Regeneration technique, we select the beginning editing scale = 6. As mentioned in main paper, τ in Algorithm 1 also controls the retention of source information in the inverse noise nt. As τ increases, the output image preserves the background more effectively (refer to Figure 7). Without τ , controlling the editing process becomes challenging, as inverse noise contains greater uncertainty about the source information, making it 15 Figure 6: Ablation of Regeneration on beginning scale for editing s. We can see that as increases, the edited image are more like the source image. For small s, the edited image is too different from the source image. Therefore, the best for editing is around 6 to 8 more sensitive and less suitable for editing controllability. Since the hyperparameter τ is crucial for editing, we perform an ablation study on it in Table 4. Our findings indicate that τ values between 14 and 20 produce the best visual editing results. VARIN Structure CLIP Similarity Background Preservation τ 14 16 18 Distance103 Whole Edited 22.12 21.72 21.49 21.28 19.21 14.33 11.46 9.85 25.73 25.32 25.05 24.79 PSNR LPIPS103 MSE104 SSIM102 23.97 25.49 26.54 27. 74.23 60.87 54.04 50.28 68.49 48.97 38.33 31.78 82.26 84.34 85.39 85.96 Table 4: Ablation on value τ ."
        },
        {
            "title": "D Gumbel Truncation Sampling",
            "content": "For the Gumbel truncation algorithm [25], we provide the detailed algorithm in Algorithm 5. Algorithm 5 GumbelTrunc Input: location ϕ, threshold 1: Uniform(0, I) Return: ϕ log(exp(ϕ ) log u)"
        },
        {
            "title": "E Editing using Only Target Prompt",
            "content": "In this section, we discuss the only target prompt VARIN, variant of source-target VARIN in the main paper. In this editing algorithm, we only use target prompt ctgt for both noise extraction and editing. First, we extract set of inverse noises using the target prompt. This extracted noise set can then be used to perform the editing process, as shown in Algorithm 6. For this algorithm, qualitative results are provided in Figure 8, demonstrating that the source-target VARIN performs well on the 16 Figure 7: Qualitative result for ablation of τ for VARIN. editing task. It is worth noting that for only target VARIN, the effective τ value is lower, typically between 10 and 14. In Table 5, we provide the ablation on τ for only target VARIN editing method. Algorithm 6 Editing by target VARIN Regeneration: 1: r1, r2, . . . , rK EVAR(Itgt) 2: n1, n2, . . . , nK VARIN((r1, r2, . . . , rK), ctgt) 3: for from to do 4: 5: 6: 7: 8: end for 9: Itgt DVAR(r1, r2, . . . , rK) 10: Return Itgt. pt pθ(rtr<t, ctgt) Gumbel(0, I) qt = pt + (1 λ) + λ nt rt = argmax(qt) is the scale we start editing pt is log probability VARIN Structure CLIP Similarity Background Preservation τ 8 10 12 14 Distance103 Whole Edited 21.96 21.63 21.37 21.23 15.00 11.76 9.91 8.84 25.54 25.17 24.93 24.74 PSNR LPIPS103 MSE104 SSIM102 25.34 26.59 27.41 27.86 62.62 54.45 50.38 48.19 50.81 38.04 31.35 28.32 83.96 85.30 85.93 86.26 Table 5: Ablation on value τ of VARIN. As the τ increases, the edited image is more like the source image and preserve background better"
        },
        {
            "title": "F More Qualitative Comparison",
            "content": "See Figure 9, Figure 10, and Figure 12. 17 Figure 8: The first column is source image. The second column is target VARIN Algorithm 6, and the third column is source-target VARIN Algorithm 3. The green and red color texts are source and target prompt, correspondingly. Visualization quality Agreement to the editing prompt DDPM-Inv DICE VARIN 35.89% 52.35% 15.89% 48.24% 41.76% 64.71% Table 6: User Study conducting on 25 people and 10 images."
        },
        {
            "title": "G Limitation",
            "content": "Here we demonstrate one of the failure case that serves the limitation of our proposed method. As demonstrated in Figure 12, our method may fail in cases involving large pose or structural changes, or complex interactions that require such changes. These scenarios can challenge the models ability to preserve consistency and produce realistic edits. 18 Figure 9: Editing results on complex scene involves two objects. Figure 10: Extending VARIN to different architectures and base models. 19 Figure 11: Comparing VARIN with different methods. Figure 12: Failure cases: large movement and complex interaction between object"
        }
    ],
    "affiliations": [
        "MIT",
        "Qualcomm AI Research",
        "Red Hat AI Innovation",
        "ReveAI",
        "Rutgers University"
    ]
}