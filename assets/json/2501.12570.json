{
    "paper_title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
    "authors": [
        "Haotian Luo",
        "Li Shen",
        "Haiying He",
        "Yibo Wang",
        "Shiwei Liu",
        "Wei Li",
        "Naiqiang Tan",
        "Xiaochun Cao",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner"
        },
        {
            "title": "Start",
            "content": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning Haotian Luo 1 Li Shen 1 Haiying He 2 Yibo Wang 3 Shiwei Liu 4 Wei Li 5 Naiqiang Tan 5 Xiaochun Cao 1 Dacheng Tao"
        },
        {
            "title": "Abstract",
            "content": "Recently, long-thought reasoning LLMs, such as OpenAIs O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the models problem-solving abilities and has achieved promising results. However, longthought reasoning process leads to substantial increase in inference time. pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLMs baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner 5 2 0 2 2 2 ] . [ 1 0 7 5 2 1 . 1 0 5 2 : r 1. Introduction Reasoning represents fundamental capability of large language models (LLMs), serving as cornerstone in the advancement of artificial intelligence research (Huang & Chang, 2023). Recently OpenAIs O1(OpenAI, 2024) 1Shenzhen Campus of Sun Yat-sen University 2China Agriculture University 3Tsinghua University 4University of Oxford 5Didichuxing Co. Ltd 6Nanyang Technological University. Correspondence to: Li Shen <mathshenli@gmail.com>. 1 have introduced long-thought reasoning models that mimic human-like problem-solving processes. In addition to O1, researchers have also developed models that inference with similar long-thought reasoning pattern, such as DeepseekR1 (DeepSeek, 2024), QwQ (Qwen, 2024) and Marcoo1(Zhao et al., 2024). These models leverage long chainof-thought framework, enabling them to tackle complex problems by iteratively identifying and correcting errors, simplifying intricate steps, and exploring alternative strategies when initial approaches prove inadequate. Furthermore, Mulberry (Yao et al., 2024) has demonstrated that O1-Like reasoning can also play significant role in multimodal reasoning. This reasoning paradigm significantly enhances the problem-solving capabilities of large language models (LLMs) by allowing them to approach complex tasks in more systematic and human-like manner, demonstrating an ability to handle problems that would otherwise be challenging or intractable for conventional LLMs. While long-thought reasoning enhances reasoning capabilities and improves accuracy, it is accompanied by longer output sequences, which result in increased computational overhead. critical challenge lies in developing mechanisms that enable LLMs to dynamically adjust the length and complexity of their reasoning processes in accordance with the difficulty of the problems they encounter. In this paper, we first revisit the long-thought reasoning processes. we observe that the reasoning processes in longthought reasoning LLMs often exhibit significant redundancies, which leads to inefficient use of computational resources. This inefficiency not only increases inference costs but also highlights fundamental limitation in the models ability to adapt their reasoning depth to suit the demands of diverse tasks. Building on this analysis, we formulate an optimization objective aimed at minimizing reasoning overhead while maintaining accuracy as constraint. To achieve this objective, we propose RL-based loss function. Our experiments are conducted using opensource long-thought reasoning LLMs, and we compare our approach against several competing methods like SFT and DPO (Rafailov et al., 2024). Through extensive experiments, we demonstrate the efficiency of our proposed methods. Additionally, we perform further studies on the influence of hyperparameters and dataset diffucilty on our approach, in O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning order to gain deeper insights into the characteristics and behavior of this novel framework. extending the length of the solution generated during reasoning can also significantly enhance the models performance. In conclusion, our contributions can be outlined as follows: 2.3. LLM Alignment We identify and experimentally validate the existence of length disharmony in the reasoning process of long thought models, which leads to redundant inference overhead. We formulate an optimization problem aimed at improving model inference efficiency while maintaining accuracy, and based on this, we propose LengthHarmonizing Fine-Tuning (O1-Pruner) approach. Through extensive experiments, we demonstrate the effectiveness of O1-Pruner and conduct in-depth analyses, to provide insights and inspiration for future research in this area. 2. Related Work 2.1. Chain of Thought Chain-of-Thought (CoT) (Wei et al., 2023) prompting has emerged as powerful technique for improving the reasoning capabilities of large language models (LLMs). By encouraging the model to solve complex problems step by step, CoT significantly enhances the accuracy and interpretability of its outputs. CoT is particularly effective for tasks that requiring multiple solving steps, such as mathematical problem-solving and logical reasoning. Beyond the basic CoT paradigm, many innovative frameworks like Tree of Thought (ToT) (Yao et al., 2023) and Graph of Thought (GoT) (Besta et al., 2024) expand upon the CoT architecture by investigating various reasoning trajectories or integrating structures based on networks. Besides, Chain-of-thought reasoning also furnishes discernible traces of logical progression, enabling human to comprehend the models decision-making pathway, thereby rendering the reasoning process both transparent and credible. 2.2. Inference-time Scaling Inference-time scaling refers to the ability of large language models (LLMs) to improve their outputs by utilizing additional computation during inference time. Recent studies (Snell et al., 2024) have explored how scaling inferencetime computation can enhance the performance of LLMs on challenging prompts. This approach draws parallels to human reasoning, where additional cognitive effort is often allocated to complex problems. In addition to increasing the number of candidate solutions or searching different steps, OpenAIs O1 inference (OpenAI, 2024) demonstrates that LLM alignment (Shen et al., 2023) constitutes technical process aimed at guaranteeing that the responses generated by large language models are not only precise and logically consistent but also secure, morally sound, and aligned with the expectations of both developers and users. Ensuring that these expansive language models are in harmony with human preference is crucial for leveraging their immense capabilities in manner that is both reliable and conscientious. Common methodologies employed in LLM alignment include Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), and Direct Preference Optimization (DPO), among others. The discourse on long thought reasoning optimization presented in this paper can be regarded as an extended setting of LLM alignment, where human preferences are inclined towards shorter outputs (faster inference) and enhanced reasoning accuracy. 3. Length Disharmony in Long Thought"
        },
        {
            "title": "Reasoning",
            "content": "We employ the term Length Disharmony to characterize the phenomenon of inefficiency in the reasoning process of long-thought reasoning, when the model generates responses of varying lengths, among which the shorter responses possess sufficiently high accuracy, thereby rendering the longer responses superfluous expenditure of computational resources. Besides, due to the quadratic complexity of the Transformer architecture, this will significantly leads to an increase in inference time. In this section, we have devised simple experiment to substantiate the disharmony inherent in long thought reasoning. We randomly selected 64 problems from the MATH (Hendrycks et al., 2021) test set (For QwQ-32B, we filtered out hard samples first). For each problem, we generated 512 solutions using both the Marco-o1 and the QwQ-32B models through Top-P sampling (Holtzman et al., 2020). For each problem, we categorize all candidate solutions into 4 intervals based on their lengths and subsequently compute Model Interval 1 Interval 2 Interval Interval 4 Marco QwQ 81.1 44.9 80.2 49.9 78.8 45.9 75.3 45. Table 1. Accuracy-Length Relationship at Distribution Level. larger interval number indicates longer solution length. For both models, the average accuracy is higher when the solution length is short. 2 O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning Figure 1. Accuracy-Length Relationship at Instance level. The relationship between length and accuracy varies significantly across problems, with peak accuracy occurring at short, medium, or long intervals. Notably, high accuracy often persists in shorter intervals. the accuracy rate for each interval. 4.1. Problem Setup From this, we can ascertain the relationship between accuracy and length at the instance level, which is shown in Fig 1. It is evident that there exists markedly inconsistent relationship between length and accuracy across different problems. The highest accuracy may manifest within the shortest, intermediate, or longest length intervals. Particularly, we observe that relatively high accuracy is maintained even within the intervals of shorter lengths. Furthermore, by calculating the average accuracy across all problems within different intervals, we have derived the relationship between accuracy and length at the distribution level, which is shown in Table 1. At the distribution level, we observe that the average accuracy is still high when the response length is short. Therefore, we can conclude that long-thought models exhibit characteristic of length disharmony during reasoning, which leads to redundant computational overhead in the inference phase. This reasoning redundancy can be mitigated, as high accuracy is still maintained even at shorter lengths. From this perspective, we propose Length-Harmonizing Fine-Tuning to optimize long-thought reasoning, enabling it to maintain high accuracy while reducing inference redundancy. 4. Methodology In this section, we elaborate on our proposed LengthHarmonizing Fine-Tuning in detail and provide simple and intuitive mathematical analysis elucidating how our method works for optimize long thought of reasoning. We consider Large Language Model (LLM) parameterized by θ and denoted as πθ. In the context of math problem solving, the LLM accepts sequence = [x1, . . . , xn], commonly termed as the problem, and then generate corresponding solution = [y1, . . . , ym]. Hence, the solution is construed as sample drawn from the conditional probability distribution πθ(x). The conditional probability distribution πθ(yx) can be decomposed as follows: πθ(yx) = (cid:89) j=1 πθ(yjx, y<j), (1) Firstly, we review the process of supervised fine-tuning (SFT). SFT is the primary method to adapt pre-trained LLM for downstream tasks with relatively smaller supervised dataset of labeled examples compared to the data of pre-training stage. In this paper, we focus on the task of mathematic problem solving where the problem-solution pairs denoted as (x, y), are drawn from specified SFT dataset D. Thus the training objective of SFT under this setting can be formulated as: (cid:104) E(x,y)D max πθ log πθ(y x) (cid:105) (2) 4.2. Length-Harmonizing Fine-Tuning To start with, lets assume that πθ is LLM that can solve math problems with long thought with redundancy and disharmony. we hypothesize that the reasoning paths represented by output thought of language model πθ contain redundancies and lack proper coordination. To address 3 O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning Figure 2. Length-Harmonizing Fine-Tuning. During the training phase, for each problem, we sample multiple times from the reference model. Subsequently, we sample from the model to be optimized and compute the reward based on the reference samples, followed by RL-style fine-tuning. During the inference phase, the model optimized through O1-Pruner demonstrates significant improvement in inference speed, along with noticeable enhancement in accuracy. this, we propose an optimization objective that ensures no degradation in accuracy while tackling the issue from two perspectives. First, at the overall level, we aim to shorten the reasoning paths. Second, we encourage the model to output shorter answers for simpler problems, while for more complex problems, we guide the model to learn the correct reasoning paths, which, according to the inference scaling law, typically involve longer reasoning sequences. Consider problem x. We define L(y) to be the length (counted by token) of solution y. Considering reference model πref , we hope to to reduce the solution length of the policy model relative to that of the reference model, which can be formulated as: max ExD (cid:20) Eyπθ(yx),yπref (yx) (cid:21) 1 L(y) L(y) (3) We subtract constant 1 from the optimization objective to ensure that the initial expected value of the optimization is zero. We then define an accuracy function A(x, y, answer), which takes the problem, solution, and the real answer as inputs, and returns 0 or 1 to indicate whether the solution is incorrect or correct. For the sake of simplicity in the notation, we omit the real answer, denoting the function as A(x, y). We aim to ensure that the models accuracy does not decrease, or even improves, during the process of optimizing for length. Thus, we derive the following constraint condition: ExD,yπθ(yx)A(x, y) ExD,yπref (yx)A(x, y) (4) Therefore, we can establish our optimization objective as: max ExD (cid:20) Eyπθ(yx),yπref (yx) (cid:21) 1 L(y) L(y) s.t.ExD,yπθ(yx)A(x, y) ExD,yπref (yx)A(x, y) (5) To solve this constrained optimization problem, we employ the Lagrange multipliers, incorporating constraint into the objective function as penalty term. Specifically, the constraint on accuracy is added to the objective with penalty weight λ 0: max ExD,yπθ(yx),yπref (yx) L(y) L(y) 1 +λ(A(x, y) A(x, y)) (6) By reorganizing the terms related with reference model 4 O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning πref , we have: 4.3. Understanding the Loss Function max ExD,yπθ(yx) Eyπref (yx)L(y) L(y) 1+ λ(A(x, y) Eyπref (yx)A(x, y)) (7) In practice, we approximate the expectation terms related with πref by sampling. For each x, we sample for times from πref (x) and calculate the mean value: Lref (x) = (cid:88) L(y i) iπref (x) Aref (x) = (cid:88) A(x, i) iπref (x) (8) (9) So that our objective can be re-written as: Lref (x) L(y) max ExD,yπθ(yx) 1 +λ(A(x, y) Aref (x)) (10) Since both L(y) and A(x, y) are not differentiable, we solving this optimization with policy gradient approach, which is shown to have strong performance despite its simplicity. Furthermore, it is worth noting that during the optimization process, frequent sampling from the current distribution πθ is required during training, which significantly increases the complexity of the training procedure. Considering that off-policy training can bring remarkable effectiveness with pre-collected data, we adopt an off-policy training approach by directly sampling from the πref instead of πθ. Besides, since our reward is derived by assessing the merit of sample within the distribution relative to the expected outcome, our reward can be regarded as an approximate advantage function. Consequently, we employ PPO-style loss (Schulman et al., 2017) to optimize the objective function, which helps for our off-policy training strategy. Defining Lref (x) the Length-Harmonizing Reward RLH (x, y) = L(y) 1 + λ(A(x, y) Aref (x)), the loss function of off-policyversion Length-Harmonizing Fine-Tuning is: LLH(θ) = ExD,yπref (yx)[min(r(θ)RLH (x, y), clip(r(θ), 1 ϵ, 1 + ϵ)RLH (x, y))] (11) , where r(θ) = πθ(yx) PPO. πref (yx) . clip() is the clipping function of This allows us to prepare the required data at the beginning of training, thereby greatly simplifying the training workflow. Our experiments show that this off-policy approach still enables our method to achieve outstanding performance, significantly surpassing other baselines. L(x,πref ) To intuitively understand how our loss function works, we begin by analyzing the RLH term. Evidently, RLH comprises two distinct components, namely the length reL(y) 1 and the accuracy reward term ward term λ(A(x, y) A(x, πref )). Obviously, the length reward term will reward shorter outputs. When the sequence length are consistent with expected output length of reference model, the length reward is 0; however, when the output is longer, the length reward becomes negative. The accuracy reward term is essential for balancing length and accuracy. For problem with relatively high accuracy expectation, solving it correctly does not yield significant accuracy reward. As result, the model tends to explore shorter solutions. For more challenging problems, solving them correctly yields higher accuracy reward, indicating that we do not want the model to prioritize shortening the output. Instead, we aim for the model to focus on generating correct solution. On this basis, if the correct solution is relatively short, the model will receive an additional length reward. 5. Experiments 5.1. Experiment Setup 5.1.1. MODEL The long thought models we chosen for our experiment are Marco-o1-7B and QwQ-32B-Preview, which demonstrate excellent performance on wide range of math problem-solving tasks. For Marco-o1-7B, we utilize fullparameter fine-tuning; however, for the larger-scale QwQ32B-Preview, our computational resources are not able to support full-parameter training. As result, we adopt Parameter-Efficient Fine-Tuning (Han et al., 2024). After evaluating both LoRA (Hu et al., 2021) and Freeze FineTune, we observed that Freeze Fine-Tune yields much better performance. Therefore, we selected this fine-tuning approach for our experiments. 5.1.2. DATASET The dataset used for training is MATH. It comprises approximately 10k math problem of high school level accompanied with both ground truth solution and ground truth answer. Since the ground truth solution is not need for our experiment, we only use the problem-answer pairs. For training, we selected 5,000 problems from the MATH Trainset. For Marco-o1-7B, we generated 16 solutions for each problem; for QwQ-32B-Preview, we generated 12 solutions for each problem. The dataset utilized for testing encompasses the test sets of MATH, GSM8k (Cobbe et al., 2021), and GaoKao (mathematical) (Zhang et al., 2024), comprising diverse range of 5 O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning Model Marco-o1-7B (full fine-tune) Baseline Fast-solving Prompt SFT DPO O1-Pruner QwQ-32B-Preview (freeze fine-tune last 48 layers) Baseline Fast-solving Prompt SFT DPO O1-Pruner MATH GSM8K GaoKao AVERAGE Acc Length AES Acc Length AES Acc Length AES Acc Length AES 73.8 71.0 73.6 71.8 77.5 90.6 90.2 90.4 91.7 91.0 1156 1113 1076 761 657 2191 1763 2031 1999 1385 0 0.15 0.08 0.42 0.58 0 0.21 0.08 0.12 0. 89.2 81.7 89.9 88.6 91.4 95.1 95.8 95.7 95.3 96.5 530 447 497 410 343 777 561 717 704 534 0 0.41 0.09 0.25 0.43 0 0.30 0.10 0.10 0. 57.1 57.1 56.3 56.6 61.6 79.0 78.4 79.5 79.7 80.3 1112 1062 1066 780 664 2183 1911 2112 2021 1446 0 0.04 0.08 0.32 0.64 0 0.15 0.05 0.10 0. 73.4 69.9 73.3 72.3 76.8 88.2 88.1 88.5 88.9 89.3 932 874 880 650 554 1717 1411 1620 1575 1121 0 0.20 0.08 0.33 0.55 0 0.22 0.08 0.11 0. Table 2. Main Experiment Results. We present the performance of two selected models optimized through different methods across three mathematical reasoning datasets. It can be observed that the models trained with O1-Pruner achieve the best trade off between accuracy and length in comparison with other approaches. mathematical problems with varying levels of difficulty. 5.1.3. COMPETING METHODS To validate the superiority of our method for long thought reasoning optimization tasks, we have selected the following comparative methods: Fast-Solving Prompt The Fast-Solving Prompt is prompting technique wherein we instruct the model within the prompt to solve the given problem as swiftly as possible, aiming to achieve the desired reduction in reasoning length. SFT For the Supervised Fine-Tuning (SFT) method, we curated the training dataset by selecting the two shortest correct solutions for each problem, ensuring that the model is exposed to examples that embody both accuracy and conciseness. These solutions were then used to train the model following the standard SFT pipeline. DPO For the implementation DPO, we meticulously selected two of the shortest correct solutions to serve as the chosen samples, which exemplify efficiency and precision in problem-solving. Conversely, to represent the reject sample, we opted for the longest solution available. 5.1.4. EVALUATION METRIC In our experiments, we employed average accuracy, average length and Accuracy-Efficiency Score (AES) as key metrics. portion of problems for which the models generated solution is correct. higher accuracy indicates better problem-solving capability. Length Length denotes the number of tokens in the generated solution. It serves as proxy for the computational cost of generating solutions, where shorter length implies greater efficiency. AES We define novel metric called AccuracyEfficiency Score (AES), to evaluate the trade off between improving accuracy and reducing computational cost. It is calculated by weighting and summing the models solution length and accuracy. Defining Length = LengthbaselineLengthmodel and Acc = AccmodelAccbaseline Accbaseline Lengthbaseline , the AES is calculated by: (cid:40) AES = α Length + β Acc , α Length γ Acc , if Acc 0 if Acc < 0 where α > 0, β > 0, and γ > 0. AES evaluates the trade-off between improving accuracy and reducing computational cost. And we emphasize the penalization of accuracy degradation by setting γ > β. We set the default values as α = 1, β = 3, γ = 5. By analyzing these three metrics, we can assess whether the model achieves desirable balance between reasoning accuracy and reasoning length. 5.2. Results Accuracy Accuracy reflects whether the model correctly solves the problem. It is measured as the proThe experimental results, summarized in Table 2, demonstrate the performance of various methods across different 6 O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning Figure 3. Comparison of inference time-cost on MATH among different models and methods. O1-Pruner achieves the shortest inference times (slightly over 1 minute for Marco-o1-7B and 4 minutes for QwQ-32B-Preview), demonstrating its effectiveness in accelerating long-thought model inference for both small and large long thought models. evaluation metrics. The proposed O1-Pruner consistently achieves superior performance in balancing reasoning accuracy and efficiency compared to baseline and competing methods. Notably, it exhibits the best trade-off between accuracy and reasoning length across all datasets, as further supported by its significantly higher Accuracy-Efficiency Score (AES) values. Across both models, Marco-o1-7B and QwQ-32B-Preview, O1-Pruner outperforms other methods in average length of generated solutions, with noticeable improvement on accuracy. For instance, in the Marco-o1-7B experiments, O1-Pruner achieves an average accuracy of 76.8%, accompanied by 40.5% reduction in solution length compared to the baseline. Similarly, for QwQ-32B-Preview, O1-Pruner yields an average accuracy of 89.3%, with 34.7% reduction in solution length. These improvements highlight the robustness of O1-Pruner in enhancing computational efficiency without sacrificing accuracy. The Fast-Solving Prompt method, while achieving moderate reduction in solution length, compromises accuracy in most cases. This trade-off is evident from its lower AES values compared to O1-Pruner, indicating that the reduction in reasoning length often comes at the cost of problem-solving performance. On the other hand, SFT provides better balance than the Fast-Solving Prompt, but its improvements in reasoning length remain marginal, with limited gains in AES. The DPO method achieves reasonable balance between accuracy and length, but it falls short of the performance achieved by O1-Pruner. Besides, the average accuracy decreases notably on Marco-o1-7B. 5.3. Inference Time-Cost In this subsection, we take the MATH test set as an example to explore the time overhead during the model inference phase. We utilize one A800 GPU and the VLLM (Kwon et al., 2023) library for inference, recording the average inference time. For the Marco-o1 model, we employ one A800 GPU, while for the QwQ-32B-Preview model, we use four A800 GPUs. As illustrated in Fig. 3, the inference time results reveal notable differences across methods and models: For the Marco-o1-7B model, the Baseline approach demonstrates an inference time of approximately 2 minutes, while the Fast-Solving Prompt and SFT methods achieve slightly shorter times. Both the DPO and O1-Pruner methods exhibit significantly reduced inference times, with O1-Pruner achieving the shortest duration, slightly exceeding 1 minute. For the larger model QwQ-32B-Preview, the overall inference time is considerably higher. The Baseline approach records the longest inference time, approaching 6 minutes, while the DPO and SFT methods achieve slightly shorter durations. Notably, the Fast-Solving Prompt reduces the inference time to around 5 minutes, likely due to the strong instruction-following capabilities of large models. Once again, O1-Pruner demonstrates the shortest duration, achieving an inference time of approximately 4 minutes. These results demonstrate that O1-Pruner can effectively accelerate the inference of long-thought models and is applicable to both smaller and larger language models. In summary, O1-Pruner represents significant advancement in optimizing long-thought reasoning for math problem-solving tasks, achieving the best balance between accuracy and efficiency while minimizing computational overhead. O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning λ"
        },
        {
            "title": "Accavg Lengthavg AESavg",
            "content": "Marco-o1-7B 0 1 2 5 74.8 76.0 76.8 76.3 527 532 554 656 0.49 0.54 0.55 0.45 Table 3. Ablation experiments on λ. Overall, the models accuracy and solution length increase with the penalty coefficient λ. λ = 2 achieves an optimal balance between accuracy and efficiency. 6. Further Evaluation 6.1. Ablations In this subsection, we conduct an ablation study on the accuracy constraint coefficient λ. We select several different values of λ (λ = 0, 1, 2, 5) and evaluate the model accordingly. For the sake of brevity, we only report the average metrics across three datasets. It can be observed that, overall, the models accuracy increases as the penalty coefficient lambda rises, while the required inference length also grows. In our experiments, for Marco-o1-7b, setting λ = 2 achieves favorable trade-off between accuracy and efficiency. Figure 4. Performance on MATH Test-set When Trained on Problems of Different Difficulty Levels. Models trained on more challenging datasets tend to generate longer solutions, while learning to solve harder problems enhances model accuracy. In contrast, for less challenging datasets, shorter solutions are produced without corresponding accuracy improvement. levels. Due to limited computational resources, we exclusively selected Marco-o1 for experimentation. Utilizing the data constructed from the MATH dataset as mentioned in prior experiments (comprising 5k problems * 16 solutions), we partition the dataset into three subsets of differing difficulty based on models average accuracy on them. In Fig 4, We observe that models trained on more challenging datasets tend to generate longer solutions, as these datasets typically contain problems requiring more complex solutions. At the same time, by learning the correct solutions of harder problems, the models improve their problemsolving capabilities and ultimately achieve higher accuracy. In contrast, for the least challenging datasets, although the generated solution lengths are reduced, there is no improvement in accuracy. These experimental results suggest that while our approach demonstrates significant effectiveness in optimizing long-thought reasoning, it remains highly influenced by the nature of the training data. 7. Conclusion In this paper, we conducted simple experiments to validate the phenomenon of length disharmony in long-thought models during reasoning, which leads to redundant computational overhead in the inference phase. To address this issue, we formulated it as an optimization problem and proposed Length Harmonizing Fine-Tuning (O1-Pruner) as solution to optimize the model. Extensive experiments demonstrate that O1-Pruner significantly reduces the length of the solutions generated by the model while achieves modest improvement in accuracy, thereby substantially enabling more efficient reasoning. Additionally, we performed an in-depth analysis, including experiments on key hyperparameter and datasets of varying difficulty, to better understand the characteristics of O1-Pruner."
        },
        {
            "title": "References",
            "content": "Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., and Hoefler, T. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1768217690, March 2024. doi: 10.1609/aaai.v38i16. URL http://dx.doi.org/10.1609/ 29720. aaai.v38i16.29720. ISSN 2159-5399. 6.2. Performance Under Dataset of Different Difficulty Levels In this subsection, we investigate the performance and characteristics of O1-Pruner across datasets of varying difficulty Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. 8 O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning DeepSeek. Deepseek-r1-lite-preview: Unleashing supercharged reasoning power. https://api-docs. deepseek.com/news/news1120, 2024. Accessed: 2024-12-29. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Han, Z., Gao, C., Liu, J., Zhang, J., and Zhang, S. Q. Parameter-efficient fine-tuning for large models: comprehensive survey, 2024. URL https://arxiv. org/abs/2403.14608. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/ 1707.06347. Shen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., Wu, X., Liu, Y., and Xiong, D. Large language model alignment: survey, 2023. URL https://arxiv. org/abs/2309.15025. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/ 2201.11903. Yao, H., Huang, J., Wu, W., Zhang, J., Wang, Y., Liu, S., Wang, Y., Song, Y., Feng, H., Shen, L., et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv.org/abs/2305.10601. Zhang, X., Li, C., Zong, Y., Ying, Z., He, L., and Qiu, X. Evaluating the performance of large language models on gaokao benchmark, 2024. URL https://arxiv. org/abs/2305.12474. Zhao, Y., Yin, H., Zeng, B., Wang, H., Shi, T., Lyu, C., Wang, L., Luo, W., and Zhang, K. Marco-o1: Towards open reasoning models for open-ended solutions, 2024. URL https://arxiv.org/abs/2411.14405. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration, 2020. URL https://arxiv.org/abs/1904.09751. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. org/abs/2106.09685. Huang, J. and Chang, K. C.-C. Towards reasoning in large language models: survey. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 10491065, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl. 67. URL https://aclanthology.org/2023. findings-acl.67/. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention, 2023. URL https:// arxiv.org/abs/2309.06180. OpenAI. llms. learning-to-reason-with-llms/, [Accessed 19-09-2024]. with Learning https://openai.com/index/ 2024. reason to Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm. github.io/blog/qwq-32b-preview/."
        }
    ],
    "affiliations": [
        "China Agriculture University",
        "Didichuxing Co. Ltd",
        "Nanyang Technological University",
        "Shenzhen Campus of Sun Yat-sen University",
        "Tsinghua University",
        "University of Oxford"
    ]
}