{
    "paper_title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
    "authors": [
        "Bartosz Cywi≈Ñski",
        "Kamil Deja"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron."
        },
        {
            "title": "Start",
            "content": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Bartosz Cywi nski 1 Kamil Deja"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUrons state-of-the-art performance. Moreover, we show that with single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https: //github.com/cywinski/SAeUron. 5 2 0 2 1 3 ] . [ 2 2 5 0 8 1 . 1 0 5 2 : r 1. Introduction Diffusion models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) have revolutionized generative modeling, enabling the creation of highly realistic images. Despite their success, these models can inadvertently generate undesirable and harmful content, pornography (Rando et al., 1Warsaw University of Technology, Poland 2IDEAS NCBR, Correspondence to: Bartosz Cywinski <bcywinPoland. ski11@gmail.com>. Figure 1. Concept unlearning in SAeUron. We localize and remove SAE features corresponding to the unwanted concept (Cartoon) while preserving the overall performance of the diffusion model. 2022; Schramowski et al., 2023) or copyrighted images e.g. cloning the artistic styles without consent (Andersen, 2024). The straightforward solution to this problem is to retrain the model from scratch with curated data. However, such an approach, due to the enormous sizes of the training datasets is both costly and impractical. As result, growing number of works focus on removing the influence of unwanted data from already pre-trained text-to-image diffusion models through machine unlearning (MU). Most existing methods build on the basic idea of fine-tuning the model while using negative gradients for selected unwanted samples (Wu et al., 2024; Gandikota et al., 2023; Heng & Soh, 2024; Kumari et al., 2023). To minimize degradation in the overall models performance, recent techniques 1 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders restrict parameter updates to attention layers (Zhang et al., 2024a) or to their most important subsets (Fan et al., 2023; Wu & Harandi, 2024). drawback of fine-tuning-based approaches is that they offer limited understanding of how the base model changes during the process. Consequently, these methods often fail to fully remove targeted concepts and instead merely mask them, leaving the models highly vulnerable to adversarial attacks (Zhang et al., 2025). In this work, we propose conceptually different approach to unlearning in diffusion models, which we dubbed SAeUron. We first adapt sparse autoencoders (Olshausen & Field, 1997) to train them in an unsupervised way on the internal activations of the Stable Diffusion (Rombach et al., 2022) text-to-image (T2I) diffusion model. By using activations extracted from all of the denoising timesteps, our SAE learns set of sparse and semantically meaningful features. This allows us to block specific concept, by identifying features associated with it and removing them during the inference. Figure 1 visualizes this idea. While the sparsity of SAE features ensures that unlearning of one concept has limited influence on the remaining ones, we additionally demonstrate that the concept-specific features targeted by our approach are interpretable. As result, we can analyze them prior to unlearning, (e.g. by highlighting their activation areas or annotating them) which significantly enhances the transparency of our approach compared to other methods. We evaluate our method on the recently proposed large and competitive unlearning benchmark UnlearnCanvas (Zhang et al., 2024b) which assesses unlearning effectiveness across 20 objects and 50 styles. We train two SAE models one for styles and one for objects each using activations gathered from single selected SD block and show that our blocking approach achieves state-of-the-art (SOTA) performance in unlearning without affecting the overall performance of the diffusion model. Importantly, due to the fact that SAEs are trained in an unsupervised way, SAeUron is highly robust to adversarial attacks and seamlessly scales to removing multiple concepts simultaneously, contrary to other methods. The summary of our contributions is as follows: We demonstrate that sparse autoencoders extract meaningful and interpretable features from the internal activations of diffusion models across multiple denoising timesteps. We propose SAeUron, an interpretable unlearning method that localizes features corresponding to unwanted concepts and ablates them, achieving state-ofthe-art performance. We demonstrate that SAeUron enables seamless unlearning of multiple concepts simultaneously and exhibits high robustness against adversarial attacks. 2. Related Work 2.1. Sparse Autoencoders (SAEs) Sparse autoencoders (Olshausen & Field, 1997) are neural networks designed to learn compact and interpretable representations of data by encouraging sparsity in the latent space. This is achieved by incorporating sparsity penalty into the reconstruction loss, ensuring that only small fraction of latent neurons activate for any given input. Recently, SAEs have emerged as an effective tool in the field of mechanistic interpretability, enabling the discovery of features corresponding to human-interpretable concepts (Huben et al., 2024; Bricken et al., 2023) and sparse feature circuits within language models (Marks et al., 2024). In this study, sparse autoencoders are used within text-to-image diffusion models to identify and disable features linked to the generative capabilities of specific concepts. 2.2. Machine Unlearning in Diffusion Models The term and problem statement for machine unlearning was first introduced by Cao & Yang (2015), where authors transform the neural network model, through an additional simple layer, into format where output is summation of independent features. Such setup allows for unlearning by simply blocking the selected summation weights or nodes. Conversely, recent works focusing on unlearning for diffusion models, usually employ fine-tuning in order to unlearn specific concepts. For example, EDiff (Wu et al., 2024) formulates this problem as bi-level optimization, ESD (Gandikota et al., 2023) leverages negative classifierfree guidance and FMN (Zhang et al., 2024a) introduces new re-steering loss applied only to the attention layer. SalUn (Fan et al., 2023) and SHS (Wu & Harandi, 2024) select parameters to adapt through saliency maps or connection sensitivity, while SA (Heng & Soh, 2024) replaces unwanted data distribution with the surrogate one, with an extension to the selected anchor concepts in CA (Kumari et al., 2023). SPM (Lyu et al., 2024) takes different approach, using small linear adapters added after each linear and convolutional layer to directly block the propagation of unwanted content. By contrast, methods that do not rely on fine-tuning include SEOT (Li et al., 2024), which eliminates unwanted content from text embeddings, and UCE (Gandikota et al., 2024), which adapts cross-attention weights using closed-form solution. Unlike these approaches, we neither modify prompt embeddings nor alter the base models weights. In this paper, we revisit the pioneering work by Cao & Yang (2015) adapting it to the text-to-image diffusion models using recent advancements in mechanistic interpretability. In particular, we train sparse autoencoder on the activations of the diffusion model and leverage its summative nature 2 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders to unlearn concepts by blocking unwanted content. Most similarly to our approach, Farrell et al. (2024) show that SAEs can be employed to remove subset of biological knowledge in large language models (LLMs), while Guo et al. (2024) benchmark several mechanistic interpretability techniques for knowledge editing and unlearning in LLMs. 2.3. Interpretability of Diffusion Models Numerous studies explored disentangled semantic directions within the bottleneck layers of UNet-based diffusion models (Kwon et al., 2023; Park et al., 2023; Hahm et al., 2024) and analyzed cross-attention layers to investigate their internal mechanisms (Tang et al., 2022). Despite these efforts, detailed interpretation of the specific functions and features learned by specific components in T2I diffusion models remains limited. Recently, Basu et al. (2023; 2024) localized knowledge about visual attributes in DMs, showing that modifying text input in few cross-attention layers can consistently alter attributes like styles, objects or facts. Additionally, Toker et al. (2024) aimed to interpret T2I models text encoders by generating images from their intermediate representations. In contrast, our work employs sparse autoencoders to achieve more fine-grained understanding of the internal representations in diffusion models. Although SAEs are widely used in the language domain, their application to vision remains limited. Early studies applied them to interpret and manipulate CLIP (Radford et al., 2021) representations (Fry, 2024; Daujotas, 2024) or traditional vision networks (Szegedy et al., 2015; Gorton, 2024). More recently, SAEs have been successfully applied in vision-language models (VLMs) to tackle problems such as mitigating hallucinations (Jiang et al., 2024) and generating interpretable radiology reports (Abdulaal et al., 2024). To date, only few studies have utilized SAEs to investigate the inner workings of T2I diffusion models. Ijishakin et al. (2024) use SAEs to identify semantically meaningful directions within the bottleneck layer. Surkov et al. (2024) trained SAEs on activations from one-step distilled SDXLTurbo diffusion model, demonstrating that SAEs can detect interpretable features within specific models blocks and enable causal interventions on them. Furthermore, Kim et al. (2024) applied SAEs to activations from the diffusion model sampled in an unconditional way. By training separate SAE model for each diffusion timestep, they revealed the visual features learned by these models and their connection to class-specific information. In contrast to prior approaches, our work involves training single SAE on activations from multiple denoising steps of standard, non-distilled Stable Diffusion model. Additionally, we leverage the well-disentangled and interpretable features learned by SAEs for downstream unlearning tasks, showcasing their potential in real-world use cases. 3. Sparse Autoencoders for Diffusion Models In this work we adapt sparse autoencoders to StableDiffusion text-to-image diffusion model. Importantly, unlike previous works utilizing SAEs for diffusion models, we train them on activations extracted from every step of the denoising diffusion process. These activations are obtained from the cross-attention blocks of the diffusion model and form feature maps. Each feature map extracted at timestep is spatially structured tensor of shape Ft Rhwd, where and denote the height and width of the feature map, and is the dimensionality of each feature vector. Each spatial position within the feature map corresponds to patch in the input image. As single SAE training sample, we consider an individual d-dimensional feature vector, disregarding the information about its spatial position. Therefore from each feature map, we obtain training samples. For simplicity, we drop the timestep index in subsequent notations. Let Rd denote the d-dimensional vector of activations from single position of feature map and let be the latent dimension in sparse autoencoder. The encoder and decoder of standard single-layer ReLU sparse autoencoder (Bricken et al., 2023) are then defined as follows: = ReLU (Wenc(x bpre) + benc) ÀÜx = Wdecz + bpre, (1) where Wenc Rnd and Wdec Rdn are encoder and decoder weight matrices respectively, bpre Rd and benc Rn are learnable bias terms. Elements of called feature activations are usually denoted as f1,...,n(x). Typically, is equal to multiplied by positive expansion factor. The objective function of SAE is defined as: L(x) = ÀÜx2 2 + Œ±Laux, (2) where ÀÜx2 2 is reconstruction error and Laux is reconstruction error using only the largest kaux feature activations that have not fired on large number of training samples, so-called dead latents. The auxiliary loss is used to prevent dead latents from occurring and is scaled by coefficient Œ±. In our work, we additionally apply two extensions over vanilla ReLU SAEs. First, we follow Gao et al. (2024) and use the TopK activation function (Makhzani & Frey, 2013) which retains only the largest latent activations for each vector x, setting the rest to zeros. While the decoder remains unchanged, the encoder is thus redefined to: = TopK (Wenc(x bpre)) . (3) Second, we leverage the BatchTopK approach introduced by Bussmann et al. (2024), which dynamically selects the largest feature activations across the entire input data SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Figure 2. Unlearning procedure in SAeUron. (a) Concept-specific features are selected for unlearning according to their importance scores. (b) During inference in the U-Net of the diffusion model, activation between selected cross-attention blocks is passed through trained SAE. The selected SAE features are then ablated by scaling them with negative multiplier Œ≥c, removing their influence on the final output. The remaining features are left unchanged, ensuring minimal impact on the overall model performance. batch of size during training. It allows the SAE to more flexibly distribute active latents across samples. During inference, is fixed to constant value. We observed that BatchTopK SAEs tend to activate more frequently in the central regions of samples while allocating fewer latents to the image borders, as presented in Appendix A. This aligns with the nature of the LAION dataset (Schuhmann et al., 2022) used for the training of the SD model. 4. Method Given trained sparse autoencoder able to reconstruct activations of the diffusion model, our SAeUron method for concept unlearning involves two steps. First, we identify which SAE features will be targeted for unlearning specific concept c. This selection is based on the importance scores associated with features. Then, during the inference of the diffusion model, we encode the original activations with SAE, ablate the selected features to remove the targeted concept associated with them, and decode them back. Thanks to the summative nature of SAEs and the sparsity of activated features, this process effectively removes the influence of the targeted concept on the final generation, while preserving the overall performance of the diffusion model. We present the overview of our method in Figure 2. 4.1. Selection of SAE features for unlearning To identify SAE features that exhibit strong correspondence exclusively to the target concept c, we define score function that measures the importance of each i-th feature for concept at every denoising timestep t. Utilizing dataset of activations from the diffusion model = Dc Dc, which includes data containing target concept Dc and data that does not Dc, we define score as: score(i, t, c, D) = ¬µ(i, t, Dc) j=1 ¬µ(j, t, Dc) + Œ¥ ¬µ(i, t, Dc) j=1 ¬µ(j, t, Dc) + Œ¥ (cid:80)n (cid:80)n (4) , where Œ¥ is small constant added to prevent division by (cid:80) zero and ¬µ(i, t, D) = 1 xD fi(xt) denotes the averD age activation of i-th feature on activations from timestep t. To ensure that features activating on many concepts do not dominate the scores, we normalize both components by the average activation values for the corresponding subsets of the dataset. Thus, features with high scores exhibit strong activation for concept while remaining weakly activated for all other concepts. Figure 3 shows histogram of scores calculated for each prompt and timestep using our validation set. Importantly, only small fraction of features achieve high scores, indicating that SAE learns limited number of concept-specific features. Consequently, we target highscoring features in our method to unlearn concepts without affecting the overall performance of model, blocking features with scores above the tunable percentile threshold œÑc. 4.2. SAE-based concept unlearning Building on the method introduced for locating features that correspond to specific concepts, we now present our SAE-based unlearning procedure, which we apply for each timestep during the inference of the diffusion model. To that end, we utilize previously trained sparse autoencoder applied to single U-Net cross-attention block. 4 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders show that different cross-attention blocks specialize in generating specific visual aspects like style or objects. Building on this fact, we apply our unlearning technique to activations from key cross-attention blocks. For style filtering, we use second to last up-sampling block up.1.2, and for object filtering up.1.1, identified empirically as the most effective. Exemplary generations demonstrating the effects of ablating these blocks are presented in Figure 11(a) and Figure 11(b) in the Appendix. UnlearnCanvas benchmark (Zhang et al., 2024b) is large benchmark aiming to extensively evaluate MU methods for DMs. Benchmark consists of 50 styles 20 objects, providing test bed both for style and object unlearning evaluation. Authors, along with dataset, also provide Stable Diffusion v1.5 model fine-tuned on the selected objects and styles from the benchmark, ensuring fair evaluation. SAE training dataset To ensure fair evaluation, the SAE training set is comprised of text prompts that are distinct from those employed in the evaluation on the UnlearnCanvas benchmark. Specifically, we utilize simple one-sentence prompts (referred to as anchor prompts), which were employed by the authors of the benchmark in training of the CA method (Kumari et al., 2023). For each of the 20 objects, we use 80 prompts. Additionally, to enable the SAE to learn the styles used in the benchmark, we append the postfix in {style} style. to each prompt. Consequently, our training set consists of total of 81,600 prompts. For each generation, we collect the internal activations from the specified cross-attention blocks across 50 denoising timesteps, utilizing the DDIM sampler (Song et al., 2021) and guidance scale set to 9. Each feature map Ft from both blocks up.1.1 and up.1.2 has shape of 16161280. Importantly, we only gather feature maps related to textconditioned generation part, discarding the unconditioned ones, and we store them in float16 precision. Nonetheless, during inference, trained SAEs reconstruct both parts of feature maps. Appendix provides details on SAE training. Validation dataset for feature score calculation To calculate feature scores during the unlearning of concept c, we collect feature activations fi(xt) at each denoising timestep using validation set of anchor prompts, similar to SAEs training set. Following the UnlearnCanvas evaluation setup, activations are gathered over 100 denoising timesteps. Despite being trained on 50 steps, SAEs generalize well to this extended range. For style unlearning, we use 20 prompts per style and for object unlearning 80 per object. Style validation prompt templates are shown in Appendix B. 5.2. Interpreting SAE features Before presenting the experimental results for our SAEbased unlearning method, we first evaluate how well the Figure 3. Feature importance scores. Most of the features have near-zero scores, indicating that SAE learns only few conceptspecific features. During the evaluation, we find threshold based on percentile of scores and block features exceeding it. To unlearn concept c, we first identify set of SAE features Fc associated with and compute their average activations on validation dataset D: Fc := { {1, . . . , n}, score(i, t, c, D) > œÑc} ¬µ(i) := ¬µ(cid:0)i, t, D(cid:1), Fc (5) Then, we cut the connection in the diffusion model between the block SAE was trained on and the subsequent one, applying trained SAE in between them. During inference, the SAE encoder decomposes each activation vector from the feature map Ft of the previous cross-attention block following Equation (3). Then, activations of selected features Fc are ablated by scaling them with negative multiplier Œ≥c < 0 normalized by the average activation on concept samples ¬µ(i, t, Dc). This removes the influence of the targeted concept on the activation vector x. In summary, each i-th latent feature activation is modified as follows: fi(x) = Œ≥c¬µ(i, t, Dc)fi(x), fi(x), if Fc fi(x) > ¬µ(i, t, D), otherwise. (6) The condition fi(x) > ¬µ(i, t, D) ensures that only significant features are selected, preventing random feature ablation when all scores are low. The modified representations are decoded back using the SAE decoder, preserving the error term, and passed to the next diffusion block. An overview of this procedure is shown in Figure 2, with pseudocode provided in Appendix M. Procedure involves two hyperparameters: œÑc and Œ≥c, further discussed in Section 5.3.2. 5. Experiments 5.1. Technical details Where to apply SAEs Recent studies on mechanistic interpretability in diffusion models (Basu et al., 2023; 2024) 5 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders sparse encoding captures the concepts to be unlearned. Specifically, we assess whether the features selected using our score-based approach correspond to the desired concepts, as selecting relevant features is critical to our methods success. Additionally, we examine the image regions where these features strongly activate to verify their connection to the targeted concepts and their alignment with human-interpretable attributes. 5.2.1. DO FEATURES EXHIBIT DISCRIMINATIVE POWER? To validate whether SAE learns meaningful visual features, we train 5-nearest neighbors classifier on SAE feature activations extracted at each timestep from the validation dataset used for the score calculation. Importantly, activations are gathered from the unconditional part of the generation to exclude the influence of text embeddings. Figure 4 shows object classification accuracies across timesteps. As expected, when we use all features the accuracy improves as denoising progresses, due to the emergence of object-relevant visual attributes. Notably, our score-based selection approach identifies the most important features, achieving high accuracy across most timesteps with just 1.9 0.83 features selected per object on average. The exact number of features selected per concept in our unlearning procedure is detailed in Figure 15 in the Appendix. Interestingly, randomly selected SAE features (matching the number chosen by the score-based method) still exhibit discriminative power, significantly outperforming the random guess baseline. These results confirm that our method effectively selects the most concept-relevant features and signifies that SAE successfully learns meaningful visual features from the diffusion model. Analogous results for style classification are shown in Figure 16 in the Appendix. 5.2.2. DO FEATURES RELATE TO CONCEPTS? To further enhance our understanding of features learned by SAEs, we visualize their activations on corresponding image patches to assess whether they relate to interpretable patterns. We generate heatmaps of activations from features selected by our score-based approach, normalized to the range [0, 1], and overlay them on the generations, as shown in Figure 5. For each timestep t, we visualize the corresponding generated image by predicting the fully denoised sample x0 from the diffusion models representation at t. The visualizations reveal that style-related features strongly activate on patches with characteristic style patterns while remaining inactive elsewhere. Notably, these features focus on style-related backgrounds while ignoring object regions, demonstrating the precision of our score-based selection in isolating style features. Similarly, object-related features activate only on the targeted object, regardless of the background or style. By analyzing activations across multiple Figure 4. Object classification with k-nearest neighbors algorithm based on SAE feature activations. Features selected with our score-based selection approach demonstrate strong discriminative power across timesteps. Even randomly selected features exhibit notably higher accuracy than random guess baseline, proving that SAE learns meaningful visual attributes. Figure 5. Activations of features selected for unlearning displayed on image patches. (Left) Features corresponding to the Bricks style strongly activate on patterns characteristic of this style. (Right) Conversely, Butterfly-related features activate successfully on image regions containing the object, regardless of the style. denoising timesteps, we observe that our selection method effectively adapts to changes during the denoising process. Displayed visualizations enhance the transparency of our method by clearly showing are targeted for unlearning. Our results demonstrate that our SAEs effectively learn features in diffusion models that correspond to humaninterpretable concepts. This highlights the potential of SAEs not only for unlearning tasks but also as general tool for interpreting diffusion models. We further extend this analysis by automating feature annotation using VLMs (Appendix N). The consistency of meaningful features across denoising steps highlights SAEs as promising tool for understanding the internal mechanisms of diffusion models in complex, multistep generation processes. 6 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Table 1. Evaluation of SAeUron against state-of-the-art methods on style and object unlearning. The best result for each metric is highlighted in bold, and the second-best is underlined. Our approach significantly outperforms others on style unlearning and performs comparably on object unlearning. Importantly, SAeUron demonstrates consistent performance across all metrics. Method UA () Style Unlearning IRA () CRA () Avg. () UA () Object Unlearning IRA () CRA () Avg. () FID () Effectiveness Efficiency Memory (GB) () Storage (GB) () ESD (Gandikota et al., 2023) FMN (Zhang et al., 2024a) UCE (Gandikota et al., 2024) CA (Kumari et al., 2023) SalUn (Fan et al., 2023) SEOT (Li et al., 2024) SPM (Lyu et al., 2024) EDiff (Wu et al., 2024) SHS (Wu & Harandi, 2024) SAeUron 98.58% 80.97% 93.96% 91.17% 92.15% 55.78% 44.23% 64.05% 65.55 88.48% 56.77% 46.60% 63.95% 45.64% 90.63% 73.46% 69.91% 131.37 98.40% 60.22% 47.71% 68.78% 94.31% 39.35% 34.67% 56.11% 182.01 54.21 60.82% 96.01% 92.70% 83.18% 46.67% 90.11% 81.97% 72.92% 61.05 86.26% 90.39% 95.08% 90.57% 86.91% 96.35% 99.59% 94.28% 62.38 56.90% 94.68% 84.31% 78.63% 23.25% 95.57% 82.71% 67.18% 59.79 60.94% 92.39% 84.33% 79.22% 71.25% 90.79% 81.65% 81.23% 92.42% 73.91% 98.93% 88.42% 86.67% 94.03% 48.48% 76.39% 81.42 95.84% 80.42% 43.27% 73.18% 80.73% 81.15% 67.99% 76.62% 119.34 61.43 96.76% 98.70% 98.10% 97.85% 84.23% 90.05% 76.78% 83.69% 17.8 17.9 5.1 10.1 30.8 7.34 6.9 27.8 31.2 2. 4.3 4.2 1.7 4.2 4.0 0.0 0.0 4.0 4.0 0.2 5.3. Concept unlearning with SAeUron 5.3.1. METRICS We evaluate our method on unlearning tasks using metrics from the UnlearnCanvas, calculated using Vision Transformer-based (Dosovitskiy et al., 2021) classifiers provided by the authors of the benchmark. Assuming that we want to remove concept c, unlearning accuracy (UA) measures the proportion of samples generated from prompts containing that are not correctly classified. In-domain retain accuracy (IRA) quantifies correctly classified samples with other concepts, while cross-domain retain accuracy (CRA) assesses accuracy in different domain (e.g., in style unlearning, we calculate object classification accuracy). Additionally, we measure the overall quality of images generated after unlearning through FID (Heusel et al., 2017). This set of metrics enables evaluating each methods effectiveness in removing concepts from the base model while preserving the generative capabilities of others. 5.3.2. HYPERPARAMETERS Our method uses two hyperparameters tunable for each concept separately: percentile threshold of score distribution œÑc and negative multiplier Œ≥c. For style unlearning we empirically observed that setting œÑc = 99.999 and Œ≥c = 1 yield satisfying results across all styles. For the case of object unlearning we tune hyperparameters on the validation dataset, presenting the selected values in Appendix G. 5.3.3. RESULTS We evaluate SAeUron on style and object unlearning tasks using the UnlearnCanvas benchmark, comparing it to stateof-the-art methods. Table 1 presents results averaged over five random seeds, with competing method results taken from UnlearnCanvas. Despite using unsupervised SAE features, SAeUron significantly outperforms all methods in style unlearning and ranks second in object unlearning. Figure 6. Evaluation on sequential unlearning of multiple concepts. Results present the average of unlearning accuracy (UA) and retaining accuracy (RA = IRA+CRA ). SAeUron achieves superior unlearning effectiveness while retaining the overall models performance. At the same time, we observe significant drop in retaining the ability of competing approaches. 2 Unlike other approaches that train separate model for each removed concept, SAeUron requires SAE training on just two cross-attention blocks once. Additionally, SAEs are lightweight, requiring minimal memory and storage. Notably, SAeUron maintains stable performance across both unlearning (UA) and preservation metrics (IRA, CRA). This is contrary to the other methods which mostly fail to effectively balance those two aspects. Figure 7 present qualitative results showcasing the workings of our method on the unlearning task. SAeUron removes unlearning target while preserving other visuals. Our findings confirm that SAEs are effective for real-world tasks like unlearning in diffusion models. Moreover, the interpretability of our method, which explicitly relies on small number of human-interpretable features, provides an additional advantage, making SAeUron transparent approach for real-world applications. 7 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Figure 7. Qualitative evaluation of SAeUron on object and style unlearning. Removing objects and styles effectively preserves other elements of the test prompt while having minimal impact on the generation quality of remaining concepts. 6. Additional experiments 6.1. Unlearning of multiple concepts Recent studies show that traditional machine unlearning approaches, while effective for removing single concept, struggle in scenarios requiring the sequential removal of multiple concepts from diffusion model (Zhang et al., 2024b). In contrast, SAeUron enables seamless filtering of multiple concepts with minimal impact on other concepts. We evaluate our approach against competing methods on sequential unlearning of 6 styles, with results presented in Figure 6. Notably, the performance of other methods drops as the number of targeted concepts increases, due to the growing degradation of the models overall performance. By selectively removing limited subset of features strongly tied to the targeted concepts, SAeUron achieves superior retention of non-targeted concepts. Further details on the evaluation setup are provided in the Appendix D. 6.2. Robustness to adversarial attacks Finally, as shown by Zhang et al. (2025), recent unlearning works do not always fully block the unwanted content, making it possible to bypass the unlearning mechanisms. In particular, authors show that when prompted with crafted adversarial inputs models can still be forced to generate unlearned concepts. We evaluate SAeUron under the UnlearnDiffAtk method (Zhang et al., 2025), optimizing 5-token prefix for 40 iterations with learning rate of 0.01. Figure 8 shows unlearning accuracies before and after the attack for all methods. Competing approaches suffer significant performance drops, suggesting they primarily mask concepts instead of unlearning them. In contrast, our method, by Figure 8. Robustness to adversarial prompts crafted using UnlearnDiffAtk method. Our blocking approach demonstrates strong robustness to adversarial prompts, as evidenced by minimal drop in unlearning accuracy under attack scenarios. In contrast, competing methods exhibit significant vulnerability. filtering internal activations of the diffusion model, remains highly robust, showing minimal performance degradation. 7. Limitations There are several limitations of our approach serving as interesting future work directions. SAeUron operates during inference, introducing 10% overhead, which slightly slows down the generation process. Additionally, training SAEs demands significant storage for activations, posing challenges for large datasets. However, as presented in Table 1, when compared to other techniques our approach has low GPU and storage requirements. The performance of our approach is highly affected by the quality of the SAE. In particular, the unconditional nature of SAE allows us to attempt unlearning of new content never seen by the SAE. As presented in Appendix F, SAeUron can successfully unlearn some of such concepts, but yields lower accuracy. 8 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders 8. Conclusions In this work, we propose SAeUron, novel method leveraging sparse autoencoders to unlearn concepts from textto-image diffusion models. Training SAEs on activations from DM, we demonstrate that their sparse and interpretable features enable precise, concept-specific interventions while maintaining overall model performance. Methods reliance on interpretable features enhances transparency, allowing for clearer understanding of the unlearning process. SAeUron achieves SOTA results on the UnlearnCanvas benchmark, showcasing robustness to adversarial attacks and the capability to unlearn multiple concepts sequentially."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work. In particular, while our method was designed to block and remove selected unwanted, biased or harmful content it can be misused to promote it instead."
        },
        {
            "title": "References",
            "content": "Abdulaal, A., Fry, H., Montana-Brown, N., Ijishakin, A., Gao, J., Hyland, S., Alexander, D. C., and Castro, D. C. An x-ray is worth 15 features: Sparse autoencoders for interpretable radiology report generation. arXiv preprint arXiv:2410.03334, 2024. Andersen. Andersen v. stability ai ltd., 2024. Basu, S., Zhao, N., Morariu, V. I., Feizi, S., and Manjunatha, V. Localizing and editing knowledge in text-to-image generative models. In The Twelfth International Conference on Learning Representations, 2023. Basu, S., Rezaei, K., Kattakinda, P., Morariu, V. I., Zhao, N., Rossi, R. A., Manjunatha, V., and Feizi, S. On mechanistic knowledge localization in text-to-image generative models. In Forty-first International Conference on Machine Learning, 2024. Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J. E., Hume, T., Carter, S., Henighan, T., and Olah, C. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html. Bussmann, B., Leask, P., and Nanda, N. Batchtopk sparse autoencoders. arXiv preprint arXiv:2412.06410, 2024. 9 Cao, Y. and Yang, J. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pp. 463480. IEEE, 2015. Daujotas, G. Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders, 2024. URL https://tinyurl.com/3u6y9wz5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=YicbFdNTTy. Fan, C., Liu, J., Zhang, Y., Wong, E., Wei, D., and Liu, S. Salun: Empowering machine unlearning via gradientbased weight saliency in both image classification and generation. arXiv preprint arXiv:2310.12508, 2023. Farrell, E., Lau, Y.-T., and Conmy, A. Applying sparse autoencoders to unlearn knowledge in language models. arXiv preprint arXiv:2410.19278, 2024. Fry, H. Towards multimodal interpretability: Learning sparse interpretable features in vision transformers, 2024. URL https://tinyurl.com/mrx5c3pc. Gandikota, R., Materzynska, J., Fiotto-Kaufman, J., and Bau, D. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 24262436, 2023. Gandikota, R., Orgad, H., Belinkov, Y., Materzynska, J., and Bau, D. Unified concept editing in diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 51115120, January 2024. Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. Gorton, L. The missing curve detectors of inceptionv1: Applying sparse autoencoders to inceptionv1 early vision. arXiv preprint arXiv:2406.03662, 2024. Guo, P., Syed, A., Sheshadri, A., Ewart, A., and Dziugaite, G. K. Mechanistic unlearning: Robust knowledge unlearning and editing via mechanistic localization. arXiv preprint arXiv:2410.12949, 2024. Hahm, J., Lee, J., Kim, S., and Lee, Isometric representation learning for disentangled latent In International conferspace of diffusion models. ence on machine learning, volume abs/2407.11451, J. SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders 2024. URL https://api.semanticscholar. org/CorpusID:271218346. Heng, A. and Soh, H. Selective amnesia: continual learning approach to forgetting in deep generative models. Advances in Neural Information Processing Systems, 36, 2024. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Huben, R., Cunningham, H., Smith, L. R., Ewart, A., and Sharkey, L. Sparse autoencoders find highly interIn The Twelfth pretable features in language models. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=F76bwRSLeK. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ijishakin, A., Ang, M. L., Baljer, L., Tan, D. C. H., Fry, H. L., Abdulaal, A., Lynch, A., and Cole, J. H. H-space sparse autoencoders. In Neurips Safe Generative AI Workshop 2024, 2024. Jiang, N., Kachinthaya, A., Petryk, S., and GandelsInterpreting and editing vision-language repman, Y. resentations to mitigate hallucinations. arXiv preprint arXiv:2410.02762, 2024. Kim, D., Thomas, X., and Ghadiyaram, D. Revelio: Interpreting and leveraging semantic information in diffusion models. arXiv preprint arXiv:2411.16725, 2024. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kissane, C., Krzyzanowski, R., Bloom, J. I., Conmy, A., and Nanda, N. Interpreting attention layer outputs with sparse autoencoders. arXiv preprint arXiv:2406.17759, 2024. Kwon, M., Jeong, J., and Uh, Y. Diffusion models alIn The Eleventh ready have semantic latent space. International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=pd1P2eUBVfq. Li, S., van de Weijer, J., Hu, T., Khan, F. S., Hou, Q., Wang, Y., and Yang, J. Get what you want, not what you dont: Image content suppression for text-to-image diffusion models. arXiv preprint arXiv:2402.05375, 2024. Lyu, M., Yang, Y., Hong, H., Chen, H., Jin, X., He, Y., Xue, H., Han, J., and Ding, G. One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 75597568, 2024. Makhzani, A. and Frey, B. J. k-sparse autoencoders. CoRR, abs/1312.5663, 2013. URL https://api. semanticscholar.org/CorpusID:14850799. Marks, S., Rager, C., Michaud, E. J., Belinkov, Y., Bau, D., and Mueller, A. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024. Olshausen, B. A. and Field, D. J. Sparse coding with an overcomplete basis set: strategy employed by v1? Vision Research, 37:33113325, 1997. URL https://api. semanticscholar.org/CorpusID:14208692. Park, Y.-H., Kwon, M., Choi, J., Jo, J., and Uh, Y. Understanding the latent space of diffusion models through the lens of riemannian geometry. Advances in Neural Information Processing Systems, 36:2412924142, 2023. Paulo, G., Mallen, A., Juang, C., and Belrose, N. Automatically interpreting millions of features in large language models. arXiv preprint arXiv:2410.13928, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Rando, J., Paleka, D., Lindner, D., Heim, L., and Tram`er, F. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610, 2022. Kumari, N., Zhang, B., Wang, S.-Y., Shechtman, E., Zhang, R., and Zhu, J.-Y. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2269122702, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 10 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Wu, J. and Harandi, M. Scissorhands: Scrub data influence via connection sensitivity in networks. In European Conference on Computer Vision, pp. 367384. Springer, 2024. Wu, J., Le, T., Hayat, M., and Harandi, M. Erasediff: Erasing data influence in diffusion models. arXiv preprint arXiv:2401.05779, 2024. Zhang, G., Wang, K., Xu, X., Wang, Z., and Shi, H. Forgetme-not: Learning to forget in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1755 1764, 2024a. Zhang, Y., Fan, C., Zhang, Y., Yao, Y., Jia, J., Liu, J., Zhang, G., Liu, G., Kompella, R. R., Liu, X., and Liu, S. Unlearncanvas: Stylized image dataset for enhanced machine unlearning evaluation in diffusion models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum? id=t9aThFL1lE. Zhang, Y., Jia, J., Chen, X., Chen, A., Zhang, Y., Liu, J., Ding, K., and Liu, S. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. In European Conference on Computer Vision, pp. 385403. Springer, 2025. Schramowski, P., Brack, M., Deiseroth, B., and Kersting, K. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2252222531, 2023. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C. W., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S. R., Crowson, K., Schmidt, L., Kaczmarczyk, R., and Jitsev, J. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https: //openreview.net/forum?id=M3Y74vmsMcY. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=St1giarCHLP. Surkov, V., Wendler, C., Terekhov, M., Deschenaux, J., West, R., and Gulcehre, C. Unpacking sdxl turbo: Interpreting text-to-image models with sparse autoencoders. arXiv preprint arXiv:2410.22366, 2024. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, In Proceedings A. Going deeper with convolutions. of the IEEE conference on computer vision and pattern recognition, pp. 19, 2015. Tang, R., Liu, L., Pandey, A., Jiang, Z., Yang, G., Kumar, K., Stenetorp, P., Lin, J., and Ture, F. What the daam: Interpreting stable diffusion using cross attention. arXiv preprint arXiv:2210.04885, 2022. Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., Carter, S., Olah, C., and Henighan, T. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https: //transformer-circuits.pub/2024/ scaling-monosemanticity/index.html. Toker, M., Orgad, H., Ventura, M., Arad, D., and Belinkov, Y. Diffusion lens: Interpreting text encoders in text-toimage pipelines. arXiv preprint arXiv:2403.05846, 2024. SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders A. BatchTopK SAEs trained for diffusion models The BatchTopK variant of SAEs enables the model to flexibly distribute active features across data batch to achieve better reconstruction performance. Specifically, our SAEs allocate more active latents to image patches with detailed content, while less important areas, such as the background, are reconstructed using fewer features. As shown in Figure 9, SAEs distribute active features unevenly across image samples. While most of the distribution centers around mean of 8192 (since = 32 and each image contains 16 16 activation vectors), notable number of samples use significantly fewer or more active features. Additionally, Figure 10 shows the average number of activated features per image patch. Central regions of the image tend to have more active features, while background areas have fewer. Interestingly, corners of the images also exhibit frequent activations. Figure 9. Number of active features per image sample. We see that SAEs assigns unequal number of active features per sample, signifying that some samples are more important than the others for SAE to obtain good reconstruction error. Figure 10. Average number of active features corresponding to image patches. Interestingly we observe that BatchTopK SAEs trained on activations from diffusion model allocate more active features to reconstruct activation vectors corresponding to central image regions. 12 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders B. Prompts from validation set for feature score calculation Below, we present the prompts used in our validation set to gather feature activations for style unlearning. The same prompts are applied to each style used in the UnlearnCanvas benchmark. For object unlearning, we use all anchor prompts from the CA work, excluding the in {style} style postfixes. Gothic cathedral with flying buttresses and stained glass windows in {style} style. bear dressed as medieval knight in armor in {style} style. bird with feathers as iridescent as an oil slick in the sunlight in {style} style. butterfly emerging from jeweled cocoon in {style} style. cat wearing superhero cape leaping between buildings in {style} style. dog wearing aviator goggles piloting an airplane in {style} style. goldfish swimming in crystal-clear bowl in {style} style. candles flame flickering in mysterious old library in {style} style. Flower blooming in the middle of snow-covered landscape in {style} style. frog with croak that sounds like jazz musicians trumpet in {style} style. Wild horse galloping across the prairie at sunrise in {style} style. man hiking through dense forest in {style} style. Jellyfish floating serenely in deep blue water in {style} style. Rabbit peering out from burrow in {style} style. classic BLT sandwich on toasted bread in {style} style. Sea waves crashing over ancient coastal ruins in {style} style. Statue of forgotten hero covered in ivy in {style} style. Tower soaring above the clouds in {style} style. majestic oak tree in serene forest in {style} style. Moonlit waterfall in serene forest in {style} style. C. Selection of cross-attention blocks to apply SAE In LLMs, SAEs are typically trained on activations from the residual stream, MLP layers, or attention layers (Kissane et al., 2024). Building on recent studies on mechanistic interpretability in diffusion models (Basu et al., 2023; 2024), we apply SAEs to cross-attention blocks. To identify the appropriate blocks for style and object unlearning, we conduct an experiment where each cross-attention block is ablated one by one, and the block causing the most significant degradation in the targeted visual attribute (style or object) is selected. Ablation involves replacing the block with identity function. Intuitively, this localizes the block most responsible for generating the analyzed attribute. Figure 11(a) shows original generated images compared to those with the object block up.1.1 ablated, while Figure 11(b) demonstrates the effect of ablating the style block up1.2. Although objects remain visible after ablating the object block, they are significantly more degraded compared to ablations of other blocks. In contrast, ablating the style block almost entirely removes the original style from the image. 13 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders (a) (b) Figure 11. Ablation of cross-attention blocks. (a) Ablating the object block up.1.1 notably degrades the quality of generated objects and (b) ablating the style block up.1.2 almost completely removes the original style of the image. D. Details of sequential unlearning evaluation The sequential unlearning evaluation assesses methods in scenario where unlearning requests arrive sequentially. This setup requires methods to progressively remove an increasing number of concepts from base model while ensuring previously unlearned targets remain unlearned. At the same time, it significantly challenges the retention of the models overall performance. To ensure fair comparison, we follow the evaluation protocol from the UnlearnCanvas paper. Specifically, we sequentially unlearn the following styles in this order: 1. Abstractionism 2. Byzantine 3. Cartoon 4. Cold Warm 5. Ukiyoe 6. Van Gogh After each phase, we compute the UA and RA metrics, where RA is the average of IRA and CRA. Figure 12 shows UA averaged over all unlearned concepts up to each phase. SAeUron consistently maintains high unlearning accuracy and significantly outperforms competing methods in retaining the ability to generate all other concepts. 14 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Figure 12. Evaluation on unlearning multiple concepts. SAeUron achieves high unlearning accuracy and outperforms other approaches in retaining generative capabilities for non-targeted concepts. E. SAE trainings details We train our BatchTopK sparse autoencoders with = 32 and an expansion factor of 16. Optimization uses Adam (Kingma, 2014) with learning rate of 0.0004 and linear scheduler without warmup. We set the batch size to 4096 and unit-normalize decoder weights after each training step. Following heuristics from (Gao et al., 2024), we set kaux to power of two close to 32 . Additionally, in line with Templeton et al. (2024), we consider latent dead if it has not activated over the last 10M training samples. We train the SAE on the up.1.1 object block for 5 epochs and on the up.1.2 style block for 10 epochs. 2 and Œ± = 1 Table 2 summarizes key training hyperparameters and metrics, while Figure 13 presents log feature density plots at the end of training. The SAE trained on up.1.1 exhibits dead latents, whereas the one trained on up.1.2 does not. Notably, very few features activate very frequently, which suggests promising interpretability. Both SAEs were trained on single NVIDIA RTX A5000 GPU. Training the SAE on the up.1.1 object block took 27 hours and 40 minutes, while training on the up.1.2 style block required 59 hours and 1 minute. (a) Log feature density of block up.1.1 (b) Log feature density of block up.1.2 Figure 13. Log feature density plots of SAEs trained on cross-attention blocks. 15 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Table 2. Summary of SAE trainings."
        },
        {
            "title": "Block",
            "content": "# Latents up.1.1 up.1.2 20480 20480 32 32 Œ± 1 32 1 32 F. SAE generalization abilities Fraction Var. Learning Batch Dead Feature Unexplained"
        },
        {
            "title": "Size",
            "content": "0.181 0.198 0.0004 0.0004 4096 4096 10M 10M"
        },
        {
            "title": "Epochs",
            "content": "5 10 Normalize Decoder We assess the generalization of SAEs by training sparse autoencoder on activations from prompts in randomly selected half (25) of the styles in the UnlearnCanvas benchmark. The training setup remains identical to our other SAEs. To evaluate the ability to unlearn concepts not seen during training, we apply SAeUron to the style unlearning task using this SAE, following the setup in Section 5.3. Table 3 presents results for SAEs trained on half of the styles, evaluating performance on all styles, in-distribution styles, and out-of-distribution (OOD) styles. Notably, we achieve over 50% unlearning accuracy on OOD data, demonstrating that SAEs effectively generalize and can unlearn concepts even when they were absent from the training set. Table 3. Unlearning performance with SAE trained on half of the data. Setup UA () IRA () CRA () Avg. () All data In-distribution Out of distribution 75.00% 90.18% 80.74% 81.97% 99.76% 99.23% 98.48% 99.16% 51.00% 67.38% 98.36% 72.25% G. Hyperparameters for object unlearning For object unlearning we tune our two hyperparameters: percentile threshold œÑc and multiplier Œ≥c for each class separately. Selected parameters are presented in Table 4. Table 4. Hyperparameters of our method for object unlearning. Object Architectures Bears Birds Butterfly Cats Dogs Fishes Flame Flowers Frogs Horses Human Jellyfish Rabbits Sandwiches Sea Statues Towers Trees Waterfalls Percentile threshold œÑc Multiplier Œ≥c 30.0 5.0 5.0 5.0 30.0 15.0 30.0 1.0 20.0 30.0 20.0 5.0 1.0 10.0 10.0 5.0 5.0 1.0 20.0 1.0 99.999 99.999 99.999 99.99 99.999 99.999 99.995 99.995 99.99 99.999 99.99 99.995 99.999 99.99 99.999 99.995 99.995 99.995 99.99 99. 16 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders H. Activation steering on style features We further explain what information is encoded by our SAE as individual features with the highest correspondence to concept c. To that end, we generate unconditional examples using diffusion model (using an empty prompt) and steer the generation process by increasing activations of the highest scoring features for given concept. Specifically, we modify feature maps Ft during forward pass of the diffusion model at each timestep in the following manner: Ft Ft + (cid:88) iFc Œ≥+ ¬µ(i, t, Dc)di, (7) > 0 is concept-specific positive where di is feature direction corresponding to i-th column of SAE decoder, Œ≥+ multiplier that determines the strength of steering and Fc = {i {1, . . . , n}, score(i, t, c, D) > œÑc} is set of chosen features where œÑc represents threshold calculated from specified percentile value. In Figure 14 we demonstrate that such steering results in generations that exhibit visual attributes corresponding to specific artistic styles. This evidences strong correspondence of features with these styles and thus supports the effectiveness of our feature localization method."
        },
        {
            "title": "With\nSteering",
            "content": "Picasso Abstractionism Superstring Pastel Color Fantasy Figure 14. Steering on unconditional generations with features selected using score-based method. Features associated with specific styles effectively produce generations that visibly reflect those styles. 17 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders I. Number of score-based selected features for unlearning For style unlearning, setting œÑc = 99.999 selects single feature at each timestep in our unlearning method. Since œÑc is tuned separately for each object, the number of selected features varies across classes, as shown in Figure 15. On average, SAeUron utilizes 1.9 0.83 SAE features during the procedure. Figure 15. Number of selected features for object unlearning. J. K-nearest neighbors classification for style features We conduct an analogous experiment to the one in Section 5.2.1, this time on style features. Figure 16 presents the results. Notably, both the score-based and random feature setups use only single feature, as our selection method identifies only one feature for style unlearning. Interestingly, accuracy remains similar between using all features and the score-based selection. Moreover, accuracy tends to increase from approximately the 30-th timestep, suggesting that style-related features emerge later compared to object-related features in the classification setup. Figure 16. Style classification with k-nearest neighbors algorithm based on SAE feature activations. 18 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders K. Distribution of feature importance scores across timesteps We analyze how the distribution of score importance varies across denoising timesteps. Figure 17 shows two high percentiles of score distributions over all 100 timesteps. We observe that the threshold value decreases as the generation process progresses, indicating that more features receive high scores early in denoising. As the process continues, only small number of features remain highly relevant to specific concepts. Figure 17. Percentiles of score distribution across denoising timesteps. L. UnlearnDiffAtk evaluation of object unlearning We also evaluate our method on adversarial prompts crafted using the UnlearnDiffAtk method for object unlearning. As shown in Figure 18, unlearning accuracy drops significantly under attack. However, this is largely due to the nature of the evaluation process, where each iteration of UnlearnDiffAtk determines attack success based on the classifiers argmax prediction. As demonstrated in Figure 19, SAeUron completely removes the targeted object from the image. However, since no other object replaces it, the classifiers predictions become largely random. Consequently, attacks are often marked as successful, even when they fail to make the model generate the unlearned object. To further validate whether images before and after the attack resemble the targeted object, we compute CLIPScore (Radford et al., 2021) between the target objects name and the image. As shown in Table 5, the CLIPScore remains nearly unchanged, indicating that the attack rarely leads to generating the targeted object. Table 5. CLIPScore between images and targeted objects before and after the attack. Although the attack is successful, the similarity to the target object barely changes. CLIPScore () Before attack After successful attack 0.2329 0.2403 19 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Figure 18. Robustness to adversarial prompts crafted using UnlearnDiffAtk method on object unlearning. Before Attack After Attack Architectures Flame Trees Statues Figure 19. Effect of UnlearnDiffAtk on object unlearning with our approach. The left column shows images generated with SAeUron, where the object should be unlearned, while the right column presents results after the adversarial attack. Despite the attacks success, the targeted object remains absent from the image. M. Pseudocode of SAeUron For ease of understanding our unlearning procedure, we present detailed pseudocode of SAeUron applied on single denoising timestep in Algorithm 2. 20 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Algorithm 1 Prepare for unlearning 1: Input: concept c, timestep t, dataset D, threshold œÑc, SAE width 2: Fc { {1, . . . , n}, score(i, t, c, D) > œÑc} 3: ¬µ(i) ¬µ(cid:0)i, t, D(cid:1) Fc 4: Output: Fc, ¬µ R(hw)d Flatten(Ft) flat Algorithm 2 SAeUron unlearning method for DMs 1: Input: target concept c, denoising timestep t, validation dataset D, percentile threshold œÑc, multiplier Œ≥c 2: Ft Rhwd feature map from cross-attention block 3: flat 4: ÀÜF flat 5: Fc, ¬µ prepare(c, t, D, œÑc) 6: for = 1 to (h w) do 7: 8: 9: 10: x(j) flat z(j), ÀÜz(j) TopK(cid:0)Wenc(x(j) bpre)(cid:1) for all Fc do [j] i (cid:1) ÀÜz(j) if ÀÜz(j) > ¬µi then Œ≥c ¬µ(cid:0)i, t, Dc ÀÜz(j) 11: end if 12: end for 13: ÀÜF flat 14: 15: end for , (h, w, d)(cid:1) 16: ÀÜFt Reshape(cid:0) ÀÜF flat 17: Output: ÀÜFt modified feature map with removed [j] Wdec ÀÜz(j) + bpre + (cid:0)z(j) ÀÜz(j)(cid:1) N. Auto-interpreting features selected for unlearning To validate whether the features selected by our score-based method correspond to meaningful and interpretable concepts, we construct simple annotation pipeline using GPT-4o (Hurst et al., 2024). To achieve this, we design prompt for the GPT model, closely following the one presented in (Paulo et al., 2024) and adapting it to our case. Below, we present this prompt: You are meticulous AI researcher conducting an important investigation into visual patterns and feature activations. of images and provide an explanation that thoroughly encapsulates the visual features that trigger particular activation. Your task is to analyze two sets You will be presented with two rows of image examples: Row 1: images provide the visual context for the feature analysis. Original Images (Context). This row contains 5 original images. These Activation Overlay Images (Feature Activation). Row 2: images. Each image in this row corresponds to the image directly above it in Row 1, but with visual overlay. The overlay marks specific regions where particular feature is strongly activated in the corresponding original image. This row contains 5 Your goal is to produce concise, final description that summarizes the shared visual features and patterns you observe in the highlighted regions of the Row 2 (Activation Overlay Images), while using the Row 1 (Original Images) for context. Please adhere to the following guidelines: Focus on summarizing the visual pattern of activation: Describe the overarching visual features common to the highlighted areas in the Row 2 (Activation Overlay 21 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Identify and explain the visual patterns you discern within these Images). overlayed regions. Do not simply describe the entire images in Row 1 or Row 2, but specifically analyze what visual elements within the activated overlays in Row 2, when seen in the context of the corresponding original images in Row 1, indicate the feature is detecting. Utilize Context from Original Images: understand the objects, scenes, or visual elements present in the areas where the feature is activated in Row 2. The original images provide crucial context for interpreting the feature. Refer to the Row 1 (Original Images) to Be concise: Keep your final explanation brief and to the point. should be single, concise sentence. The explanation Ignore uninformative examples: If some image pairs or their overlays seem unclear or do not contribute to identifying visual pattern, you may disregard them in your explanation. Omit marking details: Do not mention the specifics of the visual marking (e.g., Focus solely on the visual content of the \"red overlay,\" \"highlighted pixels\"). activated regions in Row 2 and describe only the visual content of the activated regions. Single explanation: Provide only one concise explanation, not list of possible explanations. Formatted output: The very last line of your response must be the formatted explanation, beginning with [EXPLANATION]: followed by your concise explanation. Analyze the following two rows of images (Row 1: Activation Overlay Images) and provide your formatted explanation: Original Images, Row 2: Alongside the prompt, we provide the GPT-4o model with images from each class in 10 randomly selected styles. The model generates feature annotations separately for each style. Figure 20, Figure 21, and Figure 22 visualize feature activations alongside generated annotations for different objects. As seen in the provided annotations, the GPT model successfully identified the visual features corresponding to the targeted concepts. 22 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders (a) Generated annotation: The activations are strongly triggered by the facial features and eyes of the rabbits. (b) Generated annotation: The feature activates in regions corresponding to the rabbits facial features and ears, suggesting focus on these distinct animal characteristics. (c) Generated annotation: The activation overlays predominantly highlight the facial features and ear regions of the rabbits, indicating the model is detecting distinct facial and ear characteristics. Figure 20. Activations of selected features for Rabbits unlearning with their annotations generated by the GPT model. 23 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders (a) Generated annotation: The activated regions consistently highlight branching structures typical of tree tops, indicating the feature is detecting the intricate patterns of tree branches. (b) Generated annotation: The feature activation highlights the central shape and structure of the tree tops, focusing on the dense and rounded clusters of foliage. (c) Generated annotation: The activations consistently highlight the vertical and branching structures of trees, capturing the contrast between tree trunks and foliage in various lighting conditions. Figure 21. Activations of selected features for Trees unlearning with their annotations generated by the GPT model. 24 SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders (a) Generated annotation: The feature activation emphasizes vertical flows and streaming patterns resembling waterfalls, highlighting their smooth, elongated shapes and cascading movements against textured background. (b) Generated annotation: The feature activation is consistently triggered by the elongated vertical flow and white foamy appearance typical of waterfalls in the images. (c) Generated annotation: The visual pattern of activation corresponds to the vertical flow and cascading movement of water in waterfalls, highlighted by the contrast between the bright water streams and the darker, textured background. Figure 22. Activations of selected features for Waterfalls unlearning with their annotations generated by the GPT model."
        }
    ],
    "affiliations": [
        "IDEAS NCBR",
        "Warsaw University of Technology, Poland"
    ]
}