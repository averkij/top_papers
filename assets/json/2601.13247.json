{
    "paper_title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
    "authors": [
        "Baochang Ren",
        "Yunzhi Yao",
        "Rui Sun",
        "Shuofei Qiao",
        "Ningyu Zhang",
        "Huajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability."
        },
        {
            "title": "Start",
            "content": "Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang*, Huajun Chen* Zhejiang University University of California, Los Angeles {baochang.ren, zhangningyu}@zju.edu.cn Code"
        },
        {
            "title": "Abstract",
            "content": "Current Large Language Models (LLMs) exhibit critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinationsgenerating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the openended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, framework that autonomously constructs symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability. 6 2 0 2 9 1 ] . [ 1 7 4 2 3 1 . 1 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Building World Models capable of simulating and predicting environmental dynamics is foundational to the pursuit of embodied intelligence. While traditional approaches rely on external simulators, compelling paradigm has emerged where the agent itself functions as the world model (Richens et al., 2025). By internally encoding environmental dynamics, agents can reason about causal relationships and predict outcomes without heavy-weight *Corresponding author. 1 Figure 1: Conceptual Illustration of Experiential Alignment. The agent aligns its internal world model via Process Experience and Goal Experience. external engines (Zhang et al., 2021; Fang et al., 2025b; Chae et al., 2024; Wang et al., 2025a; Yang et al., 2025c). Conceptually, this redefines the agent not merely as passive policy executor, but as an active prediction machine of the world it inhabits (Zhang et al., 2025c; Zheng et al., 2025; Cen et al., 2025; Wang et al., 2025b; Qiao et al., 2024a). However, critical modal disconnect persists: while Large Language Models (LLMs) scale effectively on semantic reasoning, they exhibit fundamental disparity between high-level logical planning and low-level physical grounding. Despite possessing vast declarative knowledge, they often lack the procedural understanding required to respect the immutable laws of the physical world. This misalignment leads to frequent physical hallucinations, where agents generate plans that are semantically coherent but physically unexecutable (e.g., attempting to slice an object without holding knife). The agent understands the high-level why, but fails to grasp the low-level how imposed by physical reality. To bridge this gap, dominant paradigms currently rely on internalizing physical laws directly into model parameters via Supervised Fine-Tuning (SFT) or Reinforcement Learning (Chae et al., 2024; Chen et al., 2025b). We argue that this approach may have fundamental limitations: it attempts to compress the infinite variability of dynamic physical environments into static parameter weights. Crucially, planning with an agentic world model presents dual challenge: the internal simulation must not only identify the optimal path (Goal) but also strictly adhere tophysical constraints (Process). Existing agents often prioritize goal achievement while neglecting physical feasibility, leading to brittle plans that fail in dynamic settings. Since static parameters cannot exhaustively cover all physical corner cases, an agent must possess the capacity for online correction, dynamically adjusting its understanding of constraints as it encounters them. This context-based flexibility (Zhang et al., 2025d; Zhou et al., 2025a; Ouyang et al., 2025) raises research question: Can agents autonomously align its world knowledge via experience learning in training-free manner? We answer this question by drawing inspiration from the cognitive theory of Predictive Coding (Huang and Rao, 2011; Friston, 2018). In this view, intelligence is not the passive absorption of data, but the continuous minimization of prediction errorthe discrepancy between internal expectation and sensory reality. Adopting this perspective, we apply paradigm shift where execution failures are not discarded as noise but utilized as corrective signals. When an agents prediction contradicts reality, it exposes boundary in the agents internal world model. We argue that LLM agents possess dormant predictive capabilities that need not be trained from scratch, but simply activated and aligned through these error signals during inference. Intuitively, as shown in Figure 1, we introduce WorldMind, framework that transforms the agent into an empirical learner capable of Aligning Agentic World Models via Knowledgeable Experience Learning. We posit that robust world model must generate simulations that satisfy two essential criteria: physical plausibility and task accuracy. Driven by this motivation, our World Knowledge Repository accumulates two distinct types of experience. First, Process Experience is derived from prediction errors to enforce physical feasibility, ensuring internal simulations strictly adhere to the immutable laws of reality. Second, Goal Experience is distilled from successful trajectories to serve as procedural heuristics, guiding the simulation to efficiently converge toward the task objective. Extensive experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves state-of-theart performance by significantly reducing physical hallucinations. Furthermore, our results reveal remarkable cross-model transferability, confirming that the constructed world knowledge captures universal physical laws independent of the specific backbone model."
        },
        {
            "title": "2 Background",
            "content": "The embodied agent interaction can be formalized as Partially Observable Markov Decision Process (POMDP), defined by the tuple S, A, P, Ω, G. Standard approaches typically treat the transition dynamics as implicit distributions approximated via reward maximization. However, this implicit modeling creates reality gap: agents learn what to do for rewards but fail to explicitly capture the immutable physical laws governing P. This limitation often leads to physically invalid plans in novel scenarios, motivating our proposal to augment the standard formulation with an explicit World Knowledge Repository. Our framework is theoretically grounded in Predictive Coding, which posits that intelligence minimizes the discrepancy between top-down expectation and bottom-up sensory reality. In this view, execution failures are not merely negative feedback but rich epistemic signals that reveal environmental boundaries. We operationalize this mechanism by leveraging prediction errors to actively refine the agents internal world model. This aligns the agents belief of with actual physical reality, mirroring how biological agents refine motor control through sensorimotor contingencies."
        },
        {
            "title": "3.1 Overview and Problem Formulation",
            "content": "As shown in Figure 2, we propose WorldMind, framework transforming autonomous agents into empirical world model learners. By externalizing environmental dynamics into explicit memory, we 2 Figure 2: Overview of the WorldMind Framework. The agent autonomously constructs World Knowledge Repository (WKR) by unifying Process Experience (from prediction errors) and Goal Experience (from successful trajectories) to guide grounded simulation. enable grounded planning through cycle of goal alignment and continuous reality verification. We formulate the problem as World Knowledge-Augmented Markov Decision Process (WK-MDP), defined by the tuple: MWK = S, A, P, Ω, G, (1) Here, and denote the state and action spaces, respectively. : represents the latent transition dynamics. The observation function Ω yields visual inputs ot, and represents the set of task objectives specified in natural language. Distinct from standard MDPs, we introduce an explicit World Knowledge Repository = {Wp, Wg}. Wp encapsulates Process Experience , collection of verbalized causal rules derived from prediction errors. This component functions as learned surrogate model ˆPp that progressively approximates the ground-truth dynamics P, ensuring simulated steps adhere to physical laws. Wg encodes Goal Experience , set of procedural heuristics distilled from successful trajectories. These priors serve to constrain the policy search space towards the optimal policy π, ensuring the simulated path converges on the correct target. The agent operates under policy π(at, ˆst+1 ot, g, W) that jointly generates an action at and predicted future state ˆst+1 given specific goal G. The learning objective is formulated as: max π P(Success g) s.t. min D(ˆst+1st+1) (2) This objective implies that maximizing the probability of task success is contingent upon minimizing the divergence between the predicted state ˆs and the actual state s."
        },
        {
            "title": "3.2 Building World Knowledge Repository",
            "content": "The core of WorldMind lies in the autonomous development of the World Knowledge Repository (WKR) W. This process involves two distinct update mechanisms: one for establishing physical boundaries termed Process Experience , and one for distilling procedural shortcuts known as Goal Experience . Process Experience Construction. To align internal simulations with environmental reality, 3 operates in Predict-Act-Verify loop. At each step t, conditioned on the current observation ot, task goal g, and retrieved knowledge W, the agent jointly generates an action and textual prediction: (at, ˆst+1) M(ot, x, g, W) (3) After execution, the environment yields the actual grounded state st+1. We then operationalize the acquisition of Process Experience through three distinct steps: State Abstraction, Judgment, and SelfReflexion. First, during State Abstraction, to prioritize abstract causal variables over low-level details, we explicitly define semantic abstraction process to convert the ground-truth states into high-level descriptions: sk = Mabs(sk), {t, + 1} (4) Second, in the Judgment phase, the agent acts as verifier, comparing its internal prediction ˆst+1 against the actual abstract outcome st+1. learning signal is triggered only when semantic discrepancy is detected, indicating physical hallucination. Finally, through Self-Reflexion, the agent addresses the identified conflict by triggering reflective module R. This module analyzes the error context alongside the interaction history τt1 to synthesize corrective causal rule. The update rule for Process Experience is formulated as: Wp Wp {R(τt1, at, st, ˆst+1, st+1)} (5) This three-step mechanism ensures that the constructed knowledge captures essential causal dynamics while remaining invariant to trivial environmental noise. Goal Experience Construction. Efficient task completion necessitates heuristic guidance, complementing the physical feasibility ensured by Process Experience . We address this by distilling procedural heuristics from successful episodes. Given successful trajectory τ , the agent autonomously analyzes the interaction history to filter context-specific noise and extract high-level strategies. The Goal Experience is updated as follows: Wg Wg M(τ ) (6) These distilled priors serve as optimized metainstructions, limiting the agents policy search space towards the optimal subspace in future tasks. 3."
        },
        {
            "title": "Inference via Constrained Simulation",
            "content": "In the inference phase, the agent leverages the constructed World Knowledge Repository to guide decision-making through constrained simulation. Specifically, the system dynamically retrieves relevant Process Experience and Goal Experience based on their semantic similarity to the current task goal g. Conditioned on this augmented context, the agent jointly generates the action at and the predicted future state ˆst+1. Crucially, to mitigate hallucinations, the generation of ˆst+1 is gated: the agent only simulates outcomes when the target object is explicitly grounded in the current observation ot or the repository W, otherwise executing the action without updating the internal world model. This selective strategy significantly enhances inference efficiency. Unlike approaches relying on computationally intensive multiple sampling or heavy external world models, our framework minimizes latency by bypassing redundant simulations for ungrounded states."
        },
        {
            "title": "4.1 Experimental Settings.",
            "content": "Datasets and Metrics. We evaluate our framework on the EB-ALFRED and EB-Habitat benchmarks from EmbodiedBench (Yang et al., 2025a), focusing on five fine-grained subsets: Base, Common Sense, Complex Instruction, Spatial Awareness, and Visual Appearance. To quantify performance, we employ two complementary metrics: Success Rate (SR) and Goal-Conditioned Success (GC). SR is strict binary metric (0 or 1) indicating whether the final goal is fully achieved. In contrast, GC evaluates process adherence by awarding partial credit for completed subgoals, even if the episode ultimately fails. Models and Baselines. We employ GPT-4.1mini and GPT-3.5-turbo as the primary backbones for our WorldMind framework. To evaluate effectiveness, using reported results of openand closedsource models from EmbodiedBench, we compare against representative baselines, including Best-ofN (BoN), ReAct (Yao et al., 2022), Synapse (Zheng et al., 2023), SimuRA (Deng et al., 2025), ReasoningBank (Ouyang et al., 2025), and AWM (Wang et al., 2024f). 4 Backbone Method Success Rate (SR) Goal Condition (GC) Avg Base Common Complex Visual Spatial Avg Base Common Complex Visual Spatial ReAct GPT-4o ReAct GPT-4o-mini Claude-3.7-Sonnet ReAct ReAct Gemini-1.5-Pro Llama-3.2-90B-Vis ReAct ReAct InternVL2.5-78B 56.8 28.8 67.2 63.2 35.2 37.0 GPT-3.5-turbo GPT-4.1-mini 44.4 ReAct 42.8 BoN 45.2 SimuRA ReasoningBank 41.6 38.8 Synapse 40.0 AWM 48.0 WorldMind 41.2 ReAct 44.4 BoN SimuRA 45.6 ReasoningBank 38.0 37.2 Synapse 41.2 AWM 49.2 WorldMind Open-source and Proprietary Models 54 28 68 64 34 40 48 42 42 36 46 32 48 40 44 44 36 32 36 68 36 70 72 44 39 46 24 68 58 28 16 Method Comparison 52 50 54 44 40 48 56 46 50 54 42 44 48 54 32 42 38 36 36 40 38 42 38 34 36 38 42 52 22 62 52 32 49 38 34 42 42 34 34 44 32 40 40 36 34 40 42 65.1 74.0 34.3 47.8 65.3 72.0 67.4 74.3 37.6 43.7 41.0 42.3 50.4 55.3 50.4 54.2 53.6 57.8 47.6 57.5 43.6 42.5 46.2 53.2 54.1 63. 47.5 55.3 49.5 50.8 52.2 61.0 42.6 46.7 42.2 41.2 46.0 48.3 55.7 61.0 60.3 35.3 66.0 66.7 37.3 35.3 53.5 46.5 47.8 41.5 51.3 39.2 52.7 42.8 48.3 50.3 38.8 37.5 42.0 61.0 64 34 68 70 38 41 52 46 50 50 38 46 50 46 52 42 40 44 50 74.0 43.5 76.7 76.5 49.2 43.3 55.3 56.5 59.7 47.0 42.7 50.7 61.0 52.2 54.7 58.2 45.8 49.5 52.5 58.8 58.3 33.3 63.0 62.8 35.3 35.7 42.7 52.0 48.5 44.2 42.0 47.0 41. 47.2 48.7 45.3 41.5 41.3 44.3 48.0 61.3 29.0 59.7 59.0 36.0 40.3 45.0 42.8 54.3 48.0 39.7 41.0 52.0 39.8 45.0 46.3 40.3 41.7 42.7 49.7 Table 1: Main Results on the EB-ALFRED Benchmark. We evaluate performance across five distinct capability subsets: Base, Common Sense, Complex Instruction, Visual Appearance, and Spatial Awareness. We report both Success Rate (SR) and Goal Condition (GC) scores. The best results for each model group are highlighted in bold."
        },
        {
            "title": "4.2 Main Results",
            "content": "perience effectively guides long-horizon behavior. WorldMind consistently outperforms baselines in strict task completion. As shown in Table 1 and Table 2, WorldMind achieves the highest Success Rate (SR) across both benchmarks. On EBHabitat with the GPT-4.1-mini backbone, it attains an SR of 50.8%, outperforming ReAct by 9.2%, while on EB-ALFRED with GPT-3.5-turbo, SR improves from 44.4% to 48.0%. Given that SR strictly measures complete task success, these gains demonstrate WorldMinds superior ability to overcome physical dead-ends and reach final goal states more consistently in practice. WorldMind achieves superior procedural correctness. As shown in Table 1 and Table 2, beyond final outcomes, WorldMind significantly outperforms baselines on the Goal-Conditioned Success (GC) metric, which evaluates the correct execution of intermediate steps. On EB-ALFRED with GPT-3.5-turbo, GC improves from 50.4% to 63.0%, while on EB-Habitat it reaches 57.2%, surpassing ReAct by large margin. These gains indicate that even when full task completion fails, WorldMind executes higher proportion of valid subgoals, demonstrating that the distilled Goal ExRobustness across diverse capabilities and backbones. Our framework exhibits consistent stability across fine-grained capability dimensions, particularly in challenging subsets such as Visual Appearance and Spatial Awareness where traditional baselines often falter. For instance, in the EB-Habitat Base subset, the Success Rate reaches 86% for both GPT-3.5-turbo and GPT4.1-mini backbones, indicating that the alignment benefits are largely model-agnostic. This stability is attributed to the Process Experience constructed via the Predict-Act-Verify loop, which effectively filters physical hallucinations and ungrounded guesses across different model scales. To further validate generalization across action granularity, we extend the evaluation to low-level navigation tasks in EB-Navigation (see Appendix A), where the consistently strong performance demonstrates effective grounding from high-level reasoning to precise physical execution."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Table 3 presents the ablation results isolating the specific impact of each component. We observe that Goal Experience primarily enhances the 5 Backbone Method Success Rate (SR) Goal Condition (GC) Avg Base Common Complex Visual Spatial Avg Base Common Complex Visual Spatial ReAct GPT-4o ReAct GPT-4o-mini Claude-3.7-Sonnet ReAct ReAct Gemini-1.5-Pro Llama-3.2-90B-Vis ReAct ReAct InternVL2.5-78B 58.0 36.4 61.2 57.2 45.6 47.6 GPT-3.5-turbo GPT-4.1-mini 43.6 ReAct 43.6 BoN 48.0 SimuRA ReasoningBank 46.4 47.2 Synapse 46.4 AWM 48.8 WorldMind 41.6 ReAct 44.4 BoN SimuRA 48.4 ReasoningBank 46.4 45.6 Synapse 43.6 AWM 50.8 WorldMind Open-source and Proprietary Models 44 22 58 52 24 30 30 34 40 40 40 42 42 26 32 42 42 42 32 44 56 32 58 48 50 68 22 62 56 32 34 Method Comparison 34 36 36 40 38 44 44 32 36 40 30 38 38 44 38 40 44 42 42 36 40 40 46 44 52 34 44 36 32 38 38 28 32 34 34 38 34 38 32 32 36 34 32 32 36 30 34 70.7 90.7 44.0 77.5 70.8 97.5 61.0 92.5 50.6 94.5 55.2 82.0 50.4 84.0 50.8 75.5 55.3 83.0 53.9 78.5 54.2 85.0 50.9 78.0 56.7 89.0 47.4 76.0 50.9 75.8 55.4 85.0 53.7 77.0 52.0 78.3 50.8 74.8 57.2 87. 56.0 32.5 68.5 53.5 32.5 43.0 34.0 35.5 47.0 41.5 43.0 43.0 47.0 30.0 38.0 45.5 45.5 46.5 37.0 48.5 86 74 90 92 94 84 82 74 82 76 84 78 86 74 74 84 76 78 74 68.0 42.0 79.5 49.5 53.0 59.0 40.0 43.0 40.0 44.0 43.0 48.0 51.0 37.0 41.5 47.0 34.5 45.5 44.5 51.5 75.2 33.1 72.0 59.4 39.7 63.9 43.4 46.9 48.9 46.2 46.3 39.7 49.8 43.7 48.9 48.2 58.4 39.7 47.9 52. 62.1 57.8 43.8 50.0 59.6 45.1 50.4 53.3 57.8 59.5 53.5 45.7 46.8 50.3 50.5 51.5 53.3 50.0 49.7 46.7 Table 2: Main Results on the EB-Habitat Benchmark. We present the comparative results of Success Rate (SR) and Goal Condition (GC) across different backbones and baselines. Performance is evaluated on five subsets: Base, Common, Complex, Visual, and Spatial. The best results for each model group are highlighted in bold. Goal Condition (GC) metric, particularly on EBALFRED with GPT-3.5-turbo. This indicates that high-level strategies effectively guide the agent through correct intermediate subgoals, preventing navigational drift. In contrast, Process Experience drives larger improvement in the strict Success Rate (SR). By internalizing physical boundaries, this component acts as safeguard against fatal execution errors, ensuring that planned actions are not just logically sound but physically feasible. The superior performance of the full model confirms the synergy between these components: effective alignment necessitates combining goalbased heuristics with physical verification."
        },
        {
            "title": "Analysis",
            "content": "mini. As shown in Figure 3a, sharing experience yields significant gains over ReAct baselines for both models. Specifically, GPT-3.5turbo using the repository from GPT-4.1-mini improves on EB-ALFRED (SR: 44.4%48.8%, GC: 50.4%57.0%), demonstrating that discovered heuristics are effectively interpretable across architectures. Conversely, GPT-4.1-mini utilizing experience from GPT-3.5-turbo achieves 10.3% SR surge on EB-Habitat (41.6%51.9%). This bidirectional success confirms that constructed experiences effectively decouple environmental dynamics from model-specific biases. These results validate the cross-model transferability of the world knowledge constructed by WorldMind, offering foundational possibility for constructing shared world models for multi-agent collaboration in broader settings. Externalizing world dynamics into symbolic knowledge ensures independence from specific architectures, unlike implicit knowledge locked in parameter weights. Our World Knowledge Repository captures semantic causal rules universally valid across agents. To verify this, we exchanged repositories between GPT-3.5-turbo and GPT-4.1-"
        },
        {
            "title": "5.2 Cross-Environment Analysis",
            "content": "To evaluate scalability, we extend our experiments to the Embodied Web Agent (Hong et al., 2025) benchmark. This setting requires dynamic context switching between web interface (for information seeking) and an embodied environment (for physical execution). We use the Indoor Cooking 6 (a) Performance of Cross-Model Experience Transfer (b) Accuracy Analysis on Embodied Web Agent Benchmark (c) Error Analysis on Embodied Web Agent Benchmark Figure 3: Experimental Results and Analysis. (a) Comparison of experience transfer capabilities between the two models. (b) Performance comparison on the Embodied Web Agent task, reporting accuracy metrics for both GPT3.5-turbo and GPT-4.1-mini. (c) Comparative error distribution analysis for both models in the same environment."
        },
        {
            "title": "Components",
            "content": "EB-ALFRED EB-Habitat"
        },
        {
            "title": "Goal Process",
            "content": "SR GC"
        },
        {
            "title": "SR GC",
            "content": "GPT-3.5-turbo GPT-4.1-mini - - - - - - - - 44.4 44.8 42.4 48.0 41.2 48.8 46.4 49.2 47.5 51.0 47.5 54.1 47.5 56.3 51.5 55. 43.6 47.2 45.2 48.8 41.6 48.4 47.6 50.8 50.4 54.6 51.3 56.7 47.4 55.7 54.0 57.2 Table 3: Ablation study on the effectiveness of Goal Guidance and Process Supervision. We report both SR and GC on EB-ALFRED and EB-Habitat. cessfully navigates the transition between environments without premature termination. These results validate WorldMinds generalization to hybrid environments, ensuring robust performance across complex digital and physical context switching."
        },
        {
            "title": "5.3 Error Analysis",
            "content": "Figure 4 details failure modes: Invalid Actions (physical violations), Timeout (exhausted steps), and Wrong Termination (logical misalignment). In EB-Habitat, WorldMind reduces Invalid Actions for GPT-3.5-turbo from 105 to 67 (Figure 4a), confirming Process Experience acts as an effective physical filter. Consequently, Timeouts rise from 4 to 30, indicating that avoiding immediate fatal errors enables prolonged exploration. In EBALFRED, Wrong Terminations for GPT-4.1-mini drop from 46 to 19 (Figure 4d), demonstrating that Goal Experience provides meta-level guidance (a) Habitat (GPT-3.5-turbo) (b) Habitat (GPT-4.1-mini) (c) ALFRED (GPT-3.5-turbo) (d) ALFRED (GPT-4.1-mini) Figure 4: Error Analysis in EB-ALFRED and EBHabitat. Comparison of error distributions between the ReAct baseline and WorldMind. Failures are categorized into three types: Invalid Actions, Timeout, and Wrong Termination. environment with 112 sampled tasks to benchmark GPT-3.5-turbo and GPT-4.1-mini. Performance is quantified using four metrics: Embodied Accuracy and Web Accuracy assess domain-specific success; Overall Accuracy requires success in both domains; and Completion Rate awards partial credit for intermediate steps to measure process adherence. Figure 3b illustrates WorldMinds robustness in this cross-domain setting. For GPT-3.5-turbo, the Completion Rate more than doubles (17.02% to 39.99%), indicating improved trajectory maintenance. Similarly, GPT-4.1-mini sees significant gain, rising from 21.88% to 41.50%. Regarding failure modes, Figure 3c reveals that WorldMind effectively mitigates physical hallucinations. By reducing execution errors, it ensures the agent suc7 to prevent premature quitting. This redistribution of errors empirically validates that WorldMind successfully transforms fatal physical and logical failures into manageable planning challenges, thereby enhancing overall robustness."
        },
        {
            "title": "6 Related Work",
            "content": "Agentic AI. Agentic AIs core is reflective, selfcorrecting loop, leveraging tools, memory, and constraints for open-world robustness and interpretability (Zhou et al., 2025b; Wang et al., 2024b; Xi et al., 2025; Sang et al., 2025; Guan et al., 2024a). The field advances via improved planning mechanisms like CoT, ToT, and self-refinement (Wei et al., 2022; Yao et al., 2023; Madaan et al., 2023; Shinn et al., 2023). To interact with the external world and achieve long-term goals, researchers have explored memory, tool-calling, embodied control, and the collaboration between agents and world models, etc (Wang et al., 2023; Schick et al., 2023; Zitkovich et al., 2023; Park et al., 2023; Patil et al., 2024; Yu et al., 2025c; Deng et al., 2025; Chen et al., 2025b; Mei et al., 2025; Fung et al., 2025; Tang et al., 2025). Specifically, while extensive related work (Zhou et al., 2025c; Zhang et al., 2025f; Yan et al., 2025; Zeng et al., 2024; Bo et al., 2025; Anokhin et al., 2024; Zhang et al., 2025g; Xu et al., 2025; Yang et al., 2025b; Huang et al., 2025; Zhang et al., 2025a; Pink et al., 2025; Cao et al., 2025; Hatalis et al., 2023; Salama et al., 2025) focuses on memory, significant research (Fang et al., 2025a; Tao et al., 2024; Gao et al., 2025; Zhang et al., 2025b; Jiang et al., 2024; Yu et al., 2025a; Wei et al., 2025; Liu et al., 2025; Cai et al., 2025a; Dou et al., 2025; Cai et al., 2025b,a; Dou et al., 2025) aims to realize self-evolving agents. Further work has expanded the ability to solve complex tasks by building multi-agent collaborative frameworks and autonomous systems (Wu et al., 2024; Li et al., 2023; Wang et al., 2024a; Fourney et al., 2024; Park et al., 2024). World Models. World Models learn internal representations of environment dynamics for prediction and planning, field spanning model-based RL, generative simulation, LLMs, and robotics (Kong et al., 2025; Ha and Schmidhuber, 2018; Ding et al., 2025; Guan et al., 2024b; Zhu et al., 2024). Originating in model-based RL (Kaiser et al., 2019; Deng et al., 2022; Ha et al., 2023), world models enable agents to imagine outcomes (Hafner et al., 2023; Wu et al., 2023). Generative world models not only predict physical environment changes by generating high-fidelity video (Brooks et al., 2024; Bruce et al., 2024; Liu et al., 2024; Bar-Tal et al., 2024; Chi et al., 2025; Yu et al., 2025b; Mendonca et al., 2023; Wang et al., 2024d; Bardes et al., 2023; Chen et al., 2025a; ParkerHolder et al., 2024), but also predict code execution results (Tang et al., 2024; Carbonneaux et al., 2025), and further extend their predictive capabilities to other diverse domains (Feng et al., 2025; Zhang et al., 2025e). World Models now underpin applications ranging from autonomous driving (Wang et al., 2024c; Jia et al., 2023; Hu et al., 2023; Wang et al., 2024e; Zhang et al., 2023) to embodied navigation (Zhong et al., 2025; Yao et al., 2025; Bar et al., 2025; Nie et al., 2025; Jin and Jia, 2025; Long et al., 2025; Liang et al., 2025; Shi et al., 2025), demonstrating their versatility across diverse tasks. This has also led to more benchmarks focused on evaluating the precise prediction of future states, physical laws, and environmental dynamics (Li et al., 2025a; Warrier et al., 2025; Duan et al., 2025; Yue et al., 2025). (Qiao et al., 2024b) introduce World Knowledge Model (WKM) which encodes world knowledge into the planner Unlike WKM, WorldMind performs training-free, online experiential alignment using execution failures. Concurrently, much research also posits that the agent itself is the world model (Tehenan et al., 2025; Zhou et al., 2024; Xiang et al., 2023; Fu et al., 2025; Li et al., 2025b; Qian et al., 2026)."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we propose WorldMind, paradigm for aligning agentic world models via experiential learning. By unifying Process Experience for physical verification and Goal Experience for heuristic guidance, our framework effectively grounds LLMbased agents in dynamic environments without the need for gradient updates. Empirical results across diverse benchmarks validate that WorldMind not only minimizes physical hallucinations but also significantly enhances complex planning capabilities. Crucially, our cross-model analysis demonstrates that the distilled world knowledge is universal, enabling experience transfer across model architectures. These findings suggest that externalizing environmental dynamics into symbolic knowledge is promising direction for developing generalist embodied agents. We believe this perspective opens new avenues for robust agentic systems with 8 reusable knowledge across tasks and embodiments."
        },
        {
            "title": "Limitations",
            "content": "Despite careful design and empirical validation, this work still exhibits several limitations that open up avenues for future exploration and enhancement. Dependence on Visual Perception Fidelity. Our framework primarily addresses the alignment of reasoning and planning with physical dynamics. However, it remains contingent on the foundational capabilities of the underlying Visual-Language Model (VLM). While Process Experience can rectify prediction errors related to interactions, it cannot fully compensate for fundamental perceptual hallucinations, such as the semantic misclassification of objects in highly cluttered scenes. Mechanistic Interpretability of Alignment Boundaries. Although we empirically demonstrate that the World Knowledge Repository effectively constrains the agents behavior, the precise mechanism by which this explicit symbolic knowledge reshapes the implicit boundaries of the agents internal world model remains to be fully mapped. Current results show that the alignment occurs, but understanding how the accumulated knowledge mathematically alters the latent transition dynamics requires deeper theoretical analysis. Future work will focus on quantifying these shifts in the decision boundary to provide more rigorous cognitive explanation. Scalability to Multi-Agent Shared World Models. While our cross-model analysis demonstrates the potential for knowledge transfer, the current framework operates on asynchronous experience sharing between individual agents. The mechanisms for real-time knowledge synchronization, conflict resolution, and consensus building required to construct unified, dynamic shared world model for simultaneous multi-agent collaboration remain to be fully explored."
        },
        {
            "title": "References",
            "content": "Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, and Evgeny Burnaev. 2024. Arigraph: Learning knowledge graph world models with episodic memory for llm agents. arXiv preprint arXiv:2407.04363. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. 2025. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1579115801. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. 2024. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111. Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. 2023. V-jepa: Latent video prediction for visual representation learning. Weihao Bo, Shan Zhang, Yanpeng Sun, Jingjing Wu, Qunyi Xie, Xiao Tan, Kunbin Chen, Wei He, Xiaofan Li, Na Zhao, et al. 2025. Agentic learner with grow-and-refine multimodal semantic memory. arXiv preprint arXiv:2511.21678. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. 2024. Video generation models as world simulators. OpenAI Blog, 1(8):1. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. 2024. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning. Yuxuan Cai, Yipeng Hao, Jie Zhou, Hang Yan, Zhikai Lei, Rui Zhen, Zhenhua Han, Yutao Yang, Junsong Li, Qianjun Pan, et al. 2025a. Building selfevolving agents via experience-driven lifelong learning: framework and benchmark. arXiv preprint arXiv:2508.19005. Zhicheng Cai, Xinyuan Guo, Yu Pei, Jiangtao Feng, Jinsong Su, Jiangjie Chen, Ya-Qin Zhang, Wei-Ying Ma, Mingxuan Wang, and Hao Zhou. 2025b. Flex: Continuous agent evolution via forward learning from experience. arXiv preprint arXiv:2511.06449. Zouying Cao, Jiaji Deng, Li Yu, Weikang Zhou, Zhaoyang Liu, Bolin Ding, and Hai Zhao. 2025. Remember me, refine me: dynamic procedural memory framework for experience-driven agent evolution. arXiv preprint arXiv:2512.10696. Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, et al. 2025. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387. Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. 2025. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539. 9 Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. 2024. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. 2025a. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074. Shiqi Chen, Tongyao Zhu, Zian Wang, Jinghan Zhang, Kangrui Wang, Siyang Gao, Teng Xiao, Yee Whye Teh, Junxian He, and Manling Li. 2025b. Internalizing world models via self-play finetuning for agentic rl. arXiv preprint arXiv:2510.15047. Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, et al. 2025. Wow: Towards world omniscient world model through embodied interaction. arXiv preprint arXiv:2509.22642. Fei Deng, Ingook Jang, and Sungjin Ahn. 2022. Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations. In International conference on machine learning, pages 49564975. PMLR. Mingkai Deng, Jinyu Hou, Zhiting Hu, and Eric Xing. 2025. Simura: world-model-driven simulative reasoning architecture for general goal-oriented agents. arXiv preprint arXiv:2507.23773. Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al. 2025. Understanding world or predicting future? comprehensive survey of world models. ACM Computing Surveys, 58(3):138. Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, Cheng Zhong, Zongzhang Zhang, et al. 2025. Evalearn: quantifying the learning capability and efficiency of llms via sequential problem solving. arXiv preprint arXiv:2506.02672. Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. 2025. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. 2025a. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407. Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. 2025b. Webevolver: Enhancing web agent selfimprovement with coevolving world model. arXiv preprint arXiv:2504.21024. Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, and Mengdi Wang. 2025. Web world models. arXiv preprint arXiv:2512.23676. Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, et al. 2024. Magentic-one: generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468. Karl Friston. 2018. Does predictive coding have future? Nature neuroscience, 21(8):10191021. Dayuan Fu, Jianzhao Huang, Siyuan Lu, Guanting Dong, Yejie Wang, Keqing He, and Weiran Xu. 2025. Preact: Prediction enhances agents planning ability. In Proceedings of the 31st international conference on computational linguistics, pages 116. Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, Hervé Jégou, Embodied ai Alessandro Lazaric, et al. 2025. arXiv preprint agents: Modeling the world. arXiv:2506.22355. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. 2025. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046. Xinyan Guan, Yanjiang Liu, Xinyu Lu, Boxi Cao, Ben He, Xianpei Han, Le Sun, Jie Lou, Bowen Yu, Yaojie Lu, et al. 2024a. Search, verify and feedback: Towards next generation post-training paradigm of foundation models via verifier engineering. arXiv preprint arXiv:2411.11504. Yanchen Guan, Haicheng Liao, Zhenning Li, Jia Hu, Runze Yuan, Guohui Zhang, and Chengzhong Xu. 2024b. World models for autonomous driving: An initial survey. IEEE Transactions on Intelligent Vehicles. David Ha and Jürgen Schmidhuber. 2018. World models. arXiv preprint arXiv:1803.10122, 2(3). Jeongsoo Ha, Kyungsoo Kim, and Yusung Kim. 2023. Dream to generalize: zero-shot model-based reinforcement learning for unseen visual distractions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 78027810. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2023. Mastering diverse doarXiv preprint mains through world models. arXiv:2301.04104. Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer. 2023. Memory matters: The need to improve long-term memory in llm-agents. In Proceedings of the AAAI Symposium Series, volume 2, pages 277280. 10 Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, and Kai-Wei Chang. 2025. Embodied web agents: Bridging physical-digital arXiv realms for integrated agent intelligence. preprint arXiv:2506.15677. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. 2023. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080. Yanping Huang and Rajesh PN Rao. 2011. Predictive coding. Wiley Interdisciplinary Reviews: Cognitive Science, 2(5):580593. Zhengjun Huang, Zhoujin Tian, Qintian Guo, Fangyuan Zhang, Yingli Zhou, Di Jiang, and Xiaofang Zhou. 2025. Licomemory: Lightweight and cognitive agentic memory for efficient long-term reasoning. arXiv preprint arXiv:2511.01448. Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang. 2023. Adriver-i: general world arXiv preprint model for autonomous driving. arXiv:2311.13549. Xun Jiang, Feng Li, Han Zhao, Jiahao Qiu, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, et al. 2024. Long term memory: The foundation of ai self-evolution. arXiv preprint arXiv:2410.15665. Li Jin and Liu Jia. 2025. Embodied world models emerge from navigational task in open-ended environments. arXiv preprint arXiv:2504.11419. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. 2019. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374. Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, et al. 2025. 3d and arXiv preprint 4d world modeling: survey. arXiv:2509.07996. Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph Gonzalez, et al. 2025a. Worldmodelbench: Judging video generation models as world models. arXiv preprint arXiv:2502.20694. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008. Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang, Cheng Qian, Zeping Li, Pony Ma, Guanhua Chen, Heng Ji, et al. 2025b. From word to world: Can large language models be implicit text-based world models? arXiv preprint arXiv:2512.18832. Wenlong Liang, Rui Zhou, Yang Ma, Bing Zhang, Songlin Li, Yijia Liao, and Ping Kuang. 2025. Large model empowered embodied ai: survey on decision-making and embodied learning. arXiv preprint arXiv:2508.10399. Genglin Liu, Shijie Geng, Sha Li, Hejie Cui, Sarah Zhang, Xin Liu, and Tianyi Liu. 2025. Webcoach: Self-evolving web agents with cross-session memory guidance. arXiv preprint arXiv:2511.12997. Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268. Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, et al. 2025. survey: Learning embodied intelligence from physical simulators and world models. arXiv preprint arXiv:2507.00917. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594. Kai Mei, Jiang Guo, Shuaichen Chang, Mingwen Dong, Dongkyu Lee, Xing Niu, and Jiarong Jiang. 2025. R-wom: Retrieval-augmented world arXiv preprint model for computer-use agents. arXiv:2510.11892. Russell Mendonca, Shikhar Bahl, and Deepak Pathak. 2023. Structured world models from human videos. arXiv preprint arXiv:2308.10901. Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, and Long Chen. 2025. Wmnav: Integrating visionlanguage models into world models for object goal navigation. arXiv preprint arXiv:2503.02247. Siru Ouyang, Jun Yan, Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long Le, Samira Daruki, Xiangru Tang, et al. 2025. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. 11 Joon Sung Park, Carolyn Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, and Michael Bernstein. 2024. Generative agent simulations of 1,000 people. arXiv preprint arXiv:2411.10109. Parker-Holder, Ball, Bruce, Dasagi, Holsheimer, Kaplanis, Moufarek, Scully, Genie 2: Shar, Shi, et al. 2024. URL: large-scale foundation world model. https://deepmind. google/discover/blog/genie-2-alarge-scale-foundation-world-model. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565. Mathis Pink, Qinyuan Wu, Vy Ai Vo, Javier Turek, Jianing Mu, Alexander Huth, and Mariya Toneva. 2025. Position: Episodic memory is the missing piece for long-term llm agents. arXiv preprint arXiv:2502.06975. Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang, Bingxiang He, Qinyu Luo, Dilek Hakkani-Tür, Gokhan Tur, Yunzhu Li, et al. 2026. Current agents fail to leverage world model as tool for foresight. arXiv preprint arXiv:2601.03905. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. 2024a. Agent planning with world knowledge model. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. 2024b. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 37:114843114871. Jonathan Richens, Tom Everitt, and David Abel. 2025. General agents need world models. In Forty-second International Conference on Machine Learning. Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, and Yassine Benajiba. 2025. Meminsight: Autonomous memory augmentation for llm agents. arXiv preprint arXiv:2503.21760. Jitao Sang, Jinlin Xiao, Jiarun Han, Jilin Chen, Xiaoyi Chen, Shuyu Wei, Yongjie Sun, and Yuhang Wang. 2025. Beyond pipelines: survey of the paradigm shift toward model-native agentic ai. arXiv preprint arXiv:2510.16720. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, and Xipeng QIu. 2025. World-aware planning narratives enhance large vision-language model planner. arXiv preprint arXiv:2506.21230. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Hao Tang, Darren Key, and Kevin Ellis. 2024. Worldcoder, model-based llm agent: Building world models by writing code and interacting with the environment. Advances in Neural Information Processing Systems, 37:7014870212. Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, et al. 2025. Agent kb: Leveraging cross-domain experience for agentic problem solving. arXiv preprint arXiv:2507.06229. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. 2024. survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387. Matthieu Tehenan, Christian Bolivar Moya, Tenghai Long, and Guang Lin. 2025. Linear spatial world models emerge in large language models. arXiv preprint arXiv:2506.02996. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291. Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, and Junshan Zhang. 2025a. Adawm: Adaptive world model based planning for autonomous driving. arXiv preprint arXiv:2501.13072. Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024a. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024b. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, and Xipeng Qiu. 2025b. World modeling makes better planner: Dual preference optimization for embodied task planning. arXiv preprint arXiv:2503.10480. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. 2024c. Drivedreamer: Towards real-world-drive world models for 12 autonomous driving. computer vision, pages 5572. Springer."
        },
        {
            "title": "In European conference on",
            "content": "Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, and Jiwen Lu. 2024d. Worlddreamer: Towards general world models for video arXiv generation via predicting masked tokens. preprint arXiv:2401.09985. Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. 2024e. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1474914759. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2024f. Agent workflow memory. arXiv preprint arXiv:2409.07429. Archana Warrier, Dat Nyugen, Michelangelo Naim, Moksh Jain, Yichao Liang, Karen Schroeder, Cambridge Yang, Joshua Tenenbaum, Sebastian Vollmer, Kevin Ellis, et al. 2025. BencharXiv preprint marking world-model learning. arXiv:2510.19788. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Tianxin Wei, Noveen Sachdeva, Benjamin Coleman, Zhankui He, Yuanchen Bei, Xuying Ning, Mengting Ai, Yunzhe Li, Jingrui He, Ed Chi, et al. 2025. Evo-memory: Benchmarking llm agent test-time learning with self-evolving memory. arXiv preprint arXiv:2511.20857. Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. 2023. Daydreamer: World models for physical robot learning. In Conference on robot learning, pages 22262240. PMLR. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. 2024. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2025. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. 2023. Language models meet world models: Embodied experiences enhance language models. Advances in neural information processing systems, 36:7539275412. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110. Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian Kersting, Jeff Pan, Hinrich Schütze, et al. 2025. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. 2025a. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560. Wei Yang, Jinwei Xiao, Hongming Zhang, Qingyang Zhang, Yanna Wang, and Bo Xu. 2025b. Coarseto-fine grounded memory for llm agent planning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1304013067. Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, and Chuang Gan. 2025c. Mindjourney: Test-time scaling with world models for spatial reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Xuan Yao, Junyu Gao, and Changsheng Xu. 2025. Navmorph: self-evolving world model for visionand-language navigation in continuous environments. arXiv preprint arXiv:2506.23468. Chenglin Yu, Yang Yu, Songmiao Wang, Yucheng Wang, Yifan Yang, Jinjia Li, Ming Li, and Hongxia Yang. 2025a. Infiagent: Self-evolving pyramid agent framework for infinite scenarios. arXiv preprint arXiv:2509.22502. Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. 2025b. Position: Interactive generative video as next-generation game engine. arXiv preprint arXiv:2503.17359. Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, and Zhou Yu. 2025c. Dyna-think: Synergizing reasoning, acting, 13 Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Loic Magne, et al. 2025. Flare: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659. Yufeng Zhong, Chengjian Feng, Feng Yan, Fanfan Liu, Liming Zheng, and Lin Ma. 2025. P3nav: unified framework for embodied navigation integrating perception, planning, and prediction. arXiv preprint arXiv:2503.18525. Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, et al. 2025a. Memento: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153. Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, and Chengqi Zhang. 2024. Wall-e: World alignment by rule learning improves arXiv preprint world model-based llm agents. arXiv:2410.07484. Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, and Chengqi Zhang. 2025b. Wall-e 2.0: World alignment by neurosymbolic learning improves world model-based llm agents. arXiv preprint arXiv:2504.15785. Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. 2025c. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841. Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. 2024. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. 2023. Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR. and world model simulation in ai agents. arXiv preprint arXiv:2506.00320. Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, and Guanghui Ren. 2025. Ewmbench: Evaluating scene, motion, and semantic quality in embodied world models. arXiv preprint arXiv:2505.09694. Ruihong Zeng, Jinyuan Fang, Siwei Liu, and Zaiqiao Meng. 2024. On the structural memory of llm agents. arXiv preprint arXiv:2412.15266. Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. 2025a. G-memory: Tracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398. Guibin Zhang, Muxin Fu, and Shuicheng Yan. 2025b. Memgen: Weaving generative latent memory for selfevolving agents. arXiv preprint arXiv:2509.24704. Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, et al. 2025c. Agent arXiv preprint learning via early experience. arXiv:2510.08558. Lunjun Zhang, Ge Yang, and Bradly Stadie. 2021. World model as graph: Learning latent landmarks for planning. In International conference on machine learning, pages 1261112620. PMLR. Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, et al. 2025d. Agentic context engineering: Evolving contexts for self-improving language models. arXiv preprint arXiv:2510.04618. Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, et al. 2025e. Dreamvla: vision-language-action model dreamed with comarXiv preprint prehensive world knowledge. arXiv:2507.04447. Yuxiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, and Jitao Sang. 2025f. Memory as action: Autonomous context curation for long-horizon agentic tasks. arXiv preprint arXiv:2510.12635. Zeyu Zhang, Quanyu Dai, Rui Li, Xiaohe Bo, Xu Chen, and Zhenhua Dong. 2025g. Learn to memorize: Optimizing llm-based agents with adaptive memory framework. arXiv preprint arXiv:2508.16629. Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. 2023. Trafficbots: Towards world models for autonomous driving simulation and motion prediction. arXiv preprint arXiv:2303.04116. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2023. Synapse: Trajectory-as-exemplar prompting with memory for computer control. arXiv preprint arXiv:2306.07863. 14 Additional Evaluation on Low-Level"
        },
        {
            "title": "Navigation Tasks",
            "content": "To further verify the generalization capability of WorldMind across different levels of action granularity, we extended our evaluation to the EBNavigation benchmark. Unlike EB-ALFRED and EB-Habitat, which primarily focus on high-level semantic planning (e.g., Pick up the apple), EB-Navigation requires the agent to control the robot using low-level atomic actions (e.g., Move forward 0.25m, Rotate 90 degrees) to locate target objects. This setting presents unique challenges in spatial reasoning and precise physical grounding. We evaluated both GPT-3.5-turbo and GPT-4.1mini backbones on four subsets: Base, Common Sense, Complex Instruction, and Visual Appearance. Task Subset GPT-3.5-turbo GPT-4.1-mini ReAct WorldMind ReAct WorldMind Base Common Sense Complex Instruction Visual Appearance Average 56.7 58.3 66.7 43.3 56.3 66.7 55.0 68.3 45.0 58. 53.3 61.7 55.0 46.7 54.2 56.7 60.0 56.7 48.3 55.4 Table 4: Success rate (%) on EB-Navigation subsets. Results are reported for four capability-oriented subsets as well as the macro-average. Performance is shown for WorldMind and the baseline methods. Analysis of Low-Level Control Generalization. Table 4 demonstrates that WorldMind effectively bridges the gap between high-level reasoning and low-level execution. For the GPT-3.5-turbo backbone, WorldMind achieves notable improvement in the Base subset (+10.0%), indicating that retrieving previously successful trajectories helps the agent maintain consistent behavior and avoid planning errors such as oscillatory movements that are commonly observed in ReAct baselines. Improvements in the Visual subset for both backbones further suggest that Process Experience helps align visual perception with physical constraints, reducing hallucinations in which the agent incorrectly assumes it has reached the target. Although gains in the Common Sense subset are mixed, the overall increase in average success rate indicates that the benefits of the proposed framework, particularly the construction of valid internal world model, 15 generalize across different action spaces and provide robust guidance for fine-grained kinematic control."
        },
        {
            "title": "B Prompts",
            "content": "B.1 WorldMind Prompt for ALFRED Prompt for WorldMind in EB-ALFRED ## You are an intelligent embodied agent operating in home environment, equipped with an internal World Model. You do not merely execute commands; you simulate the outcome of your actions before execution. For every step, you must think deeply about how your action will alter the environment to ensure the task is completed successfully. Your state prediction serves as justification for your actionproving that you understand the consequences of your move. ## Action Descriptions and Validity Rules Find: Parameterized by the name of the receptacle to navigate to. So long as the object is present in the scene, this skill is always valid Pick up: Parameterized by the name of the object to pick. Only valid if the robot is close to the object, not holding another object, and the object is not inside closed receptacle. Put down: Parameterized by the name of the object to put down to nearby receptacle. Only valid if the robot is holding an object. Drop: Parameterized by the name of the object to put down. It is different from Put down action, as this does not guarantee the held object will be put into specified receptacle. Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed and the robot is close to the receptacle. Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open and the robot is close to the receptacle. Turn on: Parameterized by the name of the object to turn on. Only valid if the object is turned off and the robot is close to the object. Turn off: Parameterized by the name of the object to turn off. Only valid if the object is turned on and the robot is close to the object. Slice: Parameterized by the name of the object to slice. Only valid if the object is sliceable and the robot is close to the object. ## The available action id (0 {}) and action names are: {}. {} ## Guidelines 1. Output Plan: Avoid generating empty plan. Each plan should include no more than 20 actions. 2. Visibility: Always locate visible object by the find action before interacting with it. 3. Action Guidelines: Make sure match the action name and its corresponding action id in the output. Avoid performing actions that do not meet the defined validity criteria. For instance, if you want to put object in receptacle, use put down rather than drop actions. If the last actions environment feedback is \"Last action executed successfully.\", you MUST NOT repeat the same action as your next step. 4. Prevent Repeating Action Sequences: Do not repeatedly execute the same action or sequence of actions. Try to modify the action sequence because previous actions do not lead to success. 5. Multiple Instances: There may be multiple instances of the same object, distinguished by an index following their names, e.g., Cabinet_2, Cabinet_3. You can explore these instances if you do not find the desired object in the current receptacle. 6. Reflection on History and Feedback: Use interaction history and feedback from the environment to refine and improve your current plan. If the last action is invalid, reflect on the reason, such as not adhering to action rules or missing preliminary actions, and adjust your plan accordingly. 7. Dynamic Reasoning from Environment Feedback: You must treat env_feedback as direct instruction. - Instruction Extraction: If feedback says \"Ladle is in CounterTop_2\", your language_plan must state: \"Feedback indicates Ladle is at CounterTop_2, navigating there now.\" - Action Alignment: Your next action MUST be \"find CounterTop_2\". Do not use generic names if specific index is provided. - Multiple Instances Handling: If the environment contains multiple instances of receptacle (e.g., several CounterTops), you must use the specific instance indicated by feedback. Failing to navigate to the correct instance (such as only using generic \"CounterTop\") will result in the target object remaining invisible or inaccessible. 8. The Anti-Loop Rule: If \"Pick up\" or \"Turn on\" action fails or the object is \"not visible\", DO NOT repeat the same action. You must change your strategy in the next plan (e.g., move to different instance, change perspective, or clear your hand). 9. Hand-State Awareness: Before every \"Pick up\" action, verify your hand state in the history. If you are holding an object, the very next action in your executable_plan MUST be \"Put down\" or \"Drop\" to clear the gripper. 10. World Model Prediction Case A/B: (Case A) If the target is VISIBLE or its specific location (e.g., CounterTop_2) is KNOWN from env_feedback, describe the specific change. (Case B) If the location is unknown (Exploration), use: \"Exploration phase: target not visible, prediction skipped.\" 11. Handle Non-Canonical Object Descriptions: When the instruction refers to an object using non-canonical or descriptive name (e.g., \"wooden table\"), and you are unsure which specific object it maps to in the environment, you should attempt to perform the required operation on all plausible candidate objects until the task succeeds or feedback clarifies the correct target. 12. Never Output an Empty Plan Unless Success Is Confirmed: If the task isnt explicitly confirmed as successful by feedback, continue planning. If you think its done but no success message appears, assume mistake was made. The output json format should be {\"lanstr, guage_plan\": \"executable_plan\": int, \"action_name\": str, List[{\"action_id\": 16 \"predicted_state\": str}...]} The fields in the above JSON follow the purpose below: 1. language_plan: Your Chain-of-Thought. You must think step-by-step based on the summarized (generalizable experiences lessons) provided in the context. Analyze the instruction, apply learned rules to avoid past mistakes, and derive logical strategy. Explain why you prioritize certain locations or actions. 2. executable_plan: list of concrete actions. Each object MUST contain: action_id, action_name, and predicted_state. - For the predicted_state field, you must strictly follow these rules: (Case A) If the target objects are VISIBLE in the current observation, describe the specific environmental and gripper change. (Case B) If the target object or destination is NOT VISIBLE (Exploration Phase), you MUST output exactly the string: \"Exploration phase: target not visible, prediction skipped.\" (Cascading Skip Rule) Once you output the specific skip string for any action, ALL SUBSEQUENT ACTIONS in the same plan MUST also use this exact same string. You cannot resume prediction after skipping it within single plan. !!! Please do not output anything other than the above-mentioned JSON, do not include json and !!! B.2 WorldMind Prompt for Habitat Prompt for WorldMind in EB-Habitat ## You are an intelligent embodied agent operating in home environment, equipped with an internal World Model. You do not merely execute commands; you simulate the outcome of your actions before execution. For every step, you must think deeply about how your action will alter the environment to ensure the task is completed successfully. Your state prediction serves as justification for your actionproving that you understand the consequences of your move. 17 **Core Philosophy: Simulate (Physics + Semantics) -> Validate -> Execute** Before selecting any action, you must mentally simulate its outcome on two levels: 1. Physical Feasibility: Can actually perform this action? (e.g., hands full). 2. Semantic Plausibility: Does this action make sense for the task? (e.g., searching for pillow in the bathroom is semantically invalid). Your predicted_state is the logical prerequisite that justifies why the selected action is the correct next step. ## Action Descriptions and Validity Rules Navigation: Parameterized by the name of the receptacle to navigate to. So long as the receptacle is present in the scene, this skill is always valid. Pick: Parameterized by the name of the object to pick. Only valid if the robot is close to the object, not holding another object, and the object is not inside closed receptacle. Place: Parameterized by the name of the receptacle to place the object on. Only valid if the robot is close to the receptacle and is holding an object. Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed and the robot is close to the receptacle. Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open and the robot is close to the receptacle. ## The available action id (0 {}) and action names are: {}. {} ## Guidelines 1. Output Plan: Avoid generating empty plan. Each plan should include no more than 20 actions. 2. Visibility: If an object is not currently visible, use the \"Navigation\" action to locate it or its receptacle before attempting other operations. 3. Action Validity: Make sure match the action name and its corresponding action id in the output. Avoid performing actions that do not meet the defined validity criteria. 4. Prevent Repeating Action Sequences: Do not repeatedly execute the same action or sequence of actions. Try to modify the action sequence because previous actions do not lead to success. 5. Multiple Instances: There may be multiple instances of the same object, distinguished by an index following their names, e.g., cabinet 2, cabinet 3. You can explore these instances if you do not find the desired object in the current receptacle. 6. Reflection on History and Feedback: Use interaction history and feedback from the environment to refine and enhance your current strategies and actions. If the last action is invalid, reflect on the reason. 7. World Model Prediction: For EACH action in your executable_plan, you MUST include predicted_state. - Explain via Prediction: This prediction is your rationale. By describing the expected future, you prove this action moves you closer to the goal. - Visual Specifics: Describe exactly what the robot will see and hold *immediately after* the action. 8. Prioritize Likely Locations via Semantic Simulation: Do not search randomly. Before navigating, run semantic simulation in your World Model: - Step (Hypothesis): \"Could target object be at location Y?\" - Step (Common Sense Check): Use everyday knowledge. - *Example 1*: Target is \"airplane\" (toy). Candidate is \"sink\". -> Simulation Result: Very Unlikely. -> Decision: REJECT. - *Example 2*: Target is \"airplane\". Candidate is \"living room table\". -> Simulation Result: Likely. -> Decision: ACCEPT. - Action: Only generate Navigation actions for locations that pass this \"Common Sense Check.\" 9. Exhaustive Local Search (The Left/Right Rule): Many receptacles have multiple parts (e.g., \"Kitchen Counter Left\" and \"Kitchen Counter Right\"). - If you navigate to one side (e.g., Left) and the object is NOT there, your immediate next step must be to check the other side (e.g., Right) before leaving the room. - Do not jump to different room until you have checked all connected segments of the current furniture. 10. Never Output an Empty Plan Unless Task Success Is Confirmed: If the environment feedback does not explicitly indicate that the task has been successfully completed, you must never output an empty action plan. Always carefully check your action history and environment feedback. If you believe the task is finished but have not received success confirmation, assume there was mistake and continue planning actions to achieve the goal. The output json format should be {\"lan- \"executable_plan\": guage_plan\": str, List[{\"action_id\": int, \"action_name\": str, \"predicted_state\": str}...]} The fields in the above JSON follow the purpose below: language_plan is for your Chain-of1. Thought. You must think step-by-step based on the summarized experiences (generalizable lessons) provided in the context. Analyze the instruction, apply these learned rules to avoid past mistakes, and derive logical solution strategy. Explicitly explain your reasoning for prioritizing certain locations or actions based on these experiences. executable_plan is list of concrete 2. actions to be executed. Each object in the list MUST contain: action_id, action_name, and predicted_state. - For the \"predicted_state\" field, you must strictly follow these rules: (Case A) If the target objects are VISIBLE in the current observation OR their location is KNOWN from interaction history, describe the specific environmental change. (Case B) If the target object or destination is NOT VISIBLE AND location is NOT KNOWN from history (Exploration Phase), you MUST output exactly the string: \"Exploration phase: target not visible, prediction skipped.\" (Cascading Skip Rule) Once you output the specific skip string for any action, ALL SUBSEQUENT ACTIONS in the same list MUST also use this exact same string. You cannot resume prediction after skipping it 18 within single plan. !!! Please do not output anything other than the above-mentioned JSON, do not include json and !!!"
        }
    ],
    "affiliations": [
        "University of California, Los Angeles",
        "Zhejiang University"
    ]
}