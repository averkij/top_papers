{
    "paper_title": "Multimodal Referring Segmentation: A Survey",
    "authors": [
        "Henghui Ding",
        "Song Tang",
        "Shuting He",
        "Chang Liu",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation."
        },
        {
            "title": "Start",
            "content": "1 Multimodal Referring Segmentation: Survey Henghui Ding, Song Tang, Shuting He, Chang Liu, Zuxuan Wu, Yu-Gang Jiang, Fellow, IEEE AbstractMultimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides comprehensive survey of multimodal referring segmentation. We begin by introducing this fields background, including problem definitions and commonly used datasets. Next, we summarize unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation. Index TermsSurvey, Multimodal Referring Segmentation, Referring Expression Segmentation, Referring Video Object Segmentation, Referring Audio-Visual Segmentation, 3D Referring Expression Segmentation, Multimodal Learning, Vision-Language"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "M ULTIMODAL referring segmentation [1], [2], [3], [4], [5], [6], [7] aims to segment the target object in visual scene of image [2], [3], video [1], [8], or 3D [7], [9] according to referring expression, such as free-form text or audio. For example, as shown in Fig. 1(b), given the text referring expression The bird flying away, the model is expected to segment and track the described target object in the video. This task presents fundamental and challenging problem in multimodal understanding and supports wide range of practical applications, such as image/video editing [10], [11], robotics [12], autonomous driving [13], etc. Because of its significant potential in practical applications, multimodal referring segmentation has received growing attention in recent years, as shown in Fig. 3. Segmentation [14], [15], [16] is one of the fundamental tasks in computer vision, forming the basis for many visual understanding tasks and applications [17]. Classic segmentation methods, e.g., semantic segmentation [14] and instance segmentation [15], typically segment the given visual scenes into set of predefined categories. Although open-vocabulary segmentation [18] expands the category coverage, it remains reliant on explicit category names, e.g., person and car. Different from these classical segmentation tasks, referring segmentation enables more flexible and userfriendly segmentation by leveraging free-form referring expressions to identify specific target objects within scene. referring expression is human-understandable linguistic construct used to describe an object in any way that uniquely and unambiguously identifies it. Such expressions are not limited to naming object categories. They may refer to the target objects position, visual attributes, motion, or relationships with other objects. As long as the expression leads to an unambiguous identification of the target, any descriptive strategy is considered valid. This high degree of expressive freedom introduces considerable challenges for fineH. Ding, S. Tang, Z. Wu, Y.G. Jiang are with Fudan University, China. henghui.ding@gmail.com S. He is with Shanghai University of Finance and Economics, China. C. Liu is with ByteDance Inc. Fig. 1: Multimodal Referring Segmentation. grained multimodal understanding and alignment. It also raises requirements for model robustness against diverse expression styles and linguistic-visual variations. Depending on the modality of the referring signal (e.g., text or audio) and the type of visual scene (e.g., image, video, auditory video, or 3D), referring segmentation can be further categorized into different tasks, as shown in Fig. 1. Despite the inherent homogeneity shared across different referring segmentation tasks, most existing surveys [24], [25], [26], [27], [28] remain limited in scope, often concentrating on isolated modalities or specific tasks. For example, recent survey [29] focuses exclusively on referring expression segmentation within 2D images, while neglecting extensions to video and 3D scenes. As result, critical gap remains in the literature due to the absence of comprehensive survey that systematically covers the diverse task formulations, input modalities, and challenges within referring segmentation. Addressing this gap is essential for fostering deeper understanding of the field and for advancing the 5 2 0 2 1 ] . [ 1 5 6 2 0 0 . 8 0 5 2 : r 2 (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (2.1) (2.2) (3.1) (3.2) (3.3) (3.4) (3.5) (3.6) (4.1) (4.2) (4.3) (4.4) (4.5) (4.6) (4.7) (5.1) (5.2) (5.3) (5.4) (A.1) (A.4) (6.1) (6.2) (6.3) (Appendix) (A.2) (A.5) (7.1) (7.2) (A.3) (A.6) Fig. 2: Overview of this survey. Different colors represent specific sections. Best viewed in color. Organization. An overview of the survey is shown in Fig. 2. We begin with background on problem definitions and datasets in Sec. 2, followed by unified meta architecture in Sec. 3 that spans various referring segmentation tasks. Based on this framework, representative methods across image, video, and 3D scenes are systematically reviewed in Sec. 4 to Sec. 7. Considering the real-world complexity, we further discuss Generalized Referring Expression (GREx) in Sec. 8. Related tasks and applications are explored in Sec. 9, followed by the conclusion and discussion in Sec. 10. Benchmark results are provided in the Appendix."
        },
        {
            "title": "2.1.1 A Unified Formulation\nThe primary objective of this survey is to provide a systematic\ninvestigation of the family of referring segmentation tasks. To\nthis end, we propose a unified formulation that generalizes across\ndifferent task variants within this domain. Specifically, let X and\nY denote the input and output spaces, respectively. The objective\nis to learn an optimal mapping function f defined as:",
            "content": "f : 7 Y, where = E, (1) where is typically instantiated as neural network. The input space consists of two components: the visual input (e.g., image, silent video, auditory video, or 3D data), and the referring signal (e.g., text, audio, etc.) that specifies the target object(s) of interest. The output space consists of segmentation mask(s) of the referred entities within V. Based on this unified formulation, we construct comprehensive taxonomy of referring segmentation tasks and formally define each task in the following sections."
        },
        {
            "title": "2.1.2 Image Scene\n• Referring Expression Segmentation (RES). RES, also known as\nReferring Image Segmentation (RIS), aims to segment the target\nobject in a given image I ∈ RH×W ×3 according to a natural\nlanguage referring expression E. The output is a binary mask\nM ∈ {0, 1}H×W that precisely delineates the object referred to by\nE. Compared to semantic or instance segmentation tasks that rely\non predefined object categories, RES introduces new challenges\nin comprehending the linguistic content of E and reasoning about\nthe complex visual relationships among objects in the scene, such\nas spatial configurations, attributes, and interactions.",
            "content": "Fig. 3: Publication statistics of multimodal referring segmentation papers in top conferences/journals of computer vision, machine learning, and artificial intelligence, collected up to July 2025. [8] [22] [23] [3] [7] [1] [21] [19] [20] Fig. 4: Overview of Multimodal Referring Segmentation tasks. Representative works are cited in the top-left corners of each task. development of generalizable and multimodal solutions. To this end, we conduct comprehensive review of over 600 papers in the field of multimodal referring segmentation. This survey seeks to unify diverse referring modalities across various visual scenes. Our goal is to offer cohesive and structured understanding of the field to enhance accessibility and facilitate cross-task insights. In addition, we highlight practical applications of referring expression techniques, demonstrating their transformative potential in emerging domains such as embodied AI. Scope. This survey focuses on recent advances in referring segmentation across three major visual scenes: image-based, videobased (including salient and auditory videos), and 3D-based scenarios, along with three primary referring modalities: text, audio, and omnimodal, as shown in Fig. 4. It primarily reviews deep learning-based methods, highlighting influential works published in top-tier conferences and journals, along with recent preprints that reflect emerging trends and future directions. 3 Fig. 5: Classic Referring Expression Segmentation (RES) and Comprehension (REC) handle expressions that refer to single target object, as shown in example (1). In contrast, Generalized Referring Expression Segmentation (GRES) [2] and Comprehension (GREC) [30], [31] support expressions referring to any number of target objects, including multi-target expressions like (2) and (3), as well as no-target expressions such as (4), thereby enhancing their applicability in complex and diverse real-world scenarios. Reasoning Segmentation. Reasoning Segmentation is special case of RES where the referring expression requires indirect reasoning rather than explicit object descriptions, e.g., the food with the most Vitamin C. Recent advances in LLMs/MLLMs have made it feasible to handle such expressions by leveraging their strong reasoning and commonsense capabilities. desired output is sequence of binary masks = {Mt}T t=1, where each Mt {0, 1}HW denotes the pixel-level segmentation of the referred object in frame Vt. The ability to handle diverse multimodal expressions makes OmniAVS both practical for real-world applications and well-suited for advancing omnimodal models with fine-grained perceptual capabilities."
        },
        {
            "title": "2.1.3 Video Scene\n• Referring Video Object Segmentation (RVOS). RVOS extends\nreferring segmentation to the video domain. Given a video V =\nt=1 with T frames, where each frame Vt ∈ RH×W ×3, and\n{Vt}T\na natural language referring expression E, the goal of RVOS is\nto generate a sequence of binary masks M = {Mt}T\nt=1, where\neach Mt ∈ {0, 1}H×W denotes the pixel-level segmentation of the\nreferred object in frame Vt. Compared to RES, RVOS introduces\nadditional challenges, such as maintaining temporal consistency\nacross frames, dealing with occlusions and appearance variations,\nand tracking the referred object despite partial or full occlusion.\n• Audio-Visual Segmentation (AVS). AVS aims to segment the\nsound-emitting objects throughout an auditory video. Given an\nauditory video {V, A}, where V = {Vt}T\nt=1 denotes T visual\nframes and A = {At}T\nt=1 is the corresponding audio stream, the\ngoal of AVS is to predict binary masks M = {Mt}T\nt=1, with each\nMt ∈ 0, 1H×W highlighting the regions in Vt associated with\nthe sound source in At. AVS is a special case of referring video\nsegmentation, where the query is implicitly defined as “segment\nthe sound-emitting objects in the video.”\n• Referring Audio-Visual Segmentation (Ref-AVS). Ref-AVS\ntarget objects in an auditory video {V, A}\naims to segment\naccording to a text referring expression E. The desired output\nis a sequence of binary masks M = {Mt}T\nt=1, where each\nMt ∈ {0, 1}H×W is the pixel-level segmentation of the referred\nobject in frame Vt. Ref-AVS enables handling scenarios that are\ndifficult to address in AVS and RVOS, e.g., “segment the person\nsinging bass in the a cappella group”. This poses the importance\nof leveraging multi-modal cues to guide visual segmentation.\n• Omnimodal Referring Audio-Visual Segmentation (Omni-\nAVS). OmniAVS aims to segment specific target objects in an\nauditory video {V, A} according to a multimodal expression E\nthat flexibly combines text, speech, sound, and visual cues. The",
            "content": "2.1.4 3D Scene 3D Referring Expression Segmentation (3D-RES). 3D-RES aims to segment the target object within 3D scene based on referring expression E. Given 3D point cloud consisting of points, denoted as = {Pi}N i=1, and referring expression E, the objective is to produce binary mask {0, 1}N that identifies the subset of points corresponding to the object referred to by E. Compared to 2D RES on structured image grids, 3D-RES involves segmenting target points in unordered, irregular, and sparse point clouds, requiring both effective languagevision alignment and deep understanding of geometric structures. Referring 3D Gaussian Splatting Segmentation (R3DGS). Given scene with multi-view RGB images = {Ii}S i=1 and corresponding referring expressions = {El}L l=1 during training, R3DGS aims to segment the target object in novel-view image RHW 3 of the scene based on given expression E. The output is binary mask {0, 1}HW that delineates the target object, potentially under occlusion. Unlike conventional 2D referring segmentation, R3DGS focuses on 3D scene reconstructed from multi-view images. Compared to existing 3D referring tasks that rely on point clouds and 3D mask supervision, R3DGS learns from 2D images without requiring explicit 3D annotations, offering more scalable and annotation-efficient paradigm."
        },
        {
            "title": "2.1.5 GREx\n• Generalized Referring Expression Segmentation (GRES).\nAs shown in Fig. 5, GRES [2] extends the scope of referring\nsegmentation by allowing expressions to refer to any number of\ntarget objects. Given a visual input V and a referring expression E,\nGRES aims to predict a binary mask M covering all relevant pixels\nor points corresponding to the described target object(s). Unlike\nconventional settings that focus solely on single object, GRES\nsupports single-target, multi-target, and no-target expressions,",
            "content": "TABLE 1: Representative Datasets for Referring Segmentation and Their Characteristics Dataset #Imgs/Vids/3D Scenes #Expressions #Objects Characterization Image Scene ReferItGame [6] RefCOCO [32] RefCOCO+ [32] RefCOCOg [32] PhraseCut [33] ReasonSeg [19] Video Scene MeViS [1] Ref-DAVIS16 [34] Ref-DAVIS17 [34] A2D Sentences [35] J-HMDB Sentences [35] Refer-Youtube-VOS [38] ReVOS [39] ReasonVOS [40] Audio-Video Scene AVSBench-S4 [41] AVSBench-MS3 [41] AVSBench-Semantic [42] Ref-AVS [21] OmniAVS [8] 3D Scene GREx ScanRefer [43] Nr3D [44] Sr3D [44] Instruct3D [7] gRefCOCO [2] Ref-ZOM [45] Multi3DRefer [46] Multi3DRes [9] 19,894 19,994 19,992 25,799 77,262 1,218 2,006 50 90 3,782 928 3,975 1,042 4,932 424 12,356 4,002 2,098 800 707 1,273 280 19,994 55,078 800 800 130,525 142,209 141,564 95,010 345,486 - 28,570 100 1,544 6,656 928 27,899 35,074 458 - - - 20,261 59, 51,583 41,503 83,572 2,565 278,232 90,199 61,926 61,926 96,654 50,000 49,856 49,822 - - Focusing on real-world expressions but is limited by simpler descriptions. Allowing both locationand appearance-based references in images. Focusing on appearance-based expressions without location-based descriptions. Including longer and more complex expressions without restrictions on location. Focusing on fine-grained expressions covering categories, attributes, and relationships in diverse scenes. Focusing on implicit, reasoning-based expressions requiring world knowledge and complex understanding. 8,171 50 205 4,825 928 7,451 - - - - - 6,888 4, 11,046 5,878 11,375 - 60,287 74,942 11,609 11,609 Focusing on motion attributes and enabling multi-object expressions for enhanced RVOS challenges. Containing single-object video sequences annotated with referring expressions. Presenting more challenging multi-object scenarios involving occlusions and distractors. Enriching A2D [36] with elaborate natural language descriptions for actor and action segmentation. Supplementing J-HMDB [37] with natural language descriptions, mainly concerning human actions. Offering pixel-level RVOS across multiple object categories. Focusing on implicit, complex text queries requiring world knowledge and video context for segmentation. Evaluating models reasoning ability using complex language queries and world knowledge. Focusing on individual sound-making objects in semi-supervised single sound source AVS tasks. Focusing on concurrent sound sources in fully supervised multiple sound source AVS scenarios. Offering semantic annotations for fully supervised audio-visual semantic segmentation. Providing pixel-level annotations for objects described in corresponding multimodal-cue expressions. Providing 8 different omnimodal expressions flexibly consisting of text, speech, sound, and image. Pioneering the first large-scale dataset for object localization via natural language expressions. Providing human-annotated descriptions for precise 3D object localization in real-world scenes. Offering synthetically generated expressions with simplified language patterns. Supporting 3D instruction segmentation from complex texts, including both multiand zero-target scenes. Extending RefCOCO [32] by supporting multi-target and no-target expressions. Supporting GRES tasks with annotations built on COCO dataset. Extending ScanRefer [43] with zero/single/multiple target descriptions for supporting 3D-GREC task. Adapting Multi3DRefer [46] with instance masks to support 3D-GRES task. improving models adaptability in real-world scenarios. This generalization introduces new challenges, particularly in achieving precise alignment between different modalities when dealing with ambiguous, descriptive, or compositional expressions. Generalized Referring Expression Comprehension (GREC). Parallel to GRES, GREC [30] is introduced to expand the scope of the classic REC task, see Fig. 5. In contrast to classic REC that generates single bounding box for sentence, GREC pursues the generation of collection of bounding boxes within the input V, denoted as = {Bi}, wherein each bounding box Bi R4 encloses an object among the entirety of target objects indicated by the expression E. The number of bounding boxes may vary from 0 to multiple, depending on the given expression. Beyond image scenes, GRES and GREC can be applied to video and 3D scenes, leading to task variants such as Video-GRES [1], AV-GRES [8], and 3D-GRES [9], expanding the applicability of referring segmentation to real-world scenarios."
        },
        {
            "title": "2.2 Datasets",
            "content": "We briefly introduce commonly used referring segmentation datasets. TABLE 1 lists more datasets and summarizes their key characteristics, and Fig. 6 presents some representative examples."
        },
        {
            "title": "2.2.1 Image Scene\n• ReferItGame [6] contains 130,525 referring expressions for\n96,654 objects across 19,894 images. As the first large-scale\ndataset for referring expression understanding, it is a valuable\nresource despite its relatively simple language expressions.\n• RefCOCO/+/g [32] contains 142,209 expressions for 50,000\nobjects (RefCOCO), 141,564 expressions for 49,856 objects (Re-\nfCOCO+), and 104,560 expressions for 54,822 objects (Ref-\nCOCOg). These datasets include over 19,000 images for Re-\nfCOCO and RefCOCO+, and 26,711 images for RefCOCOg.\nRefCOCO allows the use of spatial clues. RefCOCO+ limits\nthe use of such terms, focusing more on the visual attributes.\nRefCOCOg contains longer and more complex expressions.",
            "content": "ReasonSeg [19] sets benchmark for the reasoning segmentation. It includes 1,218 image-instruction-mask samples with implicit text instructions and target masks. Beyond the above, several other datasets [33], [47], [48], [49], [50], [51], [52], [53] contribute unique challenges to RES research."
        },
        {
            "title": "2.2.3 Auditory Video Scenes\n• AVSBench-object [41] provides two settings: Single-source\n(S4) includes 4,932 videos from 23 categories with sparse su-\npervision (only the first frame labeled), while Multi-source (MS3)\ncontains 424 fully annotated videos with multiple sound sources.\n• Ref-AVS [21] provides 20,261 text expressions for 6,888 objects\nin 4002 auditory videos. The text expressions adopt multimodal\ncues to describe objects in audio-visual scenes.\n• OmniAVS [8] provides 59,458 omnimodal expressions for 4,262\nobjects in 2,098 auditory videos. The expressions integrate 4\ndifferent modalities: text, speech, sound, and image, forming 8",
            "content": ""
        },
        {
            "title": "3 META ARCHITECTURE\n3.1 Paradigm",
            "content": "Two-Stage Paradigm. Two-stage referring segmentation methods [57], [58], [59], [60], [61] follows sequential process: the model first generates mask or tracklet proposals covering all objects in the scene or across the video, then matches these proposals to the referring expression, and finally selects the bestmatched mask as the final prediction. These region proposals are typically obtained using off-the-shelf, well-trained instance segmentation models [15]. These candidate masks are evaluated based on their feature similarity to the referring expression. These two-stage methods, e.g., TGNN [58] and MAttNet [57], have been widely adopted in early referring segmentation methods due to their modularity and interpretability. However, this paradigm is susceptible to error propagation, where inaccuracies in the proposal stage directly affect final performance. Moreover, it often incurs higher computational costs, limiting its practicality for realtime or resource-constrained applications. One-Stage Paradigm. One-stage methods [3], [62], [63], [64], [65], [66] addresses the limitations of the two-stage methods by directly predicting the target object from the input visual scene and referring expression in single forward pass. This end-to-end architecture eliminates the need for separate proposal generation and matching steps, potentially reducing error propagation and improving efficiency. One-stage methods typically employ dense prediction mechanisms where each spatial location in the feature map interacts with the referring expression. Following DETR [67], recent advances in transformer-based architectures, e.g., VLT [3], [5] and ReLA [2], have significantly enhanced the performance by enabling more effective multimodal fusion and context modeling. These models can better capture the complex relationships between visual elements and linguistic descriptions, leading to more accurate segmentation results across various visual scenes."
        },
        {
            "title": "3.2 Feature Extraction",
            "content": "Vision Encoder. Vision encoder extracts visual features from visual inputs. For image-based tasks, early methods [4], [57] rely on convolutional neural networks (CNN) such as ResNet [68]. Recently, Vision Transformers (ViT) [69] and their variants [70] have demonstrated superior performance and have been widely adopted by many methods [2], [3]. For video-based tasks, models [35], [38] incorporate temporal modeling capabilities via 3D CNN [71], [72], or transformer with temporal attention, e.g., Video Swin Transformer [73]. For 3D tasks [7], [74], specialized encoders, e.g., Sparse 3D U-Net [75] and PointNet [76], process point clouds or voxel to capture spatial geometry and relationships. Text Encoder. Text encoders transform referring expressions into text features. Early methods [4], [77], [78], [79], [80] use recurrent neural networks to model sequential dependencies in language. With the advancement of pre-trained language models, recent methods [81], [82], [83], [84] have adopted transformerbased architectures, e.g., BERT [85] and RoBERTa [86], to obtain richer and more contextualized representations. More recently, text encoders from vision-language models, e.g., CLIP [87], have gained popularity for producing text embeddings that align well with visual features in shared semantic space, enhancing multimodal alignment in referring segmentation [88], [89]. Audio Encoder. Audio encoders extract acoustic features for audio-based tasks. Raw audio is typically converted into spectrograms or mel-frequency cepstral coefficients, which are then Fig. 6: Examples from 17 commonly used referring segmentation datasets, including image, video, and 3D scene data. different referring expressions: 1) Text; 2) Speech; 3) Text with Sound; 4) Speech with Sound; 5) Text with Image; 6) Speech with Image; 7) Text with Sound and Image; and 8) Speech with Sound and Image. Each expression includes either text or speech, as they provide essential instructions for the task and other modalities 2.2.4 3D Scene ScanRefer [43] comprises 51,583 natural language queries for 11,046 objects across 800 ScanNet [55] scenes, with an average of 13.81 objects and 64.48 queries per scene. Instruct3D [7] consists of 280 scenes from ScanNet++ [56] and 2,565 expressionobject pairs, designed for 3D instruction-based (or reasoning-based) segmentation, with focus on interpreting implicit user intent from natural language."
        },
        {
            "title": "2.2.5 GREx Datasets.\n• gRefCOCO [2] contains 278,232 referring expressions across\n19,994 images, supporting both GRES [2] and GREC [30] tasks.\nIt extends RefCOCO by adding multi-target and no-target ex-\npressions alongside single-target ones. No-target expressions are\ncarefully curated to be contextually relevant yet\nintentionally\nmisleading, without being entirely unrelated to the image. These\ncharacteristics make gRefCOCO more challenging than traditional\nRES datasets and more representative of real-world scenarios.\n• Multi3DRes [9] extends Multi3DRefer [46] by incorporating\nsegmentation masks to support the 3D-GRES task [9]. It provides\n61,926 expressions for 11,609 objects across 800 scenes, enabling\nmore robust handling of real-world scenarios.",
            "content": "processed by neural networks. CNN-based models [90] treat spectrograms as images, while transformer-based models [91], [92] capture temporal dependencies in audio. Pretrained encoders like VGGish [90] and wav2vec [91] have shown strong performance in generating robust audio features for downstream tasks including AVS [41], Ref-AVS [93], and OmniAVS [8]."
        },
        {
            "title": "3.3 Multimodal Interaction",
            "content": "A. Multimodal Fusion: the process of integrating features from different modalities to create unified representation. Concatenation-based Fusion. This fusion strategy [4], [66], [77], [78], [80] combines multimodal features via simple concatenation, followed by convolutional layers or MLPs. For example, image and text features are concatenated to form joint representation: ffused = Conv(Concat[fimage; ftext]). While efficient, such simple fusion fails to capture complex inter-modal interactions, limiting its effectiveness in fine-grained cross-modal reasoning. Attention-based Fusion. To address the limitations of simple fusion, attention-based methods [3], [81], [94], [95] enable dynamic and context-aware interactions across modalities. They selectively emphasize relevant features from one modality conditioned on the other [96], facilitating fine-grained alignment. For example, in image-text tasks, attention guides the model to focus on specific word tokens when processing visual regions, and vice versa. Representative strategies include visual attention [3], [5], symmetric co-attention [89], [97], and multimodal transformers employing selfand cross-attention [63], [98]. These methods enhance models ability to capture complex inter-modal relationships and have shown superior performance over simple fusion. B. Multimodal Alignment: the process of establishing meaningful correspondences between elements of different modalities. Contrastive Learning-based Alignment. These methods [88], [97], [99] leverage contrastive objectives to align multimodal representations in shared embedding space. Methods like CLIP [87] and ALIGN [100] train on paired imagetext data to maximize the similarity of matched pairs and minimize that of mismatched pairs. This results in unified semantic space where semantically related concepts from different modalities are closely positioned, supporting cross-modal retrieval and understanding. Self-supervised Alignment. These methods [63], [97], [101], [102] leverage naturally co-occurring multimodal data to learn alignment without explicit supervision. Techniques include 1) masked multimodal modeling [63] that learns to predict masked content in one modality according to another modality and 2) reconstruction-based methods [97], [102] that encode information from one modality and decode it into another."
        },
        {
            "title": "3.4 Temporal Information Processing",
            "content": "temporal information processing is essential For video tasks, for motion understanding [1] and consistent object segmentation across frames, especially under complex scenes [103], [104]. 3D Convolutional Networks. 3D CNNs [35], [105], [106] extend 2D convolutions by adding temporal dimension to learn spatiotemporal features from videos. They process multiple frames simultaneously, capturing motion patterns and temporal context. Methods like C3D [71] and I3D [72] treat videos as 3D volumes, applying convolutions across spatial and temporal dimensions to extract features modeling object movements and transformations. 6 Temporal Attention Mechanisms. Attention-based methods [107], [108], [109], [110] capture temporal dependencies by dynamically weighting the importance of different frames. Temporal attention allows models to focus on the frames that are most informative for object tracking and segmentation, enhancing the handling of long-range dependencies and diverse motion patterns. Memory Networks. Memory-based methods [38], [111], [112], [113] explicitly store and update objects features across frames, capturing clues about their appearance, location, and context. These memory modules enable models to retrieve relevant historical cues when processing new frames, enhancing temporal consistency. Such mechanisms are particularly beneficial in complex and long videos, where target objects may temporarily disappear, reappear, or undergo significant appearance variations [1], [103]. Optical Flow and Motion Estimation. number of temporal modeling methods [34], [35], [102], [114], [115] leverage explicit motion cues by incorporating optical flow estimation. Optical flow captures pixel-level correspondences between consecutive frames, providing detailed information about object motion. This motion guidance can be effectively combined with appearance features to enhance tracking and temporal coherence of segmentation masks."
        },
        {
            "title": "3.6 Training Objectives",
            "content": "Herein we list the most commonly used training objectives. Segmentation Objectives. For segmentation tasks [2], [3], [4], [5], [41], the primary training objectives typically include binary cross-entropy (BCE) loss and Dice loss. BCE measures pixel-wise classification error, while Dice loss directly optimizes the overlap between predicted and ground truth masks. These losses are often combined to improve boundary accuracy and region completeness. Grounding Objectives. To enhance grounding performance, several methods [65], [82] incorporate grounding objectives (e.g., L1, L2, IoU, and focal loss) to enforce accurate correspondence between visual regions and referring expressions. Set-based bipartite matching [30], [67] is used to address permutation invariance. Multimodal Alignment Objectives. Beyond visual perception losses, many referring segmentation models [3], [20], [74], [88], [126], [127] incorporate alignment objectives to bridge visual"
        },
        {
            "title": "4.2.2 Enhancing Multi-Modal Interaction\n• Multimodal Fusion. Various methods [3], [4], [141] have been\nproposed to fuse vision and language features. Early methods [4],\n[77], [78], [80] typically extract vision features using CNNs and\nencode referring expressions via LSTMs. These features are then\nfused using simple operations such as concatenation, followed by\nconvolutional layers. Kesen et al. [142] extend this by performing\nfeature fusion in both the upsampling and downsampling paths of\nU-Net. With the advent of attention mechanisms, some works [94],\n[95], [96], [107], [143] leverage cross-modal attention to cap-\nture fine-grained interactions between modalities. CMSA [107]\nconstructs comprehensive multimodal features by concatenating\nimage, spatial, and language features, then processes them by a\ncross-modal self-attention module to model long-range dependen-\ncies between words and spatial regions. EFN [95] leverages an\nasymmetric co-attention module to enhance the matching between\nmulti-modal features and strengthen their targeting capability.",
            "content": "A major milestone in RES is the shift toward Transformerbased architectures, which naturally support long-range dependencies across spatial and linguistic tokens. Traditional CNNs struggle with such global context due to their localized receptive fields. VLT [3], [5] is the first to introduce Transformer-based approach to RES, reformulating it as an attention problem where language features act as queries over vision tokens. This enables joint reasoning across both modalities. Since VLT, Transformer-based methods [81], [144], [145], [146], [147] have quickly become the common and state-of-the-art methods in RES. LAVT [81] injects language into vision features at intermediate levels of vision Transformer to enhance multimodal fusion. To mitigate query collapse, LQMFormer [147] enforces margin of separation between Fig. 7: Architecture Overview of Referring Expression Segmentation. (a) Two-stage method uses an off-the-shelf instance segmentation model to generate region proposals, followed by vision-language feature matching and ranking to select the top1 mask. (b) One-stage method fuses image and text features, performing pixel-level segmentation directly on the fused features. and linguistic modalities. Contrastive losses are commonly used to pull together matched visual-language pairs and push apart mismatched ones. Effectively combining these objectives is essential for building robust models capable of accurate segmentation across diverse scenes and referring modalities. Multi-Task Learning Objectives. Some methods adopt multitask learning strategies that couple referring segmentation with auxiliary tasks such as referring comprehension or generation. For example, MCN [65] jointly models RES and REC with consistency and suppression objectives to reduce task conflict. Chen et al. [128] integrate referring generation to enforce captionaware consistency. Liu et al. [96] introduce iterative languagevision interaction and reconstruction to preserve linguistic cues. These strategies show that jointly optimizing related tasks can enhance RES performance and multimodal understanding."
        },
        {
            "title": "4.1 Two-stage Methods",
            "content": "Two-stage methods [32], [57], [59], [129] for RES typically involve an initial segmentation step followed by matching process, as shown in Fig. 7(a). These methods often leverage off-theshelf instance segmentation models to generate object proposals, and then match them to the referring expression to identify the best-matched object. For example, MAttNet [57] first segments all objects using an instance segmentation network, Mask RCNN [15], and then employs modular network to identify the best-matched instance. CMN [59] also adopts modular networks that parse referring expressions into subject, relationship, and object components using three soft attention maps. It then aligns these textual representations with image regions. ISF [130] and WiCo [131] combine the advantages of both two-stage and onestage methods to improve the performance of RES. However, this paradigm suffers from error accumulation and high computational cost, making it less practical in real-time settings. The majority of existing RES methods adopt one-stage paradigm, which we categorize in the following subsections. query representations. To address the high computational cost of Transformer, ReMamber [148] adopts Mamba-based [149] architecture for more efficient vision-language fusion in RES. Multimodal Alignment. Multimodal alignment [88], [97], [99], [150] in RES aims to explicitly or implicitly associate information from different modalities (i.e., language and vision), ensuring consistency within shared semantic space. Several methods [84], [97], [98], [99], [151] employ cross-modal attention to strengthen alignment across modalities. For example, ReSTR [84] employs self-attention encoder to effectively align visual and language features in shared semantic space. Other methods [88], [146], [152], [153], [154] employ contrastive learning to improve multimodal alignment. By contrasting matched and mismatched modality pairs, these methods pull aligned pairs closer while pushing apart misaligned ones. CRIS [88] introduces text-to-pixel contrastive loss to explicitly link textual and pixel-level visual features, addressing CLIPs limitations in RES. CGFormer [146] integrates contrastive learning with grouping strategy to associate tokens with corresponding masks. Some methods [63], [155] adopt reconstruction-based techniques to align diverse modalities. Inspired by MAE [156], BTMAE [155] introduces bidirectional reconstruction of missing features in both image and language tokens to effectively model high-dimensional relationships among multimodal tokens. Inspired by BEiT-3 [157], OneRef [63] introduces one-tower modality-shared Transformer with mask referring strategy that jointly models referring-aware image and language masks. Parameter-Efficient Tuning Methods. While large foundation models have advanced RES, fully fine-tuning remains computationally expensive. Parameter-Efficient Tuning (PET) offers costeffective alternative by freezing most of the pre-trained model and updating only small subset of parameters, often achieving comparable performance. Recent works [89], [158], [159], [160] have explored applying PET to RES, mainly focusing on enhancing modal interaction while maintaining computational efficiency. For example, ETRIS [89] leverages vision-language bridge to fuse visual inductive biases with linguistic cues, while BarLeRIa [158] employs bi-directional intertwined vision-language adapters for efficient cross-modal fusion with minimal learnable parameters."
        },
        {
            "title": "4.2.3 Optimizing Mask Decoder\nTo improve segmentation quality, several methods [66], [80],\n[161], [162], [163], [164], [165] adopt multi-stage optimization\nstrategies that progressively refine segmentation masks. Early\nmethods [66], [80], [105], [106], [166] leverage ConvLSTM [164]\nto iteratively decode multimodal features, refining the prediction\nmask. SADLR [163] iteratively refines predictions using accu-\nmulated object context to produce accurate segmentation masks.\nJMCELN [161] proposes a multi-stage cascade framework that\nrefines segmentation by dynamically updating contextual embed-\ndings based on intermediate mask predictions.",
            "content": "Segment Anything Model (SAM) [124] shows strong performance across various segmentation tasks using point, box, or mask prompts, but remains limited in text-guided segmentation. To bridge this gap, several methods [121], [150], [167], [168], [169] have been proposed to adapt SAM for RES. DIT-SAM [121] addresses SAMs limitations by projecting text into the semantic space of SAMs image encoder. Prompt-RIS [169] bridges CLIP and SAM via prompt learning. Grounded SAM [168] combines Grounding DINO [170] with SAM to enable detection and segmentation of arbitrary regions based on free-form text inputs. F8 LMM [167] uses frozen LLM to generate segmentation priors, followed by CNN-based and SAM-based mask decoder to produce the final segmentation mask. Recent works [19], [171], [172] enhance SAMs text-guided segmentation by generating prompt embeddings via LLMs (see Sec. 4.7)."
        },
        {
            "title": "4.3 Multi-Task Learning",
            "content": "Multi-task learning is widely used in segmentation, detection, and generation, typically with shared backbone and task-specific heads. Building on this paradigm, several methods [65], [181], [182], [183] jointly address RES and referring expression comprehension (REC). MCN [65] implements an explicit constraint strategy by introducing consistency loss to ensure similarity between feature activation maps in REC and RES tasks. Referring Transformer [117] and MDETR [184] use an implicit approach by sharing multi-modal representations across REC and RES heads. SeqTR [178] and Polyformer [82] further unify both tasks as sequence-to-sequence point prediction problems. Beyond REC and RES, some works [128], [185] also explore joint modeling of RES and referring expression generation (REG). Generalist models supporting multiple vision-language tasks [186], [187], [188], [189], [190], [191], [192] have shown strong performance on RES. X-Decoder [186] introduces unified framework for segmentation and vision-language tasks, including RES. SEEM [187] extends this versatility by supporting diverse prompts (clicks, boxes, polygons, scribbles, text, and image regions) via prompt encoder in joint visual-semantic space. AnyRef [193] leverages MLLMs for pixel-level grounding and region-aware expression generation across text, regions, images, and audio."
        },
        {
            "title": "4.4 Weakly-Supervised Methods",
            "content": "To reduce the annotation burden, weakly-supervised RES methods [194], [195], [196], [197] leverage incomplete, imprecise, or noisy annotations to minimize reliance on dense pixel-level labels. Text-Only Supervision. TSEG [194] is pioneering weakly supervised RES method that only uses image-level referring expressions as supervision. Subsequent methods [195], [196], [198], [199], [200], [201] adopt textual supervision and employ strategies such as visual entity discovery and gathering [196], textto-image response mapping [202], and enhanced Grad-CAM for saliency refinement [198]. TRIS [202] directly learns the text-toimage response map by contrasting target-related positive texts with target-unrelated negative texts. PPT [195] leverages point generator to connect frozen CLIP and SAM models while adopting curriculum learning to facilitate the gradual learning of the point generator. PCNet [203] decomposes the description into multiple cues to guide progressive target localization. WeakMCN [181] jointly learns WeakREC and WeakRES in collaborative manner. Bounding Box Supervision. Feng et al. [204] propose weakly supervised RES method using bounding box annotations, where pseudo labels are generated from object contours and refined through filtering. In contrast, GTMS [205] enhances pseudo label quality by incorporating both structural and semantic information. Point Supervision. Beyond text and bounding box, PKS [197] employs click-based annotations (i.e., object center/corner clicks) for model training, achieving commendable performance."
        },
        {
            "title": "4.5 Semi-Supervised Methods",
            "content": "Semi-supervised RES methods reduce reliance on labor-intensive annotations by leveraging small set of labeled image-text pairs alongside abundant unlabeled samples. Several semi-supervised RES methods [206], [207] adopt pseudo-labeling strategies, selecting high-confidence predictions as supervision for model refinement. RESMatch [208] introduces the first dedicated semisupervised RES framework, incorporating quality assessment mechanism to evaluate pseudo-labels and strongweak supervision pairs. SemiRES [206] leverages SAMs edge-segmentation capabilities to generate high-quality pseudo-labels. PseudoRIS [207] produces multiple candidate masks from distinctive words and filters misleading captions to obtain reliable supervisory signals. Some semi-supervised RES methods [177], [209] adopt mixed-supervision paradigm, typically using small portion of mask annotations alongside larger proportion of bounding box annotations as supervisory signals. Partial-RES [177] and Safari [209] adopt an auto-regressive contour-based sequence prediction strategy, requiring only small fraction of mask annotations along with supplementary bounding box annotations."
        },
        {
            "title": "4.6 Zero-Shot Methods",
            "content": "Zero-shot RES methods [210], [211], [212], [213] leverage visionlanguage foundation models (e.g., CLIP [87]) to perform segmentation without task-specific training. Global-Local CLIP [210] is the first method to explore zero-shot RES using CLIP. TAS [214] incorporates additional caption embeddings, negative text embeddings, and spatial rectifier to enhance CLIP predictions, while CaR [215] recurrently applies CLIP to iteratively refine the segmentation mask. HybridGL [216] introduces global-local feature extraction method that combines mask-specific details with contextual information to improve mask representation."
        },
        {
            "title": "4.7 Other Task Settings\n• Reasoning Segmentation. Large language models (LLMs) and\nmultimodal LLMs (MLLMs) [217], [218], [219] have significantly\nadvanced vision-language tasks by enabling strong commonsense\nreasoning, opening new opportunities for RES. Building on this,\nLISA [19] pioneers the concept of reasoning segmentation, al-\nlowing models to handle complex expressions requiring external\nknowledge, such as “Segment the food with the highest protein\ncontent.” LISA introduces an embedding-as-mask paradigm by\nextending the MLLM’s vocabulary with a special [SEG] to-\nken to prompt SAM [124] for mask generation. Inspired by\nthese developments, several methods [26], [220], [221], [222],\n[223], [224], [225], [226], [227] explore reasoning segmenta-\ntion with LLMs/MLLMs. CoReS [228] employs chain-of-thought\n(CoT) [229] to tackle complex implicit text queries that require\nmulti-step reasoning. SAM4MLLM [172] encodes object masks",
            "content": "9 as discrete text prompts, while READ [230] converts text-image similarity maps into differentiable points to prompt SAM. Unlike LISAs paradigm, LLM-Seg [231] adopts two-stage approach, decoupling the reasoning over text from the segmentation process. Several methods [171], [232], [233], [234] focus on addressing the challenges of multi-target or zero-target scenarios in images, known as GRES [2]. GSVA [171] addresses the GRES [2] task by introducing shared-weight [SEG] tokens for multi-target segmentation and [REJ] token to discard empty targets. SESAME [233] handles false-premise queries by enabling models to detect object presence, provide corrective feedback, and segment only when appropriate. In parallel, interactive and conversational segmentation has gained interest [64], [235]. For example, SegLLM [64] supports multi-turn interactions with visual and textual queries, enabling the inference of object relationships such as spatial, interactive, and hierarchical dependencies. More recently, methods [236], [237], [238], [239] have explored reinforcement learning (RL) to enhance reasoning capabilities in segmentation. Seg-Zero [236] employs pure RL to develop emergent reasoning and improve outof-domain generalization. POPEN [237] incorporates preferencebased optimization to align large vision-language models with human intent via RL strategies, such as GRPO [240]. Referring Remote Sensing Image Segmentation (RRSIS). RRSIS focuses on aerial or satellite images, posing unique challenges by varying spatial scales, object orientations, and complex backgrounds distinct from natural images. To address these issues, several tailored methods [241], [242], [243], [244], [245] are proposed. Yuan et al. [241] introduce RRSIS task and RefSegRS benchmark, along with LAVT-based [81] framework that integrates multi-scale features for segmenting small and scattered objects. RMSIN [243] addresses orientation variations using rotated convolutions to better capture multi-oriented object features. Other. Beyond the core tasks, alternative settings have been explored [246], [247], [248]. Zero-guidance segmentation [246] seeks to segment images and label all segments using natural language. Grounded Conversation Generation [247] aims to generate natural language responses interleaved with object segmentation masks. Several methods [249], [250] extend referring segmentation capabilities to embodied intelligence settings. Additionally, PTQ4RIS [251] proposes post-training quantization framework to address challenges for on-device RES inference. input"
        },
        {
            "title": "5 REFERRING VIDEO OBJECT SEGMENTATION\n5.1 Per-frame and Online Methods",
            "content": "Per-frame Methods. natural approach to Referring Video Object Segmentation (RVOS) is to treat video as sequence of images and apply image-level referring segmentation to each frame independently. RefVOS [252] follows this paradigm by fusing frame-level visual and language features using an imagebased RES network. Subsequent methods such as CMPC [105] and VLT [3] adopt similar per-frame pipelines, offering simple extension of image-level RES to the video domain. Online Temporal Understanding. Per-frame methods process each frame independently and often suffer from temporal inconsistency due to the lack of temporal context. To address this, online methods process videos sequentially while maintaining memory across frames. For example, URVOS [38] uses temporal memory attention module to improve intra-frame consistency, and OnlineRefer [253] introduces an online association framework to enhance temporal coherence and referring accuracy."
        },
        {
            "title": "5.2 Offline One-stage Methods",
            "content": "Offline Temporal Understanding. Although online methods leverage historical frame information, their inability to access future frames restricts their capacity to resolve motion-centric and temporally ambiguous expressions, e.g., Elephant that goes towards then turns back. To mitigate this limitation, many RVOS approaches employ offline processing, which enables global temporal reasoning by considering the full video sequence [107], [109], [110], [111], [126], [253], [254], [255], [256], [257], [258], [259], [260], [261]. For example, TempCD [256] employs global referent tokens and local object queries to perform video-level reasoning and frame-wise segmentation via collection-distribution mechanism. HTML [258] applies hierarchical temporal sampling to capture multi-scale temporal interactions. LBDT [110] performs early-stage spatio-temporal alignment using language-guided encoding, while LOCATER [109] utilizes finite-memory structure to dynamically gather relevant temporal context. ReferMo [262] employ motion-vectors to compress the motion in across the video. Most recent RVOS works follow this offline pipeline, with various designs optimizing different aspects of video-level reasoning. Better Representations. Some RVOS methods [20], [115], [263], [264], [265], [266], [267] focus on learning better representations to enhance model performance. They improve either visual or textual feature representations to strengthen multi-modal understanding. On the visual side, several methods [263], [264], [265], [266], [267] aim to enhance visual feature quality. MLRL [264] introduces multi-level representation learning framework that generates discriminative embeddings by integrating multi-frame temporal dynamics (video level), spatial context (frame level), and object-aware priors (object level). VD-IT [266] leverages visual features from pretrained text-to-video diffusion models, achieving improved temporal consistency across frames. On the textual side, methods such as LoSh [115] and DsHmp [20] focus on refining language representations. LoSh [115] jointly predicts with long and short expressions, using the short form to enhance appearance-based localization. To address the challenging motion expressions [1], DsHmp [20] decomposes referring expression understanding into static and motion perception components, with an emphasis on improving temporal language comprehension. Some other recent works explore using extra types of feature, such as spectrum [268] or flow map [114], to assist in representation. Enhancing Multi-Modal Interaction. Effective integration of visual and textual features is essential for RVOS. Several methods [107], [108], [269], [270], [271], [272] have been proposed to enhance cross-modal interaction. OATNet [270] concatenates visual and textual features and processes them through shared multimodal encoder to jointly model intraand inter-modal relationships. EFCMA [108] introduces an encoder fusion network with gradual language guidance and co-attention mechanism to enhance feature alignment. ReferFormer [271] designs cross-modal feature pyramid network to enable multi-scale fusion of visionlanguage features. CMSA [107] introduces gated multi-level fusion module to selectively integrate cross-modal features across visual hierarchies. YOFO [269] designs meta-transfer module that injects target-specific linguistic cues into visual features while suppressing irrelevant language variations. SSA [273] addresses the gap between linguistic descriptions and video objects, as well as interference from background clutter. Optimizing Mask Decoder. Vision foundation models (e.g., SAM [124], SAM2 [125]) have shown strong segmentation ca10 pabilities, inspiring their adaptation for RVOS. Recent methods [112], [120], [274], [275] leverage the precise mask generation of these models while extending them to handle referring expressions in video contexts. RefSAM [274] integrates multi-view information from diverse modalities and frames across time to adapt SAM for RVOS. SAMWISE [120] and MPG-SAM 2 [275] are two representative RVOS works built on SAM2 [125]. SAMWISE [120] injects temporal cues and multimodal signals via an adapter during feature extraction, while MPG-SAM 2 [275] employs mask priorbased dense prompts and multi-level global context fusion. Improving Training Objectives. Some methods [102], [126] aim to improve training objectives in RVOS. For example, SOC [126] introduces visual-linguistic contrastive loss that applies semantic supervision at the video level to align object representations across modalities. Mei et al. [102] propose general self-supervised language-video pretraining framework that learns pixel-level features by using optical flow as an intermediate supervision signal during pretraining. Some methods [83], [119], [276], [277], [278], [279], [280] address multiple video segmentation tasks via unified frameworks or multi-task learning strategies. MUTR [278] adopts DETR-style transformer to jointly handle various tasks, while AL-Ref-SAM 2 [280] explores trainingfree paradigm to unify RVOS and AVS. UniMM [277] presents unified model for both mask-referred (VOS) and languagereferred (RVOS) segmentation. UniVS [276] unifies all major video segmentation tasks within single model. Sa2VA [119] integrates SAM2 with LLaVA to enable dense grounded understanding across text, image, and video in shared LLM token space. Moreover, recent study such as VEGGIE [281] shows that generalist generative models can also be used for RVOS."
        },
        {
            "title": "5.3 Two-Stage Methods",
            "content": "Similar to RES, many early RVOS methods adopt two-stage paradigm. Some directly extend image-based methods, for example, Khoreva et al. [34] adapt image method MAttNet [57] to videos by applying frame-wise segmentation followed by postprocessing temporal smoothing. Others [61], [282] operate at the video level by generating object tracklets across the entire video sequence and selecting the one that best matches the referring expression. For example, Liang et al. [61] first detect tracklets and then perform language-tracklet matching to identify the target. Resurgence for Long and Motion Expressions. Two-stage methods were initially in the minority due to the architectural complexity introduced by decoupling the segmentation and languagevision understanding stages. While this separation increases design complexity, it offers key advantage: the generated tracklets can encode the entire temporal sequence in compressed form, preserving both short-term and long-term motion cues for downstream multi-modal understanding in the second stage. The importance of such global temporal modeling is highlighted by the challenging MeViS [1] benchmark, which emphasizes motion-centric expressions. Many expressions in MeViS describe long-term motions, requiring models to capture extended temporal dynamics. However, online methods lack access to global context, and offline one-stage models often rely on sparse frame sampling (e.g., 5 or 8 frames) to reduce computational cost, which can miss fine-grained or long-range motion patterns, leading to suboptimal performance. This challenge has motivated resurgence of two-stage designs. Recent works [283], [284] first extract full-length mask tracklets using off-the-shelf video instance segmentation models, then align them with language in separate stage. This enables comprehensive temporal modeling and more accurate understanding of motion-guided expressions."
        },
        {
            "title": "6.1.1 Fully Supervised Learning\n• Better Representation. Feature representations extracted from\naudio and visual encoders are critical to AVS. Recent works [23],\n[301], [302], [303], [304], [305] focus on enhancing these repre-\nsentations. Lin et al. [304] demonstrate that Vision Transformers\nserve as parameter-efficient audio-visual learners. TeSO [23] en-\nhances audio guidance by generating scene-level descriptions with\nLLMs and extracting sounding-object cues via Chain-of-Thought\nprompting. QDFormer [302] uses product quantization to disen-\ntangle multi-source semantics into noise-suppressed components\nand applies global-to-local distillation to refine frame-level audio\nfeatures. COMBO [301] leverages foundation model priors for\nprecise representations, while ECMVAE [306] factorizes features\ninto shared and modality-specific components.\n• Enhancing Multi-Modal Interaction. Recent methods [301],\n[303], [307], [308], [309], [310] focus on improving audio-visual\nfusion and alignment. DeepAVFusion [307] performs early fusion\nusing learnable tokens to integrate audio-visual patches in parallel\nor sequentially. GAVS [308] adopts an encoder–prompt–decoder",
            "content": "11 framework to leverage SAMs generalization capacity. Dolphin [311] achieves fine-grained spatial-temporal alignment via multi-scale adapters and interleaved fusion, projecting features into an LLM for joint understanding. RAVS [312] mitigates audio ambiguity by clustering visual features by semantic density, weighting them by audio responsiveness, and modeling uncertainty for rapid transitions. DDESeg [313] disentangles mixed audio into semantic cues, filters noise via visual context, and enhances alignment through modality-specific discriminability. Temporal Processing. Several AVS methods [301], [314], [315], [316], [317] explicitly address temporal modeling. COMBO [301] enforces temporal coherence via an adaptive inter-frame consistency loss based on cosine similarity of adjacent masks. UFE [316] employs temporal partitioning strategy, using neighboring frames for motion guidance and distant ones to enhance data diversity. Improving Training Objectives. Some methods [101], [127], [315], [318], [319], [320], [321] aim to improve AVS performance by optimizing training objectives. PIF [319] decomposes AVS into two subtasks: correlation learning, which aligns audio with visible individuals to provide positional priors, and segmentation refinement, which produces masks. While transformer-based methods [310] have advanced AVS, they suffer from cross-attention inefficiency and unstable bipartite matching. CPM [318] addresses these limitations by introducing hybrid query strategy combining class-agnostic and class-conditional queries, and further enhances training with prompt-based joint audio-visual contrastive objective. In contrast, TransAVS [101] improves audio query diversity using self-supervised losses at both query and mask levels. Other methods [320], [321] introduce silent-object-aware objectives to ensure the segmentation of all potential sounding objects."
        },
        {
            "title": "6.1.2 Weakly Supervised/Unsupervised Methods",
            "content": "Weakly-Supervised AVS. WS-AVS [322] proposes weaklysupervised method using instance-level annotations and multiscale contrastive learning to improve cross-modal alignment. Unsupervised AVS. Recent woks such as MoCA [323] leverage foundation models (e.g., SAM [124], ImageBind [324]) to achieve competitive AVS performance without manual annotations."
        },
        {
            "title": "6.2 Referring Audio-Visual Segmentation",
            "content": "Ref-AVS [93] extends referring segmentation to the audio-visual domain, aiming to segment specific objects in video using synchronized audio and free-form language expressions. To support this task, Wang et al. [93] introduce the first Ref-AVS benchmark with over 3,000 videos annotated with masks and text expressions. The proposed framework employs modality-specific encoders for audio, vision, and text, fused via query-based decoder. Ref-AVS handles scenarios beyond the scope of traditional AVS or RVOS, such as person singing bass in cappella group, by leveraging multimodal cues for fine-grained disambiguation. It also shows that adding language improves the discriminative power of audio cues [23]. Omni-R1 [325] further enhances performance on RefAVS using reinforcement learning to select keyframes and rewrite tasks, enabling efficient Ref-AVS with just one training epoch. STBridge [292] bridges the modality gap between speech and text, allowing RVOS models to effectively process noisy spoken input. Despite its potential, Ref-AVS is still in its early stages. Challenges include temporal misalignment, underused spatial audio, and limited dataset diversity. Future work may explore spatialized audio, adaptive fusion, and broader benchmarks."
        },
        {
            "title": "7.1 Fully Supervised Learning",
            "content": "Two-Stage Methods. Two-stage methods first generate object proposals via 3D instance segmentation, followed by languageguided identification of the target object. TGNN [58] is pioneering method in this paradigm, extracting high-quality instance masks and features, then modeling instancelanguage relationships using Graph Neural Network, followed by multimodal feature aggregation for final prediction. Following this, X-RefSeg3D [326] enhances cross-modal interaction by constructing scene graph that integrates linguistic entities into visual representations, and employs dual-stream relation reasoning through textual and spatial modules to more effectively identify the target object. One-Stage Methods. One-stage methods perform end-toend segmentation by directly fusing visual and linguistic features, offering improved efficiency over two-stage pipelines. 3DSTMN [327] introduces superpoint-text matching (STM) mechanism to align superpoints with referring expressions, enabling faster inference and improved multimodal alignment. LESS [328], built on SPFormer [329], adopts one-stage pipeline and introduces an area regularization loss and point-to-point contrastive loss to mitigate interference from surrounding objects and background clutter. RefMask3D [74] proposes Linguistic Primitives Construction (LPC) module to learn fine-grained semantic primitives, enhancing vision-language alignment during decoding. RGSAN [330] improves spatial reasoning by integrating text-driven localization with rule-guided weak supervision to model interobject spatial relationships. IPDN [331] incorporates multi-view image features to enrich 3D representations and employs taskguided prompts to focus on language-relevant targets, addressing point cloud incompleteness and semantic ambiguity. While most methods focus on single-object segmentation, 3D-GRES [9] extends GRES [2] to support an arbitrary number of target objects in point clouds, enabling generalized 3D referring segmentation. 3D Reasoning Segmentation. Motivated by the success of LLMs/MLLMs [218], [219] in 2D reasoning segmentation [19], recent studies have begun exploring their potential in 3D domain. SegPoint [7] and Reason3D [332] are early efforts that integrate LLMs/MLLMs to enable reasoning in 3D-RES, primarily focusing on single-object or single-category settings. To handle more complex scenarios, MORE3D [333] extends this capability to multiobject reasoning and additionally generates textual explanations alongside segmentation outputs. 3D-LLaVA [334] further introduces unified vision-language framework for 3D tasks, jointly addressing question answering, dense captioning, and referring segmentation while achieving competitive performance. Multi-Task Learning. Joint learning of 3D Referring Segmentation (3D-RES) and Referring Expression Comprehension (3DREC) is natural extension of multi-task learning, as demonstrated in 2D vision [65], [178], where shared representations benefit related tasks. 3DRefTR [335] follows this paradigm by extending 3D-REC model through reuse of query embeddings and visual tokens, adding mask prediction branch to support 3D-RES with minimal overhead. MCLN [336] further explores task synergy by jointly optimizing separate branches for 3D-REC and 3D-RES, leveraging their semantic and structural alignment. Beyond pairwise integration, recent efforts aim to develop unified models for broad range of 3D vision-language tasks. Uni3DL [337] introduces versatile framework that supports tasks such as semantic/instance segmentation, referring segmentation, captioning, retrieval, and object classification on raw point clouds Fig. 8: Comparison of different audio-visual segmentation tasks."
        },
        {
            "title": "6.3 Omnimodal Referring Audio-Visual Segmentation",
            "content": "Omnimodal Audio-Visual Segmentation (OmniAVS) [8] is recently proposed task that extends traditional text-only referring audio-visual segmentation by introducing omnimodal expressions, flexibly combining text, speech, sound, and visual cues, see Fig. 8. This formulation yields 8 types of expressions: 1) Text, 2) Speech, 3) Text + Sound, 4) Speech + Sound, 5) Text + Image, 6) Speech + Image, 7) Text + Sound + Image, and 8) Speech + Sound + Image. To support this task, Ying et al. [8] introduce the first benchmark, OmniAVS, comprising 59,458 omnimodal expressions for 4,262 objects across 2,098 auditory videos. They propose Omnimodal Instructed Segmentation Assistant (OISA) as baseline, integrating MLLM with flexible mask head. OISA uses an audiovisual interleaving strategy to align content across modalities and generates target-aware [seg] tokens for segmentation, enabling accurate tracking and understanding in complex scenes. While OISA demonstrates promising performance, several open challenges remain in OmniAVS: (i) Improving audio-visual fusion via more robust joint representations, rather than relying on post-hoc alignment; (ii) Disentangling overlapping sound sources when multiple objects emit sounds simultaneously; (iii) Effectively combining multiple modalities within expressions through unified representations or cross-modal fusion; (iv) Enhancing segmentation robustness in complex scenarios such as occlusion, disappearance, and reappearance; (v) Pretraining on larger-scale audio-visual datasets to improve generalization to unseen settings. 7 3D REFERRING EXPRESSION SEGMENTATION Compared to its 2D counterpart, 3D Referring Expression Segmentation (3D-RES) poses new challenges due to the intrinsic complexity of point cloud data. While 2D RES operates on structured, grid-like image pixels, 3D-RES involves identifying semantically relevant points within unordered, irregular, and sparse 3D point clouds. This requires not only effective multimodal alignment between language and visual features, but also strong understanding of underlying geometric structures. The demand for precise point-wise segmentation and fine-grained semantic reasoning makes 3D-RES particularly challenging yet critical direction for advancing multimodal understanding in 3D space. via shared query modeling and dynamic task routing. Similarly, UniSeg3D [338] presents unified architecture for six segmentation tasks, including open-vocabulary and referring segmentation, using shared encoders and decoders, and enhancing performance through inter-task knowledge distillation and contrastive learning."
        },
        {
            "title": "7.2 Weakly Supervised/Unsupervised Methods",
            "content": "To reduce reliance on manual labeling, recent studies explore limited-supervision settings. 3D-REST [339] addresses the semisupervised setting by generating high-quality pseudo-labels and reweighting low-confidence predictions based on reliability, enabling effective training with fewer annotations. MEN [340] tackles the weakly supervised setting without mask labels by integrating multimodal cues, e.g., global context, fine-grained attributes, and category priors, to guide accurate segmentation."
        },
        {
            "title": "8 GENERALIZED REFERRING EXPRESSION\nGeneralized Referring Expression tasks (GREx) [2], [9], [30],\n[341], [342] aim to segment or detect an arbitrary number of target\nobjects in visual scenes based on free-form referring expressions.\nThis flexibility makes GREx more applicable to real-world scenar-\nios and highlights it as a promising direction for future research.\n• Generalized Referring Expression Segmentation (GRES).\nLiu and Ding et al. [2] first introduce the GRES task, which\nextends classic RES to further support multi-target and no-target\nexpressions. They also propose ReLA, a region-based framework\nthat segments images into semantically meaningful sub-instance\nregions and explicitly models region–region and region–language\ndependencies. Following this formulation, numerous works [2],\n[45], [147], [171], [343], [344], [345], [346], [347], [348] have\nadvanced GRES toward more practical and generalizable scenar-\nios. Shah et al. [147] identify a limitation in some GRES methods,\nincluding ReLA, termed query collapse, where all queries produce\nidentical mask predictions due to traditional RES’s single-mask\nconstraint. To mitigate this, they propose dynamic query gen-\neration conditioned on expressions, along with a regularization\nstrategy to diversify query representations. DMMI [45] intro-\nduces a dual-branch decoder enabling bidirectional interaction:\none branch guides visual localization using text, while the other\nreconstructs expressions from visual features to enforce semantic\nconsistency. GSVA [171] extends the reasoning segmentation\nparadigm of LISA [19] to the GRES setting by leveraging MLLMs\nand adapting [SEG] tokens for multi-target references. It further\nintroduces a [REJ] token to to explicitly handle no-target cases.\n• Generalized Referring Expression Comprehension (GREC).\nHe and Ding et al. [30] introduce the GREC task, an extension\nof classic REC that allows expressions to refer to any number\nof target objects. They also propose new evaluation metrics tai-\nlored for this setting. As a representative method, HieA2G [31]\npresents a hierarchical multimodal semantic alignment framework\nthat facilitates cross-modal interactions at word-object, phrase-\nobject, and text-image levels. To accommodate varying numbers\nof targets, it incorporates an adaptive counting mechanism.\n• 3D Generalized Referring Expression Segmentation (3D-\nGRES). 3D-GRES [9] extends GRES [2] to the 3D domain,\naiming to segment an arbitrary number of target objects in point\ncloud scenes. The framework employs text-guided query gener-\nation and optimization to enable effective interaction between\nqueries, point cloud features, and language, while maintaining\nsemantic consistency across multiple targets.",
            "content": "13 Fig. 9: Illustration of Several Applications. 3D Generalized Referring Expression Comprehension (3DGREC). Zhang et al. [46] introduce the first 3D-GREC dataset, Multi3DRefer. They propose CLIP-based method with multimodal contrastive learning to enable online rendering of proposal objects for generating 2D visual cues. 3D DOG [349] handles paragraph-level grounding of multiple objects, while GNL3D [350] proposes group-wise grounding across related 3D scenes with flexible target counts."
        },
        {
            "title": "9 RELATED TASKS AND APPLICATIONS",
            "content": "Referring Expression Comprehension (REC). REC [32], [57], [65], [351], [352] aims to detect target object in an image based on text referring expression by predicting bounding box around the described object. Unlike RES, REC focuses on target object detection by bounding box rather than pixel-level segmentation. 3D Referring Expression Comprehension (3D-REC). 3DREC [43], [44], [353], [354] aims to detect target object in 3D scene based on text referring expression. This task takes 3D point cloud and referring expression as input, and outputting 3D bounding box around the described object. Referring Video Object Tracking (RVOT). RVOT [355], [356], [357] aims to track single target object throughout video based on text referring expression. The model takes as input video and natural language description, and produces trajectory of bounding boxes for the target object across frames. Referring Multi-Object Tracking (RMOT). RMOT [118], [358] tracks multiple objects based on text expressions. TransRMOT [118] uses decoupled object queries for detection and tracking, while iKUN [358] adopts two-stage framework with language tracker and Neural Kalman Filter for adaptive tracking. Referring Expression Generation (REG). REG [351], [359] referring expressions aims to generate unambiguous textual that uniquely identify specific objects in visual scene. Some works [96], [128] leverage the synergies between RES and REG through multi-task learning approaches. Phrase Grounding (PG). PG [360], [361] aims to ground each entity mentioned by noun phrase in an image caption to its corresponding region in the image. Panoptic Narrative Grounding (PNG). PNG [362], [363] focuses on segmenting both foreground objects and background stuff that are described in detailed narrative captions of an image. While similar to RES, PNG requires the model to identify and segment entities mentioned throughout the entire caption text. Referring Expression Counting. This task [364], [365] aims to count object subsets specified by free-form expressions, requiring fine-grained grounding (e.g., green grapes vs. purple grapes). Referring Grasp Synthesis (RGS). RGS [366], [367] predicts grasp poses for objects described by language, combining visionlanguage grounding with robotic control. It typically follows two-stage pipeline: grounding and grasp prediction, see Fig. 9(a). Referring Image Editing (RIE). RIE [10] aims to identify and modify specific objects in an image based on referring expression, see Fig. 9(b). Unlike general image editing [11], which often applies global changes, or region-based editing that relies on masks or bounding boxes, RIE focuses on editing objects grounded by referring expressions. This ensures precise region-level editing and also provides flexible and natural user interaction."
        },
        {
            "title": "10 CONCLUSION AND DISCUSSION",
            "content": "This survey presents comprehensive overview of multimodal referring segmentation across image, video, and 3D domains. We unify the taxonomy of task settings, summarize metaarchitecture, and analyze representative methods and benchmarks from classic RES and RVOS to emerging tasks like GRES, OmniAVS, and reasoning segmentation, offering holistic view of the fields evolution. Recent trends reflect clear shift toward more general, human-centric, and reasoning-driven models: Omnimodal Understanding. OmniAVS demonstrates the need for models to flexibly interpret text, speech, sound, and visual cues. This reflects broader trend toward promptable and generalist models capable of handling diverse multimodal inputs. Generalization for Practical Scenarios. Generalized Referring Expression Segmentation (GRES) extends classical settings by supporting multi-target and no-target expressions. It promotes open-world segmentation and robustness to ambiguous language. Motion-Centric Video Understanding. MeViS pushes the boundary of RVOS by introducing motion-centric expressions and complex temporal dependencies. It encourages models to perform holistic video reasoning and dynamic object tracking. Foundation Models and Reasoning Segmentation. Large vision and language models, such as SAM/SAM2 and MLLMs like LLaVA, have reshaped the landscape of referring segmentation. They enable prompt-based interfaces and spark progress in reasoning segmentation, where models must interpret abstract, indirect, or knowledge-intensive instructions. Looking ahead, several research directions remain open: 1) Developing generalist segmentation agents that scale across modalities and tasks; 2) Enabling deeper commonsense and temporal reasoning under limited supervision; 3) Enhancing model robustness, efficiency, and interpretability for real-world deployment; and 4) Establishing unified benchmarks to evaluate crossmodal, open-world, and reasoning capabilities. APPENDIX: PERFORMANCE COMPARISON In this section, we present performance comparison of multimodal referring segmentation methods. For each field, the most widely used datasets, as outlined in Sec. 2, are selected for benchmarking. Given the large volume of publications in this area, we selectively report results from representative methods published in top-tier conferences and journals. A.1 RES Performance Benchmarking Intersection over Union (IoU) measures the overlap between predicted (Mp) and ground truth (Mgt) masks: IoU = area(Mp Mgt)/area(Mp Mgt). It serves as fundamental metric for evaluating model accuracy in identifying and segmenting regions. Mean Intersection over Union (mIoU) is defined by the average of all per-image Intersection-over-Unions (IoUs): mIoU = PN i=1 IoUi/N , where is the total number of data samples. This metric provides comprehensive evaluation of overall segmentation performance across the entire dataset. Cumulative Intersection over Union (cIoU) is defined by the cumulative intersection over the cumulative union: cIoU = PN i=1 area(M gt). It is worth noting that cIoU is highly biased toward large-area objects and tends to fluctuate significantly, which can affect the reliability of performance assessments. Precision@X (Pr@X) measures the percentage of predictions that achieve an IoU score higher than given IoU threshold X. i=1 area(M gt)/PN i RES Benchmark Results. As shown in TABLE 2, we present the performance comparison of recent RES methods on three widely-used benchmarks: RefCOCO [6], RefCOCO+ [6], and RefCOCOg [368]. The field has witnessed substantial progress since 2016, with recent methods achieving remarkable improvements across all datasets. For conventional fully-supervised methods, OneRef-L [63] achieves the best performance among conventional fully-supervised methods with 75.68% mIoU on RefCOCOg validation set and 76.82% mIoU on the test set, followed closely by UNINEXT [191] with 74.70% and 76.40% mIoU respectively. These results demonstrate remarkable progress compared to early methods like LSTM-CNN [4] from 2016, which achieved only 34.06% mIoU on RefCOCOg validation set. For Weakly/SemiSupervised, Zero-shot and Generalist Methods, ADDP [175] achieves the best performance on RefCOCOg with 59.05% mIoU on the validation set and 59.60% on the test set, outperforming several conventional fully-supervised methods despite using less supervision. Among MLLM-based methods, SAM4MLLM [172] achieves the best performance with 75.50% mIoU on RefCOCOg val set and 76.40% on the test set, followed by POPEN [237] with 75.40% and 75.60%, respectively, demonstrating the effectiveness of integrating MLLMs into referring segmentation. A.2 RVOS Performance Benchmarking Evaluation Metrics. Three metrics are frequently used to in Referring Video-Object Segmentation (RVOS) evaluation: Region Jaccard is calculated by the intersection-overunion (IoU) between the predicted segmentation mask Mp and the ground-truth Mgt: = Mp Mgt/Mp Mgt, which computes the number of pixels of the intersection between Mp and Mgt, and divides it by the size of the union. Boundary Accuracy is the harmonic mean of the boundary precision Pc and recall Rc. The value of reflects how well the predicted contours match the ground-truth contours. Usually, the value of Pc and Rc can be computed via bipartite graph matching [370], then the boundary accuracy can be computed as: = 2PcRc/(Pc + Rc). &F is computed as the average of region similarity and contour accuracy: &F = (J + F)/2, providing comprehensive evaluation of both region and boundary accuracy. Evaluation Metrics. For Referring Expression Segmentation (RES) evaluation, four primary metrics are commonly used: RVOS Benchmark Results. As shown in TABLE 3, we present the performance comparison of recent RVOS methods TABLE 2: Performance Comparison on RES benchmarks. The results are evaluated on RefCOCO [6], RefCOCO+ [6], and RefCOCOg [368] (UMD partition) using mIoU. : results post-processed with DenseCRF [369]. : methods utilizing 30% mask annotations and 70% bounding box annotations. ft denotes fine-tuning on referring expression segmentation datasets. indicates methods using only 5% labeled data. UNINEXT [191] utilizes val/test set masks during training, leading to mask information leakage. Method Venue RefCOCO RefCOCO+ RefCOCOg val test test val test test val test a. Conventional Fully-supervised methods. 15 LSTM-CNN [4] RMI [78] DMN [77] KWA [79] RRN [80] MAttNet [57] CMSA [107] LSCM [166] ConvLSTM [164] MCN [65] BRINet [94] STEP (5-fold) [66] Referring Transformer [117] EFN [95] BUSNet [165] LTS [173] CMPC [107] SeqTR [178] CoupAlign [153] ReSTR [84] CRIS [88] LAVT [81] BVPR [142] BKINet [143] M3Att [96] ETRIS [89] VPD [136] TRIS [202] PolyFormer [82] CGFormer [146] VG-LAW [135] UNINEXT [191] ReLA [2] VLT [3], [5] CM-MaskSD [99] ReMamber [148] Barleria [158] UniRES [48] MagNet [152] LQMFormer [147] Prompt-RIS [169] OneRef-L [63] [ECCV16] [ICCV17] [ECCV18] [ECCV18] [CVPR18] [CVPR18] [CVPR19] [ECCV20] [TMM20] [CVPR20] [CVPR20] [NeurIPS21] [NeurIPS21] [CVPR21] [CVPR21] [CVPR21] [TPAMI21] [ECCV22] [NeurIPS22] [CVPR22] [CVPR22] [CVPR22] [CVPR22] [TMM23] [TIP23] [ICCV23] [ICCV23] [ICCV23] [CVPR23] [CVPR23] [CVPR23] [CVPR23] [CVPR23] [TPAMI23] [TMM24] [ECCV24] [ICLR24] [CVPR24] [CVPR24] [CVPR24] [CVPR24] [NeurIPS24] - 45.18 49.78 - 55.33 56.51 58.32 61.47 59.04 62.44 60.98 60.04 74.34 62.76 63.27 65.43 61.36 67.26 74.70 67.22 70.47 72.73 67.01 73.22 73.60 70.51 73.25 41.10 75.96 76.93 75.62 82.20 73.82 65.65 74.89 71.60 72.40 79.20 76.55 74.16 78.10 81.26 b. Weakly/SemiSupervised, Zero-shot and Generalist Methods. Weakly-RIS [198] SaG [196] Global-Local CLIP [210] X-Decoder [186] Partial-RES [177] GTMS [205] SAFARI [209] SemiRES [206] PPT [195] SEEM [187] PCNet [203] HybridGL [216] ADDP [175] c. MLLMs based Methods. LISA-7B (ft) [19] PixelLm-7B [220] SESAME-7B [233] AnyRef-7B (ft) [193] PerceptionGPT-13B [227] GSVA-7B (ft) [171] GLaMM-7B [247] CoReS-7B [228] SAM4MLLM-8B [172] M2SA-13B [234] SegLLM-7B [64] READ-7B [230] POPEN-7B (ft) [237] [ICCV23] [ICCV23] [CVPR23] [CVPR23] [CVPR23] [ECCV24] [ECCV24] [ICML24] [CVPR24] [NeurIPS24] [NeurIPS24] [CVPR25] [ICLR25] [CVPR24] [CVPR24] [CVPR24] [CVPR24] [CVPR24] [CVPR24] [CVPR24] [ECCV24] [ECCV24] [ICLR25] [ICLR25] [CVPR25] [CVPR25] 31.06 44.60 26.20 - 66.24 66.54 67.04 61.31 46.76 - 52.20 49.48 69.14 74.90 73.00 74.70 76.90 75.30 77.20 79.50 76.00 79.80 74.60 80.20 78.10 79. - 45.69 54.83 - 57.26 62.37 60.61 64.99 60.74 64.20 62.99 63.46 76.77 65.69 66.41 67.76 64.53 69.79 77.76 69.30 73.18 75.82 69.63 76.43 76.23 73.51 - 48.10 77.09 78.70 77.51 83.40 76.48 68.29 77.54 73.30 75.90 81.60 78.27 76.82 81.21 83.06 32.30 50.10 24.94 - 68.39 69.98 69.17 66.64 45.33 - 58.40 53.37 70.27 79.10 76.50 - 79.90 79.10 78.90 83.20 78.60 82.70 77.60 81.50 80.20 82.00 - 45.57 45.13 - 53.93 51.70 55.09 59.55 56.73 59.71 59.21 57.97 70.87 59.67 61.39 63.08 59.64 64.12 70.58 64.45 66.10 68.79 63.45 69.42 70.36 66.63 - 31.90 73.22 73.32 72.89 81.30 70.18 62.73 71.28 68.40 68.30 76.60 72.15 71.04 74.64 79.45 30.11 38.40 26.56 - 63.57 63.41 64.23 55.94 46.28 - 42.10 45.19 67.46 72.30 68.20 - 74.20 72.10 73.50 76.90 72.50 74.70 71.00 75.40 73.20 74. - 29.86 38.88 - 39.75 46.67 43.76 49.34 44.54 50.62 48.17 48.19 66.75 51.50 51.76 54.21 49.56 54.14 62.92 55.78 62.27 62.14 55.34 64.91 65.34 60.10 62.69 31.60 70.65 68.56 66.63 72.50 66.04 55.50 67.47 61.60 65.00 73.00 68.10 65.91 71.13 76.60 31.28 35.50 27.80 - 54.37 57.59 54.98 47.00 45.34 - 47.90 43.40 57.58 65.10 66.30 64.90 70.30 68.90 65.90 72.60 65.10 74.60 64.00 70.30 68.40 73.10 - 30.48 44.22 - 42.15 52.39 47.60 53.12 47.92 54.99 52.32 52.33 70.58 55.24 56.87 58.32 53.44 58.93 68.34 60.44 68.08 68.38 60.72 69.88 70.50 66.89 - 31.90 74.51 73.76 70.38 76.40 71.02 59.20 71.80 65.80 70.80 78.10 73.64 71.84 76.60 80.16 32.11 41.10 25.64 - 58.16 63.46 59.31 54.42 45.84 - 56.50 49.13 61.65 70.80 71.70 - 73.50 74.00 69.60 78.70 70.00 80.00 68.10 73.00 73.70 77. - 29.50 32.29 - 36.11 40.08 37.89 43.50 39.73 44.69 42.11 40.41 59.40 43.01 44.13 48.02 43.23 48.19 56.69 48.27 53.68 55.10 47.11 53.39 56.98 50.17 - 30.60 64.64 61.72 58.89 66.20 57.65 49.36 59.91 54.00 56.90 65.80 61.81 57.59 64.25 72.95 30.13 27.60 27.84 - 47.92 50.32 48.26 38.74 44.77 - 36.20 37.17 51.76 58.10 58.30 - 61.80 61.90 59.80 64.60 58.60 67.20 57.60 62.50 60.40 65.10 34.06 34.52 - 36.92 36.45 47.64 39.98 - 41.77 49.22 - 46.40 66.63 51.93 - 54.40 - 55.67 62.84 - 59.87 61.24 55.09 64.21 64.92 59.82 61.96 39.00 69.36 67.57 65.63 74.70 65.00 52.99 66.53 61.10 63.40 71.70 67.79 64.73 70.47 75.68 32.88 - 33.52 64.60 54.69 54.52 55.72 47.61 42.97 65.70 46.80 51.25 59.05 67.90 69.30 66.10 70.00 70.70 72.70 74.20 69.00 75.50 69.00 72.60 70.10 75. - - - - - 48.61 - - - 49.40 - - 67.39 - - 54.25 - 55.64 62.22 - 60.36 62.09 55.31 63.77 67.37 59.91 - 39.90 69.88 67.83 66.08 76.40 65.97 56.65 66.63 61.20 63.80 73.20 69.29 66.04 71.29 76.82 - - 33.67 - 54.81 54.75 55.83 50.11 - - 46.90 51.59 59.60 70.60 70.50 - 70.70 71.90 73.30 74.90 70.70 76.40 69.30 73.60 71.40 75.60 TABLE 3: Performance Comparison on RVOS Benchmarks. The results are evaluated on MeViS [1], Ref-YouTube-VOS [38], and Ref-DAVIS17 [34] datasets with and metrics. Method Venue MeViS Ref-YouTube-VOS Ref-DAVIS17 16 URVOS [38] CMPC [105] MLRL [264] LBDT [110] MTTR [282] ReferFormer [271] EFCMA [108] HTML [258] SgMg [268] TempCD [256] UniRef++ [83] OnlineRefer [253] LMPM [1] LASTC [257] Locater [109] VD-IT [266] VISA-13B [39] SOC [126] VideoLISA-3.8B [40] UniVS [276] LoSh [115] DsHmp [20] SAMWISE [120] VRS-HQ-13B [113] GLUS [290] [ECCV20] [TPAMI21] [CVPR22] [CVPR22] [CVPR22] [CVPR22] [TPAMI22] [ICCV23] [ICCV23] [ICCV23] [ICCV23] [ICCV23] [ICCV23] [TPAMI23] [TPAMI23] [ECCV24] [ECCV24] [NeurIPS24] [NeurIPS24] [CVPR24] [CVPR24] [CVPR24] [CVPR25] [CVPR25] [CVPR25] &F 27.80 - - 29.30 30.00 31.00 - - - - - 32.30 37.20 - - - 44.50 - 42.30 - - 46.40 48.30 50.90 51.30 25.70 - - 27.80 28.80 29.80 - - - - - 31.50 34.20 - - - 41.80 - 39.40 - - 43.00 45.40 48.00 48.50 29.90 - - 30.80 31.20 32.20 - - - - - 33.10 40.20 - - - 47.10 - 45.20 - - 49.80 51.20 53.70 54. &F 47.23 47.48 49.70 49.38 55.32 62.90 48.97 63.40 65.70 65.80 66.90 63.50 - 49.30 56.50 66.50 63.00 67.30 61.70 58.00 67.20 67.10 67.20 71.00 67.30 45.27 45.64 48.43 48.18 54.00 61.30 47.82 61.50 63.90 63.60 64.80 61.60 - 48.15 54.80 64.40 61.40 65.30 60.20 - 65.40 65.00 65.20 69.00 65.50 49.19 49.32 50.96 50.57 56.64 64.60 50.12 65.20 67.40 68.00 69.00 65.50 - 50.45 58.10 68.50 64.70 69.30 63.30 - 69.00 69.10 69.30 73.10 69. &F 51.63 - 57.94 54.52 - 61.10 50.23 62.10 63.30 64.60 67.20 64.80 - 54.45 - 69.40 70.40 65.80 67.70 59.40 64.30 64.90 68.50 74.40 - 47.29 - 53.85 - - 58.10 47.37 59.20 60.60 61.60 63.40 61.60 - - - 66.20 67.00 62.50 63.80 - 61.80 61.70 65.60 71.00 - 55.96 - 62.02 - - 64.10 53.08 65.10 66.00 67.60 70.90 67.70 - - - 72.60 73.80 69.10 71.50 - 66.80 68.10 71.50 77.90 - TABLE 4: Performance Comparison on AVS Benchmarks. The results are evaluated on AVSBench [41] and AVSBenchSemantic [42] with and score metrics. : weakly-supervised. Method Venue S4 MS3 AVSS J F AVSBench [41] ECMVAE [306] LAVISH [304] PIF [319] BAVS [321] C3N [309] CPM [318] Stepping Stones [371] TESO [23] WS-AVS [322] QDFormer [302] COMBO [301] VPO [372] DeepAVFusion [307] RAVS [312] DDESeg [313] - - - - - - - - - 78.74 87.90 54.00 64.50 29.77 35.20 81.74 90.10 57.84 70.80 80.10 81.40 90.00 58.90 70.90 82.68 89.75 59.63 65.89 33.59 37.52 83.11 90.80 61.72 72.20 81.37 90.47 59.80 71.00 34.53 39.57 83.20 91.30 67.30 77.60 48.50 53.20 83.27 93.30 66.02 80.10 38.96 45.10 [ECCV22] [ICCV23] [CVPR23] [TMM24] [TMM24] [TMM24] [ECCV24] [ECCV24] [ECCV24] [NeurIPS24] 34.13 51.76 30.85 46.87 [CVPR24] [CVPR24] [CVPR24] [CVPR24] [CVPR25] [CVPR25] 79.50 88.20 61.90 66.10 53.40 84.70 91.90 59.20 71.20 42.10 46.10 85.77 92.86 62.39 73.62 44.70 57.76 89.94 92.34 52.05 58.29 93.10 93.80 70.60 82.10 60.80 70.60 92.40 95.90 72.30 83.40 63.40 72.30 - - - - - - - TABLE 5: Performance Comparison on Ref-AVS Benchmark. The results are evaluated on Ref-AVS [93] dataset with and metrics. denotes the square root of the ratio between the predicted mask area and the background area, with lower values indicating better performance. Method Venue Seen Unseen Mix(S+U) Null F S () AVSBench [41] ReferFormer [271] R2VOS [345] AVGSegFormer [310] GAVS [308] RefAVS [93] TSAM [373] SAM2-LOVE [374] OISA-1B [8] [ECCV22] 23.2 51.1 32.4 54.7 27.8 52.9 20.8 [CVPR22] 31.3 50.1 30.4 48.8 30.9 49.5 17.6 [ICCV23] 25.0 41.0 27.9 49.8 26.5 45.4 18.3 [AAAI24] 33.5 47.0 36.1 50.1 34.8 48.6 17.1 [AAAI24] 28.9 49.8 29.8 49.7 29.4 49.8 19.0 0.7 [ECCV24] 34.2 51.3 49.5 64.8 41.9 58.1 [CVPR25] 43.4 56.8 54.6 66.4 1.7 [CVPR25] 43.5 51.9 66.5 72.3 55.0 62.1 23.0 [ICCV25] 51.7 58.7 58.3 65.1 54.5 61.4 9.8 - - on three widely-used benchmarks: MeViS [1], Ref-YouTubeVOS [38], and Ref-DAVIS17 [34]. The field has witnessed substantial progress since 2020, with recent methods like GLUS [290] and VRS-HQ [113] achieving remarkable improvements on all benchmarks. On the MeViS dataset, which focuses on temporal motion understanding, GLUS achieves the best performance with 51.30% &F, followed closely by VRS-HQ with 50.90%. For Ref-YouTube-VOS, VRS-HQ leads with 71.00% &F, significantly outperforming earlier methods like URVOS [38] (47.23%). On Ref-DAVIS17, VRS-HQ also demonstrates superior performance with 74.40% &F, showing substantial improvement over previous state-of-the-art methods. A.3 R-AVS Performance Benchmarking Evaluation Metrics. Region Jaccard , boundary accuracy are also widely adopted for Audio-Visual Segmentation (AVS) performance evaluation. TABLE 6: Performance Comparison on OmniAVS Benchmark. The results are evaluated on OmniAVS [8] dataset with &F as the default metric. All is the average result across 8 splits. MET.: METEOR. Method All II III IV VI VII VIII MET. 25.8 31.2 28.7 20.0 22.7 21.3 20.9 30.0 31.4 LMPM [1] 29.6 34.4 32.6 19.6 26.0 28.0 24.7 35.6 36.0 EEMC [93] 32.3 35.4 33.3 28.4 29.8 26.5 22.8 41.6 40.5 MUTR [278] LISA-7B [19] 33.6 33.3 31.2 29.2 32.7 28.6 27.3 43.4 43.1 LISA-13B [19] 36.1 36.4 32.1 30.4 35.7 31.6 30.2 46.7 45.7 41.1 40.1 38.5 34.9 38.5 35.9 35.2 52.6 53.0 OISA-1B [8] - - - 11.6 16.5 21.7 AVS Benchmark Results. As shown in TABLE 4, we present the performance comparison of recent AVS methods on AVSBench [41] (S4 and MS3 subsets) and AVSBench-Semantic [42] (AVSS) using and metrics. The field has seen significant progress since 2022, with recent methods like DDESeg [313] and RAVS [312] achieving remarkable improvements on all benchmarks. DDESeg achieves the best performance on S4 with 92.40% TABLE 7: Performance Comparison on 3D-RES Benchmarks. The results are evaluated on ScanRefer [43] dataset with Acc@K metric. denotes weakly-supervised methods. TABLE 8: Performance Comparison on GRES Benchmarks. The results are evaluated on gRefCOCO [2] dataset with the cIoU and mIoU metric. denotes zero-shot method. Method Venue Unique (19%) Multiple (81%) Overall Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5 Method Venue Val testA testB cIoU gIoU cIoU gIoU cIoU gIoU 17 TGNN [58] X-RefSeg3D [326] 3D-STMN [327] RefMask3D [74] MDIN [9] MCLN [336] [AAAI21] [AAAI24] [AAAI24] [MM24] [MM24] [ECCV24] RG-SAN [330] [NeurIPS24] UniSeg3D [338] [NeurIPS24] LESS [328] [NeurIPS24] Reason3D [332] IPDN [331] [3DV25] [AAAI25] - - 89.30 89.55 91.00 89.57 89.20 - - 88.40 91.50 - - 84.00 84.69 87.20 78.22 84.30 - - 84.20 88.00 - - 46.20 48.09 50.10 53.28 55.00 - - 50.50 53.10 - - 29.20 40.77 44.90 45.88 35.40 - - 31.70 47. 37.50 40.33 54.60 55.87 58.00 58.70 61.70 41.50 53.23 57.90 60.60 31.40 33.77 39.80 49.24 53.10 50.70 44.90 28.00 29.88 41.90 54.90 and 95.90% F, as well as on MS3 with 72.30% and 83.40% F. For the more challenging AVSS benchmark, DDESeg also leads with 63.40% and 72.30% F, demonstrating the effectiveness in audio-visual semantic segmentation task. Ref-AVS Benchmark Results. As shown in TABLE 5, we present the performance comparison of recent Ref-AVS methods on the Ref-AVS [93] dataset using and metrics across different scenarios. The field has seen remarkable progress since 2022, with recent methods like OmniAVS [8] achieving the best overall performance with 51.7% and 58.7% on the Seen set, and 58.3% and 65.1% on the Unseen set. TSAM [373] and SAM2-LOVE [374] also demonstrate competitive results, with TSAM achieving 43.4% and 56.8% on Seen, and SAM2LOVE reaching 66.5% and 72.3% on Unseen scenarios. These results highlight the significant advancement in audio-visual referring segmentation capabilities, substantially outperforming earlier methods like AVSBench [41] and ReferFormer [271]. OmniAVS Benchmark Results. As shown in TABLE 6, we present the performance comparison of recent methods on the OmniAVS [8] dataset using &F metric across eight different splits and METEOR for caption evaluation. OmniAVS [8] achieves the best overall performance with 40.5% &F across all splits and 17.0 METEOR score, demonstrating significant improvements over previous methods. MUTR [278] and LISA13B [19] also show competitive results with 35.2% and 35.4% overall performance respectively. The results across different splits show varying levels of difficulty, with splits VII and VIII generally achieving higher performance (48.2-53.6%) compared to splits III and IV (26.9-36.2%), indicating the heterogeneous nature of the benchmark and the challenges in multi-modal audio-visual understanding. A.4 3D-RES Performance Benchmarking Evaluation Metrics. IoU and Acc@X (i.e., Pr@X) are adopted for evaluating 3D-RES performance. Benchmark Results. As shown in TABLE 7, we present the performance comparison of recent 3D-RES methods on ScanRefer [43]. The evaluation is conducted on both unique and multiple reference scenarios, with unique references constituting approximately 19% of the dataset and multiple references making up the remaining 81%. IPDN [331] achieves the best overall performance with 60.60% Acc@0.25 and 54.90% Acc@0.5, demonstrating significant improvements over earlier methods. RG-SAN [330], despite being weakly-supervised, shows competitive results with 61.70% Acc@0.25, though its Acc@0.5 performance (44.90%) is lower than fully-supervised methods like IPDN [331] and MDIN [9]. Most methods perform substantially better on unique references MattNet [57] CRIS [88] LTS [173] LAVT [81] ReLA [2] VLT [3] LaSagnA [232] CoHD [348] HDC [375] MABP [346] InstAlign [347] LISA-13B [19] LQMFormer [147] GSVA-13B [171] HieA2G [31] PSALM-G5 (ft) [342] UniRES++ [50] Segment Anyword [150] RAS [376] 47.51 48.24 58.66 59.30 45.33 46.14 [CVPR18] 55.34 56.27 63.82 63.42 51.04 51.79 [ICCV21] 52.30 52.70 61.87 62.64 49.96 50.42 [CVPR21] 57.64 58.40 65.32 65.90 55.04 55.83 [CVPR22] 62.42 63.60 69.26 70.03 59.88 61.02 [CVPR23] [TPAMI23] 52.51 52.00 62.19 63.20 50.52 50.88 38.10 32.40 50.40 47.30 42.10 38.90 [arXiv24] 65.17 68.42 71.85 72.67 62.63 63.60 [arXiv24] 65.42 68.23 71.60 72.52 62.79 63.85 [arXiv24] 65.72 68.86 71.59 72.81 62.76 64.04 [arXiv24] 68.94 74.34 73.22 74.51 63.88 65.74 [arXiv24] 63.96 65.24 71.00 69.99 62.29 62.11 [CVPR24] 64.98 70.94 [CVPR24] 66.38 70.04 72.79 73.29 63.20 65.45 [CVPR24] 64.20 68.40 70.40 72.00 61.00 62.80 [AAAI25] 68.00 67.30 75.20 77.30 73.10 78.90 [CVPR25] 69.90 74.40 74.50 76.00 66.60 69.80 [arXiv25] 67.73 66.08 73.57 74.63 67.56 70.90 [ICML25] 70.48 74.64 76.99 77.45 67.90 69.42 [arXiv25] - - - - TABLE 9: Performance Comparison on GREC Task. The results are evaluated on gRefCOCO dataset [2] with Pr@(F1=1, IoU0.5) and N-acc. metrics. Method Venue Val testA testB Pr N-acc. Pr N-acc. Pr N-acc. MCN [65] MDETR [184] VLT [3] UNINEXT [191] Ferret [377] SimVG [378] Grounding Dino [170] NGDINO [379] HieA2G [31] [CVPR20] [ICCV21] [TPAMI23] [CVPR23] [ICLR24] 28.0 42.7 36.6 58.2 54.8 [NeurIPS24] 62.1 [ECCV24] [arXiv25] [AAAI25] - - 67.8 30.6 36.3 35.2 50.6 48.9 54.7 - - 60.3 32.3 50.0 40.2 46.4 49.5 64.6 45.7 46.1 66.0 32.0 34.5 34.1 49.3 45.2 57.2 79.0 83.2 60.1 26.8 36.5 30.2 42.9 43.5 54.8 44.8 45.6 56.5 30.3 31.0 32.5 48.2 43.8 57.2 76.7 78.1 56. than multiple references, highlighting the challenge of disambiguating between similar objects in 3D scenes. A.5 GREx Performance Benchmarking Evaluation Metrics. In addition to cIoU, the evaluation metrics for GRES and GREC also include: Generalized IoU (gIoU): The widely-used cIoU tends to favor larger objects. Since multi-target samples have larger foreground areas in GRES, this bias can significantly impact the evaluation results. Similar to mean IoU, gIoU calculates the mean value of per-image IoU over all samples. For no-target samples, the IoU values of true positive no-target samples are regarded as 1, while IoU values of false negative samples are treated as 0. N-acc. and T-acc.: These two metrics to assess model performance on no-target identification. N-acc. (No-target accuracy) evaluates how well model identifies samples without targets by computing the ratio of true positives, i.e., predictions with no foreground pixels for no-target samples, to total no-target samples: Nacc. = TP/(TP + FN). Meanwhile, T-acc. (Target accuracy) measures the models ability to avoid misclassifying target-containing samples as no-target samples: T-acc. = TN/(TN + FP). Precision@(F1=1, IoU>0.5): This metric computes the percentage of samples that achieve an F1 score of 1 with the IoU threshold set to 0.5. Given sample and its predicted/groundtruth bounding boxes, predicted bounding box is regarded as TP if it has matched (IoU>0.5) ground-truth bounding box. When multiple predicted bounding boxes match one ground-truth bounding box, only the one with the highest IoU is considered TABLE 10: Performance Comparison on ReasonSeg Benchmarks. The results are evaluated on the ReasonSeg [19] dataset with the mIoU and cIoU metrics. ft denotes fine-tuning on the ReasonSeg dataset. For MLLMs training methods, SFT denotes Supervised Fine-Tuning, RL denotes Reinforcement Learning. Method Venue Val testA Training Method mIoU cIoU mIoU cIoU w/o Multimodel Large Language Models (MLLMs). ReLA [2] X-Decoder [186] OVSeg [380] SEEM [187] Grounded-SAM [168] [CVPR23] [CVPR23] [CVPR23] [NeurIPS24] [arXiv24] - - - - - w. Multimodel Large Language Models (MLLMs). SFT SFT SFT SFT SFT SFT SFT SFT SFT+RL RL RL RL RL LISA-13B (ft) [19] LISA++-7B (ft) [381] GSVA-7B (ft) [171] LLM-Seg-7B [231] SAM4MLLM-8B (ft) [172] CoReS-13B (ft) [228] SegLLM-7B [64] READ-7B (ft) [230] POPEN-7B [237] Seg-Zero-7B [236] PixelThink-7B [238] SAM-R1-7B [239] VisionReasoner-7B [382] [CVPR24] [arXiv23] [CVPR24] [CVPR24] [ECCV24] [ECCV24] [ICLR25] [CVPR25] [CVPR25] [arXiv25] [arXiv25] [arXiv25] [arXiv25] 22.4 22.6 28.5 25.5 26.0 65.0 64.2 50.5 52.3 58.4 68.1 57.2 59.8 60.2 62.6 63.8 64.0 66. 19.9 17.9 18.6 21.2 14.5 72.9 68.1 56.4 47.5 60.4 - 54.3 67.6 64.5 62.0 62.7 55.8 - 21.3 21.7 26.1 24.3 21.3 61.3 57.0 - - - 65.5 52.4 56.8 - 57.5 60.2 60.2 63.6 22.0 16.3 20.8 18.7 16.4 62.2 59.5 - - - - 48.4 59.0 - 52.0 55.8 54.3 - TP while others are FP. Ground-truth bounding boxes having no matched predicted bounding box are FN while predicted bounding boxes having no matched ground-truth are FP. The F1 score 2T is calculated by F1 = 2T +F +F . sample is considered successfully predicted if the F1 score equals 1. For no-target samples, the F1 score is regarded as 1 if there is no predicted bounding box, otherwise 0. GRES Benchmark Results. As shown in TABLE 8, we present the performance comparison of recent GRES methods on the gRefCOCO [2] dataset using cIoU and mIoU metrics. The field has seen significant progress since the GRES [2] task was proposed, with recent methods like InstAlign [347] achieving the best performance with 68.94% cIoU and 74.34% mIoU on the validation set. GSVA [171] also demonstrates strong performance, reaching 66.38% cIoU and 70.04% mIoU on the validation set. On the testA and testB sets, InstAlign continues to lead with 73.22% cIoU, 74.51% mIoU and 63.88% cIoU, 65.74% mIoU respectively. These improvements highlight the rapid advancement in generalized referring expression segmentation capabilities and challenges, significantly outperforming earlier methods like MattNet [57] and VLT [3] designed for only single-object tasks. GREC Benchmark Results. As shown in TABLE 9, we present the performance comparison of recent GREC methods on the gRefCOCO [2] dataset using Pr@(F1=1, IoU0.5) and N-acc. metrics. The field has witnessed significant advancement since the introduction of GREC task, with recent methods like HieA2G [31] achieving the best overall performance with 67.8% Pr and 60.3% N-acc. on the validation set. SimVG [378] also demonstrates competitive results, reaching 62.1% Pr and 54.7% N-acc. on validation. On the testA set, SimVG leads with 64.6% Pr, while NGDINO [379] achieves the highest N-acc. at 83.2%. For testB evaluation, HieA2G maintains strong performance with 56.5% Pr and 56.0% N-acc. A.6 ReasonSeg Performance Benchmarking Evaluation Metrics. mIoU and cIoU are adopted for evaluating Reasoning Segmentation performance. ReasonSeg Benchmark Results. As shown in TABLE 10, we present the performance comparison of recent Reasoning Segmentation (ReasonSeg) methods on the ReasonSeg [19] dataset using gIoU and cIoU metrics. The results demonstrate clear performance gap between methods with and without Multimodal Large Language Models (MLLMs). Traditional methods without MLLMs, such as ReLA [2], X-Decoder [186], and OVSeg [380], achieve modest performance with gIoU scores ranging from 22.4% to 28.5% on the validation set. In contrast, MLLM-based approaches show substantial improvements, with CoReS [228] achieving the best performance at 68.1% gIoU on the validation set, followed by VisionReasoner [382] at 66.3%. On the test set, CoReS leads with 65.5% gIoU, followed by VisionReasoner at 63.6%. When comparing 7B parameter MLLM models for fair comparison, VisionReasoner achieves the best performance with 66.3% gIoU on validation and 63.6% on test, followed by SAMR1 [239] and PixelThink [238] (both 60.2% gIoU on test). These results highlight the significant advantage of leveraging MLLMs for complex reasoning tasks in segmentation, with fine-tuned models consistently outperforming their non-MLLM counterparts by large margins across both validation and test sets."
        },
        {
            "title": "REFERENCES",
            "content": "[1] [2] [3] [4] [5] [6] [7] [8] [9] H. Ding, C. Liu, S. He, X. Jiang, and C. C. Loy, MeViS: large-scale benchmark for video segmentation with motion expressions, in ICCV, 2023. 1, 2, 4, 6, 10, 11, 16 C. Liu, H. Ding, and X. Jiang, GRES: generalized referring expression segmentation, in CVPR, 2023. 1, 3, 4, 5, 6, 9, 12, 13, 15, 17, 18 H. Ding, C. Liu, S. Wang, and X. Jiang, VLT: Vision-language transformer and query generation for referring segmentation, IEEE TPAMI, 2023. 1, 2, 5, 6, 7, 9, 15, 17, 18 R. Hu, M. Rohrbach, and T. Darrell, Segmentation from natural language expressions, in ECCV, 2016. 1, 5, 6, 7, 14, 15 H. Ding, C. Liu, S. Wang, and X. Jiang, Vision-language transformer and query generation for referring segmentation, in ICCV, 2021. 1, 5, 6, 7, 15 S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg, Referitgame: Referring to objects in photographs of natural scenes, in EMNLP, 2014. 1, 4, 14, 15 S. He, H. Ding, X. Jiang, and B. Wen, SegPoint: Segment any point cloud via large language model, in ECCV, 2024. 1, 2, 4, 5, 12 K. Ying, H. Ding, G. Jie, and Y.-G. Jiang, Towards omnimodal expressions and reasoning in referring audio-visual segmentation, in ICCV, 2025. 1, 2, 4, 6, 12, 16, 17 C. Wu, Y. Liu, J. Ji, Y. Ma, H. Wang, G. Luo, H. Ding, X. Sun, and R. Ji, 3D-GRES: Generalized 3d referring expression segmentation, ACM MM, 2024. 1, 4, 5, 12, 13, 17 [10] C. Liu, X. Li, and H. Ding, Referring image editing: Object-level image editing via referring expressions, in CVPR, 2024. 1, 14 [11] X. Shuai, H. Ding, X. Ma, R. Tu, Y.-G. Jiang, and D. Tao, survey of multimodal-guided image editing with text-to-image diffusion models, arXiv, 2024. 1, 14 [13] [12] H.-S. Fang, C. Wang, M. Gou, and C. Lu, Graspnet-1billion: largescale benchmark for general object grasping, in CVPR, 2020. 1 J. Lin, J. Chen, K. Peng, X. He, Z. Li, R. Stiefelhagen, and K. Yang, Echotrack: Auditory referring multi-object tracking for autonomous driving, IEEE TITS, 2024. 1 J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for semantic segmentation, in CVPR, 2015. 1, 6 [14] [15] K. He, G. Gkioxari, P. Dollar, and R. Girshick, Mask r-cnn, in ICCV, 2017. 1, 5, 7 [16] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, Context contrasted feature and gated multi-scale aggregation for scene segmentation, in CVPR, 2018. 1 [17] X. Li, H. Ding, H. Yuan, W. Zhang, J. Pang, G. Cheng, K. Chen, Z. Liu, and C. C. Loy, Transformer-based visual segmentation: survey, IEEE TPAMI, 2024. 1 J. Wu, X. Li, S. Xu, H. Yuan, H. Ding, Y. Yang, X. Li, J. Zhang, Y. Tong, X. Jiang et al., Towards open vocabulary learning: survey, IEEE TPAMI, 2024. [18] [19] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia, Lisa: Reasoning segmentation via large language model, in CVPR, 2024. 2, 4, 8, 9, 12, 13, 15, 16, 17, 18 [20] S. He and H. Ding, Decoupling static and hierarchical motion perception for referring video segmentation, in CVPR, 2024. 2, 6, 10, 16 [21] Y. Wang, P. Sun, D. Zhou, G. Li, H. Zhang, and D. Hu, Ref-avs: Refer and segment objects in audio-visual scenes, ECCV, 2024. 2, 4 [22] A. Sokolov, S. Bhosale, and X. Zhu, 3d audio-visual segmentation, arXiv, 2024. [23] Y. Wang, P. Sun, Y. Li, H. Zhang, and D. Hu, Can textual semantics mitigate sounding object segmentation preference? in ECCV, 2024. 2, 11, 16 [24] Y. Qiao, C. Deng, and Q. Wu, Referring expression comprehension: survey of methods and datasets, IEEE TMM, 2020. 1 [25] D. Liu, Y. Liu, W. Huang, and W. Hu, survey on text-guided 3d visual grounding: Elements, recent advances, and future directions, arXiv, 2024. 1 [26] Y. Shen, C. Li, F. Xiong, J.-O. Jeong, T. Wang, M. Latman, and M. Unberath, Reasoning segmentation for images and videos: survey, arXiv, 2025. 1, 9 [27] Y. Wei, D. Hu, Y. Tian, and X. Li, Learning in audio-visual context: review, analysis, and new perspective, arXiv, 2022. 1 [28] L. Xiao, X. Yang, X. Lan, Y. Wang, and C. Xu, Towards visual grounding: survey, arXiv, 2024. 1 [29] L. Ji, Y. Du, Y. Dang, W. Gao, and H. Zhang, survey of methods for addressing the challenges of referring image segmentation, Neurocomputing, 2024. 1 [30] S. He, H. Ding, C. Liu, and X. Jiang, GREC: Generalized referring expression comprehension, arXiv, 2023. 3, 4, 5, 6, [31] Y. Wang, H. Ding, S. He, X. Jiang, B. Wei, and J. Liu, Hierarchical alignment-enhanced adaptive grounding network for generalized referring expression comprehension, in AAAI, 2025. 3, 13, 17, 18 [32] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, Modeling context in referring expressions, in ECCV, 2016. 4, 7, 13 [33] C. Wu, Z. Lin, S. Cohen, T. Bui, and S. Maji, Phrasecut: Languagebased image segmentation in the wild, in CVPR, 2020. 4 [34] A. Khoreva, A. Rohrbach, and B. Schiele, Video object segmentation with language referring expressions, in ACCV, 2018. 4, 6, 10, 16 [35] K. Gavrilyuk, A. Ghodrati, Z. Li, and C. G. M. Snoek, Actor and action video segmentation from sentence, in CVPR, 2018. 4, 5, 6 [36] C. Xu, S.-H. Hsieh, C. Xiong, and J. J. Corso, Can humans fly? action understanding with multiple classes of actors, in CVPR, 2015. 4 [37] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, Towards understanding action recognition, in ICCV, 2013. 4 [38] S. Seo, J.-Y. Lee, and B. Han, Urvos: Unified referring video object segmentation network with large-scale benchmark, in ECCV, 2020. 4, 5, 6, 9, 16 [39] C. Yan, H. Wang, S. Yan, X. Jiang, Y. Hu, G. Kang, W. Xie, and E. Gavves, Visa: Reasoning video object segmentation via large language models, in ECCV, 2024. 4, 11, 16 [41] [40] Z. Bai, T. He, H. Mei, P. Wang, Z. Gao, J. Chen, L. Liu, Z. Zhang, and M. Z. Shou, One token to seg them all: Language instructed reasoning segmentation in videos, in NeurIPS, 2024. 4, 11, 16 J. Zhou, J. Wang, J. Zhang, W. Sun, J. Zhang, S. Birchfield, D. Guo, L. Kong, M. Wang, and Y. Zhong, Audiovisual segmentation, in ECCV, 2022. 4, 6, 16, 17 J. Zhou, X. Shen, J. Wang, J. Zhang, W. Sun, J. Zhang, S. Birchfield, D. Guo, L. Kong, M. Wang et al., Audio-visual segmentation with semantics, IJCV, 2024. 4, 16 [42] [43] D. Z. Chen, A. X. Chang, and M. Nießner, Scanrefer: 3d object localization in rgb-d scans using natural language, in ECCV, 2020. 4, 5, 13, 17 [44] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas, Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes, in ECCV, 2020. 4, 13 [45] Y. Hu, Q. Wang, W. Shao, E. Xie, Z. Li, J. Han, and P. Luo, Beyond one-to-one: Rethinking the referring image segmentation, in ICCV, 2023. 4, 13 [46] Y. Zhang, Z. Gong, and A. X. Chang, Multi3drefer: Grounding text description to multiple 3d objects, in ICCV, 2023. 4, 5, 13 [47] R. Liu, C. Liu, Y. Bai, and A. L. Yuille, Clevr-ref+: Diagnosing visual reasoning with referring expressions, in CVPR, 2019. 4 [48] W. Wang, T. Yue, Y. Zhang, L. Guo, X. He, X. Wang, and J. Liu, Unveiling parts beyond objects: Towards finer-granularity referring expression segmentation, in CVPR, 2024. 4, 15 [49] R. Li, Aeroreformer: Aerial referring transformer for uav-based referring image segmentation, arXiv, 2025. 19 [50] J. Liu, W. Wang, Y. Zhang, Y. Tang, X. He, L. Guo, T. Yue, and X. Wang, Towards unified referring expression segmentation across omni-level visual target granularities, arXiv, 2025. 4, 17 [51] R. Hu, L. Zhu, Y. Zhang, T. Cheng, L. Liu, H. Liu, L. Ran, X. Chen, W. Liu, and X. Wang, Groundingsuite: Measuring complex multigranular pixel grounding, arXiv, 2025. 4 [52] Y. Shen, C. Li, C. Fan, and M. Unberath, Rvtbench: benchmark for visual reasoning tasks, arXiv, 2025. [53] D.-H. Kim, H. Song, and D. Kim, Synres: Towards referring expression segmentation in the wild via synthetic data, arXiv, 2025. 4 [54] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung, benchmark dataset and evaluation methodology for video object segmentation, in CVPR, 2016. 4 [55] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in CVPR, 2017. 5 [56] C. Yeshwanth, Y.-C. Liu, M. Nießner, and A. Dai, Scannet++: highfidelity dataset of 3d indoor scenes, in ICCV, 2023. [57] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg, Mattnet: Modular attention network for referring expression comprehension, in CVPR, 2018. 5, 7, 10, 13, 15, 17, 18 [58] P.-H. Huang, H.-H. Lee, H.-T. Chen, and T.-L. Liu, Text-guided graph neural networks for referring 3d instance segmentation, in AAAI, 2021. 5, 12, 17 [59] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko, Modeling relationships in referential expressions with compositional modular networks, in CVPR, 2017. 5, 7 [60] X. Liu, Z. Wang, J. Shao, X. Wang, and H. Li, Improving referring expression grounding with cross-modal attention-guided erasing, in CVPR, 2019. 5 [61] C. Liang, Y. Wu, T. Zhou, W. Wang, Z. Yang, Y. Wei, and Y. Yang, Rethinking cross-modal interaction from top-down perspective for referring video object segmentation, arXiv, 2021. 5, 10 [62] C. Liu, X. Jiang, and H. Ding, Primitivenet: decomposing the global constraints for referring segmentation, Visual Intelligence, 2024. 5 [63] L. Xiao, X. Yang, F. Peng, Y. Wang, and C. Xu, Oneref: Unified one-tower expression grounding and segmentation with mask referring modeling, in NeurIPS, 2024. 5, 6, 8, 14, 15 [64] X. Wang, S. Zhang, S. Li, K. Kallidromitis, K. Li, Y. Kato, K. Kozuka, and T. Darrell, Segllm: Multi-round reasoning segmentation, in ICLR, 2025. 5, 9, 15, 18 [65] G. Luo, Y. Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji, Multi-task collaborative network for joint referring expression comprehension and segmentation, in CVPR, 2020. 5, 6, 7, 8, 12, 13, 15, 17 [66] D.-J. Chen, S. Jia, Y.-C. Lo, H.-T. Chen, and T.-L. Liu, See-throughtext grouping for referring image segmentation, in ICCV, 2019. 5, 6, 8, 15 [67] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, in ECCV, 2020. 5, [68] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016. 5 [69] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2020. 5 [70] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in ICCV, 2021. 5 [71] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, Learning spatiotemporal features with 3d convolutional networks, in ICCV, 2015. 5, 6 J. Carreira and A. Zisserman, Quo vadis, action recognition? new model and the kinetics dataset, in CVPR, 2017. 5, 6 [72] [73] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, Video swin transformer, in CVPR, 2022. 5 [74] S. He and H. Ding, RefMask3D: Language-guided transformer for 3d referring segmentation, ACM MM, 2024. 5, 6, 12, 17 [75] B. Graham, M. Engelcke, and L. Van Der Maaten, 3d semantic segmentation with submanifold sparse convolutional networks, in CVPR, 2018. 5 [76] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, Pointnet: Deep learning on point sets for 3d classification and segmentation, in CVPR, 2017. 5 [77] E. Margffoy-Tuay, J. C. Perez, E. Botero, and P. Arbelaez, Dynamic multimodal instance segmentation guided by natural language queries, in ECCV, 2018. 5, 6, 7, 15 [78] C. Liu, Z. Lin, X. Shen, J. Yang, X. Lu, and A. Yuille, Recurrent multimodal interaction for referring image segmentation, in ICCV, 2017. 5, 6, 7, 15 [104] H. Ding, K. Ying, C. Liu, S. He, Y.-G. Jiang, P. H. Torr, and S. Bai, MOSEv2: more challenging dataset for video object segmentation in complex scenes, arXiv, 2025. 6 20 [79] H. Shi, H. Li, F. Meng, and Q. Wu, Key-word-aware network for referring expression image segmentation, in ECCV, 2018. 5, 7, 15 [80] R. Li, K. Li, Y.-C. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, Referring image segmentation via recurrent refinement networks, in CVPR, 2018. 5, 6, 7, 8, 15 [82] [81] Z. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, Lavt: Language-aware vision transformer for referring image segmentation, in CVPR, 2022. 5, 6, 7, 9, 15, 17 J. Liu, H. Ding, Z. Cai, Y. Zhang, R. K. Satzoda, V. Mahadevan, and R. Manmatha, Polyformer: Referring image segmentation as sequential polygon generation, in CVPR, 2023. 5, 6, 8, 15 J. Wu, Y. Jiang, B. Yan, H. Lu, Z. Yuan, and P. Luo, Segment every reference object in spatial and temporal spaces, in ICCV, 2023. 5, 10, 16 [83] [84] N. Kim, D. Kim, C. Lan, W. Zeng, and S. Kwak, Restr: Convolutionfree referring image segmentation using transformers, in CVPR, 2022. 5, 8, 15 J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, BERT: pretraining of deep bidirectional transformers for language understanding, in NAACL-HLT, 2019. [85] [86] Y. Liu, Roberta: robustly optimized bert pretraining approach, arXiv, 2019. 5 [87] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021. 5, 6, 9 [88] Z. Wang, Y. Lu, Q. Li, X. Tao, Y. Guo, M. Gong, and T. Liu, Cris: Clip-driven referring image segmentation, in CVPR, 2022. 5, 6, 8, 15, 17 [89] Z. Xu, Z. Chen, Y. Zhang, Y. Song, X. Wan, and G. Li, Bridging vision and language encoders: Parameter-efficient tuning for referring image segmentation, in ICCV, 2023. 5, 6, 8, [90] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al., Cnn architectures for large-scale audio classification, in ICASSP, 2017. 6 [91] S. Schneider, A. Baevski, R. Collobert, and M. Auli, wav2vec: Unsupervised pre-training for speech recognition, in INTERSPEECH, 2019. 6 [92] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, Ssast: Self-supervised audio spectrogram transformer, in AAAI, 2022. 6 [93] Y. Wang, P. Sun, D. Zhou, G. Li, H. Zhang, and D. Hu, Ref-avs: Refer and segment objects in audio-visual scenes, in ECCV, 2024. 6, 11, 16, 17 [94] Z. Hu, G. Feng, J. Sun, L. Zhang, and H. Lu, Bi-directional relationship inferring network for referring image segmentation, in CVPR, 2020. 6, 7, [95] G. Feng, Z. Hu, L. Zhang, and H. Lu, Encoder fusion network with co-attention embedding for referring image segmentation, in CVPR, 2021. 6, 7, 15 [96] C. Liu, H. Ding, Y. Zhang, and X. Jiang, Multi-modal mutual attention and iterative interaction for referring image segmentation, IEEE TIP, 2023. 6, 7, 13, 15 [97] S. Kim, M. Kang, D. Kim, J. Park, and S. Kwak, Extending clips image-text alignment to referring image segmentation, in NAACL-HLT, 2024. 6, 8 [98] S. Yu, I. Jung, B. Han, T. Kim, Y. Kim, D. Wee, and J. Son, simple baseline with single-encoder for referring image segmentation, arXiv, 2024. 6, 8 [99] W. Wang, X. He, Y. Zhang, L. Guo, J. Shen, J. Li, and J. Liu, Cmmasksd: Cross-modality masked self-distillation for referring image segmentation, IEEE TMM, 2024. 6, 8, 15 [100] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.- H. Sung, Z. Li, and T. Duerig, Scaling up visual and vision-language representation learning with noisy text supervision, in ICML, 2021. 6 [101] Y. Ling, Y. Li, Z. Gan, J. Zhang, M. Chi, and Y. Wang, Transavs: Endto-end audio-visual segmentation with transformer, in ICASSP, 2024. 6, [102] J. Mei, A. Piergiovanni, J.-N. Hwang, and W. Li, Slvp: Self-supervised language-video pre-training for referring video object segmentation, in WACV, 2024. 6, 10 [103] H. Ding, C. Liu, S. He, X. Jiang, P. H. Torr, and S. Bai, MOSE: new dataset for video object segmentation in complex scenes, in ICCV, 2023. 6 [105] S. Liu, T. Hui, S. Huang, Y. Wei, B. Li, and G. Li, Cross-modal progressive comprehension for referring segmentation, IEEE TPAMI, 2021. 6, 8, 9, 16 [106] S. Huang, T. Hui, S. Liu, G. Li, Y. Wei, J. Han, L. Liu, and B. Li, Referring image segmentation via cross-modal progressive comprehension, in CVPR, 2020. 6, 8 [107] L. Ye, M. Rochan, Z. Liu, and Y. Wang, Cross-modal self-attention network for referring image segmentation, in CVPR, 2019. 6, 7, 10, 15 [108] G. Feng, L. Zhang, J. Sun, Z. Hu, and H. Lu, Referring segmentation via encoder-fused cross-modal attention network, IEEE TPAMI, 2022. 6, 10, 16 [109] C. Liang, W. Wang, T. Zhou, J. Miao, Y. Luo, and Y. Yang, Localglobal context aware transformer for language-guided video segmentation, IEEE TPAMI, 2023. 6, 10, [110] Z. Ding, T. Hui, J. Huang, X. Wei, J. Han, and S. Liu, Languagebridged spatial-temporal interaction for referring video object segmentation, in CVPR, 2022. 6, 10, 16 [111] S. Cho, S. Lee, M. Lee, J. Lee, and S. Lee, Find first, track next: Decoupling identification and propagation in referring video object segmentation, arXiv, 2025. 6, 10 [112] T. Liang, K.-Y. Lin, C. Tan, J. Zhang, W.-S. Zheng, and J.-F. Hu, Referdino: Referring video object segmentation with visual grounding foundations, arXiv, 2025. 6, 10 [113] S. Gong, Y. Zhuge, L. Zhang, Z. Yang, P. Zhang, and H. Lu, The devil is in temporal token: High quality video reasoning segmentation, in CVPR, 2025. 6, 11, 16 [114] W. Zhao, K. Wang, X. Chu, F. Xue, X. Wang, and Y. You, Modeling motion with multi-modal features for text-based video segmentation, in CVPR, 2022. 6, 10 [115] L. Yuan, M. Shi, Z. Yue, and Q. Chen, Losh: Long-short text joint prediction network for referring video object segmentation, in CVPR, 2024. 6, 10, [116] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, Masked-attention mask transformer for universal image segmentation, in CVPR, 2022. 6 [117] M. Li and L. Sigal, Referring transformer: one-step approach to multi-task visual grounding, NeurIPS, 2021. 6, 8, 15 [118] D. Wu, W. Han, T. Wang, X. Dong, X. Zhang, and J. Shen, Referring multi-object tracking, in CVPR, 2023. 6, 13 [119] H. Yuan, X. Li, T. Zhang, Z. Huang, S. Xu, S. Ji, Y. Tong, L. Qi, J. Feng, and M.-H. Yang, Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos, arXiv, 2025. 6, [120] C. Cuttano, G. Trivigno, G. Rosi, C. Masone, and G. Averta, Samwise: Infusing wisdom in sam2 for text-driven video segmentation, in CVPR, 2025. 6, 10, 16 [121] X. Huang, G. Luo, C. Zhu, B. Tong, Y. Zhou, X. Sun, and R. Ji, Deep instruction tuning for segment anything model, in ACM MM, 2024. 6, 8 [122] Y. Zhang, T. Cheng, R. Hu, L. Liu, H. Liu, L. Ran, X. Chen, W. Liu, and X. Wang, Evf-sam: Early vision-language fusion for text-prompted segment anything model, arXiv, 2024. 6 [123] Y. Sun, H. Zhang, H. Ding, T. Zhang, X. Ma, and Y.-G. Jiang, Sama: Towards multi-turn referential grounded video chat with large language models, arXiv, 2025. 6, 11 [124] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, in ICCV, 2023. 6, 8, 9, 10, 11 [125] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Radle, C. Rolland, L. Gustafson et al., Sam 2: Segment anything in images and videos, in ICLR, 2025. 6, 10, [126] Z. Luo, Y. Xiao, Y. Liu, S. Li, Y. Wang, Y. Tang, X. Li, and Y. Yang, Soc: Semantic-assisted object cluster for referring video object segmentation, in NeurIPS, 2024. 6, 10, 16 [127] P. Sun, H. Zhang, and D. Hu, Unveiling and mitigating bias in audio visual segmentation, ACM MM, 2024. 6, 11 [128] Y.-W. Chen, Y.-H. Tsai, T. Wang, Y.-Y. Lin, and M.-H. Yang, Referring expression object segmentation with caption-aware consistency, BMVC, 2019. 7, 8, 13 [129] D. Liu, H. Zhang, F. Wu, and Z.-J. Zha, Learning to assemble neural module tree networks for visual grounding, in ICCV, 2019. 7 [130] C. Liu, X. Jiang, and H. Ding, Instance-specific feature propagation for referring segmentation, IEEE TMM, 2023. 21 [131] Z. Cheng, P. Jin, H. Li, K. Li, S. Li, X. Ji, C. Liu, and J. Chen, Wico: Win-win cooperation of bottom-up and top-down referring image segmentation, IJCAI, 2023. 7 [159] J. Huang, Z. Xu, T. Liu, Y. Liu, H. Han, K. Yuan, and X. Li, Densely connected parameter-efficient tuning for referring image segmentation, in AAAI, 2025. 8 [132] Y. Jiao, Z. Jie, W. Luo, J. Chen, Y.-G. Jiang, X. Wei, and L. Ma, Twostage visual cues enhancement network for referring image segmentation, in ACM MM, 2021. 7 [160] H. Yu, M. Li, A. Rezazadeh, Y. Yang, and C. Choi, parameterefficient tuning framework for language-guided object grounding and robot grasping, in ICRA, 2025. 8 [133] L. Xu, M. H. Huang, X. Shang, Z. Yuan, Y. Sun, and J. Liu, Meta compositional referring expression segmentation, in CVPR, 2023. 7 [134] P. Yue, J. Lin, S. Zhang, J. Hu, Y. Lu, H. Niu, H. Ding, Y. Zhang, G. JIANG, L. Cao et al., Adaptive selection based referring image segmentation, in ACM MM, 2024. 7 [135] W. Su, P. Miao, H. Dou, G. Wang, L. Qiao, Z. Li, and X. Li, Language adaptive weight generation for multi-task visual grounding, in CVPR, 2023. 7, 15 [136] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu, Unleashing textto-image diffusion models for visual perception, in ICCV, 2023. 7, 15 [137] S. Ha, C. Kim, D. Kim, J. Lee, S. Lee, and J. Lee, Finding nemo: Negative-mined mosaic augmentation for referring image segmentation, in ECCV, 2025. 7 [138] M. Lee, S. Lee, S. Park, D. Han, B. Heo, and H. Shim, Maskris: Semantic distortion-aware data augmentation for referring image segmentation, arXiv, 2024. [139] T. Luddecke and A. Ecker, Image segmentation using text and image prompts, in CVPR, 2022. 7 [140] J. Park, J. Lee, J. Song, S. Yu, D. Jung, and S. Yoon, Know no better: data-driven approach for enhancing negation awareness in clip, arXiv, 2025. 7 [141] J. Yang, L. Zhang, J. Sun, and H. Lu, Spatial semantic recurrent mining for referring image segmentation, arXiv, 2024. 7 [142] I. Kesen, O. A. Can, E. Erdem, A. Erdem, and D. Yuret, Modulating bottom-up and top-down visual processing via language-conditional filters, in CVPR Workshop, 2022. 7, [143] H. Ding, S. Zhang, Q. Wu, S. Yu, J. Hu, L. Cao, and R. Ji, Bilateral knowledge interaction network for referring image segmentation, IEEE TMM, 2024. 7, 15 [144] S. Ouyang, H. Wang, S. Xie, Z. Niu, R. Tong, Y.-W. Chen, and L. Lin, Slvit: Scale-wise language-guided vision transformer for referring image segmentation. in IJCAI, 2023. 7 [145] Z. Wei, X. Chen, M. Chen, and S. Zhu, Linguistic query-guided mask generation for referring image segmentation, arXiv, 2023. 7 [146] J. Tang, G. Zheng, C. Shi, and S. Yang, Contrastive grouping with transformer for referring image segmentation, in CVPR, 2023. 7, 8, 15 [147] N. A. Shah, V. VS, and V. M. Patel, Lqmformer: Language-aware query mask transformer for referring image segmentation, in CVPR, 2024. 7, 13, 15, 17 [148] Y. Yang, C. Ma, J. Yao, Z. Zhong, Y. Zhang, and Y. Wang, Remamber: Referring image segmentation with mamba twister, ECCV, 2024. 8, 15 [149] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, in COLM, 2024. 8 [150] Z. Liu, A. Saseendran, L. Tong, X. He, F. Yousefi, N. Burlutskiy, D. Oglic, T. Diethe, P. Teare, H. Zhou et al., Segment anyword: Mask prompt inversion for open-set grounded segmentation, in ICML, 2025. 8, 17 [151] S.-A. Liu, Y. Zhang, Z. Qiu, H. Xie, Y. Zhang, and T. Yao, Caris: Context-aware referring image segmentation, in ACM MM, 2023. 8 [152] Y. X. Chng, H. Zheng, Y. Han, X. Qiu, and G. Huang, Mask grounding for referring image segmentation, in CVPR, 2024. 8, [153] Z. Zhang, Y. Zhu, J. Liu, X. Liang, and W. Ke, Coupalign: Coupling word-pixel with sentence-mask alignments for referring image segmentation, NeurIPS, 2022. 8, 15 [154] Y. Cho, H. Yu, and S.-J. Kang, Cross-aware early fusion with stagedivided vision and language transformer encoders for referring image segmentation, IEEE TMM, 2024. 8 [155] M. Lee, D. Lee, J. Lee, S. Cho, H. Choi, I.-J. Kim, and S. Lee, Synchronizing vision and language: Bidirectional token-masking autoencoder for referring image segmentation, arXiv, 2023. 8 [156] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in CVPR, 2022. 8 [157] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, and F. Wei, Image as foreign language: BEIT pretraining for vision and vision-language tasks, in CVPR, 2023. [161] Z. Huang and S. Satoh, Referring image segmentation via joint mask contextual embedding learning and progressive alignment network, in EMNLP, 2023. 8 [162] Y. Iioka, Y. Yoshida, Y. Wada, S. Hatanaka, and K. Sugiura, Multimodal diffusion segmentation model for object segmentation from manipulation instructions, in IROS, 2023. 8 [163] Z. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, Semantics-aware dynamic localization and refinement for referring image segmentation, in AAAI, 2023. 8 [164] L. Ye, Z. Liu, and Y. Wang, Dual convolutional lstm network for referring image segmentation, IEEE TMM, 2020. 8, 15 [165] S. Yang, M. Xia, G. Li, H.-Y. Zhou, and Y. Yu, Bottom-up shift and reasoning for referring image segmentation, in CVPR, 2021. 8, 15 [166] T. Hui, S. Liu, S. Huang, G. Li, S. Yu, F. Zhang, and J. Han, Linguistic structure guided context modeling for referring image segmentation, in ECCV, 2020. 8, 15 [167] S. Wu, S. Jin, W. Zhang, L. Xu, W. Liu, W. Li, and C. C. Loy, F-lmm: Grounding frozen large multimodal models, in CVPR, 2025. 8 [168] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan et al., Grounded sam: Assembling open-world models for diverse visual tasks, arXiv, 2024. 8, 18 [169] C. Shang, Z. Song, H. Qiu, L. Wang, F. Meng, and H. Li, Promptdriven referring image segmentation with instance contrasting, in CVPR, 2024. 8, 15 [170] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su et al., Grounding dino: Marrying dino with grounded pre-training for open-set object detection, in ECCV, 2024. 8, 17 [171] Z. Xia, D. Han, Y. Han, X. Pan, S. Song, and G. Huang, Gsva: Generalized segmentation via multimodal large language models, in CVPR, 2024. 8, 9, 13, 15, 17, 18 [172] Y.-C. Chen, W.-H. Li, C. Sun, Y.-C. F. Wang, and C.-S. Chen, Sam4mllm: Enhance multi-modal large language model for referring expression segmentation, in ECCV, 2024. 8, 9, 14, 15, [173] Y. Jing, T. Kong, W. Wang, L. Wang, L. Li, and T. Tan, Locate then segment: strong pipeline for referring image segmentation, in CVPR, 2021. 8, 15, 17 [174] Z. Cheng, K. Li, P. Jin, S. Li, X. Ji, L. Yuan, C. Liu, and J. Chen, Parallel vertex diffusion for unified visual grounding, in AAAI, 2024. 8 [175] Z. Pang, X. Xu, and Y.-X. Wang, Aligning generative denoising with discriminative objectives unleashes diffusion for visual perception, in ICLR, 2025. 8, 14, 15 [176] G. Luo, Y. Zhou, R. Ji, X. Sun, J. Su, C.-W. Lin, and Q. Tian, Cascade grouped attention network for referring expression segmentation, in ACM MM, 2020. 8 [177] M. Qu, Y. Wu, Y. Wei, W. Liu, X. Liang, and Y. Zhao, Learning to segment every referring object point by point, in CVPR, 2023. 8, 9, 15 [178] C. Zhu, Y. Zhou, Y. Shen, G. Luo, X. Pan, M. Lin, C. Chen, L. Cao, X. Sun, and R. Ji, Seqtr: simple yet universal network for visual grounding, in ECCV, 2022. 8, 12, 15 [179] M. Lan, C. Chen, Y. Zhou, J. Xu, Y. Ke, X. Wang, L. Feng, and W. Zhang, Text4seg: Reimagining image segmentation as text generation, in ICLR, 2025. [180] H. Tang, C. Xie, H. Wang, X. Bao, T. Weng, P. Li, Y. Zheng, and L. Wang, Ufo: unified approach to fine-grained visual perception via open-ended language interface, arXiv, 2025. 8 [181] S. Cheng, Y. Liu, X. He, S. Ourselin, L. Tan, and G. Luo, Weakmcn: Multi-task collaborative network for weakly supervised referring expression comprehension and segmentation, in CVPR, 2025. 8, 9 [182] W. Kang, G. Liu, M. Shah, and Y. Yan, Segvg: Transferring object bounding box to segmentation for visual grounding, in ECCV, 2024. 8 [183] J. Wang, H. Wang, W. Zhang, K. Ji, D. Huang, and Y. Zheng, Progressive language-guided visual learning for multi-task visual grounding, arXiv, 2025. 8 [184] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion, Mdetr-modulated detection for end-to-end multi-modal understanding, in ICCV, 2021. 8, 17 [158] Y. Wang, J. Li, X. Zhang, B. Shi, C. Li, W. Dai, H. Xiong, and Q. Tian, Barleria: An efficient tuning framework for referring image segmentation, in ICLR, 2024. 8, 15 [185] X. Yang, L. Xu, H. Sun, H. Li, and S. Zhang, Enhancing visual grounding and generalization: multi-task cycle training approach for vision-language models, arXiv, 2023. 8 [186] X. Zou, Z.-Y. Dou, J. Yang, Z. Gan, L. Li, C. Li, X. Dai, H. Behl, J. Wang, L. Yuan et al., Generalized decoding for pixel, image, and language, in CVPR, 2023. 8, 15, [187] X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Wang, L. Wang, J. Gao, and Y. J. Lee, Segment everything everywhere all at once, NeurIPS, 2023. 8, 15, 18 [188] J. Wu, Y. Jiang, Q. Liu, Z. Yuan, X. Bai, and S. Bai, General object foundation model for images and videos at scale, in CVPR, 2024. 8 [189] Z. Zhang, Y. Ma, E. Zhang, and X. Bai, Psalm: Pixelwise segmentation with large multi-modal model, in ECCV, 2024. 8 [190] T.-J. Fu, Y. Qian, C. Chen, W. Hu, Z. Gan, and Y. Yang, Univg: generalist diffusion model for unified image generation and editing, arXiv, 2025. [191] B. Yan, Y. Jiang, J. Wu, D. Wang, P. Luo, Z. Yuan, and H. Lu, Universal instance perception as object discovery and retrieval, in CVPR, 2023. 8, 14, 15, 17 [192] T. Zhang, X. Li, Z. Huang, Y. Li, W. Lei, X. Deng, S. Chen, S. Ji, and J. Feng, Pixel-sail: Single transformer for pixel-grounded understanding, arXiv, 2025. 8 [193] J. He, Y. Wang, L. Wang, H. Lu, J.-Y. He, J.-P. Lan, B. Luo, and X. Xie, Multi-modal instruction tuned llms with fine-grained visual perception, in CVPR, 2024. 8, 15 [194] R. Strudel, I. Laptev, and C. Schmid, Weakly-supervised segmentation of referring expressions, arXiv, 2022. 8 [195] Q. Dai and S. Yang, Curriculum point prompting for weaklysupervised referring image segmentation, in CVPR, 2024. 8, 15 [196] D. Kim, N. Kim, C. Lan, and S. Kwak, Shatter and gather: Learning referring image segmentation with text supervision, in ICCV, 2023. 8, [197] H. Li, M. Sun, J. Xiao, E. G. Lim, and Y. Zhao, Fully and weakly supervised referring expression segmentation with end-to-end learning, IEEE TCSVT, 2023. 8, 9 [198] J. Lee, S. Lee, J. Nam, S. Yu, J. Do, and T. Taghavi, Weakly supervised referring image segmentation with intra-chunk and interchunk consistency, in ICCV, 2023. 8, 15 [199] J. Xu, S. De Mello, S. Liu, W. Byeon, T. Breuel, J. Kautz, and X. Wang, Groupvit: Semantic segmentation emerges from text supervision, in CVPR, 2022. 8 [200] X. Chen, Y. Luo, G. Luo, J. Ji, H. Ding, and Y. Zhou, Dvin: Dynamic visual routing network for weakly supervised referring expression comprehension, in CVPR, 2025. 8 [201] A. Arbelle, S. Doveh, A. Alfassy, J. Shtok, G. Lev, E. Schwartz, H. Kuehne, H. B. Levi, P. Sattigeri, R. Panda et al., Detector-free weakly supervised grounding by separation, in ICCV, 2021. 8 [202] F. Liu, Y. Liu, Y. Kong, K. Xu, L. Zhang, B. Yin, G. Hancke, and R. Lau, Referring image segmentation using text supervision, in ICCV, 2023. 8, 15 [203] Z. Yang, Y. Liu, J. Lin, G. Hancke, and R. W. Lau, Boosting weaklysupervised referring image segmentation via progressive comprehension, NeurIPS, 2024. 8, [204] G. Feng, L. Zhang, Z. Hu, and H. Lu, Learning from box annotations for referring image segmentation, TNNLS, 2024. 9 [205] H. Lyu, T. Zhong, and S. Zhao, Gtms: gradient-driven tree-guided mask-free referring image segmentation method, in ECCV, 2024. 9, 15 [206] D. Yang, J. Ji, Y. Ma, T. Guo, H. Wang, X. Sun, and R. Ji, Sam as the guide: Mastering pseudo-label refinement in semi-supervised referring expression segmentation, ICML, 2024. 9, 15 [207] S. Yu, P. H. Seo, and J. Son, Pseudo-ris: Distinctive pseudo-supervision generation for referring image segmentation, in ECCV, 2024. 9 [208] Y. Zang, R. Cao, C. Fu, D. Zhu, M. Zhang, W. Hu, L. Zhu, and T. Chen, Resmatch: Referring expression segmentation in semi-supervised manner, Information Sciences, 2025. 9 [209] S. Nag, K. Goswami, and S. Karanam, Safari: Adaptive sequence transformer for weakly supervised referring expression segmentation, ECCV, 2024. 9, 15 [210] S. Yu, P. H. Seo, and J. Son, Zero-shot referring image segmentation with global-local context features, in CVPR, 2023. 9, 15 [211] Y. Wang, J. Ni, Y. Liu, C. Yuan, and Y. Tang, Iterprime: Zero-shot referring image segmentation with iterative grad-cam refinement and primary word emphasis, AAAI, 2025. 9 [212] R. Burgert, K. Ranasinghe, X. Li, and M. S. Ryoo, Peekaboo: Text to 22 [215] S. Sun, R. Li, P. Torr, X. Gu, and S. Li, Clip as rnn: Segment countless visual concepts without training endeavor, in CVPR, 2024. [216] T. Liu and S. Li, Hybrid global-local representation with augmented spatial guidance for zero-shot referring image segmentation, in CVPR, 2025. 9, 15 [217] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv, 2023. 9, 11 [218] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, NeurIPS, 2024. 9, 11, 12 [219] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in CVPR, 2024. 9, 11, 12 [220] Z. Ren, Z. Huang, Y. Wei, Y. Zhao, D. Fu, J. Feng, and X. Jin, Pixellm: Pixel reasoning with large multimodal model, in CVPR, 2024. 9, 15 [221] Y. Lu, J. Cao, Y. Wu, B. Li, L. Tang, Y. Ji, C. Wu, J. Wu, and W. Zhu, Rsvp: Reasoning segmentation via visual prompting and multi-modal chain-of-thought, in ACL, 2025. 9 [222] M. Wahed, K. A. Nguyen, A. S. Juvekar, X. Li, X. Zhou, V. Shah, T. Yu, P. Yanardag, and I. Lourentzou, Prima: Multi-image vision-language models for reasoning segmentation, in AAAI, 2025. 9 [223] M. Siam, Pixfoundation: Are we heading in the right direction with pixel-level vision foundation models? arXiv, 2025. 9 [224] Y. Yuan, W. Li, J. Liu, D. Tang, X. Luo, C. Qin, L. Zhang, and J. Zhu, Osprey: Pixel understanding with visual instruction tuning, in CVPR, 2024. [225] C. Wei, Y. Zhong, H. Tan, Y. Zeng, Y. Liu, Z. Zhao, and Y. Yang, Instructseg: Unifying instructed visual segmentation with multi-modal large language models, arXiv, 2024. 9, 11 [226] A. Zhang, Y. Yao, W. Ji, Z. Liu, and T.-S. Chua, Next-chat: An lmm for chat, detection and segmentation, in ICML, 2024. 9 [227] R. Pi, L. Yao, J. Gao, J. Zhang, and T. Zhang, Perceptiongpt: Effectively fusing visual perception into llm, in CVPR, 2024. 9, 15 [228] X. Bao, S. Sun, S. Ma, K. Zheng, Y. Guo, G. Zhao, Y. Zheng, and X. Wang, Cores: Orchestrating the dance of reasoning and segmentation, in ECCV, 2024. 9, 15, 18 [229] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-of-thought prompting elicits reasoning in large language models, in NeurIPS, 2022. 9 [230] R. Qian, X. Yin, and D. Dou, Reasoning to attend: Try to understand how <seg> token works, in CVPR, 2025. 9, 15, 18 [231] J. Wang and L. Ke, Llm-seg: Bridging image segmentation and large language model reasoning, in CVPR Workshop, 2024. 9, 18 [232] C. Wei, H. Tan, Y. Zhong, Y. Yang, and L. Ma, Lasagna: Languagebased segmentation assistant for complex queries, arXiv, 2024. 9, 17 [233] T.-H. Wu, G. Biamby, D. Chan, L. Dunlap, R. Gupta, X. Wang, J. E. Gonzalez, and T. Darrell, See say and segment: Teaching lmms to overcome false premises, in CVPR, 2024. 9, [234] D. Jang, Y. Cho, S. Lee, T. Kim, and D.-S. Kim, Mmr: largescale benchmark dataset for multi-target and multi-granularity reasoning segmentation, in ICLR, 2025. 9, 15 [235] D. Cai, X. Yang, Y. Liu, D. Wang, S. Feng, Y. Zhang, and S. Poria, Pixel-level reasoning segmentation via multi-turn conversations, arXiv, 2025. 9 [236] Y. Liu, B. Peng, Z. Zhong, Z. Yue, F. Lu, B. Yu, and J. Jia, Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement, arXiv, 2025. 9, 18 [237] L. Zhu, T. Chen, Q. Xu, X. Liu, D. Ji, H. Wu, D. W. Soh, and J. Liu, Popen: Preference-based optimization and ensemble for lvlm-based reasoning segmentation, in CVPR, 2025. 9, 14, 15, 18 [238] S. Wang, G. Fang, L. Kong, X. Li, J. Xu, S. Yang, Q. Li, J. Zhu, and X. Wang, Pixelthink: Towards efficient chain-of-pixel reasoning, arXiv, 2025. 9, 18 [239] J. Huang, Z. Xu, J. Zhou, T. Liu, Y. Xiao, M. Ou, B. Ji, X. Li, and K. Yuan, Sam-r1: Leveraging sam for reward feedback in multimodal segmentation via reinforcement learning, arXiv, 2025. 9, [240] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv, 2024. 9 image diffusion models are zero-shot segmentors, in CVPR, 2023. 9 [241] Z. Yuan, L. Mou, Y. Hua, and X. X. Zhu, Rrsis: Referring remote [213] R. Wang and H. Zhang, Resanything: Attribute prompting for arbitrary sensing image segmentation, IEEE TGRS, 2024. 9 referring segmentation, arXiv, 2025. [214] Y. Suo, L. Zhu, and Y. Yang, Text augmented spatial-aware zero-shot referring image segmentation, EMNLP, 2023. 9 [242] Y. Pan, R. Sun, Y. Wang, T. Zhang, and Y. Zhang, Rethinking the implicit optimization paradigm with dual alignments for referring remote sensing image segmentation, in ACM MM, 2024. 9 [243] S. Liu, Y. Ma, X. Zhang, H. Wang, J. Ji, X. Sun, and R. Ji, Rotated multi-scale interaction network for referring remote sensing image segmentation, in CVPR, 2024. 9 [244] K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. S. Khan, Geochat: Grounded large vision-language model for remote sensing, in CVPR, 2024. 9 [245] X. Li, J. Ding, and M. Elhoseiny, Vrsbench: versatile visionlanguage benchmark dataset for remote sensing image understanding, in NeurIPS, 2024. [246] P. Rewatbowornwong, N. Chatthee, E. Chuangsuwanich, and S. Suwajanakorn, Zero-guidance segmentation using zero segment labels, in ICCV, 2023. 9 [247] H. Rasheed, M. Maaz, S. Shaji, A. Shaker, S. Khan, H. Cholakkal, R. M. Anwer, E. Xing, M.-H. Yang, and F. S. Khan, Glamm: Pixel grounding large multimodal model, in CVPR, 2024. 9, 15 [248] H. Shi, W. Pan, Z. Zhao, M. Zhang, and F. Wu, Unsupervised domain adaptation for referring semantic segmentation, in ACM MM, 2023. 9 [249] C. Jiang, A. Luo, and M. Jagersand, Robot manipulation in salient vision through referring image segmentation and geometric constraints, in ICRA, 2025. 9 [250] C. Jiang, Y. Yang, and M. Jagersand, Clipunetr: Assisting humanrobot interface for uncalibrated visual servoing control with clip-driven referring expression segmentation, in ICRA, 2024. 9 [251] X. Jiang, H. Yang, K. Zhu, X. Qiu, S. Zhao, and S. Zhou, Ptq4ris: Post-training quantization for referring image segmentation, in ICRA, 2025. 9 [252] M. Bellver, C. Ventura, C. Silberer, I. Kazakos, J. Torres, and X. Giro-i Nieto, closer look at referring expressions for video object segmentation, Multimedia Tools and Applications, 2022. [253] D. Wu, T. Wang, Y. Zhang, X. Zhang, and J. Shen, Onlinerefer: simple online baseline for referring video object segmentation, in ICCV, 2023. 9, 10, 16 [254] R. Yan, W. Guo, X. Liu, X. Liu, Y. Zhang, and X. Yuan, Trackingforced referring video object segmentation, in ACM MM, 2024. 10 [255] B. Miao, M. Bennamoun, Y. Gao, M. Shah, and A. Mian, Temporally consistent referring video object segmentation with hybrid memory, IEEE TCSVT, 2024. 10 [256] J. Tang, G. Zheng, and S. Yang, Temporal collection and distribution for referring video object segmentation, in ICCV, 2023. 10, 16 [257] T. Hui, S. Liu, Z. Ding, S. Huang, G. Li, W. Wang, L. Liu, and J. Han, Language-aware spatial-temporal collaboration for referring video segmentation, IEEE TPAMI, 2023. 10, 16 [258] M. Han, Y. Wang, Z. Li, L. Yao, X. Chang, and Y. Qiao, Html: Hybrid temporal-scale multimodal learning framework for referring video object segmentation, in ICCV, 2023. 10, [259] Z. Zhou, W. Xiong, L. Zhou, X. Li, Z. He, and Y. Wang, Harnessing vision-language pretrained models with temporal-aware adaptation for referring video object segmentation, arXiv, 2024. 10 [260] H. Wang, C. Deng, F. Ma, and Y. Yang, Context modulated dynamic networks for actor and action video segmentation with language queries, in AAAI, 2020. 10 [261] T. Hui, S. Huang, S. Liu, Z. Ding, G. Li, W. Wang, J. Han, and F. Wang, Collaborative spatial-temporal modeling for language-queried video actor segmentation, in CVPR, 2021. 10 [262] T. Liang, H. Jiang, Y. Yang, C. Tan, S. Li, W.-S. Zheng, and J.-F. Hu, Long-rvos: comprehensive benchmark for long-term referring video object segmentation, arXiv, 2025. 10 [263] B. McIntosh, K. Duarte, Y. S. Rawat, and M. Shah, Visual-textual capsule routing for text-based video segmentation, in CVPR, 2020. 10 [264] D. Wu, X. Dong, L. Shao, and J. Shen, Multi-level representation learning with semantic alignment for referring video object segmentation, in CVPR, 2022. 10, 16 [265] W. Chen, D. Hong, Y. Qi, Z. Han, S. Wang, L. Qing, Q. Huang, and G. Li, Multi-attention network for compressed video referring object segmentation, in ACM MM, 2022. [266] Z. Zhu, X. Feng, D. Chen, J. Yuan, C. Qiao, and G. Hua, Exploring pre-trained text-to-video diffusion models for referring video object segmentation, in ECCV, 2024. 10, 16 [267] H. Wang, C. Deng, J. Yan, and D. Tao, Asymmetric cross-guided attention network for actor and action video segmentation from natural language query, in ICCV, 2019. 10 [268] B. Miao, M. Bennamoun, Y. Gao, and A. Mian, Spectrum-guided multi-granularity referring video object segmentation, in ICCV, 2023. 10, 16 23 [270] X. Yang, H. Wang, D. Xie, C. Deng, and D. Tao, Object-agnostic transformers for video referring segmentation, IEEE TIP, 2022. [271] J. Wu, Y. Jiang, P. Sun, Z. Yuan, and P. Luo, Language as queries for referring video object segmentation, in CVPR, 2022. 10, 16, 17 [272] M. Gao, J. Yang, J. Han, K. Lu, F. Zheng, and G. Montana, Decoupling multimodal transformers for referring video object segmentation, IEEE TCSVT, 2023. 10 [273] F. Pan, H. Fang, F. Li, Y. Xu, Y. Li, L. Benini, and X. Lu, Semantic and sequential alignment for referring video object segmentation, in CVPR, 2025. 10 [274] Y. Li, J. Zhang, X. Teng, L. Lan, and X. Liu, Refsam: Efficiently adapting segmenting anything model for referring video object segmentation, arXiv, 2023. 10 [275] F. Rong, M. Lan, Q. Zhang, and L. Zhang, Mpg-sam 2: Adapting sam 2 with mask priors and global context for referring video object segmentation, arXiv, 2025. 10 [276] M. Li, S. Li, X. Zhang, and L. Zhang, Univs: Unified and universal video segmentation with prompts as queries, in CVPR, 2024. 10, [277] M. Sun, J. Xiao, E. G. Lim, C. Zhao, and Y. Zhao, Unified multilearning, modality video object segmentation using reinforcement IEEE TCSVT, 2023. 10 [278] S. Yan, R. Zhang, Z. Guo, W. Chen, W. Zhang, H. Li, Y. Qiao, H. Dong, Z. He, and P. Gao, Referred by multi-modality: unified temporal transformer for video object segmentation, in AAAI, 2024. 10, 16, 17 [279] X. Li, H. Yuan, W. Li, H. Ding, S. Wu, W. Zhang, Y. Li, K. Chen, and C. C. Loy, OMG-Seg: Is one model good enough for all segmentation? in CVPR, 2024. 10 [280] S. Huang, R. Ling, H. Li, T. Hui, Z. Tang, X. Wei, J. Han, and S. Liu, Unleashing the temporal-spatial reasoning capacity of gpt for training-free audio and language referenced video object segmentation, in AAAI, 2025. 10 [281] S. Yu, D. Liu, Z. Ma, Y. Hong, Y. Zhou, H. Tan, J. Chai, and M. Bansal, Veggie: Instructional editing and reasoning of video concepts with grounded generation, arXiv, 2025. 10 [282] A. Botach, E. Zheltonozhskii, and C. Baskin, End-to-end referring video object segmentation with multimodal transformers, in CVPR, 2022. 10, [283] S. Kim, W. Jin, S. Lim, H. Yoon, H. Choi, and S. Kim, Referring video object segmentation via language-aligned track selection, arXiv, 2024. 10 [284] H. Fang, R. Cong, X. Lu, X. Zhou, S. Kwong, and W. Zhang, Decoupled motion expression video segmentation, in CVPR, 2025. 10 [285] W. Zhao, K. Nan, S. Zhang, K. Chen, D. Lin, and Y. You, Learning referring video object segmentation from weak annotation, arXiv, 2023. 11 [286] C.-S. Lin, I. Liu, M.-H. Chen, C.-Y. Wang, S. Liu, Y.-C. F. Wang et al., Groprompt: Efficient grounded prompting and adaptation for referring video object segmentation, in CVPR Workshop, 2024. 11 [287] R. Zheng, L. Qi, X. Chen, Y. Wang, K. Wang, Y. Qiao, and H. Zhao, Villa: Video reasoning segmentation with large language model, arXiv, 2024. 11 [288] J. Zhu, Z.-Q. Cheng, J.-Y. He, C. Li, B. Luo, H. Lu, Y. Geng, and X. Xie, Tracking with human-intent reasoning, arXiv, 2023. 11 [289] Y. Shen, B. Liu, C. Li, L. Seenivasan, and M. Unberath, Online reasoning video segmentation with just-in-time digital twins, arXiv, 2025. [290] L. Lin, X. Yu, Z. Pang, and Y.-X. Wang, Glus: Global-local reasoning unified into single large language model for video segmentation, in CVPR, 2025. 11, 16 [291] S. Munasinghe, H. Gani, W. Zhu, J. Cao, E. Xing, F. S. Khan, and S. Khan, Videoglamm: large multimodal model for pixel-level visual grounding in videos, in CVPR, 2024. 11 [292] X. Li, J. Wang, X. Xu, M. Yang, F. Yang, Y. Zhao, R. Singh, and B. Raj, Towards noise-tolerant speech-referring video object segmentation: Bridging speech and text, in EMNLP, 2023. 11 [293] G. Li, M. Gao, H. Liu, X. Zhen, and F. Zheng, Learning crossmodal affinity for referring video object segmentation targeting limited samples, in ICCV, 2023. 11 [294] L. Ouyang, R. Liu, Y. Huang, R. Furuta, and Y. Sato, Actionvos: Actions as prompts for video object segmentation, in ECCV, 2024. 11 [269] D. Li, R. Li, L. Wang, Y. Wang, J. Qi, L. Zhang, T. Liu, Q. Xu, and H. Lu, You only infer once: Cross-modal meta-transfer for referring video object segmentation, in AAAI, 2022. [295] A. Deng, T. Chen, S. Yu, T. Yang, L. Spencer, Y. Tian, A. S. Mian, M. Bansal, and C. Chen, Motion-grounded video reasoning: Understanding and perceiving motion at pixel level, in CVPR, 2025. 11 [296] A. Athar, X. Deng, and L.-C. Chen, Vicas: dataset for combining holistic and pixel-level video understanding using captions with grounded segmentation, in CVPR, 2025. 11 [297] H. Liu, G. Li, M. Gao, X. Zhen, F. Zheng, and Y. Wang, Few-shot referring video singleand multi-object segmentation via cross-modal affinity with instance sequence matching, IJCV, 2025. 11 [298] K. Ying, H. Hu, and H. Ding, MOVE: Motion-guided few-shot video object segmentation, in ICCV, 2025. 11 [299] H. Liu, M. Gao, X. Luo, Z. Wang, G. Qin, J. Wu, and Y. Jin, Resurgsam2: Referring segment anything in surgical video via credible long-term tracking, arXiv, 2025. [300] R. Yuan, M. Chen, J. Xu, L. Zhou, Q. Li, Y. Zhang, R. Feng, T. Zhang, and S. Gao, Text-promptable propagation for referring medical image sequence segmentation, arXiv, 2025. 11 [301] Q. Yang, X. Nie, T. Li, P. Gao, Y. Guo, C. Zhen, P. Yan, and S. Xiang, Cooperation does matter: Exploring multi-order bilateral relations for audio-visual segmentation, in CVPR, 2024. 11, 16 [302] X. Li, J. Wang, X. Xu, X. Peng, R. Singh, Y. Lu, and B. Raj, Qdformer: Towards robust audiovisual segmentation in complex environments with quantization-based semantic decomposition, in CVPR, 2024. 11, 16 [303] K. Li, Z. Yang, L. Chen, Y. Yang, and J. Xiao, Catr: Combinatorialdependence audio-queried transformer for audio-visual video segmentation, in ACM MM, 2023. 11 [304] Y.-B. Lin, Y.-L. Sung, J. Lei, M. Bansal, and G. Bertasius, Vision transformers are parameter-efficient audio-visual learners, in CVPR, 2023. 11, 16 [305] J. Li, S. Yu, Y. Wang, L. Wang, and H. Lu, Selm: Selective mechanism based audio-visual segmentation, in ACM MM, 2024. 11 [306] Y. Mao, J. Zhang, M. Xiang, Y. Zhong, and Y. Dai, Multimodal variational auto-encoder based audio-visual segmentation, in ICCV, 2023. 11, 16 [307] S. Mo and P. Morgado, Unveiling the power of audio-visual early fusion transformers with dense interactions through masked modeling, in CVPR, 2024. 11, 16 [308] Y. Wang, W. Liu, G. Li, J. Ding, D. Hu, and X. Li, Prompting segmentation with sound is generalizable audio-visual source localizer, in AAAI, 2024. 11, 16 [309] Z. Shi, Q. Wu, F. Meng, L. Xu, and H. Li, Cross-modal cognitive consensus guided audio-visual segmentation, IEEE TMM, 2024. 11, 16 [310] S. Gao, Z. Chen, G. Chen, W. Wang, and T. Lu, Avsegformer: Audiovisual segmentation with transformer, in AAAI, 2024. 11, 16 [311] Y. Guo, S. Ma, S. Ma, X. Bao, C.-W. Xie, K. Zheng, T. Weng, S. Sun, Y. Zheng, and W. Zou, Aligned better, listen better for audio-visual large language models, in ICLR, 2025. 11 [312] C. Liu, P. Li, L. Yang, D. Wang, L. Li, and X. Yu, Robust audio-visual segmentation via audio-guided visual convergent alignment, in CVPR, 2025. 11, 16 [313] C. Liu, L. Yang, P. Li, D. Wang, L. Li, and X. Yu, Dynamic derivation and elimination: Audio visual segmentation with enhanced audio semantics, in CVPR, 2025. 11, 16 [314] T. Chen, Z. Tan, T. Gong, Q. Chu, Y. Wu, B. Liu, L. Lu, J. Ye, and N. Yu, Bootstrapping audio-visual segmentation by strengthening audio cues, IEEE TCSVT, 2025. 11 [315] D. Hao, Y. Mao, B. He, X. Han, Y. Dai, and Y. Zhong, Improving audio-visual segmentation with bidirectional generation, in AAAI, 2024. [316] J. Liu, Y. Liu, F. Zhang, C. Ju, Y. Zhang, and Y. Wang, Audio-visual segmentation via unlabeled frame exploitation, in CVPR, 2024. 11 [317] S. Gong, Y. Zhuge, L. Zhang, Y. Wang, P. Zhang, L. Wang, and H. Lu, Avs-mamba: Exploring temporal and multi-modal mamba for audiovisual segmentation, IEEE TMM, 2025. 11 [318] Y. Chen, C. Wang, Y. Liu, H. Wang, and G. Carneiro, Cpm: Classconditional prompting machine for audio-visual segmentation, ECCV, 2024. 11, 16 [319] S. Xu, S. Wei, T. Ruan, L. Liao, and Y. Zhao, Each perform its functions: Task decomposition and feature assignment for audio-visual segmentation, IEEE TMM, 2024. 11, 16 [320] C. Liu, P. P. Li, X. Qi, H. Zhang, L. Li, D. Wang, and X. Yu, Audiovisual segmentation by exploring cross-modal mutual semantics, in ACM MM, 2023. [321] C. Liu, P. Li, H. Zhang, L. Li, Z. Huang, D. Wang, and X. Yu, Bavs: bootstrapping audio-visual segmentation by integrating foundation knowledge, IEEE TMM, 2024. 11, 16 [322] S. Mo and B. Raj, Weakly-supervised audio-visual segmentation, NeurIPS, 2024. 11, 16 24 [323] S. Bhosale, H. Yang, D. Kanojia, J. Deng, and X. Zhu, Unsupervised audio-visual segmentation with modality alignment, in AAAI, 2025. 11 [324] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, Imagebind: One embedding space to bind them all, in CVPR, 2023. 11 [325] H. Zhong, M. Zhu, Z. Du, Z. Huang, C. Zhao, M. Liu, W. Wang, H. Chen, and C. Shen, Omni-r1: Reinforcement learning for omnimodal reasoning via two-system collaboration, arXiv, 2025. 11 [326] Z. Qian, Y. Ma, J. Ji, and X. Sun, X-refseg3d: Enhancing referring 3d instance segmentation via structured cross-modal graph neural networks, in AAAI, 2024. 12, [327] C. Wu, Y. Ma, Q. Chen, H. Wang, G. Luo, J. Ji, and X. Sun, 3d-stmn: Dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation, in AAAI, 2024. 12, 17 [328] X. Liu, X. Xu, J. Li, Q. Zhang, X. Wang, N. Sebe, and L. Ma, Less: Label-efficient and single-stage referring 3d segmentation, in NeurIPS, 2024. 12, 17 [329] J. Sun, C. Qing, J. Tan, and X. Xu, Superpoint transformer for 3d scene instance segmentation, in AAAI, 2023. 12 [330] C. Wu, J. Ji, H. Wang, Y. Ma, Y. Huang, G. Luo, H. Fei, X. Sun, R. Ji et al., Rg-san: Rule-guided spatial awareness network for end-to-end 3d referring expression segmentation, in NeurIPS, 2024. 12, 17 [331] Q. Chen, C. Wu, J. Ji, Y. Ma, D. Yang, and X. Sun, Ipdn: Imageenhanced prompt decoding network for 3d referring expression segmentation, in AAAI, 2025. 12, 17 [332] K.-C. Huang, X. Li, L. Qi, S. Yan, and M.-H. Yang, Reason3d: Searching and reasoning 3d segmentation via large language model, in 3DV, 2025. 12, [333] X. Jiang, L. Lu, L. Shao, and S. Lu, Multimodal 3d reasoning segmentation with complex scenes, arXiv, 2024. 12 [334] J. Deng, T. He, L. Jiang, T. Wang, F. Dayoub, and I. Reid, 3d-llava: Towards generalist 3d lmms with omni superpoint transformer, in CVPR, 2025. 12 [335] H. Lin, Y. Luo, X. Zheng, L. Li, F. Chao, T. Jin, D. Luo, Y. Wang, L. Cao, and R. Ji, unified framework for 3d point cloud visual grounding, arXiv, 2023. 12 [336] Z. Qian, Y. Ma, Z. Lin, J. Ji, X. Zheng, X. Sun, and R. Ji, Multi-branch collaborative learning network for 3d visual grounding, in ECCV, 2024. 12, 17 [337] X. Li, J. Ding, Z. Chen, and M. Elhoseiny, Uni3dl: Unified model for 3d and language understanding, in ECCV, 2024. 12 [338] W. Xu, C. Shi, S. Tu, X. Zhou, D. Liang, and X. Bai, unified framework for 3d scene understanding, in NeurIPS, 2024. 13, 17 [339] W. Chen, M. Qu, W. Kang, Y. Yan, Y. Zhao, and Y. Wei, 3drest: strong baseline for semi-supervised 3d referring expression segmentation, arXiv, 2025. 13 [340] Y. Liu, C. Wu, X. Sun, J. Ji, Y. Ma, G. Luo, l. Cao, and R. Ji, Weaklysupervised 3d referring expression segmentation, arXiv, 2025. 13 [341] H. Ding, C. Liu, S. He, X. Jiang, and Y.-G. Jiang, GREx: Generalized referring expression segmentation, comprehension, and generation, arXiv, 2025. 13 [342] Y. Zong, Q. Zhang, D. An, Z. Li, X. Xu, L. Xu, Z. Tu, Y. Xing, and O. Dabeer, Ground-v: Teaching vlms to ground complex instructions in pixels, in CVPR, 2025. 13, 17 [343] J. Wu, X. Li, X. Li, H. Ding, Y. Tong, and D. Tao, Towards robust referring image segmentation, IEEE TIP, 2024. [344] Y. Wu, Z. Zhang, C. Xie, F. Zhu, and R. Zhao, Advancing referring expression segmentation beyond single image, in ICCV, 2023. 13 [345] X. Li, J. Wang, X. Xu, X. Li, B. Raj, and Y. Lu, Robust referring video object segmentation with cyclic structural consensus, in ICCV, 2023. 13, 16 [346] W. Li, Z. Zhao, H. Bai, and F. Su, Bring adaptive binding prototypes to generalized referring expression segmentation, IEEE TMM, 2024. 13, 17 [347] E.-R. Nguyen, H. Le, D. Samaras, and M. Ryoo, Instance-aware generalized referring expression segmentation, arXiv, 2024. 13, 17, 18 [348] Z. Luo, Y. Wu, T. Cheng, Y. Liu, Y. Xiao, H. Wang, X.-P. Zhang, and Y. Yang, Cohd: counting-aware hierarchical decoding framework for generalized referring expression segmentation, arXiv, 2024. 13, 17 [349] W. Huang, D. Liu, and W. Hu, Dense object grounding in 3d scenes, in ACM MM, 2023. [350] , Advancing 3d object grounding beyond single 3d scene, in ACM MM, 2024. 13 [351] V. K. Nagaraja, V. I. Morariu, and L. S. Davis, Modeling context between objects for referring expression understanding, in ECCV, 2016. 13 25 [380] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, and D. Marculescu, Open-vocabulary semantic segmentation with mask-adapted clip, in CVPR, 2023. 18 [381] S. Yang, T. Qu, X. Lai, Z. Tian, B. Peng, S. Liu, and J. Jia, Lisa++: An improved baseline for reasoning segmentation with large language model, arXiv, 2023. [382] Y. Liu, T. Qu, Z. Zhong, B. Peng, S. Liu, B. Yu, and J. Jia, Visionreasoner: Unified visual perception and reasoning via reinforcement learning, arXiv, 2025. 18 [352] P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, and A. v. d. Hengel, Neighbourhood watch: Referring expression comprehension via languageguided graph attention networks, in CVPR, 2019. 13 [353] M. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y. Wang, and A. Mian, Free-form description guided 3d visual graph network for object grounding in point cloud, in ICCV, 2021. 13 [354] Y. Wu, X. Cheng, R. Zhang, Z. Cheng, and J. Zhang, Eda: Explicit text-decoupling and dense alignment for 3d visual grounding, in CVPR, 2023. 13 [355] M. Guo, Z. Zhang, H. Fan, and L. Jing, Divert more attention to visionlanguage tracking, NeurIPS, 2022. [356] D. Ma and X. Wu, Tracking by natural language specification with long short-term context decoupling, in ICCV, 2023. 13 [357] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao, and H. Ling, Lasot: high-quality benchmark for large-scale single object tracking, in CVPR, 2019. 13 [358] Y. Du, C. Lei, Z. Zhao, and F. Su, ikun: Speak to trackers without retraining, in CVPR, 2024. 13 [359] J. Liu, W. Wang, L. Wang, and M.-H. Yang, Attribute-guided attention for referring expression generation and comprehension, IEEE TIP, 2020. [360] B. A. Plummer, L. Wang, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik, Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models, IJCV, 2017. 13 [361] T. Gupta, A. Vahdat, G. Chechik, X. Yang, J. Kautz, and D. Hoiem, Contrastive learning for weakly supervised phrase grounding, in ECCV, 2020. 13 [362] C. Gonzalez, N. Ayobi, I. Hernandez, J. Hernandez, J. Pont-Tuset, and P. Arbelaez, Panoptic narrative grounding, in ICCV, 2021. 13 [363] C. Gonzalez, N. Ayobi, I. Hernandez, J. Pont-Tuset, and P. Arbelaez, Piglet: Pixel-level grounding of language expressions with transformers, IEEE TPAMI, 2023. 13 [364] S. Dai, J. Liu, and N.-M. Cheung, Referring expression counting, in CVPR, 2024. 13 [365] N. Amini-Naieni, T. Han, and A. Zisserman, Countgd: Multi-modal open-world counting, in NeurIPS, 2024. 13 [366] M. Shridhar and D. Hsu, Interactive visual grounding of referring expressions for human-robot interaction, in RSS, 2018. 14 [367] G. Tziafas, Y. Xu, A. Goel, M. Kasaei, Z. Li, and H. Kasaei, Languageguided robot grasping: Clip-based referring grasp synthesis in clutter, in CoRL, 2023. 14 [368] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy, Generation and comprehension of unambiguous object descriptions, in CVPR, 2016. 14, 15 [369] P. Krahenbuhl and V. Koltun, Efficient inference in fully connected crfs with gaussian edge potentials, in NeurIPS, 2011. 15 [370] D. R. Martin, C. C. Fowlkes, and J. Malik, Learning to detect natural image boundaries using local brightness, color, and texture cues, IEEE TPAMI, 2004. 14 [371] J. Ma, P. Sun, Y. Wang, and D. Hu, Stepping stones: progressive training strategy for audio-visual semantic segmentation, ECCV, 2024. 16 [372] Y. Chen, Y. Liu, H. Wang, F. Liu, C. Wang, H. Frazer, and G. Carneiro, Unraveling instance associations: closer look for audio-visual segmentation, in CVPR, 2024. 16 [373] A. Radman and J. Laaksonen, Tsam: Temporal sam augmented with multimodal prompts for referring audio-visual segmentation, in CVPR, 2025. 16, 17 [374] Y. Wang, H. Xu, Y. Liu, J. Li, and Y. Tang, Sam2-love: Segment anything model 2 in language-aided audio-visual scenes, in CVPR, 2025. 16, [375] Z. Luo, Y. Wu, Y. Liu, Y. Xiao, X.-P. Zhang, and Y. Yang, Hdc: Hierarchical semantic decoding with counting assistance for generalized referring expression segmentation, arXiv, 2024. 17 [376] S. Cao, Z. Wei, J. Kuen, K. Liu, L. Zhang, J. Gu, H. Jung, L.-Y. Gui, and Y.-X. Wang, Refer to anything with vision-language prompts, arXiv preprint arXiv:2506.05342, 2025. 17 [377] H. You, H. Zhang, Z. Gan, X. Du, B. Zhang, Z. Wang, L. Cao, S.-F. Chang, and Y. Yang, Ferret: Refer and ground anything anywhere at any granularity, in ICLR, 2024. 17 [378] M. Dai, L. Yang, Y. Xu, Z. Feng, and W. Yang, Simvg: simple framework for visual grounding with decoupled multi-modal fusion, NeurIPS, 2024. 17, 18 [379] Z. Sun, Y. Liu, H. Zhu, Y. Gu, Y. Zou, Z. Liu, G.-S. Xia, B. Du, and Y. Xu, Refdrone: challenging benchmark for referring expression comprehension in drone scenes, arXiv preprint arXiv:2502.00392, 2025. 17,"
        }
    ],
    "affiliations": [
        "ByteDance Inc.",
        "Fudan University",
        "Shanghai University of Finance and Economics"
    ]
}