{
    "paper_title": "Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach",
    "authors": [
        "Yunuo Chen",
        "Junli Cao",
        "Anil Kag",
        "Vidit Goel",
        "Sergei Korolev",
        "Chenfanfu Jiang",
        "Sergey Tulyakov",
        "Jian Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, \\eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion."
        },
        {
            "title": "Start",
            "content": "Towards Physical Understanding in Video Generation: 3D Point Regularization Approach Yunuo Chen1,2 Junli Cao1,2 Anil Kag2 Vidit Goel2 Sergei Korolev2 Chenfanfu Jiang1 Sergey Tulyakov2 1University of California, Los Angeles 2Snap Inc. Jian Ren Project Page: https://snap-research.github.io/PointVidGen/ 5 2 0 2 5 ] . [ 1 9 3 6 3 0 . 2 0 5 2 : r Figure 1. Comparison on Task-Oriented Videos. We present videos generated by different baselines (SVD [6], I2VGen-XL [41], and DynamiCrafter [37]) and compare them with our method. We use the same input conditions for all methods (except that SVD is conditioned only on the image). It can be observed that existing baselines often exhibit severe distortions of hands or objects during human hand-object interactions. In contrast, our method preserves the shapes of both the hand and object during such interactions and ensures smooth transitions throughout the video."
        },
        {
            "title": "Abstract",
            "content": "We present novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, e.g., nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as taskoriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion. 1. Introduction The recent development of video diffusion models has attracted significant research interests and seen tremendous progress [6, 14, 32]. With vast amounts of training data and sophisticated architectures, video models have greatly improved in expressiveness and aesthetics [7, 33]. Today, state-of-the-art video models can realistically represent content from the real world with high creativity, enabling various applications in the entertainment industry and scientific research [25]. However, existing video diffusion models primarily focus on improving the appearance of content and the smoothness of motion. The models are trained in way to understand the movements of 2-dimensional pixels, where the object movements in 3D space is approximated through the changing of RGB values. As result, the underlying 3-dimensional shapes and motions, which represent how objects truly exist in the physical world, are not learned well in the existing video generation models. Even worse, we observe that when the video diffusion models neglect the underlying 3D information, they struggle to understand how objects occupy space, change shape and location, and interact with each other, which is essential for generating videos with complex motion. For instance, imaging situation when perceiving scene with human performing specific tasks, e.g., hands tying shoelaces, viewpoint projection and occlusion make it impossible to capture the true 3D shape of the hand and the shoelaces using 2D pixels, complicating the modeling of how they make contact. Without understanding the underlying geometry, objects in the video may change shape arbitrarily or even appear and disappear suddenly, leading to the common issue of object morphing (as example results from prior arts in Fig. 1). This problem is exacerbated for complex objects like humans; without 3D awareness, limbs and body parts may move unnaturally or change shape abruptly, resulting in dislocated arms or twisted bodies. To address the above issue, we aim to improve video diffusion models towards the better understanding of physical world, thus generating more reasonable shape and motion. Previous works have attempted to control shape and motion directly in 2D space [23, 31]. Nonetheless, without comprehensive understanding of 3D geometry, these methods are limited to directing pixel movement confined within the 2D plane, making them unable to handle out-of-plane motions or fully resolve 3D dynamics [23]. One straightforward approach could be to model the complete 3D geometry of objects using representations such as NeRF [26] or 3D Gaussian-Splatting [18], essentially leading to 4D generation [3, 43]. While these methods provide accurate 3D understanding, they are computationally intensive and generally limited to generating simple scenes with only few specific types of objects. To bridge the gap between leveraging 3D information for better video generation, we propose method to augment and regularize video diffusion models using 3D point cloud. Specifically, our model enriches 2D videos with outof-dimension data, which is absent in traditional RGB video models, without requiring full-scale 3D reconstruction. Instead of generating complete mesh or point cloud for video diffusion model, we track the movement of 2D pixels in 3D space, creating pseudo-3D representation of objects in the video. To enable such optimization, we further develop 3D-aware video dataset, named PointVid, with segmented objects in pixel space and their corresponded 3D dynamic states. We fine-tune video diffusion models on the PointVid dataset to extend its generation capability to 3D shape and motion, enhancing the model to better understand object behavior in the physical world. Leveraging prior knowledge of 3D space, we utilize this additional information to guide pixel movements towards more physically plausible outcomes. We demonstrate the effectiveness of our approach through extensive experiments, primarily focusing on handling complex scenarios with interacting objects and accurately resolving dynamic contacts. Guided by 3D trajectories, our model achieves superior quality in terms of smooth transitions of shape and motion compared to existing works, generating visually plausible videos. We summarize our contributions as follows: 1. Injecting 3D Knowledge into Video Diffusion. We propose novel 3D point augmentation strategy to bridge the 3D spatial domain of the physical world with 2 Figure 2. Training Pipeline Overview. We sample video-point pairs, concatenate them in channel dimensions and used to train UNet. In addition to standard condition and latent cross attention, we further add cross attention between video and point in corresponding channels for better alignment between the two modalities. Furthermore, the 3D information from the points is utilized to regularize the RGB video generation by applying misalignment penalty to the video diffusion process. the pixel space of 2D videos. By explicitly incorporating 3D information into the video diffusion process, our method significantly enhances video generation quality, especially in challenging scenarios such as solid object interactions. 2. Regularization with 3D Prior. We introduce novel regularization strategy to guide the point cloud diffusion process, ensuring that the generated point cloud is wellstructured. This structured point cloud, in turn, enhances the RGB video generation by providing robust 3D priors for better spatial coherence and alignment. 3. 3D-Aware Video Dataset. We curate 3D-aware video dataset, PointVid, by tracking 3D points in the first frame of given video. We segment the first frame to obtain foreground objects and mainly track these points in the original video. The resulting dataset contains 3D coordinates corresponding to the first frame, pixel aligned with the video dataset. 2. Related Work Video Diffusion Models. Following the success of diffusion-based text-to-image (T2I) models [1, 11, 12, 17, 2022, 24, 27, 34], many video generation models have been proposed in the literature [6, 14, 25, 32]. These models allow video generation following input conditioning such as text and image. Video diffusion models can be categorized into three categories. First, auto-regressive models like VideoPoet [19] follow the causal language model paradigm of next token prediction given previous tokens. These models encode the input conditioning into latent tokens using tokenizers such as MAGVIT [39]. Second, pixel-space diffusion models [14, 25, 32] directly model the video synthesis on the pixel space to avoid artifacts arising in the compressed VAE latent spaces. While these models provide more photo-realism and better motion dynamics, they typically only offer low-resolution video generation due to higher computational cost in generating high-resolution videos. These base models are augmented with super-resolution components to increase the video resolution resulting in deep cascaded diffusion models. Imagen-video [14], UNet-based architecture, generates videos at low-resolution and utilizes sequence super-resolution models to increase the video resolution. Make-a-video [32] uses T2I as prior to encode text input and trains deep cascade of spatial and temporal layers to generate high resolution videos. SnapVideo [25] replaces the popular UNet architecture design with FIT network for improved generation efficiency. Third, latent diffusion models transform the raw pixel space into compressed spatio-temporal latent representation. It enables training higher resolution and higher capacity base model instead of pixel-space diffusion models. These include Stable Video Diffusion [6], SORA [7], VideoCrafter2 [8], ModelScopeT2V [35], Mochi [33], CogVideoX [15, 38] etc. While latent models are typically single-stage pipelines, they can extend to deep cascade pipelines to offer even higher-quality video generations. MovieGen [28] proposed transformer-based cascade latent diffusion model. While our approach is applicable to different types of video diffusion models, we mainly use ModelScopeT2V [35] as our base video-generation model and demonstrate how to incoporate 3D geometry and dynamics improve video generation. Dynamics Aware Video Generation. There have been various works [23, 31, 42] that instill physically plausible and temporally consistent dynamics in video generation. PhysGen [23] added simple control such as force or torque to an object in within the image, enabling simple physically plausible dynamics to the resulting video. MotionCraft [31] synthesized videos by incorporating optical flow in the noise diffusion process. This allows image animation with temporally consistent video content. Phys3 dreamer [42] tried distilling dynamic priors of static 3D objects from the video generation models. It creates physical material field around the 3D object, thereby easily synthesizing 3D dynamics under arbritary forces. Although these works are able to incorporate simple dynamics to static image, they assume that the underlying diffusion model understands and captures the object dynamics. This assumption is not necessarily true, as pointed by VideoPhy [4], current video generation models suffer from many inconsistencies, both physically implausible as well as temporally inconsistent. There have been other works along the line of incorporating camera intrinsics during video generation. 3DAware Video-Generation [2], first trains model by jointing diffusing noise along motion and content codes while sampling different camera poses. The model is trained using generative adversarial network based discriminator which discriminates the resulting video, by difference between frames and time duration. This enables generating videos that are physically plausible. In contrast to the previous works, we incorporate 3D knowledge and dynamics by jointly diffusing the 3D geometry alongside the video modality. This helps improve the diffusion model understanding of the dynamics during training and improves the physical plausibility and temporal consistency of the generated videos. 3. Method As mentioned earlier, current video models primarily learn moving objects as sequences of 2D pixels, without considering their true 3D states. Their understanding of the physical world is limited to projected plane, making it challenging to resolve changes in shape and motion that are not visible in the projected view. To address this, we propose incorporating partial 3D representation into the generation pipeline. The inclusion of 3D information as second modality will not only enhance the generation capability of the video model but also guide the generation of RGB videos through joint optimization. In this section, we outline the details of our 3D-aware video generation pipeline, which consists of three essential components: First, we introduce our novel 3D-aware video dataset, PointVid, in subsection 3.1, providing videos with spatial information orthogonal to color dimensions. Next, in subsection 3.2, we present our joint training approach, enhancing the video models ability to incorporate 3D geometry and dynamics. Finally, we describe our 3D regularization strategy in subsection 3.3, designed to resolve nonphysical artifacts in the generated results using the newly incorporated modality. 3.1. PointVid Dataset We build our 3D-aware video dataset upon standard 3channel videos of shape [T, H, W, C], where = 3 enFigure 3. PointVid Dataset Generation Workflow. Given an input video, we use the first frame as reference frame and perform semantic segmentation to obtain masks for foreground objects. Next, we randomly sample pixels with distribution favoring pixels inside foreground objects. We perform 3D point tracking on these queried pixels, and map these points to the input video frames. The resulting data point contains 3D coordinates of tracked foreground pixels while remaining pixels are zeroed out. codes the RGB channels, height and width digitize 2D images into pixels, and represents the temporal dimension that sequences the pixel data over time. The key challenge lies in selecting 3D representation that is suitable for learning by video model. Explicit 3D representations, such as meshes (surface or volumetric), point clouds, or voxels, can effectively model geometry but often have variable sizes due to irregular object shapes, complicating the diffusion training process. Moreover, aligning 3D points with 2D pixels in video remains non-trivial task. In contrast, implicit representations like NeRF [26] typically require substantial time to convert into an explicit form for direct usage, making them impractical for training purposes. Recent advancements in 3D point tracking [36] have made this task more feasible. Instead of representing moving objects as sequences of meshes or point clouds with arbitrary resolution, we focus exclusively on tracking the movements of pixels. Here, 2D pixels are unprojected into 3D space and tracked throughout the video. This allows the 3D locations of the pixels to be treated as three additional attributes, similar to color intensities. As result, we can format the 3 spatial coordinates as extra channels, yielding point tracking tensor of shape [T, H, W, C], matching the dimensions of the RGB video. In practice, we store the 3D tracking information using 2D pixel coordinates (u, v) and depth d, requiring only fixed camera unprojection to reconstruct the 3D world coordinates (x, y, z). By using (u, v, d), the first two channels correspond to the pixels position projected onto the 2D plane, which aligns precisely with the moving point in the RGB video, thus simplifying the task of learning the alignment between video and point tracking. We visualize the workflow of generating point track4 For each RGB video V, we extract ing in Figure 3. the first frame as reference image and perform semantic segmentation using Grounded-SAM-2 [29, 30], yielding masks for the foreground objects. Next, we randomly sample pixels on the reference image with main focus on those inside the foreground objects. These queried pixels are then tracked with trajectories of 3D Cartesian coordinates (x, y, z) throughout the video using SpaTracker [36]. For reconstruction purposes, ZoeDepth [5] is employed for depth estimation. Finally, we postprocess the tracked results to align the shape with the video: we fill out the pixels in the foreground mask to store the pixels spatial positions, formatted as 3 channels, and discard pixels in the background (setting all background pixels to zero). Since we apply sparse queries for tracking efficiency, some foreground pixels may not have corresponding trajectory; we use KDTree searching to interpolate the positions for such pixels. One remaining issue we encounter in the tracked results is that they sometimes contain high-frequency noise, possibly due to the imprecision of the tracking. This causes problems when the video model attempts to learn from its distribution, as the backbone diffusion model relies on sampling with random noise. To address this, we apply Kalman Filter for 3D point tracking to remove temporal high-frequency noise and recover its primary motion. 3.2. Point Augmentation With our 3D-aware video dataset, we now aim to extend the capabilities of the video model to generate not only the RGB video but also the corresponding 3D representation P. Aligning the video with the point cloud enables the model to inherently acquire 3D knowledge. The augmented video-point model learns not only how the object appears in 2D color image but also its 3D geometry and location. Our augmentation pipeline is visualized in Figure 2. It is worth noting that training the video-point model entirely from scratch is unnecessary, as training video diffusion model already demands substantial computational resources and vast amounts of video data. Therefore, instead of training from scratch, it is more practical to fine-tune an existing video model to incorporate the point-tracking modality. In this work, we employ similar architecture to the video model in I2VGen-XL [35, 41], UNetbased latent diffusion model conditioned on image and text prompts. To collectively learn from and generate and P, the straightforward way would be to feed them together to the diffusion model. Since we are matching the shapes of video and point tensors, we can concatenate them along the channel dimension, resulting in an point-augmented video VP with shape of [T, H, W, 2C]. This concatnated video can be seamlessly integrated into the UNet by simply adjusting the input/output dimensions to accommodate the additional channels. This increase in dimensionality does not overshadow the weights of the original model; instead, only new dimensions are introduced to handle the added modality. To leverage pretrained RGB weights, we can load the pretrained model for the corresponding RGB channels and initialize the remaining weights as zeros. For the conditioning frame, we continue using the first RGB frame and repeat the condition image tensor to prompt the generation of both video and point tracking, as our goal is for the model not to rely on the initial point positions during the inference stage. This strategy is effective because the same reference frame is used for generating the point-tracking data earlier. In addition to the pixel-wise alignment of video and point data in creating our dataset, we introduce crossattention layer at the entry of the UNet. This layer connects the first and second halves of the channel dimensions, corresponding to color information and spatial location in the latent dimension, to further enhance the alignment between them. Incorporating the aforementioned changes, The training pipeline then proceed as usual: we encode VP to latent space z, noise it to zt with randomly sampled noise ϵ and diffusion step t. The UNet model then predict the added noise and compare with true noise, where we adopt DDIM scheduling for the model. With the modified architecture, we fine-tune the diffusion model on PointVid to obtain our video-point generation model. Given an RGB image and text prompt, our model can generate both the video and the corresponding 3D trajectories. 3.3. Point Regularization With the video closely aligned to 3D trajectories, we leverage this additional modality to enhance RGB video generation. To achieve this, we introduce regularization terms on the point tracking component of the diffusion output, aligning it with the ground truth, which in turn implicitly improves the RGB video output. (see Figure 2). The first component we need is to generate directly usable point trackings for evaluation in 3D space. Since including random noise of any level in the point cloud would likely render it unusable, we employ DDIM sampling to recover noise-free stage before applying regularization. In each training step with latent input and corresponding noisy stage zt, We apply DDIM denoising to recover z0 iteratively. Following the approach of [40], we utilize gradient checkpointing to freeze the gradient flow except during the final step, preventing memory overflow. The denoised data z0 is subsequently decoded into explicit 3D point tracking as the sampled 3D trajectory of the video. With the explicit modeling of trajectories as 3D representation, we introduce regularization terms to improve the quality of the generated results, focusing on motion fidelity Table 1. Quantitative Evaluation. We evaluate various aspects of our method using the VBench [16] and VideoPhy [4] benchmarks. The evaluated metrics are as follows: (VBench) SC: subject consistency, BC: background consistency, MS: motion smoothness, AQ: aesthetic quality, IQ: imaging quality; (VideoPhy) PC: physical commonsense. By incorporating 3D knowledge, our video model shows substantial improvement in metrics such as physical commonsense, motion smoothness, and subject/background consistency. This demonstrates that our method generates significantly more temporally consistent and physically plausible videos."
        },
        {
            "title": "Method",
            "content": "SC BC MS AQ IQ PC I2VGen-XL 0.83247 0.95892 Ours 0.89147 0.95202 0.95706 0.98456 0.44055 0.43369 0.58532 0.60423 0.32665 0. Apart from the microscopic high-frequency noise, we observe macroscopic shape deformation in many of the generated point cloud (see Figure 4 bottom middle). Such deformation of arbitrary degree is undesirable as most solid objects, including human body, inderently requies conservation of volumn and local rigidity. Violating this can lead to unnatural shape deformation and morphing, which is considerd undesired in our generation task. Hence we propose the rigidity loss Lrigid to enforce local shape preservation: Lrigid = (cid:88) (cid:88) (cid:88) (cid:16) τ =1 jN (i) dist( τ , τ ) dist( i , 0 ) (cid:17)2 . (2) Here, we apply kNN search on the reference frame to construct local neighbor pairs, then penalize changes in distance between these pairs. By combining reconstruction and rigidity loss, we enhance the generation quality of the 3D point cloud, as shown in Figure 4 (right). This improvement in the point space implicitly promotes shape preservation and smooth motion transitions in the RGB video, as our video-point model closely aligns the two modalities. To prevent the geometric regularizations from dominating the semantic aspects, we jointly optimize these regularization terms with diffusion losses during training, resulting in an overall loss of the form: = λddimLddim + λreconLrecon + λrigidLrigid (3) Figure 4. Point Regularization. The reconstructed point cloud in the diffusion output often contains noise and deformations (middle). This issue is mitigated using our point regularization (right). The synthetic point cloud above (e.g. box and shoes falling on the ground) is generated by Kubric [13] and trained with our pipeline. and temporal smoothness. For simplicity, let us assume is reshaped to 3, and we use τ to denote the point cloud of shape 3 at time τ . common issue that arises during point tracking generation is the presence of excessive noise (see Figure 4 top middle). This noise can originate from various sources, such as the variational autoencoder (VAE) used to encode into the latent space. High-frequency noise in both spatial and temporal dimensions tends to obscure the main shape and motion, thereby impairing the video models ability to track 3D points effectively. To mitigate noise and increase fidelity of reconstruction, we propose the reconstruction loss Lrecon as follows: Lrecon = T (cid:88) τ =0 τ τ + c1 (cid:88) τ =1 τ τ 1 + c2 (cid:88) τ =2 τ 2 τ 1 + τ 2. (1) 4. Experiments Here the first term penalizes the difference between generated point cloud and the noise-free ground truth P, ensuring generation fidelity, while the second term penalizes the difference between consecutive time step to encourage temporal smoothness; the third term further enforeces acceleration smoothness, discouraging abrupt changes in velocity. We selected the weights so that each term are about the same scale initially. 6 In this section, we demonstrate the effectiveness of our proposed method through series of experiments. We collect around 70K videos from [9] and our proprietary dataset to form our training dataset. The videos are processed to resolution of 448256 with 16 frames. During the augmentation stage, the video-point diffusion model is trained on 8 A100 GPUs with batch size of 4. In the subsequent regularization stage, we train on single A100 GPU with batch size of 4. We evaluate the results both qualitatively suring smooth transitions throughout the video, exhibiting consistent quality improvement over the compared models. General videos The strength of our model extends to generating high-fidelity videos across variety of categories. In Figure 6, we showcase several categories: (a) dynamic objects, (b) static objects with moving camera, (c/d) humans, and (e/f) animals. For objects with camera motion, our 3Daware regularization enhances the models understanding of 3D shape and scene composition, effectively eliminating artifacts such as foreground object morphing and background degradation. For human subjects, our point augmentation prevents unnatural changes in shape and appearance. Additionally, our model accurately captures the shape and behavior of animals, avoiding morphing issues. 4.2. Quantitative Results We quantitatively evaluate our model using batch inference on the test set. We apply the metrics to test batch obtained from [9], consisting of total of 387 video clips. We compare the scores with the base model, I2VGen-XL, and summarize the results in Table 1. First, we assess the general aspects of video generation quality using VBench [16], evaluating the content and aesthetic quality of the videos. The results show that our method outperforms in most categories. Notably, our point regularization contributes to significant improvement in content consistency. Beyond the general aspects, we focus on how 3Dawareness enhances alignment with real-world physical principles. It is important to note that determining whether 2D video is physically correct is largely subjective, as ground-truth 3D physical states are not available. Therefore, we use VideoPhy [4], where the authors utilize human annotations to train discriminator that provides physical commonsense score. VideoPhy defines physical commonsense based on various perspectives, such as solid mechanics, where solid objects are expected to retain their shape and size throughout the video. As shown in Table 1, our model achieves higher score for alignment with physical principles. 4.3. Ablation Study We conduct ablation studies to demonstrate the essential role of each component of our method, namely point augmentation and regularization (see Figure 5). As baseline, we finetune the diffusion model on our dataset using only video data to eliminate bias from training videos (Figure 5 left). Next, we perform inference using the jointly trained video-point model to highlight the improvement from point augmentation alone (Figure 5 middle). Finally, we evaluate the full model with regularization (Figure 5 right). We observe that the model trained exclusively with RGB video Figure 5. Ablation on Point Augmentation and Regularization. Our point-augmented model demonstrates degree of 3Dawareness compared to the model fine-tuned on video data alone, but it still exhibits some artifacts, which are mitigated through regularization. and quantitatively, showcasing the models successful handling of contact-rich scenarios and its strong shape/motion consistency. 4.1. Qualitative Results Although our model is designed to generate sequences of 2D images, our point augmentation and regularization enhance its ability to handle inherently 3D motions. To evaluate, we test our model on unseen images and visualize the corresponding RGB video. For comparison, we apply the same prompts to the baseline model I2VGen-XL [41], as well as two additional models: SVD (stable-videodiffusion-img2vid-xt) [6] and DynamiCrafter [37]. For qualitative comparison, we focus primarily on the aspects of object interaction and shape conservation. Task-oriented videos prevalent category of scenarios that make up significant portion of real-world videos in the wild involves humans performing specific tasks (see Figure 1), such as playing with toy, cooking food, or handling tools. However, current video models often struggle to capture localized details, particularly how human hands interact with objects. The challenge arises from the complex 3D nature of the interacting parts, and relying solely on 2D data fails to capture how different parts shape, move, and contact. As result, hands or objects may undergo severe distortion during interactions, leading to unrealistic artifacts. With our 3D-aware generation framework, our model effectively preserves the shape of both hands and objects, en7 Figure 6. Comparison on General Videos. We showcase the generated videos across various categories, including static and dynamic objects, humans, and animals. Our method ensures smooth transitions in object shape and motion and eliminates morphing artifacts. can generate videos that match the appearance and semantics of the prompts. However, it suffers from severe distortions in both hands and objects. By incorporating point tracking during training, we leverage the 3D information from PointVid to gain partial awareness of spatial locations. This enhancement is evident as objects in the scene (e.g., cellphone and white stars) maintain their correct 3D shapes. This partial awareness is further enhanced by our regularization steps, leading to better control of shape and motion, significantly reducing artifacts in the videos (e.g., hands now retain their shape). 5. Conclusion and Discussion To summarize, we propose generation framework that incorporates 3D-awareness into video diffusion models. By tracking objects in 3D and aligning them in pixel space, we elevate traditional video datasets to new dimension, revealing out-of-plane information previously inaccessible to video models. Through joint training, the video model learns to perceive 3D shape and motion, acquiring physical commonsense that is natural in 3D. We then apply regularization to refine the generation process and further enhance the results, eliminating artifacts such as object morphing. Our method has demonstrated strong capability to handle content-rich scenarios, such as task-oriented videos, and shows superior visual plausibility when generating videos of general categories. Limitations. We acknowledge few limitations of our framework. The degree of 3D-awareness, rooted in our video-point joint training, is constrained by the resolution of our 3D points. Objects that are not sufficiently represented may lack quality 3D prior and may still exhibit implausible artifacts. Additionally, the generation quality of our model is somewhat limited by the backend UNet model, and could potentially be improved by employing more advanced transformer-based models. We leave these aspects as directions for future exploration."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. Stable diffusion 2.1. https://huggingface.co/stabilityai/stable-diffusion-2-1, 2022. [2] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Hao Tang, Gordon Wetzstein, Leonidas Guibas, Luc Van Gool, and Radu Timofte. 3d-aware video generation, 2023. 4 [3] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, 8 Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score disIn Proceedings of the IEEE/CVF Contillation sampling. ference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 2 [4] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 4, 6, 7, 14 [5] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. 1, 2, 3, 7, 11 [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2, 3 [8] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 3 [9] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 6, 7, 11 [10] LMDeploy Contributors. Lmdeploy: toolkit for compressing, deploying, and serving llm. https://github. com/InternLM/lmdeploy, 2023. 11 [11] DeepFloyd. Deepfloyd. https://github.com/deep-floyd/IF, 2023. [12] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers. arXiv preprint arXiv:2405.05945, 2024. 3 [13] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti, Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: scalable dataset generator, 2022. 6 [14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022. 2, 3 [15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3 [16] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 6, 7, 14 [17] Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, and Jian Ren. Ascan: Asymmetric convolution-attention networks arXiv preprint for efficient recognition and generation. arXiv:2411.04967, 2024. [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [19] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language model for zero-shot video generation, 2024. 3 [20] Black Forest Labs. Flux. https://blackforestlabs.ai/announcing-black-forest-labs/, 2024. 3 [21] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. [22] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. 3 [23] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In European Conference on Computer Vision ECCV, 2024. 2, 3 [24] Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyper-realistic human generarXiv preprint ation with latent structural diffusion. arXiv:2310.08579, 2023. 3 [25] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, and Sergey 9 Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2025. 1, 7, 11 [38] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [39] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. MAGVIT: In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 3 [40] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Alinstructing video difbanie, and Dong Ni. Instructvideo: In Proceedings of fusion models with human feedback. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64636474, 2024. 5 [41] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 1, 5, 7, 11, [42] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William T. Freeman. PhysDreamer: Physics-based interaction with 3d objects via video generation. arxiv, 2024. 3, 4 [43] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023. 2 Tulyakov. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 70387048, 2024. 2, 3 [26] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 4 [27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3 [28] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [29] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 5, 11 [30] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 5, 11 [31] Luca Savant Aira, Antonio Montanaro, Emanuele Aiello, Diego Valsesia, and Enrico Magli. Motioncraft: PhysicsIn Advances in Neural based zero-shot video generation. Information Processing Systems, 2024. 2, 3 [32] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023. 2, 3 [33] Genmo Team. Mochi 1: new sota in open-source video generation. https://github.com/genmoai/ models, 2024. 2, 3 [34] Kolors Team. Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis. arXiv preprint, 2024. [35] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3, 5 [36] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: In Proceedings of Tracking any 2d pixels in 3d space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2040620417, 2024. 4, 5, 11 [37] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying 10 Towards Physical Understanding in Video Generation: 3D Point Regularization Approach"
        },
        {
            "title": "Supplementary Material",
            "content": "Supplementary to the main paper, we provide more details on dataset creation in Appendix A, additional qualitative results in Appendix B, additional results for ablation studies in Appendix C, and human preference evaluation in Appendix D. A. PointVid Dataset Generation In this section, we present additional details on generating our 3D-aware video dataset, PointVid. visualization of the generation pipeline is provided in the main paper. Our pipeline requires no additional annotations beyond the video-text pairs in the dataset. For our experiments, we utilize Panda70M [9]. Preparation Starting with any video-text pair, we trim the videos to fixed length and shape , extracting the first frame of the clipped video as the reference frame. We then apply semantic segmentation to the reference frame using Grounded-SAM2 [29, 30] to automatically generate masks for foreground objects. To streamline the process, we utilize language model, LMDeploy [10], to infer the main moving objects in the scene based on the videos caption. For example, the caption man swinging baseball bat in studio results in {man, baseball bat}. This object-level label is subsequently fed to Grounded-SAM2 for promptbased segmentation. Point Tracking Having obtained reference frame and its segmentation, we apply SpaTracker [36] to track pixel movements accordingly. We use the same reference frame to query points during tracking so that the 3D positions align with the first frames pixel locations initially. We set downsample to 1 and gridsize to 80 in [36] to enable sparse tracking. For 448256 image, we observe that sparse tracking at this resolution provides sufficient number of pixels while remaining relatively efficient in terms of time. Increasing the resolution or using dense tracking could potentially enhance the results, but we leave this for future exploration. The model outputs point tracking data of shape 3, where is the number of sampled points. Post-processing To mitigate temporal noise caused by inaccurate tracking, we first apply Kalman Filter to the 3D point cloud to estimate global velocity and eliminate high-frequency noise. The result is then reshaped to 3, where the spatial coordinates of pixels are 11 treated as three channels, similar to RGB. To address the difference in shape between the tracked points and the video resolution, we apply the following rules: if tracked pixel is not within the foreground mask, we discard its value (effectively setting all background pixels to zero). Otherwise, we assign its value to the corresponding pixel entry in the tensor. Since we use sparse tracking, only subset of foreground pixels have values. We then apply KDTree searching on the sparse pixels based on their uv coordinates in the first frame. For all untracked pixels within the foreground mask, we approximate their trajectory by interpolating their three nearest tracked pixels. The pseudocode for post-processing is summarized in 1. Algorithm 1 Post-processing for Point Tracking Require: Tracked points RT 3, Foreground mask {0, 1}HW , Resolution (H, ) Ensure: Processed tensor RT HW 3 1: Initialize: 0 2: KalmanFilter3D(P) 3: P0 P[0, :, 0 : 2] 4: P1 P0 5: Tr KDTree(P1) 6: for each foreground pixel [u, v] = 1 do 7: 8: 9: if found P1[j] = (u, v) then P[:, u, v, :] P[:, j, :] (cid:84) else Initialize all values to zero Reduce temporoal noise Initial uv of 2D pixels Filter foreground pixels Use initial uv for searching Untracked pixel Tracked pixel ind Query(Tr, (u, v), = 3) P[:, u, v, :] Interp(P[:, ind, :]) 10: 11: 12: 13: end for 14: return end if B. Additional Qualitative Results We provide additional qualitative evaluations of our model compared to three other models: I2VGen-XL [41], SVD [6], DynamiCrafter [37]. These include both task-oriented videos (see Figure 7) and videos from non-specific categories (see Figure 8), such as humans, animals, and general objects. Our results demonstrate improved shape and motion consistency of objects, effectively minimizing common non-physical artifacts like object morphing. Figure 7. Comparison on Task-oriented Videos. 12 Figure 8. Comparison on General Categories. C. Additional Ablation Study We conduct ablation studies to evaluate the effectiveness of our training pipeline design. The results are visualized in Figure 9. As discussed earlier, our two-stage design of point augmentation and regularization progressively enhances the video model. Compared to training the model with RGB videos only (Figure 9 (i)), our point augmentation alone (Figure 9 (ii)) injects 3D awareness into the model and improves its ability to perceive 3D shapes, as evidenced by more consistent object shapes in the videos. Our point regularization (Figure 9 (iii)) further improves quality by optimizing point generation and implicitly guiding RGB generation towards higher fidelity. 13 Although the video and point data are already pixelaligned during generation, our channel-wise cross-attention mechanism is essential to ensure cross-dimensional and cross-modality alignment. Without the cross-attention (Figure 9 (iv)), 3D regularization on points is not correctly transferred to 2D pixels, leading to degraded video quality. Furthermore, in the regularization stage, we adopt joint training strategy that combines regularization loss with diffusion loss. We observe that dropping diffusion loss (Figure 9 (v)) causes 3D information to dominate semantics, resulting in completely unusable output. Figure 9. Ablation Studies on Different Components. We compare the results from (i) our full generation pipeline with (ii) model trained with RGB only, (iii) model trained with point augmentation only (no regularization), (iv) model without channel cross-attention, and (v) model trained without diffusion loss. shown in Table 2, our method demonstrates significant improvement in terms of physical plausibility by incorporating 3D awareness into the video. Table 2. User Study Results. Our model demonstrates significant improvement in physical plausibility, as assessed by human labelers. Here, Q2 asks the user to identify negative artifacts in the videos, while the other questions positively assess physical plausibility. Method Q1 Q2 Q3 Q4 Q5 I2VGen-XL 0.138 0.862 Ours 0.862 0.137 0.135 0.865 0.132 0. 0.137 0.863 D. User Study Apart from the quantitative evaluation using VBench [16] and VideoPhy [4], we additionally conduct user study on our model in comparison to the base model I2VGen-XL [41] to assess human preference. We ask 10 labelers to compare the same testing batch used in quantitative evaluation (387 video clips), focusing on identifying non-physical artifacts and evaluating overall physical plausibility. The following 5 questions are designed for the evaluation: 1. Which video appears to have more physically realistic object movements and interactions? (Consider aspects like gravity, collisions, and the natural flow of objects.) 2. (Negative) Which video contains more noticeable nonphysical artifacts, such as object morphing, stretching, or sudden changes in shape? 3. In which video do the objects maintain more consistent size, shape, and appearance throughout the entire sequence? 4. Which video better adheres to natural physics laws, such as consistent lighting, shadow behavior, and material properties (e.g., rigidity or fluidity)? 5. Overall, which video feels more coherent and believable based on the physical interactions and behavior of objects? Given the evaluation questions, we ask users to compare our generated videos side by side with results from the base model and select the one that best fits each question. As"
        }
    ],
    "affiliations": [
        "Snap Inc.",
        "University of California, Los Angeles"
    ]
}