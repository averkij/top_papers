{
    "paper_title": "Constructing a 3D Town from a Single Image",
    "authors": [
        "Kaizhi Zheng",
        "Ruijian Zhang",
        "Jing Gu",
        "Jie Yang",
        "Xin Eric Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce 3DTown, a training-free framework designed to synthesize realistic and coherent 3D scenes from a single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using a pretrained 3D object generator, followed by a masked rectified flow inpainting process that fills in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that 3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality 3D town generation is achievable from a single image using a principled, training-free approach."
        },
        {
            "title": "Start",
            "content": "Constructing 3D Town from Single Image Kaizhi Zheng1 Ruijian Zhang2 Jie Yang3 Xin Eric Wang1 Jing Gu1 2Columbia University 3Cybever AI 1UC Santa Cruz 5 2 0 2 1 2 ] . [ 1 5 6 7 5 1 . 5 0 5 2 : r {kzheng31,xwang366}@ucsc.edu Figure 1: 3D Scene Generation from Single Image. Given top-down reference image (center), 3DTown generates coherent and realistic 3D scenes that preserves geometry, texture, and layout compared to other state-of-the-art image-to-3D generation models. Our method also generalizes across diverse styles (right), producing high-quality outputs without any 3D training."
        },
        {
            "title": "Abstract",
            "content": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, lightweight alternative, generating complex 3D scenes from single top-down image, plays an essential role in realworld applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce 3DTown, training-free framework designed to synthesize realistic and coherent 3D scenes from single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using pretrained 3D object generator, followed by masked rectified flow inpainting process that fills Preprint. in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that 3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality 3D town generation is achievable from single image using principled, training-free approach. Project website: https://eric-ai-lab.github.io/3dtown.github.io/"
        },
        {
            "title": "Introduction",
            "content": "Realistic 3D towns and structured environments serve as essential assets in simulation, robotics, digital content creation, and virtual world building. They enable scalable training for autonomous agents, immersive game environments, and rapid digital twin construction. However, constructing detailed and coherent 3D scenes typically requires either expensive 3D scanning equipment, multiview data collection, or labor-intensive modeling. In contrast, generating 3D scenes from single top-down image offers lightweight and accessible alternative, making it possible to bootstrap realistic environments from minimal input. Despite its appeal, generating coherent and realistic 3D scene from single image presents several fundamental challenges. First, the synthesized scene must exhibit consistent geometry across novel views, which is difficult for pure volumetric rendering methods such as Neural Radiance Fields (NeRF) [35] or 3D Gaussian Splatting (3DGS) [18]. While these techniques excel at photorealistic appearance modeling, they often suffer from geometry artifacts, especially in occluded or sparsely visible regions, leading to multiview inconsistencies and structural implausibility [25, 5, 66, 64]. Second, the global layout of the generated scene must remain faithful to the input image. This is particularly difficult when considering the whole scene as one asset and using image-to-3D asset generators [59, 52, 24], which fail to preserve spatial relationships between elements, resulting in distorted or semantically misaligned arrangements. Third, the local fidelity of individual objects should align closely with the visual evidence in the input. Due to the resolution constraints of 3D representations and the domain shift from object-level training to scene-level inference, previous image-to-3D generators are prone to creating low-quality meshes and misaligning the texture. To address these challenges, we propose 3DTown, training-free framework for generating complex 3D scenes from single top-down image, by enhancing the capability of image-to-3D object generators. Our method combines two core components: region-based generation and spatial-aware 3D inpainting. Each targets specific challenges in existing pipelines. We divide the scene into overlapping regions and synthesize each independently. This modular approach enables spatial upscaling and improves local alignment by grounding generation on localized image crops. To maintain global coherence and object continuity across regions, we estimate coarse 3D structure from monocular depth and landmark detection, forming spatial prior. masked rectified flow mechanism then completes missing parts while preserving known content, enhancing structural consistency and object-level fidelity throughout the scene. Through extensive experiments, we demonstrate that 3DTown generates realistic, diverse, and geometrically consistent 3D scenes from single top-down image. Our method significantly outperforms strong baselines, including Trellis [59], Hunyuan3D-2 [52], and TripoSG [24], across both human preference and GPT-based evaluations. Quantitative results show notable gains in geometry quality, layout coherence, and texture fidelity, while qualitative comparisons highlight 3DTowns ability to preserve spatial structure and fine-grained detail. These results underscore the effectiveness of our modular, training-free approach to 3D scene synthesis. Our main contributions are summarized as follows: We propose 3DTown, training-free framework for generating structured 3D scenes from single top-down image, leveraging pretrained object-centric generators for zero-shot scene asset synthesis. We develop modular generation strategy that combines region-wise latent synthesis with spatial-aware 3D inpainting, effectively addressing resolution bottlenecks, image-geometry misalignment, and inter-region inconsistency. 2 We conduct comprehensive evaluations on diverse scenes and show that 3DTown outperforms state-of-the-art baselines (Trellis, Hunyuan3D-2, and TripoSG) in geometry quality, layout coherence, and texture realism under both human and GPT-4o-based assessments."
        },
        {
            "title": "2 Related Work",
            "content": "3D Scene Generation with 2D Generative Models Recent progress in 2D generative modeling [42, 13, 4, 39] has inspired growing body of work that combines image synthesis with external 3D reconstruction techniques to generate 3D scenes. common strategy is to use pretrained image generation models to outpaint given input view according to known or sampled camera trajectories, followed by reconstruction via depth fusion or volumetric representations such as Neural Radiance Fields (NeRF) [35] and 3D Gaussian Splatting (3DGS) [18]. Early approaches primarily targeted indoor scenes [56, 20, 14, 19], while more recent efforts have extended to arbitrary natural scenes [25, 1, 10, 5], achieving improved reconstruction quality through advanced depth fusion pipelines [64, 68, 66, 62, 45, 8, 63]. Despite producing visually compelling results, these methods often suffer from geometric inconsistencies caused by hallucinations during image generation, particularly in occluded or ambiguous regions. To mitigate this, recent approaches explore panoramic [46, 23, 57, 44, 26] and multiview generation [29, 51, 11] to improve spatial coverage and structure. However, these approaches typically restrict the freedom of movement or require the camera motion for close views, and still struggle with accurate 3D geometry synthesis. The concurrent work Syncity [7] proposes block-wise generation via 3D generator and assembles them to scenes, but all generated scenes have compact layouts and cannot handle the entire scene images. In contrast, we aim to generate geometry-consistent 3D assets with arbitrary layout directly from single top-down image. 3D Scene Generative Model Beyond 2D-to-3D approaches, recent research has explored the direct generation of 3D scenes using native 3D representations. One prominent line of work leverages large language models [9, 69, 2, 15, 47] or trained diffusion models [48, 28, 65, 54, 33] to synthesize scene layouts, which are then populated by retrieving or generating 3D assets according to the predicted configuration. While these methods enable semantically plausible arrangements, they are often constrained to predefined object categories and struggle to produce coherent background geometry. Furthermore, they frequently encounter inter-object collisions. Meanwhile, separate class of methods directly models the 3D scene generation process in latent 3D spaces [58, 34, 41, 31, 21, 3]. For instance, BlockFusion [58] trains diffusion model to generate triplane representations for regional scene blocks, while LT3SD [34] and XCube [41] adopt truncated unsigned distance fields (TUDFs) or voxels to represent full scenes. Despite their architectural innovations, these models require large-scale, domain-specific datasets for training, such as indoor room datasets or urban layouts, which limits their generalizability to unseen scene types or out-ofdistribution inputs. In contrast, our method avoids any training or fine-tuning and instead employs modular, training-free pipeline that generates diverse 3D scenes from top-down images. Image-to-3D Asset Generation Another line of research focuses on generating individual 3D assets from single images, aiming to reconstruct both geometry and appearance of the depicted objects. Early works [38, 27, 50, 30, 49] leverage 2D generative models in combination with NeRF or 3D Gaussian Splatting optimization to lift 2D images into 3D. However, these approaches often suffer from low geometry fidelity and slow optimization. To address these limitations, more recent methods [12, 61, 22, 53, 67] propose to directly generate 3D latent representations via diffusion models, while others [59, 52, 24] adopt rectified flow to improve both quality and efficiency. These methods typically rely on large-scale 3D datasets such as Objaverse-XL [6], which mainly consist of object-centric assets. As result, applying them directly to full-scene synthesis introduces several challenges: limited 3D resolution becomes bottleneck for encoding complex environments, and domain shift from object-level training to scene-level inference often leads to spatial inconsistencies and layout hallucinations. In this work, we build upon pretrained 3D generation models [59] and address these limitations through region-based generation strategy combined with spatial-aware 3D inpainting. This design enables us to produce 3D scene assets with both high geometric fidelity and global layout coherence from single top-down image. 3 Figure 2: Overview of the 3DTown Pipeline. Given single top-down image, we first estimate coarse scene structure via monocular depth and landmark extraction to initialize the scene latent (Spatial Prior Initialization). The scene is divided into overlapping regions for localized synthesis and progressively fused into coherent global latent (Region-based Generation & Fusion). Each region is completed using two-stage masked rectified flow pipeline with sparse structure generator Gs and structured latent generator GL (Spatial-aware 3D Completion). The final 3D scene is decoded from the completed structured latent."
        },
        {
            "title": "3 Method",
            "content": "Given single top-down image of an unknown scene, our objective is to synthesize high-quality 3D scene that is geometrically and visually consistent with the input view. Figure 2 illustrates an overview of our pipeline. From the scene image, we first construct the scene latents with structured latent representations (Sec. 3.1) from spatial priors (Sec. 3.2). Then, we divide the scene-level latent into region-level latents for sequential processing. For each region, we extract the latents from the latest scene-level structured latents and take those as priors for spatial-aware 3D completion (Sec. 3.3) by using the base 3D generator, Trellis [59]. Later, we fuse the updated region latents to the scene latents for cross-region consistency (Sec. 3.4). After finishing all regions, we leverage pretrained object decoders on the complete scene structured latents to obtain 3D scene asset. Details are explained below. 3.1 Structured Latent Representation = {(pi, fi)}L To leverage the pretrained knowledge of the base 3D generator, we construct the scene with structured latent representation [59]: i=1, (1) where pi denotes the positional index of an active voxel in the 3D grid, and fi represents the associated latent feature vector of dimension C. Here, is the resolution of the voxel grid, and is the total number of active voxels. In general, pi captures the coarse structural layout of the object, while fi encodes fine-grained local appearance and shape information. For the pretrained models, the resolution should be equal to = 64. To upscale the resolution for the scene, we construct scene structured latents with resolution , where > (M = 2N by default). pi {0, 1, . . . , 1}3, fi RC, For the general image-to-3D asset generation, there are two pretrained rectified flow transformers: sparse structure generator Gs and structured latent generator GL. At inference time, Gs first 4 generates the active voxel positions {pi}L from the noisy grids VT with Gaussian noise. These positions are then used by GL to generate the corresponding latent features {fi}L from noisy features FT . Both generators are conditioned on an input image condition CI , encoded by DINOv2 [37]. Finally, the structured latent representation is decoded into 3D object using object decoders, which include different sparse 3D VAE decoders for 3D Gaussians, Radiance Fields, and mesh generation. The overall process is described as follows: {pi}L = Gs(VT CI ), {fi}L = GL(FT CI , {pi}L), FT (0, I)[L,C] VT (0, I)[N,N,N ] = ObjectDecoder(z), = {(pi, fi)}L i=1 (2) (3) (4) 3.2 Spatial Prior Initialization Given top-down image, the image-to-3D generator may produce outputs in arbitrary orientations. To resolve this ambiguity and provide consistent structural prior, we initialize the scene using point clouds. Specifically, we employ monocular depth estimator [55] to predict depth image and infer camera parameters, from which we construct pixel-wise point clouds. Due to occlusions, these point clouds contain missing regions, which will later be filled by the image-to-3D generator. However, occluded areas of complex objects (e.g., buildings) may have multiple plausible completions. To enforce cross-region consistency in such cases, we propose first generating landmark objects independently and conditioning subsequent generations on their geometry. To achieve this, we use Florence2 [60] to propose landmark bounding boxes and SAM2 [40] to extract instance masks. Each detected landmark is then processed individually using the 3D generator to obtain instance-level meshes. These meshes are aligned with the raw point clouds using ICP [43], replacing the original landmark regions with mesh-derived point clouds. After normalization, we voxelize the aggregated scene point clouds at resolution to obtain the initial voxelized scene, including foreground voxels 0 , and the full scene voxel set V0: 0 , background voxels V0 = {V 0 , 0 } = {pi}L, pi {0, 1, . . . , 1}3 (5) Finally, we initialize the corresponding voxel features F0 as zeros and construct the initial structured latent representation for the scene as z0 = {(V0, F0)}, shown in the top-left of Figure 2. 3.3 Region-based Generation We adopt region-based strategy to overcome the limitations of applying pretrained object-centric 3D generators directly to full scenes. These models are trained on single-object data, where each object occupies the full latent space at fixed resolution . When extended to an entire scene, this limited capacity results in low-resolution geometry and missing details. Moreover, direct scene-level generation from single top-down image often leads to layout distortions and semantic hallucinations, as the model struggles to maintain spatial relationships or align 3D content with image cues. To address this, we divide the scene into overlapping regions and condition each on its corresponding image crop, enabling locally grounded and high-fidelity generation. To implement this, we divide the initial scene voxel grid V0 (of resolution ) into set of overlapping region-level subgrids {V (r) r=1, each with shape 3, where is the resolution used by the pretrained 3D generator. For each region r, we also extract the corresponding image crop to obtain localized image conditioning input (r) . This ensures that the generation in each region is locally grounded in the image evidence. 0 }R Spatial-aware 3D Completion While region-based generation improves local fidelity, it introduces new challenge: how to maintain global consistency, especially across overlapping regions. Furthermore, since the 3D generator may create assets from any orientation, we need to alleviate the misalignment between image conditions and region latents to enable further region fusion. To address these challenges, we draw inspiration from training-free inpainting methods in 2D diffusion models, such as RePaint [32], and adapt similar approach for 3D generation. We propose to use masked rectified flow pipeline for 3D completion, which treats the partially completed global scene latent 5 , we designate known active voxels as positions {p(r) as constraint and performs conditional generation over the current region by completing only the unknown parts. Given region-level subgrids (r) i,0 }Lr,0 with 0 corresponding latent features {f (r) i,0 }Lr,0. For the coarse structure generation, we define binary mask m(r) where inactive voxels are marked for regeneration. We use the sparse structure generator Gs to complete the region structure and obtain active voxel positions {p(r) }Lr . Next, for the fine-grained local features generation, we retain original features for positions overlapping with known voxels; otherwise, features are initialized with Gaussian noise. second binary mask m(r) identifies unknown features for regeneration. We then use the structured latent generator GL to obtain the inpainted local features {f (r) }Lr . Finally, we can construct the region structured latent z(r) = {(p(r) )}Lr . These two completions both leverage the masked rectified flow pipeline (Details in the next paragraph). The whole process is shown at the bottom of Figure 2 and can be formally written as follows: i,0 }Lr,0, m(r) }Lr , {f (r) ), i,0 }Lr,0, m(r) ), FT (0, I)[Lr,C] VT (0, I)[N,N,N ] , {p(r) , {p(r) {p(r) {f (r) , (r) (7) (6) I }Lr = Gs(VT (r) }Lr = GL(FT (r) , (r) z(r) = {(p(r) )}Lr (8) Masked Rectified Flow for Completion We adopt masked generation strategy based on rectified flow to complete the unknown regions of structured 3D latent. Let xknown denote the known latent values to preserve, and let {0, 1} be binary mask that indicates which parts of the latent should be regenerated (m = 1) and which should remain fixed (m = 0). We initialize the latent variable xT (0, I) with Gaussian noise, representing the unknown region at the final time step. For each timestep = T, 1, . . . , 1, we perform resampling steps to improve stability and smoothness [32]. At each resampling iteration u, we first compute the flow field vθ(xt, t) using the rectified flow model and apply an Euler update to obtain the intermediate latent: xtprev = xt vθ(xt, t), where = 1. (9) We then re-noise the known region using forward noise operator: forward_step(x, t) = (1 t) + [σmin + (1 σmin) t] ϵ, ϵ (0, I), (10) and merge it back into the latent using the mask m: xtprev xtprev + (1 m) forward_step(xknown, tprev). (11) σmin denotes the minimum noise scale used by the pretrained rectified model. If > 1 and the latent needs to be resampled, we apply additional forward noise to the merged latent: xt forward_step(xtprev, t). Otherwise, we simply continue with xt xtprev . This masked rectified flow process iterates until = 0, at which point the completed latent x0 is returned. The full procedure is outlined in Algorithm 1 in Appendix B. 3.4 Region Fusion For each generated region, we update the scene-level structured latent z0 by replacing the corresponding part with the region-level latent z(r). Because regions are extracted using patchification strategy, some may contain only partial observations of foreground landmarks. To preserve landmark integrity, we discard those structured latents corresponding to partial foregrounds during fusion. Each region is extracted from the latest version of the scene-level latent, ensuring consistency across regions. If region overlaps with previously generated ones, its overlapping voxels are constrained to match the existing content during generation. This enforces continuity and avoids inconsistencies in overlapping areas, leading to smooth transitions between adjacent regions while preserving already synthesized content. Once all regions have been processed, the final scene-level latent is decoded using object decoders to produce scene-level meshes and 3D Gaussians. The complete textured scene is then rendered using combination of physically based rendering (PBR) baking and Gaussian Splatting. Additional implementation details, including the patchification strategy, are provided in Appendix B. 6 Table 1: Quantitative comparisons of 3DTown and baselines. We report human preference win rates and GPT-4o-based weighted win rates (%) across geometry, layout, and texture quality. 3DTown consistently outperforms all baselines by large margins in both evaluations. Models Geometry Quality Layout Coherence Texture Coherence Geometry Quality Layout Coherence Texture Coherence Human Preference Win Rate (Percentage) GPT-4o-based Weighted Win Rate (Percentage) Trellis [59] 3DTown (Ours) Hunyuan3D-2 [52] 3DTown (Ours) TripoSG [24] 3DTown (Ours) 31.50% 68.50% 33.50% 66.50% 22.50% 77.50% 30.00% 70.00% 33.50% 66.50% 23.00% 77.00% 33.00% 67.00% 31.50% 88.34% 26.00% 74.00% 17.58% 82.42% 11.66% 88.34% 24.60% 75.40% 19.04% 80.96% 12.13% 87.87% 22.34% 77.66% 14.75% 85.25% 7.67% 92.33% 22.79% 77.21%"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Benchmark To the best of our knowledge, there is no established benchmark for 3D outdoor scene mesh generation from single images. Therefore, we construct custom test set by prompting GPT-4o [17] to generate 100 diverse top-down scene images in variety of styles, such as snow village, desert town, and more. The generation details can be found in Appendix C. Metrics In the absence of ground-truth meshes, we measure performance through pairwise comparisons between models. For every reference image, two generated scenes are evaluated on three criteria: geometry quality, layout coherence, and texture coherence. Geometry quality assesses which mesh contains more detailed and fine-grained structures closer to the image evidence. Layout coherence evaluates whether the generated mesh has the same layout as the reference image. Texture coherence measures how well the generated mesh texture aligns with the corresponding parts of the reference image. Each pair is scored by two independent Amazon Mechanical Turk annotators, yielding human-preference win rate. We additionally obtain GPT-4o-based weighted win rate: GPT-4o is shown the same pair, returns token probabilities for alternatives and B, and we treat (A) as soft vote when our method is option (and analogously for B). Weighted win rate is computed as (win) if our model wins, and 1 (lose) otherwise, then averaged across all pairs. All scenes are rendered to RGB images with identical Blender settings; further GPT prompt details appear in Appendix C. Baselines We compare 3DTown with three state-of-the-art image-to-3D generation methods: (1) Trellis [59] is structured latent model that uses Rectified Flow Transformers to generate 3D assets from text or image prompts, supporting outputs such as radiance fields, 3D Gaussians, and meshes. (2) Hunyuan3D-2 [52] adopts two-stage pipeline that separately synthesizes geometry and texture using flow-based diffusion transformer, enabling high-resolution asset generation. (3) TripoSG [24] is high-fidelity 3D shape synthesis model that employs large-scale rectified flow transformers and hybrid geometric supervision to generate detailed 3D meshes from single images, demonstrating strong generalization across diverse styles. They use MV-Adapter [16] for texture generation. All models are evaluated in zero-shot setting using official pretrained checkpoints. To ensure fair comparison, background removal is disabled and the full input image is encoded for all methods. 4.2 Main Results Table 1 and Figure 3 present the main quantitative and qualitative comparisons between 3DTown and baseline methods. The results clearly demonstrate that 3DTown consistently outperforms Trellis [59], Hunyuan3D-2 [52], and TripoSG [24] across geometry, layout, and texture quality, as evaluated by both human annotators and GPT-4o. These improvements can be attributed to our region-based design and spatially guided generation strategy, which together promote better alignment between image features and 3D content, while preserving scene-wide consistency. Quantitative Analysis 3DTown region-wise decomposition enables each latent block to align with localized image crop, narrowing the domain gap between object-centric training and scenelevel inference. This design results in significant improvements in texture fidelity, with GPT-4o assigning 3DTown texture win rate of 92.3% versus 7.7% for Hunyuan3D-2. Furthermore, the upscale of resolution allows for more fine-grained structure representation, which reflects on the 7 Figure 3: Qualitative comparisons between 3DTown and baselines. Given single top-down image (left column), we compare 3D scene outputs generated by 3DTown, Trellis [59], Hunyuan3D2 [52], and TripoSG [24]. 3DTown consistently produces globally coherent scenes with fine-grained geometry, accurate object layouts, and realistic textures across variety of styles and environments. In contrast, Trellis often produces oversimplified geometry, Hunyuan3D-2 suffers from structural inconsistencies and domain mismatch, and TripoSG exhibits repetition artifacts and layout drift. improvements in geometry quality, where human preferences increase by 37 percentage points over Trellis (68.5% vs. 31.5%) and 55 points over TripoSG (77.5% vs. 22.5%). Meanwhile, the use of spatial priors and masked 3D inpainting stabilizes region layout and interregion transitions, and enforces global layout constraints, significantly reducing spatial artifacts and disjoint structures. This is reflected in layout coherence scores: 3DTown achieves 70.0% in human preference compared to 30.0% for Trellis, and reaches 87.9% in GPT-4o evaluation against only 12.1% for Hunyuan3D-2. Qualitative Analysis Qualitatively, 3DTown produces scene assets with clear structure, consistent layout, and realistic surface details that closely match the reference top-down image. In contrast, Trellis often generates overly centralized, low-resolution structures and lacks peripheral detail. Hunyuan3D-2 exhibits notable issues with layout distortion and geometry hallucinations despite acceptable textures in isolated parts. TripoSG maintains some compositional structure but frequently introduces repeated objects and ignores the layout evidence within the reference image. 3DTown region-wise generation and spatial inpainting pipeline helps it avoid these artifacts while maintaining both global coherence and local fidelity. These findings confirm that spatial decomposition and prior-guided inpainting are effective principles for lifting single-view image inputs into coherent, high-quality 3D scenes. Additional qualitative comparisons are available in Figure 5 in Appendix A. 4.3 Ablation Study We conduct ablation studies to evaluate the contributions of key components in 3DTown: the regionbased generation strategy and the use of pre-generated landmarks. Both ablation studies still apply the 8 Table 2: Ablation Study Results. Win rates for geometry, layout, and texture show that removing region-based generation or landmark conditioning degrades performance, highlighting the importance of both components in 3DTown. Human Preference Win Rate (Percentage) GPT-4o-based Weighted Win Rate (Percentage) Models Geometry Quality Layout Coherence Texture Coherence Geometry Quality Layout Coherence Texture Coherence 3DTown (w/o regions) 3DTown 3DTown (w/o landmarks) 3DTown 20.00% 80.00% 41.50% 59.50% 17.00% 83.00% 46.00% 54.00% 13.50% 86.50% 42.00% 58.00% 6.11% 92.89% 36.90% 64.10% 8.48% 91.52% 44.21% 56.79% 6.08% 92.92% 38.26% 61.74% Figure 4: Qualitative Ablation Results. Left: Reference image. Middle: 3DTown without landmark conditioning. Right: 3DTown without region-based generation. Landmark conditioning ensures consistency for foreground objects, especially for objects across regions, while region-based generation preserves overall detail and coherence. spatial-aware 3D completion. Quantitative results are shown in Table 2, and qualitative comparisons are illustrated in Figure 4. Without Region-Based Generation In this setting, the entire scene latent is directly passed into the pretrained 3D generator, without being split into localized regions. This leads to severe performance drops across all metrics. The results suggest that holistic generation fails to make full use of the pretrained models capacity, which was originally trained on single-object inputs. Without localized conditioning, the model struggles to resolve spatial context and image-to-3D correspondence, producing low-resolution and spatially incoherent outputs. As illustrated in Figure 4 (right), buildings lose structural sharpness and alignment, and the overall layout becomes underspecified. Without Landmark Conditioning Here, we disable the landmark-aware initialization and rely only on monocular depth to construct the spatial prior. This results in notable decrease in geometry and layout quality, particularly in regions containing large foreground structures like gates or towers. Because region-based generation processes the scene in spatial chunks, landmarks act as anchors to maintain object continuity across patch boundaries. Without them, the model is more likely to hallucinate disjoint or mismatched content between regions. Figure 4 (middle) shows representative failure: the landmark structure in the right-side building is incorrectly completed and misaligned with its neighboring parts. These findings demonstrate that both region-wise decomposition and landmark conditioning are essential for producing coherent, high-fidelity 3D scenes. Regions allow pretrained object generators to operate within their effective receptive fields, while landmarks preserve global anchors and prevent semantic drift during inpainting."
        },
        {
            "title": "5 Conclusion",
            "content": "To address the challenge of generating high-quality, coherent 3D scenes from single image, we proposed 3DTown, training-free framework that synthesizes 3D scenes by decomposing them into overlapping regions and guiding generation with spatial priors. Central to our approach is the use of spatial-aware 3D completion, which enables localized object fidelity while maintaining global scene coherence through masked rectified flow. Empirical results show that 3DTown outperforms existing methods across geometry, texture, and layout quality. These findings highlight the potential of modular, spatially grounded generation for scalable 3D scene synthesis from minimal input."
        },
        {
            "title": "References",
            "content": "[1] Cai, S., Chan, E.R., Peng, S., Shahbazi, M., Obukhov, A., Van Gool, L., Wetzstein, G.: Diffdreamer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 21392150 (2023) [2] Çelen, A., Han, G., Schindler, K., Van Gool, L., Armeni, I., Obukhov, A., Wang, X.: I-design: Personalized llm interior designer. arXiv preprint arXiv:2404.02838 (2024) [3] Chai, L., Tucker, R., Li, Z., Isola, P., Snavely, N.: Persistent nature: generative model of unbounded 3d worlds. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 2086320874 (2023), https://api.semanticscholar.org/CorpusID: 257687856 [4] Chang, L.W., Bao, W., Hou, Q., Jiang, C., Zheng, N., Zhong, Y., Zhang, X., Song, Z., Yao, C., Jiang, Z., et al.: Flux: fast software-based communication overlap on gpus through kernel fusion. arXiv preprint arXiv:2406.06858 (2024) [5] Chung, J., Lee, S., Nam, H., Lee, J., Lee, K.M.: Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384 (2023) [6] Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A., Laforte, C., Voleti, V.S., Gadre, S.Y., VanderBilt, E., Kembhavi, A., Vondrick, C., Gkioxari, G., Ehsani, K., Schmidt, L., Farhadi, A.: Objaverse-xl: universe of 10m+ 3d objects. ArXiv abs/2307.05663 (2023), https://api.semanticscholar.org/CorpusID:259836993 [7] Engstler, P., Shtedritski, A., Laina, I., Rupprecht, C., Vedaldi, A.: Syncity: Training-free generation of 3d worlds. arXiv preprint arXiv:2503.16420 (2025) [8] Engstler, P., Vedaldi, A., Laina, I., Rupprecht, C.: Invisible stitch: Generating smooth 3d scenes with depth inpainting. arXiv preprint arXiv:2404.19758 (2024) [9] Feng, W., Zhu, W., Fu, T.j., Jampani, V., Akula, A., He, X., Basu, S., Wang, X.E., Wang, W.Y.: Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems 36, 1822518250 (2023) [10] Fridman, R., Abecasis, A., Kasten, Y., Dekel, T.: Scenescape: Text-driven consistent scene generation. Advances in Neural Information Processing Systems 36, 3989739914 (2023) [11] Gao, R., Holynski, A., Henzler, P., Brussee, A., Martin-Brualla, R., Srinivasan, P., Barron, J.T., Poole, B.: Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314 (2024) [12] Gupta, A., Gupta, A.: 3dgen: Triplane latent diffusion for textured mesh generation. ArXiv abs/2303.05371 (2023), https://api.semanticscholar.org/CorpusID:257427345 [13] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 68406851 (2020) [14] Höllein, L., Cao, A., Owens, A., Johnson, J., Nießner, M.: Text2room: Extracting textured 3d meshes from 2d text-to-image models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 79097920 (2023) [15] Hu, Z., Iscen, A., Jain, A., Kipf, T., Yue, Y., Ross, D.A., Schmid, C., Fathi, A.: Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In: Forty-first International Conference on Machine Learning (2024) [16] Huang, Z., Guo, Y., Wang, H., Yi, R., Ma, L., Cao, Y.P., Sheng, L.: Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632 (2024) [17] Hurst, A., Lerer, A., Goucher, A.P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al.: Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024) [18] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42(4), 1391 (2023) [19] Koh, J.Y., Agrawal, H., Batra, D., Tucker, R., Waters, A., Lee, H., Yang, Y., Baldridge, J., Anderson, P.: Simple and effective synthesis of indoor 3d scenes. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 11691178 (2023) [20] Koh, J.Y., Lee, H., Yang, Y., Baldridge, J., Anderson, P.: Pathdreamer: world model for indoor navigation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1473814748 (2021) [21] Lee, J., Lee, S., Jo, C., Im, W., Seon, J., Yoon, S.E.: Semcity: Semantic scene generation with triplane diffusion. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 2833728347 (2024), https://api.semanticscholar.org/CorpusID: 268363839 [22] Li, W., Liu, J., Chen, R., Liang, Y., Chen, X., Tan, P., Long, X.: Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. ArXiv abs/2405.14979 (2024), https://api.semanticscholar.org/CorpusID: [23] Li, W., Cai, F., Mi, Y., Yang, Z., Zuo, W., Wang, X., Fan, X.: Scenedreamer360: Text-driven 3dconsistent scene generation with panoramic gaussian splatting. arXiv preprint arXiv:2408.13711 (2024) [24] Li, Y., Zou, Z.X., Liu, Z., Wang, D., Liang, Y., Yu, Z., Liu, X., Guo, Y.C., Liang, D., Ouyang, W., et al.: Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608 (2025) [25] Li, Z., Wang, Q., Snavely, N., Kanazawa, A.: Infinitenature-zero: Learning perpetual view generation of natural scenes from single images. In: European Conference on Computer Vision. pp. 515534. Springer (2022) [26] Liang, Y., Yang, X., Lin, J., Li, H., Xu, X., Chen, Y.: Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 65176526 (2024) [27] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 300309 (2022), https://api. semanticscholar.org/CorpusID:253708074 [28] Lin, C., Mu, Y.: Instructscene: Instruction-driven 3d indoor scene synthesis with semantic graph prior. arXiv preprint arXiv:2402.04717 (2024) [29] Liu, A., Li, Z., Chen, Z., Li, N., Xu, Y., Plummer, B.A.: Panofree: Tuning-free holistic multiview image generation with cross-view self-guidance. In: European Conference on Computer Vision. pp. 146164. Springer (2024) [30] Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 92649275 (2023), https://api.semanticscholar.org/CorpusID:257631738 [31] Liu, Y., Li, X., Li, X., Qi, L., Li, C., Yang, M.H.: Pyramid diffusion for fine 3d large scene generation. ArXiv abs/2311.12085 (2023), https://api.semanticscholar.org/CorpusID: 265308971 [32] Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., Van Gool, L.: Repaint: Inpainting using denoising diffusion probabilistic models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1146111471 (2022) [33] Maillard, L., Sereyjol-Garros, N., Durand, T., Ovsjanikov, M.: Debara: Denoising-based 3d room arrangement generation. Advances in Neural Information Processing Systems 37, 109202109232 (2024) 11 [34] Meng, Q., Li, L., Nießner, M., Dai, A.: Lt3sd: Latent trees for 3d scene diffusion. ArXiv abs/2409.08215 (2024), https://api.semanticscholar.org/CorpusID:272600456 [35] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM 65(1), 99106 (2021) [36] OpenAI:"
        },
        {
            "title": "Introducing",
            "content": "openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/ (2025) [37] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) [38] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. ArXiv abs/2209.14988 (2022), https://api.semanticscholar.org/CorpusID: 252596091 [39] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zeroshot text-to-image generation. In: International conference on machine learning. pp. 88218831. Pmlr (2021) [40] Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024) [41] Ren, X., Huang, J., Zeng, X., Museth, K., Fidler, S., Williams, F.: Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 42094219 (2023), https://api.semanticscholar. org/CorpusID:273025441 [42] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1068410695 (2022) [43] Rusinkiewicz, S., Levoy, M.: Efficient variants of the icp algorithm. In: Proceedings third international conference on 3-D digital imaging and modeling. pp. 145152. IEEE (2001) [44] Schult, J., Tsai, S., Höllein, L., Wu, B., Wang, J., Ma, C.Y., Li, K., Wang, X., Wimbauer, F., He, Z., et al.: Controlroom3d: Room generation using semantic proxy rooms. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 62016210 (2024) [45] Shriram, J., Trevithick, A., Liu, L., Ramamoorthi, R.: Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199 (2024) [46] Stan, G.B.M., Wofk, D., Fox, S., Redden, A., Saxton, W., Yu, J., Aflalo, E., Tseng, S.Y., Nonato, F., Muller, M., et al.: Ldm3d: Latent diffusion model for 3d. arXiv preprint arXiv:2305.10853 (2023) [47] Sun, C., Han, J., Deng, W., Wang, X., Qin, Z., Gould, S.: 3d-gpt: Procedural 3d modeling with large language models. arXiv preprint arXiv:2310.12945 (2023) [48] Tang, J., Nie, Y., Markhasin, L., Dai, A., Thies, J., Nießner, M.: Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2050720518 (2024) [49] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. ArXiv abs/2309.16653 (2023), https://api. semanticscholar.org/CorpusID:263131552 [50] Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 2276222772 (2023), https://api.semanticscholar.org/ CorpusID:257757320 12 [51] Tang, L., Jia, M., Wang, Q., Phoo, C.P., Hariharan, B.: Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems 36, 13631389 (2023) [52] Team, T.H.: Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation (2025) [53] Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., Kreis, K., et al.: Lion: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems 35, 1002110039 (2022) [54] Vilesov, A., Chari, P., Kadambi, A.: Cg3d: Compositional generation for text-to-3d via gaussian splatting. arXiv preprint arXiv:2311.17907 (2023) [55] Wang, J., Chen, M., Karaev, N., Vedaldi, A., Rupprecht, C., Novotny, D.: Vggt: Visual geometry grounded transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2025) [56] Wiles, O., Gkioxari, G., Szeliski, R., Johnson, J.: Synsin: End-to-end view synthesis from single image. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 74677477 (2020) [57] Wu, T., Zheng, C., Cham, T.J.: Panodiffusion: 360-degree panorama outpainting via diffusion. arXiv preprint arXiv:2307.03177 (2023) [58] Wu, Z., Li, Y., Yan, H., Shang, T., Sun, W., Wang, S., Cui, R., Liu, W., Sato, H., Li, H., et al.: Blockfusion: Expandable 3d scene generation using latent tri-plane extrapolation. ACM Transactions on Graphics (TOG) 43(4), 117 (2024) [59] Xiang, J., Lv, Z., Xu, S., Deng, Y., Wang, R., Zhang, B., Chen, D., Tong, X., Yang, J.: Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506 (2024) [60] Xiao, B., Wu, H., Xu, W., Dai, X., Hu, H., Lu, Y., Zeng, M., Liu, C., Yuan, L.: Florence2: Advancing unified representation for variety of vision tasks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 48184829 (2024) [61] Xiong, B., Wei, S.T., Zheng, X.Y., Cao, Y.P., Lian, Z., Wang, P.S.: Octfusion: Octreebased diffusion models for 3d shape generation. ArXiv abs/2408.14732 (2024), https: //api.semanticscholar.org/CorpusID:271962988 [62] Yang, Y., Yin, F., Fan, J., Chen, X., Li, W., Yu, G.: Scene123: One prompt to 3d scene generation via video-assisted and consistency-enhanced mae. arXiv preprint arXiv:2408.05477 (2024) [63] Yu, H.X., Duan, H., Herrmann, C., Freeman, W.T., Wu, J.: Wonderworld: Interactive 3d scene generation from single image. arXiv preprint arXiv:2406.09394 (2024) [64] Yu, H.X., Duan, H., Hur, J., Sargent, K., Rubinstein, M., Freeman, W.T., Cole, F., Sun, D., Snavely, N., Wu, J., et al.: Wonderjourney: Going from anywhere to everywhere. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 66586667 (2024) [65] Zhai, G., Örnek, E.P., Chen, D.Z., Liao, R., Di, Y., Navab, N., Tombari, F., Busam, B.: Echoscene: Indoor scene generation via information echo over scene graph diffusion. In: European Conference on Computer Vision. pp. 167184. Springer (2024) [66] Zhang, J., Li, X., Wan, Z., Wang, C., Liao, J.: Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics 30(12), 77497762 (2024) [67] Zhang, L., Wang, Z., Zhang, Q., Qiu, Q., Pang, A., Jiang, H., Yang, W., Xu, L., Yu, J.: Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG) 43, 1 20 (2024), https://api.semanticscholar.org/CorpusID: 270619933 13 [68] Zhang, S., Zhang, Y., Zheng, Q., Ma, R., Hua, W., Bao, H., Xu, W., Zou, C.: 3d-scenedreamer: Text-driven 3d-consistent scene generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1017010180 (2024) [69] Zhou, X., Ran, X., Xiong, Y., He, J., Lin, Z., Wang, Y., Sun, D., Yang, M.H.: Gala3d: Towards text-to-3d complex scene generation via layout-guided generative gaussian splatting. arXiv preprint arXiv:2402.07207 (2024) Figure 5: More qualitative comparisons between 3DTown and baselines. From the image, we can find that 3DTown can generate more coherent scenes from diverse scene images. 15 Algorithm 1: Masked Rectified Flow for Completion Pipeline Input : vθ(x, t) learned flow field; xknown known latent to preserve; {0, 1} mask for regeneration (1=regenerate, 0=preserve); total steps; Resample times per step; σmin minimum noise scale Output :x0 regenerated latent /* Forward-noise operator ϵ (0, I) forward_step(x, t) = (1 t) + (cid:2)σmin + (1 σmin) t(cid:3) ϵ /* Initialization xT (0, I); for = T, 1, . . . , 1 do */ */ /* predict flow field */ /* Euler update on unknown */ /* re-noise known */ tprev 1; tprev; for = 1, . . . , do vθ(xt, t) ; xtprev xt ; ˆxtprev forward_step(xknown, tprev) ; xtprev xtprev + (1 m) ˆxtprev; if < and > 1 then xt forward_step(xtprev, t); else xt xtprev; return x0 = x0;"
        },
        {
            "title": "A More Results",
            "content": "Figure 5 presents additional qualitative results comparing 3DTown with Trellis [59], Hunyuan3D2 [52], and TripoSG [24] across diverse set of visual scenes. These examples further demonstrate the robustness and generality of our approach across different architectural styles, spatial layouts, and artistic domains. 3DTown consistently produces scene assets that are geometrically detailed, visually coherent, and well-aligned with the input reference images. In contrast, baseline models frequently suffer from artifacts such as repeated structures, layout collapse, or low-resolution textures. These comparisons further highlight the benefits of our region-based generation and spatial-aware inpainting pipeline in generating diverse 3D scenes from single image without training."
        },
        {
            "title": "B Method Details",
            "content": "Region Extraction Strategy. To generate overlapping regions with balanced seams, we first compute the tight bounding box of all occupied voxels and tile it with base grid of non-overlapping patches of size (px, py, pz). For the vertical (z) axis, start positions are evenly interpolated so that the entire height is covered with the minimum number of full patches. Next, we insert seam patches to equalise overlap: for every pair of neighbouring base-grid origins we create an additional patch whose origin is the midpoint between themfirst along the x-axis (keeping y, fixed), then along the y-axis (keeping x, fixed). The union of base and seam origins is deduplicated and sorted, after which the corresponding voxel sub-volumes are extracted. The procedure returns the list of patch origins and their binary masks, and is invoked once per scene to define the region schedule used in our region-wise generation and fusion pipeline. Masked Rectified Flow Algorithm For completeness, we provide the full algorithmic details of the masked rectified flow completion process used in our spatial-aware 3D inpainting pipeline. While the core formulation is introduced in Section 3.3, this pseudocode (Algorithm 1) clarifies the iterative 16 update, re-noising, and resampling procedures that enable conditional generation of unknown regions while preserving the known latent structure. Implementation Details For all rectified flow generators, we step the sampling time step = 50. The classifier-free guidance scales are 7.5 and 5 for the spare structure generator and structured latent generator. Resampling time during the masked rectified flow is set to 2. The whole pipeline can be loaded with an NVIDIA RTX A5000 GPU with 24G VRAM."
        },
        {
            "title": "C Experiment Settings",
            "content": "Test Set Generation To evaluate models performance under stylistically diverse conditions, we curated human-verified synthesized image test set with 100 top-down views. Given an example image, we ask ChatGPT-o3 [36] to generate image prompts for top-down scene views with this requirments: 1280 720 resolution, quasi-orthographic three-quarter (isometric) camera, one hero landmark at the image centre, 1020 surrounding buildings, daylight illumination, and no faraway object. Then, we used GPT-4o [17] to generate scene images according to the corresponding prompts. Human Evaluation Given reference image and observations of two generated scenes, we ask the human annotator to answer these three questions: Which scene, or B, has geometry that is more detailed, precise, and closer to the reference image? Which scene, or B, demonstrates spatial layout and arrangement of objects that is more coherent and closely aligned with the layout in the reference image? Which scene, or B, exhibits textures that are significantly more coherent and consistent with the reference image? GPT-4o-based Evaluation For GPT-based automatic evaluation, we ask the same questions as the human evaluation and prompt the model to directly return the answer with top-5 token log probability. Then, we extract the token probability of and (0 if not included) as the answer weights."
        },
        {
            "title": "D Limitation",
            "content": "Although 3DTown delivers strong scene-level results, several challenges remain. The pretrained 3D generator we adopt is trained on single-object imagery; even after region decomposition, the underlying distribution mismatch can lead to patch-level hallucinationsfor example, duplicated façades or unrealistic roof shapes. Future work could mitigate this via scene-level fine-tuning or domain adaptation. Our coarse spatial prior contains many holes where occlusions obscure geometry. Regions dominated by such voids sometimes inherit empty or oversmoothed surfaces from the generator. Integrating uncertainty-aware depth completion, multi-view cues, or semantic priors may yield denser scaffolds and more reliable inpainting in future work."
        }
    ],
    "affiliations": [
        "Columbia University",
        "Cybever AI",
        "UC Santa Cruz"
    ]
}