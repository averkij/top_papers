{
    "paper_title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation",
    "authors": [
        "Hoigi Seo",
        "Wongi Jeong",
        "Jae-sun Seo",
        "Se Young Chun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores."
        },
        {
            "title": "Start",
            "content": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Hoigi Seo * 1 Wongi Jeong * 1 Jae-sun Seo 2 Se Young Chun 1 3 5 2 0 2 2 1 ] . [ 1 0 9 6 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores. 1. Introduction Diffusion generative models excel at tasks such as text-toimage (T2I) synthesis (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024; Chen et al., 2024), editing (Brooks et al., 2023; Cao et al., 2023; Kawar et al., 2023), video generation (Liu et al., 2024; Polyak et al., 2024), and 3D creation (Poole et al., 2022; Cao et al., 2023; Seo et al., *Equal contribution 1Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea 2School of Electrical and Computer Engineering, Cornell Tech, USA 3INMC & IPAI, Seoul National University, Republic of Korea. Correspondence to: Se Young Chun <sychun@snu.ac.kr>. 1 (a) FLOPs ratio. (b) Parameter ratio. Figure 1. (a) FLOPs distribution during image generation in Stable Diffusion 3 (SD3) (Esser et al., 2024). (b) Parameter distribution across modules in SD3. The text encoders contributes less than 0.5% to the overall FLOPs but account for over 70% of the total model parameters. For VAE, only the decoder was considered. 2023; Wang et al., 2024c). With modern architecture and large-scale text encoders, they produce high-quality images that closely match text prompts. Despite these successes, they require significant computational resources, especially memory, making deployment and scalability challenging. To address these issues, research suggests enhancing the efficiency of the T2I diffusion model through strategies such as knowledge distillation (KD) (Castells et al., 2024b; Li et al., 2024b; Song et al., 2024b; Kim et al., 2024; Zhao et al., 2024), which transfers knowledge from larger models to smaller ones; pruning (Castells et al., 2024a; Ganjdanesh et al., 2024; Lee et al., 2024; Wang et al., 2024b), which eliminates superfluous weights; and quantization (Li et al., 2023; He et al., 2024; Ryu et al., 2025), which reduces precision by utilizing fewer bits. While effective, these methods target the denoising module. As shown in Fig. 1, the text encoders account for over 70% of the total parameters, but only 0.5% of floating-point operations (FLOPs), causing disproportionate memory usage. Despite this imbalance, efforts to reduce the text encoder size have been limited. Large language models (LLMs) also face similar challenges, where model sizes result in significant computational and memory overhead, hindering their practical use. To address this problem, studies such as KD (Hsieh et al., 2023; Huang et al., 2023; Ko et al., 2024), quantization (Xiao et al., 2023; Ashkboos et al., 2024b; Lin et al., 2024), and pruning (Sun et al., 2023; Ashkboos et al., 2024a; Men et al., 2024; Song Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation et al., 2024a; Yang et al., 2024; Zhang et al., 2024) have been proposed. While KD reduces the size of the model, it requires costly training. Quantization reduces memory usage by reducing precision, but it requires specific hardware support. In contrast, pruning offers parameter reduction with minimal performance loss, making it an efficient solution. Among pruning techniques, structured pruning has been actively studied to remove rows and columns (van der Ouderaa et al., 2023; Ashkboos et al., 2024a) of the model weights, or entire layers or blocks (Gromov et al., 2024; Men et al., 2024; Yang et al., 2024; Zhang et al., 2024) of the model to reduce the size of the model and improve the inference speed. However, these methods are designed for autoregressive LLMs and face challenges when applied to T2I diffusion models, limiting their effectiveness in this context. We propose Skip and reuse the layers (Skrr), blockwise pruning technique for T2I diffusion models. Skrr effectively reduces text encoder size, alleviating memory overhead while preserving image quality and text alignment. Skrr involves two primary stages: the Skip to detect layers for pruning, followed by the Re-use to recycle the remaining layers to mitigate performance degradation. To the best of our knowledge, this is the first work to tackle the challenge of constructing lightweight text encoder for T2I tasks. In the Skip phase, sub-blocks of the text encoder transformer are pruned using T2I diffusion-tailored discrepancy metric to align dense and pruned models. Prior methods greedily remove blocks to reduce computational costs, but often overlook block interactions, leading to suboptimal pruning. To mitigate this, we propose beam search (Freitag & Al-Onaizan, 2017)-based approach that explores multiple pruning paths simultaneously, balancing the performance of exhaustive and efficiency of greedy strategies. In the Re-use phase, we assess the discrepancy from reusing adjacent unskipped blocks to identify those that can be effectively reutilized. Additionally, we provide theoretical support showing that Re-use can enhance performance beyond mere skipping. To improve discrepancy measurement in both phases, we employ projection module identical to the one used for conditioning text embeddings in the denoising module. We conducted comprehensive experiments across various metrics, sparsity levels, and diffusion models to thoroughly evaluate the T2I performance of compressed text encoders. The results indicate that Skrr surpasses current autoregressive LLM-targeted pruning techniques on image fidelity and text-image alignment in high sparsity (> 40%). Our contributions can be summarized as follows. We propose Skrr, an effective layer pruning method for the text encoder in T2I diffusion models. We present Skip, pruning approach for lightweight T2I diffusion models, and Re-use, method to restore T2I performance by leveraging the remaining layers, supported by theoretical analysis. Skrr achieves state-of-the-art blockwise pruning for T2I synthesis, improving GenEval scores by up to 20.4% at high sparsity over 40%. 2. Related Works 2.1. Efficient diffusion model As diffusion generative models scale, T2I synthesis has achieved impressive results in generating high-fidelity, textaligned images. However, this advancement comes with significant computational and memory overhead. Previous research has primarily focused on optimizing the efficiency of the denoising module through methods such as knowledge distillation (Li et al., 2024b; Song et al., 2024b; Castells et al., 2024b; Zhao et al., 2024; Kim et al., 2024), pruning model weights (Fang et al., 2023; Ganjdanesh et al., 2024; Wang et al., 2024b; Castells et al., 2024a; Lee et al., 2024), and quantization of weights to lower precision bits (Li et al., 2023; He et al., 2024; Li et al., 2024a; Wang et al., 2024a; Ryu et al., 2025). While these approaches effectively reduce the computational and memory costs of the pipeline, they overlook the substantial memory burden imposed by the text encoder, which remains largely underexplored. To address this gap, we propose targeted pruning strategy for the text encoder of T2I diffusion model, which allows more memory efficient T2I synthesis with comparable performance. 2.2. Blockwise pruning for LLMs Although LLMs show promising performance in various tasks, their size often limits practical deployment. To address this problem, model compression techniques have been developed, with pruning emerging as promising solution due to its ability to reduce the parameter count with minimal retraining. Notably, blockwise pruning of transformers (Men et al., 2024; Yang et al., 2024; Zhang et al., 2024) effectively reduces parameters while preserving performance. ShortGPT (Men et al., 2024) proposed method to prune blocks to the desired sparsity by removing them one by one and assigning Block Influence (BI) score based on changes in cosine similarity and pruning those with lower BI first. LaCo (Yang et al., 2024) introduced strategy to reduce the size of LLMs by merging the weights of adjacent transformer blocks, effectively compressing the model. FinerCut (Zhang et al., 2024) refined this approach by pruning sub-blocks, composed of Multi-Head Attention (MHA) and Feed-Forward Network (FFN) layers with normalization, in fine-grained manner. It sequentially pruned sub-blocks while partially considering interactions between blocks, reducing the performance gap with the dense model. Despite these advances, text encoder compression in diffu2 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation (a) Illustration of Skip phase. (b) Illustration of Re-use phase. Figure 2. The visualization of overall framework of Skrr. (a) shows the Skip phase, which repeatedly assesses each sub-block by determining the output discrepancy (Disc.) between the dense and skipped models using calibration dataset (Calib. data). To account for block interactions, it keeps the top options with the smallest discrepancies and uses beam search for refined selection. (b) presents the Re-use phase, evaluating if recycling remaining block instead of skipped sub-blocks results in smaller output discrepancy. If so, hidden states are fed back into the chosen layers. This two-phase approach efficiently reduces model size with minimal T2I performance loss. sion models remains underexplored. To address this discrepancy, we propose Skrr, blockwise text encoder pruning approach tailored for T2I diffusion models. We evaluated Skrr against existing LLM-based pruning techniques, demonstrating its effectiveness in reducing the size of the model while maintaining the T2I performance of its dense model. 3. Method Skrr is built around two main parts: Skip identifies layers to prune, and Re-use selects layers to reuse from the remained ones. During the Skip phase, each multi-head attention (MHA) and feed-forward network (FFN) sub-block is individually evaluated for its importance using T2I diffusion-tailored metric. The sub-blocks are then ranked based on their significance. To optimize the pruning process, blocks with low importance are removed sequentially while exploring multiple possible combinations using beam search-based algorithm. The Re-use phase evaluates each layer to reuse layer based on the metric leveraged in Skip phase to the original output, ensuring that important information is conserved, thus reducing performance loss. The overall framework of Skrr is depicted in Fig. 2. 3.1. Skip Algorithm Feasibility of skipping blocks. Prior work (Men et al., 2024; Yang et al., 2024; Zhang et al., 2024) shows transformer blocks can be pruned based on output similarity in LLMs. We extend this analysis to text encoders in diffusion models, especially T5-XXL (Raffel et al., 2020), widely used in T2I models, as shown in Fig. 3a. We observed high degree of similarity between the hidden states of adjacent blocks. This strong similarity underscores the redundancy in the model and confirms the potential to prune blocks without significantly compromising performance. Discrepancy metric. For effective block removal, it is crucial to evaluate impact of pruning each block on output. strong metric must be defined to measure text embedding changes of post-removal. Given that text embeddings affect image quality in text-to-image models, choosing the right metric is vital. Current T2I diffusion models predominantly employ transformer frameworks to synthesize images from input noise and text embeddings. However, text embeddings from the output of the text encoder are not used as is. They undergo alignment through single linear layer or multilayer perceptron (MLP), which is represented as: = proj(E(c; θtext.); θdenoise.), (1) where is the input prompt, E(; θtext.) is the text encoder parameterized with θtext., proj(; θdenoise.) is the projection layer in the denoising module for condition vector from the text encoder. Using the features extracted in this manner, the importance of block can be evaluated by comparing the similarity between the feature fdense of the dense model and the feature fskip of the model that skips (prunes) block. 3 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation (a) Hidden states similarity. (b) Fixed input similarity. Figure 3. (a) The cosine similarity of hidden states in T5 transformer blocks demonstrates progressive variations. The result indicates specific layers could be omitted without serious performance degradation. (b) The cosine similarity of block outputs using fixed inputs. The similarity map reveals redundant role across blocks, suggesting that certain blocks could be replaced by adjacent blocks. commonly used metric in prior studies (Men et al., 2024; Yang et al., 2024; Zhang et al., 2024) for measuring discrepancy is cosine similarity, which is formulated as: Metric1(fdense, fskip) = 1 fdense fskip fdense2fskip2 (2) Another metric worth considering is the mean-squared error (MSE) between two vectors, which differs from angular metrics by accounting for both the direction (angle) and the magnitude of the vectors. The MSE is formulated as: Metric2(fdense, fskip) = 1 (cid:88) i=1 (f dense skip)2, (3) where is the dimension of the output feature vectors fdense and fskip, and represents the i-th component of vector . Metric1 considers only the angle between vectors, while Metric2 integrates both the angle and magnitude, offering more comprehensive evaluation of output discrepancies between dense and skipped models (Zhang et al., 2024). Therefore, we chose Metric2 to assess the discrepancies. Null condition discrepancy. In diffusion generative models, guidance is essential to produce high-quality images. prevalent approach is classifier-free guidance (CFG) (Ho & Salimans, 2021), which improves conditional synthesis by utilizing unconditional scores. This technique derives an unconditional score from null condition, extrapolates it with the conditional score, and is formulated as follows: ϵ(xt, z) = (1 + w)ϵ(xt, fc) wϵ(xt, f), (4) where ϵ(, ) is the denoising score network, xt is noisy sample at timestep t, fc denotes the condition vector from Eq. (1), represents the null condition vector, and is the guidance scale. While guidance improves image quality, 4 (a) Original image. (b) 7th, 22th removed. (c) 3rd, 5th removed. Figure 4. (a) An image is created by the PixArt-Σ dense text encoder using the prompt car made out of vegetables. with f2 = 0.03. For image (b), the 7th and 22th sub-blocks are excluded, resulting in Metric1 = 0.85, Metric2 = 0.002, and f2 = 0.19. Image (c) is generated by removing the 3rd and 5th sub-blocks, producing Metric1 = 0.89, Metric2 = 0.04, and f2 = 3.34. Despite Metric1 being higher in (c), the large f2 value compared to (b) leads to an abnormal image. Notably, Metric2 more accurately indicates differences in image quality. excessive scaling may cause over-saturation or artifacts. We observed that pruning certain text encoder blocks amplifies its norm by over 100 compared to the dense version, leading to abnormal images. As illustrated in Fig. 4, pruning even two blocks significantly increases f2, causing abnormalities shown in Fig. 4c. Thus, discrepancies in must be considered. Notably, in Fig. 4c, Metric1 failed to assess image quality, while Metric2 reported smaller values for Fig. 4b, confirming its reliability and effectiveness. Beam search-based strategy. Most blockwise pruning methods perform (1) rank the importance of the block per layer and pruning in order (Men et al., 2024), or (2) perform sequential pruning while re-evaluating blocks (Yang et al., 2024; Zhang et al., 2024). The first method is efficient, but may ignore block interactions and lead to suboptimal results. The second method acknowledges interactions among blocks, yet its greedy approach can still lead to suboptimal results. To address these issues, we propose novel method akin to beam search, which evaluates multiple pruning paths concurrently to better account for block interactions, achieving more effective pruning without degrading performance. Algorithm. To synthesize the proposed approach, we introduce the Skip algorithm in Algorithm 1. Before delving into the specifics, we define two key discrepancy metrics: Dfc , derived from Metric2, and Df , representing Metric2 with null inputs. The algorithm is inspired by beam search (Freitag & Al-Onaizan, 2017), iterating over each unskipped sub-block while maintaining the beams with the smallest sum of Dfcand Df . This process is repeated from the beams, iteratively updating them to ensure smaller D. Upon traversing all blocks, the algorithm produces list of Skip indices , effectively capturing interblock interactions. Finally, the blocks are pruned sequentially according to the to achieve the desired sparsity. Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Algorithm 1 Skip Algorithm Algorithm 2 Re-use Algorithm Require: Calibration dataset C, dense model M, null input c, number of layers L, beam size Require: Calibration dataset C, dense model M, null input c, skip indices list S, re-use indices dictionary for each layer index / do Bnew for each (D, S) do Ensure: Skip index list 1: [ ] 2: {(0, S)}, 3: while (D, S) such that < do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while 16: return Append(S, i) ˆM Prune(M, ) GetDiscrepancy(M, ˆM, C, c) Bnew Bnew {(D, )} end for Update Bnew to smallest candidates with (D, ) arg min(D,S ) in end for Ensure: Re-use indices dictionary 1: , ˆM Prune(M, S) 2: DM GetDiscrepancy(M, ˆM, C, c) 3: for each do max{j < / S}, min{j > / S} 4: ˆMl Update( ˆM, {i : l}) 5: ˆMr Update( ˆM, {i : r}) 6: 7: DM GetDiscrepancy(M, ˆM, C, c) 8: Dl GetDiscrepancy(M, ˆMl, C, c) 9: Dr GetDiscrepancy(M, ˆMr, C, c) if Dl < DM Dl < Dr then 10: 11: 12: 13: 14: 15: 16: end for 17: return end if ˆM Update( ˆM, R) else if Dr < DM Dr < Dl then {i : r} {i : l} Function Definitions: 1: GetDiscrepancy(M, ˆM, C, c): Dfc MSE(M(C), ˆM(C)) 2: Df MSE(M(c), ˆM(c)) 3: return Dfc + Df 4: where Fi : (zi, θi) (cid:55) Rd is the i-th block with parameters θi, and zi Rd. Assume that Fi is Li-Lipschitz in zi and Mi-Lipschitz in θi. Then, for any two parameter sets θ = (θ1, . . . , θL) and ˆθ = (ˆθ1, . . . , ˆθL), the following holds: 3.2. Re-use Algorithm Feasibility of reusing blocks. Despite extensive research on methods such as recurrent networks (Sherstinsky, 2020; Gu & Dao, 2023) which loop the output of the network back into the input for efficiency, the practice of reusing specific layers in neural networks by reintegrating hidden states into internal layers remains understudied. We conducted feasibility study to investigate whether the internal components of the T2I diffusion text encoder serve analogous functions (see Fig. 3b). We randomly sampled tokens from the embedding, passed them through each transformer block as fixed input, and measured the similarity of their output. The results indicate significant similarity between adjacent blocks, suggesting that performance can be restored by reintroducing non-omitted layers into adjacent skipped layers. We also verified the existence of condition for Re-use that achieves tighter error bound compared to Skip alone. The existence is formalized in Theorem 3.2, which theoretically demonstrates the advantages of incorporating the Re-use phase. To establish this result, we first introduce the following lemma. Lemma 3.1 (Error bound of two transformers). Let : (x, θ) (cid:55) Rd be an L-block transformer with input Rd and parameter set θ = (θ1, . . . , θL), defined as: = (cid:0)(FL + I) (FL1 + I) (F1 + I)(cid:1) (5) 5 (cid:13)M(x; θ) M(x; ˆθ)(cid:13) (cid:13) (cid:13) (cid:18) (cid:89) (cid:88) (1 + Lk) (cid:19) Mi (cid:13) (cid:13)θi ˆθi (cid:13) (cid:13) := (6) i=1 k=i+1 The proof for Lemma 3.1 is in the Appendix Sec. A.1. With the lemma, we can prove the following theorem. Theorem 3.2 (Tighter error bound of Re-use). Under the assumptions of Lemma 3.1, let θ be the parameters of the reused Fi. Define USkip as the error bound for the compressed model with Skip alone and USkip, Re-use as the error bound for the compressed model with Skip and Re-use. If θi θ < θi, then the following holds: USkip, Re-use < USkip. (7) This theorem establishes the theoretical feasibility of Reuse by showing existence of condition under which the error bound becomes tight with its application. The proof for Theorem 3.2 is shown in the Appendix Sec. A.2. Algorithm. The Re-use algorithm provided in Algorithm 2 enhances performance of pruned models by reintroducing adjacent layers for skipped layers. Starting with pruning based on skip indices S, it evaluates each skipped block for reuse with discrepancy score between pruned and dense model outputs under three configurations: current Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Table 1. Quantitative comparisons of Skrr with baselines. We compared Skrr with the baselines of ShortGPT, LaCo, and FinerCut under three different sparsity scenarios on PixArt-Σ. The results show that Skrr reliably maintains image fidelity and performs comparable to the dense model across all given sparsity levels. Unlike ShortGPT and LaCo, FinerCut and Skrr use sub-block pruning, hindering direct sparsity level alignment. Sparsity levels were matched as closely as possible for fair evaluation, reflecting how much the compressed models parameters differ from the dense encoder. ( / denotes that higher / lower metric is favorable.) Method Dense ShortGPT LaCo FinerCut Skrr (Ours) Sparsity FID CLIP DreamSim GenEval (%) 0.0 24.3 32.4 40.5 24.3 32.4 40.5 26.3 32.2 41.7 27.0 32.4 41. 22.89 24.96 27.28 55.26 19.45 24.70 21.60 20.66 20.49 20.36 20.15 20.19 19.93 0.314 0.309 0.294 0.215 0.311 0.303 0.291 0.313 0.313 0.308 0.315 0.313 0.312 Single Two Count. Colors Pos. Color attr. Overall 0.988 0.616 0. 0.795 0.108 0.944 0.834 0.306 0.909 0.781 0.784 0.947 0.903 0.841 0.956 0.928 0.913 0.381 0.197 0.025 0.336 0.227 0.162 0.465 0.409 0.306 0.434 0.397 0.410 0.431 0.291 0.090 0.394 0.250 0.150 0.394 0.344 0.306 0.425 0.413 0.450 0.715 0.537 0.100 0.713 0.606 0.489 0.737 0.697 0.628 0.763 0.774 0. 0.033 0.048 0.0 0.065 0.043 0.030 0.103 0.078 0.050 0.095 0.100 0.055 0.255 0.083 0.038 0.0 0.128 0.040 0.033 0.105 0.128 0.073 0.145 0.118 0.068 0.539 0.431 0.324 0.087 0.424 0.325 0.275 0.458 0.426 0.367 0.471 0.455 0.442 1. 0.753 0.651 0.357 0.726 0.677 0.620 0.798 0.771 0.731 0.800 0.775 0.741 state, reuse of the previous sub-block, and reuse of the subsequent sub-block. The configuration with the smallest is selected, ensuring alignment with the dense model. MHA and FFN sub-blocks are reused as their respective types, maintaining consistency and functionality. The sub-block with the lowest discrepancy is reintroduced, iteratively updating the dictionary for Re-use until all skipped layers are evaluated. This results in refined dictionary R, allowing efficient compression while maintaining performance. 4. Experiments Baselines. To evaluate the performance of Skrr, we compared multiple LLM-based blockwise pruning techniques for T2I synthesis, including ShortGPT (Men et al., 2024), LaCo (Yang et al., 2024) and FinerCut (Zhang et al., 2024). Pruning in diffusion pipelines specifically targets the T5XXL (Raffel et al., 2020), which constitutes most of the parameter size. Detailed configurations and implementation specifics are available in Appendix Sec. B.1 and Sec. B.2. Dataset. Most blockwise pruning methods rely on calibration dataset to identify and remove less influential blocks. To this end, we constructed calibration set by sampling 1k text prompts from the CC12M (Changpinyo et al., 2021), specifically tailored for T2I synthesis. The detailed configuration and example are provided in Appendix Sec. B.3. 4.1. Quantitative results Metrics. We evaluated the performance of the Skrr and baseline models with four metrics: Frechet Inception Distance (FID) (Heusel et al., 2017), CLIP (Radford et al., 2021) score and DreamSim (Fu et al., 2023) score with MS-COCO (Lin et al., 2014) 30k validation set, alongside GenEval (Ghosh et al., 2024). FID measures the real versus generated image similarity using Inception-v3 features (Szegedy et al., 2016). The CLIP score measures the semantic alignment of the image-text. DreamSim assesses the composition and color similarity in pruned text encoders versus the original. GenEval measures pruned text encoders effects on T2I synthesis across six dimensions: single object, two objects, counting, color accuracy, positional alignment, and color attribution. See Appendix Sec. B.4 for details. T2I synthesis performance comparison. We evaluated the T2I synthesis performance of Skrr against various baselines and metrics, as shown in Table 1. ShortGPT maintains performance at low sparsity, but its performance deteriorates rapidly as sparsity increases. similar pattern is observed for LaCo. FinerCut exhibits more gradual decline in performance compared to other baselines, but the generated images still show significant drop in GenEval scores. In contrast, Skrr demonstrates performance comparable to dense models in all the sparsity levels, achieving high fidelity while preserving strong text alignment metrics especially in high-sparsity settings. Interestingly, in some cases, the FID score for compressed models improves, while DreamSim, CLIP, and GenEval scores degrade. This indicates that compressing text encoders preserves or boosts image quality, while text alignment declines. We provide further detailed analysis in Sec. 5. 6 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure 5. Comparison of images generated with baseline and Skrr-compressed text encoders across PixArt-Σ, Stable Diffusion 3 (SD3), and FLUX.1-dev. At low sparsity (level 124.3% for ShortGPT and Laco, 26.3% for FinerCut, and 27.0% for Skrr), both methods perform comparably to dense models, but Skrr outperforms at higher sparsity(level 232.4% for ShortGPT and Laco, 32.2% for FinerCut, and 32.4% for Skrr, level 340.5% for ShortGPT and Laco, 41.7% for FinerCut, and 41.9% for Skrr), maintaining alignment to dense model and preserving details in the prompt such as glasses, colorful apron, and paint-splattered hands, where baseline methods fail. Computational cost. To evaluate the efficiency of Skrr, we compared the computational cost of dense and pruned models. We measured the number of parameters, memory usage, and total FLOPs within the PixArt-Σ pipeline, with the model precision standardized to Bfloat16. As shown in Table 2, Skrr significantly reduces the parameters and memory usage similar to other baselines compared to dense model. While its FLOPs are slightly higher than the other baselines, this is negligible, since the text encoder accounts for only small portion (0.6%) of the pipelines total FLOPs. Table 2. Number of parameters (Param.), memory usage (Mem.), and FLOPs were evaluated on PixArt-Σ across pruning methods, considering the entire pipeline to analyze each strategys impact on computational cost. All metrics were measured at the maximum sparsity achieved by each pruning method. Method Sparsity (%) Param. (B) Mem. (GB) TFLOPs Dense ShortGPT FinerCut Skrr (Ours) 0.0 40.5 41.7 41.9 5.42 3.49 3.43 3.43 10. 6.59 6.48 6.46 91.94 91.79 91.74 91.90 7 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Table 3. T2I performance with various beam size k. While larger values incur higher computational costs for the candidate search, they enhance T2I performance. (k = 1 is greedy approach.) Sparsity CLIP DreamSim GenEval (%) 41.9 41.9 41.9 40.7 1 2 3 0.310 0.312 0.313 0.310 0.737 0.757 0.746 0.739 Single. Count. Colors. 0.900 0.912 0.912 0.925 0.372 0.450 0.450 0.328 0.739 0.731 0.755 0. dense model, ensuring better alignment with the prompt. Influence of beam search. We performed an ablation study that evaluated the effect of the beam search at different values of k, as shown in Table 3. As increases, performance initially improves and then decreases. This trend is consistent with previous findings (Cohen & Beck, 2019; He et al., 2023), which observed similar behavior in LLM decoding in which performance increases and then deteriorates as the size of the beam increases. Based on this observation, we selected the optimal beam size = 3. 5. Discussion Previously, we noted better FID results with T2I synthesis when using text encoder with skipped layers. Beyond CFG, other guidance methods in diffusion models include perturbed attention guidance (PAG) (Ahn et al., 2024) and autoguidance (Karras et al., 2024), both of which approximate the unconditional score by modifications of denoising networks. We propose that layer skipping or merging affects the null text embedding similar to these methods. To verify this hypothesis, we perturbed f, as described below. ˆf = λz + f, (0, I), (8) where is random vector sampled from normal distribution and λ is small scalar value. The FID and CLIP scores of the original model and the unconditional output with small perturbations applied to were measured on the MSCOCO 30k dataset. The results show that the FID decreases, indicating that the fidelity improved when perturbations are introduced to the unconditional feature f. Detailed results and configurations are provided in Appendix Sec. C.3. 6. Conclusion Figure 6. Ablation study on Re-use. Without Re-use, Skip alone leads to images that often misalign with the prompt, while Re-use ensures more faithful adherence to the prompt. 4.2. Qualitative results We present qualitative results that demonstrate the performance of T2I synthesis using Skrr-compressed text encoders, extending the experiments to SD3 and FLUX, stateof-the-art diffusion models. As shown in Fig. 5, ShortGPT achieves satisfactory image quality at low sparsity, but diverges significantly at higher sparsity levels, failing to align with input text at sparsity over 40% (level 3). LaCo and FinerCut maintain image fidelity across sparsity levels but show reduced alignment with the dense model. In contrast, Skrr consistently preserves both image quality and alignment, closely resembling the outputs of dense models. For SD3 and FLUX, which use multiple text encoders, image fidelity remains intact at higher sparsity levels, but similarity to dense encoder output decreases. These results highlight Skrrs robustness in preserving image quality and alignment to original image while exhibiting consistent behavior across models under varying sparsity conditions. 4.3. Ablation study We conducted an ablation study to evaluate the contribution of each component in the Skrr framework. Specifically, we analyzed the effectiveness of Re-use, highlighting its role in minimizing performance degradation from Skip. We also examined the size of the beam, demonstrating its effectiveness in block selection. The experiments were carried out on PixArt-Σ and subset of the MS-COCO (Lin et al., 2014) validation dataset with the highest sparsity. We provide additional ablation study in Appendix Sec. C.5. Influence of Re-use. The Re-use phase addresses performance degradation from the Skip phase, optimizing within memory constraints. Its impact was assessed by comparing model performance with and without Re-use. As shown in Fig. 6, Skip alone preserves high fidelity but may fail to accurately reflect the text prompt. In contrast, incorporating Re-use produces images closely resembling those from the In this paper, we introduce Skip and Re-use Layers (Skrr), an effective compression method for the text encoder in text-to-image (T2I) diffusion models. Skrr integrates three key components: (1) pruning metric based on the Skrr dot product to identify redundant sub-blocks, (2) beam searchbased algorithm to account for interactions between transformer blocks during pruning, and (3) re-use mechanism 8 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation that mitigates performance degradation by leveraging the remaining layers to recover lost capacity from skipped blocks with theoretical supports. Extensive experiments demonstrate that Skrr consistently outperforms existing blockwise pruning techniques for text encoder compression in image synthesis tasks, achieving qualitatively and quantitatively superior results. Additionally, our analysis reveals that pruning or merging layers not only reduces model complexity but can also enhance certain aspects of performance. We further analyze these improvements from the perspective of model guidance, offering insights into how structural adjustments contribute to more effective T2I generation."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) [NO.RS-2021-II211343, Artificial Intelligence Graduate School Program (Seoul National University)] and the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. NRF2022M3C1A309202211). Also, the authors acknowledged the financial support from the BK21 FOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National University."
        },
        {
            "title": "References",
            "content": "Ahn, D., Cho, H., Min, J., Jang, W., Kim, J., Kim, S., Park, H. H., Jin, K. H., and Kim, S. Self-rectifying diffusion sampling with perturbed-attention guidance. In ECCV, pp. 117. Springer, 2024. Ashkboos, S., Croci, M. L., Nascimento, M. G. d., Hoefler, T., and Hensman, J. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024a. Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024b. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pp. 1839218402, 2023. Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., and Zheng, Y. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In CVPR, pp. 2256022570, 2023. Castells, T., Song, H.-K., Piao, T., Choi, S., Kim, B.-K., Yim, H., Lee, C., Kim, J. G., and Kim, T.-H. Edgefusion: On-device text-to-image generation. arXiv preprint arXiv:2404.11925, 2024b. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 3558 3568, 2021. Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z., Luo, P., Lu, H., and Li, Z. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, pp. 7491. Springer, 2024. Cohen, E. and Beck, C. Empirical analysis of beam search performance degradation in neural sequence models. In ICML, pp. 12901299. PMLR, 2019. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML. PMLR, 2024. Fang, G., Ma, X., and Wang, X. Structural pruning for diffusion models. In NeurIPS, 2023. Freitag, M. and Al-Onaizan, Y. Beam search strategies for neural machine translation. ACL, pp. 56, 2017. Fu, S., Tamir, N., Sundaram, S., Chai, L., Zhang, R., Dekel, T., and Isola, P. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. Ganjdanesh, A., Shirkavand, R., Gao, S., and Huang, H. Not all prompts are made equal: Prompt-based pruning of text-to-image diffusion models. arXiv preprint arXiv:2406.12042, 2024. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. NeurIPS, 36, 2024. Gromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and Roberts, D. A. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. He, J., Sun, S., Jia, X., and Li, W. Empirical analysis of beam search curse and search errors with model errors in neural machine translation. In EAMT, pp. 91101, 2023. Castells, T., Song, H.-K., Kim, B.-K., and Choi, S. Ldpruner: Efficient pruning of latent diffusion models using task-agnostic insights. In CVPR, pp. 821830, 2024a. He, Y., Liu, L., Liu, J., Wu, W., Zhou, H., and Zhuang, B. Ptqd: Accurate post-training quantization for diffusion models. NeurIPS, 36, 2024. 9 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In ECCV, pp. 740755. Springer, 2014. Ho, J. and Salimans, T. Classifier-free diffusion guidance. In NeurIPSW, 2021. Hsieh, C.-Y., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In ACL, pp. 80038017, 2023. Huang, K., Guo, X., and Wang, M. Towards efficient pretrained language model via feature correlation distillation. NeurIPS, 36:1611416128, 2023. Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. Liu, Y., Zhang, K., Li, Y., Yan, Z., Gao, C., Chen, R., Yuan, Z., Huang, Y., Sun, H., Gao, J., et al. Sora: review on background, technology, limitations, and opportunities of large vision models. URL https://arxiv. org/abs/2402.17177, 2024. Men, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han, X., and Chen, W. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, pp. 41954205, 2023. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., and Irani, M. Imagic: Text-based real image editing with diffusion models. In CVPR, pp. 60076017, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Kim, B.-K., Song, H.-K., Castells, T., and Choi, S. Bk-sdm: lightweight, fast, and cheap version of stable diffusion. In ECCV, pp. 381399. Springer, 2024. Ko, J., Kim, S., Chen, T., and Yun, S.-Y. Distillm: Towards streamlined distillation for large language models. arXiv preprint arXiv:2402.03898, 2024. Lee, Y., Lee, Y.-J., and Hwang, S. J. Dit-pruner: Pruning diffusion transformer models for text-to-image synthesis In ECCVW, pp. 19, using human preference scores. 2024. Li, M., Lin, Y., Zhang, Z., Cai, T., Li, X., Guo, J., Xie, E., Meng, C., Zhu, J.-Y., and Han, S. Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024a. Li, X., Liu, Y., Lian, L., Yang, H., Dong, Z., Kang, D., Zhang, S., and Keutzer, K. Q-diffusion: Quantizing diffusion models. In CVPR, pp. 1753517545, 2023. Li, Y., Wang, H., Jin, Q., Hu, J., Chemerys, P., Fu, Y., Wang, Y., Tulyakov, S., and Ren, J. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. NeurIPS, 36, 2024b. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. MLSys, 6:87100, 2024. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763. PMLR, 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. Ryu, H., Park, N., and Shim, H. Dgq: Distribution-aware group quantization for text-to-image diffusion models. arXiv preprint arXiv:2501.04304, 2025. Seo, H., Kim, H., Kim, G., and Chun, S. Y. Ditto-nerf: Diffusion-based iterative text to omni-directional 3d model. arXiv preprint arXiv:2304.02827, 2023. Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Sherstinsky, A. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Physica D: Nonlinear Phenomena, 404:132306, 2020. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, 2023. Song, J., Oh, K., Kim, T., Kim, H., Kim, Y., and Kim, J.-J. Sleb: Streamlining llms through redundancy verification and elimination of transformer blocks. In ICML, 2024a. Song, Y., Lorraine, J., Nie, W., Kreis, K., and Lucas, J. Multi-student diffusion distillation for better one-step generators. arXiv preprint arXiv:2410.23274, 2024b. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In CVPR, pp. 28182826, 2016. van der Ouderaa, T. F., Nagel, M., Van Baalen, M., and Blankevoort, T. The llm surgeon. In ICLR, 2023. Wang, H., Shang, Y., Yuan, Z., Wu, J., Yan, J., and Yan, Y. Quest: Low-bit diffusion model quantization via efficient selective finetuning. arXiv preprint arXiv:2402.03666, 2024a. Wang, Z., Jiang, Y., Zheng, H., Wang, P., He, P., Wang, Z., Chen, W., Zhou, M., et al. Patch diffusion: Faster and more data-efficient training of diffusion models. NeurIPS, 36, 2024b. Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. NeurIPS, 36, 2024c. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In ICML, pp. 3808738099. PMLR, 2023. Yang, Y., Cao, Z., and Zhao, H. Laco: Large language arXiv preprint model pruning via layer collapse. arXiv:2402.11187, 2024. Zhang, Y., Li, Y., Wang, X., Shen, Q., Plank, B., Bischl, B., Rezaei, M., and Kawaguchi, K. Finercut: Finer-grained interpretable layer pruning for large language models. arXiv preprint arXiv:2405.18218, 2024. Zhao, Y., Xu, Y., Xiao, Z., Jia, H., and Hou, T. Mobilediffusion: Instant text-to-image generation on mobile devices. In ECCV, pp. 225242. Springer, 2024. Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation A. Proofs A.1. Proof of Lemma 3.1 Lemma 3.1 (Error bound of two transformers). Let : (x, θ) (cid:55) Rd be an L-block transformer with input Rd and parameters θ = (θ1, . . . , θL), defined as: = (cid:0)(FL + I) (FL1 + I) (F1 + I)(cid:1), where Fi : (zi, θi) (cid:55) zi+1 is the i-th block with parameters θi, and zi+1 Rd. With each block Fi being Li-Lipschitz in the input that satisfies follows.: Fi(z; θi) Fi(z; θi) Liz And Mi-Lipschitz in the parameters which satisfies follows.: Fi(z; θi) Fi(z; θ i) Miθi θ Then, for any two parameter sets θ = (θ1, . . . , θL) and ˆθ = (ˆθ1, . . . , ˆθL), the following holds: (cid:35) Miθi ˆθi M(x; θ) M(x; ˆθ) (cid:34)(cid:32) (cid:89) (1 + Lk) (cid:88) (cid:33) i= k=i+1 (A1) (A2) (A3) (A4) Proof. With Eq. A1, we can formulate the difference of two hidden states between dense model and modified model as follows.: zi+1 ˆzi+1 = [zi + (zi; θi)] [ˆzi + (ˆzi; ˆθi)] By the triangle inequality: zi+1 ˆzi+1 zi ˆzi + (zi; θi) (ˆzi; ˆθi) (cid:125) (cid:124) (cid:123)(cid:122) (A) We now split term (A) with the assumptions in Eq. A2 and Eq. A3: (A) = (zi; θi) (ˆzi; ˆθi) Fi(zi; θi) Fi(ˆz; θi) + Fi(ˆzi; θi) Fi(ˆzi; ˆθi) Lizi ˆzi (cid:123)(cid:122) (cid:125) (cid:124) input Lipschitz + Miθi ˆθi (cid:124) (cid:125) (cid:123)(cid:122) parameter Lipschitz zi+1 ˆzi+1 zi ˆzi + Lizi ˆzi + Miθi ˆθi zi+1 ˆzi+1 (1 + Li)zi ˆzi + Miθi ˆθi Ei = zi ˆzi Ei+1 (1 + Li)Ei + Miθi ˆθi So combining, Hence, Define the error at block as Eq. A11 becomes We start from E1 = z1 ˆz1 = x = 0 (since both netowrks see the same input x). Thus: E2 (1 + L1)E1 + M1θ1 ˆθ1 = M1θ1 ˆθ1 E3 (1 + L2)E2 + M2θ1 ˆθ1 (1 + L2)[M1θ1 ˆθ1 + M2θ2 ˆθ2] If we do recursive telescoping over all blocks, we get EL+1 = zL+1 ˆzL+1 (cid:88) (cid:34)(cid:32) (cid:89) (cid:33) (cid:35) Miθi ˆθi (1 + Lk) i=1 k=i+1 12 (A5) (A6) (A7) (A8) (A9) (A10) (A11) (A12) (A13) (A14) (A15) (A16) Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Since M(x; θ) = zL+1 and M(x; ˆθ) = ˆzL+1, we have shown: M(x; θ) M(x; ˆθ) = zL+1 ˆzL+1 (cid:88) (cid:34)(cid:32) (cid:89) i=1 k=i+ (cid:33) (cid:35) Miθi ˆθi (1 + Lk) (A17) A.2. Proof of Theorem 3.2 Theorem 3.2 (Tighter error bound of Re-use). Under the same assumptions in Lemma 3.1, let θi denote the i-th block of transformer that is skipped, θ represent the corresponding Re-used block, USkip is error bound for compressed model with Skip alone, and USkip, Re-use is error bound for compressed model with skip and reuse. If following condition is satisfied, then, the following inequality holds: θi θ < θi USkip, Re-use < USkip Proof. With Lemma 3.1, we can formulate the error bound of compressed model as follows: M(x; θ) M(x; ˆθ) (cid:88) i=1 Ciθi ˆθi (A18) (A19) (A20) where Ci is constant for each i-th block. For the unskipped block, θi = ˆθi, θi ˆθi = 0. So, we define parameter set ˆθSkip that exclude θi in the parameter set θ. And we can rewrite the Lemma 3.1 as follows: M(x; θ) M(x; ˆθSkip) Ciθi ˆθi (cid:88) iS where is set of skipped (pruned) block indices. And skipped block can be represented as follows: Then, we can manipulate the error bound Eq. A21 with following. ˆθi,Skip = 0 M(x; θ) M(x; ˆθSkip) (cid:88) iS Ciθi = USkip Re-use substitute the skipped weight to the weight of adjacent block: ˆθi,Re-use = θ If we make set of that satisfies Eq. A18 and denote as then apply Re-use, (A21) (A22) (A23) (A24) M(x; θ) M(x; ˆθSkip, Re-use) < (cid:88) Ciθi + iSi /R (cid:88) jR Cjθj θ = USkip, Re-use (A25) Since the consists of block indices that satisfies Eq. A18, we have shown: USkip, Re-use = (cid:88) Ciθi + iSi /R (cid:88) jR Cjθj θ < (cid:88) kS Ckθk = USkip (A26) Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation B. Detailed Experimental Setup B.1. Baseline configurations Table A1. Block indices ordered by the Block Influence (BI) score obtained from ShortGPT with our calibration dataset. Blocks with lower BI scores are ranked earlier, while those with higher BI scores are ranked later. During block pruning, blocks with the lowest BI scores are pruned first, according to the specified pruning ratio. 10, 11, 8, 9, 12, 13, 5, 14, 6, 4, 3, 7, 15, 17, 16, 18, 2, 19, 20, 21, 22, 1, 23,"
        },
        {
            "title": "Order of block index",
            "content": "ShortGPT (Men et al., 2024) ShortGPT calculates the Block Influence (BI) score by measuring cosine similarity between intermediate features of each layer, extracted from the calibration data, and then taking its complement. We calculated the BI scores using the 1k calibration subset of the CC12M dataset that we constructed and sorted the blocks in ascending order based on their BI scores. The ordered block indices are presented in Table A1. LaCo (Yang et al., 2024) LaCo presents an algorithm that merges adjacent transformer layers on the basis of the cosine similarity between their output features and those of the original model. If similarity exceeds predefined threshold, the layers are merged to reduce the size of the model. However, our experiments revealed that LaCos performance is highly sensitive to its hyperparameter settings. In some cases, the algorithm failed to effectively reduce the number of parameters. To ensure fair comparison, we conducted thorough and extensive experiments to identify the optimal hyperparameters that maximize LaCos performance in the Text-to-Image (T2I) diffusion model. The selected hyperparameters are as follows: Layer collapse interval = 2, Number of layers to merge = 2, First layer to merge = 1, Last layer to merge = 24, cosine similarity threshold = 0.7. We applied LaCo by continuously merging layers until the compressed model achieved the desired target sparsity. Table A2. Sub-block indices ordered with MSE metrics from FinerCut with our calibration dataset. For the efficiency, we extracted top-24 sub-block indices for pruning. During the pruning process, blocks with the lowest MSE are pruned first, according to the specified pruning ratio. 46, 5, 11, 6, 47, 44, 20, 19, 18, 45, 40, 41, 1, 32, 31, 17, 24, 23, 3, 42, 43, 30, 38, Order of sub-block index FinerCut (Zhang et al., 2024) FinerCut employs finer-grained blockwise pruning strategy based on the structural decomposition of transformer blocks into two distinct sub-blocks: Multi-Head Attention (MHA) and Feed-Forward Network (FFN) sub-blocks. To assess the importance of each sub-block, FinerCut originally proposed various evaluation metrics: 1) cosine similarity, 2) mean-squared error (MSE), and 3) Jensen-Shannon divergence (JSD). In its original implementation for auto-regressive LLMs, FinerCut adopted JSD after evaluating the effectiveness of these metrics. However, since the text encoder in the T2I diffusion model lacks language modeling head, metrics based on perplexity and JSD could not be applied. For the fair comparison, we implemented FinerCut using MSE as the sole metric to evaluate sub-block importance, which is the closest metric with our discrepancy metric. We also conducted experiments of FinerCut with cosine similarity in Sec. C.6. The sorted sub-block indices determined by FinerCut are presented in Table A2. The even indices represents the MHA blocks and the odd indices denote the FFN blocks. Due to FinerCuts sub-block-level pruning granularity, it was not possible to achieve the exact sparsity ratios as ShortGPT and LaCo. Therefore, pruning was carried out up to the sub-block index that most closely matched the target sparsity for fair comparison. Skrr (Ours) This section details the order of indices determined during the Skip phase of Skrr. Because the projection module is incorporated into the denoising module of each diffusion model, the sub-block indices vary across different diffusion models. We executed the Skip phase for all models, and the resulting order of indices for each model is presented in the Table A3. This provides insight into how the indices are prioritized during the Skip phase for various diffusion architectures. 14 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Additionally, the re-use indices of Skrr in PixArt-Σ are presented in Table A4, Table A5, and Table A6. The re-use indices were independently computed using the skip indices for each sparsity level, and overall, the later blocks exhibited tendency not to be reused. Table A3. Sub-block indices ordered with discrepancy metric from Skrr with our calibration dataset in PixArt-Σ, Stable Diffusion 3 (SD3), and FLUX.1-dev. For the efficiency, we extracted top-24 sub-block indices for pruning. During the pruning process, blocks with the lowest discrepancy are pruned first, according to the specified pruning ratio."
        },
        {
            "title": "Model",
            "content": "Order of sub-blocks PixArt-Σ SD3 FLUX.1-dev 46, 5, 6, 47, 44, 2, 11, 24, 23, 17, 18, 45, 10, 30, 29, 40, 39, 38, 37, 42, 25, 22, 43, 36 46, 5, 45, 6, 11, 2, 22, 23, 17, 18, 30, 29, 43, 44, 20, 1, 41, 42, 40, 39, 38, 37, 21, 16 46, 45, 5, 6, 11, 20, 19, 22, 23, 30, 29, 47, 2, 44, 32, 31, 18, 17, 37, 36, 43, 40, 39, 42 Skipped Block Re-used Block Skipped Block Re-used Block Skipped Block Re-used Block 2 5 6 11 17 18 23 24 44 45 46 47 4 7 - 13 19 16 21 - - - - - Table A4. Skrr Re-use indices of sparsity level 1 in PixArt-Σ. 2 5 6 10 11 17 18 23 24 29 30 39 40 44 45 46 47 4 7 4 - 13 19 20 21 - - - - - - - - - Table A5. Skrr Re-use indices of sparsity level 2 in PixArt-Σ. 2 5 6 10 11 17 18 23 24 25 29 30 37 38 39 40 42 44 45 46 47 0 7 4 12 9 19 16 21 - 27 - - - - - - - - - - - Table A6. Skrr Re-use indices of sparsity level 3 in PixArt-Σ. B.2. Diffusion model and text encoder For the quantitative comparison, we performed experiments on two diffusion transformer (Peebles & Xie, 2023) (DiT) text encoders: PixArt-Σ (Chen et al., 2024). PixArt-Σ employs the T5-XXL model (Raffel et al., 2020) as its text encoder. We have also conducted experiments on compressing text encoders that leverages several text encoders. For example, Stable Diffusion 3 (SD3) leverages CLIP-L, CLIP-G, and T5-XXL. The results of compressing multiple text encoders are presented in the Sec. C.5. For quantitative and qualitative evaluations, we measured discrepancy and generated images using Bfloat16 precision with the pretrained weights PixArt-alpha/PixArt-Sigma-XL-2-1024-MS obtained from the Hugging Face Diffusers library. For PixArt-Σ, all images were generated in the resolution of 512512. Furthermore, the FLUX model and SD3, included in the qualitative results, was evaluated using the stabilityai/stable-diffusion-3-medium and black-forest-labs/FLUX.1-dev weights from the Hugging Face Diffusers library. The text encoder configuration and inference precision for both models were consistent with the aforementioned setup, using the Bfloat16 precision and applying the same pruning strategy to ensure fair and consistent evaluation. Additionally, we fixed the number of function evaluations (NFE) to 20 across all diffusion models. All other hyperparameters, such as the classifier-free guidance (Ho & Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Salimans, 2021) scale, were set to the default value. All experiments were performed on single Nvidia A100 or RTX 4090 GPU. B.3. Calibration dataset We constructed calibration set derived from the CC12M (Changpinyo et al., 2021) dataset to identify blocks for pruning. While existing LLM-based methods typically utilize calibration sets such as the Common Crawls web corpus (Raffel et al., 2020) (C4), SlimPajama (Soboleva et al., 2023) and WikiText (Merity et al., 2016), primarily focusing on perplexity-based performance metrics, these approaches are not directly applicable to T2I models. To address this gap, we curated calibration set specifically tailored for the T2I text encoder by selecting only clean captions and semantically rich prompts ranging from 150 to 250 tokens from the CC12M image-text paired dataset. This selection ensures more effective calibration for image synthesis tasks. Representative examples of prompts from the constructed dataset are provided in Table A7. B.4. Metrics Frechet Inception Distance (FID) (Heusel et al., 2017) score The Frechet Inception Distance (FID) is widely used metric for evaluating the performance of image generative models by quantifying the similarity between the distributions of real and generated images. Specifically, FID measures the Frechet distance between feature representations extracted from pre-trained image classification model, typically the Inception-V3 model. This approach leverages the models rich intermediate features to capture high-level image statistics. The FID score is formally defined as follows: dF (N (µ, Σ), (µ, Σ)) = µ µ2 2 + tr (cid:16) Σ + Σ + 2(ΣΣ) (cid:17) 1 (A27) where µ and Σ are the mean and covariance of the feature representations of real images, µ and Σ are the mean and covariance of the feature representations of generated images. lower FID score indicates that the generated images are more similar to the real images in both quality and diversity. CLIP (Radford et al., 2021) score The CLIP score evaluates the semantic alignment between given text prompt and the image generated from that prompt by measuring the cosine similarity between their CLIP embeddings. This metric leverages CLIP model trained on extensive image-text pairs to capture cross-modal relationships. The CLIP score can be formally defined as: CLIP(I, ) = cos (Eimage(I), Etext(T )) = (A28) Eimage(I) Etext(T ) Eimage(I)Etext(T ) where is the generated image, is the text prompt, Eimage() represents the CLIP image encoder, and Etext() denotes the CLIP text encoder. The cosine similarity captures how well the generated image aligns with the semantic content of the text prompt. For our experiments, we leveraged the weights of the openai/clip-vit-base-patch32 model from the Hugging Face library to calculate the CLIP score. DreamSim (Fu et al., 2023) score The DreamSim score quantifies the semantic similarity at the mid-level between two images by evaluating their compositional, stylistic, and color characteristics. It quantifies how closely the overall structure and visual attributes of the images align. Formally, the DreamSim score can be expressed as Eq. A29: DreamSim(Iref, Ipru) = 1 distDreamSim(Iref, Ipru), distDreamSim(, ) [0, 1] (A29) where Iref represents the image generated using the original text encoder, and Ipru denotes the image generated using the pruned text encoder. The function distDreamSim(, ) computes the normalized semantic distance between the two images, as output by the DreamSim model. By subtracting this distance from 1, the DreamSim score reflects higher similarity with higher values, effectively capturing the semantic consistency between the original and pruned models outputs. GenEval (Ghosh et al., 2024) GenEval is comprehensive evaluation metric designed to assess the degree to which T2I generative model aligns generated images with input text prompts. In this study, GenEval was employed to evaluate whether the image synthesis results produced by the compressed text encoder accurately reflect the intended textual descriptions. The GenEval metric comprises six sub-metrics: 1. Single Object Generation Assesses the models ability to generate images from prompts containing single object (e.g., 16 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Table A7. Examples of prompts from the calibration set across various lengths are presented. These text prompts are sampled from the CC12M dataset, featuring rich and descriptive expressions with lengths optimized for calibration. This selection ensures the prompts are semantically meaningful and well-suited for effective text-to-image model calibration."
        },
        {
            "title": "Example prompts in calibration dataset",
            "content": "1. collection of photography equipment neatly arranged on wooden surface. Items include camera, smartphone, tablet, drone, portable power bank, tripod, cleaning kit, strap, case, and backpack. The warm wooden background contrasts with the modern gear. 2. moment at subway station with vintage train numbered 7 at the platform. The platform has safety barriers and yellow line, illuminated by fluorescent lights. Text reads last stop for the 7 train and credits the photographer. 3. modern living room with minimalist design. pendant light provides warm glow. wooden table holds glass of water, book, smartphone, and notebook. white cabinet and cityscape view complete the cozy atmosphere. 4. person wearing white t-shirt with the text Day to Remember in pink and black lettering. The shirt features black collar and short sleeves, displayed plainly for product showcasing. 5. vibrant digital artwork of stylized cityscape. Buildings vary in color and pattern, resembling patchwork quilt, creating dense, lively urban environment. 6. small modern bathroom with brick-patterned walls and tiled flooring. white sink under window, glass shower enclosure, and toilet create rustic yet clean look. 7. vintage light-colored train car with blue and white stripes is parked on track under metal canopy. Metal stairs lead to the entrance, possibly part of museum exhibit. 8. wall with playful quote: In this house we are real, we make mistakes, we say Im sorry, we give hugs, we give second chances, we forgive, we laugh lot, we love each other, we are family. guitar leaning against the wall adds cozy, homey touch. 9. vibrant bouquet of flowers arranged in clear glass vase. the bouquet consists of various types of flowers, including hydrangea, calla lilies, roses, and gerbera daisies, with burgundy berries interspersed among them. the flowers are in shades of pink and purple, creating striking contrast. the arrangement is set against plain, light-colored background, which accentuates the colors and textures of the flowers. the style of the image is close-up photograph that captures the details of the floral arrangement. 10. The memorial church at stanford university, large, ornate building with prominent cross at the top, illuminated at night. the facade is adorned with intricate mosaics and sculptures, including central figure that appears to be religious figure, possibly saint or deity. the churchs architecture is reminiscent of gothic and romanesque styles, with pointed arches and large central archway that leads to the entrance. the surrounding area is dimly lit, with the church standing out as beacon of light in the darkness. 11. serene lakeside setting with houseboat that resembles private yacht. the boat is equipped with dining area featuring table set for four with blue tableware, and bar area with blender, wine glasses, and bottle of wine. the deck is furnished with multiple lounge chairs and dining table, all under retractable awning. the houseboat is docked near rocky shoreline with clear blue sky and majestic red rock formation in the distance, suggesting location like lake powell. the overall atmosphere is one of relaxation and leisure, ideal for vacation or getaway. 12. graphic design with stylized representation of face, possibly deity, with serene expression. the face is framed by green border with white outline and blue background. above the face, there is crescent moon and symbol that resembles peace sign. below the face, the word chill is prominently displayed in bold, white capital letters. the overall style of the image is modern and graphic, with clear emphasis on the word chill suggesting theme of relaxation or tranquility. photo of giraffe). 2. Two Objects Generation Evaluates the models ability to correctly generate images from prompts with two distinct objects (e.g., photo of knife and stop sign). 3. Counting Measures whether the model can accurately represent the specified number of objects (e.g. photo of three apples). 4. Colors - Verifies whether the generated image correctly reflects the color specified in the prompt (e.g., photo of pink car). 5. Position Tests the models understanding of spatial relationships described in the prompt (e.g., photo of sofa under 17 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Table A8. Sparsity ratio of the text encoder, parameter count (Param.), memory usage (Mem.), FLOPs ratio of text encoder (T5-XXL) with respect to the total pipline, and total TFLOPs of the pipline were evaluated on SD3 across pruning methods, considering the entire pipeline to analyze each strategys impact on computational cost. All metrics were measured at the maximum sparsity achieved by each pruning method and NFE with 28."
        },
        {
            "title": "Method",
            "content": "Sparsity (%) Param. (B) Mem. (GB) FLOPs (%) TFLOPs"
        },
        {
            "title": "Dense",
            "content": "ShortGPT LaCo FinerCut Skrr (Ours) 0.0 40.5 40.5 41.7 41.9 7.66 5.73 5.73 5.67 5.73 14. 10.93 10.82 10.82 10.93 0.41 0.32 0.32 0.30 0.35 174.2 174.1 174.1 174.0 174.1 cup). 6. Color Attribution Assesses the correct assignment of specified colors to multiple objects (e.g., photo of black car and green parking meter). For evaluation, we generated images using fixed random seed, producing 553 distinct prompts with four images per prompt, resulting in total of 2,212 images. This setup ensured consistent and reproducible evaluation across all sub-metrics. B.5. Ablation study setup Due to constraints in our experimental environment, it was not feasible to evaluate the entire MS-COCO 30k image dataset for the ablation study. Instead, we performed the study using 1k subset. Given that both the CLIP score and the DreamSim score effectively capture the impact of text encoder pruning, we focus our experiments on evaluating various configurations of these two metrics within the ablation study. Furthermore, for GenEval, experiments were conducted on the entire set. The experiments discussed in the Sec. C.5 were conducted using the same experimental setup as described above. C. Additional Experiments C.1. Interaction between blocks In addition to the block pairs shown in Fig. 4, we identified additional pairs that exhibit interactions. These block pairs consistently maintain cosine similarity; however, their norms change significantly. We provide several qualitative results that highlight these interactions in Fig. A1 which is experimented with PixArt-Σ. For cases where > 2, the number of k!(48k)! , making exhaustive experimentation computationally infeasible. As result, we limit combinations increases to our analysis to subset of two-block interactions. 48! C.2. Computational cost on different diffusion models In the main paper, we report the computational cost only for PixArt-Σ. In this section, we extend the analysis to include the computational cost of dense, baselines, and Skrr-compressed models on Stable Diffusion 3 (SD3). Since SD3 contains more parameters and more computationally intensive denoising module compared to PixArt-Σ, the text encoder consumes fewer FLOPs ratio in the total pipeline. However, the text encoder still accounts for significant memory usage. The detailed results are provided in Table A8. At the highest sparsity level, FinerCut achieves the lowest parameter count, memory usage, and FLOPs due to its slightly higher sparsity. Skrr exhibits the same parameter count and memory consumption as ShortGPT or LaCo at the same sparsity level (40.7%). While Skrr incurs slightly higher FLOP count than other baselines due to the re-use mechanism, the additional computational cost is minimal, accounting for less than 0.05% of the entire pipeline or under 0.1 TFLOPS. Despite this, Skrr still demands less computation than the dense model. These results underscore ability of Skrr to perform T2I synthesis in computationally efficient manner, even when applied to models with varying complexities. C.3. Perturbation to the null condition feature We conducted an experiment to evaluate the impact of perturbations on and their effect on the FID and CLIP score. Using the PixArt-Σ model, we applied small scalar parameter λ = 102 and generated images for the MS-COCO 18 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A1. Examples illustrate various block interactions. When individual blocks are skipped, the generated images remain highly similar to those produced by the dense model, showing minimal impact. However, when two blocks are skipped simultaneously, the image is severely degraded, demonstrating the presence of significant interactions between blocks. 30k validation set. The results demonstrated that the perturbed produced lower FID score (22.89 20.65) and comparable CLIP score (0.314 0.314) to the original model. Furthermore, we provide qualitative comparison between the images generated with the original null condition and those created using the perturbed feature vector ˆf in Fig. A2. C.4. Applying Re-use to baselines To further validate the effectiveness of Re-use, we conducted experiments to evaluate its compatibility with other block-wise pruning methods in plug-and-play manner. Among the baselines considered for these experiments, ShortGPT and FinerCut employed block-pruning techniques, allowing us to apply Re-use and carry out the evaluations. ShortGPT prunes layer normalization, MHA, and FFN as single unit, enabling us to perform Re-use on adjacent whole blocks. Similarly, FinerCut, which operates at the sub-block level like Skrr, also allowed the application of Re-use. The results of these experiments are presented in Fig. A3 and Fig. A4. The blocks re-used by shortGPT and Finercut are presented in Table A9 and Table A10, respectively. As shown in the results, Re-use demonstrated superior performance, further substantiating its effectiveness in alignment with both the experimental and theoretical findings presented earlier. C.5. Additional ablation study Effectiveness of Re-use. In this section, we provide additional experimental results to demonstrate the effectiveness of Re-use. This mechanism effectively mitigates the performance degradation caused by Skip by utilizing the remaining blocks in the model. To evaluate its impact, we performed extensive quantitative and qualitative experiments. Quantitatively, we 19 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A2. Images generated from the same seed under the different null condition (original vs. perturbed) reveal notable differences. While the original model occasionally exhibits poor fidelity or omits objects specified in the prompt, the images generated using the perturbed null condition feature demonstrate higher fidelity, more accurate representation of the given prompt, and improved conditional image synthesis performance. All image pairs were generated with the same seed. 20 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A3. Qualitative results of applying Re-use to ShortGPT. At high sparsity levels, ShortGPT struggles to generate high-fidelity images. Incorporating Re-use restores fidelity, text adherence, and alignment with the dense model. 21 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A4. Qualitative results of applying Re-use to FinerCut. While FinerCut performs well compares other baselines in maintaining fidelity and text alignment, its alignment with the dense model decreases. Incorporating Re-use effectively restores this alignment. 22 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Skipped Block Re-used Block Skipped Block Re-used Block 4 5 6 8 9 10 11 12 13 14 7 7 7 7 7 7 7 7 7 7 Table A9. ShortGPT re-use indices 1 3 5 6 11 17 18 19 20 23 24 31 32 40 41 42 44 45 46 47 7 - - 4 9 21 16 15 16 21 - 29 - - - - - - - - Table A10. Finercut re-use indices measured the CLIP, DreamSim, and GenEval scores of the PixArt-Σ model with and without Re-use in the maximum sparsity. The results, presented in Table A11, show that Re-use enhances the CLIP, Dreamsim and GenEval scores. Although minor performance degradation was observed in other GenEval scores, the high performance achieved in challenging metrics like counting underscores the effectiveness of Re-use, especially in scenarios where the text encoders capability plays critical role. In addition to the quantitative findings and qualitative examples presented in the main paper, we further confirmed that Re-use outperforms Skip alone in variety of settings. These results are illustrated in Fig. A5 for PixArt-Σ and Fig. A6 for FLUX.1-dev. These visualizations reinforce the effectiveness of Re-use in generating images that align closely with text prompts, while maintaining model-agnostic behavior. The results demonstrate that Re-use not only enhances adherence to textual descriptions but also improves the performance of dense models across diverse scenarios. Table A11. Quantitative ablation study for Re-use with PixArt-Σ. T2I synthesis performance with Re-use demonstrates that it effectively restores performance degraded by pruning, achieving excellent recovery without incurring additional memory overhead. Re-use CLIP DreamSim GenEval Single Two. Count. Colors Pos. Color attr. Overall 0.311 0. 0.745 0.746 0.928 0.913 0.379 0.409 0.409 0.450 0.758 0.755 0.065 0. 0.088 0.068 0.438 0.442 Compressing multiple text encoders. Modern text-to-image (T2I) diffusion generative models frequently employ multiple text encoders to enhance their performance. notable example is Stable Diffusion 3 (SD3), which incorporates CLIP-L, CLIP-G, and T5-XXL text encoders. In the experiments in the main article, we focused on compressing the T5-XXL text encoder, which is the largest and has the highest parameter count. Extending this approach, we evaluated the performance of the model when compressing all text encoders simultaneously. Specifically, for SD3, we applied compression to CLIP-L (30.6% sparsity), CLIP-G (30.3% sparsity) and T5-XXL (41.9% sparsity). The quantitative results, summarized in Table A12, reveal that although the CLIP and GenEval scores experience slight reductions, they remain sufficiently comparable to those of the dense model. Furthermore, qualitative results, illustrated 23 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A5. Qualitative results on Re-use. Images generated with PixArt-Σ and text encoder (T5-XXL) compressed by the full Skrr framework (dense, Skip only, and Skip with Re-use) are compared. With Skip only, performance noticeably degrades, particularly for detailed prompts or tasks requiring strong text encoder capabilities, such as counting, resulting in deviations from dense model outputs. 24 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A6. Qualitative results on Re-use. Images generated with FLUX.1-dev and text encoder (T5-XXL) compressed by the full Skrr framework (dense, Skip only, and Skip with Re-use) are compared. When only Skip is used, some prompts may not be accurately reflected, leading to images that deviate from those generated by the dense model or even shift to an animated style rather than realistic one. 25 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation in Fig. A7, demonstrate that images generated by the compressed model using Skrr on multiple text encoders exhibit remarkable similarity to those generated by the dense model. These findings indicate that compressing multiple text encoders simultaneously does not significantly compromise the models ability to generate high-quality, text-aligned images, thus validating the robustness of the compression approach. Table A12. Quantitative ablation study for compressing multiple text encoders with Stable Diffusion 3. The second row presents the evaluation results of the model where only the T5 text encoder is compressed, while the third row corresponds to the model in which all three text encodersT5, CLIP-L, and CLIP-Gare compressed. Compressed CLIP DreamSim GenEval Single Two. Count. Colors Pos. Color attr. Overall Dense T5 All 0.318 0.317 0.313 1.0 0.811 0.717 0.994 0.959 0. 0.869 0.773 0.654 0.600 0.500 0.534 0.856 0.803 0.835 0.280 0.210 0.105 0.538 0.423 0.353 0.689 0.611 0. Skrr without projection module. To measure the discrepancy in Skrr, we tailored the evaluation for the T2I diffusion model by analyzing features extracted from the text encoder after projection through the projection module used in the denoising process. In this section, we present both quantitative and qualitative results to validate the effectiveness of incorporating the projection module. We evaluated the performance of the PixArt-Σ model compressed with Skrr, excluding the projection module. The results, summarized in Table A13, reveal that, while performance is largely preserved without the projection module, slight degradation occurs due to loss of information. This degradation occurs because the omission of the projection module ignores crucial components that are influential in the image generation process. Table A13. Quantitative results of the ablation study on the projection module. Applying the projection improves CLIP score, DreamSim, and various GenEval tasks, demonstrating its significant impact on T2I generation performance when extracting features for discrepancy measurement. Projection CLIP DreamSim GenEval Single Two. Count. Colors Pos. Color attr. Overall 0.305 0.312 0.708 0.746 0.884 0. 0.366 0.409 0.266 0.450 0.620 0.755 0.070 0.055 0.078 0.068 0.381 0. This trend is also reflected in the qualitative results shown in Fig. A8. The images generated with the projection module exhibit higher degree of similarity to those produced by the dense model compared to the images generated without it. These findings underscore the importance of the projection module in preserving key features necessary for accurate text-to-image generation, thereby proving its effectiveness in maintaining model performance and fidelity. Qualitative results of beam size In the main manuscript, we quantitatively analyzed the performance variation with respect to the beam size k. Here, we provide qualitative results to further illustrate its impact on image generation. As shown in Fig. A9, increasing the beam size leads to better alignment between the generated images and the dense model, demonstrating the effectiveness of proper beam sizes in improving generation quality, but slight decline in performance as the beam size grows. C.6. Additional quantitative results FinerCut employs cosine similarity and Jensen-Shannon Divergence (JSD) in addition to the MSE used in our implementation. However, since the text encoder lacks language head, JSD is not applicable. Moreover, FinerCuts reported results indicate that MSE outperforms cosine similarity, leading us to adopt it in our implementation. To ensure fair comparison, we reimplemented FinerCut using cosine similarity and evaluated its performance using FID, CLIP, DreamSim, and GenEval, as presented in Table A14. Consistent with FinerCuts findings, cosine similarity yielded better performance compared to MSE. Consequently, we used the MSE-based FinerCut implementation as baseline for an equitable comparison that shows higher performance. 26 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A7. Qualitative results of Skrr-compressed text encoders in Stable Diffusion 3. We compare compressing only the Dense models output and T5-XXL (T5) versus compressing all encoders (T5, CLIP-L, and CLIP-G). Skrr maintains high image fidelity and text-image alignment across both cases. 27 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A8. Qualitative comparison of using projection layer versus no projection in PixArt-Σ. Without projection (No Proj.), text-image alignment remains similar, but the generated image deviates from the dense models output. With projection (Proj.), both text-image alignment and image fidelity closely match the dense model. 28 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A9. Qualitative analysis of the impact of beam size on image generation quality. As shown, increasing the beam size enhances the alignment between generated images and the dense model, resulting in improved visual coherence and fidelity. Larger beam sizes allow for more exhaustive exploration of the search space, leading to more refined and higher-quality outputs. These observations further support our quantitative findings presented in the main paper, demonstrating the effectiveness of using larger beam sizes. 29 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Table A14. Quantitative comparison of FinerCut similarity metrics, including cosine similarity and MSE, evaluated using FID, CLIP, DreamSim, and GenEval. The results confirm that MSE outperforms cosine similarity, which is consistent with the finding of ours.( / denotes that higher / lower metric is favorable). Method Dense FinerCut (cos.) FinerCut (MSE) Skrr (Ours) Sparsity FID CLIP DreamSim GenEval (%) 0.0 26.3 32.2 41.7 26.3 32.2 41.7 27.0 32.4 41.9 22. 19.64 21.16 21.84 20.15 20.19 19.93 20.15 20.19 19.93 0.314 0.311 0.306 0.306 0.315 0.313 0. 0.315 0.313 0.312 1.0 0.745 0.689 0.671 0.800 0.775 0.741 0.800 0.775 0.741 Single Two Count. Colors Pos. Color attr. Overall 0.988 0. 0.475 0.795 0.108 0.925 0.875 0.847 0.947 0.903 0.841 0.956 0.928 0. 0.394 0.346 0.275 0.465 0.409 0.306 0.434 0.397 0.410 0.363 0.322 0.256 0.394 0.344 0.306 0.425 0.413 0. 0.657 0.620 0.625 0.737 0.697 0.628 0.763 0.774 0.755 0.060 0.043 0.053 0.103 0.078 0.050 0.095 0.100 0. 0.255 0.085 0.058 0.033 0.105 0.128 0.073 0.145 0.118 0.068 0.539 0.414 0.377 0. 0.458 0.426 0.367 0.471 0.455 0.442 C.7. Additional qualitative results In this section, we present additional qualitative results to further illustrate the performance of our approach. We define the model compressed with 20%-30% sparsity as sparsity level 1, 30%-40% as sparsity level 2, and 40%-50% as sparsity level 3. The qualitative comparison for sparsity level 1 is shown in Fig. A10, while comparisons for sparsity level 2 are presented in Fig. A11 and Fig. A12. Finally, Fig. A13 and Fig. A14 illustrate the comparisons for sparsity level 3. The results demonstrate that Skrr generates outputs that are more closely aligned with the prompts and more consistent with the outputs of the dense model compared to other baselines. All presented results were generated using PixArt-Σ and are part of dataset comprising 30,000 images, which were produced for FID measurements. Additionally, we provide qualitative comparison of the text encoders compressed by each compression method across all sparsity levels, juxtaposed with the images generated by the dense model. By comparing images generated with the same seed at sparsity levels 1, 2, and 3, as defined above, we can assess the degree of deviation from the original image as the sparsity increases. These comparisons are illustrated in Fig. A15 and Fig. A16. The results reveal that, while the baseline methods occasionally generate images similar to those from the dense model at low sparsity, the differences become more pronounced as sparsity levels increase. In contrast, the model compressed using Skrr consistently maintains high degree of similarity to the original images across all sparsity levels, demonstrating its robustness. D. Limitations In this section, we address the limitations of Skrr. While Skrr effectively preserves Text-to-Image (T2I) performance during pruning, its performance deteriorates noticeably at extreme sparsity levels (> 50%). This degradation is likely due to significant reduction in the representational capacity of the text encoder as the number of parameters becomes excessively limited. However, such sparsity could still provide additional memory efficiency through complementary techniques such as weight quantization. Another limitation is that Skrr does not achieve performance improvements beyond that of the original dense model. While pruning the text encoder can enhance certain image quality metrics, such as improving FID scores, we observed consistent decline in performance on benchmarks like CLIP score and GenEval as sparsity increased. Addressing this performance drop and ensuring robustness across benchmarks would be an interesting future work. 30 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A10. Qualitative comparison of images generated by models compressed to sparsity range 20%-30% (sparsity level 1) using ShortGPT, LaCo, FinerCut, and Skrr, alongside images generated by dense models. Skrr demonstrates remarkable ability to produce images closely resembling those from the dense model in the majority of cases. 31 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A11. Qualitative comparison of images generated by models compressed to sparsity range 30%-40% (sparsity level 2) using ShortGPT, LaCo, FinerCut, and Skrr, alongside images generated by dense models. Skrr demonstrates remarkable ability to produce images closely resembling those from the dense model in the majority of cases. 32 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A12. Qualitative comparison of images generated by models compressed to sparsity range 30%-40% (sparsity level 2) using ShortGPT, LaCo, FinerCut, and Skrr, alongside images generated by dense models. Skrr demonstrates remarkable ability to produce images closely resembling those from the dense model in the majority of cases. 33 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A13. Qualitative comparison of images generated by models compressed to sparsity range 40%-50% (sparsity level 3) using ShortGPT, LaCo, FinerCut, and Skrr, alongside images generated by dense models. Skrr demonstrates remarkable ability to produce images closely resembling those from the dense model in the majority of cases. 34 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A14. Qualitative comparison of images generated by models compressed to sparsity range 40%-50% (sparsity level 3) using ShortGPT, LaCo, FinerCut, and Skrr, alongside images generated by dense models. Skrr demonstrates remarkable ability to produce images closely resembling those from the dense model in the majority of cases. 35 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A15. Qualitative comparison of images generated by models compressed to various sparsity level using ShortGPT, LaCo, FinerCut, and Skrr, alongside images generated by dense models. Skrr demonstrates remarkable ability to produce images closely resembling those from the dense model in all the sparsity levels. Notably, the lighting and composition of objects in the images retains even after harsh pruning to the text encoder. 36 Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Figure A16. Qualitative comparison of images generated by models compressed to various sparsity level using ShortGPT, LaCo, FinerCut, and Skrr, alongside images generated by dense models. Skrr demonstrates remarkable ability to produce images closely resembling those from the dense model in all the sparsity levels. Notably, the lighting and composition of objects in the images retains even after harsh pruning to the text encoder."
        }
    ],
    "affiliations": [
        "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
        "INMC & IPAI, Seoul National University, Republic of Korea",
        "School of Electrical and Computer Engineering, Cornell Tech, USA"
    ]
}