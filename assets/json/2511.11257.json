{
    "paper_title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
    "authors": [
        "Yuqi Yin",
        "Yibo Fu",
        "Siyuan Wang",
        "Peng Sun",
        "Hongyu Wang",
        "Xiaohui Wang",
        "Lei Zheng",
        "Zhiyong Li",
        "Zhirong Liu",
        "Jianji Wang",
        "Zhaoxi Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery."
        },
        {
            "title": "Start",
            "content": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic AI for Greener Solvents liquid discovery Yuqi Yin1#, Yibo Fu2#, Siyuan Wang3#, Peng Sun1, Hongyu Wang4, Xiaohui Wang4, Lei Zheng5, Zhiyong Li2*, Zhirong Liu1*, Jianji Wang2*, and Zhaoxi Sun4* 1College of Chemistry and Molecular Engineering, Peking University, Beijing 100871, China 2Henan Key Laboratory of Green Chemistry, Collaborative Innovation Center of Henan Province for Green Manufacturing of Fine Chemicals, Key Laboratory of Green Chemical Media and Reactions, Ministry of Education, School of Chemistry and Chemical Engineering, Henan Normal University, Xinxiang, Henan 453007, P. R. China 3School of Computer Science, Shanghai Jiao Tong University, Shanghai 201100, China 4Faculty of Synthetic Biology, Shenzhen University of Advanced Technology, Shenzhen 518107, China 5Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning and NYU-ECNU Center for Computational Chemistry, NYU-Shanghai, 1555 Century Avenue, Pudong New Area, Shanghai 200062, *To whom correspondence should be addressed: Zhiyong Li yli@htu.edu.cn Zhirong Liu LiuZhiRong@pku.edu.cn Jianji Wang jwang@htu.edu.cn Zhaoxi Sun z.sun@suat-sz.edu.cn China Abstract The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates hierarchical search architecture for molecular screening and design. Trained and evaluated on newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to AI for Greener Solvents accelerate real-world IL discovery. Keywords: Ionic Liquids, Gas Absorption, Solute Extraction, Large Language model, Agent 1. Introduction. AI for Greener Solvents Ionic liquids (IL) are salts composed of cations and anions, defined as having melting points below 100 at ambient pressure. Owing to their low volatility, high thermal stability, absorption capacity, electrochemical advantages and other favorable properties, they have found widespread use across many applications.1-6 By designing and screening the constituent cation-anion pairs, ILs offer exceptional tunability.7-11 This tunability, however, results in vast combinatorial space of potential cation-anion pairs. Yet for precisely this reason, selecting ILs that satisfy the requisite physicochemical property criteria for specific application scenarios remains the key bottleneck to practical deployment. Traditionally, the design of ILs has relied on the expert knowledge. However, any given physicochemical property typically arises from multiple intertwined interactions, making precise control difficult. For example, lengthening the alkyl chain of [RMIM]+ reduces Coulombic interactions and increases entropy, both of which tend to lower the melting point, while strengthening van der Waals interactions, which tends to raise it.12-18 Alongside expert knowledge, several computational approaches can aid researchers, ranging from molecular dynamics (MD) simulation and quantum chemistry calculations to simple linear regressions such as the Abraham model19. However, these methods often suffer from prohibitive computational cost, limited accuracy, or narrow domains of applicability.20-23 To address these challenges, deep learning provides new approach, enabling datadriven models to leverage existing experimental results for rapid, generalizable inference while maintaining strong accuracy. In the previous IL studies researchers chose to use neural networks including descriptor-based Multi-Layer Perceptrons (MLP)24-26, SMILES-sequence Recurrent Neural Networks (RNN)27-29, Convolutional Neural Networks (CNN) on 1D/2D molecular representations25,30,31, and message-passing Graph Neural Networks (GNN) for molecular graphs32-34. Nevertheless, three problems remain to be addressed. First, unlike organic chemistry and related areas where experimental measurements are abundant, labeled data for ILs are scarce.3537 major challenge, therefore, lies in how to utilize unlabeled datasets to compensate for the scarcity of labeled data. Second, molecular data are intrinsically multimodal38, and we must integrate these modalities more effectively to represent IL systems with higher fidelity. Third, workflow automation is lacking, since conventional chemical pipelines are fragmented.39 We therefore seek methods that can efficiently process data and perform diverse tasks. Concurrently, the rapid development of Large Language Models (LLMs) in recent years offers different perspective from task-specific deep learning models, which can be used to address the aforementioned issues. Since 2018, numerous works represented by BERT40 and GPT41 have proposed that we can leverage selfsupervised training to utilize vast amounts of text data to improve the performance of downstream Natural AI for Greener Solvents Language Processing (NLP) tasks. Researchers in chemistry have drawn inspiration from these NLP approaches to develop various LLMs, such as encoder-based ChemBERTa42, decoder-based ChemLLM43, and encoder-decoder-based MolT544, among others. Building on the success in NLP, computer vision researchers have also joined the effort, with multimodal works such as CLIP45, which excels in image-text understanding. In chemistry, works that align modalities based on the inherent multimodality of molecules have been emerging continuously. Examples include MMFRL46, which uses molecular graphs and five other modalities, and PointGAT47, which enhances GNN performance with additional 3D representations. To further enhance the capabilities of LLMs, works such as Toolformer48 combine them with external tools to form agents, enabling LLMs to automate pipeline execution and independently make decisions to solve problems. Chemical researchers have also adopted this approach, with projects like Coscientist49 significantly accelerating the development of automated experimentation. Building on this background, we introduce AIonopedia, to our knowledge the first efficient LLM-based intelligent agent tailored to ILs. By interacting with various specialized modules, it orchestrates the execution of multiple IL-related pipelines. AIonopedia can autonomously search and process data, enabling an end-toend solution to IL research problems. Its core module, the property predictor, is the first LLM-augmented multimodal domain foundation model for ILs reported to date. This module follows two-step training paradigm of modality alignment and fine-tuning, effectively leveraging unlabeled data from molecular graphs, SMILES sequences, and physicochemical descriptors to enhance performance. Motivated by limited species coverage and the lack of high-quality data relevant to important real-world scenarios, we compile new IL dataset for fine-tuning that contains the largest collection of known IL solute-solvent interaction data. Our method consistently achieves superior performance across wide range of property datasets, while also demonstrating robust Out-Of-Distribution (OOD) generalization. Built on this property predictor, we develop two complementary pipelines: an IL modification pipeline that performs anion replacement and cation sidechain edits and is evaluated on literature-reported systems, and hierarchical search pipeline that combines traversal and molecular similarity search for large-scale molecular screening and design. We further validate the screening pipeline in wet-lab experiments, confirming its effectiveness in real-world settings. In summary, the introduction of AIonopedia provides novel and efficient tool for IL research, advancing data-driven and automated approaches in chemistry. 2. Methodology. Inspired by series of chemistry-domain LLM-agent studies49-51, we designed AIonopedia. The complete workflow that includes the agent is presented in Fig. 1A. AI for Greener Solvents 2.1. Overview of the AIonopedia. At the core of the tool-invocation pipeline is planner powered by GPT-552, OpenAIs State-of-the-Art (SOTA) reasoning model. The exceptional performance of GPT-5, validated on benchmarks such as Aider Polyglot53, AIME 202554, and MMMU55, underpins the agent's capabilities. Following the ReAct56 methodology, the planner iteratively combines reasoning and acting to interact with six specialized tools: web searcher, PubChem searcher, SMILES canonicalizer, data processor, property predictor, and molecule searcher. The model cycles through the steps of Thought, Action, and Observation. During the Thought step, the model engages in reasoning by receiving information from the user prompt and using zero-shot chain-of-thought (CoT)57 to perform logical inferences. In the Action step, the model selects the appropriate tool and provides the necessary inputs based on the reasoning from the Thought step to execute current task. Finally, in the Observation step, the model receives the output from the tool and uses this information for the next iteration of the Thought step. By iteratively repeating these steps, the model effectively combines reasoning with actions to accomplish complex tasks. The web searcher module utilizes an LLM-controlled fused search architecture. Queries from the planner are processed by the searcher's internal GPT-5 and are sent to the Serper API58 to retrieve results from Google Scholar. For each retrieved article, the module then attempts to use Semantic Scholar API59 to obtain more detailed abstract information. Following this, general web search is employed as fallback mechanism, which ensures the tool can adapt to diverse scenarios, ranging from IL paper searches to general information retrieval. Importantly, such search-and-retrieval framework not only compensates for the inherent incompleteness of the models internal knowledge, but also markedly reduces hallucination by grounding responses in externally verifiable sources. This Retrieval-Augmented Generation (RAG)60 capability enhances both the factual reliability and adaptability of the overall agent system. In contrast to the versatile web searcher, the PubChem searcher module is specialized for chemical structure retrieval, converting molecule or ion names/synonyms into SMILES strings. While it first attempts standard PubChem search61, it also leverages an internal LLM to address database gaps for ionic species. The model provides reasoning to generate or correct results, such as converting retrieved neutral form into its proper charged state. The retrieved information is then prepared by two dedicated components. The data processor, Python code interpreter, handles the processing of data and results. Subsequently, the SMILES canonicalizer, an RDKit62 tool, normalizes the inputs for the pipelines. Once this preparation is complete, the planner extracts AI for Greener Solvents the essential data and inputs it into the property predictor. This essential data primarily includes the IL's structural information and corresponding information such as temperature. The predictor is multimodal foundation model (the technical details will be explained in Section 2.2 below) that fuses IL molecular sequences with their graph representations, trained on an IL dataset containing 100000 samples. We curated the dataset from existing literature by using automated scripts for an initial retrieval of approximately 10000 papers, subsequently applying LLMs for data extraction (such as text retrieval and OCR), and performing manual verification. The predictor currently estimates two broad classes of properties: the solute-solvent interactions related to ILs and their bulk characteristics (as shown in Fig. 1B). The former includes solvation free energy (ΔG), transfer free energy, and hydration free energy, while the latter encompasses the melting point, surface tension, viscosity, and mass density of bulk ILs. Considering the real-world demand in chemical experiments for identifying ideal solvent and solute candidates, we designed molecule searcher module to effectively explore the chemical space. Given the complexity of IL systems and the scarcity of relevant data, generative models often fail to produce chemically realistic molecules efficiently. To overcome this, we have transformed the IL discovery task into heuristic search problem, enabling more controlled and feasible navigation of this space. As illustrated in Fig. 1C, this module's pipeline leverages the property predictor to first identify the Top-K optimal ionic pairs or solutes from our property dataset. These serve as starting points for beam search conducted in both our dataset of known IL systems and external databases (e.g., PubChem), guided by Tanimoto similarity63. This approach enables the discovery of more potential IL candidates within the model's generalization range. The most promising of these are then prioritized for wet-lab validation. In summary, this ReAct-driven pipeline empowers AIonopedia to move seamlessly from information gathering to property prediction and IL screening, offering powerful, end-to-end solution to accelerate the traditional research workflow. (A) modules based on GPT-5 modules based on AIonopedia-trained models : User prompt AIonopedia AI for Greener Solvents (B) (C) Knowledge extraction & curation acquisition of primary literatures Expert-led, LLM/script-assisted processing property data aggregation Data processor Property predictor Wet-lab validation Python code interpreter: Process data and results SMILES canonicalizer Planner Multimodal-LLM based predictor: Solute-Solvent interaction Bulk property Molecule searcher multi-step experimental operations RDKit tool: Normalize input for pipelines GPT-5 plan & execute: Reason and choose the action Pubchem searcher Web searcher Exhaustive/Beam search: Search for optimal solute-solvent pairs Multi-level hierarchical search Name to SMILES: Query PubChem by names/synonyms Handle edge cases and disambiguate via LLM (D) Cross-source retrieval: Query scholarly databases + web for ILs Return concise summaries with references results characterization Solute-Solvent interaction Solvation Transfer Hydration Property predictor Gas Organic solvent/Water Gas 1. Modality alignment stage Labeled Supervised Loss Graph-Text Matching Loss Ionic liquid Ionic liquid Water pre-defined system features output FFN FFN Bulk property Graph-Text Grounding Loss concatenate Melting point Surface tension Mass density Viscosity cross-modal embedding cross-modal embedding Cross attention module Attention Cross attention module Attention Molecule searcher sequence embedding fused graph embedding LLM projector Momentum Graph-Text Contrastive Loss Graph fusion encoder per-dataset ILs existing ILs / PubChem database Exhaustive search Property predictor Large Language Model sequence representation top results Beam search molecule similarity e.g. BMIM HMIM 2. Momentum Graph-Text Contrastive Loss graph queue InfoNCE Loss LLM encoder Momentum graph encoder language modal Avg graph modal raw multi-graph embedding GNN projector Temp projector Graph transformer cation graph anion graph solute graph temperature organic solvent graph Momentum LLM encoder language queue InfoNCE Loss 3. Finetuning stage property prediction Graph encoder concatenate New FFN cross-modal embedding cross-modal embedding Two-tower base model Fig. 1. (A) The overall workflow incorporating AIonopedia, illustrating the closed-loop process from dataset collection to final application in wet-lab validation. (B) Property categories covered in the dataset. (C) The architecture of the molecule searcher, based on beam search with Tanimoto similarity. (D) The architecture of the property predictor, multimodal foundation model for the IL domain developed via two-stage training AI for Greener Solvents strategy. 2.2. Multi-stage training of the multi-modal LLM. To capture rich molecular semantics, our property predictor is inspired by series of multimodal alignment works45,64-68, which utilize contrastive learning to train dual-tower multimodal model for molecule text and graph. The language view offers effortless, multimolecule context, whereas the graph view preserves topology with permutationequivariant embeddings. Contrastively aligning them fuses these complementary strengths, giving every molecule unified, chemically grounded augmentation. At high level, the model consists of an LLM-based language encoder and graph-transformer69-based graph encoder, with cross-modality attention modules stacked on top to fuse the two views. Unlike traditional single molecule contrastive learning, we treat the entire multimolecule system as single, holistic sample (e.g. 1-Butanol in [BMPyrr]+[B(CN)4]- at 298.15K). This enables deeper information fusion than merely concatenating per-molecule embeddings at the output layer. An architecture overview of the predictor is presented in Fig. 1D. Although our labeled dataset is limited in size, it spans wide range of supervised properties, whereas unlabeled molecular data are far more abundant. To leverage both sources effectively, we adopt twostage training strategy, beginning with modality alignment stage followed by finetuning stage. For the alignment phase of the model, we gathered significantly larger unlabeled molecule dataset and performed random sampling of these molecules to compose synthetic data samples, which are random combinations of cations, anions, organic solvents, and solutes prepared at fixed temperature. Each sample is annotated with pseudolabels that integrate pre-computed physicochemical descriptors with categorical tags specifying the molecular composition and system temperature, driving the self-supervised training. Detailed information on the dataset is provided in Section 3.1 below. Once the synthetic set is prepared, we encode its language modality with an LLM to obtain the corresponding embeddings, which serve as one branch of the contrastive objective. While these LLMs are typically based on decoder-only architectures, they possess strong representation capabilities owing to their vast number of parameters. Particularly, LLMs trained on scientific corpora to augment domain-specific capabilities are intrinsically adapted to tasks requiring the comprehension of chemical problems and molecular representation. The LLMs were fine-tuned using the Low-Rank Adaptation (LoRA)70 method during the training process, which significantly reduced the computational resources and GPU memory consumption required for training. Correspondingly, the four types of molecular data (cation, anion, solute, organic solvent) AI for Greener Solvents for the graph modality are fed into the same graph encoder. The extracted encodings of both modalities are transformed using projector, consistent with the approach of LLaVA-1.571, where the projector is implemented as two-layer MLP. Next, the molecular graphs of all constituent molecules are concatenated with the temperature feature and passed through lightweight transformerbased graphfusion encoder, which integrates the signals into single graphmodality embedding for the complete system. After obtaining the embeddings for both modalities, we take the last token from each as the representation and apply momentum contrastive learning72 with InfoNCE loss73. On top of encoders, we add two cross-attention decoders to fuse modalities, where each modality's embeddings serve as queries for the other's key-value pairs. To guide both encoders and decoders, three loss functions are introduced. Among these, Graph-Text Grounding (GTG) task employs an autoregressive cross-entropy loss, computed by the decoder that receives text queries. Meanwhile, graph-text matching (GTM) task employs binary-classification loss to determine whether the two modalities originate from the same molecular system. Apart from the two losses previously used in BLIP-266, supervised loss which combines MSE term with cross-entropy term in weighted sum is utilized to quantify the discrepancy between model outputs and pseudo labels. The GTM and supervised branches share the concatenated final tokens from both modalities, which are then fed into two separate Feed-Forward Networks (FFN). In the finetuning phase, we replace the alignment phase FFNs with taskspecific regression heads and continue training all model parameters, effectively specializing the model for the different property regression tasks. 3. The multi-property IL dataset. We collected and assembled massive labeled IL datasets from vast body of literature to ensure the robustness of our property predictor. Fig. 2A presents the number of data points and the diversity of IL species in each dataset on logarithmic scale. Although the organic-water transfer dataset contains no IL species (plotted as 1 on the log scale), we include it because subtracting these values from the IL-water results yields the IL-organic transfer ΔG. All datasets were designed for broad substance diversity to prevent any single class of similar molecules from dominating training process and introducing bias. AI for Greener Solvents Fig. 2. (A) The bar plot of dataset samples and IL counts. From left to right: solvation ΔG, transfer ΔG (IL/water), transfer ΔG (organic/water), mass density, viscosity, surface tension, melting point, and the modality-alignment dataset. (B) 1D and 2D KDE distributions for six training properties (the training data for hydration ΔG is indirectly provided by solvation ΔG and transfer ΔG data). (C) Molecular similarity heatmap of finetuning dataset (1159 cations, 287 anions, and 328 neutral molecules). (D) Comparison of tSNE dimensionality reduction results using specific descriptors and molecular similarity. 3.1. Synthetic data for the modality alignment. In contrast to ILBERT74 and other works27,36 in the field of ILs, we did not leverage ultra-large databases AI for Greener Solvents such as ZINC2075 directly in Stage 1. Fewer than 10000 ILs are known so far, and the most comprehensive resource, ILThermo76, encompasses only about 3000 entries. Consequently, introducing an excessively broad spectrum of non-IL ion species during pretraining or alignment would inevitably impart undue prior bias. At the same time, given that our language encoder has already been pretrained, further scaling with ultra-large datasets is not necessary. Instead, we adopted quality-over-quantity strategy and performed self-supervised modality alignment training on carefully curated IL systems. By augmenting the diversity of existing ILs through similarity screening in large databases and combinatorially sampling the components to reduce redundancy, we generated 2.8 million synthetic virtual-system data points. The resulting dataset falls into four broad categories, corresponding to IL-solute interactions, organic solvent-solute interactions, IL bulk properties including temperature, and IL bulk properties excluding temperature. The pseudo labels, used as targets for Stage 1 labeled supervised loss training, comprised 21dimensional descriptor representation for the four types of molecules, the systems temperature feature, and 4-dimensional one-hot vector encoding the format classification. The descriptors include the number of hydrogen bond donors, the number of hydrogen bond acceptors, the number of rotatable bonds, the polar surface area,77 the number of atomic stereocenters, the octanol-water partition coefficient log and molecular reactivity with the Crippens approach,78 the fraction of sp3 carbon, the number of rings, the number of heterocycles, the number of aromatic rings, the number of aromatic heterocycles, the number of spiro atoms, the molecular weight, the number of heteroatoms, the number of heavy atoms, the kappa1&2&3 shape indices79, the Balaban index80 and the Bertz CT index81. These chemical properties provide informative characterizations of given molecule. Additionally, for each molecule we constructed graph object and traversed the atom indices in the order defined by the canonical SMILES. The node features represent atomic properties including atom type, degree, hybridization, implicit valence, aromaticity, formal charge, and hydrogen bonding potential, while the edge features capture bond characteristics including bond type, stereochemistry, conjugation, and ring participation. By leveraging synthetic unlabeled data in quantities far exceeding the labeled data, we substantially improved the models performance across all metrics. The comparison results are provided in Section 4.4. 3.2. Experimental datasets. We included one-dimensional label distributions of datasets for six properties, alongside their temperature-expanded, two-dimensional counterparts, as shown in Fig. 2B. Since melting points have no temperature dependence and transfer ΔG are only reported in the literature at 298K, their 2D distributions are AI for Greener Solvents omitted. Fig. 2C shows the Tanimoto similarity heatmap for all molecules used during fine-tuning, including 1159 cations, 287 anions, and 328 neutral molecules. To comprehensively characterize each molecule, we computed four fingerprint types: ECFP, MACCS, atom-pair and PubChem, which are based on RDKit and chemfp82, and applied hierarchical clustering to enhance visualization clarity. The low overall internal similarity, as indicated by the predominantly dark matrix with sparse highlights, underscores the breadth of our chemical coverage. For the datasets we collected on solvation ΔG and biphasic transfer ΔG properties, they cover about ionic solvents and 150 solutes, which are the largest datasets covering these properties as far as we know. Given the thermodynamic link allowing hydration ΔG to be determined from the solvation and transfer ΔG, our dataset intrinsically defines the hydration behavior of solutes. To explicitly evaluate this implicit hydration behavior, we generated ten hypothetical IL systems per solute by sampling novel ion combinations from the solvent-solute dataset, excluding any known pairs to prevent data leakage. These virtual ILs are used solely to construct held-out hydration ΔG benchmark (Section 4.3), on which all models are evaluated. Aside from solute-solvent interactions involved in the above-mentioned datasets, we additionally curate datasets for bulk properties. The considered bulk properties include mass density, viscosity, surface tension and melting points. Compared to the aforementioned datasets, our bulk properties dataset covers larger number of ILs, with nearly 6,000 systems in total, encompassing approximately 1,200 cations and 300 anions. Additionally, Fig. 2D presents t-SNE projections based on both our descriptor set and traditional molecular-similarity metrics, revealing pronounced separation between IL-forming ions and neutral, gasor drug-like molecules. Crucially, the markedly sharper separation achieved with our descriptors unequivocally validates their effectiveness in capturing the key chemical distinctions between these categories. To verify local concordance, we also mark the locations of 1-butyl-3-methylimidazolium ([Bmim]+) and 1-hexyl-3methylimidazolium ([Hmim]+) in each t-SNE plot. Under our descriptors, these two ions remain proximate, consistent with their underlying chemical similarity. 4. Results. 4.1. Data extraction capability of the agent. Reliable automated data acquisition for ILs critically depends on mapping informal notations in the literature to standardized molecular representations. In the field of ILs, researchers commonly use abbreviations to replace the lengthy ion names83. Because these abbreviations lack standardized forms and are sparsely indexed in databases, they are difficult for conventional workflows to handle (e.g., NAI for Greener Solvents octyltrimethylammonium may be abbreviated as both [C8TA]+ and [N1118]+). To systematically characterize this data-extraction challenge, we define benchmark task that requires methods to convert an ion abbreviation directly into its corresponding canonical SMILES and full name. We used dataset containing 50 ions to compare the results of four web-enabled SOTA LLMs, as shown in Fig. 3A. Given the diversity of ion full names and the potential non-uniqueness of canonical SMILES (e.g. due to delocalized charge), the results were evaluated by human experts. We reported metrics in decreasing order of difficulty: (i) Canonical SMILES Match: direct match of the expected SMILES. (ii) Structural Match: match of the expected SMILES after manual canonicalization. (iii) Full Name Match: correct full name provided. Among these, GPT-5 significantly outperformed the other LLMs on all metrics and was therefore selected as the core LLM for AIonopedia. To further evaluate the efficacy of our agent, we compared the performance of the full AIonopedia system against GPT-5 with varying degrees of tool integration, as shown in Fig. 3B. Owing to its rich tool interactions and coordinated communication across multiple LLMs, AIonopedia comprehensively surpassed the compared baselines. It proved particularly effective on the strictest metric (i), boosting the accuracy to 94.7%, and was thus validated as more effective problem-solving approach. Tool Integration Benchmark (GPT-5) LLM + Web Search Benchmark (A) a c 0.8 0.6 0.4 0.2 (B) 1 r A 0.8 0.6 0. 0.2 0 Canonical SMILES Match Structural Match Full Name Match Canonical SMILES Match Structural Match Full Name Match GPT-5 Gemini 2.5 pro Claude 4.5 sonnet DeepSeek-R AIonopedia GPT-5 GPT-5(w/o web) Fig. 3. The model effect on the abbreviation interpretation for ions in ILs. (A) Performance comparison of four different LLMs84-87 equipped with base search module. (B) Effect of tool integration on GPT-5 performance. 4.2. Evaluation of the property predictor across the various property datasets. AI for Greener Solvents In this study, we selected Meta AI's Galactica88 series, Alibaba's Qwen389 series as well as Google DeepMinds Gemma390 series to initialize the language encoder of the property predictor. Galactica and Qwen3 both explicitly state in their technical reports that they were trained on scientific texts to enhance domain knowledge, whereas Gemma3 serves as generalpurpose LLM control that makes no such claim. This setup allows us to assess how domain-specific pre-training affects downstream property prediction. We then benchmarked these property predictor variants against comprehensive suite of chemistry-domain baselines. This suite was carefully selected to be representative across two key dimensions: architecture, including both encoderand decoder-based models, and modality, covering both unimodal and multimodal LLMs. We employ multi-level 5-fold cross-validation splits for each dataset to mitigate overfitting91. For IL bulkproperty datasets, we adopted cation-based and IL-based splits; for the ternary solute-solvent interaction dataset, we additionally introduced split based on all three components. Hyperparameters derived from training with the strictest cation-based split were subsequently applied to the other splits. The detailed results are shown in Table 1-3. Considering the training cost and to ensure fair comparison with prior work, we finetuned most LLMs and multimodal LLMs using LoRA. For each model, we followed the default LoRA settings from their respective GitHub repositories, except for PRESTO92, where we lowered the default rank to match that used in works such as LlasMol93. This adjustment was made because PRESTO typically utilizes much larger LoRA rank compared to other methods, which can lead to significantly higher resource consumption. Since SPMM68 and ILBERT employ smaller, lightweight BERT40 and RoBERTa94 encoders, we utilized full finetuning instead. Furthermore, since data analysis in the IL domain typically relies on traditional machine learning methods or lightweighted neural networks rather than large pretrained models, we included an MLP baseline for comparison. This baseline was trained on the physicochemical descriptors predefined during our modality-alignment phase, confirming that our models performance does not depend solely on these pseudo-labels. Our model achieved the best performance across all metrics on nearly every dataset. In particular, the Qwen3-0.6b based model delivered the best average results, ranking first on 20 evaluated metrics and showing especially striking performance on several solute-solvent pair datasets  (Table 2)  . By contrast, excluding the smallest version based on Galactica-125m, among the other three models of similar size, the Gemma3-1b version performed the worst due to its lack of training on scientific texts. Thanks to LoRA training, our models number of trainable parameters is roughly on par with the fully finetuned BERTbased models and lower than several approaches that employ 7-8 billionparameter models. Among the baselines, ILBERT delivered the AI for Greener Solvents strongest overall performance, likely because it was pretrained on large volume of ionic data, and its results on the meltingpoint dataset were especially impressive. However, both encoderstyle models, ILBERT and SPMM, fall far behind the other models on the viscosity dataset. We attribute this gap to the strong length sensitivity of viscosity: the two encoders aggregate features with 1D CNN or single [CLS] token, making them inherently less responsive to sequence length than decoderstyle LLMs that read the final token. Although ILBERTs original paper reports higher viscosity accuracy than our reproduction, we believe the difference arises from variations in dataset collection and from our stricter cationbased split. In contrast, the MLP baseline sidesteps the issue because its input descriptors include molecular weight. To give general survey, we averaged the three evaluation metrics for each dataset together with the results from the different split schemes, yielding an overall rank for every model across the seven datasets. These ranks were then visualised as radar charts that compare the four AIonopedia variants with one another and with the baseline models, as shown in Fig. 4, which visually validates the leading position of the Qwen30.6b variant. Table 1. Comparison of AIonopedia with baselines under cation-based splits; the organic solvent/water transfer ΔG dataset uses solvent-based split. RMSE () solvation ΔG (kcal/mol) IL/water transfer ΔG (kcal/mol) melting point (K) AIonopedia (Galactica-125m) 0.3640. 0.4830.123 40.43.9 viscosity (mPas) (log scale) 0.2940. (0.2939) surface tension mass density (mN/m) (g/cm3) organic solvent/water transfer ΔG (kcal/mol) 3.860.41 0.03270.0032 0.5300.039 AIonopedia (Galactica-1.3b) 0.3220.136 0.4570.108 40.33.8 0.2980.038 3.770.24 0.03300. 0.5290.051 AIonopedia (Qwen3-0.6b) 0.3280.130 0.4410.117 39.93.4 0.2940. (0.2943 3.620.35 0.03330.0068 0.5340.038 AIonopedia (Gemma3-1b) 0.3250. 0.4590.095 40.63.2 0.3050.026 3.660.33 0.03300.0060 0.5380. MLP (pretrain descriptors) 0.4170.106 0.7550.093 45.55.3 0.4110.048 4.480. 0.04370.0067 0.8780.052 MolCA67 (Galactica-1.3b) 0.4210.078 0.7320.176 46.91. 0.3840.049 5.810.47 0.05250.0064 0.8460.222 T5chem95,96 Molinst (Llama3-8b)97, SPMM (full finetune)68 LlaSMol (Mistral-7b)93,99 PRESTO (Vicuna v1.57b)92,100 0.3790.104 0.5760. 44.92.5 0.3170.038 3.940.23 0.03570.0043 0.7020.067 0.3520. 0.5100.159 42.43.2 0.3100.036 4.080.43 0.03310.0031 0.6780. 0.4450.090 0.5310.118 46.43.9 0.5880.018 4.670.36 0.04880. 0.6700.098 0.3370.133 0.4920.150 42.64.9 0.3100.019 3.850. 0.03400.0027 0.6740.087 0.4000.144 0.4720.120 41.24.6 0.3090. 3.970.46 0.04420.0046 0.7230.124 ILBERT (full finetune)74 0.3340.105 0.4590. 39.74.3 0.5360.025 3.960.28 0.03770.0067 0.6360.031 Pearson () solvation ΔG IL/water transfer ΔG melting point viscosity (log scale) surface tension mass density organic solvent/water transfer ΔG AIonopedia (Galactica-125m) 0.95400. 0.97980.0103 0.71860.0305 0.90980.0198 0.91230.0120 0.98450.0038 0.97540. AIonopedia (Galactica-1.3b) 0.95920.0300 0.98190.0081 0.72010.0321 0.90650.0211 0.91230. 0.98390.0051 0.97510.0059 AIonopedia (Qwen3-0.6b) 0.95630.0279 0.98370.0080 0.71910. 0.90950.0146 0.91870.0097 0.98350.0065 0.97420.0050 AIonopedia (Gemma3-1b) 0.95810. 0.98300.0066 0.71130.0178 0.90220.0033 0.91620.0083 0.98400.0056 0.97340. MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.92530.0326 0.94620.0061 AI for Greener Solvents 0.63540.0584 0.81090. 0.86860.0168 0.97180.0078 0.92820.0129 0.93190.0174 0.94880.0192 0.59770. 0.85790.0164 0.78150.0436 0.95980.0079 0.94110.0219 0.95100.0251 0.98130. 0.68240.0279 0.90230.0189 0.90590.0111 0.98300.0043 0.96320.0104 0.95100. 0.97570.0136 0.66970.0469 0.89780.0179 0.89170.0251 0.98390.0031 0.95590. 0.92300.0284 0.97730.0102 0.61650.0404 0.57460.0527 0.86830.0086 0.96590. 0.95840.0168 0.95310.0284 0.97710.0117 0.66540.0395 0.89710.0150 0.90260. 0.98300.0035 0.95750.0126 0.93660.0348 0.97900.0090 0.69690.0482 0.89840. 0.89960.0128 0.97090.0055 0.94930.0193 0.95170.0277 0.97970.0136 0.72040. 0.64460.0464 0.90150.0177 0.97930.0062 0.96290.0060 Kendall τ () solvation ΔG IL/water transfer ΔG melting point viscosity (log scale) surface tension mass density organic solvent/water transfer ΔG AIonopedia (Galactica-125m) 0.8430.027 0.8830. 0.5150.029 0.8030.022 0.7780.020 0.9110.010 0.8780.014 AIonopedia (Galactica-1.3b) 0.8650.043 0.8900.016 0.5080.046 0.8000.025 0.7720.031 0.9120. 0.8810.016 AIonopedia (Qwen3-0.6b) 0.8600.046 0.8930.011 0.5050.022 0.8000. 0.7800.022 0.9080.015 0.8790.012 AIonopedia (Gemma3-1b) 0.8640.041 0.8920. 0.5000.028 0.7950.010 0.7760.022 0.9110.010 0.8770.019 MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.7810.041 0.7930.016 0.4660.027 0.6830.030 0.7060. 0.8700.017 0.7680.018 0.8020.017 0.8210.027 0.3950.056 0.7100. 0.6240.039 0.8430.023 0.8070.033 0.8430.030 0.8850.011 0.4710. 0.7830.020 0.7460.027 0.9070.005 0.8490.017 0.8530.030 0.8740. 0.4600.055 0.7750.022 0.7400.036 0.9040.013 0.8350.016 0.7780. 0.8750.021 0.4000.050 0.4100.049 0.6920.020 0.8530.029 0.8450. 0.8540.049 0.8680.018 0.4520.049 0.7740.014 0.7590.020 0.9030. 0.8330.020 0.8250.057 0.8770.015 0.4840.051 0.7750.014 0.7490. 0.8740.015 0.8250.029 0.8440.047 0.8830.019 0.5140.042 0.4550. 0.7350.034 0.8890.008 0.8500.010 Table 2. Comparison of AIonopedia with baselines under IL-based and ternary-component-based splits on the solute-solvent interaction datasets; the organic solvent/water transfer ΔG dataset uses solute-solventbased split. RMSE () solvation ΔG IL/water transfer ΔG (kcal/mol) (kcal/mol) solvation ΔG IL/water transfer ΔG organic solvent/water (ternary-component (ternary-component transfer ΔG split) (kcal/mol) split) (kcal/mol) (kcal/mol) AIonopedia (Galactica-125m) 0.3040.030 AIonopedia (Galactica-1.3b) 0.3210. 0.4730.100 0.4600.121 0.1550.013 0.1260.012 0.3730.043 0.2650. AIonopedia (Qwen3-0.6b) 0.3090.044 0.4400.132 0.1240.016 0.2380.013 AIonopedia (Gemma3-1b) 0.3110.039 0.4590.121 0.1250.017 0.2500.013 MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.4410.065 0.4000.073 0.3710.027 0.3520.038 0.4250.016 0.3150. 0.4130.144 0.3230.051 0.8800.249 0.6300.051 0.5880.137 0.5100. 0.5500.111 0.5910.132 0.5300.088 0.4520.124 0.3610.023 0.3410. 0.2400.013 0.1780.019 0.3070.023 0.1600.016 0.1600.032 0.1470. 0.7420.043 0.6440.056 0.5070.041 0.2750.005 0.4270.082 0.3360. 0.2590.022 0.2600.030 0.4150.028 0.3990.030 0.3930.035 (0.3930) 0.3930.032 (0.3934 0.8180.035 0.7240.012 0.5600.019 0.4680. 0.5750.053 0.5130.033 0.5410.089 0.4040.042 Pearson () solvation ΔG IL/water transfer ΔG (ternary-component (ternary-component solvation ΔG IL/water transfer ΔG split) split) organic solvent/water transfer ΔG AIonopedia (Galactica-125m) 0.96250.0056 0.98010. 0.99040.0021 0.98800.0028 0.98490.0025 AIonopedia (Galactica-1.3b) 0.96010.0051 0.98070. 0.99390.0014 0.99440.0011 0.98610.0021 AIonopedia (Qwen3-0.6b) 0.96150.0099 0.98160. 0.99420.0018 0.99530.0006 0.98620.0024 AIonopedia (Gemma3-1b) 0.96250.0076 0.98030. 0.99400.0018 0.99500.0007 0.98600.0023 AI for Greener Solvents MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.91850.0237 0.92000.0490 0.94440.0085 0.94650.0062 0.93600.0113 0.93400. 0.96260.0062 0.95270.0056 0.96160.0069 0.95290.0038 0.95120.0060 0.97850. 0.98150.0030 0.98740.0023 0.97860.0028 0.95040.0082 0.97630.0121 0.98680. 0.99290.0002 0.97980.0030 0.92720.0040 0.97890.0085 0.96330.0046 0.98920. 0.97130.0044 0.96050.0091 0.96810.0158 0.98930.0023 0.98920.0022 0.97550. 0.93490.0371 0.97430.0095 0.98900.0043 0.99360.0009 0.97210.0092 0.95850. 0.98010.0110 0.99090.0030 0.99350.0015 0.98490.0028 Kendall τ () solvation ΔG IL/water transfer ΔG (ternary-component (ternary-component solvation ΔG IL/water transfer ΔG AIonopedia (Galactica-125m) 0.8700.019 AIonopedia (Galactica-1.3b) 0.8630.019 AIonopedia (Qwen3-0.6b) 0.8660.031 AIonopedia (Gemma3-1b) 0.8700.014 MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.7880.017 0.8040.031 0.8480. 0.8470.022 0.7800.015 0.8600.017 0.8300.057 0.8610.016 0.8830. 0.8880.018 0.8950.020 0.8850.020 0.7720.040 0.8490.010 0.8800. 0.8740.015 0.8780.022 0.8510.027 0.8670.017 0.8870.020 split) 0.9440.002 0.9560.002 0.9620.002 0.9630.002 0.8160.009 0.8290. 0.9190.003 0.9460.003 0.8470.006 0.9410.006 0.9560.003 0.9470. split) 0.9100.008 0.9380.005 0.9440.002 0.9430.003 0.7980. 0.8470.010 0.9070.007 0.9310.003 0.9130.002 0.9080.010 0.9380. 0.9360.008 organic solvent/water transfer ΔG 0.9060.005 0.9140.004 0.9160. 0.9150.006 0.7800.015 0.8280.007 0.8840.004 0.8900.009 0.8970. 0.8770.013 0.8700.023 0.9110.007 Table 3. Comparison of AIonopedia with baselines under IL-based splits on the bulk property datasets. mass density (g/cm3) viscosity (mPas) (log scale) surface tension (mN/m) melting point (K) RMSE () 3.630.27 0.02720.0029 AIonopedia (Galactica-125m) 38.43. AIonopedia (Galactica-1.3b) 38.52.5 AIonopedia (Qwen3-0.6b) AIonopedia (Gemma3-1b) MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 38.93.5 39.63.0 44.43.6 48.24.0 43.43.2 39.73. 45.53.3 42.13.2 42.42.2 37.23.2 0.2490.018 (0.2493) 0.2490.020 (0.2492) 0.2500.017 0.2510.020 0.3680.022 0.3320. 0.2880.025 0.2680.018 0.5450.023 0.2670.016 0.2680.019 0.5070. 3.420.27 3.470.35 3.440.32 4.330.31 5.260.25 3.770. 3.500.44 4.590.37 3.590.13 3.630.34 3.510.20 Pearson () melting point viscosity (log scale) surface tension AIonopedia (Galactica-125m) 0.74700.0380 AIonopedia (Galactica-1.3b) 0.74760.0471 AIonopedia (Qwen3-0.6b) 0.74080.0613 AIonopedia (Gemma3-1b) 0.72890.0533 MLP MolCA T5chem 0.65370.0473 0.57280.0392 0.72020.0401 0.93930. 0.93920.0105 0.93890.0093 0.93780.0104 0.85750.0192 0.89490.0079 0.92790. 0.91940.0160 0.92720.0127 0.92420.0184 0.92610.0162 0.87900.0199 0.81870. 0.91850.0131 0.02670.0030 0.02660.0028 0.02640.0032 0.04270.0036 0.04990. 0.03230.0013 0.02800.0024 0.04170.0026 0.02890.0021 0.04680.0045 0.03230. mass density 0.98950.0021 0.98980.0021 0.98970.0021 0.98990.0024 0.97270. 0.96410.0058 0.98620.0022 Molinst SPMM LlaSMol PRESTO ILBERT 0.72540.0370 0.64350.0419 0.67670.0567 0.68060.0214 0.76200. AI for Greener Solvents 0.92650.0108 0.65930.0282 0.92710.0101 0.92700.0118 0.70340. 0.92100.0199 0.87420.0214 0.91820.0100 0.91560.0183 0.92140.0101 Kendall τ () melting point viscosity (log scale) surface tension AIonopedia (Galactica-125m) 0.5460.031 AIonopedia (Galactica-1.3b) 0.5430.051 0.8340.015 0.8380.016 0.8000.021 0.8070.014 AIonopedia (Qwen3-0.6b) 0.5480.048 0.8350.017 0.8040.022 AIonopedia (Gemma3-1b) 0.5360.049 0.8330. 0.8070.021 MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT (A) 0.4720.046 0.3730. 0.5160.046 0.5240.035 0.4400.049 0.4750.064 0.4880.023 0.5680. 0.7210.018 0.6620.021 0.7810.018 0.7910.021 0.7200.010 0.7870. 0.7920.011 0.7720.019 0.7080.018 0.7540.012 0.8150.012 0.8120. 0.4620.022 0.8090.015 0.8140.023 0.4970.016 (B) 0.98850. 0.97610.0019 0.98770.0022 0.96690.0099 0.98490.0023 mass density 0.9310. 0.9330.004 0.9340.005 (0.9337) 0.9340.005 (0.9336 0.8760. 0.8630.013 0.9180.005 0.9280.003 0.8840.004 0.9240.004 0.8890. 0.9100.004 Fig. 4. (A) Average performance ranks of the four AIonopedia models across all evaluated datasets. (B) Average performance ranks of the bestperforming variant, AIonopedia (Qwen30.6 b), compared with baselines across the same datasets. 4.3. System-specific benchmarks and comparison with traditional simulation baselines. Molecular simulations as classical computational approach have been extensively applied to study the AI for Greener Solvents bulk-phase properties and solvation behavior of ionic liquids, offering atomistic insights into their structural organization, transport mechanisms, and thermodynamic features.101-104 However, such all-atom simulations are often prohibitively slow, limited in their coverage of chemical space, and constrained by accessible time and length scales. Further, many experimentally measurable properties (e.g., viscosity and long-time relaxation dynamics) remain challenging to reproduce accurately within feasible simulation durations. To examine the performance of AIonopedia in more depth, we further compare the predictions of the property predictor on the solute-solvent interaction dataset against our MD simulations performed with GROMACS105. Because the simulation for an IL system typically requires several hours to days of time, running simulations across the entire set of property datasets was infeasible. Therefore, we confined the computational evaluation to five specific systems with varying chemical compositions and temperatures and their temperaturedependent subsets. Here, we adopted the ILbased split, kept the training hyperparameters identical to those used for the crossvalidation split, and report the results in Table 4, with also the aforementioned largelanguagemodel and traditional machinelearning baselines Overall, the computational method performs consistently across all datasets, whereas the other approaches vary markedly with the dataset split. For this method, the RMSE ranges from 0.524 to 0.897 kcal mol¹, all favorably lie within the chemical precision. Remarkable, most machinelearning models outperform the MD method, and our AIonopedia models again achieved the best performance on most metrics (e.g. with the RMSE as low as 0.060-0.464 kcal/mol for AIonopedia (Qwen3-0.6b)). Among fived studied systems, the ion pair [Quin8]+[TF₂N]- is absent as pair in the training set, but both constituent ions are well represented in other IL systems, so almost all learningbased models give near-perfect prediction on this system (with Pearson r>0.99, except r=0.97 for MLP and r=0.94 for MolCA). For the two other nitrogencontaining pairs, [BMIM]+[BETI]- and [EMIM]+[TF₂N]- with constituent ions appearing less frequently in the training data, several baselines show noticeable drop in accuracy. Conversely, the phosphonium cations and nonfluorinated anions that form the remaining two ILs are scarcely represented in the dataset. The anion in [P66614]+[L-Lact]- is entirely absent from the training data, and neither ion in [P4442]+[DEP]- appears at all, rendering both corresponding test sets strongly OOD. On these two most difficult systems our method exhibits pronounced advantage over all alternatives. SPMM, though weaker than our model on every other benchmark, attains comparable accuracy on the [P4442]+[DEP]- dataset. All remaining models demonstrate clear over-fitting. Especially ILBERT, which is previously the topranked model, performs poorly on [P4442]+[DEP]- and even worse on [P66614]+[LLact]-. In the latter case its RMSE climbs above 1 kcal mol¹, surpassing the accepted boundary for chemical accuracy106,107. To further benchmark AIonopedia models against the traditional method, we built small massdensity AI for Greener Solvents set for representing bulk properties and separate dataset of hydration ΔG. Experiments on the massdensity set also kept the cationsplit hyperparameters. For hydration ΔG, we created ten deduplicated virtual IL solvents per solute, drawn from the solute-solvent dataset. Each baseline used two separately trained models, one for solvation ΔG and one for transfer ΔG (IL/water). The baseline obtained hydration ΔG by subtraction, then averaged the ten solvent values. This protocol slightly underestimates performance variance but reduces smallsample bias and better matches realworld usage. On both tasks our model was best, or statistically tied for best, as shown in Table 5. Table 4. Comparison of AIonopedia with computational baseline and prior baselines on solvation ΔG datasets for fixed systems. RMSE(kcal/mol) () AIonopedia (Galactica-125m) AIonopedia (Galactica-1.3b) AIonopedia (Qwen3-0.6b) AIonopedia (Gemma3-1b) 298K [BMIM]+ [BETI]- 298K [EMIM]+ [TF2N]- 298K [P66614]+ [L-Lact]- 298K [QUIN8]+ [TF2N]- 328K [P4442]+ [DEP]- 328K 338K [QUIN8]+ [QUIN8]+ [TF2N]- [TF2N]- 348K [P4442]+ [DEP]- 0.3870.018 0.2590.005 0.3200.091 0.0880. 0.4410.042 0.0730.008 0.0750.006 0.5120.029 0.3800.030 0.2200. 0.3500.045 0.0600.003 0.3690.058 0.0510.003 0.0510.003 0.4570. 0.4380.032 0.2420.023 0.2810.094 0.0640.009 0.3610.053 0.0600. 0.0620.008 0.4640.029 0.4050.008 0.2300.022 0.3050.050 0.0620. 0.4330.060 0.0560.008 0.0540.010 0.4930.047 MD simulation 0.8060. 0.5240.042 0.7310.052 0.6820.068 0.6970.019 0.8970.045 0.5970. 0.5470.012 MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.5580.024 0.4050.014 0.5730. 0.2260.016 0.4510.046 0.2200.007 0.2200.007 0.5640.018 0.2800. 0.4050.125 0.7690.039 0.3720.021 0.5340.023 0.3400.025 0.3360. 0.6560.024 0.3270.017 0.2810.022 0.4290.061 0.1320.013 0.6390. 0.1250.008 0.1230.006 0.7500.029 0.3410.038 0.2270.039 0.7050. 0.0680.013 0.4430.136 0.0630.010 0.0640.008 0.5810.139 0.3580. 0.4450.050 0.4490.126 0.3090.030 0.3650.032 0.1590.028 0.2830. 0.5810.045 0.3410.037 0.2770.085 0.4120.133 0.0770.014 0.6480. 0.0690.014 0.0710.016 0.7460.061 0.4220.021 0.8090.024 0.5310. 0.0750.006 0.4690.072 0.0650.009 0.0640.008 0.5620.074 0.3550. 0.2610.048 1.2100.391 0.0860.009 0.5320.087 0.0620.007 0.0580. 0.6400.079 Pearson () AIonopedia (Galactica-125m) AIonopedia (Galactica-1.3b) AIonopedia (Qwen3-0.6b) AIonopedia (Gemma3-1b) 298K [BMIM]+ [BETI]- 298K [EMIM]+ [TF2N]- 298K [P66614]+ [L-Lact]- 298K [QUIN8]+ [TF2N]- 328K [P4442]+ [DEP]- 328K 338K [QUIN8]+ [QUIN8]+ [TF2N]- [TF2N]- 348K [P4442]+ [DEP]- 0.99520.0005 0.98320.0022 0.98150.0047 0.99620.0009 0.94500. 0.99690.0006 0.99690.0005 0.90180.0030 0.99460.0004 0.99040.0012 0.98440. 0.99820.0002 0.96640.0056 0.99840.0002 0.99840.0002 0.92500.0039 0.99430. 0.99080.0028 0.97180.0043 0.99870.0004 0.96850.0077 0.99850.0004 0.99840. 0.93000.0038 0.99490.0005 0.99050.0013 0.96550.0090 0.99860.0005 0.95740. 0.99860.0004 0.99860.0004 0.91800.0053 MD simulation 0.87100.0171 0.93090. 0.76840.0248 0.79570.0418 0.89400.0089 0.74950.0441 0.82310.0311 0.87610. MLP MolCA T5chem Molinst 0.94860.0026 0.95400. 0.81170.0232 0.97060.0034 0.90980.0093 0.97340.0018 0.97290.0019 0.84510. 0.98760.0029 0.96020.0275 0.74410.0310 0.93690.0066 0.90530.0057 0.94740. 0.94710.0059 0.82940.0032 0.99310.0008 0.97760.0047 0.94510.0195 0.99310. 0.93310.0037 0.99310.0007 0.99260.0009 0.87830.0032 0.99380.0008 0.98620. 0.85820.0164 0.99780.0008 0.93410.0093 0.99810.0009 0.99800.0007 0.87090. SPMM LlaSMol PRESTO ILBERT 0.99100.0017 0.98130. AI for Greener Solvents 0.99390.0011 0.93640.0247 0.97220.0069 0.99640.0008 0.99650.0008 0.90120. 0.99260.0011 0.97950.0122 0.95780.0120 0.99730.0003 0.93080.0112 0.99760. 0.99720.0012 0.87470.0071 0.99270.0010 0.78680.0132 0.88090.0256 0.99680. 0.92670.0085 0.99730.0008 0.99740.0007 0.86900.0120 0.99220.0019 0.98130. 0.87660.0478 0.99540.0009 0.93210.0097 0.99740.0005 0.99780.0006 0.89520. Kendall τ () AIonopedia (Galactica-125m) AIonopedia (Galactica-1.3b) AIonopedia (Qwen3-0.6b) AIonopedia (Gemma3-1b) 298K [BMIM]+ [BETI]- 298K [EMIM]+ [TF2N]- 298K [P66614]+ [L-Lact]- 298K [QUIN8]+ [TF2N]- 328K [P4442]+ [DEP]- 328K 338K [QUIN8]+ [QUIN8]+ [TF2N]- [TF2N]- 348K [P4442]+ [DEP]- 0.9450.002 0.9350.004 0.9070.027 0.9500.006 0.8130.010 0.9520. 0.9530.0061 0.7970.007 0.9460.002 0.9220.003 0.9050.014 0.9700. 0.8460.014 0.9660.006 0.9630.003 0.8310.010 0.9500.004 0.9430. 0.8600.012 0.9750.009 0.8500.017 0.9730.004 0.9650.010 0.8330. 0.9520.002 0.9330.009 0.8530.027 0.9700.007 0.8280.009 0.9750. 0.9670.004 0.8170.006 MD simulation 0.6780.030 0.7330.027 0.5740. 0.6200.053 0.7050.015 0.5550.045 0.6510.032 0.7020.011 MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.8490.004 0.8640.008 0.6170.025 0.8500.010 0.7530. 0.8730.005 0.8600.011 0.7240.003 0.9030.016 0.8750.032 0.5770. 0.8090.013 0.7530.010 0.8430.013 0.8330.011 0.7170.006 0.9310. 0.9000.006 0.7980.042 0.9330.008 0.7850.008 0.9350.005 0.9370. 0.7620.005 0.9370.008 0.9330.009 0.6610.009 0.9600.006 0.7870. 0.9660.001 0.9610.003 0.7570.018 0.9320.004 0.9410.003 0.8100. 0.9290.007 0.8760.014 0.9460.005 0.9540.005 0.8380.020 0.9370. 0.9330.009 0.8180.031 0.9500.008 0.7850.013 0.9580.007 0.9560. 0.7640.012 0.9410.009 0.8660.015 0.7200.034 0.9610.006 0.7850. 0.9540.008 0.9490.006 0.7650.012 0.9430.008 0.9410.004 0.6790. 0.9340.008 0.7860.010 0.9580.006 0.9540.009 0.7720.011 Table 5. Comparison of AIonopedia with computational baseline and prior baselines on small mass density dataset and hydration ΔG dataset. metric mass density hydration ΔG RMSE (g/cm3) Pearson Kendall τ RMSE (kcal/mol) Pearson Kendall τ AIonopedia (Galactica-125m) 0.01200. 0.99690.0005 0.9560.006 0.9660.027 0.94270.0030 0.9090.008 AIonopedia (Galactica-1.3b) 0.01100.0006 0.99750.0003 0.9620.006 0.8020.016 0.96110.0012 0.9400. AIonopedia (Qwen3-0.6b) 0.01060.0015 0.99740.0009 AIonopedia (Gemma3-1b) 0.01240.0008 0.99630. 0.9660.006 0.9662 0.9660.005 0.9659 0.7490.018 0.96720. 0.9480.002 0.8010.015 0.96270.0016 0.9320.003 MD simulation 0.06700. 0.98000.0002 0.8860.008 1.0010.005 0.94520.0005 0.7810.004 MLP MolCA T5chem Molinst SPMM LlaSMol PRESTO ILBERT 0.03850.0055 0.97470.0085 0.7690.060 1.3210.019 0.89400. 0.7370.006 0.03210.0039 0.97980.0022 0.9310.015 1.0350.191 0.93710. 0.8250.048 0.01320.0014 0.99630.0008 0.9470.005 1.0050.018 0.93820. 0.8970.005 0.01410.0014 0.99590.0005 0.9300.016 0.9690.037 0.95230. 0.9010.009 0.02990.0041 0.98550.0007 0.8570.005 0.8320.044 0.96100. 0.8930.005 0.01180.0006 0.99690.0005 0.9440.009 0.7270.084 0.96980. 0.9350.008 0.01560.0014 0.99500.0012 0.9480.011 1.0350.0261 0.93400. 0.8850.008 0.01930.0002 0.99250.0001 0.8430.006 0.7110.058 0.97520. 0.9330.006 4.4. Ablation study of the multi-modal LLM. AI for Greener Solvents To further assess the contribution of different components in our property predictor to the performance, we conducted an ablation study with the smallest model variant, AIonopedia (Galactica125m). The results are summarized in Table 6 where the ILbased split was adopted and all training hyperparameters kept identical. The default model attains the top score on most datasets. We first examined the contribution of the training losses. The earlier MLP baseline has showed that supervision from pseudolabels alone could not deliver adequate performance, so we evaluated the remaining loss terms here. Specifically, we removed the supervised loss and retained only the three BLIP2 losses, contrastive loss, GTG loss, and GTM loss. The results reveal that purely unsupervised graph-text semantic objective fails to capture some molecular information (e.g. the RMSE of solvation ΔG and IL/water transfer ΔG increase 22% and 38%, respectively), whereas adding physicochemicalproperty supervision does guide the model toward deeper understanding of the system. Next, we evaluated the impact of modality alignment. Completely omitting Phase 1 produced the largest loss in performance, the RMSE on the IL/water transfer ΔG dataset increases from 0.47 kcal/mol to 0.81 kal/mol, almost doubling. We also removed either the graph or the text modality separately. Both omissions impaired accuracy, but the decline was more pronounced when the text modality was excluded. Table 6. Performance metrics of the property predictor in the ablation study. solvation ΔG (kcal/mol) IL/water transfer ΔG (kcal/mol) melting point viscosity surface tension mass density solvent/water (K) (mPas) (mN/m) (g/cm3) transfer ΔG (kcal/mol) organic 0.3040.030 0.4730.100 38.43.0 0.2490.018 3.630.27 0.02720. 0.4150.028 RMSE AIonopedia (Galactica-125m) w/o supervised loss 0.3700. 0.6520.094 41.23.8 0.2640.017 3.850.27 0.02870.0025 0.4750. w/o pretrain 0.3620.036 0.8070.056 44.43.3 0.2870.011 4.290. 0.03130.0023 0.6540.024 w/o graph modal 0.3440.0810 0.4630.128 39.83. 0.2570.017 3.690.27 0.02730.0030 0.4260.031 w/o language modal 0.3790. 0.4850.410 40.93.2 0.2710.107 3.980.23 0.02980.0036 0.4380. Pearson solvation ΔG IL/water transfer ΔG melting point viscosity surface tension mass density solvent/water transfer ΔG organic AIonopedia (Galactica-125m) 0.96250.0056 0.98010.0104 0.74700.0380 0.93930.0080 0.91940. 0.98950.0021 0.98490.0025 w/o supervised loss 0.95020.0144 0.96920.0119 0.71010. 0.93340.0089 0.91180.0195 0.98870.0019 0.98080.0026 w/o pretrain 0.94960. 0.94670.0110 0.64060.0444 0.92150.0050 0.88820.0215 0.98670.0021 0.96500. w/o graph modal 0.95740.0156 0.98070.0112 0.72560.0463 0.93580.0088 0.91340. 0.98920.0023 0.98440.0025 w/o language modal 0.94410.0037 0.97660.0131 0.70150. 0.92620.0097 0.90080.0208 0.98700.0029 0.98240.0026 Kendall τ solvation ΔG IL/water transfer ΔG melting point viscosity surface tension mass density solvent/water transfer ΔG organic AIonopedia (Galactica-125m) 0.8700. 0.8830.016 0.5460.031 0.8340.015 0.8000.021 AI for Greener Solvents 0.9310. 0.9060.005 (0.9314) (0.9061) w/o supervised loss 0.8480.017 0.8710. 0.4960.046 0.8160.015 0.7860.026 0.9260.003 0.8920.005 w/o pretrain 0.8410.010 0.7960.019 0.4490.057 0.7980.014 0.7410.027 0.9180. 0.8470.005 w/o graph modal 0.8560.035 0.8850.020 0.5170.054 0.8270. 0.7900.024 0.9310.004 0.9060.004 (0.9312) (0.9056) w/o language modal 0.8090.013 0.8780.022 0.5000.040 0.8090.017 0.7580.024 0.9230. 0.8980.004 4.5. IL modification and screening for gas absorption: from literature calibration to wet-lab validation. Traditional IL design typically relies on the prior knowledge of domain experts. By defining family of structurally related ILs as the prior, researchers can confine exploration to smaller chemical space for targeted modification and optimization. Accordingly, we introduce practice-aligned workflow where AIonopedia refines specified IL via two representative strategies: anion replacement and cation side-chain engineering. In the former, we fix the cation and substitute candidate anions; in the latter, we fix the anion and systematically modify the cation side chain. We selected one targeted example for each to execute this workflow. During the process, AIonopedia iteratively performs reasoning and computation to verify whether its hypotheses are correct. To prevent cheating, the study ensured that no dataset leakage occurred and that online queries were disabled. The iteration budget was also set to five to restrict computational trials and probe the agents reasoning ability. For anion replacement, we started from [EMIM]+[SCN]- and tasked AIonopedia with optimizing CO2 absorption. The agent identified the anion [TCB]- within five iterations. The other anions explored along the way yielded solvation ΔG of CO2 (kcal/mol) calculated at 298 K, 1 atm, consistent with the corresponding absorption-capacity ranking: [SCN]- (0.5964) < [DCA]- (0.7336) < [TCM]- (1.3686) < [TF₂N]- (1.6346) < [TCB]- (1.7204)108-111. Accordingly, solvation ΔG provides thermodynamically motivated proxy for absorption capacity. For cation side-chain engineering, we used [EMIM]+[TF2N]- as the starting point to optimize NH3 uptake. With targeted prompting and few iterations, AIonopedia converged on [EtOHIM]+ as the preferred cation. The calculated solvation ΔG of NH3 for the other cationic variants also tracked the expected absorptioncapacity order: [EMIM]+ (1.8748) < [EtOHMIM]+ (1.9520) < [EIM]+ (1.9692) < [EtOHIM]+ (2.1151).112 While the workflow is effective for IL modification, dependence on prior knowledge constrains the agent and the traditional research paradigm, limiting the discovery of wholly new IL systems and applications. To assess how AIonopedia aids IL discovery in real wet lab settings, we defined an extremely rigorous task: the model must achieve zero-shot generalization to screen ILs for NH3 absorption. Unlike prior work such as ILBERT, which often validates using systems closely resembling ILs previously reported for the same AI for Greener Solvents application, we manually excluded all ILs reported for NH3 absorption and their close analogues from the data. As result, the model explores previously unexplored regions of IL chemical space. Candidate screening was guided by solvation ΔG minimization, which we previously showed to track absorption capacity. In contrast to traditional ILs with nitrogen-centered cations, we discovered the first IL with phosphorus-centered cations applicable to NH3 absorption without imposing priori family constraints, represented by [P4442]+ [DEP]-. Guided by the predictions, we synthesized this IL and evaluated its NH3 absorption performance using gravimetric method. Prior to testing, the IL was dried in vacuum oven at 70 for 24 h. Approximately 100 mg of IL was weighed into quartz crucible placed in the measurement cell, where an ionizing fan was used to eliminate electrostatic charges before the cell was sealed. At 100 and ambient pressure, the sample was purged with helium at 50 cm3/min for about 8 h, with repeated weighing until mass equilibrium, to determine the activated sample mass. Measurements were carried out at 25 with total gas flow of 50 cm3/min. During the absorption stage, the NH3 partial pressure was stepped from 5% to 95% in 5% increments. During desorption, it was stepped from 95% to 5% in 10% increments. Equilibrium at each step was defined by mass fluctuation within 0.1 mg, with an upper equilibration time of 100 min. The experimental results show that the ammonia absorption capacity increased with NH3 partial pressure and reached equilibrium at 95%, giving an equilibrium uptake of 1.80 mol/mol. Upon decreasing the NH3 partial pressure, the uptake did not decline, indicating strong interaction between the IL and NH3. Compared with literature data, this IL exhibits high absorption capacity. Notably, this phosphorus-centered IL features excellent structural tunability, offering ample scope for targeted performance optimization via cation sidechain engineering and anion replacement. The experimental setup and results are shown in Fig. 5 and Table 7. (A) Gas absorption working principle diagram absorption chamber 25 temperature sensor Constant temperature absorption chamber (B) Gases (B) Microbalance 0.00000 Vacuum chamber NH3 He Weighing hanging wire Gas pipelines MFC 2 MFC precision ﬂow meter Sample crucible 8 25 Constant temperature circulating water tank ) / m ( - ] [ + ] 2 4 4 4 [ k u 3 2 1. 1.2 0.8 0.4 0 0 Absorption Desorption 40 20 80 Gas concentration (Vi/V) 60 100 Fig. 5. (A) schematic of the multi-station gravimetric gas/vapor sorption instrument. (B) Ammonia AI for Greener Solvents absorptiondesorption isotherm of [P4442]+ [DEP]- at 298 K. Table 7. Reported absorption capacity of aprotic ILs ammonia absorbent. materials [P4442]+[DEP]- [C4C1IM]+[DBP]- [C4C1IM]+[DMP]- [C2C1IM]+[DEP]- [C2C1IM]+[DMP]- [C1C1IM]+[DMP]- [C2MIM]+[TF2N]- [C2MIM]+[BF4]- [C4MIM]+[BF4]- [C6MIM]+[BF4]- [C2MIM]+[SCN]- [C4MIM]+[SCN]- [C6MIM]+[SCN]- [C4MIM]+[TF2N]- [C4MIM]+[DCA]- [C4MMIM]+[TF2N]- [BMMIM]+[DCA]- [C4IM]+[SCN]- [C4IM]+[NO3]- [C2MIM]+[FAP]- [C2MIM]+[TFO]- T/ Pressure/kPa ammonia uptake (mol NH3/mol IL) 25 40 40 40 50 40 40 30 30 30 40 30 30 30 30 30 25 101 153 113 103 221 171 140 180 230 100 100 100 567 536 151 100 101 101 1.80 (this work) 0. 0.25113 0.20113 0.22113 0.35113 0.097114 0. 0.25114 0.37114 0.18115 0.19115 0.20115 0. 2.01117 1.60117 1.61117 2.60117 1.50117 0. 0.48118 5. Discussion In this paper, we develop AIonopedia, transformative LLM agent designed to address critical need in the field of ILs. AIonopedia delivers fully automated IL research workflow that spans from raw data acquisition through molecular screening and design. This agent transforms what used to be fragmented, manual process into seamless end-to-end pipeline, materially accelerating discovery for domain experts. AI for Greener Solvents At the core of AIonopedia is multimodal contrastive learning paradigm that unlocks the value of largescale unlabeled corpora while unifying three complementary molecular modalities for training: molecular graphs, SMILES sequences, and physicochemical descriptors. This design not only lifts overall performance beyond competing SOTA approaches, including prior IL-specific methods, chemical-domain unimodal LLMs and multimodal LLMs, but also dramatically strengthens OOD generalization. Consequently, our model can scale to broad species screening with confidence, whereas alternative methods struggle. In parallel with this modeling effort, we construct novel, large-scale labeled dataset for ILs. The resource contains more than double the number of pure IL species found in ILthermo, the largest preceding database, and includes the largest known collection of solute-solvent interaction data. This richer supervision expands coverage across chemical space and provides far more stringent test bed for evaluating ILs as next-generation solvents. Furthermore, we employed two complementary approaches to the design and discovery of ionic liquids. IL modification used iterative computation and reasoning to realize anion replacement and cation side-chain edits, with results validated on literature-reported datasets. In parallel, IL screening adopted hierarchical search architecture to balance chemical-space exploration with verification-oriented reliability. We assess the screening pipeline in challenging, application-driven wet-lab setting by posing NH3 absorption as zeroshot task and enforcing literature-agnostic protocol that excludes all previously reported ILs from the search space, thereby restricting exploration to completely new chemistries. Even under this extreme OOD regime, the method pinpoints the first IL with phosphorus-centered cations for NH3 absorption, underscoring its strong promise for ionic-liquid discovery. In conclusion, this work introduces AIonopedia not merely as tool, but as robust, validated framework that fundamentally accelerates the discovery pipeline for ILs. By successfully bridging the gap from automated data handling to SOTA multimodal prediction and real-world experimental validation, we have demonstrated powerful new paradigm for AI-driven materials science. The contributions presented here serve as the foundation for our ultimate goal of engineering fully autonomous agent capable of proposing hypotheses, analyzing data, and even directing automated experiments. This endeavor will fulfill the vision of AIonopedia as true AI-ion-wikipedia, which is comprehensive, dynamic, and interactive knowledge resource for the entire research community. References AI for Greener Solvents Dupont, J. et al. Ionic Liquids in Metal, Photo-, Electro-, and (Bio) Catalysis. Chemical Reviews 124, 5227-5420 (2024). https://doi.org:10.1021/acs.chemrev.3c00379 Numpilai, T., Pham, L. K. H. & Witoon, T. Advances in Ionic Liquid Technologies for CO2 Capture and Conversion: Comprehensive Review. Industrial & Engineering Chemistry Research 63, 19865-19915 (2024). https://doi.org:10.1021/acs.iecr.4c02072 Ovejero-Pérez, A., Nakasu, P. Y. S., Hopson, C., Costa, J. M. & Hallett, J. P. Challenges and opportunities on the utilisation of ionic liquid for biomass pretreatment and valorisation. npj Materials Sustainability 2, 7 (2024). https://doi.org:10.1038/s44296-024-00015-x Shivani, Thakur, R. C., Thakur, A., Sharma, A. & Sharma, R. Unravelling the prospects of electrolytes containing ionic liquids and deep eutectic solvents for next generation lithium batteries. Journal of Energy Chemistry 105, 482-500 (2025). https://doi.org:https://doi.org/10.1016/j.jechem.2025.01.060 Meshram, P., Agarwal, N. & Abhilash. review on assessment of ionic liquids in extraction of lithium, nickel, and cobalt vis-à-vis conventional methods. RSC Advances 15, 8321-8334 (2025). https://doi.org:10.1039/D4RA08429B Shamshina, J. L. & Rogers, R. D. Ionic Liquids: New Forms of Active Pharmaceutical Ingredients with Unique, Tunable Properties. Chemical Reviews 123, 11894-11953 (2023). https://doi.org:10.1021/acs.chemrev.3c00384 Alreshidi, M. A. et al. review on the evolution of ionic liquids: Sustainable synthesis, applications, and future prospects. Materials Today Sustainability 31, 101160 (2025). https://doi.org:https://doi.org/10.1016/j.mtsust.2025.101160 Beil, S. et al. Toward the Proactive Design of Sustainable Chemicals: Ionic Liquids as Prime Example. Chemical Reviews 121, 13132-13173 (2021). https://doi.org:10.1021/acs.chemrev.0c01265 Ong, H. C. et al. Recent advances in biodiesel production from agricultural products and microalgae using ionic liquids: Opportunities and challenges. Energy Conversion and Management 228, 113647 (2021). https://doi.org:https://doi.org/10.1016/j.enconman.2020.113647 Silva, W., Zanatta, M., Ferreira, A. S., Corvo, M. C. & Cabrita, E. J. Revisiting Ionic Liquid Structure-Property Relationship: Critical Analysis. International Journal of Molecular Sciences 21, 7745 (2020). Greer, A. J., Jacquemin, J. & Hardacre, C. Industrial Applications of Ionic Liquids. Molecules 25, 5207 (2020). Philippi, F. & Welton, T. Targeted modifications in ionic liquids from understanding to design. Physical Chemistry Chemical Physics 23, 6993-7021 (2021). https://doi.org:10.1039/D1CP00216C Bejaoui, Y. K. J. et al. Insights into structureproperty relationships in ionic liquids using cyclic perfluoroalkylsulfonylimides. Chemical Science 14, 2200-2214 (2023). https://doi.org:10.1039/D2SC06758G Tsuzuki, S., Matsumoto, H., Shinoda, W. & Mikami, M. Effects of conformational flexibility of alkyl chains of cations on diffusion of ions in ionic liquids. Physical Chemistry Chemical Physics 13, 5987-5993 (2011). https://doi.org:10.1039/C0CP02087G Tokuda, H., Hayamizu, K., Ishii, K., Susan, M. A. B. H. & Watanabe, M. Physicochemical Properties and Structures of Room Temperature Ionic Liquids. 2. Variation of Alkyl Chain Length in Imidazolium Cation. The Journal of Physical Chemistry 109, 6103-6110 (2005). https://doi.org:10.1021/jp044626d Tsuzuki, S. Factors Controlling the Diffusion of Ions in Ionic Liquids. ChemPhysChem 13, 1664-1670 (2012). https://doi.org:https://doi.org/10.1002/cphc.201100870 López-Martin, I., Burello, E., Davey, P. N., Seddon, K. R. & Rothenberg, G. Anion and Cation Effects on Imidazolium Salt Melting Points: Descriptor Modelling Study. ChemPhysChem 8, 690-695 (2007). https://doi.org:https://doi.org/10.1002/cphc.200600637 MacFarlane, D. R., Kar, M. & Pringle, J. M. Fundamentals of Ionic Liquids: From Chemistry to Applications. (Wiley-VCH Verlag GmbH & Co. KGaA, 2017). Abraham, M. H. Scales of solute hydrogen-bonding: their construction and application to physicochemical and biochemical processes. Chemical Society Reviews 22, 73-83 (1993). https://doi.org:10.1039/CS9932200073 Bradley, J.-C., Abraham, M. H., Acree, W. E. & Lang, A. S. I. D. Predicting Abraham model solvent coefficients. Chemistry Central Journal 9, 12 (2015). https://doi.org:10.1186/s13065-015-0085-4 2 3 4 6 7 8 9 10 11 13 14 15 16 17 19 20 AI for Greener Solvents Iftimie, R., Minary, P. & Tuckerman, M. E. Ab initio molecular dynamics: Concepts, recent developments, and future trends. Proceedings of the National Academy of Sciences 102, 6654-6659 (2005). https://doi.org:10.1073/pnas.0500193102 Cohen, A. J., Mori-Sánchez, P. & Yang, W. Challenges for Density Functional Theory. Chemical Reviews 112, 289320 (2012). https://doi.org:10.1021/cr200107z Durrant, J. D. & McCammon, J. A. Molecular dynamics simulations and drug discovery. BMC Biol 9, 71 (2011). https://doi.org:10.1186/1741-7007-9-71 Datta, R., Ramprasad, R. & Venkatram, S. Conductivity prediction model for ionic liquids using machine learning. The Journal of Chemical Physics 156, 214505 (2022). https://doi.org:10.1063/5.0089568 Zafer, A., Phu, N., Xiaoqi, C. & Kah Chun, L. Room temperature ionic liquids viscosity prediction from deeplearning models. Energy Materials 3, 300039 (2023). https://doi.org:10.20517/energymater.2023.38 Sheikhshoaei, A. H., Sanati, A. & Khoshsima, A. Deep learning models to predict CO2 solubility in imidazoliumbased ionic liquids. Scientific Reports 15, 26445 (2025). https://doi.org:10.1038/s41598-025-12004-8 Qu, M. et al. Machine learning-driven generation and screening of potential ionic liquids for cellulose dissolution. Journal of Cheminformatics 17, 78 (2025). https://doi.org:10.1186/s13321-025-01018-z Ali, M. et al. Prediction of CO2 solubility in Ionic liquids for CO2 capture using deep learning models. Scientific Reports 14, 14730 (2024). https://doi.org:10.1038/s41598-024-65499-y Zhang, K., Wu, J., Yoo, H. & Lee, Y. Machine Learning-based approach for Tailor-Made design of ionic Liquids: Application to CO2 capture. Separation and Purification Technology 275, 119117 (2021). https://doi.org:https://doi.org/10.1016/j.seppur.2021.119117 Zhang, R. et al. Modelling enzyme inhibition toxicity of ionic liquid from molecular structure via convolutional neural network model. SAR QSAR Environ Res 34, 789-803 (2023). https://doi.org:10.1080/1062936x.2023.2255517 Gugulothu, N., Mohan, M., Marland, M. R., Smith, J. C. & Kidder, M. K. Deep Learning Approaches for Predicting the Surface Tension of Ionic Liquids. Journal of Chemical Information and Modeling 65, 5856-5867 (2025). https://doi.org:10.1021/acs.jcim.5c00158 Baran, K. & Kloskowski, A. Graph Neural Networks and Structural Information on Ionic Liquids: Cheminformatics Study on Molecular Physicochemical Property Prediction. The Journal of Physical Chemistry 127, 10542-10555 (2023). https://doi.org:10.1021/acs.jpcb.3c05521 Feng, H., Qin, L., Zhang, B. & Zhou, J. Prediction and Interpretability of Melting Points of Ionic Liquids Using Graph Neural Networks. ACS Omega 9, 16016-16025 (2024). https://doi.org:10.1021/acsomega.3c09543 Rittig, J. G., Ben Hicham, K., Schweidtmann, A. M., Dahmen, M. & Mitsos, A. Graph neural networks for temperature-dependent activity coefficient prediction of solutes in ionic liquids. Computers & Chemical Engineering 171, 108153 (2023). https://doi.org:https://doi.org/10.1016/j.compchemeng.2023.108153 Koutsoukos, S., Philippi, F., Malaret, F. & Welton, T. review on machine learning algorithms for the ionic liquid chemical space. Chemical Science 12, 6820-6843 (2021). https://doi.org:10.1039/D1SC01000J Chen, G., Song, Z., Qi, Z. & Sundmacher, K. Generalizing property prediction of ionic liquids from limited labeled data: one-stop framework empowered by transfer learning. Digital Discovery 2, 591-601 (2023). https://doi.org:10.1039/D3DD00040K Beckner, W., Ashraf, C., Lee, J., Beck, D. A. C. & Pfaendtner, J. Continuous Molecular Representations of Ionic Liquids. The Journal of Physical Chemistry 124, 8347-8357 (2020). https://doi.org:10.1021/acs.jpcb.0c05938 David, L., Thakkar, A., Mercado, R. & Engkvist, O. Molecular representations in AI-driven drug discovery: review and practical guide. Journal of Cheminformatics 12, 56 (2020). https://doi.org:10.1186/s13321-020-00460-5 Schilling-Wilhelmi, M. et al. From text to insight: large language models for chemical data extraction. Chemical Society Reviews 54, 1125-1150 (2025). https://doi.org:10.1039/D4CS00913D Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 (2018). <https://ui.adsabs.harvard.edu/abs/2018arXiv181004805D>. OpenAI. Improving language understanding by generative pre-training, <https://openai.com/research/language21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 41 42 43 44 45 47 48 49 50 51 52 53 55 56 57 58 59 60 61 62 63 65 66 AI for Greener Solvents unsupervised/> (2018). Chithrananda, S., Grand, G. & Ramsundar, B. ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. arXiv:2010.09885 (2020). <https://ui.adsabs.harvard.edu/abs/2020arXiv201009885C>. Zhang, D. et al. ChemLLM: Chemical Large Language Model. arXiv:2402.06852 (2024). <https://ui.adsabs.harvard.edu/abs/2024arXiv240206852Z>. Edwards, C. et al. Translation between Molecules and Natural Language. arXiv:2204.11817 (2022). <https://ui.adsabs.harvard.edu/abs/2022arXiv220411817E>. Radford, A. et al. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020 (2021). <https://ui.adsabs.harvard.edu/abs/2021arXiv210300020R>. Zhou, Z., Li, Y., Hong, P. & Xu, H. Multimodal fusion with relational learning for molecular property prediction. Communications Chemistry 8, 200 (2025). https://doi.org:10.1038/s42004-025-01586-z Zhang, R., Yuan, R. & Tian, B. PointGAT: Quantum Chemical Property Prediction Model Integrating Graph Attention and 3D Geometry. Journal of Chemical Theory and Computation 20, 4115-4128 (2024). https://doi.org:10.1021/acs.jctc.3c01420 Schick, T. et al. Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv:2302.04761 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv230204761S>. Boiko, D. A., MacKnight, R., Kline, B. & Gomes, G. Autonomous chemical research with large language models. Nature 624, 570-578 (2023). https://doi.org:10.1038/s41586-023-06792-0 M. Bran, A. et al. Augmenting large language models with chemistry tools. Nature Machine Intelligence 6, 525-535 (2024). https://doi.org:10.1038/s42256-024-00832-8 Chen, K. et al. Chemist-X: Large Language Model-empowered Agent for Reaction Condition Recommendation in Chemical Synthesis. arXiv:2311.10776 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv231110776C>. OpenAI. Introducing GPT-5, <https://openai.com/index/introducing-gpt-5/> (2025). AiderAI. Aider Polyglot Coding Leaderboard, <https://aider.chat/docs/leaderboards/> (2025). Balunović, M., Dekoninck, J., Petrov, I., Jovanović, N. & Vechev, M. MathArena: Evaluating LLMs on Uncontaminated Math Competitions. arXiv:2505.23281 (2025). <https://ui.adsabs.harvard.edu/abs/2025arXiv250523281B>. Yue, X. et al. MMMU: Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. arXiv:2311.16502 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv231116502Y>. Yao, S. et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 (2022). <https://ui.adsabs.harvard.edu/abs/2022arXiv221003629Y>. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large Language Models are Zero-Shot Reasoners. arXiv:2205.11916 (2022). <https://ui.adsabs.harvard.edu/abs/2022arXiv220511916K>. Dev, S. Serper API, <https://serper.dev/> (2025). Kinney, R. M. et al. The Semantic Scholar Open Data Platform. ArXiv abs/2301.10140 (2023). Lewis, P. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 (2020). <https://ui.adsabs.harvard.edu/abs/2020arXiv200511401L>. Kim, S. et al. in Nucleic Acids Research Vol. 51 D1373-D1380 (2022). rdkit/rdkit: 2025_03_4 (Q1 2025) Release v. Release_2025_03_4 (Zenodo, 2025). Bender, A. & Glen, R. C. Molecular similarity: key technique in molecular informatics. Organic & Biomolecular Chemistry 2, 3204-3218 (2004). https://doi.org:10.1039/B409813G Li, J. et al. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems 34, 9694-9705 (2021). Yu, J. et al. CoCa: Contrastive Captioners are Image-Text Foundation Models. arXiv:2205.01917 (2022). <https://ui.adsabs.harvard.edu/abs/2022arXiv220501917Y>. Li, J., Li, D., Savarese, S. & Hoi, S. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. arXiv:2301.12597 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv230112597L>. 67 69 70 71 72 73 75 76 77 78 79 81 82 83 84 85 86 87 89 90 AI for Greener Solvents Liu, Z. et al. MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. arXiv:2310.12798 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv231012798L>. Chang, J. & Ye, J. C. Bidirectional generation of structure and properties through single molecular foundation model. Nature Communications 15, 2323 (2024). https://doi.org:10.1038/s41467-024-46440-3 Shi, Y. et al. Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification. arXiv:2009.03509 (2020). <https://ui.adsabs.harvard.edu/abs/2020arXiv200903509S>. Hu, E. J. et al. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 (2021). <https://ui.adsabs.harvard.edu/abs/2021arXiv210609685H>. Liu, H., Li, C., Li, Y. & Lee, Y. J. Improved Baselines with Visual Instruction Tuning. arXiv:2310.03744 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv231003744L>. He, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. Momentum Contrast for Unsupervised Visual Representation Learning. arXiv:1911.05722 (2019). <https://ui.adsabs.harvard.edu/abs/2019arXiv191105722H>. van den Oord, A., Li, Y. & Vinyals, O. Representation Learning with Contrastive Predictive Coding. arXiv:1807.03748 (2018). <https://ui.adsabs.harvard.edu/abs/2018arXiv180703748V>. Qiu, Y. et al. Large chemical language models for property prediction and high-throughput screening of ionic liquids. Digital Discovery 4, 1505-1517 (2025). https://doi.org:10.1039/D5DD00035A Irwin, J. J. et al. ZINC20A Free Ultralarge-Scale Chemical Database for Ligand Discovery. Journal of Chemical Information and Modeling 60, 6065-6073 (2020). https://doi.org:10.1021/acs.jcim.0c00675 Dong, Q. et al. ILThermo: Free-Access Web Database for Thermodynamic Properties of Ionic Liquids. Journal of Chemical & Engineering Data 52, 1151-1159 (2007). https://doi.org:10.1021/je700171f Ertl, P., Rohde, B. & Selzer, P. Fast Calculation of Molecular Polar Surface Area as Sum of Fragment-Based Contributions and Its Application to the Prediction of Drug Transport Properties. Journal of Medicinal Chemistry 43, 3714-3717 (2000). https://doi.org:10.1021/jm000942e Wildman, S. A. & Crippen, G. M. Prediction of Physicochemical Parameters by Atomic Contributions. Journal of Chemical Information and Computer Sciences 39, 868-873 (1999). https://doi.org:10.1021/ci990307l Hall, L. H. & Kier, L. B. in Reviews in Computational Chemistry Reviews in Computational Chemistry 367-422 (1991). Balaban, A. T. Highly discriminating distance-based topological index. Chemical Physics Letters 89, 399-404 (1982). https://doi.org:https://doi.org/10.1016/0009-2614(82)80009-2 Bertz, S. H. The first general index of molecular complexity. Journal of the American Chemical Society 103, 35993601 (1981). https://doi.org:10.1021/ja00402a071 Dalke, A. The chemfp project. Journal of Cheminformatics 11, 76 (2019). https://doi.org:10.1186/s13321-0190398-8 Niedermeyer, H., Hallett, J. P., Villar-Garcia, I. J., Hunt, P. A. & Welton, T. Mixtures of ionic liquids. Chemical Society Reviews 41, 7780-7802 (2012). https://doi.org:10.1039/C2CS35177C Comanici, G. et al. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv:2507.06261 (2025). <https://ui.adsabs.harvard.edu/abs/2025arXiv250706261C>. Claude Sonnet 4.5 (Anthropic, 2025). DeepSeek-AI. DeepSeek-R1-0528 Release, <https://api-docs.deepseek.com/news/news250528> (2025). DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 (2025). <https://ui.adsabs.harvard.edu/abs/2025arXiv250112948D>. Taylor, R. et al. Galactica: Large Language Model for Science. arXiv:2211.09085 (2022). <https://ui.adsabs.harvard.edu/abs/2022arXiv221109085T>. Yang, A. et al. Qwen3 Technical Report. arXiv:2505.09388 (2025). <https://ui.adsabs.harvard.edu/abs/2025arXiv250509388Y>. Team, G. et al. Gemma 3 Technical Report. arXiv:2503.19786 (2025). <https://ui.adsabs.harvard.edu/abs/2025arXiv250319786G>. 91 93 94 95 96 97 99 100 101 102 AI for Greener Solvents Racki, A. & Paduszyński, K. Recent Advances in the Modeling of Ionic Liquids Using Artificial Neural Networks. Journal of Chemical Information and Modeling 65, 3161-3175 (2025). https://doi.org:10.1021/acs.jcim.4c02364 Cao, H. et al. PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes. arXiv:2406.13193 (2024). <https://ui.adsabs.harvard.edu/abs/2024arXiv240613193C>. Yu, B., Baker, F. N., Chen, Z., Ning, X. & Sun, H. LlaSMol: Advancing Large Language Models for Chemistry with Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset. arXiv:2402.09391 (2024). <https://ui.adsabs.harvard.edu/abs/2024arXiv240209391Y>. Liu, Y. et al. RoBERTa: Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 (2019). <https://ui.adsabs.harvard.edu/abs/2019arXiv190711692L>. Christofidellis, D. et al. Unifying Molecular and Textual Representations via Multi-task Language Modelling. arXiv:2301.12586 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv230112586C>. Raffel, C. et al. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. arXiv:1910.10683 (2019). <https://ui.adsabs.harvard.edu/abs/2019arXiv191010683R>. Fang, Y. et al. Mol-Instructions: Large-Scale Biomolecular Instruction Dataset for Large Language Models. arXiv:2306.08018 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv230608018F>. Dubey, A. et al. The Llama 3 Herd of Models. arXiv:2407.21783 (2024). <https://ui.adsabs.harvard.edu/abs/2024arXiv240721783D>. Jiang, A. Q. et al. Mistral 7B. arXiv:2310.06825 (2023). <https://ui.adsabs.harvard.edu/abs/2023arXiv231006825J>. LMSYS. FastChat: We released Vicuna v1.5 based on Llama 2 with 4K and 16K context lengths. (2023). <https://github.com/lm-sys/FastChat>. Ishii, Y. & Matubayasi, N. Self-Consistent Scheme Combining MD and Order-N DFT Methods: An Improved Set of Nonpolarizable Force Fields for Ionic Liquids. Journal of Chemical Theory and Computation 16, 651-665 (2020). https://doi.org:10.1021/acs.jctc.9b00793 Ishii, Y., Matubayasi, N. & Washizu, H. Nonpolarizable Force Fields through the Self-Consistent Modeling Scheme with MD and DFT Methods: From Ionic Liquids to Self-Assembled Ionic Liquid Crystals. The Journal of Physical Chemistry 126, 4611-4622 (2022). https://doi.org:10.1021/acs.jpcb.2c 103 Mangin, T., Schurhammer, R. & Wipff, G. LiquidLiquid Extraction of the Eu (III) Cation by BTP Ligands into Ionic Liquids: Interfacial Features and Extraction Mechanisms Investigated by MD Simulations. J. Phys. Chem. (2022). 105 104 Wang, X. et al. Multi-temperature charge scaling of ionic solvents: Disparate responses of thermodynamic properties. J. Mol. Liq., 125445 (2024). https://doi.org:https://doi.org/10.1016/j.molliq.2024.125445 Abraham, M. J. et al. GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers. SoftwareX 1-2, 19-25 (2015). https://doi.org:https://doi.org/10.1016/j.softx.2015.06.001 Pople, J. A. Quantum Chemical Models (Nobel Lecture). Angewandte Chemie International Edition 38, 1894-1902 (1999). https://doi.org:https://doi.org/10.1002/(SICI)1521-3773(19990712)38:13/14<1894::AIDANIE1894>3.0.CO;2-H 106 107 Martin, J. M. L. & de Oliveira, G. Towards standard methods for benchmark quality ab initio thermochemistry W1 and W2 theory. Journal of Chemical Physics 111, 1843-1856 (1999). https://doi.org:10.1063/1.479454 Torralba-Calleja, E., Skinner, J. & Gutiérrez-Tauste, D. CO2 Capture in Ionic Liquids: Review of Solubilities and Experimental Methods. Journal of Chemistry 2013, 473584 (2013). https://doi.org:https://doi.org/10.1155/2013/473584 Kim, J. E., Kim, H. J. & Lim, J. S. Solubility of CO2 in ionic liquids containing cyanide anions: [c2mim][SCN], [c2mim][N(CN)2], [c2mim][C(CN)3]. Fluid Phase Equilibria 367, 151-158 (2014). https://doi.org:https://doi.org/10.1016/j.fluid.2014.01.042 Carvalho, P. J., Kurnia, K. A. & Coutinho, J. A. P. Dispelling some myths about the CO2 solubility in ionic liquids. Physical Chemistry Chemical Physics 18, 14757-14771 (2016). https://doi.org:10.1039/C6CP01896C Safarov, J. et al. Carbon dioxide solubility in 1-butyl-3-methylimidazolium-bis(trifluormethylsulfonyl)imide over 108 109 110 111 AI for Greener Solvents wide range of temperatures and pressures. The Journal of Chemical Thermodynamics 67, 181-189 (2013). https://doi.org:https://doi.org/10.1016/j.jct.2013.08.008 Jiang, H.-Y. et al. Advanced Materials for NH3 Capture: Interaction Sites and Transport Pathways. Nano-Micro Letters 16, 228 (2024). https://doi.org:10.1007/s40820-024-01425-1 Zhu, X. et al. Efficient absorption of ammonia with dialkylphosphate-based ionic liquids. New Journal of Chemistry 45, 20432-20440 (2021). https://doi.org:10.1039/D1NJ02018H Yokozeki, A. & Shiflett, M. B. Ammonia Solubilities in Room-Temperature Ionic Liquids. Industrial & Engineering Chemistry Research 46, 1605-1610 (2007). https://doi.org:10.1021/ie061260d Zeng, S. et al. Efficient and reversible absorption of ammonia by cobalt ionic liquids through Lewis acidbase and cooperative hydrogen bond interactions. Green Chemistry 20, 2075-2083 (2018). https://doi.org:10.1039/C8GC00215K Shang, D. et al. Protic ionic liquid [Bim][NTf2] with strong hydrogen bond donating ability for highly efficient ammonia absorption. Green Chemistry 19, 937-945 (2017). https://doi.org:10.1039/C6GC03026B Shang, D. et al. Enhanced NH3 capture by imidazolium-based protic ionic liquids with different anions and cation substituents. Journal of Chemical Technology & Biotechnology 93, 1228-1236 (2018). https://doi.org:https://doi.org/10.1002/jctb. 112 113 114 115 116 118 Makino, T. & Kanakubo, M. NH3 absorption in Brønsted acidic imidazoliumand ammonium-based ionic liquids. New Journal of Chemistry 44, 20665-20675 (2020). https://doi.org:10.1039/D0NJ04743K"
        }
    ],
    "affiliations": [
        "College of Chemistry and Molecular Engineering, Peking University",
        "Faculty of Synthetic Biology, Shenzhen University of Advanced Technology",
        "Henan Key Laboratory of Green Chemistry, Collaborative Innovation Center of Henan Province for Green Manufacturing of Fine Chemicals, Key Laboratory of Green Chemical Media and Reactions, Ministry of Education, School of Chemistry and Chemical Engineering, Henan Normal University",
        "School of Computer Science, Shanghai Jiao Tong University",
        "Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning and NYU-ECNU Center for Computational Chemistry, NYU-Shanghai"
    ]
}