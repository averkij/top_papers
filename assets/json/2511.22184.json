{
    "paper_title": "Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation",
    "authors": [
        "Daniel Sungho Jung",
        "Kyoung Mu Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released."
        },
        {
            "title": "Start",
            "content": "Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation Daniel Sungho Jung1 Kyoung Mu Lee1,2 1IPAI, 2Dept. of ECE & ASRI, Seoul National University, Korea {dqj5182, kyoungmu}@snu.ac.kr 5 2 0 2 7 ] . [ 1 4 8 1 2 2 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Foot contact plays critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce ground feature extractor that captures ground properties based on spatial context. As result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released. 1. Introduction Human movement and balance fundamentally depend on how the feet interact with the surrounding environment. Every step, shift in posture, or change in direction involves complex physical exchanges between the foot and the ground or an object. Understanding these interactions is essential for interpreting human motion, maintaining stability, and modeling realistic physical behavior. By accurately capturing where the foot establishes contact with the environment, systems can reason about human interaction dynamics from visual input, enabling more realistic and physically consistent human behavior perception. Despite its importance, existing works predominantly simplify foot contact into joint-level contact [32, 39, 43, 52, 60, 61] and often rely on geometric heuristics such as zerovelocity constraints [43, 61]. These formulations fail to capture the dense and spatially distributed foot contact, where the contact region is distributed across multiple fine-grained regions of the foot. Although several dense body contact estimation methods [16, 18, 45] have been proposed, their foot contact predictions remain inaccurate. Recently, study on hand contact [21] showed that dedicated dense contact estimation model for specific body part can outperform general dense body contact models. Motivated by this, we aim to develop dedicated dense foot contact estimation model trained on large-scale dataset to advance the understanding of human movement and interaction. Estimating dense foot contact is particularly challenging due to two crucial factors: appearance diversity and ground ambiguity. In the real world, feet are typically covered by shoes, which exhibit immense variation in color, texture, material, and style. Such diversity introduces spurious visual factors that correlate with contact patterns in the training data but are unrelated to the true physical interaction, often causing models to rely on misleading appearance cues rather than geometric reasoning. For instance, in training dataset, individuals wearing sneakers may frequently perform actions such as abrupt walking or skateboarding, leading the model to associate specific shoe styles with certain contact patterns. This bias can cause the model to overfit to appearance-dependent correlations and perform poorly when encountering unseen foot actions for the specific shoe type. At the same time, ground surfaces such as carpet, asphalt, or polished floors often have weak or repetitive textures, providing limited visual cues for inferring contact regions. Yet, understanding the ground is critical for footground contact estimation, since contact inherently occurs along the direction parallel to the surface. Without explicit reasoning about the ground geometry, models fail to 1 identify the support level and consequently produce inaccurate contact predictions. These difficulties are further compounded by occlusion, viewpoint changes, and illumination variation, all of which highlight the need for representations that capture geometric and physical context rather than superficial ground appearance. Therefore, building robust dense foot contact estimation model requires both strong shoe style invariance and grounded understanding of the physical surface. To address these issues, we introduce FECO, framework for dense foot contact estimation that learns shoe style-invariant and ground-aware representations from single image. FECO begins with low-level style randomization stage using progressive random convolutions [7] to eliminate reliance on local and low-level texture statistics. We then perform shoe stylecontent randomization using the external shoe image dataset UT Zappos50K [54]. In this process, adversarial adapters in the shoe content randomization branch enforce invariance to shoe style, while the shoe style randomization branch perturbs shoe appearances during training to expose the model to diverse visual styles. This dual randomization strategy effectively disentangles style from content, enabling the model to focus on structural cues relevant to dense foot contact and improving robustness to unseen shoe types and materials. Complementing style-invariant learning, FECO explicitly models the ground through pixel height maps [41] and ground normals. These capture geometric and spatial cues that correlate with ground, even for visually ambiguous surfaces. As result, FECO achieves state-of-the-art performance across diverse shoe and ground appearance with its dedicated shoe style-invariant and ground-aware learning for dense foot contact estimation. Our key contributions are as follows: We introduce FECO, novel framework for dense foot contact estimation from single image that explicitly targets shoe style diversity issue and foot-ground reasoning issue and allows robust contact prediction. To mitigate shoe style diversity issue, we propose shoe stylecontent randomization, which enforces styleinvariant yet content-preserving features using an external shoe dataset for robustness towards shoe style diversity. To reason foot-ground contact, we present ground-aware learning that encodes geometric properties of the ground with supervision from pixel height maps and ground normals, enhancing foot-ground contact reasoning. In the end, FECO, the first method for dedicated dense foot contact estimation demonstrates strong performance across diverse benchmarks. 2. Related works Foot contact estimation. Foot contact estimation has been studied through two main approaches, joint-level foot conTable 1. Dataset configuration. We leverage 10 datasets with diverse foot contact. Dataset Interaction Label # of images PROX [15] BEHAVE [2] InterCap [19] EgoBody [56] RICH [18] MOYO [46] Hi4D [53] MMVP [55] MotionPro [40] Scene Ground + Object Ground + Object Scene Scene Ground Ground + Body Ground Ground 3D Mesh 3D Mesh 3D Mesh 3D Mesh 3D Mesh Pressure Mat 3D Mesh Insole Pressure Mat 0.2K 45K 61K 220K 540K 560K 11K 44K 12.4M COFE dataset Scene 2D Keypoint 31K tact estimation and dense foot contact estimation as part of dense human body contact estimation. In the joint-level setting, Footskate Reducer [61] formulated foot contact using zero-velocity constraint, where foot joint is considered to be in contact if its 2D displacement between consecutive frames is below threshold. HuMoR [39] jointly trained human-ground contact and 3D human motion with groundtruth joint-level contact extracted with fitted ground and 3D human joints. UnderPressure [32] presented framework that predicts vertical ground reaction forces from sequences of 3D joints to extract foot joint contact predictions. PIP [52] estimated and utilized foot-ground contact as intermediate prediction for requiring ground reaction force. Foot Stabilization [60] introduced pseudo-GT annotator for foot-ground contacts by thresholding the distance between SMPL meshes and ground plane. WHAM [43] utilized the zero-velocity constraint to extract foot contact, which serves as main cue for refining 3D global trajectory. In the dense contact setting, POSA [16] learned dense contact by conditioning 3D vertex position on conditional variational autoencoder (cVAE) [44]. BSTRO [18] directly estimated dense contact from visual input through Transformer-based architecture [10]. DECO [45] introduced novel annotation technique for labeling contact on in-the-wild images. Reducing style bias for generalization. Style is arguably one of the core representations that expresses an image. In early works, image style has been widely studied in task of image style transfer [14] that separates and recombines lowlevel image style and content to new images using Gram matrix [13]. Motivated by the literature, StyleGAN [23] explored the GAN-based architecture that learns to effectively separate, control, and interpolate on high-level style attributes when generating an image. Moving on to the discriminative tasks, there have been extensive studies on how to effectively handle image style to improve performance. BIN [33] leveraged Batch Normalization [20] and Instance Normalization [47] with gating mechanism to learn which style to use across different task selectively from individual feature maps. SagNets [34] built two separate networks 2 of content-biased network and style-biased network to learn content while adversarially learn to reduce dependency on low-level style with style and content randomization. RandConv [49] proposed to construct infinite number of new domains by applying random convolution filter to image before feeding to image encoder, which allows training on random local texture. ABA [6] trained Bayesian Neural Network to augment image that serves as the most effective adversarial example for classifier to boost domain generalization. StyDeSty [25] introduced stylization and destylization module that builds domain-invariant representation to guarantee the alignment between augmented domain and target domain. To use diverse high-level and low-level shoe styles from UT Zappos50K [54] dataset, we employ variant of SagNets for our shoe style-invariant learning. Ground representation. Understanding ground information has been studied in the field of autonomous driving [4, 31, 37, 59] and depth estimation [4, 31, 51] to use ground as reference geometry for overall prediction. Mono3D [5] set heuristic prior that objects are always on the ground and place proposals for 3D bounding box on the ground. Ground-Aware [26] proposed ground-aware convolution module that utilizes each pixels prior depth value and features from pixels below the target pixel to guide ground-based reasoning. MonoGround [37] assumed that the bottom of objects 3D bounding box as ground plane and leveraged the information to extract projected dense ground depth. FGTO [31] also made assumption that dynamic objects tend to be in contact with the ground, and proposed GDS loss that aligns depth of the dynamic objects and ground in self-supervised manner. GroCo [4] proposed to predict accurate ground attention map that is utilized to guide refinement of predicted depth map. However, most of the works relied on pre-determined camera extrinsic from the vehicle to extract ground information. In the literature of shadow generation, novel representation called Pixel Height [41] was introduced to model correlation between object and ground. The Pixel Height representation was also proved effective on light effect generation by PixHt-Lab [42] where the paper utilized pixel height map for object and ground to reconstruct 3D model. Recently, ORG [29] extended the use of Pixel Height to encode object-ground relationship and make the reconstructed 3D geometry to be aligned with ground. Our FECO also extends this literature of Pixel Height but on foot contact estimation task for learning foot-ground interaction in finegrained pixel-level representation. 3. Method: FECO Figure 1 shows the overall pipeline of FECO, which consists of low-level style randomization, shoe stylecontent randomization, ground feature learning, spatial attention, and foot contact decoder. The network is trained with three synchronized inputs per sample, namely one clean image and two low-level style randomized images, and we train all modules in an end-to-end manner. 3.1. Low-Level Style Randomization Given an RGB image R3HW , where and represent the spatial resolution, we first construct three separate images that consists of one clean image I(0) lr and two auglr , I(2) mented images I(1) lr produced by Pro-RandConv [7]. Following Pro-RandConv, we randomly sample the convolution weights w, the deformable convolution offset p, and the affine parameters γ and β independently for each of the two augmentation for producing images I(1) lr . Then, using the sampled parameters, we apply deformable convolution followed by per-sample, per-channel standardization with Instance Normalization [47]. Afterward, an affine transformation with parameters γ and β is applied, followed by hyperbolic tangent, which ultimately produces the lowlevel style randomized images I(k) for {1, 2}. As lr result, we obtain one clean image and two low-level style randomized images, which we denote collectively as I(k) lr for {0, 1, 2}. During training, all three images are processed with identical operations, while at inference only the clean image is considered. For clarity in the following sections, we simply write Ilr to represent the images. lr , I(2) 3.2. Shoe Style-Content Randomization The shoe style-content randomization is designed to make FECO robust on random shoe style and make it focus more on content of the input image. We split into two pipeline to conduct shoe style randomization and shoe content randomization that each randomize shoe style during the training of dense foot contact estimation, and conduct adversarial training on shoe content-randomized feature. Before conducting shoe style-content randomization, we first resize the input image Ilr with bilinear interpolation and extract an image feature Rchw with Vision Transformer (ViT) [11] from resized image Ilr R3224224. foot segmentation mask Mf Rhw is predicted from image feature using DPT decoder [38]. The foot mask Mf is used for foot localized operations. random shoe image Is from dedicated shoe dataset [54] is also encoded with frozen ViT backbone to obtain shoe feature Fs Rchw. Unlike SagNets [34], which select style source from other samples within mini-batch, our approach uses an external shoe dataset to ensure diverse and independent style sources. Shoe content randomization. We aim to construct shoe style biased feature that preserves the content of the shoe feature Fs while adopting the style of the input feature F. To stabilize adversarial training, we introduce two adapters Aprev and Aafter, each implemented as 3 3 convolution with zero-initialized weights and scaled by learnable parameter γ initialized to 0.02. In residual form, an adapter 3 Figure 1. Overall pipeline of FECO. Our method first applies low-level style randomization on input image and encodes it into image feature using ViT backbone. From image feature, shoe style and shoe content randomization are performed with random shoe images from the UT Zappos50K [54] dataset to produce shoe style-invariant feature. This feature is then processed by ground feature encoder to extract ground feature, which is used to predict pixel height map and ground normal. Finally, the ground feature and shoe style-invariant feature are fused to form contact feature, which is decoded to produce the final foot contact prediction. operates as statistics are defined as A(X) = + γ ϕ(X), (1) where ϕ denotes the convolutional mapping. We apply Aprev to both input and shoe features, yielding = Aprev(F) and = Aprev(Fs). The channel-wise statistics are then computed as µ = µ(F), = µ(F µ s), σ = σ(F), = σ(F σ s). (2) Using (µ, σ) as the target style, we normalize the shoe feas, σ ture with its own statistics (µ s) and re-scale it as Fici = Aafter (cid:18) σ µ σ (cid:19) , + µ (3) where Fici is input content invariant feature. The resulting feature Fici preserves the content of the external shoe feature while expressing it in the style of the input. This content-randomized branch is explicitly used for adversarial training, ensuring that the final predictor does not overfit to style cues from the input image. Shoe style randomization. In parallel, we construct complementary shoe style-invariant representation. Using two additional adapters for alignment, we apply Aprev to both input and shoe features, yielding = Aprev(F) and = Aprev(Fs). random interpolation weight α is drawn from uniform distribution, and the interpolated 4 ˆµ = α µ(F) + (1 α) µ(F ˆσ = α σ(F) + (1 α) σ(F s), s). (4) AdaIN is then applied to with these interpolated statistics, and Aafter refines the results, producing the shoe styleinvariant representation (cid:18) Fssi = Aafter ˆσ µ(F) σ(F) (cid:19) + ˆµ , (5) where Fssi is shoe style-invariant feature. In the end, this branch incorporates style from external shoe image features to accomplish shoe style randomization on input image feature during training. 3.3. Ground Feature Learning To enhance the models understanding of the ground, the most frequent contact surface for the foot, we introduce ground feature learning with two supervisory signals: pixel height maps PH and ground normals Ng. This learning is applied in parallel to both input content invariant features Fici and shoe style-invariant features Fssi. Ground feature encoding. ground feature encoder Encg processes each randomized feature independently, producing multi-level representations {Fici {Fssi g,l}4 g,l}4 l=1 = Encg(Fici), l=1 = Encg(Fssi). (6) Each encoder level consists of 3 3 convolution with ReLU activation, followed by 1 1 convolution with another ReLU. The multi-level features are designed to capture fine-grained information for pixel height map decoding. For ground normal decoding, we only utilize the finallevel feature due to its global nature. Pixel height decoding. From the multi-level ground features, DPT decoder [38] predicts dense pixel height maps, the contact features Fici = Acontact(Fici = Acontact(Fssi Fssi fuse), fuse). (11) This attention-based fusion allows the network to adaptively combine information from the ground and randomized features when constructing the contact representations Fici and Fssi . PHici = Decph({Fici PHssi = Decph({Fssi g,l}4 g,l}4 l=1), l=1). (7) 3.5. Foot Contact Decoder The decoder head outputs single channel per spatial location, and predictions are upsampled to the input resolution. Motivated by depth scaling factor from Depth Anything [50], we scale the predicted height maps by the maximum side length = max(H, ) of the input image so that the outputs are expressed in pixel units. Ground normal decoding. The ground normal in camera coordinates is predicted from the final-level ground features (Fici g,4). To prevent shortcut learning from the orientation of the foot, we suppress the foot region using the foot segmentation mask Mf , yielding g,4 or Fssi Fm,ici Fm,ssi = Fici = Fssi g,4 (1 Mf ), g,4 (1 Mf ). (8) Our ground normal decoder Decgn applies global average pooling, two fully connected layers with hidden dimension of 128, tanh activation, and ℓ2 normalization. Applying it to each branch gives We follow HACO [21] and adopt Transformer-based network, that receives contact token together with the image feature to predict dense contact, as foot contact decoder. The decoder processes each contact feature Fici and Fssi through consecutive self-attention and cross-attention layers and outputs dense foot contact logits after added with learnable contact initialization. Formally, the contact decoder Decc maps the fused contact features to vertex-level logits, Cici = Decc(Fici ), Cssi = Decc(Fssi ), (12) where Cici, Cssi RV and = 265 is the number of foot vertices. The logits are then passed through sigmoid function to obtain contact probabilities, ˆCici = σ(Cici), ˆCssi = σ(Cssi). (13) Compared to HACO, which predicts contact for the MANO hand mesh with 778 vertices, our decoder adapts the output dimension to the foot mesh with 265 vertices and employs foot-specific regressors for supervision. Nici Nssi = Decgn(Fm,ici ), = Decgn(Fm,ssi ). g (9) 3.6. Final Outputs and Loss Functions which are unit-length ground normal vectors. 3.4. Spatial Attention and Fssi We fuse the randomized features (Fici and Fssi) with their corresponding ground features (Fici ) using spatial attention module. For each randomization branch, the randomized and ground features are concatenated along the channel dimension and passed through 3 3 convolution to reduce the channel dimension to 256, followed by ReLU activation and dropout layer with rate 0.2. subsequent 1 1 convolution outputs two logits, which are normalized with channel-wise softmax to produce spatially varying weights (wg, wr). The fused features are then computed as Fici Fssi fuse = wg Fici fuse = wg Fssi + wr Fici, + wr Fssi. (10) Finally, 1 1 convolution with ReLU, serving as contact adapter Acontact, transforms the fused representations into 5 Multi-level outputs. From the decoder we obtain vertexlevel logits for both branches, Cici, Cssi RV , where = 265 refers to number of foot vertices. Following HACO [21], these logits are projected to coarser levels using regressors {J vi }N i=1, Cici Cssi = vi = vi Cici, Cssi, = 1, . . . , N, = 1, . . . , N. (14) where = 3 and vi {V, 11, 3}. To obtain the contact probabilities, we apply sigmoid after projection: ˆCici = σ(Cici ), ˆCssi = σ(Cssi ). (15) For the full mesh (v1 = ), this corresponds to the foot region of SMPL-X [36] human body mesh. For v2 = 11, we partition the foot into notable regions (five toes, heel, front, bottom, left, right, back) and use region means to define keypoints. For v3 = 3, we follow OpenPose Human Foot Keypoints [3]. As there exists no official regressor that Figure 2. COFE Dataset. We manually annotate joint-level foot contact for samples in OpenPose [3], InstaVariety [22], PennAction [57], and MPII [1] datasets. In the visualization, black indicates contacting joints and white represents non-contacting joints. projects SMPL-X to OpenPose foot keypoints, we map big toe, small toe, and heel from our 11-joint definition to build SMPL-X to OpenPose regressor, which enables direct supervision with our COFE dataset introduced in Section 4. Loss functions. We train FECO end-to-end with the following total loss objective: = Lmain + Lstyle + Lstyle-adv + Lmask + Lground. (16) The main loss Lmain is binary cross-entropy loss applied to the multi-level predictions of the main branch. The style loss Lstyle is the same loss applied to the style branch, but its gradients are restricted to the contact decoder in style branch. The style adversarial loss Lstyle-adv follows SagNets [34] to compute the binary cross-entropy loss between contact prediction from style branch and uniform distribution, where only the adversarial adapters Aprev and Aafter are trained. The mask loss Lmask is the average of binary cross-entropy and Dice losses [30] computed on the predicted foot segmentation. The ground loss Lground is the sum of pixel-height loss and ground-normal loss, Lground = Lpixel-height + Lground-normal, (17) where Lpixel-height is mean absolute error on the scaled pixel height map [42] and Lground-normal is cosine similarity loss on unit-normalized ground normals. All losses are computed for FECO outputs from the clean image and two ProRandConv [7] augmentations, and then averaged. The ground loss Lground is applied symmetrically to both the main and style branches. 4. Dataset: COFE Despite the availability of accurate contact data from largescale 3D datasets  (Table 1)  , they lack in-the-wild data for training and evaluation. To fill this gap, we introduce the COFE dataset, where we manually annotate binary foot contact labels on human foot keypoints from multiple 2D keypoint datasets: OpenPose [3], InstaVariety [22], PennAction [57], and MPII [1]. After removing images with fewer than two valid foot joints, typically due to occlusion or annotation ambiguity, the dataset comprises 31,598 training and 1,567 testing images. Representative samples are shown in Figure 2. Since foot contact can be ambiguous in cases such as snowboarding, we define consistent annotation rules: snowboards are labeled as contact, skis as non-contact, and clothing as non-contact. This avoids confusion with skateboards, which are never fixed to the feet. Skis are only annotated as contact when they touch the ground or another surface, while footclothing contact (e.g., long dresses) is excluded due to its limited relevance to footground or footobject interaction. We decided to exclude OpenPose for test set as our examination showed that it predominantly contains fully contacted feet; if all joints are manually predicted as contact, the precision, recall, and F1-score are 0.712, 0.858, and 0.760, respectively. Hence, we construct our test set using the InstaVariety [22] dataset, which has diverse contact distribution and has video sequences that allow comparison with existing methods. 5. Implementation details PyTorch [35] is used for implementation. Our backbone is initialized publicly released weights of ViT-Huge pretrained with ImageNet [9]. We apply data augmentations such as random scaling, cropping, and rotation. To enhance robustness against degraded inputs, we further introduce lowresolution, noise, and blur perturbations during the augmentation. We employ the AdamW optimizer [28] with learning rate of 105 and mini-batch size of 4. To ensure stable 6 Figure 3. Qualitative comparison of dense foot contact estimation with POSA [16], BSTRO [18], and DECO [45] on MOYO [46], RICH [18], Hi4D [53] dataset. Red circles indicate exemplar regions that FECO outperforms previous methods. Table 2. Ablation of low-level randomization on MMVP [55]. Table 4. Ablation of ground-aware learning on MMVP [55]. Methods Precision Recall F1-Score Ground norm. PH map Spatial attn. Precision Recall F1-Score w/o Low-Level Rand. w/ Low-Level Rand. (Ours) 0.544 0.563 0.584 0. 0.555 0.577 Table 3. Ablation of shoe contentstyle randomization on MMVP [55]. 0.482 0.518 0.554 0. 0.527 0.560 0.610 0.613 0.506 0.527 0.569 0.577 Content Rand. Style Rand. Precision Recall F1-Score 0.515 0.504 0.541 0. 0.610 0.651 0.595 0.613 0.522 0.531 0.554 0.577 Table 5. Ablation of training dataset on COFE dataset. 3D Mocap datasets COFE dataset Precision Recall F1-Score 0.494 0.553 0.464 0.516 0.450 0.515 convergence, the learning rate is decayed by factor of 0.9 after the 5th and 10th epochs. We train FECO for 10 epochs on single NVIDIA A6000 GPU. 6. Experiments 6.1. Datasets We choose 10 datasets with diverse foot interactions with PROX [15], EgoBody [56], RICH [18] for foot-scene interaction, BEHAVE [2], InterCap [19] for foot-object interaction, MOYO [46], MMVP [55], MotionPro [40] for footground interaction, and Hi4D [53] for foot-body interaction. Additionally, our proposed COFE dataset, introduced in Section 4, is utilized for in-the-wild training samples. To reduce redundancy and balance the ratio between datasets, we sample 2, 5, 5, 10, 50, 30 for BEHAVE, EgoBody, Hi4D, InterCap, MotionPro, MOYO dataset. Our main evaluation dataset is MMVP due to its accurate foot contact annotation based on insole, which is also directly convertible to SMPLX body model. For shoe content randomization, we use the UT Zappos50K dataset [54], which contains 50,025 images across four major categories: shoes, sandals, slippers, and boots, as well as four relative attributes at the instance level: open, pointy, sporty, and comfort. 6.2. Evaluation metrics To evaluate dense foot contact estimation, we report precision, recall, and F1 score. Given predicted per-vertex probabilities ˆC [0, 1]V and binary ground truth CGT {0, 1}V , we threshold ˆC at 0.5 to obtain binary predictions and compute per-sample precision, recall, and F1, then average over the dataset. Samples without any positive contact are excluded since recall and F1 are not well defined in such cases. 7 Table 6. Comparison of feature-level style randomization techniques on MMVP [55]. denotes re-implemented results. Table 8. Comparison with SOTA methods of joint-level foot contact estimation on COFE dataset. Precision Recall F1-Score Methods Precision Recall F1-Score Method BIN [33] MixStyle [58] SagNets [34] LatentDR [24] Shoe StyleContent Rand. (Ours) 0.505 0.437 0.451 0.534 0.563 0.351 0.463 0.564 0.574 0.613 0.396 0.448 0.511 0. 0.577 Table 7. Comparison with SOTA methods of dense foot contact estimation on MMVP [55] dataset. Methods Precision Recall F1-Score POSA [16] BSTRO [18] DECO [45] FECO (Ours) 0.276 0.436 0.374 0.563 0.308 0.538 0.511 0.613 0.255 0.464 0.409 0. 6.3. Ablation study Effectiveness of low-level randomization. Table 2 shows that progressive low-level style randomization improves performance on MMVP [55]. Compared to the variant without low-level randomization, precision increases by 3.5%, recall by 5.0%, and F1-score by 4.0%. This indicates that our low-level randomization effectively helps FECO rely less on spurious low-level appearance cues and generalize better across various shoe styles. Effectiveness of style-content randomization. Table 3 shows that content randomization alone yields the highest recall but lowers precision, suggesting stronger coverage at the cost of false positives. Style randomization alone increases precision while maintaining competitive recall, indicating improved robustness to style shifts. Combining both content and style randomization produces the best trade-off with highest F1-score, confirming that disentangling content and style and exposing FECO to diverse shoe appearances from shoe image dataset UT Zappos50K dataset [54] yields complementary gains. Effectiveness of ground-aware learning. Table 4 evaluates ground-aware learning. Adding ground normals to the baseline improves F1-score, showing that global orientation of the support surface is informative. Introducing pixel height maps further lifts the F1-score by providing dense geometric context that correlates with the ground. Finally, spatial attention between randomized features and ground features yields the best result, indicating that adaptive fusion of shoe style-invariant learning and groundaware learning leads to harmonious gains across all metrics. Effectiveness of COFE dataset. Table 5 analyzes the impact of adding our proposed COFE dataset as additional training data. Compared with FECO trained with only 3D motion capture (mocap) datasets, FECO trained addition8 Footskate Reducer [61] WHAM [43] FECO (Ours) 0.399 0. 0.553 0.271 0.431 0.516 0.301 0.363 0.515 ally with our COFE dataset is better in all metrics of precision, recall, and F1-score. The gains suggest that COFE supplies diverse in-the-wild appearances and various foot interactions that complement 3D mocap datasets and reduce overfitting to appearances from the 3D mocap datasets. 6.4. Comparison with state-of-the-art methods Dense foot contact estimation. Table 7 demonstrates strong performance of FECO on MMVP dataset [55] compared to prior methods by clear margin. Compared to POSA [16], BSTRO [18], and DECO [45], FECO achieves the highest precision, recall, and F1-score, demonstrating strong performance across both positive and negative predictions. Such gains are largely attributed to diversity of training data, shoe styleinvariant learning that mitigates appearance bias and ground-aware learning that injects explicit support-surface geometry. Joint-level foot contact estimation. In Table 8, we compare joint-level foot contact estimation methods on our COFE dataset. Most existing methods rely on zero-velocity heuristics or motion cues, making them unsuitable for single-image inputs. Hence, for fair comparison, we construct the test split with only video sequences. Evaluation is conducted on two joints, toe and heel, following the common definition in the prior works. To evaluate, we use heel contact directly and aggregate the two toe contacts with logical OR operation. It is worth noting that FECO is the only method that does not have access to temporal information. Even under these settings, FECO achieves the best precision, recall, and F1-score, showing that dense contact reasoning from single images generalizes well to joint-level estimation, surpassing methods that exploit temporal cues. 7. Conclusion We propose FECO, robust and generalizable framework for dense foot contact estimation that incorporates shoe styleinvariant and ground-aware learning. To achieve style invariance, we introduce shoe stylecontent randomization using an external shoe image dataset. For ground-aware learning, we supervise both pixel height maps and ground normals to capture fine-grained and global footground interactions. With the proposed COFE dataset, our FECO surpasses previous state-of-the-art methods by significant margin and demonstrates robustness and generalization on diverse foot contact scenarios. Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide additional technical details and experimental results that were omitted from the main manuscript due to space constraints. The contents are summarized below: S1. Configuration of COFE dataset S2. Details of training FECO S3. Details of joint definitions and joint regressors S4. Details of dense foot contact labels S5. Visualization of ground-aware learning S6. Quantitative results on more datasets S7. Quantitative results on different backbones S8. Computational requirements S9. More qualitative results S10. Limitations and societal impacts S1. Configuration of COFE dataset We construct the COFE dataset by aggregating foot image samples from OpenPose [3], InstaVariety [22], PennAction [57], and MPII [1]. Table S1 presents the traintest split of the aggregated dataset, and Figure S1 illustrates the relative proportion of training samples from each source. Regarding the training set, COFE dataset contains 43.0% samples from OpenPose, 38.4% from PennAction, 11.9% from InstaVariety, and 6.7% from MPII. For image-based datasets of OpenPose and MPII, we only include frames in which the feet are clearly visible. For video-based datasets of PennAction and InstaVariety, we additionally filter out static clips, particularly those of which the person remains standing upright throughout the sequence, as fully contacted foot states are already sufficiently covered in image datasets. To ensure annotation quality, all samples are manually labeled following the OpenPose foot keypoint definition (big toe, small toe, and heel). Moreover, we primarily use videos containing single person, so as to avoid erroneous keypoint detections caused by multiple individuals within frame. Notably, the MPII videos are generally very short (less than three seconds), which limits their utility as test set of COFE dataset. Hence, we primarily use MPII dataset only for training. Although OpenPose includes small set of test samples, we found that nearly all of them correspond to fully contacted feet. As result, the evaluation is biased: for example, when predicting uniformly full contact across the entire OpenPose test set, the resulting precision, recall, and F1-score are already 0.712, 0.858, and 0.760, respectively. This suggests that OpenPose is not suitable for robust evaluation. Also, since previous methods for foot contact estiFigure S1. COFE dataset statistics. We visualize the dataset configuration of our proposed COFE dataset, which consists of foot image samples in OpenPose [3], InstaVariety [22], PennAction [57], and MPII [1]. We only include training samples. Table S1. Data split for COFE dataset. Dataset Image / Video Train Test OpenPose [3] PennAction [57] InstaVariety [22] MPII [1] Image Video Video Image 13,590 12,135 3,743 2,130 464 - 1,103 - mation are video-based and temporal in nature, we choose to perform evaluation solely on the InstaVariety test split samples within the COFE dataset. This choice allows us to coherently evaluate joint-level foot contact estimation. As shown in Figure S2, the COFE training set maintains relatively balanced distribution between contacting and non-contacting joints, with 49,927 contacting and 44,201 non-contacting cases overall. At the joint level, big toe contacts account for 63.4% of its annotations, small toe for 53.3%, while heel contacts are slightly underrepresented at 42.5%. This indicates that, the heel joint is slightly less frequent in contact compared to the toes, but the dataset still provides sufficient coverage across all three joints. When compared with the overall distribution from other training datasets used for FECO (excluding COFE), clearer contrast emerges. As visualized in Figure S3, the combined datasets exhibit skew toward non-contact, with roughly 41% contacting versus 59% non-contacting joints in total. Moreover, these datasets show contacts concentrated at the big and small toes, with the heel being signifiFigure S2. Contact and non-contact distribution of COFE dataset. Figure S3. Contact and non-contact distribution of training datasets for FECO. Table S2. Ground plane configuration. Negative height refers to datasets whose coordinate system is defined such that smaller values along the height axis correspond to higher positions. Dataset Height axis Ground axis Negative height PROX [15] BEHAVE [2] InterCap [19] EgoBody [56] RICH [18] MOYO [46] Hi4D [53] MMVP [55] MotionPro [40] z-axis y-axis y-axis y-axis y-axis z-axis y-axis y-axis y-axis x-axis, y-axis x-axis, z-axis x-axis, z-axis x-axis, z-axis x-axis, z-axis x-axis, y-axis x-axis, z-axis x-axis, z-axis x-axis, z-axis cantly underrepresented as in Figure S5. By contrast, COFE not only achieves closer balance between contact and noncontact, but also distributes annotations more evenly across joints. This makes COFE complementary resource that mitigates the biases present in existing datasets and provides more stable training signal for joint-level foot contact estimation. S2. Details of training FECO The original SagNets [34] adopt three separate optimizers, where one trains the content-biased network with task loss and two others train the style-biased network with both task loss and an adversarial loss. While this has reasonable computational cost for image classification task, this strategy becomes computationally expensive when extended to dense foot contact estimation. To reduce the cost, we train our model in an end-to-end manner with single optimizer, which allows the backbone network (e.g., ViT, ResNet), the foot contact decoders, the ground-aware decoders, and the style branch to be optimized jointly. This is achieved with gradient detaching and module freezing for irrelevant modules to resemble the training of the original SagNets. Moreover, all of the operations after low-level style randomization are conducted three parallel operation onto the original input and two low-level style randomized images with ProRandConv [7]. This allows FECO to reduce low-level style bias, which improve models stability and robustness on unseen shoe styles. Below, we further explain in detail on the process and clarify our design choice on training. 10 Figure S4. Foot part segmentation. We build foot part segmentation that consists of 11 parts. Each part is defined by 5 toes (blue), heel (neon lime green), front (yellow), bottom (bright orange), left (dark orange), right (red), back (dark red). Gradient detaching for foot segmentation. During the forward pass, the backbone produces multi-level intermediate features {Fl}n1 l=1 and final high-resolution feature map Fn, where there are layers for the backbone network. From these features {Fl}n l=1, foot segmentation mask Mf is predicted and binarized to detect the spatial region of the foot. We intentionally detach the gradient flow from later modules that utilize the foot segmentation mask Mf , so that the foot segmentation mask decoder is solely trained by the foot segmentation loss Lmask. Gradient detaching and module freezing for end-to-end training. There are two major components of training techniques of SagNets that requires modification to enable training of FECO on single optimizer. First, the style-biased loss of SagNets only train style-biased network., which each corresponds to style loss Lstyle and foot contact decoder in our style branch. This is implemented in SagNets by adopting separate optimizer that only optimizes the parameters in style-biased network with dedicated optimizer. However, this can be easily implemented by simply detaching the gradient flow for the input of style-biased network. For FECO, we therefore detach the graident flow of input content invariant feature towards foot contact decoder. This allows our style loss Lstyle to only train foot contact decoder. However, the challenging part is that we need to obtain gradient from the same foot contact decoder in the style path of our FECO to allow adversarial training with our style adversarial loss Lstyle-adv. In SagNets, the adversarial loss is applied to the affine parameters of the batch normalization layers within ResNet backbone. Similar with their style loss, they also utilize dedicated optimizer that only optimizes the affine parameters with the adversarial loss. Our FECO instead builds two adapters Aprev and Aafter, which each adapts multi-level features {Fl}n l=1 and adapts features after content randomization. Nevertheless, there is another challenge. The adapters, which are our replacement of the affine parameters in ResNet backbone from SagNets, are at the initial stages of the FECO model while adversarial loss should only train the adapters Aprev and Aafter without influencing subsequent modules such as foot contact decoder in style branch of FECO. This is difficult as loss inevitably influences all subsequent modules by nature. To tackle this issue, we freeze all parameters within foot contact decoder within style branch of FECO and make only the adapters Aprev and Aafter to be trained with adversarial loss Lstyle-adv. This enables us to strictly follow the training process of SagNets while utilizing only single optimizer, which significantly decreases the computational burden of training. S3. Details of joint definitions and joint regressors In Figure S4, we present the foot part segmentation used to extract foot joints for joint level foot contact supervision. This corresponds to v2 = 11. The most widely used foot joint definition of OpenPose [3] provides foot joint definition that corresponds to v3 = 3, which is highly sparse and captures contact only at the big toe, small toe, and heel, limiting coverage of the remaining foot surface. But, we need denser joint representation that covers the entire foot surface. Therefore, we start from the SMPL-X [36] foot mesh, which is subset of the full body model. We partition the foot mesh into eleven intuitive parts that align with functional regions of the foot. First, we segment the five toes, visualized from dark to light blue in Figure S4. Then, we segment the heel to maintain compatibility with the OpenPose joint definition. Furthermore, we segment the remaining plantar surface that frequently contacts the ground. Finally, we divide the dorsal surface into four regions in the left, right, front, and back directions. This yields eleven parts used for both foot part segmentation and the associated joints. Each foot joint is then defined as the mean of the vertex coordinates within its corresponding part. S4. Details of dense foot contact labels Following the previous work on hand contact estimation [21], we implement distance-based thresholding with Trimesh library [8] to gather ground-truth dense foot contact labels. However, unlike HACO [21], that had access to mesh of the interacting entity (i.e., 3D object mesh, 3D scene mesh), we only have few datasets [15, 18, 56] that provide 3D scene mesh. Therefore, in order to also leverage 3D motion capture datasets [2, 19, 40, 46, 53, 55] that do not provide 3D scene mesh, we extract 3D ground mesh 11 Table S3. Computational requirements of various backbone configurations. Model Backbone Train Memory (MB) Test Memory (MB) Params. (M) Speed (fps) GFLOPs FECO FECO FECO FECO ViT-H ViT-L ViT-B ViT-S FECO ResNet-152 FECO ResNet-101 FECO ResNet-50 FECO ResNet-34 FECO ResNet-18 34,328 19,683 12,269 8,743 24,410 21,732 19,037 4,757 4,463 6,418 4,528 3,304 2, 3,680 3,644 3,570 2,420 2,378 964.64 554.37 270.44 137.38 335.66 320.02 301.03 104.79 94.68 20.04 20.86 24.86 23.53 17.98 20.82 24.16 41.07 42.98 190.67 73.45 24.67 6. 99.75 87.49 72.60 4.11 2.26 and conduct distance-based thresholding between the 3D ground mesh and 3D foot mesh. To extract ground mesh from datasets without 3D scene mesh, we fit parametric ground plane to each capture sequence. Specifically, we aggregate candidate ground points by collecting the vertices that are lowest in physical height (closest to the real-world ground surface) from both the body and interacting object meshes (only if provided) across frames, and then apply regression-based plane fitting strategy to obtain coefficients (a, b, c) of the plane = ag1 + bg2 + c, where denotes the height axis (i.e., y-axis of the xyz coordinate system) and g1, g2 denote the ground axes (i.e., x-axis and z-axis of the xyz coordinate system). We also extract ground meshes from datasets with 3D scene mesh. To extract ground mesh from datasets with 3D scene mesh, we directly leverage the 3D scene mesh. We first sample the scene mesh vertices and compute their height values with respect to the vertical axis, taking into account the coordinate convention of each dataset. Among these vertices, we retain only those within the lowest p% percentile in height, which effectively restricts candidate points to the near-ground region while discarding elevated structures and outliers. We then apply RANSAC-based plane fitting [12] to these candidate points, repeating the procedure multiple times and selecting the plane with the widest spatial support, determined by how many scene mesh vertices fall close to the fitted plane within distance threshold. This yields the ground plane parameters (a, b, c) of = ag1 + bg2 + c, consistent with the coordinate definition used for datasets without 3D scene mesh. Once the ground plane is estimated, we compute signed distances between the 3D foot mesh vertices and the plane, and assign contact labels when the magnitude of the distance falls within dataset-specific tolerance. The tolerance values are determined by manually inspecting the dense foot contact results produced under different thresholds and selecting the setting that best aligns both the real-world dense foot contact and consistency between datasets. Specifically, we set the tolerance to 1 cm for MOYO, 2 cm for MotionPro, 3 cm for PROX and EgoBody, and 5 cm for BEHAVE and InterCap. For datasets that already provide ground-truth dense body contact labels, such as Hi4D and RICH, we directly adopt the provided contact annotations. This unified procedure equips all datasets, regardless of whether they include explicit scene meshes, with dense per-vertex foot contact labels that are geometrically consistent. The ground plane configurations and ground plane parameters used and extracted in this process are each summarized in Table S2 and Table S6. S5. Visualization of ground-aware learning Figure S6 presents qualitative results of our ground-aware learning module, which predicts pixel height maps and ground normals from the ground feature. The predicted pixel height maps produced by FECO closely match the ground truth. In particular, the smooth gradient patterns in the pixel height maps indicate that FECO effectively infers per-pixel height relative to the ground for regions corresponding to the foot. Although the predictions exhibit minor smoothing and reduced detail around individual toes, the height maps still provide strong signal and proof of the ground-aware learning. Furthermore, the predicted ground normals show strong alignment with the corresponding ground-truth normals. The predictions remain robust even when the foot is tilted or not aligned with the ground plane, as illustrated in the third and fourth rows of Figure S6. These accurate ground-aware representations of pixel height and ground normal enable FECO to achieve reliable and precise dense foot contact estimation. S6. Quantitative results on more datasets In addition to the MMVP [55] dataset, we further validate FECO across diverse datasets to provide additional benchmarks. On BEHAVE [2], FECO achieves strong results on all metrics with an F1-score of 0.768, demonstrating robust performance on footobject contact estimation. On RICH [18], which contains dense footscene interactions, FECO obtains reasonable performance in diverse 3D scene environments. We also evaluate on MOYO [46], motion capture dataset designed for extreme yoga poses. Due to the 12 strong out-of-distribution poses and limited scene contact supervision, performance drops compared to other datasets. Nevertheless, FECO still detects plausible contacts under such challenging conditions. Finally, on Hi4D [53], which includes humanhuman interactions, our model achieves strong overall results. These findings collectively highlight that FECO not only performs well on the main evaluation dataset (MMVP), but also yields reasonable performance across diverse real-world interaction scenarios, including object-centric, extreme-pose, and human interaction settings. S7. Quantitative results on different backbones We evaluate the performance of FECO using various backbone architectures on the MMVP [55] dataset, keeping all other components fixed. As summarized in Table S4, the ViT-H [11] backbone achieves the highest F1-score of 0.577, demonstrating the strongest overall performance among all tested models. ViT-based architectures consistently outperform convolutional alternatives, reflecting the benefit of Transformer [48]-based designs in capturing long-range dependencies crucial for dense foot contact estimation. Among the ViT variants, ViT-B attains the highest recall (0.655), suggesting better coverage of subtle contact regions, while ViT-L achieves balanced trade-off between precision and recall. ViT-S maintains competitive performance despite its compact size, underscoring the scalability of Transformer backbones under resource constraints. Among convolutional backbones, ResNet-152 [17] delivers the strongest result with an F1-score of 0.566, followed by ResNet-101 and ResNet-50. For these models, we employ decoder based on the re-implementation of FCN [27] from PyTorch [35] to predict pixel height maps. To maintain efficiency, dilation is omitted in shallower variants (ResNet18 and ResNet-34), though this leads to reduced accuracy due to their limited receptive fields. Overall, these results confirm that ViT backbones are better suited for dense foot contact estimation than convolutional counterparts, primarily due to their global context modeling and superior spatial reasoning. The strong performance of ViT-H further justifies its selection as the default backbone in our main experiments. All model variants will be publicly released. S8. Computational requirements Table S3 reports the computational requirements of FECO under different backbone configurations. Large-scale Vision Transformer backbones [11], such as ViT-H, deliver the strongest representational capacity but also incur the highest computational overhead. The ViT-H variant requires more than 34 GB of training memory and nearly 6.5 GB of inference memory. This overhead primarily arises from maintaining strong gradients across the models 964.64M Comparison of various backbone models on Table S4. MMVP [55] dataset. All backbones are initialized with ImageNet [9] dataset. Backbone Precision Recall F1-Score Model FECO FECO FECO FECO ViT-H [11] ViT-L [11] ViT-B [11] ViT-S [11] FECO ResNet-152 [17] FECO ResNet-101 [17] ResNet-50 [17] FECO ResNet-34 [17] FECO ResNet-18 [17] FECO 0.563 0.578 0.500 0.486 0.519 0.542 0.551 0.448 0.498 0.613 0.568 0.655 0.628 0.637 0.537 0.533 0.550 0.438 0.577 0.567 0.560 0.537 0.566 0.526 0.523 0.481 0. Table S5. Quantitative results on more datasets Model Evaluation Dataset Precision Recall F1-Score FECO BEHAVE [2] FECO RICH [18] FECO MOYO [46] FECO Hi4D [53] 0.753 0.590 0.516 0.750 0.871 0. 0.384 0.879 0.768 0.583 0.357 0. parameters during training. ViT-L provides more moderate alternative, reducing both memory usage and computational cost while retaining high model capacity. Smaller variants such as ViT-B and ViT-S provide efficient configurations, with training memory under 13 GB and inference speeds exceeding 23 fps, making them well-suited for practical applications. ResNet-based backbones [17] further expand the design space by offering spectrum of efficient choices. ResNet152 and ResNet-101 provide strong representational power while maintaining stable inference speeds around 20 fps. ResNet-50 achieves favorable balance, combining manageable memory consumption with fast inference exceeding 24 fps. Lightweight variants such as ResNet-34 and ResNet-18 are highly efficient, requiring fewer than 5 GB of training memory and achieving speeds above 40 fps with very low GFLOPs, making them ideal for real-time scenarios and deployment on resource-constrained hardware. Overall, FECO supports wide range of backbones, from high-capacity Transformers to lightweight ResNets, enabling its use across diverse downstream tasks that may have varying computational requirements. S9. More qualitative results Figure S7 and Figure S8 present additional qualitative comparisons of POSA [16], BSTRO [18], and DECO [45] on the Hi4D [53], MMVP [55], RICH [18], MOYO [46], and COFE dataset. Our FECO consistently outperforms previous state-of-the-art methods by large margin. DECO frequently predicts full plantar contact regardless of the actual foot contact observable from the image. POSA frequently yields false positives contact prediction when the foot pose suggests contact despite there is no contact occurrence in the image. For example, in the fifth row of Figure S7, although the foot does not touch the ground, its pose leads POSA to hallucinate non-existent contact. BSTRO fails to infer contact in the heel region. For instance, in the last row of Figure S7 and the second row of Figure S8, BSTRO does not detect heel contact even though it is well present. This behavior arises from the absence of ground-aware learning in BSTRO, whereas FECO explicitly incorporates pixel height maps and ground normals to enable ground-aware contact reasoning. Furthermore, FECO maintains accurate predictions across diverse shoe styles in all samples, which is enabled by its shoe style-invariant learning. S10. Limitations and societal impacts Limitations. Our FECO overcomes the challenge of diverse shoe appearance, which is inherent and unique problem for dense foot contact estimation. Also, to overcome the issue of lack of in-the-wild training and evaluation dataset for foot contact, we manually annotate and introduce COFE dataset. However, we mainly operate under foot cropped image, which may not provide useful information when fully occluded. Advancing towards the integration with dense full-body contact estimation methods or proposing additional module that takes information from full image without being biased with full body pose would solve such occlusion problem. Also, dense foot contact can be much easier problem with temporal information as static property of foot during contact is, should not serve as sole contributor but, important cue for contactness. Developing video-based dense foot contact estimation would greatly improve the performance of the model. Societal impacts. The proposed method has broad potential for applications in sports analytics, rehabilitation, AR/VR, and human behavior understanding. Nevertheless, deploying dense foot contact estimation in the wild entails risks related to privacy, safety, and sustainability. Patterns of foot contact during daily activity may reveal aspects of health, so any data collection should occur only with consent, minimal retention, and, when possible, on-device processing. Our method is not designed as diagnostic tool, and such use would require higher accuracy and fidelity on specific diagnostic scenarios. To build diagnostic system on top of FECO, one would need calibrated confidence estimates, detection of out-of-distribution inputs, expert oversight, and an intended-use license that restricts surveillance and other misuse. 14 Figure S5. Dataset-wise dense foot contact mean. These heatmaps show mean foot contact of ground-truth contacts from PROX [15], BEHAVE [2], InterCap [19], EgoBody [56], RICH [18], MOYO [46], Hi4D [53], MMVP [55], MotionPro [40] dataset. Figure S6. Visualization of ground-aware learning on MOYO [46], Hi4D [53], RICH [18] dataset. 16 Figure S7. Qualitative comparison of dense foot contact estimation with POSA [16], BSTRO [18], DECO [45] on Hi4D [53], MMVP [55], RICH [18] dataset. Red circles indicate exemplar regions that FECO outperforms previous methods. 17 Figure S8. Qualitative comparison of dense foot contact estimation with POSA [16], BSTRO [18], DECO [45] on RICH [18], MOYO [46], Hi4D [53], COFE dataset. Red circles indicate exemplar regions that FECO outperforms previous methods. Table S6. Coefficients of fitted ground plane of datasets. All refers to all sequences in the dataset."
        },
        {
            "title": "Sequence",
            "content": "slope of g1 slope of g2 intercept of PROX [15]"
        },
        {
            "title": "Quantitative",
            "content": "x: 0.005543 y: 0.021068 z: -0.116057 Date01 Date02 Date03 Date04 Date05 Date06 Date"
        },
        {
            "title": "All",
            "content": "x: 0.027887 x: -0.003986 x: 0.033936 x: 0.007792 x: 0.022923 x: 0.019536 x: 0.009183 z: -0.008862 z: 0.010984 z: 0.016988 z: -0.009504 z: -0.012357 z: -0.018724 z: -0.003937 y: 1.201017 y: 1.178358 y: 1.230144 y: 1.194403 y: 1.193551 y: 1.213272 y: 1.217363 x: 0.027502 z: -0.134394 y: 1. seminar g110 seminar h52 kitchen gfloor cab cab tables seminar d78 seminar d78 0318 seminar g110 0415 foodlab 0312 cab benches seminar j716 seminar h53 0218 cnb dlab 0215 seminar g110 0315 cnb dlab 0225 BBQ Gym LectureHall ParkingLot1 ParkingLot2 x: -0.012439 x: 0.001277 x: 0.001684 x: 0.000703 x: -0.000753 x: 0.002037 x: -0.001632 x: 0.000735 x: -0.005830 x: -0.000604 x: 0.000745 x: -0.000041 x: 0.000525 x: 0.003623 x: 0.000715 x: 0.028968 x: -0.055151 x: 0.027296 x: -0.022767 x: -0.018011 z: 0.002003 z: 0.001124 z: 0.000132 z: 0.006315 z: -0.000542 z: 0.000250 z: 0.002296 z: -0.001843 z: -0.000577 z: -0.000745 z: -0.001081 z: -0.003415 z: -0.000740 z: -0.000446 z: 0.001987 z: -0.137393 z: -0.237227 z: -0.299544 z: -0.128437 z: -0. y: -1.661606 y: -0.505707 y: -0.842190 y: -0.711828 y: -0.331833 y: -0.814285 y: -1.023084 y: -0.789173 y: -0.709450 y: -0.155386 y: -0.894595 y: -0.786509 y: -0.119325 y: -0.733775 y: -0.155290 y: 1.421628 y: 1.577146 y: 1.979380 y: 1.372453 y: 1.375236 All All S01 S02 S03 S04 S05 S06 S07 S09 S10 S11 S12 All x: 0.000000 z: 0.000000 y: 0.000000 x: -0.010825 z: 0.011383 y: 0. x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 x: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 z: 0.000000 y: 1.253302 y: 1.245441 y: 1.242629 y: 1.247209 y: 1.219583 y: 1.254879 y: 1.255879 y: 1.240000 y: 1.266970 y: 1.223758 y: 1.246929 x: 0.000000 z: 0.000000 y: 0. BEHAVE [2] InterCap [19] EgoBody [56] RICH [18] MOYO [46] Hi4D [53] MMVP [55] MotionPro [40]"
        },
        {
            "title": "References",
            "content": "[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2D human pose estimation: New benchmark and state of the art analysis. In CVPR, 2014. 6, 9 [2] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. BEHAVE: Dataset and method for tracking human object interactions. In CVPR, 2022. 2, 7, 10, 11, 12, 13, 15, 19 [3] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. OpenPose: Realtime multi-person 2D pose estimation using part affinity fields. In TPAMI, 2019. 5, 6, 9, 11 [4] Aurelien Cecille, Stefan Duffner, Franck Davoine, Thibault Neveu, and Remi Agier. GroCo: Ground constraint for metric self-supervised monocular depth. In ECCV, 2024. 3 [5] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. Monocular 3D object detection for autonomous driving. In CVPR, 2016. 3 [6] Sheng Cheng, Tejas Gokhale, and Yezhou Yang. Adversarial bayesian augmentation for single-source domain generalization. In ICCV, 2023. 3 [7] Seokeon Choi, Debasmit Das, Sungha Choi, Seunghan Yang, Hyunsin Park, and Sungrack Yun. Progressive random convolutions for single domain generalization. In CVPR, 2023. 2, 3, 6, 10 [8] Dawson-Haggerty et al. Trimesh. 11 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, 2009. 6, [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. In NAACL, 2019. 2 [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3, 13 [12] Martin Fischler and Robert Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. In Communications of the ACM, 1981. 12 [13] Leon Gatys, Alexander Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In NeurIPS, 2015. 2 [14] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, 2016. 2 [15] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael Black. Resolving 3D human pose ambiguities with 3D scene constraints. In ICCV, 2019. 2, 7, 10, 11, 15, 19 [16] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael Black. Populating 3D scenes by learning human-scene interaction. In CVPR, 2021. 1, 2, 7, 8, 13, 17, [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 13 [18] Chun-Hao Huang, Hongwei Yi, Markus Hoschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael Black. Capturing and inferring dense full-body human-scene contact. In CVPR, 2022. 1, 2, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19 [19] Yinghao Huang, Omid Taheri, Michael Black, and DimInterCap: Joint markerless 3D tracking of itrios Tzionas. humans and objects in interaction. In GCPR, 2022. 2, 7, 10, 11, 15, 19 [20] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 2 [21] Daniel Sungho Jung and Kyoung Mu Lee. Learning dense hand contact estimation from imbalanced data. In NeurIPS, 2025. 1, 5, [22] Angjoo Kanazawa, Jason Zhang, Panna Felsen, and Jitendra Malik. Learning 3D human dynamics from video. In CVPR, 2019. 6, 9 [23] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, 2019. 2 [24] Ran Liu, Sahil Khose, Jingyun Xiao, Lakshmi Sathidevi, Keerthan Ramnath, Zsolt Kira, and Eva Dyer. LatentDR: Improving model generalization through sample-aware latent degradation and restoration. In WACV, 2024. 8 [25] Songhua Liu, Xin Jin, Xingyi Yang, Jingwen Ye, and Xinchao Wang. StyDeSty: Min-max stylization and destylization for single domain generalization. In ICML, 2024. 3 [26] Yuxuan Liu, Yuan Yixuan, and Ming Liu. Ground-aware monocular 3D object detection for autonomous driving. In RA-L, 2021. 3 [27] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully In convolutional networks for semantic segmentation. CVPR, 2015. 13 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6 [29] Yunze Man, Yichen Sheng, Jianming Zhang, Liang-Yan Gui, and Yu-Xiong Wang. Floating no more: Object-ground reconstruction from single image. In CVPR, 2025. 3 [30] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016. 6 [31] Jaeho Moon, Juan Luis Gonzalez Bello, Byeongjun Kwon, and Munchurl Kim. From-ground-to-objects: Coarse-to-fine self-supervised monocular depth estimation of dynamic objects with ground contact prior. In CVPR, 2024. 3 [32] Lucas Mourot, Ludovic Hoyet, Francois Le Clerc, and Pierre Hellier. UnderPressure: Deep learning for foot contact detection, ground reaction force estimation and footskate cleanup. In CGF, 2022. 1, 2 [33] Hyeonseob Nam and Hyo-Eun Kim. Batch-instance normalization for adaptively style-invariant neural networks. In NeurIPS, 2018. 2, 8 [34] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 3, 6, 8, 10 [50] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth Anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 5 [51] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren. GEDepth: Ground embedding for monocular depth estimation. In ICCV, 2023. 3 [52] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt, and Feng Xu. Physical Inertial Poser (PIP): Physics-aware real-time human motion tracking from sparse inertial sensors. In CVPR, 2022. 1, 2 [53] Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Jie Song, and Otmar Hilliges. Hi4D: 4D instance segmentation of close human interaction. In CVPR, 2023. 2, 7, 10, 11, 13, 15, 16, 17, 18, 19 [54] Aron Yu and Kristen Grauman. Fine-grained visual comparisons with local learning. In CVPR, 2014. 2, 3, 4, 7, 8 [55] He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, and Xun Cao. MMVP: multimodal mocap dataset with vision and pressure sensors. In CVPR, 2024. 2, 7, 8, 10, 11, 12, 13, 15, 17, [56] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. EgoBody: Human body shape and motion of interacting people from head-mounted devices. In ECCV, 2022. 2, 7, 10, 11, 15, 19 [57] Weiyu Zhang, Menglong Zhu, and Konstantinos Derpanis. From actemes to action: strongly-supervised representaIn ICCV, 2013. 6, tion for detailed action understanding. 9 [58] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In ICLR, 2021. 8 [59] Yunsong Zhou, Yuan He, Hongzi Zhu, Cheng Wang, Hongyang Li, and Qinhong Jiang. Monocular 3D object deIn CVPR, tection: An extrinsic parameter free approach. 2021. 3 [60] Lian Zhuo, Jian Cao, Qi Wang, Bang Zhang, and Liefeng Bo. Towards stable human pose estimation via cross-view fusion and foot stabilization. In CVPR, 2023. 1, 2 [61] Yuliang Zou, Jimei Yang, Duygu Ceylan, Jianming Zhang, Federico Perazzi, and Jia-Bin Huang. Reducing footskate in human motion reconstruction with ground contact constraints. In WACV, 2020. 1, 2, 8 [35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 6, [36] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3D hands, face, and body from single image. In CVPR, 2019. 5, 11 [37] Zequn Qin and Xi Li. MonoGround: Detecting monocular 3D objects from the ground. In CVPR, 2022. 3 [38] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision Transformers for dense prediction. In ICCV, 2021. 3, 5 [39] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas Guibas. HuMoR: 3D human motion model for robust pose estimation. In ICCV, 2021. 1, 2 [40] Shenghao Ren, Yi Lu, Jiayi Huang, Jiayi Zhao, He Zhang, Tao Yu, Qiu Shen, and Xun Cao. MotionPRO: Exploring the role of pressure in human mocap and beyond. In CVPR, 2025. 2, 7, 10, 11, 15, 19 [41] Yichen Sheng, Yifan Liu, Jianming Zhang, Wei Yin, Cengiz Oztireli, He Zhang, Zhe Lin, Eli Shechtman, and Bedrich Benes. Controllable shadow generation using pixel height maps. In ECCV, 2022. 2, [42] Yichen Sheng, Jianming Zhang, Julien Philip, Yannick HoldGeoffroy, Xin Sun, He Zhang, Lu Ling, and Bedrich Benes. PixHt-Lab: Pixel height based light effect generation for image compositing. In CVPR, 2023. 3, 6 [43] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. WHAM: Reconstructing world-grounded humans with accurate 3D motion. In CVPR, 2024. 1, 2, 8 [44] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In NeurIPS, 2015. 2 [45] Shashank Tripathi, Agniv Chatterjee, Jean-Claude Passy, Hongwei Yi, Dimitrios Tzionas, and Michael Black. DECO: Dense estimation of 3D human-scene contact in the wild. In ICCV, 2023. 1, 2, 7, 8, 13, 17, 18 [46] Shashank Tripathi, Lea Muller, Chun-Hao Huang, Omid Taheri, Michael Black, and Dimitrios Tzionas. 3D human pose estimation via intuitive physics. In CVPR, 2023. 2, 7, 10, 11, 12, 13, 15, 16, 18, 19 [47] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance Normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. 2, 3 [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 13 [49] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021."
        }
    ],
    "affiliations": [
        "Dept. of ECE & ASRI, Seoul National University",
        "IPAI, Seoul National University"
    ]
}