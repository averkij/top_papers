{
    "paper_title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
    "authors": [
        "Zhangyi Liu",
        "Huaizhi Qu",
        "Xiaowei Yin",
        "He Sun",
        "Yanjun Han",
        "Tianlong Chen",
        "Zhun Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 1 ] . [ 1 5 4 7 6 1 . 2 0 6 2 : r PETS: Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency Zhangyi Liu4, Huaizhi Qu*1, Xiaowei Yin*1, He Sun3, Yanjun Han2, Tianlong Chen1, and Zhun Deng 1 1University of North Carolina at Chapel Hill 2New York University 3Yale University 4Independent Researcher Abstract Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-Time Self-Consistency), which initiates principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, new measure defined as agreement with the infinite-budget majority vote. This formulation makes sampleefficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at: this https URL."
        },
        {
            "title": "1 Introduction",
            "content": "Test-time scaling methods can substantially enhance LLMs reasoning performance [Muennighoff et al., 2025; Huang et al., 2025b; Guo et al., 2025; Snell et al., 2024; Fu et al., 2025a]. Self-consistency [Wang et al., 2022], adopted in Geminis Deep Think mode [Thang Luong & Edward Lockhart], is simple and effective approach but can be computationally expensive due to the need to sample many reasoning traces per query. natural remedy is to allocate different sampling budgets across questions (Figure 1). Most existing methods rely on heuristic signals, Equal contribution; authors are ordered alphabetically. Contact: zhundeng@cs.unc.edu 1 Figure 1: In this paper, we study how to allocate an LLMs sampling budget across questions to best match the full-budget outcome under self-consistency. Our results show that PETS substantially reduces the required budget while maintaining accuracy. such as trace-level confidence [Fu et al., 2025a] or LLM-predicted difficulty [Wang et al., 2025a], which typically lack theoretical guarantees and may use the budget inefficiently. Another line of work analyzes test-time scaling with external reward models or verifiers [Snell et al., 2024; Huang et al., 2025a; Zuo & Zhu, 2025]. However, reward models can be mis-specified in practice, especially on complex or out-of-distribution queries, leading to unreliable guidance. They also introduce additional training and deployment costs. These limitations motivate the need for theoretically principled test-time scaling methods that do not rely on auxiliary supervision, but instead leverage the intrinsic structure of self-consistency itself. Under this lens, we propose PETS, Principled and Efficient Test-Time Self-Consistency framework for trajectory allocation. Our approach is inspired by the principle that sampling effort should be distributed according to the relative difficulty of different questions, rather than allocated uniformly. Central to PETS is the optimization of self-consistency rate, defined as the probability that majority voting with finite budget matches the population majority. Because questions at different difficulty levels θ exhibit distinct convergence behaviors, they require different numbers of traces to attain high self-consistency. Our goal is therefore to allocate traces across questions so as to maximize the aggregate self-consistency rate of the question set under fixed budget. We study two practical settings: ❶ Offline batch setting, where the entire question set is available for optimization. In this regime, we connect trajectory allocation to crowdsourcing by modeling reasoning traces as workers. We adopt and extend Bayesian adaptive trajectory allocation algorithm (Section 3) that iteratively allocates additional trajectories while maintaining posterior over each questions difficulty level. ❷ Online streaming setting, where questions arrive sequentially and allocation must be made without access to the full dataset. Inspired by the offline case. we propose one-shot allocation strategy (Section 4) for PETS, which can be cast into supervised learning case as in the batch setting with the help of additional samples as the training set to estimate the distribution of question difficulties. We solve constrained allocation problem to obtain budget allocation plan over difficulty levels, and use greedy yet 2 provably optimal procedure to instantiate per-question budgets upon arrival. Experiments show that PETS significantly improves the efficiency of self-consistency while maintaining accuracy, demonstrating its strong practicability. As shown in Figure 1, compared to naive uniform allocation, it requires substantially fewer trajectories to reach full self-consistency, achieves higher self-consistency under the same budget, and ultimately translates these gains into improved accuracy. Our contribution. To summarize, we develop theoretically principled test-time scaling methods that leverage the intrinsic structure of self-consistency. Specifically, We introduce self-consistency rate, new performance measure defined as agreement with the infinite-budget majority vote. This formulation provides principled target for finite-budget inference and enables rigorous theoretical analysis of sample-efficient allocation. In the offline setting, we make the first connection between self-consistency trajectory allocation and optimal budget allocation in crowdsourcing. This allows us to employ rich body of existing tools to maximize the expected gain in self-consistency rate per allocation. In the online setting, we propose novel one-shot allocation strategy that estimates the difficulty distribution from small training set and solves constrained optimization problem to determine per-question budgets for allocation upon arrival. Extensive experiments show that PETS consistently reduces the traces needed to reach full self-consistency in both unweighted and weighted cases, improves self-consistency under fixed budgets, and yields higher accuracy than uniform baselines."
        },
        {
            "title": "2 Setup",
            "content": "We consider collection of multiple-choice (or fill-in-the-blank) questions {qi }N i=1. For each question qi, we assume access to stochastic reasoning procedure. Each sampled reasoning trace produces random answer Yi sampled from answer set . Without loss of generality, we use = {1, . . . , M} to denote the collection of possible answers. Reasoning traces are assumed to be i.i.d. conditioned on qi. Majority voting and related concepts. Given finite integer budget N+, for each question (B) (2) (1) , , qi, reasoning traces . For each answer in , the corresponding vote count is are i.i.d. drawn and lead to answers (B) , , (2) (1) , , Vi(y; B) = B(cid:88) j=1 1{Y (j) = y}."
        },
        {
            "title": "The final answer based on majority voting is denoted as",
            "content": "Maj (B) = arg max yY Vi(y; B). Similarly, for each question qi, we define weighted majority voting by considering the weighted 3 vote count for weight vector wi = (w (1) , (2) (B) , , ) RB + of all [B]. i (y; wi, B) = B(cid:88) j=1 (j) 1{Y (j) = y}. Correspondingly, we can define the final answer based on weighted majority voting as: WMaj (B) = arg max yY i (y; wi, B). In this paper, we mainly follow Fu et al. [2025a] so that for each question qi, the corresponding weight , so that the final weight vector takes the form (1) (2) wi = (w(T (j) (j) depends only on (B) ), , w(T ), w(T )). , , (1) (2) (1) (B) We make natural assumption1 that the trace and answer pairs (T are i.i.d. drawn from an unknown joint distribution {1, . . . , M} is given by pmf θi = (θi,1, θi,2, , θi,M), where θi,y = P(Yi = y) are unknown parameters. The pmf θ characterizes the uncertainty in LLMs answer to the question qi, and can thus represent the difficulty of each problem.2 (2) (B) ), (T ), . . . , (T , (j) i. The marginal distribution of i [w(Ti)1{Yi = y}], which We further define correspond to the population (weighted) majority label in the infinite sample limit for question formalize the fundamental limit of test-time scaling. With an infinite sampling qi. budget, aggregating infinitely many reasoning trajectories yields well-defined consensus prediction given by the infinite-budget (weighted) majority vote. = arg maxyY θi,y and yW , = arg maxyY E(Ti ,Yi )D and yW , i ) Given the above content, we can define the self-consistency rate of the answer obtained by majority voting and the weighted version for question qi under budget as and Maj SC(qi; B) = P(Y (B) = θi) WMaj SCW (qi; B) = P(Y (B) = yW , i). (1) (2) Intuitively, self-consistency measures how likely budget-B (weighted) vote recovers the population (weighted) majority label, providing principled target for finite-budget inference. By defining self-consistency rate, we can rigorously study how test-time samples approximate infinite-budget behavior and design sample-efficient allocation strategies under realistic computational constraints. Moreover, the rate at which it improves with provides natural notion of question difficulty. Both quantities are monotonically increasing in B, and their growth is governed by θi (or Optimal trajectory budget allocation problem. Our general goal is to design budget allocation strategies that maximize self-consistency rate under limited reasoning total budget in various in the weighted case). 1Traces are independently generated from the same question qi without fixing random seed. 2If the maximum coordinate arg maxyY θy is much larger than the remaining parts, then it shows that this problem is relatively easier. Figure 5 in Appendix B.3 shows how θ affects the self-consistency rate under different budget B. settings. Loosely speaking, given set of questions {qi allocate budget for each question, i.e., number of reasoning traces for each question. }N i=1, our aim is to find policy π to max π N(cid:88) i=1 ED Dmeta (π) (cid:104) SCW (qi; Bπ(qi)) (cid:105) , s.t. N(cid:88) i=1 Bπ(qi) Btotal. (3) Here, the joint distribution of (Ti, Yi) can be more complex than fixed distribution as it might be dependent on the policy. For instance, in Bayesian perspective, we can consider meta distribution (distribution of distributions) and consider (π). In Section 3, Dmeta (π) can be posterior distribution over parameters like θi based on π. Details are specified in later sections. We study this problem under two different information settings that can both be unified under our above formulation. ❶ Offline batch setting: the full question set {qi }N i=1 is known in advance, so the policy can allocate and adapt budgets globally across questions during execution. The outcome is summarized by final per-question budget Bπ(qi) for each qi. ❷ Online streaming } arrive sequentially as i.i.d. draws from latent distribution setting: Questions {q1, q2, . . . , qN and is not observable in advance. When qt arrives, the policy must choose budget Bt = Bπ(qt) immediately, without seeing future questions. is drawn from Dmeta i"
        },
        {
            "title": "3 Offline PETS in the Batch Setting",
            "content": "In this section, we study the problem of optimal trajectory allocation in the offline setting. Here, allocation proceeds sequentially, and the availability of all questions enables us to gradually assess their relative difficulties throughout the process. Building on our notion of the selfconsistency rate, we establish an interesting connection to the fruitful and well-developed literature on crowdsourcing. Table 1 summarizes the correspondence. Specifically, under standard majority voting, the trajectory allocation problem is closely related to optimal budget allocation in crowdsourcing. Each reasoning answer can be viewed as noisy worker label for question qi, while allocating additional traces corresponds to assigning more labeling effort to an item under global budget constraint. As result, the offline trajectory allocation problem can be formulated as learning-while-allocating problem, and we can directly adopt the Bayesian framework of Chen et al. [2013]. To sum up, the learner maintains posterior over the answer distribution of each question, analogous to the posterior over item labels in crowdsourcing, and sequentially decides which question to sample next. (b) Beyond standard majority voting, we extend the Bayesian allocation framework to confidenceweighted aggregation, strategy that has recently gained popularity in test-time scaling and self-consistency methods, e.g., [Fu et al., 2025a]. We remark that the weighted majority voting formulation in [Fu et al., 2025a] differs from the worker-reliability weighting scheme considered in [Chen et al., 2013], and thus requires additional care when extending the crowdsourcingbased allocation framework. Bayesian Setup for Trajectory Allocation. We formalize offline allocation as Bayesian decision and confidence problem. For each question qi, each sampled trace yields an answer Yi 5 Table 1: Connection to crowdsourcing. Crowdsourcing Item i, Worker label (b) Test-time self-consistency Question qi, Trace answer (b) Adaptive worker assignment to items Adaptive trajectory sampling for questions Posterior over item labels (and worker reliabilities) Posterior over answer distribution (and confidence weights) Worker-specific reliability Goal: infer the true label Trace confidence without persistent Worker identity Goal: infer the population majority answer yW , ) (or M1 denote the (unknown) answer distribution, and let score Ci = w(Ti) R+. Let θi µi = (µi,1, . . . , µi,M) RM be the (unknown) class-conditional confidence means (we fix σ 2 = 1 for simplicity). We make natural assumption that answers follow categorical distribution, and confidence is Gaussian with mean depending on the sampled answer. The joint likelihood for single trace-induced pair (Yi, Ci) factorizes as p(y, θi, µi) = p(y θi) p(c y, µi), Therefore, the weighted population-optimal label is yW , = arg max mY θi,mµi,m. The weighted self-consistency rate (2) can equivalently writes in the following form (cid:18) SCW (qi; B) = (B) = yW , WMaj (cid:12)(cid:12)(cid:12)(cid:12) θi, µi (cid:19) , (4) Since latent parameters (θi, µi) are unknown and ground-truth labels are unavailable, direct quantification of the objective (3) is unclear. We thus adopt Bayesian framework that maintains posterior over the latent parameters, for us to quantify self-consistency under posterior belief. Modeling as an MDP process. We model offline trajectory allocation as finite-horizon Bayesian Markov Decision Process (MDP), following the crowdsourcing formulation of Chen et al. [2013], with adaptations to accommodate confidence-weighted traces. At stage t, the state St summarizes the posterior beliefs over (θi, µi) for each question via their respective posterior hyperparameters. [N ] selects question to which one additional trace is allocated. After allocating The action it trace to question it, we observe (yt, ct), which induces conjugate Bayesian update of the posterior parameters in St. The process has fixed horizon H, corresponding to the total trace budget Btotal. The terminal reward is defined as the sum of posterior self-consistency across questions. {(cid:98)yi }N i=1 = arg max } {(cid:98)yi N(cid:88) i=1 6 P((cid:98)yi = yW , SH ) (5) which measures the posterior probability of recovering the (weighted) population-majority answer for each question. Please refer to Appendix A.1 for further details on MDP formulation in our paper. Lemma 3.1 (Bayes-optimal terminal decision). Given the terminal belief SH , the Bayes-optimal decision for each question is therefore (cid:98)yi arg max m[M] (cid:16) yW , = SH (cid:17) , (6) The posterior probability P(yW , = SH population majority label, which can be estimated via Monte Carlo sampling. ) quantifies the posterior belief that class is the Under the terminal decision rule (6), we seek an allocation policy that maximizes the expected terminal utility: (S0) = π (cid:20) N(cid:88) i= max m[M] (cid:16) yW , Pr = SH (cid:17) (cid:21) , (7) where the expectation is taken over all sample paths induced by policy π = (i0, . . . , iH1), which sequentially selects question to allocate one additional budget at each time step. Approximation of dynamic programming. Exact dynamic programming of Equation (7) is intractable due to the exponentially growing belief space. Our PETS-Offline therefore similarly adopt the Optimistic Knowledge Gradient (OKG) heuristic [Chen et al., 2013], which selects the question with the largest optimistic one-step improvement in the terminal utility of Equation (7). We provide the details and full algorithm (Algorithm 2) in Appendix A.2."
        },
        {
            "title": "4 Online PETS in the Streaming Setting",
            "content": "The key idea of online allocation is to assign different numbers of samples to questions with different answer distribution parameters θ (i.e., difficulty vectors) in one shot upon seeing the question. Unlike the offline setting in Section 3, where θ and voting weights can be estimated during the allocation process, the main challenge in the online regime is the lack of information about how the current questions difficulty compares to that of future questions, given the online nature. In this setting, we need to have access to prior distribution over question difficulties via additional training data that are assumed to be drawn from the same distribution. Consequently, when new question arrives, its budget could be determined immediately based solely on its estimated difficulty label and the prior distribution. To simplify the problem, we restrict our attention to the setting where we know that there will be questions in the upcoming estimation period, although we do not know their exact content or arrival times. This assumption is common in practice, since model deployers can often estimate the query volume over given time window, for example, based on historical usage statistics, service-level forecasts, or system capacity planning. } arriving sequentially. Each Mathematical formulation. There are questions {q1, q2, . . . , qN question qt is associated with difficulty vector θt = θ(qt) as defined in Section 2. With total budget Btotal, upon observing θt for question t, an allocation policy π assigns budget 7 Bt = Bπ(qt) = Bπ(θt) without access to future questions or any intermediate feedback from other questions. Therefore, in the online setting, π can only depend on the realized prior distribution and the current difficulty label θt."
        },
        {
            "title": "4.1 Execution Protocol",
            "content": "We here describe the execution protocol of PETS-Online. At high level, the procedure consists of estimating ❶ the distribution of problem parameters, ❷ the mapping from each incoming question to its corresponding parameter, and ❸ solving budget allocation optimization problem based on these estimates to obtain the final allocation plan. Estimation of parameters and sample distribution. We estimate question difficulty in 2 steps. First, we reduce the original (M 1)-dimensional difficulty parameter θi to two-dimensional surrogate (ai, bi) via Gaussian approximation and discretize the reduced space into difficulty grids with prototype parameters {(cid:98)θj}K j=1. Second, we use lightweight warm-up procedure to assign each incoming question to grid, after which the corresponding budget is allocated in one shot. The prior mass of each grid is estimated from training data by applying the same warm-up procedure and computing empirical grid frequencies. (See Appendix B.3 for the full approximation method and Appendix B.4 for the full warm-up procedure.) Optimization framework. After discretization, the self-consistency rate in (3) for question qi under budget can be approximated as3 (cid:18) SC(θbuc(qi); B) = Maj (B) = (cid:19) θbuc(qi) . (8) An online streaming policy π is therefore fully specified by vector of integer budgets {B1, . . . , BK }, where any question assigned to grid receives budget Bj. Over horizon of streaming questions with total budget Btotal, the optimal allocation problem can be rewritten as K(cid:88) j=1 max }K {Bj j=1 (cid:98)pj SC((cid:98)θj; Bj) s.t. K(cid:88) j= (cid:98)pjBj Btotal, (9) where Btotal := Btotal/N is the average budget per round, (cid:98)pj is the estimated distribution, and (cid:98)θj is the representation parameter of the grid j. Finally, we note that although (9) is static optimization problem, our final policy is dynamic streaming policy which calls (9) at every round; we refer to Appendix B.1 for more details."
        },
        {
            "title": "4.2 Optimal Budget Allocation",
            "content": "In this section, we present an efficient greedy algorithm that solves the integer program (9). We start with the binary-choice case (M = 2) in Algorithm 1, and extend it to the multi-choice case in Appendix B.3. For binary-choice question qi, the choice set contains only two choices 3A weighted version can be defined analogously. Since weights are difficult to estimate in the online setting, in experiments, we use the unweighted version for policy optimization and apply weighted majority voting only at inference time. 8 = {0, 1}, and the difficulty vector reduces to scalar parameter θ = P[Yi = 1] [0, 1]. Under i.i.d. samples and majority voting (with random tie-breaking when is even), we obtain Maj P[Y (B) = 1 θ] = (cid:80)B (cid:80)B b=(B+1)/2 (cid:0)B (cid:1)θb(1 θ)Bb, odd b=B/2+ (cid:0)B (cid:1)θb(1 θ)Bb + 1 2 (cid:0) B/2 (cid:1)θB/2(1 θ)B/2, even (10) Maj and P[Y then (B) = 0 θ] = 1 P[Y Maj (B) = 1 θ]. The corresponding self-consistency rate (1) is (cid:110) SC(θ; B) = max Maj P[Y (B) = 1 θ], P[Y (cid:111) (B) = 0 θ] , Maj (11) where SC(θ; 0) = 1 2 . We define the marginal gain R(θ, n) at budget level for question with difficulty parameter θ as the increase in self-consistency rate resulting from allocating one additional unit of budget: R(θ, n) := SC(θ; + 1) SC(θ; n). We propose greedy yet optimal algorithm (Algorithm 1) that repeatedly allocates budget to the grid with the largest current marginal improvement. Algorithm 1 Greedy Budget Allocation Algorithm Require: Grid (θj)K j=1, probabilities (pj)K average per-round budget Btotal j=1, 0 for all [K]. R(θj, 0) for all [K]. 1: Initialize current budget Bj 2: Initialize current marginal gain δj 3: Initialize total used budget 0. 4: while Btotal do arg maxj δj 5: if Bj = 0 then 6: 7: 8: 9: 10: Bj 1, + pj else Bj Bj + 2, + 2pj end if Update δj R(θj , Bj ) 11: 12: end while Theorem 4.1. Algorithm 1 outputs an optimal solution to the discretized online allocation problem (9) in expectation, with randomized rounding rule."
        },
        {
            "title": "4.2.1 Connection with the Offline Case",
            "content": "Although the offline and online cases rely on different allocation procedures, their budget proportions become nearly identical as the budget grows. As shown in Figure 2, increasing the average per-question budget leads the two allocations to converge to similar proportions. Further discussion and theoretical support are provided in Appendix C. 4This is because the remaining budget may not suffice for another iteration. See Appendix B.1 for details. 9 Figure 2: Budget allocation plan of the offline and online settings on 9 simulated binary choice questions, = {1, 2}. Each question is associated with θ = max(θ1, θ2), and larger theta indicates easier questions. Table 2: PETS-Offline Results. In the offline setting, we compare PETS against uniform sample budget allocation. (conf) denotes the trace confidence-weighted variant. # traces reports the number of sampled traces required to reach full consistency (consistency = 1); Con. and Acc. report each methods achieved consistency and accuracy evaluated at the trace count where PETS-Off. attains consistency 1. Dataset Method Qwen3-4B # traces Con. Acc. Qwen3-30B Qwen-Long GPT-20B GPT-120B # traces Con. Acc. # traces Con. Acc. # traces Con. Acc. # traces Con. Acc. GPQA AIME25 AIME HMMT BRUMO PETS-Off. PETS-Off. (conf) Uniform Uniform (conf) PETS-Off. PETS-Off. (conf) Uniform Uniform (conf) PETS-Off. PETS-Off. (conf) Uniform Uniform (conf) PETS-Off. PETS-Off. (conf) Uniform Uniform (conf) PETS-Off. PETS-Off. (conf) Uniform Uniform (conf) 2780 3667 11013 11453 212 257 470 610 259 369 861 464 579 1133 1383 280 292 826 589 1.00 0.99 0.96 0.95 1.00 0.98 0.94 0.91 1.00 0.96 0.94 0.92 1.00 0.96 0.90 0. 1.00 0.98 0.94 0.94 69.7 70.3 68.9 69.9 83.3 84.0 82.1 83.3 63.1 66.0 63.0 63.4 51.6 52.0 50.0 50.6 75.4 77.3 73.2 75. 2607 3687 10367 11607 190 223 545 763 135 120 202 179 381 361 1219 1089 148 162 237 310 1.00 0.99 0.96 0. 1.00 0.97 0.96 0.93 1.00 1.00 0.98 0.97 1.00 0.97 0.91 0.91 1.00 0.98 0.96 0.95 71.8 72.1 71.5 72.2 88.6 86.7 86.1 85. 70.0 69.9 69.6 68.6 64.7 65.2 64.7 65.1 86.7 86.1 83.7 83.6 2513 2887 9973 11400 152 203 288 752 83 91 96 363 371 1227 1165 149 150 320 421 1.00 0.99 0.96 0.95 1.00 0.97 0.97 0.94 1.00 0.99 0.99 0.98 1.00 0.97 0.91 0. 1.00 0.98 0.97 0.97 76.3 77.0 76.5 76.4 90.0 90.4 89.3 88.9 70.0 69.9 69.9 69.7 71.7 73.2 68.8 71.7 86.7 87.8 87.3 87. 3180 3693 10853 11193 181 251 410 618 200 232 665 1206 329 324 916 902 253 217 776 574 1.00 0.99 0.96 0. 1.00 0.97 0.95 0.94 1.00 0.97 0.96 0.94 1.00 0.99 0.94 0.93 1.00 0.98 0.94 0.95 75.7 76.2 75.1 75.6 90.0 89.7 90.2 89. 73.3 72.6 71.9 70.8 83.3 83.1 83.6 82.6 92.1 92.4 90.1 90.7 2580 3540 10393 10500 212 211 681 911 184 218 448 329 324 916 902 253 217 776 574 1.00 0.99 0.97 0.96 1.00 0.98 0.95 0.94 1.00 0.99 0.97 0.95 1.00 0.99 0.94 0. 1.00 0.98 0.94 0.95 82.5 82.1 81.5 80.8 93.9 93.3 91.3 91.9 74.4 73.7 73.7 71.9 83.3 83.1 83.6 82.6 92.1 92.4 90.1 90. Figure 3: Budget allocation curve in the offline setting. (conf) denotes the trace confidence-weighted variant. Consistency is computed within each matched-variant comparison group: PETS-Offline vs. Uniform, and PETS-Offline (conf) vs. Uniform (conf). 10 Table 3: PETS-Online Results. In the online setting, we compare PETS against uniform sample budget allocation. (conf) denotes the trace confidence-weighted variant. # traces reports the number of sampled traces required to reach full consistency (consistency = 1); Con. and Acc. report each methods achieved consistency and accuracy evaluated at the trace count where PETS-On. attains consistency 1. Dataset Method Qwen3-4B # traces Con. Acc. Qwen3-30B Qwen-Long GPT-20B GPT-120B # traces Con. Acc. # traces Con. Acc. # traces Con. Acc. # traces Con. Acc. GPQA AIME25 AIME24 HMMT BRUMO PETS-On. PETS-On. (conf) Uniform Uniform (conf) PETS-On. PETS-On. (conf) Uniform Uniform (conf) PETS-On. PETS-On. (conf) Uniform Uniform (conf) PETS-On. PETS-On. (conf) Uniform Uniform (conf) PETS-On. PETS-On. (conf) Uniform Uniform (conf) 4662 4888 9520 194 287 290 452 170 185 393 405 531 615 792 931 601 494 613 519 1.00 1.00 0.97 0.97 1.00 0.95 0.95 0. 1.00 0.96 0.94 0.94 1.00 0.93 0.89 0.89 1.00 0.97 0.94 0.93 68.8 69.6 68.1 69.4 75.0 76.3 74.8 76.3 56.2 60.0 56.8 58. 48.5 46.8 46.2 45.7 63.2 64.7 61.5 65.5 3826 4357 8456 9257 504 535 415 486 81 81 83 83 825 761 813 125 121 172 159 1.00 0.99 0.97 0.96 1.00 0.97 0.95 0.94 1.00 1.00 1.00 1.00 1.00 0.95 0.93 0.92 1.00 0.99 0.96 0. 71.6 71.8 71.2 72.0 83.5 82.5 81.0 81.2 65.0 64.8 64.7 64.7 56.2 56.8 56.2 56.7 80.0 79.5 77.7 77.7 4852 4146 9195 218 681 229 495 83 86 82 101 872 1061 803 959 130 138 167 189 1.00 1.00 0.97 0.97 1.00 0.96 0.95 0. 1.00 0.99 1.00 0.99 1.00 0.95 0.96 0.95 1.00 0.99 0.97 0.98 76.3 76.9 76.0 76.4 85.0 85.7 84.7 83.5 65.0 64.8 65.0 64. 67.0 68.8 66.7 67.8 80.2 81.5 80.7 81.3 6826 7269 9699 9895 195 330 221 349 130 104 237 171 248 233 475 213 162 245 184 1.00 0.99 0.98 0.98 1.00 0.96 0.96 0.93 1.00 0.99 0.98 0.98 1.00 0.97 0.92 0.92 1.00 0.99 0.95 0. 75.9 76.1 76.1 76.3 85.0 84.8 86.0 85.2 70.0 68.8 67.7 68.0 85.0 85.5 83.3 83.2 95.0 94.7 91.7 91.7 4172 5094 8109 454 493 416 459 107 92 205 144 221 263 309 458 383 241 521 353 1.00 0.99 0.98 0.97 1.00 0.98 0.95 0. 1.00 1.00 0.97 0.98 1.00 0.96 0.95 0.91 1.00 0.99 0.92 0.94 82.4 81.6 81.8 81.4 91.7 92.3 89.2 89.8 69.8 69.5 67.0 67. 85.0 86.5 84.8 85.0 88.8 89.7 86.3 87.2 Figure 4: Budget allocation curve in the online setting. (conf) denotes the trace confidence-weighted variant. Consistency is computed within each matched-variant comparison group: PETS-Online vs. Uniform, and PETS-Online (conf) vs. Uniform (conf). Oracle variant assumes access to the latent parameter θ, while in the online setting, θ is learnt from training dataset."
        },
        {
            "title": "5 Experiment",
            "content": "In this section, we conduct evaluation of PETS on widely used knowledge and reasoning benchmarks, including GPQA-Diamond [Rein et al., 2024], AIME 24 [hug, 2025], AIME 25 [mat, 2025], Brumo 25 [mat, 2026a], and HMMT Feb 25 [mat, 2026b]. We consider popular reasoning LLMs: Qwen3-4B-Thinking and Qwen3-30B-A3B-Thinking [Yang et al., 2025], gpt-oss-20b and gpt-oss-120b [Agarwal et al., 2025], and QwenLong-L1.5-30B-A3B [Shen et al., 2025]. Following our offline budget allocation (Section 3) and online budget allocation (Section 4), we evaluate 11 PETS under the offline and online scenarios. For each dataset, we sample 128 responses per question from each model. To define maximum allocation budget of 64 traces per question as the finite proxy for the infinite sampling limit, we uniformly subsample 64 responses from the 128. We repeat this subsampling process 30 times and report the mean performance. We also include confidence weighted version in our experiment, where we follow Fu et al. [2025b] to compute tail trace confidence Ci = w(Ti) as the weight for weighted majority voting at aggregation time (In PETS-offline we also involve it at sampling time as shown in Section 3). We evaluate self-consistency rate as defined in Equation 1 and Equation 2. Our primary metric is the number of traces required to reach full self-consistency (= 1), where the majority answer matches the population majority answer in the weighted version), approximated using 64 traces. We also report accuracy on the dataset. More details and full results are provided in Appendix D. (or yW , i"
        },
        {
            "title": "5.2 PETS for Online Budget Allocation",
            "content": "We next evaluate PETS in the online case, where questions arrive sequentially, and allocation decisions are irreversible. Besides Uniform, we also compare PETS-Online with PETS-Oracle, which can access the latent difficulty θ and thus serves as an upper bound without estimation error. As in the offline setting, we also consider both unweighted and confidence-weighted versions. PETS-Online allocates budget on the fly using only observed traces. Results are shown in Table 3 and Figure 4. Consistent with offline results, ❶ PETS-Online substantially reduces the traces required to reach full self-consistency, ❷ leading to improved accuracy. ❸ Confidence-weighted self-consistency further boosts accuracy by reshaping the population majority. ❹ PETS-Online closely matches PETS-Oracle in both self-consistency and accuracy, indicating that the estimation procedure in Appendix B.4 is effective."
        },
        {
            "title": "6 Related Work",
            "content": "Test-time scaling improves performance by aggregating parallel reasoning trajectories, e.g., via self-consistency or Best-of-N selection [Wang et al., 2022; Brown et al., 2024], but incurs high computational cost. Existing efficient methods rely on heuristic confidence or difficulty filtering [Fu et al., 2025a; Wang et al., 2025a] and lack guarantees. Closely related is the crowdsourcing literature, which studies sample-efficient inference from noisy parallel annotations using probabilistic models and adaptive budget allocation [Dawid & Skene, 1979; Chen et al., 2013], which we connect to efficient parallel reasoning in LLMs. Recent work further explores aggregation rules, failure modes under imperfect reward models [Huang et al., 2025a; Zuo & Zhu, 2025]. In contrast, we optimize test-time trace allocation by directly maximizing self-consistency against an infinite-compute voting oracle, without relying on reward model. Test-Time Scaling. Since the emergence of long-reasoning models such as GPT-o1 [Jaech et al., 2024] and DeepSeek-R1 [Guo et al., 2025], test-time scaling [Snell et al., 2024; Welleck et al., 2024] has gained traction as way to improve performance by allocating substantially more computation (e.g., reasoning tokens) at inference time. One line of work scales the length of chain-of-thought (CoT) trajectory by extending the reasoning process [Wei et al., 2022]; representative examples include GPT-o1 [Jaech et al., 2024], DeepSeek-R1 [Guo et al., 2025], Kimi K2 [Team et al., 2025], Qwen3 [Yang et al., 2025], gpt-oss [Agarwal et al., 2025], and Gemini 3 Pro [gem]. Another line of work scales via parallel sampling of multiple trajectories, leveraging self-consistency [Wang et al., 2022] or Best-of-N selection [Brown et al., 2024] and aggregating outputs (e.g., by majority vote). In this paper, PETS focuses on the intersection of these two lines and considers the efficient selection of multiple long reasoning trajectories. Efficient Reasoning. Although test-time scaling can improve performance, its increased inference cost has motivated works on efficient reasoning. Along the line of CoT extension, Muennighoff et al. [2025] proposes budget forcing to control test-time compute by terminating the models reasoning once preset budget is reached. Other approaches aim to elicit shorter yet effective reasoning by fine-tuning with condensed CoT traces [Chen et al., 2024a; Luo et al., 2025; Hou et al., 2025; Zhang et al., 2025]. Along the parallel sampling direction, several methods introduce more efficient variants of self-consistency that reduce the number of trajectories while maintaining accuracy [Li et al., 2024; Wan et al., 2025; Fu et al., 2024]. Most closely related to our work, DeepConf [Fu et al., 2025a] leverages per-trajectory confidence to filter low-confidence traces and terminate generation when confidence falls below threshold. Crowdsourcing. Crowdsourcing has been extensively studied as budgeted labeling problem, with classical models such as DawidSkene capturing item difficulty and annotation noise [Dawid & Skene, 1979]. Building on these probabilistic foundations, line of work has focused on adaptive task assignment and budget allocation, where items are sequentially selected for labeling based on expected benefit [Sheng et al., 2008]. In particular, Chen et al. [2013] formulate crowdsourcing as Bayesian decision process and propose the optimistic knowledge gradient (OKG) policy, providing principled approach to optimal budget allocation under majority-style aggregation. However, these methods are primarily designed for human annotators, and few works have extended them to the setting of test-time consistency in large language models. 13 Computational Resources Allocation. common family of approaches generates multiple candidate answers and then selects or aggregates them. Komiyama et al. [2025] analyze majorityvote best-of-N in the asymptotic regime , and propose an adaptive generation/earlystopping rule based on answer agreement, including extensions to weighted multi-LLM ensembles. Complementarily, Chen et al. [2024b] provide simple two-stage aggregation schemes (knockout and league) with provable failure-probability decay as test-time compute grows, using the LLM itself as black-box generator and pairwise comparator. When selection relies on an imperfect reward/judge model, scaling can backfire. Huang et al. [2025a] formalize inference-time alignment through Best-of-N sampling and show that large can induce reward hacking; they propose pessimistic alternative with scaling-monotonic guarantees. Beyond per-question scaling, allocating compute across queries is crucial under global budget. Zuo & Zhu [2025] cast Test-time scaling allocation as bandit learning problem that adapts compute online to query difficulty and solvability. Other work targets different compute axes: Lin et al. [2025] study budget tokens within single reasoning process via planning and uncertainty-aware scheduling, while Wang et al. [2025b] study efficient test-time scaling for search by allocating rollouts at the direction level to avoid candidate-count bias. Finally, several Bayesian works address reliability and evaluation. Yao et al. [2024] treat multiple LLMs as annotators and extend DawidSkene for uncertainty-aware truth aggregation. Gao et al. [2024] calibrate win-rate estimates from LLM evaluators, mitigating bias in LLM-asjudge comparisons. In contrast to reward-based TTS, our work focuses on budget allocation to optimize self-consistency defined against an infinite-compute (weighted) voting oracle, enabling principled allocation without depending on potentially misspecified reward model."
        },
        {
            "title": "7 Conclusion",
            "content": "We present PETS, principled framework that improves the efficiency of parallel sampling with self-consistency without sacrificing performance. Across challenging benchmarks and stateof-the-art reasoning models, PETS consistently increases self-consistency and accuracy while substantially reducing the number of required reasoning trajectories. Our experiments reveal that while majority voting can approximate the population majority and improve accuracy when the population majority matches the true answer, but when the majority is systematically wrong, additional sampling provides little benefit, revealing limitation of allocation-only approaches. Finally, while PETS-Online estimates the latent question difficulty θ via short warm-up phase, an important next step is to train models to predict θ directly from the question prior to generation."
        },
        {
            "title": "8 Acknowledgement",
            "content": "Z. Deng, Y. Han, and T. Chen are grateful for the kind support of Renaissance Philanthropy AI4Math fund."
        },
        {
            "title": "References",
            "content": "Gemini 3 Pro. URL https://deepmind.google/models/gemini/pro/. HuggingFaceH4/aime_2024 Datasets at Hugging Face, July 2025. URL https://huggingface. co/datasets/HuggingFaceH4/aime_2024. mathai/aime25 Datasets at Hugging Face, November 2025. URL https://huggingface.co/ datasets/math-ai/aime25. MathArena/brumo_2025 Datasets at Hugging Face, January 2026a. URL https:// huggingface.co/datasets/MathArena/brumo_2025. MathArena/hmmt_feb_2025 Datasets at Hugging Face, January 2026b. URL https:// huggingface.co/datasets/MathArena/hmmt_feb_2025. Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Ré, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Chen, X., Lin, Q., and Zhou, D. Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing. In Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 6472, Atlanta, Georgia, USA, 1719 Jun 2013. PMLR. URL https: //proceedings.mlr.press/v28/chen13f.html. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024a. Chen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Simple and provable scaling laws for the test-time compute of large language models. arXiv preprint arXiv:2411.19477, 2024b. Dawid, A. P. and Skene, A. M. Maximum likelihood estimation of observer error-rates using the em algorithm. Applied Statistics, 28(1):2028, 1979. URL /brokenurl#http://links.jstor. org/sici?sici=0035-9254%281979%2928%3A1%3C20%3AMLEOOE%3E2.0.CO%3B2-0. Esary, J. D., Proschan, F., and Walkup, D. W. Association of random variables, with applications. The Annals of Mathematical Statistics, 38(5):14661474, 1967. Fu, Y., Chen, J., Zhu, S., Fu, Z., Dai, Z., Zhuang, Y., Ma, Y., Qiao, A., Rosing, T., Stoica, I., et al. Efficiently scaling llm reasoning with certaindex. arXiv preprint arXiv:2412.20993, 2024. Fu, Y., Wang, X., Tian, Y., and Zhao, J. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025a. Fu, Y., Wang, X., Tian, Y., and Zhao, J. Deep think with confidence, 2025b. URL https: //arxiv.org/abs/2508.15260. 15 Gao, Y., Xu, G., Wang, Z., and Cohan, A. Bayesian calibration of win rate estimation with LLM evaluators. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 47574769, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645 (8081):633638, 2025. Hou, B., Zhang, Y., Ji, J., Liu, Y., Qian, K., Andreas, J., and Chang, S. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. Huang, A., Block, A., Liu, Q., Jiang, N., Krishnamurthy, A., and Foster, D. J. Is best-of-n the best of them? coverage, scaling, and optimality in inference-time alignment. In Proceedings of the 42nd International Conference on Machine Learning (ICML), volume 267 of Proceedings of Machine Learning Research, pp. 2507525126, 2025a. Huang, C., Huang, L., Leng, J., Liu, J., and Huang, J. Efficient test-time scaling via self-calibration. arXiv preprint arXiv:2503.00031, 2025b. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Komiyama, J., Oba, D., and Oyamada, M. Best-ofasymptotic performance of test-time compute. arXiv preprint arXiv:2509.21091, 2025. Li, Y., Yuan, P., Feng, S., Pan, B., Wang, X., Sun, B., Wang, H., and Li, K. Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. arXiv preprint arXiv:2401.10480, 2024. Lin, J., Zeng, X., Zhu, J., Wang, S., Shun, J., Wu, J., and Zhou, D. Plan and budget: Effective and efficient test-time scaling on large language model reasoning. arXiv preprint arXiv:2505.16122, 2025. Luo, H., Shen, L., He, H., Wang, Y., Liu, S., Li, W., Tan, N., Cao, X., and Tao, D. O1pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2028620332, 2025. Raič, M. multivariate berryesseen theorem with explicit constants. Bernoulli, 25(4A): 28242853, 2019. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Shen, W., Yang, Z., Li, C., Lu, Z., Peng, M., Sun, H., Shi, Y., Liao, S., Lai, S., Zhang, B., et al. Qwenlong-l1. 5: Post-training recipe for long-context reasoning and memory management. arXiv preprint arXiv:2512.12967, 2025. Sheng, V. S., Provost, F., and Ipeirotis, P. G. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 614622, 2008. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Thang Luong and Edward Lockhart. Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad. https://deepmind.google/blog/advanced-version-of-gemini-with-deep-thinkofficially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/. Wan, G., Wu, Y., Chen, J., and Li, S. Reasoning aware self-consistency: Leveraging reasoning paths for efficient llm sampling. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 36133635, 2025. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Wang, X., Feng, S., Li, Y., Yuan, P., Zhang, Y., Tan, C., Pan, B., Hu, Y., and Li, K. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 69046917, 2025a. Wang, X., Li, Y., Feng, S., Yuan, P., Zhang, Y., Shi, J., Tan, C., Pan, B., Hu, Y., and Li, K. Every rollout counts: Optimal resource allocation for efficient test-time scaling. arXiv preprint arXiv:2506.15707, 2025b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Welleck, S., Bertsch, A., Finlayson, M., Schoelkopf, H., Xie, A., Neubig, G., Kulikov, I., and Harchaoui, Z. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yao, P., Mathew, J. G., Singh, S., Firmani, D., and Barbosa, D. bayesian approach towards crowdsourcing the truths from LLMs. In NeurIPS 2024 Workshop on Behavior Discovery and Understanding (BDU), 2024. URL https://openreview.net/forum?id=oRW8i4EF0Z. OpenReview. Zhang, R., Khan, R. M. S., Tan, Z., Li, D., Wang, S., and Chen, T. The quest for efficient reasoning: data-centric benchmark to cot distillation. arXiv preprint arXiv:2505.18759, 2025. Zuo, B. and Zhu, Y. Strategic scaling of test-time compute: bandit learning approach. arXiv preprint arXiv:2506.12721, 2025. 17 Appendix of Offline Case (Section 3) A.1 Bayesian MDP We place independent priors on θi and µi: θi Dir(α0 ), α0 RM + , and µi,m (β0 i,m, v0 i,m), v0 i,m > 0, [M], µi priori. The Dirichlet parameters α0 with θi The Gaussian parameters (β0 confidence mean µi,m. At stage t, let St denote the global belief state, which collects posterior parameters for all questions: i,m can be viewed as pseudo-counts for class m. i,m) specify the prior mean and variance of the class-conditional i,m, v0 St (cid:16){αt }N i=1, {βt }N i=1, {vt }N i=1 (cid:17) , βt = (βt i,1, . . . , βt i,M), vt = (vt i,1, . . . , vt i,M). In the fully observed offline setting, each observation reveals (Yi, Ci) for the sampled trace, so αt admits Dirichlet-multinomial conjugate update and (βt ) admit Gaussian conjugate updates conditioned on Yi . At each state St, we would select question it and observe (Yt, Ct) = (yt, ct), the Dirichlet parameters update via the conjugate CategoricalDirichlet rule , vt αt+1 = αt + δyt , αt+1 = αt (cid:44) i, RM is the one-hot vector with 1 at entry m. where δm For the confidence model, only the component µi,yt is updated. Recall that under state St, we maintain µi,m St (βt i,m), [M], i,m, vt (Yt = m, µi,m) (µi,m, 1). The NormalNormal and we assume the observation model Ct conjugate update yields vt+1 i,yt = 1 vt i,yt + 1 1 , βt+1 i,yt = vt+1 i,yt βt i,yt vt i,yt , + ct while (βt+1 i,m , vt+1 i,m ) = (βt i,m, vt i,m) for all (cid:44) yt, and (βt+1 , vt+1 ) = (βt , vt ) for all (cid:44) i. Given belief state St and action it = i, the posterior distribution of the next label is Pr(Yt = St, it = i) = E[θi,m αt ] = αt i,m (cid:80)M k=1 αt i,k . Moreover, conditional on Yt = m, the posterior predictive confidence is Gaussian: (Yt = m, St, it = i) (cid:16) i,m, 1 + vt βt Ct i,m (cid:17) , Together, the posterior predictive distribution and the conjugate update rules characterize the state transitions and transition probabilities of the resulting Bayesian MDP. 18 A.2 Algorithm of PETS-Offline We introduce the PETS-Offline algorithm here. Before doing that, we will give further explanation on R+(St ), the expected one-step utility gain. ), which is an optimistic approximation of R(St (cid:104) (St+1 ) (St ) R(St Yt,Ct ) = M(cid:88) = P(Yt = St, it = i) (cid:105) (cid:12)(cid:12)(cid:12) St, it = (cid:104) (St+1 Ct ) (St ) (cid:12)(cid:12)(cid:12) St, it = i, Yt = (cid:105) m=1 Here the per-question terminal utility (St ) under belief St (αt , βt , vt ) is defined as (St ) max m[M] (cid:16) yW , Pr = St (cid:17) , In the optimistic version following Chen et al. [2013], we use R+(St ) to approximate R(St ) to avoid the complex computation of P(Yt = St, it = i) and to achieve consistency of the algorithm as has been proven in Chen et al. [2013]. In our setting where we have an additional parameters governing the posterior distribution of Ct, to relieve the computational burden, we make further simplification: R+ (St) max m[M] = max m[M] max m[M] (cid:18) (cid:104) (St+1 ) (St ) (cid:12)(cid:12)(cid:12) St, it = i, Yt = (cid:105) (cid:18) Ct St,it=i,Yt=m (Upd(St ; m, βt (cid:104) (Upd(St ; m, Ct)) (cid:19) i,m)) (St ) , (cid:19) (cid:105) (St ) Here Upd(St ; m, c) denotes the posterior update obtained by hypothetically observing (Yt = m, Ct = c), and the approximation plugs in the predictive mean E[Ct i,m. This policy is myopic (and thus not globally optimal in general), but provides principled and computationally efficient heuristic for adaptive budget allocation. St, it = i, Yt = m] = βt 19 Algorithm 2 Optimistic Knowledge Gradient (confidence-weighted Bayesian setting) Require: Priors {(α0 , v0 1: for = 0, 1, . . . , 1 do 2: Compute per-question optimistic utility gain for all [N ]: i=1, total budget , plug-in parameter κ 0 , β0 )}N (cid:18) (St) max R+ m[M] (Upd(St ; m, βt (cid:19) i,m)) (St ) Select it = arg maxi[N ] R+ (St). Query the LLM on question it and observe (yt, ct) with yt Dirichlet update (label): αt+1 Normal update (confidence mean for class yt): = αt it + δyt . it [M], ct R+. vt+1 it,yt = 1 vt it,yt + 1 , βt+1 it,yt = vt+1 it,yt βt it,yt vt it,yt . + ct 3: 4: 5: 6: 7: Keep all other parameters unchanged: for (cid:44) it, set (αt+1 , βt+1 , vt+1 ) = (αt j, βt , vt ), and for (cid:44) yt, set (βt+ it,m, vt+1 it,m) = (βt 8: end for 9: return Terminal labels (cid:98)yi it,m). it,m, vt arg maxm[M] Pr(yW , = ST ) for all [N ]. 1, the unweighted variant of our algorithm was proved to be consistent In the special case Ct by Chen et al. [2013]. Note that key component in this algorithm is to compute P(yW , achieved via Monte Carlo sampling. = mSt ), which can be A.3 Proof of lemma 3.1 Proof of Lemma 3.1. In the offline trajectory allocation setting, after allocating total of traces, we output terminal label (cid:98)yi for each question [N ]. Given the terminal posterior (belief) state SH = {αH }N i=1, our goal is to maximize the conditional expected self-consistency rate: {(cid:98)yi }N i= arg max }N {(cid:98)yi i=1 = arg max }N i=1 {(cid:98)yi N(cid:88) (cid:16) (cid:98)yi = yW , H (cid:17) i=1 N(cid:88) M(cid:88) i=1 m=1 I((cid:98)yi = m) (cid:16) yW , = SH (cid:17) , (12) where {F t}H and the second equality follows because conditioning on fixes the terminal posterior SH each i. t=0 denotes the filtration generated by the sample path (i0, yi0, ci0 . . . , it1, yit1, cit1), for Now observe that the objective in (12) is separable across questions. Indeed, for each fixed i, the 20 decision variable (cid:98)yi only appears in the i-th summand: I((cid:98)yi = m) (cid:16) = SH (cid:17) . M(cid:88) m=1 Therefore, maximizing the total sum is equivalent to maximizing each term independently. For any question i, we have (cid:98)yi arg max (cid:98)y[M] = arg max m[M] M(cid:88) (cid:16) yW , I((cid:98)y = m) = SH (cid:17) m=1 (cid:16) yW , = SH (cid:17) , which proves the claim. Appendix of Online Case (Section 4) B.1 Details of Algorithm 1 Streaming Policy. Recall that the discretized formulation (9) yields static grid-level allocation j=1 (cid:98)pjBj Btotal/N . In the actual online vector (B1, . . . , BK ) under the average budget constraint streaming setting, however, the policy must remain feasible under remaining-budget constraint that evolves with time. We therefore implement the online policy as state-dependent dynamic re-planning rule. (cid:80)K At time t, define the remaining total budget and the remaining horizon Rt Z0, R1 := Btotal, Ht := + 1. Equivalently, the remaining per-question average budget is Bt :="
        },
        {
            "title": "Rt\nHt",
            "content": ". Upon observing qt, the policy computes grid-level budget vector by solving (approximately) the discretized allocation problem under the current average budget Bt: (t) ) GreedyAlloc (cid:16){(cid:98)pj, (cid:98)θj}K (t) 1 , . . . , j=1, Bt (13) (cid:17) , (B where GreedyAlloc is Algorithm 1 (run with capacity Bt) and returns an integer vector satisfying K(cid:88) j=1 (cid:98)pj (t) Bt. (t) The actual budget spent on the current question is then bt := jt qt, the remaining budget updates as . After spending bt on question Rt+1 := Rt bt. 21 By construction, the policy always respects the global budget constraint Rt is nonnegative and decreases by exactly the spent amount each step. (cid:80)N t=1 bt Btotal: indeed, The rule (13) makes the policy dynamic: even though (9) is written with fixed average constraint, the online execution continuously recomputes the target grid budgets using the remaining budget-to-horizon ratio Bt. This re-planning step corrects for randomness and prevents early overspending, while retaining the same structure as in Algorithm 1. Random Rounding Method. Our greedy allocation procedure increases the grid-level budgets in discrete steps. In particular, for grid one increment corresponds to adding two additional trials, which consumes expected budget 2(cid:98)pj (since (cid:98)pj fraction of incoming questions fall into grid j). As result, near the end of the budget, the remaining expected budget may be insufficient to execute the next full 2-trial increment for the current best grid. Let δ denote the remaining expected budget (per-question average budget under the gridized constraint) at the moment when the algorithm is about to take one more 2-trial increment for the currently selected grid i. Suppose that δ (0, 2(cid:98)pi ), so full increment for grid would violate feasibility. Let = (B1, . . . , BK ) be the current integer allocation vector. Define B+ as the neighboring allocation obtained by applying one additional greedy increment to grid i: B+ = Bi + 2, = Bj (j (cid:44) i). B+ We then output randomized allocation as follows: With probability ρ := δ/(2(cid:98)pi ), set = B+. With probability 1 ρ, set = B. By construction, K(cid:88) j=1 (cid:98)pj Bj = K(cid:88) j=1 (cid:98)pj Bj + ρ 2(cid:98)pi = K(cid:88) j=1 (cid:98)pj Bj + δ, so the randomized rounding consumes exactly the remaining expected budget and keeps the expected budget constraint tight (hence feasible). B.2 Proof of Optimality of Algorithm 1 (Theorem 4.1) We first establish diminishing-returns property of SC(θ; B) when θ is scalar, which directly motivates the optimality of our greedy allocation rule in Algorithm 1. Lemma B.1. For any θ [0, 1] and N+, the marginal gain R(θ, n) is zero for odd n. Meanwhile, it is nonincreasing function of over even integers, and strictly decreasing when θ (cid:44) 1 2 . Proof of Lemma B.1. We focus on the binary case where each call returns an answer in {0, 1} and the ground-truth label is 1, since the function SC(θ; n) enjoys symmetry around θ = 1 2 : SC(θ; n) = SC(1 θ; n), θ [0, 1], N. (14) 22 Indeed, flipping the answer of 0 and 1 for each call transforms the binomial count Bin(n, θ) into Bin(B, 1 θ), while the majority rule with random tie-breaking is invariant under this relabeling. Consequently, by (14), R(θ, n) = R(1 θ, n), θ [0, 1], N. Therefore, it suffices to analyze the case θ 1 Finally, the boundary case θ = 1 around n/2, hence SC and thus 2 is trivial: when θ = 1 (cid:16) (cid:17) = (cid:16) > 2 = + 1 2 (cid:17) (cid:16) 1 2 ; (cid:17) = 1 2 , n, 2 (replace θ by 1 θ if θ < 1 2 ). 2 , we have Bin(n, 1 2 ) which is symmetric (cid:16) 1 2 , (cid:17) 0. which means, any extra budgets on questions with θ = 1 of the proof, we may assume θ > 1 2 . If is odd, then set = 2m 1, then there are two cases which will change the majority vote result: 2 bring no increment. In the remainder When there are correct answers in 2m 1 samples, so the majority is correct. if the 2m-th is wrong. then total becomes correct and wrong, which is tie. With random tie breaking, correctness drops from 1 to 0.5. So the drop amount is 0.5. Probability of the borderline configuration: : P[Choose which of the first (2m 1) are correct] = (cid:32) 2m 1 (cid:33) θm(1 θ)m1. Then : P[the 2m -th sample is wrong] = 1 θ. while the 2m-th is wrong. Then the decreasing probability is (cid:33) (cid:32) 1 2 2m 1 θm(1 θ)m1 (1 θ). When there are 1 correct answers in 2m 1 samples, and the 2m-th is correct. Then similarly, increasing probability is (cid:32) 1 2 2m 1 1 (cid:33) θm1(1 θ)m θ. Thus R(θ, 2m 1) = 0 for odd = 2m 1. Similarly, for even n, there are also two cases which will increasing the correct probability, which is correct answers in 2m samples and the 2m + 1-th is also correct or wrong, thus R(θ, 2m) = (cid:33) (cid:32) 2m 1 2 θ [θ(1 θ)]m 1 2 (1 θ) (cid:32) (cid:33) 2m [θ(1 θ)]m = (cid:18) θ 1 2 (cid:19) (cid:32) (cid:33) 2m [θ(1 θ)]m. 23 Furthermore, since R(θ, 2m + 2) R(θ, 2m) = (cid:1) (cid:0)2m+2 m+1 (cid:1) (cid:0)2m θ(1 θ) = (cid:19) (cid:18) 4 + 1 θ(1 θ) < 1. Then R(θ, 2m) is strictly decreasing for any when θ (cid:44) 1 2 . Proof of Theorem 4.1. Recall the discretized (grided) optimization problem: max Z0 {Bj } K(cid:88) j=1 (cid:98)pjSC(θj; Bj) s.t. K(cid:88) j=1 (cid:98)pjBj Btotal/N , (15) where (cid:98)pj is the estimated probability mass of grid (i.e., the fraction of problems whose difficulty falls into grid j). If we increase Bj by one (i.e. add one extra trial to grid j), then the objective in (15) increases by and the expected-budget constraint increases by pj. Therefore, the marginal gain per unit expected cost equals (cid:98)pjR(θj, Bj), (cid:98)pjR(θj, Bj) (cid:98)pj = R(θj, Bj). This shows that the grid probability (cid:98)pj appears in both the objective and the expected-budget constraint, and cancels out when comparing actions by marginal gain per expected-cost unit. Hence, choosing the next increment by maximizing R(θj, Bj) is exactly greedy with respect to the correct marginal reward per expected-cost unit. By Lemma B.1, for every θ > 1/2 we have R(θ, 2m 1) = 0 and R(θ, 0) > R(θ, 2) > > R(θ, 2m) > , i.e., the nontrivial marginal gains occur only at even budgets and form nonincreasing sequence. In particular, since R(θ, 2m 1) = 0, we have SC(θ; 2m) = SC(θ; 2m 1), so allocating the 2m-th trial does not improve the objective compared with 2m 1. Equivalently, for any 0, SC(θ; 2m + 1) SC(θ; 2m 1) = (cid:17) (cid:16) SC(θ; 2m + 1) SC(θ; 2m) = R(θ, 2m). Thus, when we think in effective increments, adding two trials to grid (from 2m 1 to 2m + 1) yields exactly the marginal reward R(θ, 2m), and these effective marginal rewards decrease with m. Consider the multiset of all effective marginal rewards (cid:110) := R(θj, 2m) : [K], = 0, 1, 2, . . . (cid:111) . 24 Any feasible allocation {Bj } in (15) corresponds to choosing prefix of the sequence R(θj, 0), R(θj, 2), R(θj, 4), . . . because one cannot obtain the (m + 1)-th effective gain for grid without also taking the first effective gains. Moreover, by Lemma B.1, each such sequence is nonincreasing. Algorithm 1 maintains the active set = {(j, Bj)}K i=1 and repeatedly selects the grid with the largest currently available effective marginal reward, i.e., it chooses arg maxj R(θj, Bj) and then increases Bj by 2. Because each grids effective marginal rewards form nonincreasing sequence, this procedure is exactly the process which picks the globally largest remaining element from collection of nonincreasing lists. Hence after effective steps, the greedy algorithm has selected the largest elements in that are feasible under the prefix constraints, which maximizes the accumulated improvement in the objective among all allocations spending the same expected budget. If the remaining budget is insufficient to complete the next 2-trial effective step for the current best grid, we can randomize the last step: suppose the remaining expected budget is δ (0, 2pj ). We perform the next 2 trials for grid with probability δ/(2pj ) and do nothing otherwise. This keeps the expected budget exactly feasible and achieves the optimal convex combination of the two neighboring integer allocations. Therefore, Algorithm 1 attains the optimum of (15). B.3 2-parameter Approximation in Multi-Choice Case In the multi-choice setting, the difficulty parameter θ is high-dimensional, making direct optimization costly. We therefore use Gaussian surrogate family {Φ(a a>0 to approximate the self-consistency rate SC(θ; n). This enables direct extension of Algorithm 1: replace SC(θ; n) with Φ(a + b) for suitable pair (a, b). n+b)} rate). Fix 2 and θ M1 with Proof of 2-parameter Approximation in Multi-Choice Case Proposition B.2 (Gaussian-probit approximation with 1/ θ1 > θ2 θM. Let := 1. Define the margin vector Rd + by := θ1 θj+1 (j = 1, . . . , d), and define the covariance matrix Σ Rdd of V1 as in (17) below. Let (cid:112) Σjj σmin := min 1jd := min 1jd Σjj, (cid:113) . Then for all 1, the majority-vote success probability satisfies (cid:12)(cid:12)(cid:12)(cid:12)SC(θ; n) Φ(a (cid:12)(cid:12)(cid:12)(cid:12) n) C(θ, M) , where one explicit admissible constant is C(θ, M) := CBEd1/4ρ(θ) + 2πσmin + (d 1)φ(a) , 25 with ρ(θ) := (cid:13)(cid:13)(cid:13)(cid:13)Σ 1/2(V (cid:13)(cid:13)(cid:13)(cid:13) ) 3 2 , φ(x) := 1 2π x2/2. and CBE is the Berry-Esseen approximation constant[Raič, 2019]. Moreover, since V1 a.s., we have the fully explicit bound ρ(θ) 8d3/2 λmin(Σ)3/2 , 2 2 hence C(θ, M) 8CBEd7/4 λmin(Σ)3/2 + 2πσmin + (d 1)φ(a) . Proof of Proposition B.2. Let {1, . . . , M} with P(A = i) = θi. After i.i.d. draws, let = (X1, . . . , XM) Multinomial(n, θ), SC(θ; n) = pn := P(arg max Xi = 1), with uniform random tie-breaking."
        },
        {
            "title": "Define the margin vector",
            "content": "D := (D2, . . . , DM) Zd, Dj := X1 Xj (j = 2, . . . , M)."
        },
        {
            "title": "Let E",
            "content": "n := {arg maxi Xi = 1}. Then deterministically {D [1, )d} {D Rd + }."
        },
        {
            "title": "Hence",
            "content": "P(D [1, )d) pn P(D Rd +). (16) Let A1, . . . , An be the i.i.d. samples. Define for each round the vector Vb = (Vb,2, . . . , Vb,M) {1, 0, 1}d, Vb,j := I(Ab = 1) I(Ab = j)."
        },
        {
            "title": "Then",
            "content": "D = n(cid:88) b=1 Vb, E[Vb] = := (θ1 θ2, . . . , θ θM) Rd +. direct computation gives the covariance matrix Σ = Cov(V1) Rdd: θj+1)2, Σjj = θ1 + θj+1 θj+1)(θ1 Σjk = θ1 (θ (θ1 θk+1) (j (cid:44) k). (17) 1 = θ1(1 θ1) 0, so Σ has nonnegative off-diagonal Note that θ1 1 implies Σjk θ1 entries. Since Σ = Cov(V1), it is always positive semidefinite. To prove Σ 0, it suffices to show that for any Rd, θ Σu = ar(u V1) = 0 = = 0. Note that V1 takes only the following values: 1d, = 1, ej, = + 1, V1 = = 1, . . . , d, 26 where 1d is the all-ones vector in Rd and ej is the j-th standard basis vector. Hence V1 = (cid:80)d uj, i=1 ui, = 1, = + 1, = 1, . . . , d. If ar(u all events {A = 1} and {A = + 1} occur with positive probability, so we must have V1 must be almost surely constant. Since θ1 > 0 and θj+1 > 0 for all j, V1) = 0, then d(cid:88) i=1 ui = uj, = 1, . . . , d. In particular, the right-hand side does not depend on j, implying u1 = = ud =: c. Substituting into the above identities yields dc = c, i.e., (d + 1)c = 0, hence = 0 and therefore = 0. Thus ar(u Let (n, nΣ). By Bentkus convex-set BerryEsseen bound[Raič, 2019], for any convex set Rd, V1) = 0 implies = 0, which proves Σ 0. (cid:12)(cid:12)(cid:12)P(D A) P(G A) (cid:12)(cid:12)(cid:12) CBEd1/4 (cid:13)(cid:13)(cid:13)(cid:13)Σ 1/2(V (cid:13)(cid:13)(cid:13)(cid:13) ) 3 2 . Both A0 := Rd + and A1 := [1, )d are convex, hence (cid:12)(cid:12)(cid:12)(cid:12) P(D Ak) P(G Ak) (cid:12)(cid:12)(cid:12)(cid:12) CBEd1/4 ρ(θ), {0, 1}. (18) Write = + with (0, Σ). Then 0 P(G A0) P(G A1) d(cid:88) j=1 P(0 Gj 1) = d(cid:88) (cid:32) j=1 Zj 1 (cid:33) ."
        },
        {
            "title": "Since Zj",
            "content": "(0, Σjj) has density bounded by 1/ (cid:33) (cid:32) Zj 1 (cid:112) 2πΣjj, 1 (cid:112) 1 2πΣjj 1 1 2π σmin . Therefore, 0 P(G A0) P(G A1) 1 2π σmin . (19)"
        },
        {
            "title": "Let Z",
            "content": "= (Z 1, . . . , d) be the coordinate-wise standardized version of Z: := Zj (cid:112) Σjj , cj := (cid:112) Σjj > 0, := min 1jd cj."
        },
        {
            "title": "Then",
            "content": "P(G A0) = P(Z ) = P(Z c) =: g( n). 27 Let arg minj cj so that cj = a. Since the event {Z upper bound c} implies For the lower bound, by Gaussian association (nonnegative correlations) the decreasing events [Esary et al., 1967]. {Z n} satisfy n) P(Z n) = Φ(a n). g( n, we have the (cid:18) d(cid:92) j=1 {Z (cid:19) n} d(cid:89) P(Z n) = Φ(a n)d. j=1 Moreover (cid:84) {Z j n} {Z c}, hence g( n) Φ(a n)d. Therefore, Let := Φ(a Then 0 Φ(a n) g( (20) n) (1/2, 1) and note that for [0, 1], 1xd1 (d 1)(1x) (Bernoulli inequality). n) Φ(a n)d. n) Φ(a pd = 1 pd1(cid:17) (cid:16) (d 1)p(1 p) (d 1)(1 p). Using Mills ratio 1 Φ(x) φ(x)/x for > 0, we obtain 0 Φ(a n) g( (cid:16) 1 Φ(a n) (d 1) n) (cid:17) (d 1) φ(a n) (d 1)φ(a) , where the last step uses φ(a From (16), (18), and (19), n) φ(a) for 1. (cid:12)(cid:12)(cid:12)pn P(G A0) (cid:12)(cid:12)(cid:12) CBEd1/ ρ(θ) + 1 2πσmin . Adding Step 5 yields (cid:12)(cid:12)(cid:12)pn Φ(a (cid:12)(cid:12)(cid:12) CBEd1/4 n) ρ(θ) + 1 2πσmin + (d 1)φ(a) . This proves the claim with = 0 and the stated explicit constant C(θ, M). Remark B.3. Indeed, in practice we use the regression method to decide parameter a, for given θ, such approximation has very great precision, see Figure 5 as reference."
        },
        {
            "title": "Recall the Gaussian surrogate family",
            "content": "(cid:16) ga,b(k) := Φ + (cid:17) , > 0, 0, where Φ is the standard normal CDF and φ := Φ is the standard normal PDF. 28 Concavity on the relevant budget range. In our allocation procedure, each question (or difficulty grid) receives at least small warm-up budget before the greedy stage. Let kmin 1 denote the smallest budget value that can occur in the greedy stage (e.g., kmin = 4 if we warm up with 4 samples). The next lemma shows ga,b is concave for all kmin as long as + is nonnegative on that range. Lemma B.4 (Concavity of ga,b for kmin). Fix > 0 and R. If (cid:112) kmin + 0, (21) then the function (cid:55) ga,b(k) is concave on [kmin, ) (in the usual continuous sense). In particular, a,b(k) is nonincreasing on [kmin, ). Proof. For > 0, let t(k) := + b. By the chain rule, Differentiating again (using φ (x) = xφ(x)) yields a,b(k) = φ(t(k)) 2 . a,b(k) = (cid:18) dk = φ (t(k)) (cid:16)t(k)φ(t(k)) = (cid:19) φ(t(k)) 2 (k) 2 (cid:17) 2 a2 t(k) 4k + (cid:32) = φ(t(k)) 2 4k3/ (cid:33) . (cid:19) + φ(t(k)) dk (cid:18) 2 + φ(t(k)) (cid:18) 4k3/2 (cid:19) Now assume (21). Then for all kmin we have t(k) = bracketed term is nonnegative, we conclude [kmin, ). + 0. Since φ(t(k)) > 0 and the a,b(k) 0 for all kmin, i.e., ga,b is concave on In our multi-choice extension, we approximate SC(θi; k) by SC(θ; k) = Φ(ai + bi), which has diminishing returns on the greedy range. Therefore, replacing SC(θi; k) by this Gaussian family preserves the key structural property needed by Algorithm 1, and the same greedy allocation rule applies verbatim in the multi-choice case. B.4 Warm-up griding and Difficulty Estimation Details In practice, we use short warm-up to map each question to coarse difficulty grid, and then apply precomputed one-shot budget for that grid. For each question q, we first draw 4 (4) (4) = (c1, c2, c3, c4) be the sorted option counts. Up to label permutation, responses and let (4) {1, . . . , 5} via deterministic rule Tq = g(C ). takes one of five patterns, inducing five grids Tq }5 Using training questions with large i.i.d. answer pools, we estimate the grid proportions {(cid:98)pj j= 29 (cid:17) (cid:16) Maj(B) = arg maxyY θy Figure 5: Probability that multinomial majority voting selects the true best option as function of budget. versus Budget {1, . . . , 64} for different ground-truth preference We plot vectors θ = (θ1, . . . , θM ). Gray curves (Exact) are computed from the true θ (exact for = 2, 4; Monte Carlo estimates for = 10), while green dotted curves (Probit) are produced by fitting two-parameter probit model and evaluating the fitted model across budgets. The fitted probit curves closely track the exact/MC curves in all regimes. Panels correspond to {2, 4, 10}. For = 4, we use: dominate [0.8, 0.1, 0.1, 0], head-heavy [0.6, 0.3, 0.05, 0.05], linear [0.4, 0.3, 0.2, 0.1], flat [0.3, 0.25, 0.25, 0.2], and uniform [0.25, 0.25, 0.25, 0.25]. For = 10, we use: dominate [0.8, 0.15, 0.05, 0, . . . , 0], head-heavy [0.25, 0.15, 0.10, 0.0714, . . . , 0.0714], linear [10, 9, . . . , 1]/55, flat normalize([10, 9, . . . , 1]0.4), and uniform [0.1, . . . , 0.1]. and fit representative surrogate curve ((cid:98)aj,(cid:98)bj) for each grid. At test time, we assign to Tq using the 4 warm-up samples and then allocate BTq additional samples in single shot. We provide the estimation details delow. Given question q, we draw 4 i.i.d. LLM responses and count how many times each answer option is selected. Let (4) = (c1, c2, c3, c4) denote the sorted (descending) counts of answer options among the first four responses, padding with zeros if fewer than four distinct options appear. Note that this definition is valid regardless of the total number of answer options: with only four samples, at most four options can appear, so the count pattern space is unchanged for multiple-choice with 4 options and also extends to other discrete-answer settings. By symmetry among answer labels, there are exactly five possible patterns: (cid:27) (cid:26) (4, 0, 0, 0), (3, 1, 0, 0), (2, 2, 0, 0), (2, 1, 1, 0), (1, 1, 1, 1) ."
        },
        {
            "title": "We define a deterministic grid mapping",
            "content": "Tq = g(C (4) ) {1, . . . , 5}, e.g., can map the patterns above to grids in the listed order (any fixed one-to-one mapping works as long as it is used consistently). For each training question q, we assume access to large pool of i.i.d. answers generated from yq Cat(θq). 30 To account for the randomness of using only four warm-up samples, we repeatedly subsample four answers (without replacement from the pool, or with replacement if the pool is large) and record the induced grid: (cid:18) (cid:19) (r) = (4) q,r , = 1, . . . , R. This yields an empirical, question-specific grid distribution (cid:98)pq(j) = 1 R(cid:88) r=1 1{T (r) = j}, {1, . . . , 5}. Averaging over training questions produces the empirical grid proportions (cid:98)pj = 1 (cid:88) train qQ train (cid:98)pq(j), {1, . . . , 5}. For each training question q, we fit its two-parameter difficulty curve ((cid:98)aq,(cid:98)bq) using the procedure in Section B.3 based on the full answer pool (so that the fit is stable). To obtain representative curve for each grid, we aggregate the per-question fits within that grid. To reduce sensitivity to borderline cases, we recommend using the soft grid weights (cid:98)pq(j): ((cid:98)aj,(cid:98)bj) = arg min (a,b) (cid:88) qQ train (cid:98)pq(j) ℓq(a, b), where ℓq(a, b) = n+b) is weighted average in parameter space, (cid:12)(cid:12)(cid:12)(cid:12)SC(θ(q); n)Φ(a (cid:12)(cid:12)(cid:12)(cid:12) is the fitting loss in Section B.3. simpler alternative (cid:98)aj = (cid:80) (cid:98)pq(j)(cid:98)aq (cid:80) (cid:98)pq(j) , (cid:98)bj = (cid:80) (cid:98)pq(j)(cid:98)bq (cid:80) (cid:98)pq(j) , which we found to work well when the per-question fits are already accurate. An Asymptotic Perspective on Online vs. Offline PETS In this section, we prove that as the total budget , the budget allocated to each question type converges to fixed proportion for both the offline (Section 3) and the online (Section 4) settings. Moreover, the limiting proportions in the offline and online cases are very close, which highlights the consistency between the two methods. C.1 Convergence of Budget Proportions for the Online Case We first show that as , every problem type will be sampled infinitely many times. Lemma C.1. For binary-choice problem set with difficulty labels {θi probabilities {pi total budget , the sample count Bi(B) for every diverges to infinity. i[N ] and corresponding i[N ] (where pi > 0), suppose we allocate budgets using Algorithm 1. Then, as the } } 31 Proof of Lemma C.1. Since difficulty labels θ and 1θ lead to the same behavior in our algorithm, (1/2, 1). For simplicity, assume each problem type without loss of generality we assume θi has been sampled once initially. Then, following Algorithm 1, the marginal accuracy gain from allocating two additional budget units at level 2m 1 is R(2m 1; θi) = pi (cid:18) θi (cid:19) (cid:32) (cid:33) 2m 1 2 [θi(1 θi)]m, N, (22) where λi is the probability mass of the problem type with label θi. At round t, the algorithm selects it arg max R(2mi(t) 1; θi), mit (t + 1) = mit (t) + 1, mj(t + 1) = mj(t) (j (cid:44) it), with the initialization mi(0) = 1. Define λi := θi(1 θi) (0, 1/4). Then R(2m + 1; θi) R(2m 1; θi) = (cid:1) (cid:0)2m+2 m+1 (cid:1) (cid:0)2m λi = (cid:19) (cid:18) 4 2 + 1 λi < 4λi < 1. Therefore, as increases, the marginal gain for each arm strictly decreases. Now suppose there exists some type that is sampled only finitely many times as . Then there exists constant such that mj(t) for all sufficiently large t, and thus R(mj(t); θj) R(M; θj) > 0 remains constant. On the other hand, at least one arm must be selected infinitely often; for that arm, we have mi(t) and hence R(mi(t); θi) 0, which contradicts the greedy choice rule (since eventually R(M; θj) would dominate). Therefore, every arm must be sampled infinitely often, i.e., mi(t) for all i. Next, we derive the convergence of sampling proportions and identify the limiting ratios. Proposition C.2. Under the same conditions as Lemma C.1, the ratio mi(B)/B converges to constant for each as . Moreover, mi(B) 1 p(θi) . where p(θ) = log(4θ(1 θ)) > 0. Proof of Proposition C.6. Define, for each difficulty label, Φi(2m 1) := log R(2m 1; θi). (23) For each update of arm i, the corresponding increment in Φi is (cid:18)(cid:18) i(2m 1) = Φi(2m + 1) Φi(2m 1) = log 4 2 + 1 (cid:19) (cid:19) pi . Since for 1 we have 4 2 m+1 [3, 4), it follows that for each fixed i, 0 < log(4pi) i(m) log(3pi) < . 32 Let max := max sup m1 i(m) = max (cid:16) log(3pi) (cid:17) < . Define the minimum and maximum potentials at time as L(t) := min (cid:16) mi(t) (cid:17) , Φi (t) := max (cid:17) (cid:16) mi(t) . Φi At time t, the algorithm samples the arm attaining the minimum potential, say it, and increases its potential from L(t) to at most L(t) + max, while leaving all other arms unchanged. Hence, (t + 1) max{U (t), L(t) + max }. Consequently, (t + 1) L(t + 1) max{U (0) L(0), max }. By induction, letting := max{U (0) L(0), max (cid:12)(cid:12)(cid:12)Φi (cid:16) mi(t) (cid:17) Φj (cid:16) mj(t) }, we obtain (cid:17)(cid:12)(cid:12)(cid:12) C, i, j. Next, we derive first-order approximation for Φi(m). By Stirlings formula, (cid:18) R(m; θi) = λi θi (cid:18) = λi θi 1 2 1 2 (cid:19) (cid:32) (cid:33) 2m pm (cid:19) 4m πm (cid:18) 1 + (cid:19)(cid:19) (cid:18) 1 pm . Therefore, Φi(m) = log R(m; θi) = log(4pi) + where bi := log (cid:19) (cid:18) λi(θi π 1 2 ) is constant, and o(1) 0 as . 1 2 log + bi + o(1), (24) (25) (26) (27) Combining (24) and (27), we obtain 1 2 Moving the logarithmic terms to the right-hand side yields log mi(t) + bi = mj(t) log(4pj) + mi(t) log(4pi) + 1 2 log mj(t) + bj + O(1). mi(t) log(4pi) = mj(t) log(4pj) + O(log t), since mj(t) and all arms are sampled infinitely often asymptotically. Fixing m1(t), this implies mi(t) = log(4p1) log(4pi) m1(t) + o(t). + = (cid:88) mi(t) = m1(t) (cid:88) log(4p1) log(4pi) + o(t). Hence, Finally, mi(t) mj(t) = log(4pj) log(4pi) + o(1) = lim mi(t) mj(t) = log(4pj) log(4pi) = (cid:17) (cid:16) 4θj(1 θj) log (cid:16) 4θi(1 θi) log (cid:17) . (28) C.2 Convergence of Budget Proportions for Offline Case Let Beta(a, b) with parameters a, N+. Its density is fa,b(x) = 1 B(a, b) xa1(1 x)b1, (0, 1), where the Beta function is B(a, b) = (cid:90) 1 0 ta1(1 t)b1 dt = Γ (a)Γ (b) Γ (a + b) = (a 1)!(b 1)! (a + 1)! . We define the tail probability at 1/2: Pa,b := P(X 1/2). We first obtain closed form for P(X x) when a, are integers: Lemma C.3. For any integer a, 1 and any [0, 1], Equivalently, P(X x) = a1(cid:88) k=0 (cid:33) (cid:32) + 1 xk(1 x)a+b1k. P(X x) = a+b1(cid:88) (cid:32) k=a (cid:33) + 1 xk(1 x)a+b1k. Proof. Define the polynomial Q(x) := a1(cid:88) (cid:32) k=0 (cid:33) + 1 xk(1 x)a+b1k, [0, 1]. We will show that Q(x) = P(X x) by verifying that has the same derivative as the Beta tail probability and the same boundary value at = 1. (i) Differentiate Q(x). For each term xk(1 x)a+b1k we have (cid:21) (cid:20) xk(1 x)a+b1k dx = kxk1(1 x)a+b1k (a + 1 k)xk(1 x)a+b2k."
        },
        {
            "title": "Hence",
            "content": "(x) = a1(cid:88) (cid:32) k= (cid:33)(cid:20) + 1 kxk1(1 x)a+b1k (a + 1 k)xk(1 x)a+b2k (cid:21) . The = 0 term in the first part is zero, so re-index the first sum by = 1: a1(cid:88) (cid:32) k=1 (cid:33) + 1 xk1(1 x)a+b1k = a2(cid:88) (cid:32) j=0 (cid:33) + 1 (j + 1) xj(1 x)a+b2j. + 1 34 Also rewrite the second part as a1(cid:88) k= (cid:33) (cid:32) + 1 (a + 1 k)xk(1 x)a+b2k. Therefore (x) = a2(cid:88) j=0 (cid:33) (cid:32) + 1 (j + 1)xj(1 x)a+b2j + 1 a1(cid:88) k= (cid:33) (cid:32) + 1 (a + 1 k)xk(1 x)a+b2k a2(cid:88) (cid:32) = (a + 1) xj(1 x)a+b2j (a + 1) (cid:33) + 2 j= a1(cid:88) k=0 (cid:33) (cid:32) + 2 xk(1 x)a+b2k. These two sums cancel term-by-term for = 0, 1, . . . , 2, leaving only the = 1 term from the second sum: (x) = (a + 1) (cid:33) (cid:32) + 2 1 xa1(1 x)b1 = (a + 1)! (a 1)!(b 1)! xa1(1 x)b1. (ii) Differentiate the Beta tail probability. Define (x) := P(X x) = (cid:90) 1 x"
        },
        {
            "title": "1\nB(a, b)",
            "content": "ta1(1 t)b1 dt. By the fundamental theorem of calculus, (x) = 1 B(a, b) xa1(1 x)b1 = (a + 1)! (a 1)!(b 1)! xa1(1 x)b1, which matches (x)."
        },
        {
            "title": "We have",
            "content": "Also, (1) = P(X 1) = 0. Q(1) = a1(cid:88) (cid:32) k= (cid:33) + 1 1k (1 1)a+b1k = 0,"
        },
        {
            "title": "Since Q",
            "content": "(x) = (x) on [0, 1] and Q(1) = (1), we conclude Q(x) = (x) for all [0, 1], i.e. P(X x) = Q(x) = a1(cid:88) (cid:32) k= (cid:33) + 1 xk(1 x)a+b1k. This proves the lemma. 35 Then, we can derive the explicit increment of self-consistency of problem from state (m, n) to (m + 1, n)(that is, add one positive answer based on positive and negative answer) if the true answer is positive. Lemma C.4. We have where m,n := Pm+1,n Pm,n = 1 2m+n (cid:33) (cid:32) + 1 . Pm,n = P(X 1/2), Beta(m, n). Proof. Apply Lemma C.3 with = 1 2 : (cid:33)(cid:18) 1 2 + 1 Pa,b = a1(cid:88) (cid:32) k=0 (cid:19)a+b1k (cid:19)k(cid:18) 1 2 (a+b1) = 2 a1(cid:88) (cid:32) k=0 (cid:33) + 1 . Let m, {1, . . . , } m,n := Pm+1,n Pm,n = P(Beta(m + 1, n) 1/2) P(Beta(m, n) 1/2)."
        },
        {
            "title": "We write",
            "content": "Pm+1,n = 2 (m+n) (cid:32) m(cid:88) k=0 (cid:33) , + Pm,n = 2 (m+n1) m1(cid:88) (cid:32) k=0 (cid:33) + 1 . Let := + n. Then Pm+1,n = 2 (cid:32) m(cid:88) k=0 (cid:33) , Pm,n = (A1) m1(cid:88) k=0 (cid:33) (cid:32) 1 ."
        },
        {
            "title": "Hence",
            "content": "(cid:32) m(cid:88) m,n = 2 Now apply Pascals identity (cid:0)A (cid:1) = (cid:0)A1 k=0 (cid:1): (cid:1) + (cid:0)A1 k1 (cid:33) (A1) 2 m1(cid:88) k= (cid:33) (cid:32) 1 . (cid:32) m(cid:88) k= (cid:33) = = m(cid:88) (cid:32) 1 (cid:32) m(cid:88) (cid:33) + (cid:33) 1 1 k=0 (cid:32) 1 (cid:33) + 2 k=0 (cid:32) m1(cid:88) k=0 (cid:33) . 1 k"
        },
        {
            "title": "Therefore",
            "content": "m,n = 2 (cid:33) (cid:18)(cid:32) 1 + m1(cid:88) (cid:32) k=0 (cid:33)(cid:19) 1 m1(cid:88) (A1) 2 (cid:33) (cid:32) 1 = (cid:33) (cid:32) 1 (cid:18) + 2 2 2 (A1) (cid:19) m1(cid:88) (cid:32) k=0 36 (cid:33) k=0 1 A = 2 (cid:32) (cid:33) , 1 Finally, substituting back = + yields m,n = 2 (m+n) (cid:32) (cid:33) + 1 = 1 2m+n (cid:32) (cid:33) + 1 1 . This is strictly positive because the binomial coefficient is positive. Remark C.5 (A simple probabilistic interpretation). Lemma C.3 can be rewritten as Binomial CDF identity: if Binomial(a + 1, 1 x), then In particular, at = 1/2, P(X x) = P(Y b). Pa,b = 2 (a+b1) a1(cid:88) (cid:32) k=0 (cid:33) + 1 = (cid:17) (cid:16) Binomial(a + 1, 1/2) 1 . Next, we derive the convergence of sampling proportions and identify the limiting ratios. Proposition C.6. Consider the offline setting that the entire set of question {qi }N i=1 is available upfront, and the underlying difficulty label of problem qi is θi. Using equal weight Offline PETS with Beta(1, 1), the ratio mi(B)/B converges to constant for each as . Moreover, mi(B) 1 p(θi) . Define the KL divergence to 1 2 : KL(q 2 ) := ln(2q) + (1 q) ln (cid:16) 2(1 q) (cid:17) , (0, 1), then p(θ) = KL(θ 1 2 ). Proof. Let θ (0, 1) be the success probability. Draw i.i.d. Bernoulli random variables Y1, Y2, . . . i.i.d. Bernoulli(θ), P(Yi = 1) = θ, P(Yi = 0) = 1 θ. For each sample size 1, define the success and failure counts mT := T(cid:88) i=1 Yi, nT := mT = T(cid:88) i=1 (1 Yi). Clearly, mT + nT = and (mT , nT ) is the empirical outcome of binomial experiment with parameter θ, by Strong Law of Large Numbers, mT nT = mT /T 1 mT /T θ 1 θ almost surely. 37 Then along the binomial sample path (mT , nT ), by Lemma C.4 mT ,nT = (cid:33) (cid:32) 1 mT 1 2T = (cid:18) 1 2 Binomial(T 1, 2 ) = mT (cid:19) , and moreover, with qT := mT /(T 1), follows from Stirlings formula applied to the binomial pmf P(Binomial(T 1, 1 2 ) = mT ) , yielding (cid:16) Binomial(T 1, 1 2 ) = mT (cid:17) (cid:112) 1 2π(T 1)qT (1 qT ) (cid:18) exp (T 1) KL(qT (cid:19) 1 2 ) . Finally, since qT θ almost surely, the exponent satisfies 1 log mT ,nT KL(θ 1 2 ) a.s. The remaining part is similar with online case in Section C.1. Which means, since the linear term of mT ,nT (resp. log R(m; θ)) converge to KL(θ 1 2 ), then we have similar expression mi(t)KL(θi 1 ) = mj(t)KL(θj 1 2 ) + O(log t). The rest derivation is similar, hence mi(B)"
        },
        {
            "title": "1\nKL(θi",
            "content": ". 1 2 ) C.3 Discussion of the Asymptotic Behavior for Online vs. Offline Cases As shown in Sections C.2 and C.1, the asymptotic limits induced by the two allocation paradigms are not exactly identical. This discrepancy mainly stems from the different statistical viewpoints adopted in each setting. In the offline case, the difficulty parameters are inferred during the allocation process from progressively collected samples, and thus optimal decision-making requires Bayesian perspective: one must update the posterior distribution via Bayesian framework, otherwise (as discussed in [Chen et al., 2013]) the optimality of the resulting allocation cannot be properly assessed. In contrast, the online streaming case assumes the answer distribution parameters are known priori (or at least can be estimated with sufficient confidence), and therefore employs frequentist-style update based on empirical frequencies. As consequence, the two settings lead to slightly different criteria for determining the majorityvoting outcome. Nevertheless, we observe that the resulting limiting behaviors are remarkably close in practice; see Figure 2 for an illustration."
        },
        {
            "title": "D Appendix of Experiment",
            "content": "D.1 Implementation Details in PETS-Offline. Confidence-weighted voting. In the confidence-weighted setting, each reasoning trajectory is associated with scalar confidence score extracted from the model output following Fu et al. 38 [2025b]. Specifically, we compute tail trace confidence for each generated reasoning trace by averaging the token-level confidence scores over the last 2048 tokens, and use this value as the weight in majority voting. To mitigate the influence of low-confidence traces, we further apply confidence-based filtering strategy: only the top 70% most confident traces are retained for voting, while the remaining 30% are assigned zero weight. We refer to this scheme as top-70% confidence-weighted majority voting. The resulting weighted voting rule is used consistently throughout the paper for defining both self-consistency and the population majority label . Note that we use confidence-weighted majority voting as proxy for the Bayes-optimal terminal decision. Uniform allocation baseline. Under uniform allocation (standard test-time scaling), we fix an average per-question budget = 1, . . . , 64 and sample exactly trajectories for every question. When comparing against PETS-Offline, we evaluate Uniform at the per-question budget that matches the trace count where PETS-Offline first reaches full self-consistency. D.2 Implementation Details in PETS-Online. In the experiment, PETS-Online is evaluated against PETS-Oracle and Uniform allocation. We now introduce each method in detail. PETS-Online streaming allocation. The PETS-Online assumes not having access to θq and implements fully deployable online streaming policy. Instead of observing the true difficulty, we use short warm-up procedure to obtain coarse difficulty proxy. Specifically, for each (4) . question we first sample four responses and compute the sorted option-count vector falls into one of five predefined patterns, which (4) Up to permutation of answer labels, deterministically map the question to one of five difficulty grids. Using training questions with large i.i.d. answer pools, we estimate (i) the empirical mass of each grid and (ii) representative two-parameter difficulty curve for each grid by fitting Gaussian-based approximation to the self-consistency curve. Based on these grid-level statistics, we pre-compute the optimal per-grid budgets using the greedy allocation algorithm 1 as in the oracle setting. At test time, each incoming question undergoes the same four-sample warm-up to determine its grid, after which the corresponding precomputed budget is allocated in single shot. This procedure respects the streaming and one-shot constraints, requires no access to future questions, and uses only minimal online sampling to approximate question difficulty θ. PETS-Oracle online streaming allocation. In the oracle online streaming experiment, we assume access to the true difficulty vector θq of each arriving question q, i.e., the underlying answer distribution of the model under infinite sampling. This oracle setting is not deployable in practice and is used solely as an upper bound to assess the optimality of the proposed online allocation strategy. Using held-out training set with large i.i.d. answer pools, we estimate the prior distribution over question difficulties by discretizing the continuous difficulty space into grids, each represented by prototype θj with prior mass pj. Based only on the prior statistics (pj, θj), we solve the discretized optimization problem in Equation (9) using the greedy algorithm (Alg. 1) to obtain fixed budget allocation {Bj }K j=1 for all grids. At test time, questions arrive sequentially in streaming manner. For each incoming question, its true difficulty vector θq is revealed, the corresponding grid index is identified, and the precomputed budget Bi is allocated in single shot. Predictions are obtained by (weighted) majority voting, and we evaluate self-consistency and accuracy under the same protocol as in the offline setting. Uniform online streaming allocation. As baseline for the online streaming setting, we consider uniform allocation strategy that assigns the same fixed budget to every question, independent of its difficulty. Given an average per-question budget constraint = Btotal/T , uniform allocation assigns Bq samples to each arriving question. 40 D.3 Full Experiment Results D.3.1 Full PETS-Offline Results Figure 6: GPQA offline Figure 7: AIME 25 offline 41 Figure 8: AIME 24 offline Figure 9: HMMT offline Figure 10: BRUMO offline 42 . ) a v ( m e m e t e . u r i ffl - P : 4 a 0 2 1 - B 0 2 - g - Q 0 3 - 3 Q 4 - 3 Q a @ c @ 1 = @ # a @ a a @ 1 = @ # a @ c @ 1 = @ # a @ a a @ 1 = @ # a @ c @ 1 = @ # t t t . ) 1 0 0 ( 5 2 8 0 . . ) 0 0 0 ( 8 9 9 . . ) 1 0 0 ( 1 2 8 0 . . ) 1 0 0 ( 0 9 9 . . ) 1 0 0 ( 5 1 8 0 . . ) 1 0 0 ( 5 6 9 . . ) 1 0 0 ( 8 0 8 0 . . ) 2 0 0 ( 7 5 9 . . ) 1 0 0 ( 9 3 9 0 . . ) 1 0 0 ( 7 9 9 . . ) 2 0 0 ( 3 3 9 0 . . ) 2 0 0 ( 6 7 9 . . ) 3 0 0 ( 3 1 9 0 . . ) 3 0 0 ( 7 4 9 . . ) 3 0 0 ( 9 1 9 0 . . ) 3 0 0 ( 3 4 9 . . ) 2 0 0 ( 4 4 7 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 7 3 7 0 . . ) 2 0 0 ( 6 8 9 . . ) 3 0 0 ( 7 3 7 0 . . ) 3 0 0 ( 7 6 9 . . ) 3 0 0 ( 9 1 7 0 . . ) 2 0 0 ( 8 4 9 . . ) 0 0 0 ( 3 3 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 1 3 8 0 . . ) 2 0 0 ( 9 8 9 . . ) 2 0 0 ( 6 3 8 0 . . ) 3 0 0 ( 8 3 9 . . ) 3 0 0 ( 6 2 8 0 . . ) 4 0 0 ( 0 3 9 . . ) 2 0 0 ( 1 2 9 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 4 2 9 0 . . ) 2 0 0 ( 4 8 9 . . ) 3 0 0 ( 1 0 9 0 . . ) 2 0 0 ( 3 4 9 . . ) 3 0 0 ( 7 0 9 0 . . ) 3 0 0 ( 2 5 9 . . ) 8 6 3 9 4 ( 0 8 5 2 . ) 0 1 8 8 7 ( 0 4 5 3 . ) 1 6 8 4 1 2 ( 3 9 3 0 1 . ) 6 7 8 8 9 1 ( 0 0 5 0 1 . ) 0 5 2 6 ( 2 1 2 . ) 7 6 5 6 ( 1 1 2 . ) 5 7 3 5 3 ( 1 8 6 . ) 5 1 2 4 5 ( 1 1 9 . ) 1 0 1 8 ( 4 8 1 . ) 6 8 3 9 ( 8 1 2 . ) 1 4 3 8 4 ( 8 4 4 . ) 4 9 1 0 5 ( 5 3 5 . ) 1 8 9 2 1 ( 9 2 3 . ) 0 8 8 1 1 ( 4 2 3 . ) 3 3 3 8 4 ( 6 1 9 . ) 9 2 4 6 4 ( 2 0 9 . ) 4 4 5 7 ( 3 5 2 . ) 6 0 7 7 ( 7 1 2 . ) 2 3 4 9 4 ( 6 7 7 . ) 9 9 0 3 4 ( 4 7 5 . ) 1 0 0 ( 7 5 7 0 . . ) 0 0 0 ( 8 9 9 0 . . ) 1 0 0 ( 2 6 7 0 . . ) 1 0 0 ( 2 9 9 0 . . ) 1 0 0 ( 1 5 7 0 . . ) 1 0 0 ( 8 5 9 0 . . ) 1 0 0 ( 6 5 7 0 . . ) 1 0 0 ( 6 5 9 0 . . ) 0 0 0 ( 0 0 9 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 1 0 0 ( 7 9 8 0 . . ) 3 0 0 ( 0 7 9 0 . . ) 2 0 0 ( 2 0 9 0 . . ) 3 0 0 ( 3 5 9 0 . . ) 3 0 0 ( 0 9 8 0 . . ) 4 0 0 ( 7 3 9 0 . . ) 0 0 0 ( 3 3 7 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 1 0 0 ( 6 2 7 0 . . ) 2 0 0 ( 9 6 9 0 . . ) 2 0 0 ( 9 1 7 0 . . ) 2 0 0 ( 2 6 9 0 . . ) 2 0 0 ( 8 0 7 0 . . ) 2 0 0 ( 1 4 9 0 . . ) 0 0 0 ( 3 3 8 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 1 0 0 ( 1 3 8 0 . . ) 2 0 0 ( 9 8 9 0 . . ) 2 0 0 ( 6 3 8 0 . . ) 3 0 0 ( 8 3 9 0 . . ) 3 0 0 ( 6 2 8 0 . . ) 4 0 0 ( 0 3 9 0 . . ) 2 0 0 ( 1 2 9 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 2 0 0 ( 4 2 9 0 . . ) 2 0 0 ( 4 8 9 0 . . ) 3 0 0 ( 1 0 9 0 . . ) 2 0 0 ( 3 4 9 0 . . ) 3 0 0 ( 7 0 9 0 . . ) 3 0 0 ( 2 5 9 0 . . ) 1 2 5 6 3 ( 0 8 1 3 . ) 4 9 4 3 5 ( 3 9 6 3 . ) 2 2 1 2 0 2 ( 3 5 8 0 1 . ) 4 7 0 6 4 1 ( 3 9 1 1 1 . ) 9 5 7 4 ( 1 8 1 . ) 5 5 7 8 ( 1 5 2 . ) 8 7 4 4 2 ( 0 1 4 . ) 5 6 2 4 3 ( 8 1 6 . ) 7 1 0 7 ( 0 0 2 . ) 8 1 2 7 ( 2 3 2 . ) 1 4 4 5 5 ( 5 6 6 . ) 6 4 6 7 6 ( 6 0 2 1 . ) 1 8 9 2 1 ( 9 2 3 . ) 0 8 8 1 1 ( 4 2 3 . ) 3 3 3 8 4 ( 6 1 9 . ) 9 2 4 6 4 ( 2 0 9 . ) 4 4 5 7 ( 3 5 2 . ) 6 0 7 7 ( 7 1 2 . ) 2 3 4 9 4 ( 6 7 7 . ) 9 9 0 3 4 ( 4 7 5 . ) 1 0 0 ( 3 6 7 0 . . ) 0 0 0 ( 8 9 9 0 . . ) 1 0 0 ( 0 7 7 0 . . ) 1 0 0 ( 2 9 9 0 . . ) 1 0 0 ( 5 6 7 0 . . ) 1 0 0 ( 8 5 9 0 . . ) 1 0 0 ( 4 6 7 0 . . ) 2 0 0 ( 0 5 9 0 . . ) 0 0 0 ( 0 0 9 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 2 0 0 ( 4 0 9 0 . . ) 2 0 0 ( 0 7 9 0 . . ) 2 0 0 ( 3 9 8 0 . . ) 3 0 0 ( 9 6 9 0 . . ) 2 0 0 ( 9 8 8 0 . . ) 3 0 0 ( 2 4 9 0 . . ) 0 0 0 ( 0 0 7 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 1 0 0 ( 9 9 6 0 . . ) 2 0 0 ( 9 8 9 0 . . ) 1 0 0 ( 9 9 6 0 . . ) 2 0 0 ( 0 9 9 0 . . ) 1 0 0 ( 7 9 6 0 . . ) 3 0 0 ( 8 7 9 0 . . ) 2 0 0 ( 7 1 7 0 . . ) 1 0 0 ( 6 9 9 0 . . ) 2 0 0 ( 2 3 7 0 . . ) 3 0 0 ( 1 7 9 0 . . ) 3 0 0 ( 8 8 6 0 . . ) 4 0 0 ( 1 1 9 0 . . ) 3 0 0 ( 7 1 7 0 . . ) 3 0 0 ( 1 0 9 0 . . ) 0 0 0 ( 7 6 8 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 2 0 0 ( 8 7 8 0 . . ) 2 0 0 ( 1 8 9 0 . . ) 2 0 0 ( 3 7 8 0 . . ) 1 0 0 ( 4 7 9 0 . . ) 2 0 0 ( 2 7 8 0 . . ) 2 0 0 ( 0 7 9 0 . . ) 3 6 9 2 5 ( 3 1 5 2 . ) 4 9 9 5 4 ( 7 8 8 2 . ) 1 3 1 3 5 2 ( 3 7 9 9 . ) 1 4 3 5 3 1 ( 0 0 4 1 1 . ) 8 4 0 6 ( 2 5 1 . ) 3 1 2 8 ( 3 0 2 . ) 7 1 4 2 2 ( 8 8 2 . ) 4 9 4 6 5 ( 2 5 7 . ) 4 0 5 4 ( 3 8 . ) 3 1 3 5 ( 1 9 . ) 3 2 2 6 ( 6 9 . ) 2 8 8 7 ( 5 1 1 . ) 8 8 7 9 ( 3 6 3 . ) 1 9 6 2 1 ( 1 7 3 . ) 1 1 3 6 4 ( 7 2 2 1 . ) 4 0 9 1 4 ( 5 6 1 1 . ) 5 9 1 5 ( 9 4 1 . ) 9 5 7 7 ( 0 5 1 . ) 1 7 0 1 3 ( 0 2 3 . ) 2 3 3 6 4 ( 1 2 4 . ) 1 0 0 ( 8 1 7 0 . . ) 0 0 0 ( 9 9 9 . . ) 1 0 0 ( 1 2 7 0 . . ) 1 0 0 ( 8 8 9 . . ) 1 0 0 ( 5 1 7 0 . . ) 1 0 0 ( 8 5 9 . . ) 1 0 0 ( 2 2 7 0 . . ) 1 0 0 ( 9 4 9 . . ) 2 0 0 ( 6 8 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 3 0 0 ( 7 6 8 0 . . ) 3 0 0 ( 6 6 9 . . ) 2 0 0 ( 1 6 8 0 . . ) 3 0 0 ( 7 5 9 . . ) 3 0 0 ( 6 5 8 0 . . ) 4 0 0 ( 8 2 9 . . ) 0 0 0 ( 0 0 7 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 9 9 6 0 . . ) 1 0 0 ( 6 9 9 . . ) 1 0 0 ( 6 9 6 0 . . ) 2 0 0 ( 7 7 9 . . ) 2 0 0 ( 6 8 6 0 . . ) 3 0 0 ( 3 7 9 . . ) 2 0 0 ( 7 4 6 0 . . ) 1 0 0 ( 9 9 9 . . ) 2 0 0 ( 2 5 6 0 . . ) 3 0 0 ( 3 7 9 . . ) 3 0 0 ( 7 4 6 0 . . ) 3 0 0 ( 3 1 9 . . ) 3 0 0 ( 1 5 6 0 . . ) 5 0 0 ( 3 1 9 . . ) 0 0 0 ( 7 6 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 1 6 8 0 . . ) 3 0 0 ( 1 8 9 . . ) 3 0 0 ( 7 3 8 0 . . ) 3 0 0 ( 8 5 9 . . ) 3 0 0 ( 6 3 8 0 . . ) 3 0 0 ( 9 4 9 . . ) 5 0 7 4 5 ( 7 0 6 2 . ) 4 4 7 5 6 ( 7 8 6 3 . ) 3 5 1 3 0 2 ( 7 6 3 0 1 . ) 8 8 4 5 1 1 ( 7 0 6 1 1 . ) 9 9 6 5 ( 0 9 1 . ) 6 2 8 5 ( 3 2 2 . ) 5 3 2 6 4 ( 5 4 5 . ) 8 9 8 0 5 ( 3 6 7 . ) 7 2 1 3 ( 5 3 1 . ) 3 2 5 3 ( 0 2 1 . ) 4 4 9 2 1 ( 2 0 2 . ) 8 3 5 1 1 ( 9 7 1 . ) 4 2 9 7 ( 1 8 3 . ) 7 4 3 0 1 ( 1 6 3 . ) 2 3 3 8 4 ( 9 1 2 1 . ) 9 8 3 7 4 ( 9 8 0 1 . ) 3 3 8 2 ( 8 4 1 . ) 0 2 3 5 ( 2 6 1 . ) 4 1 7 8 ( 7 3 2 . ) 6 0 6 5 2 ( 0 1 3 . ) 1 0 0 ( 7 9 6 0 . . ) 0 0 0 ( 7 9 9 0 . . ) 1 0 0 ( 3 0 7 0 . . ) 1 0 0 ( 9 8 9 0 . . ) 1 0 0 ( 9 8 6 0 . . ) 2 0 0 ( 7 5 9 0 . . ) 1 0 0 ( 9 9 6 0 . . ) 1 0 0 ( 7 4 9 0 . . ) 0 0 0 ( 3 3 8 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 2 0 0 ( 0 4 8 0 . . ) 2 0 0 ( 9 7 9 0 . . ) 2 0 0 ( 1 2 8 0 . . ) 3 0 0 ( 7 3 9 0 . . ) 3 0 0 ( 3 3 8 0 . . ) 5 0 0 ( 3 1 9 0 . . ) 2 0 0 ( 1 3 6 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 3 0 0 ( 0 6 6 0 . . ) 3 0 0 ( 1 6 9 0 . . ) 3 0 0 ( 0 3 6 0 . . ) 3 0 0 ( 8 3 9 0 . . ) 3 0 0 ( 4 3 6 0 . . ) 4 0 0 ( 2 2 9 0 . . ) 2 0 0 ( 6 1 5 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 3 0 0 ( 0 2 5 0 . . ) 3 0 0 ( 3 6 9 0 . . ) 3 0 0 ( 0 0 5 0 . . ) 4 0 0 ( 4 0 9 0 . . ) 4 0 0 ( 6 0 5 0 . . ) 7 0 0 ( 2 7 8 0 . . ) 2 0 0 ( 4 5 7 0 . . ) 0 0 0 ( 0 0 0 1 . . ) 3 0 0 ( 3 7 7 0 . . ) 3 0 0 ( 8 7 9 0 . . ) 3 0 0 ( 2 3 7 0 . . ) 3 0 0 ( 6 3 9 0 . . ) 3 0 0 ( 7 5 7 0 . . ) 4 0 0 ( 1 4 9 0 . . ) 5 5 5 1 5 ( 0 8 7 2 . ) 0 1 8 5 7 ( 7 6 6 3 . ) 5 8 9 7 6 1 ( 3 1 0 1 1 . ) 8 5 2 8 3 1 ( 3 5 4 1 1 . ) 3 2 7 4 ( 2 1 2 . ) 7 3 1 8 ( 7 5 2 . ) 6 1 6 8 2 ( 0 7 4 . ) 4 4 5 9 3 ( 0 1 6 . ) 8 6 0 9 ( 9 5 2 . ) 5 8 2 2 1 ( 9 6 3 . ) 6 9 1 3 5 ( 1 6 8 . ) 0 0 6 1 5 ( 2 4 9 . ) 0 4 6 3 1 ( 4 6 4 . ) 4 2 8 9 1 ( 9 7 5 . ) 1 0 9 8 4 ( 3 3 1 1 . ) 3 5 2 7 4 ( 3 8 3 1 . ) 8 8 4 7 ( 0 8 2 . ) 1 7 9 0 1 ( 2 9 2 . ) 1 2 1 8 4 ( 6 2 8 . ) 2 5 9 0 3 ( 9 8 5 ) c ( ffl - P ffl - P o U ) c ( ffl - P ffl - P f ) c ( o U ) c ( f ) c ( ffl - P ffl - P o U ) c ( ffl - P ffl - P f ) c ( o U ) c ( f ) c ( ffl - P ffl - P o U ) c ( f A 5 2 I 4 2 I H R 43 D.3.2 Full PETS-Online Results Besides the full results, here we also presents the comparison with the oracle case of the online setting, which assumes access to the latent parameter θ; in the realistic online setting, θ is unavailable and must be learned from training dataset. Figure 11: GPQA online Figure 12: AIME 25 online 44 Figure 13: AIME 24 online Figure 14: HMMT online Figure 15: BRUMO online 45 ; θ e a e e t c s s n e i e o c a . ) a v ( m e m f a u . u e n - P : 5 a . a g i a f r e u a a v s θ , i s l i l e i 0 2 1 - B 0 2 - g - Q 0 3 - 3 Q 4 - 3 Q a @ c @ 1 = @ # a @ a a @ 1 = @ # a @ c @ 1 = @ # a @ a a @ 1 = @ # a @ c @ 1 = @ # t t t . ) 1 0 0 ( 4 2 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 6 1 8 0 . . ) 1 0 0 ( 0 9 9 . . ) 1 0 0 ( 3 2 8 0 . . ) 0 0 0 ( 8 9 9 . . ) 1 0 0 ( 5 1 8 0 . . ) 1 0 0 ( 5 9 9 . . ) 1 0 0 ( 8 1 8 0 . . ) 1 0 0 ( 6 7 9 . . ) 1 0 0 ( 4 1 8 0 . . ) 1 0 0 ( 8 6 9 . . ) 2 0 0 ( 7 1 9 0 . . ) 0 0 0 ( 0 0 0 . . ) 3 0 0 ( 3 2 9 0 . . ) 2 0 0 ( 3 8 9 . . ) 3 0 0 ( 8 0 9 0 . . ) 3 0 0 ( 2 8 9 . . ) 4 0 0 ( 2 1 9 0 . . ) 4 0 0 ( 3 6 9 . . ) 4 0 0 ( 2 9 8 0 . . ) 5 0 0 ( 2 5 9 . . ) 4 0 0 ( 8 9 8 0 . . ) 4 0 0 ( 3 4 9 . . ) 1 0 0 ( 8 9 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 5 9 6 0 . . ) 1 0 0 ( 7 9 9 . . ) 1 0 0 ( 7 9 6 0 . . ) 1 0 0 ( 8 9 9 . . ) 2 0 0 ( 3 9 6 0 . . ) 2 0 0 ( 5 9 9 . . ) 2 0 0 ( 0 7 6 0 . . ) 3 0 0 ( 2 7 9 . . ) 3 0 0 ( 7 7 6 0 . . ) 3 0 0 ( 8 7 9 . . ) 0 0 0 ( 0 5 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 3 0 0 ( 5 6 8 0 . . ) 5 0 0 ( 5 5 9 . . ) 2 0 0 ( 0 5 8 0 . . ) 3 0 0 ( 5 8 9 . . ) 3 0 0 ( 7 6 8 0 . . ) 4 0 0 ( 0 5 9 . . ) 3 0 0 ( 8 4 8 0 . . ) 4 0 0 ( 7 4 9 . . ) 4 0 0 ( 0 5 8 0 . . ) 6 0 0 ( 3 1 9 . . ) 2 0 0 ( 8 8 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 7 9 8 0 . . ) 2 0 0 ( 8 8 9 . . ) 3 0 0 ( 8 6 8 0 . . ) 4 0 0 ( 8 4 9 . . ) 3 0 0 ( 0 8 8 0 . . ) 4 0 0 ( 3 6 9 . . ) 3 0 0 ( 3 6 8 0 . . ) 3 0 0 ( 5 1 9 . . ) 3 0 0 ( 2 7 8 0 . . ) 4 0 0 ( 2 4 9 . . ) 7 3 7 6 2 1 ( 2 7 1 4 . ) 7 0 3 0 4 1 ( 4 9 0 5 . ) 4 7 6 2 5 ( 0 7 9 2 . ) 0 8 6 7 2 ( 9 8 2 3 . ) 6 3 6 7 6 2 ( 9 0 1 8 . ) 6 5 6 8 6 1 ( 3 9 5 9 . ) 9 1 4 6 4 ( 4 5 4 . ) 2 0 0 9 4 ( 3 9 4 . ) 0 6 0 8 ( 5 7 1 . ) 7 0 6 8 ( 2 9 1 . ) 2 8 8 3 3 ( 6 1 4 . ) 3 3 2 3 3 ( 9 5 4 . ) 8 1 7 3 ( 7 0 1 . ) 6 2 9 1 ( 2 9 . ) 6 2 8 2 ( 3 0 1 . ) 2 7 0 2 ( 5 9 . ) 5 6 0 8 1 ( 5 0 2 . ) 5 8 0 2 1 ( 4 4 1 . ) 3 5 6 4 1 ( 1 2 2 . ) 9 0 4 3 1 ( 3 6 2 . ) 9 9 1 7 ( 6 6 1 . ) 6 5 2 7 ( 9 9 1 . ) 8 0 1 8 1 ( 9 0 3 . ) 6 3 0 8 2 ( 8 5 4 . ) 7 3 1 4 2 ( 3 8 3 . ) 4 9 2 4 1 ( 1 4 2 . ) 4 8 6 2 1 ( 7 7 2 . ) 3 9 7 9 ( 8 0 2 . ) 1 3 4 4 3 ( 1 2 5 . ) 5 8 0 6 2 ( 3 5 3 . ) 1 0 0 ( 9 5 7 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 1 6 7 0 . . ) 1 0 0 ( 1 9 9 . . ) 1 0 0 ( 9 5 7 0 . . ) 0 0 0 ( 9 9 9 . . ) 1 0 0 ( 1 6 7 0 . . ) 1 0 0 ( 6 9 9 . . ) 1 0 0 ( 1 6 7 0 . . ) 1 0 0 ( 3 8 9 . . ) 1 0 0 ( 3 6 7 0 . . ) 1 0 0 ( 8 7 9 . . ) 0 0 0 ( 0 5 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 8 4 8 0 . . ) 4 0 0 ( 0 6 9 . . ) 2 0 0 ( 7 5 8 0 . . ) 2 0 0 ( 2 8 9 . . ) 2 0 0 ( 7 4 8 0 . . ) 3 0 0 ( 8 5 9 . . ) 2 0 0 ( 0 6 8 0 . . ) 4 0 0 ( 5 5 9 . . ) 3 0 0 ( 2 5 8 0 . . ) 3 0 0 ( 0 3 9 . . ) 0 0 0 ( 0 0 7 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 8 8 6 0 . . ) 2 0 0 ( 8 8 9 . . ) 0 0 0 ( 0 0 7 0 . . ) 2 0 0 ( 2 9 6 . . ) 3 0 0 ( 7 7 6 0 . . ) 2 0 0 ( 0 8 6 . . ) 0 0 0 ( 0 5 8 0 . . ) 4 0 0 ( 5 5 8 . . ) 3 0 0 ( 8 3 8 0 . . ) 3 0 0 ( 7 4 8 . . ) 6 0 0 ( 3 3 8 0 . . ) 5 0 0 ( 2 3 8 . . ) 0 0 0 ( 0 0 0 1 . . ) 2 0 0 ( 2 9 9 . . ) 3 0 0 ( 7 7 9 0 . . ) 2 0 0 ( 0 8 9 . . ) 0 0 0 ( 0 0 0 1 . . ) 5 0 0 ( 8 6 9 . . ) 3 0 0 ( 2 8 9 0 . . ) 5 0 0 ( 5 5 9 . . ) 4 0 0 ( 0 2 9 0 . . ) 6 0 0 ( 0 2 9 . . ) 0 0 0 ( 0 5 9 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 7 4 9 0 . . ) 2 0 0 ( 3 9 9 . . ) 2 0 0 ( 5 3 9 0 . . ) 3 0 0 ( 7 7 9 . . ) 3 0 0 ( 2 3 9 0 . . ) 4 0 0 ( 5 6 9 . . ) 4 0 0 ( 7 1 9 0 . . ) 5 0 0 ( 8 4 9 . . ) 4 0 0 ( 7 1 9 0 . . ) 4 0 0 ( 8 4 9 . . ) 6 7 3 6 6 1 ( 6 2 8 6 . ) 3 7 3 4 5 1 ( 9 6 2 7 . ) 4 0 5 1 7 ( 8 5 4 4 . ) 2 9 3 7 6 ( 2 4 2 5 . ) 0 7 1 1 3 1 ( 9 9 6 9 . ) 6 2 1 0 2 1 ( 5 9 8 9 . ) 0 0 1 2 2 ( 5 9 1 . ) 9 6 6 9 2 ( 0 3 3 . ) 9 9 5 6 ( 4 3 1 . ) 1 2 7 6 ( 8 7 1 . ) 8 9 0 9 1 ( 1 2 2 . ) 6 4 2 2 2 ( 9 4 3 . ) 4 3 6 9 ( 0 3 1 . ) 0 9 0 4 ( 4 0 1 . ) 9 6 7 2 ( 3 0 1 . ) 7 4 7 1 ( 4 9 . ) 9 7 4 9 2 ( 7 3 2 . ) 0 4 4 8 1 ( 1 7 1 . ) 4 2 9 2 1 ( 8 4 2 . ) 1 8 6 9 ( 3 3 2 . ) 6 2 1 0 1 ( 5 1 2 . ) 6 5 8 7 ( 5 0 2 . ) 6 6 1 1 3 ( 5 7 4 . ) 4 9 9 1 2 ( 9 0 4 . ) 3 6 2 0 2 ( 3 1 2 . ) 6 2 9 5 ( 2 6 1 . ) 4 8 3 5 ( 1 4 1 . ) 4 6 0 3 ( 8 2 1 . ) 6 9 2 0 2 ( 5 4 2 . ) 0 6 8 9 ( 4 8 1 . ) 1 0 0 ( 3 6 7 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 9 6 7 0 . . ) 1 0 0 ( 5 9 9 . . ) 1 0 0 ( 3 6 7 0 . . ) 1 0 0 ( 5 9 9 . . ) 1 0 0 ( 9 6 7 0 . . ) 1 0 0 ( 5 9 9 . . ) 1 0 0 ( 0 6 7 0 . . ) 2 0 0 ( 3 7 9 . . ) 1 0 0 ( 4 6 7 0 . . ) 1 0 0 ( 1 7 9 . . ) 0 0 0 ( 0 5 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 3 0 0 ( 7 5 8 0 . . ) 3 0 0 ( 5 5 9 . . ) 2 0 0 ( 7 5 8 0 . . ) 3 0 0 ( 3 8 9 . . ) 3 0 0 ( 5 5 8 0 . . ) 4 0 0 ( 0 5 9 . . ) 3 0 0 ( 7 4 8 0 . . ) 4 0 0 ( 3 5 9 . . ) 3 0 0 ( 5 3 8 0 . . ) 4 0 0 ( 5 2 9 . . ) 0 0 0 ( 0 5 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 8 4 6 0 . . ) 2 0 0 ( 8 8 9 . . ) 0 0 0 ( 0 5 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 8 4 6 0 . . ) 2 0 0 ( 0 9 9 . . ) 0 0 0 ( 0 5 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 8 4 6 0 . . ) 2 0 0 ( 8 8 9 . . ) 3 0 0 ( 0 7 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 4 0 0 ( 8 8 6 0 . . ) 6 0 0 ( 0 5 9 . . ) 4 0 0 ( 5 6 6 0 . . ) 6 0 0 ( 0 6 9 . . ) 4 0 0 ( 5 7 6 0 . . ) 6 0 0 ( 7 2 9 . . ) 4 0 0 ( 7 6 6 0 . . ) 6 0 0 ( 2 6 9 . . ) 4 0 0 ( 8 7 6 0 . . ) 7 0 0 ( 7 4 9 . . ) 1 0 0 ( 2 0 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 5 1 8 0 . . ) 2 0 0 ( 5 8 9 . . ) 2 0 0 ( 5 0 8 0 . . ) 2 0 0 ( 3 9 9 . . ) 2 0 0 ( 2 1 8 0 . . ) 2 0 0 ( 2 8 9 . . ) 2 0 0 ( 7 0 8 0 . . ) 3 0 0 ( 0 7 9 . . ) 2 0 0 ( 3 1 8 0 . . ) 3 0 0 ( 5 7 9 . . ) 1 1 2 5 3 2 ( 2 5 8 4 . ) 1 1 1 4 7 1 ( 6 4 1 4 . ) 2 7 4 9 3 ( 7 2 1 3 . ) 5 7 4 8 2 ( 7 8 1 3 . ) 3 0 8 6 7 1 ( 5 9 1 9 . ) 5 5 9 4 2 1 ( 6 2 5 9 . ) 6 3 6 4 2 ( 8 1 2 . ) 2 9 2 9 4 ( 1 8 6 . ) 2 8 9 3 ( 6 2 1 . ) 7 1 5 6 ( 1 9 1 . ) 0 2 1 5 1 ( 9 2 2 . ) 3 5 1 0 3 ( 5 9 4 . ) 8 6 8 ( 3 8 . ) 2 1 4 1 ( 6 8 . ) 8 3 5 ( 2 8 . ) 0 0 4 1 ( 7 8 . ) 0 1 6 ( 2 8 . ) 2 2 4 6 ( 1 0 1 . ) 0 8 7 9 3 ( 2 7 8 . ) 0 5 2 6 3 ( 1 6 0 1 . ) 5 9 1 3 3 ( 3 0 8 . ) 3 5 4 6 3 ( 0 9 6 . ) 1 8 1 3 3 ( 3 0 8 . ) 9 2 4 0 3 ( 9 5 9 . ) 4 3 9 8 ( 0 3 1 . ) 9 6 4 1 1 ( 8 3 1 . ) 6 3 3 3 ( 2 1 1 . ) 7 6 5 3 ( 9 0 1 . ) 2 7 1 1 1 ( 7 6 1 . ) 0 8 1 3 1 ( 9 8 1 . ) 1 0 0 ( 6 1 7 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 8 1 7 0 . . ) 1 0 0 ( 3 9 9 . . ) 1 0 0 ( 5 1 7 0 . . ) 0 0 0 ( 8 9 9 . . ) 1 0 0 ( 9 1 7 0 . . ) 1 0 0 ( 5 9 9 . . ) 1 0 0 ( 2 1 7 0 . . ) 2 0 0 ( 4 7 9 . . ) 1 0 0 ( 0 2 7 0 . . ) 1 0 0 ( 4 6 9 . . ) 2 0 0 ( 5 3 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 3 0 0 ( 5 2 8 0 . . ) 4 0 0 ( 0 7 9 . . ) 4 0 0 ( 3 2 8 0 . . ) 4 0 0 ( 8 7 9 . . ) 3 0 0 ( 7 1 8 0 . . ) 5 0 0 ( 2 5 9 . . ) 4 0 0 ( 0 1 8 0 . . ) 6 0 0 ( 7 4 9 . . ) 3 0 0 ( 2 1 8 0 . . ) 6 0 0 ( 5 3 9 . . ) 0 0 0 ( 0 5 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 8 4 6 0 . . ) 1 0 0 ( 8 9 9 . . ) 0 0 0 ( 0 5 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 8 4 6 0 . . ) 1 0 0 ( 8 9 9 . . ) 1 0 0 ( 7 4 6 0 . . ) 1 0 0 ( 7 9 9 . . ) 1 0 0 ( 7 4 6 0 . . ) 1 0 0 ( 7 9 9 . . ) 3 0 0 ( 2 6 5 0 . . ) 0 0 0 ( 0 0 0 . . ) 3 0 0 ( 8 6 5 0 . . ) 5 0 0 ( 2 5 9 . . ) 3 0 0 ( 8 5 5 0 . . ) 5 0 0 ( 3 7 9 . . ) 3 0 0 ( 5 6 5 0 . . ) 6 0 0 ( 3 2 9 . . ) 4 0 0 ( 2 6 5 0 . . ) 8 0 0 ( 5 2 9 . . ) 4 0 0 ( 7 6 5 0 . . ) 1 1 0 ( 2 2 9 . . ) 0 0 0 ( 0 0 8 0 . . ) 0 0 0 ( 0 0 0 . . ) 2 0 0 ( 5 9 7 0 . . ) 3 0 0 ( 5 8 9 . . ) 2 0 0 ( 2 9 7 0 . . ) 2 0 0 ( 0 8 9 . . ) 2 0 0 ( 7 8 7 0 . . ) 4 0 0 ( 2 7 9 . . ) 4 0 0 ( 7 7 7 0 . . ) 4 0 0 ( 3 6 9 . . ) 3 0 0 ( 7 7 7 0 . . ) 4 0 0 ( 5 5 9 . . ) 9 0 5 9 6 1 ( 6 2 8 3 . ) 9 7 3 2 2 2 ( 7 5 3 4 . ) 3 0 1 4 4 ( 8 7 8 2 . ) 3 1 8 4 5 ( 1 6 0 3 . ) 5 3 4 2 0 2 ( 6 5 4 8 . ) 2 9 7 4 9 1 ( 7 5 2 9 . ) 5 6 3 9 4 ( 4 0 5 . ) 9 5 0 5 4 ( 5 3 5 . ) 9 3 1 0 1 ( 1 0 2 . ) 0 6 9 8 ( 6 2 2 . ) 4 9 9 1 3 ( 5 1 4 . ) 8 8 2 6 2 ( 6 8 4 . ) 2 8 3 ( 1 8 . ) 2 5 3 ( 1 8 . ) 0 5 5 ( 2 8 . ) 6 1 5 ( 2 8 . ) 1 6 0 1 ( 3 8 . ) 8 6 8 ( 3 8 . ) 1 5 9 8 3 ( 5 2 8 . ) 8 6 0 3 3 ( 1 6 7 . ) 8 6 8 9 1 ( 3 1 5 . ) 5 8 6 7 1 ( 3 5 4 . ) 1 5 5 4 3 ( 3 1 8 . ) 3 1 8 7 2 ( 6 8 7 . ) 8 3 8 3 ( 5 2 1 . ) 4 3 0 3 ( 1 2 1 . ) 4 7 4 3 ( 8 1 1 . ) 9 0 8 2 ( 5 1 1 . ) 3 3 7 0 1 ( 2 7 1 . ) 6 2 5 8 ( 9 5 1 . ) 1 0 0 ( 8 8 6 0 . . ) 0 0 0 ( 0 0 0 . . ) 1 0 0 ( 6 9 6 0 . . ) 1 0 0 ( 7 8 6 . . ) 1 0 0 ( 7 9 6 0 . . ) 1 0 0 ( 1 8 6 . . ) 1 0 0 ( 4 9 6 0 . . ) 0 0 0 ( 0 5 7 . . ) 3 0 0 ( 3 6 7 0 . . ) 1 0 0 ( 0 5 7 . . ) 3 0 0 ( 2 6 7 0 . . ) 2 0 0 ( 8 4 7 . . ) 3 0 0 ( 3 6 7 0 . . ) 3 0 0 ( 2 6 5 . . ) 4 0 0 ( 0 0 6 0 . . ) 3 0 0 ( 2 6 5 . . ) 5 0 0 ( 3 0 6 0 . . ) 4 0 0 ( 8 6 5 . . ) 5 0 0 ( 8 8 5 0 . . ) 2 0 0 ( 5 8 4 . . ) 2 0 0 ( 8 6 4 0 . . ) 2 0 0 ( 8 6 4 . . ) 3 0 0 ( 0 7 4 0 . . ) 3 0 0 ( 2 6 4 . . ) 4 0 0 ( 7 5 4 0 . . ) 3 0 0 ( 2 3 6 . . ) 3 0 0 ( 7 4 6 0 . . ) 4 0 0 ( 8 2 6 . . ) 3 0 0 ( 5 4 6 0 . . ) 4 0 0 ( 5 1 6 . . ) 5 0 0 ( 5 5 6 0 . . ) 1 0 0 ( 5 9 9 . . ) 0 0 0 ( 8 9 9 0 . . ) 1 0 0 ( 5 9 9 . . ) 1 0 0 ( 9 6 9 0 . . ) 2 0 0 ( 7 6 9 . . ) 0 0 0 ( 0 0 0 1 . . ) 4 0 0 ( 7 4 9 . . ) 3 0 0 ( 0 7 9 0 . . ) 5 0 0 ( 2 3 9 . . ) 3 0 0 ( 8 4 9 0 . . ) 5 0 0 ( 3 2 9 . . ) 0 0 0 ( 0 0 0 1 . . ) 4 0 0 ( 7 5 9 . . ) 2 0 0 ( 2 9 9 0 . . ) 4 0 0 ( 3 5 9 . . ) 4 0 0 ( 5 3 9 0 . . ) 5 0 0 ( 2 4 9 . . ) 0 0 0 ( 0 0 0 1 . . ) 6 0 0 ( 3 3 9 . . ) 3 0 0 ( 7 3 9 0 . . ) 6 0 0 ( 5 1 9 . . ) 5 0 0 ( 3 9 8 0 . . ) 7 0 0 ( 5 8 8 . . ) 0 0 0 ( 0 0 0 1 . . ) 6 0 0 ( 8 6 9 . . ) 3 0 0 ( 5 7 9 0 . . ) 7 0 0 ( 8 4 9 . . ) 6 0 0 ( 0 4 9 0 . . ) 8 0 0 ( 5 2 9 . . ) 6 5 1 7 7 1 ( 2 6 6 4 l - P . ) 9 7 8 4 0 2 ( 8 8 8 ) c ( l - P . ) 0 3 9 9 2 ( 8 0 0 3 . ) 3 0 4 3 3 ( 4 7 1 . ) 3 2 8 5 2 1 ( 0 2 5 9 . ) 9 4 6 5 3 1 ( 6 5 8 9 ) c ( a - P c - P ) c ( f m i . ) 5 9 1 1 1 ( 4 9 . ) 4 0 9 1 2 ( 7 8 2 . ) 0 5 9 8 ( 8 6 1 . ) 9 4 2 0 1 ( 3 2 . ) 3 1 2 6 2 ( 0 9 2 . ) 6 8 1 1 3 ( 2 5 4 . ) 5 0 2 8 ( 0 7 ) c ( l - P ) c ( a - P a - P l - P ) c ( f m i n O - P . ) 2 1 3 0 1 ( 5 8 ) c ( l - P . ) 9 4 0 6 ( 0 6 1 . ) 9 5 3 9 ( 6 7 . ) 4 7 9 6 2 ( 3 9 3 . ) 5 9 5 5 3 ( 5 0 4 . ) 0 6 9 9 1 ( 1 3 . ) 4 5 2 8 1 ( 5 1 6 . ) 0 6 9 6 1 ( 8 9 4 . ) 9 7 0 9 1 ( 7 9 . ) 1 7 3 2 3 ( 2 9 7 . ) 5 6 5 1 3 ( 1 3 9 . ) 6 4 8 7 4 ( 1 0 . ) 6 3 7 3 3 ( 4 9 4 . ) 0 3 1 0 2 ( 5 4 3 . ) 8 5 7 1 1 ( 2 1 . ) 4 9 5 1 4 ( 3 1 6 . ) 5 2 0 7 2 ( 9 1 5 ) c ( a - P c - P ) c ( f m i ) c ( l - P ) c ( a - P a - P l - P ) c ( f U o U ) c ( l - P ) c ( a - P a - P i - P ) c ( f m i Q 5 2 A 4 2 I H R"
        }
    ],
    "affiliations": [
        "New York University",
        "University of North Carolina at Chapel Hill",
        "Yale University"
    ]
}