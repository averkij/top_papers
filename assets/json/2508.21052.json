{
    "paper_title": "FakeParts: a New Family of AI-Generated DeepFakes",
    "authors": [
        "Gaetan Brison",
        "Soobash Daiboo",
        "Samy Aimeur",
        "Awais Hussain Sani",
        "Xi Wang",
        "Gianni Franchi",
        "Vicky Kalogeiton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 2 5 0 1 2 . 8 0 5 2 : r FakeParts: New Family of AI-Generated DeepFakes Gaëtan Brison1 Soobash Daiboo1 Samy Aïmeur1 Awais Hussain Sani1 Xi Wang2,1 Gianni Franchi3,1 Vicky Kalogeiton2,1 1Hi!PARIS, Institut Polytechnique de Paris 2LIX, École Polytechnique, CNRS, Institut Polytechnique de Paris 3U2IS, ENSTA Paris, Institut Polytechnique de Paris https://github.com/hi-paris/FakeParts Figure 1: FakePartsBench is the first dataset specifically designed to include FakeParts deepfakes and outputs from state-of-the-art AI generative models."
        },
        {
            "title": "Abstract",
            "content": "We introduce FakeParts, new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulationsranging from altered facial expressions to object substitutions and background modificationsblend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
        },
        {
            "title": "Introduction",
            "content": "Localized video manipulations present an alarming new frontier in the deepfake landscape. While public attention has focused on fully synthetic videos, this work identifies more insidious threat: FakePartsdeepfakes characterized by subtle, localized manipulations affecting only specific spatial regions or temporal segments of otherwise authentic videos. These partial manipulations, which may alter facial expressions, substitute objects, modify backgrounds, or manipulate individual frames, are particularly dangerous because they blend seamlessly with real content, making them exceptionally Equal supervision Preprint. difficult to detect. By preserving the majority of the original video, FakeParts leverage the surrounding original, real content to create deceptive content with unprecedented credibility. The real-world implications of these partial manipulations are concerning. Unlike obvious fullvideo deepfakes, FakeParts enable targeted reputation attacks where subtle alterations to facial expressions or gestures can change the perceived emotional context of authentic statements. They facilitate sophisticated disinformation campaigns where minimal changes to background elements or object appearances recontextualise events without triggering viewer scepticism. What makes these manipulations particularly timely is their psychological effectivenessour user study shows that even when explicitly instructed to look for AI-generated2 content, human observers fail to identify FakeParts, with detection rates dropping by over 40% compared to traditional deepfakes. This perceptual vulnerability creates challenge in deep-fake detection methods. Yet, current FakePart detection systems are not prepared to address this. This is mostly because currently no benchmark that specifically targets these partial manipulations exists. While recent advances in generative AI have enabled increasingly remarkable video manipulationswith models like Sora [9], VideoCrafter2 [11], and ModelScope [83] and specialized editing tools like ControlNet-like generation [34], video frame interpolation [92, 88, 41], camera control [36, 94, 90] and video inpainter [103, 112]research on detection methods has lagged behind, primarily due to the lack of representative data. Specifically, current deepfake detection research relies on very large-scale datasets with fully generated content or classic face-swaps [63, 60], creating significant blind spot for more nuanced alterations. To address this critical gap, we introduce FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Our dataset comprises over 25,000 videos spanning diverse manipulation techniquesfrom traditional face-swaps to state-of-theart generative videos, with particular emphasis on partial manipulations including localized inpainting, style transfer, object substitution, and frame-specific alterations and temporal interpolation. What distinguishes our dataset is its fine-grained annotation of manipulated regions, providing pixellevel and frame-level precision for evaluating FakeParts. Drawing from both public and private video sources, FakePartsBench offers unprecedented diversity in content, context, and manipulation techniques (see Figure 1 for snippets of video content in FakePartsBench). Our comprehensive evaluation reveals alarming detection gaps for these localized manipulations. Through extensive human studies involving over 60 participants, we demonstrate that FakeParts reduces human detection accuracy by 30% compared to full-video deepfakes, with certain categories of manipulation going completely undetected. Similarly, state-of-the-art detection models show performance degradation of up to 43% when confronted with partial manipulations versus fully synthetic content. Most concerning is the inverse relationship we discovered between manipulation subtlety and detection difficultythe smallest alterations often produce the most believable deceptions, precisely because they preserve maximum authentic context while changing critical semantic elements. Our main contributions are: (1) We define and characterize FakeParts as novel and increasingly prevalent family of deepfakes distinguished by partial, localized manipulations within otherwise authentic video content; (2) We present FakePartsBench, the first comprehensive benchmark dataset specifically designed to capture partial deepfakes, featuring detailed spatial and temporal manipulation annotations; (3) We conduct extensive human and detection studies demonstrating significant detection gaps for partial manipulations, establishing baseline performance metrics and revealing critical areas for improvement in next-generation detection systems. This work not only identifies critical vulnerability in current detection approaches but also provides the resources and empirical foundation to develop more robust defenses against increasingly complex video manipulations."
        },
        {
            "title": "2 Related Works",
            "content": "Generation and Detection of Deepfake Image: Before generative models, early detection methods [13, 25, 39, 46, 87] focused on low-level manipulations (e.g., copy-move, filtering), with countermeasures targeting feature inconsistencies in both images and videos. Generative Adversarial Networks (GANs) [28] first enable photorealistic image generation, especially when trained on large-scale datasets [8]. Mainstream detection methods [29, 57, 78, 99, 101] typically frame the task as binary classification, training DNN to learn noise fingerprints and artefacts that distinguish real 2In this work, we use AI-Generated and deepfakes interchangeably. 2 from fake images. However, these approaches often overfit to specific generation methods, network architecture [84], training data [29], and even image resolution [29]. This limitation has become more critical with the shift from GAN-based to diffusion-based generative models [38, 22, 71]. Recent studies [6, 15, 16] have shown that fingerprint-based detectors developed for GANs perform poorly on diffusion-generated content. In response, the field tries to cope with three main solutions: (1) extending the search for frequency artifacts or image-level cues that remain effective for diffusion models [6, 70, 106]; (2) leveraging pre-trained fundamental models (e.g. CLIP [66]) or even visionlanguage-models (VLMs) to extract more general and robust representations [17, 61, 102]; and (3) designing methods specifically tailored to the unique generation process of diffusion models [56, 91]. Image Deepfake Dataset: Naturally, datasets for training and evaluating detection models follow the prevailing trends in image generation and detection. Early works focused predominantly on GAN-generated images. Wang et al. [84] introduced the ForenSynths dataset containing content generated by various GAN models such as StyleGAN [44]. Meanwhile, growing concerns over deepfake technologies have led to the release of large amount of facial manipulation datasets, including DFDC [24], WildDeepfake [111], and FF++ [72], KoDF [48]. Limited by generative capacity and data distribution, most image-based Deepfake datasets adopt regional manipulation methods, i.e., typically modifying only part of the pixels, especially in facial regions. Generation and Detection of Deepfake Video: Early-stage video forgeries primarily involve face swapping, i.e., Deepfakes, which are mainly based on CNNs with GANs [60, 73, 8082, 95, 110]. Correspondingly, detection methods [31, 50, 65, 72, 104] largely follow image-based strategies: using backbone networks (e.g., CNNs, ResNet, or attention mechanisms) and training on paired real-fake facial datasets for binary classification. It is worth noting that most of these methods treat video frames independently, although some incorporate temporal cues or multimodal alignment, such as local part motion [35] or visual-audio synchronization [100, 108]. Unlike face-swapped videos, which are typically GAN-based and limited to facial regions, diffusionbased models [7, 9, 14, 109] can generate high-quality, realistic videos across diverse scenes. In contrast to the rapid progress in generation, detection methods are still behind, particularly for AI-generated video. As in image-based detection, recent advances focus on several directions. Song et al. [74] leverage vision-language models to both detect AI-generated videos and improve interpretability. [55] also explores frame-level consistency using pre-trained CLIP features. AIVGDet [4] incorporates optical flow to improve spatio-temporal decision-making. More recently, DeMamba [12] builds on the Mamba architecture [30] to efficiently capture temporal-spatial inconsistencies. Video Deepfake Dataset: Recently, new datasets like VidProM [89], GenVidBench [59], DeMamba [12], and GVD [4] have started exploring videos created using text-to-video (T2V) or image-to-video (I2V) methods. While recent datasets explore large-scale generation [59, 89], multiple generation models [4], and optimized prompt configurations [89], they remain limited in key aspects (see Table 1). Specifically, they focus solely on fully synthetic videos (via T2V or I2V), omitting partially manipulated content, and often lack high-resolution, high-quality samples produced by the latest or leading openand closed-source models. These limitations not only hinder progress in building robust Deepfake detection systems, but also fail to reflect real-world video forgery scenarios, which increasingly involve partial edits and content generated by high-quality proprietary models."
        },
        {
            "title": "3 What is missing from existing Deepfake datasets?",
            "content": "Video Deepfake methods span wide spectrum, not only in terms of underlying methodology (e.g., GAN [44, 60, 73, 8082, 95, 110] vs. diffusion [7, 9, 14, 109]), but also in the nature of input conditions, such as text-to-video (T2V) [11, 9, 14, 109], image-to-video (I2V) [7, 34, 43], or combination of both (TI2V) [9, 14]. They also differ in chromatic styling [93], spatial manipulation strategies, ranging from face swapping[63, 105] to regional inpainting [68] and outpainting [36, 90], as well as in temporal operations like frame interpolation [88]. To better understand the current landscape and limitations of video Deepfake datasets, we analyze recent literature and summarize key datasets in Table 1, organized by chronology, venue, methodology, and source of generation methods. From this table, we observe one main limitation: 3 Table 1: Landscape of Deepfake Video Datasets Metadata FakeParts Gen. Name Type Year Venue Spatial Temporal GAN Diffusion ForgeryNet [37] FakeAVCeleb [45] SWAN-DF [47] DiffSwap [105] AV-Deepfake1M [10] GVD [4] VidProM [89] GenVidBench [59] DeMamba [12] Face Face & Voice Face & Voice Face Face & Voice 2021 CVPR 2021 NeurIPS 2023 2023 CVPR 2024 MM IJCB General General General General PRCV 2024 2024 NeurIPS 2025 2025 arXiv arXiv FakePartsBench (Ours) General Style Table 2: Statistics of test sets of diffusion-based Deepfake Video Datasets. Statistics Dataset Dur.(s) Real Fake Hi-Res #Meth. Cls-src. GVD [55] VidProM [89] GenVidBench [59] DeMamba [12] FakePartsBench 3 3 2 4 5 0 0 13,800 10,000 16, 11,618 6,690,001 55,200 8,588 5,598 30,0001 0 2,1563 25,000 16,000 11 4 4 10 48 0 13,5002 2,082 12,000 FakeParts Realism VBench Metrics FVD_W* FVD_FP* Cons. Flick. Qual. 387.7 290.4 558.9 382.8 240.8 549.7 428.5 658.8 459. 211.5 0.932 0.916 0.932 0.934 0.968 0.946 0.964 0.967 0.656 0.605 0.448 0.651 0.940 0. 0.623 1 VidProM Hi-Res comprises 30,000 videos at 720p from StreamingT2V, Open-Sora 1.2, and CogVideoX-2B. As it lacks test set, we use these plus 10% randomly sampled from the training set. 2 GenVidBenchs closed-source data are 560p at 24 fps, generated using Pika in 2022. 3 DeMambas Hi-Res includes 700 samples from MorphStudio, 1,400 from Lavie (both from 2023), and 56 videos from Sora. * FVD_W and FVD_FP denote FVD computed against WebVid-10M and our collected real dataset, respectively. FakeParts.Modern datasets largely lack videos with regional or partial manipulations, referred to as FakeParts, which were prevalent during the GAN-dominated era but are now mostly absent from diffusion-based video generation benchmarks. Besides limitation #1, to further evaluate recent diffusion-based datasets, we benchmark key aspects: (1) generation quality through Fréchet Video Distance (FVD) [26] (against WebVid-10M [5] and our collected youtube respectively, computed on 10,000 samples) and (2) general video performance on VBench [40] (we show consistency, temporal flicker and image quality here). Table 2 summarizes these results along with dataset statistics, including total duration, number of test samples, generation methods (#Method), high-resolution samples (>720p), and closed-source content. Full experimental details are provided in the supplementary material. From Table 2, we observe one main limitation: Videos have low perceptual quality. We observe clear correlation between low resolution and low perceptual quality. For comparison, VidProM [89], which includes higher proportion of high-resolution content, achieves significantly better FVD scores than other existing datasets."
        },
        {
            "title": "4 FakePartsBench: A Benchmark for Video Deepfakes and FakeParts",
            "content": "Motivated by abore observations and analysis, we introduce FakePartsBench, benchmark designed to address the two aforementioned limitations by incorporating diverse and balanced mix of both full and partial manipulations (FakeParts); containing videos with high resolution and high quality. For the latter, we include videos generated by the latest open-source and closed-source models, such as Sora [9], Veo2 [14], and Allegro [109]. This ensures the timeliness and complementarity with existing benchmarks and also better reflects real-world scenarios, where an ever-growing share of Deepfake content online is produced by proprietary systems, often involving partial edits. FakePartsBench supports research into detecting both overt and covert forgeries. By combining large-scale coverage, high-resolution and fine-grained annotations, it enables rigorous evaluation of 4 models for next-generation Deepfake detection. We next define FakeParts and outline the structure of FakePartsBench. The following sections detail its creation process and design principles. Definition. For clarity, in this work, we refer to FakeParts as category of deepfakes. Full Deepfakes: These include entirely generated or heavily modified content, typically synthesized from single input modality: e.g., text-to-video (T2V), image-to-video (I2V), or combination of both (TI2V). See Section 4.3.I for detailed generation information. FakeParts: This category includes fine-grained manipulations that affect only specific aspects of video. We further divide these into three subtypes according to different aspect dimensions which will be elaborated in See Section 4.3.II: 1. Spatial FakeParts: Manipulations applied to specific regions, e.g., face swapping (FaceSwap), object modification (inpainting), or contextual expansion (outpainting). 2. Temporal FakeParts: Edits along the temporal axis, e.g., frame interpolation. 3. Style FakeParts: Changes in visual appearance without altering structural content: e.g., modifying color schemes or applying different visual styles."
        },
        {
            "title": "4.1 FakePartsBench Statistics",
            "content": "We present statistics of FakeParts in Table 3. FakePartsBench comprises 25,000 short video clips, most of which are at least five seconds long to balance temporal richness, crucial for capturing dynamic artifacts. The dataset covers both full Deepfakes and partial manipulations FakeParts. The latter are further categorized into spatial (inpainting, face swap, outpainting), temporal (interpolation), and style-based edits (style change). Content is generated using ten state-of-the-art models [109, 88, 42, 49, 107, 90], including recent closed-source systems such as Sora [9] and Veo2 [14], spanning both academic and proprietary sources. This diverse composition ensures that FakeParts reflects the latest advancements in generative video technologies. Compared to existing benchmarks, it offers greater diversity across methods, on average higher resolution and longer duration among diffusionbased datasets (see Table 2), while also addressing the lack of diffusion-based FakeParts combinations noted in Table 1. In addition, Figure 2 presents distributional statistics of FakePartsBench with respect to prompt length (measured by word count) and video content topics. Table 3: Number of videos per method and method type (with resolution, FPS, and duration)"
        },
        {
            "title": "Type of Methods Method",
            "content": "# Videos"
        },
        {
            "title": "Resolution",
            "content": "FPS Duration (s) T2V I2V TI2V"
        },
        {
            "title": "Real",
            "content": "Sora Veo2 Allegro AI Sora Veo2 Allegro AI Sora Veo2 Allegro AI"
        },
        {
            "title": "Framer\nRAVE\nDiffuEraser\nPropainter\nInsightFace\nAKiRa",
            "content": "4000 1000 1000 1000 1000 1000 1000 1000 1000 2000 2000 2000 2000 3000 2000 Real - ytb 30 24 15 30 24 15 30 24 15 7 10 6 6 30 8 28 5 5 5 5 6 5 5 6 3 14 7 6 13 2 7 1280 720, 1920 1080 1280 720 1280 720 1280 720 1280 720 1280 1280 720 1280 720 1280 720 512 320 680 384 1280 720 1280 720 892 500 576 320 1280 720 5 (a) Word-count distribution of FakePartsBench (b) Topic distribution of FakePartsBench Figure 2: Caption length and topic distribution of generated content in FakePartsBench."
        },
        {
            "title": "4.2 FakePartsBench collection process",
            "content": "FakePartsBench includes both real and fake videos; below, we outline their collection and generation."
        },
        {
            "title": "4.2.1 Real videos",
            "content": "Many methods, particularly those involving partial manipulations, depend heavily on real data. We incorporate several publicly available datasets as real sources, including DAVIS 2016 [62], DAVIS 2017 [64], YouTube-VOS 2019 [98], MOSE [23] and LVD-2M [96], long-take dataset with rich captions. The specific usage of each dataset per method is detailed in the supplementary materials."
        },
        {
            "title": "4.3 FakePartsBench generation process",
            "content": "Figure 3: Pipeline of FakePartsBench includes both full and FakeParts Deepfake videos. The partial manipulations, FakeParts, are categorized into three types: temporal, spatial, and style. In this section, we detail the generation process of FakePartsBench for each method. See Figure 3 for an overview and illustrative examples of the pipeline. 6 Vision-Language Models. In our benchmark, many videos and frames are captioned and, in some cases, evaluated using vision-language models (VLMs). Unless otherwise specified, we use PaLIGemma 2 (3B) fine-tuned on VQAv2 [75] I. Full Deepfakes The Full Deepfake category in FakeParts includes three primary types: Text-toVideo (T2V), Image-to-Video (I2V), and Text-Image-to-Video (TI2V). To generate them, we include the latest open-source video generator Allegro [109] (late 2024) and two state-of-the-art commercial systems: Sora [9] and Veo2 [14], all accessed between late 2024 and early 2025. The prompts used for generating videos are extracted using VLM. For methods that require an frame condition (I2V, TI2V), we extract the first frame from short segments of real videos. Both the prompt and the condition frame are stored and will be released as part of our metadata. II. FakeParts Spatial FakeParts information within the video content. In the spatial subset of FakeParts, we focus on methods that alter regional FaceSwap Generation. To create FaceSwap, we replaced the face in target video with that from source face image. This was achieved using the InsightFace library [69, 32, 27, 1, 2, 18, 20, 33, 21, 19], widely used tool for face detection and swapping. During generation, we use target videos from the Celeb-DF dataset [51] and source images from CelebA [53] to avoid the misuse of sensitive facial information. Both datasets focus exclusively on celebrities, whose facial identities are already publicly exposed. To ensure gender consistency after swapping, we prompt VLM with gender and filter the unmatched pairs. Inpainting Generation. Inpainting manipulations simulate object removal from video scenes using DiffuEraser [49] and ProPainter [107]. The pipeline involves three steps: (1) extracting an initial frame and using VLM to identify salient object for removal; (2) segmenting the object across frames with Grounded-SAM-2 [67]; and (3) applying the inpainting models to remove the object and fill the masked regions, generating realistic deepfakes. Outpainting. Unlike image outpainting [54, 3], video outpainting must account for camera motion and 3D scene consistency. Recent approaches address this challenge through explicit camera control [97, 36, 94, 90]. We adopt the AkiRA model [90], built on the SVD framework [7]. During generation, we apply backwards camera tracking trajectories to guide coherent extrapolation at the frame boundaries. Temporal FakeParts modify content along the time axis, such as frame interpolation. In the temporal subset of FakeParts, we focus on methods that complete or Interpolation Generation. For interpolation-based manipulations, we employ the Framer model [88] to synthesize smooth transitions between non-consecutive frames, simulating temporal manipulations. From each source video the first and last frames are provided to Framer, which generates 21 intermediate frames to fill the temporal gaps and produce temporally manipulated video sequence. Style FakeParts The style subset of FakeParts targets manipulations that retain the semantic content of the video but modify its appearance through style transfer techniques. Style Change Generation. Style-based manipulations alter the appearance and color of video content. We use the diffusion-based RAVE model [42] on videos from the Animal Kingdom dataset [58]. randomly selected frame is captioned by the VLM and the Gemma language model [79], and the RAVE model is prompted with: \"Change the color of the animal in the video.\""
        },
        {
            "title": "5 Experimental Protocol",
            "content": "To assess the effectiveness of deepfake detectors, we benchmark state-of-the-art methods on FakePartsBench, using both image-level and video-level models across diverse generative techniques. I. Image-Level Baselines We evaluate four strong image-level forgery detectors known for their performance on image synthesis benchmarks: 7 CNNDetection [85]: Trained on one generator (e.g., ProGAN), it generalizes to others by learning shared low-level artifacts. UnivFD [61]: CVPR 2023 approach that uses CLIP embeddings with linear probes or nearest neighbours to detect fakes from GANs, diffusion, and autoregressive models. NPR [77]: CVPR 2024 work focusing on pixel-neighbour relation-based information, trained on CNN-based datasets; FatFormer [52], CVPR 2024 work using CLIP backbone augmented with forgery-aware adapter and prompt conditioning; C2P-CLIP [76], An AAAI 2025 work with CLIP-based detector that employs categoryconcept prompts. All methods process video frames independently as 224224 image patches. We test two resizing strategies to adapt full-resolution frames: (1) Uniform resize: Warps the frame to 224224, preserving content but distorting geometry. (2) Aspect-preserving resize + crop: Resizes the shorter side to 224 and center-crops, maintaining the aspect ratio. These strategies ensure compatibility with image-level detectors while allowing analysis of sensitivity to spatial context and resizing artifacts. II. Video-Level Baselines We evaluate two recent methods that leverage temporal and motion cues: DeMamba [12]: structured state-space model capturing long-range spatial-temporal anomalies, validated on GenVideo. AIGVDet [4]: Combines spatial and optical flow branches to detect frame-level inconsistencies, fused into robust video-level predictions."
        },
        {
            "title": "5.1 Detection Performance",
            "content": "Table 4: Mean predicted probability that each fake category is classified as fake by automated detectors and human evaluators on FakePartsBench. Higher values indicate stronger detection confidence. For the FakeParts and Full Fake subsets (T2V, I2V, and IT2V), we report the performance drop (red) relative to each detectors original testset accuracy. Category AIGV [4] CNNDet [86] DeMamba [12] UnivFD [61] NPR [77] FatFormer [52] C2P [76] Human T2V I2V IT2V Stylechange Faceswap Real Interpolation Inpainting Outpainting 0.301 0.292 0.483 0.265 0.216 0.155 0.137 0.074 0.060 0.000 0.001 0.000 0.000 0.000 0.007 0.000 0.003 0.000 0.342 0.323 0.514 0.308 0.265 0.191 0.170 0.089 0. 0.073 0.083 0.072 0.295 0.031 0.052 0.228 0.337 0.025 0.579 0.417 0.666 0.105 0.000 0.038 0.056 0.264 0.014 0.183 0.129 0.161 0.100 0.620 0.008 0.360 0.213 0.096 0.176 0.157 0.131 0.288 1.000 0.004 0.396 0.171 0.125 Acc. on orig. All Avg. Full Fake Avg. FakeParts Avg. 0.914 0.240 (-0.674) 0.344 (-0.570) 0.144 (-0.770) 0.997 0.001 (-0.996) 0.000 (-0.997) 0.001 (-0.996) 0.971 0.273 (-0.698) 0.380 (-0.591) 0.173 (-0.798) 0.843 0.138 (-0.705) 0.075 (-0.768) 0.195 (-0.648) >0.925 0.325 (-0.600) 0.560 (-0.365) 0.108 (-0.817) 0.990 0.232 (-0.758) 0.164 (-0.826) 0.294 (-0.696) >0.930 0.289 (-0.641) 0.160 (-0.770) 0.408 (-0.522) 0.763 0.715 0.821 0.983 0.612 0.242 0.676 0.588 0.800 0.732 0.766 0.700 Model-wise Evaluation. Table 4 summarises the average fake-probability scores for all detectors; followed by the row repeats each models in-distribution accuracy for context. The video number weighted average results are reported in the last rows, categorised by Full Fake and FakeParts respectively. From the table, we have the following observations: (1) Universal performance drop: every detector suffers marked decline on our benchmark, across both high-quality full Deepfakes and the proposed FakeParts (FakeParts) categories. (2) CNNDetection collapses on diffusion content: trained for ProGAN images, it achieves virtually zero accuracy everywhere, revealing severe out-of-distribution (OOD) fragility. (3) AIGVDet, NPR and DeMamba handle full Deepfakes better but struggle with FakeParts: although they cope reasonably with complete forgeries 30-56%, they detect fewer than 22 % of Faceswap cases and only 67 % of Inpainting/Outpainting edits, showing that partial manipulations remain highly convincing. 8 (4) CLIP-based detectors excel on FakeParts but fail on full Deepfakes: UnivFD, FatFormer, and C2P-CLIP score higher on fine-grained edits (e.g. 34 % for UnivFD, 36 % for FatFormer, and 39 % for C2P-CLIP on Inpainting), but often fall below 20 % on the high-fidelity T2V and IT2V videos; the main exception is Faceswap, where FatFormer and C2P-CLIP achieve very high accuracy. Interestingly, from observations (3) and (4), we find that non-foundation-based models perform better on fully synthetic content, whereas foundation-model-based detectors (e.g. CLIP) generalize more effectively to partial or fine-grained manipulations. This highlights complementary strengths and weaknesses between the two model classes. We attribute this to feature reliance trade-off. Traditional binary classifiers, including recent models such as DeMamba, tend to rely heavily on low-level frequency artifacts. These signals are fragile and often degrade under domain shifts or standard pre-processing pipelines. In contrast, CLIP-based models (e.g., UnivFD, FatFormer, C2P-CLIP) leverage semantic-level representations, enabling stronger performance across many FakeParts categories. However, these semantic detectors struggle with advanced high-fidelity, semantically coherent, e.g. T2V content (Veo2 or Sora), where realism is high and visual artifacts are minimal. Interestingly, some frequency-based classifiers (such as DeMamba and NPR) exhibit comparatively better performance in these cases, suggesting that residual frequency artifacts may still persist in Full Deepfake videos. Overall, our results suggest that foundation-model-pretrained detectors generalize more effectively across diverse FakeParts manipulations, but may underperform on recent, high-quality full deepfakes. This finding is consistent with recent studies (e.g., [17], Section 6), and underscores the importance of benchmarks like FakePartsBench for evaluating detector robustness under realistic and varied manipulation conditions. Figure 4: GUI for user study: Participants classify video as real or fake, and provide an explanation."
        },
        {
            "title": "5.2 Human Perception Study",
            "content": "To assess human detection capabilities, we conducted user study using Streamlit-based survey3. total of 80 participants each labeled 20 randomly selected video clips (real or FakeParts), yielding approximately 1,600 annotations overall, half corresponding to real content and half to videos containing FakeParts. Humans achieved an average of 75.3% accuracy, outperforming all automated methods, especially on stylized and high-quality generations like Stylechange-Rave and T2V-AllegroAI. Videos with obvious stylistic manipulation (e.g., StyleChange, AllegroAI) were detected more easily than subtle manipulations like inpainting and interpolation. T2V models such as Veo2 posed greater challenges due to more coherent motion. However, subtle manipulations (e.g., inpainting, outpainting) remained challenging. More analysis is provided in the supplementary material. 3https://genaidetection.streamlit.app/"
        },
        {
            "title": "Interpolation",
            "content": "IT2V"
        },
        {
            "title": "Stylechange",
            "content": "T2V"
        },
        {
            "title": "Real",
            "content": "Figure 5: Examples of deepfake from FakeParts. The deepfake grouped by manipulation family. Each column corresponds to different technique family, showing three samples per group."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced FakePartsBench, the first benchmark dedicated to detecting FakePartsa novel and increasingly prevalent class of deepfakes characterized by localized manipulations within otherwise authentic videos. Unlike traditional fully synthetic content, these subtle edits exploit real context to evade both human and algorithmic detection. Through comprehensive experiments, we showed that FakeParts significantly reduces the performance of both human annotators and state-of-the-art detection models, exposing critical blind spot in current approaches. Our dataset bridges this gap by offering fine-grained spatial and temporal annotations, enabling rigorous evaluation of detection systems under partial manipulation conditions. We hope FakePartsBench will serve as foundation for future research, encouraging the development of detection models that can capture fine-grained inconsistencies and adapt to the evolving landscape of generative video manipulation. As generative models continue to advance, robust detection of FakeParts will be essential to ensure visual media integrity and safeguard against misinformation. Limitations. Our work still has several limitations. First, the overall dataset size, suitable for evaluation, is limited by the high cost and complex generation pipeline, making extension to large-scale training set target for future work. Second, while we include diverse manipulation types, further exploration at finer granularity: varying inpainting surface areas and their impact on detection, remains open. Lastly, although our goal is to advance detection, we acknowledge the potential risk of misuse; however, our focus is exclusively on defense and mitigation to support trustworthy AI."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was partially supported by the Hi! PARIS Foundation and Chair, and the French National Research Agency (ANR-22-CE39-0016) and Agence de lInnovation de Défense AID - via Centre Interdisciplinaire dEtudes pour la Défense et la Sécurité CIEDS - (project 2024 - FakeDetect). We also thank the participants who generously contributed their time to our user study."
        },
        {
            "title": "References",
            "content": "[1] Xiang An, Jiangkang Deng, Jia Guo, Ziyong Feng, Xuhan Zhu, Yang Jing, and Liu Tongliang. Killing two birds with one stone: Efficient and robust training of face recognition cnns by partial fc. In IEEE Conf. Comput. Vis. Pattern Recognit., 2022. [2] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, and Ying Fu. Partial fc: Training 10 million identities on single machine. In Int. Conf. Comput. Vis. Worksh., 2021. [3] Titas Anciukeviˇcius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1260812618, 2023. 10 [4] Jianfa Bai, Man Lin, Gang Cao, and Zijie Lou. Ai-generated video detection via spatial-temporal anomaly learning. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Springer, 2024. [5] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Int. Conf. Comput. Vis., 2021. [6] Quentin Bammey. Synthbuster: Towards detection of diffusion model generated images. IEEE Open Journal of Signal Processing, 5:19, 2024. [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [8] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, generation video-generation-models-as-world-simulators, 2024. Accessed: 2025-05-07. simulators. models world as and Yufei Guo. Video https://openai.com/research/ [10] Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav Dhall, Tom Gedeon, and Kalin Stefanov. Av-deepfake1m: large-scale llm-driven audio-visual deepfake dataset. In ACM Int. Conf. Multimedia, 2024. [11] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [12] Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, et al. Demamba: Ai-generated video detection on million-scale genvideo benchmark. arXiv preprint arXiv:2405.19707, 2024. [13] Vincent Christlein, Christian Riess, Johannes Jordan, Corinna Riess, and Elli Angelopoulou. An evaluation of popular copy-move forgery detection approaches. IEEE Transactions on Information Forensics and Security, 7(6):18411854, 2012. [14] Google Cloud. Veo2, May 2025. Accessed: 2025-05-07. [15] Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. Intriguing properties of synthetic images: From generative adversarial networks to diffusion models. In IEEE Conf. Comput. Vis. Pattern Recognit. Worksh., 2023. [16] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [17] Davide Cozzolino, Giovanni Poggi, Riccardo Corvi, Matthias Nießner, and Luisa Verdoliva. Raising the bar of ai-generated image detection with clip. In IEEE Conf. Comput. Vis. Pattern Recognit. Worksh., pages 43564366, June 2024. [18] Jiankang Deng, Jia Guo, Tongliang Liu, Mingming Gong, and Stefanos Zafeiriou. Sub-center arcface: Boosting face recognition by large-scale noisy web faces. In Eur. Conf. Comput. Vis., 2020. [19] Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In IEEE Conf. Comput. Vis. Pattern Recognit., 2019. [20] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Singleshot multi-level face localisation in the wild. In IEEE Conf. Comput. Vis. Pattern Recognit., 2020. [21] Jiankang Deng, Anastasios Roussos, Grigorios Chrysos, Evangelos Ververas, Irene Kotsia, Jie Shen, and Stefanos Zafeiriou. The menpo benchmark for multi-pose 2d and 3d facial landmark localisation and tracking. Int. J. Comput. Vis., 2018. [22] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Adv. Neural Inf. Process. Syst., 34:87808794, 2021. [23] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: new dataset for video object segmentation in complex scenes. In Int. Conf. Comput. Vis., 2023. 11 [24] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) dataset. arXiv preprint arXiv:2006.07397, 2020. [25] Hany Farid. Image forgery detection. IEEE Signal Processing Magazine, 26(2):1625, 2009. [26] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang. On the content bias in fréchet video distance. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 72777288, 2024. [27] Baris Gecer, Jiankang Deng, and Stefanos Zafeiriou. Ostec: One-shot texture completion. In IEEE Conf. Comput. Vis. Pattern Recognit., 2021. [28] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139 144, 2020. [29] D. Gragnaniello, D. Cozzolino, F. Marra, G. Poggi, and L. Verdoliva. Are gan generated images easy to detect? critical analysis of the state-of-the-art. In 2021 IEEE International Conference on Multimedia and Expo (ICME), pages 16, 2021. [30] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [31] Luca Guarnera, Oliver Giudice, and Sebastiano Battiato. Deepfake detection by analyzing convolutional traces. In IEEE Conf. Comput. Vis. Pattern Recognit. Worksh., pages 666667, 2020. [32] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for efficient face detection. arXiv preprint arXiv:2105.04714, 2021. [33] Jia Guo, Jiankang Deng, Niannan Xue, and Stefanos Zafeiriou. Stacked dense u-nets with dual transformers for robust face alignment. In Brit. Mach. Vis. Conf., 2018. [34] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In Int. Conf. Learn. Represent., 2023. [35] Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Lips dont lie: generalisable and robust approach to face forgery detection. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 50395049, 2021. [36] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [37] Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. Forgerynet: versatile benchmark for comprehensive forgery analysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 43604369, 2021. [38] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Adv. Neural Inf. Process. Syst., 33:68406851, 2020. [39] Chih-Chung Hsu, Tzu-Yi Hung, Chia-Wen Lin, and Chiou-Ting Hsu. Video forgery detection using correlation of noise residue. In 2008 IEEE 10th workshop on multimedia signal processing, pages 170174. IEEE, 2008. [40] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 2180721818, 2024. [41] Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. Video interpolation with diffusion models. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 73417351, 2024. [42] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models, 2023. [43] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In Int. Conf. Comput. Vis., pages 2262322633. IEEE, 2023. [44] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 44014410, 2019. 12 [45] Hasam Khalid, Shahroz Tariq, Minha Kim, and Simon Woo. Fakeavceleb: novel audio-video multimodal deepfake dataset. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. [46] Michihiro Kobayashi, Takahiro Okabe, and Yoichi Sato. Detecting forgery from static-scene video based on inconsistency in noise level functions. IEEE Transactions on Information Forensics and Security, 5(4):883892, 2010. [47] Pavel Korshunov, Haolin Chen, Philip Garner, and Sébastien Marcel. Vulnerability of automatic identity recognition to audio-visual deepfakes. In 2023 IEEE International Joint Conference on Biometrics (IJCB), pages 110. IEEE, 2023. [48] Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. Kodf: large-scale korean deepfake detection dataset. In Int. Conf. Comput. Vis., pages 1074410753, 2021. [49] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting, 2025. [50] Yuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping artifacts. arXiv preprint arXiv:1811.00656, 2018. [51] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: large-scale challenging dataset for deepfake forensics, 2020. [52] Huan Liu, Zichang Tan, Chuangchuang Tan, Yunchao Wei, Jingdong Wang, and Yao Zhao. Forgery-aware adaptive transformer for generalizable synthetic image detection. In IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [53] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Int. Conf. Comput. Vis., December 2015. [54] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1146111471, 2022. [55] Long Ma, Zhiyuan Yan, Qinglang Guo, Yong Liao, Haiyang Yu, and Pengyuan Zhou. Detecting ai-generated video via frame consistency, 2025. [56] Ruipeng Ma, Jinhao Duan, Fei Kong, Xiaoshuang Shi, and Kaidi Xu. Exposing the fake: Effective diffusion-generated images detection. arXiv preprint arXiv:2307.06272, 2023. [57] Francesco Marra, Diego Gragnaniello, Davide Cozzolino, and Luisa Verdoliva. Detection of gan-generated fake images over social networks. In 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), pages 384389, 2018. [58] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni, Si Yong Yeo, and Jun Liu. Animal kingdom: large and diverse dataset for animal behavior understanding. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1902319034, June 2022. [59] Zhenliang Ni, Qiangyu Yan, Mouxiao Huang, Tianning Yuan, Yehui Tang, Hailin Hu, Xinghao Chen, and Yunhe Wang. Genvidbench: challenging benchmark for detecting ai-generated video. arXiv preprint arXiv:2501.11340, 2025. [60] Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In Int. Conf. Comput. Vis., pages 71847193, 2019. [61] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 2448024489, 2023. [62] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In IEEE Conf. Comput. Vis. Pattern Recognit., 2016. [63] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Umé, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang, et al. Deepfacelab: Integrated, flexible and extensible face-swapping framework. arXiv preprint arXiv:2005.05535, 2020. [64] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 13 [65] Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing Shao. Thinking in frequency: Face forgery detection by mining frequency-aware clues. In Eur. Conf. Comput. Vis., pages 86103. Springer, 2020. [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn., pages 87488763. PmLR, 2021. [67] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. [68] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless. Film: Frame interpolation for large motion. In Eur. Conf. Comput. Vis., pages 250266. Springer, 2022. [69] Xingyu Ren, Alexandros Lattas, Baris Gecer, Jiankang Deng, Chao Ma, and Xiaokang Yang. Facial geometric detail recovery via implicit representation. In International Conference on Automatic Face and Gesture Recognition (FG), 2023. [70] Jonas Ricker, Simon Damm, Thorsten Holz, and Asja Fischer. Towards the detection of diffusion model deepfakes. Proceedings Copyright, 446:457. [71] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1068410695, 2022. [72] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. Faceforensics++: Learning to detect manipulated facial images. In Int. Conf. Comput. Vis., pages 111, 2019. [73] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing. In IEEE Conf. Comput. Vis. Pattern Recognit., June 2020. [74] Xiufeng Song, Xiao Guo, Jiache Zhang, Qirui Li, Lei Bai, Xiaoming Liu, Guangtao Zhai, and Xiaohong Liu. On learning multi-modal forgery representation for diffusion generated video detection. arXiv preprint arXiv:2410.23623, 2024. [75] Andreas Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, Siyang Qin, Reeve Ingle, Emanuele Bugliarello, Sahar Kazemzadeh, Thomas Mesnard, Ibrahim Alabdulmohsin, Lucas Beyer, and Xiaohua Zhai. Paligemma 2: family of versatile vlms for transfer, 2024. [76] Chuangchuang Tan, Renshuai Tao, Huan Liu, Guanghua Gu, Baoyuan Wu, Yao Zhao, and Yunchao Wei. C2p-clip: Injecting category common prompt in clip to enhance generalization in deepfake detection. In AAAI Conf. Artif. Intell., 2025. [77] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking the up-sampling operations in cnn-based generative network for generalizable deepfake detection. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 2813028139, 2024. [78] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei. Learning on gradients: Generalized artifacts representation for gan-generated images detection. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1210512114, 2023. [79] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui 14 Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. [80] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick Perez, Michael Zollhofer, and Christian Theobalt. Stylerig: Rigging stylegan for 3d control over portrait images. In IEEE Conf. Comput. Vis. Pattern Recognit., June 2020. [81] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Niessner. Face2face: In Proceedings of the IEEE Conference on Real-time face capture and reenactment of rgb videos. Computer Vision and Pattern Recognition (CVPR), June 2016. [82] Yu Tian, Xi Peng, Long Zhao, Shaoting Zhang, and Dimitris Metaxas. Cr-gan: learning complete representations for multi-view generation. arXiv preprint arXiv:1806.11191, 2018. [83] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [84] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A. Efros. Cnn-generated images are surprisingly easy to spot... for now. In IEEE Conf. Comput. Vis. Pattern Recognit., June 2020. [85] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surprisingly easy to spot... for now. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 86958704, 2020. [86] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surprisingly easy to spot...for now. In IEEE Conf. Comput. Vis. Pattern Recognit., 2020. [87] Weihong Wang and Hany Farid. Exposing digital forgeries in video by detecting double mpeg compression. In Proceedings of the 8th workshop on Multimedia and security, pages 3747, 2006. [88] Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Chunhua Shen. Framer: Interactive frame interpolation, 2024. [89] Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. In Adv. Neural Inf. Process. Syst., 2024. [90] Xi Wang, Robin Courant, Marc Christie, and Vicky Kalogeiton. Akira: Augmentation kit on rays for optical video generation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 26092619, 2025. [91] Xi Wang and Vicky Kalogeiton. Your diffusion model is an implicit synthetic image detector. In Eur. Conf. Comput. Vis. Worksh., 2024. [92] Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steven Seitz. Generative inbetweening: Adapting image-to-video models for keyframe interpolation. arXiv preprint arXiv:2408.15239, 2024. [93] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 76777689, 2023. [94] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM Trans. Graph., 2024. [95] Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, and Chen Change Loy. Reenactgan: Learning to reenact faces via boundary transfer. In Eur. Conf. Comput. Vis., September 2018. [96] Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, and Xihui Liu. Lvd-2m: long-take video dataset with temporally dense captions. arXiv preprint arXiv:2410.10816, 2024. [97] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. [98] Linjie Yang, Yuchen Fan, and Ning Xu. The 2nd large-scale video object segmentation challenge - video object segmentation track, October 2019. [99] Ning Yu, Larry S. Davis, and Mario Fritz. Attributing fake images to gans: Learning and analyzing gan fingerprints. In Int. Conf. Comput. Vis., October 2019. 15 [100] Rui Zhang, Hongxia Wang, Mingshan Du, Hanqing Liu, Yang Zhou, and Qiang Zeng. Ummaformer: universal multimodal-adaptive transformer framework for temporal forgery localization. In ACM Int. Conf. Multimedia, pages 87498759, 2023. [101] Xu Zhang, Svebor Karaman, and Shih-Fu Chang. Detecting and simulating artifacts in gan fake images. In 2019 IEEE International Workshop on Information Forensics and Security (WIFS), pages 16, 2019. [102] Yue Zhang, Ben Colman, Xiao Guo, Ali Shahriyari, and Gaurav Bharaj. Common sense reasoning for deepfake detection. In European Conference on Computer Vision, pages 399415. Springer, 2024. [103] Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. Avid: Any-length video inpainting with diffusion model. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 71627172, 2024. [104] Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei, Weiming Zhang, and Nenghai Yu. Multiattentional deepfake detection. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 21852194, June 2021. [105] Wenliang Zhao, Yongming Rao, Weikang Shi, Zuyan Liu, Jie Zhou, and Jiwen Lu. Diffswap: High-fidelity and controllable face swapping via 3d-aware masked diffusion. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 85688577, 2023. [106] Nan Zhong, Yiran Xu, Sheng Li, Zhenxing Qian, and Xinpeng Zhang. Patchcraft: Exploring texture patch for efficient ai-generated image detection. arXiv preprint arXiv:2311.12397, 2023. [107] Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and Chen Change Loy. ProPainter: Improving propagation and transformer for video inpainting. In Int. Conf. Comput. Vis., 2023. [108] Yipin Zhou and Ser-Nam Lim. Joint audio-visual deepfake detection. In Int. Conf. Comput. Vis., pages 1480014809, October 2021. [109] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024. [110] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Int. Conf. Comput. Vis., Oct 2017. [111] Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. Wilddeepfake: challenging real-world dataset for deepfake detection. In Proceedings of the 28th ACM international conference on multimedia, pages 23822390, 2020. [112] Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Rong Xiao, Kam-Fai Wong, and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility. In AAAI Conf. Artif. Intell., pages 1106711076, 2025. 16 Appendix: FakeParts: New Family of AI-Generated DeepFakes"
        },
        {
            "title": "A Extra information on the Human Perception Study",
            "content": "To rigorously assess the perceptual quality of the generated deepfake videos in our new benchmark, we developed dedicated annotation platform to collect large-scale human judgments. This platform enables participants to watch each video and indicate whether they believe it is real or fake. After submitting their judgment, they are prompted to provide short explanation justifying their decision. An illustration of the interface can be seen in Figure 6. In total, 60 individuals participated in the annotation process, providing both binary classifications and qualitative feedback. (a) First page the annotators see for Video1. (b) First page the annotators see for Video2. (c) Second page the annotators see for Video1. (d) Second page the annotators see for Video2. Figure 6: To evaluate human perception of localized manipulations, participants were first shown video and asked to classify it as either real or fake, as illustrated in (a) and (b) for two different examples. On the following screenshown in (c) and (d)they were then asked to justify their decision for the same video. Each participant annotated 20 videos, and the entire session took no more than 10 minutes. Annotators interacted with simple, intuitive UI that ensures both accessibility and consistency across evaluations. Metadata such as response times and annotator confidence levels were not collected to keep the privacy of annotators. After analysing the qualitative responses, we identified four recurring types of perceptual cues commonly cited by annotators when labelling videos as fake: 17 (a) Word-count distribution of FakePartsBench (b) Topic distribution of FakePartsBench Figure 7: The distribution in terms of the caption length and theme category of the generated video content in our FakePartsBench. Figure 8: Word cloud showing the most frequent terms from human explanations of synthetic content detection. Human Rationales. We analyzed participant comments and found four recurring patterns: 1. Temporal Inconsistencies: unnatural transitions, jitter, or overly smooth motion. 2. Facial and Body Artifacts: misaligned expressions, inconsistent eye/lip movement, weird blending. 3. Texture and Lighting Cues: blurry details, mismatched lighting/shadows. 4. Semantic Anomalies: unrealistic object behavior, physics violations. Implications. Human perception is strongly influenced by low-level visual cues and spatio-temporal coherence. To evade detection, generative models must improve: Motion realism (physically plausible dynamics), Temporal consistency (stable shadows and lighting), High-frequency detail (sharp but coherent textures)."
        },
        {
            "title": "B Extra Quantitative Results",
            "content": "A natural question arises when analysing deepfake videos: is motion information or spatial (imagelevel) information more crucial for detection? Table 5 provides insights into this by showing that the spatial-domain detector is the primary contributor to fake detection in most manipulation categories. 18 For example, categories such as IT2V, I2V, and T2V achieve detection scores of from 0.514 to 0.862, respectively, using only spatial features, while the optical detector lags behind with scores of 0.104 and 0.054. However, the optical-flow branch contributes significantly to detecting temporal artifacts in cases like Faceswap (0.366), suggesting that motion-based cues are critical for certain manipulations. The combined model, which fuses spatial and temporal scores via weighted average, consistently improves over each individual branch. This fusion is particularly beneficial in scenarios like Faceswap, where spatial evidence alone is insufficient (0.067), but motion artifacts are effectively captured. Table 6 reports the F1 score, AP, and accuracy of various deepfake detection methods evaluated on our benchmark, FakePartsBench. We observe that human annotators still achieve strong overall performance across all metrics (F1, accuracy, and AP). Among the automated methods, the results are consistent with the findings discussed in the main paper: foundation model-based approaches (e.g., UnivFD, FatFormer, and C2P-CLIP) handle FakePartsBenchbetter than non-foundation model-based methods (e.g., DeMamba). Category IT2V I2V T2V Stylechange Faceswap Real Interpolation Inpainting Outpainting Combined 0.483 0.292 0.318 0.265 0.216 0.155 0.137 0.075 0.060 Spatial-Domain Detector Optical Detector 0.862 0.514 0.581 0.522 0.067 0.310 0.273 0.031 0.119 0.104 0.071 0.054 0.007 0.366 0.000 0.000 0.118 0.000 Table 5: Breakdown of AIGVDets performance across manipulation types, comparing the combined model with its spatial and optical branches. Each branch contributes differently depending on the category."
        },
        {
            "title": "Detector",
            "content": "F1 Score Average Precision (AP) Accuracy AIGV [4] CNNDet [86] DeMamba [12] UnivFD [61] FatFormer [52] C2P-CLIP [76] NPR [77] Human Detection 0.319 0.001 0.349 0.252 0.375 0.467 0.399 0.750 0.586 0.058 0.568 0.745 0.967 0.987 0.872 0.755 0.532 0.497 0.530 0.550 0.612 0.651 0.610 0.751 Table 6: Comparative evaluation of detection methods across key performance metrics."
        },
        {
            "title": "C Implementation details with Extra Qualitative Results",
            "content": "I. Full Deepfakes The Full Deepfake category in FakePartsincludes three primary types: Text-toVideo (T2V), Image-to-Video (I2V), and Text-Image-to-Video (TI2V). To ensure the timeliness and complementarity with existing benchmarks, which primarily focus on earlier models such as SVD [7], AnimateDiff [34], and Lavie [], we include the latest open-source video generator Allegro [109], released in late 2024. In addition, we incorporate two state-of-the-art commercial systems: Sora [9] from OpenAI and Veo2 [14] from Google, accessed between late 2024 and early 2025. The prompts used for generating fake videos are extracted using vision-language models (VLMs). For methods that require an input frame as condition (I2V, TI2V), we extract the first frame from short segments of real videos. Both the prompt information and the corresponding frame are stored and will be released as part of our metadata. II. FakeParts 19 Figure 9: Examples of real videos collected from YouTube-VOS 2019 [98]. Figure 10: Examples of Full Fakes, including T2V (first two rows from SoRA [9] and second two rows are from Veo2 [14]) and IT2V from SoRA [9]. Green strokes indicate the condition image used in IT2V. 20 Figure 11: Examples of Spatial FakeParts, including FaceSwap, inpainting, and outpainting methods. In inpainting, the green mask indicates the region to be removed. Spatial FakeParts information within the video content. In the spatial subset of FakeParts, we focus on methods that alter regional FaceSwap Generation. To create FaceSwap deepfakes, we replaced the face in target video with that from source face image. This was achieved using the InsightFace library [69, 32, 27, 1, 2, 18, 20, 33, 21, 19], widely used tool for face detection and swapping. Given the special requirement of the facial information in the video we draw our target videos from Celeb-DF dataset [51] and source ones from the CelebA dataset [53] and an additional set of celebrity face images collected in 2022. To ensure gender consistency between the source and target frames, we prompt VLM with: \"What is the gender of the person in the image?\" and filter the unmatched pairs. Inpainting Generation. Inpainting-based manipulations simulate object removal within video scenes. For this task, we adopt two recent methods: DiffuEraser [49] and ProPainter [107]. The inpainting pipeline consists of three main steps. First, an initial frame from each video is extracted and analyzed using vision-language model (VLM) with the prompt: \"What is one interesting object in the image that is neither too small to be noticeable nor so large that it occupies almost the entire frame?\" to identify suitable target for removal. Second, the selected object is segmented across all frames using the Grounded-SAM-2 model [67], enabling consistent mask propagation. Finally, the masks and original videos are passed to the inpainting models (DiffuEraser or ProPainter), which remove the object and fill the masked regions, producing realistic inpainted deepfake videos. Outpainting. Unlike mainstream image inpainting and outpainting methods [54, 3], which primarily extrapolate content beyond image boundaries, video outpainting introduces additional complexities related to camera motion and 3D scene consistency. Recent methods address this by explicitly integrating camera control into video generation [97, 36, 94, 90]. In this section, we adopt the AkiRA model [90], built upon the SVD framework [7], which generates video content consistent with specified camera trajectories. Specifically, we apply backwards camera tracking as conditioning input, enabling coherent extrapolation at frame boundaries. 21 Figure 12: Examples of Temporal and Style FakeParts, including Frame interpolation and Style Transfer. Temporal FakeParts modify content along the time axis, such as frame interpolation. In the temporal subset of FakeParts, we focus on methods that complete or Interpolation Generation. For interpolation-based manipulations, we employ the Framer model [88] to synthesize smooth transitions between non-consecutive frames, simulating temporal manipulations. From each source video the first and last frames are provided to Framer, which generates 21 intermediate frames to fill the temporal gaps and produce temporally manipulated video sequence. Style FakeParts The style subset of FakeParts targets manipulations that retain the semantic content of the video but modify its appearance through style transfer techniques. Style Change Generation. Style-based manipulations involve altering the appearance or color of animals within video clips. To perform this task, we use the diffusion-based RAVE model [42]. Source videos are drawn from the Animal Kingdom dataset [58] with single frame randomly selected from each video and passed through VLM and the Gemma language model [79] for captioning and summarization. To generate the final stylized output, we prompt the RAVE model with: \"Change the color of the animal in the video.\""
        }
    ],
    "affiliations": [
        "Hi!PARIS, Institut Polytechnique de Paris",
        "LIX, École Polytechnique, CNRS, Institut Polytechnique de Paris",
        "U2IS, ENSTA Paris, Institut Polytechnique de Paris"
    ]
}