{
    "paper_title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math",
    "authors": [
        "Shrey Pandit",
        "Austin Xu",
        "Xuan-Phi Nguyen",
        "Yifei Ming",
        "Caiming Xiong",
        "Shafiq Joty"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 4 4 7 3 1 . 0 1 5 2 : r Preprint Salesforce AI Research HARD2VERIFY: STEP-LEVEL VERIFICATION BENCHMARK FOR OPEN-ENDED FRONTIER MATH Shrey Pandit, Austin Xu, Xuan-Phi Nguyen, Yifei Ming, Caiming Xiong, Shafiq Joty Salesforce AI Research Equal Contribution, {shrey.pandit, austin.xu}@salesforce.com Data: Code: https://huggingface.co/datasets/Salesforce/Hard2Verify https://github.com/SalesforceAIResearch/Hard2Verify"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, openended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as selfverification and verification-generation dynamics. Figure 1: Comparison of models evaluated on both ProcessBench (Zheng et al., 2024a) and our Hard2Verify benchmark. Past benchmarks do not sufficiently evaluate in the frontier-level math settings that Hard2Verify does; On the same error identification task, Qwen2.5-Math-PRM-72B performance drops from ProcessBench state-of-the-art at 78.3 to 37.3 on Hard2Verify."
        },
        {
            "title": "INTRODUCTION",
            "content": "Mathematical reasoning serves as gold-standard evaluation setting for benchmarking reasoning progress in large language models (LLMs). Over the past half-decade, benchmarks have been introduced to assess LLMs at the grade-school (Cobbe et al., 2021), high-school (Hendrycks et al., 2021), university (Zhang et al., 2023), and competition math level (MMA, 2025; He et al., 2024a; Gao et al., 2024). However, the progress of mathematical reasoning ability of LLMs has outpaced benchmark creation, with every subsequent release of frontier LLM saturating new benchmarks, most recently with GPT-5 Pro achieving 96.5%+ on AIME 2024. As result, recent efforts (Glazer et al., 2024; Phan et al., 2025) have written novel, unseen mathematical questions to test LLMs. 1 Preprint Salesforce AI Research Table 1: Comparison between Hard2Verify and existing step-level math benchmarks. Question Difficulty MR-GSM8K (Zeng et al., 2023) MR-MATH (Xia et al., 2025) MR-Ben (Zeng et al., 2023) ProcessBench (Zheng et al., 2024a) Easy-Hard PRMBench (Song et al., 2025) Easy Easy Easy Easy Open-Ended Responses? 10.3% Hard2Verify (Ours) Hard 78.5% Natural Responses? Generator Strength Annotator Weak Weak Weak Weak-Medium Weak Human Human Human Human Synth. + Human Check Strong Human Step-Level Labels? While the training approaches of closed frontier models remain secret, open-source progress in mathematical reasoning has been driven by scaling reinforcement learning from verifiable rewards (RLVR) (Lambert et al., 2024), with the breakthrough of DeepSeek-R1 (Guo et al., 2025) leading to an explosion of interest. This paradigm requires training data with solutions that are easily verifiable, i.e., have solutions that can be easily checked against known ground-truth by string matching or symbolic checkers. Math benchmarks, for the most part, also adopt the verifiable setup, where model response is considered correct if its final answer matches the established ground-truth. Answer correctness, while necessary condition for overall solution correctness, is not sufficient: It is now established that LLMs can produce incorrect intermediate reasoning but conclude with correct final answers (Lightman et al., 2023; Zheng et al., 2024a; Setlur et al., 2025). The next frontier for LLMs is solving problems that are hard to verify. grand example of such problem is proving the Riemann hypothesis, where the expected solution is not short phrase, but multi-step proof. To verify correctness, each step must be rigorously checked. Hints of openended problem solving abilities already exist: advanced reasoning systems (OpenAI, 2025; Google, 2025b; Huang & Yang, 2025) have achieved gold-level performance in the 2025 IMO. Here, LLM outputs were judged at the step-level by human experts who determined if steps are both correct and sufficiently supported, with supporting lemmas and claims all appropriately stated and applied. Training reasoning LLMs capable of open-ended problem solving requires scalable automatic evaluation: Not every LLM rollout during RLVR training can be audited by human experts. Rather, evaluation in open-ended settings requires step-level verifiers, typically process reward models (PRMs) or generative critic models. Such verifiers have already been used to provide dense process rewards (Lightman et al., 2023; Shao et al., 2024; Zha et al., 2025). Furthermore, step-level verifiers are also used in many test-time scaling methods, selecting the most promising candidate from multiple solutions or steps (Snell et al., 2024; Yu et al., 2025; Lifshitz et al., 2025; Zhou et al., 2025b). However, are these step-level verifiers sufficient for pushing the frontier of mathematical reasoning? This work introduces Hard2Verify, which gauges the ability of step-level verifiers to push the frontier. Hard2Verify benchmarks verifiers in assessing frontier LLM responses to difficult, recent, and open-ended math problems. We curate challenging problems from recent international mathematics competitions like IMO and Putnam, which are used to sample responses from three top-tier LLMs, GPT-5 (high) (OpenAI, 2025a), Gemini 2.5 Pro (Google, 2025a), and Claude Sonnet 4 (thinking) (Anthropic, 2025). Finally, we employ PhD-level math experts to annotate each modelgenerated step. The resulting benchmark is the culmination of over 500 hours of human effort, passing three rounds of independent agreement checks. This meticulous process yields 1860 rigorously graded steps across 200 unique model responses. Beyond operating at the frontier, Hard2Verify distinguishes itself from existing benchmarks for steplevel annotation  (Table 1)  . First, we emphasize collecting open-ended questions, with 78.5% of our samples being open-ended. This way, verifiers cannot cheat if they have seen the question or ground-truth answer during training; rather verifiers must substantively assess step correctness. Second, step correctness is judged not only on correctness, but also based on whether all invoked results, such as supporting lemmas or claims, are correctly stated and applied; saying follows from receives no credit if is not sufficiently justified or properly invoked. Third, Hard2Verify focuses on benchmarking verifiers in naturally occurring application settings: Verifiers must assess model-written responses, which often differ dramatically from human-written reference answers. We benchmark 29 models spanning proprietary models to open-weight models to PRMs. Compared to past work, Hard2Verify represents step up in difficulty, as shown in Fig. 1; Models capable of scoring 60%+ on ProcessBench (Zheng et al., 2024a) are unable to crack 20% on Hard2Verify. Our 2 Preprint Salesforce AI Research Figure 2: Breakdown of correct vs. incorrect steps (left) and responses (right) by model. We consider response incorrect if any step in the response is labeled incorrect. analysis reveals that this degraded performance is because weaker verifiers cannot identify mistakes, marking nearly every step as correct. We additionally analyze several fundamental questions in step-level verification: How should one to scale verifier compute? What are the impacts of selfverification? How much easier is generation than verification for frontier models?"
        },
        {
            "title": "2 BACKGROUND AND RELATED WORK",
            "content": "LLM-based verification. To meet demands for scalable evaluation, LLM-based evaluators have been proposed, originally focusing on chat settings (Zheng et al., 2023). However, as LLMs are deployed in challenging reasoning settings (Ke et al., 2025), recent evaluations have shown the need for more capable evaluators in reasoning domains (Frick et al., 2024; Tan et al., 2024; Zhou et al., 2025b). To get denser evaluation signal, focus quickly shifted to PRMs (Lightman et al., 2023) and synthetic ways to curate step-level training data (Wang et al., 2023; Luo et al., 2024). However, when used as dense reward signals for policy optimization, recent work has shown only limited improvement over outcome-level counterparts (Shao et al., 2024), which results from shortcomings process reward formulations. PRMs only measure if step could lead to correct, likely short-form final answer, not whether the step is correct in any absolute sense. As result, recent focused has shifted towards generative verifiers (Mahan et al., 2024; Zhang et al., 2025a; Liu et al., 2025), using the natural language generation abilities of LLMs to perform verification. This allows for more precise description of evaluation criteria and the ability to increase inference-time compute during the verification process. Benchmarking step-level verifiers in math settings. Table 1 contrasts Hard2Verify with related benchmarks. MR-GSM8K (Zeng et al., 2023) annotate model responses to GSM8K (Cobbe et al., 2021) questions on per-step basis to evaluate generative models as evaluators. MR-MATH (Xia et al., 2025) and MR-Ben (Zeng et al., 2024) follow similar approachs, increasing question difficulty with slightly harder sources like MATH (Hendrycks et al., 2021) and MMLU (Hendrycks et al., 2020). The two most relevant works to Hard2Verify are ProcessBench (Zheng et al., 2024a) and PRMBench (Song et al., 2025). ProcessBench uses mix of easy (GSM8K and MATH) and hard (OlympiadBench and Omni-MATH) questions, but is comprised largely of samples with single answer outputs1. Further, ProcessBench only evaluates first error identification ability of verifiers, rather than tasking verifiers to evalute every step. PRMBench, on the other hand, obtains step-level annotations by taking fully correct human-written and model-generated solutions from PRM800K and injecting errors with an LLM, yielding responses that are not naturally occurring: Human-and model-written text may have large differences in style and substance, while injected errors may not represent naturally occurring errors in model generation. Hard2Verify, in contrast, operates at the current frontier, tasking verifiers to evaluate responses from frontier-level LLMs to difficult, largely open-ended questions. 1To compute the fraction of open-ended questions in ProcessBench reported in Table 1, we count the number questions from the Omni-MATH split that do not appear in the Omni-MATH rule-based split. All other ProcessBench splits contain questions that are not open-ended. 3 Preprint Salesforce AI Research"
        },
        {
            "title": "3.1 DESIGN PHILOSOPHY",
            "content": "Hard2Verify is designed to test verifiers at the frontier of LLM-based math reasoning. At the question, response, and annotation level, Hard2Verify is curated based on the following philosophy: Questions. To measure progress in step-level verification, we must characterize how verifiers perform on extremely difficult, open-ended math questions. Open-ended problems represent the next frontier of mathematical reasoning, one where verifiers become increasingly important in lieu of available ground-truth answers. We focus our data collection on very recent mathematical Olympiads, prioritizing open-ended questions. Model responses. The responses that verifiers evaluate must be from highly capable, frontierlevel models. To push the frontier of math reasoning, verifiers must be able to tell when the most powerful models make potentially subtle mistakes. Moreover, such mistakes should be naturally occurring, i.e., arise naturally from the model generation process. We do not inject or edit an existing correct model-or human-written solution. This is meant to closely approximate the response distribution that verifiers will see in the wild, as they are applied in frontier math settings. Annotation process. We employ strict view of response grading: Any step that contains mistakes or is derived from previous mistake is considered incorrect, i.e., we do not employ Error Carried Forward grading. This is inspired by competitive math settings, the entire solution must be correct to receive full points. Based on this philosophy, we create Hard2Verify, as we describe in detail next."
        },
        {
            "title": "3.2 CURATING HARD QUESTIONS",
            "content": "We construct our benchmark by collecting problem statements and official solutions (Q, Aofficial) from leading math competitions including the IMO, Putnam, and INMO; We provide full list of sources in A. We focus question curation on recent (2024 and beyond) Olympiad-level math competitions. For each Olympiad, we parse the official PDFs using MathPix and extract all content in LATEX to preserve mathematical typography and ensure stable equation rendering. We exclude image-dependent problems and only keep questions that could be solved using textual information. The resulting question set comprises 80 frontier-level problems from 10 distinct Olympiads."
        },
        {
            "title": "3.3 RESPONSE GENERATION",
            "content": "Using our curated question pool, we sample responses from three frontier LLMs: GPT-5 (with high reasoning), Gemini 2.5 Pro, and Claude Sonnet 4 (Thinking). We employ standardized prompt ( C), instructing models to produce exam-style, stepwise proofs that mirror how an Olympiad participant would structure solution. We use the same prompt template and decoding settings across models and disable access to external tools, like web search or code interpreters. Each model produces single solution per problem, which we record for downstream evaluation. These samples are challenging; for example, Gemini 2.5 Pro takes up to 15 minutes to return solution via API access. After curating all model responses to all questions, we filter out responses with undesirable qualities, such as small number of long, dense steps or responses with degenerate outputs. This leaves us with compact but high quality set of 200 responses."
        },
        {
            "title": "3.4 ENSURING HIGH-QUALITY ANNOTATIONS",
            "content": "After sampling responses to our curated questions, human annotators meticulously annotate each model solution step-by-step. We partnered with Turing, research accelerator. Turing employs mathematical experts, with super-majority of our annotators having an advanced graduate level education in mathematics. To ensure consistent and high quality evaluations, we provided comprehensive annotation instructions as well official solutions Aofficial as references. Annotation began with multi-round pilot study, where we hand-annotated three model responses, then worked together with annotators to review samples, solicited feedback from annotators, and finetuned evaluation instructions accordingly. We then performed annotations in batches of samples, performing spot-checks of samples as they became available. This is in addition to internal processes at Turing, 4 Preprint Salesforce AI Research Figure 3: Count of correct (left) and incorrect (right) labels by model solution step. Models tend to begin solutions correctly, but typically start to get derailed after few steps. which include initial human annotation and three rounds of human review, where annotations were reviewed for correctness and guideline alignment. Overall, this process represents over 500 hours of manual human labor. See for more annotation details."
        },
        {
            "title": "3.5 OVERALL DATASET STATISTICS.",
            "content": "Our annotation process yields 1,860 unique model steps annotated across 200 model solutions. 58% (1,080/1,860) steps are labeled correct, while the remaining 780 are labeled incorrect. Fig. 2 shows how models perform on step-level and problem level. We consider model response correct if all steps in the solution are graded correct by humans. Claude Sonnet 4 takes the most steps but gets the least percentage of steps correct, whereas GPT-5 and Gemini 2.5 Pro are the best performing model in terms of step-level accuracy. However, at the response level, GPT-5 outperforms Gemini 2.5 Pro by larger margins. Claude Sonnet 4, while achieving over 50% step-level accuracy, fails to string correct steps together, only producing 4 entirely correct solutions out of 72. Fig. 3 visualizes how errors appear as function of steps, with all three models following similar trends: Errors tend to occur in the middle of model solutions appearing after few solution steps."
        },
        {
            "title": "3.6 EVALUATION TASKS",
            "content": "Our step-level annotations enable us to benchmark verifiers on three distinct tasks: (1) Step-level correctness (Step-Level), (2) Response-level correctness (Response-Level), and (3) First error identification (ErrorID). The Step-Level task corresponds to the setup in Song et al. (2025), whereas the ErrorID tasks corresponds to that of Zheng et al. (2024a). As we show in 4, both tasks are challenging settings for current verifiers. We provide our evaluation prompts in C. Step-Level. Here, the verifier is tasked with determining the correctness of each step. Generative verifiers are prompted to output binary yes/no label for each step, whereas PRM step-level scores are converted to binary labels via fixed threshold. Response-Level. We also consider response outcome task derived from Step-Level labels and predictions. This task reflects strict grading of open-ended math problems: For question to be correct, all steps in the solution must be deemed correct. Therefore, if any step in the solution is incorrect, we consider the solution wrong2. From human labels, we create single overall responselevel correctness label. Likewise, given step-level predictions from verifier, we create responselevel prediction. Note that this setting is more forgiving than the Step-Level setup: Exact step labels need not match exactly for verifier to agree with human at the response level. ErrorID. Here, the verifier is prompted to output the first step which contains mistake in the model solution, if present. If no error is present, generative verifier may output step 1, corresponding to No error. For generative verifiers, we note that first error step can also be derived from Step-Level labels, similar to the Response-Level setting. Following ProcessBench (Zheng et al., 2024a), we prompt the verifier to output the step index directly. This allows us to more directly compare verifiers between benchmarks; we quantify performance differences between the direct prompting approach and converting from step-level labels in 4.3. For PRMs, we select the first step below the correctness threshold. 2Because we are concerned with ensuring completely correct responses, we apply this procedure to all responses/questions in Hard2Verify, including non-open-ended questions. 5 Preprint Salesforce AI Research Table 2: Main evaluation results on Hard2Verify across our three evaluation tasks ( 3.6). We report Balanced Accuracy and Balanced F1 Score. Best and second-best scores in each category marked. Step-Level Response-Level Bal. Accuracy Bal. F1 Bal. Accuracy Bal. F1 Bal. Accuracy Bal. F1 ErrorID GPT-5 Gemini 2.5 Pro Claude Sonnet 4 GPT-5-Mini o3 o4-Mini GPT-4.1 Generative Critics, proprietary models 86.53 83.37 70.61 81.06 78.70 74.90 56.17 85.83 83.09 60.37 78.73 75.29 68.09 24.66 89.69 85.73 78.24 81.93 83.21 83.94 58.94 Generative Critics, large ( 70B) models Kimi K2 DeepSeek-R1 Qwen3-235B-A22B Qwen3-Next-80B-A3B Qwen2.5-72B-Instruct GLM-4.5-Air gpt-oss-120B Llama-3.3-70B-Instruct 61.79 68.92 72.55 67.91 56.01 57.40 78.10 54.28 42.83 62.30 64.03 54.69 26.36 29.40 74.64 18.37 65.34 73.95 79.42 75.08 61.06 61.78 83.92 57.04 89.52 85.46 73.44 81.92 82.58 81.71 33.55 51.66 72.75 77.87 68.31 46.89 41.00 83.71 28. Generative Critics, small/medium (< 70B) models Qwen3-32B Qwen3-30B-A3B ByteDance Seed-OSS-36B gpt-oss-20B Qwen3-14B Qwen3-8B Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct 63.99 70.71 66.79 75.18 65.48 65.26 60.45 48.82 51.77 61.91 53.09 70.93 52.91 53.51 47.59 22.84 67.86 73.79 72.54 83.85 74.59 77.61 63.40 55.67 63.16 71.02 63.88 83.32 70.12 72.45 63.23 44. Process Reward Models, open-source models Qwen2.5-Math-PRM-72B Qwen2.5-Math-PRM-7B Skywork-PRM-7B Skywork-PRM-1.5B ReasonFlux-PRM-7B UniversalPRM-7B 55.82 57.56 38.52 40.81 53.09 64.17 35.50 42.37 34.12 12.94 22.40 60.27 66.80 63.08 56.77 52.46 55.89 54.74 64.91 57.57 29.81 20.89 53.82 41. 70.61 52.46 53.45 65.96 60.32 67.31 52.44 49.10 54.23 60.90 58.29 26.49 41.97 63.97 49.44 51.96 58.83 59.24 46.13 53.69 45.92 43.47 29.75 41.80 35.03 11.56 8.62 42.48 26.08 69.72 52.46 39.30 60.04 57.31 57.62 21.29 31.40 45.35 50.78 43.05 16.38 17.81 60.64 2. 26.83 50.51 45.18 45.28 37.33 34.26 18.86 15.96 37.28 32.50 8.36 7.48 28.71 25."
        },
        {
            "title": "4.1 EVALUATION METRICS",
            "content": "Let TPR and TNR denote the True Positive Rate and True Negative Rate, i.e., verifier accuracies on the correct and incorrect samples, respectively. We define Balanced Accuracy as the mean of TPR and TNR and Balanced F1 Score as the harmonic mean of TPR and TNR3: Balanced F1 Score ="
        },
        {
            "title": "2 TPR · TNR\nTPR + TNR",
            "content": ", (1) We report Balanced Accuracy and Balanced F1 Score for all tasks. The ground-truth labels and model predictions vary based on task. For Step-Level, we aggregate all steps and all verifier predictions across all responses, whereas for Response-Level and ErrorID, we compute metrics at the response level. These metrics quantify verifier behavior in terms of correctly identifying mistakes versus correct answers. Balanced Accuracy and Balanced F1 both serve as aggregate measures: the former reflects average performance across both modes, while the latter penalizes imbalanced performance. An ideal verifier scores highly on both. 3This is equivalent to the F1 Score used by ProcessBench, which differs from the typically used F1 Score by using TNR instead of precision. To avoid confusion, we denote this metric Balanced F1 Score. Preprint Salesforce AI Research Table 3: Comparison of ErrorID performance using two prompting approaches, with = Step-Level ErrorID. The ErrorID prompt directly asks the verifier to identify the first step with an error, as in ProcessBench (Zheng et al., 2024a). The Step-Level prompt asks the verifier to produce correctness label for each step, from which the first step with an error is derived. Balanced Accuracy tends to improve with the Step-Level prompt, but performance variation in Balanced F1 tends to be mixed. ErrorID Prompt Bal. Accuracy Step-Level Prompt Bal. Accuracy Bal. Acc ErrorID Prompt Bal. Step-Level Prompt Bal. F1 Bal. F1 GPT-5 gpt-oss-120B GPT-5-Mini o4-Mini o3 Gemini 2.5 Pro Qwen3-235B-A22B Qwen3-30B-A3B DeepSeek-R1 gpt-oss-20B ByteDance Seed-OSS-36B Qwen3-Next-80B-A3B Claude Sonnet 4 Qwen3-14B Qwen3-8B Kimi K2 Qwen3-32B GPT-4.1 Qwen2.5-14B-Instruct GLM-4.5-Air Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct Llama-3.3-70B-Instruct 70.61 63.97 65.96 67.31 60.32 52.46 60.90 58.83 54.23 46.13 59.24 58.29 53.45 53.69 45.92 49.10 51.96 52.44 43.47 41.97 26.49 29.75 49."
        },
        {
            "title": "4.2 EVALUATED MODELS",
            "content": "76.72 69.68 66.43 67.16 68.02 66.11 65.17 60.19 61.53 66.44 58.94 63.37 60.83 56.56 57.35 54.26 52.35 51.97 40.30 53.24 48.09 43.01 50.71 +6.11 +5.71 +0.47 -0.15 +7.70 +13.65 +4.27 +1.36 +7.30 +20.31 -0.30 +5.08 +7.38 +2.87 +11.43 +5.16 +0.39 -0.47 -3.17 +11.27 +21.60 +13.26 +1.27 69.72 60.64 60.04 57.62 57.31 52.46 50.78 50.51 45.35 45.28 45.18 43.05 39.30 37.33 34.26 31.40 26.83 21.29 18.86 17.81 16.38 15.96 2.50 75.66 64.81 63.25 53.35 60.61 62.78 55.35 47.25 52.02 57.75 33.55 44.85 38.59 33.25 29.09 23.33 31.09 11.89 23.04 16.25 10.72 9.53 7.31 +5.94 +4.17 +3.21 -4.27 +3.30 +10.32 +4.57 -3.26 +6.67 +12.47 -11.63 +1.80 -0.71 -4.08 -5.17 -8.07 +4.26 -9.40 +4.18 -1.56 -5.66 -6.43 +4.81 We select variety of PRMs and generative models prompted as step-level critics. For prompted generative critics, we test closed-source frontier models as well as large ( 70B) and small-medium (<70B) open-weight models. We evaluate all reasoning models at their maximum provided reasoning level (e.g., high for GPT-5), using suggested sampling parameters for various baselines. All Qwen3 models are evaluated with thinking on. For instruction-tuned models, we use greedy decoding. For all models, we set the maximum number of output tokens to be 32K. The full set of models is enumerated in E. For PRMs, we select Qwen2.5-Math-PRM-{7B,72B} (Zhang et al., 2025b), Skywork-PRM-{1.5B,7B} (He et al., 2024b), ReasonFlux-PRM-{1.5B,7B} (Zou et al., 2025), and UniversalPRM (Tan et al., 2025). We tune PRM thresholds following Zheng et al. (2024a); See E.1 for more details."
        },
        {
            "title": "4.3 MAIN EVALUATION RESULTS",
            "content": "Table 2 presents our main results, with detailed results presented in B. Among proprietary models GPT-5 stands out in its overall ability across all three tasks, able to accurately provide step-level correctness labels and identify the first error in reasoning. Gemini 2.5 Pro follows closely for steplevel identification, but lags in error identification. Finally, Claude Sonnet 4 with Thinking enabled lags OpenAI models and Gemini 2.5 Pro, failing to match even reasoning models from previous generations, like o3 and o4-mini. Among larger open-weight models, the gpt-oss series are clear standouts, with gpt-oss-120B roughly matching the performance of o3. Recent larger Qwen3 models and DeepSeek-R1 challenge for second place in step-level verification, but all lag in terms of error identification. Notably, Llama3.3-70B, which performs admirably on ProcessBench (58.0 Balanced F1; Fig. 1) performs extremely poorly, achieving only 2.50 on Balanced F1 for the error identification task. Among smaller models, gpt-oss-20B performs extremely well on step-level and response-level tasks, but falters in identifying errors. ByteDance Seed-OSS-36B and Qwen3-30B-A3B are the next best performers, but only ByteDance Seed-OSS-36B is able to outperform random guessing4 perfor4Here, we consider verifier that randomly guesses as one that achieves 50% TPR and TNR, yielding Balanced Accuracy and Balanced F1 scores of 50.0. Preprint Salesforce AI Research Figure 4: Weaker models are unable to find mistakes, eventually considering all steps correct: TNR tends toward 0 while TPR tends towards 1. mance in error identification. Finally, even state-of-the-art PRMs, like the Qwen2.5-Math-PRM series struggle immensely on Hard2Verify, performing significantly below random guess performance in error identification. For example, Qwen2.5-Math-PRM-72B achieves only 37.28% Balanced F1. What separates strong verifiers from weak verifiers? To provide additional insights into performance variations across different models, Fig. 4 plots the TPR and TNR for all generative critics models, sorted in performance from strongest (left) to weakest (right) in terms of Balanced F1 Score. clear trend emerges: Verifier performance degrades because TNR drops quickly to near 0, while TPR rises gradually to almost 1. This indicates that almost all steps are labeled as correct, revealing that weaker verifiers cannot catch errors. Notably, the order of models from left to right approximately correlates with mathematical generation ability, i.e., the ability to solve extremely difficult math problems. As such, this may indicate that baseline level of solving ability is necessary prerequisite for verification. shows this trend holds similarly for Response-Level and ErrorID tasks. To identify errors, how should generative verifiers be prompted? Our ErrorID task adopts the setup of ProcessBench (Zheng et al., 2024a), which directly prompts the verifier to output the index of the first step with an error. However, the index of the first error can also be derived from step-level correctness labels, like those produced in the Step-Level task. In Table 3, we compare the performance under the ErrorID Prompt and Step-Level Prompt. Surprisingly, directly prompting for the given task may not yield the best performance: In terms of Balanced F1, performance across models is mixed, with some models exhibiting very small performance changes and others exhibiting significant changes. For example, switching from direct to step-level prompting significantly degrades performance for ByteDance Seed-OSS-36B from 45.18 to 33.55, while boosting performance for Gemini 2.5 Pro from 52.46 to 62.78. Overall, we find that more capable models, like GPT-5 and Gemini 2.5 Pro, benefit the most from switching to deriving first identified error from Step-Level outputs. We hypothesize that requiring step-by-step annotations requires models to more closely inspect each step, allowing for better error identification. This generally tends to benefit stronger models more capable of performing step-level verification well, but can hurt insufficiently capable models. As the change in performance is mixed across models, we advise practitioners to determine the better prompting setup on per-verifier basis."
        },
        {
            "title": "5.1 HOW SHOULD WE SCALE VERIFIER INFERENCE-TIME COMPUTE?",
            "content": "Here, we experiment with scaling verifier inference-time compute along via sequential and parallel approaches. We find sequential scaling brings substantive gains, whereas parallel scaling does not. Sequential inference-time compute scaling. Here we explore scaling inference-time compute sequentially by letting the verifier output more tokens when verifying, focusing on the Step-Level task. We use gpt-oss-20B, gpt-oss-120B, and GPT-5, which all have three distinct reasoning levels: 8 Preprint Salesforce AI Research Figure 5: Top: Scaling inference-time compute sequentially leads to higher performance in GPT-5 and gpt-oss models, with large gains for gpt-oss-20B (59.6970.93) and 120B (61.46 74.64) in terms of step-level Balanced F1. Bottom: Parallel decoding has little effect on step-level F1 performance for gpt-oss-20B, failing to bridge the gap vs. gpt-oss-20B at high-reasoning effort. low, medium, and high. In Fig. 5 (top), we plot Balanced F1. Affording the verifier to generate more thinking tokens at inference time generally improves performance, with gpt-oss-120B improving the most from low (61.46) to high (74.64) and gpt-oss-20B likewise improving significantly. Gains for GPT-5 are smaller compared to gpt-oss models, but still significant, with 12.3% relative improvement from low to high. Parallel inference-time compute scaling. Here, we attempt to match the performance of gpt-oss20B at high reasoning effort by sampling outputs in parallel from gpt-oss-20B at low reasoning effort. To do so, we sample 32 responses per sample from gpt-oss-20B and simulate best of from = 4, . . . , 16 via bootstrap sampling. Concretely, for each , we sample responses from the 32 with replacement, and aggregate predicted step-level labels via majority vote, breaking ties arbitrarily. To reduce variance, for each value of , we repeat this process for 10 trials, and report mean and standard deviation across the 10 trials in Fig. 5 (bottom). We also plot the baseline gpt-oss20B performance at low and high reasoning efforts. Surprisingly, Best-of-N does not meaningfully improve over sampling 1 response as increases. An intuitive explanation for this phenomenon is that step-level verification is inherently sequential task: Each step must be processed one-afteranother. As such, affording the verifier more time to think about each step is more effective than sampling multiple rushed judgments. Figure 6: Verifier TPR and TNR based on response generator model. For strong verifiers (GPT-5, Gemini 2.5 Pro), TPR varies based on generator, with GPT-5 exhibiting the most stable performance on per-generator basis. Claude Sonnet 4 generates the easiest to catch mistakes across all verifiers, whereas Gemini 2.5 Pro produces the hardest to catch mistakes, as measured by TNR. 9 Preprint Salesforce AI Research Figure 7: Each generators evaluates self-produced responses, and the fraction of steps correctly solved vs. fraction of steps correctly verified for given question is plotted. In general, we find that models are more successful in catching mistakes than generating error-free responses."
        },
        {
            "title": "5.2 HOW DO VERIFIERS VERIFY THEIR OWN RESPONSES?",
            "content": "We investigate the dynamics of self-verification, focusing on GPT-5, Gemini 2.5 Pro, and Claude Sonnet 4 as verifiers. Fig. 6 plots the step-level TPR and TNR performance based on response generator. The results notably depend on verifier strength: Table 2 shows that GPT-5 and Gemini 2.5 Pro are the top two performers, whereas Claude Sonnet 4 is relatively weak proprietary verifier. We find that GPT-5 and Gemini 2.5 Pro as verifiers are more likely to consider correct selfgenerated response as correct, as measured by TPR. Of the two, GPT-5 exhibits the least variation in TPR across models, while Gemini 2.5 Pro performance drops from 92.86 TPR on own-generated responses to as low as 85.09 TPR for GPT-5-generated responses. Claude Sonnet 4, on the other hand, overwhelmingly assigns Correct as label, leading to high TPRs regardless of generator. Across all threee models, it is easier to identify errors from the weaker model (Claude Sonnet 4) than it is to identify errors from the stronger models. This result is consistent with recent work (Zhou et al., 2025a) studying verification, which finds weaker generators produce easier to catch errors. Interestingly, both GPT-5 and Gemini 2.5 Pro struggle have the lowest TNR on responses from Gemini 2.5 Pro, showing that GPT-5 is more reliable in self-critique than Gemini 2.5 Pro is. The fact that Gemini 2.5 Pro has the lowest TNR on self-generated responses is consistent with recent work analyzing self-reflection (Stechly et al., 2023; 2024; Huang et al., 2023), where LLMs were shown to have difficulties correcting their own mistakes in challenging reasoning settings. In contrast, Claude Sonnet 4 as relatively weaker verifier cannot identify errors in stronger model responses. 5.3 IS VERIFYING PROBLEMS EASIER THAN SOLVING PROBLEMS? Here, we examine if generating solution is easier than verifying the same solution. We split Hard2Verify into three subsets corresponding to each of the three generator models and have the generators verify their own responses. For each response, we record the fraction of correctly generated steps (solve rate), as deemed by human annotators, and the fraction of correctly verified steps (verification rate), as deemed by agreement with human annotators. In Fig. 7, we plot the verification rate against the solve rate. We observe that the verification rate is consistently higher than the solve rate across all models; Only on few problems does the verifier have more difficult time verifying problem than generating the problem. This result offers some optimism for future work in verification: Because verifying solution tends to be easier than generating the solution, verifiers may not necessarily need to be as powerful as frontier generators to reliably identify errors."
        },
        {
            "title": "5.4 CASE STUDY: WHERE DO MODELS AND HUMANS DISAGREE?",
            "content": "We inspect outputs from relatively strong open-source verifier, ByteDance Seed-OSS-36B (Team, 2025) on multiple IMO-level problems and found recurring theme: The verifier incorrectly accepts partial or under-justified claims as correct. We provide two concrete examples below. These mismatches reflect larger systematic behavior in verifiers, revealed in 4: Current verifiers are too generous, with TPR rate tending towards 1 and TNR tending toward 0, indicating that vast majority steps are considered correct. 10 Preprint Salesforce AI Research On IMO 2023 Shortlist, question A6, Gemini 2.5 Pro makes generalized claim, but only proves the claim for single input. Human annotators catch this mistake, noting The equality holds only at one point ... not polynomial identity, so coefficients need not match. Seed-OSS-36B considers this step correct without mentioning the unfounded generalization. Similarly, on IMO 2024 Shortlist, question A1, Claude Sonnet 4 as generator constructs proof by cases by invoking Weyls equidistribution theorem, but considers only single case: if α is not an even integer, then α = + β with odd and 2/3 β < 1.... Seed-OSS-36B greenlights this step as correct, whereas human annotators find it incomplete: The case analysis ignores the branch where is even and 0 < β < 1/3,.... Further, the theorem invocation itself is deemed under-specified: justification [for invoking Weyls equidistribution theorem] should explicitly specify the estimate and the choice of n."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce Hard2Verify, human-annotated, step-level benchmark aimed to assess how step-level verifiers operate in frontier settings. We focus our data curation on recent open-ended math problems, sampling responses from frontier LLMs. The end result of over 500 hours of human annotation effort is benchmark that challenges many current open-source verifiers, which are unable to match the performance of larger, proprietary models."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "For human annotation, we partnered with Turing5, research accelerator that specializes in frontier data curation and annotation for AI applications. 5https://www.turing.com/ 11 Preprint Salesforce AI Research"
        },
        {
            "title": "REFERENCES",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Anthropic. Introducing claude 4. 2025. URL https://www.anthropic.com/news/ claude-4. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. How to evaluate reward models for rlhf. arXiv preprint arXiv:2410.14872, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. Google. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/googledeepmind/gemini-model-thinking-updates-march-2025/, 2025a. Google. Advanced version of gemini with deep think officially achieves gold-medal standard at the international mathematical olympiad. https://deepmind.google/discover/blog/advancedversion-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-internationalmathematical-olympiad/, 2025b. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024a. Jujie He, Tianwen Wei, Rui Yan, Jiacai Liu, Chaojie Wang, Yimeng Gan, Shiwen Tu, Chris Yuhao Liu, Liang Zeng, Xiaokun Wang, Boyang Wang, Yongcong Li, Fuxiang Zhang, Jiacheng Xu, Bo An, Yang Liu, and Yahui Zhou. Skywork-o1 open series, November 2024b. URL https: //doi.org/10.5281/zenodo.16998085. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. 12 Preprint Salesforce AI Research Yichen Huang and Lin Yang. Gemini 2.5 pro capable of winning gold at imo 2025. arXiv preprint arXiv:2507.15855, 2025. Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, Peifeng Wang, Silvio Savarese, et al. survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems. arXiv preprint arXiv:2504.09037, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Shalev Lifshitz, Sheila McIlraith, and Yilun Du. Multi-agent verification: Scaling test-time compute with multiple verifiers. arXiv preprint arXiv:2502.20379, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. MMA. (american invitational mathematics examination). https://maa.org, 2025. OpenAI. Gpt-5 system card. 2025a. URL https://cdn.openai.com/ gpt-5-system-card.pdf. OpenAI. Openai o3 and o4-mini system card. 2025b. URL https:// cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. OpenAI. Introducing GPT-4.1 in the api. https://openai.com/index/gpt-4-1/, April 2025. Accessed: 2025-09-25. OpenAI. Openai imo 2025 proofs. https://github.com/aw31/openai-imo-2025-proofs, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for LLM reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=A6Y7AqlzLW. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Preprint Salesforce AI Research Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesnt know its wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023. Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. On the self-verification limitations of large language models on reasoning and planning tasks. arXiv preprint arXiv:2402.08115, 2024. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784, 2024. Xiaoyu Tan, Tianchu Yao, Chao Qu, Bin Li, Minghao Yang, Dakuan Lu, Haozhe Wang, Xihe Qiu, Wei Chu, Yinghui Xu, et al. Aurora: Automated training framework of universal process reward models via ensemble prompting and reverse verification. arXiv preprint arXiv:2502.11520, 2025. ByteDance Seed Team. Seed-oss open-source models. https://github.com/ ByteDance-Seed/seed-oss, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical In Proceedings of the AAAI Conference on Artificial Intelligence, reasoning beyond accuracy. volume 39, pp. 2772327730, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Fei Yu, Yingru Li, and Benyou Wang. Scaling flaws of verifier-guided search in mathematical reasoning. arXiv preprint arXiv:2502.00271, 2025. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: metareasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080, 2023. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, et al. Mr-ben: comprehensive meta-reasoning benchmark for large language models. arXiv e-prints, pp. arXiv2406, 2024. Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane Boning, and Dina Katabi. Rl tango: Reinforcing generator and verifier together for language reasoning. arXiv preprint arXiv:2505.15034, 2025. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview.net/ forum?id=Ccwp4tFEtE. 14 Preprint Salesforce AI Research Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025b. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024a. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37: 6255762583, 2024b. Yefan Zhou, Austin Xu, Yilun Zhou, Janvijay Singh, Jiang Gui, and Shafiq Joty. Variation in verification: Understanding verification dynamics in large language models. arXiv preprint arXiv:2509.17995, 2025a. Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, and Shafiq Joty. Evaluating judges as evaluators: The jetts benchmark of llm-as-judges as test-time scaling evaluators. arXiv preprint arXiv:2504.15253, 2025b. Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. ReasonfluxarXiv preprint prm: Trajectory-aware prms for long chain-of-thought reasoning in llms. arXiv:2506.18896, 2025. 15 Preprint Salesforce AI Research"
        },
        {
            "title": "A DETAILED DATASET SOURCES",
            "content": "In table we provide the distribution of the 80 problems we sourced from different Olympiads along with the date the Olympiads were conducted. For the IMO-shortlist, we report the earliest date that the shortlist questions were made publicly available, typically the calendar year after the Olympiad was conducted."
        },
        {
            "title": "Date of Olympiad",
            "content": "# Questions IMO - Shortlist 2023 IMO - Shortlist 2024 Putnam EGMO (European Girls Mathematical Olympiad) IMO (International Mathematical Olympiad) BMO (British Mathematical Olympiad) CMO (Canadian Mathematical Olympiad) USA-JMO (Junior Mathematical Olympiad) INMO (Indian National Mathematical Olympiad) USAMO (United States of America Mathematical Olympiad)"
        },
        {
            "title": "Total",
            "content": "10 29 12 6 6 4 4 4 3 2 80 Table 4: Distribution of questions from various Olympiads with Year-wise Splits"
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "We report TPR and TNR for all evaluated models in Table 5, alongside our aggregate metrics presented in 4. We also visualize TPR and TNR trends for the Response-Level and Step-Level tasks, similar to Fig. 4. As shown in Fig. 8, TNR is the primary driver in poor Balanced F1 performance: The weaker the verifier, the more it struggles in identifying mistakes, opting to mark nearly every step as correct. Figure 8: Response-Level and ErrorID tasks follow similar trends in TPR and TNR, with weaker verifiers unable to identify mistakes. 16 Preprint Salesforce AI Research Table 5: Complete metrics for our three evaluation tasks, reporting Balanced Accuracy, Balanced F1, TPR, and TNR. Step-Level Response-Level ErrorID TPR TNR Bal. Accuracy Bal. F1 TPR TNR Bal. Accuracy Bal. TPR TNR Bal. Accuracy Bal. F1 GPT-5 Gemini 2.5 Pro Claude Sonnet 4 GPT-5-Mini o3 o4-Mini GPT-4.1 Kimi K2 DeepSeek-R1 Qwen3-235B-A22B Qwen3-Next-80B-A3B Qwen2.5-72B-Instruct GLM-4.5-Air gpt-oss-120B Llama-3.3-70B-Instruct 94.35 88.15 97.50 94.81 95.09 97.50 98.24 96.02 90.28 97.41 97.87 96.76 97.50 94.54 98. 91.94 Qwen3-32B Qwen3-30B-A3B 95.65 ByteDance Seed-OSS-36B 97.04 93.06 gpt-oss-20B 94.17 Qwen3-14B 92.96 Qwen3-8B 88.33 Qwen2.5-14B-Instruct 84.44 Qwen2.5-7B-Instruct Qwen2.5-Math-PRM-72B Qwen2.5-Math-PRM-7B Skywork-PRM-7B Skywork-PRM-1.5B ReasonFlux-PRM-7B UniversalPRM-7B 89.50 87.13 51.55 74.53 93.47 80.00 78.72 78.59 43.72 67.31 62.31 52.31 14.10 27.56 47.56 47.69 37.95 15.26 17.31 61.67 10.13 36.03 45.77 36.54 57.31 36.79 37.56 32.56 13. 22.14 27.99 25.50 7.08 12.72 48.35 Generative Critics, proprietary models 85.83 83.09 60.37 78.73 75.29 68.09 24.66 85.71 80.95 97.62 80.95 90.48 97.62 97.62 93.67 90.51 58.86 82.91 75.95 70.25 20.25 89.69 85.73 78.24 81.93 83.21 83.94 58. Generative Critics, large ( 70B) models 42.83 62.30 64.03 54.69 26.36 29.40 74.64 18.37 95.24 83.33 90.48 97.62 90.48 97.62 88.10 97.62 35.44 64.56 68.35 52.53 31.65 25.95 79.75 16.46 65.34 73.95 79.42 75.08 61.06 61.78 83.92 57.04 86.53 83.37 70.61 81.06 78.70 74.90 56. 61.79 68.92 72.55 67.91 56.01 57.40 78.10 54.28 Generative Critics, small/medium (< 70B) models 63.99 70.71 66.79 75.18 65.48 65.26 60.45 48.82 55.82 57.56 38.52 40.81 53.09 64.17 51.77 61.91 53.09 70.93 52.91 53.51 47.59 22.84 85.71 88.10 97.62 90.48 92.86 97.62 66.67 80. 50.00 59.49 47.47 77.22 56.33 57.59 60.13 30.38 67.86 73.79 72.54 83.85 74.59 77.61 63.40 55.67 Process Reward Models, open-source models 35.50 42.37 34.12 12.94 22.40 60.27 55.56 44.44 17.65 11.76 66.67 27.78 78.05 81.71 95.89 93.15 45.12 81. 66.80 63.08 56.77 52.46 55.89 54.74 89.52 85.46 73.44 81.92 82.58 81.71 33.55 51.66 72.75 77.87 68.31 46.89 41.00 83.71 28.16 63.16 71.02 63.88 83.32 70.12 72.45 63.23 44.18 64.91 57.57 29.81 20.89 53.82 41.46 78.57 52.38 80.95 85.71 73.81 92.86 92. 78.57 76.19 85.71 88.10 42.86 73.81 78.57 97.62 88.10 80.95 88.10 52.38 83.33 69.05 76.19 50.00 55.56 44.44 17.65 11.76 66.67 27.78 62.66 52.53 25.95 46.20 46.84 41.77 12.03 19.62 32.28 36.08 28.48 10.13 10.13 49.37 1.27 15.82 36.71 30.38 39.87 24.05 22.78 10.76 9. 28.05 25.61 5.48 5.48 18.29 24.39 70.61 52.46 53.45 65.96 60.32 67.31 52.44 49.10 54.23 60.90 58.29 26.49 41.97 63.97 49.44 51.96 58.83 59.24 46.13 53.69 45.92 43.47 29.75 41.80 35.03 11.56 8.62 42.48 26.08 69.72 52.46 39.30 60.04 57.31 57.62 21. 31.40 45.35 50.78 43.05 16.38 17.81 60.64 2.50 26.83 50.51 45.18 45.28 37.33 34.26 18.86 15.96 37.28 32.50 8.36 7.48 28.71 25."
        },
        {
            "title": "C PROMPTS FOR GENERATION AND EVALUATION",
            "content": "In this section we provide prompts used for generating responses to Olympiad-level math questions. We also provide the prompts used for the Step-Level and ErrorID tasks."
        },
        {
            "title": "Prompt used to generate responses to Olympiad questions",
            "content": "You are careful, rigorous math proof assistant. Provide correct, detailed, and complete proofs. Solve the following math problem formally. formal solution that can be verified by grader. Return detailed and Use start the proof with <start> followed by each step with <step>...</step>, and end with <end>. Only return the solution, in detailed steps, no headers, no explanations, no other text, only the <start> <step>...</step> <step>...</step> ... <end> tags. 17 Preprint Salesforce AI Research Prompt used for the Step-Level task The following is math problem and solution (split into steps, enclosed with tags and indexed from 0): [Math Problem] {problem} [Solution] {steps} Your task is to review and critique the solution step-by-step. For each step, determine if it is correct or incorrect. correct step is one where all of the content is correct, and is logically consistent with all previous steps and information given in the problem. An incorrect step is one where the content is incorrect, or is not logically consistent with all previous steps and information given in the problem, or is based on an error in previous step. Important: Any step that contains or is based on an error is considered incorrect. from previous step or is based on an error in the previous step, consider the step incorrect. That is, if the error is carried forward Provide reasoning for your correctness determinations. verdict should be comma-separated list of yes and nos, where each yes or no corresponds to steps correctness, with yes meaning correct and no meaning incorrect. Your final Please use the following format to return your answer: Reasoning: <your reasoning for each step> Verdict: <your comma-separated list of yes and nos> Do not use any other formatting, including markdown, bold text, code blocks, or any other formatting. your evaluation will be affected. If your formatting is incorrect,"
        },
        {
            "title": "Prompt used for the ErrorID task",
            "content": "The following is math problem and solution (split into steps, enclosed with tags and indexed from 0): [Math Problem] {problem} [Solution] {steps} Your task is to identify the first incorrect step in the solution. Instructions: - Review each step carefully for mathematical correctness and logical consistency - step is incorrect if it contains mathematical errors, logical inconsistencies, or is based on errors from previous steps - Find the FIRST step that is incorrect (0-indexed) - If ALL steps are correct, return -1 Provide your reasoning and then give your final answer as single number in the specified format. Please use the following format to return your answer: Preprint Salesforce AI Research Reasoning: <your detailed reasoning explaining which steps are correct/incorrect and why> Verdict: steps are correct> <the step number of the first incorrect step or -1 if all Examples: - If step 0 is the first incorrect step: - If step 3 is the first incorrect step: 0 3 - If all steps are correct: -1 Do not use any other formatting, including markdown, bold text, code blocks, or any other formatting. your evaluation will be affected. If your formatting is incorrect,"
        },
        {
            "title": "D ANNOTATION DETAILS",
            "content": "Each sample was annotated over four rounds: An initial annotation round and three rounds of reviews to resolve disagreements. total of 52 annotators were employed for grading, with 35 having at least graduate degree in mathematics or related fields. On average, model response took 90 minutes to grade and 63 minutes to review, with the longest response taking up to 4 hours. Annotators were given access to external tools, such as the internet, python, Wolfram Mathematica, and LLMs strictly as assistive aids. We present the detailed annotation guidelines provided to the math experts for step-by-step evaluation of each model solution below."
        },
        {
            "title": "Annotation instructions to human annotators",
            "content": "When annotating, refer to the reference answer(s) as possible solution(s)/proof(s). approaches, as these are open-ended questions. reference answer(s) is an example of valid approach; it may not be the only such valid approach. Each question may have multiple valid The provided Base your correctness decision off of the following criteria: Correct: step is considered correct if it is: Computationally valid: operations, such as addition or computing values of known functions (e.g., sin(pi/2)) There are no mistakes in rote mathematical Logically valid: and information present in the original question. There are no intermediate mistakes in the reasoning. in the step must be logically deducible from previous correct steps. The step follows logically from previous steps Any and all conclusions If step invokes any third-party mathematical results, such as known theorems / lemmas (e.g., fundamental theorem of calculus) or intermediate results from previous steps, then annotators must verify that the result is used in valid way: (1) all assumptions of the result (theorem) are met (2) the consequence of the result (theorem) is correctly described and applied to the specific problem Important: Do not apply Error carried forward\" grading. If current step is derived from previous step that is incorrect, consider the current step incorrect, even if the logic/computation of the step is correct. Example: Step 1: 1 + 1 = 3 [Incorrect] 19 Preprint Salesforce AI Research We now must add 5 to Step 1s result, which gives us 8 Step 2: [Incorrect, even though the computation in the step is correct; It is based on an incorrect Step 1] Extra note: If model produces hand-wavy\" argument, wherein Hand-waviness\": they say that new result follows by similar logic/computation as previously established result, then annotators must verify that the hand-wavy argument in-fact holds. The previously established results assumptions are met by the new result scenario (2) The previously established computation/logic is applicable to the new This means verifying (1) Example: Step N: valid proof of Case 1, yielding Result 1 Step N+1: Result 2. Case 2 follows by similar argument to Case 1, yielding [This is hand-wavy\", as the exact computation is omitted by appealing to previously computed Steps] Incorrect: step is considered incorrect if it is: Based in any way on an incorrect past step. Logically invalid: mistake. Examples: The models output contains reasoning error or Unfounded logical leap Incorrectly invoking mathematical result or past result when assumptions/conditions are not satisfied Incorrect application of mathematical result when conditions are met, i.e., mis-applying theorem. Failing to consider/cover scenario or case within proof, i.e., the proof concludes without covering all scenarios and is incomplete. If the top-level proof misses case/scenario: text not in the model output, there is no concrete step to mark as incorrect. As result, mark the conclusion of the proof (i.e., last step) as incorrect and provide corresponding justification. As this case involves If an intermediate result is stated, but the derivation of the intermediate result misses case/scenario: states the intermediate result as incorrect (as well as any subsequent steps that depend on the intermediate result). As concrete toy example Say model is doing Proof by Cases for all real numbers. Mark the step that It splits its analysis into 2 cases, Case 1 (positives) and Case For Case 1, it proves the claim for all positive 2 (negatives). integers, but does not consider non-integer reals. Mark the step that contains the conclusion of Case 1 incorrect, as well as any subsequent steps that depend on Case 1. Computationally invalid: mistake. all complex expressions, such as integrals, trigonometric functions, etc. This should be relatively easy to spot, but please verify Makes an operation / value computation This is not an exhaustive list of errors. Note: computations, and document any error that occurs, no matter how minor. Verify all"
        },
        {
            "title": "E EVALUATED BASELINES",
            "content": "Here we provide comprehensive list of models that were evaluated on our benchmark. 20 Preprint Salesforce AI Research OpenAI: GPT-5, GPT-5-Mini (OpenAI, 2025a), o3, o4-Mini (OpenAI, 2025b), GPT4.1 (OpenAI, 2025), gpt-oss-120b, gpt-oss-20b (Agarwal et al., 2025) Google: Gemini 2.5 Pro (Google, 2025a) Anthropic: Claude Sonnet 4 (Anthropic, 2025) Moonshot (Kimi): Kimi-K2-Instruct-0905 (Team et al., 2025) DeepSeek: DeepSeek-R1 (Guo et al., 2025) Alibaba Qwen: Qwen3-235-A22B, Qwen3-Next-80B-A3B, Qwen3-32B, Qwen3-30BA3B, Qwen3-14B, Qwen3-8B (Yang et al., 2025), Qwen2.5-72B-Instruct, Qwen2.5-14BInstruct, Qwen2.5-7B-Instruct (Team, 2024), Qwen2.5-Math-PRM-72B, Qwen2.5-MathPRM-7B (Zhang et al., 2025b) Zhipu GLM: GLM-4.5-Air (Zeng et al., 2025) Meta: Llama-3.3-70B-Instruct (Grattafiori et al., 2024) ByteDance: ByteDance Seed-OSS-36B (Team, 2025) Skywork: Skywork-PRM-7B, Skywork-PRM-1.5B (He et al., 2024b) ReasonFlux-PRM-7B (Zou et al., 2025) UniversalPRM-7B (Tan et al., 2025) For Kimi K2, DeepSeek-R1, and GLM-4.5-Air, we used together.ai for inference. All other open-weight baselines were run locally, hosted via vLLM (Kwon et al., 2023) or SGLang (Zheng et al., 2024b). E.1 PRM THRESHOLD TUNING To decide the cutoff threshold for evaluated PRMs, we select 100 responses at random from our benchmark and tune PRM performance against this subset, following (Zheng et al., 2024a). The same 100 responses are kept fixed across all baselines, and we sweep the threshold from 0.1 to 0.9 in increments of 0.05. To select the threshold, we compute the harmonic mean of the three task-specific Balanced F1 Scores, prioritizing selecting threshold that yields strong yet balanced performance. We find that PRM performance can vary considerably based on chosen threshold."
        }
    ],
    "affiliations": [
        "Salesforce AI Research"
    ]
}