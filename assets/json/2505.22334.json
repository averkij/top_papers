{
    "paper_title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start",
    "authors": [
        "Lai Wei",
        "Yuting Li",
        "Kaipeng Zheng",
        "Chen Wang",
        "Yue Wang",
        "Linghe Kong",
        "Lichao Sun",
        "Weiran Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While \"aha moment\" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on MathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 3 3 2 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Advancing Multimodal Reasoning via\nReinforcement Learning with Cold Start",
            "content": "Lai Wei1,3 Yuting Li1 Kaipeng Zheng1,2 Chen Wang3 Yue Wang3 Linghe Kong1 Lichao Sun4 Weiran Huang1,2 1 School of Computer Science, Shanghai Jiao Tong University 2 Shanghai Innovation Institute 3 Zhongguancun Academy 4 Lehigh University Figure 1: Performance comparison between our models and other advanced models on different multimodal reasoning benchmarks at both the 3B and 7B scales."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing crucial role in this progress. While aha moment patternswhere models exhibit self-correction through reflectionare often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present comprehensive study on enhancing multimodal reasoning through two-stage approach: (1) supervised fine-tuning (SFT) as cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3%73.4% on MathVista, 62.9%70.4% on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start. Correspondence to Weiran Huang (weiran.huang@outlook.com)."
        },
        {
            "title": "Introduction",
            "content": "LLMs have demonstrated remarkable progress in chain-of-thought reasoning, most notably exemplified by OpenAIs o1, o3, and o4 models [19]. Follow-up studies have sought to replicate and extend these complex reasoning abilities, revealing that targeted post-training methods can further improve model performance on challenging tasks [13, 14, 44, 63]. Among these, DeepSeek-R1-Zero [14] demonstrated that an aha moment can autonomously emerge during reinforcement learning (RL), showcasing advanced problem-solving strategies. DeepSeek-R1 [14] further showed that RL with supervised fine-tuning (SFT) as cold start is particularly effective at enhancing the overall reasoning ability of LLMs. While these advances predominantly focus on text-only models, the potential of incorporating similarly complex reasoning strategies within Multimodal Large Language Models (MLLMs) has remained relatively underexplored. Recent follow-up works have attempted to advance the reasoning ability in MLLMs [10, 34, 54, 58, 60, 69] using various techniques, including SFT and RL scaling. Many works focus on Zero RL (i.e., directly applying RL without cold start) training, claiming that the aha moment can emerge through Zero RL [34, 69] in MLLMs, which indicate improved reasoning ability. Some concurrent works assert that Zero RL outperforms the combination of SFT and RL [4, 61]. In this paper, we first observe that the so-called aha moment pattern already exists in MLLMs before RL training, but this presence does not necessarily correlate with improved reasoning capabilities. Our analysis reveals that, while RL may increase the frequency of such reflective pattern, it may be more of mirage rather than an indicator of genuine improvements in reasoning capability. This finding challenges the prevailing assumptions about emergent reasoning in MLLMs and suggests need for more deliberate approaches to enhancing multimodal reasoning. Motivated by DeepSeek-R1s approach of collecting cold-start data to fine-tune the model as the initial RL actor, we conduct comprehensive empirical study of SFT and RL in unified framework in multimodal domain. Our methodology consists of two stages: (1) cold start phase where we develop multimodal Chain-of-Thought [51] (CoT) patterns for supervised fine-tuning, and (2) reinforcement learning phase using GRPO [40] to further enhance reasoning capabilities. In particular, we systematically investigate how different cold start strategies during SFT affect downstream RL performance in the multimodal domain, aiming to establish robust foundation for the subsequent RL training. Our experiments demonstrate that SFT-based cold start provides strong basis for RL scaling. Our approach achieves state-of-the-art results among open-source models at both parameter scales, as illustrated in Figure 1. For the 7B model, we observe substantial improvements over the base models (e.g., 66.3%73.4% on MathVista, 62.9%70.4% on We-Math), surpassing all models at the same scale and even outperforming some larger models, including GPT-4o and Skywork R1V-38B. Notably, our best 7B model achieves an average score improvement of +6.19 points across all benchmarks compared to the base model. Similarly, our 3B model demonstrates significant gains, achieving an average improvement of +10.84 points and performance competitive with, or even exceeding, several 7B models such as Qwen2.5-VL-7B and VLAA-Thinker-7Bhighlighting the effectiveness of our training paradigm. To further elucidate the interplay between SFT and RL, we conduct extensive ablation studies exploring how various SFT strategies and data qualities impact subsequent RL performance. Our findings reveal that while both SFT-only and RL-only approaches yield notable improvements over base models, their combination consistently delivers superior performance across wide range of experiments. Furthermore, we observe strong positive correlation between cold start (SFT) performance and final model quality after RL, suggesting that investing in high-quality supervision during the initial stage is crucial for maximizing overall gains. Our deeper analysis indicates that the structural patterns present in Chain-of-Thought reasoning can be learned independently of solution correctness, underscoring the importance of reasoning format, while the presence of the aha moment pattern does not necessarily correlate with enhanced reasoning ability. Our contribution can be summarized as follows: We empirically investigate the prevalence and effectiveness of aha moment patterns in MLLMs, demonstrating that these reflective patterns exist prior to RL but may not necessarily indicate enhanced reasoning capabilities. 2 Figure 2: The frequency and accuracy of models responses with and without aha moment. The results show that the presence of aha moment does not necessarily correlate with higher accuracy. Based on the above insights, we present the first comprehensive study of the impact of supervised fine-tuning as cold start for reinforcement learning in the multimodal domain, exploring diverse SFT strategies to enhance reasoning abilities. Empirical results demonstrate that an SFT-based cold start provides robust foundation for RL scaling. The combination of targeted SFT and subsequent RL yields substantial improvements across challenging multimodal reasoning benchmarks. Using this simple yet effective approach, we train state-of-the-art models at both the 3B and 7B parameter scales."
        },
        {
            "title": "2 Related Work",
            "content": "Recently, the mathematical reasoning abilities of MLLMs have become central focus of research [11, 12, 1517, 23, 57, 67, 70]. In contrast to traditional LLM-based mathematical reasoning [32, 62], which primarily relies on text, multimodal approaches must both process and interpret visual inputs, significantly increasing the complexity of tasks such as geometric problem-solving and chart interpretation [5, 33]. Several works in this field have sought to collect or synthesize large scale of math-centric multimodal data with explicit reasoning steps [9, 35, 42, 65]. Notably, the recent emergence of o1-like reasoning models [19] represents an initial step toward activating the slow-thinking capabilities of MLLMs, as demonstrated by several SFT-based methods, such as LLaVA-CoT [54], LLaVA-Reasoner [66], MAmmoTH-VL [15], and Mulberry [58]. Moreover, some concurrent works have further explored reinforcement learning approaches, particularly GRPO, in the post-training stage of MLLMs to enhance performance on multimodal reasoning tasks [6, 10, 34, 36, 41, 50, 60, 69]. These approaches typically follow these strategies: either directly applying supervised fine-tuning (SFT) with long reasoning data or reinforcement learning (RL) to train models from scratch, or first performing SFT before applying RL. In contrast, our work explores different direction by focusing on how the cold start stage impacts the subsequent RL stage for MLLMs in the unified two-stage post-training (SFT+RL) process. We systematically study various cold start strategies during the SFT phase and empirically demonstrate how they build strong reasoning foundations before applying RL, resulting in more effective multimodal reasoning capabilities."
        },
        {
            "title": "Reasoning Ability",
            "content": "Recent works claim that reinforcement learning (usually GRPO [40]) can trigger reflective thinking patterns, often characterized as an aha moment and considered an emergent phenomenon, which showcases the improved reasoning ability of the models [34, 69]. In our study, we observe that such reflective patternsindicative of an aha momentcan actually be found in Qwen2.5-VL [2] before conducting GRPO. Specifically, we sample the models responses 16 times for each query in the MathVision dataset [49] and analyze the presence of keywords such as re-evaluate and re-check in the responses. Interestingly, our analysis reveals that while these reflective expressions already exist, their presence In particular, we conduct does not necessarily correlate with improved reasoning capabilities. 3 experiments on Qwen2.5-VL-7B and other two models tuned using GRPO based on Qwen2.5-VL-7B: MM-EUREKA [34] and VLAA-Thinker [4]. According to Figure 2, we find that the frequency of aha moment-like expressions increases from 6.07 in Qwen2.5-VL to 6.59 in VLAA-Thinker and 11.98 in MM-EUREKA. However, the associated accuracies when such expressions appear actually drop. Specifically, the accuracy is 20.17 for Qwen2.5-VL, but only 18.98 for VLAA-Thinker and 18.68 for MM-EUREKA in these aha moment cases. These findings suggest that the so-called aha moment may be more of mirage than reliable indicator of genuine reasoning ability. While GRPO can increase the frequency of such patterns, they may not correlate with actual enhancement of reasoning performance. In many cases, these reflective patterns are ineffective or even misleading, sometimes introducing hallucinations rather than improving solution quality. We illustrate this issue with representative example below, where the model does begin to reflect and re-evaluatebut its reasoning remains flawed and hallucinatory. We also provide more case studies in Appendix D.1. Demo: An example of Aha Moment which does not indicate effective reflection pattern. Question: In the given parallelogram ABCD, if the sum of angle and angle measures 100, what is the measure of angle A? Qwen2.5-VLs Response: To solve this problem, we need to use the properties of parallelogram. ... Now, assume: angle = x, 100 This is incorrect, so we need to re-evaluate the problem. The correct interpretation should be that the sum of angle and angle is 180, not 100. Therefore, the measure of angle is: 140 angle = 180 x. Given: + (180 x) = 100 180 = To address this, we argue that reinforcement learning alone may struggle to induce new and effective reasoning strategies. Instead, we propose two-stage approach: (1) supervised fine-tuning (SFT) to initialize the model with high-quality, reasoning-rich patterns, followed by (2) reinforcement learning to activate and refine these patterns."
        },
        {
            "title": "4 Reinforcement Learning with Cold Start",
            "content": "In this section, we present our methodology for enhancing multimodal reasoning capabilities via reinforcement learning with cold start. Our method consists of two main stages: (1) cold start phase where we explore to construct diverse reasoning patterns in Chain-of-Thought [51] (CoT) data for supervised fine-tuning, and (2) reinforcement learning phase to further activate and improve the models reasoning capability."
        },
        {
            "title": "4.1 Cold Start",
            "content": "As demonstrated in the previous section, we observe that the current aha-moment like reflection patterns may not be effective, and directly applying RL does not sufficiently enable models to acquire strong reasoning capabilities in multimodal domain. We argue that it is crucial to first conduct supervised fine-tuning (SFT) as cold start to equip the model with strong reasoning abilities before introducing RL. In our study, we focus on exploring efficient ways to construct multimodal cold-start data for advanced reasoning. Specifically, we leverage synthetic data to generate different types of CoT reasoning patterns via distillation. Distilled-CoT. Synthesizing data using model distillation is powerful and widely-used technique [7, 24, 25]. Here, we use larger models (Qwen2.5-VL-7B [2] and Qwen2.5-VL-32B [2]) to generate CoT responses from seed dataset with ground truth annotations by rejection sampling [46, 47]. These distilled datasets are used for the supervised fine-tuning that serves as cold start stage. More details of the distillation are shown in Appendix A.2. Reflection-CoT. Recent studies highlight the great potential in solving challenging problems through explicit reflection [9, 14, 39]. To explore this, we consider two reflection-based CoT settings. Reflection-CoT (v1): In the rejection sampling process, we obtain both correct and incorrect responses, denoted as y+ and y. We explore simple two-step reasoning pattern inspired by prior work [9]: 4 Figure 3: Method overview. Our approach consists of two stages: (1) cold start phase using supervised fine-tuning with Chain-of-Thought data, and (2) reinforcement learning phase using GRPO to further enhance reasoning capabilities. + Wait, perhaps we could consider it from different perspective. Lets re-evaluate the problem step by step to ensure accuracy. + y+ This approach combines the incorrect response with the correct one, and resembles the Best-of-N test-time scaling strategy (with N=2), where the model selects the best response during multiple generations via reflection. Reflection-CoT (v2): As noted in Section 3, we find that aha moment already exists before RL training. Building on this, we conduct further rejection sampling using Qwen2.5-VL-32B to collect correct responses that exhibit such reflection pattern. Caption-CoT. We utilize classical multimodal CoT pattern [11, 45, 54], where the model first describes an image and then provides an answer. This approach aims to encourage the model to focus on image details before engaging in reasoning. Specifically, we generate caption for each image using Qwen2.5-VL-7B [2] and concatenate the caption with the previously sampled correct response. Self-Critic-CoT. Additionally, we adopt the prompting strategy in Wen et al. [53] to collect SelfCritic-CoT via rejection sampling. Specifically, Self-Critic-CoT is an iterative reasoning structure where models engage in self-comment and refinement of their initial responses. In this process, the model first generates draft answer, then critically evaluates its own response by providing detailed comments, before producing refined final output. The prompt used for this process is detailed in Appendix A.1. Generally, we explore several popular and straightforward methods to construct the CoT datasets for cold start. By conducting SFT on the aforementioned datasets, we aim to establish robust cold start foundation for subsequent RL training stages."
        },
        {
            "title": "4.2 Reinforcement Learning",
            "content": "After the cold start stage, we employ reinforcement learning using GRPO algorithm [40], following the strategy in DeepSeek-R1 [14], to further activate the reasoning ability of the models. In particular, GRPO optimizes computational efficiency by eliminating the need for separate value model; instead, it directly utilizes group-normalized rewards to estimate advantages. Specifically, for question and the correlated image from the training dataset Q, GRPO samples group of responses = {oi}G i=1 from the old policy πold and then optimizes the policy model by maximizing the following objective: min γi,t(θ) ˆAi,t, clip (γi,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t βDKL (cid:35) (cid:104) πθπref (cid:41) (cid:105) , (θ) = (q,I)Q,{oi}G i=1πθold (Oq,I) (cid:34) (cid:40) oi (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi t=1 5 Table 1: Performance comparison on different multimodal reasoning benchmarks. Our models achieve state-of-the-art performance at the scale of 3B and 7B. Models Size MathVision MathVerse MathVista We-Math Average Close-Source Models GPT-4o [18] Claude-3.5-Sonnet [1] - - 33.95 46.48 48.83 57. 59.50 68.20 Open-Source Multi-Modal Large Reasoning Models QvQ-Preview [2] Skywork R1V [36] 72B 38B 35.56 39.31 52.81 40. 69.20 67.50 Open-Source 7B Multi-Modal Models 65.00 73.05 65.29 60.06 51.82 61.34 55.72 51. Qwen2.5-VL [2] MM-Eureka [34] OpenVLThinker [10] ThinkLite-VL [50] VLAA-Thinker [4] Ours Qwen2.5-VL [2] LMM-R1 [60] TBAC-VLR1 [55] VLAA-Thinker [4] Ours 7B 7B 7B 7B 7B 7B 3B 3B 3B 3B 3B 24.87 28.36 25.30 27.71 26.61 30.26 (+5.39) 43.83 49.52 42.79 47.41 47.49 48.58 (+4.75) 66.30 70.40 64.10 69.00 66.20 73.40 (+7.10) 62.87 66.03 63.91 67.99 66.49 70.40 (+7.53) 49.47 53.58 49.03 53.03 51.70 55.66 (+6.19) Open-Source 3B Multi-Modal Models 19.47 25.30 25.03 22.41 27.04 (+7.57) 33.58 42.79 38.17 35.44 44.03 (+10.45) 56.30 64.10 59.80 59.40 68.20 (+11.90) 50.63 63.91 63.28 56.03 64.08 (+13.45) 40.00 49.03 46.57 43.32 50.84 (+10.84) where γi,t(θ) = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) , πref represents the reference model, and the term DKL introduces KL divergence constraint to limit how much the model can deviate from this reference. The advantage estimate ˆAi measures how much better the response oi is compared to the average response, which is computed using group of rewards {r1, r2, . . . , rG} for the responses in set O: ˆAi = rimean({r1,r2,...,rG}) . std({r1,r2,...,rG})"
        },
        {
            "title": "5 Experiments",
            "content": "To validate the effectiveness of our method, we demonstrate our experimental setup and main results in the following sections."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Training Datasets. For the cold start training phase, we curate diverse dataset of 50k examples from established open-source resources, including Geometry3K [27], GeoQA [5], GeoQA-Plus [3], Geos [38], AI2D [21], TQA [22], FigureQA [20], TabMWP [30], ChartQA [33], IconQA [28], Clevr-Math [26], M3CoT [8], and ScienceQA [29]. These datasets cover range of tasks, such as chart understanding, scientific question answering, and geometric reasoning. We apply different sampling strategies to construct Chain-of-Thought trajectories following the methodology described in Section 4.1. For the subsequent reinforcement learning phase after cold start, we apply the same dataset as the cold start phase. Baselines. We compare our models with series of advanced multimodal large language models. GPT-4o [18] and Claude-3.5-Sonnet [1] are advanced close-source MLLMs. QvQ-Preview-72B [2] and Skywork R1V-32B [36] are multimodal large reasoning models specifically designed for R1-like chain-of-thought capabilities. As for 3B models, we compare with LMM-R1-3B [60], TBACVLR1-3B [55], and VLAA-Thinker-Qwen2.5VL-3B [4]. For models at 7B scale, our comparisons include MM-Eureka-7B [34], OpenVLThinker-7B [10], ThinkLite-VL-7B [50], and VLAA-ThinkerQwen2.5VL-7B [4]. These 3B and 7B models are all trained from Qwen2.5-VL that are enhanced 6 Table 2: Effective rank and eRank of our models before and after post-training on four multimodal reasoning benchmarks. Effective Rank [52] MathVision MathVerse MathVista We-Math Qwen2.5-VL-3B [2] Ours (3B) Difference of Effective Rank (eRank) Qwen2.5-VL-7B [2] Ours (7B) Difference of Effective Rank (eRank) 74.30 101.68 +27.38 73.58 90.67 +17.09 66.25 85.91 +19.66 62.35 77.16 +14. 66.43 79.59 +13.16 63.85 75.85 +12.00 60.21 79.96 +19.75 60.14 73.16 +13.02 multimodal reasoning through various techniques such as fine-tuning, reinforcement learning, and iterative self-improvement for mathematical reasoning. Evaluation Benchmarks. We evaluate the MLLMs on four prominent multimodal mathematical reasoning benchmarks: MathVision [48], MathVista [31], MathVerse [64], and We-Math [37]. These benchmarks offer comprehensive evaluations with diverse problem types, including geometry, charts, and tables, featuring multi-subject math problems and meticulously categorized visual math challenges across various knowledge concepts and granularity levels. We provide our evaluation details in Appendix A.3."
        },
        {
            "title": "5.2 Main Results",
            "content": "In our experiments, we conduct reinforcement learning (RL) using GRPO [40] with cold-start strategy. Specifically, we begin with supervised fine-tuning (SFT) on distilled dataset generated from Qwen2.5-VL-32B [2], which serves as strong teacher model. This distilled data is then used to initialize smaller Qwen2.5-VL variants at both the 7B and 3B scales as the cold start stage. Following this initialization, we apply GRPO to further enhance the models multimodal reasoning capabilities. The performance of our models is summarized in Table 1. Our approach yields consistently strong performance across all four multimodal reasoning benchmarks, demonstrating the effectiveness of our training paradigm. In particular, our 7B model delivers an overall average improvement of +6.19 score over the base model (Qwen2.5-VL-7B). It also achieves state-of-the-art performance among all open-source 7B-scale models, outperforming strong baselines such as MM-Eureka [34], VLAA-Thinker [4], and ThinkLite-VL [50]. Furthermore, our model surpasses many larger models, such as GPT-4o [18] and Skywork R1V [36]. It is also competitive with QvQ-72B [43]. Besides, the 3B variant of our model also sets new state-of-the-art among open-source 3B multimodal models. It outperforms all existing 3B baselines, including LMM-R1, TBAC-VLR1, and VLAA-Thinker, by significant margins on all four benchmarks. Notably, our 3B model can achieve competitive results on par with, and in some cases surpassing, larger 7B models such as Qwen2.5-VL-7B and VLAA-Thinker-7B. In addition, we also calculate the effective rank and the difference (eRank) [52] for models before and after post-training in Table 2. On fix-sized model, the value of effective rank usually correlates with the amount of knowledge the model comprehends. After post-training (SFT and RL), extra knowledge is injected into the model, which leads to consistent increase in effective rank on different benchmarks. Overall, these results highlight the effectiveness of our cold-start RL pipeline in enhancing reasoning capabilities. They also demonstrate the potential of scalable training strategies in narrowing the performance gap between small and large multimodal language models."
        },
        {
            "title": "6 Ablation Studies",
            "content": "We conduct various ablation studies to explore the interplay between supervised fine-tuning (SFT) and reinforcement learning (RL). The experiments below are conducted based on Qwen2.5-VL-3B."
        },
        {
            "title": "6.1 The Effectiveness of Cold Start",
            "content": "To better understand the impact of our cold-start strategy, we conduct an ablation study comparing four variants: the base model, RL-only training, SFT-only training (cold start), and the full pipeline 7 Table 3: Ablation study comparing the effectiveness of different training strategies: Supervised Fine-Tuning (SFT) only, Reinforcement Learning (RL) only, and the combined SFT+RL approach on Qwen2.5-VL models at 3B and 7B scales. Method MathVision MathVerse MathVista We-Math Average Qwen2.5-VL-3B [2] + SFT + RL + SFT and RL Qwen2.5-VL-7B [2] + SFT + RL + SFT and RL 19.47 25.20 25.10 27. 24.87 28.62 29.80 30.26 33.58 44.39 39.95 44.03 43.83 46.90 49.29 48.58 56.30 67.60 66.50 68.20 66.30 71.20 73.50 73.40 50.63 60.63 63.62 64. 62.87 67.76 67.82 70.40 40.00 49.46 48.79 50.84 49.47 53.62 55.10 55.66 Table 4: Ablation study on different cold-start strategies for Qwen2.5-VL-3B. We find that the cold start performance correlates well with the final performance."
        },
        {
            "title": "Strategies",
            "content": "MathVision MathVerse MathVista We-Math"
        },
        {
            "title": "Average",
            "content": "None (Base model) + RL Cold Start (32B-Distilled-CoT) + RL Cold Start (7B-Distilled-CoT) + RL Cold Start (Caption-CoT) + RL Cold Start (Reflection-CoT v1) + RL Cold Start (Self-Critic-CoT) + RL 19.47 25.10 25.20 27.04 24.40 25.86 22.83 25.26 23.98 24.21 23.22 25. 33.58 39.95 44.39 44.03 41.95 43.65 40.74 42.23 40.43 41.98 40.91 41. 56.30 66.50 67.60 68.20 64.30 66.40 62.70 65.10 65.00 66.10 61.40 66. 50.63 63.62 60.63 64.08 59.14 64.13 58.97 63.16 60.17 62.59 57.76 64. 40.00 48.79 49.46 50.84 47.45 50.01 46.31 48.94 47.39 48.72 45.87 49. combining SFT and RL. The results are presented in Table 3. In particular, for 3B models, SFT provides substantial boost in performance over the base model, improving the average score from 40.00 to 49.46. While RL alone also improves performance to 48.79, it is slightly less effective than SFT. This suggests that RL without advanced reasoning prior is less effective at guiding the model toward structured problem-solving. When SFT is followed by RL, the model achieves new peak of 50.84, demonstrating that the cold-start stage equips the model with strong reasoning patterns, which are further refined by RL. The 7B models follow similar trend. Starting from stronger base (49.47), both SFT (+4.15) and RL (+5.63) lead to notable improvements. The best performance is achieved through the combination of SFT and RL, which yields an average score of 55.66, outperforming either individual component. These results collectively highlight that cold-start (SFT) plays crucial role in unlocking the full potential of reinforcement learning for multimodal reasoning. SFT with high-quality CoT data represents more direct and effective approach to enhancing models fundamental reasoning capabilities, rather than relying solely on RL. When combined, SFT and RL offer complementary and scalable path to enhance multimodal reasoning."
        },
        {
            "title": "6.2 Different Types of CoT Data in SFT",
            "content": "We explore various cold-start strategies beyond direct distillation from 32B teacher model (Qwen2.5VL-32B), focusing specifically on the 3B student model. These other strategies are detailed in Section 4.1 based on rejection sampling using Qwen2.5-VL-7B. As shown in Table 4, the choice of cold-start strategy significantly influences final model performance. Among all strategies, using CoT data distilled from strong 32B teacher model yields the best results, achieving an average score of 49.46 after SFT and 50.84 after subsequent RL. Direct distillation from Qwen2.5-VL-7B also provides substantial improvements over the base model, but is slightly less effective than the 32B teacher, suggesting that the quality and scale of the teacher model are important for constructing highquality cold start data. Other strategies, such as Caption-CoT, Reflection-CoT, and Self-Critic-CoT, 8 Figure 4: Comparison of model performance when trained on data with aha moment patterns (Reflection-CoT v2) versus randomly selected 32B-distilled data. Model trained on randomly selected data consistently outperform that trained on aha moment data, suggesting that these reflective patterns do not necessarily correlate with advanced reasoning capabilities. Table 5: Impact of data quality on model performance. Results show that while rejection sampling (correct data) yields the best performance, models trained on unjudged or even incorrect data still outperform the base model. Strategies MathVision MathVerse MathVista We-Math Average None (Base model) + RL Cold Start (Correct Data) + RL Cold Start (Unjudged Data) + RL Cold Start (Wrong Data) + RL 19.47 25.10 25.20 27.04 25.59 27. 24.41 24.44 33.58 39.95 44.39 44.03 42.08 42.44 37.77 40.38 56.30 66. 67.60 68.20 65.80 67.10 63.30 64.70 50.63 63.62 60.63 64.08 60.06 62. 55.40 63.51 40.00 48.79 49.46 50.84 48.38 50.00 45.22 48.26 also lead to notable gains compared to the base model, but their improvements are generally smaller than those achieved by simple teacher distillation. For example, Caption-CoT and Reflection-CoT (v1) achieve average scores of 46.31 and 47.39 after SFT, respectively, while Self-Critic-CoT yields 45.87. Nevertheless, all these strategies benefit from additional RL, with performance consistently increasing after the RL stage. In addition, we observe strong positive correlation between the performance of the cold start stage (SFT) and the final model performance after conducting RL. This suggests that selecting an effective cold start strategy is crucial for maximizing the overall benefits. In practice, investing effort in constructing strong SFT data may yield greater performance gains than relying on RL alone to enhance the models reasoning capabilities."
        },
        {
            "title": "6.3 Revisiting “Aha Moment”",
            "content": "As we observe in Section 3, the so-called aha moment in MLLMs does not necessarily correspond to genuinely useful reasoning patterns. To further investigate this, we conduct targeted experiment. As described in Section 4.1, we build dataset by applying rejection sampling to Qwen2.5-VL-32B, retaining only correct responses that exhibit the aha moment pattern. This yields 10K samples, referred to as Reflection-CoT (v2). For comparison, we also randomly sample 10K instances from the 32B-distilled dataset. We then perform SFT on each dataset, followed by GRPO. Results in Figure 4 reveal that the model trained on aha moment data actually underperforms the model trained on randomly selected data. This suggests that the presence of the aha moment in the responses does not necessarily translate into improved reasoning performance."
        },
        {
            "title": "6.4 Data Quality in SFT",
            "content": "We investigate the impact of data quality on model performance by examining different sampling strategies for SFT data collection: (1) Rejection Sampling: Selecting only responses that yield correct answers (our default approach). (2) Wrong-Only Sampling: Deliberately selecting responses that lead to incorrect answers. (3) Unjudged Sampling: Randomly selecting responses without considering correctness. As shown in Table 5, the highest performance is achieved with rejection sampling, 9 where only correct data is used. Interestingly, models trained on unjudged datawhere correctness is not consideredstill demonstrate notable improvements in reasoning ability. Even when trained exclusively on incorrect data, the model outperforms the base model after the cold start phase and continues to benefit from subsequent RL. These results suggest that the structural patterns present in Chain-of-Thought reasoning can be learned independently of solution correctness, highlighting the importance of reasoning format in addition to answer accuracy."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we demonstrate that aha moment patterns already exist in MLLMs before RL training but may not necessarily correlate with improved reasoning capabilities. We propose two-stage approach to enhance multimodal reasoning: first conducting supervised fine-tuning (SFT) with high-quality Chain-of-Thought data to establish strong foundation, followed by reinforcement learning (RL) to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods, achieving state-of-the-art performance among open-source MLLMs at both 3B and 7B scales."
        },
        {
            "title": "Acknowledgement",
            "content": "This project is supported by the National Natural Science Foundation of China (No. 62406192), Opening Project of the State Key Laboratory of General Artificial Intelligence (No. SKLAGI2024OP12), Tencent WeChat Rhino-Bird Focused Research Program, and Doubao LLM Fund."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, 2024. Preprint. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th international conference on computational linguistics, pages 15111520, 2022. [4] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. https://github.com/UCSC-VLAA/VLAA-Thinking, 2025. [5] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 513523, 2021. [6] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. In European Conference on Sharegpt4v: Improving large multi-modal models with better captions. Computer Vision, pages 370387. Springer, 2024. [8] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024. [9] Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. Vision-language models can self-improve reasoning via reflection. arXiv preprint arXiv:2411.00855, 2024. [10] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [11] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. 10 [12] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. [13] Google. Introducing gemini 2.0: our new ai model for the agentic era, 2024. URL https: //blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/ #ceo-message. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. [16] Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [17] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [20] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. [21] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [22] Daesik Kim, Seonhoon Kim, and Nojun Kwak. Textbook question answering with multi-modal context graph understanding and self-supervised open-set comprehension. arXiv preprint arXiv:1811.00232, 2018. [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [24] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. [25] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143, 2025. [26] Adam Dahlgren Lindström and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. [27] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Intergps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [28] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [29] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [30] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022. [31] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [32] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. [33] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [34] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning, 2025. URL https://github.com/ModalMinds/MM-EUREKA. [35] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. [36] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599, 2025. [37] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [38] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. [39] Darsh Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, et al. Rethinking reflection in pre-training. arXiv preprint arXiv:2504.04022, 2025. [40] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [41] Haozhan Shen, Zilun Zhang, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. https://github.com/om-ai-lab/VLM-R1, 2025. Accessed: 2025-02-15. [42] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [43] Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.github.io/ blog/qvq-72b-preview/. [44] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. [45] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [46] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Advances in Neural Information Processing Systems, 37:78217846, 2024. [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 12 [48] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. [49] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, 2024. [50] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. [51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [52] Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. Large language model evaluation via matrix entropy. arXiv preprint arXiv:2401.17139, 2024. [53] Pengcheng Wen, Jiaming Ji, Chi-Min Chan, Juntao Dai, Donghai Hong, Yaodong Yang, Sirui Han, and Yike Guo. Thinkpatterns-21k: systematic study on the impact of thinking patterns in llms. arXiv preprint arXiv:2503.12918, 2025. [54] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. CoRR, abs/2411.10440, 2024. [55] Junzhe Xu and Yuyang yin. Tbac-vlr1-3b-preview, 2025. URL https://huggingface.co/ TencentBAC/TBAC-VLR1-3B-preview. [56] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [57] Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Yuxiao Dong, and Jie Tang. Mathglm-vision: Solving mathematical problems with multi-modal large language model. arXiv preprint arXiv:2409.13729, 2024. [58] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [59] Zheng Yaowei, Lu Junting, Wang Shenzhi, Feng Zhangchi, Kuang Dongdong, and Xiong Yuwen. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [60] Peng Yingzhe, Zhang Gongrui, Zhang Miaosen, You Zhiyuan, Liu Jie, Zhu Qipeng, Yang Kai, Xu Xingzhong, Geng Xin, and Yang Xu. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. [61] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [62] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [63] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. [64] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [65] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024. 13 [66] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [67] Yu Zhang, Kehai Chen, Xuefeng Bai, Zhao Kang, Quanjiang Guo, and Min Zhang. Question-guided knowledge graph re-scoring and injection for knowledge graph question answering. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 89728985, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.524. URL https://aclanthology.org/2024. findings-emnlp.524/. [68] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517. [69] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros \"aha moment\" in visual reasoning on 2b non-sft model, 2025. URL https://arxiv.org/abs/2503. 05132. [70] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024."
        },
        {
            "title": "A More Implementation Details",
            "content": "A.1 Prompts We provide the prompt used to synthesize Self-Critic-CoT [53] via rejection sampling. When analyzing any query or task, please follow the structure below: 1. Draft Response: Generate an initial response. 2. Critical Comments: Analyze your draft response by considering: Potential weaknesses or gaps Logical flaws or inconsistencies Missing perspectives or alternatives Areas for improvement Suggestions for better version Steering toward the given answer The critical comments should: Be specific and actionable Reference particular parts of the draft Suggest concrete improvements Consider different angles or approaches Guide towards more comprehensive solution Output Format: Draft Response: Your initial complete response to the instruction. Critical Comments: Your analysis of the draft response, highlighting areas for improvement and suggesting specific enhancements. Final Answer: Put your final answer within boxed{}. A.2 Rejection Sampling To construct the distillation datasets from Qwen2.5-VL-7B and Qwen2.5-VL-32B, we employ rejection sampling technique [46]. We try at most 24 times to obtain one correct response. We observe that this approach results in approximately the same amount of data (both 52K samples) for Qwen2.5-VL-7B and Qwen2.5-VL-32B. A.3 Benchmarks We provide some details about the benchmarks we use to evaluate the models reasoning ability. MathVision [48] is challenging benchmark containing 3040 mathematical problems with visual contexts from real-world math competitions across 12 grades. It covers 16 subjects over 5 difficulty levels, including specialized topics like Analytic Geometry, Combinatorial Geometry, and Topology. 15 MathVision [48] is challenging benchmark containing 3040 mathematical problems with visual contexts from real-world math competitions across 12 grades. It covers 16 subjects over 5 difficulty levels, including specialized topics like Analytic Geometry, Combinatorial Geometry, and Topology. MathVista [31] is comprehensive benchmark for evaluating mathematical reasoning in visual contexts. It contains 1000 questions featuring diverse problem types including geometry, charts, and tables. MathVerse [64] is an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. The test set contains 3940 multi-subject math problems with diagrams from publicly available sources, focusing on Plane Geometry and Solid Geometry. We-Math [37] meticulously collect and categorize 1740 visual math problems in the test set, spanning 67 hierarchical knowledge concepts and 5 layers of knowledge granularity. For all benchmarks, we prompt the models to place their final answers within designated box format. We then employ Qwen2.5-32B-Instruct [56] to evaluate answer correctness by comparing the extracted responses with ground truth answers, which often contain complex mathematical expressions. Note that our reported benchmark scores may differ from those in the original papers due to variations in evaluation protocols. A.4 Training Details We conduct post-training using Qwen2.5-VL-3B and Qwen2.5-VL-7B [2]. In particular, we employ ms-swift [68] framework for the cold start training (SFT), using 3 epochs with learning rate of 1 105. Besides, we adopt the EasyR1 [59] framework for GRPO training. We mainly follow the default hyperparameters in EasyR1. We set the training episodes to 2 with learning rate of 1 106. During GRPO training, we adopt accuracy reward, which assigns rewards solely based on the correctness of the generated response: correct final answer receives reward of +1, while an incorrect one receives reward of 0."
        },
        {
            "title": "B Compute Resources",
            "content": "We conduct our experiments using NVIDIA H800-80G and A800-40G GPUs. The experimental time using 8 A800 for training Qwen2.5-VL-7B using GRPO is around 12 hours."
        },
        {
            "title": "C Limitations",
            "content": "While our study provides comprehensive insights into the interplay between supervised fine-tuning (SFT) and reinforcement learning (RL) for multimodal reasoning, several limitations remain. Firstly, we primarily evaluate models at the 3B and 7B parameter scales due to limited computation resources. The generalizability of our findings to larger models (e.g., 30B, 70B, or beyond) remains to be validated. Our approach adopts GRPO [40] as the RL optimization algorithm, which is currectly most popular. We do not systematically compare alternative RL methods, which we leave as future work. In addition, the identification of aha moment patterns is based on keyword matching and heuristic rules, which may introduce subjectivity or incompleteness."
        },
        {
            "title": "D Case Studies",
            "content": "We provide several detailed case studies of our observation and curated CoT datasets as follows. D.1 Aha Moment in MLLM In the first case, the correct answer should be 130, but the model fails to arrive at this solution. The associated reflection pattern exhibits hallucinations and proves ineffective. In contrast, in the second case, the model successfully answers the question, and the reflection pattern contributes meaningfully to the reasoning process. 16 The First Case. Question: In the given parallelogram ABCD, if the sum of angle and angle measures 100, what is the measure of angle A? Models response: To solve this problem, we need to use the properties of parallelogram. The properties are: 1. Opposite sides are parallel. 2. Opposite angles are equal. 3. Consecutive angles are supplementary (they add up to 180). Given: The sum of angle and angle is 100. Since AD BC and AB CD, angles and are consecutive angles. Therefore, they are supplementary, meaning they add up to 180. So, we can write: angle + angle = 180 Given: angle + angle = 100 This implies that the given sum (100) is incorrect for the sum of angles and in parallelogram, which should be 180. However, lets proceed with the given information. Since angle and angle are consecutive angles, we can find the measure of angle by using the fact that opposite angles are equal in parallelogram. Therefore, angle is equal to angle C. Now, assume: angle = x, angle = 180 Given: + (180 x) = 100 180 = 100 This is incorrect, so we need to re-evaluate the problem. The correct interpretation should be that the sum of angle and angle is 180, not 100. Therefore, the measure of angle is: 140 The Second Case. Question: In the given diagram, quadrilateral ABCD is inscribed in circle O. The diameter BE is part of circle and AE is connected to it. If the measure of angle BCD is twice the measure of angle BAD, then what is the degree measure of angle DAE? Models response: To solve the problem, we need to use some properties of cyclic quadrilaterals and the relationships between angles in circle. 1. Identify the given information: Quadrilateral ABCD is inscribed in circle O. Diameter BE of circle means that BE is straight line passing through the center of the circle. AE is connected to and E. BCD = 2 BAD. 2. Use the property of cyclic quadrilaterals: In cyclic quadrilateral, the sum of the opposite angles is 180. Therefore, BAD + BCD = 180. 3. Express BCD in terms of BAD: Given BCD = 2 BAD, substitute this into the equation: BAD + 2 BAD = 180, 3 BAD = 180, BAD = 60, BCD = 2 60 = 120. 4. Determine ADE: Since BE is diameter, ADE is right angle because the angle subtended by diameter in semicircle is 90: ADE = 90. 17 5. Determine DAE: DAE is part of triangle ADE, where ADE = 90. The sum of the angles in triangle ADE is 180: DAE + ADE + AED = 180, DAE + 90 + AED = 180, DAE + AED = 90. 6. Determine AED: AED is the same as BCD because they subtend the same arc CD: 7. Solve for DAE: Substitute AED = 120 into the equation: AED = 120. DAE + 120 = 90, DAE = 30. Since DAE cannot be negative, we must re-evaluate the problem. The correct interpretation is that AED = 60 and not 120, because AED does not subtend the same arc as BCD. Instead, the triangle should be analyzed differently, considering interior angles. Thus, the correct value is: 30 . D.2 Distilled Data from Different Models The main difference between the Qwen2.5-VL-7B distilled CoT and the Qwen2.5-VL-32B distilled CoT lies in their level of detail and organization. The Qwen2.5-VL-32B distilled CoT is more comprehensive and better structured, offering richer, more informative reasoning steps compared to the more concise and less detailed Qwen2.5-VL-7B distilled CoT. The Distilled CoT Data from Qwen2.5-VL-7B and Qwen2.5-VL-32B. Question: In the accompanying diagram, ABC is an equilateral triangle with perimeter of 30. What is the length of altitude of this triangle? Choices: 5 3, 10 2, 10 2, 5 3 Qwen2.5-VL-7Bs response: To find the length of the altitude of an equilateral triangle with perimeter of 30, we can follow these steps: 1. Calculate the side length of the equilateral triangle: The perimeter of an equilateral triangle is the sum of the lengths of its three sides. Given the perimeter is 30, we can find the length of one side by dividing the perimeter by 3: 2. Use the properties of an equilateral triangle: Side length = 30 3 = In an equilateral triangle, the altitude also bisects the base, creating two 30-60-90 right triangles. In 30-60-90 triangle, the ratio of the sides opposite the 30, 60, and 90 angles is 1 : 3 : 2. 3. Determine the length of the altitude h: The altitude is the side opposite the 60 angle in the right triangle. The side opposite the 30 angle (which is half the base) is 5 (since the base is 10 and it is bisected). Using the ratio 1 : 3 : 2, the altitude is: = 5 3 Therefore, the length of the altitude is: Qwen2.5-VL-32Bs response: Step-by-Step Solution: 1. Understand the Problem: 5 3 We are given an equilateral triangle ABC with perimeter of 30. We need to find the length of the altitude of this triangle. 18 2. Find the Side Length of the Triangle: An equilateral triangle has all three sides of equal length. Let the side length of the triangle be s. The perimeter of the triangle is given by: Perimeter = + + = 3s We are given that the perimeter is 30: 3s = 30 Solving for s: = 3 = 10 Therefore, the side length of the triangle is = 10. 3. Use the Formula for the Altitude of an Equilateral Triangle: The altitude of an equilateral triangle with side length is given by the formula: Substitute = 10 into the formula: Simplify the expression: 4. Verify the Result: = 3 2 h = 3 2 10 = 5 3 The formula for the altitude of an equilateral triangle is derived from the Pythagorean theorem. In an equilateral triangle, the altitude bisects the base, creating two 30-60-90 right triangles. In 30-60-90 triangle, the sides are in the ratio 1 : 3 : 2. The altitude corresponds to the side opposite the 60-degree angle, which is This confirms that the formula = 3 2 is correct. 5. Select the Correct Answer: The length of the altitude is 5 3. 3 2 times the hypotenuse (the side of the equilateral triangle). 5"
        }
    ],
    "affiliations": [
        "Lehigh University",
        "School of Computer Science, Shanghai Jiao Tong University",
        "Shanghai Innovation Institute",
        "Zhongguancun Academy"
    ]
}