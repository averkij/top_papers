{
    "paper_title": "Causal Motion Diffusion Models for Autoregressive Motion Generation",
    "authors": [
        "Qing Yu",
        "Akihisa Watanabe",
        "Kent Fujiwara"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency."
        },
        {
            "title": "Start",
            "content": "Qing Yu1 Akihisa Watanabe2 Kent Fujiwara1 1LY Corporation 2Waseda University {yu.qing, kent.fujiwara}@lycorp.co.jp, akihisa@ruri.waseda.jp 6 2 0 2 6 ] . [ 1 4 9 5 2 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), unified framework for autoregressive motion generation based on causal diffusion transformer that operates in semantically aligned latent space. CMDM builds upon Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and longhorizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency. 1. Introduction Synthesizing realistic human motion conditioned on natural language remains fundamental problem in computer vision and graphics. successful text-to-motion generation model should not only synthesize spatially accurate body movements but also maintain temporal coherence across long sequences. Recent progress in motion diffusion models [5, 6, 42, 52] has led to significant improvements in motion quality and diversity, benefiting from the strong generative capacity of diffusion-based frameworks [7, 14]. However, most existing diffusion models rely on bidirectional Figure 1. Overview of the existing methods and the proposed method. Existing diffusion-based methods (left) perform fullsequence denoising using the same noise level across all frames. In contrast, our proposed CMDM (right) introduces causal diffusion forcing mechanism that operates on semantic causal latent features with frame-wise noise levels. denoising over the entire sequence, which inherently breaks temporal causality and prevents online generation. Autoregressive models [24, 45, 50, 54] offer an alternative by predicting future frames from past ones, ensuring causal consistency and supporting online motion generation. Yet, their sequential dependency often leads to error accumulation, making long-horizon synthesis unstable and inefficient. The key challenge lies in achieving temporally ordered, high-quality motion generation with both the fi1 delity of diffusion models and the causal structure of autoregressive transformers. To address these challenges, we propose Causal Motion Diffusion Models (CMDM), unified framework that integrates causal diffusion and autoregressive modeling within semantically aligned latent space as shown in Fig. 1. CMDM is built upon our Motion-LanguageModels-Aligned Causal Variational Autoencoder (MACVAE), which encodes human motion into temporally causal latent representations guided by motion-language pretraining [31, 35, 48]. This foundation enables CMDM to operate in compact and semantically meaningful latent space, preserving alignment between linguistic semantics and motion dynamics. On top of MAC-VAE, we design Causal Diffusion Transformer (Causal-DiT) that performs diffusion denoising in an autoregressive manner. Unlike conventional diffusion models that jointly process all frames, Causal-DiT applies causal self-attention to ensure each frame depends only on preceding frames. This design enforces strict temporal ordering, allowing streaming motion generation. To accelerate inference, we introduce frame-wise sampling schedule with causal uncertainty, which allows each frame to be progressively refined from partially denoised preceding frames rather than requiring fully autoregressive denoising step. Inspired by Diffusion Forcing [4], originally designed for next-token prediction, during training we perturb each frame with independent noise levels while maintaining causal dependencies across time, enabling the model to learn temporally consistent denoising transitions. During sampling, the model iteratively predicts the subsequent frames based on previously denoised frames with varying noise levels, gradually reducing uncertainty in causal order. This hierarchical denoising process significantly reduces inference steps, achieving efficient and temporally coherent motion generation. CMDM unifies the stability and realism of diffusion models with the causality and efficiency of autoregressive architectures. The framework enables high-fidelity text-tomotion generation, fast inference, and long-horizon synthesis within unified causal formulation. Extensive evaluations on HumanML3D and SnapMoGen demonstrate that CMDM consistently outperforms state-of-the-art diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while reducing inference latency by an order of magnitude. Our main contributions are summarized as follows: Causal motion diffusion framework. We propose CMDM, the first motion diffusion framework that unifies causal autoregression and diffusion denoising within motionlanguagealigned latent space. Semantically aligned causal latent modeling. We introduce MAC-VAE, motionlanguagealigned causal VAE that learns temporally causal and semantically meaningful latent representations for text-to-motion generation. Frame-wise sampling with causal uncertainty. We design novel frame-wise sampling schedule that models causal uncertainty, allowing each frame to be predicted from partially denoised preceding frames for efficient, low-latency, and temporally consistent motion synthesis. Comprehensive empirical validation. CMDM achieves state-of-the-art performance on HumanML3D and SnapMoGen, surpassing existing diffusion and autoregressive methods on text-to-motion generation and long-horizon motion generation. 2. Related Works 2.1. Motion-Language Alignment Recent advances in visionlanguage models [27, 35] have shown that large-scale training can robustly align text and visual semantics. This has led to surge of interest in exploring motionlanguage alignment to enable practical control of motion using natural language. MotionCLIP [41] maps single frame to CLIP space but fails to capture temporal dynamics. Subsequent methods, including TMR [31] and MotionPatches [48], learn joint motiontext embeddings via contrastive or generative objectives, while PartTMR [49] introduces body-part-level features for finer alignment. However, most motionlanguage models focus on retrieval tasks. Methods such as ReMoGPT [49] and ReMoMask [19] extend to text-to-motion generation but rely on retrieval-augmented generation rather than integrating motionlanguage alignment directly into the generation. 2.2. Diffusion-based Motion Generation Text-conditioned motion generation has been explored through both non-diffusion and diffusion [7, 13, 14] paradigms. Early works used CNNor RNN-based architectures [46, 55] and action-conditioned frameworks [9, 30] to synthesize motion from predefined semantics. More recently, diffusion-based methods [7, 36] have set new benchmarks for motion realism and diversity [5, 42, 52]. MDM [42] and MotionDiffuse [52] operate directly in motion space, while MLD [5], MotionLCM [6], EnergyMoGen [51] and SALAD [15] perform diffusion in latent space for greater stability and efficiency. However, these diffusion models rely on bidirectional attention over entire sequences, breaking temporal causality and limiting realtime or streaming generation. 2.3. Autoregressive Motion Generation Autoregressive (AR) modeling enforces temporal causality by predicting future frames from past context. Discretetoken methods such as T2M-GPT [50] and MotionGPT [17] treat motion as language, enabling powerful sequence modeling but suffering from exposure bias and cumula2 including Motive errors. VQ-VAE-based approaches, Mask [11], MMM [33], and ParCo [56], quantize motion into discrete tokens and predict them autoregressively. Recent works explore causal paradigms for streaming generation: Dart [54] predicts short future segments from limited two historical frames, while MARDM [24] and MotionStreamer [45] employ masked autoregressive transformers [18] with diffusion heads. However, their reliance on teacher forcing [44] and large diffusion heads causes instability in long-horizon inference and high computational cost, limiting real-time deployment. Our work differs from existing methods in two key aspects: (1) we introduce causal diffusion process within motionlanguagealigned latent space, preserving semantic consistency while enforcing temporal causality; and (2) we design frame-wise sampling schedule that enables highquality, streaming motion generation. 3. Method Our proposed framework, Causal Motion Diffusion Models (CMDM), enables temporally ordered, text-conditioned motion generation by integrating causal latent encoding, causal diffusion forcing, and efficient frame-wise sampling. As illustrated in Fig. 2, CMDM consists of three core components: (1) Motion-Language-Aligned Causal VAE (MAC-VAE) that encodes motion sequences into semantically aligned and temporally causal latent spaces, (2) Causal Diffusion Transformer (Causal-DiT) that performs frame-wise diffusion with causal self-attention to ensure autoregressive temporal dependencies, and (3) Frame-Wise Sampling Scheduler (FSS) that models causal uncertainty by assigning higher noise to future frames and lower noise to past frames, allowing each new frame to be predicted from partially denoised preceding frames. 3.1. Motion-Language-Aligned Causal VAE To obtain temporally structured and semantically consistent latent representation, CMDM employs causal variational autoencoder aligned with motion-language foundation model. Given motion sequence x1:T RT D, where is the number of frames and is the dimension of the joint representation, the proposed MAC-VAE encoder Eϕ and decoder Dψ operate causally such that each frame depends only on its past: zt = Eϕ(xt), ˆxt = Dψ(zt), (1) where zt Rdz denotes the latent representation at timestep t. For motion sequence with frames, we encode it into /4 temporal steps in the latent space, effectively achieving 4 temporal downsampling ratio. This compression balances representation compactness and temporal resolution, reducing redundancy while preserving the underlying motion dynamics. The encoder and decoder are adapted from [45] and are composed of 1D causal convolution and 1D causal ResNet blocks, ensuring temporal causality during both encoding and reconstruction. In this design, each frame only depends on preceding frames, while future frames are excluded from computation, explicitly modeling temporal causality in the latent space. During inference, reconstructed motions can be decoded sequentially in real time, enabling streaming generation without requiring access to future frames. To enhance semantic alignment, motion features are projected through pretrained motionlanguage encoder (PartTMR [49]), which provides part-level semantic supervision. The MAC-VAE objective combines three terms: the standard VAE reconstruction loss, the KullbackLeibler divergence, and newly introduced motionlanguage alignment loss: LMAC-VAE = Lrec + βDKL (cid:0)qϕ(zx) p(z)(cid:1) + λLalign. (2) Motion Alignment Loss. To enforce fine-grained semantic alignment between motion and text, we introduce motion alignment loss Lalign that measures both point-topoint feature similarity and relative structural consistency between motion embeddings and motion-language features extracted from the motion encoder of the pretrained Part-TMR model [49]. Specifically, we employ two complementary objectives, following the design in VAVAE [47]: (1) marginal cosine similarity loss that minimizes local feature gaps, and (2) marginal distance matrix similarity loss that preserves the relational geometry of feature spaces, as defined below: Lalign = Lmcos + Lmdms. (3) Given aligned feature maps and from the latent space and the motionlanguage encoder, respectively, we project to match the feature dimensionality of via linear transformation: = Z, (4) where Rdf dz is learnable projection matrix. The marginal cosine similarity loss Lmcos minimizes the similarity gap between corresponding features Lmcos = 1 (cid:88) i,j (cid:32) ReLU 1 m1 ij and fij: (cid:33) ij fij ij fij , (5) where is the total number of temporal feature elements, and m1 is similarity margin that encourages stronger alignment for less similar pairs. This loss focuses learning on semantically misaligned regions, improving featurelevel consistency. Complementary to Lmcos, the marginal distance matrix similarity loss Lmdms enforces the alignment of internal 3 Figure 2. Overview of the proposed CMDM framework. CMDM consists of three key components: (a) MAC-VAE, which encodes motion sequences into motionlanguagealigned and temporally causal latent features using causal encoderdecoder structure supervised by motion-language model alignment; (b) Causal-DiT, which performs diffusion denoising with causal self-attention and cross-attention to text embeddings, ensuring temporally ordered and semantically consistent frame refinement; and (c) Causal Diffusion Forcing, which applies independent frame-level noise during training and causal uncertainty schedule during inference, where the redness intensity represents the noise level. This design enables CMDM to achieve temporally consistent, semantically aligned, and efficient text-to-motion generation suitable for streaming and long-horizon synthesis. structural relationships between motion and text embeddings by matching their pairwise distance matrices. Formally, we compute: Lmdms = 1 2 (cid:88) i,j ReLU (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) zi zj zizj fi fj fifj (cid:12) (cid:12) (cid:12) (cid:12) (cid:19) m2 , (6) where m2 is distance margin that relaxes the alignment constraint for similar pairs. This objective promotes structural consistency between latent and text spaces, ensuring that the relative geometry of motion embeddings matches that of the aligned foundation features. 3.2. Causal Diffusion Forcing We extend Diffusion Forcing [4], originally proposed for next-token prediction, to the motion domain to model autoregressive temporal dynamics in the latent space. Unlike conventional diffusion models that jointly denoise all frames, our CMDM introduces frame-level noise with independent diffusion timesteps for each motion frame in the latent sequence, enforcing causal dependencies between past and future frames. In standard full-sequence diffusion, the same level of noise [0, K], where is the total number of diffusion steps, is applied to the entire sequence as: zk = αk zk + 1 αk ϵk, ϵk (0, I), (7) where denotes the text embedding. In causal diffusion forcing, each frame is assigned an independent noise level kt [0, K], and the noisy latent representation is defined as: = (cid:112)αkt zkt zkt + (cid:112)1 αkt ϵkt , ϵkt (0, I). (9) The diffusion transformer ϵθ predicts the noise residual using causal self-attention, which restricts each frame to attend only to its past representations: LDF = kt,ϵkt (cid:104) ϵkt ϵθ(zt, kt, c)2 2 (cid:105) . (10) This causal training setup ensures that the denoising process evolves forward in time, with each prediction depending solely on the available history, effectively bridging diffusion and autoregression. By replacing the globally synchronized noise schedule with frame-specific perturbations, CMDM achieves several advantages. First, the model learns to operate under diverse noise conditions at each frame, which improves temporal robustness and generalization to variable-length sequences. Second, the causal attention mask explicitly enforces temporal order within the transformer backbone, preventing information leakage from future frames and enabling real-time or streaming generation. Finally, the perframe stochasticity of diffusion forcing acts as natural regularizer, encouraging smooth temporal transitions while preserving motion diversity. and the denoising model ϵθ is trained to recover the original sequence simultaneously: 3.3. Causal Diffusion Transformer = Ek,ϵk (cid:104) ϵk ϵθ(zk, k, c)2 2 (cid:105) , (8) To model the temporal dependencies of diffusion forcing, CMDM employs causal diffusion transformer (Causal4 DiT) that performs diffusion-based denoising under strict causal constraints. Unlike conventional transformers with bidirectional attention, Causal-DiT uses causal masking so each frame accesses only its past and current context, ensuring sequential generation consistent with autoregressive reasoning while maintaining diffusion fidelity. Each transformer block integrates three key mechanisms: (1) causal self-attention, which employs lowertriangular attention mask to prevent each frame from attending to future frames. This constraint preserves the causal order required for autoregressive modeling and ensures that the model predicts future motion dynamics based solely on previously observed information. (2) cross-attention, which establishes the link between motion and language by conditioning frame-level motion latents on word-level text embeddings extracted from DistilBERT [37]. Through this mechanism, semantic cues from natural language guide the temporal evolution of motion features, allowing the model to synthesize actions that remain coherent with the textual descriptions across the sequence. (3) adaptive layer normalization (AdaLN) [29] combined with rotary positional encoding (ROPE) [39], where AdaLN embeds frame-wise diffusion timestep information, ensuring that temporal noise levels kt are seamlessly integrated into the denoising process, while ROPE stabilizes long-horizon denoising with relative positional encoding. During training, each frame is diffused with an independent noise level kt, and the model learns to denoise them as: ϵθ(zt, kt, c) = CausalDiT(zt, kt, c), (11) where zt denotes the partially noised causal latent sequence and is the text embedding. 3.4. Inference and Streaming Generation During inference, CMDM generates motion autoregressively by progressively denoising each frame in causal manner. Given text condition and an initial noise sequence {zK t=1 (0, I), the model predicts each frame conditioned on previously denoised latents: }T zkt1 = Gθ({z <t, zkt }, kt, c), ˆxt = Dψ(z0 t), (12) where Gθ denotes the causal diffusion generator. This formulation ensures strictly causal synthesis and, with keyvalue caching for autoregressive rollout, enables real-time generation. However, this scheme is prone to accumulating single-step errors, as it treats the predicted z0 as ground truth observation, practice more broadly referred to as exposure bias [26, 38]. Frame-wise Sampling Schedule (FSS). To accelerate inference and mitigate exposure bias, CMDM adopts framewise sampling schedule with causal uncertainty, assigning 5 lower noise to past frames and higher noise to future ones. At each step, the model refines the next frame using partially denoised histories. For example, causal uncertainty schedule with uncertainty scale can be defined as: Km,t ="
        },
        {
            "title": "K\nK",
            "content": "K KL K2L KL . . . ... 0 0"
        },
        {
            "title": "K\nK\nK",
            "content": "KL ... 0 , (13) where Km,t indicates the noise applied to frame at iteration and the uncertainty scale indicates that the denoising of the next frame begins at step KL of the current frame. Intermediate steps between K, KL, K2L, and so on are omitted for clarity. Each partially denoised frame zt is reused as context for subsequent predictions, enabling continuous, low-latency generation with high temporal coherence and smooth transitions, while greatly reducing inference cost compared to full autoregressive diffusion. 4. Experiments 4.1. Experimental Setup Datasets. To evaluate CMDM, we conduct experiments on two benchmarks: HumanML3D [10] and SnapMoGen [12]. HumanML3D contains 14,616 motion clips from AMASS [23] paired with 44,970 short textual descriptions of common actions (e.g., walking, jumping, sitting). SnapMoGen includes 20,450 motion capture clips with 122K expressive captions (average 48 words) covering about 43.7 hours of data. Unlike HumanML3D, SnapMoGen features temporally continuous, long-horizon activities (e.g., sports and performances), allowing evaluation of smooth, consistent motion generation. For fair comparison, we follow the standard 3D motion representation of each dataset: 263 dimensions for HumanML3D and 296 for SnapMoGen, including joint velocities, positions, and rotations. Evaluation Metrics. Following prior work [10, 12, 17], we evaluate CMDM using several standard metrics. (1) Motion quality is measured by Frechet Inception Distance (FID), which assesses the realism of generated motions relative to ground truth. (2) Multi-modality quantifies the diversity of motions generated from identical text prompts. (3) Textmotion alignment is evaluated by R-Precision (R@1, R@2, R@3) and Multi-Modal Distance (MM Dist) using pretrained textmotion retrieval model. We also report the CLIP-Score [24], which measures the cosine similarity between generated motion features and their corresponding captions in the CLIP embedding space. Methods GT T2M-GPT [50] MMM [33] MoMask [11] MDM-50Steps [42] MLD V2 [5] MotionLCM V2 [6] StableMoFusion [16] EnergyMoGen [51] SALAD [15] MARDM [24] MotionStreamer [45] VQ Diffusion Autoregressive Transformer Framework Top R-Precision Top 2 Top 3 FID MM-Dist MModality CLIP-score 0.511.003 0.703. 0.797.002 0.002.000 2.974.008 0.492.003 0.515.002 0.521.002 0.455.006 0.542.002 0.548.002 0.553.003 0.523.003 0.581. 0.517.003 0.496.003 0.679.002 0.708.002 0.713.002 0.645.007 0.735.002 0.743.002 0.748.002 0.715.002 0.769.003 0.708.003 0.695.002 0.775.002 0.804.002 0.807.002 0.749.002 0.827.002 0.835.002 0.841.002 0.815.002 0.857. 0.805.003 0.793.002 0.141.005 0.089.005 0.045.002 0.489.025 0.078.004 0.092.003 0.098.003 0.188.006 0.076.002 0.116.006 0.201.005 0.079.003 0.068.003 3.121.009 2.926.007 2.958. 3.330.025 2.808.006 2.760.008 2.715.006 2.915.007 2.649.009 2.968.010 3.041.009 2.647.011 2.620.010 1.831.048 1.226.035 1.241.040 2.290.070 1.676.060 1.800.047 1.774.051 2.205.041 1.751.062 1.923.105 1.463. 1.951.086 1.785.074 0.639.001 0.607.005 0.635.003 0.637.003 0.481.001 0.645.003 0.646.003 0.651.001 0.671.001 0.642.001 0.610.002 0.675.001 0.685. CMDM w/ AR CMDM w/ FSS Autoregressive Diffusion 0.576.003 0.588.004 0.768.004 0.778.002 0.853.002 0.860.003 Table 1. Results of text-to-motion generation on HumanML3D. The average is reported over 10 runs with 95% confidence intervals. Methods marked with were originally implemented with different motion representations and have been re-trained using our codebase to ensure fair comparison. Bold indicates the best result, while underline denotes the second-best result. Implementation Details. MAC-VAE comprises seven causal convolution layers and two causal ResNet blocks with left padding in both the encoder and decoder. The latent feature dimension is set to 64. We modify and retrain Part-TMR [49] to extract frame-level semantic motionlanguage features for supervising MAC-VAE. The loss weight λ for semantic alignments is automatically adjusted according to the gradient norm at the final layer of the encoder to maintain balance with other losses. The CausalDiT is implemented as lightweight Transformer [43] with eight layers, four attention heads, and hidden dimension of 512. Flow Matching [1, 2, 21, 22] is adopted as the ODE sampler for causal diffusion forcing. For FSS, we set 50 denoising step with = 50 for each frame and start to denoise the next frame at 2 (i.e., = 2) during inference. 4.2. Quantitative Results Results on HumanML3D. We compare CMDM with (1) VQstate-of-the-art models across three paradigms: based [11, 33, 50]; (2) Diffusion-based [5, 6, 15, 16, 42, 51]; and (3) Autoregressive-based [24, 45]. As shown in Table 1, CMDM consistently achieves superior or comparable results across all metrics. CMDM with frame-wise sampling (CMDM w/ FSS) attains the best overall performance, achieving an R-Precision of 0.588/0.778/0.860 (Top-1/2/3), the second lowest FID (0.068), and the highest CLIP-Score (0.685). These results indicate high motion fidelity and strong textmotion alignment. Compared to standard autoregressive sampling (CMDM w/ AR), FSS further improves temporal stability and smoothness while reducing inference latency. This improvement arises from the causal uncertainty mechanism, where each subsequent frame is generated from partially denoised preceding frames. This design allows the model to reduce the accumulated error of autoregressive and adaptively refine local temporal transitions while maintaining global coherence, leading to smoother motion dynamics and improved semantic alignment for stable generation. Results on SnapMoGen. We further evaluate the proposed CMDM on the motion clips of SnapMoGen, which contains expressive motion sequences paired with rich textual descriptions. As shown in Table 2, CMDM achieves state-of-the-art performance across all evaluation metrics, demonstrating its strong generalization to complex motions. CMDM with the frame-wise sampling schedule (CMDM w/ FSS) achieves the best overall results, surpassing all previous VQ-, diffusion-, and autoregressive-based methods. It also achieves the lowest FID score and high CLIP-Score, indicating superior motion realism and semantic alignment. Long-Horizon Motion Generation To evaluate CMDM on long-horizon motion generation, we compare it with the motion composition method FlowMDM [3] and the autoregressive model MARDM [24]. Following the protocol of FlowMDM, we synthesize 64 long-horizon sequences on HumanML3D by composing 32 captionduration pairs per sequence, evaluating 32 subsequences and 31 transitions for local quality and temporal continuity. The groundtruth metrics are computed using randomly sampled motion clips from HumanML3D. For SnapMoGen, which provides ground-truth long sequences, we select 128 samples with over five continuous motions and use the same captions for generation. We further employ Peak Jerk (PJ) and Area Under the Jerk (AUJ) [3] to measure transition smoothness. Owing to differences in skeleton scale, the magnitude of the metrics differs from those reported on HumanML3D. The results are shown in Table 3. Although FlowMDM reports lower PJ and AUJ values on SnapMoGen, this is primarily because its generated motions often remain static or 6 Methods Framework Top 1 R-Precision Top 2 Top FID MModality CLIP-score GT T2M-GPT [50] MoMask [11] MoMask++ [12] VQ MDM [42] StableMoFusion [16] MARDM [24] MotionStreamer [45] CMDM w/ AR CMDM w/ FSS Diffusion Autoregressive Transformer Autoregressive Diffusion 0.940.001 0.976.001 0.985.001 0.001.000 0.618.002 0.777.002 0.802. 0.503.002 0.679.002 0.648.002 0.631.002 0.773.002 0.888.002 0.905.002 0.653.002 0.823.002 0.801.002 0.791.002 0.812.002 0.927.002 0.938. 0.727.002 0.888.002 0.856.002 0.836.002 32.629.087 17.404.051 15.061.065 57.783.092 27.801.063 26.348.208 30.023.131 0.824.002 0.831. 0.918.004 0.926.003 0.951.002 0.958.002 15.008.074 14.451.074 9.172.181 8.183.184 7.259.180 13.412.231 9.064.138 9.883.147 7.543. 9.735.186 9.521.196 0.837.000 0.573.001 0.664.001 0.685.001 0.481.001 0.605.001 0.601.001 0.580.001 0.699.001 0.702. Table 2. Results of text-to-motion generation on SnapMoGen. The average is reported over 10 runs with 95% confidence intervals. Methods GT 3 FlowMDM [3] MARDM [24] CMDM GT n FlowMDM [3] MARDM [24] CMDM R-Top3 FID Div MM-Dist FID Div PJ AUJ Subsequence Transition 0.7960.004 0.000. 9.340.08 2.970.01 0.000.00 9.540.15 0.040.00 0.070. 0.6850.004 0.7410.004 0.7820.003 0.9970.006 0.4850.009 0.6440.007 0.8520.003 0.290.01 0.240.01 0.120.04 0.000.00 9.580.12 9.290.12 9.440.13 19.740.05 3.610.01 3.380.01 3.040.01 14.830.01 1.380.05 2.720.09 1.660.06 0.000. 8.790.09 8.280.11 8.720.10 19.210.08 69.100.70 40.800.17 32.490.16 19.540.07 19.680.06 19.800.07 31.850.08 28.930.10 24.940.05 62.910.63 54.100.46 38.730.65 19.040.08 19.130.07 19.500. 0.060.00 0.070.00 0.040.00 1.110.01 0.910.03 8.120.23 2.540.06 0.510.01 0.570.01 0.420.01 45.160.33 23.011.68 130.331.08 70.350.68 Table 3. Results of long-horizon motion generation on HumanML3D and SnapMoGen. The motion quality of each subsequence and the smoothness of each transition are evaluated. frozen as can be seen from the ensuing qualitative analysis in Fig. 3. In contrast, CMDM produces temporally consistent, smoothly transitioning, and realistic long-horizon motions at real-time speed, demonstrating its effectiveness for streaming and continuous text-to-motion generation. 4.3. Qualitative Results Fig. 3 shows qualitative comparisons with FlowMDM [3] and MARDM [24] on long-horizon motion generation. Given sequence of captions, CMDM generates continuous and seamless motions with accurate semantics and smooth transitions across segments. In contrast, FlowMDM and MARDM often produce incorrect actions, e.g., no jumping or unnatural transitions, e.g., skeleton flips. These improvements stem from causal latent encoding and framewise sampling schedule of CMDM, which condition each frame on partially denoised preceding frames to ensure stability and temporal consistency. Please refer to the supplementary videos for the complete motion sequences and more visualization results. 4.4. Analysis Computational Efficiency Compared to other existing autoregressive methods, CMDM achieves substantial improvement in inference efficiency through its advanced architecture and frame-wise sampling schedule, which significantly reduces generation time while maintaining superior motion realism. We evaluate the computational efficiency of our framework by generating 6-second motion sequences on an NVIDIA A100 GPU over 100 repetitions. As result, MARDM operates with 310M parameters at 20 fps, and MotionStreamer with 318M parameters at 11 fps. In contrast, our proposed CMDM contains only 114M parameters (including both the MAC-VAE and Causal-DiT) and achieves 28 fps using the standard autoregressive process, and up to 125 fps with the proposed frame-wise sampling schedule. These highlight the remarkable efficiency of our causal diffusion framework and the effectiveness of the proposed sampling strategy for real-time motion generation. Ablation Studies To investigate the impact of each component in CMDM, we conduct ablation studies on causal latent modeling, causal diffusion forcing, and the framewise sampling schedule (FSS) on HumanML3D. As shown in Table 4, replacing the MAC-VAE with standard VAE significantly degrades motion quality, text fidelity, and transition smoothness, confirming the importance of causal latent modeling and semantic supervision. Removing motionlanguage alignment (C-VAE w/o MA) produces motion quality comparable to MAC-VAE but introduces semantic inconsistencies, underscoring the importance of semantic features for maintaining fidelity and coherence. Substituting causal diffusion with full-sequence diffusion increases transition FID and AUJ, (w/ Full-Seq. Diff.) 7 Figure 3. Qualitative results of long-horizon motion generation. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences. Methods CMDM T2M Generation R-Top1 FID Motion Transition AUJ FID 0.588.004 0.068.003 1.660.06 0.420. VAE w/o MA C-VAE w/o MA w/ Full-Seq. Diff. w/o adaLN w/o ROPE w/o adaLN + ROPE w/o FSS FSS = 50, = 1 FSS = 50, = 5 FSS = 50, = 10 FSS = 20, = 1 FSS = 20, = 5 FSS = 20, = 10 0.561.005 0.575.004 0.591.004 0.583.002 0.581.002 0.580.002 0.576.003 0.587.005 0.583.005 0.579.004 0.585.004 0.576.005 0.572. 0.107.006 0.070.004 0.071.003 0.076.004 0.087.004 0.091.005 0.079.003 0.070.003 0.077.005 0.080.005 0.074.004 0.088.006 0.103.005 2.040.06 1.720.06 1.960.07 1.780.06 1.850.08 2.330.07 1.820.05 1.710.05 1.640.06 1.750.06 1.700.05 1.770.05 1.750. 0.520.01 0.440.01 0.720.01 0.470.01 0.510.01 0.540.01 0.480.01 0.430.01 0.380.01 0.450.01 0.430.01 0.420.01 0.430.01 Table 4. Ablation studies of CMDM. verifying that per-frame causal diffusion enforces stronger temporal stability. Eliminating AdaLN (w/o AdaLN) or ROPE (w/o ROPE) results in higher FID and weaker longhorizon coherence, while removing both further amplifies these degradations. Finally, the FSS variants show that smaller uncertainty scales (L=5) achieve smoother transitions and lower AUJ, whereas excessively large or smaller degrade stability. These results show that all the components jointly contribute to the high performance of CMDM. 5. Limitations Although CMDM achieves state-of-the-art performance in text-conditioned and long-horizon motion generation, sevthe causal latent encoderal limitations remain. First, ing relies on motionlanguage alignment quality from pretrained motion-language models such as Part-TMR, which may limit performance when processing highly abstract or ambiguous text descriptions. Second, while the framewise sampling schedule substantially improves inference efficiency, it may still accumulate minor temporal artifacts when generating extremely long sequences, e.g., over several minutes. Incorporating motion-aware feedback or adaptive re-anchoring mechanisms could further improve long-horizon stability. Finally, CMDM focuses primarily on single-person motion and has not yet been extended to interactive or multi-character scenarios [8, 20, 28, 40], which will be an interesting direction for future work. 6. Conclusion In this paper, we presented CMDM, unified framework that combines the realism and stability of diffusion models with the temporal causality and efficiency of autoregressive generation. CMDM introduces MAC-VAE for semantically grounded causal latent encoding, Causal-DiT for temporally ordered diffusion denoising, and FSS that enables real-time streaming generation. Extensive experiments on HumanML3D and SnapMoGen demonstrate that CMDM achieves superior motion fidelity, semantic alignment, and efficiency compared to existing diffusion and autoregressive models. We believe CMDM provides promising step toward scalable, real-time, and semantically coherent motion generation."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. 6 [2] Michael Albergo, Nicholas Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 6 [3] German Barquero, Sergio Escalera, and Cristina Palmero. Seamless human motion composition with blended positional encodings. In CVPR, 2024. 6, 7, 14 [4] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In NeurIPS, 2024. 2, 4 [5] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In CVPR, 2023. 1, 2, 6, 13 [6] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In ECCV, 2024. 1, 2, 6, [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 1, 2 [8] Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, Jiangning Zhang, Yabiao Wang, Chengjie Wang, and Lizhuang Ma. Freemotion: unified framework for numberfree text-to-motion synthesis. In ECCV, 2024. 8 [9] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In ACM MM, 2020. 2 [10] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In CVPR, 2022. 5 [11] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modIn CVPR, 2024. 3, 6, 7, 13, eling of 3d human motions. [12] Chuan Guo, Inwoo Hwang, Jian Wang, and Bing Zhou. Snapmogen: Human motion generation from expressive texts. arXiv preprint arXiv:2507.09122, 2025. 5, 7 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 2 [15] Seokhyeon Hong, Chaelin Kim, Serin Yoon, Junghyun Nam, Sihun Cha, and Junyong Noh. Salad: Skeleton-aware latent diffusion for text-driven motion generation and editing. In CVPR, 2025. 2, 6, 13 [16] Yiheng Huang, Hui Yang, Chuanchen Luo, Yuxi Wang, Shibiao Xu, Zhaoxiang Zhang, Man Zhang, and Junran Peng. Stablemofusion: Towards robust and efficient diffusion-based motion generation framework. In ACMMM, 2024. 6, 7, 14 [17] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. In NeurIPS, 2023. 2, [18] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. 3 [19] Zhengdao Li, Siheng Wang, Zeyu Zhang, and Hao Tang. Remomask: Retrieval-augmented masked motion generation. arXiv preprint arXiv:2508.02605, 2025. 2 [20] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generation under complex interactions. IJCV, 2024. 8 [21] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. 6 [22] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. 6 [23] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. Amass: Archive of motion capture as surface shapes. In ICCV, 2019. [24] Zichong Meng, Yiming Xie, Xiaogang Peng, Zeyu Han, and Huaizu Jiang. Rethinking diffusion for text-driven human motion generation. In CVPR, 2024. 1, 3, 5, 6, 7, 11, 12, 13, 14 [25] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentencet5: Scalable sentence encoders from pre-trained text-to-text models. In ACL, 2022. 14 [26] Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Onal Ertugrul. Elucidating the exposure bias in diffusion models. In ICLR, 2024. 5 [27] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2 [28] Sakuya Ota, Qing Yu, Kent Fujiwara, Satoshi Ikehata, and Ikuro Sato. Pino: Person-interaction noise optimization for long-duration and customizable motion generation of arbitrary-sized groups. In ICCV, 2025. 8 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, 2023. 5, 11 [30] Mathis Petrovich, Michael Black, and Gul Varol. Actionconditioned 3d human motion synthesis with transformer vae. In ICCV, 2021. 2 [31] Mathis Petrovich, Michael Black, and Gul Varol. Tmr: Text-to-motion retrieval using contrastive 3d human motion synthesis. In ICCV, 2023. 2, 13, 14 [32] Mathis Petrovich, Or Litany, Umar Iqbal, Michael Black, Gul Varol, Xue Bin Peng, and Davis Rempe. Multi-track timeline control for text-driven 3d human motion generation. In CVPR-W, 2024. 12, 13 [33] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In CVPR, 2024. 3, 6, 13 [51] Jianrong Zhang, Hehe Fan, and Yi Yang. Energymogen: Compositional human motion generation with energy-based diffusion model in latent space. In CVPR, 2025. 2, 6, 12, 13 [52] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 1, 2 [53] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In ICCV, 2023. 13 [54] Kaifeng Zhao, Gen Li, and Siyu Tang. Dartcontrol: diffusion-based autoregressive motion model for real-time text-driven motion control. In ICLR, 2025. 1, 3 [55] Rui Zhao, Hui Su, and Qiang Ji. Bayesian adversarial human motion synthesis. In CVPR, 2020. 2 [56] Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, and Xiangyang Ji. Parco: Partcoordinating text-to-motion synthesis. In ECCV, 2024. 3 [34] Abhinanda Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael Black. Babel: Bodies, action and behavior with english labels. In CVPR, 2021. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 14 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [37] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. In NeurIPS-W, 2019. 5, 11, 14 [38] Florian Schmidt. Generalization in generation: closer look at exposure bias. In EMNLP-IJCNLP, 2019. [39] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 5, 11 [40] Mikihiro Tanaka and Kent Fujiwara. Role-aware interaction generation from textual description. In ICCV, 2023. 8 [41] Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In ECCV, 2022. 2 [42] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit Bermano, and Daniel Cohen-Or. Human motion diffusion model. In ICLR, 2023. 1, 2, 6, 7, 13 [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 6 [44] Ronald Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural computation, 1989. 3 [45] Lixing Xiao, Shunlin Lu, Huaijin Pi, Ke Fan, Liang Pan, Yueer Zhou, Ziyong Feng, Xiaowei Zhou, Sida Peng, and Jingbo Wang. Motionstreamer: Streaming motion generation via diffusion-based autoregressive model in causal latent space. In ICCV, 2025. 1, 3, 6, 7, 12 [46] Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, and Dahua Lin. Convolutional sequence generation for skeletonbased action synthesis. In CVPR, 2019. [47] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. 3 [48] Qing Yu, Mikihiro Tanaka, and Kent Fujiwara. Exploring vision transformers for 3d human motion-language models with motion patches. In CVPR, 2024. 2, 13, 14 [49] Qing Yu, Mikihiro Tanaka, and Kent Fujiwara. Remogpt: Part-level retrieval-augmented motion-language models. In AAAI, 2025. 2, 3, 6, 11, 13, 14 [50] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. In CVPR, 2023. 1, 2, 6, 7,"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. MAC-VAE The proposed MAC-VAE consists of seven causal convolutional layers and two causal ResNet blocks with left padding in both the encoder and decoder to ensure strict temporal causality. Each convolutional layer uses kernel size of 3 and stride of 1, followed by ReLU activation. The latent feature dimension is set to 64, and motion sequences are downsampled/upsampled by factor of 4 along the temporal axis using stride-2 convolutional layers within the ResNet blocks. To achieve semantic alignment between motion and text, we modify Part-TMR [49] to extract frame-level motionlanguage embeddings. Part-TMR uses [class] token to aggregate frames into global feature, whereas we directly extract features from each frame and align them with the corresponding text features via contrastive learning, which serves as the supervision signal for MAC-VAE. The loss weighting coefficient is set to β=1.0, and the margin parameters are set to m1=0.5 and m2=0.25. We train MAC-VAE using the AdamW optimizer with learning rate of 1104 and batch size of 128 for 50 epochs on single NVIDIA A100 GPU. The learning rate follows cosine decay schedule, and gradient clipping with maximum norm of 1.0 is applied for training stability. A.2. Causal-DiT implemented as The Causal-DiT is lightweight transformer-based denoiser with 8 layers, 4 attention heads, and hidden dimension of 512. Causal selfattention is applied using lower-triangular mask to enforce temporal order, while cross-attention conditions motion latents on text embeddings extracted from DistilBERT [37]. We incorporate Adaptive Layer Normalization (AdaLN) [29] and Rotary Positional Encoding (ROPE) [39] to embed timestep information and stabilize long-horizon attention. During training, the text condition is randomly dropped with probability of 0.1 to enable classifier-free guidance. The model is optimized using AdamW with the same hyperparameter settings as MAC-VAE. The scale of classifier-free guidance is set to 3.0 during inference. A.3. Causal Diffusion Forcing In CMDM, causal diffusion forcing is employed to enable temporally ordered denoising while maintaining framelevel stochasticity. During training, each frame is perturbed with an independent noise level kt [0, K], where K=1000 denotes the total number of diffusion steps. The Causal-DiT serves as the denoiser, learning to predict noise residuals ϵθ(zt, kt, c) conditioned on all preceding latent frames and the text embedding c. This formulation ensures that each frame is denoised based solely on its causal history, thereby enforcing strict temporal dependencies. The overall training process is summarized in Algorithm 1. During inference, we adopt the Frame-Wise Sampling Schedule (FSS) with diffusion steps K=50 and uncertainty In this setting, the denoising of frame t+1 scale L=2. begins at step KL of frame t, allowing partially denoised frames to guide subsequent generations. This causal scheduling mechanism significantly accelerates inference by reducing redundant diffusion steps while maintaining temporal consistency across frames. The overall inference process with FSS is summarized in Algorithm 2. B. Additional Quantitative Results B.1. Experiments on BABEL We further evaluate CMDM on the BABEL dataset [34] to assess its generalization ability to diverse motion compositions. BABEL contains densely annotated sequences with multiple actions and transitions, making it suitable for long-horizon motion synthesis and evaluation. We train CMDM by constructing training samples from adjacent subsequences in BABEL, where each pair of consecutive segments is used to learn motion continuation across long sequences. As shown in Table 5, our method achieves the best overall performance across both subsequence and transition metrics, demonstrating the advantage of CMDM in maintaining consistency across action boundaries and generating smooth, continuous motions. B.2. Evaluation on Other Motion Features To further examine the generalization ability of CMDM, we conduct experiments using motion features with redundant dimensions removed, following the analysis in [24]. As discussed in prior work, the standard HumanML3D motion representation contains redundant components such as local joint rotations and contact features that do not directly influence the final human pose. Removing these redundant features yields more compact and physically meaningful representation better suited for continuous diffusion modeling. Table 6 reports the results on HumanML3D using only essential motion features. Compared to the baseline methods, CMDM consistently improves generation quality and semantic alignment under both autoregressive (AR) and diffusion (FSS) configurations. Specifically, CMDM w/ Algorithm 1 CMDM Training with Causal Diffusion Forcing Require: Pretrained MAC-VAE encoder Eϕ, text embedding c, Causal-DiT ϵθ, diffusion schedule {αk, αk}K 1: for each minibatch do 2: 3: 4: 5: 6: Sample independent noise level kt U{0, 1, . . . , K} Diffuse latent: zkt αktzt + Predict noise with causal conditioning: ˆϵt ϵθ(zt, kt, c) Encode motion sequence: z1:T Eϕ(x1:T ) for = 1 to do ϵt (0, I) 1 αktϵt, k=0 end for Compute loss: LDF 1 Update θ θ ηθLDF 7: 8: 9: 10: end for (cid:80)T t=1 ϵt ˆϵt2 2 Algorithm 2 CMDM Streaming Generation with Frame-wise Sampling Schedule (FSS) Require: Causal-DiT ϵθ, MAC-VAE decoder Dψ, text embedding c, schedule matrix NM 1: Initialize zK 2: for = 1, . . . , do 3: 4: (0, I) for t=1, . . . , for = 1, . . . , do (cid:17) ˆϵt + σk w, (0, I) 5: 6: Obtain noise level Km,t Predict noise with previous frames: ˆϵt ϵθ(z Denoise the current frame: k1 if = 0 then 1 αk (cid:16) t, k, c) 1αk 1 αk Decode final clean latent: ˆxt Dψ(z 0 t) 7: 8: 9: 10: 11: end for 12: return Decoded motion ˆx1:T (or latents end for end if 1:T ) FSS achieves the best overall performance, reaching an RPrecision of 0.563/0.759/0.849 for Top-1/Top-2/Top-3 accuracy and the lowest FID of 0.078, confirming that our causal diffusion formulation effectively models temporally coherent motion even in compact feature spaces. These results demonstrate that CMDM remains robust across different motion representations, further validating its adaptability to feature compression and reparameterized motion distributions. B.3. Compositional Motion Generation We evaluate CMDM on the compositional motion generation task following the protocol of Multi-Track Timeline (MTT) [32], which requires generating coherent motions conditioned on multiple temporally structured text descriptions. This task evaluates both semantic composition, i.e., correctly realizing multiple concepts within single sequence, and temporal composition, i.e., ensuring smooth and consistent transitions across segments. Specifically, following prior work [32, 51], we report per-crop semantic correctness metrics (R@1, R@3, and TMR-Score for M2T and M2M), as well as realism metrics including FID and transition distance. As shown in Table 7, CMDM, under the single-track multi-crop setting, consistently outperforms EnergyMoGen and other compositional baselines across all metrics. Notably, CMDM achieves substantial improvements in semantic alignment while simultaneously reducing FID and transition distance, demonstrating stronger long-horizon consistency and smoother transitions between composed motion segments. B.4. Latency analysis To evaluate the practical efficiency of different causal motion generation methods, we measure the latency for generating each token (4 frames) on single NVIDIA A100 GPU. MARDM [24], MotionStreamer [45], and CMDM w/ AR require approximately 210 ms, 360 ms, and 150 ms, respectively, to generate the first token, with similar latency for each subsequent token. This is because these autoregressive diffusion methods perform full diffusion denoising for each token independently, requiring multiple denoising steps per frame regardless of its temporal position. In contrast, CMDM w/ FSS takes about 220 ms for the first token but only 30 ms per subsequent token, achieving 512 speedup for streaming generation. This dramatic reduction in per-token latency stems from our frame-wise sampling 12 Methods R-prec FID Div MM-Dist FID Div PJ AUJ Subsequence Transition 0.7150.003 GT FlowMDM 0.7020.004 0.7110.005 Ours 0.000.00 8.420.15 3.360.06 0.000.00 6.200.06 0.020. 0.000.00 0.990.04 0.900.06 8.360.13 8.470.20 3.450.02 3.390.05 2.610.06 2.450.05 6.470.05 6.730. 0.060.00 0.050.00 0.130.00 0.110.01 Table 5. Comparison of long-horizon motion generation on BABEL. Subsequence metrics evaluate motion quality and diversity within segments, while transition metrics assess temporal continuity and smoothness between segments. Methods Framework T2M-GPT [50] MMM [33] MoMask [11] MDM-50Step [42] MotionDiffuse [42] MLD [5] ReMoDiffuse [53] SALAD [15] VQ Diffusion MARDM-DDPM [24] Autoregressive MARDM-SiT [24] Transformer Top 0.470.003 0.487.003 0.490.004 0.440.007 0.450.006 0.461.004 0.468.003 0.552.003 0.492.006 0.500.004 R-Precision Top 2 0.659.002 0.683.002 0.687.003 0.636.006 0.641.005 0.651.004 0.653.003 0.748. 0.690.005 0.695.003 Top 3 0.758.002 0.782.002 0.786.003 0.742.004 0.753.005 0.750.003 0.754.005 0.839.002 0.790.005 0.795.003 FID MM-Dist MModality CLIP-score 0.335.003 0.132.004 0.116.006 0.518.032 0.778.035 0.431.014 0.883.021 0.124.005 0.116.004 0.114.007 3.505.017 3.359.019 3.353.010 3.640.028 3.490.023 3.445.019 3.414.020 2.990. 3.349.010 3.270.009 2.018.053 2.241.073 1.263.079 3.604.031 3.179.046 3.506.031 2.703.154 1.833.081 2.470.053 2.231.071 1.810.068 1.827.094 0.607.005 0.635.003 0.637. 0.578.003 0.606.004 0.615.003 0.621.003 0.671.001 0.637.005 0.642.002 0.675.001 0.685.001 Ours w/ AR Ours w/ FSS Autoregressive Diffusion 0.550.004 0.563. 0.747.004 0.759.003 0.838.003 0.849.002 0.085.004 0.078.003 2.987.011 2.920.007 Table 6. Results of text-to-motion generation on HumanML3D without redundant features. The average is reported over 10 runs with 95% confidence intervals. Bold indicates the best result, and underline denotes the second-best result. Input type Per-crop semantic correctness Realism #tracks #crops R@1 R@3 M2T M2M FID Transition Model Config. Reconstruction FID MPJPE R-Top1 Generation FID MM-Dist Method GT - - 55.0 73.3 0.748 1.000 0.000 EnergyMoGen [51] Single Single 15.9 28.0 0.591 0.567 0.604 MDM-SMPL [32] Single Single 12.1 23.5 0.573 0.578 0.484 w/ DiffCollage [32] Single Multi 29.1 49.7 0.675 0.656 0.446 Multi Multi 30.5 50.9 0.675 0.665 0.459 w/ STMC [32] Single Multi 41.7 57.9 0.690 0.672 0.438 CMDM (Ours) 1.5 1.6 1.8 1.2 0.9 1.2 Table 7. Comparison with prior compositional motion generation methods on the Multi-track timeline (MTT) dataset [32]. schedule, which allows each frame to be predicted from partially denoised preceding frames rather than requiring full iterative refinement. B.5. Ablation Studies Architecture of MAC-VAE. We evaluate several configurations of MAC-VAE to analyze the effects of latent dimension and temporal downsampling rate on both reconstruction and generation performance. The notation (d, r) denotes the latent dimension and the temporal downsampling rate r. As shown in Table 8, increasing the latent dimension improves reconstruction accuracy but also introduces redundancy that slightly affects generation quality in terms of FID. Conversely, larger temporal downsampling rates (e.g., = 1/8) reduce temporal resolution and lead to minor degradation in R-Precision and MM-Dist due to information loss. Among all configurations, MAC-VAE with (64, 1/4) achieves the best balance between reconstruction fidelity (FID= 0.000, MPJPE= 0.012) and generation quality (R-Top1= 0.588, FID= 0.068, MM-Dist= 2.620), 13 VAE C-VAE MAC-VAE MAC-VAE MAC-VAE MAC-VAE MAC-VAE MAC-VAE 64, 1/4 64, 1/4 64, 1/4 32, 1/4 16, 1/4 64, 1/8 32, 1/8 16, 1/8 0.001 0.000 0.000 0.002 0.011 0.002 0.006 0. 0.016 0.012 0.012 0.033 0.077 0.035 0.060 0.101 0.561 0.575 0.588 0.583 0.573 0.570 0.566 0. 0.107 0.070 0.068 0.065 0.071 0.069 0.057 0.054 2.706 2.650 2.620 2.628 2.647 2.664 2.704 2. Table 8. Comparison of reconstruction and generation performance on HumanML3D. MPJPE is measured in millimeters. The notation (d, r) denotes the latent dimension and the temporal downsampling rate r. which we adopt as the default setting in all subsequent experiments. These results confirm that compact latent space with moderate temporal compression effectively captures semantic and temporal dependencies for downstream motion generation. Motion-Language Models To evaluate the effectiveness of different motionlanguage alignment strategies, we compare several pretrained motionlanguage models integrated into the MAC-VAE framework, including TMR [31], MotionPatches [48], and Part-TMR [49]. As shown in Table 9, all motionlanguage models improve generation quality while maintaining reconstruction performance compared to the baseline VAE and C-VAE, demonstrating the effectiveness of semantic alignment between motion and text. Model VAE C-VAE Part-TMR [49] MotionPatches [48] TMR [31] Reconstruction FID MPJPE R-Top1 Generation FID MM-Dist 0.001 0.001 0.000 0.000 0.001 0.016 0. 0.012 0.013 0.013 0.561 0.575 0.588 0.586 0.580 0.107 0.070 0.068 0.070 0.070 2.706 2. 2.620 2.622 2.638 Table 9. Comparison of motion-language models in MAC-VAE on HumanML3D. MPJPE is measured in millimeters. Model Size Config. R-Precision Top2 Top Top3 FID MM-Dist H2, L4, D512 (19M) H4, L8, D512 (38M) (129M) H6, L12, D768 XL (304M) H8, L16, D1024 0.543 0.588 0.585 0.590 0.738 0.778 0.779 0.779 0.834 0.860 0.859 0. 0.247 0.068 0.044 0.042 2.845 2.620 2.621 2.610 Table 10. Comparison of model sizes on HumanML3D. The notation (H, L, D) denotes the number of attention heads H, layers L, and hidden dimension D. Text Encoder Embedding R-Precision Top Top1 Top3 FID MM-Dist DistilBERT [37] CLIP [35] CLIP [35] Sentence-T5 [25] Word Word Sentence Sentence 0.588 0.556 0.527 0. 0.778 0.751 0.717 0.754 0.860 0.843 0.809 0.841 0.068 0.086 0.145 0.126 2.620 2.717 2.941 2.737 Table 11. Comparison of text encoders on HumanML3D. Among them, Part-TMR achieves the best overall performance with the lowest reconstruction error (FID= 0.000, MPJPE= 0.012) and the highest R-Precision (0.588), confirming its strong ability to capture fine-grained part-level correspondences between text and motion. These results validate the choice of Part-TMR as the alignment backbone in MAC-VAE, enabling more semantically coherent and temporally consistent motion generation. Model Size of Causal-DiT. We investigate the impact of model size on generation quality by varying the number of attention heads (H), layers (L), and hidden dimensions (D) in Causal-DiT. As shown in Table 10, larger models generally achieve better performance due to increased representational capacity. The medium-sized model (38M parameters) already provides strong results with an R-Precision of 0.588 and FID of 0.068, balancing quality and efficiency. Further scaling to 304M parameters yields marginal improvements (R-Precision= 0.590, FID= 0.042), demonstrating that Causal-DiT scales effectively while maintaining computational practicality. Unless otherwise specified, we use the medium (38M) configuration in all main experiments. Text Encoder We compare several pretrained language models as text encoders to evaluate their impact on semantic alignment and motion quality. As shown in Table 11, the choice of text encoder influences both textmotion correspondence (R-Precision) and visual realism (FID). DistilBERT [37], which provides word-level embeddings, achieves the best overall performance with the highest RPrecision (0.588) and lowest FID (0.068), demonstrating its ability to capture fine-grained semantic cues that align well with motion features. Using the CLIP-based encoder, the word-level variant, which is identical to that employed in StableMoFusion [16], also outperforms StableMoFusion, further confirming the benefits of word-level representations. Such token-level embeddings are crucial for maintaining causal dependencies between linguistic tokens and motion frames, which is necessary for stable autoregressive generation in CMDM. In contrast, sentence-level embeddings from CLIP [35] exhibit reduced precision and higher FID due to the loss of temporal granularity. Meanwhile, Sentence-T5 [25] performs better than the CLIPbased models and also outperforms MotionLCM V2 [16], despite MotionLCM V2 also using Sentence-T5. These findings validate our choice of DistilBERT as the text encoder for CMDM, as it effectively preserves local semantics and enables causally consistent motionlanguage modeling. C. Additional Qualitative results To further demonstrate the effectiveness of CMDM, we provide additional qualitative comparisons on long-horizon and text-to-motion generation. Fig. 4 and Fig. 5 compares CMDM with FlowMDM [3] and MARDM [24] on longhorizon motion generation for HumanML3D and SnapMoGen, respectively. CMDM produces temporally coherent and semantically accurate motions without content drift or skeleton flipping, whereas previous methods often suffer from static poses, incorrect transitions, or inconsistent actions across segments. These examples highlight the ability of CMDM to maintain smooth temporal dynamics and causal consistency throughout extended sequences. Fig. 6 presents qualitative results on HumanML3D. Compared with MoMask [11], MotionLCM [6], and StableMoFusion [16], CMDM generates motions that more faithfully reflect fine-grained textual semantics (e.g., arm rotations, leg movements, or walking direction) while preserving natural body articulation. Fig. 7 shows additional results on SnapMoGen, where CMDM directly uses the raw text prompts without LLM-based augmentation and still produces more realistic motions than prior methods. Please refer to the supplementary videos on the demo page for full-length visualizations. D. Sample Code The code will be released at https://github.com/ YU1ut/CMDM. We provide the training codes for building and evaluating the proposed CMDM with the HumanML3D dataset. Please refer to the README file in the code directory for details. 14 Figure 4. Qualitative results of long-horizon motion generation on HumanML3D. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences. 15 Figure 5. Qualitative results of long-horizon motion generation on SnapMoGen. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences. 16 Figure 6. Qualitative results of text-to-motion generation on HumanML3D. CMDM produces motions that better capture fine-grained textual semantics and maintain natural body articulation compared to previous methods. Please refer to the supplementary videos for clearer visualization. Figure 7. Qualitative results of text-to-motion generation on SnapMoGen. Comparison between our CMDM and previous methods. We directly use the raw text prompts without any LLM-based augmentation and CMDM still achieves strong generation quality. Please refer to the supplementary videos for clearer visualization."
        }
    ],
    "affiliations": [
        "LY Corporation",
        "Waseda University"
    ]
}