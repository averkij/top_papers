{
    "paper_title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
    "authors": [
        "Zhongang Cai",
        "Yubo Wang",
        "Qingping Sun",
        "Ruisi Wang",
        "Chenyang Gu",
        "Wanqi Yin",
        "Zhiqian Lin",
        "Zhitao Yang",
        "Chen Wei",
        "Xuanke Shi",
        "Kewang Deng",
        "Xiaoyang Han",
        "Zukai Chen",
        "Jiaqi Li",
        "Xiangyu Fan",
        "Hanming Deng",
        "Lewei Lu",
        "Bo Li",
        "Ziwei Liu",
        "Quan Wang",
        "Dahua Lin",
        "Lei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 4 1 3 1 . 8 0 5 2 : r Has GPT-5 Achieved Spatial Intelligence? An Empirical Study Zhongang Cai,1, Yubo Wang,1, Qingping Sun,1, Ruisi Wang,1, Chenyang Gu,1, Wanqi Yin,1, Zhiqian Lin,1, Zhitao Yang,1, Chen Wei,1, Xuanke Shi1, Kewang Deng1, Xiaoyang Han1, Zukai Chen1, Jiaqi Li1, Xiangyu Fan1, Hanming Deng1, Lewei Lu1, Bo Li2, Ziwei Liu2, Quan Wang(cid:0),1, Dahua Lin(cid:0),1, Lei Yang,(cid:0),1 Core Contributors, (cid:0) Corresponding Authors, 1SenseTime Research, 2S-Lab, Nanyang Technological University"
        },
        {
            "title": "Abstract",
            "content": "Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit decisive advantage when facing the most difficult problems. In addition, we conduct qualitative evaluation across diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models. Date: August 19,"
        },
        {
            "title": "Introduction",
            "content": "Spatial understanding and reasoning [2, 7, 9, 11, 14, 15, 23, 25, 30, 39, 45, 49, 52, 56, 74] constitute critical yet underexplored dimension of intelligence, one that is indispensable for artificial general intelligence (AGI): without spatial intelligence, an embodied agent cannot fully operate in, adapt to, or interact with the physical world. Despite the impressive advancements in multi-modal large language models (MLLMs) [1, 3, 6, 19, 20, 27, 28, 34, 35, 50, 51, 53, 58, 61, 69, 72], it has become evident that even the most advanced MLLMs often fail at spatial tasks that are trivially easy for humans as shown in Fig. 1. Recent work [32] has shown that spatial intelligence (SI) is fundamentally distinct skill, arguably one of the last underexplored frontiers, compared to the multi-modal capabilities measured by mainstream benchmarks [4, 5, 8, 13, 16, 18, 29, 33, 3638, 4042, 47, 48, 59, 64, 65, 67, 68, 73]. With the very recent release of GPT-5 [44], the community is naturally curious about its performance on this dimension of intelligence: Has GPT-5 (or any other state-of-the-art models) achieved spatial intelligence? We begin by providing systematic survey of the eight key benchmarks designed for evaluating spatial intelligence and used in this technical report. Notably, most of these benchmarks [10, 17, 21, 22, 24, 31, 32, 46, 54, 55, 57, 60, 62, 63, 70, 71] have been released within the past three months, underscoring the growing research attention in this field. As 1 Figure 1 While GPT-5 [44] excels at solving complex problems (left) that are considered challenging for humans, it still struggles with the basic spatial intelligence tasks (right), which even human child can comprehend effortlessly. For simplicity, GPT-5s detailed reasoning process is not shown here. each benchmark focuses on different aspects and adopts its own taxonomy, we consolidate them into six fundamental capabilities, and label all the benchmarks subcategories accordingly. Furthermore, we discuss evaluation protocols as well as challenges and solutions for ensuring accuracy and fairness, given that results can be highly sensitive to factors beyond model capability. Specifically, we standardize the prompts, evaluation strategies, and metrics to ensure fair comparison across benchmarks. We then present detailed results for GPT-5 [44], alongside other recent high-performing MLLMs that have not yet been thoroughly evaluated, on the key benchmarks, including VSI-Bench [60], SITE [55], MMSI [62], OmniSpatial [22], MindCube [63], STARE [31], CoreCognition [32], and SpatialViz [54]. Our empirical findings reveal that (1) GPT-5 has become the state of the art model on spatial intelligence, surpassing strong baselines such as Gemini-2.5-pro [51] and InternVL3 [72], even reaching human-level performance in cases that rely on \"Metric Measurement(MM)\" and \"Spatial Relations(SR)\"; (2) Nevertheless, there is still considerable performance gap between GPT-5 and humans on most of benchmarks, especially for \"Mental Reconstruction(MR)\", \"Perspective-taking(PT)\", \"Deformation and Assembly(DA)\", and \"Comprehensive Reasoning(CR)\" capabilities; (3) Overall, state-of-the-art MLLMs perform significantly worse on fundamental tasks of spatial intelligence than they do on non-SI tasks; (4) Proprietary models do not show significant advantage over their open-sourced counterparts on challenging SI tasks. In addition, we conduct case study on noteworthy failure cases drawn both from these benchmarks and from in-the-wild sources, illustrating the strengths and limitations of GPT-5 and other state-of-the-art models. The findings from the case study align closely with the quantitative results: MM and SR perform relatively well, while the other four tasks show poor performance. We identified some interesting findings: (1) Although SR performs well overall, the model still exhibits certain blind spots such as the inadequate ability to handle the perspective effect. (2) The model demonstrates significant improvement in MR compared to other MLLMs, successfully solving some problems for the first time. However, it still struggles with some tasks that are straightforward for humans. (3) PT, DA, and CR remain particularly challenging due to their reliance on integrated capabilities and multi-stage reasoning. An analysis of its reasoning process indicates that the lack of fundamental spatial abilities prevents the model from completing correct reasoning, even when it adopts an appropriate problem-solving approach. We hope that this technical report will provide foundation for advancing future research on spatial intelligence in MLLMs. Beyond benchmarking current progress, our work also highlights the unique challenges posed by SI, clarifies fundamental task categories, and standardizes evaluation practices. Together, these contributions aim to establish shared basis for comparing future models, guiding methodological improvements, and fostering cumulative advances in the pursuit of spatial intelligence."
        },
        {
            "title": "2.1 Six Fundamental Capabilities",
            "content": "Existing benchmarks focus on distinct aspects of spatial intelligence and often adopt varying taxonomies to characterize cognitive and reasoning abilities. To accommodate all benchmarks within unified framework, we distill six fundamental capabilities from existing benchmarks with spatial intelligence components [2, 7, 10, 11, 1315, 17, 21, 22, 24, 31, 32, 39, 40, 45, 46, 49, 52, 5457, 60, 62, 63, 70, 71], as conceptually illustrated in Fig. 2. Metric Measurement (MM). Inferring 3D dimensions (such as metric depth or lengths) from 2D observations is inherently ambiguous without additional information such as camera intrinsics. Hence, the ability to make reasonable estimate reflects the understanding of the physical scale and typical object sizes. Mental Reconstruction (MR). This category assesses models fine-grained geometric understanding of an object from one or more constrained viewpoints, requiring it to infer the complete 3D structure from limited 2D observations and sometimes perform virtual manipulation. While alternative viewpoints may be used to test this capability, MR differs from perspective-taking in that it involves constructing detailed mental representation of the object, as in mental rotation tasks. Such skill empowers real-world engineering applications, including interpreting or producing three-view drawings, and aligns closely with research areas such as single-view 3D object reconstruction. Spatial Relations (SR). This capability concerns understanding the relative positions and orientations of multiple objects within the camera view. Such tasks can be seen as building upon Metric Measurement (MM) and Mental Reconstruction (MR). Typical applications include describing an objects location relative to nearby objects. While SR does not require imagining viewpoint transformation, it often involves conceptualizing and applying virtual coordinate system to support the reasoning process. Perspective-taking (PT). This ability involves reasoning across distinct viewpoints (e.g., aligning ego-centric and exocentric perspectives). PT could subsume three components: (i) MR-like construction of mental 3D representation of the scene, (ii) SR-like reasoning over multiple objects at the scene level, and (iii) explicit reasoning under changing camera viewpoints. related research domain is cross-view correspondence matching. Notably, PT problem does not necessarily involve multiple images: imagining viewpoint changes from single image also falls within this category. Deformation and Assembly (DA). While the preceding capabilities typically assume shape consistency, many spatial reasoning tasks go beyond this assumption. DA focuses on understanding and reasoning about deformations or structural changes. Examples include knot tying, interpreting box unfolding diagrams, and assembling multiple parts. This capability is essential for the embodied AI, where manipulation requires reasoning over such structural transformations. Comprehensive Reasoning (CR). This category of tasks requires the coordinated use of various spatial capabilities in conjunction with extended memory and multi-stage reasoning. Examples include navigation in large, dynamic environments, and tackling spatial reasoning challenges such as long-horizon puzzle solving or mentally simulating complex physical interactions. Figure 2 Six Fundamental Capabilities of Spatial Intelligence."
        },
        {
            "title": "2.2 Benchmark Statistics",
            "content": "To comprehensively evaluate model performance in spatial intelligence, we assess them on eight key benchmarks. We summarize the key aspects of these benchmarks in Tab. 1. We highlight that these benchmarks are released very recently, indicating the increasing research attention on spatial intelligence. In particular, MindCube [63] contains 21K questions, 3 significantly exceeding other benchmarks. However, the three subsets of MindCube (among, around, rotation) are imbalanced, with the among subset containing 18K questions. Therefore, we adopt MindCube-Tiny for testing, which includes 1,050 QA pairs with balanced distribution (among:around:rotation = 600:250:200) and 428 unique images. Across all eight benchmarks, each standard evaluation (non-circular) was evaluated on approximately 31K images, 4.5K videos, and 24K QA in total. Benchmark YY/MM #Image #Video #QA CoT Annotation Fundamental Capabilities VSI-Bench [60] SITE [55] MMSI [62] OmniSpatial [22] MindCube [63] STARE [31] CoreCognition [32] SpatialViz [54] 24/12 25/05 25/05 25/06 25/06 25/06 25/06 25/ - 13.2K 2K 1.3K 3.2K 10.3K 1.5K 1.2K 288 3.8K - - - - 217 - 5K 8.1K 1K 1.5K 21.1K 4K 1.5K 1.2K Man.+Tem. Man.+LLM N Man.+Tem. Man.+Tem. Man. Man. Tem. Tem. MM,SR,PT,CR MM,SR,PT,CR MM,MR,PT,CR MM,PT,CR PT PT,DA,CR SR,PT MR,SR,DA,CR Table 1 Key aspects of benchmarks for spatial intelligence. YY/MM: the year and the month of release, depending on published date or first version on arXiv. CoT: whether to have chain-of-thought labels. Annotation: annotation method (Man.: manually annotated, LLM: curated with LLM, Tem.: generated with templates). Fundamental Capabilities: see Sec. 2.1 for definitions."
        },
        {
            "title": "2.3 Evaluation Protocols",
            "content": "Despite rapid advancements in spatial benchmarks, variations in metrics, system prompts, answer-matching methods, and evaluation strategy complicate cross-benchmark evaluations, as outlined in Tab. 5. Metrics. Our focus lies on two types of tasks: multiple-choice questions (MCQ) and Numerical Answer (NA) questions. For multiple-choice tasks, where the number of options varies across questions, we employ the Chance-Adjusted Accuracy (CAA) from SITE [55] to eliminate the confounding effects of random guessing. For NA, since only four subsets in VSI-Bench [60] include such tasks, we adopt the Mean Relative Accuracy (MRA) as used in VSI-Bench [60]. The detailed calculation formulas for both metrics can be found in Appendix A.2. It is well known that system prompts significantly impact model performance [22, 31, 62], as System Prompts. well as testing efficiency and answer matching. Different benchmarks adopt varying system prompts. For example, MMSI-Bench [62] operates without additional prompts by default, while SpatialViz [54] employs tailored prompts to improve answer-matching precision. OmniSpatial [22] compares Direct QA, zero-shot CoT, and manual CoT, observing that both CoT variants consistently outperform the Direct QA baseline. To maximize the spatial reasoning capabilities of models, we adopt the zero-shot CoT approach from OmniSpatial [22] and follow the answer templates specified in SpatialViz [54]. The complete system prompt is provided in Fig. 4. Answer-Matching Methods. Variations in answer-matching methods across benchmarks also introduce inconsistencies, as both under-extraction and incorrect extraction hinder objective evaluations of spatial capabilities. Following best practices from VLMEvalKit [12] and LMMS-Eval [26, 66], we employ three-step matching process: 1) Initial Rule-Based Matching: Extract answers enclosed within the <answer></answer> tags, as required by our system prompt. 2) Extended Rule-Based Matching: If the first step fails, we draw on SpatialViz [54] to extract answers using additional patterns such as \"<answer>, \"Answer:, \"Final answer, and similar formats. 3) LLM-Assisted Extraction: For cases where rule-based methods fail, we follow OmniSpatial [22] by using an LLM to extract the corresponding answer. If all three steps fail, the response is considered incorrect. Circular Evaluation. In addition, to reduce option-position bias, we employ circular evaluation strategy [36]. In this setup, each multiple-choice question with possible answers is presented times, with the answer options rotated each time. Scores are computed in two variants: 1) Soft-circular scoring: aligned with CoreCognition [32], we measure the proportion of correct selections across all rotations. 2) Hard-circular scoring: following MMBench [36] response is only considered correct if it selects the right answer in every rotation. However, since testing time and cost increase k-fold, we primarily adopt the standard (non-circular) manner, applying circular evaluation only for some representative benchmarks, related results can be seen in Sec. 3.2.2."
        },
        {
            "title": "3.1 Main Results",
            "content": "Models Random Choice VSI [60] SITE [55] MMSI [62] OmniSpatial [22] MindCube [63] STARE [31] CoreCognition [32] SpatialViz [54] 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 Proprietary Models Seed-1.6-2025-06-15 [50] 27.28 Gemini-2.5-pro-2025-06 [51] 33.65 GPT-5-nano-2025-08-07 [44] 14.87 34.51 GPT-5-mini-2025-08-07 [44] GPT-5-2025-08-07 [44] 36. Open-source Models 12.36 Qwen2.5-VL-3B-Instruct [1] Qwen2.5-VL-7B-Instruct [1] 9.94 Qwen2.5-VL-72B-Instruct [1] 12.75 13.99 InternVL3-8B [72] InternVL3-78B [72] 25.48 53.87 56.39 41.31 53.55 64.18 28.41 32.24 44.09 38.53 49.42 17.65 18.23 8.38 10.84 22.47 -0.13 2.80 9.47 5.33 6.27 33.40 41.27 38.41 42.19 50. 20.90 18.64 30.65 28.39 35.18 19.14 20.20 3.05 20.29 21.67 1.18 -4.27 -1.97 6.64 1.33 20.46 26.73 18.14 28.75 33.04 -1.56 5.36 10.00 6.20 12.43 52.98 69.26 55.34 67.85 78. 29.99 31.86 50.76 38.87 51.29 9.94 29.26 11.76 29.25 15.96 -4.07 3.62 6.44 5.99 9.49 Human Evaluation Human 95.08 67. 96.27 90.18 91.94 94.63 79.10 76. Table 2 Evaluation on eight recent spatial benchmarks. Note that the reported metric is Chance-Adjusted Accuracy (CAA) [55] for consistency across benchmarks and elimination of random bias. Specifically, all values are calibrated such that random choice is always kept at 0.0. For human evaluations, we converted the original scores to CAA using Eq. (1). Benchmark-specific metrics, aligned with the original papers, are provided in the Appendix B. For VSI, only MCQ results are reported here, with full MCQ and Numerical Answer results available in the B.2. MindCube denotes MindCube-Tiny. denotes minimum thinking. Dark purple highlights the best result and light purple indicates the second-best result within Proprietary and Open-source models, respectively. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those reported in the original paper. We summarize the results of GPT-5 variants against other strong proprietary and open-source models in Tab. 2. Our key findings include: GPT-5 sets new state of the art in spatial intelligence. As shown in Tab. 2, GPT-5 surpasses both strong proprietary and open-source models by convincing margins. It achieves clear advantages on the vast majority of subcategories of SITE (Appendix B.3), MindCube (Appendix B.6), and STARE (Appendix B.7), while maintaining highly competitive overall performance across other benchmarks. In some cases, GPT-5 even reaches human-level performance. For example, on MM tasks (e.g., absolute distances, object and room sizes) in VSI-Bench (Appendix B.2) and SR tasks (e.g., in SITE and CoreCognition (Appendix B.8). It also demonstrates substantial improvements on perspective-taking (PT) tasks, as seen in VSI-Bench, SITE, STARE, and CoreCognition. Despite these advances, GPT-5 has not yet achieved spatial intelligence. While GPT-5s performance represents significant leap forward, it still falls short of human-level ability in several fundamental capabilities. Notable gaps remain in Mental Reconstruction (3 out of 8 benchmarks), Perspective-taking (6 out of 8), Comprehensive reasoning (3 out of 8), and Deformation and Analysis in SpatialViz (Appendix B.9). Spatial intelligence (SI) tasks pose greater challenges than non-SI tasks. prominent example is MMSI (Appendix B.4), highly challenging and comprehensive benchmark where even GPT-5 remains far from human-level performance. Across OmniSpatial (Appendix B.5), STARE, CoreCognition, and SpatialViz, SI tasks consistently exhibit much larger performance gap between the best model and human performance than non-SI tasks. Particularly, models have reached human-level performance on handful of non-SI tasks, such as Boundary, Perceptual Constancy, Conservation, and the entire Formal Operation category in CoreCognition. Proprietary models do not hold decisive advantage over open-source models on difficult SI tasks. While proprietary models generally outperform open-source ones overall, this advantage diminishes on the most challenging SI categories, particularly Mental Reconstruction (MR), Perspective-taking (PT), Deformation and Assembly (DA), and Comprehensive Reasoning (CR). In benchmarks such as MMSI, OmniSpatial, STARE, and SpatialViz, both proprietary and open-source 5 models perform similarly and remain far from human-level proficiency. This parity on the hardest tasks presents timely opportunity for the research community to drive advances by building on open-source models. Note that all values presented in Tab. 2 are run with OpenAI chat completion API, and protocols explained in Sec. 2.3 to ensure fair comparison among benchmarks that use different evaluation strategies. We also include the results in the metrics (but not the system prompts, which are standardized as shown in Appendix A.1) that follow the original papers in Appendix to facilitate comparison within each benchmark."
        },
        {
            "title": "3.2 Ablation Study",
            "content": "Thinking Mode Accuracy Reasoning tokens (Average) Reasoning tokens (Max) Runtime(s) (Average)"
        },
        {
            "title": "Minimal\nLow\nMedium\nHigh",
            "content": "48.31 54.24 56.78 52.54 0 1899 5860 8567 0 6636 13760 16064 11.69 53.89 140.3 305.2 Table 3 Ablation on thinking mode of GPT-5 [44] on the SpatialViz-Tiny set (sampled at one-tenth per task from full SpatialViz set), with max_completion_tokens=16,384. In High mode, 28 questions exceeded the 15-minute time limit or hit token limit, and were counted as incorrect, resulting in an accuracy of 52.54%; excluding these cases yields 68.89%."
        },
        {
            "title": "3.2.1 Thinking modes of GPT-5",
            "content": "In Tab. 2, we set max_completion_tokens to 2,048. Even when increased to 4,096, GPT-5 [44] often generates excessively long chains of thought in SpatialViz [54], ultimately failing to produce result more than 50% of the time. To obtain valid results, we switch GPT-5 to the minimal thinking mode. However, significant performance drop is observed, detailed results can be seen in Tab. 14. Consequently, we conduct an ablation study on GPT-5 using four reasoning modes (by experimenting with the effort parameter of the API): Minimal, Low, Medium, and High. We evaluate GPT-5s performance across different thinking modes using the SpatialViz-Tiny, which is constructed by sampling one-tenth of the instances from each task in the original dataset, resulting in total of 118 questions. For all experiments, we set max_completion_tokens to 16,384. As shown in Tab. 3, reasoning tokens generally increase with the thinking mode level, and accuracy improves from Minimal to Medium, indicating benefits from reasoning. In High mode, accuracy drops to 52.54% mainly because 28 of the 118 questions timed out (>15 min) or hit token length limit, and those were counted as incorrect. Excluding these cases, High mode reaches 68.89%, the best raw accuracy. In practice, however, while High mode typically performs best, its substantially higher time and compute costsalong with the risk of overlong reasoning that times out or is truncatedmust be weighed carefully; Medium often offers more balanced accuracycost trade-off."
        },
        {
            "title": "3.2.2 Circular Strategies",
            "content": "In this section, we compare model performance under three evaluation protocols: Non-circular, Soft-circular, and Hard-circular, as mentioned in Section Sec. 2.3, as an ablation to ensure the robustness of our findings. As shown in Tab. 4, for given model, large drop from Non-circular to Soft-circular or Hard-circular indicates that part of its accuracy in the Non-circular setting may come from successful random guesses in MCQ tasks. In particular, the Hard-circular metric, which requires all rotated variants of question to be answered correctly, serves as stricter measure of true task competence and more reliably discriminates among model capabilities. Occasionally, Soft-circular scores exceed Non-circular scores, because easier questions with more options are essentially repeated more times, which inflates the Soft-circular average. We observe this in Tab. 4 on CoreCognition for both GPT-5-nano and GPT-5. Since Soft-circular scoring can, in edge cases, change conclusions, we report CoreCognition in both views: Tab. 2 presents CAA under Non-circular protocol, while Tab. 13 in Appendix B.8 reports ACC under Soft-circular protocol. Across Non-circular and Hard-circular evaluation modes, model rankings are broadly consistent, indicating that reporting only Non-circular results suffices for fair comparison while substantially reducing evaluation time and computational cost. 6 Models SITE MMSI CoreCognition Non-circular Soft-circular Hard-circular Non-circular Soft-circular Hard-circular Non-circular Soft-circular Hard-circular GPT-5-nano-2025-08-07 [44] GPT-5-mini-2025-08-07 [44] GPT-5-2025-08-07 [44] 71.02 75.94 80.09 61.51 69.07 78. 47.96 58.30 72.43 31.29 33.13 41.86 29.2 32.24 41.37 9.76 13.8 26.14 72.28 80.20 86.60 72.43 79.78 87. 66.81 71.35 83.42 Table 4 Ablation study on circular test. Non-circular stands for standard tests without rotating options. For SITE, the MultiV subset is excluded, as its large number of questions would make circular testing prohibitively time-consuming."
        },
        {
            "title": "4 Case Study",
            "content": "In Fig. 3, we conducted an extensive qualitative evaluation of GPT-5 to assess its spatial intelligence, with particular attention to potential improvements over its predecessor, GPT-o3 [43]. Note that the more detailed thinking processes are included in Appendix C. Moreover, in this report, we use the website platform for all case studies (GPT-5 and GPT-5-thinking), and API for quantitive evaluation in other sections (GPT-5 with four levels of thinking intensity). While GPT-5 demonstrates flashes of potential, excelling in some tasks, it remains far from achieving human-level spatial intelligence. Its successes are often restricted to specific, constrained problem settings, and it still lacks the fundamental, generalized reasoning skills of human cognition. Our evaluation highlights several key findings: Metric Measurement (MM). GPT-5 performs reliably on basic real-world images, as shown in Fig. 3 MM1. This indicates GPT-5 is likely to possess basic knowledge of object dimensions in the real world. Note that MM is inherently an ambiguous task without camera intrinsics, and thus large error margin is tolerated for the estimations. Mental Reconstruction (MR). This category shows mixed results. On the one hand, GPT-5, for the first time, demonstrates strong capabilities in reconstructing an object from multiple views (Fig. 3 MR2). Moreover, it significantly outperforms o3 [43] in novel view generation, particularly when the thinking mode is activated, resulting in correct top-down views (Fig. 3 MR3). However, we also observed that it is highly sensitive to prompts, with only specific prompts capable of eliciting correct view generation. In addition, MR4 presents surprising case where question that is considered simple for human child, unexpectedly fails all state-of-the-art MLLMs. Spatial Relations (SR). SR is generally well-addressed. However, there are still cases that may confuse the models. As shown in Fig. 3 SR5, the scene becomes more complex with multiple objects and visual illusions due to the perspective effect. GPT-5 fails to recognize the true sizes of the objects, with no substantial improvement compared to o3 [43], revealing lack of robust understanding of spatial relationships between objects and their impact on the apparent physical scale. This remains limitation in GPT-5s spatial reasoning capabilities. Perspective-taking (PT). GPT-5 struggles to reason between changing camera viewpoints, especially when the view overlap is relatively minimal (Fig. 3 PT6), which is difficult for all state-of-the-art models. From its thinking process Appendix C, we observed that GPT-5 attempts to establish visual correspondence across different perspectives. However, it misinterprets the cameras rotation, suggesting that it has not developed solid ability for perspective transformation. Deformation and Assembly (DA). This remains critical weakness. GPT-5 fails in tasks requiring mental folding or reasoning about structural transformations, such as folding 2D net into 3D cube (Fig. 3 DA7) and assembly of objects (Fig. 3 DA8), highlighting its limitations in reasoning beyond rigid shapes. Comprehensive Reasoning (CR). CR involves multi-stage spatial reasoning tasks that require extended memory and logical deduction. We selected counting partially occluded objects question (Fig. 3 CR9), as it represents basic spatial reasoning task. However, GPT-5 struggles with this challenge: while it can recognize visible blocks, it fails to infer the presence of hidden ones through spatial reasoning. The detailed thinking process is showcased at Appendix C."
        },
        {
            "title": "5 Conclusion",
            "content": "In this technical report, we show that spatial intelligence poses unique challenges that remain insufficiently addressed even by state-of-the-art large multimodal models. While GPT-5 demonstrates exceptional performance and sets new state of the art in spatial intelligence, there remain key areas in which even the most advanced models fall short of human performance. Furthermore, we propose set of fundamental capabilities to unify existing spatial intelligence benchmarks and provide detailed analysis of the remaining limitations of the latest models. 7 Figure 3 Case Study. We compare the performance of GPT-5 with thinking capability (GPT-5-thinking), the standard GPT-5 model, the previous strong thinking model GPT-o3 [43], and another leading reasoning model, Doubao-Seed-1.6-thinking [50]. While GPT-5-thinking exhibits notable improvements over its predecessors, it remains far from conquering the full spectrum of spatial intelligence. For MR2 and MR3, Doubao-Seed-1.6-thinking is exempted from visual comparisons because it cannot generate images. Note in this comparison, the web-based services are used. The reasoning output and more examples can be found in Appendix C."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [2] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. arXiv preprint arXiv:2406.13642, 2024. [3] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [4] Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024. [5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 2418524198, 2024. [7] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062 135093, 2024. [8] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. [9] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [10] Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, et al. Internspatial: comprehensive dataset for spatial reasoning in vision-language models. arXiv preprint arXiv:2506.18385, 2025. [11] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. arXiv preprint arXiv:2406.05756, 2024. [12] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [13] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. [14] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In Proceedings of the European Conference on Computer Vision, pages 148166. Springer, 2024. [15] Ziyang Gong, Wenhao Li, Oliver Ma, Songyuan Li, Jiayi Ji, Xue Yang, Gen Luo, Junchi Yan, and Rongrong Ji. Space-10: comprehensive benchmark for multimodal large language models in compositional spatial intelligence. arXiv preprint arXiv:2506.07966, 2025. [16] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [17] Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, and Jiangmiao Pang. Egoexobench: benchmark for first-and third-person view video understanding in mllms. arXiv preprint arXiv:2507.18342, 2025. 9 [18] Kaiyuan Hou, Minghui Zhao, Lilin Xu, Yuang Fan, and Xiaofan Jiang. Tdbench: Benchmarking vision-language models in understanding top-down images. arXiv preprint arXiv:2504.03748, 2025. [19] Xiaohu Huang, Jingjing Wu, Qunyi Xie, and Kai Han. Mllms need 3d-aware representation supervision for scene understanding. arXiv preprint arXiv:2506.01946, 2025. [20] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [21] Yuheng Ji, Yipu Wang, Yuyang Liu, Xiaoshuai Hao, Yue Liu, Yuting Zhao, Huaihai Lyu, and Xiaolong Zheng. Visualtrans: benchmark for real-world visual transformation reasoning. arXiv preprint arXiv:2508.04043, 2025. [22] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. [23] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [24] Fei Kong, Jinhao Duan, Kaidi Xu, Zhenhua Guo, Xiaofeng Zhu, and Xiaoshuang Shi. Lrr-bench: Left, right or rotate? vision-language models still struggle with spatial understanding tasks. arXiv preprint arXiv:2507.20174, 2025. [25] Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. [26] Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimoal models, March 2024. URL https://github.com/EvolvingLMMs-Lab/lmms-eval. [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [28] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Joshua Adrian Cahyono, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [29] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [30] Jianing Li, Xi Nan, Ming Lu, Li Du, and Shanghang Zhang. Proximity qa: Unleashing the power of multi-modal large language models for spatial proximity analysis. arXiv preprint arXiv:2401.17862, 2024. [31] Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay Krishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations. arXiv preprint arXiv:2506.04633, 2025. [32] Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert Hawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, et al. Core knowledge deficits in multi-modal language models. arXiv preprint arXiv:2410.10855, 2024. [33] Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. Mmsci: multimodal multi-discipline dataset for phd-level scientific comprehension. In AI for Accelerated Materials Design-Vienna 2024, 2024. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/. [36] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision, pages 216233. Springer, 2024. [37] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 10 [38] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [39] Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso de Melo, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [40] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems, 37:9596396010, 2024. [41] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [42] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In Proceedings of the International Conference on Document Analysis and Recognition, pages 947952. IEEE, 2019. [43] OpenAI. Openai o3 and o4-mini system card, 2025. URL https://openai.com/research/o3-o4-mini-system-card. [44] OpenAI. GPT-5 System Card. Technical report, OpenAI, August 2025. Accessed: 2025-08-10. [45] Md Imbesat Hassan Rizvi, Xiaodan Zhu, and Iryna Gurevych. Spare: Single-pass annotation with reference-guided evaluation for automatic process supervision and reward modelling. arXiv preprint arXiv:2506.15498, 2025. [46] Zijian Song, Xiaoxin Lin, Qiuming Huang, Guangrun Wang, and Liang Lin. Siri-bench: Challenging vlms spatial intelligence through complex reasoning tasks. arXiv preprint arXiv:2506.14512, 2025. [47] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, et al. Parrot: Multilingual visual instruction tuning. arXiv preprint arXiv:2406.02539, 2024. [48] Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi Chen, Kai Zhang, Yunlong Zhang, Dan Wan, Xiaoxiao Lan, Mengyue Zheng, et al. Pathmmu: massive multimodal expert-level benchmark for understanding and reasoning in pathology. In Proceedings of the European Conference on Computer Vision, pages 5673. Springer, 2024. [49] Emilia Szymanska, Mihai Dusmanu, Jan-Willem Buurlage, Mahdi Rad, and Marc Pollefeys. Space3d-bench: Spatial 3d question answering benchmark. In Proceedings of the European Conference on Computer Vision, pages 6885. Springer, 2024. [50] ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [52] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [53] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. arXiv preprint arXiv:2504.01901, 2025. [54] Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench: Automatically generated spatial visualization reasoning tasks for mllms. arXiv preprint arXiv:2507.07610, 2025. [55] Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, and Boqing Gong. Site: towards spatial intelligence thorough evaluation. arXiv preprint arXiv:2505.05456, 2025. [56] Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. Spatial457: diagnostic benchmark for 6d spatial reasoning of large mutimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 2466924679, 2025. [57] Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, and Weidi Xie. Spatialscore: Towards unified evaluation for multimodal spatial understanding. arXiv preprint arXiv:2505.17012, 2025. [58] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. [59] Dawei Yan, Yang Li, Qing-Guo Chen, Weihua Luo, Peng Wang, Haokui Zhang, and Chunhua Shen. Mmcr: Advancing visual language model in multimodal multi-turn contextual reasoning. arXiv preprint arXiv:2503.18533, 2025. [60] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 1063210643, 2025. [61] Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, et al. Egolife: Towards egocentric life assistant. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 2888528900, 2025. [62] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [63] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. arXiv preprint arXiv:2506.21458, 2025. [64] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [65] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [66] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. URL https://arxiv.org/abs/2407.12772. [67] Yunhang Shen Yulei Qin Mengdan Zhang, Xu Lin Jinrui Yang Xiawu Zheng, Ke Li Xing Sun Yunsheng Wu, Rongrong Ji Chaoyou Fu, and Peixian Chen. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2021. [68] Zicheng Zhang, Haoning Wu, Chunyi Li, Yingjie Zhou, Wei Sun, Xiongkuo Min, Zijian Chen, Xiaohong Liu, Weisi Lin, and Guangtao Zhai. A-bench: Are lmms masters at evaluating ai-generated images? arXiv preprint arXiv:2406.03070, 2024. [69] Duo Zheng, Shijia Huang, Yanyang Li, and Liwei Wang. Learning from videos for 3d world: Enhancing mllms with 3d vision geometry priors. arXiv preprint arXiv:2505.24625, 2025. [70] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. [71] Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Xin Eric Wang, and Achuta Kadambi. Vlm4d: Towards spatiotemporal awareness in vision language models. arXiv preprint arXiv:2508.02095, 2025. [72] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [73] Xiaorong Zhu, Ziheng Jia, Jiarui Wang, Xiangyu Zhao, Haodong Duan, Xiongkuo Min, Jia Wang, Zicheng Zhang, and Guangtao Zhai. Gobench: Benchmarking geometric optics generation and understanding of mllms. arXiv preprint arXiv:2506.00991, 2025. [74] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023."
        },
        {
            "title": "A Detailed Evaluation Protocol",
            "content": "A.1 Evaluation Prompts As presented in Tab. 5, we summarize the metric, system prompt formats and output requirements used or compared in the original studies. Benchmark Metric System Prompt Output Format VSI-Bench [60] SITE [55] MMSI [62] OmniSpatial [22] MindCube [63] STARE [31] CoreCognition [32] Acc Acc SpatialViz [54] MRA, Acc Direct QA Single Letter Direct QA CAA Single Letter Single Letter Direct QA, Zero-shot CoT Acc Direct QA, Zero-shot CoT, Manual CoT Single Letter Acc Single Letter Direct QA Acc Template Direct QA, Zero-shot CoT Acc, F1 No Template Direct QA Template Direct QA, CoT Table 5 Overview of evaluation configurations for the eight representative spatial benchmarks. Columns list the reported metric(s), the system-prompt, and the required output format. Definitions of all metrics are provided in Appendix A.2. We adopt Direct QA, Zero-shot CoT, and Manual CoT following OmniSpatial [22], while SpatialViz Cot requires output think process. For output formats: Single Letter requires only the option label (e.g., AD); No Template means returning an answer with no extra requirements; Template requires wrapping the answer in specified pattern (e.g., <answer>...</answer>). Due to significant variations across works, we designed unified prompt for fair comparison, as illustrated in Fig. 4. Building on the observation from OmniSpatial [22] that chain-of-thought (CoT) reasoning outperforms Direct QA, we adopt the zero-shot CoT approach following OmniSpatial [22]. Additionally, to improve answer matching accuracy, we incorporate answer templates inspired by SpatialViz [54], requiring the model to enclose its responses within specific tags. Figure 4 System prompts used in our cross-benchmark evaluation. A.2 Evaluation Metrics In Table 2, we primarily use CAA from SITE [55] to calculate all MCQ scores. The CAA is computed as follows: CAA = (cid:32) (cid:88) i=1 Xi (cid:88) i=1 1 ni (cid:33) (cid:46) (cid:32) (cid:33) . (cid:88) i=1 1 ni (1) Here, denotes the total number of questions and ni represents the number of options for the i-th question.Let Xi = 1{ˆyi = yi}, where 1() is the indicator function.CAA = 1 indicates all questions were answered correctly, CAA = 0 matches random guessing, and CAA < 0 is worse than random. In VSI-Bench [60], for Numerical Answer questions, we follow the original paper and report the results using MRA: RA = 1 10 (cid:88) 1 θC (cid:18) ˆy (cid:19) < 1 θ , (2) where and ˆy represent the ground truth and prediction, respectively, while θ denotes the confidence threshold. prediction is considered correct only if the relative error rate ˆy y/y is below 1 θ. To ensure more reliable evaluation, MRA averages the scores across 10 thresholds, where = {0.5, 0.55, ..., 0.95}. And in detailed benchmark results reported in Appendix B, we report other mertics include Accuracy(Acc) and F1 score(F1). The F1 score is given by: F1 = 2 Precision Recall Precision + Recall , Precision = TP TP + FP , Recall = TP TP + FN , where TP, FP, and FN denote the number of true positives, false positives, and false negatives. For Acc, let yi and ˆyi denote the ground-truth and predicted labels for the i-th question. The Accuracy is defined as: ACC ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1 (ˆyi = yi) , Let denote the expected accuracy of random guessing on this set: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1 ni . Since Acc = 1 (cid:80)N i=1 Xi, the definition of CAA in (1) gives: (cid:80) (cid:80) CAA = Xi (cid:80) (cid:80) 1 ni 1 ni 1 = Xi 1 (cid:80) 1 1 (cid:80) 1 ni 1 ni = Acc 1 . Hence, for any two models evaluated on the same benchmark (so is fixed), their difference CAA: CAA = Acc 1 Acc = (1 r) CAA. (3) (4) (5) (6) (7) (8) 1 1r > 1, so CAA amplifies score differences relative to Acc on the same dataset. Implication. Because 0 < < 1, the factor The amplification is stronger when is larger (e.g., binary questions with 0.5), and weaker when is smaller (many-option questions)."
        },
        {
            "title": "B Detailed Benchmark Results",
            "content": "In this section, we provide detailed results on all eight benchmarks to facilitate direct comparison with the original papers. To maintain consistency, the reported metrics adhere to the definitions in their original papers. 14 B.1 Main Results (Metrics Aligned with Original Paper) The evaluation metrics include Accuracy(Acc) , Chance-adjusted Accuracy(CAA) , F1 score(F1) , and Mean Rank Accuracy(MRA) . Definitions for all four metrics are provided in Appendix A.2. Since our evaluations use Chain-of-Thought (CoT) style system prompt, the reported results may differ from those in the original papers. Models Metric Random Choice Proprietary Models Seed-1.6-2025-06-15 [50] Gemini-2.5-pro-2025-06 [51] GPT-5-nano-2025-08-07 [44] GPT-5-mini-2025-08-07 [44] GPT-5-2025-08-07 [44] Open-source Models Qwen2.5-VL-3B-Instruct [1] Qwen2.5-VL-7B-Instruct [1] Qwen2.5-VL-72B-Instruct [1] InternVL3-8B [72] InternVL3-78B [72] Human Evaluation Human VSI [60] SITE [55] MMSI [62] OmniSpatial [22] MindCube [63] STARE [31] CoreCognition [32] SpatialViz [54] MRA, Acc CAA 28.60 0.0 44.50 51.77 43.16 49.59 53.14 32.95 29.85 34.18 41.18 45.85 53.87 56.39 41.31 53.55 64.18 28.41 32.24 44.09 38.53 49. Acc 25.00 38.24 38.67 31.29 33.13 41.86 24.9 27.10 32.10 29.00 29.70 Acc 24. 50.10 61.58 53.83 56.66 62.70 40.70 39.01 48.01 46.31 51.40 Acc 32.35 45.87 47.05 35.10 46.63 47.59 33.85 30.19 31.73 37.50 33. Acc, F1 34.80 46.32 49.02 45.14 51.97 56.67 36.06 40.34 41.36 40.95 42.87 Acc 37. 68.06 82.27 69.81 78.62 79.44 49.85 56.00 70.40 60.69 68.54 Acc 25.08 32.46 46.94 31.36 37.71 36.97 21.95 27.71 29.83 29.49 32. 96.58 67.5 95.7 92.63 94.55 96. 86.98 82.46 Table 6 Evaluation on eight recent spatial benchmarks. Note that each metric in each column is consistent with the original paper, as well as the overall-score computation rule. Metrics across different columns are not directly comparable if they differ. MindCube denotes MindCube-Tiny. denotes minimum thinking. Dark purple highlights the best result and light purple indicates the second-best result within Proprietary and Open-source models, respectively. B.2 VSI-Bench GPT-5 ranks first or very close to the top across all evaluation metrics on VSI-Bench in Appendix B.2. On Metric Measurement (MM), GPT-5 effectively closes the humanmodel performance gap, achieving parity in Absolute Distance and surpassing human performance in Object Size and Room Size. This advantage likely derives from robust geometric priors acquired through large-scale training, similar to humans reliance on heuristic assumptions about typical object sizes. Nevertheless, across the remaining spatial intelligence capabilities such as Perspective-taking (PT) and Comprehensive Reasoning (CR), GPT-5 continues to underperform relative to humans, indicating that while its proficiency in basic geometric estimation is comparable to or exceeds human ability, it remains less adept at handling complex, dynamic, or transformation-intensive reasoning tasks. Models Avg. Numerical Answer Multiple-Choice Answer Obj. Count Abs. Dist Obj. Size Room. Size Rel. Dir Rel. Dis Route. Plan Appr. Order CR MM MM MM PT SR,MM CR CR Proprietary Models (API) Seed-1.6-2025-06-15 [50] 44.55 Gemini-2.5-pro-2025-06 [51] 51.77 GPT-5-nano-2025-08-07 [44] 43.16 GPT-5-mini-2025-08-07 [44] 49.59 GPT-5-2025-08-07 [44] 53.14 Open-source Models Qwen2.5-VL-3B-Instruct [1] 32.95 Qwen2.5-VL-7B-Instruct [1] 29.58 Qwen2.5-VL-72B-Instruct [1] 34.18 41.18 InternVL3-8B [72] InternVL3-78B [72] 45.85 37.36 44.94 47.30 51.14 53.61 29.50 26.62 19.54 59.77 50. 31.89 37.91 31.02 24.74 33.62 23.97 24.51 25.18 36.45 37.89 54.52 70.55 63.42 66.49 73.72 34.94 26.61 43.77 55.16 56.59 38.09 51.81 45.52 39.51 50.53 31.15 22.99 39.76 32.50 44. 36.67 54.37 54.23 44.11 41.97 30.83 56.90 43.18 43.15 63.73 41.84 35.63 39.98 34.08 38.17 37.40 42.68 42.56 43.90 51.41 42.78 43.30 34.54 46.63 41.24 31.96 28.35 28.87 29.38 30.93 Human Evaluation (Best Model,Human) Human -26.06 79. -34.53 94.3 -9.09 47.0 13.32 60.4 5.91 45.9 -30.97 94.7 -51.69 95. -49.17 95.8 60.68 67.31 50.65 68.16 65.53 34.63 33.50 40.78 30.91 51.29 -31.84 100.0 Table 7 Evaluation on VSI-Bench. Numerical Answer results are reported using MRA scores, while Multiple-Choice Answer (MCQ) results are reported using accuracy (Acc) scores. The overall score is computed as the simple average of these metrics, following the original paper. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those reported in the original paper. B.3 SITE Models Overall Count Loc 3D Inf MultiV Rel Mov - - MM,SR PT SR CR Proprietary Models Seed-1.6-2025-06-15 [50] 53.87 61.87 65.45 58.28 33.66 69.67 36.17 Gemini-2.5-pro-2025-06 [51] 56.39 59.51 71.12 54.24 36.47 72.83 47.27 GPT-5-nano-2025-08-07 [44] 41.31 48.30 54.23 46.10 9.50 56.81 19.57 GPT-5-mini-2025-08-07 [44] 53.55 57.15 64.18 51.62 35.54 68.94 44.70 64.18 66.45 73.34 59.89 59.88 74.64 47.05 GPT-5-2025-08-07 [44] Open-source Models 28.41 43.17 34.78 15.58 5.66 47.62 17.26 Qwen2.5-VL-3B-Instruct [1] Qwen2.5-VL-7B-Instruct [1] 32.24 47.85 42.06 19.35 9.21 53.68 14.28 Qwen2.5-VL-72B-Instruct [1] 44.09 54.20 56.62 42.90 18.40 65.63 26.72 38.87 53.45 49.05 38.98 9.73 58.19 23.86 InternVL3-8B [72] 49.42 64.73 61.90 56.65 12.85 70.68 33.93 InternVL3-78B [72] Human Evaluation (Best Model,Human) Human -3.32 67. 0.45 66 -9.96 5.19 83.3 54.7 -27.62 87.5 1.64 73 -5.23 52.5 Table 8 Evaluation on SITE. All reported values are (CAA) scores, aligned with the original paper. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those reported in the original paper. Tab. 8 shows results on SITE benchmark, following the official protocol. GPT-5 far outperforms all open-source models and is the only one to demonstrate strong performance in multi-view & cross-image reasoning\" category 16 (PT), particularly in egocentricexocentric view transitions. However, it remains less proficient in other forms of subject-centric viewpoint transformation. For example, reasoning about orientation and answering questions when hypothetically positioned next to specific object. Moreover, GPT-5 achieves human-level performance on Counting & Existence (Count), 3D Information Understanding (3D Inf), and Spatial Relationship Reasoning (Rel). However, we would like to refer to Tab. 2 and point out that SITE is the only dataset with human performance at 60, whereas others at >75 or even >90. B.4 MMSI Models Proprietary Models Seed-1.6-2025-06-15 [50] Gemini-2.5-pro-2025-06 [51] GPT-5-nano-2025-08-07 [44] GPT-5-mini-2025-08-07 [44] GPT-5-2025-08-07 [44] Open-source Models Qwen2.5-VL-3B-Instruct [1] Qwen2.5-VL-7B-Instruct [1] Qwen2.5-VL-72B-Instruct [1] InternVL3-8B [72] InternVL3-78B [72] Human Evaluation (Best Model,Human) Human Avg. 38.24 38.67 31.29 33.13 41.86 24.90 27.10 32.10 29.00 29.70 -53.84 95. Positional Relationship Attribute Motion Cam.Cam. Obj.Obj. Reg.Reg. Cam.Obj. Obj.Reg. Cam.Reg. Meas. PT PT PT PT PT PT MM 40.86 40.51 37.08 33.33 38. 21.51 25.81 22.58 23.66 33.33 -58.04 98.9 32.98 31.25 26.60 30.85 30.59 29.79 21.28 30.85 25.53 22.34 -64.52 97.5 34.57 36.76 25.32 23.46 36. 28.40 23.46 35.80 33.33 32.10 -57.44 94.2 36.05 50.00 29.07 32.56 44.71 26.74 27.91 23.26 33.72 18.60 -48.8 98.8 42.35 36.76 35.29 29.41 43. 24.71 37.65 38.82 32.94 34.12 -53.19 96.4 51.81 49.30 35.37 48.19 64.20 28.92 28.92 30.12 34.94 24.10 -31.1 95.3 56.25 57.14 43.75 53.12 60. 29.69 31.25 43.75 32.81 43.75 -37.56 98.5 Appr. MR 30.77 22.64 20.00 36.92 28.81 15.15 22.73 27.27 18.18 28. -61.68 98.6 Cam. PT 29.73 31.67 28.38 18.92 34.25 14.86 24.32 33.78 17.57 29.73 -64.45 98. Obj. PT 40.79 29.85 30.26 32.89 38.16 25.00 32.89 34.21 35.53 32.89 -56.21 97.0 MSR CR 33.33 38.31 32.07 31.31 41.29 25.76 25.25 33.84 29.29 30.30 -55.91 97.2 Table 9 Evaluation on MMSI. All reported values are (Acc) scores, aligned with the original paper. To ensure fair benchmark comparison, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those reported in the original paper. In Tab. 9, our results show minimal differences among proprietary and open-source models, with overall performance still far below human level. Existing models remain limited in their ability to handle viewpoint transformations, particularly tasks requiring them to be hypothetically positioned next to specific object and reason from that objects perspective, highlighting persistent weaknesses in Perspective-taking (PT). To ensure fair benchmark comparison, this table uses unified prompt ( Appendix A.1). Hence, some results may deviate from those reported in the original paper. B.5 OmniSpatial In Tab. 10, we observe that proprietary models generally outperform their open-source counterparts, with substantial performance gaps of approximately 1020 points. The CR and PT tasks exhibit the largest disparities between model and human performance and also represent the lowest-scoring tasks overall. We highlight that across all baselines, the worstperforming subtasks are concentrated under Complex Logic and Perspective Taking, except for the Ego-Centric subtask. However, despite being categorized as PT in the OmniSpatial paper, this subtask primarily involves analyzing objects 2D relative positions, counts, and other aspects from single image, without necessitating any spatial capabilities, and therefore should not be regarded as an SI task. For fair comparison across benchmarks, this table uses unified prompt ( Sec. 2.3). Hence, some results may deviate from those reported in the original paper. 17 Models Dynamic Reasoning Spatial Interaction Complex Logic Perspective Taking Avg. Manipulate Motion Analysis Traffic Analysis Locate Geospatial Strategy Pattern Recognition Geometric Reasoning Ego Centric Allo Centric Hypothetical - MM,CR CR - - CR CR - PT PT Proprietary Models Seed-1.6-2025-06-15 [50] 50.10 Gemini-2.5-pro-2025-06 [51] 61.58 GPT-5-nano-2025-08-07 [44] 53.83 GPT-5-mini-2025-08-07 [44] 56.66 GPT-5-2025-08-07 [44] 62.70 Open-source Models Qwen2.5-VL-3B-Instruct [1] 40.70 Qwen2.5-VL-7B-Instruct [1] 39.01 Qwen2.5-VL-72B-Instruct [1] 48.01 46.31 InternVL3-8B [72] InternVL3-78B [72] 51.40 63.51 62.16 55.07 71.62 68.92 60.81 55.41 66.22 68.92 67.57 63.01 69.94 64.24 65.61 69.86 38.73 31.21 61.27 55.49 62. 56.47 68.57 69.41 73.33 56.47 72.82 62.35 77.14 81.90 69.41 48.24 45.71 52.94 50.48 52.94 59.05 58.82 45.71 54.12 55.24 59.09 67.89 56.07 69.09 74.29 50.91 46.36 54.55 60.91 58.18 31.96 38.14 32.14 45.16 43.86 17.53 21.65 27.84 24.74 38. 30.97 39.46 33.33 28.86 46.05 32.90 28.39 29.03 27.74 38.06 75.49 35.64 84.31 38.56 79.41 40.92 81.37 45.21 82.35 48.48 55.88 36.97 62.75 35.11 78.43 32.98 67.65 34.04 38.56 78.43 33.73 37.35 40.24 42.17 47.56 43.37 46.99 38.55 45.78 40. Human Evaluation (Best Model,Human) Human -29.6 92.3 -24.91 96.53 -27.36 97.30 -15.24 -23.53 92.94 97.14 -20.26 94. -46.14 91.30 -41.58 87.63 -47.26 -14.71 99.02 95.74 -46.42 93.98 Table 10 Evaluation on OmniSpatial. All reported values are (Acc) scores, aligned with the original paper. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those originally reported by the official paper. B.6 MindCube Models Avg. Rotation Among Around PT PT PT Proprietary Models 33.90 36.00 Seed-1.6-2025-06-15 [50] Gemini-2.5-pro-2025-06 [51] 47.05 85.50 25.95 38.40 GPT-5-nano-2025-08-07 [44] 35.10 38.50 33.05 37.20 GPT-5-mini-2025-08-07 [44] 46.63 84.50 37.12 38.80 47.59 93.33 34.17 41.63 GPT-5-2025-08-07 [44] 45.87 93.50 Open-source Models Qwen2.5-VL-3B-Instruct [1] 33.85 28.00 36.95 31.20 Qwen2.5-VL-7B-Instruct [1] 30.19 26.00 30.17 33.60 33.90 28.40 Qwen2.5-VL-72B-Instruct [1] 31.73 29.50 37.50 26.00 InternVL3-8B [72] 36.00 42.03 33.94 28.50 37.12 30.80 InternVL3-78B [72] Human Evaluation (Best Model,Human) Human -46.96 94.55 - - - - - - Table 11 Evaluation on MindCube. All reported values are (Acc) scores, aligned with the original paper. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those originally reported by the official paper. In Tab. 11, we observe that proprietary models generally outperform the open-source ones. It is interesting to find out that amongst the three subtasks, proprietary models performs no significantly better than open-sourced counterparts on Among\" and Around\", but the Rotation\" task exhibits pronounced disparity between model families, with leading closed-source systems (e.g., GPT-5 [44], Seed [50], Gemini [51]) achieving accuracy around 8595 points. Despite the high performance, we point out that the Rotation\" task involves relatively simple camera transformation: the 18 camera remains fixed in position while rotating in place, eliminating the need for mental translation of viewpoints. Consequently, the task reduces primarily to determining the angular differences between perspectives, which is restricted to discrete values of 90 (left/right) and 180. B.7 STARE Models Overall 2D Trans. 3D Trans. Cube Net Tangram - CR DA - VSim VSim VSim VSim VSim VSim VSim VSim Temporal Perspective PT PT Proprietary Models Seed-1.6-2025-06-15 [50] 46.32 44.76 Gemini-2.5-pro-2025-06 [51] 49.02 48.51 GPT-5-nano-2025-08-07 [44] 45.14 40.64 GPT-5-mini-2025-08-07 [44] 51.97 55.09 50.16 GPT-5-2025-08-07 [44] 56.67 Open-source Models 36.06 21.91 Qwen2.5-VL-3B-Instruct [1] Qwen2.5-VL-7B-Instruct [1] 40.34 27.39 Qwen2.5-VL-72B-Instruct [1] 41.36 31.30 40.95 27.86 InternVL3-8B [72] InternVL3-78B [72] 32.55 42. 50.59 48.70 39.69 56.26 60.93 26.24 28.37 34.75 32.15 38.53 34.97 35.13 35.93 35.46 36.05 27.78 27.45 32.19 31.54 28.43 34.31 35.78 32.33 37.50 37.63 24.02 30.64 27.70 29.41 32. 71.31 58.76 59.65 67.13 47.06 62.86 63.54 65.96 65.51 67.68 67.05 69.64 75.16 71.52 88.89 48.89 67.84 65.54 66.29 66.29 54.09 58.19 70.31 74.35 66.13 57.19 61.32 54.51 52.97 58. 62.39 37.79 30.40 70.00 47.77 34.00 60.41 37.78 26.00 70.99 46.50 31.20 55.44 48.00 86.27 51.28 32.70 23.60 54.48 33.12 28.40 43.40 39.28 31.20 58.47 37.58 26.00 49.79 36.09 33.60 Human Evaluation (Best Model,Human) Human -39.91 -39.83 96.50 95.00 -36.07 97.00 -59.95 96. -59.87 97.50 -27.69 99.00 -10.11 99.00 -13.15 87.50 -7.73 -50.4 -42.66 94.00 98.10 98.40 Table 12 Evaluation on STARE. MCQ results are reported using accuracy (Acc) scores, while binary yes/no tasks (including CubeNet and Tangram) are evaluated using the F1 score. The overall evaluation metric is the macro-average performance across all tasks, following the original paper. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those originally reported by the official paper. We present results on STARE [31] in Tab. 12. Proprietary models exhibit pronounced advantage across all tasks, with an average gap of approximately 20 points compared to open-source models. Notably, GPT-5 demonstrates strong ability to leverage information from visual simulations (VSim), whereas other models tend to underperform in this regard. This suggests that strong base model is essential for effectively utilizing such information, aligning with recent findings in the literature [62]. In the Cube Net task, which requires determining whether given 2D net can be folded into 3D cube, GPT-5 initially scored relatively low at 47.06 points. However, when provided with visual simulation image input, its accuracy rose sharply to 88.89 points, approaching human-level performance. This substantial improvement indicates that visual simulation significantly facilitates decision-making and that GPT-5 possesses strong capacity for understanding processes depicted across multiple images. Furthermore, model performance on SI tasks remains consistently lower than on non-SI tasks, consistent with our observations from OmniSpatial ( Tab. 10). B.8 CoreCognition From Tab. 13, we observe that, regardless of whether the tasks involve spatial intelligence, proprietary models generally outperform open-source models. In the Formal Operation category (non-SI task), several models exceed human performance. Notably, open-source models exhibit high sensitivity to prompt variations, with accuracy fluctuations of up to 15 points when the prompt differs from that used in the original paper. In the Perspective-taking (PT) sub-task, GPT-5 substantially outperforms all other proprietary and open-source models, yet still remains far below human performance. Models Avg. Sensorimotor Concrete Operation Formal Operation Boundary - Continuity - Permanence - Spatiality SR Perceptual Constancy - Intuitive Physics - Perspective Taking PT Conservation - Hierarchical Relation - Intentionality Understanding - Mechanical Reasoning - Tool Using - Proprietary Models Seed-1.6-2025-06-15 [50] 68.06 Gemini-2.5-pro-2025-06 [51] 82.27 GPT-5-nano-2025-08-07 [44] 69.81 GPT-5-mini-2025-08-07 [44] 78.62 79.44 GPT-5-2025-08-07 [44] Open-source Models Qwen2.5-VL-3B-Instruct [1] 49.85 Qwen2.5-VL-7B-Instruct [1] 56.00 Qwen2.5-VL-72B-Instruct [1] 70.40 60.69 InternVL3-8B [72] 68.54 InternVL3-78B [72] 80.79 87.34 82.10 85.59 80.90 69.00 72.05 84.72 77.73 88.65 71.07 72.73 69.01 62.81 54.69 45.92 50.34 69.42 70.25 73.97 Human Evaluation (Best Model,Human) Human -4.71 86.98 2.94 85.71 -4.92 78.89 55.00 55.00 35.00 60.00 62.50 6.06 28.79 50.00 37.50 37. -25.6 88.10 45.82 69.45 43.68 63.96 57.72 32.48 40.84 47.26 31.98 40.33 89.93 92.71 88.19 92.71 92.31 48.55 63.87 87.85 90.28 86.81 52.28 72.23 59.05 70.04 78. 29.49 46.15 60.00 50.00 58.33 48.59 47.94 43.17 59.22 68.65 23.43 28.63 24.95 26.25 31.02 -6.12 75.57 2.01 90.70 -12.58 91. -23.34 91.99 25.35 97.24 70.97 85.71 92.38 68.12 86.46 83.87 83.87 76.04 8.35 88.89 69.71 87.06 69.71 73.82 82.52 51.18 62.65 67.65 62.94 77. 15.18 71.88 82.97 95.66 83.72 90.98 88.12 67.07 47.94 80.88 73.28 81.62 13.68 81.98 75.00 90.93 67.65 81.62 79.82 53.49 60.85 82.16 59.75 79. 93.08 99.82 96.17 98.91 99.19 92.53 89.80 98.72 69.22 82.70 3.21 87.72 7.95 91.87 Table 13 Evaluation on CoreCognition. Results are reported using the Soft circular scoring mentioned in Section 2.3. potential misalignment may occur as human scores are measured by non-circular accuracy, while model scores are based on soft-circular accuracy. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those originally reported by the official paper. B.9 SpatialViz Models Avg. Mental Rotation Mental Folding Visual Penetration Mental Animation 2DR - 3DR MR 3VP MR Avg - PF DA CU DA CR DA Avg - CS DA CC - CA DA Avg - AM - BM SR MS CR Avg - Proprietary Models Seed-1.6-2025-06-15 [50] 32.46 25.00 16.25 47.00 30.77 25.83 30.00 30.83 28.89 35.00 35.83 16.25 30.63 61.25 16.25 48.75 42.08 Gemini-2.5-pro-2025-06 [51] 46.94 68.75 31.25 47.96 49.22 41.67 25.83 33.33 33.61 46.67 64.17 38.75 51.25 85.00 37.50 53.75 58.75 GPT-5-nano-2025-08-07 [44] 31.36 47.50 20.00 37.00 35.00 15.00 27.50 26.67 23.06 24.17 40.00 45.00 35.31 27.50 25.00 51.25 34.58 GPT-5-mini-2025-08-07 [44] 37.71 76.25 33.75 33.00 46.54 10.00 29.17 35.00 24.72 32.50 35.00 51.25 38.12 61.25 31.25 48.75 47.08 GPT-5-2025-08-07 [44] 36.97 42.50 28.75 35.00 35.38 21.05 27.50 35.83 28.25 40.83 43.33 35.00 40.31 78.95 23.33 48.05 49.48 Open-source Models Qwen2.5-VL-3B-Instruct [1] 21.95 17.50 22.50 23.00 21.15 24.17 9.17 14.17 15.83 21.67 22.50 28.75 23.75 17.50 33.75 37.50 29.58 Qwen2.5-VL-7B-Instruct [1] 27.71 20.00 10.00 37.00 23.46 34.17 28.33 28.33 30.28 15.83 30.00 32.50 25.31 22.50 28.75 43.75 31.67 Qwen2.5-VL-72B-Instruct [1] 29.83 27.50 32.50 32.00 30.77 25.83 17.50 22.50 21.94 20.00 39.17 46.25 33.75 30.00 30.00 46.25 35.42 29.49 20.00 32.50 27.00 26.54 16.67 15.83 30.83 21.11 20.83 45.00 40.00 34.69 25.00 38.75 51.25 38.33 InternVL3-8B [72] 32.12 33.75 22.50 36.00 31.15 21.67 15.00 30.83 22.50 26.67 43.33 55.00 40.00 31.25 35.00 45.00 37.08 InternVL3-78B [72] Human Evaluation (Best Model,Human) Human -27.5 -24.17 -5.00 -48.75 -33.75 -29.58 -35.52 -13.75 -45.41 -39.54 -36.34 -52.08 -45.00 -37.09 -46.95 -26.25 -6.66 82.46 90.00 79.16 87.50 85.56 93.75 75.00 72.92 80.56 72.92 70.83 82.50 75.42 90.00 87.50 87.50 88.33 Table 14 Evaluation on SpatialViz. All reported values are (Acc) scores, aligned with the original paper. Note that GPT-5-2025-0807 [44] is tested with minimal thinking. For fair comparison across benchmarks, this table uses unified prompt (Appendix A.1). Hence, some results may deviate from those originally reported by the official paper. From Tab. 14, we observe that proprietary models generally outperform open-source models. On non-SI tasks such as Arrow Moving (AM) and Cube Counting (CC), model performance approaches human-level accuracy; however, on spatial tasks, all models still lag substantially behind humans, with Paper Folding (PF), representing Deformation and Assembly (DA), showing the largest gap and underscoring its difficulty as spatial intelligence challenge. In Cube Counting (CC), Gemini-2.5 achieves performance close to human levels, whereas GPT-5 shows no notable advantage over open-source models. Notably, GPT-5 performs poorly on Paper Folding (PF), even underperforming compared to open-source baselines."
        },
        {
            "title": "C Elaborated Results in the Case Study",
            "content": "In this section, we showcase some typical test cases and GPT-5-thinkings response. Correct reasoning is marked green, whereas problematic reasoning is marked red. Metric Measurement (MM) Prompts: What is the height of region 1 in meters? GT: 2.7m. Model Responses: Ill identify \"Region 1\" using the bounding box shown in the image, and then estimate its height through pixel-to-meter conversion, based on the datasets expected measurements. Im not sure if theres scale involved or if its more straightforward. Without numeric labels, it seems \"Region 1\" just refers to bounding box, likely the garage door area. If had to guess, the height could be around 2 meters, which is typical for garage doors, though Im not fully certain. The task might be from dataset used in visual language models, like those evaluating layout and measurements. The region height could potentially be 2 meters, especially since it appears to be garage door common dimension in Australia. Im leaning towards this answer, assuming no scale is required. Answer: 2m. Metric Measurement (MM) Prompts: In Figure 1, comparing the brown door at the bottom right corner with the door of the house with brown roof and gray walls in Figure 2, which door is taller? Options: A: The door in Figure 2, B: The door in Figure 1, C: Sometimes the one in Figure 1 is taller, sometimes the one in Figure 2 is taller, D: They are the same height GT: Model Responses: need to examine two images and compare the height of doors. In the first image, theres brown door located at the bottom right corner of the gray building labeled PIZZERIA PINOCCHIO. In the second image, the building with brown roof and gray walls seems to be the one with brown shingled roof and light gray walls. Now, Ill focus on comparing these details. Fig2s door seems taller, though there could be scale considerations. But since the question asks for one, Ill go with Fig2. Answer: 21 Mental Reconstruction (MR) Prompts: generate realistic front view of this chair. GT: - Model Responses: The user asked for realistic front view, so Ill set the neutral gray studio background. The content needs to stay non-sensitive, and need to follow the style guidelinesavoiding extra text since the tool output will be blank. Ill go ahead and generate the image without asking for further clarification unless specifically needed. This isnt an editing taskits about generating the image from scene description. Including the image ID is optional, given the tool doesnt seem to require it. Ill focus on describing the chair features in detail to ensure it captures everything accurately in the generated image. Answer: Mental Reconstruction (MR) Prompts: Generate 90 degrees top-down view of this scene. GT: - Model Responses: need to ensure the objects are positioned correctly in the top-view image. From the users image, theres gold cube slightly left of center, brown cylinder to the right, and big green cylinder at the back. Theres also small green cylinder in front of the big one. Ill carefully note the purple cylinder and small cyan sphere near the left. Ill create detailed prompt for the image: top-down view of small tabletop scene. Itll include objects like gold rounded cube, brown and green cylinders, and teal sphere. Ill specify soft studio lighting, smooth gray floor, and realistic materials with ray traced reflections and shadows. Ill specify the camera position directly above for top view. Ill also detail the scale and relative positioning of the objects, as seen in the image. Answer: 22 Mental Reconstruction (MR) Prompts: Given the front, side and top-down view of 3D object, analyze its structure and reconstruct it in 3D axis. GT: - Model Responses: Im analyzing the first image: the front view shows base with width of 30 and thickness of 6. The 21 indicates the total height from the base to the top. Theres also 6 indicating base thickness. Above the base is U-shaped bracket with side walls of 15 height, and central inner pillar thats smaller but with horizontal dimension of 6. Ill need to check the top view for clarification. In the top view, see that the width is 18, and theres recess near the top. The depth is 20, shown in red, and the thickness of the recessed shelf is 6 vertically. The shape resembles \"T\"-slot, which aligns with the front views central pillar. The overall dimensions match the front view, where width is likely 30 and depth is 20. The object seems symmetrical with base of 30x20x6. On top, theres rectangular frame in U-shape, surrounding central pillar. The side view shows triangular gusset with 45-degree angle. This results in varying heights across different views, but overall, it fits block with central T-shaped ridge and sloped sides. The structure should include base with dimensions 30x20x6, crossbar (18x6x15), and central pillar (6x?x10). The front view indicates horizontal line at 21 along the width 18, with lower 16 at the central region. The side ramp slopes from 21 to 6 based on the defined measurements. The ramp is triangular prism with base at z=6 (y=4) and sloping up to z=16 at y=14, with width of 6 units. The structure, including the base, ramp, and crossbar, aligns with the top views shape and the front views height relationships. Answer: Mental Reconstruction (MR) Prompts: Draw 3D shape whose top view looks like this, and composed of 3D cubes. GT: - Model Responses: The user requested 3D object with top view resembling an L-tetromino, made out of cubes. Its 4-square footprint, with varied possible heights. The simplest format is an isometric view with single cube stacked. Ill create the requested 3D shape and generate the image without needing extra edits or references. Answer: 23 Mental Reconstruction (MR) Prompts: which option is the correct top-down view of the above object? GT: Model Responses: Looking at the 3D arrangement of cubes, it seems to form an L-shape on the bottom plane with 4 cubes. The leftmost front cube is orange, the middle one is purple, and the rightmost is cyan. The green cube seems to sit above the purple, slightly back and left from it. Option shows an shape with four squares: green above orange, followed by orange, purple, and cyan in horizontal line. Option also forms an shape, but with green at the top-left corner. Option only has three squares (green, purple, and orange). Option shows three squares as well, with green placed above orange in the left column, and cyan at the bottom right. Only Option and show four squares. The 3D structure seems to have four visible cubes, plus one stacked on top. The green cube appears stacked above, while the base includes three visible cubes: orange, purple, and cyan. Its possible theres hidden base cube under green, but this depends on visibility. The bottom row in Option suggests that the cubes are placed in straight row: orange, purple, and cyan. For the green cube, its position above the leftmost cube (likely orange) is questioned by the visibility of each cubes top face. The green cube appears to sit behind orange and purple at junction, but wouldnt fully obstruct oranges top face. The green cube isnt stacked on any of the base cubes since we can see the tops of purple and orange. The green is positioned behind these cubes, elevated, making it appear higher due to perspective. It likely sits on the ground, aligning with the top rows cubes, but its behind them geometrically. Option correctly shows green above orange. Option B, however, places purple on top next to green, which seems like misplacement when compared to the first. Answer: three cubes in row (orangepurpleblue) with one green cube behind the orange. Mental Reconstruction (MR) Prompts: Rotate the leftmost image along the red line by the direction of the arrow. if rotate 90 degrees, which of will it be like? Consider the shape as thin layer in 3D space, and the rotation is in 3D space. GT: Model Responses: The red line is along the plates lower-right corner,with an arrow indicating clockwise rotation. This suggests the left side of the plate moves downward, away from the viewer. Initially flat, the plate rotates to stand vertically, with the triangular holes orientation adjusting accordingly. Now, Ill focus on matching it with the options. The correct option is the one where the triangle is not mirrored across the pivot edge. After rotation, the pivot edge becomes the front vertical edge, but the triangles orientation relative to the pivot stays the same. In option C, the triangular base is vertical, and its apex points toward the right, which seems like mirror of the original triangles layout. The original apex pointed up and to the right, while in C, it points similarly to the right edge. Given the pivot seems to align with the right edge in vertical configurations, appears plausible. Answer: 24 Spatial Relations (SR) Prompts: Which object is higher in the 3D world space, the clock or the house in the back? GT: The house in the back. Model Responses: The question is asking which object is higher in physical elevation, the clock tower or the house behind it, not just perceived height. The clock tower seems taller and extends above the house roof, even factoring in the potential slope of the ground. So, the clock tower is likely the higher object in 3D world space. Answer: The clock. Spatial Relations (SR) Prompts: Consider the real-world 3D locations and orientations. Is the blue bridge parallel or perpendicular to the train with orange stripes? GT: Perpendicular. Model Responses: In considering the 3D world orientation, not the 2D perspective, the blue bridge crosses above the canal perpendicular to the train, which runs along the canal. While the bridge may appear diagonal in the image, its orientation is most likely perpendicular to the train. There might be some nuance, but overall, its best categorized as perpendicular. The blue bridge, which crosses the canal, has triangular support that runs perpendicular to the orange-striped train track. Though the bridge may appear to cross the track at an angle, its still generally perpendicular. So, the final answer is that the blue bridge is perpendicular to the train. Initially, thought the blue bridge crossed the canal, and the orange train ran along it, but it turns out the orange train also crosses the canal in the same direction. Hmm, the train at the station may be oriented along the canal, but there seems to be some visual illusion. Looking at the full scene, the orange-striped train does indeed curve from right to left, crossing the canal below the yellow train on the blue bridge. The two tracks are not perpendicular, but rather nearly parallel, both aligned left to right. The blue bridge deck slopes slightly, but overall, they run along the same direction. Answer: Parallel. Perspective-taking (PT) Prompts: The images are frames from video. The first image is from the beginning of the video and the second image is from the end. Is the camera moving left or right when shooting the video? GT: Left. Model Responses: OK, so the camera seems to be moving around the bottle. In the first image, we see more of the labels left side and small circle \"72\". In the second image, the front label (\"每益添\") faces more towards us. This suggests the camera moved clockwise to the right, revealing more of the bottles front. Both shots show the same stack of boxes, but the perspective shifts. If the camera is moving right, the background would shift left relative to the bottle. Im comparing the handle slots position relative to the bottle in each image. The first image places the handle on the right, while the second places it on the left, suggesting the camera moved. Answer: Right. 25 Perspective-taking (PT) Prompts: You are given two first-person perspective images, representing the start and end frame of video clip: <image><image>. And then there are four third-person perspective frames, each captured during the video clip but in random temporal order: <image ><image ><image><image>. Analyze the images carefully based on scene details, lighting, and object positions, and determine which option gives the correct temporal order of these 4 third-person perspective frames. GT: 3 4 2 1. Model Responses: The first-person start frame shows the man reaching near the blue stand, possibly indicating his location. The end frame shows him facing the window with the bike still on the stand. The first-person frames help establish the timeline, implying that the thirdperson frames should be ordered based on his movement from entering the frame to adjusting the bike. He starts on the far right of the room, turning toward the bike stand (frame 2), then walks left past the front wheel (frame 1). Next he stops at the stand and reaches down to adjust it (frame 4), and finally hes positioned behind the front wheel inspecting/working on the bike (frame 3). Answer: 2 1 4 3. Perspective-taking (PT) Prompts: Based on these four images (image 1, 2, 3, and 4) showing the red wooden chair from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: If am standing at the same spot and facing the same direction as shown in image 3, then turn left and move forward, will get closer to the corridor and black chair? GT: Yes. Model Responses: From image 3 (back view), turning left puts you facing the orientation of image 4 (right-side view). The corridor with the black chair is in image 2 (left-side view). So after turning left youd head the opposite way, not toward the corridor/black chair. Answer: No. 26 Deformation and Assembly (DA) Prompts: Flip the shape in image 1 to form 3D cube. Which of the image 2, 3, 4, 5 is possible view of the formed cube? GT: Image 4. Model Responses: Reason: From the net, opposite faces are: triangle (top) v.s. small diamond (bottom), big diamond v.s. small square, long diamond v.s. the split-triangle face. Any valid cube view must show three faces that meet at one corner (no opposites touching). Image 3 shows triangle next to the small diamond (opposites) impossible. Image 4 puts the small square next to the big diamond (opposites) impossible. Image 5 has the long diamond on top even though the top is the triangle impossible. Only Image 2 has three mutually adjacent faces in configuration consistent with the net. Answer: Image 2. Deformation and Assembly (DA) Prompts: How to use the first three to build the last structure? show draft image. GT: - Model Responses: Heres quick draft showing how to place the pieces: the long 212 plate is the base, the rounded 12 caps the far-left end, and the 22 corner/step sits directly to its right. Answer: Deformation and Assembly (DA) Prompts: For each of to E, analyze if it can be built by stacking the two structures in the first image. GT: B, Model Responses: We have 2 cubes + 3 cubes total of 5 cubes must be used. No. Only 4 cubes shown. Yes. Exactly 5 cubes. No. Shows 6 cubes (a row of 4 with column of 3 sharing one). No. Only 4 cubes. Yes. Exactly 5 cubes. Answer: 27 Deformation and Assembly (DA) Prompts: Which of is possible to be built when rotating and combining the two 3D structure in Image 1? GT: Model Responses: and are possible; is not. Reason: Each piece is an L-shaped tri-cube. In options and B, the pink fits and the remaining white cubes form the second L. In option C, after placing the pink L, the remaining white cubes cant make another (youre left with only two adjacent whites with no spot for the third), so that shape cant be built. Answer: and B. Comprehensive Reasoning (CR) Prompts: How many 3D blocks in the image? GT: 8 Model Responses: 8 visible cubes, with some potential hidden ones for support. estimate there are at least 10 cubes. After double-checking, Im sticking with 9 cubes. Bottom row has 3 (green, blue, red); back row shows 1 left (purple) and 2 on the right (orange, red); theres tall blue stack behind the center thats 2 cubes high (only the top is visible, one is hidden); plus the green cube on top = 3 + 3 + 2 + 1 = 9 The front faces match too, so think this count works. Answer: 9 28 Comprehensive Reasoning (CR) Prompts: You are robot beginning at the door facing the brown single-seat sofa. You want to navigate to the chair next to the water fountain. You will perform the following actions (Note: for each [please fill in], choose either turn back, turn left, or turn right.): 1. Go forward until the brown single-seat sofa. 2. [please fill in] 3. Go forward until passing by the brown two-seats sofa. 4. [please fill in] 5. Go forward until the chair. You have reached the final destination. [A. Turn Left, Turn Left, B. Turn Back, Turn Right, C. Turn Left, Turn Right, D. Turn Right, Turn Left] GT: Model Responses: The robot starts facing brown armchair, and need to deduce the best turns to reach the chair near the water fountain. Ill review images 2, 8, and 9 for more clarity on the layout and confirm if the chair by the fountain is in these frames. Frame 4 shows brown armchair near white wall, but its not clear to me what the context is. Frame 5 depicts two single chairs with table, and frame 6 shows two ottomans, possibly being part of two-seat sofa. Frame 8, 9 all display water fountains and the chair location. need to figure out the robots orientation based on the images. Frame 9 shows the water fountains, and the rug and furniture give clues. After step 1, the robot faces brown single-seat sofa. Step 2 requires turn, and step 3 means passing by the two-seats sofa along the red wall. Deciding whether to turn left or right is key. Im trying to figure out where everything is relative to each other. think the route from the armchair to the water fountain involves crossing rug and passing by the two-seats sofa, which might be along the red wall. Im considering the layout and analyzing different options for navigating from the door to the chair next to the water fountain. Option Dturning right, passing by the two-seats along the red wall, then turning left toward the water fountainseems to align well with the scenes features. Im leaning toward it as most probable. Im exploring the pathway to the water fountain, considering turns and placements. Based on the layout, it seems that Option Cturn left to pass the two-seats, then turn right toward the water fountainaligns well with the scenes features. It makes sense based on the furniture arrangement and directionality in the frames. Answer: C. Turn Left, Turn Right"
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research"
    ]
}