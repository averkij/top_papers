{
    "paper_title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
    "authors": [
        "Yuechen Zhang",
        "Jinbo Xing",
        "Bin Xia",
        "Shaoteng Liu",
        "Bohao Peng",
        "Xin Tao",
        "Pengfei Wan",
        "Eric Lo",
        "Jiaya Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 6 8 6 1 . 5 0 5 2 : r Training-Free Efficient Video Generation via Dynamic Token Carving Yuechen Zhang1 Jinbo Xing1 Bin Xia1 Shaoteng Liu1 Bohao Peng1 Xin Tao3 Pengfei Wan3 Eric Lo1 Jiaya Jia2, 1CUHK 2HKUST 3Kuaishou Technology 4SmartMore Project page: https://julianjuaner.github.io/projects/jenga Figure 1: Jenga generates high-quality videos with an efficient DiT inference pipeline. (a): (b): We minimize token Extremely sparse attention can preserve details in generated videos. interactions via dynamic sparse attention with progressive resolution design. We present videos generated by Jenga (sub-sampled 48 frames) among different models, marked with the DiT latency and relative speedup rate. Please use Adobe Acrobat Reader for live video visualization. Abstract Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83 speedup with 0.01% performance drop on VBench). As plug-andplay solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to secondswithout requiring model retraining. Code: https://github.com/dvlab-research/Jenga Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "The advancement of Latent Diffusion Models [1, 2, 3, 4, 5, 6, 7] has significantly propelled the development of image and video generation. Recently, Diffusion Transformers (DiT) [8, 9, 10, 11, 12, 13, 14, 15, 16] have emerged as the predominant architecture for foundation models due to their inherent scalability and superior generative capabilities. As high-resolution video generation techniques continue to advance and DiT-based models scale to unprecedented sizes, the computational efficiency of generating high-quality content has become critically important. For example, generating mere 5-second 720P video using HunyuanVideo [12] on single NVIDIA H800 GPU requires approximately 27 minutes, severely limiting its practical applications in real-world scenarios. This challenge stems from two orthogonal factors: (1) Self-Attention versus massive token length . The continuously increasing token length for high-resolution generation causes computational bottleneck due to the O(N 2) computational complexity of self-attention in Transformers. Even with efficient attention mechanisms [17], self-attention in HunyuanVideo [12] still consumes 77.8% of the total processing time. (2) The multi-step nature of Diffusion models. The denoising process requires forwarding through the DiT architecture times, introducing -fold computational overhead compared to non-diffusion models [18, 19] of similar specifications. To address these challenges, various approaches have been explored. One branch focuses on operator-based acceleration, particularly attention optimization, to eliminate computational bottlenecks. STA [20], CLEAR [21], and SVG [22] predefine head-aware attention sparsity patterns in temporal or spatial dimensions. However, these approaches inadequately account for dynamic variations in attention patterns across inputs and achieve only modest speedup ratios (1.52), insufficient for practical deployment. Orthogonal approaches optimize the diffusion generation pipeline through distillation [23, 24, 25, 26], quantization [27, 28, 29], or feature reuse [30, 31, 32]. However, distillation incurs significant training costs while often degrading output quality. Similarly, feature reusing and quantization methods also face limitations in achieving adequate acceleration ratios necessary for practical applications. Based on the two orthogonal factors identified, we propose Jenga, progressive, fully sparse inference pipeline with dynamic, generalizable Attention Carving kernel. Studies have shown that the diffusion denoising process progresses from low to high-frequency generation [33, 34], where earlier steps establish content structures while later steps refine details. The core idea of Jenga is: Early denoising steps do not require high-resolution latents, while later steps do not require dense full attention. Once video content is established, the inherent redundancy in video latents means that not every token must participate in attention computations; at high resolutions, attention is inherently sparse, and fine details can be generated without full attention. Accordingly, Jenga designs device-friendly Attention Carving kernel that decomposes latents into contiguous latent blocks using space-filling curves, and employs block-wise attention to selectively compute key-value pairs, creating an efficient attention mechanism. As illustrated in Fig. 1 (a), video details can be preserved even when we only keep 1% key-value blocks using Attention Carving. Generating content layouts does not require huge latent inputs, so we introduce multi-stage Progressive Resolution (ProRes) strategy that generates video through phased resizing and denoising of latents, effectively reducing the token interactions. Under this strategy, we face the challenge of generating resolution-dependent variations in the field of view that affect content richness. For example, low-resolution generation focuses on zoomed-in details rather than global scenes. To counteract this, we introduce text-attention amplifier that reduces local neighborhood focus, enhancing condition information utilization, and producing more content-informative results, which are similar to generating content directly using high-resolution. As illustrated in Fig. 1(b), Jenga is combination of two complementary techniques: ProRes handles robust content generation with lower resolutions, while Attention Carving processes sparse attention, reducing token interactions. Like optimally arranged real-world Jenga blocks, these techniques deliver efficient, high-quality video generation with high block sparsity. Empowered by Jenga, we achieve impressive results across multiple state-of-the-art DiT-based video diffusion models. For instance, we obtain 4.688.83 speedup on HunyuanT2V [12] while maintaining comparable performance on VBench [35]. Similarly, we demonstrate significant acceleration on HunyuanVideo-I2V (4.43), the distilled model AccVideo [25] (2.12), and Wan2.1 1.3B [13] (4.79). Further, when deployed on an 8H800 GPU computing node, Jenga reduces the DiT inference time to 39 seconds for HunyuanVideo and 12 seconds for AccVideo. 2 Our contributions are threefold: (1) we propose novel dynamic block-wise attention carving approach that enables high-efficiency sparse attention computation for video generation; (2) we introduce Progressive Resolution, which decouples the content generation and detail refinement stages, reducing token interactions and achieving further acceleration; and (3) as plug-and-play inference pipeline, Jenga achieves unprecedented speedup across various modern video DiT architectures."
        },
        {
            "title": "2 Related Works",
            "content": "Efficient attention design in Transformers represents critical research direction focused on mitigating computational demands arising from the quadratic O(N 2) complexity relative to the token sequence length . In Language Models, efficient attention methods like MInference [36], HIP [37, 38], MoBA [39], and NSA [40, 41, 42, 43] adopt partial or hierarchical key-value selections for efficient long-context understanding. To process dense vision features, efficient attention designs are also adopted in ViT and Diffusion Models, including linear attention [44, 45] and cascade attentions [46]. All these approaches aim to reduce the number of tokens actively participating in attention computations, thereby achieving acceleration and decreasing memory requirements. Efficient video generation has garnered substantial research interest concurrent with the rapid evolution of video Diffusion Transformers (DiTs) [8, 11, 12, 47, 13]. Early acceleration techniques focused on reducing sampling steps, primarily through step distillation methodologies [25, 26] or training-free approaches that leverage step-wise feature reuse, such as TeaCache [31] and RAS [32, 48]. Bottleneck Sampling [49] employs variable resolution strategy across different sampling stages, thereby utilizing fewer tokens during intermediate computational phases. Complementary to step reduction strategies, various efficient attention mechanisms for DiTs have emerged, including CLEAR [21], STA [20], and SVG [22], which operate on the fundamental assumption of localized attention distribution patterns. While this localization assumption preserves consistent attention structures, it inherently constrains the models capacity for long-range feature aggregation. Recent advancements in block-wise attention architectures, such as SpargeAttn [50, 27] and AdaSpa [51], implement selective processing based on block-level mean values, achieving approximately two-fold acceleration in video generation pipelines. Nevertheless, their optimization potential remains limited by rigid block partitioning structures and attention sparsity parameters that require further finetune."
        },
        {
            "title": "3 Jenga: Token-Efficient Optimization for Video Diffusion Transformers",
            "content": "Latent Diffusion Models (LDMs) [1] learns to reverse noise corruption process, transforming random noise into clean latent-space samples. At time step {0, . . . , }, the model predicts latent state xt conditioned on xt+1: pθ(xtxt+1) = (xt; µθ(xt+1, t), σ2 ), where θ represents the model parameters, µθ denotes the predicted mean, and σt is the predetermined standard deviation schedule. For Diffusion Transformers [8], during each timestep t, the model processes noisy visual latent tokens xt together with tokenized conditional embeddings xc (e.g., text prompt), predicting the noise component ϵ added at that timestep. scheduler [52] then guides the progressive denoising process to compute the next denoised state xt1 = scheduler(xt, ϵ, t), gradually yielding fully denoised video latent x0, which is then converted back to pixel space with pre-trained VAE decoder. The overview of our method is illustrated in Fig. 2. Jenga aims to minimize the computational complexity by reducing the number of tokens processed in each operation within video DiTs [8]. This is achieved through two primary optimizations: (1) enhancing the efficiency of the self-attention mechanism (Sec. 3.1) and (2) streamlining the inference pipeline (Sec. 3.2). In video DiT, we typically process Nv = numel(zv) = thw visual tokens, where t, h, and represent the temporal length, height, and width of the video latent zv in the latent space, after the visual patch embedding layer, zv = patchemb(xv)."
        },
        {
            "title": "3.1 Block-Wise Attention Carving\nAs observed in [20, 50], the proportion of time spent on self-attention operations within transformer\nforward passes becomes increasingly dominant as the number of tokens grows. The 3D full-attention\nmechanism in video transformers can be represented in its most fundamental form as:",
            "content": "Attention(Qi, Ki, Vi) = softmax (cid:16)"
        },
        {
            "title": "QiKi",
            "content": "(cid:112) / dk (cid:17) Vi , (1) where Qi, Ki, Vi RN dk represent the query, key, and value features for the attention head i, respectively. We define as the embedding dimension, and as the number of attention heads with 3 Figure 2: Overview of Jenga. The left part illustrates the attention carving. 3D video latent is partitioned into local blocks before being passed to the Transformer layers. block-wise attention is processed to get head-aware sparse block-selection masks. In each selected block, dense parallel attention is performed. The right part illustrates the Progressive Resolution strategy. The number of tokens and timesteps is compressed to ensure an efficient generation. Figure 3: Attention Carving (AttenCarve). Here we illustrate toy example of 4 4 4 latent, where = 8 latent items form block. Left: The latent 3D re-ordering and block partition via space filling curves (SFC). Right: After the block-wise attention in Eq. (3), we can construct the Importance Mask, combined with the pre-computed Condition Mask and Adjacency Mask, block-wise dense attention mask is passed to the customized kernel for device-efficient attention. dk = d/h. = Nv + Nc denotes the total number of tokens, comprising Nv vision tokens and Nc condition tokens. In the context of video diffusion models, this attention operation incurs significant computational overhead due to its quadratic complexity O(N 2) concerning the token count across spatial and temporal dimensions. Due to the inherent redundancy in video latents, direct approach to improve efficiency is to reduce the number of key-value pairs each query attends to. We adopt block-wise coarse key-value selection method, as shown in Fig. 3. FlashAttention [53, 17] and other GPU-optimized approaches [50, 20] uniformly divide and KV into blocks with = N/M tokens each, corresponding to parallel threads in the attention computation, to compute exact attention results across all 2 blocks through parallel processing. For simplicity, we assume Nv and Nc are padded lengths divisible by m. Our objective is therefore to reduce KV pairs at the block level. First, to obtain tokens with higher internal similarity within 3D blocks, we reorder the 1D vision tokens zthw (flattened along thw dimensions) into block-wise order zblk before subsequent partitioning. The reordering and its inverse process are represented by: zblk = G(zthw) , zthw = G1(zblk), (2) where G() represents an index permutation function implemented via the Generalized Hilbert reordering [54, 55, 56], toy example of which is illustrated in the left part of Fig. 3. Compared with vanilla linear hwt ordering, this space-filling curve (SFC) ordering ensures that tokens in 1D proximity within zblk effectively preserve their 3D neighborhood relationships from the original space. Thus, this approach enables uniform partitioning directly in the flattened dimension when computing attention operations. For KV-block selection, we build one-hot block-wise 2D mask RM for each attention head to represent the selection result of the block-sparse attention. It is union of three masks, as shown in the right part of Fig. 3: (1) Importance Mask Btop. For importance-based block attention selection, inspired by MoBA [39] from large language models, we employ block-wise mean values to compute an attention probability map that roughly identifies which block pairs require attention computation. Specifically, for the reordered inputs, we express the relevance between blocks using: = softmax( ˆQ ˆK / (cid:112) dk), (3) 4 Figure 4: Progressive Resolusion (ProRes). Left: brief illustration of stage switch and timestep skip. Before the rescale in stage s, we revert the latent to clean state ˆxs 0, then re-noise on the upsampled clean latent. Right & Bottom: We add bias on the video-text attention score, to enable scalable Field of View (FOV) in low-resolution content generation. where ˆ() is mean-pooling operator for each block of size m. Then for the i-th query block, we set rate k, and keep the top kM key-value blocks in R. Meanwhile, for each query block, we set constraint to fulfill the cutoff approximate accumulated probability. This means after all kM blocks are selection, we still need to select blocks for some block-head combination with top probabilities, until the accumulated probability meets cutoff softmax probability threshold p, defined by (cid:80) jBtop[i]R[i][j] > p. This constraint is set to avoid global context lost, especially for some attention heads to aggregate global information. (2) Condition Mask. Bcond = {i > Nv/m > Nv/m}, where i, are mask indices in querykey block dimensions. This means all condition-related attentions should be fully computed. (3) Adjacency Mask. Badja = {adja(i, j)}, which represents whether i-th and j-th blocks are adjacent in the 3D thw space. The adjacency mask is beneficial in fixing border artifacts between spatially adjacent blocks. In Jenga, Bcond and Badja are pre-computed, and only determined by the resolution and the partition function G. The final selection array is defined as the union of three one-hot masks, = Btop Bcond Badja. For the block-wise attention, we skip the computation of indices that B[i][j] = 0, hence achieve an attention complexity O(N ), in which = (cid:80) B/M is the average number of selected tokens."
        },
        {
            "title": "3.2 Progressive Resolution\nBlock-wise Attention Carving significantly reduces the latency of each DiT forward pass, but since\ndiffusion sampling is an iterative process, compressing the number of tokens at the diffusion pipeline\nlevel is also crucial for accelerating generation. Leveraging the coarse-to-fine nature of diffusion\ndenoising [33, 57], we decompose the generation inference process of T timesteps into S stages,\nstarting from a low resolution R1 = {t, h1, w1, r, d} and progressively increasing the resolution at\neach stage until reaching the final target resolution RS = {t, h, w, r, d}, where r represents the latent\npatch size and d is the channel dimension. The stage switch is illustrated in the left part of Fig. 4. At\n0 ∈ RRs and resize\nthe end of each intermediate stage s at timestep t, we predict the clean latent at ˆxs\nit to a higher resolution Rs+1, then re-noised following an approach similar to [49]. The progressive\nresolution process between stages is defined as:",
            "content": "xt1 = (1 σt) U(ˆxs (4) Here U() is latent upsample function in 3D space, for which we employ area interpolation. ϵt is the prediction at timestep t, and σt is the time-dependent standard deviation in the scheduler [52]. By reducing resolution, the earlier stages involve significantly fewer tokens in inference, while the denoising at higher resolutions ensures the generated videos maintain high-quality details. 0 = xt σtϵt , ϵ (0, ) . 0) + σtϵ , where ˆxs Text-Attention Amplifier. Unlike bottleneck-style sampling [49], ProRes determines video content and structure during the low-resolution stage, without preserving the original resolution in the initial stage. While Video DiT generates coherent low-resolution videos, we observe that the Field of View (FOV) degrades with decreasing resolution, effectively transforming ProRes into super-resolution process on videos with constrained FOV. We illustrate this phenomenon in Fig. 4, which occurs because tokens at lower resolutions disproportionately attend to their spatial neighborhoods. To maintain stable FOV across resolutions, we introduce text-attention amplifier with resolutiondependent bias β that \"hypnotizes\" the model in the first low-resolution stage by enhancing textattention weights, thereby reducing the focus on spatial neighborhoods. This concept is illustrated in Figs. 2 and 4. Specifically, when processing vision query block qv and condition key block kc in attention, the biased vision-condition attention score is calculated as: qvk + β where β = ρ log(numel(Rs)/numel(RS)) is computed based on the token count ratio between the current stage resolution Rs and the target resolution RS, with ρ serving as balancing factor. Case-Agnostic Timestep Skip. Timestep reduction is one of the most common optimization directions in efficient diffusion pipelines. Methods like TeaCache [31, 32] approximate outputs by caching input features to dynamically determine which steps can be skipped. However, in practical implementation, we observe that TeaCaches skip mechanism is effectively static timestep scheduler, rather than truly case-wise dynamic step skipping approach. Therefore, we employ fixed timestep skip setting (23 steps, same as TeaCache-fast) that samples more densely at the beginning and end while sampling sparsely in the middle, eliminating the additional computation overhead of TeaCache."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details. Settings. Our experiments are primarily conducted on the HunyuanVideo [12] architecture with 50-step configuration. All generated HunyuanVideo videos maintain resolution of 1257201280, corresponding to patchified video latent size of = 32 45 80, approximately 115K tokens. Unless specified, all experiments are performed on one NVIDIA H800 GPU. For Attention Carving block partitioning, we employ Generalized Hilbert [54] as G() with block size of = 128. We implement the Attention Carving kernel using Triton [58] and adopt progressive top-K selection strategy when computing the importance mask: = 0.3 at stage 1, and = 0.2 for subsequent stages. The probability threshold is set to = 0.3. When calculating the adjacency mask Badja, it incorporates 26-neighborhood in 3D latent space. For ProRes stages, we provide two basic configurationsBase and Turbocorresponding to implementations using 1 (straight 720P) and 2 stages (starting with 540P, 50% steps each stage). We also introduce 2-stage Jenga-Flash setting, which applies smaller values in both stages to further enhance efficiency. The balancing factor of the text-attention amplifier is set to ρ = 0.5. After timestep skipping, 23 of the original 50 timesteps are retained, while additional steps will be added after the stage-switch process. We adopt TeaCache-style [31] latent reuse, where features are reused before the image unpatchify layers. Comprehensive details are provided in Appendix B.1. Multi-GPU Adaptation. Our method seamlessly integrates into multi-GPU parallel processing configurations. We have implemented adaptations based on xDiT [59, 60] within our approach using the HunyuanVideo [12] framework. This enables parallel processing of attention operations across the head dimension, while all operations except patchification are parallelized across multiple GPUs along the token dimension. Utilizing an 8-GPU parallel configuration, Jenga-Flash achieves further 6.28 speedup (245s 35s) with identical computational operations, which is also 5.8 faster than the official 8-GPU implementation in HunyuanVideo [12]. Detailed latency results are shown in Tab. 2b. We provide specifications of this implementation in Appendix B.2. Distilled Model & Image-to-Video. Jenga demonstrates considerable generalizability across diffusion model architectures. It not only achieves substantial acceleration ratios in Text-to-Video (T2V) models [12, 13], but our adaptive attention carving technique can also be effectively implemented in models refined through step-distillation [25, 26] with 2.12 speedup. Furthermore, when applied to Image-to-Video (I2V) models [12], our approach achieves 4.43 speed improvement in I2V generation [12] tasks even without employing ProRes. Detailed results are shown in Tab. 2. Evaluation and Metrics. For speed assessment, we report the diffusion time consumedspecifically the DiT forward pass timeas the VAE decoding component remains constant across all configurations. We also report FLOPs and step-wise FLOPs to provide an intuitive comparison of computational complexity. For qualitative evaluation, we employ the widely adopted CLIP-based metric CLIPScore [61] to measure text-video alignment, and utilize the comprehensive benchmark suites VBench [35] and VBench-I2V [62] with their original full-set prompts. We evaluate each prompt with fixed random seed to ensure both evaluation consistency and statistical reliability. Additionally, we conducted user study to assess human preference rates between Jenga and various efficient generation baselines, including direct comparison with vanilla inference. 6 Figure 5: Qualitative comparisons. (a): Jenga maintains strong semantic performance while producing high-quality videos. (b): Examples across multiple Jenga settings, we also demonstrate how the text-amplifier stabilizes Field of View (FOV) across different initial resolutions."
        },
        {
            "title": "4.2 Comparisons",
            "content": "Attention Efficiency. We benchmarked our Attention Carving (AttenCarve) approach against stateof-the-art training-free attention optimization methods, specifically MInference [36], CLEAR [21], and SVG [22], as shown in Tab. 1. From theoretical perspective, CLEAR (3D local window) and SVG (spatial-temporal windows) can be viewed as specialized instances of our more general Jenga framework. To establish robust block-selection baseline, we adapted MInference [36] for video generation by removing causal masks and modifying selection optimizations. Jengas dynamic block selection mechanism more effectively identifies crucial key-value pairs in video content while preserving important local information aggregation. Consequently, AttenCarve achieves superior acceleration ratios (2.17) with reduced computational requirements while maintaining higher generation quality, particularly in terms of semantic adherence, compared to existing approaches. Sampling Efficiency. We further compared our Progressive Resolution (ProRes) approach with TeaCache [31] in Tab. 1. We observe that ProRes and timestep skipping represent orthogonal solutions that address different aspects of efficient sampling. By incorporating ProRes, we achieve significant reduction in step-wise FLOPs while maintaining high-quality video outputs. Qualitative evaluations confirm that our Progressive Resolution strategy effectively preserves generated video quality while substantially improving computational efficiency (3.28 speedup). Qualitative Evaluation. By orthogonally combining AttenCarve and ProRes with different stage configurations, we developed three variants of Jenga: Jenga-Base (1-stage), Jenga-Turbo (2-stage), and Jenga-Flash (2-stage, higher sparsity). These variants effectively balance generation quality and speed, achieving 4.688.82 acceleration while maintaining high-quality outputs. Notably, both Jenga-Base and Jenga-Turbo surpass the baseline on VBench [35] metrics, with particularly significant improvements in the semantic score (72.84% 77.48%). This demonstrates that our approach not only accelerates inference but can also enhance the semantic coherence of generated videos. It is worth highlighting that when combining AttenCarve with timestep skipping alone (Jenga-Base), our quality metrics were not negatively affected. Jengas focus on key information selection improves semantic performance. We provide visualization cases in Fig. 5. Meanwhile, our static case-agnostic timestep skip schedule performs similar behaviour to the TeaCache in both HunyuanVideo and HunyuanVideo-I2V [12]. Results are reported in Tabs. 1 and 2a. User Study. We conducted user study employing the standard win-rate methodology to evaluate our approach. Questionnaires were constructed, each containing 12 randomly selected videos generated 7 Table 1: Evaluation results on HunyuanVideo [12]. We report evaluations of the baseline (row 1), attention optimization methods (row 2-4), pipeline optimization methods (row 5-8), and the combined results of Jenga (row 9-11). Here VBench-Q and VBench-S stand for Quality and Semantic metrics in VBench [35]. Best and the second best scores are highlighted. Computation Loads Quality Evaluation [35, 61] Latency & Speed Methods NFE PFLOPs PFLOPs / step VBench VBench-Q VBench-S CLIP-score DiT time Speedup HunyuanVideo [12] CLEAR (r=32) [21] MInference [36]non-causal SVG [22] AttenCarve TeaCache-slow [31] TeaCache-fast [31] ProRes ProRes-timeskip Jenga-Base Jenga-Turbo Jenga-Flash 50 50 50 50 50 31 23 50 23 24 24 534.44 479.97 187.79 243.36 163.04 331.35 245.84 353.21 162.29 75.49 47.77 32.97 10. 82.74% 85.21% 72.84% 9.60 3.76 4.86 3.26 10.68 10.68 7.06 6.76 3.28 1.99 1. 86.06% 82.68% 85.41% 83.36% 83.11% 85.87% 83.42% 85.31% 82.53% 82.39% 82.85% 82.57% 83.34% 83.07% 82.73% 85.64% 85.51% 86.20% 85.78% 85.19% 84.47% 84.01% 69.17% 75.16% 72.07% 75.85% 70.09% 69.91% 69.43% 69.73% 75.92% 77.48% 77.58% 30.67 30.43 30.73 30.63 30.60 30.42 30.39 30.03 30.13 30.59 30.78 30. 1625s 1848s 815s 988s 748s 967s 703s 1075s 495s 347s 225s 184s 1.00 0.89 1.99 1.64 2.17 1.68 2.31 1.51 3.28 4.68 7.22 8.83 Table 2: Model adaptation & parallel computing. All latencies are DiT forward time. We evaluate VBench [35] on T2V models and VBench-I2V [62] for I2V models. (a) Jenga on HunyuanVideo-I2V [12] (row 1-4) and Wan2.1 [13] (row 5-7), while we report timestep skip result as the efficiency baseline. (b) Jenga on distilled model [25] (row 1-4) and multiGPU inference (row 5-7). For multi-GPU, benchmark results are the same as Jenga-Flash in Tab. 1. Methods HunyuanI2V [12] + TimeSkip + Jenga NFE VBench 87.49% 50 87.67% 23 87.75% 23 Wan2.1-1.3B [13] + TeaCache-fast [31] + Jenga 50 15 15 83.28% 82.63% 82.68% latency 1499s 720s 338s 115s 34s 24s speedup 1.00 2.08 4.43 1.00 3.48 4.79 Methods AccVideo [25] + Jenga + Jenga-8GPU # GPU speed-up rate HunyuanVideo [12] + Jenga-Flash # GPU VBench CLIP latency speedup 1.00 2.12 13.41 83.82% 31.23 83.39% 31.07 83.39% 31. 161s 76s 12s 1 1 8 1 8.8 2 7.9 4 7.0 8 5.8 VBench 82.74% 440s 1625s 82.73% 63s 184s 844s 107s 225s 39s Figure 6: User study. We report pair-wise preference rates for visual, semantic, and overall quality. using Sora prompts [63]. The videos were presented in randomized order, and participants were asked to evaluate them along three dimensions: visual, semantic, and overall quality. We collected total of 70 completed feedback forms, with results presented in Fig. 6. The findings demonstrate that our method is perceptually indistinguishable from multiple efficient generation baselines [22, 12, 31] when subjected to human evaluation."
        },
        {
            "title": "4.3 Ablation Study and Discussions\nTo rigorously validate the effectiveness of our proposed method, we conducted comprehensive\nablation studies on both Attention Carving and Progressive Resolution, with results in Tab. 3.",
            "content": "Attention Carving. As shown in Tab. 3a, we ablated selection rates and truncation probability p. Our results demonstrate robust performance even with smaller (82.73% for 0.1-0.2 selection rate). The findings reveal gradual decline in both latency and generation quality as selection rates increase in the second stage, while in the first stage has minimal impact on latency. The probability constraint enhances global information gathering, as illustrated in Fig. 7, but large cutoff value (p = 0.5) disrupts the selection balance among attention heads, leading to slight performance degradation. Tab. 3b ablates latent-reorder and block selection strategies. Our experiments revealed that conventional linear partitioning can introduce shift artifacts in videos. Furthermore, this scanning approach disregards locality and consequently requires more blocks than space-filling curve (SFC) 8 Table 3: Ablation Studies. All latencies are DiT forward time. (a) Cutoff probability and multi-stage selection rates. We report VBench / latency results. The second column and the head row represents drop rates in the first and second stages, respectively. (b) Block partition & block selection masks (left and right are two separate tables, in row 1-3), stage numbers (row 47), and text amplifier bias (row 8-10). VB-Q/S represents VBench Quality and Semantic. select rates 0.5 0.3 0.0 0.4 0.3 0.2 0.4 0.3 0.2 0.4 0.3 0. 0.3 0.2 82.70% / 283s 82.59% / 242s 82.35% / 216s 82.98% / 266s 82.75% / 232s 82.43% / 204s 83.07% / 253s 82.89% / 222s 82.60% / 198s 0.1 82.90% / 277s 82.61% / 237s 82.61% / 205s 82.87% / 262s 83.07% / 225s 82.96% / 195s 82.88% / 252s 82.85% / 214 82.73% / 184s partition hwt linear SFC VBench 82.82% 83.07% latency 229s 225s selection VBench w/o Bcond 81.82% w/o Badja 82.42% latency 221s 222s stage number VBench VB-Q 1 (720P) 2 (540-720P) 3 (360-540-720P) VB-S 83.34% 85.19% 75.92% 83.07% 84.47% 77.48% 80.53% 81.66% 76.00% latency 347s 225s 157s speed 4.68 7.22 10.35 82.60% / 260s 82.47% / 237s 82.42% / 205s 82.87% / 261s 82.85% / 227s 82.84% / 196s 83.01% / 248s 82.85% / 212s 82.67% / 183s bias factor ρ VBench CLIP-score 0.0 -0.5 0.5 82.06% 82.40% 83.07% 82.87% 82.80% 31.05 30.78 30. 30.94 30.60 1.0 1.5 Figure 7: Qualitative results for ablations. Left: Missing Adjacency Mask Badja causes grid effects on block borders. Right: Cutoff probability helps gain global contexts. partitioning, resulting in marginally increased latency. Fig. 7 and Tab. 3b also validate the effectiveness of incorporating the adjacency mask Badja and condition mask Bcond, demonstrating their necessity. Progressive Resolution. The ablation studies presented in Tab. 3b demonstrate the effectiveness of our multi-stage approach. We found that 2-stage configuration maintains strong generation quality, while increasing to = 3 stages introduces some quality degradation due to latent alignment challenges. Nevertheless, the 3-stage variant still delivers satisfactory quality while achieving 10.35 speedup. Additionally, we evaluated the impact of various text-attention amplifier scales on generation quality. As shown in Tab. 3b, excessively high amplifier values introduce more global context and shift in softmax distribution, resulting in some quality reduction. However, appropriately scaled amplifiers enhance content richness without compromising generation quality. Limitation Analysis & Future Works. While Jenga demonstrates compelling efficiency gains, some limitations remain. Foremost is maintaining latent alignment during resolution transitionsdirect latent resizing offers computational advantages over pixel-domain operations (after VAE processes), but occasionally produces boundary artifacts. We found that these artifacts can be mitigated using detailed and comprehensive prompts. Our current implementation employs non-adaptive SFC block partitioning without leveraging semantic context for token importance, presenting clear improvement opportunity. Future work could integrate learnable attention carving strategies during training rather than applying them post-hoc, potentially yielding optimal token selection while preserving Jengas efficiency benefits. Detailed limitations are discussed in Appendix C.1."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce Jenga, training-free inference pipeline that addresses computational bottlenecks in DiT-based video generation by dynamically managing token interactions. Our approach combines block-wise Attention Carving with Progressive Resolution, effectively decoupling content generation from detail refinement to significantly reduce computational complexity while preserving generation quality. Extensive experiments demonstrate substantial speedups up to 8.83 across leading models, including text-to-video, image-to-video, and step distilled models. As plug-and-play solution requiring no model retraining, Jenga represents significant advancement toward making high-quality video generation more practical and accessible for real-world applications. 9 Appendix Training-Free Efficient Video Generation via Dynamic Token Carving Figure 8: Overview of the Supplementary. We hope all readers enjoy this work in detail. We summarize common possible questions and important technical points here to arrange the supplementary. We strongly recommend that all readers open the link https://julianjuaner.github. io/projects/jenga/ in your browser for video result visualizations."
        },
        {
            "title": "A Algorithmic Implementation",
            "content": "For more comprehensive understanding of the method component of Jenga, we provide pseudo-code algorithmic workflows in Algorithm 1 (Progressive Resolution), Algorithm 2 (Attention Carving pipeline), and Algorithm 3 (building block mask B). A.1 Details in Pipeline and ProRes In the Progressive Resolution algorithm, we highlight three key technical details that were not fully elaborated in the main text. Frequency re-ordering. Prior to each attention layer, input latent patches undergo positional embedding operations such as RoPE [64], which typically establish frequency maps based on the standard thw ordering. Since we employ to re-order the latents, we similarly apply fblk = G(f ) to re-order the frequency components across different dimensions, ensuring alignment with the latent ordering. As this operation is performed only once per stage, its computational overhead is negligible. Ordering back before unpatchify. Since the block selection in AttenCarve occurs after patchification, and both patchify and unpatchify operations need to be performed in the thw space, Algorithm 1 Progressive Resolution Framework for Jenga Video Generation Require: Text prompt c, stage number S, resolutions R1, . . . , RS, block size m, block selection rates k1, . . . , kS, cutoff probability p, text-amplifier ρ, timestep lists T1, . . . , TS Ensure: Diffusion model Mθ, flow-matching scheduler Initial noise ϵ (0, I) RRs , xT ϵ if = 1 Compute block reordering G, G1 and adjacency masks Badja Remap positional frequencies: fblk getFreq(Rs, G) for in Ts do Reorder tokens: zt G(patchfiy(xt)) Apply sparse attention: zt Mθ(zt, xc, ks, ρ, fblk, Badja) Restore order: ϵt unpatchfiy(G1(zt)) Denoise step: xt1 scheduler(xt, ϵt, t) 0 xt σtϵt Predict clean latent: ˆxs Resolution transition: xt1 (1 σt) (ˆxs Reset text amplifier: ρ 0 for > 1 Increase sampling shift: α α + 2 0) + σtϵ 1: Text tokens: xc = LM(c) 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end if 17: 18: end for 19: return Final prediction x0 end for if > 1 then Algorithm 2 Block-Sparse Attention with Conditional Enhancement Require: Query Q, Key K, Value , top-k, block size m, text blocks Mc, probability threshold p, adjacency mask Badja Ensure: Attention output 1: Get visual blocks Mv N/m Mc 2: if Mv > 0 then 3: Extract Qv from first vision blocks tokens 4: BuildMask(Qv, K, k, p, Mc Badja Ov AttenCarve(Qnormal, K, V, B) 5: 6: end if 7: if Mc > 0 then 8: 9: Extract Qc from remaining tokens Oc FullAttention(Qc, K, ): Text blocks see all. 10: end if 11: return concat(Ov, Oc) Algorithm 3 Build Block-wise Attention Mask Require: Query Qv, Key K, top-k, probability threshold p, visual blocks Mv, adjacency mask Badja Ensure: Block selection mask 1: ˆQ, ˆK BlockPool(Qv), BlockPool(K), mean pooling per block. 2: Block attention scores: ˆQ ˆK / 3: Convert to probabilities: softmax(S) 4: Sort probabilities: Rsorted, sort(R, desc = dk True) 5: cumsum(Rsorted) 6: Nk max(sum(C p) + 1, Mv) 7: Initialize: Btop zeros(B, H, Mv, Mtotal) 8: Fill Btop using indices I[:, :, :, 0 : Nk] 9: Bcond {i > Mv > Mv} 10: Btop Badja Bcond 11: return we must execute reordering after patchification. Subsequently, before unpatchification, we apply the inverse operation G1 from Eq. (2), ensuring that all transformations are performed in the appropriate space. Scheduler re-shift. Following the re-noise process in Eq. (4), although theoretically we maintain the same noise strength, the clean state ˆxs 0 still exhibits discrepancy from the true distribution. To address this, we employ an approach similar to BottleNeck Sampling [49, 65], progressively increasing the timestep shift factor α of the rectified flow scheduler across stages. A.2 Details in AttenCarve The implementation of AttenCarve builds upon the official codebase of block-wise MInference [36]. To enhance attention efficiency, we decoupled the vision and text query blocks as = concat(Qv, Qc), and applied FlashAttention2 [17] directly to the condition blocks. For the cutoff probability constraint when constructing the importance mask Btop, we formulate the optimization 11 Algorithm 4 Block-Sparse Attention with Text Amplification Kernel Require: Query Q, Key K, Value , sequence lengths, qk scale, text amplifier ρ, text block start index, block mask B, block dimensions Ensure: Output features // Current query block // Batch * head index is_valid_block B[off_hz, start_m, block_idx] if is_valid_block then 1: start_m program_id(0) 2: off_hz program_id(1) 3: Load sequence length and check bounds 4: Initialize offsets for data loading 5: Load query block and scale by qk_scale 6: Initialize accumulators mi , li 0, acc 0 7: for block_idx = 0 to NUM_BLOCKS 1 do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end if 18: 19: end for 20: Normalize: acc acc/li 21: Write results to output Load key-value block k, at offset block_idx BLOCK_N Compute attention scores qk kT Apply sequence length mask to qk // Apply text amplification is_text_block block_idx text_block_start qk qk + ρ if is_text_block else qk Compute attention weights exp(qk max(qk)) Update accumulators with standard attention updates problem as minimizing the number of selected blocks: min Btop[i] Btop[i] subject to (cid:88) jBtop[i] R[i][j] > (5) To satisfy this constraint, our implementation employs sort-then-greedily-select approach. For block index selection operations, we leverage vectorized indexing techniques to circumvent large-scale for loops, thereby substantially improving computational efficiency. In line 2 of Algorithm 3, we address an omission in the original Eq. (3) by explicitly incorporating the dimension dk in multi-head attention. Additionally, we implemented several engineering optimizations based on the MInference [36] block selection mechanism, including replacing the original einsum operations with CUBLAS-optimized torch.bmm() functions for enhanced latency performance. A.3 Index Re-Order and Block Partition Figure 9: real block partition example. We adopt resolution-independent Space-Filling Curve (SFC) [54] to accommodate wider range of resolutions compared to static 3D partitions. The right portion illustrates the local adjacent blocks using look-up mask Badja. To provide readers with better understanding of the block partition characteristics in Jenga, beyond the toy example in Fig. 3, we demonstrate the Space-Filling Curve (SFC) implementation in real 720P video latent space in Fig. 9. We employ Generalized Hilbert curves to overcome the limitation of standard Hilbert curves, which are only suitable for (2n, 2n, 2n) 3D spaces. It is important to 12 Table 4: Detailed parameters. We report the error bars for DiT latency measurements. The bolded steps indicate the additional steps required during stage transitions. AttenCarve ProRes Performance Settings NFE list Rs step ratio ρ α latency VBench HunyuanVideo [12] Jenga-Base Jenga-Turbo Jenga-Flash Jenga-3Stage HunyuanVideo-I2V [12] + Jenga AccVideo [25] + Jenga Wan2.1-1.3B [13] + Jenga 50 23 24 24 24 50 23 5 5 50 [0.3, 0.2] [0.3, 0.2] [0.3, 0.2] RS =[32, 45, 80] RS [1.0, 1.0] 0.3 1 0.3 2 RS [0.75, 1.0] 0.3 2 RS [0.75, 1.0] [0-24, 25-49] [0-24, 25-49] [0-24, 25-49] 0.5 0.5 0.5 [7] [7, 9] [7, 9] [0.7, 0.8, 0.8] 0.3 3 RS [0.5, 0.75, 1.0] [0-14, 15-24, 25-49] 0.5 [7, 9, 11] [0.3, 0.2] 0.3 1 [0.3, 0.2] 0.3 1 [0.2, 0.1] 0.9 1 RS =[32, 45, 80] RS [1.0, 1.0] RS =[32, 44, 78] RS [1.0, 1.0] RS =[20, 30, 52] RS [1.0, 1.0] [0-24, 25-49] 0. [7] [0-24, 25-49] 0.5 [7] [0-24, 25-49] 0. [7] 1625 15s 82.74% 83.34% 83.07% 82.73% 80.53% 347 6s 225 5s 184 3s 157 3s 1499 12s 87.49% 87.75% 338 4s 161 4s 76 2s 115 3s 24 2s 83.84% 83.39% 83.28% 82.68% note that each block in Jenga is not regular rectangular prism, but rather local cluster of tokens that are naturally partitioned. This design provides Jenga with minimal constraints regarding video dimensionswithout requiring padding along physical dimensions, it only necessitates that the total token count thw be divisible by the block count m. The continuity property of SFC in the original space also ensures certain degree of semantic similarity among tokens within each block. We further demonstrate how to utilize the Adjacency Mask Badja to identify blocks that are spatially adjacent in 3D space based on their SFC representation. As illustrated, for block 450, by identifying the blocks to which neighboring tokens belong, we located 20 adjacent blocks that are subsequently incorporated into the attention computation for the current block."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Detailed Parameter Settings In Tab. 4, we provide comprehensive list of almost all key parameters used in this work. It is worth noting that although Jenga-Base employs single-stage pipeline, we utilized different drop rates (0.7, 0.8) at different timesteps, effectively dividing our steps into two segments. We discovered that using higher cutoff probability (i.e., = 0.9) in Wan2.1 [13] significantly improved results without incurring additional computational time, suggesting the presence of few attention heads that concentrate on global features. We briefly describe our ProRes adaptation specifically for HunyuanVideo [12] (i.e., Jenga-Base). We will implement ProRes adaptation for Wan2.1 [13] in the future. B.2 Multi-GPU Adaptation Figure 10: Multi-GPU adaptation in Jenga. We highlight the computation for each GPU in yellow. For multi-GPU parallelism, we adapted our approach based on the xDiT [59] foundation used in HunyuanVideo. As illustrated in Fig. 10, we implemented parallelization across GPUs. The parallelism within Transformer blocks remains consistent with the original implementation (i.e., state A: parallelization along the token dimension before and after attention, and state B: parallelization along the head dimension within attention). We modified the corresponding LongContextAttention interface to make AttenCarve compatible with this parallel paradigm. Additionally, we discovered that when utilizing multi-GPU parallelism, the block selection process becomes the performance bottleneck. As explained in Appendix A.2, employing more efficient torch.bmm operations significantly accelerates multi-GPU execution (reducing processing time from 77s to 34s with 8 GPUs). For parallelism outside transformer blocks, since we have naturally serialized tokens using SFC, we can directly partition them according to their SFC indices before feeding them into state A. This straightforward implementation also eliminates the previous requirement that latent sizes be divisible by along specific dimensions. B.3 Image-to-Video & Distilled Model For Image-to-Video [12] adaptation, two specific details warrant clarification. Since this model performs specialized modulation operations on image conditions (latent at t[0]), we provide an additional token-level mask G(m), = {1 if = 0, else 0} when inputting tokens into the model. This enables decoupled modulation operations on the re-ordered latents. Additionally, the condition mask Bcond incorporates both text conditions and conditioning features from the first frame. Given that the first frame already contains the overall content of the video, we did not implement the text-attention amplifier. For the distilled model AccVideo [25], which inherently requires fewer sampling steps, we employed single-stage Jenga-Base setting as detailed in Tab. 4. Other configurations, including multi-GPU implementation, remain consistent with our HunyuanVideo setup. B.4 Compared Baselines To establish uniform evaluation standard, we standardized the test prompts, utilized the more widely adopted FlashAttention2 [17], and maintained consistent input video dimensions across experiments. Below are the specific configurations for comparison methods beyond the baseline: CLEAR [21]. We implemented based on the original FlexAttention [66] with 3D radius = 32. When calculating FLOPs, since CLEAR does not account for GPU parallelism capabilities, we used the actual block sparsity (11.1% instead of the theoretical 56%) to compute effective FLOPs. Combined with the kernel optimization overhead of FlexAttention itself, the resulting generation speed could not even surpass the baseline. MInference [36]. As explained in Sec. 4.2, we enhanced the block-wise attention mechanism from MInference. We removed the causal mask designed for LLMs and implemented selection rate of = 0.3. Notably, several approaches similar to MInference exist, such as block-sparse attention [67] and MoBA [39], which employ essentially identical methodologies. SVG [22]. We utilized SVGs original implementation and resolution, incorporating its optimized RoPE and Normalization kernels with sparsity setting of 0.2. TeaCache [31]. We employed the official thresholds (0.1 for slow, 0.15 for fast configurations). For Wan2.1, we set the threshold to 0.2 and enabled the use_ret_step parameter, which provided further acceleration while preserving result quality. B.5 Details about User Study Fig. 11 presents the Google Form questionnaire and anonymous website interface used to display video assets in our user study. We randomly sampled 12 prompts from pool of 63 paired results and randomized the left-right ordering of videos within each comparison pair. To ensure data quality, we excluded invalid responses with completion times less than 5 minutes or greater than 1 hour. We also removed 3 submissions exhibiting highly homogeneous selection patterns (e.g., consistently choosing the \"left video\" or \"same\" for all comparisons). The results from the remaining 70 valid questionnaires are presented in Fig. 6. 14 Figure 11: User study. (a): Questionnaire form example using Google Form. (b): Anonymous video preview website for live comparison."
        },
        {
            "title": "C Discussions and Analysis",
            "content": "C.1 Limitation Analysis & Alternative Solutions Figure 12: Some failcases. We present two potential failure cases that may occur when using more stages (S > 3), as well as scenarios where this setting is more suitable. Table 5: Results with different prompt formats. Generation with enhanced prompts can eliminate quality degradation and boost multi-stage results (comparable video quality with 10.35 speedup). HunyuanVideo [12] 1.00 Jenga-Turbo (2-stage) 7.22 Jenga-3Stage (3-stage) 10."
        },
        {
            "title": "Prompt\nStandard\nEnhanced",
            "content": "VBench Total VBench-Q VBench-S VBench Total VBench-Q VBench-S VBench Total VBench-Q VBench-S 82.74% 82.61% 85.21% 83.98% 72.84% 77.11% 83.07% 83.29% 84.47% 84.22% 77.48% 80.53%-2.21% 81.66%-3.55% 76.00%+3.16% 79.57% 82.34%-0.27% 83.65%-0.33% 77.08%-0.03% As discussed in Sec. 4.3, Jenga faces certain challenges when implementing Progressive Resolution (ProRes). Several studies [68, 69] have examined the disparities between latent-space resizing and pixel-space resizing. Even with substantial re-noising (σt > 0.9), we cannot guarantee that edges in 15 the pixel space will be perfectly denoised in the final result. Since our work focuses on transformer acceleration, we opted against using the VAE decode-resize-encode approach, as tiled decode-encode operations during stage transitions would introduce additional latency of nearly 50 seconds. Fig. 12 illustrates some failure cases and usage scenarios of our current solution in 3-stage Jenga (results shown in Tab. 3b, 10.35 faster). We observed that generation quality occasionally deteriorates in static scenes or scenarios with clear boundaries (as well as in the Image-to-Video scenario). However, these issues tend to diminish when generating more complex textures or scenes with intricate motion patterns. We validated both the baseline and multi-stage results on VBench using enhanced prompts, as shown in Tab. 5. This enables users to obtain satisfactory video results with significant acceleration when using more complex prompts (such as Sora-style prompts, as demonstrated in Fig. 5 (b), the SUV case). Beyond the training-based improvements discussed in Sec. 4.3, another promising direction for optimization is developing enhanced block partition methods. While the current SFC approach possesses many desirable properties, it remains fundamentally static. Extending context-based SFC approaches [70] into 3D video latent space could potentially yield better utilization of block selection. C.2 Block Selection: Attention Patterns Figure 13: Attention patterns. Visualization of attention distributions across different layers and timesteps for the first block (at the corner position) containing 128 latent items. We visualize the block-aware attention scores in Fig. 13. Our analysis reveals four key characteristics in the attention patterns: (1) In shallow layers, most patterns exhibit strong locality features, or (2) attention patterns highly correlate with position, forming stripe or planar distributions. In deeper layers of the model, (3) semantic-aware attention patterns emerge, where attention shifts according to the videos semantic content. (4) Simultaneously, we observe hybrid patterns combining the three aforementioned characteristics, as well as global patterns with attention sinks. Our cut-off probability threshold is specifically designed to capture information from these latter heads. These visualized patterns not only demonstrate the inherent sparsity characteristics of attention mechanisms but also highlight the necessity for dynamic block selection in our approach. C.3 Resolution-Aware Field of View In addition to the influence of the text-attention amplifier on Field of View (FOV) demonstrated in Figs. 4 and 5, we present additional examples in Fig. 14 showing dynamic FOV changes achieved by adjusting the factor ρ. We observed that in certain scenarios, not utilizing the text-attention amplifier results in an overly localized focus, ultimately reducing the content coverage in the frame. By introducing the bias parameter β, we can exert degree of control over different field-of-view ranges. Figure 14: Dynamic FOV. We demonstrate the impact of the balancing factor ρ on field of view in both static and dynamic scenes. Additional ablation examples are presented in the HTML supplement. Figure 15: Latency analysis. (a, b) Visual token counts and generation times at different resolutions. (c) Acceleration of AttenCarve vs. FlashAttention2 [17]. (d) Time breakdown across AttenCarve components. C.4 Speed Analysis & Additional Overheads In this section, we provide an in-depth analysis of our methods latency. First, as illustrated in Fig. 15 (a)-(b), we demonstrate the necessity of directly reducing token count by adjusting resolution. At 360P, only 1/4 of the input tokens, the generation speed achieves 10 improvement compared to 720P. In Fig. 15 (c), we specifically evaluate the acceleration achieved by AttenCarve compared to FlashAttention2 [17], which achieves 3.7 speedup in attention computation. Furthermore, Fig. 15 (d) provides detailed time breakdown across different components of AttenCarve, showing that Block selection introduces only 2.8% computational overhead. Additionally, we analyzed the memory efficiency of our approach. Without any specialized optimizations, when generating 720P videos, Jenga introduces minimal additional memory overhead of only 3.7% (71.84 74.49 GiB). Despite the series of optimizations in Jenga, numerous avenues remain for potential performance improvements. These include incorporating quantization optimizations mentioned in SVG [22] and SpargeAttn [50], as well as kernel optimizations for RoPE [64] and normalization operations. From hardware perspective, adapting FlashAttention3-based [71] attention kernels on the Hopper architecture shows significant speed enhancement potential. Additionally, parallelization and sparsification strategies for the VAE component have not been fully explored. These directions represent promising areas for future engineering optimizations and continued investigation in our work."
        },
        {
            "title": "D Additional Results",
            "content": "D.1 Detailed Benchmarks Tab. 6 provides comprehensive evaluation results across all 16 dimensions of VBench [35]. As shown, Jenga achieves notable advantages in multiple semantic score dimensions while maintaining high performance in quality metrics. 17 Table 6: Detailed VBench [35] results. We omit the percentage symbol % for better preview. Quality Metrics Semantic Metrics n i c o c c s o e s r i fl o t n o o m i q e e i q g e e m d c o i m l e n c m r c s t r t e s a p l s r e e c s o a o Methods HunyuanVideo [12] 96.59 98.06 99.63 99.54 61.11 72.23 60.83 82.03 68.75 94.00 93.75 78.86 38.60 20.51 23.22 26. CLEAR [21] MInference [36] SVG [22] AttenCarve 97.15 97.82 99.61 99.57 63.03 68.88 45.83 58.59 48.89 92.00 93.27 69.41 44.18 20.97 22.61 26.36 94.90 97.66 99.41 99.47 61.62 69.78 65.27 75.00 83.08 88.00 93.75 77.18 42.28 20.80 23.08 27.17 96.40 97.75 99.61 99.55 61.78 69.96 61.11 74.52 63.56 94.00 90.36 77.25 34.16 20.20 23.39 26.23 95.94 97.85 99.30 99.18 62.47 69.09 70.83 86.71 73.02 93.00 90.67 75.45 47.17 19.50 23.43 26.36 TeaCache-slow [31] 96.70 97.89 99.30 99.49 61.54 69.18 59.72 67.24 63.41 88.00 85.19 72.09 36.11 20.05 23.11 25.80 96.68 97.79 99.32 99.50 61.42 68.59 56.94 64.08 64.71 90.00 85.99 71.22 36.26 20.12 23.12 25.77 TeaCache-fast [31] ProRes 96.16 97.58 99.72 99.55 63.75 70.36 70.83 82.81 55.15 89.00 88.24 67.26 26.10 20.46 21.89 26.79 ProRes-timeskip 95.57 97.68 99.74 99.54 62.93 68.97 72.22 76.95 59.19 90.00 88.24 67.11 29.04 20.66 21.75 27.04 Jenga-Base Jenga-Turbo Jenga-Flash 95.09 97.86 99.31 99.18 62.47 69.09 72.22 86.71 73.02 88.00 90.67 75.45 47.17 19.51 23.43 26.36 93.42 96.85 99.31 98.85 63.89 66.64 77.78 94.14 66.91 94.00 95.31 73.76 50.37 19.85 23.74 27.98 92.75 97.19 99.27 98.57 62.29 66.71 85.71 73.61 63.60 90.00 99.26 71.97 56.25 20.27 24.43 28.05 AccVideo [25] +Jenga 95.92 97.53 99.35 99.28 61.40 67.98 58.33 89.40 76.30 88.00 92.50 80.29 51.09 20.49 24.43 26.73 95.36 96.97 99.26 99.02 61.38 68.10 66.67 90.37 75.41 86.00 93.62 78.83 46.72 20.57 24.11 26.92 Wan2.1-1.3B [13] + TeaCache [31] +Jenga 96.46 98.40 99.52 98.72 64.08 67.36 59.72 75.00 47.64 82.00 81.87 71.49 23.11 19.82 23.68 23.59 96.40 98.25 99.38 98.70 62.03 65.59 58.33 76.39 47.48 78.00 82.47 69.16 24.13 19.83 23.14 22.99 95.40 97.92 99.44 98.55 61.13 65.37 61.11 74.76 53.89 78.00 88.42 70.08 26.53 20.25 23.34 23.49 Quality Metrics I2V Semantic Metrics Total n i c o c c s o e s n o n o i q e e i q g i r c n r y a o m m c s o u k b n i c j e S 2 o a Methods HunyuanVideo-I2V [12] + timeskip +Jenga 95.67 95.75 93.99 96.39 96.86 95.75 99.21 99.22 99.00 61.55 61.93 60.84 70.37 70.84 70. 21.14 21.54 40.65 78.30 78.64 79.31 51.38 51.51 49.80 98.90 98.92 98.43 99.38 99.42 99.14 96.67 96.71 96. 87.49 87.67 87.74 Regarding detailed results in Tab. 6, there are two key points to clarify. First, we discovered that compared to the static local patterns used in CLEAR [21], our query/head-aware dynamic patterns significantly enhance the dynamic degree of generated results (45.83% 70.83%). Overall, Jenga introduces larger motion amplitude at the quality level, while presenting some trade-offs in subject consistency when the selection rate is small (Jenga-Flash). At the semantic level, Jenga demonstrates substantially better semantic adherence across multiple dimensions (color, object class, scene, and overall consistency). D.2 More Visual Results We showcase additional results of Jenga in different settings, as illustrated in Fig. 16, and Fig. 17. We recommend viewing the video files in the provided HTML to better evaluate the effectiveness of our method. 18 Figure 16: Visualization results. From top to bottom, each three videos is from the same setting. 19 Figure 17: Visualization results for model adaptations. Prompts are from VBench [35]."
        },
        {
            "title": "E Social Impacts",
            "content": "This paper introduces novel framework for efficient video generation that is based on current pretrained Diffusion Transformers. Although this application has the potential to be misused by malicious actors for disinformation purposes, significant advancements have been achieved in detecting malicious generation. Consequently, we anticipate that our work will contribute to this domain. In forthcoming iterations of our method, we intend to introduce the NSFW (Not Safe for Work) test for detecting possible malicious generations. Through rigorous experimentation and analysis, our objective is to enhance comprehension of video generation techniques and alleviate their potential misuse."
        },
        {
            "title": "References",
            "content": "[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. 2, 3 [2] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. 2 [3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. 2 [4] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:6840 6851, 2020. 2 [5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. Technical report, OpenAI, 2023. 2 [6] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 2 [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. 2 [8] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 2, 3 [9] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. TMLR, 2025. 2 [10] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. 2 [11] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [12] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3, 6, 7, 8, 13, 14, 15, [13] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 6, 8, 13, 18 [14] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 2 [15] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation. arXiv preprint arXiv:2502.04299, 2025. 2 [16] Yuechen Zhang, Yaoyang Liu, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, and Jiaya Jia. Magic mirror: Id-preserved video generation in video diffusion transformers. arXiv preprint arXiv:2501.03931, 2025. 2 21 [17] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. 2, 4, 11, 14, 17 [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139 144, 2020. 2 [19] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In CVPR, pages 1012410134, 2023. 2 [20] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. 2, 3, [21] Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Clear: Conv-like linearization revs pre-trained diffusion transformers up. arXiv preprint arXiv:2412.16112, 2024. 2, 3, 7, 8, 14, 18 [22] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 2, 3, 7, 8, 14, 17, 18 [23] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. ICML, 2023. 2 [24] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In CVPR, pages 1429714306, 2023. [25] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. Accvideo: Accelerating video diffusion model with synthetic dataset. arXiv preprint arXiv:2503.19462, 2025. 2, 3, 6, 8, 13, 14, 18 [26] Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, and Hao Zhang. Efficient-vdit: Efficient video diffusion transformers with attention tile, 2025. 2, 3, 6 [27] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jun Zhu, Jianfei Chen, et al. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. arXiv preprint arXiv:2410.02367, 2024. 2, 3 [28] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In ICCV, pages 1753517545, 2023. 2 [29] Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim, Xing Mei, Xiaohui Shen, and Liang-Chieh Chen. 1.58-bit flux. arXiv preprint arXiv:2412.18653, 2024. 2 [30] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In CVPR, pages 62116220, 2024. 2 [31] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. arXiv preprint arXiv:2411.19108, 2024. 2, 3, 6, 7, 8, 14, 18 [32] Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025. 2, 3, 6 [33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. ICLR, 2022. 2, 5 [34] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. ICLR, 2023. 2 [35] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, pages 2180721818, 2024. 2, 6, 7, 8, 17, 18, [36] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. NeurIPS, 37:5248152515, 2024. 3, 7, 8, 11, 12, 14, 18 [37] Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, and Sung Ju Hwang. Hip attention: Sparse sub-quadratic attention with hierarchical attention pruning. arXiv e-prints, pages arXiv2406, 2024. 3 22 [38] Heejun Lee, Geon Park, Jaduk Suh, and Sung Ju Hwang. Infinitehip: Extending language model context up to 3 million tokens on single gpu. arXiv preprint arXiv:2502.08910, 2025. 3 [39] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. 3, 4, [40] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. ICLR, 2020. 3 [41] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. ICLR, 2023. 3 [42] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. 3 [43] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. [44] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 3 [45] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer: Vision transformer using focused linear attention. In ICCV, pages 59615971, 2023. 3 [46] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient vision transformer with cascaded group attention. In CVPR, pages 1442014430, 2023. 3 [47] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. 3 [48] Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, and Yuqing Yang. Region-adaptive sampling for diffusion transformers. arXiv preprint arXiv:2502.10389, 2025. 3 [49] Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, and Bin Cui. Training-free diffusion acceleration with bottleneck sampling. arXiv preprint arXiv:2503.18940, 2025. 3, 5, 11 [50] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. 3, 4, 17 [51] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. 3 [52] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. ICLR, 2023. 3, [53] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. NeurIPS, 35:1634416359, 2022. 4 [54] Jakub ˇCervený and contributors. Gilbert: Space-filling curve for rectangular domains of arbitrary size. https://github.com/jakubcerveny/gilbert, 2025. Accessed: April 16, 2025. 4, 6, 12 [55] Hans Sagan. Space-filling curves. Springer Science & Business Media, 2012. 4 [56] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In CVPR, pages 48404851, 2024. [57] Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, and Ping Luo. Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. 5 [58] Philippe Tillet. Introducing triton: Open-source gpu programming for neural networks. https:// openai.com/index/triton/, 2021. 6 [59] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transformers (dits) with massive parallelism. arXiv preprint arXiv:2411.01738, 2024. 6, 13 [60] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. 6 [61] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. EMNLP, 2021. 6, 8 [62] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. 6, 8 [63] OpenAI. Video generation models as world simulators. [64] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 10, 17 [65] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 11 [66] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. 14 [67] Junxian Guo, Haotian Tang, Shang Yang, Zhekai Zhang, Zhijian Liu, and Song Han. Block Sparse Attention. https://github.com/mit-han-lab/Block-Sparse-Attention, 2024. [68] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising high-resolution image generation with no $. In CVPR, pages 61596168, 2024. 15 [69] Zhen Yang, Guibao Shen, Liang Hou, Mushui Liu, Luozhou Wang, Xin Tao, Pengfei Wan, Di Zhang, and Ying-Cong Chen. Rectifiedhr: Enable efficient high-resolution image generation via energy rectification. arXiv preprint arXiv:2503.02537, 2025. 15 [70] Revital Dafner, Daniel Cohen-Or, and Yossi Matias. Context-based space filling curves. In Computer Graphics Forum, volume 19, pages 209218. Wiley Online Library, 2000. 16 [71] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. NeurIPS, 37:6865868685, 2024."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "Kuaishou Technology",
        "SmartMore"
    ]
}