{
    "paper_title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding",
    "authors": [
        "Chengyue Wu",
        "Hao Zhang",
        "Shuchen Xue",
        "Zhijian Liu",
        "Shizhe Diao",
        "Ligeng Zhu",
        "Ping Luo",
        "Song Han",
        "Enze Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$ throughput} improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 1 6 2 2 . 5 0 5 2 : r 2025-5-29 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding Chengyue Wu1,2* Hao Zhang2* Shuchen Xue4 Zhijian Liu2 Shizhe Diao2 Ligeng Zhu2 Ping Luo1 Song Han2,3 Enze Xie2 1The University of Hong Kong *Equal contribution. 2NVIDIA 3MIT 4Independent Researcher Abstract: Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce Fast-dLLM, method that incorporates novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, Fast-dLLM also proposes confidence-aware parallel decoding strategy that selectively decodes tokens exceeding confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to 27.6 throughput improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs. Links: Github Code Project Page 1. Introduction Diffusion-based large language models (Diffusion LLMs) have recently attracted increasing attention due to their potential for parallel token generation and the advantages of bidirectional attention mechanisms. Notably, Mercury [13] runs at over 1,000 tokens per second, and Gemini Diffusion [8] by Google DeepMind has demonstrated the ability to generate over 1,400 tokens per second, highlighting the promise of significant inference acceleration. However, current open-source Diffusion LLMs [21, 36] have yet to close such throughput gap in practice, and their actual speed often falls short of autoregressive (AR) models. This is primarily due to two issues. First, diffusion LLMs do not support key-value (KV) caching, critical component in AR models for speeding up inference. Second, the generation quality tends to degrade when decoding multiple tokens in parallel. For example, recent findings such as those from LLaDA [21] indicate that Diffusion LLMs perform best when generating tokens one at time and soon degrades when decoding multiple tokens simultaneously. To bridge the performance gap with AR models that benefit from KV Cache, we present Fast-dLLM, fast and practical diffusion-based language modeling framework. First, Fast-dLLM introduces an approximate KV Cache tailored to Diffusion LLMs. While the bidirectional nature of attention in Diffusion LLMs precludes fully equivalent KV Cache, our approximation closely resembles an ideal cache in practice. To support KV Cache, we adopt block-wise generation manner. Before generating block, we compute and store KV Cache of the other blocks to reuse. After generating the block, we recompute the KV Cache of all the blocks. Visualizations confirm the high similarity with adjacent inference steps within the block, and our experiments show that this approximation preserves model performance during inference. We further propose DualCache version that caches Keys and Values for both prefix and suffix tokens. In parallel, Fast-dLLM investigates the degradation in output quality when generating multiple tokens simultaneously. Through theoretical analysis and empirical studies, we identify that simultaneous sampling of interdependent tokens under conditional independence assumption disrupts critical token dependencies. To address this issue and fully exploit the parallelism potential of Diffusion LLMs, we propose novel confidence-thresholding strategy to select which tokens can be safely decoded simultaneously. Instead of selecting the tokens with top confidence to decode as in LLaDA, we select tokens with confidence larger than threshold. Our theoretical justification and experimental results demonstrate that this strategy maintains generation quality while achieving up to 13.3 inference speed-up. In summary, our contributions are threefold: 2025 NVIDIA. All rights reserved. Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding (a) Throughput vs. Accuracy across methods (b) Throughput and tokens per step across methods (c) End-to-end speedup over vanilla LLaDA baseline. Figure 1 Effectiveness of components of Fast-dLLM across different approaches. We use NVIDIA A100 GPU with single batch size and no inference speedup frameworks.. (a) Inference throughput (tokens/sec) and GSM8K (5-shot) accuracy across various designs and models under maximum generation length of 256. Caching mechanism and parallel decoding can significantly accelerate inference, while the combination provides up to an 8.1 increase in throughput with negligible accuracy reduction. (b) We break down the contributions of each method by showing both the number of tokens generated per step (line) and total throughput (bars). (c) With long prefilling (8-shot) and maximum generation length of 1024, our combined approach achieves up to 27.6 end-to-end speedup compared to the vanilla LLaDA baseline. 1. Key-Value Cache for Block-Wise Decoding We introduce block-wise approximate KV Cache mechanism specifically designed for bidirectional attention. Our approach reuses cached activations from previously decoded blocks by exploiting the high similarity of KV activations between adjacent steps. By caching both prefix and suffix blocks, the DualCache strategy enables substantial computational reuse. 2. Confidence-Aware Parallel Decoding We propose novel confidence-aware parallel decoding method. Unlike prior approaches that select fixed number of tokens per step, our method dynamically selects tokens whose confidence exceeds global threshold, enabling safe and effective parallel decoding. This approach significantly accelerates inference by 13.3 while preserving output quality. 3. State-of-the-Art Acceleration Results We conduct comprehensive experiments on multiple open-source Diffusion LLMs (LLaDA, Dream) and four mainstream benchmarks (GSM8K, MATH, HumanEval, MBPP). Results demonstrate that our Fast-dLLM consistently deliver order-of-magnitude speedups with minimal or no degradation in accuracy, confirming the generality and practical value of our approach for real-world deployment. Fast-dLLM achieves hgiher acceleration (up to 27.6) when generation length is longer (1024). 2 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding 2. Preliminary 2.1. Masked Diffusion Model Diffusion models for discrete data were first explored in [29, 11]. Subsequently, D3PM [2] proposed more general framework, defining the forward noising process via discrete state Markov chain with specific transition matrices 𝑄𝑡, and parameterized 𝑝𝜃(𝑥0𝑥𝑡) for learning the reverse process by maximizing the Evidence Lower Bound (ELBO). CTMC [3] further extended D3PM to continuous time, formalizing it within continuous-time Markov Chain (CTMC) framework. In different approach, SEDD [17] parameterizes the likelihood ratio 𝑝𝑡(𝑦) 𝑝𝑡(𝑥) for learning the reverse process, and employs Denoising Score Entropy to train this ratio. Among the various noise processes in discrete diffusion, Masked Diffusion Models (MDMs), also termed absorbing state discrete diffusion models, have gained considerable attention. MDMs employ forward noising process where tokens are progressively replaced by special [MASK] token. This process is defined by the transition probability: 𝑞𝑡0 (𝑥𝑡𝑥0) = 𝑛 𝑖=1 𝑞𝑡0 (𝑥𝑖 𝑡𝑥𝑖 0 ) = ("
        },
        {
            "title": "Cat",
            "content": "𝑛 𝑖=1 𝑥𝑖 𝑡; (1 𝑡)𝛿𝑥𝑖 0 + 𝑡𝛿[MASK] ) . (1) Here, 𝑡 [0, 1] denotes the diffusion time (or masking level), controlling the interpolation between the original data 𝑥0 (at 𝑡 = 0) and fully masked sequence (at 𝑡 = 1). More recently, work by MDLM [28, 27, 38] and RADD [22] has shown that for MDMs, different parameterizations are equivalent. Furthermore, they demonstrated that the training objective for MDMs can be simplified or directly derived from the data likelihood. This leads to the following objective function, an Evidence Lower Bound (ELBO) on log 𝑝𝜃(𝑥): E𝑞𝑡0(𝑥𝑡𝑥0) 𝑖:𝑥𝑖 0=[MASK] log 𝑝𝜃(𝑥𝑖 0𝑥𝑡) d𝑡 := ℒMDM. (2) log 𝑝𝜃 (𝑥) 1 1 𝑡 2.2. Generation Process of MDMs The analytical reverse of the forward process defined in Equation 1 is computationally inefficient for generation, as it typically involves modifying only one token per step [3, 17]. common strategy to accelerate this is to employ 𝜏 -leaping [6] approximation for the reverse process. In the context of MDMs, this allows for an iterative generation process where multiple masked tokens can be approximately recovered in single step from noise level 𝑡 to an earlier level 𝑠 < 𝑡. 𝑞𝑠𝑡 = 𝑛1 𝑖= 𝑞𝑠𝑡(𝑥𝑖 𝑠𝑥𝑡), where 𝑞𝑠𝑡(𝑥𝑖 𝑠𝑥𝑡) = 1, 𝑠 𝑡 , 𝑡𝑠 𝑡 𝑞0𝑡(𝑥𝑖 𝑥𝑖 𝑥𝑖 𝑠𝑥𝑡), 𝑥𝑖 𝑡 = [MASK], 𝑥𝑖 𝑡 = [MASK], 𝑥𝑖 𝑡 = [MASK], 𝑥𝑖 𝑠 = 𝑥𝑖 𝑡 𝑠 = [MASK] 𝑠 = [MASK]. (3) 𝑠𝑥𝑡) (when 𝑥𝑖 Here, 𝑞0𝑡(𝑥𝑖 𝑡 = [MASK]) represents distribution over the vocabulary for predicting non-[MASK] token, provided by the model. In scenarios involving conditional data, such as generating response 𝑥0 to prompt 𝑝, the MDMs reverse process, as defined in Equation 3, requires adaptation. Specifically, the models predictive distribution 𝑞0𝑡(𝑥𝑖 𝑠 is now also conditioned on the prompt 𝑝, as 𝑞0𝑡(𝑥𝑖 𝑠𝑥𝑡) for unmasking token 𝑥𝑖 𝑠𝑥𝑡, 𝑝). Curse of Parallel Decoding Directly reversing the forward process from Equation 1 for generation is slow, typically altering just one token per step [3, 17]. common strategy to accelerate this is to employ 𝜏 -leaping [6] approximation for the reverse process. For MDMs, this means multiple masked tokens will be generated in parallel in single step. However, significant challenge arises in multiple token prediction due to the conditional independence assumption. Consider an example from [30]: The list of poker hands that consist of two English words are: _ _. The subsequent two words could be, for instance, high card, two pair, full house, or straight flush. Notably, correlation exists between these two words. However, the multi-token prediction procedure in MDMs first generates probability distribution for each token and then samples from these distributions independently. This independent sampling can lead to undesirable combinations, such as high house. To formalize this, consider unmasking two token positions, 𝑖 and 𝑗. MDMs sample these from 𝑝(𝑥𝑖 𝑠𝑥𝑡) due to the conditional independence assumption. However, the true joint probability requires accounting for the dependency: 𝑠𝑥𝑡) 𝑝(𝑥𝑗 3 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding Figure 2 Illustration of our Key-Value Cache for Block-Wise Decoding. (a) During prefix-only caching, the KV cache is computed once for the prompt and reused across multiple decoding steps within each block. The cache is updated after completing block to maintain consistency, with negligible overhead. (b) DualCache extends this approach by caching both prefix and masked suffix tokens, further accelerating decoding. The high similarity of KV activations across steps allows effective reuse with minimal approximation error. 𝑠, 𝑥𝑗 𝑠𝑥𝑡) 𝑝(𝑥𝑗 𝑠𝑥𝑡) = 𝑝(𝑥𝑖 𝑝(𝑥𝑖 𝑠) (or symmetrically, by conditioning 𝑖 on 𝑗). This discrepancy between the assumed independent generation and the true dependent data distribution can degrade the quality and coherence of the generated sequences. The issue is more problematic when large number of tokens are unmasked simultaneously in single step. 𝑠𝑥𝑡, 𝑥𝑖 3. Methodology 3.1. Pipeline Overview Our approach, Fast-dLLM, builds on the Masked Diffusion Model (MDM) architecture to enable efficient and highquality sequence generation. To accelerate inference, the overall pipeline incorporates two key strategies: efficient attention computation through Key-Value (KV) Cache and parallel decoding scheme guided by prediction confidence. Specifically, we adopt Key-Value Cache for Block-Wise Decoding, which allows reusing attention activations across steps and significantly reduces redundant computation. Within each block, we further propose Confidence-Aware Parallel Decoding, enabling selective updates of tokens based on confidence scores to improve efficiency while maintaining output quality. By combining these strategies, Fast-dLLM significantly speeds up inference for MDMs with minimal impact on generation performance. The overall procedure is summarized in Algorithm 1. 3.2. Key-Value Cache for Block-Wise Decoding As shown in Figure 2, we adopt block-wise decoding strategy to support the use of Key-Value (KV) Cache. Initially, we compute and store the KV Cache for the prompt, which is reused throughout Block 0. Within each block, the same cache is reused for multiple decoding steps. After completing the decoding of block, we update the cache for all tokens (not just the newly generated ones). This cache update can be performed jointly with the decoding step, so compared to not using caching, there is no additional computational overhead. This approach results in an approximate decoding process, due to the use of full attention in masked diffusion models [21, 36]. The effectiveness of our approximate KV Cache approach stems from the observation that KV activations exhibit high similarity across adjacent inference steps, as illustrated in Figure 3. The red boxed region in Figure 3a highlights the similarity scores within block, which are consistently close to 1. This indicates that the differences in prefix keys and values during block decoding are negligible, allowing us to safely reuse the cache without significant loss in accuracy. Furthermore, we implement bidirectional version of our KV caching mechanism, named DualCache, that caches not Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding only the prefix tokens but also the suffix tokens, which consist entirely of masked tokens under our block-wise decoding scheme. As shown in Table 3, DualCache results in further acceleration. The red boxed region in Figure 3b further demonstrates that the differences in suffix keys and values during block decoding are negligible. (a) Prompt block (b) Last block Figure 3 Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA-Instruct. (a) Cosine similarity heatmap for the prompt block, averaged over all prompt tokens. (b) Cosine similarity heatmap for the last block, averaged over all tokens in the last block (used to represent suffix tokens, as the last block always belongs to the suffix before its own decoding). In both (a) and (b), high similarity is observed near the diagonal (𝑖 𝑗), indicating that Key-Value activations at adjacent inference steps within block are highly similar. The red boxed regions highlight this effect, supporting the use of an approximate block-wise KV Cache: cached activations from previous steps can be safely reused during block decoding with minimal loss in accuracy. The DualCache strategy, which additionally caches suffix tokens, further demonstrates negligible differences in activations during block decoding, enabling greater acceleration with competitive accuracy 3.3. Confidence-Aware Parallel Decoding While approaches like employing auxiliary models to explicitly capture these dependencies exist [16, 34], they typically increase the complexity of the overall pipeline. In contrast to these approaches, we propose simple yet effective confidence-aware decoding algorithm designed to mitigate this conditional independence issue. Concretely, at each iteration, rather than aggressively unmasking all masked tokens using their independent marginal probabilities, we compute confidence score for each token (e.g., the maximum softmax probability). Only those with confidence exceeding threshold are unmasked in the current step; the rest remain masked and are reconsidered in future steps. If no tokens confidence exceeds the threshold, we always unmask the token with the highest confidence to ensure progress and prevent an infinite loop. This strategy accelerates generation while reducing errors from uncertain or ambiguous predictions. critical question, however, is: When is it theoretically justifiable to decode tokens in parallel using independent marginals, despite the true joint distribution potentially containing dependencies? We address this with the following formal result, which characterizes the conditions under which greedy parallel (product of marginal distribution) decoding is equivalent to greedy sequential (true joint distribution) decoding in the high-confidence regime, and quantifies the divergence between the two distributions. Prior to presenting the theorem, we will define the mathematical notation used in its statement. Let 𝑝𝜃(𝐸) denote the conditional probability mass function (PMF) given by an MDM condition on 𝐸 (comprising prompt 𝑝0 and previously generated tokens). Suppose the model is to predict 𝑛 tokens for positions 𝑖1, . . . , 𝑖𝑛 not in 𝐸. Let 𝑋 = (𝑋𝑖1, . . . , 𝑋𝑖𝑛 ) be the vector of 𝑛 tokens, where each 𝑋𝑖𝑗 takes values in vocabulary 𝒱. Let 𝑝(𝑋𝐸) 𝑝𝜃(𝑋𝑖1, . . . , 𝑋𝑖𝑛 𝐸) be the joint conditional PMF according to the model. Let 𝑝𝑗(𝑋𝑖𝑗 𝐸) 𝑝𝜃(𝑋𝑖𝑗 𝐸) be the marginal conditional PMF for position 𝑖𝑗. Parallel decoding generates tokens using the product of marginals: 𝑞(𝑋𝐸) = 𝑛 𝑗=1 𝑝𝑗(𝑋𝑖𝑗 𝐸). The proof of Theorem 1 and relevant discussions are in Appendix A. 5 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding Theorem 1 (Parallel Decoding under High Confidence). Suppose there exists specific sequence of tokens 𝑥* = (𝑥𝑖1, . . . , 𝑥𝑖𝑛 ) such that for each 𝑗 {1, . . . , 𝑛}, the model has high confidence in 𝑥𝑖𝑗 : 𝑝𝑗(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸) > 1 𝜖 for some small 𝜖 > 0. Then, the following results hold: 1. Equivalence for Greedy Decoding: If (𝑛 + 1)𝜖 1 (i.e., 𝜖 1 𝑛+1 ), then argmax 𝑧 𝑝(𝑧𝐸) = argmax 𝑞(𝑧𝐸) = 𝑥*. 𝑧 (4) This means that greedy parallel decoding (selecting argmax 𝑞) yields the same result as greedy sequential decoding (selecting argmax 𝑝). This bound is tight: if 𝜖 > 1 for which argmax𝑧 𝑝(𝑧𝐸) = argmax𝑧 𝑞(𝑧𝐸). 𝑛+1 , there exist distributions 𝑝(𝑋𝐸) satisfying the high-confidence marginal assumption 2. Distance and Divergence Bounds: Let 𝑝(𝐸) and 𝑞(𝐸) be denoted as 𝑝 and 𝑞 for brevity. 𝐿𝑝 Distance (𝑝 1): For 𝑛 > 1, 𝐷𝑝 (𝑝, 𝑞) < ((𝑛 1)𝑝 + 2𝑛)1/𝑝𝜖. Specifically, for Total Variation Distance (𝐷𝑇 𝑉 (𝑝, 𝑞) = 1 2 𝐷1 (𝑝, 𝑞)): 𝐷𝑇 𝑉 (𝑝, 𝑞) < 3𝑛1 𝜖. 2 Forward KL Divergence: For 𝑛 > 1, 𝐷KL (𝑝𝑞) < (𝑛 1)(𝐻𝑏(𝜖) + 𝜖 ln(𝒱 1)), where 𝐻𝑏(𝜖) = 𝜖 ln 𝜖 (1 𝜖) ln(1 𝜖) is the binary entropy function, and 𝒱 is the size of the vocabulary. Algorithm 1 Block-wise Confidence-aware Parallel Decoding with (Dual) KV Cache Require: 𝑝𝜃, prompt 𝑝0, answer length 𝐿, blocks 𝐾, block size 𝐵, steps per block 𝑇 , threshold 𝜏 , use_DualCache 1: 𝑥 [𝑝0; [MASK], ..., [MASK]] 2: Initialize KV Cache (single or dual) for 𝑥 (fuse with decoding). 3: for 𝑘 = 1 to 𝐾 do 4: 𝑠 𝑝0 + (𝑘 1)𝐵, 𝑒 𝑝0 + 𝑘𝐵 for 𝑡 = 1 to 𝑇 do Use cache, run 𝑝𝜃 on 𝑥[𝑠,𝑒) if use_DualCache else 𝑥[𝑠,:) For masked 𝑥𝑖, compute confidence 𝑐𝑖 = max𝑥 𝑝𝜃(𝑥𝑖) Unmask all 𝑖 in [𝑠, 𝑒) with 𝑐𝑖 𝜏 , always unmask max 𝑐𝑖 if all 𝑥[𝑠,𝑒) unmasked then break 5: 6: 7: 8: 9: 10: end for Update KV cache: if use_DualCache: prefix & suffix; else: prefix. end if 11: 12: 13: 14: end for 15: return 𝑥 // KV Cache Init // Cache Reuse // Confidence scoring // Parallel decoding // Cache Update 4. Experiments 4.1. Experimental Setup All experiments are conducted on an NVIDIA A100 80GB GPU. The proposed approach, Fast-dLLM, comprises two components: Key-Value Cache mechanism and Confidence-Aware Parallel Decoding strategy. The KV Cache component introduces hyperparameter, the cache block size, varied between 4 and 32. The parallel decoding strategy uses confidence threshold hyperparameter, explored in the range of 0.5 to 1.0. Unless otherwise specified, we use PrefixCache with block size of 32 and the threshold to 0.9. We evaluate Fast-dLLM on two recent diffusion-based language models: LLaDA [21] and Dream [36]. Benchmarks include four widely-used datasetsGSM8K, MATH, HumanEval, and MBPPto assess performance across diverse reasoning and code generation tasks. We also test under varying generation lengths to evaluate scalability and robustness. Inference throughput is measured as the average number of output tokens generated per second, calculated over the full sequence until the end-of-sequence (<eos>) token is reached. This metric reflects true end-to-end decoding speed. All evaluations are conducted using the standardized lm-eval library to ensure consistency and reproducibility. 6 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding Table 1 Comprehensive benchmark results on the LLaDA-Instruct suite. Each cell presents the accuracy and the decoding throughput in tokens per second with relative speedup to the LLaDA baseline (bottom row, blue: tokens per second/orange: relative speedup). The highest throughput and speedup for each configuration are highlighted. Benchmark Gen Length LLaDA +Cache +Parallel +Cache+Parallel (Fast-dLLM) GSM8K (5-shot) MATH (4-shot) HumanEval (0-shot) MBPP (3-shot) 512 256 512 256 512 512 79.3 6.7 (1) 77.5 3.2 (1) 33.5 9.1 (1) 37.2 8.0 (1) 79.5 21.2 (3.2) 77.0 10.4 (3.3) 33.3 23.7 (2.6) 36.2 19.7 (2.5) 79.2 16.5 (2.5) 77.6 18.6 (5.8) 33.4 24.8 (2.7) 36.8 23.8 (3.0) 41.5 42.7 43.9 30.5 (1) 40.7 (1.3) 101.5 (3.3) 43. 45.7 18.4 (1) 29.3 (1.6) 29.4 6.0 (1) 14.8 4.3 (1) 29.6 17.0 (2.8) 13.4 10.1 (2.3) 43.3 57.1 (3.1) 28.4 24.8 (4.1) 15.0 22.3 (5.1) 78.5 54.4 (8.1) 77.2 35.3 (11.0) 33.2 51.7 (5.7) 36.0 47.1 (5.9) 43.3 114.1 (3.7) 44.5 73.7 (4.0) 28.2 44.8 (7.5) 13.8 39.5 (9.2) 4.2. Main Results: Performance and Speed We report decoding performance and efficiency gains for Fast-dLLM on both the LLaDA-Instruct and Dream-Base models across the four benchmarks in Tables 1 and 2. Overall, introducing the KV Cache mechanism yields significant speed improvements for all tasks and sequence lengths, typically achieving 2 to 3.6 speedup compared to the vanilla backbone. When the parallel decoding strategy is applied individually, we see additional acceleration, often pushing speedups to 46 for the evaluated settings, particularly as the generation length increases. When both techniques are combined, the improvements become even more pronounced. On LLaDA, for example, combined KV Cache and parallel decoding methods boost throughput by up to 11 (GSM8K, length 512) and 9.2 (MBPP, length 512) over the standard baseline. Similarly, on Dream-Base, the largest throughput gains are observed on MBPP (7.8 at length 512) and GSM8K (5.6 at length 512). These results indicate that not only are our methods effective individually, but they are also highly complementary, resulting in the combined acceleration. Importantly, these efficiency gains are achieved with negligible impact on accuracy. Across all benchmarks and settings, the accuracy of our accelerated methods remains within 12 points of the backbone, and in several cases, accuracy is even slightly improved. This demonstrates that the speedup comes at almost no cost to task performance, ensuring reliability for practical deployment. We also observe that longer sequences, which are common in few-shot and code generation scenarios, benefit proportionally more from our caching and parallelization techniques due to greater opportunities for cache reuse and batch computation. Figure 4 Impact of Cache Block Size on Accuracy and Throughput. The orange line illustrates the effect of varying cache block size on throughput, while the blue line depicts the corresponding impact on accuracy. Furthermore, the improvements generalize across model architectures (LLaDA and Dream) and task types (math reasoning, program synthesis, etc.), confirming that Fast-dLLM is practical and broadly applicable framework for accelerating masked diffusion-based language models. Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding Table 2 Comprehensive benchmark results on Dream-Base variants over four tasks with different generation lengths (256 and 512). Each cell shows accuracy (top row) and decoding throughput in tokens per second with relative speedup to Dream-Base baseline (bottom row, blue: tokens per second/orange: relative speedup). Numbers in yellow indicate the highest throughput and speedup per configuration. Benchmark Gen Length Dream +Cache +Parallel +Cache+Parallel (Fast-dLLM) GSM8K (5-shot) MATH (4-shot) HumanEval (0-shot) MBPP (3-shot) 256 512 256 512 256 256 512 75.0 9.1 (1) 76.0 7.7 (1) 38.4 11.4 (1) 39.8 9.6 (1) 49.4 23.3 (1) 54.3 16.3 (1) 56.6 11.2 (1) 55.6 9.4 (1) 74.3 32.5 (3.6) 74.3 25.6 (3.3) 36.8 34.3 (3.0) 38.0 26.8 (2.8) 53.7 35.2 (1.5) 54.9 27.8 (1.7) 53.2 34.5 (3.1) 53.8 26.7 (2.8) 74.2 14.2 (1.6) 73.4 14.6 (1.9) 37.9 27.3 (2.4) 39.5 31.6 (3.2) 49.4 45.6 (2.0) 51.8 29.8 (1.8) 53.8 31.8 (2.8) 55.4 37.6 (4.0) 74.8 48.2 (5.3) 74.0 42.9 (5.6) 37.6 66.8 (5.9) 39.3 63.3 (6.5) 54.3 62.0 (2.8) 54.3 52.8 (3.2) 56.4 76.0 (6.8) 55.2 73.6 (7.8) Figure 5 (a) The red line shows the GSM8K (5-shot) accuracy across different confidence thresholds. Numbers along the red line indicate the average number of tokens decoded at each step. The three dashed lines represent the accuracy of the baseline method when selecting the top 2, 4, or 8 tokens per step. (b) The number of inference steps required under varying confidence thresholds. (c) comparison between our method and the baseline on GSM8K (5-shot) accuracy, plotted against the average number of tokens per step. Our method consistently outperforms the baseline. 4.3. Ablations and Analysis We conduct extensive ablation studies to understand how different components of Fast-dLLM contribute to performance, focusing on factors such as prefill length, generation length, cache mechanism variants, cache block size, and confidence thresholds. Influence of Prefill and Generation Length on Acceleration Table 3 and Table 4 indicate that both prefill length (n-shot) and generation length markedly impact overall speedup. Specifically, as the prefill length increases from 5-shot to 8-shot, the speedup obtained by both versions of KV Cache rises significantly (e.g., speedup for DualCache increases from 19.6 in 5-shot to 27.6 in 8-shot for generation length 1024). Similarly, extending the generation length amplifies the potential for cache reuse, leading to higher speedup. Notably, for 8-shot, speedup with DualCache grows from 9.4 (gen len 256) up to 27.6 (gen len 1024). This aligns with the theoretical expectation that amortizing computation over longer sequences yields more pronounced efficiency gains. Comparison of prefix KV Cache vs. DualCache We further compare our prefix KV Cache and DualCache versions in multiple settings. As shown in Table 4, DualCache generally achieves higher speedup than the prefix KV Cache, Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding Table 3 Performance and Speedup Comparison on LLaDA Between 5-Shot and 8-Shot Settings at Generation Length 1024. This table compares the accuracy and throughput speedups of different decoding strategies under 5-shot and 8-shot configurations using generation length of 1024. The results demonstrate how increased prefill length enhances the effectiveness of caching strategies, particularly for DualCache. Table 4 Impact of Generation Length on Accuracy and Speedup Under 8-Shot for LLaDA. This table illustrates the effect of varying generation lengths (256, 512, and 1024) on decoding performance and efficiency for different caching strategies under the 8-shot setting. Longer generation lengths lead to higher throughput gains, especially for DualCache, validating the scalability of our approach. Setting. LLaDA 5-shot 8-shot 77.0 1.1 (1) 77.3 0.7 (1) No Cache 77.4 11.7 (10.6) 78.0 9.3 (13.3) Parallel Decoding PrefixCache 75.2 14.4 (13.1) 75.7 13.0 (18.6) DualCache 74.7 21.6 (19.6) 76.0 19.3 (27.6) Len. LLaDA 256 1024 77.6 4.9 (1) 78.9 2.3 (1) 77.3 0.7 (1) No Cache 77.9 16.4 (3.3) 78.9 14.0 (6.1) 78.0 9.3 (13.3) Parallel Decoding PrefixCache 77.3 49.2 (10.0) 74.8 32.0 (13.9) 75.7 13.0 (18.6) DualCache 76.9 46.3 (9.4) 75.4 36.4 (15.8) 76.0 19.3 (27.6) especially for longer generation lengths. For gen len 512 and 1024, DualCache demonstrates up to 27.6 speedup, outperforming the prefix KV Caches 18.6 in the same scenario. Importantly, DualCache maintains competitive accuracy, with only minor trade-offs relative to the cache-only variant. This highlights DualCaches effectiveness in exploiting parallelism and cache locality for both efficiency and accuracy. Effect of Cache Block Size Figure 4 analyzes the influence of the cache block size hyperparameter. We observe that smaller block sizes tend to maximize accuracy but incur overhead due to frequent cache updates. In contrast, larger block sizes may diminish accuracy owing to increased context mismatch. Block size of 32 achieves the best trade-off, substantially improving throughput while largely preserving accuracy. This hyperparameter thus offers practical knob for balancing latency and precision in real deployments. Dynamic Threshold vs. Fixed Token-per-Step Strategies Finally, we evaluate our Confidence-Aware Parallel Decoding method against fixed token-per-step baselines on GSM8K (Figure 5). Our adaptive strategy consistently outperforms fixed baselines across key metrics: it delivers higher accuracy at comparable or reduced number of function evaluations (NFE) and generates more tokens per step on average while closely tracking accuracy. In the rightmost panel, the dynamic method approaches or exceeds the accuracy of the 1-token (non-parallel) baseline, but with much greater throughput. The result demonstrates the effectiveness of Confidence-Aware Parallel Decoding, offering practical advantages. 5. Related Work 5.1. Diffusion LLM Diffusion models have emerged as transformative paradigm in generative modeling, initially achieving remarkable success in continuous domains such as image [25, 19, 23, 26] and audio synthesis [35, 12] before expanding into natural language processing. Recent advancements in discrete diffusion models [2, 20, 21, 11, 3, 10, 18, 24, 31, 14, 39, 4, 37, 27, 28, 38, 5] have reshaped the landscape of text generation, offering viable alternative to autoregressive (AR) paradigms in large language models (LLMs). These models address the inherent challenges of discrete data by redefining noise injection and denoising processes through innovative mathematical formulations. Theoretical Foundations of Discrete Diffusion Diffusion models for discrete data were first explored in [29, 11]. Subsequently, D3PM [2] provided more general framework. This framework models the forward noising process as discrete state Markov chain using specific transition matrices. For the reverse process, D3PM learns parameterized model of the conditional probability of the original data given noised version by maximizing the Evidence Lower Bound (ELBO). CTMC [3] further extended D3PM to continuous-time setting, formalizing it as continuous-time Markov Chain (CTMC). In distinct approach, SEDD [17] learns the reverse process by parameterizing the ratio of marginal likelihoods for different data instances at given noising timestep. This ratio model is then trained using Denoising Score Entropy objective. More recently, research on Masked Diffusion Models (MDMs) by MDLM [28, 27, 38] and 9 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding RADD [22] has introduced significant clarifications. These studies have demonstrated that different parameterizations of MDMs can be equivalent. Integration with Pre-trained Language Models critical breakthrough involves combining discrete diffusion with existing LLM architectures. Diffusion-NAT [40] unifies the denoising process of discrete diffusion with BARTs [15] non-autoregressive decoding, enabling iterative refinement of masked tokens. By aligning BARTs inference with diffusion steps, this approach leverages pre-trained knowledge while maintaining generation speed 20 faster than comparable AR transformers. Similarly, the LLaDA [21] and DiffuLLaMA [7] framework scales diffusion to 7B parameters using masked denoising, while LLaDA and Dream [36] demonstrating competitive performance with autoregressive baselines like LLaMA3 [9] through recursive token prediction across diffusion timesteps. 5.2. LLM Acceleration Key-Value Cache. Key-Value (KV) Cache is fundamental optimization technique in modern large language model (LLM) inference with Transformer architecture [32]. It enables efficient autoregressive text generation by storing and reusing previously computed attention states. However, it is non-trival to apply KV Cache in diffusion langauge models such as LLaDA due to full attention. Block diffusion [1] overcomes key limitation of previous diffusion langauge models by generating block-by-block so that key and values of previously decoded blocks can be stored and reused. Non-Autoregressive Generation Non-autoregressive (NAR) generation marks fundamental shift from sequential token generation by enabling the simultaneous generation of multiple tokens, significantly accelerating inference [33]. Initially introduced for neural machine translation, NAR methods have since been extended to variety of tasks, including grammatical error correction, text summarization, dialogue systems, and automatic speech recognition. Although NAR generation offers substantial speed advantages over autoregressive approaches, it often sacrifices generation quality. Diffusion LLMs represent recent paradigm for non-autoregressive text generation; however, prior work [21] has struggled to realize the expected acceleration due to notable drop in output quality. 6. Conclusion In this work, we tackle key limitations in the inference efficiency of Diffusion-based Large Language Models (Diffusion LLMs), which have historically lacked support for KV Cache and exhibited performance degradation during parallel decoding. To bridge the gap with autoregressive models, we propose Fast-dLLM, diffusion-based framework that introduces an approximate KV Cache mechanism tailored to the bidirectional attention characteristics of Diffusion LLMs, enabled by block-wise generation scheme. Furthermore, we identify that the main obstacle to effective parallel decoding is the disruption of token dependencies arising from the conditional independence assumption. To address this, Fast-dLLM employs Confidence-Aware Parallel Decoding strategy that facilitates safe and efficient multi-token generation. Extensive experiments across multiple benchmarks and model baselines (LLaDA and Dream) show that Fast-dLLM achieves up to 27.6 speedup with minimal loss in accuracy. These findings offer practical solution for deploying Diffusion LLMs as competitive alternatives to autoregressive models in real-world applications. 10 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding A. Proof In this section, we will give the comprehensive proof and discussion of Theorem 1. Proof. Step 1: Show that 𝑥* is the unique maximizer of 𝑞(𝑥). 𝑗 = 𝑝𝑗(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸). We are given 𝑝* Let 𝑝* product-of-marginals probability mass function (PMF) is 𝑗 > 1 𝜖. Let 𝜖 𝑗 = 1 𝑝* 𝑗 = 𝑝𝑗(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸). Thus, 𝜖 𝑗 < 𝜖. The 𝑞(𝑧𝐸) = 𝑛 𝑗=1 𝑝𝑗(𝑋𝑖𝑗 = 𝑧𝑗𝐸). To maximize 𝑞(𝑧𝐸), we must maximize each term 𝑝𝑗(𝑋𝑖𝑗 = 𝑧𝑗𝐸) independently. The condition (𝑛 + 1)𝜖 1 implies 𝜖 1/(𝑛 + 1). Since 𝑛 1, it follows that 1/(𝑛 + 1) 1/2. So, 𝜖 1/2. Therefore, for the chosen 𝑥𝑖𝑗 : 𝑝* 𝑗 = 𝑝𝑗(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸) > 1 𝜖 1 1/2 = 1/2. This means 𝑥𝑖𝑗 is the unique maximizer for 𝑝𝑗(𝐸). So, argmax 𝑧 𝑞(𝑧𝐸) = (𝑥𝑖1, . . . , 𝑥𝑖𝑛 ) = 𝑥*. Step 2: Show that 𝑥* is the unique maximizer of 𝑝(𝑥). We want to show 𝑝(𝑥*𝐸) > 𝑝(𝑧𝐸) for all 𝑧 = 𝑥*. Using the Bonferroni inequality: 𝑝(𝑥*𝐸) = 𝑝(𝑛 𝑗=1{𝑋𝑖𝑗 = 𝑥𝑖𝑗 }𝐸) 1 𝑛 𝑗=1 𝑝(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸) = 1 𝑛 𝑗=1 𝜖 𝑗. Since 𝜖 𝑗 < 𝜖 for all 𝑗, we have 𝑛 𝑗=1 𝜖 𝑗 < 𝑛𝜖. So, Now consider any 𝑧 = (𝑧1, . . . , 𝑧𝑛) such that 𝑧 = 𝑥*. This means there is at least one index 𝑘 such that 𝑧𝑘 = 𝑥𝑖𝑘 . The event {𝑋 = 𝑧} is sub-event of {𝑋𝑖𝑘 = 𝑧𝑘}. So, 𝑝(𝑥*𝐸) > 1 𝑛𝜖. Since 𝑧𝑘 = 𝑥𝑖𝑘 , Thus, 𝑝(𝑧𝐸) 𝑝𝑘(𝑋𝑖𝑘 = 𝑧𝑘𝐸). 𝑝𝑘(𝑋𝑖𝑘 = 𝑧𝑘𝐸) 𝑝𝑘(𝑋𝑖𝑘 = 𝑥𝑖𝑘 𝐸) = 𝜖 𝑘 < 𝜖. For 𝑝(𝑥*𝐸) > 𝑝(𝑧𝐸) to hold, it is sufficient that 𝑝(𝑧𝐸) < 𝜖. 1 𝑛𝜖 𝜖, which simplifies to 1 (𝑛 + 1)𝜖, or 𝜖 1 strict inequalities 𝑝(𝑥*𝐸) 1 𝜖 𝑛+1 . The theorem assumes (𝑛 + 1)𝜖 < 1, which is exactly this condition. The 𝑗 > 1 𝑛𝜖 and 𝑝(𝑧𝐸) 𝜖 𝑘 < 𝜖 ensure that 𝑝(𝑥*𝐸) > 𝑝(𝑧𝐸). Thus, argmax 𝑧 𝑝(𝑧𝐸) = 𝑥*. Combined with the argmax of 𝑞, this proves the main statement of Part 1: argmax 𝑧 𝑝(𝑧𝐸) = argmax 𝑞(𝑧𝐸) = 𝑥*. 𝑧 Step 3: Tightness of the bound 1 𝑛+1 . 11 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding 𝑛+1 is tight. This means if 𝜖 > The bound 𝜖 1 𝑝𝑗(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸) > 1 𝜖 hold, but argmax𝑧 𝑝(𝑧𝐸) = 𝑥* (which is argmax𝑧 𝑞(𝑧𝐸) as long as 𝜖 1/2). Consider vocabulary 𝒱 = {0, 1} and let 𝑥𝑖𝑗 = 0 for all 𝑗, so 𝑥* = (0, . . . , 0). For each 𝑗 {1, . . . , 𝑛}, let e𝑗 be the 𝑛+1 (𝜖 1 vector with 1 at position 𝑗 and 0 elsewhere. Let 𝜂 = 1 𝑛 𝜂, 1 𝑗 𝑛 and 𝑝(𝑥*𝐸) = 1 𝑛+1 , one can construct scenario where the marginal conditions 𝑛+1 𝜂 , then 𝑥* / argmax𝑧 𝑝(𝑧𝐸). The marginal probabilities are: 𝑛+1 ) > 0. Set 𝑝(e𝑗𝐸) = 1 𝑛+1 + 1 𝑝𝑗(𝑋𝑖𝑗 = 1𝐸) = 𝑝(e𝑗𝐸) = 1 𝑛 + 1 + 1 𝑛 𝜂, 1 𝑗 𝑛. 𝑝𝑗(𝑋𝑖𝑗 = 0𝐸) = 1 𝑝𝑗(𝑋𝑖𝑗 = 1𝐸) = 1 𝜖𝑐 = 𝑛 𝑛 + 1 𝑛 𝜂 > 1 𝜖, because 1 𝑛 𝜂 = 1 𝑛(𝑛 + 1) (𝜖 1 𝑛 + 1 ) < 𝜖 1 𝑛 + 1 So, the marginal condition 𝑝𝑗(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸) > 1 𝜖 (with 𝑥𝑖𝑗 = 0) holds. As shown, argmax𝑧 𝑝(𝑧𝐸) can be made different from 𝑥*. Thus, if 𝜖 > 𝑛+1 , the argmax of 𝑝 and 𝑞 may not be the same. Step 4: Bound the 𝐿𝑝 distance. Let 𝐴𝑗 be the event {𝑋𝑖𝑗 = 𝑥𝑖𝑗 }. 𝐷𝑝 (𝑝, 𝑞)𝑝 = 𝑝(𝑥*𝐸) 𝑞(𝑥*𝐸)𝑝 + 𝑝(𝑧𝐸) 𝑞(𝑧𝐸)𝑝. 𝑧=𝑥* The term 𝑝(𝑛 𝑗=1𝐴𝑗𝐸) 𝑛 𝑗=1 𝑝(𝐴𝑗𝐸) (using 𝑝(𝐴𝑗𝐸) for 𝑝𝑗(𝑋𝑖𝑗 = 𝑥𝑖𝑗 𝐸)) can be bounded. Since 1 𝑛 𝑗= 𝑗 𝑝(𝑛 𝜖 𝑗=1𝐴𝑗𝐸) min 1𝑗𝑛 𝑝(𝐴𝑗𝐸) = 1 max 1𝑗𝑛 𝜖 𝑗, 1 𝑛 𝑗=1 𝜖 𝑗 𝑛 𝑗=1 (1 𝜖 𝑗) = 𝑛 𝑗=1 𝑝(𝐴𝑗𝐸) 1 max 1𝑗𝑛 𝜖 𝑗. Thus, 𝑝(𝑥*𝐸) 𝑞(𝑥*𝐸) < (𝑛 1)𝜖. For 𝑧 = 𝑥*: 𝑝(𝑧𝐸) < 𝜖 and 𝑞(𝑧𝐸) < 𝜖. So, The sum 𝑧=𝑥* 𝑝(𝑧𝐸) 𝑞(𝑧𝐸) can be bounded: 𝑝(𝑧𝐸) 𝑞(𝑧𝐸) < 𝜖. 𝑧=𝑥* 𝑝(𝑧𝐸) 𝑞(𝑧𝐸) 𝑧=𝑥* (𝑝(𝑧𝐸) + 𝑞(𝑧𝐸)) = 𝑝(𝑋 = 𝑥*𝐸) + 𝑞(𝑋 = 𝑥*𝐸). 𝑝(𝑋 = 𝑥*𝐸) = 1 𝑝(𝑥*𝐸) < 1 (1 𝑛 𝑗=1 𝑛 𝜖 𝑗) = 𝜖 𝑗 < 𝑛𝜖. 𝑞(𝑋 = 𝑥*𝐸) = 1 𝑞(𝑥*𝐸) < 1 𝑛 𝑗=1 (1 𝜖 𝑗) 𝑧=𝑥* 𝑝(𝑧𝐸) 𝑞(𝑧𝐸) < 2𝑛𝜖. So, Then, 𝑗=1 𝑛 𝜖 𝑗 < 𝑛𝜖. 𝑗=1 𝑧=𝑥* 𝑝(𝑧𝐸) 𝑞(𝑧𝐸)𝑝 ( sup 𝑧=𝑥* 𝑝(𝑧𝐸) 𝑞(𝑧𝐸))𝑝1 𝑝(𝑧𝐸) 𝑞(𝑧𝐸) 𝑧=𝑥* < 𝜖𝑝1(2𝑛𝜖) = 2𝑛𝜖𝑝. 12 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding 𝐷𝑝 (𝑝, 𝑞)𝑝 < ((𝑛 1)𝜖)𝑝 + 2𝑛𝜖𝑝 = ((𝑛 1)𝑝 + 2𝑛)𝜖𝑝. Therefore, So, For 𝑝 = 1, And for Total Variation Distance, 𝐷𝑝 (𝑝, 𝑞) < ((𝑛 1)𝑝 + 2𝑛)1/𝑝𝜖. 𝐷1 (𝑝, 𝑞) < (𝑛 1 + 2𝑛)𝜖 = (3𝑛 1)𝜖. 𝐷𝑇 𝑉 (𝑝, 𝑞) = 1 2 𝐷1 (𝑝, 𝑞) < 3𝑛 1 2 𝜖. Step 4: Bound the forward KL divergence. 𝐷KL (𝑝𝑞) = 𝑝(𝑧𝐸) log 𝑝(𝑧𝐸) 𝑞(𝑧𝐸) 𝑧 = 𝐼(𝑋𝑖1; . . . ; 𝑋𝑖𝑛 𝐸). The conditional total correlation can be expanded using the chain rule: 𝐼(𝑋𝑖1 ; . . . ; 𝑋𝑖𝑛 𝐸) = 𝑛 𝑘=2 𝐼(𝑋𝑖𝑘 ; 𝑋𝑖1 , . . . , 𝑋𝑖𝑘1 𝐸). Each term is bounded by the conditional entropy: 𝐼(𝑋𝑖𝑘 ; 𝑋𝑖1 , . . . , 𝑋𝑖𝑘1𝐸) 𝐻(𝑋𝑖𝑘 𝐸). The conditional entropy 𝐻(𝑋𝑖𝑘 𝐸) is bounded. Since 𝑝𝑘(𝑋𝑖𝑘 = 𝑥𝑖𝑘 𝐸) > 1 𝜖, it implies 𝑝𝑘(𝑋𝑖𝑘 = 𝑥𝑖𝑘 𝐸) = 𝜖 The entropy is maximized when the remaining probability 𝜖 𝑘 is spread uniformly, leading to: 𝑘 < 𝜖. 𝐻(𝑋𝑖𝑘 𝐸) 𝐻𝑏(𝜖 𝑘) + 𝜖 𝑘 ln(𝒱 1) < 𝐻𝑏(𝜖) + 𝜖 ln(𝒱 1). Summing (𝑛 1) such terms (for 𝑘 = 2, . . . , 𝑛): 𝐷KL (𝑝𝑞) < (𝑛 1)[𝐻𝑏(𝜖) + 𝜖 ln(𝒱 1)]. Remark 1. Assumption of Well-Defined Joint 𝑝𝜃(𝑋𝑖1 , . . . , 𝑋𝑖𝑛 𝐸): The theorem and proof rely on 𝑝𝜃(𝑋𝑖1 , . . . , 𝑋𝑖𝑛 𝐸) being well-defined joint probability mass function from which the marginals 𝑝𝜃(𝑋𝑖𝑗 𝐸) are consistently derived. This implies that the joint PMF is coherent and its definition does not depend on specific factorization order beyond what is captured by the conditioning on 𝐸. In practice, while MDM may not strictly satisfy this property, its behavior typically offers close approximation. The theorem holds for an idealized 𝑝𝜃 that possesses these properties. As MDMs become larger and more powerful, their learned distributions might better approximate such consistency. Worst-Case Analysis: The conditions and bounds provided in the theorem (e.g., (𝑛 + 1)𝜖 1) are derived from worst-case analysis. This means the bounds are guaranteed to hold if the conditions are met, regardless of the specific structure of 𝑝𝜃(𝑋𝐸) beyond the high-confidence marginal property. In practice, the actual case might be \"better behaved\" than the worst-case scenario. For instance, the dependencies between 𝑋𝑖𝑗 and 𝑋𝑖𝑘 (given 𝐸) might be weaker than what the worst-case construction assumes. Consequently, the argmax equivalence (Result 1) might still hold frequently even if (𝑛 + 1)𝜖 is slightly greater than 1 (but not much larger). The condition identifies threshold beyond which guarantees break down in the worst case, but practical performance can be more robust. Similarly, the actual 𝐿𝑝 distances or KL divergence might be smaller than the upper bounds suggest if the true joint 𝑝𝜃(𝑋𝐸) is closer to the product of marginals 𝑞(𝑋𝐸) than the worst-case configurations. 13 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding B. Case Study Table 5 Qualitative comparison of responses across methods. Prompt: robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?"
        },
        {
            "title": "DualCache",
            "content": "The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, the robe takes 2 + 1 = 3 bolts of fiber. So, the value is 3 The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, the robe takes 2 + 1 = 3 bolts of fiber. So, the value is 3 The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, it takes 2 bolts + 1 bolt = 3 bolts of fiber. The final result is 3 Table 6 Qualitative comparison of responses with varying block size for DualCache. Prompt: robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? Block Size Block Size 16 Block Size 32 The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, the robe takes 2 + 1 = 3 bolts of fiber. So, the value is 3 The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, the robe takes 2 + 1 = 3 bolts of fiber. So, the value is 3 The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, the robe takes 2 + 1 = 3 bolts of fiber. So, the value is 3 Table 7 Qualitative comparison of responses under different threshold settings. Prompt: robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? Threshold 0.7 Threshold 0.8 Threshold 0.9 The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, it takes takes 2 + 1 = 3 bolts of fiber. So, the value is 3 (NFE: 9) The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, the robe takes 2 + 1 = 3 bolts of fiber. So, the value is 3 (NFE: 12) The robe takes 2 bolts of blue fiber. It also takes half that much white fiber, so it takes 2/2 = 1 bolt of white fiber. In total, the robe takes 2 + 1 = 3 bolts of fiber. So, the value is 3 (NFE: 20) B.1. Effect of Caching Strategies on Response Quality Table 5 qualitatively compares answers from the Original, PrefixCache, and DualCache methods for the arithmetic prompt. All correctly compute the answer (3 bolts), following similar step-by-step reasoning, with only minor differences in phrasing. This shows cache strategies maintain answer accuracy and logical clarity while improving efficiency; semantic fidelity and interpretability are unaffected. B.2. Effect of Block Size in DualCache Table 6 examines different block sizes (8, 16, 32) in DualCache. For this arithmetic prompt, all settings yield correct, clearly explained answers with no meaningful output differences. Thus, DualCache is robust to block size for such problems, allowing efficiency improvements without compromising quality. B.3. Impact of Dynamic Threshold Settings Table 7 investigates dynamic threshold values (0.7, 0.8, 0.9). The model consistently produces the correct answer and clear explanations, regardless of threshold. While higher thresholds increase computational effort (NFE from 9 to 20), answer quality remains stable, indicating threshold adjustment mainly affects efficiency, not correctness, for straightforward arithmetic questions. 14 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding"
        },
        {
            "title": "References",
            "content": "[1] Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models, 2025. [2] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. [3] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. [4] Zixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, and Quanquan Gu. Fast sampling via de-randomization for discrete diffusion models. arXiv preprint arXiv:2312.09193, 2023. [5] Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. arXiv preprint arXiv:2407.15595, 2024. [6] Daniel Gillespie. Approximate accelerated stochastic simulation of chemically reacting systems. The Journal of chemical physics, 115(4):17161733, 2001. [7] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. [8] Google DeepMind. Gemini diffusion. https://deepmind.google/models/gemini-diffusion, 2025. Accessed: 2025-05-24. [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, et al. The llama 3 herd of models, 2024. [10] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022. [11] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:1245412465, 2021. [12] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models, 2023. [13] Inception Labs. Introducing mercury: The first commercial diffusion-based language model. https:// www.inceptionlabs.ai/introducing-mercury, 2025. Accessed: 2025-05-24. [14] Ouail Kitouni, Niklas Nolte, James Hensman, and Bhaskar Mitra. Disk: diffusion model for structured knowledge. arXiv preprint arXiv:2312.05253, 2023. [15] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019. [16] Anji Liu, Oliver Broadrick, Mathias Niepert, and Guy Van den Broeck. Discrete copula diffusion. arXiv preprint arXiv:2410.01949, 2024. [17] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. [18] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:3453234545, 2022. [19] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022. [20] Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text, 2025. [21] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025. 15 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding [22] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. [23] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021. [24] Machel Reid, Vincent J. Hellendoorn, and Graham Neubig. Diffuser: Discrete diffusion via edit-based reconstruction, 2022. [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. [26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. [27] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. [28] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. [29] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [30] Jiaming Song and Linqi Zhou. Ideas in inference-time scaling can benefit generative pre-training algorithms. arXiv preprint arXiv:2503.07154, 2025. [31] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022. [32] Ashish Vaswani. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. [33] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie yan Liu. survey on non-autoregressive generation for neural machine translation and beyond, 2023. [34] Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, and Arash Vahdat. Energy-based diffusion language models for text generation. arXiv preprint arXiv:2410.21357, 2024. [35] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation, 2023. [36] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. [37] Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Quanquan Gu. Diffusion language models can perform many tasks with scaling and instruction-finetuning. arXiv preprint arXiv:2308.12219, 2023. [38] Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. [39] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [40] Kun Zhou, Yifan Li, Wayne Xin Zhao, and Ji-Rong Wen. Diffusion-nat: Self-prompting discrete diffusion for non-autoregressive text generation, 2023."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "MIT",
        "NVIDIA",
        "The University of Hong Kong"
    ]
}