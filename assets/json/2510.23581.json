{
    "paper_title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation",
    "authors": [
        "Junyoung Seo",
        "Rodrigo Mira",
        "Alexandros Haliassos",
        "Stella Bounareli",
        "Honglie Chen",
        "Linh Tran",
        "Seungryong Kim",
        "Zoe Landgraf",
        "Jie Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 8 5 3 2 . 0 1 5 2 : r LOOKAHEAD ANCHORING: PRESERVING CHARACTER IDENTITY IN AUDIO-DRIVEN HUMAN ANIMATION Junyoung Seo1 Rodrigo Mira2 Alexandros Haliassos2 Stella Bounareli Honglie Chen Linh Tran2 Seungryong Kim1 Zoe Landgraf 2 Jie Shen2 1 KAIST 2 Imperial College London"
        },
        {
            "title": "ABSTRACT",
            "content": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Audio-driven human animation aims to generate realistic human videos synchronized with input audio, with widespread applications in film production, virtual assistants, and digital content creation. The advent of Diffusion Transformers (DiTs) (Peebles & Xie, 2022) has significantly advanced this field, enabling natural human video generation not only for portrait videos but also in diverse environments with complex backgrounds (Xu et al., 2024; Chen et al., 2025a). These models exhibit strong performance in capturing facial expressions, head movements, and lip synchronization across various settings. However, current DiT-based models can only handle short clips at time, typically around 5 seconds, due to the quadratic complexity of diffusion transformer architectures. To address this, segment-wise autoregressive generation has emerged as promising approach, in which models synthesize video segments conditioned on preceding frames in an autoregressive fashion, in principle enabling videos of arbitrary length (Cui et al., 2024; Gan et al., 2025; Kong et al., 2025). Yet, these autoregressive strategies often suffer from character identity drift: since each new segment is conditioned on previously generated frames, small errors compound over time, causing the characters appearance to gradually deviate from the original reference. One solution is to add reference image conditioning to anchor character features (Cui et al., 2024; Gan et al., 2025), but this creates an inherent conflict between two conditions: starting frames and reference images. The model must start from the exact final frames of the previous segment, acting as rigid constraints that cannot be violated, while simultaneously following general appearance guidance from reference images. Since reference images do not specify where character features should appear in the timeline, models often prioritize the immediate starting frame constraints, gradually Project lead. 1 Figure 1: Lookahead Anchoring enables robust long-form audio-driven animation. While autoregressive generation with HunyuanAvatar (Chen et al., 2025a) (left) and OmniAvatar (Gan et al., 2025) (right) progressively loses character identity and lip sync quality, our approach maintains both throughout extended generation. We provide video results in the project page. losing the reference characters appearance (see Fig. 1). To mitigate identity drift, recent efforts (Bigata et al., 2025; Ji et al., 2025; Yin et al., 2023) have focused on enhancing character consistency. For instance, KeyFace (Bigata et al., 2025) generates lip-synced sparse keyframes from audio input which serve as endpoint conditions for each autoregressive segment. By enforcing these keyframes as boundary constraints along the timeline, this method successfully reduces identity drift. However, it requires 2-step inference of keyframe generation and subsequent interpolation and is bound by the upper limit of the quality and expressiveness of the initial keyframes. To address the limitations introduced by keyframe interpolation, we ask: must keyframes necessarily function as rigid boundaries for the generated segment? To answer this, we propose Lookahead Anchoring, which extends the keyframe logic by leveraging keyframes that are ahead of the segment that is being generated. For instance, when generating the segment from 5 to 10 seconds, the keyframe is positioned at 13 seconds; for the next segment from 10 to 15 seconds, it shifts to 18 seconds, always staying 3 seconds ahead of the current generation window. This temporal separation fundamentally changes the keyframes role: instead of imposing rigid constraints on identity, expression and pose, it provides soft directional guidance that maintains identity while enabling expressive video generation. This approach offers substantial advantages. First, the keyframe no longer needs to match the exact lip movements and expressions required by the audio at that timestamp, since it represents distant target rather than an immediate constraint. This enables self-keyframing: directly using the reference image as recursive anchor, removing the need for separate keyframe generation stage. Furthermore, the temporal distance becomes control parameter to balance between reference adherence and motion expressiveness. Our analysis reveals that longer distances increase motion dynamics while shorter distances improve facial consistency, with an optimal range that maximizes lip-sync performance. To integrate Lookahead Anchoring with existing human animation models, we propose tailored fine-tuning strategy, and demonstrate its effectiveness across three recent DiT-based models, proving that our findings generalize to multiple different architectures. For autoregressive generation, we find that our method achieves superior lip-sync accuracy, character consistency, and video quality compared to baselines in long video generation. Finally, we highlight the versatility of our strategy through narrative-driven long video generation application, where it seamlessly integrates with external prompt-based image editing models (Batifol et al., 2025; Google, 2025)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Video diffusion transformers. Early video diffusion models extended U-Net architectures from text-to-image generation through temporal attention modules (Guo et al., 2023; Singer et al., 2022; Chen et al., 2023; Blattmann et al., 2023). The field has since shifted to Diffusion Transformers (DiTs) (Peebles & Xie, 2022), which leverage full transformer architectures for superior scalability and spatiotemporal modeling. Recent video DiTs like Wan (Wan et al., 2025), CogVideoX (Yang et al., 2024), and HunyuanVideo (Kong et al., 2024) demonstrate improved generation quality across 2 diverse content including human motion and dynamic scenes. These capabilities prove particularly valuable for downstream applications requiring precise temporal control. Audio-driven human animation. Early methods like SadTalker (Zhang et al., 2023; 2021) animated talking heads using 3D morphable models but struggled with expressive facial dynamics and natural lip synchronization. Diffusion models with U-Net architectures (Xu et al., 2024; Wei et al., 2024; Chen et al., 2025b) improved temporal coherence by leveraging their pretrained video priors, though remained limited to portrait generation. DiT-based methods transformed this landscape through their superior spatiotemporal modeling capabilities, enabling diverse in-the-wild videos with full-body scenarios and complex backgrounds. Hallo3 (Cui et al., 2024) pioneered DiT-based audio-driven animation with identity reference networks and HunyuanVideo-Avatar (Chen et al., 2025a) introduced emotion control, while OmniAvatar (Gan et al., 2025) and MultiTalk (Kong et al., 2025) expanded to full-body and multi-person scenarios. Despite these advances, DiT models inherit the quadratic complexity of transformers, preventing single-pass generation of long videos. Long-term video generation. Generating long sequences with video diffusion models remains fundamentally challenging. Inference-time approaches such as FreeNoise (Qiu et al., 2023) and FreeLong (Lu et al., 2024) employ noise manipulation techniques but achieve limited gains without training integration. Training-based methods like Diffusion Forcing (Chen et al., 2024) and Self Forcing (Huang et al., 2025) control per-frame noise to reduce temporal errors, yet lack identity preservation mechanisms. While FramePack (Zhang & Agrawala, 2025) prevents drift through reverse order generation with first frame conditioning, this contradicts the sequential nature of audiodriven animation. For audio-driven human animation, DiT-based approaches including OmniAvatar (Gan et al., 2025) and Hallo3 (Xu et al., 2024) employ temporal autoregression with reference conditioning, but identity degradation persists beyond tens of seconds due to error accumulation. Sonic (Ji et al., 2025) employs sliding windows at inference but, without training integration, achieves limited improvement in long term consistency. KeyFace (Bigata et al., 2025) successfully reduces drift through keyframe generation and interpolation, but requires an additional model and rigidly constrains motion to predetermined keyframes, limiting expressiveness. While we share KeyFaces temporal anchoring insight, our approach eliminates keyframe generation overhead and transforms hard constraints into flexible guidance, enabling arbitrary-length generation with preserved expressiveness."
        },
        {
            "title": "3 METHOD",
            "content": "We address the task of generating arbitrarily long, audio-driven human animation videos while maintaining character identity and video quality. Given an audio sequence and reference image Iref, the goal is to generate video that maintains consistent identity with Iref while exhibiting natural motion synchronized to a. In this section, we first establish the limitations of existing keyframe-based methods (Sec. 3.1), then introduce our concept of Lookahead Anchoring (Sec. 3.2), and finally detail the integration of this approach into video DiTs (Sec. 3.3). 3.1 PRELIMINARIES AND MOTIVATION Temporal autoregressive video generation. Conventional temporal autoregressive approaches tackle this by dividing the video into segments of length L, as in Gan et al. (2025); Cui et al. (2024). For the i-th segment Vi spanning frames [iL, (i+1)L), the generation process Gauto() is conditioned on the corresponding audio segment aiL:(i+1)L and the final frames of the previous segment end i1: Vi = Gauto(aiL:(i+1)L, end i1, Iref). (1) While this enables theoretically unlimited video length, errors in end causing progressive identity drift and quality degradation. i1 accumulate across segments, To preserve character identity, several methods implement the reference conditioning for Iref. For instance, Hallo3 (Xu et al., 2024) employs feature concatenation through ReferenceNet (Hu, 2024), while OmniAvatar (Gan et al., 2025) uses channel concatenation for reference injection. However, we empirically find that none of these strategies are sufficient for maintaining long-term identity consistency, as shown in Fig. 1 and 4. 3 Figure 2: Motivation. (a) We depart from the convention of using conditional keyframes as generation window endpoints. Instead, we reposition keyframes as temporally distant anchors beyond the window, decoupling them from the actual generated sequence. This eliminates constraints such as audio synchronization requirements while enabling flexible conditioning. (b) Models naturally learn that longer temporal distances allow for greater scene variation. We exploit this prior strategically: distant keyframes provide high-level guidance without imposing strict physical constraints, enabling diverse yet coherent generation. Keyframe-based anchoring. Recent keyframe-based methods such as KeyFace (Bigata et al., 2025) address identity drift by introducing temporal anchors throughout generation. Specifically, these approaches employ two-stage framework. First, specialized audio-conditional model Gkey generates sparse keyframes = {kL1, k2L1, ...} where each kt is single frame synchronized with audio at time t: kt = Gkey(atϵ:t+ϵ, Iref), (2) where ϵ is the context window size. Then, each video segment Vi is generated with keyframes serving as boundary constraints: Vi = Ginterp(aiL:(i+1)L, kiL, k(i+1)L1), (3) where Ginterp() is generative video interpolation model, with kiL and k(i+1)L1 serving as boundary constraints for segment Vi. This two-stage approach successfully reduces drift by providing regular identity anchors, but introduces its own limitations: it requires training an additional keyframe generation model, increasing overall complexity; it constrains the model to produce exact poses at predetermined timestamps, which can suppress natural temporal dynamics and reduce flexibility in motion generation; and it limits the visual quality of the final video, since it is ultimately upper-bounded by the quality of the generated keyframes themselves. 3.2 LOOKAHEAD ANCHORING To overcome these limitations while retaining the identity-anchoring benefits of keyframes, we propose Lookahead Anchoring. Our key idea is simple: instead of forcing the model to reach specific keyframes at segment boundaries, we position them perpetually ahead, always visible but never reached. Formally, when generating segment Vi, our model GLA() conditions on keyframe positioned frames beyond the segment endpoint, where this lookahead distance N, > 0 becomes control parameter: Vi = GLA(aiL:(i+1)L, end i1, k(i+1)L1+D), (4) where end i1 denotes starting frames to ensure temporal continuity. This modification changes how the model interprets the keyframe. Instead of being rigidly bound to match the keyframe at the segments edge, the model is encouraged to generate expressive motion that progressively moves towards the target appearance. It is perpetually chasing the keyframe, which maintains consistent distance from each generation window throughout the autoregressive process. It ensures that the model never directly reaches the keyframe target while continuously receiving its guidance signal. 4 Lookahead distance as degree of keyframe relevance. Following this intuition, the lookahead temporal distance can be seen as parameter that controls the keyframes influence. shorter distance grounds the model more firmly in the keyframe, whereas longer one affords greater motion freedom, suggesting natural way to balance identity adherence and expressiveness. We empirically verify this relationship in Sec. 4.3, where Fig. 6 demonstrates that increasing temporal distance systematically trades facial consistency for enhanced motion dynamics. Sync-free keyframes. fundamental limitation of traditional keyframe methods is the tight coupling of identity and motion cues within single audio-synchronized frame. Positioning the keyframe in the distant future decouples identity and motion: the keyframe anchors identity, while the current audio drives motion. This temporal separation eliminates the need for an audio-synchronized keyframe, as precise audiopose alignment at that distant timestamp is no longer necessary. Consequently, any image can serve as directional identity anchor during inference. We can formally replace the keyframe with condition image Itrg: k(i+1)L1+D Itrg. (5) For instance, we can directly leverage prompt-based image editing models (e.g., FLUX (Batifol et al., 2025), Nano Banana (Google, 2025)) to generate diverse Itrg representing different expressions, poses, or contexts, enriching the final video without training specialized audio-conditional models (see Fig. 7) Self-keyframing. Crucially, this decoupling also enables novel and effective strategy we call Self-keyframing: leveraging the reference image Iref as the perpetual distant target, i.e., Itrg := Iref. This further simplifies the framework, by eliminating the keyframe generation stage entirely while maintaining consistent identity guidance. Unlike conventional reference conditioning that injects static features, our approach assigns Iref, specific future temporal position. The model interprets this timestamped reference as future target, creating persistent identity pull throughout generation. 3.3 LEVERAGING LOOKAHEAD ANCHORING FOR AUDIO-CONDITIONAL VIDEO DITS Modern video DiTs generally process noisy video latents through multiple layers of full 3D attention, progressively denoising them across diffusion timesteps. VAE encoder produces spatiotemporally compressed latent representations, which are then patchified and flattened into token sequences. Each token receives positional embedding px,y,t = px py pt, where denotes concatenation, encoding its spatial (x, y) and temporal (t) position. For sequence of latent frames, we denote the temporal embedding at position ℓ as pt[ℓ] where ℓ {0, 1, ..., 1}. These latent frames correspond to original video frames after temporal compression with ratio = L/n. We focus on manipulating temporal embeddings to encode distant inter-frame relationships. Distant keyframe conditioning. We realize this by assigning the keyframe tokens to positions corresponding to distant future timestamps in the DiT input sequence. During training, given latent sequence = {z0, ..., zn1}, we append distant clean latent zn1+d as condition to form = {z0, ..., zn1, zn1+d}, where = D/r is the temporal distance in latent frames. While video latents are progressively denoised, the conditioning latent remains clean throughout the denoising process. We handle this asymmetry with projection layer ϕ that maps clean conditioning latents into the space of noisy generation tokens. The appended latent receives temporal embedding pt[n 1 + d], signaling its future position. At inference, we replace zn1+d with ztrg, latent of the encoded conditional image Itrg while maintaining the distant time positional embedding: (cid:1) with = (cid:8)pt[0], ..., pt[n 1], pt[n 1 + d](cid:9), = concat(cid:0){z0, ..., zn1}, ztrg (6) where concat() denotes concatenation. This design enables the model to leverage temporal distance as control signal while maintaining clean identity guidance throughout the denoising process. Do video DiTs understand distant frames? Our approach presumes that video DiTs can meaningfully process frames at non-consecutive temporal positions. To validate this assumption, we investigate whether pretrained video DiTs generalize to distant temporal relationships. We conduct 5 Figure 3: Exploration of distant frame relationships in pretrained video DiT (Gan et al., 2025). Given conditional frame, we generate separate two-frame videos with artificially increased temporal gaps. Testing beyond the training distribution naturally degrades visual quality but reveals adaptive motion behavior. We propose fine-tuning to harness this observed temporal structure. simple experiment: using pretrained audio-conditional video DiT (Gan et al., 2025), we generate two-frame videos, conditioning on the first frame while manipulating the temporal positional embedding of the second frame to simulate an artificially large time gap (detailed in Appendix B). Fig. 3 demonstrates that the pretrained model exhibits an adaptive behavior: increased temporal distances yield larger motion magnitudes, suggesting the model has an intrinsic understanding of temporal dynamics. However, without explicit training on distant frame pairs, the generation quality deteriorates under distant conditioning, producing artifacts and inconsistent identity. This finding validates our hypothesis that video DiTs contain exploitable temporal structure, while simultaneously motivating fine-tuning strategy to refine this raw capability into precise control mechanism. Fine-tuning strategy. naive approach of fine-tuning exclusively with distant keyframes would force the model to abruptly abandon its learned temporal dynamics. Instead, we sample keyframes from continuous range of temporal positions during training. Given training video sequence, we sample the keyframe position ℓ from range that spans both inside and outside the current generation window: ℓ U[0, 1 + dmax], where dmax > 0, (7) where dmax denotes the maximum temporal distance in latent frames during training. When ℓ falls within the generation window (ℓ < n), the model learns to produce accurate reconstructions at the specified position. When ℓ exceeds the window boundary (ℓ ), the model learns to modulate the keyframes influence based on its temporal distance, thereby instantiating the desired soft guidance mechanism. This soft exposure allows the model to learn smooth and generalizable function of how the keyframes influence attenuates with distance, which we explore further in Sec. 4.3."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "To demonstrate its generalizability, we integrate our approach into three audio-driven human animation DiTs (base models): Hallo3 (Cui et al., 2024), HunyuanVideo-Avatar (Chen et al., 2025a), and OmniAvatar (Gan et al., 2025). We fine-tune on the Hallo3 training dataset and 160 hours of collected portrait video data. All baseline DiT models are fine-tuned on the same data to ensure fair comparison. We evaluate on HDTF (Zhang et al., 2021) and AVSpeech (Ephrat et al., 2018) test splits, sampling videos exceeding 30 seconds (average length: 48s), which require 9-15 iterative segment generations. HDTF provides portrait evaluation (shoulder-to-face), while AVSpeech enables in-the-wild assessment with complex backgrounds and varied body compositions. For finetuning, we follow the original training configurations of each baseline DiT, including loss functions and learning rates. Fine-tuning details for each baseline model are provided in Appendix B. 4.2 ADAPTING STATE-OF-THE-ART MODELS WITH LOOKAHEAD ANCHORING Quantitative comparisons. Tab. 1 and 2 present quantitative evaluations on HDTF (Zhang et al., 2021) and AVSpeech (Ephrat et al., 2018), comparing baselines with our approach. Following established protocols (Zhang et al., 2023; Xu et al., 2024), we report SyncNet (Prajwal et al., 2020) 6 Table 1: Quantitative results on HDTF (Zhang et al., 2021) in temporal segment-wise autoregressive scheme. We compare temporal autoregressive long video generation of DiT-based models with and without our proposed approach. Additionally, we provide comparisons with five GANand Diffusion U-Net-based models. For fair comparison, all three DiT-based methods are fine-tuned on the same training data used in our approach. HunyuanAvatar is modified to take 5 starting frames to compare in the temporal autoregressive generation scheme. Models Sync-D Sync-C Face-Con. Subj-Con. FID FVD MS SadTalker (Zhang et al., 2023) Hallo (Xu et al., 2024) AniPortrait (Wei et al., 2024) EchoMimic (Chen et al., 2025b) KeyFace (Bigata et al., 2025) Hallo3 (Cui et al., 2024) + LA (Ours) HunyuanAvatar (Chen et al., 2025a) + LA (Ours) OmniAvatar Gan et al. (2025) + LA (Ours) 7.64 7.96 11.60 8.66 10.28 7.89 7.53 7.77 7. 8.48 7.19 7.59 7.39 3.52 7.31 4.91 7.79 8.22 8.27 8.27 7.47 9.28 0.9326 0.9245 0.9235 0.9254 0. 0.8628 0.9267 0.6109 0.9135 0.7904 0.8628 0.9934 0.9893 0.9903 0.9920 0.9902 0.9774 0.9843 0.9470 0. 0.9568 0.9686 86.20 37.27 37.20 44.56 17.01 21.61 12.44 37.84 15.98 35.26 24.09 423.18 243.11 278.09 444.14 157. 180.30 142.97 501.80 182.23 467.90 302.71 0.9956 0.9944 0.9954 0.9949 0.9952 0.9939 0.9939 0.9925 0. 0.9953 0.9923 Table 2: Quantitative results on AVSpeech (Ephrat et al., 2018) in temporal segment-wise autoregressive scheme. We compare temporal autoregressive long video generation of DiT-based models with and without our proposed approach. Models Sync-D Sync-C Face-Con. Subj-Con. FID FVD MS Hallo3 Xu et al. (2024) + LA (Ours) HunyuanAvatar Chen et al. (2025a) + LA (Ours) OmniAvatar Gan et al. (2025) + LA (Ours) 9.87 8. 8.45 8.27 8.51 8.10 4.36 6.03 6.49 6.55 6.41 7.36 0.7422 0. 0.5282 0.8943 0.6378 0.8415 0.9519 0.9734 0.9415 0.9879 0.9268 0.9670 44.67 24. 59.15 22.89 81.15 35.83 357.49 351.90 455.52 271.15 601.12 510.93 0.9950 0. 0.9954 0.9963 0.9966 0.9946 distance and confidence for lip synchronization quality. Face and subject consistency are measured via cosine similarity of ArcFace (Deng et al., 2019) and DINO (Zhang et al., 2022) features, respectively, following VBench (Huang et al., 2024). We additionally report FID (Heusel et al., 2017), FVD (Unterthiner et al., 2018), and motion smoothness (MS) (Huang et al., 2024) to assess overall generation quality. To analyze temporal stability, we compute FID scores using 1-second sliding windows normalized by the initial window in Fig. 5. While baseline methods show progressive degradation over time, our approach maintains consistent quality, with the most substantial improvements observed in OmniAvatar, followed by HunyuanAvatar and Hallo3. User studies in Tab. 3 demonstrate superior preference for our approach across all baselines in lip synchronization, character consistency, and overall quality, among 34 participants. Detailed user study protocols are described in Appendix C. Qualitative comparisons. Fig. 4 presents qualitative results on AVSpeech (Ephrat et al., 2018) and HDTF (Zhang et al., 2021) under temporal autoregressive scheme, where the last frames from previous segments initialize subsequent generation. We evaluate three audio-conditional DiTs with and without our proposed approach. While baseline methods exhibit gradual identity drift and diverge from reference image details as generation progresses, our method maintains consistent character identity and superior visual quality throughout. More qualitative results are provided in Fig. 8, 9 and 10 in the appendix. Comparisons with other long video approaches. Tab. 4 compares our method with other long video generation strategies on AVSpeech. Based on HunyuanAvatar, we evaluate time-aware position shifting technique proposed in Sonic (Ji et al., 2025), KeyFace-style approach that first generates sparse audio-synchronized keyframes and then fills intermediate frames through video interpolation, and past-time conditioning variant that uses anchor frames located 8 frames before the generation window. Our lookahead anchoring achieves the best lip synchronization performance Figure 4: Qualitative results. We compare our method with three audio-conditioned DiT baselines under the temporal sement-wise autoregressive framework on AVSpeech (Ephrat et al., 2018) and HDTF (Zhang et al., 2021), presenting mid-sequence frames to demonstrate generation quality. Video results are available in the project page. Table 3: User study. Models Lip-Sync. Character-Con. Overall Quality Hallo3 + LA (Ours) HunyuanAvatar* + LA (Ours) OmniAvatar + LA (Ours) 21.6% 79.4% 21.6% 79.4% 46.1% 53.9% 17.6% 82.4% 10.9% 89.2% 29.4% 70.6% 25.5% 74.5% 15.7% 84.3% 36.3% 63.7% Table 4: Comparison with other long-term video generation approaches. Approach Sync-D Sync-C Face-Con MS Baseline w/ Sonic w/ Keyframe Gen. w/ Past-time Cond. w/ Ours 8.45 8.58 8.33 8.55 8. 6.49 6.44 6.53 6.36 6.55 0.5282 0.8900 0.8685 0.8753 0.8943 0.9954 0.9954 0.9964 0.9946 0. Figure 5: Performance over time. We report FID computed with 1-second sliding windows, normalized relative to the first window. and facial consistency while maintaining high motion smoothness and eliminating the need for additional architectural complexity or separate generation stages. 4.3 ANALYSIS Temporal distance. We analyze the impact of temporal distance between keyframes and generation windows in Fig. 6. We vary the distance from 4 to 80 frames and measure its effect on facial consistency, dynamic degree (quantified as the variance of facial landmarks across frames (Ma et al., 2023)), and lip synchronization quality. We find that increasing the distance yields higher dynamic degree while shorter distances strengthen identity preservation. Remarkably, lip synchronization 8 Figure 6: Effect of temporal distance between conditional keyframes and generation windows. As we increase the temporal distance, our approach (blue line) yields increased dynamicity over the baseline (green line), at the expense of facial consistency. On the other hand, lip synchronization is noticeably improved when the temporal distance is smaller ( 12), but degrades sharply when it is increased further. Figure 7: Narrative-driven generation. We edit reference image using text prompts to create keyframes with different states, then position them as lookahead anchors during autoregressive generation to guide narrative transitions while maintaining audio synchronization. More results can be found in Fig. 11. peaks arounds 12 frames, revealing an optimal zone that maximizes both expressiveness and character consistency. Training strategy. We compare two training approaches in Tab. 5: fixed anchoring, which fixes the keyframes position at ℓ = + d, and our flexible anchoring, which samples keyframe positions from both inside and outside the generation window. The latter forces the model to learn how the keyframes influence varies with temporal distance, encouraging further generalization. In practice, we find that flexible anchoring achieves superior lip synchronization and facial consistency. Table 5: Ablation on anchoring strategies. Approach Sync-D Sync-C Face-Con Train w/ fixed anchor Train w/ flexible anchor No time P.E. Learnable time P.E. Distant time P.E. 8.50 8.27 11.17 8.68 8.27 6.45 6. 2.78 6.16 6.55 0.8859 0.8943 0.7251 0.6068 0.8943 Ablation on time positional embeddings. We ablate different temporal embedding strategies for the conditional keyframe in Tab. 5. We compare our approach of assigning distant future temporal embeddings against two alternatives: learnable embeddings optimized during training, and zero embeddings without positional information. Our distant future embedding achieves the best performance, as it explicitly encodes the temporal relationship between the generation window and the conditional keyframe. 4.4 NARRATIVE-DRIVEN GENERATION WITH EXTERNAL IMAGE MODELS Our lookahead anchoring allows the conditioning keyframe to be any image with consistent character identity, as it serves as distant target rather than an immediate constraint requiring audio synchronization. Leveraging this flexibility, we generate narrative-driven videos by using text-based image editing models to create diverse keyframes representing different emotional states or contexts from single reference image. By conditioning on these narrative keyframes at appropriate intervals, we produce dynamic videos that follow desired storylines while maintaining natural audio 9 synchronization, as demonstrated in Fig. 7 using Nano Banana (Google, 2025). Crucially, the model does not replicate these keyframes exactly but rather converges toward similar states (e.g., emotions, expressions) through soft guidance, preserving natural, audio-synchronized motion."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced Lookahead Anchoring, strategy that achieves robust identity preservation in long video generation by positioning keyframes at temporally distant future timesteps. This approach eliminates the need for specialized keyframe generation models, while providing intuitive control over the identity-expressiveness trade-off through temporal distance. The consistent improvements across three architectures demonstrate that rethinking the fundamental role of keyframes, from rigid constraints to directional guidance, offers practical path toward high-quality, arbitrarily long audiodriven human animations."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our method generates highly realistic long-form human animation videos, offering significant benefits for content creation, education, and accessibility applications. We acknowledge the potential risks of misuse, including unauthorized impersonation and misinformation. By releasing our work as open-source, we will aim to promote transparency and enable the development of detection mechanisms while advancing scientific understanding."
        },
        {
            "title": "REFERENCES",
            "content": "Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pp. arXiv2506, 2025. Antoni Bigata, Michał Stypułkowski, Rodrigo Mira, Stella Bounareli, Konstantinos Vougioukas, Zoe Landgraf, Nikita Drobyshev, Maciej Zieba, Stavros Petridis, and Maja Pantic. Keyface: Expressive audio-driven facial animation for long sequences via keyframe interpolation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 54775488, 2025. A. Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, and Dominik Lorenz. Stable video diffusion: Scaling latent video diffusion models to large datasets. ArXiv, abs/2311.15127, 2023. URL https://api.semanticscholar.org/ CorpusID:265312551. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023. Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, and Qinglin Lu. Hunyuanvideo-avatar: High-fidelity audio-driven human animation for multiple characters. arXiv preprint arXiv:2505.20156, 2025a. Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike In Proceedings of the audio-driven portrait animations through editable landmark conditions. AAAI Conference on Artificial Intelligence, volume 39, pp. 24032410, 2025b. Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks. arXiv e-prints, pp. arXiv2412, 2024. 10 Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 46904699, 2019. Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audiodriven avatar video generation with adaptive body animation. arXiv preprint arXiv:2506.18866, 2025. Google. Nano banana (gemini 2.5 flash image), 2025. URL https://aistudio.google. com/models/gemini-2-5-flash-image. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81538163, 2024. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, et al. Sonic: Shifting focus to global audio perception in portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 193 203, 2025. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jia-Liang Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fan Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Peng-Yu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhen Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Z. Xu, Yang-Dan Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. ArXiv, abs/2412.03603, 2024. URL https://api.semanticscholar.org/CorpusID:274514554. Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. Advances in Neural Information Processing Systems, 37: 131434131455, 2024. 11 Pingchuan Ma, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, and Maja Pantic. Auto-avsr: Audio-visual speech recognition with automatic labels. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. William S. Peebles and Saining Xie. Scalable diffusion models with transformers. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41724182, 2022. URL https: //api.semanticscholar.org/CorpusID:254854389. KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pp. 484492, 2020. Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Textto-video diffusion models with an expert transformer. ArXiv, abs/2408.06072, 2024. URL https://api.semanticscholar.org/CorpusID:271855655. Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image In Proceedings of the IEEE/CVF conference on computer vision and talking face animation. pattern recognition, pp. 86528661, 2023. 12 Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with high-resolution audio-visual dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 36613670, 2021."
        },
        {
            "title": "A MORE QUALITATIVE RESULTS",
            "content": "We provide additional qualitative results to further demonstrate the effectiveness of our approach across various scenarios and applications. Diverse characters and environments. Fig.8 presents qualitative comparisons across wide range of subjects, backgrounds, and lighting conditions, based on AI-generated image inputs. Each row shows the progression of generated frames at different timestamps, comparing baseline methods with our approach. These results demonstrate that our method consistently preserves character identity and maintains visual quality. Extended qualitative comparisons on AVSpeech. Fig. 9 and 10 provide extensive qualitative comparisons on the AVSpeech dataset (Ephrat et al., 2018). For each example, we show baseline models in the top rows and our approach in the bottom rows, presenting frames throughout the generation sequence. These comparisons reveal that while baseline methods suffer from progressive identity drift, particularly visible in facial features and hair details after multiple autoregressive steps, our method maintains consistent character appearance and preserves fine facial details throughout the entire sequence. Narrative-driven genration application. Fig. 11 showcases the versatility of our approach through narrative-driven video generation. By leveraging an external text-based image editing model (Google, 2025) to create keyframes representing different emotional states or contexts, we demonstrate how our method can generate videos that follow desired storylines while maintaining natural audio synchronization. The figure illustrates smooth transitions between different narrative states, from neutral expressions to various emotional responses, all while preserving character identity and lip synchronization quality."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "Experimental setup for the pilot study in Fig. 3. We conduct our pilot study using the OmniAvatar (Gan et al., 2025) model to explore how pretrained video DiTs handle temporally distant frames. The model employs video VAE encoder that compresses input frames into (N 1)/4+1 latent frames through asymmetric temporal compression: the first frame maintains 1:1 correspondence in latent space, while subsequent frames undergo 4:1 temporal compression. To generate two frame videos with controlled temporal gaps with distance of k, we position the conditioning frame at temporal index ℓ = in the latent space while placing the frame to be generated at index ℓ = 0 (initialized with noise). For instance, when setting the latent distance to 3, we assign the conditioning frame positional embedding pt[3] and generate the target frame at position pt[0]. This configuration ensures the generated content corresponds to exactly one frame in pixel space after VAE decoding. By maintaining the generated frame at position 0, we preserve the direct correspondence between latent and pixel space representations, enabling precise evaluation of motion magnitude as function of simulated temporal distance. B.1 APPLYING OUR APPROACH TO EACH ANIMATION MODEL We detail the specific implementation of our approach for each of the three audio-conditional DiT models: Hallo3 (Cui et al., 2024), HunyuanVideo-Avatar (Chen et al., 2025a), and OmniAvatar (Gan et al., 2025), which are built upon CogVideoX (Yang et al., 2024), HunyuanVideo (Kong et al., 2024), and Wan2.1 (Wan et al., 2025), respectively. We train each model on 4 NVIDIA H200 GPUs with batch size of 8, which takes approximately 2 days. HunyuanVideo-Avatar (Chen et al., 2025a) and HunyuanVideo (Kong et al., 2024). HunyuanVideo-Avatar extends the HunyuanVideo model by incorporating reference images through three pathways: LLaVA (Liu et al., 2023) embeddings, feature addition via projection layers, and token concatenation along the sequence dimension. To integrate our approach, we modify the temporal positional embeddings of the concatenated reference tokens, assigning them positions beyond the generation window to encode their temporal distance. Specifically, we introduce seperate projection layer for the distant conditional frame, initialized from the weights of the existing projection layer for denoising tokens. Since HunyuanVideo-Avatar employs the sliding window mechanism proposed in Sonic for long video scenario, we adapt it for temporal autoregressive generation by incorporating 5 starting frames as boundary conditions, by concatenating their tokens to the input sequence. Each generation chunk produces 101 frames conditioned on the preceding 5 frames and the reference frame. All other training settings follow the standard procedures of HunyuanVideo and HunyuanVideo-Avatar. In our experiments, we compare our method against the baseline that uses 5 starting frames with LLaVA embeddings and feature addition for reference conditioning, evaluating the impact of adding temporally distant keyframe conditioning. OmniAvatar (Gan et al., 2025) and Wan2.1 (Wan et al., 2025). OmniAvatar builds upon Wan2.1, performing audio-conditional human animation by fine-tuning only the audio injection module and parameters for Low-Rank Adaptation (LoRA). The reference image is injected through channel-wise concatenation with the DiT input tokens, while starting frames are conditioned via token replacement (replacing the initial portion of noisy video tokens with clean starting frame tokens) to enable temporal autoregressive generation. To integrate our approach, we adopt similar strategy to HunyuanVideo-Avatar: distant keyframes are conditioned through token concatenation along the sequence dimension with separate projection layer, while their temporal positional embeddings are set to positions beyond the generation window. Following OmniAvatars training protocol, we finetune only the LoRA parameters while keeping all other parameters frozen. All remaining settings follow the standard OmniAvatar training procedures. Hallo3 (Cui et al., 2024) and CogVideoX (Yang et al., 2024). Hallo3 leverages CogVideoX as its base model, implementing reference image conditioning through transformer-based reference network and face embeddings from face encoder. Building upon Hallo3s architecture, we implement our distant keyframe conditioning through the existing reference network. Unlike Hallo3s original conditioning scheme, we apply temporal positional embeddings to the reference network input using the same encoding format as the DiTs positional encoding, enabling the network to perceive temporal distance between the current generation window and the distant keyframe."
        },
        {
            "title": "C USER STUDY PROTOCOL",
            "content": "We conducted comprehensive user study with 34 participants to evaluate the perceptual quality improvements when our approach is applied to existing baseline methods. Each participant evaluated 9 randomly selected video pairs from pool of 69 pre-generated pairs, resulting in total of 306 paired comparisons. The study design ensured balanced evaluation across different baseline models. For each participant, the 9 video pairs were distributed as follows: 3 pairs comparing HunyuanVideoAvatar baseline with our method applied on top of it, 3 pairs for OmniAvatar comparisons, and 3 pairs for Hallo comparisons. For each video pair, participants were asked to indicate their preference based on four distinct criteria: Character Consistency: Which model shows better character consistency (the person in the video is similar to the input image)? Overall Quality: Which model shows better overall quality results? Expression Realism: Which model shows more natural and realistic expressions and motion? Lip Sync Performance: Which model shows better lip sync performance? The presentation order of our method and the baseline was randomized and anonymized for each comparison. Participants were not informed which video corresponded to which method. An example of the user interface presented to participants is shown in Fig. 12. The full results of the user study are summarized in Tab. 6. 15 Models Lip-Sync Performance Character Consistency Expression Realism Overall Quality Hallo3 + LA (Ours) HunyuanAvatar* + LA (Ours) OmniAvatar + LA (Ours) 21.6% 79.4% 21.6% 79.4% 46.1% 53.9% 17.6% 82.4% 10.9% 89.2% 29.4% 70.6% 21.6% 78.4% 26.5% 73.5% 39.2% 60.8% 25.5% 74.5% 15.7% 84.3% 36.3% 63.7% Table 6: User study result."
        },
        {
            "title": "D LIMITATIONS AND FUTURE WORK",
            "content": "Our approach is designed to leverage pretrained video priors from base DiT models through minimal architectural modifications. Consequently, when certain base models exhibit specific limitations, such as challenges in generating precise hand gestures or complex body movements, our framework inherently preserves these characteristics. Additionally, when text prompts specify dramatic scene transitions that significantly deviate from the reference image context (e.g., transitioning from an indoor setting to outdoor environments), the model may struggle to synthesize plausible transitions. While our narrative-driven generation demonstrates that strategically placed keyframe anchors can facilitate such transitions, exploring fully dynamic scene changes with extreme background variations remains an interesting direction for future research. 16 Figure 8: Additional qualitative results across diverse characters and scenarios. Our method consistently maintains character identity and visual quality across different subjects, backgrounds, and lighting conditions. Each row compares baseline methods with our Lookahead Anchoring approach, demonstrating superior identity preservation and natural motion generation. 17 Figure 9: Additional qualitative comparisons on AVSpeech (Ephrat et al., 2018). We compare baseline models (top rows) with our Lookahead Anchoring approach (bottom rows). Our method maintains superior character consistency and facial detail preservation throughout extended generation sequences, while baselines exhibit progressive identity drift. Figure 10: Additional qualitative comparisons on AVSpeech (Ephrat et al., 2018). We compare baseline models (top rows) with our Lookahead Anchoring approach (bottom rows). Our method maintains superior character consistency and facial detail preservation throughout extended generation sequences, while baselines exhibit progressive identity drift. 19 Figure 11: Narrative-driven long video generation application. Our approach seamlessly integrates with external text-to-image editing models to create dynamic storylines. By positioning edited reference images as distant lookahead anchors, we achieve smooth narrative transitions while maintaining audio synchronization and character identity throughout the sequence. Figure 12: An example of the screen shown to participants in user study."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "KAIST"
    ]
}