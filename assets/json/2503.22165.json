{
    "paper_title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models",
    "authors": [
        "Zhanke Zhou",
        "Zhaocheng Zhu",
        "Xuan Li",
        "Mikhail Galkin",
        "Xiao Feng",
        "Sanmi Koyejo",
        "Jian Tang",
        "Bo Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 6 1 2 2 . 3 0 5 2 : r Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models Zhanke Zhou1 2 Zhaocheng Zhu3 4 Xuan Li1 Mikhail Galkin6 Xiao Feng1 Sanmi Koyejo2 Jian Tang3 5 Bo Han1 Abstract Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to model that predicts the property they observe. We showcase this advantage by adapting our tool to lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have revolutionized the paradigm of solving problems with their broad spectrum of capabilities. In particular, several Equal Contribution, 1TMLR Group, Hong Kong Baptist University, 2Stanford University, 3Mila - Quebec AI Institute, 4Universite de Montreal, 5HEC Montreal, 6Intel AI Lab. 1 Figure 1: Landscape of thoughts for visualizing the reasoning steps of LLMs. The darker regions in landscapes indicate more thoughts, with indicating incorrect answers and marking correct answers. Given question, we sample few thoughts from an LLM and divide them into two categories based on correctness. We visualize the landscape of each category by projecting the thoughts into two-dimensional feature space, where each density map reflects the distribution of states at reasoning step. With these landscapes, users can easily discover the reasoning patterns of an LLM or decoding algorithm. In addition, predictive model is applied to predict the correctness of landscapes and can help improve reasoning. useful applications of LLMs, such as tool use (Schick et al., 2023), retrievalaugmented generation (Lewis et al., 2020), and agents (Yao et al., 2023b), heavily rely on their capability of step-by-step reasoning (Wei et al., 2022; Kojima et al., 2022). Although many base models, e.g., OpenAI o1 (Jaech et al., 2024), and decoding algorithms, e.g., test-time scaling-up search (Snell et al., 2024), have been introduced to advance the performance of LLMs on these applications, the underlying reasoning behavior of LLMs remains unclear to the community. This not only hinders the development of algorithms but also poses potential risks at deployment (Anwar et al., 2024). few attempts (Wang et al., 2023a; Saparov and He, 2023; Saparov et al., 2023; Dziri et al., 2024) have been made to understand the reasoning capacity of LLMs. Nevertheless, these findings are often tied to certain decoding algorithms and tasks, which may not be so instructive for users working with their own algorithms and tasks. Instead, there is strong demand for tools that can be applied to analyze the reasoning behavior 2 of LLMs in the users scenarios. We foresee that such tools will benefit three groups of practitioners: 1) engineers can iterate their solutions faster based on the feedback from the tool; 2) researchers can improve decoding algorithms based on insights revealed by the tool; 3) safety researchers can utilize the tool to monitor, understand, and improve the behavior of LLMs. We made small but meaningful step towards the above goal by introducing the landscape of thoughts, tool for visualizing the reasoning paths produced by chain-of-thought and other step-by-step reasoning algorithms. Given any multi-choice reasoning dataset, our tool visualizes the distribution of intermediate states and any reasoning path of interest w.r.t. the answer choices, which enables users to uncover reasoning patterns of LLMs in both success and failure cases  (Fig. 1)  . The core idea is to characterize the textual states in reasoning path as features that quantify their distances to all answer choices. These distances are estimated by the perplexity metric, with the same LLM to generate thoughts and explain for itself. The state features are then projected to two-dimensional space via t-SNE (van der Maaten and Hinton, 2008), non-linear dimensionality reduction method to preserve manifolds in the original high-dimensional space. We examine our tool with different combinations of model sizes, decoding algorithms, and benchmark datasets. Our tool reveals several qualitative observations regarding the reasoning behaviors of LLMs. Some notable observations include: 1) the convergence speed of reasoning paths towards correct answers reflects the accuracy, no matter what base model, decoding algorithm, or dataset is used; 2) the convergence speed of reasoning paths in success and failure cases is distinct, indicating that we may use the convergence speed of reasoning path to predict its accuracy; 3) low consistency and high uncertainty are generally observed in the intermediate thoughts, presenting the unstable properties of the reasoning process. To our knowledge, these observations have not been reported by previous works that analyze chain-of-thought mostly based on performance metrics. Since our tool is built on the top of state features, it can be adapted to machine-learning model to quantitatively predict certain properties, such as the findings mentioned above. We showcase this advantage by training lightweight model to predict the success and failure cases, which is equivalent to verifiers commonly used in LLM reasoning (Cobbe et al., 2021). Even though this verifier is lightweight compared to most LLM-based verifiers, it consistently improves the reasoning performance on most combinations of models, decoding algorithms, and datasets in our experiments. Hence, users can further leverage this advantage to predict other potential properties that they discover in their own scenarios. 3 In summary, our main contributions are three-fold: We introduce the first visualization tool for inspecting the reasoning dynamics of different LLMs and decoding algorithms on any multi-choice reasoning dataset (Sec. 2). Our tool reveals several observations regarding the reasoning behaviors of different models, algorithms, and datasets, offering new insights into the reasoning (Sec. 3). Our tool can also be adapted to model to predict certain properties and guide the reasoning process, improving LLM reasoning without modifying parameters (Sec. 4)."
        },
        {
            "title": "2 Visualizing Multi-step Reasoning of LLMs",
            "content": "This section outlines general framework for language models and reasoning algorithms compatible with our tool (Sec. 2.1), demonstrates how it visualizes reasoning by projecting thoughts into two-dimensional space (Sec. 2.2), and introduces metrics for quantitative analysis (Sec. 2.3)."
        },
        {
            "title": "2.1 Problem Formulation",
            "content": "Our goal is to visualize the reasoning process of LLMs across variety of problem types. To achieve this, we aim for formulation that is sufficiently general to encompass wide range of use cases. Specifically, we focus on datasets consisting of multiple-choice questions, where each sample (x, y, C) comprises question x, correct answer y, and finite set of candidate choices = {cj }k j=1, all represented in textual format. The visualization tool applies to the following language models and reasoning algorithms. Language models. To explore the landscape of thoughts generated by an LLM pLLM(), it is necessary for the model to produce diverse reasoning paths for solving given problem. This requires the LLM to support sampling during inference (cid:98)y pLLM(yx, C). For chain-of-thought reasoning, thoughts x, C,(cid:98)t1, . . . ,(cid:98)ti1). Namely, each are sampled autoregressively as (cid:98)ti thought (cid:98)ti is conditioned on the problem x, the candidate set C, and the sequence of preceding thoughts (cid:98)t1, . . . ,(cid:98)ti1. To characterize intermediate states within these reasoning paths, the LLM must also function as likelihood estimator, enabling the computation of the probability pLLM((cid:98)yx, C,(cid:98)t1, . . . ,(cid:98)ti) of any generation (cid:98)y. These two requirements are generally satisfied by most open-source LLMs, such as Llama (Dubey et al., 2024), Mistral (Jiang et al., 2024), and DeepSeek (Liu et al., 2024). However, proprietary LLMs, such as pLLM(ti 4 GPT-4 (Achiam et al., 2023) and Gemini (Team et al., 2023), are excluded as they do not support likelihood estimation. Reasoning algorithms. While there are many approaches to solving reasoning problems with LLMs (Creswell et al., 2022; Kazemi et al., 2023), this work focuses on chain-of-thought (CoT) (Wei et al., 2022) and its derivatives (Zhou et al., 2023; Yao et al., 2023a), owing to their widespread use and development. These decoding algorithms generally guide the model in generating structured path of intermediate reasoning thoughts before arriving at the final answer. Note that to visualize large number of reasoning thoughts effectively, these thoughts should be automatically parsed into distinct units (e.g., via sentence tokenization). This requirement is typically satisfied by most variants of CoT."
        },
        {
            "title": "2.2 Landscape of Thoughts",
            "content": "Given collection of reasoning paths generated by an LLM, our tool seeks to visualize how different paths lead to either correct or incorrect answers within two-dimensional (2D) space, as illustrated in Fig. 1. key challenge lies in the absence of direct mapping from the textual space of thoughts to 2D coordinates. To address this gap, we first utilize the same LLM to represent intermediate states as numerical vectors. These state vectors are then projected into 2D space for visualization. For simplicity, we use the notation ti instead of (cid:98)ti, which is clear in the following. Characterizing the states. Here, the intermediate thoughts {ti }n i=1 in reasoning path naturally define sequence of states {si }n i=0, where s0 = [x] and si = [x, t1, t2, . . . , ti]. Here, we propose to characterize the states as feature vectors using the likelihood function of the LLM. Specifically, the k-dim feature vector si for state si is defined as follows: si = [d(si, c1), d(si, c2), . . . , d(si, ck)] , (2.1) where d(si, cj) measures the distance between state si and choice cj. In this context, the vector si indicates the relative distances from the state j=1. To reduce the effect of length on choices, si to all possible choices {cj }k we implement the distance calculation of d(si, cj) through the perplexity metric (Shannon, 1948; Manning, 1999) shown as below: 1 d(si, cj) = pLLM(cj si) 1/cj , (2.2) 1The (cid:18) 1 cj exp be perplexity can (cid:19) (cid:80)cj t=1 log pLLM(cj [t]si , cj [: t]) . also equivalently expressed as PPL(cj si ) = 5 is the number of tokens in cj, and pLLM(cj where cj si) is the accumulated probability in an autoregressive manner. We further normalize the vector si to have unit L1 normalization. Additionally, to represent the choices as landmarks in the visualization, it is necessary to encode the choices as feature vectors. Notably, we observe that perplexity decreases as the models prediction confidence increases. To align with this observation, we define the feature vector cj for choice cj in manner consistent with the perplexity: cj = 1 [1(j (cid:44) 1), . . . , 1(j (cid:44) k)] . (2.3) For paths, each with states, we compute the feature vectors for all states. 2 Together with the feature vectors of choices, we obtain feature matrix Rk(rn+k) = [s(1) 1 , . . . , s(1) , . . . , s(r) 1 , . . . , s(r) , c1, . . . , ck]. (2.4) Note that sufficiently large number of paths is necessary to generate comprehensive visualization of the reasoning landscape. However, visualizing all samples in dataset under this setting incurs significant computational cost. In practice, we found it more efficient to visualize paths with samples projected into the same space. This approach retains much of the visualization quality while substantially reducing the number of paths required for each sample. The key idea is to rearrange the order of choices such that the correct answer consistently aligns with the same dimension in the k-dimensional feature space across all the samples. Visualization. After constructing the feature matrix S, we project the states and choices into 2D space for visualization. This dimensionality reduction step can be accomplished using various existing algorithms (Pearson, 1901; van der Maaten and Hinton, 2008; McInnes et al., 2018). In this study, we employ t-SNE (van der Maaten and Hinton, 2008) due to its ability to preserve the underlying manifolds of the original high-dimensional space and its robustness to wide range of transformations. By applying t-SNE to the k-dim S, we obtain the 2-dim coordinates R2(rn+k). The coordinates of the states define discrete density function in the 2D space. To create more intuitive and visually interpretable representation, we smooth this density function using Parzen window estimator (Silverman, 2018). The smoothed density at given coordinate is as follows, 2Our tool can also be applied to paths with different numbers of states. We assume states for demonstration purposes. 6 where the σ controls the radius of Gaussian kernels. (cid:33) (cid:32) 1 rn (cid:88) S exp s2 2σ . (2.5) p( v) ="
        },
        {
            "title": "2.3 Metrics",
            "content": "Besides the qualitative 2D visualization, we introduce three quantitative metrics to help understand the behavior of the LLM at different reasoning steps, which are defined on the intermediate states introduced in Sec. 2.2. Consistency. To understand whether the LLM knows the answer before generating all thoughts, we compute the consistency of state si by checking whether si and sn agree Consistency(si) = 1(argmin si = argmin sn). (2.6) Uncertainty. To know how confident the LLM is about its predictions at intermediate steps, we compute the uncertainty of state si as the entropy of si (note = 1) (cid:80) dsi Uncertainty(si) = log d. (cid:88) dsi (2.7) Perplexity. We are also interested in how confident the LLM is about its thoughts. We use the perplexity of thought ti, since it is comparable across thoughts of different length Perplexity(ti) = pLLM(ti si1) 1/ti . (2.8)"
        },
        {
            "title": "3 Results and Observations",
            "content": "In this section, we utilize the landscape of thoughts to analyze the reasoning behavior of LLMs. Specifically, we conduct comprehensive evaluation and extract several insightful observations by comparing the landscape of thoughts across (1) reasoning algorithms (Sec. 3.1), (2) reasoning tasks (Sec. 3.2), and (3) language models (Sec. 3.3). To help understand the qualitative visualizations, we quantitatively calculate the consistency and uncertainty of states, as well as the perplexity of thoughts, all previously introduced in Sec. 2.3. Unless stated otherwise, we employ Llama-3.1-70B with CoT as the default configuration in evaluations. More visualization cases are in Appendix B. 7 ) ( ) ( C ) ( ) ( Figure 2: Comparing the landscapes and corresponding metrics of four reasoning algorithms (using Llama-3.1-70B on the AQuA dataset). Through the reasoning progression, spanning from early (0-20% states) to the later stages (80-100% states), the visualization shows correct cases (bottom row in blue) with incorrect cases (top row in red). Note the darker regions indicating incorrect answers represent higher density of states, with and marking correct answers. The accuracy of reasoning for the four subfigures is: (a) 84.4%, (b) 82.2%, (c) 75.8%, and (d) 81.6%, respectively."
        },
        {
            "title": "3.1 Comparison across Reasoning Algorithms",
            "content": "Setup. We evaluate the default model with four reasoning algorithms: chainof-thought (CoT) (Wei et al., 2022), least-to-most (LtM) (Zhou et al., 2023), MCTS (Zhang et al., 2024), and tree-of-thought (ToT) (Yao et al., 2023a). We run these algorithms on 50 problems randomly selected from the AQuA dataset. The corresponding landscapes are presented in Fig. 2, which yield the following observations. Observation 3.1 (The landscapes converge faster to the correct answers are of higher reasoning accuracy). By comparing the four groups of landscapes in Fig. 2, we observe that the states scatter dispersedly at early stages and 8 gradually converge to correct (or incorrect) answers in later stages. Here, converge means the trend of reasoning path approaching one answer. As can be seen from Fig. 2, different reasoning algorithms present diverse landscapes. Generally, methods with more scattered landscapes (converge slower) present lower accuracy than those that converge faster. Observation 3.2 (Wrong paths quickly converge to wrong answers, while correct paths slowly step to correct answers). By comparing the landscapes of failure and success paths, it is found that the failure paths usually converge to the wrong answers at 20-40% states. By contrast, the states in the success paths converge to the correct answers at 80-100% states. This implies that early states can lead to any potential answers (from model perspective), while the correct answers are usually determined at the end of reasoning paths. Observation 3.3 (Compared to failure paths, the intermediate states in correct paths have higher consistency w.r.t. the final state). By comparing the consistency plots in Fig. 2, we found that the model generally has low consistency between the intermediate states and the final state. Notably, the consistency of wrong paths is significantly lower than that of correct paths. This implies that the reasoning process can be quite unstable. Even though decoding algorithms like CoT and LtM are designed to solve problem directly (without explorations), the generated thoughts by these methods do not consistently guide the reasoning path to the answer."
        },
        {
            "title": "3.2 Comparison across Reasoning Tasks",
            "content": "Setup. Besides the AQuA, we include MMLU, CommonsenseQA, and StrategyQA datasets. We run the base model with CoT on 50 problems per dataset. The observations followed are derived from the landscapes in Fig. 3. Observation 3.4 (Similar reasoning tasks exhibit similar landscapes). The landscapes of AQuA, MMLU, and StrategyQA exhibit organized search behavior with higher state diversity, while CommonSenseQA presents concentrated search regions, reflecting direct knowledge retrieval rather than step-bystep reasoning processes. These distinct landscape patterns demonstrate the potential to reveal underlying domain relationships across different reasoning tasks. Observation 3.5 (Different reasoning tasks present significantly different patterns in consistency, uncertainty, and perplexity). The histograms in Fig. 3 show that path perplexity consistently increases during reasoning across all datasets. AQuA and MMLU show distinctly higher levels of uncertainty. 9 A ) ( M ) ( g r ) ( s n o ) ( Figure 3: Comparing the landscapes and corresponding metrics of different datasets (using Llama-3.1-70B with CoT). Darker regions represent higher state density, with indicating incorrect answers and marking correct ones. In addition, the accuracy of reasoning for the four subfigures is: (a) 84.4%, (b) 80.2%, (c) 75.8%, and (d) 64.8%, respectively. For StrategyQA, correct paths show increasing consistency that surpasses incorrect paths at around 60% states, while incorrect paths show decreasing consistency. However, extending beyond the typical three-step requirement (Geva et al., 2021), the later stages (60-100% states) show increasing perplexity as well as lower uncertainty."
        },
        {
            "title": "3.3 Comparison across Language Models",
            "content": "Setup. In this part, we study several LLMs behavior across different parameter scales (1B, 3B, 8B, and 70B). We run each model with CoT on 50 problems from the AQuA dataset. The landscapes of these models are shown in Fig. 4. 10 . 1 - 2 3 - l . 3 - 2 3 - l . 8 - 1 3 - l . 0 7 - 1 3 - l Figure 4: Comparing the landscapes and corresponding metrics of different language models (with CoT on the AQuA dataset). Darker regions represent indicating incorrect answers and marking higher state density, with correct ones. In addition, the accuracy of reasoning for the four subfigures is: (a) 15.8%, (b) 42.0%, (c) 53.2%, and (d) 84.4%, respectively. Observation 3.6 (The landscape converges faster as the model size increase). As model parameters scale from 1B to 70B, the corresponding landscape demonstrates faster convergence to the correct answers with higher density in the last 20% states, aligning with the increasing accuracy. With more parameters to store information, larger models can access broader knowledge (Allen-Zhu and Li, 2024). This leads to more confident solutions, demonstrated by more focused answer patterns and lower uncertainty. Observation 3.7 (Larger models have higher consistency, lower uncertainty, and lower perplexity). As the model size increases, the consistency increases, at the same time, the uncertainty and perplexity decrease significantly. This also aligns with the higher accuracy for the large models."
        },
        {
            "title": "4 Adapting Visualization to Predictive Models",
            "content": "One advantage of our method is that it can be adapted to model to predict any property users observe. Here, we show how to convert our method to lightweight verifier for voting reasoning paths, following the observations in Sec. 3. Note that this methodology is not limited to verifiers. Users can use the same technique to adapt the visualization tool to monitor other properties in their own scenarios."
        },
        {
            "title": "4.1 A Lightweight Verifier",
            "content": "Observation 3.2 and 3.3 show that the convergence speed and consistency of intermediate states can distinguish correct and wrong paths. Inspired by these observations, we build model : R(k+1)n {0, 1} to predict }n the correctness of reasoning path based on the state features {si i=1 and consistency metric {Consistency(si)}n i=1. The insight is that the state features, used to compute the 2-D visualization, encode rich location information of the states and can be used to estimate the convergence speed. Due to the small dimensionality of these features, we parameterize with random forest (Breiman, 2001) to avoid overfitting. We use this model as verifier to enhance LLM reasoning (Cobbe et al., 2021). Unlike popular verifiers (Lightman et al., 2023) that involve moderately sized language model on textual thoughts, our verifier operates on state features and is super lightweight. We train verifier on thoughts sampled on the training split of each dataset and apply it to vote reasoning paths at test time. Given paths sampled by decoding algorithm, the final prediction is produced by weighted majority voting given as follows: (cid:98)y = argmax cC q(cid:88) i= 1((cid:98)y(i) = c) ({si i=1, {Consistency(si)}n }n i=1). (4.1)"
        },
        {
            "title": "4.2 Experimental Results",
            "content": "We evaluate our numerical verifier against an unweighted voting baseline (Wang et al., 2023b) with various language models, decoding algorithms, and reasoning datasets. Detailed experimental settings are in Appendix B.1. Effectiveness of the verifier. We first compare our verifier against the unweighted voting baseline, each applied to 10 reasoning paths. As shown in Fig. 5, our verifier consistently enhances the reasoning performance of all 12 Figure 5: The accuracy of reasoning (averaging across all four datasets). Results for each dataset are in Appendix B.4. (a) Transfer across datasets (b) Transfer across models Figure 6: Demonstration of the inference-time scaling effect of the verifier. We show the voting accuracy (%) on StrategyQA scales with the number of reasoning paths. Figure 7: Absolute accuracy changes ( Acc) with the verifier, compared to performance in Fig. 5 (without the verifier). The verifier is trained on each column (dataset or model) and evaluated on all rows (other datasets or models). Positive values indicate improvement in accuracy with the verifier. models and decoding algorithms, even though our verifier does not use any pre-trained language model. Notably, smaller language models (1B and 3B) show significant performance gains with the verifiers assistance, achieving substantial improvements over their original capabilities of reasoning. Test-time scaling. While the improvement of the verifier seems marginal with 10 reasoning paths, our verifier can provide substantial performance gain with more reasoning paths. We adjust the number of reasoning paths from 1 to 50, and plot the results of the verifier and the unweighted voting baseline in Fig. 6. Models with our verifier exhibit significantly stronger scaling behaviors, achieving over 65% accuracy. In contrast, the performance of the baseline saturated around 30% accuracy. These results suggest that our state features, which are used in both the visualization tool and the verifier, capture important information about the reasoning behavior of LLMs. Thus, the verifier can boost test-time scaling, especially in solving complex problems. 13 Cross-dataset and cross-model transferability. One interesting property of the state features and metrics is that their shape and range are agnostic to the model and dataset, suggesting that we may deploy the verifier trained on one dataset or model in another setting. As illustrated in Fig. 7, we evaluate how the verifier transfers across reasoning datasets (e.g., train on AQuA and test on MMLU) and model scales (e.g., train on 1B model and test on 70B model). We observe some positive transfers across datasets and models. For example, verifier trained on AQuA can improve the performance of StrategyQA by 4.5%. verifier trained on the 70B model also improves the performance of the 3B model by 5.5%. However, some cases do not benefit from the transferring verifiers. We leave improving the transferability of the state features and metrics as future work."
        },
        {
            "title": "5 Related Work",
            "content": "Reasoning with large language models. Chain-of-Thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022) has empowered LLMs to tackle multi-step reasoning problems by generating intermediate steps before producing final answer. Building upon CoT, numerous methods have been proposed to address various challenges, including compositional generalization (Zhou et al., 2023; Khot et al., 2023), planning (Yao et al., 2023a; Hao et al., 2023), and rule learning (Zhu et al., 2023) within the CoT reasoning. Beyond solving reasoning tasks, CoT has also emerged as foundational framework for other techniques, such as fine-tuning LLMs (Zelikman et al., 2022), enabling LLM-based agents (Yao et al., 2023b), and facilitating testtime scaling (Snell et al., 2024). Nevertheless, most of these approaches are developed in trial-and-error manner, largely due to the absence of proper tools for analyzing the CoT. Understanding chain-of-thought reasoning. There are few studies that explore what makes CoT prompting effective by perturbing its exemplars. To be specific, Madaan and Yazdanbakhsh (2022) found that the text and patterns of exemplars help CoT generate sentences resembling correct answers. Besides, Wang et al. (2023a) highlighted the importance of maintaining the correct order of reasoning steps, while Ye et al. (2022) demonstrated that using complementary exemplars can enhance reasoning performance. Furthermore, CoT can benefit from longer reasoning chains, even without new information to the prompt (Jin et al., 2024). Another line of research investigates CoTs general behavior (Tang et al., 2023; Saparov and He, 2023; Saparov et al., 2023; Shi et al., 2023). For exam14 ple, CoT heavily depends on the semantic structure of the problem to perform reasoning (Tang et al., 2023), struggles with planning and unification in deductive reasoning (Saparov and He, 2023), has difficulty generalizing to longer reasoning paths (Saparov et al., 2023), and can be easily misled by irrelevant information in the context (Shi et al., 2023). However, these observations are derived from specific reasoning tasks and prompt settings, limiting their applicability to other scenarios. In contrast, we introduce general-purpose tool that allows users to analyze reasoning in their contexts. Tools for analyzing chain-of-thought. To the best of our knowledge, the only existing tool for analyzing CoT is gradient-based feature attribution (Wu et al., 2023), which computes saliency score for each input token based on the models output. However, these token-level saliency scores do not directly capture the thought-level, multi-step reasoning process of LLMs. Consequently, the main finding in (Wu et al., 2023) is that CoT stabilizes saliency scores on semantically relevant tokens compared to direct prompting. Metrics designed to quantify CoT performance (Chen et al., 2024; Ton et al., 2024) can also be used to analyze the reasoning behaviors of LLMs. For instance, Ton et al. (2024) employs information gain to identify failure modes in reasoning paths, aligning with Observation 3.2 in this paper. However, our 2-D visualization offers significantly deeper insights than single information gain metric. Additionally, the verifier derived from our tool is conceptually related to outcome-supervised reward models (Cobbe et al., 2021)."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces the landscape of thoughts, visualization tool for analyzing the reasoning paths produced by large language models with chain-of-thought. Built on top of feature vectors of intermediate states in reasoning paths, our tool reveals several insights into LLM reasoning, such as the relationship between convergence and accuracy, as well as issues like low consistency and high uncertainty. Our tool can also be adapted to predict the observed property, which is demonstrated by lightweight verifier developed based on the feature vectors and our observations. We foresee that the landscape of thoughts will create several opportunities for developing, understanding, and monitoring LLM reasoning. One limitation of the landscape of thoughts is its applicability only to multiple-choice tasks. Future work could focus on adapting this tool for 15 open-ended reasoning tasks, such as mathematical problem-solving, code generation, and planning, where reasoning paths are less structured and more complex. Additionally, further research could aim to make the tool more accessible by generating intuitive visual and textual explanations, enabling non-experts to better understand and trust the reasoning processes of LLMs. Another promising direction is the development of automated methods to detect reasoning failures at scale, which could enhance the reliability of LLMs across diverse applications."
        },
        {
            "title": "Impact Statement",
            "content": "Our work presents tool for visualizing and understanding reasoning steps in large language models. We foresee our work will introduce more interpretability and transparency into the development and deployment of LLMs, advancing us toward more trustworthy machine learning. However, we must acknowledge that malicious activities can also be augmented by our tool. For example, attackers may use this tool to find prompts that bypass the alignment safeguards in LLMs. We believe such risks will be mitigated if this tool is widely adopted by safety researchers. Overall, the positive societal consequences of our work outweigh the negative ones, which stem primarily from misuse."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. et al. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Alain, G. and Bengio, Y. (2016). Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644. Allen-Zhu, Z. and Li, Y. (2024). Physics of language models: Part 3.3, knowledge capacity scaling laws. arXiv preprint arXiv:2404.05405. Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O. et al. (2024). Foundational challenges in assuring alignment and safety of large language models. TMLR. Breiman, L. (2001). Random forests. Machine learning. 16 Chen, Q., Qin, L., Wang, J., Zhou, J. and Che, W. (2024). Unlocking the capabilities of thought: reasoning boundary framework to quantify and optimize chain-of-thought. In NeurIPS. Clark, K., Khandelwal, U., Levy, O. and Manning, C. D. (2019). What does bert look at? an analysis of berts attention. In ACL Workshop BlackboxNLP. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R. et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Creswell, A., Shanahan, M. and Higgins, I. (2022). Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A. et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., Welleck, S., West, P., Bhagavatula, C., Le Bras, R. et al. (2024). Faith and fate: Limits of transformers on compositionality. In NeurIPS. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T. et al. (2021). mathematical framework for transformer circuits. Transformer Circuits Thread. Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D. and Berant, J. (2021). Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. TACL. Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D. and Hu, Z. (2023). Reasoning with language model is planning with world model. In EMNLP. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J. (2021). Measuring massive multitask language understanding. In ICLR. Hewitt, J. and Liang, P. (2019). Designing and interpreting probes with control tasks. In EMNLP. Ippolito, D., Tram`er, F., Nasr, M., Zhang, C., Jagielski, M., Lee, K., Choquette-Choo, C. A. and Carlini, N. (2022). Preventing verbatim memorization in language models gives false sense of privacy. arXiv preprint arXiv:2210.17546. 17 Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A. et al. (2024). Openai o1 system card. arXiv preprint arXiv:2412.16720. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F. et al. (2024). Mixtral of experts. arXiv preprint arXiv:2401.04088. Jin, M., Yu, Q., Shu, D., Zhao, H., Hua, W., Meng, Y., Zhang, Y. and Du, M. (2024). The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925. Kazemi, M., Kim, N., Bhatia, D., Xu, X. and Ramachandran, D. (2023). Lambada: Backward chaining for automated reasoning in natural language. In ACL. Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P. and Sabharwal, A. (2023). Decomposed prompting: modular approach for solving complex tasks. In ICLR. Kobayashi, G., Kuribayashi, T., Yokoi, S. and Inui, K. (2020). Attention is not only weight: Analyzing transformers with vector norms. In EMNLP. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. and Iwasawa, Y. (2022). Large language models are zero-shot reasoners. In NeurIPS. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., uttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T. et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I. and Cobbe, K. (2023). Lets verify step by step. arXiv preprint arXiv:2305.20050. Ling, W., Yogatama, D., Dyer, C. and Blunsom, P. (2017). Program induction by rationale generation: Learning to solve and explain algebraic word problems. In ACL. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C. et al. (2024). Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. 18 Madaan, A. and Yazdanbakhsh, A. (2022). Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686. Manning, C. (1999). Foundations of statistical natural language processing. The MIT Press. McInnes, L., Healy, J. and Melville, J. (2018). Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426. Pearson, K. (1901). Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science. Saparov, A. and He, H. (2023). Language models are greedy reasoners: systematic formal analysis of chain-of-thought. In ICLR. Saparov, A., Pang, R. Y., Padmakumar, V., Joshi, N., Kazemi, M., Kim, N. and He, H. (2023). Testing the general deductive reasoning capacity of large language models using ood examples. In NeurIPS. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N. and Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. In NeurIPS. Shannon, C. E. (1948). mathematical theory of communication. The Bell System Technical Journal. Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., Scharli, N. and Zhou, D. (2023). Large language models can be easily distracted by irrelevant context. In ICML. Silverman, B. W. (2018). Density estimation for statistics and data analysis. Routledge. Snell, C., Lee, J., Xu, K. and Kumar, A. (2024). Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Speer, R., Chin, J. and Havasi, C. (2017). Conceptnet 5.5: An open multilingual graph of general knowledge. In AAAI. Talmor, A., Herzig, J., Lourie, N. and Berant, J. (2019). Commonsenseqa: question answering challenge targeting commonsense knowledge. In NAACL. Tang, X., Zheng, Z., Li, J., Meng, F., Zhu, S.-C., Liang, Y. and Zhang, M. (2023). Large language models are in-context semantic reasoners rather than symbolic reasoners. arXiv preprint arXiv:2305.14825. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K. et al. (2023). Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Tenney, I., Das, D. and Pavlick, E. (2019). Bert rediscovers the classical nlp pipeline. In ACL. Ton, J.-F., Taufiq, M. F. and Liu, Y. (2024). Understanding chain-of-thought in llms through information theory. arXiv preprint arXiv:2411.11984. van der Maaten, L. and Hinton, G. E. (2008). Visualizing data using t-sne. JMLR. Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L. and Sun, H. (2023a). Towards understanding chain-of-thought prompting: An empirical study of what matters. In ACL. Wang, X., Amayuelas, A., Zhang, K., Pan, L., Chen, W. and Wang, W. Y. (2024). Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation. In ICML. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A. and Zhou, D. (2023b). Self-consistency improves chain of thought reasoning in language models. In ICLR. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V. and Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models. In NeurIPS. Wu, S., Shen, E. M., Badrinath, C., Ma, J. and Lakkaraju, H. (2023). Analyzing chain-of-thought prompting in large language models via gradientbased feature attributions. arXiv preprint arXiv:2307.13339. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y. and Narasimhan, K. R. (2023a). Tree of thoughts: Deliberate problem solving with large language models. In NeurIPS. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R. and Cao, Y. (2023b). React: Synergizing reasoning and acting in language models. In ICLR. 20 Yao, Y., Zhang, N., Xi, Z., Wang, M., Xu, Z., Deng, S. and Chen, H. (2024). Knowledge circuits in pretrained transformers. In NeurIPS. Ye, X., Iyer, S., Celikyilmaz, A., Stoyanov, V., Durrett, G. and Pasunuru, R. (2022). Complementary explanations for effective in-context learning. arXiv preprint arXiv:2211.13892. Zelikman, E., Wu, Y., Mu, J. and Goodman, N. (2022). Star: Bootstrapping reasoning with reasoning. In NeurIPS. Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y. and Tang, J. (2024). Rest-mcts*: Llm self-training via process reward guided tree search. In NeurIPS. Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V. and Chi, E. H. (2023). Least-to-most prompting enables complex reasoning in large language models. In ICLR. Zhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans, D. and Dai, H. (2023). Large language models can learn rules. arXiv preprint arXiv:2310.07064."
        },
        {
            "title": "Appendix",
            "content": "A Further Discussions 22 Supplementary Results and Analysis 23 B.1 Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.3 Decoding Algorithms . . . . . . . . . . . . . . . . . . . . . . . 24 B.4 Visulizations . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Further Discussions",
            "content": "Currently, the fundamental mechanisms behind both successful and unsuccessful reasoning attempts in LLMs remain inadequately understood. Traditional performance metrics, such as accuracy, provide insufficient insights into model behavior. While human evaluation has been employed to assess the quality of sequential thoughts (e.g., logical correctness and coherence), such approaches are resource-intensive and difficult to scale. We identify three challenges in developing automated analysis systems for LLMs reasoning: Challenge 1: Bridging the token-thought gap. Current explanatory tools, including attention maps (Clark et al., 2019; Kobayashi et al., 2020), probing (Alain and Bengio, 2016; Tenney et al., 2019; Hewitt and Liang, 2019), and circuits (Elhage et al., 2021; Yao et al., 2024), primarily operate at the token-level explanation. While these approaches offer valuable insights into model inference, they struggle to capture the emergence of higher-level reasoning patterns from lower-level token interactions. Additionally, the discrete nature of natural language thoughts poses challenges for traditional statistical analysis tools designed for continuous spaces. Understanding how thought-level patterns contribute to complex reasoning capabilities requires new analytical frameworks that can bridge this conceptual gap. Challenge 2: Analyzing without training data access. Existing investigations into LM reasoning have predominantly focused on correlating test questions with training data (Ippolito et al., 2022; Wang et al., 2024). This approach becomes particularly infeasible given the reality of modern LLMs: many models are closed-source, while some offer only model weights. Therefore, desired analysis framework should operate across varying levels of model accessibility. Challenge 3: Measuring reasoning quality. Beyond simple performance metrics, we need new ways to evaluate the quality and reliability of model 22 reasoning. This includes developing techniques to understand reasoning paths, creating intermediate representations that capture both token-level and thought-level patterns, and designing metrics that can assess the logical coherence and validity of reasoning steps. Consequently, we propose that viable analysis of reasoning behavior should satisfy multiple criteria: it should operate in post-hoc manner with varying levels of model access, bridge the gap between token-level and thought-level analysis, and provide meaningful metrics for evaluating reasoning quality. Given the absence of tools meeting these requirements, we identify the need for new analytical framework that can address these challenges while providing useful insights for improving model reasoning capabilities."
        },
        {
            "title": "B Supplementary Results and Analysis",
            "content": "B.1 Settings Visualizing the landscape of thoughts fundamentally relies on the decoding probability of LLMs. To this end, we adopted four open-source models with varying parameter sizes, namely Llama-3.2-1B, Llama-3.2-3B, Llama-3.18B, and Llama-3.1-70B. We repeatedly sample 10 times from the target LLM using the same reasoning strategy as self-consistency (Wang et al., 2023b). For visualization purposes, we randomly sample 50 questions from the testing split of each dataset and generate reasoning paths with the setup described above. For training the lightweight verifier, we randomly sample 20 questions from the training split of each dataset to obtain the feature matrix S. We extract these features using three model scales: Llama-3.2-3B, Llama-3.1-8B, and Llama-3.1-70B. Despite the relatively small training set, it proves sufficient for our lightweight verifier, which we subsequently evaluate on the data for visualization in Sec. 3. B.2 Datasets AQuA (Ling et al., 2017). This dataset develops to challenge language models quantitative reasoning capabilities. The AQuA presents complex algebraic word problems in multiple-choice format, where only one is correct. Each problem requires numerical computation, deep linguistic understanding, and logical inference. It provides nuanced assessment of models ability to translate textual information into algebraic reasoning. 23 MMLU (Hendrycks et al., 2021). Spanning 57 distinct academic and professional domains, MMLU provides rigorous test of language models capabilities across humanities, social sciences, hard sciences, and technical disciplines. StrategyQA (Geva et al., 2021). This dataset is designed to evaluate implicit reasoning and multi-hop question answering. The dataset is characterized by yes/no questions that demand implicit reasoning strategies. Unlike straightforward factual queries, these questions require models to construct elaborate reasoning paths, showing hidden logical connections. CommonsenseQA (Talmor et al., 2019). This dataset assesses commonsense reasoning through multi-choice questions derived from the ConceptNet knowledge graph (Speer et al., 2017). The dataset aims to test models understanding of commonsense concepts and ability to make logical inferences. However, the questions often require the model to incorporate external knowledge to select the correct answer from plausible distractors. Note that AQuA, MMLU, and StrategyQA all demand exploratory traversal of intermediate reasoning states, resulting in diverse but structured landscapes. CommonsenseQA, conversely, represents distinct domain where answers depend on static knowledge rather than emergent reasoning pathways. B.3 Decoding Algorithms Chain of Thought (CoT) (Wei et al., 2022). CoT elicits the LLMs reasoning capabilities by incorporating few-shot examples that demonstrate explicit reasoning steps. It provides the model with exemplar reasoning traces to guide its problem-solving process. Zero-shot CoT (Kojima et al., 2022). The core idea of this prompt strategy lies in adding simple instructions, e.g., Lets think step by step. to the prompt, enabling models to generate reasoning traces without assigned task-specific examples. Least-to-Most (LtM) (Zhou et al., 2023). LtM is an innovative reasoning approach that systematically breaks down complex problems into progressively simpler subproblems. This approach mirrors human cognitive problem-solving strategies, where individuals naturally break down complex tasks into smaller, more comprehensible parts. Tree-of-Thought (ToT) (Yao et al., 2023a). ToT expanded this concept by creating more sophisticated, multi-branching reasoning framework. While CoT follows linear path of reasoning, ToT introduces more dynamic 24 exploration, allowing models to generate multiple reasoning paths simultaneously, evaluate them, and strategically prune less promising trajectories. Monte Carlo tree search (MCTS) (Zhang et al., 2024). MCTS is powerful computational algorithm originally developed for game-playing strategies, particularly in complex decision-making environments like chess and Go. The method uses probabilistic sampling and tree exploration to systematically navigate potential solution spaces, balancing exploring new possibilities with exploiting promising paths. We adopt the task-agnostic node expansion and evaluation prompt from ReST-MCTS (Zhang et al., 2024) to conduct our experiment across different tasks. B.4 Visulizations In this part, we provide the full visualization of the verifier performance and landscapes. In Fig. 8 to Fig. 11, we visualize the average voting accuracy (%) of different LLMs reasoning with and without verification on various datasets and methods. In Fig. 12 to Fig. 15, we display the landscape of different models on various datasets using four methods. We also provide case studies by visualizing the landscape with corresponding states in Fig 16 to Fig. 19. 25 Figure 8: Average voting accuracy (%) of reasoning with and without verification on AQuA. Figure 9: Average voting accuracy (%) of reasoning with and without verification on MMLU. Figure 10: Average voting accuracy (%) of reasoning with and without verification on StrategyQA. Figure 11: Average voting accuracy (%) of reasoning with and without verification on CommonSenseQA. 26 (a) Llama-3.2-1B with CoT on AQuA (b) Llama-3.2-1B with LtM on AQuA (c) Llama-3.2-1B with ToT on AQuA (d) Llama-3.2-1B with MCTS on AQuA Figure 12: The landscapes of various reasoning methods (using Llama-3.21B on the AQuA dataset). 27 (a) Llama-3.2-3B with CoT on AQuA (b) Llama-3.2-3B with LtM on AQuA (c) Llama-3.2-3B with ToT on AQuA (d) Llama-3.2-3B with MCTS on AQuA Figure 13: The landscapes of various reasoning methods (using Llama-3.23B on the AQuA dataset). 28 (a) Llama-3.1-8B with CoT on AQuA (b) Llama-3.1-8B with LtM on AQuA (c) Llama-3.1-8B with ToT on AQuA (d) Llama-3.1-8B with MCTS on AQuA Figure 14: The landscapes of various reasoning methods (using Llama-3.18B on the AQuA dataset). 29 (a) Llama-3.1-70B with CoT on AQuA (b) Llama-3.1-70B with LtM on AQuA (c) Llama-3.1-70B with ToT on AQuA (d) Llama-3.1-70B with MCTS on AQuA Figure 15: The landscapes of various reasoning methods (using Llama-3.170B on the AQuA dataset). 30 Figure 16: Case Study: Landscape of thoughts of Llama-3.2-1B on AQuA using CoT. Figure 17: Case Study: Landscape of thoughts of Llama-3.2-3B on AQuA using CoT. 31 Figure 18: Case Study: Landscape of thoughts of Llama-3.1-8B on AQuA using CoT. Figure 19: Case Study: Landscape of thoughts of Llama-3.1-70B on AQuA using CoT."
        }
    ],
    "affiliations": [
        "HEC Montreal",
        "Intel AI Lab",
        "Mila - Quebec AI Institute",
        "Stanford University",
        "TMLR Group, Hong Kong Baptist University",
        "Universite de Montreal"
    ]
}