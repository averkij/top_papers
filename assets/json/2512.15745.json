{
    "paper_title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "authors": [
        "Tiwei Bie",
        "Maosong Cao",
        "Kun Chen",
        "Lun Du",
        "Mingliang Gong",
        "Zhuochen Gong",
        "Yanmei Gu",
        "Jiaqi Hu",
        "Zenan Huang",
        "Zhenzhong Lan",
        "Chengxi Li",
        "Chongxuan Li",
        "Jianguo Li",
        "Zehuan Li",
        "Huabin Liu",
        "Ling Liu",
        "Guoshan Lu",
        "Xiaocheng Lu",
        "Yuxin Ma",
        "Jianfeng Tan",
        "Lanning Wei",
        "Ji-Rong Wen",
        "Yipeng Xing",
        "Xiaolu Zhang",
        "Junbo Zhao",
        "Da Zheng",
        "Jun Zhou",
        "Junlin Zhou",
        "Zhanchao Zhou",
        "Liwang Zhu",
        "Yihong Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced."
        },
        {
            "title": "Start",
            "content": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B Tiwei Bie1, Maosong Cao1, Kun Chen1, Lun Du1, Mingliang Gong1, Zhuochen Gong1, Yanmei Gu1, Jiaqi Hu1,3, Zenan Huang1, Zhenzhong Lan1,4,, Chengxi Li1, Chongxuan Li2, Jianguo Li1,, Zehuan Li1, Huabin Liu1, Ling Liu1, Guoshan Lu1, Xiaocheng Lu1,5, Yuxin Ma1, Jianfeng Tan1, Lanning Wei1, Ji-Rong Wen2, Yipeng Xing1, Xiaolu Zhang1, Junbo Zhao1,3,, Da Zheng1,, Jun Zhou1, Junlin Zhou1, Zhanchao Zhou1,4, Liwang Zhu1, Yihong Zhuang1 1Ant Group, 2Renmin University of China, 3Zhejiang University, 4Westlake University, 5HongKong University of Science and Technology"
        },
        {
            "title": "Abstract",
            "content": "This paper presents LLaDA2.0 tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models establishing new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts pre-trained AR model into dLLM with novel 3-phase block-level WSD based training scheme: progressive increasing blocksize in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced. Huggingface: https://hf.co/collections/inclusionAI/llada-20 5 2 0 2 0 1 ] . [ 1 5 4 7 5 1 . 2 1 5 2 : r Figure 1: LLaDA2.0-flash main results. Authors are listed in alphabetical order based on last name. indicates tech-leaders."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models have achieved remarkable success through the AR paradigm, modeling sequences via next-token prediction with strict left-to-right causal dependencies (Hurst et al., 2024; Grattafiori et al., 2024; Yang et al., 2025). This approach naturally aligns with the sequential structure of language and enables efficient training through next-token likelihood maximization. However, the very success of this paradigm creates fundamental limitations: the sequential generation process imposes severe inference bottlenecks, precluding parallelization, and increasing latency at scale, while the rigid causal structure can be suboptimal for tasks requiring bidirectional reasoning and holistic understanding. Discrete Masked Diffusion Language Models (MDLM) have emerged as compelling alternative to the prevailing AR paradigm. By reconstructing sequences from random masked inputs, these models inherently support parallel generation and leverage full bidirectional context, offering different architectural approach (Gong et al., 2025; Yu et al., 2025). Although these conceptual advantages are clear, the field is still in an early developmental stage. Current research is actively focused on key challenges, including the refinement of specialized training regimes, the design of efficient sampling strategies, the efficient inference of opensource models, and reinforcement learning for MDLM. As result of this ongoing exploration, most existing diffusion models, including recent advancements like Block Diffusion Language Models (BDLMs) (Arriola et al., 2025), operate at smaller scale (e.g., 8B parameters). Bridging this scale difference to the hundreds of billions of parameters seen in the leading mainstream AR models is primary frontier for enabling diffusion models to fully capture complex linguistic patterns for practical deployment. In this work, we introduce LLaDA2.0 series with 100B/16B total parameters diffusion language models that resolves these fundamental challenges through novel two-stage continual pre-training (CPT) paradigm. Rather than attempting to train diffusion models from scratch, we leverage existing AR checkpoints as the foundation for systematic conversion process that preserves linguistic knowledge while introducing diffusion capabilities. The first stage, CPT aims to transform the foundational AR model into capable diffusion language model. However, direct conversion is challenging due to the inherent data distribution gap between left-to-right generation and bidirectional denoising. Although the BDLM formulation partially reduces this gap through blockwise masked reconstruction, it suffers from low data utilization, limiting the effective exploitation of large-scale corpora. To this end, we introduce the WarmupStableDecay (WSD) strategy, smoothly bridging the AR-to-dLLM gap while substantially improving CPT efficiency. WSD gradually expands the models receptive field to introduce diffusion-style context (Warmup), strengthens global denoising under full-sequence training (Stable), and then refines the model into an efficient blockwise structure (Decay). This progressive adjustment enables stable and data-efficient transition to diffusion-based learning. Additionally, under full attention in packed training sequences, diffusion models risk forming spurious dependencies across document boundaries, leading to semantic confusion and instability in bidirectional training. To prevent such cross-document interference, we introduce document-level attention mask that restricts self-attention within individual documents, ensuring coherent context modeling. The second stage, Post-training for Practical Deployment, transitions the model from raw predictive engine into capable and efficient assistant. The random masking nature of the diffusion fine-tuning objective means any single sample provides only partial learning signal. We address this by employing complementary masking strategy, which ensures near-100% data utilization and accelerates convergence by guaranteeing every token contributes to the models learning. With an efficient foundation for instruction tuning, we then align the model with human preferences by adapting modern techniques like Direct Preference Optimization (DPO)originally designed for AR modelsby reformulating the objective over the models reconstruction loss. Beyond alignment, practical deployment hinges on inference speed. To realize the full promise of parallel decoding, which is often limited by models lack of predictive confidence, we incorporate an auxiliary confidence prediction loss. This trains the model to be sharper and more certain, unlocking aggressive and efficient parallel generation without degrading quality. We release instruction-tuned variants for practical deployment: LLaDA2.0-mini (16B parameters) for resourceconstrained applications and LLaDA2.0-flash (100B parameters) for high-performance scenarios. Both variants retain the parallel decoding advantages of our diffusion training while being optimized for instruction following and safety through comprehensive post-training alignment. Our contributions provide practical recipe for the community to leverage AR stability while achieving diffusion parallelism, opening new possibilities for efficient large-scale language modeling."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Train dLLMs from scratch Auto-regressive language models (Ling et al., 2025; Moonshot, 2025; Liu et al., 2024; Meta-AI, 2025) are typically trained by maximizing the likelihood of predicting the next token. Under this paradigm, model performance has been shown to scale effectively with increasing model size, dataset volume, and computational resources, following well-established scaling laws. Recently, MDLMs (Song et al., 2025; Ye et al., 2025; Nie et al., 2025) have emerged as an alternative generative framework, reformulating text generation as an iterative denoising process. In each forward step, subset of tokens is randomly masked, and the model is trained to recover the original tokens conditioned on the remaining unmasked context. Encouraged by this paradigm shift, several studies have explored training MDLMs from scratch to assess their full potential. For instance, LLaDA (Nie et al., 2025) demonstrated that 8B dense MDLM, trained entirely from scratch, achieves performance competitive with similarly sized AR counterparts. Building upon this, LLaDA-MoE (Zhu et al., 2025) introduced the Mixture-of-Experts (MoE) architecture into the MDLM for the first time, showing that scratch-trained MoE-based MDLM can surpass dense models in both efficiency and capability, thereby validating the compatibility and scalability of MDLMs with advanced MoE designs. Moreover, due to the fundamentally different training dynamics compared to AR models, established training practices and hyperparameter recipes from the AR domain are often suboptimal for MDLMs. To address this gap, recent efforts such as Quakka (Ni et al., 2025) and OpenMoE2 (Ni & team, 2025) have begun investigating the scaling properties and optimal training strategies specifically tailored for MDLMs, laying the groundwork for principled scaling in this emerging paradigm. However, from-scratch trained MDLMs still lag behind state-of-the-art AR models in overall performance. This gap can be largely attributed to the disparity in training data volume and the maturity of infrastructure supportfactors that have been extensively optimized over years of development for AR models. Moreover, due to the high computational cost and long training cycles required for pretraining from scratch, MDLMs mentioned above are typically limited in model scale (8B), whereas leading AR models now routinely scale into tens or even hundreds of billions. 2.2 Scaling dLLMs with AR initialization Given the strong knowledge capacity and performance of AR models, several recent studies have explored initializing dLLMs from pre-trained AR models to reduce training costs and narrow the performance gap between AR models and dLLMs. For instance, DiffusionLLaMA (Gong et al., 2025) and Dream-7B (Ye et al., 2025) adopt mask annealing strategy to gradually transition from causal attention to bidirectional attention during training, while employing CART-based loss reweighting scheme to balance token-level learning dynamics. In contrast, RND1 (Keshigeyan et al., 2025) takes more direct approach by immediately converting the causal attention mechanism of the AR model into bidirectional one upon initialization. Notably, RND1 observes that when initializing DLM training from an AR model, preserving knowledgeintensive capabilities requires constraining updates to the models dense layers to prevent catastrophic forgetting. Block Diffusion Language Models (BDLMs) (Arriola et al., 2025) provide hybrid paradigm that balances efficiency and performance by combining diffusion and AR modeling. Tokens are generated block-wise: within each block, diffusion process reconstructs masked tokens, while blocks are produced auto-regressively. This design enables variable-length generation and supports KV-cache reuse during decoding, enhancing inference efficiency. Consequently, BDLMs can be effectively initialized from AR models, narrowing the performance gap. For example, SDAR (Cheng et al., 2025) leverages the Qwen-3 series (Yang et al., 2025) to train more efficient BDLMs. By exploring various block sizes and optimization strategies, it achieves performance comparable to its AR base model. However, one key limitation across all existing methods is their restricted model scaleranging only from 7B to 30B parametersleaving the feasibility and scalability of AR-initialized diffusion models largely unexplored at larger scales. Besides, the low training efficiency of block diffusion hinders its widely application to large-scale corpus for large-size models. Whether such initialization strategies can effectively generalize to models beyond the 30B scale remains an open question. 2.3 dLLMs post-training Beyond pre-training, post-training is crucial for unlocking the full potential of dLLMs by aligning them with specific tasks and human preferences. This process typically involves supervised fine-tuning (SFT) to instill 3 instruction-following capabilities, reinforcement learning (RL) to enhance complex reasoning, and inference optimization to address efficiency bottlenecks. Recent work has explored SFT to adapt dLLMs for specialized domains. For instance, Dream-Coder (Xie et al., 2025) fine-tunes 7B dLLM for code generation, demonstrating unique abilities like adaptive sketchthen-fill strategies for complex algorithms. Similarly, the general-purpose model Dream-7B (Ye et al., 2025) leverages SFT to achieve performance on par with top-tier AR models, while uniquely excelling at tasks requiring complex planning and constraint satisfaction. Other studies have investigated specialized finetuning strategies to balance quality and efficiency. Seed-Diffusion (Song et al., 2025), for example, employs two-stage curriculum learning strategy to train high-speed code generation model, while LiDAR (Liu et al., 2025) introduces hybrid think in diffusion, generate in AR architecture through fine-tuning, significantly boosting inference throughput while maintaining quality. To further enhance dLLMs reasoning abilities, researchers have begun adapting reinforcement learning techniques. However, applying standard policy gradient methods is challenging due to the intractable loglikelihood of dLLMs. To address this, SPG (Wang et al., 2025a) proposes novel Sandwich Policy Gradient algorithm that obtains more robust and less biased gradient by maximizing an evidence lower bound for high-reward samples and minimizing an evidence upper bound for low-reward ones. Another line of work, TraceRL (Wang et al., 2025d), focuses on aligning the training objective with the models multi-step generation trajectory. This framework led to the TraDo series of models, which have not only surpassed strong AR models on reasoning benchmarks but also produced the first dLLM capable of long-chain-of-thought reasoning. significant challenge for dLLMs is their slow inference speed, stemming from the iterative nature of the denoising process. To mitigate this, several acceleration methods have been proposed. DPad (Chen et al., 2025a) offers training-free solution by treating future tokens as dynamic scratchpad and using sliding window and distance-based pruning to reduce redundant computations, achieving dramatic speedup, especially for long sequence generation. In contrast, D2F (Wang et al., 2025c) introduces hybrid autoregressive-diffusion paradigm that enables parallel denoising of future text blocks even before preceding ones are fully generated. This approach allows dLLMs to leverage KV-caching and, for the first time, surpass the inference speed of equivalently sized AR models. Despite these advances, the field of dLLM post-training is still nascent. Systematic exploration of how these techniquesSFT, RL, and accelerationinteract with one another, and how they scale to models with hundreds of billions of parameters, remains an open and critical area for future research."
        },
        {
            "title": "3 LLaDA2.0 Training Paradigm",
            "content": "Figure (2) illustrates the holistic training pipeline of LLaDA2.0, staged and scalable framework designed to transform AR language models into highly efficient diffusion language models. Our paradigm follows three-stage progression: (1) Continual Pre-training from AR to MDLM, (2) Block Diffusion Pre-training to transition from token-level to block-level diffusion modeling, and (3) Post-training for alignment and task specialization. The process begins with strong AR base model. We first perform continual pre-training to adapt this model into an MDLM, where it learns to reconstruct randomly masked tokens in bidirectional, denoising fashion. This phase bridges the gap between AR and diffusion-based generation while preserving the representational geometry of the original model. Building upon the trained MDLM, we then introduce block diffusion pre-training, during which the model is further trained to denoise contiguous spans of textreferred to as blocksrather than individual tokens. This shift enables higher computational efficiency and better long-range coherence during generation. Finally, after mastering non-autoregressive generation at both token and block levels, the model undergoes post-trainingincluding SFT and DPO to align its outputs with human intent, instruction-following capability, and downstream application requirements. This stage ensures that the powerful generative backbone developed during diffusion pre-training translates into practical performance gains across diverse tasks. Overall, LLaDA2.0s training paradigm emphasizes knowledge inheritance, progressive adaptation, and efficiency-aware design, enabling seamless evolution from AR models to fluent, flexible, and fast diffusion large language models. Figure 2: schematic of the progressive training framework for transforming an AR model into MDLM. Continual Pre-Training Stage facilitates the Warmup-Stable-Decay strategies by scheduling block size LB enables smooth, stable, and effective attention mask adaptation. Post-training Stage facilitates the same block diffusion configuration conducting the instruction SFT, Confidence-Aware Parallel SFT, and DPO. The right panel illustrates the document-level block diffusion attention mask,which enables an efficient, vectorized forward pass by constructing single input sequence from multiple noisy and clean examples, such as [xnoisy1, . . . , xclean1, . . . ]. The forward pass then employs combination of block-diagonal (MBD), offset block-causal (MOBC), and block-causal (MBC) masks."
        },
        {
            "title": "4 Continual Pre-training via Warmup-Stable-Decay (WSD)",
            "content": "(cid:17) Takeaway (1) Warmup-Stable-Decay enables smooth and data-efficient conversion from AR to dLLMs. (2) The document-level attention mask ensures coherent bidirectional modeling within semantic boundaries. (3) Top-k Checkpoint Merge enhances performance and generalization by averaging the top model checkpoints. Converting pre-trained AR language model into high-performance diffusion language model is fundamentally challenging due to the misalignment in architectural inductive biases and training objectives. While AR models generate tokens sequentially from left to right, diffusion-based models rely on bidirectional context and learn to reconstruct corrupted sequences in arbitrary unmasking orders. direct objective switch often leads to unstable optimization and severe degradation of pretrained knowledge. To address this gap, we propose WarmupStableDecay (WSD) continual pre-training strategy that enables smooth, stable, and effective transition from AR to dLLM. WSD decomposes the conversion into three coordinated phases: Warmup: Progressively increase the block size in block diffusion language models (BDLM) to gradually transform the AR model into full-sequence masked diffusion language model (MDLM). Stable: Stabilize and enrich the models understanding of diffusion dynamics through large-scale training under the MDLM paradigm. Decay: Revert back to compact BDLM with smaller block sizes to achieve better speed-efficiency trade-offs during inference. This progressive schedule preserves the AR models priors while steadily adapting it to the structural requirements of diffusion modeling. Moreover, the document-level attention mask is applied throughout training to all input sequences. This mechanism is crucial for handling packed heterogeneous documents, preventing the model from forming spurious connections across unrelated texts, thereby ensuring semantic coherence and improving learning 5 stability within each document. In addition, we adopt top-k checkpoint merging strategy (Tian et al., 2025), to enhance generalization by averaging the parameters of the best-performing checkpoints, smoothing the parameter landscape, and yielding more robust final model with boosted performance. 4.1 Warmup-Stable-Decay Conversion Strategy We begin with the AR base models Ling-mini-2.0 and Ling-flash-2.0 (Ling et al., 2025), which can be viewed as special case of BDLM with block size 1. This perspective allows us to treat AR models as the initial BDLM configuration with minimal granularity. Phase-1: Progressive Block Size Warmup The core idea of the warmup phase is to gradually increase the block size, thereby expanding the receptive field within which the model performs joint denoising. Starting from block size LB = 1, we incrementally scale it up to 4, 32, then 64, and ultimately reach LB = 4096 at which point the entire sequence is treated as one single block. To avoid fragmented blocks, we require the sequence length to be divisible by the current block size. At the final enlargement, the BDLM becomes equivalent to standard MDLM that operates over fully masked sequences with global attention. Crucially, each block-size transition is trained on moderate-scale data to ensure smooth adaptation. This progressive enlargement allows the model to smoothly adapt its internal representations to handle larger contextual spans and more complex masking patterns. Phase-2: Large Scale Stable Training Once the block size reaches 4096 and the model transitions to the MDLM pattern, the clean part of the attention computation (see Figure 2) no longer needs to be maintained. This significantly reduces the computational cost of attention, allowing data to be processed far more efficiently under the MDLM paradigm. With the model now fully adapted to this regime, the stable training phase focuses on deepening its understanding of diffusion dynamics through extensive training on large-scale corpora. At this stage, the block size is fixed at 4096, effectively making every input single-block sequence, equivalent to the classical MDLM setting. Phase-3: Block Size Decay Finally, after large-scale MDLM training, we gradually reduce the block size from 4096 to small block size (e.g., 32) to convert the model back into an efficient BDLM. This decay process distills the global contextual knowledge learned during MDLM into compact blockwise structure. By decreasing the block size step-by-step (e.g., starting from 4096 to 2048) rather than abruptly, the model smoothly adapts from global to local conditioning, preserving its semantic understanding while regaining BDLMs practical benefits such as KV-cache reuse and fast variable-length generation. Overall Training Objective The optimization objective of BDLM (Arriola et al., 2025) is designed to enable the model to accurately reconstruct the original, uncorrupted tokens within these designated masked blocks using standard cross-entropy loss. Specifically, we define the training loss during warmup and decay phases (phase-1&3) under the BDLM paradigm as: LB BDLM(θ) = Et,x0,xt t,k = [MASK]] log pθ(xi 0,kx0,<k, xt,k) 1[xi (1) (cid:34) (cid:35) , α 1 αt k=1 i=1 where the expectation is over timestep t, the clean sequence x0, and its corrupted version xt (tokens masked with probability 1 αt). Indicator 1[] ensures predictions are made only for masked tokens, and α t/(1 αt) is the diffusion-derived time weight. Here = Ltotal/LB is the number of blocks, LB the block size, xi t,k the i-th token in block k, x0,<k the preceding clean blocks, and xt,k the noisy version of the current block. During the stable training (phase-2) of MDLM (i.e., K=1), the objective simplifies to: MDLM(θ) = Et,x0,xt [ α 1 αt i=1 1[xi = [MASK]] log pθ(xi 0xt)]. (2) 4.2 Document-level Attention Mask Our training sequences are formed by packing heterogeneous documents into fixed-length segments to maximize throughput. However, this introduces artificial long-range dependencies across semantically unrelated texts. Without careful handling, standard attention would incorrectly attend across document boundaries, leading to contextual confusion and significantly hindering the models ability to perform robust bidirectional modeling crucial for denoising. To mitigate this fundamental challenge and preserve semantic coherence, we redefine the attention mechanism with specialized block-wise document-level attention mask. This mask ensures that attention operates 6 strictly within document boundaries, preventing cross-document contamination and allowing the model to fully leverage bidirectional context for accurate reconstruction of corrupted blocks. The native Block Diffusion vectorizes the training process to achieve parallel training of blocks, and this mask is applied accordingly. Specifically, for concatenated sequence ull of length 2L (comprising xt followed by x0), and assuming tokens and are already confined to the same document segment (as enforced by the initial document-level mask), the attention mask {0, 1}2L2L is constructed by dividing each sequence (xt and x0) into contiguous blocks. Let b(k) = k/LB denote the block index for token given block size LB. The mask is defined as: Mij = 1 1 0 b(i)=b(j) b(i)>b(jL) b(iL)b(jL) if xt and xt if xt and x0 if x0 and x0 otherwise (3) Where i, {0, 1, . . . , 2L 1} are the indices in the full sequence. The first condition (1 b(i)=b(j)) implements block-diagonal attention within the noisy sequence xt. The second (1 b(i)>b(jL)) enables cross-attention from xt to x0, but only from blocks in xt to earlier blocks in x0. The third (1 b(iL)b(jL)) imposes causal block attention pattern within the clean sequence x, allowing block to attend to itself and all preceding blocks. The otherwise condition corresponds to zero matrix, explicitly preventing attention from queries in x0 to keys in xt. This allows each block to leverage context from relevant blocks (according to the mask) for reconstruction, capturing inter-block dependencies while maintaining the causal and block-diagonal principles essential for stable diffusion training. During our exploration, we also experimented with other tricks like random-length (Xie et al., 2025) and CART (Ye et al., 2025). However, the results demonstrate that the document-level attention mask is more fundamental in CPT training compared to these techniques, and it consistently achieves superior performance. As illustrated in Figure 2, this forms structured attention layout that balances locality and global document coherence. For MDLM, the document-level attention mask simplifies to {0, 1}LL, where: Mij = (cid:26)1, if i, belong to the same document, 0, otherwise. (4) 4.3 Top-k Checkpoint Merge To further enhance the generalization and robustness of our Block Diffusion Language Model, we employ top-k checkpoint merging strategy. Upon completion of BDLM pre-training, we identify the top bestperforming model checkpoints, typically selected based on validation metrics like perplexity. The parameters (weights and biases) of these checkpoints are then arithmetically averaged to form single, unified BDLM. Based on WSM scheduler (Tian et al., 2025), this merge strategy can effectively ensemble diverse knowledge captured by the model at various optimal or near-optimal training states. This smooths the parameter landscape, mitigates overfitting, and yields more stable and generalizable model. key advantage of the WSM approach is its optimizer-agnostic nature, allowing seamless integration without altering the underlying training pipeline. Crucially, this post-training Top-k Merge fundamentally differs from the Exponential Moving Average (EMA). While EMA is an in-training technique that continuously smooths parameters, merging is an offline procedure. It explicitly selects and averages distinct, high-performing model states, consolidating their strengths rather than merely smoothing the final training step."
        },
        {
            "title": "5 Post-training",
            "content": "(cid:17) Takeaway (1) Applying complementary masking and mask ratio bandwidth during SFT improves sample efficiency and stabilizes convergence. (2) An auxiliary confidence loss is incorporated to sharpen predictions, which is crucial for efficient parallel decoding. (3) DPO is adapted by defining sequence log-probabilities over masked tokens, enabling effective preference alignment for the diffusion model. 7 5.1 Supervised Fine-Tuning with Block Diffusion Following the pre-training phase, the model is aligned to follow user instructions through supervised finetuning (SFT). This is achieved by adapting the diffusion training objective to be conditional on an input prompt, c. The model is thus trained to generate the desired response x0 by minimizing the following loss function: SFT(θ) = t,(c,x0),xt (cid:34) α 1 αt k=1 LB i=1 1[xi t,k = [MASK]] log pθ(xi 0,kc, x0,<k, xt,k) (cid:35) . (5) Here, the model pθ learns to predict the original tokens xi 0,k of clean response from noisy version xt. The loss is computed only on masked tokens within the current noisy block xt,k. To do this, the prediction is conditioned on the prompt c, auto-regressive context from prior clean blocks x0,<k, and the current noisy block xt,k that it must denoise. Padding strategies & Mask ratio bandwidth To ensure compatibility with our block-wise attention mask, we quantize each sequences length. Specifically, the original length is rounded up to the nearest multiple of the block size, b. This process defines an effective length for each sequence, guaranteeing its boundaries align perfectly with the block boundaries required by the attention mechanism. To optimize the training dynamics, we further implement mask ratio bandwidth strategy. Standard discrete diffusion processes typically sample mask probabilities across the full unit interval, αt U[0, 1]. However, as identified by Arriola et al. (2025), extreme masking rates induce high gradient variance while offering minimal learning signal: near-zero masking renders reconstruction trivial, while near-total masking reduces the objective to simply learning data marginals. To mitigate this, we clip the noise schedule, constraining the sampling of mask rates to bounded interval [αmin, αmax] rather than the full range. This bandwidth restriction focuses the training objective on the noise regimes that provide the most informative gradients, thereby stabilizing convergence and improving the models generative perplexity. Complementary Masking Complementary Masking (Li et al., 2025) is training optimization that enhances the data efficiency of the MDLM objective, MDLM(θ). The strategys core principle is to generate two antithetical training instances from single source sequence x0. primary noised sequence, xt, is formed using random mask, while complementary sequence, t, is simultaneously produced using that masks logical inverse1. By incorporating both xt and into the same training batch, this method provides deterministic guarantee: every token position across the sequence length is presented to the model in its uncorrupted state exactly once within the pair. This not only doubles the effective data utilization from each sample, thereby accelerating convergence, but also entirely eliminates token-level sampling bias. Consequently, the model benefits from more comprehensive and uniform learning signal at every optimization step, leading to enhanced robustness. Data Recipe Curation balanced, high-quality SFT dataset underpins the models capabilities, achieved through strategic composition of tasks spanning three principal pillars: Reasoning, General, and Industrial. The Reasoning pillar hones analytical and logical faculties through mathematics and code generation. The General pillar cultivates linguistic richness and social intelligence via creative and dialogic tasks. The Industrial pillar embeds domain-specific expertise by simulating end-to-end workflows under real-world constraints. This integrated methodology ensures holistic skill profile, preventing capability skew and enabling fluid shifts between abstract reasoning and applied problem-solving. 5.2 Confidence-Aware Parallel Training To enhance the models predictive confidence, which is crucial for efficient parallel decoding, we propose Confidence-Aware Parallel (CAP) Training. We incorporate an auxiliary confidence loss, conf, inspired by dParallel (Chen et al., 2025b). The primary objective, SFT, ensures correctness but provides diminishing incentive to sharpen the predictive distribution for tokens that are already correctly predicted. The confidence loss addresses this by selectively minimizing the entropy of the models output distribution, pθ(x0xt, c), but only for the subset of tokens that are correctly predicted in given step. This compels the model to increase its certainty on its correct predictions. The final training objective is weighted combination of the two losses: SFT(θ) + λL 1We also try complementary masking in CPT and find it only works fine on corpus less than 100B tokens, while it L(θ) = conf(θ), (6) does not show advantages with more training data, so that we only adopt it in post-training. 8 Figure 3: Average score and tokens-per-forward (TPF) for LLaDA2.0-flash with and without CAP across 12 benchmarks. Inference speed (tokens per second) of LLaDA2.0-flash compared with similarly sized AR models on 4 code and math benchmarks. where λ is hyperparameter that balances the two objectives. As illustrated in Figure 3, CAP training effectively improves the decoding efficiency of LLaDA2.0-flash while maintaining competitive compression performance, demonstrating favorable trade-off between generation quality and inference speed. 5.3 DPO Building upon the SFT stage, we further align the policy model πθ with human intent using Direct Preference Optimization. To support this, we constructed comprehensive dataset comprising 1.5 million preference pairs across diverse domains, including general knowledge, mathematics, and instruction following. To ensure stable transition in optimization, the learning rate for the DPO stage is initialized consistently with the final learning rate of the preceding SFT phase. Since the policy model πθ is trained to reconstruct clean tokens x0 from noisy blocks xt conditioned on context c, the standard DPO formulationwhich requires exact log-likelihoodsis intractable. Following established practices for diffusion models, we substitute the conditional log-likelihoods with their ELBO. We first define the conditional Block Diffusion ELBO, BBDLM(θ, xc), for response x. This term mirrors the inner objective of our SFT loss (equation 5) and is estimated via single Monte Carlo sample over timesteps and noise: BBDLM(θ, xc) = Et,xt (cid:34) α 1 αt k= LB i=1 1[xi t,k = [MASK]] log pθ(xi kc, x<k, xt,k) (cid:35) . (7) Given preference pair (xw, xl), where xw is the preferred response and xl is the dispreferred response, the DPO objective maximizes the margin between the ELBO estimates of the policy πθ and the frozen reference model πθref (initialized from the post-SFT model). The final loss function is defined as: DPO(θ) = (c,xw,xl )D [log σ (β [B(xwc) B(xlc)])] , (8) where B(xc) = BBDLM(θ, xc) BBDLM(θref, xc) represents the ELBO advantage of the policy over the reference model, and β is hyperparameter (set to 0.1) that controls the deviation from the reference policy. 5.4 Inference We sample one block at diffusion step, conditioned on previously sampled blocks pθ(xb ). The generation of each block is itself multi-step iterative refinement process. At each step, candidate tokens are sampled for all remaining unfilled positions within the block. hybrid acceptance strategy is then employed: we first accept all tokens whose sampling probability exceeds predefined confidence threshold. If an insufficient number of tokens meet this criterion, low-confidence fallback is triggered, where we instead accept fixed number of the most probable tokens regardless of their absolute confidence. This dual mechanism ensures steady generation progress. c, x<b"
        },
        {
            "title": "6 Evaluation",
            "content": "6.1 Setup To comprehensively evaluate the quality of instruction-tuned models, we employ diverse suite of benchmarks categorized into five dimensions: Knowledge: MMLU (Hendrycks et al., 2020), MMLU-Pro (Wang et al., 2024), GPQA-Diamond (Rein et al., 2024), ARC (Clark et al., 2018), CMMLU (Li et al., 2023a) C-Eval (Huang et al., 2023), GAOKAO-Bench (Zhang et al., 2023), SciBench (Wang et al., 2023), PHYBench (Qiu et al., 2025), TriviaQA (Joshi et al., 2017) Reasoning: SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019), KOR-Bench (Ma et al., 2024), HellaSwag (Zellers et al., 2019), BIG-Bench Hard (Suzgun et al., 2023), BIG-Bench Extra Hard (Kazemi et al., 2025), MuSR (Sprague et al., 2023), ZebraLogic (Lin et al., 2025), PrOntoQA (Saparov & He, 2022), PIQA (Bisk et al., 2020), OCNLI (Hu et al., 2020), BIG-Bench Hard-CN (team, 2023c) Coding: CRUXEval (Gu et al., 2024), MBPP (Austin et al., 2021), MultiPL-E (Cassano et al., 2023), HumanEval (Chen et al., 2021), BigCodeBench (Zhuo et al., 2024), LiveCodeBench (Jain et al., 2024), Spider (Yu et al., 2018), BIRD (Li et al., 2023b), HumanEval+ (Liu et al., 2023), MBPP+ (Liu et al., 2023), HumanEvalFix (Muennighoff et al., 2023), Aider (team, 2023a), HumanEval-CN (team, 2023c) Math: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), OlympiadBench (He et al., 2024), AIME 2025 (AIME, 2025), Omni-MATH (Gao et al., 2024), HARDMath2 (Roggeveen et al., 2025), GSM-Plus (Li et al., 2024), CMATH (Wei et al., 2023) Agent & Alignment: BFCL (Patil et al., 2025), IFEval (Zhou et al., 2023), CodeIF-Bench (Wang et al., 2025b), Nexus Function Calling Benchmark (team, 2023b) This extensive evaluation suite, comprising total of 47 benchmarks, provides holistic foundation for assessing model capabilities. In our experiments, we compare the LLaDA2.0 series against strong open-source auto-regressive (AR) models. For all LLaDA2.0 models, we utilize temperature of 0.0, block size of 32, and decoding threshold of 0.95. 6.2 Results The overall results, presented in the following tables, indicate that the LLaDA2.0 architecture is not only highly competitive, but also shows promising trend of closing the performance gap with, and even surpassing, AR models in specific key areas. Our models consistently demonstrate strong, and often superior, performance in complex, structured tasks. For instance, LLaDA2.0-mini already outperforms comparable AR model (Qwen3-8B) in the domains of Reasoning, Coding, and Math. This signal is amplified in our larger model, as LLaDA2.0-flash achieves parity with the powerful Qwen3-30B-A3B-Instruct-2507 and establishes lead in the critical Coding and Agent domains. This suggests that as diffusion models scale, their inherent strengths in structured generation and tool use become increasingly apparent. As shown in Table 1, LLaDA2.0-mini achieves competitive average score of 64.34, closely approaching its AR peer, Ling-mini-2.0 (65.77). This demonstrates the fundamental viability of the diffusion approach. More importantly, it shows promising signals in complex tasks, outperforming its direct competitor on reasoning benchmarks like SQuAD 2.0 (86.50) and demonstrating more robust instruction following on IFEval (80.78). Its strong performance in coding tasks such as HumanEval (86.59) further suggests an early aptitude for structured generation. This potential becomes even more evident with our larger model, LLaDA2.0-flash. As shown in Table 2, with an average score of 73.18, it stands firmly on par with strong AR models such as Qwen3-30B-A3B-Instruct2507 (73.60). Crucially, LLaDA2.0-flash begins to exhibit clear advantages in complex generative tasks, sign that the diffusion architecture may hold inherent strengths. In the critical domain of coding, it consistently outperforms its AR peers, scoring higher on HumanEval (94.51), MBPP (88.29) and MultiPL-E (74.87). This trend of surpassing AR models also extends to agent capabilities (BFCL v3: 75.43) and advanced mathematics (AIME 2025: 60.00). In conclusion, the LLaDA2.0 series successfully demonstrates that diffusion-based language models are powerful and scalable alternative to the dominant auto-regressive paradigm. While rapidly narrowing the gap on general benchmarks, they are already showcasing the potential to surpass traditional architectures in complex, structured domains like code generation and tool use. This positions diffusion models as highly promising direction for the future of language generation. Benchmark Average MMLU MMLU-Pro CMMLU C-EVAL GAOKAO-Bench ARC-c GPQA SciBench PHYBench TriviaQA BIG-Bench Hard BIG-Bench Extra Hard bbh-zh MuSR ZebraLogic PrOntoQA PIQA OCNLI HellaSwag KOR-Bench DROP SQuAD 2.0 CRUXEval-O MBPP MBPP+ MultiPL-E HumanEval HumanEval+ HumanEvalFix HumanEval-cn BigCodeBench-Full LiveCodeBench Aider BIRD-SQL Spider GSM8K MATH OlympiadBench AIME 2025 HARDMath2 Omni-MATH GSM-Plus CMATH IFEval-strict-prompt BFCL v3 CodeIF-Bench Nexus FC Table 1: Benchmark Performance of LLaDA2.0-mini Qwen3-8B (no think) Ling-mini-2.0 LLaDA2.0-mini-preview LLaDA2.0-mini 63.42 80.94 65.48 79.17 81.36 84.94 93.35 46.59 2.85 9.76 52.51 79.48 18.27 80.09 70.02 37.48 93.12 88.30 61.49 79.56 54.48 84.56 85. 74.06 78.92 71.96 61.70 84.76 78.66 76.02 74.39 36.05 26.38 55.64 36.11 72.80 93.63 86.28 55.33 22.08 7.58 33.20 86.09 95.42 86.90 70.08 50.00 37.71 65.77 Knowledge 82.15 63.72 80.84 82.10 87.23 93.09 56.80 5.28 14.59 55. Reasoning 83.70 14.81 66.11 71.36 79.85 96.06 87.54 60.17 69.02 62.72 78.80 75.56 Coding 76.12 84.07 76.46 67.09 85.98 81.71 82.83 71.34 35.00 34.97 49.62 39.67 76.43 Math 94.62 94.66 72.30 47.66 9.95 48.80 87.82 96. Agent & Alignment 76.16 53.98 46.00 34.38 11 54.67 72.49 49.22 67.53 66.54 74.46 89.15 23.74 4.10 5.08 50.49 70.64 12.36 66.62 56.77 14.80 70.00 84.33 58.68 74.01 37.26 79.49 85. 61.88 77.75 66.67 62.43 80.49 71.95 60.16 73.17 30.44 19.82 28.57 27.71 75.64 89.01 73.50 36.30 10.00 0.95 19.20 81.44 90.53 62.50 74.11 48.00 33.68 64.34 80.53 63.22 79.50 81.38 84.30 93.56 47.98 3.53 11.70 51.33 78.21 16.47 75.75 71.48 64.20 86.00 86.51 64.51 79.01 50.40 81.91 86. 71.62 81.50 74.07 67.46 86.59 79.88 74.90 78.66 32.89 31.50 39.85 39.34 76.76 94.24 93.22 67.70 36.67 0.47 41.70 86.24 95.72 80.78 70.90 48.00 35.18 Benchmark Average MMLU MMLU-Pro CMMLU C-EVAL GAOKAO-Bench ARC-c GPQA SciBench PHYBench TriviaQA BIG-Bench Hard BIG-Bench Extra Hard BIG-Bench Hard - CN MuSR ZebraLogic PrOntoQA PIQA OCNLI HellaSwag KOR-Bench DROP SQuAD 2.0 CRUXEval-O MBPP MBPP+ MultiPL-E HumanEval HumanEval+ HumanEvalFix HumanEval-CN Bigcodebench-Full LiveCodeBench Aider Spider BIRD-SQL GSM8K MATH OlympiadBench AIME 2025 HARDMath2 Omni-MATH GSM-Plus CMATH IFEval-strict -prompt BFCL v3 CodeIF-Bench Nexus FC Table 2: Benchmark Performance of LLaDA2.0-flash Qwen3-30B-A3B-Instruct-2507 Ling-flash-2.0 LLaDA2.0-flash-preview LLaDA2.0-flash 65.97 83.15 66.16 79.64 79.28 86.12 93.90 41.92 5.13 7.58 69.25 82.85 16.70 83.38 78.75 39.90 93.50 91.84 69.39 86.00 53.28 88.17 90.61 74.50 86.65 75.93 72.38 88.41 82.32 83.33 84.76 40.44 29.07 51.13 81.37 45.34 95.75 83.52 49.33 23.33 3.79 24.60 88.25 95.26 75.60 74.86 56.00 47. 73.60 87.13 74.23 86.36 88.17 94.53 95.81 57.34 4.54 29.84 65.61 85.54 37.80 86.18 79.15 90.97 97.12 91.57 71.59 86.31 68.00 87.57 89.51 86.75 86.65 78.04 70.67 93.29 88.41 91.16 87.20 41.49 41.63 71.43 81.79 47.75 96.36 96.70 77.59 61.88 4.27 54.00 89.45 96.58 84.29 73.19 54.00 49. 72.15 Knowledge 87.98 76.84 86.59 88.03 93.24 95.08 67.12 4.14 27.67 69.76 Reasoning 89.36 23.24 75.09 82.72 87.60 97.88 91.95 65.36 81.59 68.96 88.32 81.32 Coding 82.75 85.01 76.19 65.76 85.98 85.98 92.68 74.39 40.70 44.11 71.43 80.58 47.49 95.45 96.10 76.19 55.89 23.70 53.00 89.83 96.52 Math Agent & Alignment 81.52 67.57 56.00 36.25 73.18 87.69 73.36 85.13 86.75 93.90 95.93 61.98 4.13 30.06 66.88 86.75 27.86 87.52 80.48 82.30 96.50 92.76 71.63 84.97 64.24 87.90 90.00 85.12 88.29 79.63 74.87 94.51 87.80 90.24 89.02 41.58 42.29 66.92 82.49 45.76 96.06 95.44 74.07 60.00 4.27 50.30 89.64 96.90 81.70 75.43 58.00 50. Figure 4: Score/TPF vs threshold/block size Figure 5: Performance on the RULER benchmark. 6.3 Analysis Analysis of Inference Hyper-parameters In addition to our main evaluation, we conducted brief analysis to tune key inference hyperparameters. To ensure efficiency, this analysis was performed on our LLaDA2.0mini model, using representative subset of our benchmarks to understand the trade-off between generation quality (score) and inference speed (measured as TPF - Tokens Per Forward; higher is faster). Denoising Threshold. We first investigate the impact of the Denoising Threshold. While keeping the Block Size fixed at 32, we varied the threshold and observed its effect on quality and speed. As shown in Figure 4, the results reveal clear trade-off. threshold of 0.95 achieved the highest quality score (70.15) at the cost of the lowest inference speed (2.55 TPF). Lowering the threshold to 0.85 boosted the speed to its peak (3.31 TPF), but led to an unacceptable degradation in quality, with the score dropping to 67.90. Block Size. Subsequently, we analyze the effect of Block Size. We set the Denoising Threshold to 0.95, the optimal value identified in the prior experiment. The results in Figure 4 demonstrate similar trade-off. block size of 16 yielded the highest score (70.26) but with the slowest inference (2.44 TPF). In contrast, increasing the block size to 32 substantially improved the speed to 2.55 TPF with only marginal quality drop to 70.15. Further increasing the block size to 64 proved suboptimal, as it degraded both score and speed relative to the size-32 setting. Therefore, block size of 32 emerges as the most compelling choice, offering significant speed-up for negligible performance cost. In summary, based on this analysis, the configuration for our main evaluation is well-supported. The Denoising Threshold of 0.95 is the clear choice for maximizing quality. For block-size, the setting of 32 represents an optimal balance, providing the highest throughput with virtually no sacrifice in performance compared to the slightly higher-scoring but slower setting of 16. Analysis of Context Length To rigorously validate our models performance across various context lengths, we conducted series of evaluations using the RULER benchmark. As shown in Figure 5, both models demonstrate strong performance and stability within context length of 32k. The LLaDA2.0-flash model is particularly robust, maintaining score above 93 across all lengths from 4k to 32k. The LLaDA2.0-mini model also achieves high scores, starting at 93.29 for 4k but showing degradation to 83.94 at 32k. To test the models extrapolation capabilities, we extended the context length to 64k. This was achieved by employing dynamic RoPE scaling during inference, specifically using the YaRN method with scaling factor of 2.0. However, this extension resulted in performance degradation for both models, demonstrating clear trade-off between context length extension and task accuracy. In summary, this evaluation highlights two key findings: (1) The LLaDA2.0 models are exceptionally robust for long-context tasks within their native 32k window. (2) They can be successfully extended to handle 64k sequences via YaRN scaling, providing flexibility for extreme-length applications, albeit with predictable performance cost."
        },
        {
            "title": "7 Training & Inference Infrastructure",
            "content": "7.1 Pretraining We adopt Megatron-LM (Shoeybi et al., 2019) as the pretraining backend to enable efficient training of 100B-parameter model with long sequences, leveraging data parallelism (DP), pipeline parallelism (PP), 13 tensor parallelism (TP), context parallelism (CP), and expert parallelism (EP), as Figure 6 shows. To ensure consistency of masked tokens, we generate masked tokens on single model-parallel (MP, that is, TP and PP) rank and then broadcast to all other ranks within the MP ranks. Efficient Block Diffusion Training For flexible support of arbitrary block diffusion attention mask, we utilize cuDNN as the backend for the attention mechanism. This approach achieves more than 1.3x end-to-end speedup and over 90% memory savings in the attention layer compared to the unfused attention implementation in TransformerEngine when training LLaDA2.0-mini. We further apply zig-zag partitioning strategy to the block diffusion attention mask to achieve effective load balancing across the CP group. Numerical Stability During the transition from AR to diffusion models, training can suffer from gradient explosion, especially at high mask ratios within document. This issue stems from the fact that masked token embeddings are set to zero during AR training, as these tokens are never observed, leading their corresponding weights to gradually decay to zero. straightforward fix, randomly reinitializing the masked token embeddings upon loading the AR model, may disrupt other welltrained parameters, potentially causing catastrophic forgetting. To mitigate this while preserving pre-trained knowledge, we instead add independent Gaussian noise to the output of the embedding layer for each masked token during the initial iterations of training. This ensures that the L2 norm of the masked tokens embedding remains significant to avoid gradient explosion, thereby stabilizing the training process. Figure 6: Parallelism overview. 7.2 Post-Training For the post-training phase, we leverage dFactory2 (InclusionAI, 2025), repository providing efficient training recipes for dLLMs. Built upon the VeOmni (Ma et al., 2025a) distributed training framework, dFactory allows us to effectively implement complex parallelization schemes. Specifically, our setup for fine-tuning LLaDA2.0 combines Data Parallelism (DP) and Expert Parallelism (EP) to ensure scalable and stable training. To further enhance data throughput and hardware utilization, we adopt data packing strategy analogous to those used in continued pre-training, which concatenates multiple short sequences into single longer sequence. This integrated approach provides robust and high-performance infrastructure for the post-training of our model. 7.3 Inference Engine We adapt dInfer3 (Ma et al., 2025b)originally built for high-performance diffusion LLM inferenceto efficiently support block diffusion inference. This requires the inference engine to leverage optimization techniques traditionally designed for AR models. For instance, the framework can now effectively exploit KV-cache reuse to substantially reduce prefill computation. As block diffusion inference closely resembles auto-regressive generation in the execution pattern, we also incorporated block diffusion inference support into SGLang4 (Zheng et al., 2024), allowing it to benefit from the same class of system-level optimizations designed for AR models. More mature features in dInfer are undergoing to transport to SGLang. Inference speed Figure 3 compares the average inference throughput (Tokens Per Second, TPS = #decoding tokens/#total-time) of our optimized LLaDA2.0-flash models against state-of-the-art AR models of similar scale on four reasoning and code-generation benchmarks (HumanEval, MBPP, GSM8K, and CRUXEval). All models are evaluated under consistent generation setup. For diffusion-based models (LLaDA2.0flash and LLaDA2.0-flash-CAP), we adopt threshold decoder with threshold of 0.95. The AR baselines (Ling-flash-2.0 and Qwen3-30B-A3B-Instruct-2507) are deployed using SGLang, while the diffusion models 2https://github.com/inclusionAI/dFactory 3https://github.com/inclusionAI/dInfer 4https://github.com/sgl-project/sglang/issues/12766 14 are served with dInfer, ensuring fair performance comparison in real inference environments. As shown, LLaDA2.0-flash-CAP reaches 535 TPS, outperforming the standard LLaDA2.0-flash (383 TPS) and providing up to 2.1 speed-up over the AR baselines (256 TPS and 237 TPS)."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we introduced LLaDA2.0, discrete diffusion language models scaling up to 100B total parameters through systematic conversion from auto-regressive models, as well as set of novel and comprehensive recipes designed to smooth and effectively transform traditional AR language models into highly efficient and performant Masked Diffusion Language Models. Through extensive evaluations, it validates the feasibility of the training paradigm. The LLaDA2.0-mini and LLaDA2.0-flash models achieve performances that are competitive with their AR counterparts. Slightly surprisingly, LLaDA2.0-flash seems to have demonstrated advantages in complex, structured domains such as code generation, mathematical reasoning, and agentic tool use. These may have opened new door to future work in the agentic LLM era while solidifying gaugeable potential of dLLM for test-time scaling. Future work may point to further scaling of the parameter volume, RL/thinking paradigm and extending the decoding speed to its extreme. References AIME. AIME Problems and Solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php/ AIME Problems and Solutions. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv:2503.09573, 2025. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv:2108.07732, 2021. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. MultiPL-E: Scalable and Polyglot Approach to Benchmarking Neural Code Generation. IEEE Transactions on Software Engineering, 49(7):36753691, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv:2107.03374, 2021. Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai Helen Li, and Yiran Chen. DPad: Efficient Diffusion Language Models with Suffix Dropout, August 2025a. arXiv:2508.14148. Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, and Xinchao Wang. dParallel: Learnable Parallel Decoding for dLLMs. arXiv:2509.26488, 2025b. Shuang Cheng, Yihan Bian, Dawei Liu, Linfeng Zhang, Qian Yao, Zhongbo Tian, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, et al. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv:2510.06303, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168, 2021. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: Reading Comprehension Benchmark Requiring Discrete Reasoning over Paragraphs. arXiv:1903.00161, 2019. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv:2410.07985, 2024. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. Scaling diffusion language models via adaptation from autoregressive models. In The Thirteenth International Conference on Learning Representations, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 Herd of Models. arXiv:2407.21783, 2024. Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CruxEval: Benchmark for Code Reasoning, Understanding and Execution. arXiv:2401.03065, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems. arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving with the Math Dataset. arXiv:2103.03874, 2021. Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra ubler, and Lawrence Moss. Ocnli: Original chinese natural language inference. arXiv:2010.05444, 2020. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-Eval: Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o System Card. arXiv:2410.21276, 2024. InclusionAI. dFactory: Easy and Efficient dLLM Fine-Tuning, 2025. URL https://github.com/inclusionAI/ dFactory. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv:2403.07974, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv:1705.03551, 2017. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Yuanzhu Peter Chen, et al. Big-bench extra hard. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2647326501, 2025. Chandrasegaran Keshigeyan, Thomas Armin, and others at Radical Numerics. Training diffusion language models at scale using autoregressive models. https://github.com/RadicalNumerics/RND1, 2025. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring Massive Multitask Language Understanding in Chinese. arXiv:2306.09212, 2023a. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36:4233042357, 2023b. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. arXiv:2402.19255, 2024. 16 Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. LaViDa: Large Diffusion Language Model for Multimodal Understanding, June 2025. arXiv:2505.16839. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv:2502.01100, 2025. Team Ling, Ang Li, Ben Liu, Binbin Hu, Bing Li, Bingwei Zeng, Borui Ye, Caizhi Tang, Changxin Tian, Chao Huang, Chao Zhang, et al. Every activation boosted: Scaling general reasoner to 1 trillion open language foundation. arXiv:2510.22115, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. DeepSeek-V3 Technical Report. arXiv:2412.19437, 2024. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Jingyu Liu, Xin Dong, Zhifan Ye, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang, and Pavlo Molchanov. TiDAR: Think in Diffusion, Talk in Autoregression, November 2025. arXiv:2511.08923. Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, et al. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks. arXiv:2410.06526, 2024. Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, et al. Veomni: Scaling any modality model training with model-centric distributed recipe zoo. arXiv:2508.02317, 2025a. Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qiand Xinyuan Zhang, et al. dinfer: An efficient inference framework for diffusion language models. arXiv:2510.08666, 2025b. Meta-AI. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation, 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Moonshot. Kimi K2. https://github.com/MoonshotAI/Kimi-K2/, 2025. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In NeurIPS 2023 workshop on instruction tuning and instruction following, 2023. Jinjie Ni and team. Openmoe 2: Sparse diffusion language models. https://jinjieni.notion.site/ OpenMoE-2-Sparse-Diffusion-Language-Models-277d8f03a8668065a4ecd23f23bd6aac, 2025. Notion Blog. Jinjie Ni, Qian Liu, Chao Du, Longxu Dou, Hang Yan, Zili Wang, Tianyu Pang, and Michael Qizhe Shieh. Training optimal large diffusion language models. arXiv:2510.03280, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, et al. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv:2504.16074, 2025. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. arXiv:1806.03822, 2018. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: Graduate-Level Google-Proof Q&Q Benchmark. In First Conference on Language Modeling, 2024. 17 James Roggeveen, Erik Wang, Will Flintoft, Peter Donets, Lucy Nathwani, Nickholas Gutierrez, David Ettel, Anton Marius Graf, Siddharth Dandavate, Arjun Nageswaran, et al. Hardmath2: benchmark for applied mathematics built by students as part of graduate class. arXiv:2505.11774, 2025. Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. arXiv:2210.01240, 2022. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053, 2019. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, and Hao Zhou. Seed diffusion: large-scale diffusion language model with high-speed inference, 2025. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. arXiv:2310.16049, 2023. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-ofthought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 13003 13051, 2023. Aider-AI team. Aider-ai/aider, 2023a. URL https://github.com/Aider-AI/aider. Nexusflow.ai team. Nexusraven-v2: Surpassing gpt-4 for zero-shot function calling, 2023b. URL https: //nexusflow.ai/blogs/ravenv2. Opencompass team. Open-compass/opencompass, 2023c. URL https://github.com/open-compass/ opencompass. Changxin Tian, Jiapeng Wang, Qian Zhao, Kunlong Chen, Jia Liu, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, and Jun Zhou. Wsm: decay-free learning rate schedule via checkpoint merging for llm pre-training. arXiv:2507.17634, 2025. Chenyu Wang, Paria Rashidinejad, DiJia Su, Song Jiang, Sid Wang, Siyan Zhao, Cai Zhou, Shannon Zejiang Shen, Feiyu Chen, Tommi Jaakkola, Yuandong Tian, and Bo Liu. Spg: Sandwiched policy gradient for masked diffusion language models. arXiv:2510.09541, 2025a. Peiding Wang, Li Zhang, Fang Liu, Lin Shi, Minxiao Li, Bo Shen, and An Fu. Codeif-bench: Evaluating instruction-following capabilities of large language models in interactive code generation. arXiv:2503.22688, 2025b. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv:2307.10635, 2023. Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion LLMs Can Do FasterThan-AR Inference via Discrete Diffusion Forcing, August 2025c. arXiv:2508.09192. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv:2509.06949, 2025d. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: More Robust and Challenging Multi-Task Language Understanding Benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test? arXiv:2306.16636, 2023. Zhihui Xie, Jiacheng Ye, Lin Zheng, Jiahui Gao, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream-Coder 7B: An Open Diffusion Language Model for Code, September 2025. arXiv:2509.01142. 18 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 Technical Report. arXiv:2505.09388, 2025. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv:2508.15487, 2025. Runpeng Yu, Qi Li, and Xinchao Wang. Discrete Diffusion in Large Language and Multimodal Models: Survey, September 2025. arXiv:2506.13759. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv:1809.08887, 2018. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv:1905.07830, 2019. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv:2305.12474, 2023. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-Following Evaluation for Large Language Models. arXiv:2311.07911, 2023. Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, and Ji-Rong Wen. Llada-moe: sparse moe diffusion language model, 2025. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv:2406.15877, 2024."
        }
    ],
    "affiliations": [
        "Ant Group",
        "HongKong University of Science and Technology",
        "Renmin University of China",
        "Westlake University",
        "Zhejiang University"
    ]
}