{
    "paper_title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models",
    "authors": [
        "Xinyi Wang",
        "Xun Yang",
        "Yanlong Xu",
        "Yuchen Wu",
        "Zhen Li",
        "Na Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 7 1 0 0 1 . 1 1 5 2 : r AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models Xinyi Wang1,, Xun Yang1,, Yanlong Xu1, Yuchen Wu2, Zhen Li3, Na Zhao2, 1 University of Science and Technology of China 2 Singapore University of Technology and Design 3 Chinese University of Hong Kong, Shenzhen Figure 1: We propose fine-grained 3D embodied reasoning: given 3D scene and language task instruction, the agent must identify relevant affordance elements and predict structured triplet for each: its 3D mask, motion type, and motion axis direction."
        },
        {
            "title": "Abstract",
            "content": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in 3D scene, structured triplet comprising its spatial location, motion type, and motion axis, based on task instruction. To solve this task, we propose AffordBot, novel framework that integrates Multimodal Large Language Models (MLLMs) with tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs. *This work was carried out during Xinyis visit to the IMPL Lab at SUTD. Corresponding authors. 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
        },
        {
            "title": "Introduction",
            "content": "For intelligent agents to collaborate effectively with humans and operate autonomously in complex 3D physical worlds, they must perceive and interact with their surroundings at fine-grained, actionable level [1, 2, 3]. This requirement aligns with the concept of affordance, originally introduced in ecological psychology [4], which describes how elements in the environment offer possibilities for action. For example, to execute an instruction like unplug the Christmas tree lights, an agent must recognize and attend to the fine details of the plug and reason about the interaction they afford, rather than merely identifying the larger object, such as the lights. Meeting this challenge requires agents to understand not only what elements in scene afford interaction, but also where they are located and how to manipulate them. Such fine-grained embodied understanding is essential for grounded task execution in real-world environments [5, 6, 7, 8, 9, 10, 11, 12, 13]. While recent developments in multimodal large language models (MLLMs) [14, 15, 16, 17, 18, 19, 20, 21] and 3D scene understanding [22, 23, 24, 25, 26, 27] have advanced object-centric 3D perception, existing approaches [28, 29, 30, 31, 32, 33] stop at high-level object recognition and spatial grounding. However, they often overlook finer-grained structures required to infer how parts of objects afford specific interactions. SceneFun3D [34] makes notable step forward by introducing benchmarks for fine-grained affordance grounding and motion estimation. However, it treats these subtasks in isolation and assumes instruction-agnostic motion prediction, requiring agents to infer motion parameters for all functional parts regardless of task context, limiting its applicability in instruction-conditioned scenarios. To address these limitations, we propose unified and instruction-conditioned task: Fine-grained 3D Embodied Reasoning, which jointly performs 3D affordance grounding and motion estimation based on natural language instruction. Specifically, the task is formulated as structured triplet prediction problem: for each referenced affordance element, the agent predicts triplet comprising affordance mask, motion type, and motion axis direction. This formulation explicitly couples spatial grounding and interaction reasoning under natural language guidance, forming coherent inference pipeline tailored for instruction-conditioned embodied tasks. As solution, we introduce AffordBot, novel framework that integrates 3D geometric information with the reasoning capabilities of MLLMs. Unlike prior work [34, 35, 36, 37, 38] that relies on video-based inputs, which incur high computational overhead by processing redundant visual frames and often suffer from viewpoint limitations, AffordBot operates directly on 3D point clouds. However, MLLMs are inherently designed for 2D input and general-purpose reasoning, presenting significant challenge when applying them to 3D spatial tasks that require physical grounding. To bridge the modality gap between 3D input and 2D-native MLLMs, AffordBot begins by constructing rich multimodal representation of the 3D scene. Specifically, we render surround-view images from the 3D point cloud and project structured 3D affordance candidates onto these images, establishing explicit 3D-to-2D correspondences. This enables dense and spatially aligned visual context to be provided to the MLLM, without relying on redundant video streams. On top of this foundation, we develop task-specific chain-of-thought (CoT) reasoning paradigm that systematically guides the MLLM through physically grounded, step-by-step logical inference. The process begins with an active perception phase, which we specifically designed to empower the MLLM to effectively interpret the task instruction and autonomously select the most informative viewpoint. This initial observe step improves reasoning focus by reducing input redundancy and emphasizing task-relevant visual cues. Subsequently, the selected viewpoint anchors the models reasoning process. Then the MLLM is guided through two distinct reasoning stages: affordance grounding, where it localizes the target part in the scene, and interaction inference, where it predicts the motion type and axis direction based on the scene context and instruction. By conditioning every step on spatial input and task intent, our CoT paradigm enables physically plausible and semantically aligned reasoning, enhancing the agents embodied intelligence. We make three key contributions: (1) We introduce new task formulation for fine-grained, taskdriven embodied reasoning, 3D affordance grounding and motion estimation as structured triplet prediction from natural language. (2) We present AffordBot, novel framework that integrates 3D perception and MLLM-based reasoning via holistic multimodal representation construction and tailored chain-of-thought process. (3) We achieve state-of-the-art results on SceneFun3D, validating the effectiveness of our approach in physically grounded, instruction-conditioned 3D reasoning. 2 Figure 2: AffordBot Overview. Our method first constructs holistic multimodal representation designed to bridge 3D scenes with 2D-native MLLMs. This process involves view synthesis, extraction of geometric-semantic descriptors, and their association. Then, our designed Chain-of-Thought (CoT) paradigm guides the MLLM to ultimately predict structured triplet for the task."
        },
        {
            "title": "2 Related Work",
            "content": "Affordance Understanding. Understanding affordances, the action possibilities offered by the environment [5, 39, 40, 41, 42, 43, 44, 45] , is crucial for robot interaction in 3D scenes. SceneFun3D [34] introduced challenging task and dataset for grounding referred affordance elements based on task descriptions within complex 3D indoor environments, unlike earlier works focusing on simpler settings [5, 6, 7, 8, 9, 10, 12, 13]. Fun3DU [37] further leveraged VLMs [46] and universal segmentation model [47] to parse instructions and localize the target elements in video frames. 3D Motion Estimation. 3D motion enables agents to predict and comprehend how objects move and can be manipulated [48, 49, 50, 51, 52, 53, 54, 55, 56, 57]. Earlier research efforts often focused on estimating the articulated motion and mobility of individual interactable objects with predefined structures, such as hinged parts [48, 49, 50]. These approaches typically rely on analyzing the geometric structure of those individual objects to infer their motion properties. In contrast, SceneFun3D [34] broadened this by formulating scene-level motion estimation task across all affordance elements, providing dataset for comprehensive evaluation. MLLMs for 3D Understanding. Multimodal large language models (MLLMs) [14, 15, 16, 17, 18, 19] are being applied to 3D understanding through two main approaches: developing native 3D-aware models [30, 58, 59, 31, 60] for direct processing of spatial data, and adapting existing 2D VLMs [61, 62, 63, 64] by transforming 3D data into 2D representations. These efforts highlight MLLMs potential for enhancing 3D visual understanding and semantic reasoning. Building upon these, we leverage the MLLM empowered by the tailored chain-of-thought paradigm for fine-grained 3D embodied reasoning, jointly tackling affordance grounding and motion estimation tasks."
        },
        {
            "title": "3 Methodology",
            "content": "We introduce new task termed Fine-grained 3D Embodied Reasoning, which aims to equip embodied agents with the ability to interpret natural linguistic instructions and reason about actionable elements in complex 3D environments. Given 3D scene represented as point cloud and natural language task instruction describing human-intended interaction (e.g., open the left part of the window door), the agent is required to predict set of structured triplets {(Mi, ti, ai}N i=1, where each triplet corresponds to referenced affordance element in the scene. Note that N=1 when only one unique element is referenced in the instruction. Mi indicates 3D instance mask identifying the spatial region of the element involved in the interaction; ti denotes the motion type (e.g., Translation); ai denotes the motion axis (e.g., Horizontal outwards) representing the axis along which the motion occurs. The task requires joint perception and reasoning over geometry, semantics, and language intent, and presents challenges in grounding ambiguous task references, understanding object affordances, and predicting physically plausible interaction cues in 3D space. 3 (a) Lack of Context. (b) Incomplete target coverage. Figure 3: Illustrations of video-based method limitations: (a) Instructions like Unplug the Christmas tree lights or Adjust the rooms temperature using the radiator dial next to the curtain require anchors (e.g., Christmas tree, curtain) that are missing from the limited video frame. (b) Target objects or parts (e.g., cabinet, door handle) in instructions like Open the bottom drawer of the wooden cabinet... or Open the left part of the window door are partially visible within the frame. 3.1 AffordBot Overview To address the fine-grained 3D embodied reasoning task, we present AffordBot, framework that leverages MLLMs to enable instruction-conditioned reasoning over 3D point cloud scenes. An overview of the framework is shown in Fig. 2. AffordBot integrates 3D perception with visionlanguage reasoning through two key components: 1) holistic multimodal representation (Sec.3.2) that bridges the modality gap between 3D input and the 2D-native input format of MLLMs, and 2) chain-of-thought reasoning paradigm (Sec. 3.3) that enables interpretable and accurate prediction. For the first component, we begin by generating set of surround-view images to effectively capture the 3D scene. Following this, we extract geometric-semantic descriptors from the 3D scene and project them onto the rendered views by an adaptive labeling strategy, establishing robust 3D-to-2D association. This representation effectively eliminates traditional video processing bottlenecks while preserving comprehensive information for downstream reasoning, as illustrated in green in Fig. 2. Based on this constructed representation and the given instruction, the MLLM engages in tailored chain-of-thought process to actively perceive and select the most informative view, localize the target element, and infer its required motion. By decomposing the task into sequence of interpretable steps, our method enables the MLLM to perform robust and physically grounded inference for complex embodied tasks, depicted in blue in Fig. 2. 3.2 Holistic Multimodal Representation In this section, we construct the holistic multimodal representation foundational for 2D MLLM reasoning. We design an enriched visual synthesis approach using dynamic surround-view generation to overcome limitations of traditional video data. Next, we describe the extraction and representation of 3D geometry and semantics via geometry-semantic descriptors. Finally, we establish the 3D-2D associations by projecting 3D information onto the generated 2D views with adaptive labeling. Enriched Visual Synthesis. Bridging the gap between 3D input and 2D MLLMs is nontrivial, primarily because it requires establishing accurate and robust associations between 3D structures and their corresponding 2D visual representations. Existing methods typically rely on video sequences collected from datasets [37, 34, 59]. However, these methods face fundamental limitations: due to the limited field of view, it is often difficult to simultaneously capture the target and its associated anchors within the same frame, as shown in Fig. 3. Furthermore, the process of extracting key information from large number of frames is both time-consuming and bottlenecks the final accuracy. To overcome the limitations of static video frames, we propose dynamic surround-view generation strategy. Inspired by human visual exploration of unfamiliar environments, our robot performs 360 horizontal panoramic scan centered on the scenes central viewpoint. This scan produces set of candidate views = {V1, . . . , VN }, where the i-th view Vi is captured at rotation angle θi = (i 1) 2π . Compared to relying on traditional video data, this method provides more comprehensive field of view, thereby capturing more scene information. This effectively alleviates the problem of 4 missing information caused by limited field of view and incomplete coverage typical of traditional video data obtained through random sampling. Furthermore, by scanning each 3D scene to acquire corresponding set of high-quality views, we eliminate the overhead of performing keyframe extraction or detection for each task instruction, thereby completely removing the time processing overhead and accuracy bottleneck associated with analyzing redundant video frames. This ability enables agents to efficiently acquire comprehensive, high-quality visual context. Geometry-Semantic Descriptors. For the input 3D scene P, our method employs instance segmentation [65] to extract affordance elements, and encodes their geometric and semantic features for downstream reasoning. During training, we optimize segmentation quality by combining Dice loss to encourage region-level alignment and cross-entropy loss for accurate point-wise classification. The overall training objective is defined as: Ltotal = λ1 LDice + λ2 LCE, (1) where LDice denotes the Dice loss and LCE denotes the cross-entropy loss. The weights λ1 and λ2 balance the trade-off between the region-level and point-level supervision. To handle the challenge of small element segmentation, we implement the coarse-to-fine curriculum strategy from [34] with progressive ground-truth mask dilation. At curriculum stage t, each groundtruth mask is dilated within according to the following: (cid:98)Qδt = { min yQ y2 < δt}, δt = δ0 βt/τ . (2) Here δ0 is the initial dilation radius, β is the dilation factor, and τ is the step length for the dilation factor update. Subsequently, for each predicted affordance element j, we construct its geometry descriptor Gj, which captures the elements spatial properties, formally defined as Cj and Σj denote the position and size, respectively. We also employ semantic descriptor Sj, which, in conjunction with the geometric one, represents the affordance type for element j. In summary, these descriptors together form compact yet visually unified representation of the 3D scene: D(P) = {(Cj R3, Σj R3, Sj)}N j=1, (3) where is the total number of predicted affordance elements in the scene P. These descriptors, capturing both geometric and semantic information of the affordance elements, provide structured representation of the scene that facilitates subsequent reasoning. 3D-2D Associations. Using the generated surround-view images V, we ground 3D affordance elements in these 2D views. For the predicted 3D elements with their descriptors D, we project their 3D geometry onto every view Vi V. Specifically, we compute the elements 2D bounding box projection based on its 3D position and dimensions, assigning each projected box both unique identifier linking it to the original 3D element and its corresponding affordance type Sj mapped to the 2D region. This process is formalized as: ˆVi = M3D2D (cid:0)D(P), Vi (cid:1), (4) where M3D2D denotes our projection operator. This process effectively transfers key 3D information onto the 2D images. Furthermore, to ensure legible MLLM input, we introduce an adaptive-labeling refinement strategy that resolves label collisions. This involves pre-defining candidate anchor positions around each projected box. When annotating an element, our pipeline iterates through these anchors, evaluating nonoverlap with existing elements, and selects the first suitable location. Such lightweight spatial check effectively prevents label stacking, maintains clear object visibility, and provides an uncluttered canvas for subsequent MLLM reasoning. Together, this collaborative representation enables downstream modules to access both the finegrained geometry of affordance elements and their visual context within the scene, laying the groundwork for subsequent reasoning. Figure 4: AffordBots Chain-of-Thought Pipeline for Embodied Reasoning. This structured observe-then-infer process leverages multimodal inputs to perform: (1) Active View Selection to identify the most informative view, which may involve zooming in to better see the details of the images, followed by (2) Affordance Grounding to localize target elements, and finally (3) Motion Estimation to infer the required action details. 3.3 Chain-of-thought Reasoning This section presents our method for enabling MLLM to perform embodied reasoning. Specifically, we design tailored chain-of-thought paradigm that follows structured observe-then-infer pipeline, as shown in Fig. 4. Grounded in the real physical world, our pipeline leverages visual observations to guide the MLLM through sequence of inference steps: first actively selecting the most informative viewpoint (observe), followed by identifying targets and their required interactions in context (infer). Step 1: Active View Selection. The goal of this step is to select the most informative view from the generated surround-view images ˆV. Unlike prior methods that rely on instruction parsing and heuristic filtering over pre-processed features, we leverage the multimodal reasoning capabilities of an MLLM to guide view selection directly, enhancing both flexibility and accuracy. Given the set of annotated views { ˆV1, . . . , ˆVN }, where each includes projected 3D elements with unique IDs and affordance types, along with the instruction , the MLLM receives these as inputs. It is then tasked with selecting the view in which all referenced elements are visible and their identifiers are clearly shown, providing the most relevant visual content. The model directs attention to the selected view ˆVselected, which serves as the semantically grounded visual input for subsequent reasoning. Step 2: Affordance Grounding. This step aims to localize the specific affordance elements referenced by the linguistic instruction within the scene. Using the selected view ˆVselected from the previous step, the MLLM receives this annotated view, the original instruction , and detailed descriptors of the 3D affordance elements, including their unique IDs and spatial attributes. The MLLM interprets the instruction and visual cues to identify the region that best matches the described task. This process yields unique IDs of the localized target elements, which serve as critical intermediate decision for the subsequent motion inference stage. Step 3: Motion Estimation. This final step focuses on inferring the motion details of the elements localized in Step 2. The MLLM is provided with the original instruction , the image ˆVselected, and its localized information from the previous step. Using these inputs, the MLLM deduces the intended action for the target element, including both the motion type and the direction of the motion axis. To enable compatibility between continuous direction vectors and MLLM outputs, we discretize the former into interpretable categories. These categories broadly distinguish motion directions as horizontal or vertical, with specific refinements for translational movements (e.g., inward/outward relative to the objects centroid) and rotational axes. This structured discretization ensures physical plausibility while remaining interpretable by the MLLM. The final output is an affordance-motion tuple that integrates the discretized motion representation with the models language-driven reasoning. 6 Table 1: Quantitative comparisons of our fine-grained embodied reasoning task. We report the quantitative results of affordance grounding and motion estimation task on SceneFun3D [34] dataset. Task Method OpenMask3D [36] LERF [35] OpenMask3D-F [34] OpenIns3D [66] Fun3DU [37] Fun3DU (+motion) AffordBot Raw 2D Input mIoU AP AP50 AP25 +T +TD Grounding Motion - - - 0.0 11.5 10.0 14.0 - - - 0.0 6.1 4.6 15. 0.0 4.9 8.0 0.0 12.6 9.9 20.0 0.0 11.3 17.5 0.0 23.1 18.7 23.3 - - - - - 11.5 18.3 - - - - - 4.0 10."
        },
        {
            "title": "4 Experiments",
            "content": "This section presents comprehensive experimental evaluation of our proposed fine-grained embodied reasoning framework, validating its effectiveness in joint affordance grounding and motion estimation. In addition, we conduct an in-depth analysis to assess how key module optimizations impact the systems overall accuracy and robustness. 4.1 Experimental Setup Dataset. We conduct experiments on SceneFun3D [34], currently the only dataset that provides comprehensive annotations for fine-grained affordance grounding and motion estimation in 3D indoor scenes. It comprises total of 230 richly annotated scenes, including 200 scenes for training, 30 for validation. Each scene provides dense point clouds annotated with element-level affordance masks, motion types, and motion axis directions. To facilitate instruction-oriented embodied reasoning for our task, we curate the annotation with task-specific annotation triples. Evaluation Metrics. We adopt standard evaluation metrics [34] to assess performance on 3D affordance grounding and motion estimation. Specifically, we report mean Intersection-over-Union (mIoU), mean average precision (mAP), and average precision at IoU thresholds (AP, AP25, AP50), between predicted and ground-truth masks. To incorporate motion parameter accuracy, we adapt the AP25 metric as proposed in [34, 51, 50], extending it with additional constraints on motion type and direction. Specifically, we further constrain mask prediction based on whether the model correctly predicts the motion type (+T), and both the motion type and motion axis direction (+TD). Implementation Details. For visual-language reasoning, we employ Qwen2.5-VL-72B [15] locally deployed on four NVIDIA A800 GPUs. To construct the geometry descriptors and get segmented elements masks, we fine-tune Mask3D [65] from pretrained checkpoint on ScanNet200 [67]. We train for 1,000 epochs on an NVIDIA A800 with the learning rate of 0.0001, batch size of 2, and 2cm voxelization to preserve spatial detail. 4.2 Quantitative Results The quantitative results for the fine-grained embodied reasoning on SceneFun3D dataset, as presented in Table 1, demonstrate the effectiveness of our AffordBot approach. By encoding 3D scenes into structured representations and processing them with task instructions via the MLLM, AffordBot outperforms existing methods including OpenMask3D [36, 34], LERF [35], OpenIns3D [66], and Fun3DU [37], as well as our enhanced Fun3DU (+motion) baseline. As shown in the table, AffordBot reports higher scores in both affordance grounding and motion estimation. Notably, the results of our reproduced Fun3DU (+motion) baseline (second to last row) highlight the impact of incorporating motion estimation branch into their original affordance grounding framework. For this baseline, we prompted Molmo [46] to infer motion parameters based on 2D segmentation results of affordance elements. The significant outperformance of AffordBot across all reported metrics underscores the advantage of our approach in accurately identifying and understanding the potential motions associated with affordance elements in 3D scenes. Specifically, our higher AP score, which is the average precision over IoU thresholds ranging from 0.5 to 0.95, 7 Table 2: Ablation on key components of our AffordBot. ALR denotes Adaptive Label Refinement, EVS denotes Enriched Visual Synthesis, and AVS denotes Active View Selection. Each variant incrementally incorporates one module, and finally EX4 corresponds to our AffordBot. Table 3: Ablation on viewpoint selection. BEV projects bird-eye view; Video Frame uniformly samples frames from dataset, while Query-Aligned picks the query-matching view; Ours renders surround views for MLLM selection. ALR EVS AVS AP AP50 AP25 Method AP AP50 AP25 EX1 EX2 EX3 EX 9.7 9.7 14.8 15.5 12.8 13.0 19.4 20.0 15.7 16.1 22.1 23. BEV Video Frame Query-Aligned Ours 6.1 9.4 9.7 15.5 9.1 11.4 13.0 20.0 12.7 15.6 16.1 23.3 indicates that AffordBot not only performs well in rough localization but also maintains high precision under stricter localization requirements (i.e., higher IoU thresholds). This is crucial for robotic manipulation tasks, where precise segmentation is essential for accurate grasping and manipulation. Furthermore, the results suggest the importance of grounding accuracy for subsequent tasks and indicate the enhanced spatial awareness provided by 3D-based motion reasoning. 4.3 Ablation Studies To quantify the contribution of AffordBot, we conduct ablation studies focusing solely on the affordance-grounding task, which is the critical prerequisite for motion estimation and downstream execution. This targeted approach is justified because the investigated modules (representation design and the MLLM-driven view-selection mechanism) operate entirely upstream of motion estimation. Once target is accurately grounded, motion prediction relies solely on that grounded object and fixed MLLM prompt. Consequently, any modifications to these upstream components propagate through the entire pipeline and are comprehensively reflected in grounding performance metrics. Ablation on Key Components. Through systematic component-wise analysis, we demonstrate how progressive module integration contributes to the performance, as shown in Tab. 2. Baseline EX1, adapted from [68], initially employs the MLLM to parse instructions and identify target affordance types. It then renders all matching segmented elements from the scenes center view, annotating each with unique identifiers at their 2D centroids. Finally, the MLLM processes these rendered views to localize the correct element. Adding Adaptive Label Refinement (ALR, EX2) identifier labels to avoid occlusion, our method yields modest but consistent lift of +0.4% AP25. The major improvement comes from enriched visual synthesis (EVS, EX3). The model gains much richer context, pushing AP25 from 16.1% to 22.1% (+6.0%). This significant improvement demonstrates that global, information-dense observation, as provided by EVS, is much more valuable than the single frame used in [68]. Finally, EX4 employs the active view selector (AVS). This focus-then-infer pipeline both trims redundant visual information and exploits the best evidence, raising AP25 to 23.3% and achieving the highest overall accuracy. Ablation on Viewpoint selection. To probe how viewpoint choice affects subsequent inference, we evaluate this in Tab. 3. While Birds-Eye View (BEV) representations provide scene overview, they prove ineffective for our task as affordance elements typically require fine-grained appearance details due to their small size. Sampling images directly from the video stream (i.e. Video Frame baseline) also yields limited effectiveness, outperforming the previous results. Query-aligned baseline retrieves single pre-tagged frame whose affordance class matches the query. Our Dynamic strategy takes different route: it first synthesises dense 360 sweep of surround views, then asks the MLLM to select the frame that best matches the instruction. This active observethen-infer routine supplies rich global context while keeping the final input compact, boosting AP25 to 23.3%, an absolute gain of 7.6% over the query-aligned method, as shown in Tab. 3. Probing the Primary Bottleneck of AffordBot. To investigate the primary bottlenecks constraining upstream segmentation and downstream active view selection performance, we progressively replace Mask3Ds predicted masks with ground-truth masks (GT proposals) and provide an ideal front-view perspective (GT proposals + views), as summarized in Table 4. The first row shows the baseline configuration (Mask3D proposals), which corresponds to our AffordBot. Replacing the predicted Table 4: Probing the bottleneck of our method. Mask3D proposals refers to our Affordbot, which uses predicted proposals. GT proposals denotes the use of ground-truth masks, while GT proposals & views additionally adopt groundtruth views. Method AP50 AP25 AP Mask3D proposals GT proposals GT proposals & views 15.5 35.7 38.3 20.0 39.4 42. 23.3 45.4 47.4 Table 5: Comparison of different MLLMs. Deployable models (LLaVA-v1.6-34B, Qwen2.5-VL-72B) and commercial GPT APIs show consistent trends, with larger models yielding stronger performance. Method AP AP50 AP25 LLaVA-v1.6-34B Qwen2.5-VL-72B GPT-4o GPT-o 10.6 15.5 16.5 24.8 14.2 20.0 22.1 30.3 16.9 23.3 28.9 33.4 Table 6: Performance variation across affordance types. Segment measures upstream segmentation accuracy, while Reason reports final grounding. Type rotate key_press tip_push hook_pull pinch_pull hook_turn foot_push plug_in unplug Segment Reason 0.0 2.5 11.3 30.4 5.3 5. 27.5 18.0 27.1 23.5 67.8 45.1 100.0 100.0 11.1 8.3 15.3 16. Table 7: Performance variation with different numbers of target elements. We compare tasks involving single ground-truth element (Unique) versus multiple elements (Multiple). Method mIoU AP AP50 AP +T +TD Unique Multiple Overall 13.2 19.1 14.0 13.8 27.2 15.5 18.2 32.1 20. 21.4 35.8 23.3 16.5 30.2 18.3 9.9 17.0 10.8 masks with GT proposals results in 22.1% boost in AP25, highlighting that instance segmentation noise is the primary limiting factor. With perfect segmentation, adding the optimal viewpoint further improves performance by +2.0% AP25, suggesting that while active perception can still be optimized, it is not the dominant bottleneck. Comparison of Different MLLMs. We replace the default Qwen2.5-VL-72B with several representative alternatives (LLaVA-v1.6-34B, GPT-4o, and GPT-o1), as reported in Tab. 5. While the commonly used Qwen achieves 23.3% AP25, adopting the more advanced GPT-o1, which features superior reasoning and visual understanding, further boosts performance to 33.4% AP25. This demonstrates that leveraging stronger MLLMs can unlock even greater potential within our framework. Performance Variation Analysis. We conduct detailed performance analysis to uncover variations across different affordance types and target element counts. Tab. 6 highlights significant disparities in AP50 across different affordance types, partly reflecting dataset class imbalance and strong dependence on initial segmentation quality. Performance ranges widely (e.g., 100% for foot_push vs. 0% for rotate]), limiting grounding accuracy. The MLLM improves performance for some categories using linguistic cues (e.g., key_press), but challenges remain in aligning visual and language cues for others (e.g., tip_push degradation). Further analysis of task subsets (Tab. 7) reveals better performance for Multiple target elements than Unique ones. This difference is primarily due to Mask3D struggling with the small, weaklytextured objects typical of Unique instances, resulting in noisier descriptors that hinder subsequent affordance grounding and motion estimation. 4.4 Qualitative Results Fig. 5 provides detailed qualitative results of our fine-grained reasoning task. AffordBot generally shows more accurate and consistent grounding of target elements, particularly in complex scenarios with multiple or small targets. Overall, qualitative evidence further suggests that AffordBot achieves significantly improved affordance understanding compared to the prior SOTA method [37]. 9 Figure 5: Qualitative Results. The figure showcases visual examples of AffordBot performing fine-grained grounding. The illustrated examples include: (1) Turn on the TV using the remote control on the table. (2) Open the middle drawer of the TV stand. (3) Close the bedroom door. (4) Open the window above the radiator. Please zoom in digitally to view more details."
        },
        {
            "title": "5 Conclusion",
            "content": "Fine-grained embodied reasoning in 3D worlds serves as crucial bridge from perception to action, making it pivotal for agents to perform sophisticated tasks. While prior work has explored affordance grounding and motion estimation in isolation, our work unifies these tasks under structured reasoning framework, AffordBot, bridging perception and action through instruction-aware triplet prediction. By leveraging MLLM with tailored chain-of-thought paradigm, our method ensures physically grounded reasoning that advances from scene perception to affordance localization and motion synthesis. Through extensive experiments, AffordBot not only advances state-of-the-art performance but also demonstrates the feasibility of efficient, task-coherent embodied reasoning, paving the way for more intuitive human-agent interaction in complex 3D spaces. In the future, we will empower AffordBot with more advanced multimodal understanding ability [69]."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the National Natural Science Foundation of China (NSFC) under Grant U22A2094, and also supported by the Ministry of Education, Singapore, under its MOE Academic Research Fund Tier 2 (MOE-T2EP20124-0013). We also acknowledge the support of the Supercomputing Center of USTC for providing advanced computing resources and of the NSFC with Grant No. 62573371."
        },
        {
            "title": "References",
            "content": "[1] Mohammed Hassanin, Salman Khan, and Murat Tahtali. Visual affordance and function understanding: survey. ACM Computing Surveys (CSUR), 54(3):135, 2021. [2] Paola Ardón, Eric Pairet, Katrin Lohan, Subramanian Ramamoorthy, and Ronald Petrick. Building affordance relations for robotic agents-a review. arXiv preprint arXiv:2105.06706, 2021. [3] Dongpan Chen, Dehui Kong, Jinghua Li, Shaofan Wang, and Baocai Yin. survey of visual affordance recognition based on deep learning. IEEE Transactions on Big Data, 9(6):14581476, 2023. [4] James Gibson. The theory of affordances:(1979). In The people, place, and space reader, pages 5660. Routledge, 2014. [5] Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and Kui Jia. 3d affordancenet: benchmark for visual object affordance understanding. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17781787, 2021. [6] Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and Leonidas Guibas. O2o-afford: Annotation-free large-scale object-object affordance learning. In Conference on robot learning, pages 16661677. PMLR, 2022. [7] Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, and Zheng-Jun Zha. Grounding 3d object affordance from 2d interactions in images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1090510915, 2023. [8] Tushar Nagarajan and Kristen Grauman. Learning affordance landscapes for interaction exploration in 3d environments. Advances in Neural Information Processing Systems, 33:20052015, 2020. [9] Chao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin Zhu, and Siyuan Huang. Partafford: Part-level affordance discovery from 3d objects. arXiv preprint arXiv:2202.13519, 2022. [10] Ruihai Wu, Kai Cheng, Yan Zhao, Chuanruo Ning, Guanqi Zhan, and Hao Dong. Learning environmentaware affordance for 3d articulated object manipulation under occlusions. Advances in Neural Information Processing Systems, 36, 2024. [11] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 815824, 2023. [12] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: From pixels to actions for articulated 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 68136823, 2021. [13] Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, and Hao Dong. Where2explore: Few-shot affordance learning for unseen novel categories of articulated objects. Advances in Neural Information Processing Systems, 36, 2024. [14] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [15] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [16] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [18] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 11 [19] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [20] Zhen Zeng, Leijiang Gu, Xun Yang, Zhangling Duan, Zenglin Shi, and Meng Wang. Visual-oriented fine-grained knowledge editing for multimodal large language models. In ICCV, 2025. [21] Haowen Pan, Yixin Cao, Xiaozhi Wang, Xun Yang, and Meng Wang. Finding and editing multi-modal neurons in pre-trained transformers. In Findings of the Association for Computational Linguistics: ACL 2024, pages 10121037, 2024. [22] Yucheng Han, Na Zhao, Weiling Chen, Keng Teck Ma, and Hanwang Zhang. Dual-perspective knowledge enrichment for semi-supervised 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 20492057, 2024. [23] Pengkun Jiao, Na Zhao, Jingjing Chen, and Yu-Gang Jiang. Unlocking textual and visual wisdom: Openvocabulary 3d object detection enhanced by comprehensive guidance from text and image. In European Conference on Computer Vision, pages 376392. Springer, 2024. [24] Linfeng Li and Na Zhao. End-to-end semi-supervised 3d instance segmentation with pcteacher. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 53525358. IEEE, 2024. [25] Jiangyi Wang and Na Zhao. Uncertainty meets diversity: comprehensive active learning framework for indoor 3d object detection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2032920339, 2025. [26] Na Zhao, Tat-Seng Chua, and Gim Hee Lee. Few-shot 3d point cloud semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 88738882, 2021. [27] Chaofan Luo, Donglin Di, Xun Yang, Yongjia Ma, Zhou Xue, Chen Wei, and Yebin Liu. Trame: Trajectoryanchored multi-view editing for text-guided 3d gaussian splatting manipulation. IEEE Transactions on Multimedia, 2025. [28] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pages 202221. Springer, 2020. [29] Xinyi Wang, Na Zhao, Zhiyuan Han, Dan Guo, and Xun Yang. Augrefer: Advancing 3d visual grounding via cross-modal augmentation and spatial relation-based referring. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 80068014, 2025. [30] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. [31] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, SongChun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. [32] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2642826438, 2024. [33] Kuan-Chih Huang, Xiangtai Li, Lu Qi, Shuicheng Yan, and Ming-Hsuan Yang. Reason3d: Searching and reasoning 3d segmentation via large language model. In International Conference on 3D Vision 2025, 2025. [34] Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, and Francis Engelmann. Scenefun3d: Fine-grained functionality and affordance understanding in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14531 14542, 2024. [35] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1972919739, 2023. 12 [36] Ayça Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Openmask3d: Open-vocabulary 3d instance segmentation. arXiv preprint arXiv:2306.13631, 2023. [37] Jaime Corsetti, Francesco Giuliari, Alice Fasoli, Davide Boscaini, and Fabio Poiesi. Functionality understanding and segmentation in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [38] Zhihao Yuan, Shuyi Jiang, Chun-Mei Feng, Yaolun Zhang, Shuguang Cui, Zhen Li, and Na Zhao. Scene-r1: Video-grounded large language models for 3d scene reasoning without 3d annotations. arXiv preprint arXiv:2506.17545, 2025. [39] Thanh-Toan Do, Anh Nguyen, and Ian Reid. Affordancenet: An end-to-end deep learning approach for object affordance detection. In 2018 IEEE international conference on robotics and automation (ICRA), pages 58825889. IEEE, 2018. [40] Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, and Tat-seng Chua. Laso: Language-guided affordance segmentation on 3d object. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1425114260, 2024. [41] Timo Luddecke and Florentin Worgotter. Learning to segment affordances. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 769776, 2017. [42] Xue Zhao, Yang Cao, and Yu Kang. Object affordance detection with relationship-aware network. Neural Computing and Applications, 32(18):1432114333, 2020. [43] Gen Li, Deqing Sun, Laura Sevilla-Lara, and Varun Jampani. One-shot open affordance learning with In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern foundation models. Recognition, pages 30863096, 2024. [44] Edmond Tong, Anthony Opipari, Stanley Lewis, Zhen Zeng, and Odest Chadwicke Jenkins. Oval-prompt: Open-vocabulary affordance localization for robot manipulation through llm affordance-grounding. arXiv preprint arXiv:2404.11000, 2024. [45] Claudia Cuttano, Gabriele Rosi, Gabriele Trivigno, and Giuseppe Averta. What does clip know about In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern peeling banana? Recognition, pages 22382247, 2024. [46] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [47] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [48] Hanxiao Jiang, Yongsen Mao, Manolis Savva, and Angel Chang. Opd: Single-view 3d openable part detection. In European Conference on Computer Vision, pages 410426. Springer, 2022. [49] Cheng-Chun Hsu, Zhenyu Jiang, and Yuke Zhu. Ditto in the house: Building articulation models of indoor scenes through interactive perception. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 39333939. IEEE, 2023. [50] Xiaohao Sun, Hanxiao Jiang, Manolis Savva, and Angel Chang. Opdmulti: Openable part detection for multiple objects. In 2024 International Conference on 3D Vision (3DV), pages 169178. IEEE, 2024. [51] Yongsen Mao, Yiming Zhang, Hanxiao Jiang, Angel Chang, and Manolis Savva. Multiscan: Scalable rgbd scanning for 3d environments with articulated objects. Advances in neural information processing systems, 35:90589071, 2022. [52] Yuki Kawana, Yusuke Mukuta, and Tatsuya Harada. Unsupervised pose-aware part decomposition for man-made articulated objects. In European Conference on Computer Vision, pages 558575. Springer, 2022. [53] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, and Kai Xu. Shape2motion: Joint analysis of motion parts and attributes from 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88768884, 2019. 13 [54] Xiaolong Li, He Wang, Li Yi, Leonidas Guibas, Lynn Abbott, and Shuran Song. Category-level articulated object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37063715, 2020. [55] Xianghao Xu, Yifan Ruan, Srinath Sridhar, and Daniel Ritchie. Unsupervised kinematic motion detection for part-segmented 3d shape collections. In ACM SIGGRAPH 2022 Conference Proceedings, pages 19, 2022. [56] Liu Liu, Jianming Du, Hao Wu, Xun Yang, Zhenguang Liu, Richang Hong, and Meng Wang. Categorylevel articulated object 9d pose estimation via reinforcement learning. In Proceedings of the 31st ACM International Conference on Multimedia, pages 728736, 2023. [57] Liu Liu, Anran Huang, Qi Wu, Dan Guo, Xun Yang, and Meng Wang. Kpa-tracker: Towards robust and real-time category-level articulated object 6d pose tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 36843692, 2024. [58] Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Runsen Xu, Ruiyuan Lyu, Dahua Lin, and Jiangmiao Pang. Grounded 3d-llm with referent tokens. arXiv preprint arXiv:2405.10370, 2024. [59] Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3d scenes from videos with vision-language models. arXiv preprint arXiv:2501.01428, 2025. [60] Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, and Yue Gao. Hyper-3dg: Textto-3d gaussian generation via hypergraph. International Journal of Computer Vision, 133(5):28862909, 2025. [61] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual understanding and reasoning. arXiv preprint arXiv:2403.11401, 2024. [62] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. Advances in Neural Information Processing Systems, 36:3719337229, 2023. [63] Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. In European Conference on Computer Vision, pages 289310. Springer, 2024. [64] Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. Chat-scene: Bridging 3d scene and large language models with object identifiers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [65] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 82168223. IEEE, 2023. [66] Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, and Joan Lasenby. Openins3d: Snap and lookup for 3d open-vocabulary instance segmentation. European Conference on Computer Vision, 2024. [67] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic segmentation in the wild. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. [68] Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, and Junwei Liang. Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding. arXiv preprint arXiv:2412.04383, 2024. [69] Zhiyuan Han, Beier Zhu, Yanlong Xu, Peipei Song, and Xun Yang. Benchmarking and bridging emotion conflicts for multimodal emotion reasoning. In ACM Multimedia, 2025."
        },
        {
            "title": "A Overview",
            "content": "This document complements the main paper by providing (i) comprehensive implementation details covering geometric descriptor generation, view synthesis, adaptive label refinement, motion axis direction discretization, and the specifically designed chain-of-thought prompts for our method (Sec. B); (ii) additional qualitative results on our fine-grained embodied reasoning task (Sec. C); (iii) an in-depth discussion of current limitations and potential future research directions (Sec. D); (iv) the analysis of AffordBots broader impact for real-world applications (Sec. E); and (v) licensing information for the used assets (Sec. F). Together, these materials offer technical details and deeper insight into our fine-grained embodied-reasoning task and framework."
        },
        {
            "title": "B Implementation Details",
            "content": "In this section, we present more implementation details of our AffordBot framework to support reproducibility and scalable practical deployment. Geometric Descriptor Generation. For affordance element segmentation, we implement the Mask3D architecture [65] with the curriculum-based training strategy from [34] to enhance the segmentation of tiny elements. Our method progressively dilates ground-truth masks during training, parameterized by initial radius δ0 = 0.1, decay rate β = 0.5, and update interval τ = 200 iterations. Crucially, to enable affordance-aware segmentation, we explicitly redefine ground-truth labels based on affordance types rather than category names. Our framework recognizes nine distinct affordance types: rotate, key_press, tip_push, hook_pull, pinch_pull, hook_turn, foot_push, plug_in, and unplug. Consistent with the vanilla Mask3D, we initialize the transformer decoder with 80 instance queries to capture fine-grained instance information. For supervision, we combine region-level and point-wise objective losses with weights λ1 = 5 and λ2 = 2, respectively. Enriched Visual Synthesis. To obtain the initial observation position, we first compute the geometric center of all segmented affordance elements within each 3D scene. Recognizing that these elements are often low in height and thus suboptimal as direct camera viewpoints for agents or humans, we anchor the vertical coordinate of the observation center to the global center height of the point cloud. This yields our final 3D observation center. Subsequently, we position virtual camera at the center and synthesize =8 views by rotating it at evenly spaced angles over full 360 on fixed horizontal plane. The virtual camera intrinsics are fixed, featuring 90 field of view and an output resolution of 680 680 pixels. The rendered images serve as the bridge between 3D input and 2D MLLM, eliminating the need for 2D input. Adaptive Label Refinement. We introduce lightweight yet effective label refinement procedure to ensure that each segmented proposal is consistently and compactly annotated across rendered views. This algorithm prioritizes visual clarity and spatial efficiency by strategically placing labels near their corresponding object projections while avoiding overlaps with existing annotations. Unlike more complex layout optimization techniques, our method strikes practical balance between simplicity and precision. By iteratively searching among small set of candidate anchor positions, it ensures that labels remain legible and non-intrusive. The full procedure is described in Algorithm 1. Motion Axis Discretization. To enable classification over motion axis directions, we discretize the continuous vector ai R3 (provided in SceneFun3D [34] dataset) into the interpretable motion direction primitive. This abstraction offers simple yet effective foundation for learning and reasoning about motion in the context of embodied interaction. We extract the dominant axis of motion by identifying the dimension of ai with the highest absolute magnitude. For translational motions, we further infer inwards or outwards (as in SceneFun3D), yielding four categories: horizontal inwards, horizontal outwards, vertical inwards, and vertical outwards. For rotational motions, we retain only two categories: horizontal and vertical, as the direction of rotation is less semantically significant. This discretization results in total of six motion direction primitives, streamlining the prediction objective while preserving the essential interaction semantics for agents. Prompts for Chain-of-Thought Reasoning. We design specialized prompts to guide our MLLM through three sequential reasoning steps in AffordBot. For active view selection, we provide the MLLM with all the surround-view images and the task description to identify the index of the most 15 relevant, as shown in Fig. 8. Next, for affordance grounding, we input the selected image, task description, and 3D spatial information for all the segmented elements to the MLLM. The model predicts the IDs of the elements that best match the description, as illustrated in Fig. 9. Finally, to analyze the motion parameters, we present the MLLM with the same image, task description, and the previously predicted IDs, including their spatial information. The prompt asks for the motion type and the axis direction for the target, as depicted in Fig. 10. Algorithm 1 Adaptive Label Refinement Input: Rendered images V, segmented proposals and their identifiers L, camera parameters Output: Annotated images ˆV = (xmin, ymin, xmax, ymax) ProjectBBox(Pj, Ci) Roccupied Roccupied {b} 0 while < do ak A[k] ranchor PlaceAtAnchor(ak, b) if ranchor Rimg and ranchor Roccupied = then (W, H) GetImageSize(Vi) Rimg [0, ) [0, H) Roccupied for all (Pj, Lj) zip(P, L) do 1: ˆV 2: Anchor candidates: = top-left, top-right, left, right 3: for all (Vi, Ci) zip(V, C) do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end for 29: return ˆV rfallback GetTopRightFallback(b) ˆVi Annotate(Vi, Lj, rfallback) Roccupied Roccupied {rfallback} ˆVi Annotate(Vi, Lj, ranchor) Roccupied Roccupied {ranchor} break end while if == then end for ˆV ˆV { ˆVi} end if + end if"
        },
        {
            "title": "C Additional Qualitative Results",
            "content": "In this section, we present additional qualitative results on the fine-grained embodied reasoning task to further substantiate the superiority of our AffordBot. The evaluation encompasses both affordance grounding and motion parameter estimation. As illustrated in Fig. 6, our method achieves markedly higher localization accuracy than the current state-of-the-art (SOTA) approach [37] and produces crisper element boundaries. This stronger spatial grounding yields more reliable motion parameter predictions, thereby corroborating the effectiveness of our framework. 16 Figure 6: Additional qualitative results on our fine-grained embodied reasoning task."
        },
        {
            "title": "D Limitations and Future Directions",
            "content": "While AffordBot demonstrates significant advancements over existing approaches, there remain directions for potential improvement. The performance of our framework heavily depends on the accuracy of the initial element segmentation. For example, missing or inaccurate elements severely impair subsequent reasoning, as illustrated in Fig. 7a. This observation indicates that continued progress in this fundamental component could yield significant improvement to our method. Beyond improving segmentation, further exploration of our effective surround-view synthesis and active view selection strategy also presents important research opportunities. Specifically, future work could synthesize multi-level views to capture information at different granularities, or leverage feedback from the MLLM to guide camera placement and select truly optimal viewpoints. Rendering solely from scene-centric position, as in our method, may cause occlusion or poor visibility (see Fig. 7b). We plan to explore these directions to further strengthen the robustness and accuracy. 17 (a) Segmentation Failure: Affordance segmentation misses the bottom drawer, preventing it from subsequent reasoning about the corresponding instruction. (b) Viewpoint Limitation: Improper observation position induces occlusion, resulting in partial or ambiguous scene observation. Figure 7: Illustrations of failure cases."
        },
        {
            "title": "E Broader Impact",
            "content": "AffordBot enhances human-agent collaboration by enabling precise and physically grounded interactions, benefiting the development in areas such as home robotics, healthcare, and industrial automation. However, improved automation capabilities may lead to job displacement, and reliance on Multimodal Large Language Models (MLLMs) poses risks of inherent biases."
        },
        {
            "title": "F Licenses for Used Assets",
            "content": "SceneFun3D dataset [34] is under CC-BY-4.0 license. Qwen2.5-VL-72B model [15] is under Apache-2.0 license. 18 Figure 8: Chain-of-thought prompt for active view selection. Figure 9: Chain-of-thought prompt for affordance grounding. 19 Figure 10: Chain-of-thought prompt for motion estimation."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong, Shenzhen",
        "Singapore University of Technology and Design",
        "University of Science and Technology of China"
    ]
}