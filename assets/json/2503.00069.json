{
    "paper_title": "Societal Alignment Frameworks Can Improve LLM Alignment",
    "authors": [
        "Karolina Stańczak",
        "Nicholas Meade",
        "Mehar Bhatia",
        "Hattie Zhou",
        "Konstantin Böttinger",
        "Jeremy Barnes",
        "Jason Stanley",
        "Jessica Montgomery",
        "Richard Zemel",
        "Nicolas Papernot",
        "Nicolas Chapados",
        "Denis Therien",
        "Timothy P. Lillicrap",
        "Ana Marasović",
        "Sylvie Delacroix",
        "Gillian K. Hadfield",
        "Siva Reddy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in large language models (LLMs) has focused on producing responses that meet human expectations and align with shared values - a process coined alignment. However, aligning LLMs remains challenging due to the inherent disconnect between the complexity of human values and the narrow nature of the technological approaches designed to address them. Current alignment methods often lead to misspecified objectives, reflecting the broader issue of incomplete contracts, the impracticality of specifying a contract between a model developer, and the model that accounts for every scenario in LLM alignment. In this paper, we argue that improving LLM alignment requires incorporating insights from societal alignment frameworks, including social, economic, and contractual alignment, and discuss potential solutions drawn from these domains. Given the role of uncertainty within societal alignment frameworks, we then investigate how it manifests in LLM alignment. We end our discussion by offering an alternative view on LLM alignment, framing the underspecified nature of its objectives as an opportunity rather than perfect their specification. Beyond technical improvements in LLM alignment, we discuss the need for participatory alignment interface designs."
        },
        {
            "title": "Start",
            "content": "Karolina Sta nczak 1 2 Nicholas Meade 1 2 Mehar Bhatia 1 2 Hattie Zhou 1 3 4 Konstantin Böttinger 5 Jeremy Barnes 6 Jason Stanley 6 Jessica Montgomery 7 Richard Zemel 8 Nicolas Papernot 9 10 Nicolas Chapados 1 6 Denis Therien 2 6 Timothy Lillicrap 10 Ana Marasovic 11 Sylvie Delacroix 12 Gillian Hadfield 13 Siva Reddy 1 2 6 5 2 0 2 7 2 ] . [ 1 9 6 0 0 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in large language models (LLMs) has focused on producing responses that meet human expectations and align with shared values process coined alignment. However, aligning LLMs remains challenging due to the inherent disconnect between the complexity of human values and the narrow nature of the technological approaches designed to address them. Current alignment methods often lead to misspecified objectives, reflecting the broader issue of incomplete contracts, the impracticality of specifying contract between model developer, and the model that accounts for every scenario in LLM alignment. In this paper, we argue that improving LLM alignment requires incorporating insights from societal alignment frameworks, including social, economic, and contractual alignment, and discuss potential solutions drawn from these domains. Given the role of uncertainty within societal alignment frameworks, we then investigate how it manifests in LLM alignment. We end our discussion by offering an alternative view on LLM alignment, framing the under-specified nature of its objectives as an opportunity rather than perfect their specification. Beyond technical improvements in LLM alignment, we discuss the need for participatory alignment interface designs. 1. Introduction As large language models (LLMs) advance to unprecedented levels of proficiency in generating human-like language, Work done by HZ prior to joining Anthropic. 1Mila Quebec AI Institute 2McGill University 3Université de Montréal 4Anthropic 5Fraunhofer AISEC 6ServiceNow 7University of Cambridge 8Columbia University 9University of Toronto 10Google DeepMind 11University of Utah 12Kings College London 13Johns Hopkins University. Correspondence to: Karolina Stanczak <karolina.stanczak@mila.quebec>. 1 aligning their behavior with human values has become critical challenge to ensuring their usability in real-world applications (Leike et al., 2018; Gabriel, 2020; Ouyang et al., 2022; Shen et al., 2023). This alignment encompasses both explicit values, such as following instructions and being helpful, and implicit values, such as remaining truthful and avoiding biased or otherwise harmful outputs (Askell et al., 2021). In fact, the rise of LLM-based chat assistants has largely been driven by their ability to follow instructions and engage in open-ended dialogue, demonstrating the importance of alignment, enabled by algorithms such as reinforcement learning from human feedback (RLHF; Ouyang et al. 2022; Ziegler et al. 2020). Despite these advancements, aligning LLMs with human values remains formidable challenge (Wei et al., 2023; Williams et al., 2024; Greenblatt et al., 2024). This difficulty primarily stems from the fundamental gap between the intricacies of human values and the often narrow technological solutions (Hadfield-Menell & Hadfield, 2019). Current LLM alignment methods, such as RLHF, often result in misspecified alignment objectives, where reward functions reflect human values only within designer (or annotators) provided scenarios, finite set among an infinite set of values, failing to generalize in unforeseen contexts (Amodei et al., 2016; Hadfield-Menell & Hadfield, 2019; Turner et al., 2020; 2021; Skalse et al., 2024). While developers acknowledge the problem of misspecification (Leike et al., 2018; Shen et al., 2023; Ouyang et al., 2022), the root causes of this issue have been largely overlooked. To better understand this misalignment, we frame LLM alignment within principal-agent1 framework (Eisenhardt, 1989), well-established paradigm in economic theory. As shown in Figure 1, in this framework, the LLM acts as the agent and the model developer (or user) serves as the principal. We define contract as pair: an action taken by the agent and the corresponding reward assigned by the principal. For example, contract in LLM training 1We use agent in the contract theory sense, referring to an entity acting on behalf of principal, rather than the broader AI notion of autonomous systems. Societal Alignment Frameworks Can Improve LLM Alignment Figure 1. We view human-LLM interactions as principal-agent framework, where principal (a system designer) incentivizes an agent (an LLM) to take an action by offering reward r. This framework assumes that the agents action is driven by its reward function, forming pair (a, r) that serves as contract between the agent and the principal. However, this contract is incomplete. To address this incompleteness, we explore societal alignment mechanisms of social, economic, and contractual alignment as guiding principles for LLM alignment in the incomplete contracting environment. could reward the model for generating responses that follow factual accuracy constraints while penalizing hallucinated outputs. The principal is able to steer the agents behavior toward intended objectives with an appropriate reward. In an ideal scenario, complete contract would perfectly align the agents actions with the principals objectives in all possible states of the world. However, designing fully specified contract that anticipates every possible scenario in model training is infeasible (Hadfield-Menell & Hadfield, 2019; Zhuang & HadfieldIn LLM alignment, this challenge is reMenell, 2020). flected in the reward function, which is derived from explicitly elicited values or implicitly implied values in the form of human preferences. Yet, quantifying complex and often diverging human values is difficult (Leike et al., 2018; Feffer et al., 2023), and capturing them effectively incurs high annotation costs (Klingefjord et al., 2024). Aggregating these values into unified reward signal is nontrivial (Kemmer et al., 2020; Ilvento, 2020). These alignment challenges are not unique to LLMs. In fact, they echo broader alignment problems that humans encounter daily due to incomplete contracts. Institutions such as society, economy, and law enable us to thrive despite incompleteness. In this position piece, we advocate for leveraging insights from societal alignment frameworks to guide the development of LLM alignment within incomplete contracting environments. Drawing on principles from social alignment (Section 4.1), economic alignment (Section 4.2), and contractual alignment (Section 4.3), we propose solutions to guide behavior in incomplete contracting environments, much like they have for human societies (see Figure 1). However, even within these frameworks, uncertainty remains an inherent factor in incomplete contracting environments (Seita, 1984). In Section 5, we examine how uncertainty manifests in LLM alignment. For instance, an LLM analyzing patients symptoms to suggest diagnosis might lack access to the patients full medical history or contextual background. In such cases, the model must navigate uncertainty to avoid overwhelming the user with complex, unfiltered medical information, which could lead to confusion or misinterpretation. Finally, we offer an alternative view on LLM alignment (Section 6), framing its under-specified objectives as an opportunity rather than flaw that can be resolved solely through technological solutions. Accordingly, we discuss the need for participatory alignment interface designs that actively engage diverse stakeholders in LLM alignment. 2. Contemporary Approach to LLM"
        },
        {
            "title": "Alignment",
            "content": "Aligning LLMs with human values is commonly understood as training them to act in accordance with user intentions (Leike et al., 2018). The objective of LLM alignment is often conceptualized as fulfilling three core qualities, often referred to as the 3H framework: honesty (regarding their capabilities, internal states, and knowledge), helpfulness (in performing requested tasks or answering questions within safe bounds), and harmlessness (encompassing both the re2 Societal Alignment Frameworks Can Improve LLM Alignment fusal to fulfill harmful requests and the avoidance of generating harmful content) (Askell et al., 2021; Bai et al., 2022a). prominent approach to achieve this alignment is through preference-based approach like RLHF. The RLHF pipeline usually includes three stages: supervised fine-tuning (SFT), preference sampling and reward model training (Christiano et al., 2017; Stiennon et al., 2020), and reinforcement learning fine-tuning either using proximal policy optimization (PPO; Schulman et al. 2017), or directly through policy optimization (DPO; Rafailov et al. 2023). The process usually starts with generic pre-trained language model, which undergoes supervised learning on high-quality dataset for specific downstream tasks. In this paper, we focus on the implications of the reward modeling stage due to its connection to an incomplete contract, which we will lay out in Section 3. 2.1. Reward modeling from human preference. In the reward modeling stage, for given input prompt x, the SFT model generates paired outputs, y0, y1 Y, where denotes the set of all possible outputs that the model can generate in response to given input. Human evaluators then select their preferred response, y0, y1, providing data that guides the alignment process (Christiano et al., 2017; Stiennon et al., 2020). Human preferences are modeled probabilistically using frameworks like the Bradley-Terry model (Bradley & Terry, 1952). The preference probability for one response over another is expressed as p(y1 y2 x) = exp(r(x, y1)) exp(r(x, y1)) + exp(r(x, y2)) , (1) where r(x, y) is latent reward function approximated by parametric reward model, rϕ(x, y). Using dataset of comparisons D, the reward model is trained by minimizing the negative log-likelihood LR(rϕ, D) = E(x,yw,yl)D (2) log σ(cid:0)rϕ(x, yw) rϕ(x, yl)(cid:1)(cid:105) , (cid:104) where σ is the logistic function and yw and yl denote the preferred and dispreferred completions among (y1, y2). 3. LLM Alignment as Contract In the following, we formalize LLM alignment through the lens of contract theory (Bolton & Dewatripont, 2005; Echenique et al., 2023), subfield of economics that studies how agreements are designed under conditions of incomplete information. We describe human-LLM interactions as principal-agent relationship, where principal (e.g., the user, system designer, or company) seeks to incentivize an agent (an LLM) to act in desired manner (Garen, 1994) (see Figure 1). This framework provides way to conceptualize how the principal tries to align the agents behavior with their objectives, using the agents action and its reward function as contract. In this section, we explore the contract formalization (Section 3.1) and how the incompleteness of this contract (Section 3.2) directly leads to misalignment (Section 3.3) in the context of LLM alignment. 3.1. Contract Formalization Following Echenique et al. (2023), we define contract as pair (a, r), where represents an action of an agent and : (X Y) is reward function.2 The function determines the agents reward based on the observed inputoutput pair (x, y). In the context of user-LLM interaction, an input corresponds to user prompt, and output is the LLM-generated response. contract might be, for instance, positive reward if the model avoids hate speech in the output. Here, the reward function would be trained on prompt-response pairs, awarding higher scores to responses that do not contain hate speech. The framework is initiated, for instance, when user, acting as the principal, initiates the interaction by prompting an LLM, thus implicitly proposing contract. The LLM, acting as an agent, then implicitly either accepts or rejects this contract. Rejection of the contract manifests in the LLM not converging towards the desired output, which is generated response without hate speech. Upon implicitly accepting the contract, the LLM conducts an action a, which can be viewed as probability distribution over all possible model outputs that satisfy the contract. We note that the user does not directly observe the LLMs internal decision of its action but only the output y. Consequently, the agent is rewarded according to the agreed-upon reward function, r(x, y), implemented as reward signal during the training phase. The principal experiences the utility derived from the output y; that is, the user benefits from the generated response but also suffers if the model behaves adversarially. This is illustrated when, despite contract penalizing hate speech, the LLM generates responses that subtly convey harmful biases. 3.2. The Challenge of Incomplete Contracting in AI Although the specific implications of incomplete contracting for LLM alignment remain underexplored, the concept has been studied in the broader context of AI alignment (Hadfield-Menell & Hadfield, 2019). In theory, alignment between the principal and the agent theoretically requires complete contract (Williamson, 1973; Hadfield-Menell & Hadfield, 2019). complete contract would perfectly align the principals objectives with the agents behavior in all possible states of the world. This requires that action 2Here we loosely refer to mean one action or series of actions that lead to an LLM output. 3 Societal Alignment Frameworks Can Improve LLM Alignment and reward function r(x, y) be optimally defined for all input-output pairs. However, achieving complete contracts is practically infeasible for AI systems, rendering incomplete contracting unavoidable (Hadfield-Menell & Hadfield, 2019). This is primarily due to the fact that machine learning systems inherently operate with underspecified objectives (DAmour et al., 2022), which stems from the practical difficulty in defining reward function r(x, y) that fully captures the complexities of the desired behavior. The difficulty in specifying such complete reward function arises from several issues. First, key challenge for AI alignment generally, real-world applications are too complex to generate all possibilities, hindering the specification of every possible (a, r) pair (OpenAI, 2016). The space of possible outcomes, denoted by in the formalization is not tractable. This mirrors the challenge of LLM in generating outputs for new input it might receive during inference. The challenge extends beyond the practical limitations of fully specifying objectives. Second, and particularly relevant for LLMs, even beyond these practical limitations, the challenge of translating complex human values into reward functions remains. Ambiguities and gaps in defining the desired action contribute to unintended and often undesirable outcomes. 3.3. Misalignment due to an Incomplete Contract We frame LLM alignment as challenge of incomplete contracting, which leads to misalignment. In the context of LLMs, this misalignment occurs when the reward function, r(x, y) is underspecified, and thus might incentivize outputs that diverge from the userss true objectives. common outcome of reward misspecification is reward hacking, where an agent optimizes for the reward itself rather than the intended behavior. For example, LLMs may exploit gaps in the specifications, such as in the jailbreaking phenomenon. Here, carefully crafted prompts elicit harmful responses by bypassing weak guardrails because their reward function is not specific enough, allowing the model to optimize without complying with safety requirements (Chao et al., 2024; Zou et al., 2023). Another example of reward hacking is an LLM trained to generate helpful responses might learn to produce lengthy and verbose answers to prompts, as this might result in higher score from the reward function even if it is not actually helpful to the user (Saito et al., 2023). related issue, fake alignment, occurs where the agents superficially comply with the training objective without adopting the intended internal goals (Greenblatt et al., 2024). Another challenge is the inherent context dependence of reward functions, which need to adapt appropriately to evolving contexts. contract might specify desired behavior in narrow scenario, but leave ambiguities for broader applications (Hadfield-Menell et al., 2017). For example, contract that stipulates no harmful bias in model is inherently underspecified since the definitions of harmful and bias are context-dependent. 4. Societal Alignment Frameworks We present societal alignment frameworks that can provide guidelines for LLM alignment in an incomplete contracting environment. In the following, we discuss the alignment mechanisms of social theory (Section 4.1), economic theory (Section 4.2), and contractual theory (Section 4.3), and explore potential solutions for improving current LLM alignment approaches. 4.1. Social Alignment Human communication relies on complex, largely implicit set of norms, values, and cues that guide individuals in interpreting each others intentions and the world around them (Bicchieri, 2017). However, this process is inherently ambiguous, as much of the meaning is conveyed implicitly rather than explicitly stated. Nonetheless, humans possess unique ability called normative competence, which allows them to understand and judge whether certain behaviors are appropriate or inappropriate in given context (Schutz, 1976). This capability is often ingrained in cultures across the world (Hershcovich et al., 2022; Schäfer et al., 2015; Hanel et al., 2018), shaping shared understanding that facilitates communication and fosters mutual understanding (Mercier & Sperber, 2017). similar challenge arises in user-LLM interactions, where the absence of shared norms and values can result in misaligned outputs. For example, an LLM providing evening activity recommendations without accounting for cultural context might suggest visiting bar or consuming alcohol in region where such activities are prohibited or socially unacceptable, leading to responses that fail to align with local norms. Incorporating societal norms and values into LLMs could equip them with mechanisms to interpret and dynamically adapt to human normative systems (Dragan et al., 2013), much like these aid alignment within human interactions (Bicchieri, 2017). 4.1.1. INSTILLING NORMS AND VALUES While norms constitute context-dependent behavioral rules that individuals follow, values represent broader ideals representing overarching goals and aspirations, shaping what individuals strive for (Matsumoto, 2007). As fundamental tool of cooperative intelligence, language plays crucial role in expressing and reinforcing both norms and values. These can be instilled during LLM alignment in several ways. LLMs, trained on vast datasets, absorb multitude of signals about norms and values during training. However, while some attention has been given to broad ethical principles like helpfulness and harmlessness, an important aspect 4 Societal Alignment Frameworks Can Improve LLM Alignment remains underexplored: contextual rules human norms related to cultural conventions. These contextual rules, while not directly influencing primary optimization objectives, are often followed due to tradition, or social norms. Despite their indirect nature, such rules can provide valuable signals about broader societal dynamics, thereby guiding the alignment of LLMs, as discussed by Hadfield-Menell et al. (2019) and Köster et al. (2020) within the broader context of AI alignment. Although efforts such as Ziems et al. (2022), Zhan et al. (2024), and Chiu et al. (2024) introduce datasets with collections of social norms, the influence of the collected norms on improving alignment in LLMs remains underexplored (Aakanksha et al., 2024). Contextual rules could guide the style of language to align with cultural expectations. For instance, when interacting with users from diverse cultural backgrounds, LLM could account for cultural preferences by avoiding humor that might not translate well across cultures. However, existing models have been shown to predominantly reflect Western values, as they have been primarily trained on Western-centric data, which limits their ability to represent multi-cultural values (Durmus et al., 2024; Nayak et al., 2024). Human social norms and values are continuously shaped and evaluated through daily interactions with others. These interactions involve the exchange of multimodal signals, such as language, facial expressions, and gestures (Levinson & Holler, 2014). However, when interacting with LLMs, these cues are inherently absent, creating normative gap in communication. Exploring multimodality for alignment integrating non-verbal forms of communication such as visual, or auditory signals can serve as promising line of research to address this normative void. By incorporating multimodal interactions, models could better align with the implicit social expectations typically conveyed through non-verbal cues. 4.1.2. ALLOWING FOR DYNAMIC NORMS AND VALUES Norms and values are not static objects but dynamic equilibria that evolve through ongoing social interactions (Morsky & Akçay, 2019). They are continuously re-articulated and negotiated within social contexts, evolving to address new challenges and cultural shifts (Gelfand et al., 2024). Stereotypes, as form of social norm, are accordingly fluid, emerging and transforming over time. An example is the shifting perception of remote work. Once seen as unprofessional or less productive, it is now widely accepted in many industries. If an LLM were trained primarily on pre-COVID data, it could reinforce outdated assumptions. While model editing and continual learning have been extensively explored for updating factual knowledge in LLMs (Mitchell et al., 2022; Benavides-Prado & Riddle, 2022), their application for adapting to evolving societal values and norms remains underexplored. Developing approaches to enable LLMs to dynamically identify, adapt to, and mitigate emerging biases dynamically is crucial area for future research. Notably, even factual updates pose significant challenges, as highlighted by recent work on knowledge editing (Cooper et al., 2024; Hase et al., 2024). 4.2. Economic Alignment Economic systems rely on specialization and the division of labor, requiring coordination among groups of people to ensure efficient allocation of resources (Arrow, 1951). central challenge in modern economic theory is aligning individual actors interests with collective objectives (Hadfield-Menell & Hadfield, 2019). Welfare economics provides complementary perspective by formalizing optimization functions for resource allocation to maximize overall system objectives under given constraints. Similarly, aligning LLMs with diverse human values involves navigating trade-offs between individual and collective goals. Additionally, coherent social welfare objective function for LLMs cannot rely solely on subjective values. Instead, realworld implementations demand collective decisions about which values to prioritize (Arrow, 1951; Sen, 1985). Building from this, we explore strategies for integrating economic alignment frameworks to coordinate individual preferences to achieve collective, fair objectives, and facilitating grouplevel aggregation, offering an alternative view to imposing monolithic objective functions across diverse user groups. 4.2.1. ECONOMIC MECHANISMS FOR FAIR ALIGNMENT In theoretical economics, perfect markets are often posited as achieving Pareto-efficient distribution of welfare under utilitarian framework (Arrow, 1951). Pareto efficiency refers to state, where no individual can be made better off without making someone worse off, and is benchmark for efficient resource allocation (Black et al., 2017). As shown in Boldi et al. (2024), Pareto efficiency offers valuable lens for balancing competing human preferences and can serve as foundation for techniques optimizing specific notions of group fairness, ensuring inclusive and equitable LLM alignment. Achieving such efficiency would mean tailoring the models behavior to address diverse needs equitably, ensuring no group is disproportionately advantaged or disadvantaged without justification. This problem has been investigated in the field of social welfare economics, where the aggregation of diverse preferences must be balanced to ensure the collective well-being of multiple groups (dAspremont & Gevers, 2002). For LLM alignment, these objective functions can guide the development of reward systems. As shown in general RLHF, developing welfarecentric objectives can improve fairness objectives (Pardeshi et al., 2024; Cousins et al., 2024). 5 Societal Alignment Frameworks Can Improve LLM Alignment 4.2.2. ECONOMIC MECHANISMS FOR PLURALISTIC"
        },
        {
            "title": "ALIGNMENT",
            "content": "Decision-making often involves multiple actors with diverse and sometimes conflicting preferences. In the context of LLMs, this necessitates approaches that account for broad range of values. Pluralistic alignment addresses this challenge by designing models that can represent and respect diverse perspectives (Sorensen et al., 2024; Tanmay et al., 2023). Unlike monolithic approaches, which attempt to impose singular objective function, pluralistic alignment embraces the complexity of modern societies. critical aspect of LLM alignment involves determining how to elicit and aggregate preferences when multiple humans are affected by the behavior of an artificial agent (Rossi et al., 2011; Rao et al., 2023; Conitzer et al., 2024). This challenge extends beyond individual alignment to group alignment, where many societal issues arise from collective behavior rather than isolated actions and can be addressed by incorporating multiple objectives into the alignment process, leveraging methods such as few-shot learning to capture diverse perspectives (Zhou et al., 2024; Zhao et al., 2024). Another critical issue in enabling pluralistic values is the trade-off between developing general-purpose models and specialized models. While specialized models tailored to specific domains, such as healthcare or justice, can better align with local norms and regulatory frameworks, they risk fragmenting values. Conversely, general-purpose models may provide broader applicability but struggle to adapt to ethically complex, domain-specific requirements. Cooperative game theory offers framework to navigate these tensions by promoting fair resource allocation, fostering collaboration among stakeholders, and ensuring equitable outcomes (Chalkiadakis et al., 2011). 4.3. Contractual Alignment Law-making and legal interpretation serve as mechanisms to translate opaque human goals and values into explicit, actionable directives. Legal scholars have long recognized the inherent impossibility of drafting complete contracts (Macneil, 1977; Williamson, 1973; Shavell, 1980; Maskin & Tirole, 1999; Tirole, 1999; Aghion & Holden, 2011). This limitation stems from several key challenges. First, certain states of the world are either unobservable or unverifiable, e.g., hiding assets in complex financial arrangements can be difficult for tax authorities to identify (Sears, 1921). Second, the limited rationality of humans restricts their ability to anticipate and optimize across the entire, combinatorially large space of potential scenarios (Williamson, 1973). Consequently, precisely computing optimal outcomes becomes intractable. Furthermore, the very description of all possible contingencies is often beyond human foresight, leading to loopholes in the design of rules (Katz, 2010). Even if feasible, the costs associated with drafting and enforcing fully specified contracts would likely be prohibitive. Given that these challenges are analogous to those encountered in aligning LLMs, where developers aim to ensure that models produce safe and correct outputs even for inputs not directly represented in training or alignment data, we investigate insights from contract theory as potential solutions for improving LLM alignment. 4.3.1. EXTERNAL CONTRACTUAL ALIGNMENT The formalization of contracts offers framework for anticipating and specifying desired behaviors in human-LLM interactions (Jacovi et al., 2021). In this context, standardized documentation plays crucial role in defining and communicating the LLMs performance characteristics. Initiatives such as datasheets (Gebru et al., 2021), data statements (Bender & Friedman, 2018), model cards (Mitchell et al., 2019), reproducibility checklists (Pineau, 2020), fairness checklists (Madaio et al., 2020), and factsheets (Arnold et al., 2019) exemplify efforts to create clear, standardized guidelines that could inform the development of future regulations and legal frameworks for LLM alignment and data governance. The rules that guide LLM alignment are currently largely constructed in consultation with domain and legal experts, by adapting documents such as the UN Declaration of Human Rights (Anthropic, 2023a), through public input (Anthropic, 2023b), or in some cases, relying on designer instincts (Anthropic, 2023a; Solaiman & Dennison, 2021). Importantly, the European Commission has developed detailed guidelines for trustworthy AI, which provide structured approach to ensuring that AI systems, including LLMs, adhere to ethical principles and societal norms.3 These documents serve as critical tools for defining the terms of human-LLM contracts and offer principled way to ensure that the view not only reflects the developers personal views. 4.3.2. INTERNAL CONTRACTUAL ALIGNMENT While the above discussion focused on aligning LLMs through external rules, another approach takes inspiration from how parties in contract, laws, and democratic institutions enforce principles. Instead of relying solely on external oversight, this approach embeds normative principles directly within the models internal mechanisms. Known as constitutional AI, this method enables LLMs to develop an internalized set of principles that guide the model to self-critique and re-write the response to ensure alignment with predefined norms. By integrating desired rules into the training objectives, constitutional 3The guidelines are available at https://ec.europa.eu/d igital-single-market/en/news/ethics-guidelines-tru stworthy-ai/. 6 Societal Alignment Frameworks Can Improve LLM Alignment AI aims to instill structural governance within models, much like how legal frameworks encode societal values into enforceable policies. These methods provide scalable oversight precisely because they move beyond the need for direct, case-by-case human intervention. Traditional preference-based training methods, such as collecting annotations on preferred and rejected outputs, aggregate multiple annotators judgments into shared standard, but they still require extensive human effort at scale (Shen et al., 2023; Amodei et al., 2016). In contrast, scalable oversight techniques generalize beyond individual preferences by structuring decision-making mechanisms, similar to how democratic systems use institutionalized processes to apply laws across diverse contexts (Shen et al., 2023). One such method, debate (Irving et al., 2018; Irving & Askell, 2019), mirrors adversarial legal reasoning: agents (i.e., LLMs) propose answers, engage in structured argumentation, and refine their positions, with human judge selecting the best-supported response (Hafner, 2001). Similarly, constitutional AI guides LLMs using concise constitution of high-level principles (e.g., promoting fairness or avoiding harm) (Bai et al., 2022b; Sun et al., 2023). This constitution provides the basis for generating synthetic comparison examples, which are then used to fine-tune the LLMs policy. While primarily developed for integrating human values, these methods have the potential to enforce norms and regulations in structured manner, drawing parallels to how societal governance mechanisms uphold laws and ethical standards. 5. Societal Alignment Frameworks and their"
        },
        {
            "title": "View on Uncertainty",
            "content": "By framing LLM alignment as problem of contractual incompleteness and analyzing it through the lens of societal alignment frameworks, we observe that these frameworks recognize establishing contracts, much like alignment, as inherently uncertain (Seita, 1984). In the following, we examine uncertainty in the specific case of LLM alignment through the lens of societal alignment frameworks. First, we analyze the sources of unwanted uncertainty in LLM alignment (Section 5.1). Next, we explore types of uncertainty that are essential for ethical alignment (Section 5.2). Finally, we highlight the need for reliable uncertainty communication in LLM alignment (Section 5.3). epistemic uncertainty, often failing to recognize their own knowledge limitations. For example, Shorinwa et al. (2024) illustrate how an LLM confidently responded to the question, What is the lowest-ever temperature recorded in Antarctica? with incorrect information (128.6F/89.2C instead of the actual record of 135.8F/94.7C), claiming 100% confidence despite factual inaccuracy. This inability to calibrate confidence scores to actual knowledge reflects fundamental limitation of current LLM architectures. However, uncertainty in aligned LLMs presents additional complexity. The conversational nature of LLMs often creates an illusion of omniscience, making it difficult for users to discern the models uncertainty (Delacroix, 2024). Furthermore, human interaction with models, combined with their in-context learning capabilities (Brown et al., 2020), allows users to provide task-specific context that can inadvertently bypass safety guardrails and mitigations implemented during training. As highlighted by Glukhov et al. (2024), this can lead to models leaking unsafe information or performing harmful actions despite their intended safeguards. 5.2. Uncertainty Needed in LLM Alignment While the unwanted epistemic uncertainty can undermine the reliability of language models, certain types of uncertainty are not only unavoidable but essential for their ethical deployment (Delacroix, 2024). In the context of LLMs, this essential uncertainty can arise from evolving human values, conflicting societal norms, and the difficulty of translating abstract principles into model behavior. Aligning models to navigate trade-offs, such as between helpfulness and harmlessness or accuracy and fairness, requires addressing conflicting and often underspecified priorities, which introduces another source of uncertainty (Zollo et al., 2024; Yaghini et al., 2023). For instance, when deploying an LLM, we often want to maximize performance subject to some constraints or guardrails on behavior, e.g., chatbot should give users their desired output, as long as it is not too toxic. The effectiveness of balancing these conflicting priorities and the unintended consequences are often difficult to predict. However, this balancing act is also essential because it allows models to operate within complex, context-dependent environments where rigid adherence to single objective could lead to harmful outcomes. 5.3. Uncertainty Communication 5.1. Unwanted Uncertainty in LLM Alignment Prior research has identified epistemic uncertainty as one of the main challenges in LLM development (Shorinwa et al., 2024). This form of uncertainty arises from gaps in the models knowledge, leading to uncertainty about factual information (Shorinwa et al., 2024; Jiang et al., 2021; Yadkori et al., 2024). Even aligned models remain susceptible to Building on the above, the inherent uncertainty in LLM alignment is not weakness but often valuable feature that enables models to handle complex scenarios ethically (Delacroix, 2024). In fact, as highlighted by Bhatt et al. (2021), uncertainty communication can be useful for obtaining fairer models by revealing data biases, improving decision-making by guiding reliance on predictions, and 7 Societal Alignment Frameworks Can Improve LLM Alignment building trust in automated systems. Therefore, it is essential to develop methods for communicating uncertainty to users. Unlike humans, however, LLMs lack the non-verbal and contextual cues that naturally support communication (Bisconti, 2021). Existing research has shown that LLMs struggle to convey their uncertainty to users, both implicitly (e.g., hedging language) and explicitly (e.g., confidence scores), skill that humans possess intuitively (Alkaissi & McFarlane, 2023; Liu et al., 2024; Shorinwa et al., 2024). On the other hand, humans themselves have varying levels of understanding regarding probability and statistics, which are needed to interpret model uncertainty estimates (Bhatt et al., 2021; Galesic & Garcia-Retamero, 2010). Furthermore, human cognition is subject to biases that can impede accurate interpretation of uncertainty (Kahneman, 2011; Reyna & Brainerd, 2008). These challenges can be partially addressed by choosing the appropriate communication methods, key consideration for the design of effective user interfaces (Hullman et al., 2019), and by designing collaborative interaction environments, as discussed by Montemayor (2021). 6. Alternative View: The Democratic Opportunity Inherent in the Under-specified Nature of LLMs Objectives The challenge of aligning LLMs is often framed as technical problem, one that can be solved through better reward modeling, training objectives, or oversight mechanisms. However, alignment is not merely technological issue. It is fundamentally societal one. To understand the significance of this alternative view, one needs to take step back and start from the following: we humans are constantly in the process of finding our way around the world. Part of that process involves imagining better ways of living together. We may find some of our practices to be inadequate, for instance, but may not always be able to articulate why. In such cases, we often resort to conversations, to refine our intuitions and distill their underlying structures. These evolving dialogues shape and refine our moral and social expectations, which, in turn, influence the values that guide our decision-making. The fact that these values change and often clash, is good sign sign of ongoing critical engagement and willingness to question existing norms. Now consider team of engineers considering how to design AI tools that will be deployed within contexts such as education, healthcare, or justice practices. Some of these tools, like LLMs, can be used as conversational partners. The feedback given as context can be leveraged to refine LLMs behavior. Given the inherently dynamic nature of the values that inform education, healthcare, or justice practices, as we previously discussed, the key problem is to establish how to structure this feedback process. Different groups of users will evolve different values over time. Are there ways of incentivizing collective, critical engagement with LLMs? Can bottom-up, iterative refinements be configured to support users in defining the very values that preside over their practices (Delacroix, 2024)? The contrast between the above and characterization of the AI alignment problem as fundamentally challenge of incomplete contracting is significant. The contract metaphor, as discussed by Goldoni & Wilkinson (2018), oversimplifies complex systems by framing alignment as straightforward agreement between stakeholders, neglecting the broader socio-political forces, conflicting norms, and inherent tensions that shape such systems. This technology-centric framing risks oversimplifying the dynamic and pluralistic nature of alignment challenges. While societal alignment frameworks aim to address these issues, they too often rely on oversimplified assumptions. Beyond the issues with the contract metaphor, the focus on incompleteness (i.e., information asymmetries between the principal/agent) frames alignment as an epistemic designer-centric issue, rather than recognizing it first and foremost as political question (Terzis, 2024). Given LLMs unavoidable, normative effect on the practices within which they are deployed, the under-specified nature of LLMs objectives presents an opportunity not to perfect our specification methods, but to democratize the very process of determining what LLMs should optimize for. The implications of this reframing extend to both research and practice. It suggests that alongside technical work such as reward modeling, we need equally sophisticated work on participatory interface designs. This dual focus acknowledges that effective participation requires not just theoretical frameworks for inclusion, but also concrete mechanisms through which diverse stakeholders can meaningfully shape LLM development (Kirk et al., 2024). This might include developing new methodologies for collective value articulation (Bergman et al., 2024), creating institutional structures for meaningful public participation in LLM development, and establishing mechanisms for ongoing societal oversight and input into LLMs objectives and constraints. 7. Conclusions In this paper, we have argued that addressing these challenges requires reframing LLM alignment through the lens of contract theory. We model LLM alignment within principal-agent framework, where the principal (a user or developer) defines contract. This contract consists of an action taken by the agent (the LLM) and the corresponding reward assigned by the principal. We then draw connections between the challenges of contract formation in societal alignment frameworks and those in LLM alignSocietal Alignment Frameworks Can Improve LLM Alignment ment, arguing that insights from societal alignment can improve LLM alignment within incomplete contracting environments. While contract theory provides us with some formalization tools, social alignment emphasizes the role of instillation of societal norms and values, economic alignment points to solutions to group alignment and allocation challenges, and contractual alignment provides mechanisms for regulating LLM behavior, both externally through legal frameworks and internally through scalable oversight mechanisms. Finally, we present an alternative view on LLM alignment, advocating for shifting the paradigm from developer-centered to collaborative, user-centric, and iterative approaches to LLM alignment."
        },
        {
            "title": "Impact Statement",
            "content": "This paper highlights the importance of further research and collaboration among experts in law, economics, social sciences, and LLM developers. We believe the approach proposed in this paper would result in systems that better align with societal values by incorporating diverse perspectives into their design and oversight. In this sense, it holds positive societal impacts. To underscore our position that LLM alignment decisions should not be made exclusively by system designers, we discuss in Section 6 the necessity of examining who is responsible for these decisions and exploring approaches to create more participatory alignment frameworks."
        },
        {
            "title": "Acknowledgements",
            "content": "This paper originated from the Bellairs Invitational Workshop on Contemporary, Foreseeable, and Catastrophic Risks of Large Language Models in April 2024. We thank all workshop participants for their valuable discussions and contributions."
        },
        {
            "title": "References",
            "content": "Aakanksha, Ahmadian, A., Ermis, B., Goldfarb-Tarrant, S., Kreutzer, J., Fadaee, M., and Hooker, S. The multilingual alignment prism: Aligning global and local preferences In Al-Onaizan, Y., Bansal, M., and to reduce harm. Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1202712049, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.186 53/v1/2024.emnlp-main.671. URL https://aclantho logy.org/2024.emnlp-main.671/. Aghion, P. and Holden, R. Incomplete contracts and the theory of the firm: What have we learned over the past Journal of Economic Perspectives, 25(2): 25 years? 18197, June 2011. doi: 10.1257/jep.25.2.181. URL https://www.aeaweb.org/articles?id=10.1257/j ep.25.2.181. Alkaissi, H. and McFarlane, S. I. Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus, 15, 2023. URL https://pmc.ncbi.nlm.nih.gov/artic les/PMC9939079/. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. Concrete problems in AI safety, 2016. URL https://arxiv.org/abs/1606.06565. Anthropic. Claudes constitution. https://www.anthropi c.com/index/claudes-constitution, May 9 2023a. Accessed: 2025-01-27. Anthropic. Collective constitutional AI: Aligning language model with public input. https://www.anthropi c.com/index/collective-constitutional-ai-a ligning-a-language-model-with-public-input, October 17 2023b. Accessed: 2025-01-27. Arnold, M., Bellamy, R. K. E., Hind, M., Houde, S., Mehta, S., Mojsilovic, A., Nair, R., Ramamurthy, K. N., Olteanu, A., Piorkowski, D., Reimer, D., Richards, J., Tsay, J., and Varshney, K. R. FactSheets: Increasing trust in AI services through suppliers declarations of conformity. IBM Journal of Research and Development, 63(4/5):6:1 6:13, 2019. doi: 10.1147/JRD.2019.2942288. Arrow, K. J. An extension of the basic theorems of classical welfare economics. In Proceedings of the second Berkeley symposium on mathematical statistics and probability, volume 2, pp. 507533. University of California Press, 1951. URL https://projecteuclid.org/ebooks/b erkeley-symposium-on-mathematical-statistic s-and-probability/Proceedings-of-the-Secon d-Berkeley-Symposium-on-Mathematical-Stati stics-and/chapter/An-Extension-of-the-Basic -Theorems-of-Classical-Welfare-Economics/bsm sp/1200500251. Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. general language assistant as laboratory for alignment, 2021. URL https://arxiv.org/abs/2112.00861. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Dassarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. Training Societal Alignment Frameworks Can Improve LLM Alignment helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022a. URL https://arxiv.org/abs/2204.05862. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lukosuite, K., Lovitt, L., Sellitto, M., Elhage, N., Schiefer, N., Mercado, N., DasSarma, N., Lasenby, R., Larson, R., Ringer, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Lanham, T., Telleen-Lawton, T., Conerly, T., Henighan, T., Hume, T., Bowman, S. R., Hatfield-Dodds, Z., Mann, B., Amodei, D., Joseph, N., McCandlish, S., Brown, T., and Kaplan, J. Constitutional AI: Harmlessness from AI feedback, 2022b. URL https://arxiv.org/abs/2212.08073. Benavides-Prado, D. and Riddle, P. theory for knowledge transfer in continual learning. In Chandar, S., Pascanu, R., and Precup, D. (eds.), Proceedings of The 1st Conference on Lifelong Learning Agents, volume 199 of Proceedings of Machine Learning Research, pp. 647660. PMLR, 22 24 Aug 2022. URL https://proceedings.mlr.pres s/v199/prado22a.html. Bender, E. M. and Friedman, B. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587604, 12 2018. ISSN 2307-387X. doi: 10.1162/tacl_a_00041. URL https://doi.org/10.1162/tacl_a_00041. Bergman, S., Marchal, N., Mellor, J., Mohamed, S., Gabriel, I., and Isaac, W. STELA: community-centred approach to norm elicitation for AI alignment. Scientific Reports, 14:6616, 2024. doi: 10.1038/s41598-024-56648-4. URL https://doi.org/10.1038/s41598-024-56648-4. Bhatt, U., Antorán, J., Zhang, Y., Liao, Q. V., Sattigeri, P., Fogliato, R., Melançon, G., Krishnan, R., Stanley, J., Tickoo, O., Nachman, L., Chunara, R., Srikumar, M., Weller, A., and Xiang, A. Uncertainty as form of transparency: Measuring, communicating, and using uncertainty. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES 21, pp. 401413, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384735. doi: 10.1145/3461702.3462571. URL https://doi.or g/10.1145/3461702.3462571. Bicchieri, C. Norms in the Wild: How to Diagnose, Measure, and Change Social Norms. Oxford University Press, 02 2017. ISBN 9780190622046. doi: 10.1093/acprof: oso/9780190622046.001.0001. URL https://doi.or g/10.1093/acprof:oso/9780190622046.001.0001. Bisconti, P. How robots unintentional metacommunication affects humanrobot interactions: systemic approach. Minds & Machines, 31:487504, 2021. doi: 10.1007/s1 1023-021-09584-5. URL https://doi.org/10.1007/ s11023-021-09584-5. Black, J. D., Hashimzade, N., and Myles, G. (eds.). Dictionary of Economics. Oxford University Press, Oxford, 5th edition, 2017. URL https://books.google.ca/ books?id=WyvYDQAAQBAJ&pg=PT459&redir_esc=y#v =onepage&q&f=false. Boldi, R., Ding, L., Spector, L., and Niekum, S. Paretooptimal learning from preferences with hidden context, 2024. URL https://arxiv.org/abs/2406.15599. Bolton, P. and Dewatripont, M. Contract Theory, volume 1 of MIT Press Books. The MIT Press, December 2005. ISBN ARRAY(0x6d98ece8). URL https://ideas.re pec.org/b/mtp/titles/0262025760.html. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. ISSN 00063444, 14643510. URL http://www.jstor.org/stable/233 4029. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/p aper/2020/file/1457c0d6bfcb4967418bfb8ac142f 64a-Paper.pdf. Chalkiadakis, G., Elkind, E., and Wooldridge, M. Computational Aspects of Cooperative Game Theory (Synthesis Lectures on Artificial Inetlligence and Machine Learning). Morgan & Claypool Publishers, 1st edition, 2011. ISBN 1608456528. URL https://link.springer.co m/book/10.1007/978-3-031-01558-8. Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G. J., and Wong, E. Jailbreaking black box large language models in twenty queries, 2024. URL https://arxiv. org/abs/2310.08419. Chiu, Y. Y., Jiang, L., Antoniak, M., Park, C. Y., Li, S. S., Bhatia, M., Ravi, S., Tsvetkov, Y., Shwartz, V., and Choi, Y. CulturalTeaming: AI-assisted interactive red-teaming Societal Alignment Frameworks Can Improve LLM Alignment for challenging LLMs(lack of) multicultural knowledge. arXiv preprint arXiv:2404.06664, 2024. 1532-4435. URL https://dl.acm.org/doi/pdf/10. 5555/3586589.3586815. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/p aper/2017/file/d5e2c0adad503c91f91df240d0cd4 e49-Paper.pdf. Conitzer, V., Freedman, R., Heitzig, J., Holliday, W. H., Jacobs, B. M., Lambert, N., Mosse, M., Pacuit, E., Russell, S., Schoelkopf, H., Tewolde, E., and Zwicker, W. S. Position: Social choice should guide AI alignment in dealing with diverse human feedback. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 93469360. PMLR, 2127 Jul 2024. URL https://pr oceedings.mlr.press/v235/conitzer24a.html. Cooper, A. F., Choquette-Choo, C. A., Bogen, M., Jagielski, M., Filippova, K., Liu, K., Chouldechova, A., Hayes, J., Huang, Y., Mireshghallah, N., Shumailov, I., Triantafillou, E., Kairouz, P., Mitchell, N., Liang, P., Ho, D. E., Choi, Y., Koyejo, S., Delgado, F., Grimmelmann, J., Shmatikov, V., De Sa, C., Barocas, S., Cyphert, A., Lemley, M. A., Boyd, D., Wortman Vaughan, J., Brundage, M., Bau, D., Neel, S., Jacobs, A., Terzis, A., Wallach, H., Papernot, N., and Lee, K. Machine unlearning doesnt do what you think: Lessons for generative AI policy, research, and practice. SSRN, December 2024. doi: 10.2139/ssrn.5060253. URL https://ssrn.com/abstract=5060253. Cousins, C., Asadi, K., Lobo, E., and Littman, M. On welfare-centric fair reinforcement learning. Reinforcement Learning Journal, 3:11241137, 2024. URL https: //rlj.cs.umass.edu/2024/papers/Paper133.ht ml. DAmour, A., Heller, K., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein, J., Hoffman, M. D., Hormozdiari, F., Houlsby, N., Hou, S., Jerfel, G., Karthikesalingam, A., Lucic, M., Ma, Y., McLean, C., Mincu, D., Mitani, A., Montanari, A., Nado, Z., Natarajan, V., Nielson, C., Osborne, T. F., Raman, R., Ramasamy, K., Sayres, R., Schrouff, J., Seneviratne, M., Sequeira, S., Suresh, H., Veitch, V., Vladymyrov, M., Wang, X., Webster, K., Yadlowsky, S., Yun, T., Zhai, X., and Sculley, D. Underspecification presents challenges for credibility in modern machine learning. Journal of Machine Learning Research, 23(1), January 2022. ISSN dAspremont, C. and Gevers, L. Social welfare functionals and interpersonal comparability. In Handbook of Social Choice and Welfare, volume 1 of Handbook of Social Choice and Welfare, pp. 459541. Elsevier, 2002. doi: https://doi.org/10.1016/S1574-0110(02)80014-5. URL https://www.sciencedirect.com/science/articl e/pii/S1574011002800145. Delacroix, S. Lost in conversation? Hermeneutics, uncertainty and large language models. SSRN, April 21 2024. URL https://ssrn.com/abstract=4751774. Dragan, A. D., Lee, K. C., and Srinivasa, S. S. Legibility and predictability of robot motion. In Proceedings of the 8th ACM/IEEE International Conference on HumanRobot Interaction, HRI 13, pp. 301308. IEEE Press, 2013. ISBN 9781467330558. URL https://ieeexplo re.ieee.org/document/6483603. Durmus, E., Nguyen, K., Liao, T., Schiefer, N., Askell, A., Bakhtin, A., Chen, C., Hatfield-Dodds, Z., Hernandez, D., Joseph, N., Lovitt, L., McCandlish, S., Sikder, O., Tamkin, A., Thamkul, J., Kaplan, J., Clark, J., and Ganguli, D. Towards measuring the representation of subjective global opinions in language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=zl16jLb91v. Echenique, F., Immorlica, N., Vazirani, V., and Roth, A. Contract Theory, pp. 614624. Cambridge University Press, 2023. ISBN 9781108831994. URL https://bo oks.google.ca/books?id=1ea-EAAAQBAJ. Eisenhardt, K. M. Agency theory: An assessment and review. The Academy of Management Review, 14(1):57 74, 1989. ISSN 03637425. URL http://www.jstor. org/stable/258191. Feffer, M., Heidari, H., and Lipton, Z. C. Moral machine or tyranny of the majority?, 2023. URL https://arxi v.org/abs/2305.17319. Gabriel, I. Artificial intelligence, values, and alignment. Minds & Machines, 30(3):411437, 2020. doi: 10.1007/ s11023-020-09539-2. URL https://doi.org/10.100 7/s11023-020-09539-2. Galesic, M. and Garcia-Retamero, R. Statistical numeracy for health: cross-cultural comparison with probabilistic national samples. Archives of Internal Medicine, 170(5): 462468, 03 2010. ISSN 0003-9926. doi: 10.1001/arch internmed.2009.481. URL https://doi.org/10.100 1/archinternmed.2009.481. Societal Alignment Frameworks Can Improve LLM Alignment Garen, J. E. Executive compensation and principal-agent theory. Journal of Political Economy, 102(6):11751199, 1994. ISSN 00223808, 1537534X. URL http://www. jstor.org/stable/2138783. Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., III, H. D., and Crawford, K. Datasheets for datasets. Commun. ACM, 64(12):8692, November 2021. ISSN 0001-0782. doi: 10.1145/3458723. URL https://doi.org/10.1145/3458723. Gelfand, M. J., Gavrilets, S., and Nunn, N. Norm dynamics: Interdisciplinary perspectives on social norm emergence, persistence, and change. Annual Review of Psychology, 75(Volume 75, 2024):341378, 2024. ISSN 1545-2085. doi: https://doi.org/10.1146/annurev-psych-033020-0 13319. URL https://www.annualreviews.org/cont ent/journals/10.1146/annurev-psych-033020-0 13319. Glukhov, D., Han, Z., Shumailov, I., Papyan, V., and Papernot, N. Breach by thousand leaks: Unsafe information leakage in safe AI responses, 2024. URL https://arxiv.org/abs/2407.02551. Goldoni, M. and Wilkinson, M. A. The material constitution. The Modern Law Review, 81(4):567597, 2018. ISSN 00267961, 14682230. URL http://www.jstor.org/ stable/26647134. Greenblatt, R., Denison, C., Wright, B., Roger, F., MacDiarmid, M., Marks, S., Treutlein, J., Belonax, T., Chen, J., Duvenaud, D., Khan, A., Michael, J., Mindermann, S., Perez, E., Petrini, L., Uesato, J., Kaplan, J., Shlegeris, B., Bowman, S. R., and Hubinger, E. Alignment faking in large language models, 2024. URL https: //arxiv.org/abs/2412.14093. Hadfield-Menell, D. and Hadfield, G. K. Incomplete In Proceedings of the contracting and AI alignment. 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 19, pp. 417422, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450363242. doi: 10.1145/3306618.3314250. URL https://doi.org/10.1145/3306618.3314250. Hadfield-Menell, D., Milli, S., Abbeel, P., Russell, S. J., and Dragan, A. Inverse reward design. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/pap er_files/paper/2017/file/32fdab6559cdfa4f167 f8c31b9199643-Paper.pdf. Hadfield-Menell, D., Andrus, M., and Hadfield, G. Legible normativity for AI alignment: The value of silly rules. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 19, pp. 115121, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450363242. doi: 10.1145/3306618.3314258. URL https://doi.org/10.1145/3306618.3314258. Hafner, C. Legal reasoning models. In Smelser, N. J. and Baltes, P. B. (eds.), International Encyclopedia of the Social & Behavioral Sciences, pp. 86758677. Pergamon, Oxford, 2001. ISBN 978-0-08-043076-8. doi: https: //doi.org/10.1016/B0-08-043076-7/00586-6. URL https://www.sciencedirect.com/science/articl e/pii/B0080430767005866. Hanel, P. H. P., Maio, G. R., Soares, A. K. S., Vione, K. C., de Holanda Coelho, G. L., Gouveia, V. V., Patil, A. C., Kamble, S. V., and Manstead, A. S. R. Cross-cultural differences and similarities in human value instantiation. Frontiers in Psychology, 9, 2018. ISSN 1664-1078. doi: 10.3389/fpsyg.2018.00849. URL https://www.fronti ersin.org/journals/psychology/articles/10.33 89/fpsyg.2018.00849. Hase, P., Hofweber, T., Zhou, X., Stengel-Eskin, E., and Bansal, M. Fundamental problems with model editing: How should rational belief revision work in LLMs? Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?i d=LRf19n5Ly3. Hershcovich, D., Frank, S., Lent, H., de Lhoneux, M., Abdou, M., Brandl, S., Bugliarello, E., Cabello Piqueras, L., Chalkidis, I., Cui, R., Fierro, C., Margatina, K., Rust, P., and Søgaard, A. Challenges and strategies in cross-cultural NLP. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 69977013, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.482. URL https://aclanthology.org/2022.acl-long.482/. Hullman, J., Qiao, X., Correll, M., Kale, A., and Kay, M. In pursuit of error: survey of uncertainty viIEEE Transactions on Visualsualization evaluation. ization and Computer Graphics, 25(1):903913, 2019. doi: 10.1109/TVCG.2018.2864889. URL https: //ieeexplore.ieee.org/document/8457476. Ilvento, C. Metric Learning for Individual Fairness. In Roth, A. (ed.), 1st Symposium on Foundations of Responsible Computing (FORC 2020), volume 156 of Leibniz International Proceedings in Informatics (LIPIcs), pp. 2:12:11, Dagstuhl, Germany, 2020. Schloss Dagstuhl Leibniz-Zentrum für Informatik. ISBN 978-3-95977142-9. doi: 10.4230/LIPIcs.FORC.2020.2. URL 12 Societal Alignment Frameworks Can Improve LLM Alignment https://drops.dagstuhl.de/entities/docum ent/10.4230/LIPIcs.FORC.2020.2. Irving, G. and Askell, A. AI safety needs social scientists. Distill, 2019. doi: 10.23915/distill.00021. URL https: //distill.pub/2019/safety-needs-social-sci entists/. Irving, G., Christiano, P., and Amodei, D. AI safety via debate, 2018. URL https://arxiv.org/abs/1805.0 0899. Jacovi, A., Marasovic, A., Miller, T., and Goldberg, Y. Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in AI. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, pp. 624635, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445923. URL https://doi.org/10.1145/3442188.3445923. Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can we know when language models know? On the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962 977, 2021. doi: 10.1162/tacl_a_00407. URL https: //aclanthology.org/2021.tacl-1.57/. Kahneman, D. Thinking, Fast and Slow. Allen Lane, 2011. URL https://books.google.ca/books?id=AV9x8 XakdV0C. Katz, L. theory of loopholes. The Journal of Legal Studies, ISSN 00472530, 15375366. URL 39(1):131, 2010. http://www.jstor.org/stable/10.1086/649046. Kemmer, R., Yoo, Y., Escobedo, A., and Maciejewski, R. Enhancing collective estimates by aggregating cardinal and ordinal inputs. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 8(1):73 82, Oct. 2020. doi: 10.1609/hcomp.v8i1.7465. URL https://ojs.aaai.org/index.php/HCOMP/article /view/7465. Kirk, H. R., Whitefield, A., Röttger, P., Bean, A. M., Margatina, K., Mosquera, R., Ciro, J. M., Bartolo, M., Williams, A., He, H., Vidgen, B., and Hale, S. A. The PRISM alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum ?id=DFr5hteojx. Klingefjord, O., Lowe, R., and Edelman, J. What are human values, and how do we align AI to them?, 2024. URL https://arxiv.org/abs/2404.10636. Köster, R., Hadfield-Menell, D., Hadfield, G. K., and Leibo, J. Z. Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS 20, pp. 18871888, Richland, SC, 2020. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450375184. URL https: //dl.acm.org/doi/10.5555/3398761.3399016. Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. Scalable agent alignment via reward modeling: research direction, 2018. URL https://arxiv.org/ abs/1811.07871. Levinson, S. C. and Holler, J. The origin of human multimodal communication. Philosophical Transactions of the Royal Society B: Biological Sciences, 369(1651): 20130302, 2014. doi: 10.1098/rstb.2013.0302. URL https://royalsocietypublishing.org/doi/abs/1 0.1098/rstb.2013.0302. Liu, Y., Yao, Y., Ton, J.-F., Zhang, X., Guo, R., Cheng, H., Klochkov, Y., Taufiq, M. F., and Li, H. Trustworthy LLMs: survey and guideline for evaluating large language models alignment, 2024. URL https: //arxiv.org/abs/2308.05374. Macneil, I. R. Contracts: Adjustment of long-term economic relations under classical, neoclassical, and relational contract law. Northwestern University Law Review, 72:854, 1977. URL https://heinonline.org/HOL/L andingPage?handle=hein.journals/illlr72&div= 46&id=&page=. Madaio, M. A., Stark, L., Wortman Vaughan, J., and Wallach, H. Co-designing checklists to understand organizational challenges and opportunities around fairness In Proceedings of the 2020 CHI Conference in AI. on Human Factors in Computing Systems, CHI 20, pp. 114, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450367080. doi: 10.1145/3313831.3376445. URL https://doi.or g/10.1145/3313831.3376445. Maskin, E. and Tirole, J. Unforeseen Contingencies and Incomplete Contracts. The Review of Economic Studies, 66(1):83114, 01 1999. ISSN 0034-6527. doi: 10.1111/ 1467-937X.00079. URL https://doi.org/10.1111/ 1467-937X.00079. Matsumoto, D. Culture, context, and behavior. Journal of Personality, 75(6):12851320, 2007. doi: https:// doi.org/10.1111/j.1467-6494.2007.00476.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1 111/j.1467-6494.2007.00476.x. 13 Societal Alignment Frameworks Can Improve LLM Alignment Mercier, H. and Sperber, D. The Enigma of Reason. Harvard University Press, 2017. ISBN 9780674368309. URL http://www.jstor.org/stable/j.ctv2sp3dd8. URL https://proceedings.neurips.cc/paper_f iles/paper/2022/file/b1efde53be364a73914f588 05a001731-Paper-Conference.pdf. Mitchell, E., Lin, C., Bosselut, A., Manning, C. D., and Finn, C. Memory-based model editing at scale. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 15817 15831. PMLR, 1723 Jul 2022. URL https://procee dings.mlr.press/v162/mitchell22a.html. Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D., and Gebru, T. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* 19, pp. 220229, New York, NY, USA, ISBN 2019. Association for Computing Machinery. 9781450361255. doi: 10.1145/3287560.3287596. URL https://doi.org/10.1145/3287560.3287596. Montemayor, C. Language and intelligence. Minds & Machines, 31:471486, 2021. doi: 10.1007/s11023-021 -09568-5. URL https://doi.org/10.1007/s11023 -021-09568-5. Morsky, B. and Akçay, E. Evolution of social norms and correlated equilibria. Proceedings of the National Academy of Sciences, 116(18):88348839, 2019. doi: 10.1073/pnas.1817095116. URL https://www.pnas.o rg/doi/abs/10.1073/pnas.1817095116. Nayak, S., Jain, K., Awal, R., Reddy, S., Steenkiste, S. V., Hendricks, L. A., Stanczak, K., and Agrawal, A. Benchmarking vision language models for cultural In Al-Onaizan, Y., Bansal, M., and understanding. Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 57695790, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlpmain.329. URL https: //aclanthology.org/2024.emnlp-main.329/. OpenAI. Faulty reward functions in the wild. https: //openai.com/index/faulty-reward-functions/, 2016. Accessed: 2025-01-10. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2773027744. Curran Associates, Inc., 2022. Pardeshi, K. S., Shapira, I., Procaccia, A. D., and Singh, A. Learning social welfare functions. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/for um?id=7O6KtaAr8n. Pineau, J. The machine learning reproducibility checklist, 2020. URL https://www.cs.mcgill.ca/jpineau /ReproducibilityChecklist.pdf. Accessed: 202502-18. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/for um?id=HPuSIXJaa9. Rao, A. S., Khandelwal, A., Tanmay, K., Agarwal, U., and Choudhury, M. Ethical reasoning over moral alignment: case and framework for in-context ethical policies in LLMs. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1337013388, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.892. URL https: //aclanthology.org/2023.findings-emnlp.892/. Reyna, V. F. and Brainerd, C. J. Numeracy, ratio bias, and denominator neglect in judgments of risk and probability. Learning and Individual Differences, 18(1):89107, 2008. ISSN 1041-6080. doi: https://doi.org/10.1016/j.lindif.2 007.03.011. URL https://www.sciencedirect.com/ science/article/pii/S1041608007000428. Rossi, F., Venable, K. B., and Walsh, T. Short Introduction to Preferences: Between AI and Social Choice. Morgan & Claypool Publishers, 1st edition, 2011. ISBN 1608455866. URL https://dl.acm.org/doi/10.55 55/2049991. Saito, K., Wachi, A., Wataoka, K., and Akimoto, Y. Verbosity bias in preference labeling by large language modIn NeurIPS 2023 Workshop on Instruction Tunels. ing and Instruction Following, 2023. URL https: //openreview.net/forum?id=magEgFpK1y. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Schutz, A. The Problem of Rationality in the Social World, pp. 6488. Springer Netherlands, Dordrecht, 1976. ISBN 14 Societal Alignment Frameworks Can Improve LLM Alignment 978-94-010-1340-6. doi: 10.1007/978-94-010-1340-6_3. URL https://doi.org/10.1007/978-94-010-134 0-6_3. Schäfer, M., Haun, D. B. M., and Tomasello, M. Fair is not fair everywhere. Psychological Science, 26(8):1252 1260, 2015. doi: 10.1177/0956797615586188. URL https://doi.org/10.1177/0956797615586188. PMID: 26115962. Sears, J. H. Effective and lawful avoidance of taxes. Virginia Law Review, 8(2):7785, 1921. ISSN 00426601. URL http://www.jstor.org/stable/1064452. Seita, A. Y. Uncertainty and contract law. University of Pittsburgh Law Review, 46(75), 1984. URL https://ss rn.com/abstract=1692858. Sen, A. Social choice and justice: review article. Journal of Economic Literature, 23(December), 1985. URL ht tps://scholar.harvard.edu/sen/publications /socialchoiceandjustice- reviewarticle. Review article on K.J. Arrows Collected Papers: Social Choice and Justice. Shavell, S. Damage measures for breach of contract. Bell Journal of Economics, 11(2):466490, 1980. URL http s://EconPapers.repec.org/RePEc:rje:bellje:v: 11:y:1980:i:autumn:p:466-490. Shen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., Wu, X., Liu, Y., and Xiong, D. Large language model alignment: survey, 2023. URL https://arxiv.org/ abs/2309.15025. Shorinwa, O., Mei, Z., Lidard, J., Ren, A. Z., and Majumdar, A. survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions, 2024. URL https://arxiv.org/ abs/2412.05563. Skalse, J., Howe, N. H. R., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward hacking. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088. URL https://proceedings.neur ips.cc/paper_files/paper/2022/hash/3d719fee3 32caa23d5038b8a90e81796-Abstract-Conference. html. Sorensen, T., Moore, J., Fisher, J., Gordon, M. L., Mireshghallah, N., Rytting, C. M., Ye, A., Jiang, L., Lu, X., Dziri, N., Althoff, T., and Choi, Y. Position: roadmap to pluralistic alignment. In Proceedings of the 41st International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=gQ pBnRHwxM. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. URL https://dl.acm.org/doi/abs /10.5555/3495724.3495977. Sun, Z., Shen, Y., Zhou, Q., Zhang, H., Chen, Z., Cox, D., Yang, Y., and Gan, C. Principle-driven self-alignment of language models from scratch with minimal human supervision. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. URL https://proceedings.neurips.cc/paper _files/paper/2023/hash/0764db1151b936aca5924 9e2c1386101-Abstract-Conference.html. Tanmay, K., Khandelwal, A., Agarwal, U., and Choudhury, M. Probing the moral development of large language models through defining issues test, 2023. URL https: //arxiv.org/abs/2309.13356. Terzis, P. Against digital constitutionalism. European Law Open, First View, July 2024. doi: 10.2139/ssrn.4896078. URL https://ssrn.com/abstract=4896078. Tirole, J. Incomplete contracts: Where do we stand? Econometrica, 67(4):741781, 1999. ISSN 00129682, 14680262. URL http://www.jstor.org/stable/299 9457. Turner, A., Smith, L., Shah, R., Critch, A., and Tadepalli, P. Optimal policies tend to seek power. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2306323074. Curran Associates, Inc., 2021. URL https://proceedings.neur ips.cc/paper_files/paper/2021/file/c26820b8a 4c1b3c2aa868d6d57e14a79-Paper.pdf. Solaiman, I. and Dennison, C. Process for adapting language models to society (PALMS) with values-targeted datasets. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393. URL https://dl.acm.org/doi/abs /10.5555/3540261.3540709. Turner, A. M., Hadfield-Menell, D., and Tadepalli, P. Conservative agency via attainable utility preservation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, AIES 20, pp. 385391, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371100. doi: 10.1145/3375627.3375851. URL https://doi.org/10.1145/3375627.3375851. 15 Societal Alignment Frameworks Can Improve LLM Alignment Wei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does LLM safety training fail? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=jA235J GM09. Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1576315773. Curran Associates, Inc., 2020. URL https://procee dings.neurips.cc/paper_files/paper/2020/file /b607ba543ad05417b8507ee86c54fcb7-Paper.pdf. Williams, M., Carroll, M., Narang, A., Weisser, C., Murphy, B., and Dragan, A. On targeted manipulation and deception when optimizing LLMs for user feedback, 2024. URL https://arxiv.org/abs/2411.02306. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/1909.08593. Ziems, C., Yu, J., Wang, Y.-C., Halevy, A., and Yang, D. The moral integrity corpus: benchmark for ethical dialogue systems. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 37553773, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.261. URL https://aclant hology.org/2022.acl-long.261/. Zollo, T. P., Morrill, T., Deng, Z., Snell, J., Pitassi, T., and Zemel, R. Prompt risk control: rigorous framework for responsible deployment of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.n et/forum?id=5tGGWOijvq. Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models, 2023. URL https: //arxiv.org/abs/2307.15043. Williamson, O. Markets and Hierarchies, Analysis and Antitrust Implications: Study in the Economics of Internal Organization. Study in the economics of internal organization. Free Press, 1973. URL http://www.jstor.or g/stable/1817092. Yadkori, Y. A., Kuzborskij, I., György, A., and Szepesvári, C. To believe or not to believe your LLM, 2024. URL https://arxiv.org/abs/2406.02543. Yaghini, M., Liu, P., Boenisch, F., and Papernot, N. Learning to walk impartially on the pareto frontier of fairness, privacy, and utility. In NeurIPS 2023 Workshop on Regulatable ML, 2023. URL https://openreview.net/f orum?id=R5MTSLPyYZ. Zhan, H., Li, Z., Kang, X., Feng, T., Hua, Y., Qu, L., Ying, Y., Chandra, M. R., Rosalin, K., Jureynolds, J., Sharma, S., Qu, S., Luo, L., Zukerman, I., Soon, L.-K., Semnani Azad, Z., and Haf, R. RENOVI: benchmark towards remediating norm violations in socio-cultural conversations. In Duh, K., Gomez, H., and Bethard, S. (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 31043117, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.196. URL https: //aclanthology.org/2024.findings-naacl.196/. Zhao, S., Dang, J., and Grover, A. Group preference optimization: Few-shot alignment of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/f orum?id=DpFeMH4l8Q. Zhou, Z., Liu, J., Shao, J., Yue, X., Yang, C., Ouyang, W., and Qiao, Y. Beyond one-preference-fits-all alignment: Multi-objective direct preference optimization. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1058610613, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.630. URL https: //aclanthology.org/2024.findings-acl.630/. Zhuang, S. and Hadfield-Menell, D. Consequences of misaligned AI. In Larochelle, H., Ranzato, M., Hadsell, R.,"
        }
    ],
    "affiliations": [
        "Anthropic",
        "Columbia University",
        "Fraunhofer AISEC",
        "Google DeepMind",
        "Johns Hopkins University",
        "Kings College London",
        "McGill University",
        "Mila Quebec AI Institute",
        "ServiceNow",
        "University of Cambridge",
        "University of Toronto",
        "University of Utah",
        "Université de Montréal"
    ]
}