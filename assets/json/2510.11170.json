{
    "paper_title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling",
    "authors": [
        "Daniel Scalena",
        "Leonidas Zotos",
        "Elisabetta Fersini",
        "Malvina Nissim",
        "Ahmet Üstün"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt. This enables exploration of different reasoning paths toward the correct solution, however, allocates the same compute budget for each prompt. Grounded on the assumption that different prompts carry different degrees of complexity, and thus different computation needs, we propose EAGer, a training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and concurrently improve overall performance. EAGer allows branching to multiple reasoning paths only in the presence of high-entropy tokens, and then reallocates the saved compute budget to the instances where exploration of alternative paths is most needed. We find that across multiple open-source models on complex reasoning benchmarks such as AIME 2025, EAGer can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, EAGer generates up to 65% fewer tokens (hence saving compute) and achieves up to 37% improvement in Pass@k compared to the Full Parallel Sampling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 0 7 1 1 1 . 0 1 5 2 : r Preprint. Under review. EAGER: ENTROPY-AWARE GENERATION FOR ADAPTIVE INFERENCE-TIME SCALING Daniel Scalena1,2 Leonidas Zotos1 Elisabetta Fersini2 Malvina Nissim1 Ahmet Ust un3,4 2University of Milano - Bicocca 1University of Groningen 3Cohere Labs 4Cohere"
        },
        {
            "title": "ABSTRACT",
            "content": "With the rise of reasoning language models and test-time scaling methods as paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt. This enables exploration of different reasoning paths toward the correct solution, however, allocates the same compute budget for each prompt. Grounded on the assumption that different prompts carry different degrees of complexity, and thus different computation needs, we propose EAGER, training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and concurrently improve overall performance. EAGER allows branching to multiple reasoning paths only in the presence of highentropy tokens, and then reallocates the saved compute budget to the instances where exploration of alternative paths is most needed. We find that across multiple open-source models on complex reasoning benchmarks such as AIME 2025, EAGER can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, EAGER generates up to 65% fewer tokens (hence saving compute) and achieves up to 37% improvement in Pass@k compared to the FULL PARALLEL sampling. Our results show that EAGER consistently maximizes the efficiency-performance trade-off by enabling dynamic control over computation expenditure. Figure 1: Left: We introduce EAGER, generation method that dynamically allocates the perprompt budget during decoding, branching only when high-entropy peaks are detected. For each prompt, the total number of allowed sequences is capped at , and we track the actual budget consumed by our preparatory stage, EAGER-init. The remaining budget is evenly allocated among prompts reaching the cap (EAGER-adapt) or, in case targets labels are accessible, prompts not reaching correct final solution (i.e. with Pass@k = 0; our full EAGER), in contrast to the fixedbudget allocation of FULL PARALLEL sampling. Right: Our approaches (EAGER -init, -adapt and full EAGER) consistently reduce token usage compared to the standard FULL PARALLEL sampling approach when scaling the limit [4, 8, 16, 24, 32]. In addition, EAGER always achieves clear performance advantage over all other decoding methods. 1 Preprint. Under review."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in large language models (LLMs) have led to substantial improvements in complex reasoning tasks, particularly with the adoption of chain-of-thought (CoT) prompting (Wei et al., 2022). Such tasks often admit multiple valid reasoning paths that converge to the same correct solution (Stanovich & West, 2002). Rather than relying on single greedy decoding path, the single generation can be replaced by multiple sampled candidate sequences, thereby producing diverse set of reasoning paths and corresponding final answers (Wang et al., 2023). This strategy has been shown to enhance performance on challenging reasoning problems: by exploring multiple reasoning paths, the model reduces its reliance on the stochasticity of single greedy generation and increases the likelihood of arriving at correct solution. Despite its success, CoTs introduce an inherent computational inefficiency: reasoning sequences tend to be long, and large portion of the tokens generated are predictable continuations rather than genuine decision points (Wang et al., 2025). This inefficiency is amplified in approaches that explore multiple reasoning paths in parallel, where each path independently regenerates identical prefixes before diverging. For prompts with simple problems, many of these paths converge to the same solution with little variation, resulting in redundant computation. For more complex prompts, however, the diversity of reasoning paths becomes crucial, and additional generations may be necessary to discover correct solution (Snell et al., 2025; Muennighoff et al., 2025). This observation suggests that per-problem decision to let or not let the model explore alternative paths would be desirable. We argue that such decision can be guided by monitoring model uncertainty during generation towards an adaptive allocation of computating budget. Intuitively, when the models predictions are confident and stable, only few candidate sequences are needed, while at points of high uncertainty, where multiple reasoning paths are plausible, additional exploration becomes critical. To address these issues, we introduce EAGER, an Entropy-Aware Generation method that monitors token-level uncertainty during decoding to guide where new parallel reasoning traces should start. By branching only at high-entropy tokens, we avoid regenerating identical low-entropy continuations, substantially reducing computation overhead without sacrificing coverage of diverse reasoning traces. Furthermore, reducing the parallel samples for easy prompts, EAGER dynamically allocates the unused sampling budget towards more challenging ones, maximizing the benefits of inference-time scaling for difficult prompts.1 We evaluate EAGER on diverse set of benchmarks, spanning from complex math problems to science-related questions and code generation tasks. All the tested LMs, from the smallest 3B to the biggest 20B parameter model, show performance boost of up to 37% when using EAGER compared to our baseline FULL PARALLEL sampling setting. Our main contributions are as follows: We empirically show that token-wise entropy peaks as form of online (i.e., measured during generation) uncertainty is good proxy that shows when more exploration is needed during the generation, hence reflecting the difficulty of prompt for the model used. We introduce, novel, training-free decoding method that leverages entropy distribution during generation to dynamically reduce compute cost while maintaining the benefits of inference-time scaling. EAGER generates up to 65% fewer tokens and saves up to 80% of the entire generation budget across all our benchmarks and models. To maximize the benefits of inference-time scaling, we show that EAGER enables adaptive use of the given sampling budget, where it spends more compute on the hard problems. This yields up to 13% improvements in test-time settings without access to target labels, and up to 37% when labels are available, on the AIME 2025 reasoning benchmark."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In the inference-time scaling paradigm, language model generates multiple parallel sequences so that it can explore various reasoning paths to find correct solution (Welleck et al., 2024; Snell 1Code and data: https://github.com/DanielSc4/EAGer . 2 Preprint. Under review. et al., 2025). This is oftentimes facilitated by sampling completions from the model with relatively high temperature using methods such as nucleus sampling (Holtzman et al., 2020). We refer to this standard approach as the FULL PARALLEL sampling generation procedure. This approach is useful in various settings, including for the generation of diverse solutions to problem, or in large-scale reinforcement learning (RL) pipelines such as in RLVR (DeepSeek-AI, 2025), where among the diverse set of generated sequences, only the correct ones are selected to update the policy. Our objective is to optimize this process by exploiting uncertainty during generation, allowing an efficient allocation of resources (in terms of number of generated sequences) to solve given prompt."
        },
        {
            "title": "2.1 UNCERTAINTY IN LLMS’ GENERATIONS",
            "content": "Among the different techniques for uncertainty quantification in LLMs, we focus our attention on top-K token entropy, as token entropy has been shown to be powerful uncertainty quantification measure (Fomicheva et al., 2020). We define top-K token entropy as: (K) := (cid:88) p(K) t,i log p(K) t,i , iI(K) (1) where (K) {1, . . . , } is the index set of the tokens with highest pt,i probability with denoting the vocabulary of the LM and N+ indexes the current generation step. Specifically, the quantity pt,i represents the probability assigned by the model to token at step after the softmax computation. We denote (K) {1, . . . , } the index set of the tokens with the highest probability pt,i and p(K) t,i their re-normalized probabilities, given by: p(K) t,i := (cid:80) pt,i jI(K) , pt,j (K) , (2) where (cid:80) iI(K) p(K) t,i = 1. Compared to more precise and computationally intensive uncertainty quantification methods found in the literature (Vashurin et al., 2025; Kuhn et al., 2023; Duan et al., 2024, e.g.,), top-K token entropy provides strong approximation to the entropy of the full-vocabulary, as it computes the dominant contributions from the most probable tokens with minimal computational overhead 2."
        },
        {
            "title": "2.2 ENTROPY IN LONG CHAIN-OF-THOUGHT REASONING",
            "content": "For our goal of saving resources in parallel sampling by leveraging model uncertainty, we first need to determine whether and how token entropy values relate to the models final performance. To this end, we analyze the entropy patterns of the CoT sequences generated by an LLM to solve challenging problems. We monitor the entropy of each token during generation, and rather than analyzing the entire entropy sequence, we focus on identifying significant spikes as signals of higher uncertainty. We hypothesize that this peak-entropy measure can serve as proxy for the models perceived difficulty of problem and thus its (in)ability to solve it: high peaks indicate moments where the model is highly uncertain about the next step in its reasoning chain, low entropy indicates that the model is more confident about what to generate next. Given the input prompt x, we sample independent candidate sequences {t(m)}M m=1 from the language model. During generation, for each token position in each sequence m, we record the token entropy (K) (y(m)), with = 20. For each sequence, we define the peak entropy value (m) peak as the mean of all entropy values that lie in the pth percentile of the sequences entropy distribution: (m) peak (pth) := 1 peak (pth) (cid:88) (K) (y(m)), tT peak (pth) (3) 2Full-vocabulary entropy computation can be costly due to large vocabulary sizes, often in the tens of thou3 Preprint. Under review. where (pth) := {t : (K) peak (y(m)) pth (cid:16) {H (K) (y(m))}t (cid:17) }, (4) and pth() denotes the pth percentile of the entropy sequence. We run Qwen3 4B3, strong open-source LLM with long CoT reasoning capabilities, on five standard reasoning benchmarks for math, science and code generation tasks (see Section 4 for the benchmarks details), allowing for = 32 parallel sequences to be generated. peak Figure 2 shows the Pass Rate accuracy, i.e., the proportion of correct answers out of = 32 generations, for each prompt and the corresponding average peak entropy (m)(pth) . We focus on the top percentile, specifically pth = 99.9, to isolate the highest entropy peaks. We observe statistically significant negative correlation (ρ 0.55), between the peak entropy during generation and model Pass Rate accuracy. This suggests that higher entropy peaks, indicative of greater uncertainty during generation, are associated with lower performance. Thus, additional path exploration during these phases may help to improve performance. Conversely, when entropy remains low, the model is more confident on the generated solutions (hence, the long CoT reasoning sequences), suggesting that further exploration may be less likely to yield significant improvements. This observation is in line with recent work which found that high-entropy tokens disproportionately contribute to performance gains during RL training (Wang et al., 2025). Figure 2: For each sequence generated by Qwen3 4B with FULL PARALLEL sampling (M = 32), we report its Pass Rate accuracy and the average entropy peak (pth = 99.9). The results reveal negative correlation (r = 0.547) between Pass Rate and the average entropy peak across sequences. Notably, sequences exhibiting higher entropy at any generation step are less likely to yield correct answer. Given this evidence, we ask: Can token entropy be leveraged to develop decoding adaptive strategy that allocates more compute to uncertain regions while limiting effort in more confident segments?"
        },
        {
            "title": "3 ENTROPY-AWARE GENERATION EXPLAINED",
            "content": "We introduce EAGER, training-free inference-time scaling approach aimed at optimizing parallel sampling by leveraging token entropy to guide resource allocation. EAGER consists of two stages: in the first one, EAGER-init dynamically adjusts the generation process to focus on sequences where the most effort is needed, while pruning unnecessary generations. In the second stage, the saved computational budget is reallocated to enhance performance on the remaining challenging prompts."
        },
        {
            "title": "3.1 EAGER-INIT: SAVE COMPUTE VIA TOKEN ENTROPY",
            "content": "EAGER-init represents the first stage of our approach and operates by identifying potentially easy questions during generation. Instead of sampling constant generations for every prompt, EAGER-init computes token entropy (K) at each step t, and compares it to predefined threshold θ.4 If the observed entropy exceeds this threshold, the current sequence is branched, creating new candidate continuation at that position. If the entropy is below the threshold, the generation continues with the existing sequence. During the branching step, we reuse the token distribution from the model but adopt temporally greedy approach; we select the top two most likely tokens to ensure sands. By restricting calculations to the top-K most probable tokens, the token entropy significantly reduces computational overhead while maintaining efficiency during generation. 3https://huggingface.co/Qwen/Qwen3-4B 4We empirically find the best threshold for model. See Section 4.1 for detailed analysis of the threshold. 4 Preprint. Under review. that the two new sequences always start with different tokens. This process continues until the total number of active sequences reaches predefined limit , at which point no further branching occurs. detailed overview of the EAGER-init algorithm is provided in Algorithm 1. This process yields generation tree where the root is the initial sequence, and each branching node corresponds to high-entropy token (Figure 1); the generation stops when the total number of nodes is equal to . For implementation efficiency, we restrict the branching procedure to long CoT sequences only, and entropy monitoring is halted if no branching has occurred within the previous 1000 tokens from the last branch. Algorithm 1: EAGER-init sequence generation Input: Prompt x, entropy threshold θ > 0, max active sequences , temperature τ , top-K for entropy K, maximum steps is the top-K token entropy at step under distribution p( x, y). // initial continuation from prompt t Output: Completed set of sequences Notation: (K) Initialize active set {y(1)} ; Initialize completed set ; for 1 to do if = then break foreach sequence do Compute next-token distribution p( x, y) with temperature τ ; Compute entropy (K) if (K) from top-K probabilities ; t θ and < then a1 arg maxa p(a x, y) ; a2 second-most-likely token under ; Update a1 ; Create branch a2, add to ; // most likely token // greedy continuation else Sample p( x, y) and update ; if ends with EOS or length limit then Move from to ; return ; Reducing test-time compute through EAGER-init. EAGER-init, saves computational budget through two mechanisms. The first arises directly from the branching logic: if branch occurs at token position t, all preceding tokens (0, . . . , 1) are reused across branches rather than being regenerated independently. The second, and more substantial source of savings occurs when the generation process does not saturate the maximum number of sequences set per prompt. For easy queries, the models default sampling converges to identical or near-identical completions, so that EAGER-init may terminate with only single sequence, saving 1 full generations compared to fixed-budget baseline which would let the model generate sequences for any given prompt. This surplus capacity can then be reallocated where it is most needed."
        },
        {
            "title": "3.2 EAGER: DYNAMICALLY ALLOCATE THE SAVED COMPUTE",
            "content": "The key challenge is to devise the best strategy to reallocate the compute which has been saved. We start by defining an additional budget b, computed as: = min(Mtheoretical Mactual, 2M ) (5) where Mtheoretical = is the maximum possible number of sequences that could be generated for the dataset D, and Mactual = (cid:80)D i=1 # Seqi is the total number of sequences actually produced under entropy-aware generation. 5 Preprint. Under review. i=1, initial generations {Yi}N i=1 (from EAGER-init), max Algorithm 2: Full EAGER algorithm Input: Dataset = {(xi, zi)}N sequences per prompt , entropy threshold θ i=1 i}N Output: Augmented generations {Y Compute Mtheoretical ; Compute Mactual (cid:80)N i=1 Yi ; Set remaining budget Mtheoretical Mactual ; Identify challenging prompts = {i Pass@k(Yi, zi) = 0} ; if = 0 or = then return {Yi} Assign additional budget = min(b, 2M ) uniformly across all ; foreach do if Yi < then // underutilizing prompt Set θ 0.8 θ ; Generate up to + sequences for xi using Algorithm 1 with θ ; else // prompt already saturated at Set θ θ ; Generate up to + sequences for xi using Algorithm 1 with θ ; Append new sequences to Yi ; return {Y i}N i=1 ; The term MtheoreticalMactual represents the surplus budget created by early stopping in easy prompts. We cap at 2M to avoid pathological cases where extremely large surpluses would lead to disproportionately high generation budgets for single prompts.5 We consider two scenarios: (i) test-time setting where target labels are unavailable and reallocation must rely solely on model signals, and (ii) setting where target labels are accessible, as in reinforcement learning pipelines where only correct generations are used for policy updates. In the absence of target labels, we identify saturating Budget reallocation without target labels. prompts, those that hit the maximum branching cap , as candidates for additional budget. The rationale is that when serves as hard limit, promising reasoning paths may remain unexplored. To mitigate this, we start from low branching threshold (θ = 2.0, see Section 4.1) and reallocate saved budget exclusively to these prompts. We denote this strategy as EAGER-adapt. Fine-grained budget reallocation with target labels. When target labels are available, we instead focus on challenging prompts, defined as those that fail to achieve Pass@k accuracy under EAGERinit (i.e., none of the generated sequences match the correct answer). For underutilizing prompts (fewer than sequences under EAGER-init), we lower the entropy threshold θ by 20%, encouraging earlier and more frequent branching. For saturating prompts (exactly sequences under EAGER-init), we extend generation up to new per-prompt limit +b, thereby deepening exploration where additional sequences may yield correct solutions. Reallocation in this setting is uniform across all failing prompts, while the generation strategy adapts based on each prompts prior behavior. By redirecting unused capacity from easier prompts to harder ones, this approach increases coverage without exceeding the global budget Mtheoretical (see Algorithm 2). Importantly, savings from branch-based token reuse persist even when > 0, and all additional sequences remain governed by Algorithm 1, ensuring that total token usage stays below an equivalent fixed-budget FULL PARALLEL sampling baseline. 5Especially in larger datasets, budget savings for easy prompts were large enough to allocate hundreds, if not thousand, of additional sequences to single failing prompts; this cap prevents excessive unbalanced allocation. Preprint. Under review."
        },
        {
            "title": "Sampling",
            "content": "AIME 2025 c@k p@k PR GPQA-Diamond PR c@k p@k HumanEval Plus PR c@k p@k SmolLM 3B Qwen3 4B DeepSeek 8B GPT-Oss 20B FULL PARALLEL EAGER-INIT EAGER FULL PARALLEL EAGER-INIT EAGER FULL PARALLEL EAGER-INIT EAGER FULL PARALLEL EAGER-INIT EAGER 0.53 0.53 0.73 0.80 0.77 0.83 0.80 0.70 0. 0.90 0.93 0.97 0.00 0.07 0.33 0.70 0.70 0.73 0.67 0.63 0.67 0.83 0.80 0.80 0.06 0.11 0. 0.62 0.61 0.69 0.65 0.64 0.67 0.67 0.66 0.68 0.49 0.59 0.85 0.75 0.79 0.81 0.95 0.93 0. 0.96 0.97 0.99 0.00 0.10 0.12 0.51 0.51 0.59 0.15 0.25 0.25 0.68 0.71 0.72 0.03 0.15 0. 0.43 0.43 0.54 0.18 0.24 0.25 0.65 0.66 0.66 0.00 0.68 0.75 0.91 0.86 0.94 0.95 0.96 0. 0.95 0.97 0.97 0.00 0.46 0.56 0.82 0.86 0.87 0.90 0.85 0.90 0.83 0.88 0.89 0.00 0.44 0. 0.78 0.86 0.86 0.86 0.77 0.89 0.79 0.85 0.85 Table 1: Comparison of FULL PARALLEL, EAGER-INIT and EAGER in AIME-2025, GPQADiamond and HumanEval Plus. We report pass@k, cons@k and Pass Rate where is number of samples generated (while always 32 for the baseline, differs per prompt for EAGER-init and EAGER). EAGER consistently achieves the best results and EAGER-init performs very competitive with FULL PARALLEL sampling while saving significant amount of compute as shown in Figure 3."
        },
        {
            "title": "4 EXPERIMENTAL SETTING AND RESULTS",
            "content": "Models. We evaluate multiple reasoning models from different model families and sizes to test EAGER in comparison to the FULL PARALLEL sampling baseline: SmolLM-3B (HuggingFaceTB, 2025), Qwen3-4B (Team, 2025), DeepSeek-R1-0528-Qwen3-8B (DeepSeek-AI, 2025) and GPT-oss 20B (OpenAI, 2025). Additional generation parameters and EAGER hyper-parameters are available in Appendix C. Benchmarks. We evaluate our approach saved resources (compute metrics) for generation and performance on set of diverse reasoning benchmarks on various tasks: AIME 2024 and 2025, and the 2025 Harvard MIT Math Tournament (Balunovic et al., 2025) for math, GPQA-Diamond (Rein et al., 2023) for scientific domains, and HumanEval Plus (Liu et al., 2023; 2024) for code generation. Compute metrics. We evaluate efficiency improvements using two complementary metrics: The first is the average sequence Count (#Seq). FULL PARALLEL sampling uses fixed budget of sequences, in contrast, EAGER uses dynamic #Seq that depends on the branching behavior. The second metric is the average token Count (#Token) generated. While #Seq provides general measure of computational efficiency, #Tokens is more precise indicator since, branching at step t, reuses previously generated tokens (0, . . . , 1) as prefix across new branches rather than regenerating them, that can lead to substantial savings even when #Seq is comparable. Performance metrics. We evaluate performance using three complementary metrics. Pass@k shows whether the model produces at least one correct final solution, Cons@k aggregates responses through majority voting across generations. Lastly, Pass Rate measures the proportion of correct answers over all generated outputs. We report the average metric across each entire benchmark."
        },
        {
            "title": "4.1 RESULTS",
            "content": "EAGER-init and EAGER yield significant savings in computation. Figure 3 (top row) illustrates the efficiency advantages of EAGER-init and EAGER across all benchmarks and model scales. Starting with EAGER-init, the total number of generated tokens is typically less than half of that required by FULL PARALLEL sampling. Building on this, EAGER (and EAGER-adapt) leverages small fraction of the saved budget to further improve accuracy, while still generating substantially fewer sequences than FULL PARALLEL sampling. On the performance side, EAGER consistently achieves higher Pass Rate accuracy than FULL PARALLEL sampling, indicating superior performance per unit of computation. It is worth noting that the performance of SmolLM 3B is 0.0 across all metrics in the parallel-sampling setting. This is caused by the generation of sequences in which the same tokens are repeatedly produced (e.g., The answer is: The answer is: The ...). This effect, along with the effect of temperature is discussed in Appendix B. 7 Preprint. Under review. Figure 3: Compute and performance trade-offs of EAGER-init and EAGER. Across all benchmarks and model size, the efficiency of EAGER-init and EAGER consistently outperforms FULL PARALLEL sampling, requiring only half as many tokens in most cases (top). In addition, they achieve higher pass rate accuracy (bottom). For issues specific to the smallest 3B model, see Appendix B."
        },
        {
            "title": "Data",
            "content": "AIME 2024 AIME 2025 HMMT 2025 GPQA-Dia. HE-Plus p@k # p@k # p@k # p@k # p@k # SmolLM 3B Qwen3 4B FP 0.60 27 0.53 28 0.23 28 0.49 185 0."
        },
        {
            "title": "EAGER\nadapt",
            "content": "0.63 19 0.60 15 0.33 15 0.76 133 0.68 94 full 0.83 19 0.73 15 0.40 15 0.85 119 0.75 20 init 0.53 18 0.53 19 0.33 20 0.68 137 0.04 94 FP 0.90 14 0.80 17 0.50 18 0.75 65 0."
        },
        {
            "title": "EAGER\nadapt",
            "content": "0.85 6 0.80 12 0.47 15 0.81 52 0.92 12 full 0.90 6 0.83 12 0.53 14 0.85 59 0.94 12 init 0.80 6 0.77 8 0.43 7 0.79 46 0.86 10 FP 0.93 20 0.80 22 0.57 24 0.95 68 0.95 25 DeepSeek 8B EAGER adapt init 0.90 7 0.73 8 0.43 8 0.93 37 0.96 0.90 9 0.77 13 0.50 15 0.93 42 0.96 13 full 0.93 9 0.80 12 0.57 15 0.96 43 0.98 13 GPT-Oss 20B EAGER adapt init 0.93 4 0.90 5 0.70 6 0.93 13 0.96 0.95 5 0.95 5 0.70 6 0.97 14 0.97 5 full 1.00 5 0.97 5 0.70 6 0.99 15 0.97 5 FP 0.93 8 0.90 10 0.63 13 0.96 24 0.95 5 Table 2: Reallocation of the additional budget (EAGER-adapt) only on Saturating prompts (i.e., prompts that reach = 32 generated sequences). All experiments use threshold of 2.0, which we found to provide good balance between number of tokens (# 1e5) used and performance (p@k) across models and benchmarks. Bold are best results, underline second best. Saturation is good proxy for budget reallocation. In the absence of target labels, EAGERadapt reallocates additional budget to saturating prompts as proxy for identifying challenging cases. Table 2 reports results across all benchmarks and models6. On average, this strategy not only achieves substantial token savings during generation, but also improves exploration, yielding higher Pass@k compared to the FULL PARALLEL sampling baseline. In other words, EAGER-adapt saves large fraction of compute while at the same time uncovering more successful reasoning paths. For comparison, Table 2 also reports the full EAGER approach. While it benefits from the unfair advantage of access to target labels, redirecting budget to saturating sequences still achieves the second-best performance in most settings. EAGER always achieves better performances than FULL PARALLEL sampling. As shown in Figure 3, EAGER consistently outperforms FULL PARALLEL sampling in terms of Pass Rate. Table 1 shows more comprehensive overview using Pass@k, Cons@k, and Pass Rate. While Pass@k is highest under EAGER, Pass Rate is consistently equal or better even for EAGER-init compared to FULL PARALLEL sampling. This suggests that EAGER-init effectively prunes unproductive generations (higher Pass Rate) at the cost of reduced exploration (lower Pass@k). In general, Pass@k is 6The EAGER-init results in Table 2 differ from those reported elsewhere because we fix the threshold to θ = 2.0 for the EAGER-adapt experiments. In this context, EAGER-init is treated as the first step of the overall process. In other sections, EAGER-init is evaluated as an independent decoding strategy with its optimal threshold selected for both efficiency and performance (see Section 4.1 for details and Appendix for threshold selection transparency). 8 Preprint. Under review. Figure 4: Performance comparison with scaling the total allowed sequences for generating (M {1, 4, 8, 16, 24, 32}). As increases (lines markers), EAGER consistently improves Pass@k (yaxis) while reducing the number of tokens needed to find the correct solution (x-axis), further shifting the Pareto frontier of the performanceefficiency trade-off. particularly useful in scenarios where obtaining at least one correct answer is critical, for example, when the user prioritizes correctness and exploration over efficiency as per in Reinforcement Learning applications. In contrast, Pass Rate and Cons@k capture different dimension of quality: (i) higher values indicate that EAGER focuses computation more effectively on promising generations, and (ii) given the extreme efficiency gains of EAGER-init compared to FULL PARALLEL sampling, the trade-off is often strongly favorable. EAGER scales effectively under budget constrains. We evaluate the effect of scaling the maximum number of allowed generations, , on overall performance. As shown in Figure 4, increasing improves the probability of obtaining at least one correct solution (Pass@k). This trend is expected, as larger generation budget naturally enables more extensive exploration. Notably, EAGER-init and even more so EAGER achieve superior Pass@k under the same constraints, often with significantly fewer tokens. In other words, EAGER not only benefits from larger but also allocates its computational budget more efficiently, resulting in consistent shift of the Pareto frontier, where higher accuracy is achieved at lower token cost. Threshold guides the trade-off between performance and compute. Efficiency metrics (# Tokens, # Seq) are directly shaped by the choice of entropy threshold θ. In our experiments, we explore values in the interval [1.8, 2.7], which captures the majority of observed entropy peaks (see Section 2.2). Across different model families and sizes, we find consistent efficiency improvements relative to the FULL PARALLEL sampling baseline throughout this range. The optimal setting of θ remains taskand model-dependent. Under EAGER-init, lower thresholds encourage more frequent branching, which increases both the number of generated sequences (#Seq) and total tokens (#Token). Higher thresholds, in contrast, restrict branching, yielding fewer continuations and lower computational cost. The balance between these regimes varies across architectures, scales, and datasets. Full results are available in Appendix A."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Since the recent introduction of test-time scaling (Snell et al., 2025; Welleck et al., 2024), multiple approaches have been proposed to improve its efficiency and performance. Wu et al. (2025) propose REBASE (REward BAlanced SEarch) branching method that expands reasoning trajectories that are evaluated as being of high quality by reward model. While powerful, REBASE is significantly more computationally expensive compared to directly computing token entropy at test-time. DeepConf (Deep Think with Confidence, Fu et al., 2025) is method that also leverages local confidence measures to increase performance and efficiency during generation. DeepConf uses this confidence measure to truncate sequences where it is lower than pre-defined threshold (determined during warm-up stage). This is in contrast to our proposed approach where the certainty measure drives branching, instead of truncation. Kang et al. (2025) introduce self-certainty, sequence-level measure closely related to cross-entropy. The authors demonstrate that self-certainty discriminates well between correct and incorrect answers and is robust to reasoning length. The authors additionally illustrate that self-certainty driven answer selection (through voting mechanism) leads to improve9 Preprint. Under review. ments in reasoning benchmarks. While the the current work is closely related to the work by Kang et al., we demonstrate that token-level certainty (in contrast to sequence-level) can function as useful tool to modulate performance and efficiency in reasoning LLMs."
        },
        {
            "title": "6 CONCLUSION AND FUTURE DIRECTIONS",
            "content": "By leveraging token-level entropy, EAGER-init proves to be highly performant training-free genIn eration method with significantly higher efficiency compared to FULL PARALLEL sampling. applications such as RLVR, where the correct answer is known, EAGER surpasses the Pass@k performance by up to 37% compared to FULL PARALLEL sampling while using up to 65% fewer tokens. Even without verifier at test time, EAGER-adapt improves exploration, surpassing FULL PARALLEL sampling performance while generating up to 40% fewer tokens. In both cases, the methods demonstrate the ability to save large amounts of compute and simultaneously enhance exploration. Finally, we show that the approaches are domain (math, science and coding) and temperature (see Appenidx B) agnostic. While our current work uses token-level entropy to create branching reasoning streams, future research could explore other methods for quantifying uncertainty. For example, using KullbackLeibler (KL) Divergence to measure token uncertainty is promising direction, inspired by the work of Kang et al. (2025). At the same time, key consideration is that the uncertainty quantification method must be lightweight, as computationally expensive approach would undermine the goal of improving generation efficiency."
        },
        {
            "title": "REFERENCES",
            "content": "Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https: //matharena.ai/. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 50505063, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.276. URL https: //aclanthology.org/2024.acl-long.276/. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frederic Blain, Francisco Guzman, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised Quality Estimation for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8: 539555, September 2020. ISSN 2307-387X. doi: 10.1162/tacl 00330. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence, 2025. URL https://arxiv.org/abs/2508.15260. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=rygGQyrFvH. HuggingFaceTB. Smollm3: smol, multilingual, long-context reasoner, 2025. URL https:// huggingface.co/blog/smollm3. Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty, 2025. URL https://arxiv.org/abs/2502.18581. 10 Preprint. Under review. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= VD-AYtP0dve. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id=1qvx610Cu7. Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efficient code generation. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=IBCBMeAhmC. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. OpenAI."
        },
        {
            "title": "Introducing",
            "content": "gpt-oss, 2025. URL https://openai.com/index/ introducing-gpt-oss/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute In The Thirteenth Inoptimally can be more effective than scaling parameters for reasoning. ternational Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=4FWAwZtd2n. Keith E. Stanovich and Richard F. West. Individual Differences in Reasoning: Implications for the Rationality Debate?, pp. 421440. Cambridge University Press, 2002. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Roman Vashurin, Maiya Goloburda, Albina Ilina, Aleksandr Rubashevskii, Preslav Nakov, Artem Shelmanov, and Maxim Panov. Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency. arXiv e-prints, art. arXiv:2502.04964, February 2025. doi: 10.48550/arXiv.2502.04964. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning, 2025. URL https://arxiv.org/abs/ 2506.01939. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language In The Eleventh International Conference on Learning Representations, ICLR 2023, models. Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/ forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=eskQMcIbMS. Survey Certification. 11 Preprint. Under review. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for LLM problem-solving. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=VNckp7JEHn. Preprint. Under review."
        },
        {
            "title": "A COMPLETE RESULTS",
            "content": "Table 3 presents complete overview of the results of our experiments. θ SmolLM3-3B Qwen3-4B Deepseek 8B GPT-oss 20B p@1 c@32 PR # # p@1 c@ PR # # p@1 c@32 PR # # p@1 c@32 PR # # 0.60 2.0 2.2 2.3 2.4 2. 2.0 2.2 2.3 2.4 2.5 0.53 0.53 0.52 0.67 0.57 0.73 0.73 0.83 0.77 0.67 0.53 1.8 2.0 2.2 2.3 2.4 2.5 2. 1.8 2.0 2.2 2.3 2.4 2.5 - 0.53 0.43 0.37 0.53 0.43 - - 0.63 0.57 0.57 0.67 0.73 0.23 1.8 2.0 2.2 2.3 2.4 2. 2.0 1.8 2.0 2.2 2.3 2.4 2.5 0.23 0.33 0.23 0.27 0.27 0.23 - 0.27 0.40 0.33 0.33 0.40 0.40 0. 2.0 2.2 2.3 2.4 2.5 2.0 2.2 2.3 2.4 2.5 0.68 0.62 0.61 0.61 0.59 0.79 0.81 0.75 0.79 0.85 0.00 1.8 2.0 2.2 2.3 2.4 2. 1.8 2.0 2.2 2.3 2.4 2.5 - 0.04 0.04 0.68 0.52 0.52 - - - - 0.75 - 0.03 0.03 0.10 0.10 0.20 0.50 0.10 0.10 0.17 0.20 0. 0.00 - 0.00 0.00 0.10 0.07 0.23 - - 0.00 0.00 0.10 0.07 0.33 0.00 0.03 0.03 0.00 0.10 0.17 0. - 0.03 0.03 0.00 0.10 0.17 0.17 0.00 0.04 0.07 0.02 0.06 0.10 0.04 0.07 0.03 0.08 0.12 0. - 0.01 0.01 0.46 0.37 0.48 - - - - 0.52 - 0.09 0.10 0.15 0.18 0.25 0.40 0.13 0.16 0.18 0.28 0.42 0. - 0.05 0.07 0.14 0.11 0.26 - - 0.06 0.08 0.15 0.13 0.31 0.03 0.06 0.07 0.06 0.12 0.18 0.17 - 0.06 0.09 0.06 0.13 0.19 0.19 27 18 18 17 14 5 19 19 19 19 14 28 - 19 19 17 17 - - 20 20 19 19 15 28 20 20 18 17 14 10 - 20 20 19 19 17 32.0 0.90 28.8 28.4 26.8 22.7 16.5 32.0 32 33.0 32.8 31.3 0.80 0.70 0.67 0.77 0.73 0.90 0.87 0.90 0.83 0. 32.0 0.80 - 28.8 29.9 27.8 27.6 17.5 0.77 0.77 0.67 0.60 0.70 0.60 - - - 32.0 32.0 32.1 32.2 33.6 0.80 0.83 0.80 0.80 0.80 0.83 32.0 0.50 31.7 30.8 28.5 27.5 23.8 17.4 0.43 0.43 0.37 0.40 0.33 0. - - 32.0 32.0 32.0 32.0 32.2 32.7 0.47 0.53 0.53 0.57 0.50 0.60 0.03 32.0 0.75 0.09 0.11 0.07 0.10 0.15 0.09 0.12 0.09 0.12 0.18 137 130 133 126 113 137 132 135 128 30.8 30.0 30.7 29.8 27.6 32.0 32.0 32.0 32.0 32.1 0.79 0.71 0.64 0.56 0.49 0.85 0.81 0.81 0.81 0.81 0.00 32.0 0.91 - 0.01 0.01 0.44 0.38 0.47 - - - - 0.56 - - 94 65 33 13 3 - - - - 20 - - 30.7 26.3 17.3 9.0 2.6 - - - - 19.3 - 0.87 0.86 0.86 0.84 0.81 0.82 - 0.94 0.92 0.92 0.94 0.93 0.80 0.73 0.67 0.67 0.77 0. 0.80 0.77 0.80 0.80 0.83 0.70 0.70 0.70 0.63 0.60 0.70 0.60 - 0.70 0.73 0.70 0.77 0.73 0.73 0. 0.37 0.37 0.37 0.40 0.33 0.43 - 0.37 0.37 0.37 0.40 0.40 0.43 0.51 0.51 0.47 0.49 0.45 0.46 0.51 0.50 0.53 0.51 0. 0.82 0.76 0.79 0.86 0.82 0.81 0.82 - 0.79 0.86 0.87 0.87 0.86 0.74 0.70 0.68 0.65 0.77 0.73 0.74 0.76 0.75 0.79 0. 0.62 0.60 0.61 0.64 0.60 0.70 0.60 - 0.63 0.63 0.70 0.71 0.71 0.69 0.34 0.33 0.35 0.36 0.40 0.33 0. - 0.33 0.36 0.37 0.41 0.37 0.44 0.43 0.43 0.43 0.43 0.44 0.45 0.43 0.45 0.47 0.50 0.54 0. 0.76 0.80 0.86 0.82 0.81 0.82 - 0.82 0.87 0.86 0.86 0.86 AIME 2024 (math) 0.93 32.0 14 6 1 0.3 1 0.2 9 5 5 5 5 15.3 1.7 1.0 1.0 1. 23.4 11.3 12.4 10.5 10.7 0.90 0.80 0.77 0.70 0.73 0.93 0.90 0.87 0.90 0.90 AIME 2025 (math) 0.80 32.0 17 0.90 8 1 0.3 0.3 0.3 - 13 12 6 8 5 7 24.5 18.4 3.1 1.1 1.1 1.0 - 0.73 0.70 0.70 0.63 0.63 - - 32.8 30.0 14.7 17.5 10.7 15. - 0.80 0.80 0.80 0.80 0.80 HMMT (math) 32.0 18 0.57 10 7 1 1 0.4 0.3 - 15 14 11 10 11 10 22.7 15.5 3.3 2.3 1.1 1.0 - 0.43 0.40 0.40 0.37 0.37 - - 32.9 32.0 24.5 22.6 25.2 21. - 0.57 0.57 0.53 0.57 0.50 GPQA-Diamond (science) 32.0 0.95 65 46 38 23 12 3 59 55 44 37 26.8 22.1 14.7 7.4 2.5 32.4 32.4 27.3 22.4 19.9 0.93 0.91 0.81 - 0.66 0.96 0.97 0.97 - 0.94 HumanEval Plus (code) 32.0 0. 27 16 10 1 0.5 0.5 0.4 - 17 9 11 12 12 9.9 6.20 1.1 1.1 1.1 1.0 - 11.2 5.7 9.9 10.9 11.8 0.95 0.96 0.94 0.87 0.92 0. - 0.97 0.96 0.98 0.97 0.96 0.80 0.87 0.80 0.77 0.70 0.73 0.87 0.80 0.83 0.80 0.77 0.67 - 0.60 0.63 0.67 0.60 0. - - 0.63 0.63 0.73 0.70 0.70 0.43 - 0.37 0.37 0.40 0.37 0.37 - - 0.40 0.43 0.43 0.43 0. 0.15 0.25 0.18 0.21 - 0.20 0.25 0.18 0.25 - 0.26 0.90 0.82 0.85 0.86 0.82 0.88 0.87 - 0.77 0.87 0.88 0.90 0. 0.73 0.85 0.79 0.76 0.70 0.73 0.85 0.80 0.81 0.78 0.77 0.65 - 0.59 0.64 0.66 0.61 0.61 - - 0.63 0.68 0.71 0.68 0.68 0.37 - 0.33 0.38 0.35 0.35 0.37 - - 0.35 0.43 0.38 0.39 0.39 0. 0.24 0.22 0.18 - 0.21 0.26 0.25 0.25 - 0.33 0.86 0.79 0.77 0.83 0.80 0.88 0.86 - 0.74 0.83 0.86 0.89 0.90 32.0 0.93 7 2 0.4 0.3 0.3 9 5 5 6 7 15.8 4.5 1.3 1.0 1.0 20.0 12.0 10.1 13.0 14. 0.93 0.90 0.90 0.93 0.90 0.93 0.93 0.93 0.97 1.00 22 32.0 0.90 - 8 2 1 0.5 0. - - 12 7 6 7 6 - 17.4 4.9 2.2 1.2 1.2 - 0.90 0.93 0.93 0.93 0.90 - - - 28.0 16.1 13.1 15.3 14.0 - 0.90 0.97 0.93 0.93 0.93 24 32.0 0.63 - 8 3 2 1 0. - 18.1 6.2 4.7 1.5 1.1 - 0.70 0.67 0.63 0.63 0.67 - - 15 13 11 11 11 68 37 33 23 - 43 43 46 - 45 25 18 21 13 6 3 1 - 22 15 13 8 8 - - - 32.1 27.5 24.0 24.0 23.5 - 0.70 n/a 0.67 0.63 0.67 32.0 0.96 28.5 25.0 17.4 - 3.1 32.5 33.8 35.4 - 35. 0.93 0.97 0.95 0.94 0.95 0.99 0.98 0.97 0.99 0.98 32.0 0.95 25.0 23.0 14.0 7.4 3.5 1.5 - 24.7 15.9 16.9 8.5 9. - 0.96 0.97 0.93 0.97 0.95 - 0.97 0.97 0.97 0.97 0.97 0.80 0.77 0.80 0.80 0.83 0.83 0.77 0.83 0.80 0.83 0.87 0. - 0.83 0.80 0.73 0.80 0.83 - - 0.83 0.80 0.73 0.80 0.83 0.53 - 0.43 0.43 0.47 0.43 0.53 - - 0.43 n/a 0.47 0.43 0.53 0.68 0.72 0.71 0.66 0.70 0.68 0.72 0.71 0.66 0.70 0.68 0.83 - 0.88 0.88 0.81 0.88 0. - 0.89 0.88 0.90 0.89 0.88 0.71 0.70 0.70 0.68 0.75 0.69 0.70 0.71 0.68 0.75 0.71 0.67 - 0.66 0.67 0.64 0.66 0. - - 0.66 0.68 0.64 0.66 0.69 0.38 - 0.37 0.36 0.40 0.38 0.40 - - 0.37 n/a 0.40 0.38 0. 0.65 0.66 0.66 0.65 0.66 0.65 0.66 0.66 0.65 0.66 0.66 0.79 - 0.83 0.82 0.74 0.85 0.82 - 0.84 0.82 0.84 0.85 0. 8 4 4 4 4 4 5 5 5 5 5 32.0 30.7 29.7 30.2 27.6 28.0 32.0 32.2 32.1 32.0 32. 10 32.0 - 5 5 5 5 5 - - 5 5 5 6 7 - 31.1 30 31.4 29.8 26. - - 32.0 32.9 32.2 33.0 32.0 13 32.0 - 6 6 6 6 6 - - 6 n/a 7 7 7 24 13 14 14 14 13 15 15 14 15 14 5 - 4 3 3 3 - 5 5 5 5 5 - 31.9 32.0 30.9 30.8 30.5 - - 32.0 n/a 32.0 32.0 32.0 32.0 29.7 27.6 30.9 25.8 24. 32.3 29.9 30.9 29.9 27.3 32.0 - 24.7 23.3 22.8 20.3 18.6 - 29.3 29.0 28.8 26.4 27.2 Table 3: All models, benchmarks and entropy-thresholds θ configurations. Higher is better for Pass@k (p@k), Cons@k (c@32) and Pass Rate (PR); lower is better for # Token. # Token are in 1e5 unit. Results for FULL PARALLEL sampling generations, EAGER-init generations, and full EAGER. Bold is best overall, underline is best within each category always including the FULL PARALLEL sampling one. Preprint. Under review."
        },
        {
            "title": "B EFFECT OF TEMPERATURE",
            "content": "The temperature hyperparameter, τ , plays critical role during autoregressive decoding by scaling the logits used by the sampling method (decoding becomes more greedy as τ 0). In this section, we conduct short exploration on the effect of temperature on EAGER. This is especially important in the current context, where higher diversity among the generated sequences can intuitively have an effect on the performance metrics. For this exploration, we focus on two LLMs, SmolLM 3B & DeepSeek 8B, two temperature settings, τ {0.6, 0.9} and AIME 2025 as the evaluation dataset. Furthermore, we conduct the analysis for varying entropy threshold levels θ {2.0, 2.2, 2.3, 2.4, 2.4, 2.5}. Figure 5: Pass@k and Cons@k at low (τ = 0.6) and high(τ = 0.6) temperature settings. Horizontal lines show the performance for the default sampling method, while the bars show EAGERs performance for varying entropy threshold levels θ. As shown in Figure 5, SmolLM 3B generally performs best at the high temperature setting while the opposite is true for DeepSeek 8B. Importantly, at both temperature levels, EAGER is competitive with the corresponding default baselines, often surpassing them. direct comparison between the low and high temperature setting including all metrics for default, EAGER and EAGER-init generations is presented in Table 4. Low Temperature (τ = 0.6) High Temperature (τ = 0.9) Low Temperature (τ = 0.6) High Temperature (τ = 0.9) p@1 c@32 PR # # p@ c@32 PR # # p@1 c@32 PR # # p@1 c@32 PR # # SmolLM3-3B Deepseek 8B 0.53 0.53 0.43 0.37 0.53 0.43 0.63 0.57 0.57 0.67 0.73 0.00 0.00 0.00 0.10 0.07 0. 0.00 0.00 0.10 0.07 0.33 0.06 0.05 0.07 0.14 0.11 0.26 0.06 0.08 0.15 0.13 0.31 28 19 19 17 17 20 20 19 19 15 32.0 0.63 28.8 29.9 27.8 27.6 17.5 32.0 32.0 32.1 32.2 33.6 0.67 0.53 0.57 0.60 0. 0.67 0.53 0.70 0.70 0.63 0.37 0.30 0.37 0.33 0.40 0.37 0.30 0.33 0.33 0.40 0.37 0.25 0.25 0.25 0.26 0.32 0. 0.25 0.25 0.27 0.33 0.29 44 26 27 26 22 21 27 28 28 29 29 32.0 0. 31.0 30.9 29.9 25.8 24.7 32.0 32.0 32.1 32.0 32.4 0.73 0.70 0.70 0.63 0.63 0.80 0.80 0.80 0.80 0.80 0.67 0.60 0.63 0.67 0.60 0. 0.63 0.63 0.73 0.70 0.70 0.65 22.0 32.0 0.70 0.59 0.64 0.66 0.61 0. 0.63 0.68 0.71 0.68 0.68 8.0 2.0 1.0 0.5 0.4 12.0 7.0 6.0 7.0 6.0 17.4 4.9 2.2 1.2 1.2 28.0 16.1 13.1 15.3 14.0 0.77 0.70 0.57 0.57 0. 0.77 0.77 0.73 0.73 0.77 0.60 0.63 0.60 0.53 0.53 0.60 0.63 0.63 0.63 0.60 0.67 0.57 0.58 0.58 0.53 0.53 0. 0.58 0.62 0.63 0.59 0.66 7.8 3.2 1.9 0.7 0.7 0.3 4.4 3.5 3.2 3.1 2.4 32.0 22.7 11.3 4.4 4.0 2. 30.2 21.0 21.1 18.9 14.9 θ - 2.0 2.2 2.3 2.4 2.5 2.0 2.2 2.3 2.4 2.5 Table 4: AIME 2025 results for default, EAGER-init, and EAGER generations for low and high temperature τ and varying entropy threshold θ. Best results per temperature and threshold setting are marked in boldface. Notably, the performance of SmolLM 3B is particularly higher in the high temperature setting when measured by the Cons@max rate. We find that this is result of the reduction of generations in which 14 Preprint. Under review. the same tokens are repeatedly produced (e.g., The answer is: The answer is: The ...). Specifically, in the high temperature setting, this phenomenon occurs, on average, 59.1% less compared to the low temperature setting. This behaviour was only observed with SmolLM 3B, suggesting it results from the smaller model size. An exception arises with the HumanEval Plus benchmark, where SmolLM 3B failed to solve any tasks, resulting in all metrics being zero under the FULL PARALLEL sampling setting. In contrast, EAGER-init and EAGER appeared to partially mitigate this issue. Lastly, we also find that the temperature has an effect on the number of tokens generated which, by extension, impact performance. For example, when EAGER is used at the high temperature setting, Deepseek 8B generates, on average, less than half the number of tokens compared to the low temperature setting. In contrast, SmolLM3-3B generates more tokens at the high-temperature setting. In both cases, and in line with the test-time scaling paradigm, we find that higher performance is achieved in whichever temperature setting more tokens are generated."
        },
        {
            "title": "C GENERATION PARAMS",
            "content": "All models are used with their longest thinking configuration to get their best performances. Furthermore we limit their context window to 32k tokens. All sequences are generated with temperature of τ = 0.60 and top-p of 95%. The effect of temperature is discussed in Appendix B. Table 5 reports the thresholds used for each benchmark and model. Following the discussion in Section 4.1, we select thresholds independently based on their intended use. The EAGER-init sampling method is designed to save budget without significantly compromising performance (lower threshold), whereas EAGER aims to preserve as much performance as possible for later reuse, higher threshold are preferred in such scenario. SmoLM 3B Qwen3 4B DeepSeek 8B GPT-oss 20B AIME 2024 AIME 2025 HMMT 2025 GPQA-Diamond HumanEval Plus EAGER-init 2.5 2.4 2.5 2.5 2.3 EAGER 2.5 2.5 2.5 2.5 - EAGER-init 2.0 2.0 2.5 2.0 2.2 EAGER 2.3 2.5 2.5 2.5 2.4 EAGER-init 2.0 2.2 2.4 2.0 2.0 EAGER 2.0 2.5 2.5 2.3 2. EAGER-init 2.4 2.4 2.5 2.2 2.4 EAGER 2.5 2.5 2.5 2.0 2.4 Table 5: Best thresholds for every benchmark and model."
        }
    ],
    "affiliations": [
        "Cohere",
        "Cohere Labs",
        "University of Groningen",
        "University of Milano - Bicocca"
    ]
}