{
    "paper_title": "Wonderland: Navigating 3D Scenes from a Single Image",
    "authors": [
        "Hanwen Liang",
        "Junli Cao",
        "Vidit Goel",
        "Guocheng Qian",
        "Sergei Korolev",
        "Demetri Terzopoulos",
        "Konstantinos N. Plataniotis",
        "Sergey Tulyakov",
        "Jian Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 1 9 0 2 1 . 2 1 4 2 : r Wonderland: Navigating 3D Scenes from Single Image Hanwen Liang1,2*, Junli Cao2,3*, Vidit Goel2, Guocheng Qian2, Sergei Korolev2, Demetri Terzopoulos3, Konstantinos N. Plataniotis1, Sergey Tulyakov2, Jian Ren2 1University of Toronto, 2Snap Inc., 3University of California, Los Angeles https://snap-research.github.io/wonderland/ Figure 1. Visual results generated by Wonderland. Given single image, Wonderland reconstructs 3D scenes from the latent space of camera-guided video diffusion model in feed-forward manner."
        },
        {
            "title": "Abstract",
            "content": "This paper addresses challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose novel pipeline to overcome these limitations. Specifically, we introduce large-scale reconstruction model that uses latents from video diffusion model to predict 3D Gaussian Splattings for the scenes in feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train *Equal contribution. the 3D reconstruction model to operate on the video latent space with progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that 3D reconstruction model can be effectively built upon the latent space of diffusion model to realize efficient 3D scene generation. 1. Introduction Humans have an innate ability to perceive and imagine three-dimensional information from single image. We intuitively estimate distances, recognize shapes, and mentally infer occluded regions in an instant. This powerful and effective cognitive process allows us to interpret complex spatial arrangements, envision depth and relative object size, and hallucinate unseen regions of 3D scenes. However, replicating this cognitive process from single image with learnable algorithms is extremely hard, since single view provides limited information about object sizes and distances. And it is more challenging to estimate the geometry of unseen areas from single image. Recent advancements in learnable scene representations (e.g., Neural Radiance Fields (NeRF) [32] and 3D Gaussian Splatting (3DGS) [19]) show promising results for rendering photorealistic 3D scenes. However, they have two major limitations that dramatically hinder their scalability and flexibility. First, they require dense multi-view images for training and, second, they use time-consuming per-scene optimization strategy. To alleviate the need for multi-view data, several studies [10, 25, 27, 38, 55] integrate generative priors from image diffusion models [15, 37, 43] for 3D synthesis from sparse views or single image. Although these methods alleviate data requirements, they suffer from limited 3D consistency in novel view synthesis: e.g., incorrect or distorted generation of the occluded regions [7, 38, 41], blurry background [10, 48], indicating image diffusion models have limited ability to reason within complex 3D context. To avoid lengthy optimization process when building 3D representation of scene, recent efforts explore regression-based models that perform novel view synthesis in feed-forward manner [4, 16, 44, 53, 56]. However, these methods face significant memory and computation challenges, as model training and complex scene rendering involve processing and predicting vast number of pixels based on overlapping, high-resolution input views. Consequently, existing methods are mostly limited to object-level generation or scenes with narrow view angles and limited scope [16, 51, 56], where fewer input views and reduced computation are needed. In this paper, we introduce Wonderland, which effectively addresses the above two challenges. From single image, Wonderland can efficiently generate high-quality point-based 3D representation (i.e., 3DGS [19]) of scene with wide scope. We explore rich 3D scene understanding instilled in foundational video diffusion models and build 3D representation directly from the video latents, dramatically reducing memory requirements. The 3DGS are regressed from video latents in feed-forward manner, significantly accelerating the reconstruction process. To achieve these features, we propose the following techniques: First, we introduce representation for controllable 3D generation by leveraging the generative priors from camera-guided video diffusion models. Unlike image models, video diffusion models are trained on extensive video datasets, capturing comprehensive spatial relationships within scenes across multiple views and embedding form of 3D awareness in their latent space, which allows us to maintain 3D consistency in novel view synthesis. Second, to achieve controllable novel view generation, we empower the video models with precise control over specified camera motions. Namely, we introduce novel dual-branch conditioning mechanism that effectively incorporates the desired diverse camera trajectories into the video diffusion model, enabling it to expand single image into multi-view consistent capturing of 3D scene with precise pose control. Third, to achieve efficient 3D reconstruction, we directly transform video latents into 3DGS. We propose novel latent-based large reconstruction model (LaLRM) that lifts video latents to 3D in feed-forward manner. With such designs, during inference, our model directly predicts 3DGS from single input image, effectively aligning the generation and reconstruction tasksand bridging image space and 3D spacethrough the video latent space. Compared with reconstructing scenes from images, the video latent space offers 256 spatialtemporal reduction, while retaining essential and consistent 3D structural details. Such high degree of compression is crucial, as it allows the LaLRM to handle wider range of 3D scenes within the reconstruction framework, with the same memory constraints. We extensively evaluate Wonderland and verify that it achieves state-of-the-art performance in single-view conditioned 3D scene generation and favors several unique advantages, as follows: Leveraging the dual-branch camera conditioning strategy, our video diffusion model generates 3D consistent multiview capturing of the scene with more precise pose control over existing works (Table 1 and Figure 3). Under the setting of zero-shot novel view synthesis, by using single-image as input for feed-forward 3D scene reconstruction, our approach outperforms existing works on various benchmark datasets; i.e., RealEstate10K [59], DL3DV [23], and Tanks-and-Temples [20]  (Table 2)  . By working on the latent space, our reconstruction pipeline harnesses the generative capabilities of the video diffusion model, enabling it to render high-quality images, wide-scope views, and more generic and diverse scenes (e.g., out-of-domain) that go far beyond objectlevel reconstruction (Figure 4 and Figure 5). 2. Related Work 3D Scene Generation. The object-level 3D generation [26, 35, 36, 60] has made significant progress in terms of quality and speed, whereas 3D scene generation still lags behind. Most 3D scene generation approaches follow twostage process, where novel views are generated from single image and then used to per-scene optimize 3D repFigure 2. Overview of Wonderland. Given single image, camera-guided video diffusion model follows the camera trajectory and generates 3D-aware video latent, which is leveraged by the latent-based large reconstruction model to construct the 3D scene in feedforward manner. The video diffusion model involves dual-branch camera conditioning to fulfill precise pose control. The LaLRM operates in latent space and efficiently reconstructs wide-scope and high-fidelity 3D scene. resentations. Early methods [7, 41, 54] combine depthbased warping with diffusion-based inpainting for novel view synthesis. However, reliance on monocular depth estimation and per-view inpainting can introduce severe distortions and artifacts, which limits the 3D consistency. Recent works [10, 48] incorporate camera conditioning into image diffusion models to control the poses in novel view generation, yet the 3D inconsistency issue persists due to the limitations with image-based models and they struggle with blurry backgrounds. More recent approaches utilize video diffusion models and global point clouds to improve multiview consistency [25, 55] and guide the novel view generation process. However, this is sensitive to the point clouds construction quality and is constrained to scenes with narImportantly, all previous methods depend on row scope. time-consuming per-scene optimization with NeRF [32] or 3DGS [19]. By contrast, our approach integrates explicit camera control into video diffusion model to fulfill precise-pose control and multi-view consistency for expansive 3D scene generation. large 3D reconstruction model is developed to efficiently construct 3D scenes directly from video latents in feed-forward manner. Camera-Conditioned Video Diffusion Models have attracted much attention [1, 3, 47]. Prior efforts explore grouping camera movements [3, 12] and apply LoRA [17] modules in video diffusion models for specific-type camera control. Later research explores injecting the camera matrices into the video diffusion models [13, 47, 50], yet these approaches can degrade visual generation quality. Recently, VD3D [1] adopts controlnet [57]-like conditioning with cross-attention mechanism. Due to the heavy computation cost, they integrate pose control only to low-resolution video generator in cascaded generation pipeline, resulting in imprecise pose control and unsatisfactory visual quality. Comparatively, we devise dual-branch conditioning applied to the video diffusion foundations that obtain precise pose control and high generation quality. The sum-up operations in controlnet-branch make the integration simpler. Sparse-View Reconstruction. Using vanilla NeRF or 3DGS for reconstruction has strict requirements for dense multi-view consistent images. Some studies lift the requirement for dense capture [11, 18, 29, 32]. Yet, they are slow to optimize. The recent efforts on feed-forward 3D reconstruction have gained growing interests [6, 16, 53, 56]. Trained on large-scale 3D datasets, these methods use transformers to regress 3D representations directly from sparse input images. Though effective, the computational overhead and number of views (e.g., 2 6 views) limit these methods to narrower tasks, e.g., object-level reconstructions. We alleviate these limitations by operating in latent space of video diffusion models, allowing us to work with large number of views while keeping computational efficiency in check. 3. Method This section presents our unified framework for feedforward 3D scene generation conditioned on single image. We first develop camera-guided video diffusion transformer to generate video latents covering wide scope of the scene. Precise pose control is achieved with our novel dual-branch camera conditioning module. The generated video latents are compact and 3D-aware, since they encapsulate multi-view capturing of the scene consistent in both structure and appearance, making them ideal to be lifted to 3D. Subsequently, we propose novel Latent Large Reconstruction Model (LaLRM) to directly decode the video latents to 3D Gaussian Splatting (3DGS) [19] for scene construction in feed-forward and memory-efficient manner. 3.1. Preliminaries Latent Video Diffusion Transformers. Recent video diffusion models work in latent space [3, 28, 52] for efficiency and use transformer-based architectures for scalability and quality [30, 33]. Specifically, given source video RT HW 3 with spatial dimension and frames, an encoder from 3D-VAE [28, 52] first comh = , rt = presses it to latent Rthwc, where t, h, w, and are temporal length, height, width, and channel dimension. The spatial and temporal compression rates are defined as rs = . During training, noisy latent is created in the forward diffusion process as zτ = ατ + στ ϵ, where ϵ (0, I), and α and σ depend on the noise scheduler parameterized via diffusion-time τ . Then, zτ is passed to the transformer model Dθ, parameterized by θ, that first patchifies zτ into visual tokens as long sequence ov RNvdv of length Nv and dimensionality dv. Later the ov is passed through series of transformer blocks. The transformer output is unpatchified and projected to restore the original latent shape for the loss computation, formulated as Lθ Expdata,τ pτ [Dθ(zτ ; y, τ ) ϵ2 2]. is the conditional signal. The exact objective may vary depending on the models parameterization [3, 30, 52]. Gaussian Splatting. We use 3DGS as 3D representation for its fast rendering speed and high quality results [19]. 3DGS represents scene using collection of Gaussian points (parameterized by position, 3D covariance matrix, color, and opacity) to model appearance and geometry. 3.2. Camera-Guided Video Latent Generation Video diffusion models have made significant progress in generating high-quality, physically plausible visualizations of the scenes. However, it is non-trivial to use them to synthesize 3D-aware latents, since they lack explicit control over pose trajectory and may produce dynamic scenes that are unsuitable for downstream 3D reconstruction. To address this, we enhance the video generation models with precise camera control and adapt them to generate static scenes. This enables the model to do controllable and comprehensive exploration of the scenes and generate 3Daware latents that are suitable for 3D reconstruction. Camera Representation. To achieve precise pose control, instead of using frame-level camera parameters, we enrich the condition information with more fine-grained pixellevel positional representations; i.e., the Plucker embedding. Given camera parameters of frame consisting of rotation Rf , translation tf and intrinsic Kf , the Plucker coordinate of pixel at position (uf , vf ) in frame is for- ) R6, where , mulated as puf ,vf = (tf () = d() denotes cross-product and is the normalized d() ray direction defined as duf ,vf = Rf K1 [uf , vf , 1]T + tf . For each video x, computing Plucker coordinates at pixellevel results in Plucker embedding RT HW 6, encompassing spatial-temporal camera pose information. Dual Branch Camera Guidance. It is challenging to adapt pretrained video diffusion transformer to generate static scenes that precisely follow the camera trajectory while preserving high visual quality. The reason is that such models have entangled attention operations over uf ,vf uf ,vf spatial-temporal visual tokens. Even minor modifications to the original architecture can lead to degradation in visual quality [1]. Also, directly fine-tuning the model on smallscale static scene datasets without architectural changes can result in overfitting and losing the generalized knowledge acquired from large-scale pre-training. To address these challenges, we propose dual-branch conditioning mechanism inspired by the principles of ControlNet [57] and LoRA [17], known for their excellent compatibility and extensions to pretrained models without alternating original weights. ControlNet is effective in integrating conditions and LoRA is cost-effective to finetune diffusion models with less risk of overfitting to customized training datasets. As illustrated in Figure 2, given the Plucker embedding of the camera, we first feed it into two lightweight camera encoders and get two sets of camera tokens: octrl and olora. Both camera encoders contain 3D convolution (3DConv) layers to spatial-temporally compress p, and unfolding operation, resulting in camera tokens of the same dimension as video tokens ov (RNvdv ). At the end of the camera encoders, motivated by prior work [5], we design zero-linear layer (Fdv,dv ) with weight and bias initialized as zero. To build the ControlNet branch, we create trainable copy of the first base transformer blocks of the foundation video model, for trade-off between controllability and memory cost. The octrl is element-wise added with ov and fed into the first trainable block. The output of ith trainable block is connected to zero-liner layer and then elementwisely added to the output of the corresponding ith frozen block of the pre-trained video diffusion model. Such design facilitates an in-depth and effective integration of camera conditional information. To further enhance camera control, we inject camera information into the main branch and add LoRA to the pre-trained video transformer. We do channelwise concatenation of olora and ov, and feed the concatenated tokens into tailored linear layer (F2dv,dv ). The weights WF R2dv,dv of this layer are initialized with identity matrix in ov part and zero matrix in olora part, so that we can get the output identical to ov at the initial training stage. The output is forwarded into frozen transformer blocks in the main branch, where the trainable cameraLoRA module is learned [52]. This design allows us to fine-tune the foundation model with minimal computation cost. Also, the camera-LoRA module helps the model better adapt to the static scene training set, enhancing the overall static nature of the generated scenes. The effects of ControlNet and LoRA branches are analyzed in Table 1. 3.3. Latent Large Reconstruction Model Given single image, our camera-guided video model generates compact video latents, that capture multi-view consistent appearance and geometry information of 3D scene. At this stage, we lift the video latents to explicit 3D representations (i.e., 3DGS) in feed-forward manner with our Latent Large Reconstruction Model (LaLRM), largely reducing the memory and time cost compared with imagelevel per-scene optimized strategy. Different from previous feed-forward 3D reconstruction methods that work in image space [44, 56], LaLRM operates on latent space with three main advantages. First, the video latent provides diverse and high-fidelity visual information contained in the video frame sequence. Second, the latent space is highly compressed so that the model can cover large scenes in memory-efficient way. Third, latent space is robust representation since video diffusion models are trained on webscale datasets, which helps our model to generalize well in out-of-domain scenes. As result, developed upon latent space, our model can reconstruct high-quality, wide-scope, and generic 3D scenes efficiently. pl Latent-based 3D Reconstruction Model. To perform large-scale reconstruction, we draw inspiration from prior work [56] and employ transformer-based architecture that regresses Gaussian attributes in pixel-aligned manner. Given the video latent Rthwc and the corresponding camera poses in Plucker embedding format RT HW 6, we first transform them into latent tokens and pose tokens. For the video latent, we patchify along spatial dimensions with patch size of pl to obtain ol RNldl , where Nl = . For the Plucker embedding, we do 3Dpl patchify along spatial-temporal dimensions with 3DConv. We intentionally set the spatial patch size as pl rs and temporal patch size as rt (defined in Section 3.1), producing pose tokens op that match the length with ol. These two sets of tokens are channel-wise concatenated, linearly projected for lower channel dimension, and fed into chain of base transformer blocks. We devise lightweight latent decoding module that uses the output tokens to regress Gaussian attributes. We fulfill pixel-level correspondence between the Gaussians and source video in RGB space via the latent decoding module, which involves 3D-DeConv layer with upsampling strides (rt, pl rs, pl rs) and output channel 12, giving us Gaussian feature map R(T HW )12. Following [56], the output 12-channel features correspond to 3D Gaussian features (3-channel RGB, 3-channel scale, 4-channel rotation quaternion, 1-channel opacity, and 1for dechannel ray distance). Please refer to the Supp. tails about our designed modules. During training, we render the images from the predicted Gaussians by randomly choosing supervision views, and minimizing the image reconstruction loss with combination of mean squared error loss (Lmse) and perceptual loss (Lperc) based on VGG19 [42] network. The total loss is formulated as Lrecon = λ1Lmse + λ2Lperc, where λ1 and λ2 are losses weights. Progressive Training Strategy. Due to the large domain gap between the video latent and Gaussian Splats, we face several challenges to train our large reconstruction model: To realize 3D geometry construction and avoid overfitting to the seen views (the views that can be directly decoded from video latent by 3DVAE), we need to involve more unseen views to guarantee 3D consistency. To facilitate LaLRM for generic 3D scene generation, the reconstruction model should adapt to in-the-wild video latent generated from the video diffusion model. We target at reconstructing the scene at high resolution for superior visual quality. Therefore, we adopt progressive training strategy in terms of the data source and image resolution. We initiate the model training at lower video resolution using benchmark video datasets with known camera poses. Each video has abundant frames sufficient for seen views and unseen views sampling. We use large stride of to sample video clip with frames, covering scene range frames. The sampled frames are considered seen views. The rest frames in the scene range, and the frames in the same video out of the scene range are considered unseen views. The VAE encoder projects the video clip to the video latent, which is forwarded into our reconstruction model for GS prediction. We use seen views and unseen views for supervision during training. Then, we scale up the training to higher resolutions. At this stage, we incorporate both benchmark datasets and substantial volume of out-ofdomain videos generated by our own camera-guided video diffusion model. For these generated samples, we use the video latents along with their conditioned camera poses as inputs, while the decoded video frames from these latents provide supervision views. The progressive training enables our model to better generalize to out-of-domain videos, enhancing robustness and fidelity in 3D reconstruction. 4. Experiments Implementation Details. We use an image-conditioned transformer-based video diffusion model (i.e., CogVideoX5B-I2V [52] that generates 49 frames with resolutions 480 720) as base model to build our camera-guided video generation model. 3DVAE is used to compress video clips with temporal and spatial ratios of rt = 4 and rs = 8, producing latents of dimensions 136090. To build the ControlNet branch, we use the first = 21 base transformer blocks from the video model to initialize the weights. And the camera-LoRA has low rank of dimension 256. To build the transformer architecture for LaLRM, we use 24 base transformer blocks with hidden dimension of 1024. The model is initially pretrained on low-resolution video clips(49 240 360) and then finetuned with higher resolution clips(49 480 720). total of = 48 supervision views are used, for which we randomly select = 24 frames from the source video clip as seen views, and an additional 24 frames disjoint from the video clip as unseen views. Figure 3. Qualitative comparison to prior arts in camera-guided video generation. The 14th frame in each sample is shown for comparison, with the first column displaying the conditional image and camera trajectory (bottom-right). Blue bounding boxes denote reference areas to assist comparison and orange bounding boxes highlight low-quality generations. We also show our last frames in the rightmost column. Our method outperforms the priors in both precise camera control and high-quality and wide-scope video generation. Table 1. Quantitative comparison to the prior arts in cameraguided video generation on RealEstate10K, DL3DV, and Tanks and Temples dataset. We report the performance for visual quality (FID and FVD), camera-guidance precision (Rerr and Terr), and visual similarity (LPIPS, PSNR, and SSIM). Method Dataset RealEstate10K MotionCtrl [47] VD3D [1] ViewCrafter [55] Ours DL3DV MotionCtrl [47] VD3D [1] ViewCrafter [55] Ours Tanks and Temples MotionCtrl [47] VD3D [1] ViewCrafter [55] Ours Ablations on RE10K Lora-branch Ctrl-branch Dual-branch FID FVD Rerr Terr LPIPS PSNR SSIM Metrics 22.58 21.40 20.89 16.16 25.58 22.70 20.55 17.74 30.17 24.33 22.41 19.46 229.34 187.55 203.71 153.48 248.77 232.97 210.62 169.34 289.62 244.18 230.56 189. 0.231 0.053 0.054 0.046 0.467 0.094 0.092 0.061 0.834 0.117 0.125 0.094 0.794 0.126 0.152 0.093 1.114 0.237 0.243 0.130 1.501 0.292 0.306 0. 0.296 0.227 0.212 0.206 0.309 0.259 0.237 0.218 0.312 0.284 0.245 0.221 19.02 18.75 17.22 212.74 205.45 183.54 0.102 0.058 0. 0.157 0.104 0.095 - - - 14.68 17.26 18.91 19.71 14.35 16.28 17.10 17.56 14.58 15.35 16.20 16.87 - - - 0.402 0.514 0.501 0.557 0.385 0.487 0.519 0.543 0.386 0.467 0.506 0.529 - - - Training Datasets. We utilize three benchmark datasets with camera pose annotations to train our models, including RealEstate10K (RE10K) [59], ACID [24], and DL3DV [23]. RE10K consists of around 80K videos, primarily capturing static real estate environments. We use dynamic stride {3, 4, 5} to sample video clips, covering wide scene ranges of approximately 150 250 frames. ACID contains videos of natural landscapes, with 11K scenes in the training set and 20K in the test set. Since most videos contain fewer than 100 frames with annotated camera poses, we set {1, 2} for this dataset. DL3DV, comprising training split DL3DV-10K and test split DL3DV140, includes diverse real-world indoor and outdoor scenes. Each video contains 200 to 300 keyframes with camera pose annotations. Due to the dramatic view changes between consecutive keyframes, we sample clips using stride {1, 2}. For all three datasets, we utilize only the standard training splits during model deployment. For out-ofdomain data used in training LaLRM, we generate 20K videos with image prompts from Flux.1 [21] and camera poses sampled from RE10K. 4.1. Comparison of Camera-Guided Video Generation We evaluate our camera-guided latent generation model (Section 3.2) by comparing the visual generation quality and camera-guidance precision against three baselines; i.e., MotionCtrl [47], VD3D [1], and ViewCrafter [55]. Evaluation Datasets includes three benchmark datasets: RE10K [59]. We randomly select 300 videos from the RE10K test set. For each video, we sample starting frame as the image condition and the subsequent camera poses at stride of 3 for pose guidance, with determined by the video temporal length. DL3DV-140 [23]. We randomly sample 300 video clips from DL3DV-140 dataset with sampling stride of 2. Similar to RE10K, starting frame is considered as the image condition and the subsequent camera poses provide pose guidance. Tanks-and-Temples (Tanks) [20] is used to test out-ofdomain generalization. We randomly sample 100 video sequences from all 14 scenes with sampling stride 4. We also use COLMAP to annotate poses for source videos as they lack dense pose annotations for all frames. Evaluation Metrics. We compare generated videos with ground truth video clips using various metrics. Visual Quality and Temporal Coherence of the generated videos are evaluated using Frechet Inception Distance (FID) [14] and Frechet Video Distance (FVD) [45]. Camera-guidance Precision is measured by following prior works [1, 47, 55] to use rotation error (Rerr) and translation error (Terr) metrics. Camera poses of generated videos are obtained via COLMAP [39, 40], followed by converting the camera systems to be relative to the first frame and normalizing all cameras into unified scale [1]. Since different methods vary in video length (e.g., from 14 to 49 frames), we report mean errors across all frames. Visual Similarity is assessed by calculating PSNR, SSIM [46], and LPIPS [58] between the generated images and ground-truth views. To facilitate fair comparison, we evaluate different methods over the first generated 14 frames. The reason is that the generated videos tend to deviate from the conditional single-view and present diverse appearances as the scene progresses. It becomes less reliable to use similarity metrics to evaluate generation quality and understand the difference between the generated frames and ground-truth views. Qualitative Comparisons are presented in Figure 3, where the conditioned images and the camera trajectories are displayed in the left-most columns. As can be seen, although MotionCtrl produces images with good quality, it fails in precise alignment with the given camera conditions. This is because MotionCtrl adopts high-level camera embedding to control camera poses, which lacks fine-grained pose control. The frames generated by VD3D show limited quality due to the low resolution. ViewCrafter has artifacts across frames, mainly because it uses incomplete point clouds to render images as conditions, which have irregular missing regions. In contrast, our method demonstrates superior quality for precise pose control and higher visual quality. Quantitative Comparisons are shown in Table 1. Our approach consistently outperforms the baselines in all the metrics. The lower FID and FVD suggest our model better follows the data distribution of the ground-truth data. The lower LPIPS and higher PSNR and SSIM indicate better visual quality and higher similarity to the groundtruth. Moreover, our method is capable of generating more precise camera control than baselines (i.e., lower Rerr and Terr). 4.2. Comparison of 3D Scene Generation We evaluate our method and two baseline approaches (i.e., ZeroNVS [38] and ViewCrafter [55]) for 3D scene generation on real-world and in-the-wild synthetic datasets. Table 2. Quantitative comparison on various benchmark datasets for 3D scene novel view synthesis with single view condition. Method Metrics RealEstate10K DL3DV Tanks-and-Temples LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM ZeroNVS [38] ViewCrafter [55] Ours Ablation-LaLRM RGB-14 RGB-49 Latent-based 0.448 0.341 0.292 0.137 0.126 0.122 13.01 16.84 17.15 21.39 25.06 27.10 0.378 0.514 0. 0.751 0.830 0.864 0.465 0.352 0.325 0.205 0.196 0.159 13.35 15.53 16.64 18.76 20.94 23.25 0.339 0.525 0. 0.696 0.733 0.786 0.470 0.384 0.344 0.221 0.192 0.170 12.94 14.93 15.90 19.70 20.54 22.66 0.325 0.483 0. 0.605 0.687 0.743 ViewCrafter generates 3D scenes conditioned on single image and pose trajectory, and ZeroNVS reconstructs 360degree scenes without any pose conditioning. Comparison on Benchmark Datasets. To evaluate 3D scene generation on the benchmark datasets, we sample 100, 100, and 50 images along with camera trajectories from RE10K, DL3DV, and Tanks test sets, respectively, using the same sampling strategy detailed in Section 4.1. For quantitative comparison, we measure the LPIPS, SSIM, and PSNR by comparing the renderings against ground-truth frames from the source video. Evaluating in this underconstrained setting is challenging, as multiple 3D scenes can be considered as consistent generations for given view [10]. Therefore, we follow Section 4.1 and measure the metrics using frames that are temporally close to conditional image, i.e., total of 14 sampled frames and poses that right after the conditional image. The qualitative comparison, illustrated in Figure 4, reveals the superior 3D generation capabilities of our model. ZeroNVS produces renderings that are noticeably blurry and lack details. ViewCrafter shows improved results in visible regions from the conditional image while failing to handle occluded areas properly. In contrast, our model excels at preserving intricate details and accurately reconstructing the visible regions in the condition image. By leveraging priors from the video diffusion model, our approach generates high-fidelity and visually satisfactory novel views, even in unseen regions. We further report the quantitative comparison in Table 2, indicating our method outperforms all baselines by large margin across multiple datasets. These results demonstrate our model can generate high-fidelity and geometrically consistent 3D scenes from single views. Comparison on In-the-wild Scene Generation. We generate synthetic images using SDXL [34], and randomly sample trajectories from RE10K dataset for novel view synthesis. At this point, we focus on wide scene generation, with qualitative comparisons against ViewCrafter [55] and Wonderjourney [54], due to their notable performance in expansive 3D scene synthesis. Since Wonderjourney generates scenes by extending the conditional image only in zooming-out manner, thus, our comparisons with Wonderjourney are limited to this direction. For ViewCrafter, we perform comparisons on more complex pose trajectories to evaluate the varied and wide-scope scene generation. In Figure 5, we observe that ViewCrafter can only generFigure 4. Qualitative comparison for 3D scene generation. Blue bounding boxes show visible regions from conditional images and yellow bounding boxes show low-quality regions. Our approach generates much higher quality novel views from one conditional image. Figure 5. Comparison with ViewCrafter (left) and WonderJourney (right) for in-the-wild 3D scene generation from single input images. ate 3D scenes within very limited area and the quality deteriorates significantly when the view range becomes large. While Wonderjourney can produce scenes with broader field of view, the generated views tend to be blurry and contain many artifacts. Comparatively, our method generates expansive scenes that maintain high realism and are consistent in both appearance and 3D geometry. Comparison on Mip-NeRF. In this part, we compare our method with prior work Cat3D [10] on more complex scenes from Mip-NeRF [2]. Due to the lack of open-source code for Cat3D, we retrieve the demonstration results directly from the source webpage of [10]. The images in Figure 6 are rendered from 3DGS generated with orbital camera trajectories. For each scene, we show renderings from two viewpoints: one at the conditional image (starting) view and another at around 120rotation from the starting view. We observe that for views close to the conditional image, our method achieves rendering quality similar to Cat3D and noticeably better than ZeroNVS. However, as the view deviates from the conditional image, Cat3D suffers from severe blurring, particularly in the background. In contrast, our method generates scenes with clearer textures, sharper details, and greater consistency with the conditional images. Comparison on Latency. To highlight the efficiency of the controllability and generation quality of the scene. More ablations are provided in the Supp. file. Latents vs. RGB for 3D Reconstruction. We compare the methods for using video latents and RGB-frames for 3D reconstruction. We develop two RGB-based reconstruction models that adopt similar architecture as LaLRM: RGB-49: we sample 49-frame clips from the training sets by using the same sampling strategy as LaLRM. RGB-14: we sample 14-frame clips from the training sets and increase the sampling stride by factor of four to cover the same scene range as LaLRM. To ensure comparable computational and memory cost with LaLRM, we introduce 3DConv layer at the entrance of the RGB-49 model and 2DConv layer in the RGB-14 model. This setup embeds RGB inputs into lower-dimensional space, matching the input latents in LaLRM. All model architectures and training strategies are the same as LaLRM. We evaluated the three reconstruction models on the test sets of RE10K and DL3DV, as well as the Tanks dataset. From RE10K, we sample 100 testing clips covering the scene range of 49 3 frames in source videos. From DL3DV, we sample 100 testing clips covering the scene range of 49 2 frames. For Tanks, we sample 50 testing clips covering the scene range of 49 4 frames. The sampled clips or the embedded latents are fed into the models to construct the scenes. The remaining frames within each scene range are treated as unseen views for evaluation, using LPIPS, PSNR, and SSIM metrics. As we observe in the second section of Table 2, the LaLRM (denoted as Latentbased), taking latents as input, performs the best among the others. 5. Conclusions We have introduced Wonderland, novel framework for high-fidelity 3D scene generation from single image in feed-forward manner. Unlike traditional methods that operate in pixel space or rely on per-scene optimization, our approach leverages the rich generative priors embedded in pose-conditioned video diffusion models. By operating within the compact latent space, Wonderland achieves efficient and scalable 3D scene synthesis while addressing the challenges of temporal-spatial consistency and pose controllability. We introduced dual-branch camera conditioning mechanism, enabling precise pose control and diverse trajectory generation for novel view synthesis. Furthermore, our latent-based large reconstruction model (LaLRM) seamlessly integrates the generative capabilities of video diffusion models with 3D Gaussian Splatting, ensuring computational efficiency and scalability to wide-view coverage. Our extensive evaluations across diverse datasets have demonstrated the superior performance of our approach in generating visually consistent and highIt outperforms existing state-of-thefidelity 3D scenes. art methods in both video generalization and 3D rendering quality. Figure 6. Comparison to ZeroNVS and Cat3D with Mip-Nerf dataset on 3D scene generation from single input images. For each scene, the conditional image is shown in the left-most column. We show renderings from two viewpoints, one at the conditional image (starting) view (upper) and another at around 120-rotation from the starting view(lower). our pipeline, we report the end-to-end latency on single NVIDIA A100 GPU and compare it with recent works, including Cat3D [10], ViewCrafter [55], and ZeroNVS [38]. Cat3D requires approximately 16 minutes(1min with 16A100), ViewCrafter takes around 6 minutes (25-frame video), and ZeroNVS demands an extensive 3 hours. In contrast, our method completes scene generation with 5 minutes, achieving 3.2 speedup over Cat3D, 1.2 over ViewCrafter, and an impressive 36 improvement over ZeroNVS. This demonstrates the superior efficiency of our pipeline in single-image 3D scene generation. 4.3. Analysis of Architecture Design Dual-branch Camera-pose Guidance. We analyze the effect of each conditioning branch in the camera-pose guidance video diffusion model. We develop two single-branch camera-pose guidance video diffusion models and provide the comparison in the second section of Table 1: Ctrl-branch that has the ControlNet-based conditioning. Lora-branch that has feature concatenation and lorabased finetuning. We sample 100 video clips from RE10K test set and measure the metrics FID, FVD, Rdist and Tdist. Results show that compared with Lora-branch, Ctrl-branch contributes more for precise pose control. Adding Lora-branch on top of Ctrl-branch, which gives Dual-branch, further improves"
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffuarXiv preprint sion transformers for 3d camera control. arXiv:2407.12781, 2024. [2] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields supplemental materials. [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. Pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1945719467, 2023. [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [6] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. 2024. [7] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes, 2023. [8] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. [9] Kingma Diederik. Adam: method for stochastic optimization. (No Title), 2014. [10] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T. Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models, 2024. [11] Guangcong, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. Proc. ICLR, 2024. [13] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [16] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [18] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent few-shot view synthesis. In CVPR, pages 58855894, 2021. [19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. [20] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017. [21] Black Forest Labs. Flux: Decentralized computation framework, 2023. Accessed: 2024-11-14. [22] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. [23] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. [24] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from sinIn Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, pages 1445814467, 2021. [25] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model, 2024. [26] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10072 10083, 2024. [27] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. [28] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [29] Jinjie Mai, Wenxuan Zhu, Sara Rojas, Jesus Zarzar, Abdullah Hamdi, Guocheng Qian, Bing Li, Silvio Giancola, and Bernard Ghanem. Tracknerf: Bundle adjusting nerf from sparse and noisy views via feature tracks. 2024. [30] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video In Proceedings of the IEEE/CVF Conference synthesis. on Computer Vision and Pattern Recognition, pages 7038 7048, 2024. [31] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. [32] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [35] Ben Poole, Ajay Jain, Jonathan Barron, and Ben MildenICLR, hall. Dreamfusion: Text-to-3d using 2d diffusion. 2022. [36] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. In ICLR. OpenReview.net, 2024. [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [38] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. Zeronvs: Zeroshot 360-degree view synthesis from single image, 2024. [39] Johannes Lutz Schonberger and Jan-Michael Frahm. In Conference on ComStructure-from-motion revisited. puter Vision and Pattern Recognition (CVPR), 2016. [40] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unIn European Conference on structured multi-view stereo. Computer Vision (ECCV), 2016. [41] Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion, 2024. [42] Karen Simonyan. Very deep convolutional networks arXiv preprint large-scale image recognition. for arXiv:1409.1556, 2014. [43] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [44] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation, 2024. [45] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. [46] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [47] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [48] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski. Reconfusion: 3d reconstruction with diffusion priors, 2023. [49] Dejia Xu, Hanwen Liang, Neel Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos Plataniotis, and Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint arXiv:2403.16993, 2024. [50] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. [51] Hongbin Xu, Weitao Chen, Zhipeng Zhou, Feng Xiao, Baigui Sun, Mike Zheng Shou, and Wenxiong Kang. Controlrm: Fast and controllable 3d generation via large reconstruction model. arXiv preprint arXiv:2410.09592, 2024. [52] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2024. [53] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images, 2021. [54] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. [55] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis, 2024. [56] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting, 2024. [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [58] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [59] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. Stereo magnification: [60] Peiye Zhuang, Songfang Han, Chaoyang Wang, Aliaksandr Siarohin, Jiaxu Zou, Michael Vasilkovsky, Vladislav Shakhrai, Sergey Korolev, Sergey Tulyakov, and HsinYing Lee. Gtr: Improving large 3d reconstruction models through geometry and texture refinement. arXiv preprint arXiv:2406.05649, 2024. Wonderland: Navigating 3D Scenes from Single Image"
        },
        {
            "title": "Table of Contents",
            "content": "A. More Analysis on Controllable Video Generation A.1. Effect of LoRA on Static Scene Generation and Camera Controllability . . A.2. Analysis of the Design for ControlNet-branch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. More Analysis on 3D Reconstruction B.1 . Fine-tuning with In-the-wild Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. More discussion with Related Works D. Implementation Details D.1. More Details for Model Architectures . . D.2. Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 1 3 3 5 5 6 6 E. Limitations and Future Work A. More Analysis on Controllable Video Generation Here we provide more ablation analysis to better understand our design for camera-conditioned video generation. A.1. Effect of LoRA on Static Scene Generation and"
        },
        {
            "title": "Camera Controllability",
            "content": "In our camera-guided video generation model, we employ LoRA fine-tuning in the main branch. LoRA is particularly advantageous for its compatibility with pre-trained models, as it introduces extensions without altering the original weights. The lightweight module offers cost-effective approach to fine-tune large-scale models, requiring minimal computational resources and reducing the risk of overfitting to customized training datasets. In our framework, we take advantage of LoRA to enhance both static scene generation and camera controllability. To evaluate the impact on static scene generation, we fine-tune the I2V source model using LoRA on customized datasets dominated by static scenes, including RealEstate10K (RE10K), ACID, and DL3DV. In this setup, no pose control is applied. We compare the performance of the fine-tuned model against the source model on 20 in-thewild image prompts (along with text descriptions). The results show that the fine-tuned model generates significantly more static scenes compared to the source model, especially for cases with humans and animals. Visualizations provided in Fig. A1 illustrate that LoRA enables the generation of more static scenes without compromising visual quality. To assess the role of LoRA in enhancing camera controllability, we train our full model without incorporating LoRA modules. Under such settings, in the main branch, the camera embeddings are fed into the network in channel-concatenation manner without LoRA tuning in the main backbone. Only the LoRA-camera encoder and linear processing layers at the top are learned. Following the experimental setup outlined in Sec. 4.3, we evaluate the models on 100 video clips sampled from RealEstate10K. As shown in the middle section of Tab. A2, comparison between the Dual w/o LoraModule and Dual-branch configurations reveals that LoRA plays critical role in fine-tuning the main branch. Excluding LoRA results in noticeable performance drop in both visual quality and camera-guidance precision. A.2. Analysis of the Design for ControlNet-branch In our full model, the ControlNet branch is designed by utilizing trainable copy of the first 21 base transformer blocks of the foundational video model, which consists of 42 blocks in total. Here we extensively evaluate the designs by using the ControlNet branch only. Specifically, we train Expression commonly used E rs rt diffusion used τ ατ , στ zτ Dθ ov octrl, olora reconstruction used pl ol Nl op Table A1. Overview of the notations used in the paper."
        },
        {
            "title": "Explanation",
            "content": "x RT HW 3 - Rthwc - rs = rt = RT HW 6 = source video clip stride to sample clip from source video video latent embedded from encoder from 3D-VAE spatial compression rate temporal compression rate Plucker embedding of cameras of video clip - - - - ov RNvdv octrl, olora RNvdv - diffusion time step diffusion noise scheduler parameters noisy video latent diffusion model parameterized by θ visual tokens as sequence in diffusion model camera tokens as sequence in diffusion model number of transformer blocks in ControlNet branch - ol RNldl Nl = pl op RNldl - R(T HW )12 pl spatial patch size applied to in LaLRM visual latent tokens as sequence in LaLRM number of visual latent tokens in LaLRM camera tokens as sequence in LaLRM number of supervision views in LaLRM Gaussian feature map in LaLRM Figure A1. Comparison of video generations between the source model (upper row) and the model fine-tuned on static-scene datasets with LoRA modules (lower row). The results demonstrate that fine-tuning the model on static-scene datasets equipped with LoRA produces significantly more static scenes. the model with ControlNet conditioning under various configurations: using the first 21 blocks without weight copying (denoted as w/o weight copy), and using the first 1, 10, or 30 blocks with weight copying, denoted as block-1, blocks-10, and blocks-30, respectively. As shown in the third section of Tab. A2, comparisons among different architecture configurations and theCtrl-branch reveal that weight copying improves all metrics lot, particularly visual quality. Using only one block results in weak camera controllability, while increasing the number of blocks strengthens the ability of the model to guide camera poses. Notably, using 21 blocks (Ctrl-branch) achieves similar levels of pose controllability as using 30 blocks, while maintaining high visual quality. Based on these observations, we select the trainable copy of the first 21 base transformer blocks as it provides an optimal balance between pose controllability and comFigure A2. Comparison of 3D rendering performance between latent reconstruction models fine-tuned without in-the-wild dataset (upper row) and with in-the-wild dataset (lower row). Involving in-the-wild datasets during fine-tuning improves the generalization capability. Table A2. Analysis on architecture designs in camera-guided video generation model. We report the performance for visual quality (FID and FVD) and pose control precision (Rerr and Terr) from models trained on RealEstate10K dataset. The first section of the table is adopted from Tab.1 in the main paper. Table A3. Analysis on involving in-the-wild dataset to finetune LaLRM. We report the performance on various benchmark datasets for novel view synthesis of 3D scenes, which are built from single view condition."
        },
        {
            "title": "Metrics",
            "content": "Lora-branch Ctrl-branch Dual-branch FID FVD Rerr Terr 19.02 18.75 17.22 212.74 205.45 183. 0.102 0.058 0.052 0.157 0.104 0.095 Dual w/o LoraModule 17.84 195.07 0. 0.101 Ctrl-branch only w/o weight copy block-1 blocks-10 blocks-30 18.92 19.90 19.15 20.15 206.75 214.66 210.74 221.61 0.065 0.114 0.075 0.056 0.108 0.162 0.126 0. putational efficiency. B. More Analysis on 3D Reconstruction In the following section, we provide more analysis to understand our choices for building the large-scale 3D reconstruction model. B.1. Fine-tuning with In-the-wild Dataset In the deployment of Latent Large Reconstruction Model (LaLRM), we adopt progressive training strategy. During the second stage, we fine-tune the model by involving self-generated in-the-wild dataset. To assess the impact of this dataset, we further fine-tune separate reconstruction model, LaLRM, without incorporating in-the-wild dataset. We quantitatively compare the performance of LaLRM and LaLRM on benchmark datasets and qualitatively evaluate them on 20 disjoint in-the-wild image prompts. The results shown in Tab. A3 indicate that incorporating inthe-wild dataset during fine-tuning enhances the generalizaMethod RealEstate10K DL3DV Tanks-and-Temples Metrics LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LaLRM LaLRM 0.295 0.292 17.06 17.15 0.538 0.550 0.343 0.325 16.62 16.64 0.570 0. 0.359 0.344 15.85 15.90 0.502 0.510 tion capabilities of our model. Furthermore, as shown in Fig. A2, LaLRM demonstrates noticeably better rendering quality compared to LaLRM, further validating the benefits of using in-the-wild data in the fine-tuning process. C. More discussion with Related Works We provide detailed discussion between our work and the related studies that use generative priors for 3D rendering. Significant advancements have been achieved in 3D object generation from text or single images [22, 26, 35, 36, 49, 60], with notable improvements in quality and efficiency. However, progress in 3D scene generation has remained relatively limited. Most approaches to 3D scene generation follow two-stage process: First, generating novel views from single image; Second, using these views to train 3D representation with per-scene optimization strategy. Early methods, such as LucidDreamer [7] and RealmDreamer [41], explore scene-level 3D generation conditioned on text descriptions or single images. They rely on the 3D priors from incomplete point clouds constructed via depth prediction from single images. Then, they combine depth-based warping with diffusion-based image inpainting to complete the scenes in an autoregressive manner. However, these methods often struggle with inconsistencies in occluded regions, as the per-view inpainting process can introduce severe artifacts and discontinuities, particularly in unseen areas. WonderJourney [54], which targets widescene generation, also employs image inpainting diffusion models to fill unseen regions rendered from limited point clouds. However, as shown in our main comparisons, this Figure A3. Structure of Dual-branch Camera-guided Video Diffusion Model. We show the skeletons of the training pipeline, where random noise is added to the video latents. The conditional image is merged to the noisy latents via feature concatenation. The camera guidance is integrated with LoRA-branch (left) and ControlNet-branch (right). We ignore the text tokens, the diffusion time embeddings, the positional embeddings, and some reshaping operations for simplicity in the figure. In the foundation diffusion transformer, the text tokens are concatenated along number-of-token dimension with visual tokens. Thus we apply zero-padding to camera tokens to guarantee the same length before concatenation or element-wise sum. By default, we use SiLu as our activation function. method similarly suffers from 3D incoherence in occluded areas. Also, all these works do not have automatic and explicit control over camera poses during the generation process. Other works, such as Cat3D [10] and ReconFusion [48], address multi-view consistency by incorporating camera conditioning into image diffusion models. Nonetheless, noticeable issue is their tendency to produce blurry or distorted background regions, particularly when conditioned on single image. This arises from their usage of image diffusion models to obtain dense views auto-regressively, which are then used for 3D reconstruction via per-scene optimization. Image diffusion models lack built-in mechanisms to guarantee cross-view consistency and such multiple-shots generation strategy often introduces inconsistencies, especially for wide-view scenarios. [55], recent works, and like ReconX [25] More ViewCrafter leverage video diffusion models and global point clouds to enhance multi-view consistency. However, as demonstrated in our main comparisons, these methods are sensitive to the initialization of point clouds and are restricted to generating scenes with narrow scope. Additionally, they lack explicit pose control during the generation process. Importantly, all the previous methods depend on timeconsuming per-scene optimization like NeRF [32] or 3DGS [19]. In contrast, our approach integrates explicit camera control into video diffusion model to enable precise and expansive scene generation. We develop largescale 3D reconstruction model capable of efficiently constructing 3D scenes directly from video latents. The design effectively aligns the generation and reconstruction tasks and bridges the image space and 3D space through the video latent space, fulfilling single-image to 3D scene generation in feed-forward manner, as well as eliminating the need for time-consuming per-scene optimization. D. Implementation Details We provide list of notations in Tab. A1 to facilitate the presentation for our neural network architectures and formulas in the paper. D.1. More Details for Model Architectures In our framework, we develop camera-guided video diffusion model and latent large reconstruction model to directly generate 3D scenes, which are conditioned on single images. Such designs effectively align the generation and reconstruction tasks and bridge the image space and 3D space through the video latent space. In this section, we provide detailed illustrations of the proposed model architectures in Fig. A3 and Fig. A4. In Fig. A3, we show the details of integrating camera embeddings to the pre-trained video diffusion transformer during the training stage, with ControlNet-branch and LoRA-branch. Each branch involves lightweight camera encoder composed of Convolutional layers and zero-linear layers. The camera encoders project the camera embedding into camera tokens of the same dimension as the visual tokens. The visual tokens and camera tokens are concatenated or element-wisely added before feeding into the main branch and the ControlNet branch. The visual-camera tokens are further processed with sequence of transformer blocks and mapped to the same dimension of added noise Figure A4. Structure of Latent Large Reconstruction Model (LaLRM). Given video latent and the camera embeddings p, the LaLRM directly regresses the 3DGS features in feed-forward manner. by unpatchify module. Note that in Fig. A3, we ignore the text tokens, diffusion time embeddings, and positional embeddings for simplicity. In Fig. A4, we show the details of the Latent Large Reconstruction Model (LaLRM). Given the video latents which can be generated from the video diffusion model (during inference time) or can be embedded from source video clips (during training time), lightweight tokenization module projects video latents to visual tokens. The camera embeddings are also projected to camera tokens with 3D-patchify operations. The two sets of tokens are merged with channel-wise concatenation and fed into sequence of transformer blocks to regress the Gaussian points features. D.2. Training Details In the camera-guided video diffusion model, we use transformer-based video diffusion model (i.e., CogVideoX5B-I2V [52] that generates 49 frames with resolution as 480 720). An encoder from 3DVAE is used to compress video clips with ratios of rt = 4 and rs = 8, giving latents with dimension as 136090. To build ControlNet branch, we use the first = 21 base transformer blocks from the video model to initialize the weights. The camera-LoRA has low rank of dimension 256. The model is trained with batch size of 24 for 40K steps, using Adam optimizer [9] with learning rate of 2 105, β1 = 0.9, β2 = 0.95, and weight decay = 1 104. For the latent-based large reconstruction model, we use patch size pl = 2 for visual latent and use temporal patch size 4 and spatial patch size 16 for camera Plucker embedding. We follow [56] and use the same architecture for transformer blocks. we use 24 base transformer blocks with hidden dimension of 1024. The latent decoding module has 3D-DeConv layer with upsampling strides (4, 16, 16). The backbone transformer network is implemented efficiently with FlashAttentionV2 [8] and optimized with mixed-precision training [31] with BF16 datatype. We first pre-train the model with low-resolution video clips with dimensions 49 240 360 and their corresponding latents with dimensions 13 30 45. Then, we fine-tune the model with high-resolution video clips with dimensions as 49 480 720 and the corresponding latents with the dimension as 13 60 90. At this stage, we modify 3D-DeConv layer in latent decoding module with upsampling strides (4, 8, 8). Even with smaller upsampling rate, for each 3D scene, our Gaussian prediction brings us 2 an enormous quality of Gaussians points, i.e., 2 (4,233,600), to construct each scene. total of = 48 supervision views are used, for which we randomly select = 24 frames from each sampled video clip as seen views, and an additional 24 frames disjoint from the video clip as unseen views. The model is pre-trained and finetuned respectively for 200K and 100K iterations with cosine annealing schedule at peak learning rate of 4 104 and 1 105. We use batch size of 24 with Adam optimizer β1 = 0.9, β2 = 0.95 and weight decay = 1 104. E. Limitations and Future Work While our method achieves superior generation performance and higher efficiency compared to prior works, there are still some limitations. Although the overall process is efficient, the inference speed of the video generation model remains bottleneck. Most of the computational time in our pipeline is consumed during the video generation phase. This limitation could be improved through parallel computing, e.g., using xDiT1 for parallel inference, or utilizing more efficient denoising strategy. Also, our approach primarily focuses on static scenes. In the future, we aim to extend this pipeline to dynamic scenes, exploring its potential for generating 4D content that incorporates temporal dynamics. By addressing these limitations, our framework could be further optimized for broader applications and enhanced performance. 1https://github.com/xdit-project/xDiT"
        }
    ],
    "affiliations": [
        "Snap Inc.",
        "University of California, Los Angeles",
        "University of Toronto"
    ]
}