{
    "paper_title": "From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors",
    "authors": [
        "Liangbing Zhao",
        "Le Zhuo",
        "Sayak Paul",
        "Hongsheng Li",
        "Mohamed Elhoseiny"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models."
        },
        {
            "title": "Start",
            "content": "From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors Liangbing Zhao 1 Le Zhuo 2 3 Sayak Paul 4 Hongsheng Li 2 Mohamed Elhoseiny 1 6 2 0 2 5 2 ] . [ 1 8 7 7 1 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Instruction-based image editing has achieved in semantic alignment, remarkable success yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as discrete mapping between image pairs, which provides only boundary transition dynamics conditions and leaves underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an endto-end framework equipped with textual-visual dual-thinking mechanism. It combines frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledgegrounded editing, setting new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models. All code, checkpoints, and datasets are available at https://liangbingzhao.github.io/statics2dynamics/. 1. Introduction Instruction-based image editing, which aims to generate new image following the given instruction, enables users 1KAUST 2CUHK MMLab 3Krea AI 4Huggingface. Correspondence to: Hongsheng Li <hsli@ee.cuhk.edu.hk>, Mohamed Elhoseiny <mohamed.elhoseiny@kaust.edu.sa>. Preprint. February 26, 2026. Figure 1. Bridging semantic alignment and physical plausibility. (Top) Despite high semantic fidelity, existing editing models frequently violate physical principles. (Bottom) Traditional image editing treats editing as black box, learning discrete mapping with underspecified constraints. Our approach reformulates editing as Physical State Transition, leveraging continuous dynamics to constrain the state transition space from unreal hallucinations to physically valid trajectories. to do visual creation much more easily. As user demands evolve from simple style transfer or object replacement to more complex scenarios involving hypothetical or counterfactual changes, the focus has shifted toward reasoningbased editing (Huang et al., 2024; He et al., 2025; Zhuo et al., 2025b). This demand has contributed to the emergence of unified multi-modal models (UMMs), such as Bagel (Deng et al., 2025) and Nano Banana (Google, 2025), which leverage the inherent textual reasoning capabilities of Multimodal Large Language Model (MLLMs) to bridge the gap between abstract user intent and visual execution. While these unified frameworks have established new backbone for image editing, their reasoning modules operate primarily at the semantic level rather than physical causality. Consequently, despite high semantic fidelity, state-of-the-art models frequently hallucinate artifacts that violate fundamental physical principles, as they prioritize object matching over physical plausibility. This drawback becomes particularly apparent in scenarios governed by strict physical interactions. Consider common scenario: inserting straw into transparent glass of water. As shown in Figure 1, while existing models can correctly Submission and Formatting Instructions for ICML 2026 identify the object (straw) and the location (in the glass of water), they frequently fail to render the optical refraction phenomenon, where the straw should appear disjointed or bent at the water surface. Instead, they tend to generate naive straws position, maintaining geometric rigidity while violating optical physical laws. This discrepancy reveals fundamental limitation: current models maximize semantic alignment at the cost of physical plausibility. To bridge this gap, we argue that the next frontier in visual creation lies in physics-aware image editing: paradigm where generated content must rigidly adhere to the causal rules of the physical world. To achieve this goal, we propose fundamental shift in problem formulation: the editing process should not be viewed as static mapping between independent images, but rather as predictive physical state transition. Under this formulation, the source image represents an initial state, and the edit instruction specifies an external interaction or trigger that drives the scene toward subsequent state under physical laws. central challenge is that standard paired images supervision in image editing provides only boundary conditions, leaving the transition itself underspecified. By contrast, temporal sequences show intermediate evidence of how states evolve, making video natural source of supervision for learning transition priors. Therefore, we construct PhysicTran38K, large-scale video-based dataset tailored to physical state transitions. Unlike existing benchmarks that predominantly emphasize semantic operations (e.g., add/remove/replace), PhysicTran38K is organized around interaction-driven triggers and law-governed transitions. We design hierarchical physics categories covering five major physical domains, 16 intermediate sub-domains, and spanning 46 distinct transition types. By employing two-stage filtering pipeline, we finally get and annotate 38,000 highquality transition data, providing explicit supervision for how physical states evolve over time. However, leveraging video data introduces practical mismatch: while videos supervise training, intermediate states are unavailable during inference. We address this train-test discrepancy by introducing PhysicEdit, physics-aware editing framework built on Qwen-Image-Edit (Wu et al., 2025a) that learns from video trajectories while remaining compatible with the single-image inference workflow. We propose textual-visual dual-thinking mechanism that decouples physical understanding into two branches. First, physicallygrounded reasoning branch uses frozen Qwen2.5-VL7B (Bai et al., 2025) to produce structured physical constraints as textual context. Second, an implicit visual thinking branch introduces learnable transition queries that are trained to learn transition priors from video. Concretely, intermediate keyframes provide supervision through two complementary encoders (DINOv2 (Oquab et al., 2023) for structural semantics and VAE (Wu et al., 2025a) for finegrained appearance), and the transition queries are aligned to these structureand texture-level targets via dual projection heads. Finally, we align this guidance with diffusions coarse-to-fine generation through timestep-aware modulation strategy that emphasizes structure at high noise and texture details at low noise. In summary, our contributions are as follows: We propose PhysicEdit, an end-to-end physics-aware editing framework with textual-visual dual-thinking mechanism. It combines physically-grounded reasoning with implicit visual thinking to leverage transition priors learned from videos, enabling physically faithful edits. We construct PhysicTran38K, large-scale videobased dataset of approximately 38k video-instruction pairs organized by hierarchical physics categories. Extensive experiments demonstrate that PhysicEdit achieves state-of-the-art performance among evaluated open-source models, while performing comparably to leading proprietary models, establishing strong baseline for physics-aware image editing. 2. Related Works Benefit from powerful diffusion models (Ho et al., 2020; Song et al., 2021; BAKR et al.) in generating high-fidelity images, instruction-based image editing has made rapid progress. Early diffusion-based editors (Hertz et al., 2022; Zhao et al., 2023) typically manipulate cross-attention or invert latents to trade off content preservation and edit strength, but often lack fine-grained controllability under complex structural changes. This motivates instruction-tuned approaches (Brooks et al., 2023; Labs, 2024; Wei et al., 2024; Zhuo et al., 2025a;b) and, more recently, unified multimodal models for generalized instruction alignment. representative example is Qwen-Image-Edit (Wu et al., 2025a), which conditions an MMDiT (Esser et al., 2024) on frozen Qwen2.5-VL (Bai et al., 2025) multimodal representations to replace standard text embeddings. In parallel, video data has been explored as prior for improving editing. Earlier works (Deng et al., 2025; Xiao et al., 2024; Chen et al., 2025) construct training pairs from video keyframes to enhance consistency, whereas recent efforts (Wu et al., 2025c; Rotstein et al., 2025) shift toward generative reasoning. Notably, the concurrent work ChronoEdit (Wu et al., 2025c) explicitly synthesizes intermediate frames as reasoning steps but incurs substantial computation and potential error accumulation. In contrast, we adopt an implicit paradigm that distills physical state transition priors into compact latent queries, enabling efficient feature-space dynamics simulation while inheriting physical fidelity from video data. more detailed discussion of related work is provided in Appendix C. 2 Submission and Formatting Instructions for ICML 2026 Figure 2. Overview of the PhysicTran38K construction pipeline. Starting from hierarchical physics categories, we synthesize videos using Wan2.2-T2V-A14B, filtered by ViPE with an adaptive strategy to preserve high-dynamic transitions. Candidate videos conduct principle-driven verification by GPT-5-mini, adhering to rigorous retention rule. Finally, Qwen2.5-VL-7B performs constraint-aware annotation, generating instructions and structured reasoning while incorporating verification results to prevent hallucinations. 3. Method 3.1. Problem Formulation To bridge the gap between semantic alignment and physical fidelity, we first formalize the task of physics-aware image editing. Conventionally, instruction-based image editing is modeled as discrete conditional mapping. Given source image Isrc and an editing instruction Tedit, the model approximates function F: Itgt = F(Isrc, Tedit), (1) where the goal is to generate target image Itgt that follows Tedit while preserving relevant content from Isrc. While effective for semantic edits (e.g., changing dog to cat), this formulation treats the transformation as black-box pixel update and does not explicitly model the underlying dynamics that govern how the scene should evolve. In this work, we treat physics-aware editing not as static mapping, but as Physical State Transition. Let Isrc represent the initial physical state S0 of scene, and Tedit represent an external force or interaction trigger ( e.g., drop the glass). The editing process is effectively simulation of the time-evolution of the system under physical laws Ω ( e.g., gravity, fluid dynamics): Sf inal = S0 + (cid:90) τ 0 Φ(St, Tedit; Ω) dt, (2) where Φ denotes the state transition dynamics and τ is the duration of the interaction. The desired target image Itgt is the visual outcome of the accumulated state Sf inal. The fundamental challenge arises because standard image editing dataset provide only the boundary conditions, i.e., (Isrc, Itgt), leaving the transition dynamics Φ completely underspecified. As illustrated in Figure 1, for given pair (Isrc, Tedit), there may exist multiple visually plausible endpoints, yet only subset results from valid physical trajectory under Ω. Without constraints on the intermediate integral, models tend to violate specific physical laws that satisfy the instruction. We therefore leverage videos as supervision, since they expose intermediate states and directly constrain how states evolve over time, motivating our construction of PhysicTran38K (Section 3.2). During inference, however, intermediate states are unavailable, so we propose PhysicEdit (Section 3.3) to distill these transition priors from videos into latent representation, effectively guiding the generation along physically valid trajectory. 3 Submission and Formatting Instructions for ICML 2026 Figure 3. Overview of the PhysicEdit framework. (a) Training: We distill physical transition priors from video data into learnable transition queries. These queries are supervised by complementary visual features extracted from intermediate keyframes. (b) Inference: PhysicEdit follows sequential workflow. The frozen MLLM first generates physically-grounded reasoning, which is then concatenated with the learned transition queries to serve as the condition for the diffusion backbone. 3.2. Physics-Driven Data Construction We construct PhysicTran38K, video-based dataset for physics-aware image editing, by casting editing as physical state transitions. As illustrated in Figure 2, our data construction pipeline is designed to transform hierarchical physics categories into high-quality video data through three stages: structured generation with video generation model, camera movement and principle-driven verification, and constraint-aware reasoning generation. We provide all the system prompts in Appendix G. Hierarchical Physics Categories. We begin by establishing comprehensive physics taxonomy to ensure broad coverage of physical dynamics. As shown in Figure 2(a), we organize physical laws into five primary state domains: Mechanical, Thermal, Material, Optical, and Biological. Under these domains, we define 16 intermediate sub-domains and 46 transition types (e.g., refraction, melting, germination), which serve as the specific physical laws Ω governing our dataset. For each transition, we curate specialized object pools containing entities that naturally exhibit the corresponding physical changes. Structured Generation. Leveraging the transition categories, we synthesize videos utilizing structured generation pipeline. We utilize GPT-5-mini to sample objects from the object pool and to instantiate fixed Wan Prompt template used for video generation: [Start State] + [Trigger Event] + [Transition Description] + [Final State]. Using Wan2.2T2V-A14B (Wan et al., 2025), we generate 1,000 raw videos per transition type. We include static-camera constraint in the prompt so that pixel variations primarily reflect state {St} transition rather than viewpoint changes. As shown in Figure 2(b), this pipeline successfully synthesizes diverse high-fidelity transitions across all defined physical domains. Camera Movement and Principle-driven Verification. As illustrated in Figure 2(c), raw generations undergo twostage filtering process to ensure both viewpoint stability and physical correctness. First, we notice that current text-tovideo models instruction following ability is not perfect, leading to unwanted viewpoint shifts. We therefore apply ViPE (Huang et al., 2025) as geometric stability filter. In practice, ViPE can confuse large non-rigid deformations with viewpoint changes. To reduce false rejections, we adopt an adaptive strategy that relaxes the ViPE threshold for transition types expected to induce substantial deformation. Next, we verify whether the video is consistent with its intended physical laws. Given the Wan Prompt and transition type, we prompt GPT-5-mini to propose (typically 3) transition-specific principles and classify them as align, contradict, or unknown based on the keyframes. We then calculate verification score Sverify and adopt rigorous retention rule, preserving the video only if Sverify = Nalign/Ntotal 0.5. Crucially, rather than simply discarding flawed parts, we record contradicted principles as negative evidence to be used as constraints during the reasoning generation phase. Constraint-Aware Reasoning Generation. Finally, we convert each retained video into training supervision that matches our formulation in Section 3.1. From each clip, we extract the first and last frames to form an editing pair (Isrc, Itgt) We also sample intermediate keyframes along the video trajectory to serve as the ground truth for our 4 Submission and Formatting Instructions for ICML 2026 visual guidance mechanism in Section 3.3. We then use Qwen2.5VL-7B (Bai et al., 2025) to generate an editing instruction Tedit describing the trigger event and brief desired evolution, along with structured transition reasoning that summarizes observable evidence of the initial state S0, middle transition process S1...ST 1, and the final state ST . To ensure text-visual consistency, we integrate the verification outcomes as logical constraints during annotation: principles classified as align are encouraged as mechanisms consistent with Ω, while contradicted or unknown principles are treated as hard negative constraints and must be explicitly excluded. This constraint-aware annotation mitigates over-optimistic physical descriptions when samples contain artifacts, allowing us to retain more usable videos and yielding approximately 38,000 samples. 3.3. PhysicEdit Framework We present PhysicEdit, an end-to-end physics-aware editing framework built upon the Qwen-Image-Edit backbone. To bridge the gap between static image editing and dynamic physical laws, we instantiate textual-visual dual-thinking mechanism with two complementary branches: physicallygrounded reasoning, which uses frozen Qwen2.5-VL-7B to provide explicit physical constraints as textual context, and implicit visual thinking, which introduces learnable transition queries to implicitly reconstruct transition priors in the latent space. Physically-Grounded Reasoning. This branch produces detailed physics reasoning that equips the following editing with explicit constraints before visual generation. Given the source image and the editing instruction, the frozen Qwen2.5-VL generates structured physics reasoning trace that describes: (i) what physical laws and constraints must be respected, (ii) how the change should unfold causally in the scene, and (iii) how the relevant materials should behave throughout the transition. During training, we use the precomputed constraint-aware reasoning annotations from PhysicTran38K and ask Qwen2.5-VL to generate the reasoning trace on the fly during inference. This step serves as contextual grounding for the subsequent generation, while requiring no parameter updates to the MLLM. Implicit Visual Thinking. While text reasoning provides logical constraints, precise physical rendering requires visual transition priors that are not explicitly observable from the input image or text reasoning. To avoid modeling entire video frames which is cumbersome, we introduce Implicit Visual Thinking, as illustrated in Figure 3. Inspired by MetaQuery (Pan et al., 2025), we append (typically 64) learnable transition queries immediately after the reasoning prefix. These queries are processed by the frozen Qwen2.5-VL together with the text and source image, producing context-aware query embeddings. Dur5 ing training, we supervise these embeddings with teacher branch derived from PhysicTran38K. We extract intermediate keyframes from the corresponding video, encode them using two complementary visual encoders (DINOv2 for semantic structure and the Qwen-Image-Edit VAE for finegrained texture), and compress the resulting features into fixed-length latent features via feature extractors. We then map the transition-query embeddings through dual projection heads and align them to the structureand texture-level In this way, the transition queries targets, respectively. learn to implicitly represent the missing evolution between Isrc and Itgt, so that at inference time they can be instantiated solely from the source image, the instruction, and the physically-grounded reasoning. More details please refer to Appendix B.2 Timestep-Aware Dynamic Modulation. Since diffusion models exhibit coarse-to-fine generative trajectory, where global structure is formed before local texture is refined, we dynamically mix the compressed DINO latent feature FDINO and VAE latent feature FVAE according to the diffusion timestep. Let [0, 1] denote the diffusion timestep, where larger corresponds to higher noise. We define the mixed transition feature as: (t) tran = FDINO + (1 t) FVAE. (3) When 1, the modulation prioritizes FDINO to enforce valid physical structure; when 0, it shifts toward FVAE to refine texture details. The fused feature (t) tran is then injected into the diffusion model alongside the reasoning condition to guide the physics-aware generation. End-to-End Optimization. We jointly optimize the learnable transition queries, feature extractors, dual-stream projection heads, and the diffusion backbone. The training objective is formulated as composite loss function: Ltotal = Ldiff + αLtran, (4) where Ldiff is the standard flow-matching loss applied to the diffusion backbone. The transition loss Ltran is designed to distill knowledge from the video frames into the transition queries in timestep-aware manner: Ltran = (cid:13) (cid:13)FDINO ˆFDINO (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 + (cid:0)1 t(cid:1) (cid:13) (cid:13)FVAE ˆFVAE (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 , (5) where ˆFDINO and ˆFVAE denote the pseudo ground-truth embeddings extracted from video frames by the frozen encoders. We set α = 1 in all experiments. Notably, the diffusion loss Ldiff updates only the diffusion transformer and the feature extractors, while the transition queries and their projection heads are updated exclusively by the transition loss Ltran. Therefore, this enforces disentangled gradient update for learning how to compress and predict visual features independently. Submission and Formatting Instructions for ICML 2026 Table 1. Quantitative comparison on PICABench-Superficial evaluated by GPT-5 for instruction-based editing models, where LP, LSE, RFL, RFR, DFM, CSL, GST, LST denote Light propagation, Light Source Effects, Reflection, Refraction, Deformation, Causality, Global State Transition, Local State Transition, respectively."
        },
        {
            "title": "Models",
            "content": "LP LSE RFL RFR DFM CSL GST LST Overall Nano Banana GPT-Image-1 Seedream 4.0 Nano Banana Pro GPT-Image-1.5 Bagel Bagel-Think OmniGen2 Hidream-E1.1 Step1X-Edit Flux.1 Kontext Uni-CoT ChronoEdit Qwen-Image-Edit PhysicEdit 60.29 61.26 62.71 60.53 62.95 46.97 49.88 49.64 49.15 45.04 54.96 54.56 59.92 62.95 64.88 59.30 66.04 65.50 70.62 73.15 39.35 50.40 48.79 48.25 47.44 57.41 59.82 67.48 61.19 76."
        },
        {
            "title": "Proprietary Models",
            "content": "53.95 59.21 53.51 57.02 62.28 59.90 59.66 59.17 64.79 65.53 Open-Source Models 42.54 43.42 39.04 46.49 34.21 36.40 55.12 56.18 55.26 62.22 44.25 49.88 44.74 44.50 45.72 51.83 49.12 56. 48.66 60.76 65.94 62.39 65.77 70.32 68.13 49.41 47.05 56.49 49.07 53.46 57.50 49.01 57.94 62.90 67.72 55.27 52.88 53.45 58.65 58.51 39.24 38.68 39.80 40.51 42.90 38.12 48.79 51. 48.95 59.23 60.60 70.75 65.12 72.74 74.39 46.80 45.70 51.10 56.40 55.85 48.79 60.98 64.14 67.33 67.67 59.88 59.04 66.11 70.27 71.52 49.27 50.94 39.09 40.33 46.57 47.61 51.70 55. 54.89 60.52 59.87 61.08 61.91 66.16 67.05 45.07 46.48 46.79 47.90 48.23 48.93 53.56 58.67 61.26 64.86 4. Experiments 4.1. Experimental Setup Implementation Details. We build PhysicEdit on top of the Qwen-Image-Edit backbone and fine-tune it on PhysicTran38K using LoRA (Hu et al., 2022). Unless otherwise specified, we run LoRA fine-tuning for single epoch with learning rate 5 105, batch size 1 per GPU, and LoRA rank 128. All experiments are conducted on 4 NVIDIA A100 GPUs, and training takes approximately 12 hours under this setup. For further details, refer to Appendix B. Evaluation. We evaluate PhysicEdit on two benchmarks, PICABench (Pu et al., 2025) and KRISBench (Wu et al., 2025d), which probe complementary aspects of instructionbased image editing: physical realism and knowledgegrounded reasoning. PICABench focuses on whether edits produce physically realistic effects beyond instruction following, covering optics, mechanics, and state transitions, which directly matches our goal of physics-consistent state evolution. KRISBench serves as diagnostic benchmark that categorizes editing tasks into factual, conceptual, and procedural knowledge types; in particular, its conceptual-knowledge tasks and the temporal-perception subset within factual knowledge are closely related to our physical-transition setting. Accordingly, we expect improvements to concentrate on these KRISBench categories, while other categories may change minimally. 6 Comparison Baselines. We extensively evaluate our method against diverse set of state-of-the-art proprietary and open-source image editing models. For proprietary models, we include Nano Banana, Nano Banana Pro (Google, 2025), GPT-Image-1 (OpenAI, 2025a), GPTImage-1.5 (OpenAI, 2025b), Seedream 4.0 (ByteDance, 2025b), Gemini 2.0 Flash (Kampf & Brichtova, 2025), Doubao (ByteDance, 2025a), and Step 3o vision (stepfun, 2025). For open-source models, we compare against leading instruction-based methods, including Flux.1 Kontext (Labs et al., 2025). For recent advanced unified models, we include BAGEL (Deng et al., 2025) (and its reasoning variant BAGEL-Think), OmniGen2 (Wu et al., 2025b), HidreamE1.1 (Cai et al., 2025), Step1X-Edit (Liu et al., 2025), UniCoT (Qin et al., 2025), and our backbone model QwenImage-Edit (Wu et al., 2025a). We also include two concurrent works: ChronoEdit (Wu et al., 2025c), which addresses image editing by leveraging video generation models, and EditThinker (Li et al., 2025a), which trains specialized reasoning MLLM to critique generated results and refine instructions for the backbone model. 4.2. Main Results Performance on Physical Realism. As shown in Table 1, PhysicEdit achieves an overall score of 64.86, establishing new state-of-the-art among open-source models. Compared to the baseline Qwen-Image-Edit-2509, PhysicEdit improves all physical dimensions, supporting the effectiveness of our textual-visual dual-stream mechanism. Noteably, Submission and Formatting Instructions for ICML 2026 Table 2. Quantitative comparisons on KRIS. indicates applying EditThinker on Qwen-Image-Edit. 0.0 indicates that the model was not evaluated on multi-image editing. Models Doubao Step 3o vision Gemini-2.0 GPT-4o FLUX.1 Kontext [Dev] OmniGen2 BAGEL BAGEL-Think Uni-CoT ChronoEdit EditThinker Qwen-Image-Edit PhysicEdit Factual Knowledge Conceptual Knowledge Procedural Knowledge Attribute Temporal Average Social Natural Average Logical Perception Perception Perception Score Science Science Score Reasoning Decompose Score Spatial Overall Instruction Average Score 70.92 69.67 66.33 83.17 64.83 59.92 64.27 67.42 72.76 79.22 78.48 75.48 77.64 59.17 61.08 63.33 79.08 60.92 52.25 62.42 68.33 72.87 73.08 73.83 80.58 81. Proprietary Models 63.30 66.70 65.26 79.80 65.50 66.88 68.19 85.50 61.19 60.88 56.94 80.06 Open-Source Models 53.28 57.36 60.26 66.18 71.85 78.18 77. 76.00 78.29 48.94 47.56 55.40 63.55 70.81 74.20 76.20 65.78 74.49 50.81 43.12 56.01 61.40 66.00 71.49 70.69 59.75 71.57 40.58 63.25 63.92 68. 0.00 54.75 42.45 58.67 67.10 81.22 0.00 71.73 76.13 62.23 62.32 59.65 81.37 50.36 44.20 55.86 61.92 67.16 72.14 72.02 61.22 72.27 47.75 49.06 54.13 71. 46.06 32.50 52.54 48.12 53.43 57.97 65.23 52.25 59.02 60.58 54.92 71.67 85.08 39.00 63.08 50.56 50.22 73.93 61.17 66.89 71.76 70.81 54.17 51.99 62.90 78. 42.53 47.79 51.69 49.02 63.68 59.35 65.94 60.61 64.06 60.70 61.43 62.41 80.09 49.54 49.71 56.21 60.18 68.00 70.96 71.91 65.56 72.16 the largest gains occur in categories requiring implicit dynamics: Light Source Effects increases from 61.19 to 76.16, Deformation rises by 12.0 points to 60.76, and Causality improves from 48.95 to 59.23. PhysicEdit also yields consistent improvements on Refraction and Local State Transition, indicating stronger adherence to global optical constraints and fine-grained state transition. Performance on Physics-Related Knowledge. Table 2 presents the evaluation results on KRISBench. PhysicEdit achieves an overall score of 72.16, surpassing all opensource baselines and outperforming proprietary models such as Gemini-2.0 and Doubao. Consistent with our formulation of editing as state transition, the improvements are clearly concentrated in categories that demand dynamic physical understanding. In the Factual Knowledge domain, the Temporal Perception score improves from 71.73 to 76.13, suggesting that our video-based training has endowed the model with stronger sense of time evolution. Furthermore, within the Conceptual Knowledge domain, the Natural Science score improves by 11.9 points to 71.57. This validates that the physically-grounded reasoning stream effectively activates the models latent scientific knowledge, enabling it to handle edits governed by strict natural laws rather than relying solely on surface-level visual patterns. Qualitative Analysis. To demonstrate the physical fidelity of PhysicEdit, we visualize representative cases from PICABench in Figure 4. These examples span all the physical domains of PICABench, including Optics, Mechanics, and State Transitions. For example, in the Turn off the lamp case, our model correctly renders the global illumination decay and shadow propagation, whereas comparison models Table 3. Ablation Study on physically-grounded reasoning and implicit visual thinking effectiveness."
        },
        {
            "title": "Mechanics Optics State Overall",
            "content": "Qwen-Image-Edit + SFT + Physical Reasoning + Visual Thinking PhysicEdit 55.04 53.43 57.43 52.38 59.79 64.29 62.85 61.26 66.33 63.33 61.79 64.15 64.21 62.31 68.05 64.05 62.41 68.19 65.10 64.86 Table 4. Ablation Study on DINO and VAE feature effectiveness."
        },
        {
            "title": "Only DINO\nOnly VAE\nHard Switching\nPhysicEdit",
            "content": "54.73 58.31 57.39 59.79 66.61 70.16 57.50 65.45 66.23 60.40 65.43 68.84 62.08 68.19 67.67 60.52 63.00 63.05 63.52 64.86 often struggle to maintain lighting consistency or merely darken the image globally without geometric correctness. These results support our quantitative findings, confirming that our physics-aware training enables the model to generate edits that are not only semantically aligned but also visually natural and physically law-grounded. Further detailed results are shown in Appendix and Appendix F. 4.3. Ablation Studies To validate our proposed components, we conduct comprehensive ablation studies on the PICABench dataset. Following PICABenchs rule, we group LP, LSE, RFL, and RFR as Optics; DFM and CSL as Mechanics; GST and LST as State. The results are shown in Table 3 and Table 4. 7 Submission and Formatting Instructions for ICML 2026 Figure 4. Qualitative comparison on PICABench. We visualize editing results across diverse physical domains, including Optics, Mechanics, Global State, and Local State. Compared to the backbone Qwen-Image-Edit and proprietary models, PhysicEdit consistently generates more physically plausible and visually natural results, avoiding the physical inconsistencies observed in baseline methods. Effectiveness of Textual-Visual Dual-Stream Thinking. We first investigate the necessity of our architectural design by comparing it against standard supervised fine-tuning baseline. As shown in Table 3, simply fine-tuning QwenImage-Edit on our dataset yields only marginal gain and exposes trade-off. Optics increases, whereas Mechanics deteriorates, indicating that data-only supervision is insufficient to reliably induce abstract physical constraints. Incorporating Physically-Grounded Reasoning significantly boosts the Mechanics score to 57.43, confirming that explicit textual planning is essential for enforcing logical physical constraints. However, Optics changes only marginally, suggesting limited effectiveness for optics-dominant effects when relying on textual constraints alone. In contrast, using only Implicit Visual Thinking markedly strengthens Optics (68.05), implying that learnable transition queries provide effective visual guidance; yet Mechanics drops to 52.38 in the absence of textual constraints, reflecting weakened logical consistency. The full PhysicEdit achieves the highest performance across all metrics, demonstrating that the two streams are not redundant but deeply complementary: the textual stream ensures logical plausibility, while the visual stream handles the rendering of dynamic state transitions. Effectiveness of Implicit Visual Thinking Design. We further analyze the architectural design of our implicit visual thinking module, with emphasis on whether both DINOv2 and VAE features are necessary. Table 4 compares our dualstream approach against single-stream variants, where we split State into GST and LST for more detailed analysis. Using DINO features alone yields the highest GST of 70.16, confirming its structural strength, yet it suffers in Mechanics due to insensitivity to local deformations. Conversely, VAE features improve texture-related LST to 60.40 but lack global coherence. To notice, our model performs slightly worse on GST, which reflects necessary trade-off: we relax DINOs excessive structural rigidity to enable substantial physical deformations, thereby achieving superior overall physical fidelity. We then study the effectiveness of our fusion strategy by comparing it with Hard Switching baseline, which rigidly utilizes DINO for the structural formation stage (t [1, 0.7]) and abruptly switches to VAE for texture refinement (t < 0.7). While this strategy outperforms single-stream variants, it still falls short of our full model. We attribute this to the continuous nature of the diffusion process, where hard cutoff introduces semantic discontinuity in the guidance signal. In contrast, PhysicEdit employs Timestep-Aware Modulation to smoothly interpolate between structural and textural guidance. This alignment with the inherent coarseto-fine trajectory of diffusion enables the model to attain the best overall performance of 64.86. We provide more analyses and discussions with both quantitative and qualitative results in Appendix E. 5. Conclusion We presented paradigm shift in image editing by modeling the process as continuous Physical State Transition. We introduce PhysicTran38K, video-based dataset curated with principle-driven verification and constraint-aware annotation, and propose PhysicEdit, which combines physically grounded reasoning with latent transition queries to leverage video supervision while preserving standard single-image inference. Our method significantly outperforms existing baselines in physical plausibility, validating the effectiveness of transition-centric supervision. This work underscores the importance of physical laws in visual generation and provides robust framework for future physics-aware generation research. 8 Submission and Formatting Instructions for ICML"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning, specifically in enhancing the physical plausibility of generative editing. Our framework, PhysicEdit, and the constructed dataset, PhysicTran38K, significantly improve the visual realism of edited content by enforcing adherence to physical laws. While this advancement offers substantial benefits for creative industries, virtual prototyping, and education, we acknowledge that the increased realism of manipulated images could potentially be misused to create misleading content or misinformation that is harder to distinguish from reality. We advocate for the responsible use of this technology and encourage future research into detection methods that can identify synthetic physical inconsistencies. There are no other specific societal consequences that we feel must be highlighted here."
        },
        {
            "title": "References",
            "content": "Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., Zhong, H., Zhu, Y., Yang, M., Li, Z., Wan, J., Wang, P., Ding, W., Fu, Z., Xu, Y., Ye, J., Zhang, X., Xie, T., Cheng, Z., Zhang, H., Yang, Z., Xu, H., and Lin, J. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. BAKR, E. M., Zhao, L., Hu, V. T., Cord, M., Perez, P., and Elhoseiny, M. Toddlerdiffusion: Interactive structured image generation with cascaded schrodinger bridge. In The Thirteenth International Conference on Learning Representations. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1839218402, 2023. ByteDance. Doubao: Bytedances ai chat assistant. https://www.doubao.com/chat/, 2025a. Accessed: 2025-05-08. ByteDance. Seedream 4.0. 2025b. URL https://seed. bytedance.com/en/seedream4_0/. Cai, Q., Chen, J., Chen, Y., Li, Y., Long, F., Pan, Y., Qiu, Z., Zhang, Y., Gao, F., Xu, P., et al. Hidream-i1: highefficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Chen, X., Zhang, Z., Zhang, H., Zhou, Y., Kim, S. Y., Liu, Q., Li, Y., Zhang, J., Zhao, N., Wang, Y., et al. Unireal: Universal image generation and editing via learning realworld dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1250112511, 2025. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution In Proceedings of the International image synthesis. Conference on Machine Learning (ICML), 2024. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Google. Nano banana. 2025. URL https://gemini. google/overview/image-generation/. He, Q., Chen, X., Wang, C., Pan, Y., Hu, X., Gan, Z., Wang, Y., Wang, C., Li, X., and Zhang, J. Reasoning to edit: Hypothetical instruction-based image editing with visual reasoning. arXiv preprint arXiv:2507.01908, 2025. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., and Cohen-Or, D. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 2020. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. Proceedings of the International Conference on Learning Representations (ICLR), 2022. Huang, J., Zhou, Q., Rabeti, H., Korovko, A., Ling, H., Ren, X., Shen, T., Gao, J., Slepichev, D., Lin, C.-H., et al. Vipe: Video pose engine for 3d geometric perception. arXiv preprint arXiv:2508.10934, 2025. Huang, Y., Xie, L., Wang, X., Yuan, Z., Cun, X., Ge, Y., Zhou, J., Dong, C., Huang, R., Zhang, R., et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 83628371, 2024. Kampf, K. and Brichtova, N. Experiment with gemini 2.0 flash native image generation, march 2025. URL https://developers. googleblog. com/en/experiment-withgemini-20-flash-native-image-generation/. Accessed, pp. 0508, 2025. 9 Submission and Formatting Instructions for ICML 2026 Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., Kulal, S., Lacey, K., Levi, Y., Li, C., Lorenz, D., Muller, J., Podell, D., Rombach, R., Saini, H., Sauer, A., and Smith, L. Flux.1 kontext: Flow matching for incontext image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Li, H., Zhang, M., Zheng, D., Guo, Z., Jia, Y., Feng, K., Yu, H., Liu, Y., Feng, Y., Pei, P., et al. Editthinker: Unlocking iterative reasoning for any image editor. arXiv preprint arXiv:2512.05965, 2025a. Li, Z., Liu, Z., Zhang, Q., Lin, B., Wu, F., Yuan, S., Yan, Z., Ye, Y., Yu, W., Niu, Y., et al. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025b. Liu, S., Han, Y., Xing, P., Yin, F., Wang, R., Cheng, W., Liao, J., Wang, Y., Fu, H., Han, C., et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. OpenAI. Gpt-image-1. 2025a."
        },
        {
            "title": "URL",
            "content": "https://openai.com/index/ introducing-4o-image-generation/. OpenAI. Gpt-image-1.5. 2025b."
        },
        {
            "title": "URL",
            "content": "https://openai.com/index/ new-chatgpt-images-is-here/. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Pan, X., Shukla, S. N., Singh, A., Zhao, Z., Mishra, S. K., Wang, J., Xu, Z., Chen, J., Li, K., Juefei-Xu, F., et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Pu, Y., Zhuo, L., Han, S., Xing, J., Zhu, K., Cao, S., Fu, B., Liu, S., Li, H., Qiao, Y., et al. Picabench: How far are we from physically realistic image editing? arXiv preprint arXiv:2510.17681, 2025. Qin, L., Gong, J., Sun, Y., Li, T., Yang, M., Yang, X., Qu, C., Tan, Z., and Li, H. Uni-cot: Towards unified chain-ofthought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. Rotstein, N., Yona, G., Silver, D., Velich, R., Bensaid, D., and Kimmel, R. Pathways on the image manifold: Image editing via video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 78577866, 2025. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. stepfun. Step-3o-vision: Stepfuns ai chat assistant. https: //www.stepfun.com/chats/new/, 2025. Accessed: 2025-07-24. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models, 2025. URL https: //arxiv.org/abs/2503.20314. Wei, C., Xiong, Z., Ren, W., Du, X., Zhang, G., and Chen, W. Omniedit: Building image editing generalist models In Proceedings of the through specialist supervision. International Conference on Learning Representations (ICLR), 2024. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., Yin, S.-m., Bai, S., Xu, X., Chen, Y., et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Wu, C., Zheng, P., Yan, R., Xiao, S., Luo, X., Wang, Y., Li, W., Jiang, X., Liu, Y., Zhou, J., et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Wu, J. Z., Ren, X., Shen, T., Cao, T., He, K., Lu, Y., Gao, R., Xie, E., Lan, S., Alvarez, J. M., et al. Chronoedit: Towards temporal reasoning for image editing and world simulation. arXiv preprint arXiv:2510.04290, 2025c. Wu, Y., Li, Z., Hu, X., Ye, X., Zeng, X., Yu, G., Zhu, W., Schiele, B., Yang, M.-H., and Yang, X. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025d. Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Submission and Formatting Instructions for ICML 2026 Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Li, C., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1329413304, 2025. Zhao, L., Zhang, Z., Nie, X., Liu, L., and Liu, S. Crossattention and seamless replacement of latent prompts for high-definition image-driven video editing. Electronics, 13(1):7, 2023. Zhuo, L., Han, S., Pu, Y., Qiu, B., Paul, S., Liao, Y., Liu, Y., Shao, J., Chen, X., Liu, S., et al. Factuality matters: When image generation and editing meet structured visuals. arXiv preprint arXiv:2510.05091, 2025a. Zhuo, L., Zhao, L., Paul, S., Liao, Y., Zhang, R., Xin, Y., Gao, P., Elhoseiny, M., and Li, H. From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1532915339, 2025b. 11 A. Detailed Dataset Statistics and Taxonomy Submission and Formatting Instructions for ICML In this section, we provide granular breakdown of the PhysicTran38K taxonomy and the distribution of the collected video data. A.1. Statistical Overview As summarized in Table 5, our dataset is constructed based on three-level hierarchy: 5 primary physical domains, 16 intermediate sub-domains, and 46 distinct Transition Types. After our rigorous filtering pipeline, we retained total of 38,620 high-quality video-instruction pairs. The distribution across domains is relatively balanced, ensuring that the model learns diverse set of physical laws ranging from rigid body mechanics to biological growth. Table 5. Detailed statistics of PhysicTran38K across five primary physical domains. The video counts represent the final data used for training after filtering."
        },
        {
            "title": "Primary Domain",
            "content": "Sub-domains"
        },
        {
            "title": "Total",
            "content": "3 5 2 3 3 16 12 12 8 8 6 46 10,245 10,242 6,602 6,245 5,286 38, A.2. Hierarchical Taxonomy Definition To ensure the coverage of fundamental physical interactions, we define specific intermediate sub-domains for each primary category. The complete taxonomy is organized as follows: 1. Mechanical State. This domain covers the kinematics and dynamics of rigid and non-rigid bodies. It is further divided into 3 sub-domains: Positional: Changes in spatial location or orientation driven by external forces. Transitions: Translation, Rotation, Oscillation, Gravity, Buoyancy. Motion: Changes in velocity or momentum patterns. Transitions: Motion Direction Change. Structural: Physical alterations to the objects structure. Transitions: Compression, Tension, Bending, Torsion, Fracture, Collapse. 2. Biological State. This domain focuses on the life processes of organic entities. It comprises 5 sub-domains: Vital: Fundamental signs of life or death. Transitions: Alive to Dead. Growth: Expansion or development over time. Transitions: Decay, Fruit Ripening, Germination, Mature to Flower, Seedling to Mature. Functional: Active behaviors or movements specific to living organisms. Transitions: Active to Dormant, Dormant to Active. Health: Indications of well-being or sickness. Transitions: Disease to Health, Health to Disease. Surface Biological: External biological changes. Transitions: Mold Growth, Moss Algae Growth. 12 Submission and Formatting Instructions for ICML 3. Optical State. This domain captures how light interacts with matter. It includes 3 sub-domains: Light Interaction: Modifications to the radiometric, spectral, and spatial properties of light sources. Transitions: Light Color Change, Light Direction Change, Light Intensity Change, Light Temperature Change. Transparency: Changes in opacity or translucency. Transitions: Clear to Translucent. Light Path Change: Geometric optics phenomena. Transitions: Light Reflection, Light Refraction. 4. Thermal State. This domain governs thermodynamic processes and heat transfer. It consists of 2 sub-domains: Temperature: Visible effects of heating or cooling. Transitions: Cooling, Heating. Phase: Transitions between solid, liquid, and gas states. Transitions: Condensation, Deposition, Evaporation, Freezing, Melting, Sublimation. 5. Material State. This domain describes the intrinsic physical properties of matter. It is categorized into 3 sub-domains: Integrity: The structural wholeness and physical continuity of the material volume. Transitions: Intact to Broken. Surface Condition: Degradation or physical alteration of the materials exterior layer due to wear or exposure. Transitions: Coating Peeling, Abrasion, Weathering. Mechanical Property: Variations in the materials rigidity, compliance, and resistance to deformation. Transitions: Hardening, Softening. We provide some detailed data illustrations in Figure 5 and Figure 6. B. Model Setup Details B.1. Model Specifications and Assets To ensure the reproducibility of our experiments and facilitate future research, we detail the specific versions and source URLs of all foundation models and codebases employed in our framework. Codebase and Backbone. Our implementation is developed based on DiffSynth Studio1, high-efficiency diffusion framework. Specifically, our PhysicEdit is built upon the Qwen-Image-Edit-25092. We use this version as the initialization for our diffusion backbone and keep it frozen during the visual encoder training phase to preserve its original text-following capabilities. Data Construction Model. For the data construction pipeline, specifically in the generation of prompts, transition principles, and verification steps, we utilize the GPT-5-mini3. This model serves as the core reasoning engine for ensuring the physical correctness of our collected transitions. Visual Encoders. Our implicit visual thinking module leverages two distinct encoders to capture complementary information. For structural semantics, we employ the DINOv2-Base with Registers4, which provides robust geometric and layout priors. For fine-grained texture, we utilize the specific VAE component5 from the Qwen-Image-Edit repository to ensure precise alignment with the backbones latent space. 1https://github.com/modelscope/DiffSynth-Studio 2https://huggingface.co/Qwen/Qwen-Image-Edit-2509 3https://platform.openai.com/docs/models/gpt-5-mini 4https://huggingface.co/facebook/dinov2-with-registers-base 5https://huggingface.co/Qwen/Qwen-Image-Edit-2509/tree/main/vae Submission and Formatting Instructions for ICML 2026 Figure 5. More detailed illustrations on Mechanical, Biological and Thermal data. 14 Submission and Formatting Instructions for ICML 2026 Figure 6. More detailed illustrations on Optical and Material data. B.2. Implementation Details of Implicit Visual Thinking In this section, we provide further details regarding the architectural implementation and training strategies of the Implicit Visual Thinking module. Transition Query Injection and Parameterization. The transition queries are implemented as sequence of learnable special tokens. During the forward pass, these tokens are concatenated to the input sequence immediately following the edit instruction and the generated physically-grounded reasoning text. The combined sequence is then processed by the frozen Qwen2.5-VL, allowing the transition queries to attend to both the visual context of the source image and the textual reasoning cues. Crucially, these queries serve as global learnable parameters shared across all data samples. While we explored the possibility of using domain-specific query banks (e.g., maintaining separate sets of queries for Optical vs. Mechanical transitions), empirical results indicated that such segregation led to overfitting on the training distribution and reduced generalization on external benchmarks. We believe that shared global parameter set forces the model to learn more abstract and universal physical representations, thereby enhancing its robustness to diverse and unseen physical scenarios. Video Supervision Strategy. To distill physical priors from the video data, we employ uniform sampling strategy to select = 6 intermediate keyframes from each video clip. Since our PhysicTran38K is constructed via rigorous principle-driven generation pipeline, the physical state transitions are distributed consistently throughout the video duration. 15 Submission and Formatting Instructions for ICML 2026 Consequently, the physical information contained in the video is rich and temporally balanced. Our preliminary experiments suggested that simple uniform sampling is sufficient to capture these dynamics, and more complex sampling strategies (e.g., motion-based or attention-based selection) yielded negligible performance gains in our setting. Feature Extraction and Transition Delta. To provide explicit supervision for the learnable transition queries, our feature extractors are designed to capture dynamic evolution rather than static image content. Specifically, given intermediate video frames, we first extract their raw visual representations using the frozen DINOv2 and VAE encoders. To align these high-dimensional spatial features with our 1D transition queries, we employ Perceiver Resampler-like mechanism, which compresses the encoder outputs into fixed-length sequence of tokens. Crucially, since our objective is to learn the change in physical states, we process the source image through this exact same pipeline to obtain baseline latent representation. The final supervision signal is then formulated as the residual difference between the compressed features of the intermediate frame and those of the source image. By explicitly targeting this latent delta, we force the transition queries to focus exclusively on the physical dynamics and transformations, effectively bypassing the redundant reconstruction of static background information. C. Detailed Related Works C.1. Instruction-Based Image Editing The field of image editing has witnessed paradigm shift from domain-specific Generative Adversarial Networks (Goodfellow et al., 2020) to high-fidelity Diffusion Models (Ho et al., 2020; Song et al., 2021; BAKR et al.). Early diffusion-based approaches (Hertz et al., 2022; Zhao et al., 2023) primarily manipulate cross-attention maps or invert latents to balance content preservation with editing strength, yet they often lacked precise controllability for complex structural changes. This limitation catalyzed the emergence of instruction-tuned methods (Brooks et al., 2023; Labs, 2024; Wei et al., 2024; Zhuo et al., 2025b) which explicitly learn the mapping between input images and textual instructions. Recent advancements have culminated in unified multimodal frameworks that integrate vision and language modalities for robust instruction alignment. representative model is Qwen-Image-Edit (Wu et al., 2025a), which features dual-stream architecture composing frozen Qwen2.5-VL (Bai et al., 2025) and Multi-Modal Diffusion Transformer (MMDiT) (Esser et al., 2024). In this design, the MLLM-encoded representations replace standard text embeddings to serve as the condition input for the MMDiT. Despite their remarkable success in semantic manipulation, current paradigms prioritize perceptual quality while largely neglecting fundamental physical laws. C.2. Video Prior for Image Editing Recent research has increasingly explored leveraging video data to enhance image editing. While early attempts (Deng et al., 2025; Xiao et al., 2024; Chen et al., 2025) primarily constructed training pairs from video keyframes to enhance consistency, recent works (Wu et al., 2025c; Rotstein et al., 2025) have shifted towards generative reasoning paradigm. Notably, the concurrent work ChronoEdit (Wu et al., 2025c) adapts video generation models to explicitly synthesize intermediate frames as reasoning steps for editing. However, such explicit pixel-level generation is computationally demanding and prone to error accumulation. Distinct from these approaches, we propose an implicit paradigm: instead of generating full video frames, we distill physical state transition laws into compact latent queries. This allows our model to simulate dynamics flexibly and efficiently in the feature space, bypassing the heavy burden of explicit video synthesis while retaining the physical fidelity derived from temporal data. D. General Editing and Reasoning Editing Results Performance on General Image Editing. To ensure that acquiring physical priors does not compromise the fundamental semantic capabilities of the backbone, we evaluate PhysicEdit on standard general image editing benchmarks, namely ImgEdit-Bench and GEdit-Bench-EN. As detailed in Table 6, PhysicEdit not only maintains but consistently improves upon the performance of the base Qwen-Image-Edit model. Specifically, the overall score on ImgEdit-Bench increases from 4.35 to 4.40, and the overall score (G O) on GEdit-Bench-EN rises from 7.56 to 7.87. It demonstrates robust performance across diverse fundamental operations, including addition, replacement, and style transfer, remaining highly competitive among state-of-the-art open-source models. This confirms that our transition-centric fine-tuning strategy effectively injects dynamic physical knowledge without causing catastrophic forgetting of the backbones original text-alignment and semantic editing proficiency. 16 Submission and Formatting Instructions for ICML 2026 Table 6. Comparison results on general image editing, including ImgEdit-Bench and GEdit-Bench-EN. indicates applying EditThinker on Qwen-Image-Edit. All use GPT-4.1 for evaluation. Model Add Adjust Extract Replace Remove Background Style Hybrid Action Overall SC PQ ImgEdit-Bench GEdit-Bench-EN GPT-4o (OpenAI, 2025a) 4.61 4. 2.9 OmniGen (Xiao et al., 2025) Step1X-Edit (Liu et al., 2025) BAGEL (Deng et al., 2025) OmniGen2 (Wu et al., 2025b) FLUX.1 Kontext [Dev] (Labs et al., 2025) EditThinker (Li et al., 2025a) UniWorld-V2 (Li et al., 2025b) Qwen-Image-Edit PhysicEdit 3.47 3.88 3.56 3.57 3.76 4.23 4.29 4.32 4.36 3.04 3.14 3.31 3.06 3.45 4.43 4. 4.36 4.32 1.71 1.76 1.70 1.77 2.15 4.24 4.32 4.04 3.95 Proprietary Models 4.35 3.66 Open-source Models 2.94 3.40 3.30 3.74 3.98 4.20 4. 2.43 2.41 2.62 3.20 2.94 4.21 4.72 4.64 4.71 4.52 4.57 4.57 4.93 3. 4.89 4.20 7.74 8.13 7.49 3.21 3.16 3.24 3.57 3.78 4.44 4. 4.37 4.35 4.19 4.63 4.49 4.81 4.38 4.76 4.91 4.84 4.88 2.24 2.64 2.38 2.52 2.96 3.91 3.83 3.39 3.54 3.38 2.52 4.17 4.68 4.26 4.68 4. 4.71 4.91 2.96 3.06 3.20 3.44 3.52 4.40 4.49 4.35 4.40 5.96 7.66 7.36 7.16 6.52 8.30 8.39 8.00 8.68 5.89 7.35 6.83 6.77 7.38 7.86 8. 7.86 7.54 5.06 6.97 6.52 6.41 6.00 7.73 7.83 7.56 7.87 Table 7. Comparison of model performance on RISE-Bench. indicates applying EditThinker on Qwen-Image-Edit. Model Temporal Causal Spatial Logical Overall Proprietary Models Seedream-4.0 GPT-Image-1 Gemini-2.5-Flash-Image 12.9 34.1 25.9 12.2 32.2 47.8 Open-source Models Step1X-Edit Ovis-U1 FLUX.1-Kontext-Dev BAGEL Qwen-Image-Edit BAGEL-Think EditThinker Qwen-Image-Edit PhysicEdit 0.0 1.2 2.3 2.4 4.7 5.9 10. 4.7 21.2 2.2 3.3 5.5 5.6 10.0 17.8 23.3 10.0 23.3 11.0 37.0 37.0 2.0 4.0 13.0 14.0 17.0 21.0 27.0 17.0 25. 7.1 10.6 18.8 3.5 2.4 1.2 1.2 2.4 1.2 8.2 2.4 3.5 10.8 28.9 32.8 1.9 2.8 5.8 6.1 8.9 11.9 17.8 8.9 18. Performance on Reasoning-Based Editing. We further assess our model on RISE-Bench, challenging benchmark designed to evaluate complex reasoning capabilities in editing tasks. As shown in Table 7, PhysicEdit achieves substantial breakthrough, more than doubling the overall score of the base Qwen-Image-Edit from 8.9 to 18.6. Consistent with our physical state transition formulation, the most dramatic improvements are observed in dimensions that strictly demand dynamic and logical understanding: the Temporal score improves from 4.7 to 21.2, and the Causal score improves from 10.0 to 23.3. Notably, PhysicEdit attains the highest overall performance among all evaluated open-source models. These results validate that our textual-visual dual-thinking mechanism successfully equips the model with deep causal and temporal reasoning abilities, significantly narrowing the gap with leading proprietary models in complex physical scenarios. E. Analysis and Discussion Table 8. Comparison between explicit and implicit visual thinking. Table 9. Comparison between different data construction logic."
        },
        {
            "title": "PICA KRIS",
            "content": "ChronoEdit 58.67 +SFT PhysicTran38K 61.43 64.86 PhysicEdit 70.96 69.59 72.16 Qwen-Image-Edit 61.26 +SFT PICA-100K 62.96 64.86 PhysicEdit 65.56 66.00 72.16 17 Submission and Formatting Instructions for ICML Figure 7. Comparison with explicit visual thinking (ChronoEdit). ChronoEdit employs an explicit paradigm that synthesizes intermediate video frames to model the editing process. As visualized in the rollouts, this pixel-level generation suffers from severe error accumulation over long horizons, causing the final results to degenerate (e.g., background distortion or object disappearance). In contrast, by encoding dynamics into latent transition queries (Implicit Visual Thinking), our method avoids these synthesis errors, maintaining structural integrity and yielding precise, high-fidelity edits. Explicit vs. Implicit Visual Thinking. We further compare PhysicEdit with ChronoEdit (Wu et al., 2025c), concurrent approach that follows an explicit visual thinking paradigm by synthesizing intermediate frames to represent transition trajectories. Such frame rollout is computationally intensive and can accumulate errors over long horizons (Figure 7). In contrast, PhysicEdit adopts implicit visual thinking by encoding transition dynamics into latent transition queries, avoiding pixel-level intermediate-frame synthesis at inference. As shown in Table 8, PhysicEdit is better than ChronoEdit on both benchmarks, increasing PICABench from 58.67 to 64.86 and KRISBench from 70.96 to 72.16. To disentangle the effect of supervision from modeling choice, we fine-tune ChronoEdit on PhysicTran38K. This raises its PICABench score to 61.43 but reduces its KRISBench score to 69.59, suggesting trade-off when adapting an explicit trajectory-based pipeline with transition-centric fine-tuning. PhysicEdit mitigates this trade-off by design: transition-specific learning is localized in lightweight transition queries and adapters while keeping the VLM frozen, which helps incorporate physical priors without compromising the backbones broad generalization capabilities. Principle Acquisition vs. Instruction Overfitting. critical question is whether the improvements stem from learning fundamental physical laws or merely overfitting to the distribution of editing instructions. To investigate this, we compare our data construction strategy against PICA-100K, dataset specifically curated for the PICABench benchmark. As shown in Table 9, when fine-tuning the base Qwen-Image-Edit on PICA-100K, the improvements are marginal: the PICABench score rises slightly from 61.26 to 62.96, and the KRISBench score sees negligible increase from 65.56 to 66.00. This suggests that the PICA-100K construction primarily teaches the model to mimic surface-level editing patterns rather than internalizing underlying dynamics. In contrast, training on PhysicTran38K with our PhysicEdit framework boosts performance substantially to 64.86 on PICABench and 72.16 on KRISBench. This confirms that our hierarchical, 18 Submission and Formatting Instructions for ICML principle-driven data construction enables true Principle Acquisition. By exposing the model to pure state transitions (e.g., optical paths, material deformations) rather than just editing pairs, the model internalizes the physics of change, allowing it to generalize to the diverse and complex scenarios presented in knowledge-grounded benchmarks like KRIS. F. More Qualitative Results Figures 8, 9, and 10 present more detailed comparison results on PICABench. Figure 8. More qualitative results on PICABench, showing light propogation related results. 19 Submission and Formatting Instructions for ICML Figure 9. More qualitative results on PICABench, showing light source effects results. 20 Submission and Formatting Instructions for ICML 2026 Figure 10. More qualitative results on PICABench, showing refraction results. 21 Submission and Formatting Instructions for ICML G. System Prompts We provide all the system prompts used throughout our work. Listing 1. System prompt used for generating physics-grounded video instructions. Template variables {EXTRA CONSTRAINTS} and {SUBJECT POOL} are populated at runtime. You are domain-agnostic instruction generator for learning and evaluating physical laws via data. Your job is to produce cohesive paragraphs (a \"detailed instruction\") for video generative model (wan2.2) given only: (1) State and (2) exactly one State Transition (which may include driver/condition such as gravity, buoyancy, heating, cooling, catalyst, electric/magnetic field, phase change, biological growth, etc.). No other hints will be provided. CAMERA POSE CONSTRAINT (critical): The camera position and orientation are fixed across frames; do not describe or imply any change in camera pose (no pan/tilt/roll/dolly/truck/orbit). Other aspects ( exposure, zoom, focus, lighting, background activity, etc.) may vary if desired. PER-TRANSITION OVERRIDES (optional, edit per run): Semantics: HARD: must be obeyed. REQUIRE: evidence/details that must appear. FORBID: items that must not appear. SOFT: preferences; follow if feasible. EXTRA CONSTRAINTS: {EXTRA_CONSTRAINTS} SUBJECT POOL CONSTRAINT (mandatory this run): Use ONLY the following ten canonical subject names as the primary subjects for this batch (one per paragraph, all distinct; no repeats): {SUBJECT_POOL}. You may add neutral descriptors (material, color, size) and safe, inert props or environments that make the transition legible; the subjects identity must remain the same canonical class. PHYSICS COMPLIANCE (mandatory): The chosen subject + scene + Motion/Change must obey ordinary physical laws for the given State + single Transition; do not rely on hidden mechanisms or impossible behavior. Provide visible or inferable cause consistent with the Transition (e.g., deflector contact, slope/roughness change, buoyancy, magnetic field), not an additional Transition. Avoid anti-physical cues such as: magnetic alignment of nonmagnetic items without visible ferromagnetic element; rigid bending without hinges /elasticity; ripples/splashes with no force or entry; instantaneous velocity changes with no interaction; frictionless deflection on rough surfaces; sealed containers exchanging matter without port; light-only \"forces\" moving massive objects. SUBJECT SUBSTITUTION (escape hatch, only if needed): If any pool subject cannot yield physically plausible depiction of the specified Transition--even after reasonable scene/prop choices--replace that subject with ONE new, non-branded everyday subject that is compatible. This replacement may be outside the pool. Keep ten unique subjects across the batch (no duplicates). Prefer replacements that are semantically related to the original class (e.g., swap smooth plastic disc for marked gear or hinged lid for rotation). CLARIFICATIONS: It is acceptable to make the subject compatible by adding explicit enablers that do not constitute new Transition (e.g., hinge/pivot/spindle for rotation; incline or rough patch for deceleration; deflector edge or crossflow for trajectory change; visible ferromagnetic insert for magnetic alignment). Do not \"solve\" physics by moving the camera; camera pose remains fixed as specified elsewhere in this prompt. Output requirements: - Produce EXACTLY TEN paragraphs in English, 70-120 words (2-4 sentences). - The paragraph must implicitly include five elements without labels or section markers: Subject, Scene, Process Dynamics (Motion/Change), Aesthetic Control, Stylization. - Subject: choose single, ordinary, photographable subject that naturally exhibits the given transition. - Scene: choose an environment/medium that makes the transition observable (surfaces, containers, supports, lighting, background material). 22 Submission and Formatting Instructions for ICML 2026 - Process Dynamics: describe ONLY the single provided transition in present/ongoing form , emphasizing mid-transition, observable evidence (e.g., trajectory blur, droplets/ beads, color shift, cracking, bubbles rising, filaments forming, iron filings aligning, shadow displacement). - If driver/condition is inherent to the transition, mention it naturally (cause-> effect) without using field names. - If the phenomenon requires unusual scale/time, briefly indicate capture method ( macro, microscopy, high-speed, time-lapse, or CG) to keep it physically plausible. - Aesthetic Control: weave in composition, viewpoint, focal length feel, lighting, depth of field, shutter/temporal feel (or render equivalents), and material/texture cues-- integrated into prose, not bullet points. - Stylization: state an overall treatment (e.g., photo-realistic, cinematic, macroscientific, illustration, CG render) with restrained descriptor of intensity; do NOT use numerical style scores. - Physical/causal consistency is mandatory: the described evidence, environment, and driver must align; avoid mutually exclusive signals in the same local region unless you justify them with spatial separation or temperature/field gradient. - Safety & scope: omit recipes, concentrations, step-by-step experimental procedures, or dangerous instructions; keep biological processes non-invasive and ethically neutral; avoid privacy, copyrighted characters, text overlays, arrows, or formulas in the scene. - No exaggerated or non-physical \"VFX\"/cinematic effects: avoid shockwaves or energy flares without cause, teleport jumps, glitch trails, time-scrubs/reversals/freezeframes, gravity-defying debris or liquids, oversized particle/splash blooms, or volumetric beams/lens artifacts without plausible sources. Any visible effect must be physically plausible, small-scale consequence directly caused by the single Transition and the described scene, and consistent with what real fixed-pose camera could capture at the stated shutter/aperture/lighting. - Formatting: output ONLY the paragraph text--no titles, labels, bullet lists, JSON, quotes, code fences, or extra commentary. - Do NOT print any labels or section markers in the output. BAN these (case-insensitive) when they appear as tokens before the first period or at the start of any sentence: \"subject:\", \"scene:\", \"motion:\", \"process dynamics:\", \"aesthetic control:\", \" stylization:\", \"hard:\", \"require:\", \"forbid:\", \"soft:\", \"extra constraints:\", \"case\", \"pool:\". If your draft contains any of them, rewrite into plain prose before returning. Internal checklist for the model (do not print in the output): 1) Exactly one primary transition is described. 2) Mid-transition visual evidence is explicit. 3) Scene/medium makes the cause->effect legible. 4) At least one visual reference or comparator exists (container, surface, shadow, reflection, nearby object). 5) Capture method noted if scale/time demands it. 6) Aesthetic and stylization cues are present but not overstuffed. 7) No unsafe procedures or data-like recipes appear. 8) Final output is exactly one paragraph in English with no labels or scaffolding. Listing 2. System prompt for generating physics-grounded principles. The model outputs structured JSON containing verifiable principles for evaluating video frame consistency. You produce an INSTRUCTION PLAN of physical/biological rules (principles) to verify later against video frames. INPUTS: - prompt: free-form generation text (may include subjects/setting/actions). - transition_name: the named transition to be depicted (e.g., melting, decay, fracture). - state_category: high-level bucket such as \"Biological State\", \"Mechanical State\", \" Thermal State\", etc. - max_principles: hard cap for number of principles to output. EVIDENCE & CONTENT POLICY: - Build principles from commonly accepted, domain-general knowledge consistent with transition_name and state_category. - Use nouns that appear in the prompt for context_subjects (bare names, no adjectives). 23 Submission and Formatting Instructions for ICML 2026 If none are explicit, leave empty. - DO NOT introduce new concrete objects, quantities, or specific materials not present in the prompt; use neutral placeholders (e.g., \"the subject\", \"material\", \"container \") when needed. - NO camera/film/style words. No speculation about lighting, lens, exposure, etc. - Keep rules verifiable from images/time ordering (i.e., rely on visually checkable cues ). OUTPUT STRICT JSON ONLY with this schema (no code fences, no extra text): { \"transition_name\": \"<as given>\", \"state_category\": \"<as given>\", \"context_subjects\": [\"<bare noun from prompt>\", \"...\"], \"principles\": [ { \"id\": \"snake_case_identifier\", \"type\": \"phase_changecontactconservationkinematicsfluidenergychemical biologicalfracturedeformationmixingdissolutiongrowthdecaycombustionadhesion diffusionosmosiselasticityother\", \"instruction\": \"plain, verifiable rule about what should happen/hold true, and description about what it would look like.\", \"visual_cues\": [\"short cue 1\",\"short cue 2\",\"...\"], \"required_contacts\": [\"subject touching container\",\"subject in fluid\"], \"ordering\": \"start->mid->end\", \"negations\": [\"instant jump with no intermediate\",\"solid reappears intact\"], \"priority\": \"highmediumlow\" } ], \"invariants\": [\"mass continuity (no sudden disappearance)\",\"no teleportation\",\"no interpenetration without deformation\"], \"assumptions\": [\"generic ambient conditions\"] } CONSTRAINTS: - principles: up to max_principles items (if knowledge is limited, still produce at least 3 general principles). - Every string should be concise, lowercase preferred unless proper name appears in prompt. - No mention of frames, camera, style, or these instructions. Listing 3. System prompt for the visual rule critic. The model evaluates whether extracted video frames support, contradict, or provide insufficient evidence for each physics principle. You are visual rule critic. You will receive: - KEY FRAMES (chronological stills from video), - list of HIGH-PRIORITY PRINCIPLES, each with {instruction, visual_cues}. Goal: For EACH principle, determine whether the frames visibly SUPPORT it, CONTRADICT it, or provide INSUFFICIENT evidence. Please think it in critical way. Evaluation rules: - Use only what is visible in the frames; ignore camera/film/style. - Prefer short, concrete evidence: frame indices and which listed visual_cues (if any) are visibly present. - \"supported\": frames show content consistent with the instruction; ideally at least one listed visual cue is visible. - \"contradicted\": frames show content opposite or incompatible with the instruction. - \"unknown\": cues not visible and frames insufficient to support or contradict. Output STRICT JSON ONLY: { \"rule_checks\": [ { \"id\": \"<string or index you set>\", 24 Submission and Formatting Instructions for ICML 2026 \"result\": \"supportedcontradictedunknown\", \"evidence_frames\": [<ints, chronological>], \"matched_cues\": [\"<subset of provided visual_cues>\"], \"comment\": \"<1 short sentence grounded in what is visible>\" } ], \"summary\": {\"supported\": <int>, \"contradicted\": <int>, \"unknown\": <int>} } Do not mention cameras or styles. Do not add new rules or extra text outside the JSON. Listing 4. System prompt for transforming video frames and physics principles into structured state-transition descriptions. The model generates detailed prompts for initial, intermediate, and final states while respecting supported principles and avoiding contradicted ones. You will transform ONE input text (prompt or caption) TOGETHER WITH: (1) chronological sequence of KEY FRAMES sampled from video and (2) two sets of physical principles: SUPPORTED_PRINCIPLES (aligned) and CONTRADICTED_PRINCIPLES (to avoid), into THREE DETAILED prompts. MAPPING FROM FRAMES TO OUTPUTS: - first_state_prompt: describe exactly what is visible in the FIRST key frame (initial state). - middle_transition_prompt: use the INTERMEDIATE key frames (2..N-1) in temporal order to describe the transition step-by-step; this section MUST be longer and more elaborate than the other two. Favor mechanisms and cues that are consistent with SUPPORTED_PRINCIPLES. - final_state_prompt: describe exactly what is visible in the LAST key frame (final state). EVIDENCE & PRECEDENCE: - Primary: visual evidence in the key frames. - Secondary: SUPPORTED_PRINCIPLES as positive constraints/mechanisms (use them only when consistent with what is visible). - Tertiary: the input text (prompt/caption) as additional context if it does not conflict with what is visible. - If any source conflicts with what is visible, FOLLOW THE FRAMES. - STRICTLY AVOID describing mechanisms or outcomes that match CONTRADICTED_PRINCIPLES. COMPLETENESS & OMISSION: - You MAY supplement missing but VISIBLE details from the key frames (components, contacts, local changes). - If detail is neither visible nor clearly stated/entailed by SUPPORTED_PRINCIPLES, OMIT it; DO NOT GUESS. - If mechanism is suggested by SUPPORTED_PRINCIPLES but NOT visible, describe it only at high level and only if it does not introduce new unobserved entities or attributes. STRICT CONTENT RULES: - No invention: do NOT introduce new objects, substances, tools, agents, causes, numbers , or attributes that are not visible in the frames or explicitly stated by the text or entailed by SUPPORTED_PRINCIPLES. - ABSOLUTELY NO camera/filming/shot/lens/motion/angle/stabilization/exposure/ISO/focus/ DoF/lighting-style words, and NO artistic style/medium descriptors (e.g., photorealistic, watercolor, cinematic, aesthetic). - Use only entities, materials, colors, shapes, counts, spatial relations, contact interfaces, conditions, and sequence details that are visible/stated/entailed; never contradict the frames. LEVEL OF DETAIL: - All three fields must be richly detailed (multi-clause paragraphs). Include parts/ components, spatial layout, relevant contacts/interfaces, and salient physical properties that are visible or 25 Submission and Formatting Instructions for ICML 2026 safely entailed. - The middle_transition_prompt should explain the mechanism of change using the intermediate frames: intermediate configurations, ordering, interactions (e.g., contact, mixing, dissolution, phase change, displacement, deformation), observable cues, and before->after indicators. Prefer cues supported by SUPPORTED_PRINCIPLES; avoid any content aligned with CONTRADICTED_PRINCIPLES. FORMATTING: - Return STRICT JSON ONLY: { \"first_state_prompt\": \"...\", \"middle_transition_prompt\": \"...\", \"final_state_prompt\": \"...\" } - Each field: paragraph (no bullet points), plain text, no quotes or code fences. - Do NOT mention \"prompt\", \"caption\", \"frame(s)\", \"image(s)\", \"principle(s)\", \"camera\", \"style\", or these instructions. QUALITY CHECKS before returning: - middle_transition_prompt is longer than the other two. - No invented content; no camera/style words; no mention of frames/images/principles. - No mechanisms or outcomes that match CONTRADICTED_PRINCIPLES."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "Huggingface",
        "KAUST",
        "Krea AI"
    ]
}