{
    "paper_title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models",
    "authors": [
        "Mu Cai",
        "Reuben Tan",
        "Jianrui Zhang",
        "Bocheng Zou",
        "Kai Zhang",
        "Feng Yao",
        "Fangrui Zhu",
        "Jing Gu",
        "Yiwu Zhong",
        "Yuzhang Shang",
        "Yao Dou",
        "Jaden Park",
        "Jianfeng Gao",
        "Yong Jae Lee",
        "Jianwei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 2 8 1 8 0 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TemporalBench: BENCHMARKING FINE-GRAINED TEMPORAL UNDERSTANDING FOR MULTIMODAL VIDEO MODELS Mu Cai1,, Reuben Tan2, Jianrui Zhang1, Bocheng Zou1, Kai Zhang3, Feng Yao4, Fangrui Zhu5, Jing Gu6, Yiwu Zhong7, Yuzhang Shang8, Yao Dou9, Jaden Park1, Jianfeng Gao2,, Yong Jae Lee1,, Jianwei Yang2, 1University of Wisconsin-Madison 3 Ohio State University 6 University of California, Santa Cruz 8 Illinois Institute of Technology 2Microsoft Research, Redmond 4 University of California, San Diego 5 Northeastern University 7 Chinese University of Hong Kong 9 Georgia Institute of Technology https://TemporalBench.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of 10K video question-answer pairs, derived from 2K high-quality human annotations detailing the temporal dynamics in video clips. As result, our benchmark provides unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-ofthe-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating significant gap ( 30%) between humans and AI in temporal understanding. Furthermore, we notice critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find centralized description as cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models temporal reasoning capabilities. Both dataset and evaluation code will be made available."
        },
        {
            "title": "INTRODUCTION",
            "content": "The ability to understand and reason about events in videos is crucial aspect of artificial intelligence, with applications ranging from activity recognition and long-term action anticipation to perception for autonomous driving and robotics. Recently, there has been an emergence of highly capable multimodal generative models, including proprietary ones such as GPT-4o (OpenAI, 2024) and Gemini (Gemini Team, 2024) as well as open-sources ones (Liu et al., 2023a; Zhu et al., 2024b; Bai et al., 2023), that have demonstrated impressive results on existing video benchmarks (Xu et al., 2016; Chen & Dolan, 2011; Yu et al., 2019a; Mangalam et al., 2024). However, these benchmarks often do not truly evaluate the abilities of the aforementioned models to understand video content due to their generally coarse-grained annotations. The lack of fine-grained temporal details in the annotations often leads to existing video understanding benchmarks suffering from strong language prior bias. This is similar to observations in visual Work done during the internship at Microsoft Research, Equal Advisory Contribution."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The tasks of TemporalBench. TemporalBench starts from fine-grained video descriptions and supports diverse video understanding tasks including video QA, video captioning, long video understanding, etc. It differs from existing benchmarks by the average number of words per video (middle top), word density (center) and the coverage of various temporal aspects (middle bottom). question answering with images (Antol et al., 2015). For example, prior works (Tan et al., 2024; Li et al., 2023a) show that language models such as Flan-T5 (Chung et al., 2024) and Llama-2/3 (Touvron et al., 2023) perform comparably to video models on EgoSchema (Mangalam et al., 2024) and SeedBench (Li et al., 2023a) without using any information from videos. Furthermore, the lack of fine-grained temporal details often results in the single frame bias of current video understanding benchmarks (Lei et al., 2023). These benchmarks are often biased toward spatial reasoning, where static information from single frame suffices to achieve high performance. They often fail to test models ability to reason about temporal sequences, leading to inflated evaluations of AI models that are not genuinely capable of understanding temporal events. Specifically, vision-language models (VLMs) (Liu et al., 2024a;b) that are trained on image-level datasets, including FreeVA (Wu, 2024), IG-VLM (Kim et al., 2024) and 3 (Cai et al., 2024b), often outperform their video counterparts on popular video question answering benchmarks such as MSRVTT (Xu et al., 2016), MSVD (Xu et al., 2017), and TGIF (Jang et al., 2017). To address this limitation, we propose TemporalBench (Figure 1), new video understanding benchmark that evaluates multimodal video models on understanding fine-grained activities, and consists of 10K question and answer pairs curated from 2K high-quality human-annotated captions with rich activity details. Unlike static image-based tasks, video understanding requires models to reason effectively about both spatial and temporal information. The temporal dynamics inherent in videos introduce significant complexity, as actions and events often unfold over time and cannot be captured in single frame. With this in mind, we designed our benchmark to focus on areas where current models often struggle, emphasizing annotations related to long-range dependencies, fine-grained visual observations, and event progression. As shown in Figure 2, we first collect video clips from existing video grounding benchmarks that span diverse domains, including procedural videos (Tang et al., 2019), human activities (Krishna et al., 2017; Gao et al., 2017), ego-centric videos (Grauman et al., 2024), movie descriptions (Rohrbach et al., 2015), professional gymnasium videos (FineGym from Shao et al. (2020)), and unexpected humor videos (Epstein et al., 2020). The positive captions include rich and fine-grained details about actions and activities, which are annotated by highly qualified Amazon Mechanical Turk (AMT) workers and authors of this paper. Then, we generate the negative captions with respect to the actions using powerful Large Language Models (LLMs) and filter them according to our defined rules. Our resulting TemporalBench contains 10K video descriptions and matching questions of high quality. Furthermore, the rich temporal context of annotations in our diverse corpus creates solid foundation for the development of additional benchmarks in related tasks such as spatio-temporal localization and causal inference. We hope that our benchmark can pave the road for further development of multimodal video models capable of fine-grained video understanding and reasoning."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of the annotation pipeline for TemporalBench. In step 1, we fist collect high-quality captions for the videos using qualified AMT annotators followed by refining them. In step 2, we leverage existing LLMs to generate negative captions by replacing select words and reordering the sequence of actions before filtering them ourselves. In contrast to existing video benchmarks, TemporalBench has the following defining characteristics: Emphasis on fine-grained action understanding. Due to the highly descriptive video captions, our negative captions highlight fine-grained temporal differences shown in Figure 3, such as sliced the ginger three times versus sliced the ginger twice, and put on the eyeglasses versus push the eyeglasses. Evaluations on both short (<20 seconds) and long (<20 minute) videos. Since the videos clips are sampled from existing videos, our benchmark can also support evaluations on long video understanding by concatenating the descriptions of multiple and non-overlapping video clips from the same source video. Extends to video captioning, video grounding, and video generation. Besides the task of video question answering, the nature of the positive captions in our benchmark allows it to seamlessly extend to evaluation of other tasks such as video temporal grounding and dense captioning. Evaluations of both video embedding and question-answering models. Given the annotated positive and negative captions in TemporalBench, it also supports the evaluation of discriminative and contrastive learning-based models such as XCLIP (Ni et al., 2022), ImageBind (Girdhar et al., 2023) as well as multimodal generative models such as GPT-4o and Gemini. Furthermore, we notice critical pitfall for multi-choice QA. If every negative answer choice is generated by changing small part of the correct answer, the LLM can detect those changes to find centralized description and use that cue for its prediction. Therefore, we propose Multiple Binary Accuracy (MBA) to correct such bias. Among other observations, our empirical evaluations show that state-of-the-art multimodal video models like GPT-4o only achieve an average accuracy of 38.5% on our benchmark (short videos) using our proposed multiple binary QA accuracy metric, compared to 67.9% obtained by humans. Models show even worse results on long videos. This result highlights that the aforementioned models are able to understand static visual concepts but are still limited in reasoning about the fine-grained temporal relationships of objects and events in videos. More significantly, we highlight critical issue with using LLMs to answer multi-choice QA."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Large Multimodal Models. Large Language Models (LLMs) like ChatGPT (OpenAI, 2023b), GPT4 (OpenAI, 2023c), and Llama (Touvron et al., 2023) have demonstrated impressive reasoning and generalization capabilities for text. The introduction of models that integrate visual data has brought about significant shift in the landscape of LLMs, such as GPT-4V(ision)(OpenAI, 2023a). Building upon open-source LLMs (Touvron et al., 2023; Chiang et al., 2023), wide range of multimodal models has achieved remarkable progress, led by pioneering models such as LLaVA (Liu et al., 2023a; 2024a) and MiniGPT-4 (Zhu et al., 2024b), which combine LLMs capabilities with CLIP (Radford et al., 2021) based image encoder. Recently, growing number of LMMs have been developed to handle wider range of tasks and modalities, such as region-level LMMs (Cai et al., 2024a; Zhang et al., 2023c; Chen et al., 2023; Peng et al., 2023; Zhang et al., 2023b), 3D LMMs (Hong et al., 2023), and video LMMs (Lin et al., 2023; Zhang et al., 2023a; 2024b). Multimodal Understanding Benchmarks. The recent significant advancements have resulted in more versatile multimodal models, making it imperative to thoroughly and extensively evaluate their visual understanding and reasoning abilities. Conventional multimodal benchmarks like VQA (Antol et al., 2015), GQA (Hudson & Manning, 2019) and VizWiz (Gurari et al., 2018) have been revitalized and used for evaluating the general visual question answering performance for LMMs. Some other question answering benchmarks like TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021) and InfoVQA (Mathew et al., 2022) have also been employed to validate the text-oriented understanding. Recent studies have introduced variety of new benchmarks, such as SEED-Bench (Li et al., 2023a), MMBench (Liu et al., 2023b) and MM-Vet (Yu et al., 2024b) for evaluating the models integrated problem-solving capabilities, and MMMU (Yue et al., 2024a) and MathVista (Lu et al., 2024) for scientific and mathematical reasoning. In addition, the commonly known hallucination problem also appears in LMMs, and is also investigated in POPE (Li et al., 2023b), MMHalBench (Sun et al., 2023) and Object HalBench (Yu et al., 2024a), etc. Video Understanding Benchmarks. Recently, an increasing amount of research is transitioning its focus from the image to the video domain. Videos differ from images in that they possess more complex content with temporal dynamics. This unique aspect calls for different set of metrics and benchmarks. Many efforts have leveraged existing video question answering benchmarks (Xu et al., 2017; Yu et al., 2019b; Xiao et al., 2021) built on top of video-text datasets (Chen & Dolan, 2011; Xu et al., 2016; Zhang et al., 2019). More recently, several LMM-oriented benchmarks have been proposed for different aspects such as long-form egocentric understanding with EgoSchema (Mangalam et al., 2024), and temporal understanding and ordering like Tempcompass (Liu et al., 2024c). MV-Bench (Li et al., 2024b) compiles existing video annotations from different disciplines into new benchmark, while Video-MME (Fu et al., 2024) and MMWorld (He et al., 2024b) claim to support comprehensive evaluation of video understanding and world modeling, respectively. Our TemporalBench serves the common goal of evaluating models for video understanding but differs in several aspects. On the one hand, we exhaustively curate videos from different domains and ask human annotators to annotate the visual contents with as much detail as possible. On the other hand, we particularly focus on temporal dynamics such as human actions and human-object interactions that exist exclusively in videos and which are crucial for video understanding, reasoning and forecasting. While the ShareGPT4Video dataset (Chen et al., 2024) also contains long captions, theirs differ from ours by being entirely generated by GPT-4o instead of annotated by humans."
        },
        {
            "title": "3 TemporalBench",
            "content": "Compared to static images, videos inherently contain significantly more fine-grained temporal information, as they capture the unfolding of actions and events over time. Existing multimodal video understanding benchmarks (Xu et al., 2016) mostly evaluate models coarse-level understanding of videos. An example from the recent Seed-Bench dataset is the question, What action is happening in the video? with the answer, moving something up. However, such types of coarse-level video questions have been demonstrated to be easily solved with just single frame (Wu, 2024) or even by text-only LLM (Tan et al., 2024; Mangalam et al., 2024). Such phenomena arises due to fundamental limitation in the text descriptions in those benchmarks. As result of their coarseness, the positive and negative options for video question-answering can usually be distinguished without understanding the temporal dynamics, such as the models only needing to choose between The man is cooking and The man is exercising."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Comparison of negative captions generated from the original captions and our detailed captions in TemporalBench. With fine-grained details, the negatives are more difficult and temporal centric. To address this limitation, we carefully design human annotation pipeline to curate highly detailed descriptions about the activities in the videos. Given the detailed video clip descriptions, such as right hand holds piece of peeled ginger while knife is held in the left and makes 3 slices off the ginger., the negative captions can be curated to truly reflect whether model understands the temporal dynamics, such as changing three slices into two slices. In nutshell, such highly detailed temporal annotations can be used to carefully examine whether multimodel video model truly understands the temporal state transition in videos. Our benchmark enriches several fundamental video understanding tasks due to its detailed captions: Fine-grained video question answering. Given detailed positive caption, multimodal video models need to distinguish it from the associated negative where slight modification is made to temporal descriptions, e.g., push the eyeglasses up versus pull the eyeglasses down, or cut 3 slices off versus cut 2 slices off. Fine-grained video captioning. Our detailed video captions can naturally enrich the video captioning task, different from current video captioning tasks such as MSRVTT (Xu et al., 2016) which focus on coarse-level descriptions. Long video understanding with fine-grained activity inspection. Since the video clips are extracted from long source video, the respective video clip descriptions can be concatenated to form longer video description which can be pivoted to the long video understanding task, where we find that all current multimodal video models suffer. Dense video-text matching and retrieval. Our detailed video captions can be naturally employed to evaluate video-language embedding models such as XCLIP (Ni et al., 2022). Given positive caption and several negative captions, we can evaluate whether CLIP (Radford et al., 2021) based video embedding models can distinguish the subtle differences in captions. In addition, given set of positive video-text pairs, video retrieval performance can be evaluated, similar to image retrieval on COCO (Lin et al., 2014) and Flickr30K (Young et al., 2014). Video grounding from detailed text descriptions. Since the video clips are cropped from the source video, with the documented starting and ending time, our benchmark can serve as finegrained moment localizing benchmark from text descriptions. This is different from existing video grounding datasets such as Charades-STA (Gao et al., 2017), COIN (Tang et al., 2019), Ego4D (Grauman et al., 2024) where the text descriptions are usually very short, possibly resulting in low temporal localization performance due to the vague and coarse descriptions. Text-to-Video (T2V) generation with detailed prompts. Given our highly detailed description, T2V generation model can be evaluated by verifying if the generated videos reflect the fine-grained action details. Next, we detail the dataset curation and evaluation setup for TemporalBench."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Video length distribution of (a) short video clips and (b) long videos in TemporalBench."
        },
        {
            "title": "3.1 VIDEO COLLECTION",
            "content": "We collect video clips from wide range of sources across diverse domains, where the majority comes from existing video grounding benchmarks. Our dataset includes wide spectrum of video types from seven sources, including (1) procedure videos e.g., COIN (Tang et al., 2019), (2) human activities e.g., ActivityNet-Captions (Yu et al., 2019a) and Charades (Krishna et al., 2017), (3) ego-centric videos e.g., EgoExo4D (Grauman et al., 2024), (4) movie descriptions (Rohrbach et al., 2015), (5) professional gymnasium videos e.g., FineGym (Shao et al., 2020), and (6) unexpected humor videos Oops (Epstein et al., 2020). We sample around 300 video clips from the validation and test sets of each video dataset, which results in 2K videos. The statistics of TemporalBench is shown in Table 1. We intentionally filter out video clips that (1) are mostly static by leveraging optical flow (Farneback, 2003), (2) contain multiple scene transitions by leveraging PySceneDetect 1 and (3) last longer than 20 seconds. We observe that the large amount of information in long videos make it difficult for annotators to provide detailed action descriptions. The distribution of video lengths is shown in Figure 4 (a). Additionally, we remove the audio from the videos during annotation to ensure that all informative signals come solely from the visual frames, preventing the answers from being influenced by the audio."
        },
        {
            "title": "3.2 VIDEO CAPTION ANNOTATION PROCESS",
            "content": "Positive Captions Annotation. We employ two-stage human labeling process for curating video captions with fine-grained activity descriptions, where the qualified Amazon Mechanical Turk (AMT) workers are first instructed to give detailed video caption. Then, the authors of this work refine the caption by correcting the mistakes and adding missing details w.r.t. the actions. The overall pipeline is shown in Figure 2. All video clips are annotated following the same pipeline except for Finegym (Shao et al., 2020) as it has already provided accurate and detailed action descriptions for professional gymnasium videos. Consequently, we reuse its annotations. We first use 3 probing video captioning questions with 2 in-context examples as the onboarding task for AMT master workers. We manually inspect the soundness and amount of temporal details of the AMT worker captions to select high quality AMT video captioning workers. During the annotation process by AMT workers, we also continue to remove the unqualified workers based on the ratio of the captions that authors in this paper refined. In this way, we ensure that the AMT provides high quality initial point for positive captions. Negative Caption Annotation. Our negative captions are aimed at confusing multimodal video models with respect to fine-grained activity details, such as changing cut ginger twice using knife to cut ginger three times using knife. We construct negatives upon two granularities: word level and event level. Specifically, word level negatives denote the case where certain word or phrase is replaced while event level negatives denote the case where the order of two events are reversed. Empirically, we find that LLMs can produce more creative and diverse negatives compared to AMT workers and authors. Therefore, we leverage three leading LLMs, GPT-4o (OpenAI, 2024), 1https://www.scenedetect.com/"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: An illustration of multi-choice QA with (a) original and (b) heuristics-guided negative captions. Orange blocks indicate the altered contents from the positive option (green box). Gemini-1.5-Pro (Gemini Team, 2024) and Llama-3.1-405b (Meta, 2024) to curate diverse set of negative caption candidates instructed by 3 in-context examples, with up to 9 negatives at word level and 6 negatives at event level. Afterwards, the authors of this work review those negative caption candidates in the format of multi-choice QA, which results in our complete TemporalBench dataset with 2K high-quality human-annotated video captions and 10K video question-answer pairs."
        },
        {
            "title": "3.3 A PITFALL IN MULTI-CHOICE QUESTION ANSWERING",
            "content": "A conventional approach to evaluate large multimodal models is using the multi-choice questionanswering format, which is adopted by the majority of current benchmarks including MMMU (Yue et al., 2024a), MathVista (Lu et al., 2024), EgoSchema (Mangalam et al., 2024) etc. However, indicated by recent studies by (Cai et al., 2024b) and (Yue et al., 2024b), pure LLM can achieve comparable or even stronger performance on those benchmarks without looking at the visual content at all. Recent studies argue that (1) some questions are not designed well so that the question can be answered without looking at the visual content, or (2) the model memorizes the QA pairs, i.e., data contamination occurs. While developing our benchmark, we notice another previously ignored but critical pitfall for multichoice QA. Specifically, if every negative answer choice is generated by changing small part of the correct answer, the LLM can detect those changes to find centralized description and use that cue for its prediction. To study this, given positive caption and its associated negative caption (C), we intentionally derive few negatives from N1(C) (instead of for C), resulting in N1(N1(C)) and N2(N1(C)), resulting in [C, N1(C), N1(N1(C)), N2(N1(C))] as options, so that N1(C) becomes the centralized description (see Fig. 5). Surprisingly, we find that 66.4% of text-only GPT-4os predictions correspond to (C), while only 6.4% of its predictions correspond to C. Our findings also align with human behavior analysis from psychology (Furman & Wang, 2008), where humans can achieve better than random chance performance on multi-choice QAs using similar cues. Motivated by this findings, we propose to decompose single multi-choice QA into multiple binary QAs. In this case, we eliminate the centralized option due to the fact that there are only two options to choose from. As result, given negatives, the multiple binary QAs will query model times, where the random chance performance changes from 1 +1 for every > 2, multiple binary QA is more difficult task than multi-choice QA. 2 )M . Given that ( 1 2 )M > 1 +1 to ("
        },
        {
            "title": "4.1 EXPERIMENT SETUP",
            "content": "We evaluate both (1) multimodal video text generation models, including GPT-4o (OpenAI, 2024), Gemini-1.5-Pro (Gemini Team, 2024), Claude-3.5-Sonnet (Anthropic, 2024), Qwen2VL (Wang et al., 2024), LLaVA-OneVision (Li et al., 2024a), LLaVA-Next-Video (Zhang et al., 2024b), Phi3.5-Vision (Abdin et al., 2024), MiniCPM-2.6 (Yao et al., 2024), MA-LMM (He et al., 2024a), VideoLLaVA (Lin et al., 2023), InternLM-Xcomposer-2.5 (Zhang et al., 2024a), Matryoshka Multimodal Models (M 3) (Cai et al., 2024b), and (2) multimodal video embedding models, including XCLIP (Ni et al., 2022), ImageBind (Girdhar et al., 2023), and LanguageBind (Zhu et al., 2024a). We exponentially increase the number of frames to study its effect on video understanding. More details can be found in Appendix D."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Model performance on TemporalBench with varying frames. To study the effect of single frame bias and text bias, we also evaluate models trained on single images, including LLaVA-1.5 (Liu et al., 2024a), LLaVA-NeXT (Liu et al., 2024b), and Phi-3V (Abdin et al., 2024). In the latter case, we evaluate the LLMs including GPT-4o (OpenAI, 2024), Gemini-1.5Pro (Gemini Team, 2024), Yi-34B (Young et al., 2024), Vicuna (Chiang et al., 2023) and Flan-T5 (Wei et al., 2021) without using videos at all."
        },
        {
            "title": "4.2 HUMAN PERFORMANCE",
            "content": "We use Amazon Mechanical Turk to evaluate human performance. Note that we exclude the positive caption annotators to ensure that there is no data contamination. Again, we use an onboarding test using held out binary video QA evaluation set which has clear answers. Next, we show the performance on each task."
        },
        {
            "title": "4.3 FINE-GRAINED VIDEO QUESTION ANSWERING ON SHORT VIDEOS",
            "content": "The results for multimodal generative models and embedding models are shown in Table 2 and Figure 7 (a). Note that we show the result with the best average multiple binary QA (MBA) performance for each model with respect to the number of frames. Results under different frames can be found in Appendix D. Several interesting findings arise: The performance of any video model is far from human performance. As shown in the table, humans show an average performance of 67.9%, which is significantly higher than the best models, GPT-4o and Qwen2VL-72B, by 30%. Therefore, there is large gap between models performance and human performance. Note that we are employing standard AMT workers instead of domain experts, meaning that the expert-level accuracy can be even higher, especially for professional video understanding like FineGym. Models show limited performance gains with more frames. As shown in Figure 6, with more frames, multimodal video models usually show better performance. However, performance generally saturates around 8-16 frames, meaning that models struggle to improve fine-grained activity understanding even with more frames. This is clear contrast with human performance, showing that there is still large space for multimodal video models to improve. Multiple Binary QA is more challenging metric. Multiple Binary QA, as proposed in Section 3.3, prevents model from exploiting cues in the answer choices, and evaluates whether model truly understands the temporal dynamics in the video by splitting single + 1-way multiple choice question into binary choice questions. For example, GPT-4o receives 75.7% accuracy but only 38.5% on multiple binary accuracy, showing huge gap. These results indicate that understanding"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Dataset characteristics including number of samples, average number of words in original captions and our fine-grained captions."
        },
        {
            "title": "Dataset",
            "content": "Number of Samples Org. Avg. # words Ours Avg. # words ActivityNet (Krishna et al., 2017) EgoExo4D (Grauman et al., 2024) Charades (Gao et al., 2017) MPI Movie Description (Rohrbach et al., 2015) Oops (Epstein et al., 2020) COIN (Tang et al., 2019) FineGym (Shao et al., 2020) TemporalBench (ours) 281 307 298 326 294 385 288 2179 13.03 7.73 6.21 12.39 10.06 5.01 21. 10.91 49.55 47.79 44.16 35.33 43.27 50.06 21.92 41.72 Table 2: TemporalBench performance of various multimodal generative models and embedding models under the binary QA accuracy (BA) and multiple binary QA settings (MBA) for short videos. The prefix Tindicates MBA performance for the annotated subset in our TemporalBench. We show the result with the best average MBA performance for each model with respect to the number of frames, denoted as # Frames. Model # Frames T-ActivityNet T-Charades T-FineGym T-Movie T-Oops T-COIN T-EgoExo4D BA MBA Human Performance Random Chance XCLIP ImageBind LanguageBind GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet Qwen2-VL-72B Qwen2-VL-7B LLaVA-OneVision-72B LLaVA-OneVision-7B LLaVA-NeXT-Video-34B LLaVA-NeXT-Video-7B InternLM-XC2.5 VideoLLaVA MiniCPM-V2.6 Phi-3.5-Vision MA-LMM 3 GPT-4o LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-NeXT-34B Phi-3-Vision GPT-4o Gemini-1.5-Pro Yi-34B Vicuna7b-1-5 Flan-T5-XL Flan-T5-XXL - - 8 2 8 16 1FPS 8 32 32 8 32 32 8 1FPS 8 1FPS 2 4 6 1 1 1 1 1 0 0 0 0 0 68.7 11.0 82.2 13.7 36.1 6.1 74.2 12.0 69.7 5.6 Video Embedding Models: Text + Multiple Frames as Input 14.2 17.4 22.4 16.1 16.8 15.1 7.3 7.3 6.6 19.9 19.0 19.3 8.8 11.2 10.9 70.6 11. 15.6 16.1 15.6 Video Multimodal Generative Models : Text + Multiple Frames as Input 48.8 34.9 29.9 43.8 32.4 45.2 30.2 30.6 33.5 25.3 35.2 33.1 25.3 12.5 21.0 42.6 24.5 27.5 42.6 32.2 36.2 23.2 26.8 32.6 21.5 29.2 25.8 20.1 16.4 20.1 18.8 8.3 11.1 16.7 4.9 11.8 5.9 10.4 10.8 8.7 13.5 8.0 5.2 3.5 6.6 41.7 35.6 28.2 45.1 35.9 41.1 27.3 24.8 28.2 24.8 25.5 29.1 22.7 11.0 19. 31.6 22.8 16.3 36.7 18.4 31.0 18.0 18.0 17.3 11.9 20.7 13.6 12.2 5.1 10.2 Large Multimodal Models (LMMs): Text + 1 Frame as Input 32.0 16.0 25.3 20.6 23.1 30.2 17.1 25.8 22.5 19.8 15.3 9.4 8.7 9.4 4.5 31.3 16.6 19.3 21.5 17. 26.5 6.1 9.2 15.3 8.5 Large Language Models (LLMs): Text as Input 30.2 22.4 17.4 11.4 24.9 19.2 31.9 20.5 27.5 17.4 23.5 16.8 16.7 4.5 10.4 6.6 5.6 8.3 27.9 19.9 21.8 11.3 19.9 18. 22.8 10.2 11.2 5.1 11.9 7.8 46.5 34.3 29.6 43.6 25.5 34.5 25.5 25.2 22.9 18.4 32.5 23.4 18.2 11.4 15.1 33.8 16.4 21.8 21.6 17.7 27.5 16.9 23.4 12.2 23.4 19.7 71.0 5.6 89.7 50. 67.9 9.5 6.8 9.1 11.1 36.5 21.8 20.5 37.1 21.8 30.3 16.3 17.3 19.9 14.0 20.2 16.0 13.7 4.9 10.4 27.7 9.1 16.6 13.7 13.7 28.0 17.9 16.9 7.8 14.0 14.0 51.6 53.0 52. 75.7 67.5 65.5 75.8 64.4 72.1 61.9 64.0 65.1 58.8 67.1 62.3 58.0 48.0 56.4 70.0 55.7 60.5 60.5 54.4 67.7 58.1 59.9 50.5 57.9 55.1 12.9 14.0 14.5 38.5 26.6 23.6 38.3 24.7 33.0 21.2 22.0 23.6 17.9 25.5 21.4 16.9 9.4 14.8 28.4 13.1 18.3 18.0 15. 26.5 16.1 18.7 10.4 17.9 15.1 the fine-grained temporal dynamics is still challenging task for current proprietary models and open-sourced models. Video Embedding models show near chance performance. All multimodal video embedding models, including XCLIP, LanguageBind, and ImageBind show near random chance performance. One reason could be that their small embedding size (typically vector with size around 768-2048) is insufficient to capture fine-grained temporal details. Low single-frame bias and language bias. As shown in Figure 6 and Table 8, the performance of models like GPT-4o gradually increases with more frames. Excluding GPT-4o, all remaining VLMs trained with single images e.g., LLaVA-1.5, Phi-3V, and text-only LLMs such as Yi-34B and Vicuna-7B show poor performance."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Comparison of models for video captioning using Caption Similarity, CIDEr, BLEU, and ROUGE metrics. Cosine similarity using sentence transformer reflects the captioning quality the best."
        },
        {
            "title": "Model",
            "content": "Similarity CIDEr ROUGE BLEU 1 BLEU 2 BLEU 3 BLEU 4 Video Multimodal Generative Models : Text + Multiple Frames as Input"
        },
        {
            "title": "61.3\nGPT-4o\n56.5\nGemini-1.5-Pro\n54.1\nClaude-3.5-Sonnet\n56.1\nQwen2-VL-72B\n51.9\nQwen2-VL-7B\n55.0\nLLaVA-OneVision-72B\nLLaVA-OneVision-7B\n50.1\nLLaVA-NeXT-Video-34B 53.1\n50.1\nLLaVA-NeXT-Video-7B\n52.4\nInternLM-XC2.5\n46.0\nVideoLLaVA\n47.2\nMiniCPM-V2.6\n42.9\nPhi-3.5-Vision\n38.7\nMA-LMM\nM 3\n47.8",
            "content": "7.3 10.9 8.6 9.3 6.9 9.7 0.3 5.3 2.3 2.3 4.5 1.5 3.7 3.1 3.0 19.6 19.1 17.1 19.1 18.0 18.7 14.5 15.9 15.8 15.9 16.9 14.2 16.5 15.0 16.4 24.1 19.0 24.4 15.7 12.5 23.7 11.1 21.4 18.1 17.8 12.6 15.5 20.4 10.1 16.7 11.8 9.2 10.3 8.0 6.1 11.3 5.1 9.2 7.0 7.1 5.4 5.4 8.4 4.8 6.9 Large Multimodal Models (LMMs): Text + 1 Frame as Input GPT-4o LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-NeXT-34B Phi-3-Vision 52.3 47.9 45.7 49.1 42.0 7.3 4.9 6.9 6.2 4.0 17.1 18.0 17.8 16.7 16.1 25.1 22.6 22.0 24.2 19.9 11.1 9.8 9.5 10.4 8.3 5.8 4.5 4.4 4.1 3.0 5.6 2.2 4.0 2.6 2.8 2.3 1.9 3.4 2.2 2. 5.0 4.2 4.2 4.6 3.4 3.0 2.4 2.1 2.2 1.6 2.9 1.1 1.8 1.1 1.2 1.0 0.8 1.6 1.1 1.2 2.4 2.0 2.0 2.2 1."
        },
        {
            "title": "4.4 VIDEO CAPTIONING",
            "content": "Our detailed video captions also enables analyzing models fine-grained video captioning capabilities. For this, we prompt multimodal video models to generate caption for an input video, with 3 captioning examples in the prompt as guidance to mimic the style of our detailed video captions. Note that we remove the FineGym captions due to its different structure compared to other video captions, resulting in 1891 samples. We evaluate the resulting video captioning performance using classical image captioning metrics, CIDEr (Vedantam et al., 2015), BLEU (Papineni et al., 2002) at different n-gram levels, ROUGE (Lin, 2004), as well as the embedding similarity with sentence transformer (Reimers & Gurevych, 2019) between the ground truth caption and the generated caption. Note that we for each model, we use the same number of frames as in Section 4.3. Results in Table 3 show that GPT-4o achieves the best performance. Interestingly, the results indicate that the embedding similarity aligns most closely with the video QA task results from Sec 4.3. Other classical captioning metrics show inconsistent results. For example, GPT-4o obtains similar performance with one compared to 64 frames on both CIDEr and BLEU scores (e.g., for BLEU 1 24.1 vs. 25.1). On the other hand, all models show similar ROUGE scores. Thus, for the zero-shot captioning task, our findings indicate that text embedding similarity may be the most reliable metric."
        },
        {
            "title": "4.5 LONG VIDEO UNDERSTANDING",
            "content": "Since our benchmark is annotated at the video clip level, we can easily extend it to long video understanding by concatenating the captions of different video clips within the same original video. In our study, we choose video datasets from AcitivityNet, Charades, EgoExo4D, COIN and FineGym. We randomly sample video clips within the same original video, and then crop new video segment whose starting time corresponds to that of the earliest sampled video clip and whose ending time corresponds to that of the latest sampled video clip. We then concatenate all the sampled video captions together to form single long detailed description corresponding to the new video segment. Given this positive caption, we generate negative captions for it by replacing the positive caption of one of the sampled video clips with its negatives. The model is then tasked to choose the correct long caption out of multiple choices. We control the random chance multiple binary QA performance to be 9.5%, resulting in an apple-to-apple comparsion with in Sec 4.3. In this way, we investigate"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Visualization of binary accuracy for short video QA per (a) subset and (b) negative type. Human performance is much better than GPT-4o, Qwen2-VL-72B, LLaVA-OneVision-72B, and Gemini-1.5-Pro. whether multimodal video models can understand and distinguish fine details in long video. Finally, we sampled 1,574 videos with durations ranging between [0, 20] minutes, as shown in Figure 4. We show in Table 4, that all multimodal video models show significant performance drop for this task compared to short video understanding. This is also reflected in all models performing better on relatively shorter videos (e.g., Charades) compared to longer videos (e.g., FineGym). These results indicate that finding the subtle temporal dynamic differences in long video is indeed an extremely difficult task. It is similar in nature to the needle-in-the-sea task (Kamradt, 2023) in NLP except in the temporal domain. We hope that TemporalBench for long video understanding can serve as very challenging task for future video understanding model development. 5 IN-DEPTH ANALYSIS"
        },
        {
            "title": "5.1 WHY MULTIPLE BINARY QA INSTEAD OF MULTI-CHOICE QA?",
            "content": "As discussed in Section 3.3, in the standard multi-choice QA setting, if negatives are all slightly variations of the positive caption, we find that LLMs can determine the centralized caption, and take shortcut to achieve better performance. To demonstrate this, based on one negative caption (C) in TemporalBench, we intentionally generate two negative captions derived from (C) (instead of C), resulting in N1(N (C)) and N2(N (C)). Given two set of options [C, N1(C), N2(C)), N3(C))] and [C, N1(C), N1(N1(C)), N2(N1(C))] shown in Figure 5, text-only GPT-4o displays different behaviors. As shown in Table 5, under the intentionally designed negative options, GPT-4o will choose N1(C) under 66.4% cases. This again demonstrates the necessity and advantage of our multiple binary QA accuracy (MBA) metric design over the standard multi-choice QA setting."
        },
        {
            "title": "5.2 PERFORMANCE ON CATEGORIES",
            "content": "Broadly, TemporalBench evaluates word level replacement and event level re-ordering. Here we further breakdown the word level replacement into following categories: (1). Action order (change the order); (2). Action frequency (1 times v.s. two times); (3). Action type (put v.s. pull); (4). Motion magnitude (slightly v.s. intensively); (5). Motion Direction/Orientation (forward v.s. backward, circular v.s. back-and-forth). (6). Action effector (cutting with left hand v.s. cutting with right hand) (7). Others. We prompt GPT-4o to perform 7-way classification and show the per-category performance in Table 7 and Figure 7 (b). Results indicate that multimodal video models shows better performance on others category rather than the other categories related to actions. Among the"
        },
        {
            "title": "Preprint",
            "content": "Table 4: TemporalBench performance of various multimodal generative models and embedding models under long video understanding with binary QA accuracy (BA) and multiple binary QA accuracy (MBA). The MBA performance under each dataset is also included. We show the result with the best average MBA performance for each model with respect to the number of frames, denoted as # Frames."
        },
        {
            "title": "Model",
            "content": "# Frames T-ActivityNet T-Charades T-FineGym T-COIN T-EgoExo4D BA MBA"
        },
        {
            "title": "Random Performance",
            "content": "- 9.2 4.3 11.2 11.4 9. 50.2 9."
        },
        {
            "title": "XCLIP\nImageBind\nLanguageBind",
            "content": "Video Embedding Models: Text + Multi-Frames as Input 8 2 8 11.1 10.2 11.7 12.4 8.1 10.8 6.5 9.3 10.3 10.8 10.8 11. Video Multimodal Generative Models : Text + Multi-Frames as Input GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet Qwen2-VL-72B Qwen2-VL-7B LLaVA-OneVision-72B LLaVA-OneVision-7B LLaVA-NeXT-Video-34B LLaVA-NeXT-Video-7B InternLM-XC2.5 VideoLLaVA MiniCPM-V2.6 Phi-3.5-Vision MA-LMM 3 64 1FPS 8 8 32 4 32 4 8 1FPS 8 1FPS 4 4 6 40.0 32.1 28.9 32.4 22.2 28.6 21.3 23.5 18.1 21.0 20.0 14.3 23.2 10.2 10.8 37.8 18.4 22.2 20.5 20.0 19.5 13.0 22.2 21.6 18.4 16.8 16.8 11.9 9.2 8.6 16.8 18.7 16.8 21.5 9.3 18.7 13.1 19.6 10.3 20.6 15.9 6.5 19.6 2.8 12. 32.7 24.8 22.2 18.9 18.3 16.5 11.4 17.9 18.5 14.0 9.8 17.1 10.2 11.4 13.0 GPT-4o LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-NeXT-34B Phi-3-Vision GPT-4o Gemini-1.5-Pro Yi-34B Vicuna7b-1-5 Flan-T5-XL Flan-T5-XXL Large Multimodal Models (LMMs): Text + 1 frame as Input 1 1 1 1 1 0 0 0 0 0 27.9 14.3 9.2 21.6 18.1 23.2 11.9 11.9 20.5 12.4 19.6 10.3 10.3 19.6 15.0 Large Larguage Models (LLMs): Text as Input 27.6 22.9 19.7 6.3 21.6 20.0 32.4 19.5 19.5 9.2 15.7 11. 17.8 17.8 14.0 9.3 23.4 18.7 25.2 15.4 12.8 18.9 15.4 24.2 19.3 15.9 10.6 18.1 15.7 11.8 12.4 14.1 29.3 23.8 26.7 33.1 18.7 30.9 19.8 19.2 15.6 11.4 16.6 14.1 13.3 11.6 12.4 22.9 14.7 14.5 19.8 15. 33.5 23.4 20.6 12.0 19.8 17.1 51.7 51.0 51.6 70.5 65.2 64.6 64.7 59.7 63.4 56.9 60.3 57.2 55.8 56.0 60.3 54.5 47.1 53.1 64.7 54.8 53.2 60.5 56.0 67.6 62.2 59.5 51.1 60.1 56.9 11.1 10.7 12. 32.7 24.7 24.5 26.2 18.8 23.8 16.2 20.0 17.3 15.6 15.1 19.3 14.5 9.2 11.8 24.5 14.2 12.3 19.9 15.6 28.2 21.2 18.4 9.9 19.4 16.7 Table 5: Effect of the Centralized Caption on text-only GPT-4o. Percentage of Predictions Aligned with [C, N1(C), N2(C)), N3(C))] [C, N1(C), N1(N1(C)), N2(N1(C))] 83.3 17.7 N1(C) 6.4 66.4 seven categories, models struggle most on action frequency (counting), which show that they do not memorize repeated occurrences well. The visualizations of failture cases in GPT-4o is shown in Figure 8."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "We propose TemporalBench, novel video understanding benchmark, to evaluate the fine-grained temporal understanding abilities of multimodal video models. The video captions in our benchmark Table 6: TemporalBench statistics on negative caption types."
        },
        {
            "title": "Overall",
            "content": "129 530 2,802 320 1,536 1, 2,099 1,342 9,"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: The failure cases of GPT-4o in TemporalBench. GPT-4o does not understand the finegrained details well, including motion direction, action frequency, action type, and motion direction. are significantly denser than existing datasets such as MSRVTT and TGIF, offering detailed temporal annotations. TemporalBench also provides more challenging set of tasks that push current multimodal models beyond coarse-level understanding. The empirical results reveal substantial gap between human performance and current state-of-the-art models. We also found critical pitfall for multichoice QA, where we devise multiple binary accuracy (MBA) to address thi issue. We hope that this benchmark fosters further research in developing models with enhanced temporal reasoning capabilities. Our benchmark could also be easily utilized for other fundamental video tasks such as spatio-temporal localization and text-to-video generation with fine-grained prompts. Limitations. One cannot fully analyze the behavior of proprietary models included in this paper due to the lack of access to these models, which are GPT-4o, Gemini-1.5-Pro and Claude 3.5 Sonnet."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This work was supported in part by NSF IIS2404180, and Institute of Information & communications Technology Planning & Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 20220-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training), and Microsoft Accelerate Foundation Models Research Program."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We attach part of the dataset in the submissions supplementary materials. We will also publicly release it along with the code used to evaluate the LMMs upon the papers acceptance."
        },
        {
            "title": "Preprint",
            "content": "Table 7: TemporalBench performance under each category under BA. Multimodal videos models struggle on certain tasks such as action frequency. We show the result with the best average MBA performance for each model with respect to the number of frames."
        },
        {
            "title": "XCLIP\nImageBind\nLanguageBind",
            "content": "GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet Qwen2-VL-72B Qwen2-VL-7B LLaVA-OneVision-72B LLaVA-OneVision-7B LLaVA-NeXT-Video-34B LLaVA-NeXT-Video-7B InternLM-XC2.5 VideoLLaVA MiniCPM-V2.6 Phi-3.5-Vision MA-LMM 3 GPT-4o LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-NeXT-34B Phi-3-Vision GPT-4o Gemini-1.5-Pro Yi-34B Vicuna7b-1-5 Flan-T5-XL Flan-T5-XXL"
        },
        {
            "title": "The Number Action\nOrder",
            "content": "of Frames"
        },
        {
            "title": "Event",
            "content": "- - 89.9 50.0 82.6 50.0 91.9 50.0 87.5 50.0 85.9 50. 90.0 50.0 Video Embedding Models: Text + Multi-Frames as Input 8 2 8 46.5 44.2 43.4 50.8 44.7 41.5 50.9 55.4 53. 56.9 50.9 55.0 51.2 52.5 51.4 Video Multimodal Generative Models : Text + Multi-Frames as Input 16 1FPS 8 32 32 8 32 32 8 1FPS 8 1FPS 2 4 6 69.8 67.4 62.0 72.1 65.9 73.6 63.6 61.2 69.0 55.8 69.8 59.4 53.5 54.3 51.9 64.7 60.1 57.4 69.2 45.8 56.0 45.5 56.0 65.7 42.5 70.2 52.3 55.3 43.0 53. 80.6 70.6 70.7 79.9 67.3 76.2 62.9 66.4 68.2 62.7 71.4 65.5 60.1 48.0 58.9 78.4 70.7 70.3 78.7 66.1 70.3 56.9 61.6 62.2 62.5 70.0 62.5 55.9 47.8 56.3 67.9 58.7 60.0 65.9 54.6 65.2 52.8 58.5 66.5 52.6 70.6 54.1 54.0 46.3 52.2 Large Multimodal Models (LMMs): Text + 1 frame as Input 1 1 1 1 1 0 0 0 0 0 67.4 57.4 62.0 51.2 46.5 65.1 51.9 61.5 55.7 45.5 74.1 57.6 62.2 61.2 56.0 70.3 53.8 54.1 60.0 55.6 Large Larguage Models (LLMs): Text as Input 65.1 54.3 51.9 55.8 53.5 55. 59.8 42.5 62.3 47.2 57.7 62.5 73.7 60.4 60.1 51.7 60.2 59.0 70.0 62.2 60.3 48.4 59.7 58.4 64.2 50.4 61.4 54.8 48.8 61.5 53.6 57.1 50.1 56.1 54.2 51.7 50.5 46. 67.2 59.5 57.8 69.5 54.7 62.4 54.0 59.3 68.6 51.1 70.2 53.3 52.2 48.8 53.7 62.6 53.9 64.9 53.0 49.2 60.1 53.3 55.1 49.4 56.9 48.2 89.1 50.0 50.1 48.6 51.0 75.8 67.9 61.3 76.0 69.7 73.2 66.5 63.4 52.2 58.3 50.5 63.5 55.3 48.6 50. 68.7 54.2 51.0 65.0 56.9 69.3 64.8 65.4 49.9 54.9 49.3 93.4 50.0 55.6 61.8 65.9 85.6 79.2 76.2 85.7 75.7 84.2 77.1 74.1 74.3 70.7 75.5 74.7 69.4 49.6 68.6 78.4 63.1 67.9 67.5 62. 68.6 57.4 58.0 51.4 60.7 58.9 89.7 50.0 51.6 53.0 52.8 75.7 67.5 65.5 75.8 64.4 72.1 61.9 64.0 65.1 58.8 67.1 62.3 58.0 48.0 56.4 70.0 55.7 60.5 60.5 54.4 67.7 58.1 59.9 50.5 57.9 55."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research primarily utilizes publicly available video datasets, which have been collected and annotated by qualified annotators and authors, ensuring compliance with ethical standards. We have made every effort to ensure that the data used respects privacy and contains no personally identifiable information. Furthermore, we acknowledge the potential implications of fine-grained video understanding, especially in sensitive applications such as surveillance and autonomous systems. As such, we advocate for responsible and ethical use of this research, urging caution in deploying these models in real-world scenarios to avoid harmful or unintended consequences."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Anthropic. Claude-sonnet-3.5. claude-3-5-sonnet, 2024. https://www.anthropic.com/news/ Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 24252433, 2015. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023."
        },
        {
            "title": "Preprint",
            "content": "Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Making large multimodal models understand arbitrary visual prompts. In IEEE Conference on Computer Vision and Pattern Recognition, 2024a. Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee. Matryoshka multimodal models. arXiv preprint arXiv:2405.17430, 2024b. David L. Chen and William B. Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011), Portland, OR, June 2011. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Dave Epstein, Boyuan Chen, and Carl Vondrick. Oops! predicting unintentional action in video. CVPR, 2020. Gunnar Farneback. Two-frame motion estimation based on polynomial expansion. In Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad, Sweden, June 29July 2, 2003 Proceedings 13, pp. 363370. Springer, 2003. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The firstever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024. URL https://arxiv.org/abs/2405.21075. Moran Furman and Xiao-Jing Wang. Similarity effect and optimal control of multiple-choice decision making. Neuron, 60(6):11531168, 2008. Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pp. 52675275, 2017. Gemini Team. Gemini: family of highly capable multimodal models, 2024. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1938319400, 2024. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018."
        },
        {
            "title": "Preprint",
            "content": "Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024a. Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, and Xin Eric Wang. Mmworld: Towards multi-discipline multi-faceted world model evaluation in videos, 2024b. URL https://arxiv.org/abs/2406.08407. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. NeurIPS, 2023. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatiotemporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27582766, 2017. Gregory Kamradt. Needle in haystack - pressure testing llms. https://github.com/ gkamradt/LLMTest_NeedleInAHaystack, 2023. Accessed: 2024-10-01. Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zero-shot video question answering using vlm. arXiv preprint arXiv:2403.18406, 2024. Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pp. 706715, 2017. Jie Lei, Tamara Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 487507, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.29. URL https://aclanthology.org/2023.acl-long.29. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark, 2024b. URL https://arxiv.org/abs/2311.17005. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 292305, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/ v1/2023.emnlp-main.20. URL https://aclanthology.org/2023.emnlp-main.20. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013."
        },
        {
            "title": "Preprint",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023b. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos?, 2024c. URL https: //arxiv.org/abs/2403.00476. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. URL https://arxiv.org/abs/2310.02255. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In Adv. Neural Inform. Process. Syst., 2024. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 16971706, 2022. Meta. Llama-3. https://ai.meta.com/blog/meta-llama-3/, 2024. Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. In European Conference on Computer Vision (ECCV), 2022. OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_ Card.pdf, 2023a. OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023b. OpenAI. Gpt-4 technical report. 2023c. OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023."
        },
        {
            "title": "Preprint",
            "content": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908. 10084. Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. dataset for movie description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: hierarchical video dataset for fine-grained action understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan Plummer, Bryan Russell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1358113591, 2024. Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12071216, 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575, 2015. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Wenhao Wu. Freeva: Offline mllm as training-free video assistant. arXiv preprint arXiv:2405.07798, 2024. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97779786, 2021. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pp. 16451653, 2017."
        },
        {
            "title": "Preprint",
            "content": "Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In EMNLP, 2021. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 52885296, 2016. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai. arXiv, 2024. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1380713816, 2024a. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In Forty-first International Conference on Machine Learning, 2024b. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, volume 33, pp. 91279134, 2019a. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynetqa: dataset for understanding complex web videos via question answering, 2019b. URL https://arxiv.org/abs/1906.02467. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Da Zhang, Xiyang Dai, and Yuan-Fang Wang. Dynamic temporal pyramid network: closer look In Computer VisionACCV 2018: 14th Asian at multi-scale modeling for activity detection. Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part IV 14, pp. 712728. Springer, 2019. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023a. Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, and Jianwei Yang. Llava-grounding: Grounded visual chat with large multimodal models, 2023b."
        },
        {
            "title": "Preprint",
            "content": "Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024a. Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023c. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to nmodality by language-based semantic alignment. In The Twelfth International Conference on Learning Representations, 2024a. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ICLR, 2024b."
        },
        {
            "title": "A BROADER IMPACT",
            "content": "TemporalBench, comprehensive benchmark for video understanding, has the potential to significantly advance research in this field by offering improved metrics for model evaluation. Our work aims to enhance the temporal reasoning capabilities of future video understanding models. However, the broader impact of more advanced video understanding technologies raises important societal concerns, including the risk of mass surveillance, privacy violations, and the development of harmful applications like autonomous weapons. Therefore, we strongly encourage thoughtful consideration when deploying these models in real-world scenarios to mitigate negative or unintended consequences."
        },
        {
            "title": "B MORE VISUALIZATIONS OF OUR BENCHMARK",
            "content": "In this section, we present comprehensive visualizations of our fine-grained annotations with both positive and negative descriptions. For each benchmark mentioned in Table 1, we provide one video example with its positive annotation and one of the corresponding negative descriptions (there are more than one negative for single video in our dataset) in Figures 9 & 10. The video examples (a - ) are displayed in the same order as their sources in Table 1 (7 in total)."
        },
        {
            "title": "C PER SUBSET RESULTS FOR SHORT AND LONG VIDEO QA UNDER BINARY",
            "content": "ACCURACY (BA) The per subset results (denoted as T-) for short and long video QA under Binary Accuracy (BA) are shown in Table 9, and Table 10, respectively. Still, human achieve much better performance than all multimodal videos. Interestingly, both human and Finegym, the professional subset,"
        },
        {
            "title": "D MORE RESULTS WITH EXTENDED FRAMES",
            "content": "In the main paper, we only report the performance of each multimodal video models with the the number of frams that leads to the best performance. Here we extend the results to show the results of more frames in Table 8."
        },
        {
            "title": "E DATA ANNOTATION PLATFORM",
            "content": "Positive Captions We use Amazon Mechanical Turk (AMT) 2 for positive caption annotation, and then use Label Studio 3 to let authors refine the caption. As shown in Figure 11, authors can edit the caption from AMT workers. Also, we provide the original short video captions to let people better understand our task. Negative Captions We first prompt LLMs (GPT-4o, Gemini, and Llama-3.1-405b) to get initial negative captions, and then ask authors to choose the negatives that can reflect the temporal dynamic. The visualization of the multi-choice platform in shown in Figure 12. 2https://www.mturk.com/ 3https://labelstud.io/"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Visualizations (I) of our fine-grained annotations of the videos with both positive and negative descriptions."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Visualizations (II) of our fine-grained annotations of the videos with both positive and negative descriptions."
        },
        {
            "title": "Preprint",
            "content": "Table 8: TemporalBench performance of various models under binary QA accuracy (BA) and multiple binary QA accuracy (MBA) setting for short and long question answering with different number of frames. Overall denotes the average performance of short and long video QA performance. Model # Frames Overall MBA Overall BA Short MBA Short BA Long MBA Long BA Captioning Human Performance Random Chance XCLIP ImageBind LanguageBind GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet Qwen2-VL-72B Qwen2-VL-7B LLaVA-OneVision-72B LLaVA-OneVision-7B LLaVA-NeXT-Video-34B LLaVA-NeXT-Video-7B InternLM-XC2."
        },
        {
            "title": "VideoLLaVA",
            "content": "MiniCPM-V2.6 Phi-3.5-Vision MA-LMM M3 LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-NeXT-34B Phi-3-Vision Gemini-1.5-Pro Yi-34B Vicuna7b-1-5 Flan-T5-XL Flan-T5-XXL - - 8 2 64 32 16 8 4 2 1 0 1FPS 0 16 8 4 2 1 32 16 8 4 2 32 16 8 4 2 32 16 8 4 2 32 16 8 4 2 1 32 16 8 4 2 1 32 16 8 4 2 1 1FPS 8 1FPS 32 16 8 4 2 1 4 6 1 1 1 0 0 0 0 - 9.5 12.0 12.4 13.3 35.4 32.9 34.3 32.9 31.9 30.3 26.5 27. 25.7 18.7 23.2 24.1 23.2 21.1 18.7 31.7 31.5 30.1 28.6 27.3 21.8 21.2 19.2 17.4 16.4 26.6 27.2 28.1 27.6 25.7 23.3 18.7 17.9 17.3 16.4 14.8 12. 19.9 20.3 20.5 20.4 19.9 19.0 15.9 19.3 20.5 20.0 19.2 17.6 16.8 20.3 20.4 14.1 14.7 14.9 15.0 15.5 14. 9.1 13.3 13.7 15.3 19.0 15. 18.6 18.5 10.1 18.6 15.9 - 50.0 51.7 52.0 52.2 73.3 71.5 72.8 72.0 71.2 70.3 67.4 67.7 66.4 60. 64.2 65.1 64.2 62.2 58.9 70.2 70.1 68.9 68.4 67.6 62.1 61.5 59.4 58.5 56.9 66.6 67.3 67.9 67.3 66.3 64.1 59.4 58.8 57.8 56.3 54.4 51.4 61.1 61.1 61.9 61.7 61.2 59. 57.1 59.9 61.2 60.7 60.3 59.1 57.3 61.6 61.3 54.3 55.1 55.6 56.0 56.2 55.9 47. 54.7 55.1 56.8 60.5 55.2 60. 59.7 50.8 59.0 56.0 24 67. 9.5 12.9 14.0 14.5 38.0 38.3 38.5 37.4 35.8 33.3 28.4 26.5 26.6 16. 23.5 23.6 23.1 21.2 18.4 38.3 36.9 34.0 31.2 27.5 24.7 23.6 21.1 19.3 17.7 30.7 32.1 33.0 31.4 29.2 27.1 21.2 20.1 19.5 18.9 16.8 13.4 22.0 21.8 21.4 20.7 20.0 19. 17.3 22.4 23.6 23.0 21.5 19.1 17.9 25.5 21.4 15.6 15.9 15.9 15.5 16.9 16.5 9. 14.8 13.1 18.3 18.0 15.1 16. 18.7 10.4 17.9 15.1 89.7 50. 51.6 53.0 52.8 76.0 75.9 75.7 75.1 74.4 72.7 70.0 67.7 67.5 58.1 65.9 65.5 64.8 61.9 58. 75.8 74.6 73.1 71.5 69.2 64.4 63.3 61.1 59.5 57.8 70.5 71.2 72.1 71.2 69.6 67.9 61.9 60.9 59.9 58.9 56.1 53.3 64.0 63.7 63.3 63.0 61.8 60.5 59.5 64.0 65.1 64.2 63.1 62. 58.8 67.1 62.3 56.8 57.2 57.4 57.5 58.0 57.7 48.0 56. 55.7 60.5 60.5 54.4 58.1 59. 50.5 57.9 55.1 - 9.5 11. 10.7 12.0 32.7 27.4 30.1 28.3 28.0 27.3 24.5 28.2 24.7 21.2 22.9 24.5 23.3 20.9 18.9 25.0 26.1 26.2 26.0 27. 18.8 18.7 17.2 15.4 15.0 22.4 22.3 23.1 23.8 22.1 19.5 16.2 15.6 15.0 13.9 12.7 10.6 17.7 18.7 19.5 20.0 19.7 18.9 14.5 16.1 17.3 17.0 16.8 16.1 15. 15.1 19.3 12.6 13.5 13.8 14.5 14.1 12.5 9.0 11.8 14. 12.3 19.9 15.6 21.2 18.4 9. 19.4 16.7 - 50.0 51.7 51. 51.6 70.5 67.0 69.8 68.8 68.0 67.8 64.7 67.6 65.2 62.2 62.4 64.6 63.6 62.4 59.3 64.5 65.5 64.7 65.3 66.0 59.7 59.7 57.7 57.5 56. 62.7 63.4 63.6 63.4 63.0 60.2 56.9 56.6 55.7 53.7 52.7 49.5 58.2 58.4 60.4 60.3 60.6 59.1 54.7 55.7 57.2 57.2 57.4 56.1 55.8 56. 60.3 51.7 53.0 53.7 54.5 54.4 54.0 46.9 53.1 54.5 53. 60.5 56.0 62.2 59.5 51.1 60. 56.9 - - - - - 63.5 63.2 61.3 60.3 58.8 55.3 52.3 - 56.5 - 54.1 53.1 51.9 48.2 41.0 56.1 54.1 51.4 48.3 43.9 51.9 50.3 48.4 46.1 42.0 53.9 54.2 55.0 54.2 51.1 48. 50.1 50.4 50.2 49.7 47.1 44.1 53.1 53.3 53.4 52.5 48.9 46.2 51.6 49.9 50.1 49.2 46.8 44.0 52.4 46.0 47. 48.4 48.9 48.3 44.0 42.9 42.1 38.7 47.8 47.9 45.7 49. 42.0 - - - - -"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Positive caption refinement platform."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Negative caption annotation platform."
        },
        {
            "title": "Preprint",
            "content": "Table 9: TemporalBench performance of various multimodal generative models and embedding models under the binary QA accuracy (BA) and multiple binary QA settings (MBA) for short videos. The prefix Tindicates BA performance for the annotated subset in our TemporalBench. We show the result with the best average MBA performance for each model with respect to the number of frames, denoted as # Frames. Model # Frames T-ActivityNet T-Charades T-FineGym T-Movie T-Oops T-COIN T-EgoExo4D BA MBA Human Performance Random Chance XCLIP ImageBind LanguageBind GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet Qwen2-VL-72B Qwen2-VL-7B LLaVA-OneVision-72B LLaVA-OneVision-7B LLaVA-NeXT-Video-34B LLaVA-NeXT-Video-7B InternLM-XC2.5 VideoLLaVA MiniCPM-V2.6 Phi-3.5-Vision MA-LMM 3 GPT-4o LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-NeXT-34B Phi-3-Vision GPT-4o Gemini-1.5-Pro Yi-34B Vicuna7b-1-5 Flan-T5-XL Flan-T5-XXL - - 8 2 8 91.1 50. 93.8 50.0 77.0 50.0 93.1 50.0 92.6 50.0 Video Embedding Models: Text + Multiple Frames as Input 52.7 52.9 56. 52.8 52.6 50.1 49.0 47.5 48.2 53.9 55.4 55.8 53.5 56.8 55.1 Video Multimodal Generative Models : Text + Multiple Frames as Input 16 1FPS 8 32 32 8 32 32 8 1FPS 8 1FPS 2 4 1 1 1 1 1 0 0 0 0 0 0 78.5 70.7 68.5 76.6 67.0 76.0 66.5 67.5 68.0 61.0 71.8 66.1 62.0 49.8 59.5 74.8 63.0 62.4 74.5 65.2 70.4 60.0 62.9 66.5 57.9 63.4 59.6 55.8 48.8 54.9 64.8 55.0 62.7 65.4 49.9 59.3 49.4 56.3 56.7 50.6 61.6 54.1 50.0 42.3 51.1 77.2 72.5 68.2 79.8 70.5 76.1 68.0 68.0 69.9 63.5 68.2 68.0 64.1 48.0 60. 77.9 70.3 64.2 77.7 66.5 75.2 61.6 66.1 66.1 60.3 68.5 63.1 58.2 49.9 58.9 Large Multimodal Models (LMMs): Text + 1 Frame as Input 69.1 57.6 64.2 59.7 57.4 67.1 54.3 58.6 60.3 54.5 64.8 51.9 55.7 55.0 45.2 71.0 56.8 61.0 61.8 57. Large Language Models (LLMs): Text as Input 66.2 58.5 59.1 49.7 60.5 56.7 67.4 57.6 62.3 49.5 59.2 49.3 65.6 50.6 54.9 50.2 50.5 52.0 65.6 59.8 59.7 50.7 60.7 59.0 71.9 53.2 57.5 62.0 52. 68.9 57.6 57.7 50.5 56.8 54.6 90.2 50.0 52.3 52.4 51.1 79.2 70.2 65.4 77.2 66.5 73.5 64.6 63.4 65.2 59.2 68.9 62.7 57.7 49.0 54.9 71.0 58.1 62.7 61.0 55.8 67.8 58.6 63.1 50.0 58.7 56. 92.5 50.0 89.7 50.0 67.9 50.0 48.1 53.4 52.8 78.3 70.8 66.8 79.7 66.6 74.9 64.4 64.5 65.0 59.7 67.3 62.7 58.9 48.8 55.2 74.0 57.8 63.9 63.7 58. 71.7 64.3 63.6 52.1 60.3 56.2 51.6 53.0 52.8 75.7 67.5 65.5 75.8 64.4 72.1 61.9 64.0 65.1 58.8 67.1 62.3 58.0 48.0 56.4 70.0 55.7 60.5 60.5 54.4 67.7 58.1 59.9 50.5 57.9 55.1 12.9 14.0 14. 38.5 26.6 23.6 38.3 24.7 33.0 21.2 22.0 23.6 17.9 25.5 21.4 16.9 9.4 14.8 28.4 13.1 18.3 18.0 15.1 26.5 16.1 18.7 10.4 17.9 15."
        },
        {
            "title": "Preprint",
            "content": "Table 10: TemporalBench performance of various multimodal generative models and embedding models under long video understanding with binary QA accuracy (BA) and multiple binary QA accuracy (MBA). The BA performance under each dataset is also included. We show the result with the best average MBA performance for each model with respect to the number of frames, denoted as # Frames."
        },
        {
            "title": "Model",
            "content": "# Frames T-ActivityNet T-Charades T-FineGym T-COIN T-EgoExo4D BA MBA"
        },
        {
            "title": "Random Performance",
            "content": "- 50.0 50.0 50.0 50.0 50. 50.0 50."
        },
        {
            "title": "XCLIP\nImageBind\nLanguageBind",
            "content": "Video Embedding Models: Text + Multi-Frames as Input 8 2 8 51.9 50.3 51.9 48.7 52.6 46.4 47.9 47.9 48.2 52.6 51.3 52. Video Multimodal Generative Models : Text + Multi-Frames as Input GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet Qwen2-VL-72B Qwen2-VL-7B LLaVA-OneVision-72B LLaVA-OneVision-7B LLaVA-NeXT-Video-34B LLaVA-NeXT-Video-7B InternLM-XC2.5 VideoLLaVA MiniCPM-V2.6 Phi-3.5-Vision MA-LMM 3 64 1FPS 8 8 32 4 32 4 8 1FPS 8 1FPS 4 4 6 74.8 67.0 66.8 68.5 60.7 67.0 60.0 59.4 60.9 59.6 61.2 53.7 60.3 47.4 52.5 73.8 61.6 63.7 59.6 58.0 63.5 53.6 63.0 58.6 58.9 57.0 58.6 52.3 51.7 52.9 61.2 60.6 56.7 62.5 49.9 61.2 57.6 57.6 51.5 57.0 59.5 41.3 58.1 36.4 51. 70.1 65.9 63.1 59.6 59.8 55.8 53.2 59.5 56.7 54.9 50.1 54.8 50.3 50.1 53.4 GPT-4o LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-NeXT-34B Phi-3-Vision GPT-4o Gemini-1.5-Pro Yi-34B Vicuna7b-1-5 Flan-T5-XL Flan-T5-XXL Large Multimodal Models (LMMs): Text + 1 frame as Input 1 1 1 1 1 0 0 0 0 0 67.6 55.1 51.2 60.6 56.9 64.3 52.3 53.4 60.8 53.9 62.8 52.9 51.5 57.0 52.1 Large Larguage Models (LLMs): Text as Input 67.1 62.8 59.0 49.0 61.3 59.4 68.1 59.4 60.2 52.4 57.7 53. 63.6 55.6 56.5 49.3 59.8 59.5 65.9 55.0 51.8 59.8 55.6 65.1 60.7 59.5 51.2 58.8 56.3 52.8 51.3 53.7 68.7 65.9 66.6 70.0 61.9 69.3 59.8 61.4 56.1 52.8 57.3 53.9 55.1 51.2 53.6 62.0 54.8 56.2 61.8 57. 71.3 65.7 60.4 52.2 61.7 56.5 51.7 51.0 51.6 70.5 65.2 64.6 64.7 59.7 63.4 56.9 60.3 57.2 55.8 56.0 60.3 54.5 47.1 53.1 64.7 54.5 53.2 60.5 56.0 67.6 62.2 59.5 51.1 60.1 56.9 11.1 10.7 12. 32.7 24.7 24.5 26.2 18.8 23.8 16.2 20.0 17.3 15.6 15.1 19.3 14.5 9.2 11.8 24.5 14.2 12.3 19.9 15.6 28.2 21.2 18.4 9.9 19.4 16."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong",
        "Georgia Institute of Technology",
        "Illinois Institute of Technology",
        "Microsoft Research, Redmond",
        "Northeastern University",
        "Ohio State University",
        "University of California, San Diego",
        "University of California, Santa Cruz",
        "University of Wisconsin-Madison"
    ]
}