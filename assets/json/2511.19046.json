{
    "paper_title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
    "authors": [
        "Anglin Liu",
        "Rundong Xue",
        "Xu R. Cao",
        "Yifan Shen",
        "Yi Lu",
        "Xiang Li",
        "Qianqian Chen",
        "Jintai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 6 4 0 9 1 . 1 1 5 2 : r MedSAM3: Delving into Segment Anything with Medical Concepts Anglin Liu1*, Rundong Xue2,, Xu R. Cao3,, Yifan Shen3, Yi Lu1, Xiang Li3, Qianqian Chen4, Jintai Chen1,5, 1 The Hong Kong University of Science and Technology (Guangzhou) 2 Xian Jiaotong University 3 University of Illinois Urbana-Champaign 4 Southeast University 5 The Hong Kong University of Science and Technology"
        },
        {
            "title": "Abstract",
            "content": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via openvocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-SLiu/MedSAM3. 1. Introduction Medical segmentation is the cornerstone of the modern healthcare system, providing the quantitative analysis necessary for accurate diagnosis, precise treatment planning, and effective monitoring of disease progression [5]. While deep learning has driven considerable progress, the development of specialist models for every unique task, modality, and pathology is inefficient and scales poorly. Such models *Co-first authors. Corresponding to: jintaiCHEN@hkust-gz.edu.cn (J. Chen), xucao2@illinois.edu (X. Cao). lack generalizability and demand extensive, time-consuming manual annotation for each new clinical application. The emergence of large-scale foundation models, such as the Segment Anything Model (SAM) [28, 47], has marked paradigm shift towards building generalist systems that can handle diverse tasks. In the medical field, this approach was successfully validated by models like MedSAM [41], MedSAM-2 [64] and MedSAM2 [43], which adapted the original SAM for medical-specific challenges. MedSAM2, in particular, demonstrated the power of promptable foundation model for segmenting 3D medical images and videos, proving that such systems can drastically reduce manual annotation costs [43]. However, these models primarily rely on geometric prompts, which can still be laborious for complex structures and do not fully capture the rich semantic intent of clinicians. In addition, these models can only serve as one tool, lacking potential to connect with the agentic ecosystem supported by multimodal large language models (LLMs) [2, 33, 58]. The recent introduction of SAM 3 marks significant leap in interactive segmentation with its Promptable Concept Segmentation (PCS) capability [4]. Unlike methods reliant on geometric cues, SAM 3 can detect and segment objects based on open-vocabulary conceptual prompts, such as natural language descriptions (e.g., yellow school bus) or visual exemplars. This ability to operate on semantic concepts presents transformative opportunity for medical imaging, where clinical language is inherently conceptual (e.g., segment the tumor and surrounding edema or identify all enlarged lymph nodes). This directly addresses fundamental limitation of prior text-guidance segmentation models, such as BiomedParse [62], which were constrained to fixed, pre-defined vocabulary and thus could not generalize to the vast and nuanced range of concepts encountered in clinical practice [61]. To address these limitations, we present MedSAM-3, concept-driven framework designed to segment medical imFigure 1. Overview of concept-driven medical image and video segmentation across multiple modalities using MedSAM-3, highlighting that concise clinical concepts directly guide MedSAM-3 to produce reliable segmentations and thereby simplify physicians workflow. agery through semantic guidance (Figure 1). We began by benchmarking the original SAM 3 on multiple medical segmentation datasets to validate its baseline capabilities on both text prompting and visual prompting. However, raw SAM 3 struggled with the healthcare domain. Consequently, we fine-tuned the architecture on curated dataset of diverse medical images paired with rich conceptual labels. The resulting model allows users to segment complex anatomical structures and pathologies using simple text descriptions or visual references from interor intra-scan examples. This paradigm shifts the interaction from simple geometric prompting to an intuitive, clinically aligned semantic workflow. Through comprehensive experiments, we demonstrate that MedSAM-3 not only establishes new state-of-the-art for generalist medical segmentation but also significantly streamlines clinical annotation, paving the way for more intelligent, collaborative medical AI systems. Our main contributions are: We propose MedSAM-3, adapting the SAM 3 architecture to the medical domain to enable precise Promptable Concept Segmentation (PCS) using medical text and visual prompts. We introduce the MedSAM3 Agent, an agentic framework that extends MedSAM-3 to process complex, long-form clinical instructions and improve accuracy through an iterative agent-in-the-loop paradigm. We conduct extensive experiments across diverse medical imaging modalities, demonstrating the effectiveness of our design and providing valuable insights into the deployment of concept-based segmentation models in healthcare. 2. Related Works Segmentation in Medical Images. The field of medical image segmentation has witnessed remarkable evolution, primarily driven by the transition from convolutional neural networks (CNNs) to Transformer-based architectures. Early advancements were cemented by the U-Net series [25, 26, 44, 46, 49, 63]. With the advent of Vision Transformers, researchers sought to overcome the limited receptive field of CNNs, proposing CNN-Transformer hybrid architecture in medical image segmentation [9, 10, 24]. Despite their success, these specialist models typically require training from scratch for specific organs or modalities, limiting their scalability and generalization across the diverse landscape of clinical tasks [5, 29]. The focus has shifted towards developing large-scale foundation models capable of universal segmentation. The SAM [28] demonstrated unprecedented zero-shot generalization in natural images, sparking wave of adaptations for the medical domain. Initial efforts [11, 41, 55] adapted SAM via fine-tuning or adapter layers to handle medical modalities. This was further extended to 3D volumetric data by models [16, 43, 53, 64], which leverage temporal or spatial consistency. While these models excel at geometric prompting (points/boxes), they often lack semantic understanding. Recent works like BiomedParse [62] and UniverSeg [7] have attempted to integrate text guidance. However, as noted in recent studies, these systems are often constrained to fixed vocabularies or lack the reasoning capabilities to interpret complex, open-ended clinical concepts, necessitating shift towards more agentic architectures [43, 47]. Meanwhile, specialized vision language model (VLM) designed for medical image segmentation have emerged [36, 56]. In the 3D domain, models such as M3D-LaMed [6] and other promptable frameworks [32, 35, 39] have further extended these capabilities. However, due to the suboptimal performance and lack of interactivity in these static segmentation VLMs, developing agent-based systems has become promising future trend for handling complex clinical scenarios. Segmentation Agent. The integration of Large Language Models (LLMs) with vision systems has given rise to \"Segmentation Agents\" capable of complex reasoning and interactive understanding. This paradigm moves beyond simple instruction following to \"Reasoning Segmentation,\" where the model must interpret implicit queries (e.g., \"segment the reason for the patients pain\"). Pioneering works in the general domain include LISA [31] and PixelLM [48]. This direction was further advanced by multimodal agents [38, 59] and SAM 3 [4]. These agents differ from static models by maintaining working memory and iteratively refining predictions based on user feedback, critical feature for highstakes decision-making processes in healthcare [45, 57]. In the specialized domain of professional workflows, agentic systems are rapidly transforming how experts interact with data across various verticals. In radiology, agents and models such as MedRAX [17], LLaVA-Med [34], and RadFM [54] have been proposed. Beyond healthcare, similar trends are observed in other fields with systems like mDocAgent [21], NovelSeek [52], ViperGPT [50], and ChemLLM [60]. Our work unifies these directions by proposing MedSAM3 Agent, an agent tailored specifically for medical segmentation that combines the reasoning of LLMs with the precise, conceptdriven segmentation capabilities of the MedSAM-3 architecture. 3. Methodology 3.1. Enabling Medical Concepts in SAM 3 Our MedSAM-3 is developed as generalization of MedSAM-2 and MedSAM2, adopting the unified architecture of SAM 3 to support both the novel Promptable Concept Segmentation (PCS) task and the traditional Promptable Visual Segmentation (PVS) tasks. In the PVS setting, the model accepts diverse visual prompts, such as points, boxes, or masks, to spatially and temporally define individual objects for segmentation. For medical PVS, MedSAM-3 retains the box-based prompting strategy supported by MedSAM2; compared to points and masks, bounding boxes offer less ambiguous method for specifying clinically useful targets, making them particularly effective for delineating organs and lesions. In addition to PVS, MedSAM-3 introduces support for PCS, allowing the model to target objects using short medical noun phrases, including those with positional adjectives. While the original SAM 3 model is optimized for these concise atomic prompts, MedSAM-3 can handle more complex language queries and reasoning by composing the model with MLLM within an agentic pipeline. Figure 2 illustrates the MedSAM-3 architecture, which features dual encoder-decoder transformer design. This consists of detector for image-level capabilities and tracker paired with memory module for video tasks. The memory blocks, inherited from SAM 2, employ transformer layers with self-attention and cross-attention mechanisms to condition current frame features on predictions from previous frames via streaming memory bank. Both the detector and tracker ingest aligned vision-language inputs from shared Perception Encoder (PE) backbone. 3.2. Supervised Fine-Tuning with Medical Concepts Based on the SAM 3 architecture, the MedSAM-3 model freezes the image and text encoder and updates the remaining detector components during fine-tuning. This design preserves the strong visual and concept prior established by SAM 3 while allowing the task-specific modules to adapt to medical concepts efficiently. The model is optimized using paired medical images and concise concept phrases, each limited to no more than three words and selected strictly according to the datasets official documentation or repository descriptions. Such careful curation is motivated by several findings from the SAM 3 evaluation (Section 5) and is intended to ensure semantic precision, minimize ambiguity in textual guidance, and reduce noise arising from overly broad or inconsistent language. Through this approach, MedSAM3 strengthens its ability to map semantic medical concepts to anatomically meaningful structures, ultimately enhancing segmentation robustness across diverse clinical scenarios. 3.3. Scalable Medical Segmentation Agent We introduce the MedSAM-3 Agent (Figure 3), an agentic framework that dynamically reasons, plans, and executes multistep medical segmentation workflows. Unlike previous approaches, the MedSA-3 Agent integrates multimodal reasoning with concept-guided segmentation capabilities. Given any medical image and user request, general MLLM or medical vision-language model (VLM) acts as the core planner: it analyzes the image, devises step-bystep plan, and invokes MedSAM-3 to generate segmentation Figure 2. Overview of MedSAM-3. masks. After each step, the agent inspects the results, using visual and textual feedback to update its understanding of the environment. This perception-action agent-in-the-loop enables the agent to continuously revise its plan and decide the next action. The process continues until the agent is confident it has satisfied the users goal (or determines that no valid mask exists), at which point it returns final set of masks. The resulting pipeline can handle queries far more complex than simple noun phrases, allowing it to understand relationships between anatomical structures and apply visual common sense. We further conduct an experiment to validate Gemini 3 Pro supported MedSAM-3 Agent can surpass MedSAM-3. 4. Experiments and Results 4.1. Datasets To evaluate the segmentation performance of SAM 3 across various medical scenarios and build MedSAM-3, we collected several datasets encompassing multiple imaging modalities, including X-ray, MRI, ultrasound (US), OCT, fundus, dermoscopy, histopathology, nuclear imaging, infrared, endoscopy, and CT, as well as different dimensions, including 2D, 3D, and video. We converted all 3D datasets into frame-sequence formats. When dataset does not provide an official traintest split, we divide the data using an 80%20% (4:1) ratio. The detailed information of these datasets is presented below. COVID-QU-Ex [51]. The COVID-QU-Ex dataset contains 33,920 chest X-ray images and their corresponding infection GTs, including COVID-19 infection, nonCOVID-19 infection, and normal cases. total of 11,956 COVID-19 infection cases were used in this study. BUSI [1]. The Breast Ultrasound Images Dataset (BUSI) includes 780 breast ultrasound images with corresponding breast cancer GTs, covering standard, benign, and malignant cases. iChallenge-GOALS [18]. The Glaucoma OCT Analysis and Layer Segmentation (GOALS) dataset consists of 300 OCT images and their corresponding GTs for the retinal nerve fiber layer, ganglion cell layer, and choroid layer. RIM-ONE [19]. The RIM-ONE retinal dataset includes 485 retinal fundus images with corresponding optic disc and cup GTs, comprising 313 images from healthy subjects and 172 from glaucoma patients. ISIC 2018 [13]. The International Skin Imaging Collaboration (ISIC) 2018 dataset contains 2,594 training images and 1,000 test images of skin images, each with associated lesion GTs. MoNuSeg [30]. The Multi-organ Nucleus Segmentation Challenge (MoNuSeg) dataset includes 51 histology tissue images from patients with tumors of different organs, each providing list of annotated nuclei instances. DSB 2018 [8]. The 2018 Data Science Bowl (DSB 2018) dataset consists of 670 segmented nuclei images acquired under various conditions, differing in cell type, magnification, and imaging modality. RAVIR [23]. The Retinal Arteries and Veins in Infrared Reflectance Imaging (RAVIR) dataset contains 42 infrared reflectance (IR) images and their corresponding retinal artery and vein GTs. Kvasir-SEG [27]. The Kvasir-SEG dataset includes 1,000 gastrointestinal endoscopy images and their corresponding polyp GTs. Parse2022 [40]. The Pulmonary Artery Segmentation Challenge 2022(Parse2022) dataset comprises 100 3D lung CT scans with corresponding pulmonary artery GTs. LiTS [12]. The Liver Tumor Segmentation Challenge Dataset COVID-QU-Ex DSB 2018 BUSI GOALS RIM-ONE ISIC 2018 RAVIR Kvasir-SEG MoNuSeg PolypGen LiTS PROMISE12 ISLES 2024 Parse2022 Phrase input lung infection nuclei breast tumor RNFL & GCIPL & choroid optic cup & optic disc skin lesion retinal arteries & retinal veins polyp nuclei polyp liver & liver tumor prostate ischemic stroke lesion pulmonary artery Table 1. The phrase inputs used for different datasets. datasets are shown in Table 1. The comprehensive evaluation of SAM 3 is presented in Section 5. We conducted supervised fine-tuning to build MedSAM3 on four representative 2D medical datasets from diverse imaging modalities: BUSI, RIM-ONE(Cup), ISIC 2018, and Kvasir-SEG. In addition, the previous prompt-based SOTA medical segmentation model, MedSAM, was included in the evaluation. Critically, our fine-tuning exclusively targeted the detector module of the underlying SAM 3 architecture, specific adaptation designed to optimize domain-specific feature detection for medical tasks. To comprehensively assess the models responsiveness to different types of prompts during the adaptation process, we adopted two distinct finetuning paradigms: 1) Pure Text Prompt Fine-tuning (MedSAM-3 T): In this setting, the model is trained using only the input image and the text description as the prompt. This paradigm aims to enhance the models ability to ground medical concepts in visual features without explicit spatial guidance. 2) Text Prompt + Bounding Box Fine-tuning (MedSAM3 T+I): In this setting, the model is provided with both the semantic text description and bounding box derived from the ground truth mask. This paradigm evaluates the synergistic effect of combining semantic intent with geometric cues to improve segmentation precision. To evaluate the MedSAM-3 Agent, the test set of the BUSI dataset was used for inference, where the agent model was implemented using Gemini 3 Pro. All training and inference experiments were conducted on one or two A100 GPUs, each equipped with 80 GB of memory. 4.3. MedSAM-3 Performance The performance of MedSAM-3 is summarized in Table 2 and Figure 4. The text-only inference (MedSAM-3 T) shows clear limitations across all datasets, indicating that text signals alone are insufficient for reliable medical image segmenFigure 3. Overview of MedSAM-3 Agent refinement loop. The MedSAM-3 Agent plans and executes multi-step medical image segmentation using MLLM, generating masks and refining them iteratively with visual and textual feedback. (LiTS) dataset contains 130 3D abdominal CT scans and their corresponding liver and liver tumor GTs. PROMISE12 [37]. The Prostate MR Image Segmentation 2012 (PROMISE12) dataset includes 80 3D transversal T2-weighted MRI scans and their corresponding prostate GTs. ISLES 2024 [14]. The Ischemic Stroke Lesion Segmentation Challenge 2024 (ISLES 2024) dataset consists of 250 3D brain MRI scans with corresponding ischemic stroke lesion GTs. PolypGen [3]. The PolypGen dataset is an open-access dataset that comprises 1,537 polyp images, 2,225 positive video sequences with polyp GTs, and 4,275 negative frames. 4.2. Experimental settings For the performance evaluation on the 2D datasets, we employed three classical 2D segmentation networks: UNet [49], Unet3+ [25], and Polyp-PVT [15]. For the performance evaluation on the 3D datasets, we also adopted three representative 3D segmentation networks: nn-Unet [26], Swin UNETR [22], and U-Mamba [42]. To ensure fair comparison across different methods, all competing approaches except SAM 3 were trained on the training split of each dataset and evaluated on the corresponding test split. SAM 3 was directly tested on the test set without additional training. For the experiments on the 2D datasets, SAM 3 was evaluated under two settings. In the first setting, the concept input consisted solely of short phrase describing the target, limited to no more than three words(hereafter referred to as SAM 3 T). In the second setting, the concept input included both the textual phrase and bounding box enclosing the largest connected component of the target as an image-based reference(hereafter referred to as SAM 3 T+I). For the experiments on the 3D datasets, only the textual phrase was used as the concept input. The phrase inputs used for different Methods MLLM U-Net [49] MedSAM [41] MedSAM-3 T+I MedSAM-3 Agent Gemini 3 Pro [20] - - - BUSI 0.7618 0.7514 0.7772 0.8064 Table 3. Using Gemini 3 Pro as controlled and evaluation agent can improve the result.  (Table 3)  . By integrating Gemini 3 Pro [20] as the core multimodal LLMorchestrating query interpretation and three rounds of iterative feedbackwe achieved significant performance boost. Specifically, the Dice score improved from 0.7772 to 0.8064. It reveals that agentic workflow could significantly enhance the performance of segmentation models. Figure 4. Performance comparison between MedSAM-3 and competing methods on four medical datasets. 5. Discussion tation. In contrast, the text-and-image inference (MedSAM3 T+I) yields consistent performance gains and achieves the best results on all four benchmarks, demonstrating visual features with medical-domain text priors enhances robustness across medical imaging conditions. In summary, MedSAM3 demonstrates strong potential for extension toward universal medical image segmentation. We also present qualitative visualizations of the segmentation results on the BUSI, RIM-ONE (Cup), ISIC 2018, and Kvasir-SEG datasets. As shown in Figure 5, MedSAM-3 achieves consistently accurate and visually coherent segmentation across diverse modalities, demonstrating strong performance even in challenging low-contrast or irregularboundary regions. In contrast, SAM 3 shows noticeable performance degradation. Remarkably, MedSAM-3 attains these improvements with only small amount of domainspecific fine-tuning data, highlighting the substantial potential of lightweight adaptation for medical image segmentation. More discussion would be posted in Section 5. Methods U-Net MedSAM SAM 3 SAM 3 T+I BUSI 0.7618 0.7514 0 0.7110 RIM-ONE(Cup) ISIC 2018 Kvasir-SEG 0.8480 0.8479 0 0.8303 0.8760 0.9177 0.2189 0.8178 0.8244 0.7657 0 0.7671 MedSAM-3 MedSAM-3 T+I 0.2674 0.7772 0.0080 0.0826 0.8977 0.0497 0.5687 0.9058 0.0119 0.1441 0.8831 0.0587 Table 2. Performance comparison between MedSAM-3 and other methods on four datasets. The best result on each dataset is highlighted in bold. Colored arrows indicate SAM performance changes relative to the best method per dataset. 4.4. MedSAM-3 Agent Performance We compared the performance of MedSAM-3 against its agentic variant, MedSAM3 Agent, using the BUSI test set Prior to developing MedSAM-3, we evaluated the off-theshelf capabilities of SAM 3 within the medical segmentation domain. Our experiments revealed that the standard SAM 3 model lacks the necessary generalization to handle diverse clinical modalities effectively. These limitations motivated the design of MedSAM-3 and the agentic MedSAM-3 Agent framework. 5.1. SAM 3 Performance 5.1.1. SAM 3 Performance on 2D/Video Datasets Table 4 summarizes the segmentation results of SAM 3 on several representative 2D/video medical imaging datasets. Overall, SAM 3 exhibits highly uneven performance, showing strong subject bias. In some datasets, such as RIMONE, the model achieves relatively high accuracy. However, in datasets like DSB 2018 and RAVIR, it almost completely fails. This inconsistency may arise from the models limited sensitivity to medical concepts and its instability in distinguishing semantically similar medical terms. Furthermore, even though part of the evaluation data may have been included in SAM 3s pretraining corpus, its transferability to medical scenarios remains weak, revealing the difficulty of cross-domain generalization. Notably, incorporating bounding box guidance (Text + BBX) leads to substantial improvement in segmentation quality, where the Dice scores approach or even surpass those of conventional supervised methods. This demonstrates the critical role of geometric cues in assisting conceptual understanding, consistent with the findings in the natural domain. In summary, SAM 3 performs inconsistently across 2D medical segmentation tasks, with strong reliance on spatial hints to compensate for its limited grasp of fine-grained medical semantics. This observation suggests that combining conceptual and geometric information remains essential for achieving reliable segmentation in medical imaging. Figure 5. Visualization of the segmentation performance of MedSAM-3, SAM 3(both T+I versions), and other comparison methods. Methods COVID-QU-Ex DSB 2018 U-Net Unet3+ Polyp-PVT 0.7880 0.7928 0. 0.8936 0.9545 0.9420 BUSI 0.7618 0.7782 0.7457 GOALS RIM-ONE(Cup) RIM-ONE(Disc) ISIC RAVIR Kvasir-SEG MoNuSeg PolypGen 0.7902 0.8513 0.8487 0.8480 0.8206 0. 0.9514 0.9545 0.9420 0.8760 0.8797 0.8917 0.7539 0.7681 0.5284 0.8244 0.8321 0.8536 0.6696 0.6595 0.3472 0.6897 0.7634 0. SAM 3 SAM 3 + 0.0305 0.7623 0.7405 0.0523 0.0803 0.8742 0.6953 0.2592 0 0.7782 0.7110 0.0672 0 0.8513 0.8108 0.0405 0 0.8480 0.8303 0. 0.3858 0.5687 0.9270 0.0275 0.2189 0.6728 0.8178 0.0739 0 0.7681 0.2163 0.5518 0 0.8536 0.7671 0.0865 0 0.6696 0.4135 0.2561 0 0.7634 0.6903 0. Table 4. Performance comparison between SAM 3 and traditional segmentation models. The best result on each dataset is highlighted in bold. Colored arrows indicate SAM performance changes relative to the best method per dataset. large gaps on more challenging data such as LiTS and ISLES 2024. These observations indicate that SAM 3 remains limited when applied to volumetric segmentation. Methods Parse2022 nn-Unet Swin UNETR U-Mamba 0.8311 0.8134 0. LiTS 0.7714 0.7425 0.7910 PROMISE12 ISLES2024 0.9011 0.8934 0.9002 0.7718 0.7523 0. SAM 3 0.5295 0.3016 0.1374 0.6536 0.6110 0.2901 0.3033 0.4685 Table 5. Performance comparison between SAM 3 and other methods on 3D medical image datasets. The best result on each dataset is highlighted in bold. Colored arrows indicate SAM performance changes relative to the best method per dataset. Figure 6 also visually illustrates the substantial performance disparities of SAM 3 across different datasets. 5.2. Findings We summarizes the key observations from applying SAM 3 to medical image segmentation tasks, further fine-tuning the model on domain-specific datasets to develop MedSAMFigure 6. Radar charts of different models performance on different datasets. Left: 2D/video scene; Right: 3D scene. 5.1.2. SAM 3 Performance on 3D Datasets Table 5 summarizes the performance of different methods on several 3D medical image datasets. Overall, nn-UNet, Swin UNETR, and U-Mamba achieve relatively stable and high Dice scores across tasks, whereas SAM 3 shows consistently lower performance on all four datasets, with particularly 3, and employing an advanced MLLM to construct the MedSAM-3 Agent. Through systematic evaluation across different modalities and targets, several recurring patterns were identified. These observations reflect the limitations of SAM 3 when applied directly to medical scenarios and the behaviors that persist or newly emerge after task-specific adaptation. The analysis further indicates that fine-tuning can substantially improve performance, yet its effectiveness is constrained by the limited scale of medical datasets and the scarcity of high-quality data containing rich clinical terminology and domain-specific textual descriptions. Moreover, integrating an MLLM-based agent reveals additional potential of MedSAM-3, enabling more flexible interaction and better utilization of medical knowledge. Together, these findings highlight both the challenges and opportunities in adapting general-purpose vision-language models to meet the precision and structural requirements of medical image analysis. The main findings are outlined below. Finding 1: Substantial Performance Discrepancy Between SAM 3 and Established Medical Segmentation Baselines. Across all evaluated datasets, SAM 3 shows large and unusual performance gap compared with standard medical segmentation models. This pattern is consistent across 2D, video, and 3D tasks. representative example is the PROMISE12 dataset. Although PROMISE12 has clear anatomy and minimal semantic ambiguity, some are segmented reasonably well while many others fail severely among the 30 test cases shown in Figure 7. Finding 2: Systematic Misalignment Between Concept Prompts and Anatomical Target Regions in SAM 3. SAM 3 exhibits consistent pattern of misalignment between concepts and the regions it predicts, as shown in Figure 8. In the LiTS liver segmentation task, concept specifying liver often leads the model to segment the left and right lungs while the true liver region is almost entirely ignored. similar phenomenon occurs in the ISIC 2018 dataset. When the concept is lesion, the model frequently produces prediction that covers broad non-lesion areas and excludes the actual lesion. The recurrence of this behavior across tasks suggests that SAM 3 does not reliably associate textual prompts with their corresponding visual targets. Finding 3: Limited Semantic Discrimination of FineGrained Medical Terminology by SAM 3. The SAM 3 model struggles to distinguish between closely related biological or anatomical concepts. We take the MoNuSeg dataset and DSB 2018 dataset for example, shown in Figure 9. The concept nucleus or nuclei was totally unrecognized, even the full name of the MoNuSeg dataset is actually the Multi-organ Nucleus Segmentation Challenge, whereas the more generic prompt cell produced reasonable segmentation results. The details are shown in Table 6. Finding 4: MedSAM-3 Demonstrates Promising Performance Through Efficient Domain Adaptation. Through Methods SAM 3 SAM 3 + MoNuSeg DSB nuclei 0 0.4135 cell nuclei 0.6786 0.7907 0.0803 0. cell 0.5753 0.7384 Table 6. Performance of SAM 3 on the MoNuSeg and DSB 2018 under different concepts. domain-specific fine-tuning with curated medical concept annotations, MedSAM-3 shows encouraging improvements in concept alignment and segmentation reliability across diverse clinical imaging modalities. While further scaling would benefit from broader concept-annotated datasets, the current approach demonstrates meaningful progress toward more generalizable medical segmentation systems. Finding 5: The Agentic Framework Effectively Raises the Performance Ceiling of MedSAM-3. Integrating an MLLM-based agent brings measurable improvements to MedSAM-3, particularly in handling complex clinical instructions and performing iterative refinement. By orchestrating query interpretation through Gemini 3 Pro with iterative feedback loops, the agentic framework elevates segmentation accuracy through multi-step reasoning, intelligent prompt refinement, and error correction. These results highlight the promising potential of agentic workflows to enhance foundation models for increasingly complex and nuanced medical segmentation scenarios in real-world clinical practice. 5.3. Why MedSAM-3? Directly applying SAM 3 in medical scenarios leads to substantial performance degradation because the model lacks the domain-specific semantic grounding required for precise conceptregion alignment. Medical structures often exhibit low contrast, subtle boundaries, and significant inter-patient variability, and these properties are not represented in the natural-image corpus used for SAM 3 pre-training. As result, the model struggles to interpret fine-grained anatomical or pathological terms and frequently produces unstable masks under text-only or weak prompt settings. MedSAM-3 addresses these limitations by introducing medical concepts with explicit, high-quality supervision. The fine-tuning stage exposes the model to medically meaningful terminology, consistent labeling rules, and clinically relevant spatial patterns, enabling it to learn the specialized semantic relationships absent from the original pre-training. This adaptation substantially improves its reliability across modalities and tasks, allowing the model to generate semantically grounded predictions even when visual cues are subtle or ambiguous. Our empirical results show that the improvements brought by the agentic framework depend critically on the quality of the underlying model. The agent can refine prompts and perform iterative correction, but its effectiveness diminishes when the base segmentation is semantically misaligned. In Figure 7. Comparison of per-case performance between SAM 3 and nn-Unet on the PROMISE12 test set. The bar chart shows the absolute Dice values of different methods across cases, while the line plot illustrates the performance differences between the two methods for each case. Figure 8. Examples of SAM 3 inference. On the left, using the LiTS dataset, the model segments the lung regions instead of the liver with the concept liver. On the right, using the ISIC2018 dataset, the model segments surrounding non-lesion regions instead of the lesion with the concept lesion. from broadly capable vision-language model into one that can consistently operate within the constraints and expectations of the medical domain, thereby establishing reliable performance baseline for both direct inference and agentassisted segmentation. 6. Conclusion We propose MedSAM-3, extending the SAM 3 architecture to address the unique challenges of medical concept grounding. Through domain-specific fine-tuning on PCS task, MedSAM-3 significantly outperforms the original SAM 3, particularly in handling complex medical semantics and temporal consistency. We further enhanced this backbone with the MedSAM-3 Agent, an agent-in-the-loop framework that improves usability via iterative feedback. Our analysis reveals that MedSAM-3 determines the fundamental segmentation quality, while the agent leverages reasoning to correct errors and optimize prompts, effectively pushing the performance ceiling. This work demonstrates the potential of coupling domain adaptation with agentic workflows. Future work will address current limitations in concept granularity Figure 9. An example illustrating the large performance gap of SAM 3 on the MoNuSeg and DSB2018 dataset when provided with semantically similar concept inputs. contrast, MedSAM-3 provides stable and medically aligned starting point, ensuring that subsequent agentic reasoning operates on meaningful foundation. Overall, MedSAM-3 serves as the essential bridge between general-purpose foundation models and the precision required in clinical image analysis. It transforms SAM 3 and textimage alignment, with the aim of scaling MedSAM3 to wider range of clinical applications. We will release our code and models to support the community."
        },
        {
            "title": "References",
            "content": "[1] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in brief, 28:104863, 2020. 4 [2] Abdul Mohaimen Al Radi, Xu Cao, Fanyang Yu, Yuyuan Liu, Fengbei Liu, Chong Wang, Yuanhong Chen, Jintai Chen, Hu Wang, Yanda Meng, et al. Agentic large-language-model systems in medicine: systematic review and taxonomy. Authorea Preprints, 2025. 1 [3] Sharib Ali, Debesh Jha, Noha Ghatwary, Stefano Realdon, Renato Cannizzaro, Osama Salem, Dominique Lamarque, Christian Daul, Michael Riegler, Kim Anonsen, et al. multi-centre polyp detection and segmentation dataset for generalisability assessment. Scientific Data, 10(1):75, 2023. 5 [4] Anonymous. SAM 3: Segment anything with concepts. In Submitted to The Fourteenth International Conference on Learning Representations, 2025. under review. 1, 3 [5] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128, 2022. 1, 2 [6] Fan Bai, Yuxin Du, Tiejun Huang, Max Q-H Meng, and Bo Zhao. M3d: Advancing 3d medical image analysis with multi-modal large language models. arXiv preprint arXiv:2404.00578, 2024. [7] Victor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert Sabuncu, John Guttag, and Adrian Dalca. Universeg: Universal medical image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2143821451, 2023. 3 [8] Juan Caicedo, Allen Goodman, Kyle Karhohs, Beth Cimini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim Becker, Minh Doan, Claire McQuin, et al. Nucleus segmentation across imaging experiments: the 2018 data science bowl. Nature methods, 16(12):12471253, 2019. 4 [9] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European conference on computer vision, pages 205218. Springer, 2022. 2 [10] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021. 2 [11] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, and Yu Qiao. Sam-med2d, 2023. 2 Dexuan Xie, Dan Mu, et al. Deep learning-driven pulmonary artery and vein segmentation reveals demography-associated vasculature anatomical differences. Nature Communications, 16(1):2262, 2025. [13] Noel Codella, Veronica Rotemberg, Philipp Tschandl, Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019. 4 [14] Ezequiel de la Rosa, Ruisheng Su, Mauricio Reyes, Roland Wiest, Evamaria Riedel, Florian Kofler, Kaiyuan Yang, Hakim Baazaoui, David Robben, Susanne Wegener, et al. Isles24: Improving final infarct prediction in ischemic stroke using multimodal imaging and clinical data. arXiv preprint arXiv:2408.10966, 2024. 5 [15] Bo Dong, Wenhai Wang, Deng-Ping Fan, Jinpeng Li, Huazhu Fu, and Ling Shao. Polyp-pvt: Polyp segmentation with pyramid vision transformers. arXiv preprint arXiv:2108.06932, 2021. 5 [16] Yuxin Du, Fan Bai, Tiejun Huang, and Bo Zhao. Segvol: Universal and interactive volumetric medical image segmentation. Advances in Neural Information Processing Systems, 37:110746110783, 2024. 3 [17] Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, and Bo Wang. Medrax: Medical reasoning agent for chest x-ray. arXiv preprint arXiv:2502.02673, 2025. 3 [18] Huihui Fang, Fei Li, Huazhu Fu, Junde Wu, Xiulan Zhang, and Yanwu Xu. Dataset and evaluation algorithm design for goals challenge. In International Workshop on Ophthalmic Medical Image Analysis, pages 135142. Springer, 2022. [19] Francisco Fumero, Silvia Alayón, José Sanchez, Jose Sigut, and Gonzalez-Hernandez. Rim-one: An open retinal imIn 2011 24th inage database for optic nerve evaluation. ternational symposium on computer-based medical systems (CBMS), pages 16. IEEE, 2011. 4 [20] Gemini Team, Google. new era of intelligence with gemini 3. Technical report, Google DeepMind, 2025. Accessed: 2025-11-23. 6 [21] Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, and Huaxiu Yao. Mdocagent: multi-modal multiagent framework for document understanding. arXiv preprint arXiv:2503.13964, 2025. 3 [22] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI brainlesion workshop, pages 272284. Springer, 2021. 5 [23] Ali Hatamizadeh, Hamid Hosseini, Niraj Patel, Jinseo Choi, Cameron Pole, Cory Hoeferlin, Steven Schwartz, and Demetri Terzopoulos. Ravir: dataset and methodology for the semantic segmentation and quantitative analysis of retinal arteries and veins in infrared reflectance imaging. IEEE Journal of Biomedical and Health Informatics, 26(7):3272 3283, 2022. 4 [12] Yuetan Chu, Gongning Luo, Longxi Zhou, Shaodong Cao, Guolin Ma, Xianglin Meng, Juexiao Zhou, Changchun Yang, [24] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574584, 2022. 2 [25] Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro Iwamoto, Xianhua Han, Yen-Wei Chen, and Jian Wu. Unet 3+: full-scale connected unet for medical image segmentation. In ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 10551059. Ieee, 2020. 2, 5 [26] Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. 2, 5 [27] Debesh Jha, Pia Smedsrud, Michael Riegler, Pål Halvorsen, Thomas De Lange, Dag Johansen, and Håvard Johansen. Kvasir-seg: segmented polyp dataset. In International conference on multimedia modeling, pages 451462. Springer, 2019. 4 [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 1, 2 [29] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Medclip-sam: Bridging text and image towards universal medical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 643653. Springer, 2024. 2 [30] Neeraj Kumar, Ruchika Verma, Deepak Anand, Yanning Zhou, Omer Fahri Onder, Efstratios Tsougenis, Hao Chen, Pheng-Ann Heng, Jiahui Li, Zhiqiang Hu, et al. multiorgan nucleus segmentation challenge. IEEE transactions on medical imaging, 39(5):13801391, 2019. [31] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 3 [32] Wenhui Lei, Wei Xu, Kang Li, Xiaofan Zhang, and Shaoting Zhang. Medlsam: Localize and segment anything model for 3d ct images. Medical Image Analysis, 99:103370, 2025. 3 [33] Binxu Li, Tiankai Yan, Yuanting Pan, Jie Luo, Ruiyang Ji, Jiayuan Ding, Zhe Xu, Shilong Liu, Haoyu Dong, Zihao Lin, et al. Mmedagent: Learning to use medical tools with multi-modal agent. arXiv preprint arXiv:2407.02483, 2024. 1 [34] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. 3 [35] Mingjian Li, Mingyuan Meng, Shuchang Ye, Michael Fulham, Lei Bi, and Jinman Kim. Language-guided medical image segmentation with target-informed multi-level contrastive alignments. arXiv preprint arXiv:2412.13533, 2024. 3 [36] Zihan Li, Yunxiang Li, Qingde Li, Puyang Wang, Dazhou Guo, Le Lu, Dakai Jin, You Zhang, and Qingqi Hong. Lvit: language meets vision transformer in medical image segmentation. IEEE transactions on medical imaging, 43(1):96107, 2023. [37] Geert Litjens, Robert Toth, Wendy Van De Ven, Caroline Hoeks, Sjoerd Kerkstra, Bram Van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge. Medical image analysis, 18(2):359 373, 2014. 5 [38] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2359223601, 2023. 3 [39] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation and tumor detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2115221164, 2023. 3 [40] Gongning Luo, Kuanquan Wang, Jun Liu, Shuo Li, Xinjie Liang, Xiangyu Li, Shaowei Gan, Wei Wang, Suyu Dong, Wenyi Wang, et al. Efficient automatic segmentation for multilevel pulmonary arteries: The parse challenge. arXiv preprint arXiv:2304.03708, 2023. 4 [41] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024. 1, 2, 6 [42] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing longrange dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722, 2024. [43] Jun Ma, Zongxin Yang, Sumin Kim, Bihui Chen, Mohammed Baharoon, Adibvafa Fallahpour, Reza Asakereh, Hongwei Lyu, and Bo Wang. Medsam2: Segment anything in 3d medical images and videos. arXiv preprint arXiv:2504.03600, 2025. 1, 3 [44] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565571. Ieee, 2016. 2 [45] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan Krumholz, Jure Leskovec, Eric Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259265, 2023. 3 [46] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018. 2 [47] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 1, 3 [48] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel Ouyang, et al. Chemllm: chemical large language model. arXiv preprint arXiv:2402.06852, 2024. [61] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, et al. Biomedparse: biomedical foundation model for image parsing of everything everywhere all at once. arXiv preprint arXiv:2405.12971, 2024. 1 [62] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, et al. foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nature methods, 22(1): 166176, 2025. 1, 3 [63] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: nested u-net architecture for medical image segmentation. In International workshop on deep learning in medical image analysis, pages 311. Springer, 2018. 2 [64] Jiayuan Zhu, Abdullah Hamdi, Yunli Qi, Yueming Jin, and Junde Wu. Medical sam 2: Segment medical images as video via segment anything model 2. arXiv preprint arXiv:2408.00874, 2024. 1, 3 reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. 3 [49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 2, 5, [50] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1188811898, 2023. 3 [51] Anas Tahir, Muhammad EH Chowdhury, Amith Khandakar, Tawsifur Rahman, Yazan Qiblawey, Uzair Khurshid, Serkan Kiranyaz, Nabil Ibtehaz, Sohel Rahman, Somaya Al-Maadeed, et al. Covid-19 infection localization and severity grading from chest x-ray images. Computers in biology and medicine, 139:105002, 2021. 4 [52] NovelSeek Team, Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Xiaohan He, Songtao Huang, Shaowei Hou, Zheng Nie, et al. Novelseek: When agent becomes the scientistbuilding closed-loop system from hypothesis to verification. arXiv preprint arXiv:2505.16938, 2025. 3 [53] Haoyu Wang, Sizheng Guo, Jin Ye, Zhongying Deng, Junlong Cheng, Tianbin Li, Jianpin Chen, Yanzhou Su, Ziyan Huang, Yiqing Shen, et al. Sam-med3d: vision foundation model for general-purpose segmentation on volumetric medical images. IEEE Transactions on Neural Networks and Learning Systems, 2025. 3 [54] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Hui Hui, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. Nature Communications, 16(1):7866, 2025. 3 [55] Junde Wu, Ziyue Wang, Mingxuan Hong, Wei Ji, Huazhu Fu, Yanwu Xu, Min Xu, and Yueming Jin. Medical sam adapter: Adapting segment anything model for medical image segmentation. Medical image analysis, 102:103547, 2025. 2 [56] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1815518165, 2022. [57] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. 3 [58] Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, et al. survey on agentic multimodal large language models. arXiv preprint arXiv:2510.10991, 2025. 1 [59] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820228211, 2024. 3 [60] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli"
        }
    ],
    "affiliations": [
        "Southeast University",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "University of Illinois Urbana-Champaign",
        "Xian Jiaotong University"
    ]
}