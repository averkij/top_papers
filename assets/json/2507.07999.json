{
    "paper_title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology",
    "authors": [
        "Haochen Wang",
        "Xiangtai Li",
        "Zilong Huang",
        "Anran Wang",
        "Jiacong Wang",
        "Tao Zhang",
        "Jiani Zheng",
        "Sule Bai",
        "Zijian Kang",
        "Jiashi Feng",
        "Zhuochen Wang",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human \"thinking with images\". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 9 9 9 7 0 . 7 0 5 2 : r Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology Haochen Wang1,2, Xiangtai Li3, Zilong Huang3, Anran Wang3, Jiacong Wang2,3 , Tao Zhang3, Jiani Zheng3, Sule Bai3, Zijian Kang3, Jiashi Feng3 , Zhuochen Wang3, Zhaoxiang Zhang1,2 1NLPR, MAIS, CASIA 2UCAS 3ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human thinking with images. However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual questionanswering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing visiongrounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR. Date: July 11, 2025 Correspondence: wanghaochen2022@ia.ac.cn, zhaoxiang.zhang@ia.ac.cn"
        },
        {
            "title": "Introduction",
            "content": "Recent breakthroughs in Large Language Models (LLMs) reasoning, such as OpenAI-o1 [42] and DeepSeekR1 [15] with remarkable test-time scaling properties, have motivated researchers to explore reasoning for Large Multimodal Models (LMMs) [6, 18, 64, 65]. These models are typically remarkable in their mathematical and scientific reasoning, particularly through text-space reasoning. However, they exhibit critical limitations when applied to perception-heavy tasks [19] or general multimodal benchmarks [62], primarily due to accumulated language bias from their exclusive reliance on textual reasoning pathways. paradigm shift toward visual grounded reasoning emerged with models like OpenAI-o3 [43], which is able to think with images by dynamically referencing and amplifying task-relevant regions during reasoning, resulting in image-text interleaved reasoning pathways. Yet, despite growing interest, the community currently lacks comprehensive evaluation benchmarks for assessing these capabilities. Classical benchmarks like POPE [26], MMBench [30], SEED-Bench [24], and MMMU [71] usually overlook 1 Figure 1 Qualitative examples from TreeBench for each discipline. Each question requires focused visual parsing on mere objects, and some even request second-order reasoning beyond precise localization. Moreover, the bounding boxes of all target instances are provided, ensuring traceable evaluation. All these questions are challenging, as OpenAI-o3 [43] and Gemini-2.5-Pro [10] cannot answer them correctly simultaneously. fine-grained localization and verifiable reasoning chains. Others [11, 55, 56, 63, 66, 72, 73] partially address localization but lack traceability or complex reasoning: V* Bench [66] is restricted to simple spatial queries (e.g., Is left of B?) and risks data contamination with COCO-derived images [27]; MME-RealWorld [72], HR-Bench [63], and document benchmarks [4, 31, 39] support high-resolution inputs but lack traceable evidence and second-order reasoning such as perspective shifts. In short, these benchmarks fail to adequately evaluate three key elements central to visual grounded reasoning: nuanced visual grounding, traceable multi-step reasoning, and dynamic cross-modal interaction through interleaved box-text reasoning pathways. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), designed around three foundational principles essential for evaluating true thinking with images capabilities: Focused Visual Perception. It evaluates models ability to identify subtle targets within cluttered, real-world scenes using detailed, precise, and unique textual descriptions, which requires hierarchical scene understanding and the discrimination of extremely similar distractors. Traceable Evidence. It not only evaluates the final accuracy but also pioneers quantifiable evaluation of reasoning chains, resulting in an explainable, reliable, and transparent evaluation. Vision-Centric Second-Order Reasoning Capabilities. It moves beyond simple object localization and primitive what/where queries. It focuses on complex physical interactions between objects (such as contact and occlusion), as well as spatial containment (inside/outside, above/below) and relative relationships with perspective transformation. To conduct TreeBench, we sample 1K images from SA-1B [20], prioritizing images with dense objects, as SA-1B [20] offers high-resolution, real-world scenes with large number of small and varied objects, making it particularly suitable for evaluating visual grounded reasoning. Subsequently, 8 experts with solid technical backgrounds are involved in hand-crafted annotation for 10 sub-tasks, as demonstrated in Figure 1. In particular, we present semi-automated pipeline. Each of OpenAI-o3 [43] and Gemini-2.5-Pro [10] is required to create three distinct questions belonging to specific subtask, accompanied by multiple-choice options and the respective correct answers. Subsequently, experts curated or replaced these to ensure quality and Benchmark Resolution V* Bench [66] HR-Bench-4K [63] HR-Bench-8K [63] MME-RealWorld [72] TreeBench 2,2461,583 4,0233,503 5,7274,430 2,0761,434 2,1521,615 Traceable Evidence Annotation Mean Area of Target Objects () 3.05% Qwen2.5-VL-72B Performance () 85.9 79.3 76.0 62.9 42.2 Table 1 Comparison between benchmarks related to thinking with images. TreeBench features traceable evidence annotations, as well as high input resolution and challenging questions. indicates the lower, the more challenging. difficulty. We additionally incorporate cross-verification stage for further quality control. Finally, TreeBench incorporates 405 high-quality and extremely challenging VQA pairs with accurate bounding boxes of target instances. comprehensive comparison between our TreeBench and other related benchmarks is provided in Table 1. Key advantages are summarized as follows: Annotation Quality. Unlike benchmarks relying on LMM-generated labels such as MMT-Bench [69] and SEED-Bench [24], our expert-driven process ensures correctness and extreme difficulty. However, relying on models would inevitably introduce significant noise, compromising the quality of the annotations. On the contrary, our TreeBench is manually designed by 8 LMM experts, ensuring the annotation correctness and ensuring the difficulty of all questions. Small Target Objects. All questions in TreeBench focus on extremely small objects in complex real-world scenes, where target instances occupy an average of 3.05% of the image. Traceable Evidence Evaluation. Our TreeBench provides bounding box annotations of each target instance. It not only evaluates the final answer, but also reveals the quality of intermediate reasoning steps. Those predicted bounding boxes serve as window into its process, helping to diagnose the source of errors, i.e., whether the model misunderstood the question or failed to locate the relevant object. Task Difficulty. While models approach saturation (>90%) on benchmarks like V* Bench [66], even open-sourced state-of-the-art performers like Qwen2.5-VL-72B [2] achieve only 42.2 on our TreeBench, implying large potential improvement for future works. Beyond evaluation, we further introduce TreeVGR (Traceable Evidence for Visual Grounded Reasoning), training paradigm enhancing localization-driven visual reasoning. Previous attempts like [5, 13, 32, 44, 46, 50, 60, 76] solely supervise final answers and neglect intermediate region-of-interest generation processes. It becomes hard to quantify the actual contribution of the grounding-then-answering framework. On the contrary, we propose TreeVGR, novel training methodology emphasizing traceable evidence through reinforcement learning (RL), which explicitly supervises bounding box generation. Building on RL with conventional accuracy-based and formatting rewards, TreeVGR leverages novel dual IoU reward to ensure both precision and recall in localizing the ground-truth bounding boxes for each target instance. To implement this, we curate 37K samples for RL training, each comprising an image, question, an answer, and corresponding bounding box annotations for all target instances. Empirically, initialized from Qwen2.5-VL-7B [2], TreeVGR brings significant improvements on various benchmarks, i.e., +16.8 on V* Bench [66], +12.6 on MME-RealWorld-Lite [72], and +13.4 on our TreeBench. Moreover, as illustrated in Figure 2, compared with related approaches, our TreeVGR enables traceable and explainable reasoning pathways with more accurate localizations (mIoU), and finally contributes to bootstrapped overall performance. Figure 2 Normalized performance comparison with our TreeVGR and other works [2, 50, 76] on our TreeBench for each category. 3 In conclusion, TreeBench pioneers the evaluation of how models think with images, while TreeVGR establishes blueprint for training them. Together, they significantly advance the depth and utility of multimodal reasoning assessment with traceable evidence."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multimodal Models. Initial breakthroughs in Large Multimodal Models (LMMs), such as Flamingo [1] and BLIP-2 [25], achieved this by integrating visual features into the LLM backbone via cross-attention mechanisms. significant shift towards efficiency emerged with LLaVA [28], which introduced much more efficient approach. It projects visual features from pre-trained encoder (e.g., CLIP [45]) directly into the LLMs semantic space using simple two-layer MLP. This paradigm of feature projection catalyzed rapid advancement. Subsequent research has dramatically scaled LMM capabilities and tackled increasingly complex tasks [2, 22, 23, 29, 5759, 61, 67, 77]. critical frontier has been handling high-resolution inputs. Models like LLaVA-NeXT [29] and InternVL-1.5 [8] adopt any resolution strategy. Qwen2-VL [61] and Qwen2.5-VL [2] introduce multimodal Rotary Position Embedding (mROPE) to support arbitrary resolution inputs. Beyond resolution, scaling pretraining with high-quality data is also vital, as demonstrated by InternVL3 [77]. Collectively, these models represent the state-of-the-art, forming robust baselines for diverse real-world multimodal applications. Our work builds upon these advances by leveraging their strong native visual grounding capabilities. However, existing LMMs do not naturally perform an explicit \"groundingthen-answering\" process, often resulting in misaligned or incomplete responses. By explicitly modeling this sequential process, our approach ensures more accurate and interpretable answers through grounded reasoning. Reasoning LMMs. The groundbreaking reasoning capabilities of LLMs, exemplified by systems like OpenAIo1 [42] and DeepSeek-R1 [15] have motivated efforts to extend similar competencies to multimodal settings using reinforcement learning (RL) [51]. Early approaches primarily focused on equipping LMMs to solve complex math and science problems involving image inputs [6, 18, 64, 65]. Other approaches [3, 33, 48] directly adopt GRPO [47] to open-ended visual grounding. Moreover, some attempts [34, 40, 44, 46] focus on regions-of-interest localization before actually answering the question. recent milestone, OpenAI-o3 [43], advanced multimodal reasoning by enabling dynamic image manipulation, e.g., cropping and zooming into regions of interest, to emulate human-like \"thinking with images.\" Subsequent research has sought to replicate this capability through diverse strategies: constructing SFT data [60], vanilla RL [13], framing grounding as function [76], decoupling grounding and answering [5], multi-task reinforcement learning [32], and curiosity-driven reasoning [50]. Critically, these RL-based methods supervise only the final answer. In contrast, our TreeVGR emphasizes traceable evidence during RL training, i.e., supervising generated bounding boxes to ensure precise localization throughout the reasoning process. By doing so, TreeVGR enables more transparent, reliable, and fine-grained control over the reasoning pipeline. Benchmarks for LMMs. Current benchmarks lack comprehensive evaluation of multimodal models ability to think with images, capability demanding three core competencies: (1) focused visual perception (identifying small targets in large scenes), (2) traceable evidence (evaluating generated bounding boxes for explainability), and (3) second-order reasoning (deriving insights beyond precise instance localization). Some benchmarks may partially satisfy the first condition. While some benchmarks address isolated aspects, critical gaps persist. Classical benchmarks like POPE [26], MMBench [30], SEED-Bench [24], and MMMU [71] usually overlook fine-grained localization and verifiable reasoning chains. V* [66] evaluates detailed attributes and spatial relationships (e.g., Is left of B?) but relies on COCO-derived images [27], introducing high contamination risk. MME-RealWorld [72] and HR-Bench [63] support high-resolution inputs but lack traceable evidence, and their questions often become easy when grounded precisely. Crucially, no benchmark integrates all three requirements, particularly the need for complex reasoning conditional on precise grounding, e.g., perspective transform: From the perspective of person A, what is the relative direction of object B?. To bridge this gap, we propose TreeBench, the first benchmark designed explicitly for thinking with images with traceable, multistep evaluation. Beyond accuracy, TreeBench assesses: (1) region quality, i.e., faithfulness of generated regions-of-interest in visual reasoning chains, and (2) second-order reasoning, i.e., capabilities requiring inference beyond localization. State-of-the-art models, Gemini-2.5-Pro [10] and OpenAI-o3 [43], perform poorly on TreeBench (<60%), underscoring its rigor and the unmet challenges in multimodal reasoning."
        },
        {
            "title": "3 TreeBench",
            "content": "TreeBench is designed to address critical gap in multimodal evaluation by establishing the first comprehensive benchmark for assessing thinking with images capabilities. Specifically, it mainly evaluates (1) the ability of identifying small target objects with long, detailed, and unique text captions in large, complex, and real-world scenes, (2) the explainability of reasoning pathways and traceable evidence, and (3) second-order reasoning beyond precise localization. Our TreeBench systematically evaluates 10 core competencies through 405 distinct questions, organized into two progressive protocols, i.e., Perception and Reasoning, with representative examples demonstrated in Figure 1."
        },
        {
            "title": "3.1 Task Definition\nPerception evaluates the model’s ability to accurately “see” and “identify” specific content, which is one of the\nbasic capabilities of directly extracting and interpreting visual information from every detail of the provided\nimage. These tasks primarily evaluate first-order visual reasoning capabilities, where correct answers usually\ndepend on the accurate localization of target questions (e.g., objects, regions, or text) and directly recognize\ntheir explicit attributes without requiring higher-level logical inference or abstract conceptualization. The\nsuccess hinges on precise perceptual grounding. It includes:",
            "content": "1. Attributes evaluates the ability to identify and describe specific visual properties (e.g., color, shape, material, or precise classification) of objects or elements within images, particularly requiring attention to fine details, subtle distinctions, and accurate recognition of small-scale or context-dependent features. 2. Material measures the ability to analyze and distinguish material properties (e.g., texture, surface finish, composition, or physical state) through visual cues such as light reflection, transparency, wear patterns, or microscopic structural characteristics, requiring precise reasoning about tactile qualities and material-specific visual indicators. 3. Physical State assesses the ability to assess structural integrity (e.g., damage, wear, or breakage), detect positional states (e.g., open/closed, bent/straight), and interpret age-related features (e.g., freshness, decay) through precise analysis of visual cues like cracks, alignment anomalies, lighting/shadow patterns, or contextual degradation markers. 4. Object Retrieval probes the ability to interpret linguistically complex, spatially explicit descriptions and map them to visually subtle or contextually embedded targets in images, testing the integration of natural language understanding, spatial grounding, and discriminative object recognition under high specificity constraints. 5. OCR-Integrated Question-Answering evaluates the ability to extract text-based questions and answer options from images, requiring seamless integration of OCR, natural language understanding, and multimodal alignment to produce accurate responses grounded in both textual and visual modalities. Reasoning evaluates the ability to analyze and infer meaningful conclusions beyond recognition. These tasks demand second-order visual reasoning capabilities, where correct answers require not only accurate localization but also higher-level cognitive operations over aggregated visual evidence. Precise perceptual grounding is just the first step for these tasks. It includes: 1. Perspective Transform measures the capacity to perform viewpoint transformations (e.g., aligning viewer-centric and agent-centric frames of reference) and interpret spatial relations under mirror-reversed or perspective-shifted conditions, testing the ability to disambiguate directional relationships that depend on the visualized entitys orientation rather than the images literal pixel layout. 2. Ordering evaluates the ability to analyze linearly ordered arrangements of objects (e.g., left-to-right, front-to-back, or depth-based sequences) and resolve ordinal relationships by integrating spatial context with discriminative feature recognition, requiring precise localization within continuous layouts and contextual comparison of positional cues (e.g., adjacency, centrality, or extremity) to answer questions dependent on sequential alignment and relative placement. 5 3. Contact and Occlusion measures the ability to analyze physical interactions between multiple objects (e.g., direct contact, occlusion layers, or shadow-based overlaps) and resolve ambiguities in object identification by leveraging spatial dependencies, requiring precise parsing of contact cues (e.g., alignment, boundary fusion), occlusion boundaries (e.g., partial/full coverage, layer stacking), and contextual constraints to answer questions that hinge on understanding how objects physically coexist and obscure one another in complex scenes. 4. Spatial Containment benchmarks the ability to analyze hierarchical spatial relationships (e.g., containment, surface attachment, or regional boundaries) by parsing visual cues like object boundaries, spatial context, and contextual containment rules, requiring precise interpretation of containment hierarchies, surface dependencies, and regional constraints to resolve questions dependent on explicit spatial membership rather than isolated positional attributes. 5. Comparison assesses to compare attributes across multiple objects (e.g., distance, size, color) and resolve spatial or perceptual differences, requiring precise parsing of attribute discrimination and contextual distance estimation to answer questions demanding explicit comparison of visually co-present entities."
        },
        {
            "title": "3.2 Annotation Pipeline\nThe TreeBench dataset was constructed through a systematic pipeline combining automated sampling, LMM-\nassisted generation, and three rounds of human validation. The annotation team contains eight human experts\nin LMMs, including six Ph.D candidates and two senior research scientists.",
            "content": "Image Selection. total of 1K images are initially sampled from the SA-1B [20], with deliberate prioritization of images containing high-density objects (e.g., scenes with overlapping or clustered items), as SA-1B [20] offers high-resolution, real-world scenes with large number of small and varied objects, making it particularly suitable for evaluating visual grounded reasoning. To ensure balanced representation across categories, 100 images are initially allocated per category. First Round Quality Control. The annotation team manually evaluates the relevance and quality of each image for its assigned category. This step is critical for addressing category-specific requirements, e.g., the Ordering category necessitates images with visually similar or repetitive objects for practical reasoning tasks. Following this review, 647 images meet the criteria for inclusion in subsequent stages. Automated Question Generation. Question-option-answer trios are then generated using two advanced LMMs, i.e., OpenAI-o3 [43] and Gemini-2.5-Pro [10], each tasked with producing three diverse, high-quality questions per image. Prompts are designed to emphasize task-specific complexity and visual-semantic alignment. Second Round Quality Control. Human experts then manually review all six model-generated questions per image. For each image, annotators selected the most semantically coherent and task-relevant question from the pool of six, prioritizing: (1) alignment with the target subtask, (2) avoidance of trivial or ambiguous object referring, and (3) clarity and unambiguous answerability. If none of the six questions met these criteria, annotators manually constructed new question tailored to the image. This step ensured that only high-quality, human-vetted questions advanced to the next stage. Difficulty Filtering. Questions deemed insufficiently challenging are removed through model-based consensus screening. Specifically, any question answered correctly by all four state-of-the-art vision-language models (Qwen2.5-VL-72B [2], InternVL3-78B [77], GPT-4o [41], Gemini-2.5-Flash [9]) was excluded to ensure the benchmark retained meaningful difficulty. Third Round Quality Control. The final cross-verification phase engages independent human annotators to cross-validate the accuracy and relevance of each question-option-answer pair. The final dataset comprised 405 rigorously validated questions."
        },
        {
            "title": "3.3 Statistics\nDistribution of Each Subtask. As demonstrated in Figure 3, TreeBench emphasizes advanced reasoning tasks,\naccounting for 63% of the total subtasks (256 questions), while basic perception-related tasks constitute",
            "content": "6 Figure 3 Distribution of each discipline in TreeBench, which prioritizes reasoning over perception. Figure 4 The ground-truth distribution of TreeBench with 3 instances of and 4 instances of F. Figure 5 Distribution of the number of instances in TreeBench, with one question with 8 target instances. 37% (149 questions). Within the reasoning category, key subtasks reflect focus on complex spatial and relational understanding. This structure underscores deliberate prioritization of higher-order reasoning over foundational perceptual tasks, aligning with the goal of challenging models to process nuanced relationships and transformations rather than mere object recognition or attribute detection. Distribution of Answers. As illustrated in Figure 4, the ground-truth distribution of TreeBench is dominated by four main categories: (28.6%, 116 instances), (27.9%, 113 instances), (27.9%, 113 instances), and (13.8%, 56 instances). These account for 98.2% of the total 405 instances. The remaining 1.8% (7 instances) includes (3 instances) and (4 instances). This structure highlights balanced emphasis on categories A, B, and C, with as notable secondary group, while and represent minor but distinct components. Distribution of the Number of Target Instances. Figure 5 shows the distribution of the number of target instances per question. The majority of questions in TreeBench require identifying 1 or 2 target instances, accounting for 41.5% (168 questions) and 44.9% (182 questions) of the total, respectively. Questions requiring 3, 4, 5, or 6 targets constitute smaller fractions: 4.2% (17 questions), 4.0% (16 questions), 4.0% (16 questions), and 1.2% (5 questions), respectively. Notably, single question (highlighted in gray) demands 8 target instances, representing an extreme case. Overall, 86.4% of questions focus on 12 targets, suggesting balance between simplicity and complexity in task design while incorporating rare multi-target scenarios for comprehensive evaluation. Distribution of Target Instance Area. We compute the relative area for each target instance using its bounding box, i.e., area = 1 HW (y2 y1)(x2 x1), where and are the input resolution. Figure 6 is the histogram of the mean area for each question. It illustrates that the majority of target instances in TreeBench are extremely small, with sharp peak near 0.0 and long tail extending to larger areas (up to 0.7). The mean area across all questions is 0.0305, confirming that targets are predominantly tiny. Most questions (highest frequency bin) involve target instances with areas clustered around 0.0 to 0.05, while only small fraction require identifying larger objects. This distribution highlights the importance of addressing challenging scenarios where small-scale object detection and reasoning are crucial, potentially compromising model performance due to limited visual information. Figure 6 The histogram of mean target instance areas per question with low average of 0.0305 (indicating small target instances). 7 Figure 7 Training pipeline of TreeVGR, including (a) cold-start initialization stage and (b) reinforcement learning with traceable evidence post-training stage."
        },
        {
            "title": "4 TreeVGR",
            "content": "In this section, we introduce our TreeVGR. Specifically, we leverage the native grounding capabilities of pre-trained LMMs and unlock visual grounded reasoning capabilities, i.e., localizing regions-of-interest first and answering the question next, through two-stage training pipeline shown in Figure 7, i.e., cold initialization introduced in Section 4.1 and reinforcement learning with traceable evidence elaborated in Section 4.2. Notably, our TreeVGR does not require actually replaying cropped images as previous approaches [50, 60, 76] do, as text-space grounding is already effective. It leads to much more efficient training and inference procedures."
        },
        {
            "title": "4.1 Cold-Start Initialization",
            "content": "While end-to-end reinforcement learning (RL) has demonstrated validity by [76] for visual grounded reasoning (VGR) tasks, its practical deployment remains hindered by extreme computational demands. Specifically, DeepEyes-7B [76] requests RL training on 47K samples across 32 episodes, process requiring 32 H100 (80GB) GPUs operating continuously for 48 hours. Such resource intensity creates barriers to iterative experimentation and broader accessibility. To address these limitations, we investigate computationally efficient alternative. Initial attempts revealed significant training inefficiencies when applying direct RL to VGR: models required extensive iterations to autonomously identify task-relevant visual regions before generating answers. This bottleneck motivates our adoption of cold initialization strategy as illustrated in Figure 7a. Specifically, we introduce supervised fine-tuning (SFT) phase using curated dataset comprising multimodal samples: each sample includes an image, question, reasoning trajectories with corresponding bounding boxes, and final answer. This structured initialization ensures VGR capabilities are established prior to RL. Data Construction. We base our supervised fine-tuning (SFT) dataset on VGR-158K [60], which provides 8 pseudo-chain-of-thought annotations paired with bounding boxes for visual reasoning tasks. However, to align with the grounding capabilities of our base model (Qwen2.5-VL series [2]), which outputs absolute coordinates rather than the normalized coordinates (ranging from 0 to 1) used by LLaVA-NeXT [29] in [60], we perform coordinate system conversion. Specifically, for each bounding box, we transform normalized coordinates [rx1 , ry1, rx2 , ry2] into absolute coordinates via [x1, y1, x2, y2] = [W rx1, Hry1, rx2 , Hry2], where is the resolution of the input image. Next, we filter samples to prioritize complex reasoning pathways, retaining only entries with multiple bounding boxes (i.e., more than one box per reasoning trajectory). This yields 35K samples, as multi-box interactions demand stronger spatial-temporal reasoning compared to single-box tasks. Subsequently, we construct reflective subset of 4.7K samples among them by introducing controlled perturbations: for each sample, we (1) inject synthetic error by inserting randomly generated incorrect bounding box into the reasoning sequence, and (2) append the meta-cognitive prompt Wait, this box seems to be wrong immediately afterward, resulting in our TreeVGR-SFT-35K. This design explicitly trains the model to detect and correct erroneous visual grounding, which is critical skill for robust real-world deployment. Optimization. Initialized from Qwen2.5-VL-7B-Instruct [2], we train TreeVGR-7B-CI (CI here stands for Cold Initialization) with 8 GPUs using LLaMA-Factory [74], where the AdamW optimizer [36] with learning rate of 5e-6 and global batch size of 256 is utilized. The learning rate is decayed following cosine schedule [35] with warmup ratio of 0.1."
        },
        {
            "title": "4.2 Reinforcement Learning with Traceable Evidence\nWe proceed to reinforcement learning (RL) to refine reasoning trajectories through traceable evidence supervision\nas demonstrated in Figure 7b. Specifically, the bounding boxes generated are evaluated using a box intersection-\nover-union (IoU) reward, a precise and interpretable metric that measures the alignment between predicted\nand ground-truth regions. This reward ensures explicit accountability to human-annotated visual evidence,\nguiding the policy toward spatially accurate and logically coherent reasoning pathways.",
            "content": "Reward Design. The total reward consists of three parts: an accuracy reward Racc {0, 1}, formatting reward Rformat {0, 1}, and dual Intersection-over-Union (IoU) reward RIoU [0, 1]: = Racc + Rformat + RIoU, (1) where the accuracy reward assesses whether the final answer is correct. We utilize exact-matching for multiplechoice questions, and leverage an online reward model, i.e., Qwen2.5-72B-Instruct [52], to judge whether the prediction is correct given the question and the ground-truth answer. The formatting reward ensures the reasoning process and the final answer must be enclosed between <think> and </think>, and <answer> and </answer>, respectively. The dual IoU reward measures the quality of predicted boxes against ground-truths. Specifically, for predicted bounding boxes {ˆbi}N , i=1 where bk = [xk , where ˆbi = [ˆxi 2 ], the dual IoU is an average of recall term and precision term. 2] and ground-truths {bk}M 1 , xk 2, yk 1, yk 1, ˆxi 1, ˆyi 2, ˆyi k=1 RIoU = 1 2 (RR IoU + RP IoU), (2) where the first term RR recall term ensures that each ground-truth bounding box bk is matched with at least one prediction. indicates the recall and the second term RP means the precision. Specifically, the IoU IoU RR IoU ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) k=1 (cid:104)"
        },
        {
            "title": "IoU",
            "content": "{ˆbi}N i=1, bk (cid:105) , (3) (cid:104) {ˆbi}N (cid:105) i=1, bk = maxi IoU(ˆbi, bk) indicates the maximum IoU between all predictions {ˆbi}N i=1 and where IoU each ground-truth bk. Maximizing this term ensures each ground-truth bk is matched with at least one prediction. However, we empirically find that the policy model tends to enumerate all possible boxes to obtain larger recall. Therefore, we introduce dual term, i.e., RP , to ensure the precision and discourage empty boxes that do not match with any ground-truths: IoU RP IoU ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:104)"
        },
        {
            "title": "IoU",
            "content": "{bk}M k=1, ˆbi (cid:105) . i=1 (4)"
        },
        {
            "title": "Material",
            "content": "State Phy. Retr. Obj."
        },
        {
            "title": "R\nC\nO",
            "content": "Trans. Per."
        },
        {
            "title": "Ordering",
            "content": "Oc. & Con. Cont. Spa."
        },
        {
            "title": "Comparison",
            "content": "Overall mIoU Perception Private Models Reasoning Gemini-2.5-Flash-0520 [9] GPT-4o-1120 [41] Gemini-2.5-Pro-0605 [10] o3-0416 [43] LLaVA-OneVision-7B [23] LLaVA-OneVision-72B [23] Qwen2.5-VL-7B [2] Qwen2.5-VL-32B [2] Qwen2.5-VL-72B [2] InternVL3-8B [77] InternVL3-38B [77] InternVL3-78B [77] DeepEyes-7B [76] Pixel-Reasoner-7B [50] 45.9 46.9 54.1 54.8 37.3 40.5 37.0 42.5 42.2 38.8 42.0 46.4 37.5 39.0 TreeVGR-7B v.s. Qwen2.5-VL-7B 50.4 13. 48.3 51.7 51.7 69.0 53.9 61.5 61.5 69.2 69.6 65.2 56.5 65.2 68.8 43.8 75.0 68. 75.0 69.1 83.8 79.4 Open-source General Models 55.2 62.1 55.2 51.7 65.5 51.7 51.7 62.1 53.8 53.8 53.8 53.8 69.2 69.2 61.5 61.5 56.5 65.2 56.5 69.6 56.5 56.5 52.2 52.2 50.0 62.3 62.5 62.5 56.3 56.3 68.8 68. 32.4 36.8 27.9 54.4 48.5 33.7 51.5 52.9 15.3 18.8 20.0 22.4 21.2 12.9 20.0 16.5 11.8 21.2 12.9 16.5 19.3 38.6 36.8 38.6 22.8 28.1 35.1 33.3 33.3 24.6 33.3 33.3 56.1 48.8 65.9 61. 41.5 53.7 39.0 46.3 51.2 39.0 56.1 61.0 72.4 72.4 86.2 86.2 72.4 65.5 44.8 62.1 72.4 72.4 65.5 86.2 43.2 43.2 54.6 50.0 36.4 47.7 43.2 38.6 38.6 43.2 38.6 45.5 Open-source Visual Grounded Reasoning Models 30.0 35.7 44.0 62.1 58.6 53.8 61.5 65.2 65.2 68.8 50. 51.5 48.5 11.8 14.1 24.6 31.6 36.6 39.0 53.8 65.5 11.7 0.0 82.6 26. 68.8 6.3 63.3 35.4 22.4 2.2 36.8 1.7 61.0 22.0 51.7 44. 69.0 24.2 47.7 40.9 45.5 2.3 Table 2 Selected results of different models on TreeBench. Evaluations of open-source general models are implemented using VLMEvalKit [12], while evaluations of visual grounded reasoning models are conducted by us. Reasoning pathways of o3 [43] are unavailable, and thus traceable evaluations are not valid. Best performances for open-source models are highlighted in bold. Our TreeVGR-7B achieves comparable performance with InternVL3-78B [77]. (cid:104) (cid:105) {bk}M = maxk IoU(bk, ˆbi) indicates the maximum IoU between all ground-truths bk k=1, ˆbi Similarly, IoU and each prediction {ˆbi}N . Maximizing this term encourages each prediction ˆbi to be matched with at least i=1 one ground-truth. Therefore, simultaneous optimization of both recall and precision eliminates the need for exhaustive enumeration of bounding boxes, thereby contributing to more accurate reasoning pathways. Data Construction. As discussed above, TreeVGR incorporates novel dual IoU reward, which means each sample should contain ground-truth bounding boxes during the RL phase. To this end, we filter hard samples from the original 191K training set of V* [66] using Qwen2.5-VL-7B-Instruct [2], resulting in 30K samples. Additionally, we incorporate the VisDrone dataset [78], which is originally designed for detection and tracking under UAV images, which offers extremely high-resolution, real-world scenes with large number of small and varied objects and their corresponding bounding box annotations. We reformulate the training set and the validation set into 38K multiple-choice counting problems, and only retain samples with the ground-truth number ranging from 5 to 10, contributing to the final 7K samples. Finally, our TreeVGR-RL-37K consists of 30K open-ended question-answering samples from V* [66] and 7K multiple-choice problems from VisDrone [78]. Optimization. Initialized from TreeVGR-7B-CI, we train our final TreeVGR-7B with 8 GPUs, with another 8 GPUs serving the reward model, i.e., Qwen2.5-72B-Instruct [52], using vLLM [21]. We adopt Group Relative Policy Optimization (GRPO) [47], which has been proved to be effective and efficient for diverse tasks. We have also tried DAPO [70], but we find it unstable compared with GRPO. Therefore, we simply utilize the original GRPO [47]. We implement using EasyR1 [75], which is clean fork of veRL [49]. We train our TreeVGR-7B with 5 epochs on TreeVGR-RL-37K, which is significantly less than DeepEyes-7B [76] (which is trained on 47K samples with 32 epochs). 10 Figure 8 Distribution of IoU for each question in TreeBench. Figure 9 Distribution of IoU for each question in TreeBench-Perception. Figure 10 Distribution of IoU for each question in TreeBench-Reasoning. Figure 11 Performance decoupling with AI2D [17]. Figure 12 Performance decoupling with MathVista [37]. Figure 13 Performance decoupling with MMStar [7]."
        },
        {
            "title": "5.1 Main Properties on TreeBench\nResults on TreeBench. We evaluate four state-of-the-art private models, including GPT-4o-1120 [41] and o3-\n0416 [43] from OpenAI, and Gemini-2.5-Flash-0520 [9] and Gemini-2.5-Pro-0605 [10] from Google DeepMind.\nAdditionally, 8 representative open-source general models are incorporated, including LLaVA-OneVision\nseries [23], Qwen2.5-VL series [2], and InternVL3 series [77]. Furthermore, two very recent visual grounded\nreasoning models are also included, i.e., DeepEyes [76] and Pixel-Reasoner [50], as both of them follow a\n“grounding then answering” pipeline, with the capability of “thinking with images”. Table 2 presents per\nper-category performance of different models. Overall, OpenAI’s o3-0416 [43], the state-of-the-art visual\ngrounded reasoning model, demonstrates the strongest perception abilities, as expected. Larger models\nusually contribute to better performance, while Qwen2.5-VL-32B [2] and its 72B counterpart as an exception.\nNotably, our TreeVGR-7B even achieves comparable performance with InternVL3-78B [77], demonstrating the\neffectiveness of the visual grounded reasoning pipeline.",
            "content": "Correlation between Localization and Performance. Importantly, for visual grounded reasoning models, our traceable evaluation demonstrates positive correlation between localization precision and the overall performance, as illustrated in Table 2. This positive correlation between precise localization (mIoU) and overall performance is evident in the progressive improvement from DeepEyes-7B [76] to Pixel-Reasoner-7B [50] to our final TreeVGR-7B. As mIoU increases, the overall scores rise correspondingly, with TreeVGR-7B achieving the highest mIoU and strongest overall performance at the same time. Beyond global analysis, we further plot the histogram of IoU for each question in Figure 8, where blue bars represent wrong predictions and orange bars are correct predictions. Overall, wrong predictions tend to have smaller IoU values. However, by going deeper through the lens of perception and reasoning, the relationship between mIoU and performance diverges. Precise localization (mIoU) aligns closely with perception performance demonstrated in Figure 9. In contrast, as shown in Figure 10, reasoning performance reveals weaker correlation with mIoU, as improvements in localization alone fail to fully translate to complex reasoning tasks. This disconnect suggests that reasoning questions of TreeBench require second-order cognitive 11 V* Bench [66] HR-Bench-4K [63] HR-Bench-8K [63] Overall Attr. Spatial Overall Single Cross Overall Single Cross GPT-4o-1120 [41] o3-0416 [43] LLaVA-OneVision-7B [23] LLaVA-OneVision-72B [23] InternVL3-8B [77] InternVL3-38B [77] InternVL3-78B [77] Qwen2.5-VL-7B [2] Qwen2.5-VL-32B [2] Qwen2.5-VL-72B [2] Private Models Open-source General Models 73.0 80.9 73.0 77.4 75.7 77.4 83.5 90.8 60.5 63.2 71.1 77.6 77.6 69.7 89.5 80.9 64.3 66.3 70.8 76.3 75.5 72.1 74.8 79.4 74.8 76.5 79.3 83.5 84.5 88.8 89.3 88. 53.8 56.0 62.3 69.0 66.5 55.5 60.3 70.0 66.0 95.7 70.7 73.8 72.3 77.5 76.4 74.3 85.9 84.8 Open-source Visual Grounded Reasoning Models Pixel-Reasoner-7B [50] DeepEyes-7B [76] 80.6 90. 83.5 92.1 76.3 86.8 TreeVGR-7B v.s. Qwen2.5-VL-7B 91.1 16.8 94.0 16.6 87.0 17. 72.9 75.1 77.1 5.0 86.0 91.3 90.3 1.5 60.3 59.0 64.0 8. 59.8 60.9 62.0 67.0 67.3 68.8 71.6 76.3 66.9 72.6 73.1 4.3 65.3 68.8 64.3 71.3 71.8 83.5 86.5 84.3 80.0 86.8 86.5 3.0 54.3 53.0 59.8 62.8 62.8 54.0 56.8 68.3 54.3 58.5 59.8 5. Table 3 Comparison with state-of-the-art alternatives on V* Bench [66] and HRBench [63]. All results are self-collected. Best performances of visual grounded reasoning models are highlighted in bold. capabilities that go beyond precise spatial localization. Correlation with Other Multimodal Benchmarks. We systematically compare our TreeBench with three existing multimodal benchmarks: AI2D [17], MathVista [37], and MMStar [7], in Figure 11, Figure 12, and Figure 13, respectively, to investigate potential performance correlations. Our analysis reveals decoupling of performance characteristics. For instance, while GPT-4o-1120 [41] ranks among the top performers on TreeBench, it lags significantly behind alternatives on other benchmarks. This dissociation underscores the unique emphasis on thinking with images of our TreeBench."
        },
        {
            "title": "5.2 Main Properties of TreeVGR\nBaselines and Benchmarks. We mainly compare our TreeVGR with state-of-the-art private models, including\nGPT-4o-1120 [41] and o3-0416 [43] from OpenAI, and Gemini-2.5-Flash-0520 [9] and Gemini-2.5-Pro-0605 [10]\nfrom Google DeepMind. Additionally, 8 representative open-source general models are incorporated, including\nLLaVA-OneVision series [23], Qwen2.5-VL series [2], and InternVL3 series [77]. Furthermore, two very recent\nvisual grounded reasoning models are also included, i.e., DeepEyes [76] and Pixel-Reasoner [50], as both of\nthem follow a “grounding then answering” pipeline, with the capability of “thinking with images”. Evaluations\nare mainly conducted on TreeBench, V* Bench [66], HR-Bench [63], and MME-RealWorld-Lite [72].\nResults on High-Resolution Benchmarks. As demonstrated in Table 2, our TreeVGR-7B achieves 45.2 on\nour TreeBench, even surpasses Qwen2.5-VL-72B [2], which is comparable with open-source state-of-the-art\nInternVL3-78B [77]. Moreover, compared with visual grounded reasoning models, our TreeVGR not only\nachieves a higher overall performance, but also obtains larger mIoU, indicating its effectiveness in precisely\nlocalizing target objects. Additionally, in Table 3, TreeVGR also achieves open-source state-of-the-art on V*\nBench [66] with 94.3 accuracy, which is even comparable with o3 [43]. On HR-Bench [63] and MME-RealWorld-\nLite [72] illustrated in Table 3 and Table 4, respectively, our TreeVGR brings significant improvements over\nour base model, Qwen2.5-VL-7B [2].",
            "content": "In Table 5, we compare our TreeVGR with its base model Results on Other Multimodal Benchmarks. Qwen2.5-VL-7B [2] on variety of conventional multimodal benchmarks. Specifically, we select CV-Bench [53], MMVP [54], and RealWorldQA [68], to evaluate vision-centric question-answering capabilities. MMBench [30], 12 Perception Reasoning Overall OCR RS DT MO AD OCR DT MO AD Qwen2.5-VL-7B [2] Qwen2.5-VL-32B [2] Qwen2.5-VL-72B [2] LLaVA-OneVision-7B [23] LLaVA-OneVision-72B [23] InternVL3-8B [77] InternVL3-38B [77] InternVL3-78B [77] Pixel-Reasoner-7B [50] DeepEyes-7B [76] 42.3 45.6 43.7 43.7 48.7 47.9 51.0 52.3 49.7 53.2 TreeVGR-7B v.s. Qwen2.5-VL-7B 54.9 12. General Models 87.6 87.2 90.8 80.0 79.2 83.6 85.6 87.6 32.7 40.7 34.0 40.0 50.7 49.3 56.0 54.7 83.0 83.0 87.0 56.0 67.0 75.0 71.0 77.0 27.3 29.5 27.9 31.7 37.9 34.5 42.6 42.6 30.0 40.7 30.6 39.4 40.0 36.9 40.0 36. 72.0 74.0 74.0 65.0 76.0 70.0 77.0 76.0 62.0 60.0 61.0 33.0 41.0 44.0 45.0 56.0 28.7 27.3 26.7 38.0 38.7 40.0 47.3 46.0 23.0 29.5 25.5 32.0 39.3 37.0 35.0 40.3 Visual Grounded Reasoning Models 89.6 90. 87.6 0.0 52.0 52.7 50.7 18.0 86.0 89.0 83.0 0.0 38.9 43. 30.9 33.4 47.0 19.7 43.4 13.4 71.0 76.0 74.0 2.0 72.0 69. 66.0 4.0 46.0 44.0 32.5 35.0 51.3 22.6 39.0 16.0 Table 4 Comparison with state-of-the-art alternatives on MME-RealWorld-Lite [72]. All results are self-collected. The best performance is highlighted in bold. Capability Benchmark Qwen2.5-VL-7B [2] Vision-centric question answering General visual question answering Document and chart CV-Bench-2D [53] CV-Bench-3D [53] MMVP [54] [30] MMBenchen dev POPE [26] HallusionBench [14] AI2Dtest [17] ChartQAtest [38] 74.1 72.6 66.7 83.1 86.7 48.2 84.9 85. TreeVGR-7B 76.9 2.8 77.6 5.0 75.3 8.6 84.4 1.3 87.2 0.5 50.1 1.9 84.8 0.1 85.8 0.2 Qwen2.5-VL-72B [2] 77.7 87.0 66.7 88.6 84.9 55.2 88.7 89.5 Table 5 Comparison with state-of-the-art alternatives on other multimodal benchmarks. Results are obtained from [16], otherwise are self-collected. POPE [26], and HallusionBench [14] are selected for evaluating general VQA capabilities, and AI2D [17] and ChartQA [38] for comprehension with document and chart. We observe significant improvements in most cases, especially for vision-centric benchmarks. Notably, TreeVGR-7B achieves 75.3 on MMVP [54], even surpasses Qwen2.5-VL-72B [2] by significant margin."
        },
        {
            "title": "5.3 Ablation Studies\nThe core contribution of TreeVGR is the traceable training pipeline, where the dual IoU reward RIoU is\nincorporated in conventional RL training. Therefore, we aim to evaluate the effectiveness of including this\ntraceable term. As demonstrated in Table 6, we ablate each component of our TreeVGR, including the\ncost-start initialization and reward functions.",
            "content": "The cold-start stage is quite beneficial for visual grounded reasoning, when compared with 1 and 2. This means the formatting of outputting bounding boxes of target instances is useful for conventional visual grounded reasoning benchmarks like V* Bench [66] and MME-RealWorld-Lite [72]. Note that these benchmarks can be regarded as Out-of-Domain (OOD) samples for the SFT dataset. Traceable visual grounded reasoning is more effective than untraceable one, when compared with 3 and 4. Starting from the same cold-start checkpoint, integrating dual IoU rewards into the RL framework yields substantial performance gains, particularly on our TreeBench and MME-RealWorld-Lite [72], which represent out-of-distribution (OOD) scenarios relative to the RL training data. Notably, on TreeBench, our TreeVGR 13 Rewards TreeBench V* [66] MME-RW [72] Cold-Start Racc + Rformat RR IoU RP 1 Qwen2.5-VL-7B [2] 2 Cold-Start 3 TreeVGR 4 w/o Traceable Evidence 5 w/o Precision 6 w/o Recall 7 Text-Only RL IoU Acc mIoU Acc 71.2 37.0 76.4 23.4 39.0 50.4 44.0 91.1 87.9 27.2 38.0 0.0 78.3 0.0 89.5 20.6 86.9 45.4 39.0 Acc 42.3 48.4 54.9 51.6 0.0 52.6 46.3 Table 6 Ablations of each component of our TreeVGR. MME-RW stands for MME-RealWorld-Lite [72], and Acc represents the multiple-choice accuracy. This improvement mainly comes from the training set, as many training samples from V* [66] are included in RL. The model tends to enumerate candidate boxes to obtain larger IoU recall, and fails to produce final answers. demonstrates significant enhancements in both overall accuracy and mIoU. This dual improvement suggests that precise and interpretable reasoning pathways are critical for achieving optimal performance, underscoring the value of structured reward design in complex, real-world tasks. The precision term is crucial for alleviating the repetition problem, when compared with 3 and 5. As illustrated in Figure 14, without precision, the mean response length grows rapidly. When evaluating this model, we find that it tends to enumerate candidate bounding boxes to obtain larger IoU recall and thus always fails to produce final answers. The recall term is crucial for precise and complete localization, when compared with 3 and 6. On TreeBench, without the recall term, the model achieves significant accuracy improvements, but the localization accuracy (mIoU) remains limited, usually grounding incomplete target instances. Vanilla text-only RL is not so effective as visual grounded reasoning, when compared with 3 and 7. Vanilla RL in text-based tasks demonstrates value through its text-space reasoning capabilities. However, when integrating visual grounded reasoning with traceable evidence, the performance gains become more significant. This highlights the critical role of two factors: (1) pre-answer contextual grounding to anchor responses in multimodal evidence, and (2) accurate spatial localization to refine decision-making precision. Figure 14 Mean response length with different IoU rewards. The precision term is crucial for alleviating the repetition problem."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces TreeBench, benchmark designed to rigorously evaluate visual grounded reasoning (VGR) or thinking with images in large multimodal models, and TreeVGR, two-stage training framework that enhances VGR methods through traceable evidence supervision. TreeBench addresses critical gaps in existing benchmarks by focusing on three principles: focused visual perception (identifying subtle targets in cluttered scenes), traceable evidence (quantifiable reasoning chains via bounding box annotations), and vision-centric second-order reasoning. Constructed through expert-driven annotation and multi-stage quality control, TreeBench features 405 high-difficulty visual question-answer pairs with precise bounding boxes, emphasizing small objects in real-world scenarios. It reveals the limitations of state-of-the-art models, e.g., OpenAI-o3 [43] scores 54.8%, while setting new standard for assessing nuanced visual grounding, multi-step reasoning transparency, and cross-modal interaction. TreeVGR advances VGR training through reinforcement learning guided by dual IoU rewards, which explicitly supervise bounding box generation to ensure both precision and recall. This approach enables explainable reasoning pathways and achieves significant improvements across benchmarks. TreeVGR achieves superior 14 performance in just 5 epochs compared to 32 epochs for [76]. Limitation and future works. The current implementation of TreeVGR is based on 7B parameter model, which may limit scalability compared to larger architectures. TreeBench contains only 405 rigorously curated question-answer pairs. Expanding the benchmark with additional samples across broader domains would further challenge model capabilities. Scaling up would be future work."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS), 35:2371623736, 2022. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning. arXiv preprint arXiv:2505.14231, 2025. [4] Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, and Manmatha. Latr: Layout-aware transformer for scene-text vqa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Meng Cao, Haoze Zhao, Can Zhang, Xiaojun Chang, Ian Reid, and Xiaodan Liang. Ground-r1: Incentivizing grounded visual reasoning via reinforcement learning. arXiv preprint arXiv:2505.20272, 2025. [6] Liang Chen, Lei Li, Haozhe Zhao, and Yifan Song. R1-v: Reinforcing super generalization ability in vision-language models with less than $3, 2025. [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, arXiv preprint Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [9] Google DeepMind. Gemini-2.5-flash. https://deepmind.google/models/gemini/flash/, 2025. [10] Google DeepMind. Gemini-2.5-pro. https://deepmind.google/models/gemini/pro/, 2025. [11] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption. arXiv preprint arXiv:2405.19092, 2024. [12] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [13] Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. [14] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 15 [17] Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. [18] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [19] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer In Proceedings of the IEEE/CVF Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. International Conference on Computer Vision (ICCV), pages 40154026, 2023. [21] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [22] Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, and Zilong Huang. The scalability of simplicity: Empirical analysis of vision-language learning with single transformer. arXiv preprint arXiv:2504.10462, 2025. [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [24] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning (ICML), pages 1973019742, 2023. [26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740755, 2014. [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 36:3489234916, 2023. [29] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. https://llava-vl.github.io/blog/2024-01-30-llava-next/, 2024. [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. [31] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [32] Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025. [33] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [34] Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. [35] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 16 [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [37] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [38] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [39] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2021. [40] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. Kam-cot: In Proceedings of the AAAI Conference on Knowledge augmented multimodal chain-of-thoughts reasoning. Artificial Intelligence (AAAI), volume 38, pages 1879818806, 2024. [41] OpenAI. Openai-gpt-4o. https://openai.com/index/gpt-4o-system-card/, 2024. [42] OpenAI. Openai-o1. https://openai.com/o1/, 2024. [43] OpenAI. Openai-o3. https://openai.com/index/introducing-o3-and-o4-mini/, 2025. [44] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236, 2024. [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 87488763, 2021. [46] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual-cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. [47] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [48] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [49] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [50] Alex Su, Haozhe Wang, Weimin Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. [51] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [52] Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [53] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems (NeurIPS), 37:8731087356, 2024. [54] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95689578, 2024. [55] Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, Yulin Wang, et al. Geollava-8k: Scaling remote-sensing multimodal large language models to 8k resolution. arXiv preprint arXiv:2505.21375, 2025. 17 [56] Fengxiang Wang, Hongzhen Wang, Zonghao Guo, Di Wang, Yulin Wang, Mingshuo Chen, Qiang Ma, Long Lan, Wenjing Yang, Jing Zhang, et al. Xlrs-bench: Could your multimodal llms understand extremely large ultra-high-resolution remote sensing imagery? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1432514336, 2025. [57] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [58] Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. In International Conference on Learning Representations (ICLR), 2025. [59] Jiacong Wang, Bohong Wu, Haiyong Jiang, Zhou Xun, Xin Xiao, Haoyuan Guo, and Jun Xiao. World to code: Multi-modal data generation via self-instructed compositional captioning and filtering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 46084623, 2024. [60] Jiacong Wang, Zijiang Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025. [61] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [62] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [63] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 39, pages 79077915, 2025. [64] Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, and Lichao Sun. Unsupervised post-training for multi-modal llm reasoning via grpo. arXiv preprint arXiv:2505.22453, 2025. [65] Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, and Weiran Huang. Advancing multimodal reasoning via reinforcement learning with cold start. arXiv preprint arXiv:2505.22334, 2025. [66] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1308413094, 2024. [67] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [68] xAI. Grok. 2024. [69] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. [70] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [71] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [72] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. 18 [73] Yuan Zhang, Tao Huang, Chun-Kai Fan, Hongyuan Dong, Jiawen Li, Jiacong Wang, Kuan Cheng, Shanghang Zhang, Haoyuan Guo, et al. Unveiling the tapestry of consistency in large vision-language models. Advances in Neural Information Processing Systems, 37:118632118653, 2024. [74] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. [75] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [76] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [77] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [78] Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Heng Fan, Qinghua Hu, and Haibin Ling. Detection and tracking meet drones challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 44 (11):73807399, 2021. Appendix This appendix provides qualitative examples and failure cases of our TreeVGR on each category of TreeBench."
        },
        {
            "title": "A Attributes",
            "content": "Figure 15 Qualitative examples (left) and failure cases (right) on the Attributes protocol of TreeBench."
        },
        {
            "title": "B Material",
            "content": "Figure 16 Qualitative examples (left) and failure cases (right) on the Material protocol of TreeBench."
        },
        {
            "title": "C Physical State",
            "content": "Figure 17 Qualitative examples on the Physical State protocol of TreeBench."
        },
        {
            "title": "D Object Retrieval",
            "content": "Figure 18 Qualitative examples (left) and failure cases (right) on the Object Retrieval protocol of TreeBench. 22 OCR-Integrated Question-Answering Figure 19 Qualitative examples on the OCR-Integrated Question-Answering protocol of TreeBench."
        },
        {
            "title": "F Perspective Transform",
            "content": "Figure 20 Qualitative examples (left) and failure cases (right) on the Perspective Transform protocol of TreeBench."
        },
        {
            "title": "G Ordering",
            "content": "Figure 21 Qualitative examples (left) and failure cases (right) on the Ordering protocol of TreeBench."
        },
        {
            "title": "H Contact and Occlusion",
            "content": "Figure 22 Qualitative examples (left) and failure cases (right) on the Contact and Occlusion protocol of TreeBench."
        },
        {
            "title": "I Spatial Containment",
            "content": "Figure 23 Qualitative examples (left) and failure cases (right) on the Spatial Containment protocol of TreeBench."
        },
        {
            "title": "J Comparison",
            "content": "Figure 24 Qualitative examples (left) and failure cases (right) on the Comparison protocol of TreeBench."
        }
    ],
    "affiliations": [
        "ByteDance",
        "NLPR, MAIS, CASIA",
        "UCAS"
    ]
}