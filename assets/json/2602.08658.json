{
    "paper_title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
    "authors": [
        "Mingzi Cao",
        "Xingwei Tan",
        "Mahmud Akhter",
        "Marco Valentino",
        "Maria Liakata",
        "Xi Wang",
        "Nikolaos Aletras"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks."
        },
        {
            "title": "Start",
            "content": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models Mingzi Cao α*, Xingwei Tan α*, Mahmud Akhter β, Marco Valentino α Maria Liakata β γ, Xi Wang α, Nikolaos Aletras α αSchool of Computer Science, University of Sheffield βSchool of Electronic Engineering and Computer Science, Queen Mary University of London γThe Alan Turing Institute {mcao20,xingwei.tan,m.valentino,xi.wang,n.aletras}@sheffield.ac.uk {m.akhter,m.liakata}@qmul.ac.uk 6 2 0 2 9 ] . [ 1 8 5 6 8 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs reasoning behavior. To this end, we first collect new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform dense model to mixtureof-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to 14.60) across realistic tasks."
        },
        {
            "title": "Introduction",
            "content": "Deduction, induction, and abduction are fundamental reasoning paradigms, forming the cornerstone of human logical reasoning (Frankfurt, 1958; Salmon, 1994). Deduction infers conclusions based on rules and observations. Induction abstracts rules based on patterns in premises and observations, and Abduction infers the most plausible explanation to account for an observation, given set of rules. These paradigms play crucial role in understanding and developing the reasoning capabilities of large language models (LLMs) (Dougrez-Lewis et al., 2025; Tan et al., 2025; Hu et al., 2025). * Equal contribution. 1 Our code is available at https://github.com/ voalmciaf/FR-OOD. Figure 1: We induce fundamental reasoning skills with the trajectories generated over symbolic tasks, assessing generalization to realistic natural language tasks. Previous work often conflates induction, deduction, and abduction, overlooking how they individually contribute to improving LLM reasoning (Zheng et al., 2025; Hu et al., 2025). While Tan et al. (2025) individually induce deductive skills via fine-tuning (FT) to improve OOD generalization, they neglect induction and abduction. Moreover, it is common practice to use data that entangles world knowledge and reasoning to improve LLM reasoning (Hu et al., 2025; Guha et al., 2025; DeepSeekAI, 2025), making it unclear whether gains stem from genuine reasoning or factual recall. Consequently, two main questions remain unexplored: (1) to what extent do distinct deduction, induction, and abduction skills improve general reasoning; and (2) what are the most effective methods for inducing these capabilities? To answer these questions, we first enrich procedurally-generated symbolic reasoning dataset introduced by Hu et al. (2025) with reasoning trajectories from teacher LLMs. The dataset consists of tasks, each targeting one of the three fundamental paradigms while abstracting the inference from world knowledge. However, it only consists of questions and answers without associated trajectories. The trajectories serve as demonstrations for inference patterns independent of world knowledge. Subsequently, we induce student LLMs with fundamental reasoning abilities, investigating the impact and interplay of deduction, induction, and abduction. We systematically evaluate, for the first time, battery of inducing methods beyond standard fine-tuning, including up-scaling to increase model depth (Wu et al., 2024), allowing the effective absorption of new reasoning capabilities; and upcycling converting standard LLMs to Mixtureof-Experts (MoE) to scale model capacity while avoiding interference between existing and newly induced information (Komatsuzaki et al., 2023). Finally, we comprehensively evaluate the induced models both in-domain (i.e., on the same symbolic tasks) and Out-of-Domain (OOD) on the rest of the symbolic tasks and realistic tasks requiring realworld knowledge, across two model families (see Figure 1). Our contributions are as follows: We collect and release new symbolic reasoning dataset for deduction, induction, and abduction. It contains total of 17K symbolic problems enriched with more than 160K reasoning trajectories generated via two teacher LLMs from different families. We show that inducing smaller student LLMs with fundamental reasoning skills, decoupled from world knowledge, yields substantial performance gains across realistic OOD tasks (up to 14.60) across different inducing methods and LLM families. We find that deductive abilities generally offer the highest OOD performance gains across models. We also observe strong crossparadigm improvements, which reveal how fundamental reasoning skills in isolation benefit LLM reasoning as whole. On the other hand, the most effective inducing method is model-dependent, with upcycling achieving the best overall results."
        },
        {
            "title": "2 Related Work",
            "content": "Eliciting Reasoning in LLMs Large reasoning models integrate step-by-step reasoning during post-training (Olmo et al., 2025). Recent work explores how fundamental reasoning paradigms impact the effectiveness of LLM reasoning. Zheng et al. (2025) evaluate different logical inference paradigms, but they only comparing inductive reasoning against the combination of abduction and deduction, lacking understanding of individual fundamental reasoning paradigms. Hu et al. (2025) separately specialize LLMs on deductive, inductive, and abductive reasoning, and subsequently merge the resulting models, conflating all reasoning paradigms together. This makes it difficult to isolate how each individual fundamental reasoning paradigm impacts performance. Therefore, we learn each specific fundamental reasoning paradigm individually via training on reasoning trajectories collected by teacher LLMs. This approach provides clearer and fine-grained understanding of their impact on LLM reasoning for the first time. Synthetic Reasoning Trajectories LLM reasoning requires large-scale trajectories in post-training (Liu et al., 2024; Yang et al., 2025). However, human-curated reasoning data is often noisy (Morishita et al., 2024), and difficult to scale (Sun et al., 2024). Therefore, synthetic reasoning trajectories from teacher models are increasingly used to train student models (DeepSeek-AI, 2025; Olmo et al., 2025). While they offer substantial gains in general (Guha et al., 2025) and specialized domains (Yu et al., 2024; Zhao et al., 2025; Yu et al., 2025), the trajectories include world knowledge, making it difficult to isolate fundamental reasoning improvements. Tan et al. (2025) use symbolic reasoning trajectories to decouple world knowledge but focus exclusively on deductive reasoning, leaving other fundamental reasoning paradigms unexplored."
        },
        {
            "title": "3 Fundamental Reasoning Data",
            "content": "Our aim is to investigate the impact of inducing LLMs with the three fundamental reasoning types: deduction, induction, and abduction. For this purpose, we need corresponding trajectories."
        },
        {
            "title": "3.1 Symbolic Reasoning Tasks",
            "content": "We enrich an existing fundamental reasoning dataset developed by Hu et al. (2025). We opted for this data because it is publicly available and comprises 3,600 deductive, 4,500 inductive, and 9,000 abductive questions with their answers, but without associated reasoning trajectories that are essential to our study. The data is procedurally-generated based on three symbolic tasks (Figure 2). Deduction. The task consists of Boolean Satisfiability (SAT) questions, requiring the truth value of the variables to satisfy given formula. All problems are guaranteed to be satisfiable. The original questions list the formulas separately without using Conjunctive Normal Form (CNF), i.e., standardmost likely set of premises (antecedent truth values). However, the original task instructions do not clarify that known atoms do not necessarily equate to true atoms, leading to misinterpretations by LLMs. Moreover, the original answers only check for goal reachability despite requiring LLMs to identify related premises and trace back the truth values of the known atoms. This oversimplification causes the model to simply check whether the goals appear in the premises. To overcome these limitations, we keep the original formulas but modify the task instructions and the answers by searching the possible truth values of known atoms that make the reachable goal true using Prolog (Wielemaker et al., 2012). Table 3 (Appendix A) shows comparison between the original and our abductive task."
        },
        {
            "title": "3.2 Reasoning Trajectories Sampling",
            "content": "Data Splits. From each task type, we use 100 questions and answer pairs for testing and 100 for development, reserving the remaining for training. We collect reasoning trajectories only for the training set. question-answer pair and its trajectories only appear in one of the splits. Teacher Models. We sample reasoning trajectories using two widely used open-weight LLMs: Qwen3-30B-Instruct (Yang et al., 2025) and Llama3.3-70B-Instruct (Grattafiori et al., 2024).2 Sampling. Questions are fed to teacher LLM (see Appendix for prompts) in zero-shot setting. For each question, we sample five responses using different random seeds. We set the maximum number of generated tokens to 10,000, ensuring the trajectories reach the conclusion. We exclude responses with fewer than 20 words since such short texts are unlikely to contain valid reasoning trajectories. For the Qwen model, this results in filtering out 554 deductive, 2,430 inductive, and 19,872 abductive trajectories. However, no trajectories from the Llama model were filtered out. We find that the filtered responses of Qwen are either empty or strings including random tokens, perhaps generated due to confusion. Due to the fact that automatically verifying the logic consistency of textual reasoning trajectories remains an open problem and LLMs learn more from structure instead of content (Li et al., 2025a), we do not filter out more trajectories. The final training set comprises 16,000 questions together with 82,000 and 60,000 trajec2We use the vLLM API (Kwon et al., 2023), ensuring efficient inference. Figure 2: Symbolic reasoning tasks, and the fundamental reasoning inducing process. ized way to write Boolean formulas as conjunction of clauses, where each clause is disjunction of variables or their negations. Our preliminary experimentation showed that this creates confusion for LLMs. Furthermore, the original answers are presented in plain text, which complicates automated evaluation. Therefore, we modify the original formulas into CNFs and express the answers in JSON for collecting reasoning trajectories. Induction. This task requires the model to predict the next number in given sequence of numbers, representing basic form of inductive reasoning. The numerical sequences consist of repeating patterns that involve addition, subtraction, or multiplication. We use the original input-output pairs without any modifications. Abduction. The task uses logic formulas and known atoms with hidden truth values. The objective is to first identify all reachable goals from the atoms by checking if formulas related to the goals exist. The model requires abductive reasoning to work backward from an assumed conclusion (i.e., the goals are true) to hypothesizing the Teacher Type # Quest. # Traject. # Tokens Avg. Tokens Llama3.3-70B-IT Qwen3-30B-IT De. In. Ab. De. In. Ab. 3,400 4,300 8,800 3,400 4,300 8,800 17,000 21,500 44,000 16,446 19,070 24,128 18.6M 11.7M 51.2M 23.0M 47.8M 135.9M 1,093 544 1,164 1,397 2,507 5,631 Table 1: Fundamental reasoning training data statistics. tories generated by Llama and Qwen teacher models, respectively. Table 1 shows detailed statistics. We note that Qwen generates longer trajectories than Llama, particularly on inductive and abductive tasks (four times longer on average). We further observe that Qwen tends to repeat formulas and sequences from the questions in its reasoning steps. It also frequently uses tables to summarize its responses and generates many special tokens such as <tool_call>, <IActionResult], and <pal>. This is likely due to post-training on tool calling. Upcycling. Upcycling (Komatsuzaki et al., 2023; Nakamura et al., 2025) transforms dense Transformer layer to MoE layer by expanding the MLP block into multiple expert MLPs. During inference, router chooses experts. It has been successfully used for multilingual (Zhou et al., 2025; Zhang et al., 2025) and domain adaptation (Li et al., 2025b). Compared to the depth expansion of upscaling, upcycling is an efficient width expansion approach. We use Sparse-Upcycling (Komatsuzaki et al., 2023) as it does not require additional training for the router (Gritsch et al., 2025), or extra modules beyond the MoE structure (Zhou et al., 2025). Sparse-Upcycling initializes new expert MLPs by copying the original pre-trained MLP weights, with the router randomly initialized."
        },
        {
            "title": "5 Experimental Setup",
            "content": ""
        },
        {
            "title": "5.1 Training Settings",
            "content": "We experiment with various inducing methods, from standard FT to more sophisticated approaches that introduce structural changes to the model. Full FT. First, we simply update all model parameters via next-token prediction on the collected reasoning trajectories, similar to Tan et al. (2025). LoRA FT. As an efficient alternative, we also experiment with Low-Rank Adaptation (Hu et al., 2022, LoRA) FT. Instead of updating an existing weight matrix Rdk, LoRA injects lowrank decomposition = BA, where Rrk and Rdr, updating only and B. Up-scaling. LoRAs low-rank constraint while efficient, limits its effectiveness in domain adaptation (Biderman et al., 2024; Pletenev et al., 2025). Model up-scaling (Wu et al., 2024; Cao et al., 2025) increases model capacity by inserting additional layers into pre-trained LLM. By updating only the new layers, up-scaling preserves original parametric knowledge. It also allows better acquisition of new knowledge for language (Choudhury et al., 2025; Lee et al., 2025) and domain adaptation (Hoffmann et al., 2025; Lim et al., 2025). We up-scale models with LLaMAPro (Wu et al., 2024) by dividing base models into multiple layer groups and insert new layers atop each group. The new layers are initialized by copying weights from the corresponding base layers, with the attention and MLP output projection matrices set to zero. To study how each of the fundamental reasoning paradigms influence reasoning behavior, we train LLMs separately using Deduction, Induction, and Abduction reasoning trajectories respectively. For comparison, we also induce LLMs with all three reasoning types together (i.e., Mix) for assessing whether they offer complementary inductive bias."
        },
        {
            "title": "5.2 Models & Baselines",
            "content": "We use two student models: Llama-3.1-8B-Instruct (Grattafiori et al., 2024) with 32 layers; and Qwen38B (Yang et al., 2025) with 36 layers. We compare inducing methods to (1) student models before FT (vanilla); and (2) teacher models used for collecting the trajectories (3.2). 5."
        },
        {
            "title": "Implementation Details",
            "content": "For fair comparison, we keep the parameters of all induced models at similar size, updating only the newly-added parameters, except of Full FT. We apply LoRA to all linear layers of the students, with ranks set to 1,024 for Llama-3.1-8B-Instruct and 512 for Qwen3-8B. For up-scaling, we divide each student into four layer groups, inserting three new layers into each group. We upcycle odd-numbered dense layers into MoE layers by adding single expert MLP per layer, following Fedus et al. (2022). Further details are in Appendix B."
        },
        {
            "title": "5.4 Evaluation",
            "content": "Symbolic In-Domain. We evaluate on the test set (3.2) that corresponds to the reasoning type used Figure 3: Symbolic task accuracy of induced models across different settings. X-axis and y-axis denote inducing paradigms and symbolic test tasks, respectively. Solid/striped bars denote teacher for generating trajectories. Dotted lines denote performance for the vanilla student and teacher models. for training (e.g., training and testing on inductive tasks). When training on the Mix dataset, the test sets of all three reasoning types are in-domain. in terms of coherence, consistency, and relevance."
        },
        {
            "title": "6 Results",
            "content": "Symbolic OOD. To test symbolic generalization, we evaluate on symbolic tasks not seen during training (e.g., train on abductive, test on deductive). Figures 3 and 4 show model performance on symbolic and realistic tasks respectively. Tables 4 and 5 (Appendix D) show full results and std deviations. Realistic OOD. We include the following realistic OOD datasets: True Detective (Del and Fishel, 2023) contains detective puzzles with multi-choice questions; αN LI (Zhao et al., 2023) consists of multi-choice questions of the most probable explanation given context; WinoWhy (Zhang et al., 2020) includes explanations for the questions in Winograd Challenge (Levesque et al., 2012), aiming to distinguish plausible reasons from similar but wrong reasons; FOLIO (Han et al., 2024) contains logical reasoning questions in natural language. Given the context, set of first-order logic formulas, the model predicts if statement is true, false, or uncertain; RECV (Dougrez-Lewis et al., 2025) contains claims from Twitter and Wikipedia, for determining whether they are true or false, requiring deductive or abductive reasoning. Metrics. We report the accuracy of the final answer, judged by Qwen3-30B-Instruct. We use this judge based on recent analysis by (Haldar and Hockenmaier, 2025), which indicates it is more robust than competitive alternatives (including Llama-3 and Deepseek-R1 (DeepSeek-AI, 2025))"
        },
        {
            "title": "6.1 Symbolic Tasks",
            "content": "In-Domain: Specialized training yields the highest gains. The subplots on the main-diagonal and the last column in Figure 3 represent in-domain settings, where the train and test paradigms match. In five of the six scenarios (2 student models 3 symbolic tasks), specialist induced models substantially outperform the vanilla student. We find that inducing Deduction yields the largest improvements (e.g., Llama-3.1-8B gains 56.00 via up-scaling, while Qwen-3-8B is up 12.33 via upcycling). Induction follows (46.00 via Full FT, 9.67 via Upcycling), while inducing Abduction produces the smallest comparative gains (41.66 via LoRA, 10.67 via Upcycling). One possible explanation of the larger deduction gains is that the transformer architecture has the right structure for deduction (Aggarwal et al., 2025). Notably, the Mix setting, which trains LLMs on mixture of all three paradigms, offers competitive inducing strategy. While the Mix setting does not yield the largest in-domain performance gains achieved by single-paradigm specialists, it still outperforms the vanilla Qwen-3Figure 4: Realistic OOD task performance when inducing student model across different settings. 8B on all symbolic tasks in six out of eight cases (2 teachers 4 inducing methods). OOD: Asymmetric transfer between paradigms. Turning to the other subplots in Figure 3, we analyze OOD generalization, where models trained on one paradigm are tested on other. We observe distinct order of effectiveness: Deduction > Induction > Abduction. Gains in deduction transfer strongly to other tasks. Across 16 symbolic cases per paradigm, deduction gains transfer in 12 cases, improving induction and abduction in nine cases each. In contrast, induction transfers moderately (co-occurring with gains in 7 cases), while abduction transfers poorly, benefiting deduction in only four cases and induction in five cases. For instance, when inducing Qwen-3-8B via upcycling, deduction yields the most transferable gains ( induction: 73.00, abduction: 34.67). Induction achieves moderate transfer, exhibiting asymmetric generalization (deduction: 89.00, abduction: 11.00). In contrast, inducing abduction does not provide comparable gains (deduction: 74.67, induction: 62.00). These results show the transfer between fundamental reasoning paradigms is asymmetric. While abduction is the hardest to learn (lowest in-domain gains), deduction is the most transferable skill, serving as universal donor for reasoning capabilities. Inducing methods: Capacity expansion vs. efficient adaptation. Looking at different inducing methods, we observe that the most effective method is model-dependent, though upcycling generally offers more robust gains. For Qwen3-8B, upcycling consistently offers the highest gains across paradigms (e.g., 12.33 in deduction, 9.67 in induction and 10.67 in abduction). In contrast, Llama3.1-8B benefits from diverse range of inducing methods. Up-scaling drives its peak gains in deduction (56.00) and Full FT dominates in induction (46.00). While LoRA is competitive efficient alternative, surprisingly achieving the best abduction improvement (41.66), it generally lags capacityexpanding methods (upcycling), suggesting that fundamental reasoning skills benefit from the additional capacity offered by structural adaptation. Stronger teachers lead to higher student performance. Qwen-3-30B substantially outperforms Llama-3.3-70B on symbolic tasks (deduction: 82.00 vs. 33.00, induction: 81.00 vs. 55.33, abduction: 74.33 vs. 16.00). Both student LLMs benefit more from the stronger Qwen teacher for symbolic tasks. For example, Qwen3-8B, using the Llama teacher, causes performance drops in most cases (43/48). When shifting to Qwen teacher, its deduction performance substantially improves (up to 92.00 via deduction upcycling) even surpassing the teacher model (82.00). Such improvements on the deduction task are observed across different inducing paradigms (except abduction), further supporting the asymmetric transfer between paradigms. The performance difference between teachers highlights that the quality of reasoning traces is crucial for symbolic generalization."
        },
        {
            "title": "6.2 Realistic Tasks",
            "content": "Deduction > Induction > Abduction. We observe that the order of effectiveness established in the symbolic experiments (6.1) persists in realistic tasks. As shown in Figure 4, inducing deduction offers the strongest realistic OOD performance. For example, both Qwen-3-8B and Llama-3.1-8B achieve the highest average performance when inducing deduction (70.14 via upcycling, 58.70 via up-scaling) using traces from the Llama teacher. Induction is slightly lower (69.49 via upcycling, 57.58 via up-scaling), while abduction yields more modest gains (69.34 via upcycling, 56.79 via Full FT). The Mix strategy is worse than any singleparadigm approach (69.04 via upcycling, 56.07 via up-scaling), contrary to the symbolic performance. We hypothesize that, unlike symbolic tasks, realistic tasks are less structured, causing the Mix setting to dilute the strong inductive bias provided by single-paradigms, especially deduction. Knowledge-free inducing goes beyond the teacher. Inducing reasoning paradigms, abstracted from world knowledge, enables student LLMs to surpass their teachers even in realistic tasks. For example, when training Llama student (RECV: 71.98) with Llama teacher (RECV: 77.07), 13 out of 16 induced models (four inducing paradigms four inducing methods) outperform the teacher, achieving up to 81.73 via abductive Full FT. Strikingly, this improvement remains observable even when the student initially outperforms the teacher. For example, when training Qwen student (αNLI: 81.26) with Qwen teacher (αNLI: 75.77), the student performs better in the majority of cases (13/16), reaching up to 84.24 via inductive upcycling. Such improvements are observed across tasks under different teacherstudent pairs. We attribute these gains to the knowledgefree symbolic trajectories. The teacher transfers better fundamental reasoning skills to the student through symbolic structures, avoiding the content bias of world knowledge, pervasive in real tasks. Cross-paradigm transfer: The Abduction Paradox. Inducing the reasoning paradigm corresponding to realistic task does not always yield best performance. This is evident in realistic tasks that rely heavily on abduction. For True Detective, training on deduction yields the best performance for both Llama-3.1-8B and Qwen3-8B (26.18 and 44.50). In contrast, training on abduction performs the worst (21.47 and 38.57). similar pattern emerges on αNLI, where inducing induction (79.73/85.76) consistently outperforms abduction (77.37/82.73). Since abduction is inherently difficult to learn, models benefit more from the transferable logic acquired via deduction or induction. Task-specific concentration of gains. OOD generalization is highly concentrated on specific tasks. Compared to the vanilla model, gains are higher on αNLI (improving in 50 out of 64 cases) and Folio (41/64 cases). However, the source of these gains varies by teacher family. Llama teachers are more effective on RECV (28/32 cases), whereas Qwen teachers struggle (16/32) but perform better on WinoWhy (increasing cases from 5 to 13). This shows that while the training paradigm decides OOD effectiveness, teacher models determine which tasks benefit from the transfer. Students learn better from teachers from the same family. While symbolic tasks benefit from the stronger Qwen teacher regardless of model family, realistic OOD generalization improves when the teacher and student belong to the same model family, contradicting our findings in symbolic tasks (6.1). For example, when inducing deduction for Llama-3.1-8B, the best performance on Case 1 Premises: (1) People in this club who perform in school talent shows often attend and are very engaged with school events. . . . (3) People in this club who chaperone high school dances are not students who attend the school. . . . (6) Bonnie is in this club and she either (i) both attends and is very engaged with school events and is student who attends the school, or (ii) is not someone who both attends and is very engaged with school events and is not student who attends the school. Conclusion Bonnie performs in school talent shows often. Model Vanilla Llama Deductive Full FT Llama Deductive Up-scaling Llama Reasoning 1. If Bonnie is student... then she must perform in school talent shows often... 5. This means that if Bonnie is not student... then she must chaperone high school dances, which contradicts the fact that she is not student ...we cannot definitively say if she performs in school talent shows often......we have premise that supports this conclusion if she is student, we can say that the conclusion is True if we assume Bonnie is student... ...cannot directly conclude that Bonnie performs in school talent shows often... premise 6 does not directly state that Bonnie is student or that she is not student. It only gives two possible scenarios for Bonnies situation... cannot be certain of it. Analysis Directionality Error: Illegally reverses implication (P as ) and hallucinates contradiction to force determinate conclusion. Conditional Reasoning Drift: Recognizes uncertainty but nonetheless resolves it via assumptiondependent reasoning. Disjunction Preservation: Correctly treats the disjunction in Premise 6 as epistemic uncertainty and maintains logical validity. Final Answer True True Uncertain Case 2 Premises: (1) Barutin Cove is cove named after the Bulgarian settlement of Barutin. (2) Barutin Cove is on the southwest coast of Snow Island. (3) Snow Island, Greenwich Island, and Deception Island are located in the South Shetland Islands. (4) Antarctica is located on the South Shetland Islands. (5) If place is located in place and place is located in place C, then place is located in place C. Conclusion Barutin Cove is not located in Antarctica. Model Vanilla Qwen Abductive Upcycling Qwen Deductive Upcycling Qwen Reasoning ...the South Shetland Islands are not part of Antarctica, but rather located near Antarctica... which seems to be contradiction... ...theres potential confusion in the premises... Antarctica is continent, while the South Shetland Islands are group of islands that are geographically close to Antarctica... ...So maybe the premise is incorrect? But have to take the premises as given, even if they might be factually wrong... Analysis World-Knowledge Bias: Overrides given premises using real-world knowledge, invents spurious contradiction, and derives formally correct answer by invalidating the logical setting. Plausibility Reinterpretation: Treats premises as revisable observations, resolves tension via plausibility-based reinterpretation, and violates premise-faithful logical entailment. Premise-Faithful Deduction: Explicitly distinguishes logical validity from factual correctness, enforces strict premise adherence, and rejects the conclusion through rule-consistent inference. Final Answer True True False Table 2: Qualitative comparison of reasoning traces on realistic tasks across models before and after inducing fundamental reasoning skills. Full example for Case 1 and Case 2 are included in Appendix E. realistic tasks is achieved with the Llama teacher (58.70) rather than the Qwen teacher (54.79). This pattern is consistent across both student models when inducing deduction or induction. We speculate that realistic tasks are more sensitive to stylistic mismatches inherent in cross-family distillation. This aligns with prior work on cross-model trajectory mismatch (Wen et al., 2025), suggesting that, minimizing distributional shift between teacher and student is more critical than teacher capability. Inducing Methods: Consistent with symbolic tasks. We observe similar pattern to the symbolic domain (6.1): although the most effective inducing method is model-dependent, upcycling provides more robust improvements overall. Across all eight settings (two teacher models four inducing paradigms), upcycle Qwen-3-8B achieves the highest realistic performance in seven out of eight cases. For example, with Llama teacher, upcycling consistently yields the best realistic performance for Qwen-3-8B across all paradigms (deduction: 70.14, induction: 69.49, abduction: 69.34, Mix: 69.04). In contrast, Llama-3.1-8B benefits from diverse range of inducing methods (i.e. Full FT, LoRA, Up-scaling). This observation shows the consistency of OOD generalization across symbolic and realistic tasks when the same inducing method is applied. Furthermore, the superior performance of upcycling further illustrates that more sophisticated inducing methods outperform the current practice on using simpler Full FT, i.e., Tan et al. (2025)."
        },
        {
            "title": "7 Qualitative Analysis",
            "content": "Table 2 shows examples of reasoning behavior usIn Case 1, the ing different inducing methods. vanilla Llama and deductive Full FT Llama fail by prematurely resolving epistemic uncertainty, committing to determinate conclusion through implicit assumption-making. In contrast, the deductive up-scaling Llama succeeds by preserving the uncertainty deriving from the disjunctive premise. This indicates that stronger structural constraints introduced by inducing methods support logical validity, preventing overconfident conclusions on underdetermined realistic reasoning problems. In Case 2, the vanilla Qwen and abductive upcycling Qwen fail by prioritizing world knowledge over logical reasoning, revising implausible premises and overriding the given logical rules. Conversely, the deductive upcycling Qwen succeeds by strictly adhering to the stated premises and applying logical rules regardless of content. This behavior aligns with the intrinsic nature of abduction and deduction. Abduction focuses on exploring the most plausible observations under set of rules and thus encourages premise reinterpretation. In contrast, deduction derives conclusions from rules and observations and inherently promotes premise faithfulness. This further emphasizes how inducing reasoning paradigms decoupled from world knowledge reduce content biases, leading to more robust reasoning generalization on realistic tasks. We further include an analysis on symbolic tasks, detailed in Appendix F."
        },
        {
            "title": "8 Conclusion & Takeaways",
            "content": "We systematically studied how inducing fundamental reasoning types (i.e., deduction, induction, and abduction) shapes the generalization of LLM reasoning. By training on reasoning trajectories abstracted from world knowledge, we isolate the effect of each paradigm. We evaluate range of inducing methods, from standard fine-tuning to capacity expansion approaches such as up-scaling and upcycling. Our results show that knowledgefree inducing yields substantial and transferable OOD gains. Deduction is the most robust paradigm and upcycling provides the largest overall improvements, enabling student LLMs to acquire structural inductive biases that even surpass their teachers on realistic tasks. Future work will extend this framework and explore principled ways of combining multiple reasoning paradigms without diluting their individual contribution."
        },
        {
            "title": "Limitations",
            "content": "All data and evaluations are in English given the availability and diversity of reasoning datasets. We acknowledge that the linguistic expression and underlying cognitive strategies for fundamental reasoning may differ across languages which is an interesting avenue for future work. In addition, while many different symbolic tasks can be generated to represent the three fundamental reasoning paradigms, this work explored single representative task for deduction, induction, and abduction, respectively. We focused, in fact, on the interplay between fundamental paradigms and inducing methods, prioritising depth of analysis rather than exhaustive coverage of the whole spectrum of symbolic reasoning tasks. While we found the proposed tasks to be sufficient to induce strong OOD generalisation, future work might investigate the potential impact of different symbolic data on performance and generalisation."
        },
        {
            "title": "References",
            "content": "Naman Aggarwal, Siddhartha R. Dalal, and Vishal Misra. 2025. The bayesian geometry of transformer attention. Preprint, arXiv:2512.22471. Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John Patrick Cunningham. 2024. LoRA learns less and forgets less. Transactions on Machine Learning Research. Featured Certification. Mingzi Cao, Xi Wang, and Nikolaos Aletras. 2025. Progressive depth up-scaling via optimal transport. Preprint, arXiv:2508.08011. Monojit Choudhury, Shivam Chauhan, Rocktim Jyoti Das, Dhruv Sahnan, Xudong Han, Haonan Li, Aaryamonvikram Singh, Alok Anil Jadhav, Utkarsh Agarwal, Mukund Choudhary, Debopriyo Banerjee, Fajri Koto, Junaid Bhat, Awantika Shukla, Samujjwal Ghosh, Samta Kamboj, Onkar Pandit, Lalit Pradhan, Rahul Pal, and 11 others. 2025. Llama-3-nanda-10bchat: An open generative large language model for hindi. Preprint, arXiv:2504.06011. Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR). DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Maksym Del and Mark Fishel. 2023. True detective: deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4. In Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 314322, Toronto, Canada. Association for Computational Linguistics. John Dougrez-Lewis, Mahmud Elahi Akhter, Federico Ruggeri, Sebastian Löbbers, Yulan He, and Maria Liakata. 2025. Assessing the reasoning capabilities of LLMs in the context of evidence-based claim verification. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2060420628, Vienna, Austria. Association for Computational Linguistics. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Harry Frankfurt. 1958. Peirces notion of abduction. The Journal of Philosophy, 55(14):593597. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Nikolas Gritsch, Qizhen Zhang, Acyr Locatelli, Sara Hooker, and Ahmet Üstün. 2025. Nexus: Adaptive upcycling to efficiently pretrain mixture of experts. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2436424381, Suzhou, China. Association for Computational Linguistics. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, and 31 others. 2025. Openthoughts: Data recipes for reasoning models. Preprint, arXiv:2506.04178. Rajarshi Haldar and Julia Hockenmaier. 2025. Rating roulette: Self-inconsistency in LLM-as-a-judge frameworks. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 24986 25004, Suzhou, China. Association for Computational Linguistics. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alexander Wardle-Solano, Hannah Szabó, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, and 16 others. 2024. FOLIO: Natural language reasoning with first-order In Proceedings of the 2024 Conference on logic. Empirical Methods in Natural Language Processing, pages 2201722031, Miami, Florida, USA. Association for Computational Linguistics. Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Hoi-Fong Mak, Alice Zhang, Dmitry Gaynullin, and Nicolay J. Hammer. 2025. Llama-genba-10b: trilingual large language model for german, english and bavarian. Preprint, arXiv:2509.05668. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, and Junnan Li. 2025. Beyond aha!: Toward systematic meta-abilities alignment in large reasoning models. Preprint, arXiv:2505.10554. Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2023. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In The Eleventh International Conference on Learning Representations. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Wooseong Lee, Minseo Kim, Taeil Hur, Gyeong Hwan Jang, Woncheol Lee, Maro Na, and Taeuk Kim. 2025. ENGinius: bilingual LLM optimized for plant construction engineering. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track), pages 13501364, Vienna, Austria. Association for Computational Linguistics. Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR12, page 552561. AAAI Press. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025a. Language models can easily learn to reason from demonstrations. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 15979 15997, Suzhou, China. Association for Computational Linguistics. Junzhuo Li, Bo Wang, Xiuze Zhou, and Xuming Hu. 2025b. Dynamic expert specialization: Towards catastrophic forgetting-free multi-domain MoE adaptation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1847818493, Suzhou, China. Association for Computational Linguistics. Junghwan Lim, Gangwon Jo, Sungmin Lee, Jiyoung Park, Dongseok Kim, Jihwan Kim, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Kibong Choi, Jaeyeon Huh, Beomgyu Kim, Jangwoong Kim, Taehyun Kim, Haesol Lee, Jeesoo Lee, Dongpin Oh, Changseok Song, and Daewon Suh. 2025. Expanding foundational language capabilities in opensource llms through korean case study. Preprint, arXiv:2509.03972. Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. 2024. Datasets for large language models: comprehensive survey. Preprint, arXiv:2402.18041. Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. 2024. Enhancing reasoning capabilities of llms via principled synthetic logic corpus. In Advances in Neural Information Processing Systems, volume 37, pages 7357273604. Curran Associates, Inc. Taishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, and Jun Suzuki. 2025. Dropupcycling: Training sparse mixture of experts with partial re-initialization. In The Thirteenth International Conference on Learning Representations. Team Olmo, :, Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, Jacob Morrison, Jake Poznanski, Kyle Lo, Luca Soldaini, Matt Jordan, Mayee Chen, Michael Noukhovitch, Nathan Lambert, and 50 others. 2025. Olmo 3. Preprint, arXiv:2512.13961. Sergey Pletenev, Maria Marina, Daniil Moskovskiy, Vasily Konovalov, Pavel Braslavski, Alexander Panchenko, and Mikhail Salnikov. 2025. How much knowledge can you pack into LoRA adapter without harming LLM? In Findings of the Association for Computational Linguistics: NAACL 2025, pages 43094322, Albuquerque, New Mexico. Association for Computational Linguistics. Merrilee Salmon. 1994. Introduction to logic and critical thinking. Wadsworth Publishing Co Inc. Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, and Sadao Kurohashi. 2024. Rapidly developing high-quality instruction data and evaluation benchmark for large language models with minimal human effort: case study on Japanese. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1353713547, Torino, Italia. ELRA and ICCL. Xingwei Tan, Marco Valentino, Mahmud Elahi Akhter, Maria Liakata, and Nikolaos Aletras. 2025. Enhancing logical reasoning in language models via symbolically-guided Monte Carlo process superviIn Proceedings of the 2025 Conference on sion. Empirical Methods in Natural Language Processing, pages 3187431888, Suzhou, China. Association for Computational Linguistics. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. Xiangyu Wen, Junhua Huang, Zeju Li, Min Li, Jianyuan Zhong, Zhijian Xu, Mingxuan Yuan, Yongxiang Huang, and Qiang Xu. 2025. Reasoning scaffolding: Distilling the flow of thought from llms. Preprint, arXiv:2509.23619. Jan Wielemaker, Tom Schrijvers, Markus Triska, and Torbjörn Lager. 2012. SWI-Prolog. Theory and Practice of Logic Programming, 12(1-2):6796. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, and Ping Luo. 2024. LLaMA pro: Progressive LLaMA with block expansion. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 65186537, Bangkok, Thailand. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Hongzhou Yu, Tianhao Cheng, Yingwen Wang, Wen He, Qing Wang, Ying Cheng, Yuejie Zhang, Rui Feng, and Xiaobo Zhang. 2025. Finemedlm-o1: Enhancing medical knowledge reasoning ability of llm from supervised fine-tuning to test-time training. Preprint, arXiv:2501.09213. Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations. Hongming Zhang, Xinran Zhao, and Yangqiu Song. 2020. WinoWhy: deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 57365745, Online. Association for Computational Linguistics. Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, and Jie Zhou. 2025. Less, but better: Efficient multilingual expansion for LLMs via layer-wise mixture-of-experts. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1794817963, Vienna, Austria. Association for Computational Linguistics. Wenting Zhao, Justin Chiu, Claire Cardie, and Alexander Rush. 2023. Abductive commonsense reasoning exploiting mutually exclusive explanations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1488314896, Toronto, Canada. Association for Computational Linguistics. Xiaoke Zhao, Zhaowen Zhou, Lin Chen, Lihong Wang, Zhiyi Huang, Kaiyuan Zheng, Yanjun Zheng, Xiyang Du, Longfei Liao, Jiawei Liu, Xiang Qi, Bo Zhang, Peng Zhang, Wei Wang, and Zhe Li. 2025. Agentardeepfinance-100k: large-scale financial dataset via systematic chain-of-thought synthesis optimization. Preprint, arXiv:2507.12901. Tianshi Zheng, Cheng Jiayang, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Wong, and Simon See. 2025. LogiDynamics: Unraveling the dynamics of inductive, abductive and deductive logical inferences in LLM reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2072120742, Suzhou, China. Association for Computational Linguistics. Hao Zhou, Zhijun Wang, Shujian Huang, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Weihua Luo, and Jiajun Chen. 2025. Moe-lpr: multilingual extension of large language models through mixture-ofexperts with language priors routing. In Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI25/IAAI25/EAAI25. AAAI Press. Training and Evaluation Setup For training all methods, we use cosine scheduler with warm-up and maximum learning rate of 5e-6 (degraded to 5e-7). We fine-tune models using SFT trainer from package TRL (von Werra et al., 2020) on one NVIDIA A100 (80GB). We employ FlashAttention 2 (Dao, 2024) and mixed-precision bf16 for accelerated training. We use the package Transformers (Wolf et al., 2020) for reproducible output generation and vLLM (Kwon et al., 2023) for evaluation. Format-Robust Evaluation. Due to requirements ( 3.1), the final answer is explicitly separated from the reasoning chain, and Deduction and Abduction tasks need JSON-formatted answers. Induced models sometimes deviate from the required format while still producing correct answers. To address this issue, we use Qwen3-30BInstruct (Yang et al., 2025) to compare the generated answer with the gold answer. Specifically, the model is instructed to determine whether the generation is consistent with the gold label and to return TRUE or FALSE. The full prompt is provided in Appendix C."
        },
        {
            "title": "C Prompt Examples",
            "content": "Below is the prompt used to compare generated answers with gold answers."
        },
        {
            "title": "Flexible Match",
            "content": "Instruction: Please check whether the generation results is consistent with the gold label. Generation Results:{output} Gold Label:{answer} Please output TRUE if they are consistent, otherwise output FALSE."
        },
        {
            "title": "D Full Results",
            "content": "Tables 4 and 5 present the full results of inducing reasoning paradigms using different inducing methods across experimental settings."
        },
        {
            "title": "A Reasoning Task Examples",
            "content": "The following boxes present the questions and corresponding gold answers used in our synthetic fundamental reasoning tasks."
        },
        {
            "title": "Deductive Task",
            "content": "This is <Deductive> reasoning task. Below are some formulas connected by conjunctions: (((A A) (F ))) (((E) (B)) ((H ) (H ))) (((H A) (G D))) (((F C) (G G)) ((D A) (F G))) (((C) (F D)) ((B ))) Please list the truth value of each variable to make the whole conjunction true using JSON dictionary, which maps variable names to their truth values, then enclose the answer in <answer><answer>. Please put all the intermediate reasoning steps in <think><think>. Gold Answer: { A: False,B: False, C: True, D: False, E: False, F: True, G: True, H: True }"
        },
        {
            "title": "Inductive Task",
            "content": "This is <Inductive> reasoning task. Given the following sequence, [5, 10, 6, 12, 15, 30, 26, 52, 55, 110, ?] What is the value at the question mark? Please enclose the answer in <answer><answer>, and put all the intermediate reasoning steps in <think><think>. Gold Answer: 106 Rev. Rule-Graph Search (Hu et al., 2025) Assumption Trace-Back (Ours) Premises: [(L) => L, (((NOT D) OR (NOT M))) => N, ((M OR M)) => C, ((M OR L)) => LI, (M) => M, ((L OR LI)) => G] Known Atoms: [L, M, LI, D, N] Goals: [LI, KM, NK] Instruction: For each goal, first assume it is true and determine what conditions must be satisfied. Identify which premises directly lead to these goals, then trace back what must be established to satisfy those premises. Finally, check whether the known atoms can lead to these conditions Answer: (1) is reachable (2) KM is unreachable (3) NK is unreachable Premises: [(L) => L, (((NOT D) OR (NOT M))) => N, ((M OR M)) => C, ((M OR L)) => B, (M) => M, ((L OR B)) => G] Known Atoms: [L, M, A, D, N] Goals: [B, A, C] Instruction: For each goal, identify which premises directly lead to the goal. Then, trace back what the true value of the atoms must be to make each of the goal true. Only the atoms in the known atoms are known but their values are not shown. Finally, return the reachable goals with the true values of the known atoms that make it true. Please enclose the final answer with <answer><answer>. All the intermediate thinking steps should be enclosed in <think><think> tags. Answer: Goal is reachable. Sol 1: {D: false, L: false, M: false} Sol 2: {D: false, L: false, M: true} ... Sol 8: {D: true, L: true, M: true} Table 3: Comparison between the original abductive task by Hu et al. (2025) and ours."
        },
        {
            "title": "B Implementation Details",
            "content": "Data Sampling. The generation of training data takes approximately 280 hours for LLaMA trajectories and 304 hours for Qwen trajectories when executed on an AMD MI300X accelerator."
        },
        {
            "title": "Method",
            "content": "FT Train/Total"
        },
        {
            "title": "Detect",
            "content": "αNLI"
        },
        {
            "title": "Avg",
            "content": "Llama-3.3-70B-IT Llama-3.1-8B-IT - - 0/70.0 0/8.03 33.000.3 14.001.0 55.332.5 32.000.0 16.002.6 1.670. 39.960.3 28.800.0 79.280.2 73.970.0 65.750.2 48.600.4 60.590.9 53.690.5 77.070.1 71.980.0 64.530.2 55.410."
        },
        {
            "title": "Realistic Tasks",
            "content": "Full FT LoRA Up-Scaling Upcycling Full FT LoRA Up-Scaling Upcycling Full FT LoRA Up-Scaling Upcycling Full FT LoRA Up-Scaling Upcycling 8.03/8.03 15.001. 57.000.0 5.670.6 19.720.3 79.340.1 25.440.1 59.280. 78.550.0 52.470.1 3.73/10.71 11.000.0 59.000.0 14.670. 18.150.3 79.730.0 33.870.1 56.321.0 80.070.0 53.630. Induct 3.67/10.65 13.000.0 56.000.0 11.001.0 24.780. 78.970.1 47.001.9 59.610.0 77.550.0 57.580.4 3.87/10. 13.671.5 53.000.0 8.672.1 24.080.0 78.500.1 43.610. 55.830.6 76.820.0 55.770.2 8.03/8.03 24.670.6 39.000. 8.671.2 26.180.0 77.240.1 45.920.7 59.610.9 78.600. 57.510.1 3.73/10.71 31.331.5 37.670.6 4.001.0 24.050. 77.180.0 52.270.9 57.140.5 78.380.0 57.810.3 Deduct 3.67/10.65 37.671.5 35.000.0 4.000.0 23.890.6 76.340. 58.921.1 59.280.6 75.050.0 58.700.3 3.87/10.85 34.002. 33.000.0 6.671.5 25.130.0 69.800.4 20.920.1 55.340. 73.360.1 48.910.0 8.03/8.03 13.671.2 28.000.0 5.671. 21.470.0 75.740.0 47.060.4 57.970.3 81.730.0 56.790. 3.73/10.71 10.331.5 35.330.6 9.001.0 18.850.0 75.920. 46.600.7 56.650.0 79.870.0 55.580.1 Abduct 3.67/10. 13.000.0 36.000.0 5.671.5 21.120.3 77.160.0 46.420. 55.341.2 79.130.1 55.830.4 3.87/10.85 14.000.0 29.000. 8.001.0 19.900.0 77.370.0 32.020.1 56.161.7 78.160. 52.720.4 8.03/8.03 26.000.0 0.000.0 5.330.6 18.850. 76.030.1 29.930.9 55.830.3 80.510.0 52.230.2 3.73/10. 28.333.2 0.000.0 8.330.6 21.120.6 78.220.2 43.784. 56.321.6 79.761.1 55.840.8 Mix 3.67/10.65 30.003. 0.000.0 9.671.5 20.601.2 78.250.2 45.584.6 56.821. 79.091.1 56.070.8 3.87/10.85 32.330.6 56.000.0 4.672. 26.180.0 77.020.0 43.630.6 54.190.0 77.330.0 55.670. Llama-3.3-70B-IT Qwen3-8B - - 0/70.0 0/8.19 33.000.3 79.672.1 55.332.5 67.330.6 16.002.6 35.330. 39.960.3 40.140.8 79.280.2 81.260.0 65.750.2 70.980.7 60.590.9 78.331.0 77.070.1 71.400.0 64.530.2 68.420. Full FT LoRA Up-Scaling Upcycling Full FT LoRA Up-Scaling Upcycling Full FT LoRA Up-Scaling Upcycling Full FT LoRA Up-Scaling"
        },
        {
            "title": "Upcycling",
            "content": "8.19/8.19 68.670.6 58.000.0 24.671.2 32.982.7 78.900. 55.681.4 75.210.3 71.910.0 62.940.9 3.88/10.83 61.670. 60.000.0 14.330.6 30.890.9 78.250.0 39.071.6 67.160. 72.180.0 57.510.5 Induct 3.56/10.51 34.001.7 57.000. 40.670.6 33.865.1 82.060.0 67.601.0 73.890.5 72.670. 66.021.1 3.96/10.91 72.671.5 72.000.0 31.671.2 41.192. 81.880.0 74.541.0 77.670.3 72.200.0 69.490.7 8.19/8. 35.330.6 64.330.6 19.330.6 34.213.3 81.550.0 67.260. 71.760.3 70.530.0 65.060.6 3.88/10.83 28.001.0 58.331. 15.002.6 31.761.2 83.250.0 55.261.1 80.300.0 70.960. 64.310.0 Deduct 3.56/10.51 28.330.6 62.000.0 12.000. 36.825.1 74.290.1 52.601.0 82.430.3 70.000.0 63.230. 3.96/10.91 72.000.0 71.000.0 12.001.0 42.762.1 82.770. 73.590.9 80.130.3 71.450.0 70.140.4 8.19/8.19 33.331. 69.330.6 17.331.2 40.310.0 83.810.1 67.790.7 71.100. 73.960.0 67.390.1 3.88/10.83 43.670.6 64.000.0 16.002. 39.610.8 81.960.1 53.331.2 71.760.7 74.110.0 64.160. Abduct 3.56/10.51 44.330.6 60.000.0 14.331.2 37.000. 81.920.0 44.090.9 72.250.3 73.470.0 61.740.1 3.96/10. 53.330.6 68.330.6 13.670.6 47.820.6 83.740.0 68.080. 75.530.3 71.510.1 69.340.1 8.19/8.19 31.000.0 60.000. 20.330.6 35.780.6 82.340.0 62.840.1 73.560.3 72.400. 65.380.2 3.88/10.83 33.001.0 61.332.3 11.672.5 31.940. 81.520.2 59.567.5 66.505.6 73.820.3 62.670.4 Mix 3.56/10.51 33.001.7 62.332.1 12.672.1 31.940.0 81.420. 53.638.8 69.295.8 73.980.3 62.050.6 3.96/10.91 48.330. 64.330.6 12.001.0 43.450.9 81.890.1 71.130.7 77.340. 71.400.0 69.040.3 Table 4: Accuracy of the fine-tuned models on the fundamental reasoning tasks using trajectories collected from Llama-3.3-70B-Instruct. Method indicates the base model and the applied PEFT methods. FT shows the fundamental reasoning task used for fine-tuning. Train/Total denotes the number of trainable and total parameters. The best and second-best methods for each fundamental reasoning task are indicated in bold and underlined, respectively. Scores that exceed the base model performance are shown in green ."
        },
        {
            "title": "Method",
            "content": "FT Train/Total"
        },
        {
            "title": "Detect",
            "content": "αNLI"
        },
        {
            "title": "Avg",
            "content": "Qwen3-30B Llama-3.1-8B-IT - - 0/30.5 0/8.03 82.001.0 14.001.0 81.004.0 32.000.0 74.334.2 1.670. 33.513.2 28.800.0 75.770.1 73.970.0 72.730.2 48.600.4 72.410.5 53.690.5 80.560.3 71.980.0 67.000.6 55.410."
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "Upcycling",
            "content": "8.03/8.03 25.671.5 78.000.0 2.330.6 25.130.0 77.710. 43.050.1 54.840.3 78.290.0 55.800.1 3.73/10.71 24.331. 69.330.6 1.671.2 21.820.3 78.230.0 44.330.4 56.000. 76.820.0 55.440."
        },
        {
            "title": "Induct",
            "content": "3.67/10.65 34.671.5 69.330.6 5.331.2 24.430.3 67.370. 35.250.1 56.980.6 72.360.5 51.280.3 3.87/10.85 26.001. 64.000.0 5.671.5 24.080.0 78.180.1 51.670.3 51.962. 74.730.0 56.130.5 8.03/8.03 67.330.6 33.000.0 7.671. 24.610.0 48.730.1 41.510.3 56.650.0 77.800.0 49.860. 3.73/10.71 39.003.5 36.000.0 6.001.0 29.490.3 70.350. 40.420.4 56.650.0 77.020.0 54.790."
        },
        {
            "title": "Deduct",
            "content": "3.67/10.65 70.001.7 32.000.0 3.331.5 27.230.0 57.260. 42.930.3 58.460.3 73.870.0 51.950.1 3.87/10.85 47.001. 40.000.0 6.331.5 26.180.0 69.870.1 37.690.1 56.650. 73.290.0 52.740.0 8.03/8.03 34.671.5 28.000.0 39.670. 18.670.3 77.270.1 63.200.8 59.610.5 77.200.1 59.190. 3.73/10.71 34.672.9 29.331.2 43.331.5 23.560.0 75.590. 56.742.9 59.770.8 75.090.0 58.150."
        },
        {
            "title": "Abduct",
            "content": "3.67/10.65 34.330.6 39.000.0 39.331.2 23.040.0 70.570. 48.190.2 52.380.3 70.890.0 53.010.0 3.87/10.85 41.670. 36.000.0 35.002.6 26.700.0 68.020.1 51.760.3 63.050. 75.730.1 57.050.1 8.03/8.03 50.670.6 63.330.6 45.002. 24.080.0 77.540.0 55.170.1 57.640.5 78.240.0 58.530. 3.73/10.71 69.332.3 65.671.2 43.671.2 23.390.3 79.260. 44.890.2 58.290.7 77.670.0 56.700."
        },
        {
            "title": "Mix",
            "content": "3.67/10.65 62.001.0 65.000.0 38.331.2 24.080.0 76.570. 49.590.2 56.490.7 75.820.0 56.510.2 3.87/10.85 58.000. 58.330.6 34.332.9 21.990.0 74.360.1 52.230.1 58.130. 74.160.1 56.170.1 Qwen3-30B Qwen3-8B - - 0/30.5 0/8.19 82.001.0 79.672. 81.004.0 67.330.6 74.334.2 35.330.6 33.513.2 40.140.8 75.770.1 81.260.0 72.730.2 70.980.7 72.410.5 78.331. 80.560.3 71.400.0 67.000.6 68.420."
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "LoRA",
            "content": "Up-Scaling"
        },
        {
            "title": "Upcycling",
            "content": "8.19/8.19 68.330.6 49.000.0 7.670.6 27.230.0 84.570. 79.340.2 77.180.3 70.420.0 67.750.1 3.88/10.83 82.670. 75.331.2 2.670.6 40.840.9 85.560.1 77.880.1 77.830. 70.640.0 70.550."
        },
        {
            "title": "Induct",
            "content": "3.56/10.51 6.330.6 41.000.0 0.000.0 43.630.6 83.530. 56.000.6 78.490.3 70.180.2 66.370.0 3.96/10.91 89.000. 77.001.0 11.001.0 40.141.1 80.340.2 75.650.2 84.240. 71.130.0 70.300.3 8.19/8.19 88.001.7 60.000.0 48.001. 36.650.0 81.920.0 70.300.4 75.040.8 70.980.0 66.980. 3.88/10.83 84.001.0 68.671.2 21.331.5 42.760.3 82.700. 69.830.2 80.791.0 71.130.0 69.440."
        },
        {
            "title": "Deduct",
            "content": "3.56/10.51 85.331.2 63.330.6 34.672.1 37.350.6 80.840. 19.820.1 80.300.0 71.400.0 57.940.1 3.96/10.91 92.001. 73.000.0 34.670.6 44.500.5 84.000.0 73.580.2 81.940. 70.820.1 70.970.2 8.19/8.19 74.330.6 67.670.6 32.002. 38.570.3 81.540.1 69.030.8 77.180.3 69.820.2 67.230. 3.88/10.83 77.671.5 70.670.6 41.331.5 36.130.0 81.970. 66.720.6 76.031.0 70.020.0 66.180."
        },
        {
            "title": "Abduct",
            "content": "3.56/10.51 68.670.6 62.330.6 35.670.6 34.200.3 76.050. 27.050.0 70.440.0 68.910.1 55.330.1 3.96/10.91 74.671. 62.000.0 46.001.7 33.860.3 82.730.0 68.200.6 79.150. 70.490.0 66.890.1 8.19/8.19 88.331.2 77.330.6 41.331. 39.440.3 81.690.0 73.440.2 74.550.3 70.290.0 67.880. 3.88/10.83 85.331.5 74.670.6 23.673.1 40.320.9 84.100. 71.020.1 74.550.3 69.200.1 67.840."
        },
        {
            "title": "Mix",
            "content": "3.56/10.51 85.670.6 76.330.6 32.670.6 44.680.3 84.160. 51.150.4 82.260.9 69.760.0 66.400.1 3.96/10.91 87.000. 69.000.0 51.001.0 42.580.3 81.320.0 68.280.1 81.120. 70.560.1 68.770.1 Table 5: Accuracy of the fine-tuned models using trajectories collected from Qwen3-30B-Instruct. Out-of-Domain Generation Example Examples 13 show the generated outputs for question from FOLIO. The vanilla Llama-3.1-8BInstruct and the deductive Full FT model fail to answer the question correctly, whereas deductive up-scaling produces the correct answer. Examples 46 show the generated outputs for another question from FOLIO. The vanilla Qwen3-8B and the abductive upcycling model fail to answer the question correctly, whereas the deductive upcycling model produces the correct answer. Example 1: Vanilla Llama Based on the given premises, is the conclusion correct? Please respond with True, False, or Uncertain enclosing in <answer><answer>. Premises: People in this club who perform in school talent shows often attend and are very engaged with school events. People in this club either perform in school talent shows often or are inactive and disinterested community members. People in this club who chaperone high school dances are not students who attend the school. All people in this club who are inactive and disinterested members of their community chaperone high school dances. All young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. Bonnie is in this club and she either both attends and is very engaged with school events and is student who attends the school or is not someone who both attends and is very engaged with school events and is not student who attends the school. Conclusion: Bonnie performs in school talent shows often. Generated Output: in this case, we can break down the information as follows: 1. If Bonnie is student who attends the school, then she must perform in school talent shows often (from the first premise). 2. If Bonnie is not student who attends the school, then she must be an inactive and disinterested community member (from the second premise). 3. If Bonnie is an inactive and disinterested community member, then she must chaperone high school dances (from the fourth premise). 4. However, we know that people who chaperone high school dances are not students who attend the school (from the third premise). 5. This means that if Bonnie is not student who attends the school, then she must chaperone high school dances, which contradicts the fact that she is not student who attends the school. 6. Therefore, Bonnie must be student who attends the school. 7. Since Bonnie is student who attends the school, she must perform in school talent shows often (from the first premise). So, the conclusion is correct: <answer>True</answer>. Example 2: Deductive Full FT Llama Generated Output: in order to determine if the conclusion is correct, we need to analyze the premises and apply them to Bonnies situation. From the premises, we know: 1. People in this club who perform in school talent shows often attend and are very engaged with school events. 2. People in this club either perform in school talent shows often or are inactive and disinterested community members. 3. People in this club who chaperone high school dances are not students who attend the school. 4. All people in this club who are inactive and disinterested members of their community chaperone high school dances. 5. All young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. Given that Bonnie is in this club, we can start by looking at the last premise: All young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. This implies that if Bonnie is young child or teenager and wishes to further her academic career and educational opportunities, then she must be student who attends the school. However, we also know from premise 3 that people in this club who chaperone high school dances are not students who attend the school. If Bonnie is student who attends the school, then she cannot be chaperone for high school dances. Now, looking at premise 2: People in this club either perform in school talent shows often or are inactive and disinterested community members. If Bonnie is student who attends the school, then from premise 1, she must perform in school talent shows often because she is very engaged with school events. However, premise 4 states that all people in this club who are inactive and disinterested members of their community chaperone high school dances. This means that if Bonnie is not student who attends the school, then she must be an inactive and disinterested member of her community, which would make her chaperone for high school dances, which contradicts our earlier deduction that if she is student, she cannot be chaperone. Considering all premises, we can conclude that if Bonnie is student who attends the school, then she must perform in school talent shows often. But we also know from premise 3 that if she is chaperone, she cannot be student. So, if we consider the possibility that Bonnie is not student, then she would be an inactive and disinterested member of her community, which would make her chaperone, but this contradicts our initial assumption that she is not student. Given the premises, the most consistent conclusion we can draw about Bonnie is that if she is student who attends the school, then she must perform in school talent shows often. But since we cannot definitively conclude that Bonnie is student or not based on the given information, we cannot definitively say if she performs in school talent shows often. However, looking closely at premise 5 and the structure of the premises, we can see that if Bonnie is student who attends the school and is very engaged with school events, then she must perform in school talent shows often. But we cannot conclude this with absolute certainty without more information about Bonnies status as student or non-student. Therefore, based on the given premises, we cannot definitively conclude that Bonnie performs in school talent shows often without assuming that she is student who attends the school. But since the conclusion is about whether Bonnie performs in school talent shows often, and we have premise that supports this conclusion if she is student, we can say that the conclusion is True if we assume Bonnie is student who attends the school and is very engaged with school events. <answer>True</answer> Example 3: Deductive Up-scaling Llama Generated Output: in order to determine if the conclusion is correct, we need to analyze the premises: 1. People in this club who perform in school talent shows often attend and are very engaged with school events. 2. People in this club either perform in school talent shows often or are inactive and disinterested community members. 3. People in this club who chaperone high school dances are not students who attend the school. 4. All people in this club who are inactive and disinterested members of their community chaperone high school dances. 5. All young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. 6. Bonnie is in this club and she either both attends and is very engaged with school events and is student who attends the school or is not someone who both attends and is very engaged with school events and is not student who attends the school. From premise 6, we know that Bonnie is in this club and has two possible scenarios: - She both attends and is very engaged with school events and is student who attends the school. - She is not someone who both attends and is very engaged with school events and is not student who attends the school. From premise 1, if Bonnie is someone who both attends and is very engaged with school events, then she must perform in school talent shows often. From premise 3, if Bonnie is student who attends the school, then she cannot chaperone high school dances. From premise 4, if Bonnie is an inactive and disinterested member of her community, then she must chaperone high school dances. From premise 5, if Bonnie is young child or teenager in this club who wishes to further her academic careers and educational opportunities, then she must be student who attends the school . Considering all premises, we cannot directly conclude that Bonnie performs in school talent shows often from the given information. However, we can deduce that if Bonnie is student who attends the school and is very engaged with school events, then she must perform in school talent shows often. But premise 6 does not directly state that Bonnie is student or that she is not student. It only gives two possible scenarios for Bonnies situation. Therefore, the conclusion that Bonnie performs in school talent shows often is not directly supported by the premises, and we cannot be certain of it. <answer>Uncertain</answer> Example 4: Vanilla Qwen Based on the given premises, is the conclusion correct? Please respond with True, False, or Uncertain enclosing in <answer><answer>. Premises: Barutin Cove is cove named after the Bulgarian settlement of Barutin. Barutin Cove is on the southwest coast of Snow Island. Snow Island, Greenwich Island, and Deception Island are located in the South Shetland Islands. Antarctica is located on the South Shetland Islands. If place is located in place and place is located in place C, then place is located in place C. Conclusion: Barutin Cove is not located in Antarctica. Generated Output: from the premises, we know that barutin cove is on the southwest coast of snow island. snow island is part of the south shetland islands. the south shetland islands are not part of antarctica, but rather located near antarctica. the premises also state that antarctica is located on the south shetland islands, which seems to be contradiction. however, the correct understanding is that the south shetland islands are group of islands located near antarctica, not part of it. thus, barutin cove, being on snow island, is not located in antarctica. the conclusion is correct. <answer>True</answer> Example 5: Abductive Upcycling Qwen Generated Output: The premises state that Barutin Cove is on the southwest coast of Snow Island. Snow Island, along with Greenwich Island and Deception Island, is located in the South Shetland Islands. Furthermore, it is stated that Antarctica is located on the South Shetland Islands. theres potential confusion in the premises. The However, statement Antarctica is located on the South Shetland Islands is incorrect. In reality, the South Shetland Islands are located in the Southern Ocean, near Antarctica, but they are not part of Antarctica itself. Antarctica is continent, while the South Shetland Islands are group of islands that are geographically close to Antarctica but are not part of it. Given the logical structure provided: If place is located in place and place is located in place C, then place is located in place C. Applying this to the premises: - Barutin Cove is located in Snow Island (A in B). - Snow Island is located in the South Shetland Islands (B in C). - Therefore, by transitivity, Barutin Cove is located in the South Shetland Islands (A in C). But the conclusion states that Barutin Cove is not located in Antarctica. Since the South Shetland Islands are not part of Antarctica, and Barutin Cove is in the South Shetland Islands, it follows that Barutin Cove is not located in Antarctica. However, the premise that Antarctica is located on the South Shetland Islands is factually incorrect. This might lead to confusion. But based on the logical structure provided and assuming the premises are to be taken as true, even if they contain factual errors, the conclusion that Barutin Cove is not located in Antarctica would still follow from the transitive property of the given premises. Therefore, based on the premises as given, the conclusion is correct. <answer>True</answer> Example 6: Deductive Upcycling Qwen Generated Output: Okay, lets try to figure this out step by step. So, the question is whether the conclusion Barutin Cove is not located in Antarctica is correct based on the given premises. Let me break down the information provided. First, the premises say that Barutin Cove is cove named after the Bulgarian settlement of Barutin. Then, its located on the southwest coast of Snow Island. Next, Snow Island, Greenwich Island, and Deception Island are all in the South Shetland Islands. Then theres statement that Antarctica is located on the South Shetland Islands. Wait, that seems bit off. Wait, actually, the South Shetland Islands are group of islands located north of the Antarctic Peninsula, and they are part of the Antarctic region, but Antarctica itself is continent. So maybe theres mistake here? Let me check that again. The fourth premise says: Antarctica is located on the South Shetland Islands. That doesnt sound right. Antarctica is continent, and the South Shetland Islands are group of islands that are part of the Antarctic region, but they are not part of Antarctica itself. Wait, actually, the South Shetland Islands are considered part of the Antarctic Treaty System, but geographically, they are separate from the continent of Antarctica. So maybe the premise is incorrect? But have to take the premises as given, even if they might be factually wrong. So, according to the premises, if place is located in place and place is located in place C, then place is located in place C. Thats transitive property. So, applying that. Barutin Cove is on Snow Island. Snow Island is in the South Shetland Islands. Then, the premise says Antarctica is located on the South Shetland Islands. Wait, thats confusing. If the South Shetland Islands are in Antarctica, then Barutin Cove would be in Antarctica. But the conclusion says its not. But according to the premises, if the South Shetland Islands are part of Antarctica, then Barutin Cove, being on Snow Island (which is in South Shetland), would be in Antarctica. Therefore, the conclusion that its not located in Antarctica would be false. But wait, the premise says Antarctica is located on the South Shetland Islands. That seems like reversal. Normally, the South Shetland Islands are located in the Antarctic region, but not part of Antarctica. But if the premise states that Antarctica is located on the South Shetland Islands, then that would mean that the South Shetland Islands are part of Antarctica. So, if thats the case, then Barutin Cove is in Antarctica. Therefore, the conclusion that its not located in Antarctica would be false. But wait, the premises might be conflicting. Let me re-express: Premise 1: Barutin Cove is on Snow Island. Premise 2: Snow Island is in South Shetland Islands. Premise 3: South Shetland Islands, Greenwich Island, Deception Island are in South Shetland Islands. Wait, thats redundant. Wait, the third premise says Snow Island, Greenwich Island, and Deception Island are located in the South Shetland Islands. So, Snow Island is in South Shetland Islands. Premise 4: Antarctica is located on the South Shetland Islands. So, if the South Shetland Islands are in Antarctica, then Barutin Cove is in Antarctica. But the conclusion says its not. Therefore, the conclusion would be false. But wait, maybe the premise is that the South Shetland Islands are located in Antarctica? Wait, the wording is Antarctica is located on the South Shetland Islands. That would mean that Antarctica is part of the South Shetland Islands, which is not correct in reality, but according to the premises, we have to take it as given. So, if Antarctica is located on the South Shetland Islands, then the South Shetland Islands are part of Antarctica. Therefore, Barutin Cove, being on Snow Island (which is in South Shetland), would be in Antarctica. Therefore, the conclusion that its not located in Antarctica would be false. Therefore, the answer should be False. But wait, maybe Im misunderstanding the premise. Let me check again. Wait, the fourth premise says Antarctica is located on the South Shetland Islands. Thats bit confusing. Normally, the South Shetland Islands are located in the Antarctic region, but they are not part of Antarctica. However, if the premise states that Antarctica is located on the South Shetland Islands, that would imply that the South Shetland Islands are part of Antarctica. So, if thats the case, then Barutin Cove is in Antarctica. Therefore, the conclusion that its not located in Antarctica would be false. Therefore, the answer should be False. But wait, maybe the premise is that the South Shetland Islands are located in Antarctica? But the wording is Antarctica is located on the South Shetland Islands, which is the opposite. So, if Antarctica is located on the South Shetland Islands, then the South Shetland Islands are part of Antarctica. Therefore, Barutin Cove is in Antarctica. Hence, the conclusion is false. Therefore, the answer is False. <answer>False</answer>"
        },
        {
            "title": "F Symbolic Generation Example",
            "content": "The following examples shows the generated output of answering symbolic inductive task. The vanilla Llama-3.1-8B-Instruct failed to answer the question, while Inductive and Deductive Full FT can generate the correct answer. The vanilla model (Example 7) fails on inductive task, while inductive (Example 8) and deductive (Example 9) Full FT succeed. Example 8 adopts pattern-driven, data-oriented perspective, while Example 9 follows rule-based, hypothesis-testing process, consistent with their respective paradigms. Interestingly, both induced models exhibit self-reflection by revising earlier reasoning. These observations indicate that inducing different reasoning paradigms steers models toward distinct reasoning perspectives, while shared behaviors across induced models demonstrate transferability among these paradigms (6.1). Example 7: Untrained Llama 3.1 8B This is <Inductive> reasoning task. Given the following sequence, [2, 5, 20, 23, 26, 104, 107, 110, 440, 443, ?] What in <answer><answer>, and put all <think><think>. Please enclose the answer the intermediate reasoning steps in the question mark? is the value at Generated Output: <think>The given sequence appears to be formed by alternating between two different operations: multiplication and addition. To identify the pattern, lets break down the sequence into two separate sequences, one for the numbers that are obtained by multiplication and one for the numbers that are obtained by addition.</think> Sequence 1 (Multiplication): 2, 20, 104, 440 Sequence 2 (Addition): 5, 23, 107, 443 <think>Now, lets analyze Sequence 1. The numbers in this sequence are obtained by multiplying the previous term by 10 and then adding 2. This can be represented as: 2 * 10 + 2 = 20, 20 * 10 + 2 = 202, but 202 is not in the sequence, however 20 * 10 + 4 = 204 is not in the sequence either, but 20 * 10 + 2 = 202 is not in the sequence, however 20 * 10 + 4 = 204 is not in the sequence... Keeping Repeatitive until hit max number of tokens Example 8: Inductive Full FT Llama This is an <Inductive> reasoning task. Given the following sequence, [2, 5, 20, 23, 26, 104, 107, 110, 440, 443, ?] What is the value at the question mark? Please enclose the answer in <answer><answer>, and put all the intermediate reasoning steps in <think><think>. Generated Output: <think> To find the pattern in the sequence, lets first examine the differences between consecutive numbers: 5 - 2 = 3, 20 - 5 = 15, 23 - 20 = 3, 26 - 23 = 3, 104 - 26 = 78, 107 - 104 = 3, 110 - 107 = 3, 440 - 110 = 330, 443 - 440 = 3 From the differences, we can observe pattern where the difference alternates between 3 and larger number. The larger numbers are increasing (15, 78, 330), which suggests pattern of multiplication or addition that increases each time. Looking closer at the larger differences: - 15 * 5.2 = 78 (approximately) - 78 * 4.23 = 330 (approximately) However, more precise pattern can be observed: 15 = 3 * 5, 78 = 15 * 5 + 3, 330 = 78 * 4 + 6 But simpler pattern can be observed if we consider the sequence of operations: Start with 2, Add 3 to get 5, Multiply by 4 to get 20, Add 3 to get 23, Add 3 to get 26, Multiply by 4 to get 104, Add 3 to get 107, Add 3 to get 110, Multiply by 4 to get 440, Add 3 to get 443 Following this pattern, the next operation should be to add 3 again, as the pattern alternates between multiplying by 4 and adding 3. So, the next number in the sequence after 443 would be 443 + 3 = 446. </think> <answer>446</answer> Example 9: Deductive Full FT Llama Generated Output: <think> To solve this, lets analyze the given sequence for any patterns or rules that could help us determine the next value. The sequence is: 2, 5, 20, 23, 26, 104, 107, 110, 440, 443, ?. Looking at the sequence, we can see that it starts with 2 and 5, which are both prime numbers. Then, 20 and 23 are not prime, but 26 is not prime either, which makes us think that the sequence might not follow simple pattern of prime numbers. However, if we look closer, we can see that the sequence seems to be formed by adding certain number to the previous term to get the next term. Lets try to find this pattern. From 2 to 5, the difference is 3. From 5 to 20, the difference is 15. From 20 to 23, the difference is 3. From 23 to 26, the difference is 3. From 26 to 104, the difference is 78. From 104 to 107, the difference is 3. From 107 to 110, the difference is 3. From 110 to 440, the difference is 330. From 440 to 443, the difference is 3. We can see that the differences are not consistent except for the differences between 2 and 5, 23 and 26, 104 and 107, 107 and 110, and 440 and 443, which are all 3. However, the other differences are not consistent. But if we look at the sequence again, we can see that the numbers 20, 104, and 440 are all multiples of 4, and 23, 26, 107, and 110 are all 3 more than multiple of 4. So, the pattern seems to be that the sequence is formed by adding 3 to the previous term when the previous term is 3 more than multiple of 4, and the term before that is multiple of 4. Lets apply this pattern to find the next term: - The last term before the question mark is 443, which is 3 more than multiple of 4 (440). - So, the next term should be 443 + 3 = 446. Therefore, the value at the question mark is 446. </think> <answer>446</answer>"
        }
    ],
    "affiliations": [
        "School of Computer Science, University of Sheffield",
        "School of Electronic Engineering and Computer Science, Queen Mary University of London",
        "The Alan Turing Institute"
    ]
}