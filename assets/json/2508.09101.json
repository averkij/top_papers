{
    "paper_title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators",
    "authors": [
        "Jason Chou",
        "Ao Liu",
        "Yuchi Deng",
        "Zhiying Zeng",
        "Tao Zhang",
        "Haotian Zhu",
        "Jianwei Cai",
        "Yue Mao",
        "Chenchen Zhang",
        "Lingyun Tan",
        "Ziyan Xu",
        "Bohui Zhai",
        "Hengyi Liu",
        "Speed Zhu",
        "Wiggin Zhou",
        "Fengzong Lian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 1 0 1 9 0 . 8 0 5 2 : r 2025-08-13 AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators Jason Chou Ao Liu Yuchi Deng Zhiying Zeng Tao Zhang Haotian Zhu Jianwei Cai Yue Mao Chenchen Zhang Lingyun Tan Ziyan Xu Bohui Zhai Hengyi Liu Speed Zhu Wiggin Zhou Fengzong Lian Hunyuan Team, Tencent {wigginzhou,faxonlian}@tencent.com Homepage"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios."
        },
        {
            "title": "Introduction",
            "content": "Recently, Large Language Models (LLMs) have undergone rapid development, demonstrating impressive capabilities in various tasks (OpenAI, 2024; Gemini, 2025; DeepSeek-AI, 2025b; Tencent, 2025). Among these, code generation has emerged as key indicator of both model intelligence and practical utility, attracting growing attention from both academia and industry (Chen et al., 2021; Jimenez et al., 2024; Jiang et al., 2024). By generating executable code, LLMs have the potential to significantly enhance programming automation and alleviate the burden of manual coding. Many popular and powerful LLMs like Claude 4 (Anthropic, 2025) and DeepSeek-V3 (DeepSeek-AI, 2025a) have already been widely adopted in AI-assisted coding scenarios (Cursor, 2025). To measure and improve the code generation capabilities of LLMs, numerous code generation benchmarks are proposed (Wang et al., 2025a). Early works such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) laid the foundation by evaluating LLMs abilities on short, algorithm-focused Python problems. However, these simple and narrowly defined tasks have quickly become outdated with the rapid development in LLMs. Recent benchmarks (Quan et al., 2025; Jain et al., 2025; Zheng et al., 2025; Zhu et al., 2025) have shifted focus to more challenging, competition-level Python problems with the help of professional manual annotations. On the other hand, some other benchmarks (Peng et al., 2024; Chai et al., 2025; Bytedance, 2025), such as FullStackBench (Bytedance, 2025), emphasize more practical multilingual programming scenarios. However, manually crafting these programming problems and test cases is time-consuming and labor-intensive, making it difficult to simultaneously achieve high Equal Contributions. Corresponding Authors. 1 Benchmark MLing MLogi HFree BDist Difficulty Category Data Size Problem Len HumanEval MBPP BigCodeBench LiveCodeBench FullStackBench McEval AutoCodeBench / / / / 5 6 8 4 12 9 164 378 1140 1100 1687 2007 3920 134.1 50.5 146.7 469.6 184.3 146.7 498.2 Table 1: Comparison of Existing Code Generation Benchmarks and AutoCodeBench. MLing: MultiLingual; MLogi: MultiLogical, refers to programming problems that require the model to simultaneously implement multiple core functionalities. HFree: Human-Free; BDist: Balanced Distribution of multiple languages. The Difficulty is rated based on the performance of DeepSeek-V3-0324. The number of Category is obtained using predefined labels from DeepSeek-V3-0324. The Problem Length is calculated using Qwen2.5-32B-Instruct tokenizer. difficulty and multilingual coverage, as shown in Table 1. Therefore, the community currently needs benchmark that combines high difficulty, practical diversity, and balanced multilingual distribution to comprehensively evaluate the code generation capabilities of LLMs. In this paper, we propose AutoCodeGen, an automated workflow centered around LLM-Sandbox Interaction, to accurately synthesize high-difficulty multilingual code generation datasets without any manual annotations. Unlike previous data synthesis strategies (Luo et al., 2024; Wei et al., 2024b; Xu et al., 2025; Li et al., 2025), we ensure high data quality by constructing multilingual sandbox to generate test outputs and synthesizing programming problems in reverse order. Concretely, AutoCodeGen consists of the following key steps: 1) Solution Generation: Based on real-world code snippets, LLMs evolve self-contained code solutions, ensuring their practicality and diversity. 2) Test Function Generation: LLMs generate test inputs, which are concatenated with the code solutions and executed in the sandbox to obtain the test outputs. The two are then combined to form complete test function. Compared to directly generating complete test cases as in KodCode (Xu et al., 2025) or using Input Generator (Li et al., 2025), our method efficiently ensures the correctness and completeness of the test cases. 3) Problem Generation: LLMs generate challenging programming problems based on heuristic specifications and integrates public test cases into them. 4) Filtering: Finally, we filter the data by Multiple Sampling, LLM-as-Critic, and Tagging to maintain the high-difficulty, high-quality, and diversity. Based on the automation workflow mentioned above, we propose AutoCodeBench, large-scale, humanfree code generation benchmark featuring 3,920 problems, as shown in Table 1. Compared with previous multilingual benchmarks (Cassano et al., 2022; Bytedance, 2025; Chai et al., 2025), ours simultaneously offers high difficulty, diversity, and practicality, with balanced distribution of problems across 20 programming languages. Besides, we intentionally include multi-logical problems to assess the LLMs ability to handle multi-logical reasoning. We believe this capability is important in the code agents scenarios like SWE-Bench (Jimenez et al., 2024). The key contributions of this paper are as follows: 1. AutoCodeGen. We propose an automated workflow based on LLM-Sandbox Interaction, where LLMs generate test inputs and obtain test outputs through the sandbox, to create high-quality code generation benchmarks. Worth mentioning is that this workflow can also be applied to synthesize high-quality training data. 2. AutoCodeBench. We introduce AutoCodeBench, large-scale code generation benchmark with 3,920 problems, evenly distributed across 20 programming languages, featuring high-difficulty, practicality, and diversity. Based on the evaluation results, we construct simplified version AutoCodeBench-Lite. Besides, We tailor AutoCodeBench-Complete, completion-based code generation benchmark, to assess the performance of base models. 3. Multilingual Sandbox. We open-source sandbox that supports 20+ programming languages. It is capable of high concurrency and request-based calls, making it suitable for multilingual evaluation and large-scale training. 4. Experimental Results. We evaluate 30+ open-source and proprietary models. The results show that even the most advanced LLMs still struggle with complex and diverse multilingual programming tasks, especially in multi-logical scenarios. 5. Data and Experiment Analysis. We conduct comprehensive analysis of AutoCodeGen and 2 #Problems #Test Cases #Langs Prob Len Solu Len Difficulty (Easy/Med/Hard) ACB ACB-Lite 3,920 1,586 37,777 15,341 20 20 498.2 517. 487.5 469.3 646/846/2428 263/421/902 Table 2: Statistics of ACB and ACB-Lite. ACB: AutoCodeBench; Langs: Languages; Prob: Problem; Solu: Solution; Len: Length; Med: Medium. The difficulty level is determined by the number of passes in ten samplings of DeepSeek-Coder-V2-Lite. Problems with zero correct solutions are classified as hard, 1-5 correct solutions as medium, and those with more than five as easy. Figure 1: Tag and Language Distribution across our AutoCodeBench. AutoCodeBench, focusing on key aspects such as the quality, difficulty, and diversity of the generated data, as well as potential model biases during the generation process. We hope that these insights can provide valuable experience for the community in the development of future code generation benchmarks."
        },
        {
            "title": "2 AutoCodeBench: A Challenging, Practical, and Diverse Multilingual Benchmark",
            "content": "In this section, we first provide an overall data statistics of the AutoCodeBench and its simplified version AutoCodeBench-Lite. Following this, we introduce the automated workflow, AutoCodeGen, used to generate AutoCodeBench(-Lite). 2.1 Data Overview As shown in Table 1 and 2, AutoCodeBench is large-scale, high-difficulty multilingual benchmark. Over 60% of the problems are classified as hard problems, with each problem averaging 498.2 characters and accompanied by 9.6 test cases, providing challenging and comprehensive evaluation standard. The 20 languages are as follows: Python, C++, Java, JavaScript(JS), Go, Shell, C#, Dart, Elixir, Julia, Kotlin, Perl, PHP, Racket, R, Ruby, Rust, Scala, Swift, TypeScript(TS). To analyze the diversity and language coverage of AutoCodeBench, we first use Claude Sonnet 4 to generate 20 language-agnostic task categories, and then employ DeepSeek-V3-0324 to classify each problem accordingly. Categories with less than 2% representation are merged into the Other group. As shown in Figure 1, AutoCodeBench covers 14 categories, demonstrating comprehensive coverage of practical programming scenarios. Besides, we analyze the distribution of problems across the 20 programming languages. AutoCodeBench exhibits relatively balanced distribution across languages, with no significant bias toward any specific one, further validating its completeness and representativeness as multilingual benchmark. Category labels and statistics of other benchmarks are provided in Appendix A. 3 Figure 2: The overview of AutoCodeGen. It first generates code solution and the corresponding public/private test input functions based on multilingual code snippets ( 1). They are concatenated and executed in sandbox to obtain test outputs, which are then combined by the LLM into complete test functions ( 2, 3, 4). Based on the code solution and test function, the LLM is prompted to generate accurate programming problems ( 5). Finally, three-stage data filtering is applied: multiple sampling to remove too easy problems ( 6), LLM-as-Critic to discard low-quality ones ( 7), and diversity-based tagging to ensure distributional variety ( 8). 2.2 Automated Workflow Our AutoCodeGen based on LLM-Sandbox interaction for constructing code generation benchmarks is fully automated. It first generates large-scale multilingual data with guaranteed executability and correctness, then applies three-stage filtering strategy to ensure the benchmark is challenging, highquality, and diverse. As illustrated in Figure 2, the workflow includes four key stages: Code Solution Generation ( 1), Test Function Generation ( 2, 3, 4), Programming Problem Generation ( 5), and Data Filtering ( 6, 7, 8). 2.2.1 Code Solution Generation We begin by extracting multilingual code snippets from Stack-Edu (Allal et al., 2025), large-scale dataset of educational code filtered from The Stack v2 (Lozhkov et al., 2024), as seeds. These seeds span function-level, class-level, and file-level code, sourced from real GitHub repositories, ensuring diversity and practicality. Using language-specific few-shot prompt, we guide DeepSeek-V3-0324 to refine and evolve these seeds into verifiable and self-contained code solutions. During this process, the model removes non-essential logic and adds appropriate comments for clarity. We then validate the correctness of the generated solutions by multilingual sandbox. 2.2.2 Test Function Generation We enhance efficiency and edge-case coverage by first generating test inputs via LLMs and then executing them in sandbox to obtain the corresponding outputs. Specifically, Test Function Generation is divided into the following three steps: Test Input Generation The test input functions (both public and private) are generated alongside the above code solution, ensuring alignment between the solution and its inputs. The public test input function includes no more than 3 basic cases and serves demonstration purposes; it will be embedded into the final programming problem as an illustrative usage. In contrast, the private test input function contains 7+ inputs, including edge cases, and functions as the comprehensive test for verifying the correctness of the code solution. Test Output Generation We concatenate the code solution with test input functions and execute them in the sandbox to obtain the corresponding test outputs. Input-Output Integration We prompt DeepSeek-V3-0324 with both the test input functions and output results to generate coherent and verifiable test functions. Finally, we validate the correctness by executing the code solution together with the generated public and private test functions in the sandbox. 4 Origin Target Origin Target Origin Target Origin Target Origin Target Python Python Python Elixir Swift Ruby Python Python Julia Python Racket Java Java Java Scala Kotlin Dart Java JavaScript JavaScript C# PHP Typescript Shell C++ Perl Rust Table 3: Programming language translation pairs. 2.2.3 Programming Problem Generation Generating high-quality programming problems is challenging, as it requires detailed and accurate problem descriptions. We find that models often omit key information when generating programming problems, such as the entry point specified in the test function. Therefore, we define set of specifications that generated problems must meet: Language Specification: Explicitly states the programming language to be used. Problem Description: precise and unambiguous description of the task. Function/Class Naming: Clear identification of all functions and classes involved in the test function. Input/Output Format: Explicit definitions of input and output types and value ranges. Example Usage: Provides sample tests embedded with the generated public test functions for reference. No Solution Hints: The problem description must not include hints to the solution. Using these guidelines, we prompt DeepSeek-V3-0324 to generate high-quality programming problems based on the code solution (with appropriate comments) and the corresponding test function, while embedding the public test function as example usage. Through these three steps, we obtain large-scale multilingual dataset, where each instance is represented as tuple <programming problem, code solution, public test function, private test function>. 2.2.4 Data Filtering Finally, We apply three filtering and sampling steps to ensure the high-difficulty, high-quality, and diversity of the final benchmark. Difficulty Control Programming problems that are too simple are not meaningful for evaluating the code generation capabilities of current LLMs. To address this, we employ moderately capable code model, DeepSeek-Coder-V2-Lite, to filter out too easy problems. Specifically, we sample answers for each problem ten times using the model and validate the correctness via sandbox execution. Problems that are solved correctly in all ten attempts are discarded. Take Python as an example, DeepSeek-Coder-V2-Lite can filter out 25.1% of overly simple problems. Quality Control During the aforementioned problem generation stage, we define six specifications to guide the generation of detailed and accurate programming problems. To further ensure high quality, we employ DeepSeek-R1-0528 to critique each <problem, test function> pair. Specifically, assuming the problem is entirely correct, we evaluate the test functions consistency with problem based on the following seven concepts: Whether the function/class names or signatures match the problem description. Whether the test cases involve randomness or are non-reproducible. Whether the testing objective aligns with the stated purpose of the problem. Whether numerical precision is handled appropriately. Whether exception handling is used in way that invalidates the test cases. Whether the test cases check for requirements beyond the problem description. Whether the test cases are comprehensive. Diversity Sampling We aim for our benchmark to cover as many real-world scenarios as possible. To this end, we perform diversity-based sampling on the existing data to construct the final benchmark. We use DeepSeek-V3-0324 to label each problem. We then divide the problems into different pools by category and perform cyclic sampling, ensuring broad representation of programming scenarios. Average Python Cpp Java JS Go Shell Csharp Dart Elixir Julia Kotlin Perl PHP Racket Ruby Rust Scala Swift TS Count Current Upper Bound Claude Opus 4 (20250514) Claude Sonnet 4 (20250514) o3-high (20250416) o4-mini (2025-04-16) Grok-4 Gemini2.5 Pro DeepSeek-R1-0528 Seed1.6-enabled (250615) Seed1.6-Thinking-250715 Seed1.6-Thinking-250615 GLM-4.5-enable GLM-4.5-Air-enable ERNIE-X1-Turbo-32K Qwen3-235B-A22B-Thinking-2507 Qwen3-235B-A22B Qwen3-32B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Claude Opus 4 (20250514) Claude Sonnet 4 (20250514) GPT4.1 (2025-04-14) GPT4o (2024-11-20) Gemini2.5 Flash DeepSeek-V3-0324 DeepSeek-Coder-V2 DeepSeek-Coder-33B-Instruct DeepSeek-Coder-6.7B-Instruct Kimi-K2-0711-Preview Hunyuan-TurboS-20250716 Hunyuan-Coder-7B-Preview Seed1.6-disabled (250615) Seed-Coder-8B-Instruct Qwen3-Coder-480B-A35B-Instruct Qwen3-235B-A22B-Instruct-2507 Qwen3-32B Qwen3-14B Qwen3-8B Qwen3-1.7B Qwen2.5-Coder-32B Qwen2.5-72B Qwen2.5-Coder-7B Qwen2.5-Coder-1.5B OpenCoder-8B 74. 52.4 51.1 51.1 50.0 50.9 48.7 50.2 45.3 45.0 44.7 46.6 40.8 39.6 47.7 45.9 41.7 37.6 28.5 24.3 11.2 50.9 49.3 48.0 41.1 45.7 48.1 37.7 28.5 20.5 47.8 43.8 33.4 42.9 32.3 44.8 43.1 31.0 28.6 23.3 7.9 35.8 34.3 22.5 10.3 19.3 196 63.3 40.3 37.2 40.8 42.3 41.2 40.3 38.8 39.8 40.3 38.8 41.0 39.3 39.4 37.8 36.7 37.8 37.8 28.1 27.6 16.8 37.8 35.7 37.2 33.7 39.3 36.7 29.1 25.0 18.9 38.8 34.2 25.5 35.2 23.5 39.4 35.7 26.5 24.5 22.4 8.7 29.6 33.2 19.9 12.2 14.3 186 74. 188 78.7 184 59.2 191 69.1 188 70.7 199 88.4 200 78. Reasoning Mode 44.1 46.8 47.3 46.8 48.7 47.5 43.6 44.6 45.2 47.0 43.2 37.6 17.8 41.9 43.5 38.7 35.5 22.6 17.7 5.4 45.7 47.3 46.8 37.1 44.1 48.4 34.9 24.2 12.9 42.5 34.9 26.9 40.3 23.7 41.1 38.2 21.5 22.6 11.3 1.1 27.4 24.7 8.6 2.7 9.7 55.9 52.7 53.2 51.6 50.0 53.2 52.7 46.3 50.0 49.5 47.9 39.4 33.2 48.4 47.3 39.9 35.1 21.8 22.3 4.8 50.0 52.7 48.9 45.2 50.0 52.7 34.0 29.3 19.7 47.9 47.9 34.0 46.8 33.5 51.1 49.5 29.5 32.4 25.0 2.7 33.0 31.9 22.3 4.8 19.1 38.6 34.8 40.8 40.2 37.5 37.0 35.9 28.3 33.2 38.0 34.8 31.0 32.6 39.7 36.4 32.6 30.4 28.3 25.5 12. 38.0 38.0 34.8 34.8 33.2 31.5 27.7 24.5 19.6 37.5 32.6 27.2 32.6 28.8 27.9 29.3 28.0 27.2 22.8 8.2 29.9 28.8 21.2 12.0 12.5 37.2 41.9 22.0 31.4 41.4 37.2 38.7 40.8 38.2 31.4 37.8 39.8 37.4 39.8 37.7 36.1 30.4 29.3 24.1 9.9 35.6 37.7 37.2 30.9 33.0 34.6 29.8 29.8 21.5 31.4 34.6 21.5 34.6 22.5 31.4 33.5 25.5 16.8 18.3 3.7 23.0 19.9 12.6 7.3 17.3 51.6 48.9 49.5 45.2 47.3 45.2 46.8 44.1 39.9 38.8 43.9 36.7 33.9 45.2 42.0 39.4 36.2 27.1 28.2 11.7 74.9 72.4 68.3 68.3 72.4 70.9 75.4 60.3 67.3 62.3 70.5 66.3 46.0 71.9 70.9 67.8 60.8 52.8 42.2 19.6 54.0 53.5 55.0 54.0 49.5 54.0 52.5 39.5 36.5 41.0 42.0 38.0 33.0 46.0 45.5 34.5 29.0 21.0 13.0 7. 198 97.5 80.3 81.8 80.8 82.3 76.8 68.7 77.3 69.7 67.7 70.7 72.5 61.5 68.9 79.8 68.7 65.2 62.1 43.9 33.3 20.7 Non-Reasoning Mode 47.3 47.9 36.7 29.3 37.8 37.8 31.4 22.3 16.0 40.4 38.3 29.3 35.1 20.7 41.1 40.4 24.0 23.9 22.3 11.7 29.3 31.9 21.3 14.4 21.3 73.9 72.9 74.4 65.3 68.3 72.9 63.8 54.8 44.2 75.9 64.8 54.8 70.9 54.8 63.0 67.3 59.3 50.8 42.2 9.5 58.3 54.3 38.7 17.6 33.2 57.0 51.0 46.5 43.5 49.5 48.0 33.5 17.5 11.5 50.0 44.5 36.0 42.5 30.5 36.5 39.5 27.5 21.0 17.0 3.5 34.5 34.0 18.5 6.5 15. 82.3 74.2 76.8 62.6 64.0 75.8 60.6 67.7 47.5 80.3 70.7 60.1 69.7 57.1 73.7 59.1 52.0 42.4 41.4 17.2 59.6 65.7 47.0 35.4 34.3 200 78.0 55.5 49.0 54.5 49.0 55.0 54.0 52.0 51.0 51.0 45.0 47.5 42.0 54.0 48.5 46.0 42.5 34.5 29.0 20.0 11.0 55.0 51.0 50.0 36.0 47.5 49.0 37.5 14.5 15.5 52.0 47.0 30.5 45.0 33.0 49.5 46.0 28.0 22.5 18.5 6.0 35.5 30.5 18.0 4.0 15.5 200 89.5 72.5 71.5 72.0 74.0 70.0 72.0 70.0 58.0 61.0 68.0 66.0 53.0 49.5 58.0 60.0 52.0 44.5 36.0 29.5 9. 75.5 72.0 72.0 67.0 70.0 69.0 58.5 52.0 45.5 68.0 62.0 57.0 62.0 52.5 63.1 59.5 45.0 42.0 36.0 11.5 56.0 50.5 39.0 16.5 30.0 200 64.5 199 52.8 44.5 45.0 44.0 44.0 44.0 41.0 39.0 41.5 41.0 39.0 43.5 40.5 39.5 40.5 39.0 40.5 37.5 35.5 34.5 19.5 43.0 44.0 43.5 43.0 39.5 42.5 35.5 29.5 21.5 41.5 42.0 34.0 39.5 34.0 41.0 44.5 34.5 34.5 29.5 15.5 38.5 30.0 27.5 15.0 26.5 28.1 34.7 32.7 30.2 27.1 29.7 28.6 25.6 26.1 25.1 28.6 27.1 23.9 29.1 29.1 27.6 23.1 18.6 16.1 7. 26.6 30.7 29.2 26.6 24.1 28.1 25.1 19.1 10.6 28.1 30.2 22.2 23.1 25.1 27.2 26.1 21.6 24.6 18.1 7.5 26.1 22.6 15.1 5.0 14.6 196 88.3 68.9 68.9 53.1 45.4 63.8 52.6 56.1 52.6 51.0 47.5 50.0 40.3 45.3 56.6 52.0 37.8 44.9 13.3 11.7 5.6 64.8 63.8 50.5 37.2 38.3 59.2 41.8 28.1 15.3 57.1 45.9 37.8 49.5 36.7 56.3 49.5 28.6 28.1 23.5 4.6 35.7 39.3 24.0 7.1 15.8 198 74.2 52.5 50.5 47.5 43.4 48.5 49.5 50.5 51.0 44.9 47.5 45.0 39.0 44.3 49.0 47.0 44.9 36.9 30.8 23.7 9. 47.0 44.4 42.4 32.8 51.5 45.0 35.4 18.7 13.1 40.4 39.9 27.3 40.4 28.3 42.7 44.0 22.7 26.3 16.2 7.6 31.3 27.3 17.7 7.6 17.2 200 79.5 61.0 54.5 59.0 59.0 61.5 56.5 58.5 52.0 55.5 50.5 54.5 47.0 48.0 55.0 56.5 47.0 43.5 37.0 27.5 21.0 54.0 51.5 54.0 43.0 53.0 52.5 45.0 33.0 27.5 52.5 53.0 40.5 46.5 35.5 51.5 46.5 36.5 33.5 27.0 14.5 40.0 42.0 29.5 15.5 29.5 199 61.3 38.7 36.2 42.2 40.2 37.7 24.6 37.2 28.6 27.6 30.7 31.6 25.1 20.8 35.7 31.7 28.1 24.6 12.6 8.5 0. 38.2 35.2 37.2 29.6 36.2 37.2 22.6 8.0 6.0 36.7 30.7 11.1 28.1 15.6 25.4 24.6 16.1 17.1 7.5 0.5 23.1 23.1 7.0 1.0 6.0 199 77.4 50.3 48.2 51.3 50.3 52.8 46.7 51.8 41.7 37.2 39.7 41.0 30.5 40.4 40.4 43.7 37.2 28.6 21.1 20.1 2.5 46.7 45.2 44.2 38.2 44.2 46.7 33.2 24.1 11.1 42.7 39.2 31.2 32.7 29.6 42.1 37.7 26.6 20.6 19.1 3.0 29.6 32.7 19.1 4.5 17.1 200 78.0 50.0 48.0 59.0 54.0 51.5 49.5 55.0 47.5 46.5 41.5 46.0 38.5 44.0 46.0 41.5 42.0 36.0 22.0 20.0 10. 51.5 45.5 49.5 45.0 46.5 48.0 38.0 18.0 8.0 47.0 39.5 27.6 40.0 28.0 47.5 46.0 30.0 26.5 17.5 6.0 39.0 29.5 17.0 5.0 15.0 199 61.3 47.2 44.2 47.2 45.7 40.7 41.7 41.2 41.2 38.7 40.2 42.2 42.7 37.7 44.2 41.7 40.2 38.7 37.7 39.2 19.6 46.7 44.2 46.2 39.2 41.2 43.7 35.7 29.1 23.6 42.2 42.2 31.2 42.7 31.2 41.9 43.2 35.2 32.7 29.1 14.6 35.2 33.2 24.6 11.1 21.1 Table 4: Pass@1 (%) performance of different models for AutoCodeBench. Current Upper Bound represents the Pass@1 value calculated by taking the union of problems correctly solved by all models. 2.2.5 Approximate Language Translation For Python, C++, Shell, Java, JavaScript, and Go, we directly use the workflow described above. For the other 14 languages, while the proposed workflow is still applicable, we choose to employ an approximate language translation approach due to their limited data resources and lack of diversity. We extract unused data from the data pool generated in Section 2.2.3 and translate them into the target low-resource language, as shown in Table 3. This ensures sufficient and diverse dataset, which is further refined through the Data Filtering process, as described in Section 2.2.4. 2.2.6 AutoCodeBench-Lite Construction To facilitate quicker and more efficient model evaluations, we create the AutoCodeBench-Lite, simplified subset of AutoCodeBench. Specifically, we collect the problem-solving results from all models and sort the problems in ascending order based on the number of passes. After discarding problems with fewer than 2 passes, we select approximately 1,500 problems based on their pass count in ascending order. These problems, which have been solved correctly by existing models at least twice and have certain level of difficulty, are selected to amplify the differences between the models. We use these problems as the set for the Lite version."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup LLMs We evaluate diverse set of open-source models with sizes ranging from 1.5B to 1T parameters, as well as leading proprietary models on AutoCodeBench. These models are classified based on their families: OpenAI: o3 and o4-mini (OpenAI, 2025); GPT4.1 (OpenAI, 2025), and GPT4o (OpenAI, 2024). Claude: Claude Opus 4 and Claude Sonnet 4 (Anthropic, 2025). Gemini: Gemini 2.5 Pro and Gemini 2.5 Flash (Gemini, 2025). DeepSeek: DeepSeek-R1-0528 (DeepSeek-AI, 2025b), DeepSeek-V3-0324 (DeepSeek-AI, 2025a) and DeepSeek-Coder Series (DeepSeek-AI et al., 2024; Guo et al., 2024). 6 Count Current Upper Bound 100.0 65 100.0 78 100.0 88 100.0 57 100.0 75 100. 72 100.0 80 100.0 94 100.0 61 100.0 82 100.0 84 100. 73 100.0 60 100.0 106 100.0 89 100.0 88 100.0 80 100. 97 100.0 95 100.0 62 100.0 Average Python Cpp Java JS Go Shell Csharp Dart Elixir Julia Kotlin Perl PHP Racket Ruby Rust Scala Swift TS Claude Opus 4 (20250514) Claude Sonnet 4 (20250514) o3-high (20250416) o4-mini (2025-04-16) Grok-4 Gemini2.5 Pro DeepSeek-R1-0528 Seed1.6-Thinking-250715 Seed1.6-enabled (250615) Seed1.6-Thinking-250615 GLM-4.5 (enabled) GLM-4.5-Air (enabled) ERNIE-X1-Turbo-32K Qwen3-235B-A22B-Thinking-2507 Qwen3-235B-A22B Qwen3-32B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Claude Opus 4 (20250514) Claude Sonnet 4 (20250514) GPT4.1 (2025-04-14) GPT4o (2024-11-20) Gemini2.5 Flash DeepSeek-V3-0324 DeepSeek-Coder-V2-Instruct DeepSeek-Coder-33B-Instruct DeepSeek-Coder-6.7B-Instruct Kimi-K2-0711-Preview Hunyuan-TurboS-20250716 Hunyuan-Coder-7B-Preview Seed1.6-disabled (250615) Seed-Coder-8B-Instruct Qwen3-Coder-480B-A35B-Instruct Qwen3-235B-A22B-Instruct-2507 Qwen3-32B Qwen3-14B Qwen3-8B Qwen3-1.7B Qwen2.5-72B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-1.5B-Instruct OpenCoder-8B-Instruct 64.5 62.0 63.2 60.5 63.0 59.1 61.5 53.9 53.2 51.2 55.0 46.2 44.4 57.3 54.7 47.6 40.7 28.9 23.2 10.8 61.7 59.8 56.9 45.8 52.9 56.8 40.5 27.7 19.9 55.4 50.3 36.4 48.8 32.7 51.5 49.8 32.3 27.8 21.4 7.3 35.1 37.0 21.5 10.2 20.1 56.9 53.9 61.5 63.1 60.9 56.9 55.4 56.9 52.3 52.3 56.1 53.9 50.8 55.4 53.9 50.8 46.2 29.2 33.9 21.5 49.2 44.6 49.2 41.5 49.2 49.2 29.2 23.1 20.0 50.8 43.1 32.3 41.5 20.0 52.3 43.1 29.2 24.6 26.2 7.7 41.5 33.9 23.1 12.3 13.9 55.1 60.3 59.0 55.1 64.1 59.7 56.4 60.3 57.7 64.9 54.7 44.9 26.3 52.6 57.7 43.6 39.7 24.4 15.4 2.6 57.7 61.5 57.7 42.3 56.4 61.5 37.2 25.6 11.5 52.6 46.2 28.2 51.3 25.6 44.9 47.4 25.6 20.5 6.4 0.0 24.4 26.9 6.4 3.9 11. 64.8 60.2 60.2 54.6 55.7 59.1 61.4 58.0 51.1 53.4 56.1 45.5 37.9 54.6 55.7 46.6 39.8 19.3 18.2 4.6 54.6 60.2 48.9 53.4 54.6 63.6 37.5 23.9 20.5 51.1 54.6 43.2 50.0 31.8 59.1 55.7 31.8 30.7 31.8 1.1 31.8 38.6 21.6 3.4 20.5 57.9 50.9 64.9 57.9 56.1 54.4 56.1 49.1 45.6 56.1 49.0 45.6 49.1 64.9 50.9 43.9 42.1 35.1 24.6 14.0 59.7 57.9 52.6 49.1 50.9 42.1 26.3 35.1 28.1 57.9 45.6 35.1 47.4 42.1 45.6 43.9 31.6 38.6 28.1 7.0 40.4 40.4 24.6 17.5 17.5 53.3 64.0 33.3 45.3 61.3 50.7 53.3 58.7 60.0 42.7 54.2 50.7 48.0 53.3 53.3 46.8 37.3 38.7 30.7 10.7 56.0 56.0 38.7 38.7 45.3 50.7 42.7 37.3 29.3 45.3 46.7 24.0 50.7 28.0 42.7 44.0 32.0 16.0 20.0 5.3 24.0 28.0 12.0 12.0 25. Reasoning Mode 82.5 78.8 71.3 66.3 82.5 75.0 83.8 73.8 62.5 62.5 76.3 72.5 46.8 80.0 80.0 72.5 62.5 51.3 38.8 18.8 59.6 61.7 67.0 66.0 55.3 64.9 60.6 41.5 43.6 38.3 39.5 38.3 28.7 50.0 50.0 36.2 24.5 18.1 11.7 6.4 65.6 63.9 63.9 73.8 59.0 62.3 60.7 52.5 52.5 60.7 59.3 50.8 53.5 68.9 55.7 55.7 52.5 39.3 27.9 13.1 Non-Reasoning Mode 81.3 83.8 83.8 66.3 75.0 77.5 66.3 52.5 40.0 87.5 75.0 57.5 78.8 52.5 68.8 73.8 56.3 47.5 38.8 8.8 52.5 57.5 42.5 16.3 30. 66.0 56.4 54.3 46.8 56.4 52.1 30.9 14.9 10.6 53.2 43.6 37.2 42.6 27.7 37.2 38.3 25.5 16.0 16.0 3.2 34.0 29.8 16.0 6.4 17.0 70.5 62.3 59.0 50.8 45.0 65.6 52.5 59.0 39.3 63.9 68.9 50.8 52.5 45.9 59.0 45.9 36.1 31.2 24.6 8.2 50.8 52.5 32.8 31.2 32.8 68.1 65.3 68.1 62.5 68.1 62.5 63.9 48.6 55.6 44.4 59.4 40.3 32.4 62.5 54.2 48.6 40.3 29.2 30.6 15.3 62.5 65.3 47.2 29.2 45.8 41.7 36.1 26.4 16.7 50.0 43.1 32.0 43.1 20.8 52.8 54.2 36.1 22.2 20.8 12.5 38.9 30.6 22.2 19.4 23.6 68.3 53.7 62.2 56.1 63.4 64.6 61.0 59.8 57.3 50.0 52.4 42.7 63.0 56.1 52.4 47.6 41.5 28.1 24.4 9.8 63.4 53.7 58.5 37.8 51.2 56.1 34.2 11.0 15.9 61.0 56.1 35.4 50.0 34.1 56.1 46.3 31.7 19.5 17.1 6.1 29.3 31.7 18.3 1.2 12. 71.4 66.7 70.2 67.9 69.1 71.4 67.9 60.7 54.8 66.7 63.1 44.1 42.9 47.6 54.8 44.1 33.3 23.8 19.1 7.1 72.6 69.1 70.2 63.1 63.1 63.1 52.4 46.4 39.3 57.1 56.0 51.2 52.4 41.7 56.0 50.0 33.3 34.5 23.8 8.3 39.3 46.4 33.3 14.3 22.6 63.0 64.4 64.4 64.4 63.0 58.9 52.1 54.8 57.5 53.4 61.6 56.2 53.4 50.7 50.7 60.3 50.7 45.2 46.6 26.0 61.6 65.8 61.6 61.6 54.8 63.0 48.0 39.7 24.7 54.8 53.4 47.9 56.2 41.1 57.5 67.1 43.8 45.2 30.1 21.9 34.3 52.1 34.3 23.3 30.1 45.0 61.7 55.0 53.3 41.7 51.7 45.0 43.3 41.7 41.7 50.0 45.0 38.3 46.7 55.0 45.0 33.3 25.0 16.7 10.0 43.3 55.0 55.0 43.3 40.0 46.7 40.0 33.3 16.7 46.7 48.3 36.7 33.3 41.7 46.7 43.3 38.3 41.7 23.3 13.3 33.3 43.3 25.0 8.3 23. 77.4 76.4 51.9 46.2 72.6 53.8 65.1 51.9 52.8 46.2 49.5 41.5 45.3 65.1 50.9 35.9 41.5 8.5 5.7 1.9 73.6 71.7 53.8 34.0 34.0 63.2 40.6 23.6 13.2 61.3 42.5 34.9 50.0 36.8 57.6 53.8 25.5 22.6 18.9 2.8 38.7 30.2 17.9 5.7 15.1 64.0 59.6 60.7 50.6 59.6 60.7 64.0 55.1 60.7 53.9 57.8 48.3 49.4 60.7 59.6 52.8 41.6 34.8 23.6 13.5 56.2 50.6 47.2 34.8 59.6 55.1 40.5 14.6 10.1 44.9 50.6 28.1 46.1 25.8 43.8 51.7 22.5 29.2 15.7 9.0 24.7 34.8 21.4 6.7 15.7 70.5 62.5 69.3 70.5 73.9 63.6 63.6 60.2 58.0 50.0 58.0 47.7 50.0 62.5 60.2 44.3 43.2 37.5 28.4 18.2 58.0 56.8 58.0 42.1 58.0 53.4 47.7 27.3 23.9 59.1 52.3 39.8 47.7 33.0 53.4 47.7 35.2 28.4 25.0 12.5 42.1 40.9 25.0 12.5 29. 67.5 57.5 72.5 68.8 65.0 37.5 61.3 46.3 43.8 52.5 50.7 33.8 31.2 57.5 48.8 43.8 42.5 18.8 7.5 0.0 62.5 56.3 63.8 47.5 58.8 58.8 30.0 6.3 8.8 55.0 42.5 13.7 48.8 18.7 38.8 40.0 25.0 25.0 8.8 0.0 28.8 28.8 5.0 0.0 7.5 60.8 56.7 57.7 58.8 60.8 53.6 60.8 40.2 41.2 44.3 42.9 29.9 42.7 44.3 48.5 37.1 22.7 18.6 16.5 1.0 52.6 49.5 48.5 37.1 46.4 49.5 34.0 26.8 11.3 44.3 39.2 32.0 29.9 28.9 40.2 37.1 23.7 13.4 11.3 2.1 32.0 22.7 16.5 3.1 16.5 54.7 52.6 74.7 63.2 54.7 57.9 66.3 47.4 51.6 42.1 52.1 40.0 45.3 51.6 42.1 46.3 33.7 20.0 12.6 8.4 56.8 49.5 55.8 48.4 51.6 51.6 39.0 13.7 4.2 48.4 43.2 24.2 40.0 21.1 53.7 48.4 25.3 22.1 11.6 4.2 24.2 35.8 10.5 3.2 14. 75.8 66.1 77.4 72.6 66.1 61.3 61.3 62.9 66.1 56.5 61.3 67.7 61.3 66.1 64.5 64.5 61.3 56.5 54.8 27.4 71.0 71.0 79.0 56.5 61.3 66.1 46.8 33.9 32.3 66.1 64.5 51.6 71.0 46.8 67.7 66.1 51.6 45.2 43.6 22.6 46.8 48.4 33.9 21.0 30.7 Table 5: Pass@1 (%) performance of different models for AutoCodeBench-Lite. Hunyuan: Hunyuan-TurboS and Hunyuan-Coder-7B-Preview (Tencent, 2025). Qwen: Qwen3-235B-A22B-Thinking-2507, Qwen3-235B-A22B-Instruct-2507, and Qwen3 Series (Yang et al., 2025); Qwen3-Coder-480B-A35B-Instruct (Qwen, 2025), Qwen2.5-Coder Series (Hui et al., 2024a) and Qwen2.5-72B (Qwen et al., 2025) Seed: Seed1.6-Thinking (Seed, 2025), Seed1.6 (Seed, 2025) and Seed-Coder-8B (Seed et al., 2025). GLM: GLM-4.5 and GLM-4.5-Air (Zhipu, 2025). Other Models: ERNIE-X1-Turbo-32K (Baidu, 2025), Kimi-K2 (Kimi-Team, 2025), and OpenCoder-8B (Huang et al., 2025). Evaluation Details We use the Pass@1 (Chen et al., 2021) as the default evaluation metric. In terms of inference parameters, for proprietary LLMs, Qwen3-235B-A22B-Thinking-2507, Qwen3-235B-A22B-Instruct-2507, Kimi-K2, Qwen3-Coder-480B-A35B-Instruct, and Hunyuan-TurboS, we directly call their APIs without any additional parameters. For DeepSeek-V3-0324, DeepSeek-R1-0528, Seed-Coder-8B, the Qwen3 series, we use their officially recommended parameters. The remaining models use greedy decoding with temperature set to 0. All non-APIs models are deployed using the VLLM (Kwon et al., 2023) framework. All models are provided with our custom system prompt, which standardizes the output format: You are an expert programmer. Your task is to provide code solution within single Markdown code block for the given programming problem. Do not include any direct execution commands, test cases, or usage examples within the code block. 3.2 Main Results We comprehensively evaluate the performance on AutoCodeBench (ACB) and AutoCodeBench-Lite (ACBLite), with results across different programming languages shown in Tables 4 and 5. The corresponding leaderboards are shown in Figures 7 and 11. ACB Exhibits High Difficulty. None of the evaluated models achieve an average score above 53, highlighting the high difficulty of the tasks in ACB. This underscores that the benchmark is specifically designed to challenge current models, pushing the limits of their code generation capabilities. The results further indicate that these models still struggle to solve complex, practical multilingual problems effectively. ACB-Lite amplifies the performance differences between models. Since ACB-Lite is derived by filtering problems based on the number of passes from all models, problems solved by the majority of models are Figure 3: The performance comparison of different models across two language sets. excluded. Therefore, ACB-Lite amplifies the performance differences between models compared to ACB, making it more effective for comparing model performance. Claude Opus 4 Shows State-of-the-art Performance. Regardless of whether the mode are reasoning or non-reasoning, Claude Opus 4 consistently ranks first in ACB(-Lite). This highlights the superior performance of Claude models in addressing diverse and complex tasks that involve practical coding scenarios across multiple languages. This conclusion aligns with the results observed in SWE-bench (Jimenez et al., 2024) and Multi-SWE-bench (Zan et al., 2025). The Reasoning Mode Helps LLMs Solve Multilingual Challenges. Overall, reasoning-based models perform better than non-reasoning models across various programming languages. We believe that reasoning and thinking in the reasoning mode help to solve the complex, multi-logical problems in ACB. Upper Bound Reveals Models Complementary Strengths and Improvement Potential. Although all models exhibit moderate performance (below 53) on the ACB benchmark, their combined upper bound reaches 74.8, highlighting the potential for models to learn from each other. The state-of-the-art model only achieves 64.5 pass@1 on ACB-Lite further emphasizes this, as it suggests that no single model excels across all problems. Additionally, the fact that no model dominates across all languages underscores the varying multilingual capabilities of each model, revealing significant room for improvement. 3.3 Performance Across Popular and Low-Resource Programming Languages We select four popular languages (Python, C++, Java, C#) and four low-resource languages (Racket, Shell, Elixir, TS) based on the TIOBE Index 1to evaluate model performance across different language scenarios. As shown in Figure 3, the difference in average Pass@1 scores among the models for popular languages is small, with scores ranging from 50.4 to 53.8. This indicates that they have been adequately trained on these widely-used languages. However, when faced with low-resource languages, the performance gap between models from different families widens (ranging from 45.3 to 62.0). Claude Opus 4 outperforms other models significantly in both reasoning and non-reasoning modes. This result highlights the insufficient attention given to low-resource programming languages in the development of most models. Besides, since we use the moderately capable DeepSeek-Coder-V2-Lite to filter simple problems, the Pass@1 scores of top models on popular languages are relatively low. However, these models perform significantly better on low-resource languages. This indicates that the performance gap between models of different sizes is more pronounced on low-resource languages, likely because DeepSeek-Coder-V2-Lite struggles to filter out simple problems in these scenarios due to its limited capability in handling lowresource languages. 3.4 Performance Across Multi-Logic Programming Problems One key feature of AutoCodeBench, compared to previous benchmarks, is the inclusion of multi-logical problems. These problems require models to implement multiple distinct functions or classes within single task, challenging their ability to handle multiple core demands simultaneously. 1https://tiobe.org.cn/tiobe-index/ Figure 4: Performance drop of models on multi-logic problems (1,622) compared to full dataset. Figure 5: Scaling laws for different models. We use DeepSeek-V3-0324 to identify all multi-logical problems in AutoCodeBench and evaluate model performance on them. The results, shown in Figure 4, reveal significant performance drop for all models when faced with multi-logical tasks. Claude Opus 4 shows relatively smaller decline, while the DeepSeek series, Gemini2.5 Pro, and GPT 4.1 exhibit larger drops. This indicates that current models need to enhance their ability to process multi-logical problems, which is particularly crucial in real-world code agent scenarios. 3.5 Performance Analysis of Scaling Laws Figure 5 compares parameter scaling and test-time sampling scaling across different models. The parameter scaling law (left) shows significant variation between models, with Qwen3 (Think) Series demonstrating the steepest scaling curve, indicating that chain-of-thought reasoning particularly benefits larger models. The test-time sampling scaling law (right) reveals more uniform behavior, with three models showing similar improvement rates from increased sampling during inference. These results suggest that while test-time sampling provides consistent benefits regardless of model size, reasoning capabilities scale more aggressively with model size. 3.6 Performance Analysis of Multi-Turn Refinement with Sandbox Feedback As shown in Figure 6, we evaluate how models leverage execution error messages to iteratively refine their code solutions. The results highlight the substantial value of our multilingual sandbox error feedback across all evaluated models. DeepSeek-V3-0324 achieves remarkable improvement from 48.1% to 59.7% after three refinement turns. Qwen2.5-Coder-32B-Instruct demonstrates even greater relative 9 Figure 6: Performance improvement across multi-turn refinement with sandbox feedback. Count Average Python Cpp Java 50 50 JS 50 Go 50 Shell Csharp Dart Elixir 50 50 50 Julia Kotlin Perl PHP Racket 50 50 50 50 50 Ruby Rust Scala Swift 50 50 50 TS 50 DeepSeek-Coder-V2-Base Qwen2.5-Coder-32B Qwen2.5-72B Seed-Coder-8B-Base OpenCoder-8B-Base Qwen2.5-Coder-7B DeepSeek-Coder-6.7B-Base Qwen3-8B-Base 39.0 35.5 35.9 31.6 26.1 24.6 22.9 22. 24.0 36.0 32.0 26.0 22.0 20.0 20.0 20.0 32.0 34.0 22.0 22.0 6.0 10.0 14.0 14.0 40.0 32.0 38.0 40.0 28.0 22.0 26.0 18. 44.0 32.0 40.0 30.0 34.0 28.0 34.0 34.0 34.0 38.0 22.0 32.0 30.0 24.0 18.0 20.0 26.0 34.0 34.0 12.0 24.0 14.0 18.0 12. 30B+ Models 38.0 30.0 22.0 8B Models 24.0 10.0 8.0 8.0 6.0 64.0 58.0 62.0 54.0 52.0 46.0 50.0 50.0 52.0 42.0 42.0 48.0 42.0 46.0 44.0 34.0 46.0 38.0 38. 30.0 32.0 32.0 20.0 26.0 56.0 52.0 46.0 48.0 28.0 42.0 38.0 24.0 38.0 40.0 42.0 28.0 26.0 30.0 28.0 32.0 36.0 32.0 46. 36.0 24.0 30.0 18.0 30.0 32.0 30.0 28.0 22.0 20.0 14.0 12.0 8.0 26.0 26.0 26.0 26.0 20.0 20.0 14.0 20.0 40.0 30.0 38. 32.0 28.0 18.0 34.0 30.0 26.0 18.0 28.0 18.0 14.0 16.0 6.0 8.0 36.0 30.0 28.0 20.0 26.0 24.0 10.0 14.0 42.0 34.0 30. 36.0 14.0 14.0 4.0 16.0 48.0 44.0 54.0 48.0 42.0 34.0 42.0 36.0 Table 6: Pass@1 (%) performance of different base models for 3-shot AutoCodeBench-Complete. gains, advancing from 35.8% to 47.4%, while Qwen3-8B shows consistent progress from 23.3% to 30.2%. The most significant performance gains occur during the first refinement turn, with diminishing returns in subsequent iterations. This pattern suggests that models can effectively leverage execution feedback to identify and correct common coding errors, though the complexity of remaining problems increases with each iteration. The consistent improvement across different model scales indicates that multi-turn refinement with sandbox feedback is valid strategy for enhancing code generation quality. 3.7 AutoCodeBench-Complete: Evaluating Base Model Capabilities Existing benchmarks for evaluating base models on code generation tasks, such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and their multilingual counterpart MultiPL-E (Cassano et al., 2022), primarily emphasize simple algorithmic problems. They fail to comprehensively measure the programming capabilities of base models in diverse, real-world scenarios. Building on the diversity and comprehensiveness of AutoCodeBench, we present AutoCodeBench-Complete, completion-based evaluation benchmark tailored to assess the code generation capabilities of base models. To build this benchmark, we select 1,000 datas from ACB-Lite, ensuring balanced distribution of 50 problems per programming language. We use 3-shot demonstrations to evaluate the performance of base models. Table 6 presents the performance of various base models on ACB-Complete. Among models with 8B parameters or fewer, Seed-Coder-8B-Base demonstrates superior performance, consistent with its strong showing on ACB(-Lite). This consistency suggests that the pretraining process effectively equipped Seed-Coder models with strong multilingual programming capabilities, enabling them to handle diverse coding scenarios across multiple languages. Additionally, an interesting observation arises when comparing Qwen2.5-Coder-7B and OpenCoder-8B. While Qwen2.5-Coder-7B-Instruct outperforms OpenCoder-8B-Instruct on ACB(-Lite), the trend reverses on ACB-Complete. We believe that Qwen2.5-Coder7B-Instruct incorporate more comprehensive multilingual code dataset during its post-training phase, thereby enhancing its code understanding and generation abilities. We hope that AutoCodeBench-Complete will serve as comprehensive benchmark for evaluating the code generation capabilities of base models. We believe it will provide researchers and practitioners with testbed that more closely mirrors the complexity and diversity of real-world programming tasks. 10 Figure 7: AutoCodeBench leaderboard showing Pass@1 performance of various LLMs."
        },
        {
            "title": "4 Further Discussion",
            "content": "4.1 Manual Verification In our automated workflow, we employ carefully designed problem specifications and an LLM-asCritic mechanism to enforce automated quality control. However, since LLMs cannot guarantee 100% accuracy, the overall quality of AutoCodeBench remains uncertain. To address this, we employ 6 professional annotators to quantitatively assess the quality of AutoCodeBench. Specifically, we develop visual annotation interface that displays each data instance, including the programming question, its corresponding test function, and the annotation contents from DeepSeek-R1-0528. Based on this interface, annotators are asked to assess the correctness of the test function and its alignment with the problem description. They then assign binary label (yes/no) indicating whether the instance is valid. We select six programming languages for this verification: Python, C++, Java, JavaScript, Go, and Shell. The results show that AutoCodeBench achieves 87.6% accuracy rate, demonstrating the reliability and feasibility of our automated benchmark construction process. In comparison, the model upper bounds and the performance of Claude Opus 4 (Reasoning) in these six languages are only 66.9(20.7) and 44.6(43.0), respectively, highlighting the significant potential for improvement. More detailed analysis is provided in Appendix B. 11 Initial Stage (Rank) After Simple Problem Filtering (Rank) After Critic Filtering (Rank) DeepSeek-V3-0324 DeepSeek-R1-0528 o3 Gemini2.5 Pro Qwen2.5-Coder-32B-Instruct 47.1 (3) 48.9 (2) 46.4 (4) 51.4 (1) 39.9 (5) 25.7 21.4 (4) 28.7 20.2 (2) 28.1 18.3 (3) 31.6 19.8 (1) 17.1 22.8 (5) 31.6 +5.9 (4) 36.2 +7.5 (2) 34.9 +6.8 (3) 38.7 +7.0 (1) 22.0 +4.9 (5) Table 7: The average pass@1 scores and rankings of models at different stages. 4.2 Hypotheses on Model Bias in the Generation Process It is well-known that models exhibit inherent biases, particularly their tendency to favor their own outputsa common phenomenon in automated data synthesis and evaluation tasks (Panickssery et al., 2024; Chen et al., 2025a). Our automated workflow is no exception to this issue. While completely eliminating such bias is challenging, we employ several mitigation strategies. Specifically, we intentionally only use DeepSeek series models in the workflow to prevent bias from affecting other model families. We hypothesize that using DeepSeek-V3-0324 for code generation and DeepSeek-R1-0528 for the Critic process may introduce favorable bias toward DeepSeek families. To counteract this, we employ DeepSeek-Coder-V2-Lite during the simple problem filtering phase, creating push-and-pull mechanism that balances potential biases across different stages. To quantitatively assess bias, we sampled 3,600 data points across six programming languages (Python, C++, Java, JS, Go, and Shell) and tracked performance changes at each generation stage, as shown in Table 7. The results reveal nuanced bias patterns: simple problem filtering negatively impacts smaller models (Qwen2.5-Coder-32B-Instruct) more than DeepSeek series, while the Critic process benefits DeepSeek-R1-0528 but surprisingly provides greater improvements to reasoning models (o3 and Gemini 2.5 Pro) than to DeepSeek-V3-0324. This suggests that model bias depends not only on model family but also on factors like model size and reasoning modes. Furthermore, as mutual distillation between models from different families continues, this bias becomes increasingly difficult to measure. In conclusion, we believe that our automated process may introduce favorable bias toward the DeepSeek family of models, but the impact is minimal."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Code Generation Benchmarks The rapid evolution of Code Large Language Models (LLMs)spanning open-source models like CodeLLama (Roziere et al., 2023), DeepSeek-Coder (Zhu et al., 2024), and Qwen-Coder (Hui et al., 2024b), as well as proprietary systems like Claude (Anthropic, 2025) series, GPT (OpenAI, 2024; 2025) series, and the Gemini (Gemini, 2025) familyhas fundamentally reshaped software development. This progress necessitates robust and contemporary benchmarks to accurately assess their capabilities in tasks such as code generation and debugging. Pioneering benchmarks like HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) established foundation by evaluating functional correctness on short, algorithm-centric Python problems. However, they are hampered by limitations such as potential data contamination, narrow programming language coverage, and disconnect from real-world applications. To overcome these shortcomings, subsequent benchmarks (Jimenez et al., 2024; Jain et al., 2025; Wang et al., 2025b; Guo et al., 2025; Zhang et al., 2025a;b) have targeted more complex programming tasks. notable line of work focuses on algorithmic challenges from programming contests (Hendrycks et al., 2021; Li et al., 2022; Jain et al., 2025; Wang et al., 2025b; Zheng et al., 2025). LiveCodeBench (Jain et al., 2025) mitigates data contamination by continuously sourcing new problems from competitive programming platforms. OJBench (Wang et al., 2025b) presents rigorous, competition-level benchmark with 232 problems from prestigious contests like NOI and ICPC, demanding advanced code reasoning. APPS (Hendrycks et al., 2021) offers large-scale dataset of 10,000 problems categorized by difficulty. Another stream of research evaluates more comprehensive and multilingual capabilities (Cassano et al., 2022; Peng et al., 2024; Zhang et al., 2024; Chai et al., 2025; Bytedance, 2025). McEval (Chai et al., 2025) is massively multilingual benchmark covering 40 languages for generation, explanation, and completion tasks. FullStackBench (Bytedance, 2025) assesses LLMs in realistic, multi-domain scenarios across 16 languages, employing novel execution environment. Besides, some works propose more sophisticated evaluation frameworks. For example, ArtifactsBench (Zhang et al., 2025b) introduces automated multimodal evaluation for visual code generation, while CodeCriticBench (Zhang et al., 2025a) focuses on holistic code critique evaluation. common thread across these benchmarks is their reliance on labor-intensive manual curation for collecting problems, authoring ground-truth solutions, and designing test cases. 12 In contrast, our proposed work introduces fully automated, dynamically constructed evaluation system. Unlike traditional benchmarks, it eliminates manual intervention, enabling more scalable, consistent, and continuously evolving evaluation. Furthermore, it is designed to maintain balanced difficulty distribution and support multiple languages, addressing the increasing demand for evaluating LLMs in diverse programming languages and contexts. 5.2 Code Data Synthesis To reduce dependence on manually curated data, growing body of research explores automatic data synthesis to augment the training of Code LLMs (Luo et al., 2024; Wei et al., 2024b; Zheng et al., 2024; Wu et al., 2024b; Yu et al., 2024; Ahmad et al., 2025; Xu et al., 2025). For instance, Evol-Instruct (Luo et al., 2024) uses heuristic prompts to guide LLMs in evolving existing programming problems, thereby increasing their diversity and difficulty. OSS-Instruct (Wei et al., 2024b) prompts LLMs to generate new coding problems and solutions from raw, open-source code snippets. KodCode (Xu et al., 2025) synthesizes broad spectrum of Python coding tasksincluding questions, solutions, and test casesand ensures correctness through systematic self-verification procedure. Some other methods focus on model selfimprovement (Wu et al., 2024a; Wei et al., 2024a; Chen et al., 2025b; Zhou et al., 2025). Inverse-Instruct (Wu et al., 2024a) is self-improvement technique that generates new instructions by back-translating code from an LLMs own training set, reducing the need to distill from more powerful proprietary models. Similarly, SelfCodeAlign (Wei et al., 2024a) introduces pipeline for self-aligning code LLMs without extensive human annotation, using the same base model for both data generation and validation. Collectively, these data synthesis methods significantly reduce the reliance on manual curation and enable the continuous expansion of the problem space for training. Our work extends this paradigm of automation from data augmentation to the benchmark creation process. By leveraging extensive LLM-sandbox interaction, our pipeline not only automates the synthesis of verifiable test problems but can also be naturally repurposed for synthesizing high-quality training datasets."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce AutoCodeGen, an automated workflow based on LLM-Sandbox interaction, designed to generate multilingual verifiable code data without any manual annotation. Through this novel approach, we have successfully built AutoCodeBench, large-scale, human-free code generation benchmark. AutoCodeBench contains 3,920 problems, evenly distributed across 20 programming languages, and is characterized by its high difficulty, practicality, and diversity. We also provide AutoCodeBench-Lite (a simplified version) and AutoCodeBench-Comp (a completion-based benchmark specifically designed for base models). Furthermore, we open-sourced multilingual sandbox that supports over 20+ programming languages to enable high-concurrency evaluation and training. Our evaluation of more than 30 mainstream open-source and proprietary LLMs reveals that even the most advanced models still face challenges when confronted with the complex and diverse multilingual tasks set by AutoCodeBench, especially when handling multi-logic scenarios. We hope that the AutoCodeBench series will become valuable resource, inspiring the community to focus on more challenging and practical multilingual code generation scenarios. In addition, our comprehensive analysis of AutoCodeGen and AutoCodeBench provides valuable insights for the future development of code generation benchmarks."
        },
        {
            "title": "7 Acknowledgements",
            "content": "In addition to all the authors of this paper, we would like to thank the following individuals from Tencent for their contributions to the multilingual sandbox and manual annotation work: Hebin Li, Jinxu Hu, Bin Li, Zhihua Xu, Yunqing Sun, Xian Wu, Xiaohan Lu, Bingxian Liu, Bo Li, Bo Song, Cheng Zhang, Wenqi Xie, Zhirong Zheng."
        },
        {
            "title": "References",
            "content": "Wasi Uddin Ahmad, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Vahid Noroozi, Somshubra Majumdar, and Boris Ginsburg. Opencodeinstruct: large-scale instruction tuning dataset for code llms, 2025. URL https://arxiv.org/abs/2504.04030. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martn Blazquez, Guilherme Penedo, Lewis Tunstall, Andres Marafioti, Hynek Kydlcek, Agustn Piqueres Lajarn, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https: //arxiv.org/abs/2502.02737. Anthropic. Introducing claude 4, 2025. URL https://www.anthropic.com/news/claude-4. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021. URL https://arxiv.org/abs/2108.07732. Baidu. Ernie-x1-turbo-32k, 2025. URL https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Wm9cvy6rl. Bytedance. Fullstack bench: Evaluating llms as full stack coders, 2025. URL https://arxiv.org/abs/ 12.00535. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. Multipl-e: scalable and extensible approach to benchmarking neural code generation, 2022. URL https://arxiv.org/abs/2208.08227. Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, JinKe, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, Noah Wang, Boyang Wang, Xianjie Wu, Bing Wang, Tongliang Li, Liqun Yang, Sufeng Duan, Zhaoxiang Zhang, and Zhoujun Li. Mceval: Massively multilingual code evaluation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.n et/forum?id=UunCPtPOlZ. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/ abs/2107.03374. Wei-Lin Chen, Zhepei Wei, Xinyu Zhu, Shi Feng, and Yu Meng. Do llm evaluators prefer themselves for reason?, 2025a. URL https://arxiv.org/abs/2504.03846. Xiancai Chen, Zhengwei Tao, Kechi Zhang, Changzhi Zhou, Xinyu Zhang, Wanli Gu, Yuanpeng He, Mengdi Zhang, Xunliang Cai, Haiyan Zhao, and Zhi Jin. Revisit self-debugging with selfgenerated tests for code generation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1800318023, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-251-0. URL https://aclanthology.org /2025.acl-long.881/. Cursor. The ai code editor, 2025. URL https://cursor.com/en. DeepSeek-AI. Deepseek-v3 technical report, 2025a. URL https://arxiv.org/abs/2412.19437. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025b. URL https://arxiv.org/abs/2501.12948. DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli 14 Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, and Wenfeng Liang. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence, 2024. URL https://arxiv.org/abs/2406.11931. Gemini. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming the rise of code intelligence, 2024. URL https://arxiv.org/abs/2401.14196. Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi LI, Ruibo Liu, Yue Wang, Shuyue Guo, Xingwei Qu, Xiang Yue, Ge Zhang, Wenhu Chen, and Jie Fu. Codeeditorbench: Evaluating code editing capability of large language models, 2025. URL https: //arxiv.org/abs/2404.03543. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. Siming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, Jiaheng Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models, 2025. URL https://arxiv.org/abs/2411.04905. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024a. URL https://arxiv.org/abs/2409.12186. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024b. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation, 2024. URL https://arxiv.org/abs/2406.00515. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770. Kimi-Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. CodeIO: Condensing reasoning In Forty-second International Conference on Machine patterns via code input-output prediction. Learning, 2025. URL https://openreview.net/forum?id=feIaF6vYFl. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):10921097, December 2022. ISSN 1095-9203. doi: 10.1126/science.abq1158. URL http://dx.doi.org/10.1126/science.abq1158. 15 Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu noz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evolinstruct. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=UnUwSIgK5W. OpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Introducing gpt-4.1 in the api, 2025. URL https://openai.com/index/gpt-4-1/. OpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/introducing-o3-a nd-o4-mini/. Arjun Panickssery, Samuel R. Bowman, and Shi Feng. LLM evaluators recognize and favor their own In The Thirty-eighth Annual Conference on Neural Information Processing Systems, generations. 2024. URL https://openreview.net/forum?id=4NJBV6Wp0h. Qiwei Peng, Yekun Chai, and Xuhong Li. Humaneval-xl: multilingual code generation benchmark for cross-lingual natural language generalization, 2024. URL https://arxiv.org/abs/2402.16694. Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, and Junyang Lin. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings, 2025. URL https://arxiv.org/abs/2501.01257. Qwen. Qwen3-coder: Agentic coding in the world, 2025. URL https://qwenlm.github.io/blog/qwen 3-coder/. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Seed. Introduction to techniques used in seed1.6, 2025. URL https://seed.bytedance.com/en/seed1 6. ByteDance Seed, Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, Jinhua Zhu, Shulin Xin, Dong Huang, Yetao Bai, Lixin Dong, Chao Li, Jianchong Chen, Hanzhi Zhou, Yifan Huang, Guanghan Ning, Xierui Song, Jiaze Chen, Siyao Liu, Kai Shen, Liang Xiang, and Yonghui Wu. Seed-coder: Let the code model curate data for itself, 2025. URL https://arxiv.org/abs/2506.03524. Tencent. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought, 2025. URL https://arxiv.org/abs/2505.15431. Kaixin Wang, Tianlin Li, Xiaoyu Zhang, Chong Wang, Weisong Sun, Yang Liu, and Bin Shi. Software development life cycle perspective: survey of benchmarks for code large language models and agents, 2025a. URL https://arxiv.org/abs/2505.05283. Zhexu Wang, Yiping Liu, Yejie Wang, Wenyang He, Bofei Gao, Muxi Diao, Yanxu Chen, Kelin Fu, Flood Sung, Zhilin Yang, et al. Ojbench: competition level code benchmark for large language models. arXiv preprint arXiv:2506.16395, 2025b. 16 Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, and Lingming Zhang. Selfcodealign: Self-alignment for code generation. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 6278762874. Curran Associates, Inc., 2024a. URL https://proceedings.neurips.cc/paper files/paper/2024/file/72da1 02da91a8042a0b2aa968429a9f9-Paper-Conference.pdf. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with OSS-instruct. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 5263252657. PMLR, 2127 Jul 2024b. URL https://proceedings.mlr.press/v235/wei24h.html. Yutong Wu, Di Huang, Wenxuan Shi, Wei Wang, Lingzhe Gao, Shihao Liu, Ziyuan Nan, Kaizhao Yuan, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Yewen Pu, Dawei Yin, Xing Hu, and Yunji Chen. Inversecoder: Self-improving instruction-tuned code llms with inverse-instruct, 2024a. URL https://arxiv.org/abs/2407.05700. Yutong Wu, Di Huang, Wenxuan Shi, Wei Wang, Lingzhe Gao, Shihao Liu, Ziyuan Nan, Kaizhao Yuan, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Yewen Pu, Dawei Yin, Xing Hu, and Yunji Chen. Inversecoder: Self-improving instruction-tuned code llms with inverse-instruct, 2024b. URL https://arxiv.org/abs/2407.05700. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding, 2025. URL https://arxiv.org/abs/2503.029 51. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. Wavecoder: Widespread and versatile enhancement for code large language models by instruction tuning, 2024. URL https://arxiv.org/abs/2312.14187. Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. Multi-swe-bench: multilingual benchmark for issue resolving, 2025. URL https://arxiv.org/abs/2504.02605. Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, Yuanxing Zhang, Zhexu Wang, Weixun Wang, Yancheng He, Ken Deng, Wangchunshu Zhou, Wenhao Huang, and Zhaoxiang Zhang. Codecriticbench: holistic code critique benchmark for large language models, 2025a. URL https://arxiv.org/abs/2502.16614. Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, and Fengzong Lian. Artifactsbench: Bridging the visual-interactive gap in llm code generation evaluation, 2025b. URL https://arxiv.org/abs/2507.04952. Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Naturalcodebench: Examining coding performance mismatch on humaneval and natural user prompts, 2024. URL https://arxiv.org/abs/2405.04520. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. OpenCodeInterpreter: Integrating code generation with execution and refinement. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1283412859, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.762. URL https://aclanthology.org/2024.findings-acl.7 62/. Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Aleksandra Korolova, Peter Henderson, Sanjeev Arora, Pramod Viswanath, Jingbo Shang, and Saining Xie. Livecodebench pro: How do olympiad medalists judge llms in competitive programming?, 2025. URL https://arxiv.org/ abs/2506.11928. Zhipu. Glm-4.5: Reasoning, coding, and agentic abililties, 2025. URL https://z.ai/blog/glm-4.5. 17 Changzhi Zhou, Xinyu Zhang, Dandan Song, Xiancai Chen, Wanli Gu, Huipeng Ma, Yuhang Tian, Mengdi Zhang, and Linmei Hu. Refinecoder: Iterative improving of large language models via adaptive critique refinement for code generation, 2025. URL https://arxiv.org/abs/2502.09183. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. Yaoming Zhu, Junxin Wang, Yiyang Li, Lin Qiu, ZongYu Wang, Jun Xu, Xuezhi Cao, Yuhuai Wei, Mingshi Wang, Xunliang Cai, and Rong Ma. Oibench: Benchmarking strong reasoning models with olympiad in informatics, 2025. URL https://arxiv.org/abs/2506.10481. Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen GONG, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=YrycTjllL0. 18 (a) AutoCodeBench (b) AutoCodeBench-Lite (c) FullStackBench (d) McEval Figure 8: Category Distribution of Different Benchmarks. (a) AutoCodeBench (b) AutoCodeBench-Lite (c) FullStackBench (d) McEval Figure 9: Language Distribution of Different Benchmarks."
        },
        {
            "title": "A Data Category and Language Distribution Statistics",
            "content": "We prompt Claude Sonnet 4 to generate 20 language-agnostic category labels for classification: Core Programming Concepts: Language Fundamentals, Functions & Modules, Object-Oriented Programming, Functional Programming, Memory Management & Performance, Error Handling & Debugging Data and Algorithms: Data Structures & Collections, Algorithms & Problem Solving, String & Text Processing, File & I/O Operations, Concurrency & Async Programming Application Domains: Network Programming & Communication, Database Operations & Persistence, Web Development & Frameworks, Mobile & Cross-platform Development, Systems Programming & Low-level Development Advanced Topics and Tooling: Data Science & Analytics, Machine Learning & AI, Testing & Quality Assurance, Development Tools & Ecosystem In addition to AutoCodeBench, we conduct task tagging and language distribution analysis for AutoCodeBenchLite, FullStackBench, and McEval. The results are presented in Figures 8 and 9. FullStackBench demonstrates comparable category diversity to AutoCodeBench(-Lite) but suffers from an imbalanced language distribution. In contrast, McEval exhibits well-balanced multilingual distribution but lacks diversity and balance in its category coverage. Our AutoCodeBench(-Lite) achieves the most comprehensive category coverage while maintaining balanced multilingual distribution, enabling thorough and accurate evaluation of LLMs multilingual code generation capabilities."
        },
        {
            "title": "B Manual Verification",
            "content": "Due to the involvement of multiple languages and domains in the data, directly verifying the quality of the data through manual annotation presents significant challenges. To address this issue, we employ Human-LLM collaboration approach for data quality validation. Specifically, we design prompts in the native languages of the annotators and use the DeepSeek-R1-0528 to generate detailed reasoning processes and checklist-based annotation results. The prompt is shown in Figure 10. During the annotation process, we assume that the programming problems are completely correct. The primary task of the annotators is to assess the correctness of the test functions and their alignment with the programming problem, 19 Problem Accuracy 87.6 83.5 Average Python C++ 88.0 Java 86.0 JS 89.0 Go 86.0 Shell 93.3 Current Upper Bound Claude 4 Opus (Reasoning) 66.920.7 44.643.0 61.721.8 40.343.2 71.516.5 44.143.9 76.19.9 55.930.1 58.230.8 38.650. 65.420.6 37.248.8 68.624.7 51.641.7 Table 8: Comparison of Accuracy, Upper Bound, and Model Performance. based on the LLMs output. We allow for test cases that may not cover all boundary conditions, focusing primarily on the correctness of the test functions rather than their comprehensiveness. The annotators pay particular attention to the following aspects: Whether the function names, class names, variable definitions, and return types are consistent with the problem description; Whether the test cases exhibit randomness or non-reproducibility; Whether the test cases contradict the logic presented in the problem statement; Whether there are any precision issues with the test cases; Whether the test functions include test cases that are not addressed in the problem description. To facilitate annotation, we design front-end interface, including question, test function, critic reasoning process and results from LLM. Based on the information provided in the interface, the annotators generate binary classification output. Using the annotation results, we calculate the problem accuracy rates for different programming languages (Python, C++, Java, JavaScript, Go, Shell), as shown in Table 8. The results indicate that, despite the presence of some noisy data, our benchmark model still demonstrates high accuracy (87.6%). Furthermore, even after removing the noise, the current SOTA model shows significant room for improvement (43.0), further validating the high difficulty level of our benchmark. Besides, we find that, compared to logic errors in the problem description and errors in the test functions, the most frequently occurring issue is incomplete problem descriptions. For example, some test functions reference class or function names that are essential but not explicitly mentioned in the problem description, or they require natural language outputs for edge cases that are not explicitly specified in the problem statement, leading to mismatches between the generated code and the test functions. Interestingly, we observe similar issues in manually annotated BigCodeBench (Zhuo et al., 2025), highlighting the significant challenge of creating comprehensive and accurate programming problems for annotators."
        },
        {
            "title": "C Multilingual Code Sandbox Service",
            "content": "This service offers secure and high-performance environment for the compilation and execution of code in over 30 programming languages. It supports large-scale code data validation, making it suitable for high-volume, automated testing scenarios. Our multilingual sandbox has the following features: Multilingual Support: The service supports more than 30 programming languages, including popular ones like Python, JavaScript, Go, Java, C++, and Rust, providing versatility for various use cases. Security Isolation: Code execution is isolated within Docker containers, ensuring that each execution environment is separate. Additionally, iptables firewall rules are applied to maintain high level of security, preventing unauthorized access or interference. Smart Code Integration: The system automatically manages the integration of function code with testing code. It adapts to language-specific syntax, ensuring seamless execution without requiring manual intervention for code merging. High Performance: Powered by Gunicorn multi-process architecture, the sandbox supports concurrent execution of multiple code instances, making it capable of handling high volume of requests efficiently. RESTful API: The service provides clean and easy-to-use HTTP-based API, allowing developers to interact with the sandbox programmatically, whether for integrating into larger applications or automating tasks. Extensive Language Support: Beyond the mainstream languages, the sandbox also supports emerging and niche languages, allowing it to cater to wide variety of development environments and user needs. 20 Figure 10: The English prompt of annotation and critic. Custom Execution Environments: Users can configure specific environments for their tasks, enabling tailored execution conditions based on their unique requirements."
        },
        {
            "title": "D Prompts for Automated Workflow",
            "content": "The prompt of generating code solution is shown in Figure 12. The prompt of generating test function is shown in Figure 13. The prompt of generating programming problem is shown in Figure 14. The prompt of LLM-as-Critic is shown in Figure 10. The prompt of translating languages is shown in Figure 15. Figure 11: AutoCodeBench-Lite leaderboard showing Pass@1 performance of various LLMs. 22 Figure 12: The prompt of generating code solution. Due to the excessive length of the prompt, we have omitted the latter part. 23 Figure 13: The prompt of generating test function. Due to the excessive length of the prompt, we have omitted the latter part. Figure 14: The prompt of generating programming problem. Figure 15: The prompt of translating languages."
        }
    ],
    "affiliations": [
        "Hunyuan Team, Tencent"
    ]
}