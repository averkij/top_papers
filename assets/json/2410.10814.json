{
    "paper_title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free",
    "authors": [
        "Ziyue Li",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 2 4 1 8 0 1 . 0 1 4 2 : r YOUR MIXTURE-OF-EXPERTS LLM IS SECRETLY AN EMBEDDING MODEL FOR FREE Ziyue Li, Tianyi Zhou Department of Computer Science University of Maryland, College Park {litzy619,tianyi}@umd.edu Project: https://github.com/tianyi-lab/MoE-Embedding"
        },
        {
            "title": "ABSTRACT",
            "content": "While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on diverse class of embeddingfocused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MOEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MOEE to LLM-based embedding without further finetuning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1: Comparison of hidden state (HS) and MOEE (ours) on six types of tasks from the Massive Text Embedding Benchmark (MTEB), where MOEE consistently outperforms HS on all tasks. Both HS and MOEE are extracted from DeepSeekMoE-16B (Dai et al., 2024) without further finetuning. Mixture-of-Experts (MoE) (Jacobs et al., 1991; Jordan & Jacobs, 1994), as versatile architecture originally developed in the 1990s, can improve model generalization and reduce inference cost by distributing tasks to specialized experts (Shazeer et al., 2017). Over time, MoE is gaining prominence in fields such as natural language processing (Shen et al., 2023) and computer vision (Li et al., 2023; 1 Zong et al., 2024; Lin et al., 2024; Shi et al., 2024), especially attracting growing attention in the development of large language models (LLMs) (Muennighoff et al., 2024a; Dai et al., 2024; Jiang et al., 2024). key component of MoE is the dynamic routers, which intelligently assign each input to the most relevant expert. This allows MoE to tailor its computations to the unique characteristics of each input, optimizing both efficiency and accuracy. However, most recent LLMs and MoE LLMs are built upon the decoder-only architecture trained for autoregressive next-token prediction. While excelling on generative tasks, their final or intermediate hidden state (HS) is not designed to capture the key features of input tokens and cover all their information. Instead, HS can be biased towards the information of the next output token. Although it is common empirical practice to extract the last tokens hidden state (HS) as embedding (Wang et al., 2024), it may even perform much poorer than smaller encoder models specifically trained for embedding tasks (Lei et al., 2024; Muennighoff et al., 2024b). Take classification as an example, inputs with subtly different semantics may be associated with the same label, so the last HS aiming to predict the label may ignore the input difference. Although extra finetuning specifically for representation learning (Lee et al., 2024; Muennighoff et al., 2024b) can greatly strengthen LLMs capability as an embedding model, it raises the question of whether pre-trained LLMs can be claimed as generalists, given the broad application of embedding tasks. Can we extract high-quality embedding directly from LLMs without additional training? In this paper, we find Yes-answer to the question when studying MoE LLMs. Our main discovery is that the routers in MoE can serve as an off-the-shelf embedding model and the produced routing weights (RW) provide complementary information to the widely used HS as embedding. Compared to HS focusing on the final prediction results from the input, RW reflects the intermediate reasoning choices of MoE on the input for each layer of LLMs. Hence, as byproduct of the routing mechanism, RW completes the input information missing in HS. As evidence, our comparative analysis of RW and HS shows that they reveal different clustering structures and topics of inputs, while RW captures the inputs underlying themes and semantic structures. Moreover, we conducted an error analysis of the embedding task instances on which either HS or RW failed. As shown in Fig. 2, the proportion of cases where one embedding succeeds and the other fails exceeds 50%, indicating large room for improvement if combining RW and HS. Figure 2: Complementarity of DeepSeekMoE16Bs routing weights (RW) and hidden state (HS) as embedding in the task of similarity ranking on STS12 datasets. In the error analysis of instances where at least one embedding fails1, we report the proportion of (1) HS succeeds and RW fails ; (2) HS fails and RW succeeds, and (3) both RW and HS fail. In most cases, the proportion of (1)+(2) exceeds (3), indicating the complementarity of RW and HS. Motivated by the analysis, we propose the first attempt to combine RW and the widely-used HS of MoE LLMs, resulting in training-free, contextual-rich, and holistic embedding called MoE Embedding (MOEE) that excels in embedding tasks. Specifically, we experiment with various combination strategies and find that while simple concatenation of RW and HS (denoted by MOEE (concat)) improves either of them, weighted sum of the two similarities computed on RW and HS separately (denoted by MOEE (sum)) often achieves the best results. The weighted sum of similarities avoids the fusion and alignment between the two different types of embedding while allowing us to balance output-dependent information with input-sensitive features, optimizing performance across diverse tasks. We conduct extension evaluations of MOEE and compare it with baselines on the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022), which covers wide range of tasks designed to test embedding quality. MOEE consistently outperforms embedding derived solely from HS or MoEs 1Success/Failure is determined by how closely the ranking based on the embedding matches the ground truth, with deviations beyond threshold marked as failures. RW, as shown in Figure 1. Notably, MOEE (sum) achieves significant gains in tasks requiring an in-depth understanding of the input, such as semantic textual similarity, classification, and clustering. The rest of the paper is organized as follows: 2 reviews related work on existing embedding methods and MoE. 3 outlines our methodology for integrating RW of MoE with the widely-used HS embedding. 4 reports experimental results on MTEB, highlighting MOEEs advantages on performance and interpretability. Finally, 5 discusses the implications and future research directions. Results in the paper except 4 are conducted on DeepSeekMoE-16B (Dai et al., 2024) unless specified."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Training-Based Embedding (pre-LLM) Early work on sentence embedding, such as SkipThought (Kiros et al., 2015), leveraged the distributional hypothesis by predicting surrounding sentences from given input. These methods typically employed sequence-to-sequence architectures, following the success of Word2Vec (Mikolov, 2013). Recent advancements have shifted toward contrastive learning, which has gained prominence for its effectiveness in self-supervised representation learning. Contrastive methods, such as SimCSE (Gao et al., 2021), exploit different views of the same sentence through data augmentation or dropout, treating different outputs as positive pairs and negative pairs as unrelated sentences. This approach helps models better capture semantic similarities by maximizing the similarity between positive pairs while minimizing it between negative ones. Contrastive learning has been widely applied in sentence embedding due to its simplicity and competitive performance (Wu et al., 2020; Wang et al., 2021; Meng et al., 2021). Other methods like InfoNCE (Oord et al., 2018) and MoCo (He et al., 2020) have also contributed to the development of contrastive frameworks, further enhancing embedding quality. While effective, these approaches rely on static architectures that may overlook input variability. In contrast, MoE models dynamically route inputs through specialized experts, producing more nuanced, context-aware embedding without additional training. Training-Based Embedding with LLMs Recent advances in language modeling have demonstrated the potential of LLMs to generate high-quality sentence embedding (Muennighoff et al., 2024b; Meng et al., 2024). For instance, some methods, such as Sentence-T5 (Ni et al., 2021), employ contrastive learning and are capable of generating embedding that rivals fine-tuned models, even with billions of parameters. However, these methods often depend on complex pretraining and large-scale contrastive objectives, limiting their flexibility for new tasks without retraining. Training-Free Embedding with LLMs Training-free approaches seek to directly extract embedding from pre-trained LLMs without the need for additional finetuning. While this process is relatively straightforward for encoder-decoder models (Ni et al., 2021), it presents challenges for the more common decoder-only LLMs, where deriving meaningful embedding is less intuitive. Current approaches typically utilize the generated hidden state(s) of these models (Jiang et al., 2023). To improve the quality of these embedding, prompt-based techniques have gained traction (Jiang et al., 2022; Lei et al., 2024). One such method, Prompt with Explicit One Word Limitation (PromptEOL) (Jiang et al., 2023), distills sentence meaning into compact embedding by prompting the model with the instruction: This sentence: [text] means in one word: . In pre-trained decoder-only LLMs, embedding is typically derived from the hidden state of the final layer. Given an input sequence = [x1, x2, . . . , xT ], let H(l) RT represent the hidden state at the l-th layer, where is the sequence length, is the hidden state dimension, and = 1, 2, . . . , is the layer index. To extract single embedding eHS that represents the entire input sequence, one approach is to use the last tokens hidden state in the final layer, expressed as: eHS = H(L) Rd Another approach is to apply pooling over all tokens in the last layer. For example, mean pooling . These methods provide flexibility based on task averages the hidden states as: requirements, with the resulting embedding capturing the context of the input sequence as modeled by the LLM. i=1 H(L) (cid:80)T 1 Mixture-of-Experts (MoE) MoE models have been predominantly used in multitask learning and efficient large-scale training scenarios (Shazeer et al., 2017). However, their potential for generating 3 instance-level embedding has been underexplored. Our method leverages the routing decisions made by MoE models to generate embedding that are sensitive to the inputs structure and semantics. This results in more flexible and interpretable embedding compared to static models, without the overhead of task-specific retraining."
        },
        {
            "title": "3 MIXTURE-OF-EXPERTS EMBEDDING (MOEE)",
            "content": "Our approach leverages the dynamic routing mechanisms of pre-trained, decoder-only LLMs equipped with MoE modules to generate enriched, input-sensitive embedding. This section details the key steps of our methodology, including embedding extraction, expert routing across layers, and the final integration of embeddingall achieved using pre-trained models without any additional training."
        },
        {
            "title": "3.1 MOE ROUTING WEIGHTS (RW) AS EMBEDDING",
            "content": "Our approach capitalizes on the dynamic routing capabilities of MoE models embedded in pre-trained, decoder-only LLMs. These MoE modules operate across multiple layers, enabling the model to specialize in processing different aspects of the input at varying depths. Each MoE model at layer consists of (l) experts, denoted by E(l) , where = 1, 2, . . . , (l). Each expert is specialized sub-network that focuses on specific input characteristics at that layer, allowing for more granular understanding of the input as it passes through the network. However, the true strength of this architecture lies in the dynamic routing mechanism, governed by gating function g(l)(H(l)) RN (l) , which determines which experts will be activated at each layer based on the input. This gating function outputs probability distribution over the available experts in each layer, dynamically selecting the most relevant ones for the current input. The routing weights g(l) (H(l)) indicate the contribution of each expert to the final output of layer l, formulated as: (cid:80)N (l) (H(l))E(l) i=1 g(l) (H(l)), where (cid:80)N (l) (H(l)) = 1, ensuring weighted combination of experts. The gating function is typically implemented as softmax over set of logits z(l)(H(l)), making the routing decision both flexible and data-driven: i=1 g(l) g(l) (H(l)) = (H(l))) exp(z(l) j=1 exp(z(l) (H(l))) (cid:80)N (l) . By leveraging the routing weights from all layers, our approach captures richer representation of the input that accounts for both shallow and deep contextual features. This enables the model to provide nuanced information at every level of abstraction, which is critical for tasks requiring sensitivity to both low-level and high-level input details. By concatenating the dynamic routing weights from all layers, we form comprehensive routingbased embedding eRW: eRW = [g(1)(H(1)); g(2)(H(2)); . . . ; g(L)(H(L))] R(cid:80)L l=1 (l) . This embedding captures how the input is routed through different experts across all layers, offering holistic view of the models interaction with the input. Importantly, it reflects the full depth of the models decision-making process, making it powerful representation for downstream tasks where diverse semantic and structural features of the input are essential. 3.2 COMPARATIVE & COMPLEMENTARY ANALYSIS OF ROUTING WEIGHTS & HIDDEN STATE In this section, we investigate how routing weight (RW) embedding and hidden state (HS) embedding, generated from MoE models, capture different aspects of input data. Understanding the distinct roles these embedding play is crucial to determining how they complement each other. While HS embedding from pre-trained LLMs provides broad, context-driven representation of sentences, they may overlook the nuanced, token-specific information that RW embedding can capture through MoEs dynamic routing. This distinction suggests that RW and HS may excel in different contexts, potentially encoding complementary information. To explore this, we first analyze their clustering behavior using k-means 4 Table 1: Correlation of the clustering results achieved on the routing weight (RW) and hidden state (HS) embedding extracted from MoE LLMs. Low scores indicate the complementarity of RW and HS. Metric Score (max value) Adjusted Mutual Information (AMI) Normalized Mutual Information (NMI) Jaccard Similarity Exact Matching (%) 0.29 (1.00) 0.29 (1.00) 0.06 (1.00) 45.54% (100.00%) Figure 3: Word clouds of the top-20 topics from 3 clusters achieved on RW and HS separately, highlighting their captured distinct semantic features. Table 2: Prompts used in Fig 4-5. ID Prompt 1 This sentence: *sent* means in one 2 3 4 5 7 8 9 word: In one word, describe the style of the following sentence - *sent*: In one word, describe the sentiment of the following sentence (positive, neutral, or negative) - *sent*: In one word, describe the tone of the following sentence - *sent* (e.g., formal, informal, humorous, serious): In one word, describe the intent behind the following sentence (e.g., request, suggestion, command) - *sent*: In one word, rate the complexity of the following sentence (simple, moderate, complex) - *sent*: In one word, describe whether the following sentence is subjective or objective - *sent*: In one word, describe the language style of the following sentence (e.g., academic, conversational, literary) - *sent*: In one word, describe the grammatical structure of the following sentence (simple, compound, complex) - *sent*: Figure 4: Heatmap of Spearmans rank correlation between RW and HS embedding achieved using nine different prompts (defined in Table 1). The top-left (HS-HS) and bottom-right (RW-RW) blocks show the correlations between embedding when using different prompts, with mean scores of 0.52 and 0.63 (excluding the diagonal entries), respectively. This implies RW is more robust to varying prompts than HS. The top-right and bottom-left blocks reflect correlations between RW and HS when using the same or different prompts, both with mean score of 0.51. This lowest score indicates the complementarity between RW and HS. clustering and perform correlation analysis to quantify the differences between their respective cluster structures. We then leverage the BERTopic framework (Grootendorst, 2022) to examine the topics associated with each cluster, providing insights into the embeddings capacity to capture thematic content. Finally, we evaluate their performance in identifying semantically similar text pairs, further confirming their complementary nature. RW and HS embedding exhibit distinct clustering behaviors and encode different topics. Our analysis shows that the clustering results from RW and HS embedding are markedly different. As reflected in Table 1, the clustering metrics show moderate overlap (AMI and NMI at 0.29), but with low Jaccard Similarity of 0.06 and only 45.54% exact matching2 between clusters, underscoring the distinct ways each method structures the data. This difference in clustering behavior is further reflected in the topics captured by the embedding. As shown in Figure 3, the word clouds reveal that 2Exact matching refers to the proportion of data points that are grouped into identical clusters by two different methods (in this case, RW and HS embeddings). 5 the clusters from RW and HS embedding emphasize different thematic topics, highlighting how the two methods capture divergent aspects of the input data. Complementary nature of RW and HS embedding. Previous analyses suggest that RW and HS embedding capture different aspects of input data. To validate this hypothesis and quantify their complementarity, we need to examine how these two embedding relate to one another. We approach this by conducting Spearman correlation analysis using the STS12 dataset, which contains 6,216 sentence pairs. For each pair, we generate embedding from both RW and HS and calculate the similarity between the sentences to assess how each embedding captures semantic relationships. To ensure that any observed differences are not caused by prompt variation, we employ nine distinct prompts (listed in Table 2). As shown in Figure 4, notably, the correlation between RW and HS embedding is the lowest across all comparisons, with mean value of 0.51. This low correlation highlights that RW and HS capture largely unrelated aspects of the data, reinforcing their complementary nature. Further evidence supporting this complementarity is presented in the error analysis (Figure 2) and the experimental results (Section 4)."
        },
        {
            "title": "3.3 THE PROPOSED MOE EMBEDDING (MOEE)",
            "content": "Building on the analysis of routing weight (RW) and hidden state (HS) embedding, we propose our method MOEE, which combines RW and HS to form more comprehensive embedding representation. We introduce two approaches for this combination as follows. Concatenation-based Combination. In this method, the embedding generated by the hidden state (eHS) and the routing weights (eRW) are concatenated to form the final embedding. This approach is denoted as MOEE (concat), and the final embedding is computed as: efinal = [eHS; eRW] RdHS+dRW , where dHS is the dimensionality of the hidden state embedding, and dRW is the dimensionality of the routing weight embedding. This method preserves the distinct information captured by each component while allowing downstream tasks to leverage the combined representation. Weighted Sum Integration. The second method performs weighted sum of the similarity scores calculated from RW and HS embedding, denoted as MOEE (sum). For tasks like STS, given sentence pair (s1, s2), we first compute the similarity score between the two sentences using both HS-based embedding and RW-based embedding independently, as eHS(s1), eHS(s2), eRW(s1), and eRW(s2). Then, weighted sum of the similarity scores is performed before comparing the result to the ground truth: simHS = cosine_similarity(eHS(s1), eHS(s2)), simRW = cosine_similarity(eRW(s1), eRW(s2)) The final similarity score is then computed as: simfinal = simHS + α simRW, where α is used as hyperparameter to control the contribution of RW. Finally, we compute the rank correlation (e.g., Spearmans rank correlation) between the predicted similarity scores simfinal and the ground truth similarity. This framework can be applied consistently across other tasks, adapting the weighted sum to task-specific needs."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EVALUATION SETUP We evaluate our method on subset of tasks from the MTEB, representing broad range of natural language processing challenges. These tasks cover typical downstream applications for sentence embedding, including Classification, Clustering, Pair Classification, Re-ranking, Retrieval, Semantic Textual Similarity (STS), and Summarization. To ensure consistent and fair comparisons, we follow the evaluation framework provided by MTEB and use task-specific metrics from Muennighoff et al. (2022): Accuracy for Classification, V-Measure for Clustering, Average Precision for Pair Classification, Mean Average Precision for Re-ranking, nDCG for Retrieval, and Spearmans correlation for STS and Summarization. Our experiments use three MoE models: 6 DeepSeekMoE-16B (Dai et al., 2024): 28 layers, with 64 experts per layer. Qwen1.5-MoE-A2.7B (Team, 2024): 24 layers, each containing 60 experts. OLMoE-1B-7B (Muennighoff et al., 2024a): 16 layers, with 64 experts per layer. All models use per-token routing, but MOEE uses the last tokens routing weights, which consistently outperform averaging across all tokens. Ablation studies supporting this choice are provided in Section 4.3. Baselines Our goal is to extract advanced embedding from MoE LLMs by combining hidden state (HS) and routing weights (RW) without further training. To demonstrate the effectiveness of MOEE, we compare it against both RW and HS individually, as well as to several self-supervised and supervised methods that require training. We also assess performance across different prompt strategies, specifically comparing methods without prompts and with PromptEOL (Jiang et al., 2023). Table 3: Performance across MTEB Tasks without prompts, including Classification (CLF), Clustering (Clust.), Pair Classification (Pair CLF), Re-ranking (Rerank), STS, and Summarization (Summ.). MTEB Tasks CLF Clust. PairCLF Rerank STS Summ. Avg. Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) 44.79 44.06 44.93 48.74 46.41 38.99 44.81 50. 44.23 43.54 44.62 48.54 DeepSeekMoE-16b 25.87 17.53 24.15 32.83 44.34 50.59 51.88 52.12 38.13 35.94 41.20 47.88 Qwen1.5-MoE-A2.7B 24.31 10.55 26.75 31.35 44.43 42.26 49.79 51. 44.91 33.53 49.23 49.82 OLMoE-1B-7B 23.79 17.66 22.83 30.67 47.56 53.12 51.64 50.93 45.60 40.91 46.58 47.77 34.54 41.11 46.82 48. 28.36 23.97 37.93 45.75 35.44 44.68 48.84 49.45 24.51 26.22 31.17 29.89 22.65 27.44 27.61 24.00 20.94 28.68 31.67 28.77 35.36 35.91 40.03 43. 35.18 29.46 39.35 42.25 36.26 38.10 41.03 42.69 4.2 MAIN RESULTS Our method demonstrates consistent performance improvements across variety of MTEB tasks, as shown in Tables 3 and 4. Results for datasets under each task type are detailed in Appendix A. MOEE that combines routing weights with hidden state consistently outperforms both standalone methods (RW and HS) in most cases, highlighting the complementary nature of these two components. For tasks evaluated without prompts, the results show that MOEE (sum) achieves the highest average performance across models, with notable improvements in tasks such as Classification, Re-ranking, and STS. Specifically, DeepSeekMoE shows substantial boost from 35.36 (HS) to 43.30 (MOEE (sum)), 22.45% improvement. This pattern holds across Qwen1.5-MoE and OLMoE, where MOEE (sum) achieves consistent gains over both individual methods. When PromptEOL is introduced  (Table 4)  , we observe even greater performance gains, with 25.96% improvement for DeepSeekMoE. Across all models, MOEE (sum) again leads to the best results, with OLMoE achieving the highest overall average of 55.16 and Qwen1.5-MoE following closely at 55.04. While MOEE shows marginal gains over HS in the Classification task, this is expected, as the final layer HS is more aligned with output-specific features, which benefits classification. Although MOEE initially trails behind self-supervised and supervised methods without prompts, the introduction of PromptEOL leads to significant shift. As shown in Table 4, MOEE surpasses supervised approaches like SimCSE and coCondenser, achieving superior performance without requiring additional training. This underscores both its effectiveness and efficiency. 4.3 ABLATION STUDY This ablation study investigates how different methods of extracting routing weights (RW) and hidden state (HS) affect embedding quality across the STS12-16 datasets, with results presented in Table 5. 7 Table 4: Performance across MTEB Tasks when PromptEOL (Jiang et al., 2023) is applied to MoE. Baselines marked with are sourced from the MTEB leaderboard (Muennighoff et al., 2022) and require training. MTEB Tasks CLF Clust. PairCLF Rerank STS Summ. Avg. Self-Supervised Methods Glove (Reimers, 2019) Komninos (Reimers, 2019) BERT (Devlin, 2018) SimCSE-BERT-unsup (Gao et al., 2021) 51.04 50.21 52.36 54.80 23.11 24.96 23.48 22.59 SimCSE-BERT-sup coCondenser-msmarco (Gao & Callan, 2021) SPECTER (Cohan et al., 2020) 58.98 53.89 42.59 29.49 32.85 27.94 Supervised Methods Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) DeepSeekMoE-16b 58.24 49.52 54.21 58.31 24.64 19.97 26.10 34. Qwen1.5-MoE-A2.7B 59.34 47.84 54.23 59.57 29.50 16.74 27.18 38.33 OLMoE-1B-7B 58.18 32.83 19.93 45.02 33.92 52.59 36.46 57.46 62.90 66.63 66.10 70.79 75.82 74.56 56. 48.76 68.30 72.44 70.95 74.29 64.85 73.93 72.21 72.10 61.58 71.85 71.26 48.72 50.03 48.47 52.42 53.61 60.08 55.87 38.13 37.48 53.31 55. 56.51 43.55 56.12 56.25 58.31 43.91 56.69 60.43 60.52 61.73 52.89 75.00 79.97 76.41 60.68 59.66 59.52 67.59 70.66 67.39 51.71 68.52 72. 72.91 54.33 71.13 74.63 28.87 30.49 29.82 31.15 23.31 29.50 27.66 24.38 29.26 28.89 29.22 23.01 27.74 28.57 31.09 27.96 29.49 30.21 30. 45.86 47.34 45.52 51.13 53.53 54.55 45.16 42.30 44.01 50.42 53.28 51.67 42.07 51.43 55.04 53.72 42.38 52.73 55.16 Table 5: Ablation study on different ways of using routing weights (RW) and hidden state (HS). STS Datasets STS12 STS13 STS14 STS15 STS16 Avg. HS - last token, last layer HS - last token, all layers HS - all tokens, last layer HS - all tokens, all layers DeepSeekMoE-16b 69.56 60.59 34.42 62.46 54.68 45.20 26.77 46.90 51.99 59.82 30.95 60.81 RW - last token RW - all tokens MOEE (best) 61.97 50.76 67.39 65.86 46.42 81.43 51.38 41.47 68. 58.04 51.08 34.90 52.38 65.86 43.68 67.76 68.47 58.88 37.11 59.99 62.49 48.37 74. 60.40 55.03 32.78 56.34 61.18 46.03 71.75 As detailed in Section 3.1, RW integrates routing decisions across all layers, capturing information at multiple depths. In contrast, HS from only the last layer may miss important intermediate details. Therefore, we evaluate the use of hidden states from all layers (HS - last token, all layers) to see if it can match RW, which naturally leverages multi-layered information. We also assess the impact of using only the last token versus averaging across all tokens. While the last token often condenses crucial sequence information, mean pooling across all tokens may offer broader view by incorporating contributions from every token. Thus, we compare HS - last token with HS - all tokens, and RW - last token with RW - all tokens. For multi-layer or multi-token cases, mean pooling is applied. Our results show that focusing on the last token, whether from HS or RW, consistently delivers the best performance. This indicates that the last token captures the most critical semantic information, while pooling across tokens or layers introduces noise. Notably, RW outperforms HS, underscoring its superior ability to capture nuanced, dynamic information that HS alone cannot replicate."
        },
        {
            "title": "4.4 A STABILITY COMPARISON OF RW AND HS USING DIFFERENT PROMPTS",
            "content": "Figure 5: Box plots of the performance of the two embedding methods (RW or HS) using nine different prompts (listed in Table 2) on five STS datasets. The higher variance and wider spread of HS in the box plots indicate its sensitivity to the prompt choice, while RW is more robust (lower variance) with better mean performance. Prompts are commonly used to boost the performance of embedding models across diverse downstream tasks (Lei et al., 2024), as shown by the improved results of PromptEOL  (Table 3)  compared to no prompts  (Table 4)  . However, the effectiveness of these prompts can vary, and methods robustness depends on its ability to handle these variations. To assess the prompt sensitivity of RW and HS, we measure their Spearman correlation scores across STS12-16 datasets using 9 different prompts listed in Table 2. We then compute the mean and variance of these scores for each dataset, capturing how performance fluctuates under different prompt conditions and whether the methods remain stable when exposed to prompt variations. Figure 5 highlights the performance variance for both methods. HS exhibits significantly higher variance, particularly in datasets like STS12, STS13, and STS14, indicating that its performance is highly dependent on the specific prompt used. This suggests that HS is more sensitive to prompt formulation, leading to inconsistent results that could hinder its reliability in broader applications. Figure 4 (see Section 3.2) further supports this from another perspective3, showing smaller mean correlation of 0.52 for HS using different prompts, reflecting higher variance than RW. In contrast, RW demonstrates greater stability, with consistently lower variance and narrower box plots across all datasets, indicating its robustness to prompt choice. In Figure 4, RW also achieves higher mean correlation of 0.63 between different prompts, underscoring its ability to maintain stable performance across different prompts. This makes MOEE more reliable option for tasks where prompt variability is expected. 4.5 CASE STUDY: WHEN HS OUTPERFORMS RW & VICE VERSA In this section, we analyze instances where HS embedding performs better than RW embedding  (Table 6)  , as well as cases where RW outperforms HS  (Table 7)  . This helps identify the strengths and weaknesses of each method and offers insights into when one may be preferred over the other. From the results, HS embeddings excel in capturing formal linguistic consistency, particularly when sentence structure undergoes only superficial changes. They effectively represent the overall structure and meaning of sentences, making them useful in cases with minimal semantic variation. In contrast, RW embedding performs better when handling paraphrasing, synonym use, and nuanced stylistic shifts. The RW mechanisms sensitivity to input variations allows it to capture deeper contextual changes, even when the overall meaning of the sentence is preserved. 3The Spearman correlation in Figure 5, as performance metric, is between HS/RW and the ground truth, while the Spearman correlation in Figure 4 is to compare different embedding. 9 Table 6: Semantically similar sentence pairs correctly predicted by HS embedding but not by RW embedding. Differences between the sentences are highlighted to show subtle variations that influence prediction outcomes. Sentence 1 the vote will take place today at 5.30 p.m 2 the are standards scarcely comparable, let alone transferable 3 that provision could open the door wide to arbitrariness 4 woman puts flour on piece of meat 5 the fishermen tired inactive, disappointed are and take Sentence 2 the vote will place at 17h30 the norms are hardly comparable and still less transferable this point of procedure opens the door to the arbitrary woman is putting flour onto some meat. fishermen are inactive, tired and disappointment Table 7: Semantically similar sentence pairs correctly predicted by RW embedding but not by HS embedding. Sentence 1 He did, but the initiative did not get very far. 2 then perhaps we could have avoided catastrophe 3 it increases the power of the big countries at the expense of the small countries 4 festive social event, celebration 5 group of people defined by specific profession Sentence 2 What happened is that the initiative does not go very far. we might have been able to prevent disaster it has the effect of augmenting the potency of the big countries to the detriment of babies an occasion on which people can assemble for social interaction and entertainment. organization of performers and associated personnel (especially theatrical)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we explore the untapped potential of MoE as effective embedding generators without extra training. Our analysis reveals that RW derived from MoE models complements the widely-used HS embedding, offering deeper understanding of input semantics. By leveraging both RW and HS, we propose MOEE, which significantly improves embedding performance across diverse tasks in the MTEB benchmark. Our results demonstrate that combining RW and HS boosts generalization, making MoE models versatile tools for embedding tasks. Future work would further explore how to leverage MOEE adaptively for task-specific scenarios."
        },
        {
            "title": "REFERENCES",
            "content": "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180, 2020. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Luyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540, 2021. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021. Maarten Grootendorst. Bertopic: Neural topic modeling with class-based tf-idf procedure. arXiv preprint arXiv:2203.05794, 2022. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97299738, 2020. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088. Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, and Qi Zhang. Promptbert: Improving bert sentence embeddings with prompts. arXiv preprint arXiv:2201.04337, 2022. Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling sentence embeddings with large language models. arXiv preprint arXiv:2307.16645, 2023. Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994. Ryan Kiros, Yukun Zhu, Russ Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. Advances in neural information processing systems, 28, 2015. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, and Andrew Yates. Meta-task prompting elicits embedding from large language models. arXiv preprint arXiv:2402.18458, 2024. Ziyue Li, Kan Ren, Xinyang Jiang, Yifei Shen, Haipeng Zhang, and Dongsheng Li. Simple: Specialized model-sample matching for domain generalization. In The Eleventh International Conference on Learning Representations, 2023. Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and Armen Aghajanyan. Moma: Efficient early-fusion pre-training with mixture of modality-aware experts, 2024. URL https://arxiv.org/abs/2407.21770. 11 Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfrembedding-mistral: enhance text retrieval with transfer learning. Salesforce AI Research Blog, 3, 2024. Yu Meng, Chenyan Xiong, Payal Bajaj, Paul Bennett, Jiawei Han, Xia Song, et al. Coco-lm: Correcting and contrasting text sequences for language model pretraining. Advances in Neural Information Processing Systems, 34:2310223114, 2021. Tomas Mikolov. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. doi: 10.48550/ARXIV.2210.07316. URL https://arxiv.org/abs/2210.07316. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060, 2024a. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint arXiv:2402.09906, 2024b. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et al. Mixture-of-experts meets instruction tuning: winning combination for large language models. arXiv preprint arXiv:2305.14705, 2023. Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, and Guilin Liu. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv:2408.15998, 2024. Qwen Team. Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters\", February 2024. URL https://qwenlm.github.io/blog/qwen-moe/. Kexin Wang, Nils Reimers, and Iryna Gurevych. Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning. arXiv preprint arXiv:2104.06979, 2021. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1189711916, 2024. Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive learning for sentence representation. arXiv preprint arXiv:2012.15466, 2020. Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context, 2024. URL https://arxiv.org/abs/2404.13046."
        },
        {
            "title": "A MTEB RESULTS",
            "content": "We present detailed evaluation results of task types, including STS  (Table 8)  , classification  (Table 9)  , pair classification  (Table 10)  , clustering  (Table 11)  , and re-ranking  (Table 12)  tasks. We show the performance of our method across different models and prompts, and compares it to baseline methods like Hidden State (HS) and Routing Weight (RW). Table 8: Detailed Results of STS Tasks. The DeepSeekMoE, Qwen1.5-MoE, and OLMoE models are evaluated on tasks from STS12 to STSBenchmark. The MOEE method (without and with PromptEOL) significantly improves performance across most benchmarks. Prompt STS12 STS13 STS STS15 STS16 BIOSSES SICK-R STSBenchmark Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) none none none none PromptEOL PromptEOL PromptEOL PromptEOL none none none none PromptEOL PromptEOL PromptEOL PromptEOL none none none none PromptEOL PromptEOL PromptEOL PromptEOL 20.90 45.22 46.26 46.41 51.99 61.97 66.79 67.39 8.39 27.96 33.36 35.72 55.05 54.39 64.44 65.54 21.53 47.16 48.82 49.59 65.51 55.76 67.35 68. 47.15 50.36 54.19 54.98 68.47 62.49 71.22 74.26 38.11 36.29 47.16 50.61 73.49 56.96 71.48 75.43 51.49 51.91 56.06 56.11 77.19 57.88 73.35 76.88 29.87 34.14 41.20 42.33 45.29 53.97 61.96 62.09 28.69 25.40 39.06 53.40 61.42 43.65 64.87 67.84 44.11 44.30 54.58 54.58 73.54 56.28 73.02 73. 42.66 51.98 53.66 53.70 63.78 57.93 66.29 69.98 51.73 29.42 53.92 62.35 67.01 55.46 69.01 71.15 39.98 52.89 52.24 52.82 66.62 56.02 67.51 70.56 30.61 38.44 43.06 44.36 65.48 56.68 68.72 73.41 36.88 22.80 43.09 54.11 67.42 50.54 69.71 75.57 22.36 40.77 42.02 42.16 71.51 50.72 70.47 75. DeepSeekMoE-16b 37.75 38.63 42.37 42.85 58.04 65.86 64.60 67.76 24.02 28.75 37.90 41.50 54.68 51.38 63.56 68.98 43.39 41.38 55.88 60.58 69.56 65.86 77.60 81.43 Qwen1.5-MoE-A2.7B 22.08 17.11 25.86 31.00 73.60 48.11 67.18 72.88 15.76 13.88 24.68 31.51 63.63 45.49 64.05 71.39 25.23 18.89 36.30 47.29 77.48 59.05 77.38 82. OLMoE-1B-7B 41.47 43.92 52.69 54.19 81.86 60.01 80.13 84.34 22.71 32.62 37.48 38.87 69.37 48.08 68.42 74.02 39.88 43.87 46.80 47.27 77.64 49.88 68.76 73.81 13 Table 9: Detailed Results of Classification Tasks, including sentiment extraction, emotion classification, and toxic conversations classification. The performance of different methods (Hidden State, Routing Weight, and MOEE) with and without PromptEOL is shown. Prompt TweetSentimentExtractionClassification EmotionClassification ToxicConversationsClassification DeepSeekMoE-16b Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) none none none none PromptEOL PromptEOL PromptEOL PromptEOL 49.14 52.37 52.64 50.32 60.13 57.68 61.12 59.32 Qwen1.5-MoE-A2.7B Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) none none none none PromptEOL PromptEOL PromptEOL PromptEOL none none none none PromptEOL PromptEOL PromptEOL PromptEOL 48.83 42.80 49.60 48.84 61.14 55.33 60.78 60.72 OLMoE-1B-7B 50.29 50.15 51.59 51.00 59.58 52.79 59.72 59.92 27.55 26.49 28.02 27.52 49.11 35.57 45.59 46.86 31.02 20.63 30.93 32.76 48.09 33.82 46.10 47. 30.29 25.53 28.76 29.75 47.50 28.51 42.78 45.63 57.69 53.32 54.12 68.39 65.47 55.32 55.93 68.76 59.38 53.53 53.90 70.50 68.80 54.37 55.82 70.03 52.10 54.93 53.51 64.86 67.46 53.75 55.27 66.84 Table 10: Pair classification task results on TwitterURLCorpus and TwitterSemEval2015. Prompt TwitterURLCorpus TwitterSemEval2015 Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) DeepSeekMoE-16b 49.04 53.39 57.27 58.99 36.72 76.58 80.08 79.20 none none none none PromptEOL PromptEOL PromptEOL PromptEOL Qwen1.5-MoE-A2.7B 45.71 48.78 53.74 57.78 82.50 73.72 82.34 80. none none none none PromptEOL PromptEOL PromptEOL PromptEOL OLMoE-1B-7B Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) none none none none PromptEOL PromptEOL PromptEOL PromptEOL 55.07 54.25 56.97 57.03 82.32 70.37 82.32 80.98 39.63 47.79 46.48 45.25 60.79 60.01 64.79 62.70 43.14 35.74 45.83 45.95 66.07 55.98 65.51 64.20 40.04 51.99 46.31 44.82 61.87 52.79 61.38 61.53 Table 11: Clustering task results, showing performance on TwentyNewsgroupsClustering and MedrxivClusteringS2S. MOEE (sum) consistently performs best without prompt, while the MOEE method with PromptEOL delivers substantial gains. Prompt TwentyNewsgroupsClustering MedrxivClusteringS2S Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) none none none none PromptEOL PromptEOL PromptEOL PromptEOL none none none none PromptEOL PromptEOL PromptEOL PromptEOL none none none none PromptEOL PromptEOL PromptEOL PromptEOL DeepSeekMoE-16b 25.62 15.33 22.94 31.44 27.02 21.89 29.13 35.77 Qwen1.5-MoE-A2.7B 26.14 9.71 28.99 32.07 34.04 16.94 30.45 42.05 OLMoE-1B-7B 21.05 17.14 20.72 27.58 38.96 22.13 41.23 38.58 26.11 19.72 25.35 34.22 22.26 18.04 23.06 33.27 22.48 11.38 24.51 30.62 24.95 16.54 23.91 34. 26.52 18.17 24.94 33.75 26.69 17.72 26.60 34.33 Table 12: Re-ranking task results, showing performance on AskUbuntu, SciDocsRR, and StackOverflow duplicate questions re-ranking tasks. Prompt AskUbuntuDupQuestions SciDocsRR StackOverflowDupQuestions Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) Hidden State (HS) Routing Weight (RW) MOEE (concat) MOEE (sum) none none none none PromptEOL PromptEOL PromptEOL PromptEOL none none none none PromptEOL PromptEOL PromptEOL PromptEOL none none none none PromptEOL PromptEOL PromptEOL PromptEOL 45.23 42.65 53.43 70.79 45.23 42.65 72.63 76. 60.91 36.85 68.42 70.85 75.06 55.03 75.69 74.53 69.08 54.17 70.33 72.54 78.24 55.43 77.14 81.19 DeepSeekMoE-16b 43.75 41.97 44.10 45.26 43.75 46.57 50.66 52.93 Qwen1.5-MoE-A2.7B 43.71 41.00 44.95 44.30 54.69 44.65 52.15 51. OLMoE-1B-7B 43.67 42.83 43.91 44.57 55.32 45.11 52.81 56.68 15 25.79 23.21 26.06 27.58 25.41 23.21 36.65 38.88 30.12 22.75 34.31 34.31 39.79 30.96 40.51 42.91 24.05 25.72 25.49 26.20 41.36 31.20 40.13 43."
        }
    ],
    "affiliations": [
        "Department of Computer Science University of Maryland, College Park"
    ]
}