{
    "paper_title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
    "authors": [
        "Heejeong Nam",
        "Quentin Le Lidec",
        "Lucas Maes",
        "Yann LeCun",
        "Randall Balestriero"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa."
        },
        {
            "title": "Start",
            "content": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions Heejeong Nam 1 Quentin Le Lidec 2 * Lucas Maes 3 4 * Yann LeCun 2 Randall Balestriero 1 1Brown University, 2New York University, 3Mila, 4Université de Montréal"
        },
        {
            "title": "Abstract",
            "content": "World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, simple and flexible objectcentric world model that extends masked joint embedding prediction from image patches to objectcentric representations. By applying object-level masking that requires an objects state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide formal analysis demonstrating that object-level masking induces causal inductive bias via latent interventions. Our code is available at github.com/galilai-group/cjepa. 6 2 0 2 1 1 ] . [ 1 9 8 3 1 1 . 2 0 6 2 : r 1. Introduction World models (Ha & Schmidhuber, 2018) provide unifying abstraction for learning, predicting, and reasoning about the dynamics of complex environments, enabling scalable planning and control directly in latent space from highdimensional observations (Hafner et al., 2019; 2025). At their core, effective world modeling benefits from represen- *Equal contribution 1Brown University, GalilAI 2New York University 3Mila 4Université de Montréal. Correspondence to: Heejeong Nam <heejeong_nam@brown.edu>. Preprint. February 13, 2026. 1 Figure 1. C-JEPA training pipeline. frozen encoder extracts object-centric representations, followed by selective masking across history. The predictor recovers masked history slots and predicts future latent states, conditioned on optional auxiliary variables, via joint masked-history and forward-prediction objective. tations that expose entities and their interactions, avoiding reliance on spurious pixel-level correlations (Zholus et al., 2022; Nishimoto & Matsubara, 2025; Ferraro et al., 2023; Feng et al., 2025; Lei et al., 2025). Inspired by this perspective, object-centric models have been extensively developed for learning visual dynamics (Kipf et al., 2022; Zadaianchuk et al., 2023; Wu et al., 2023; Mosbach et al., 2025; Villar-Corrales et al., 2023), and more recently adopted as foundation for learning world models (Ferraro et al., 2023; Nishimoto & Matsubara, 2025). However, learning on top of object-centric representations alone is not sufficient. Existing works have shown that, without explicit mechanisms to guide interaction learning, models can easily fall back on object self-dynamics or exploit incidental correlations (VillarCorrales et al., 2023; Lei et al., 2025). This realization has spurred growing interest in object-centric world models that explicitly learn interactions (Kipf et al., 2020; Feng et al., 2025; Mosbach et al., 2025) or causal structure (Zholus et al., 2022; Nishimoto & Matsubara, 2025; Lei et al., 2025). Causal-JEPA: Learning World Models through Object-Level Latent Interventions These approaches often enforce interactions by separating temporal dynamics from object interactions (Mosbach et al., 2025; Villar-Corrales et al., 2023; Feng et al., 2025), regularizing attention sparsity (Lei et al., 2025), leveraging graph structures (Kipf et al., 2020), or relying on downstreamspecific methods (Ferraro et al., 2023; Nishimoto & Matsubara, 2025). Together, these works emphasize the importance of interactions in world modeling, while leaving open the question of how interaction structure can be made functionally necessary through the learning objective itself. Accordingly, we propose Causal-JEPA (C-JEPA), simple yet effective approach for object-centric world modeling with causal inductive bias from object-level intervention. This design is motivated by the fact that existing patchbased masked prediction approaches (He et al., 2022; Fan et al., 2023; Thoker et al., 2025; Tessler et al., 2024; Tong et al., 2022) optimize for local patch correlations, without enforcing object-level interaction reasoning. We propose an object-level masking that trains the model to infer an objects latent trajectory from others evolving states, inducing counterfactual-like interventions during learning. Our masking strategy prevents shortcut solutions such as trivial temporal interpolation, making interaction reasoning necessary for minimizing the prediction objective. At the same time, C-JEPA remains architecturally flexible and can incorporate auxiliary variables such as actions and proprioception. Notably, the compactness of object-centric representations, when combined with the Joint Embedding Predictive Architecture (JEPA) (LeCun, 2022), encourages the model to concentrate on the essential factors governing object dynamics within low-dimensional latent space, thereby reducing both computational and memory overhead. In contrast to patch-based predictors, where attention scales quadratically with large number of patch tokens, entity-level representations apply the same quadratic attention mechanism over much smaller token set, resulting in significantly lower training and inference costs. In Sec. 5, we evaluate C-JEPA from two perspectives, visual reasoning and predictive control. We first evaluate our approach on CLEVRER (Yi et al., 2020), video QA benchmark involving multi-object interactions, and show that object-level masking leads to consistent improvements. In particular, C-JEPA improves visual question answering, with roughly 20% absolute gain on counterfactual reasoning over the same architecture without object-level masking. We further evaluate C-JEPA in model predictive control (MPC) setting on the Push-T manipulation task. Compared to patch-based world models such as DINO-WM (Zhou et al., 2025), C-JEPA achieves comparable task performance while using only 1.02% of the total input feature size, which translates to more than 8 faster model predictive control. Finally, in Sec. 6, we formally characterize object-level masked prediction as causal inductive bias, showing that latent interventions make interaction reasoning necessary for minimizing the training objective. Taken together, this leads to four main contributions: We introduce C-JEPA, an object-centric world model that uses object-level masking as latent intervention, which enforces interaction-dependent prediction. C-JEPA consistently improves visual reasoning over multi-object interactions, with particularly large gains on counterfactual questions. C-JEPA enables highly efficient predictive control, achieving comparable performance to patch-based world models while using orders-of-magnitude fewer tokens and substantially faster MPC planning. We provide an analysis of how object-level masked prediction can induce causal inductive bias, making interaction reasoning functionally necessary. 2. Related Works 2.1. Structured World Models for Dynamics Learning World modeling in dynamical scenes requires structured representations capturing entities and their interactions. Some approaches rely on weak supervision over entity information (Zholus et al., 2022; Ferraro et al., 2023; Lei et al., 2025) or reinforcement-learningspecific architectures (Ferraro et al., 2023; Nishimoto & Matsubara, 2025), while others build self-supervised and generally applicable world models using object-centric representations. For example, several methods explicitly decompose self-dynamics and interactions through architectural factorization (Feng et al., 2025; Villar-Corrales et al., 2023; Mosbach et al., 2025). Subsequent work move beyond reconstruction to reduce redundancy in learned representations. While reconstruction-free, SlotFormer (Wu et al., 2023) does not explicitly enforce interaction structure, and C-SWM (Kipf et al., 2020) relies on fixed relational graph, limiting adaptability. SPARTAN (Lei et al., 2025) learns world models directly in latent space and encourages interaction selectivity via sparse attention. However, none of these works induces interactionaware structure through the learning objective itself. 2.2. Masking-Based Representation Learning Masked image modeling was originally introduced as scalable self-supervised learning paradigm to improve representation quality (He et al., 2022; Tong et al., 2022). Subsequent work explored guided masking strategies to enhance representations by selectively masking regions informed by motion or dynamics (Fan et al., 2023; Thoker et al., 2025; 2 Causal-JEPA: Learning World Models through Object-Level Latent Interventions Tessler et al., 2024). Nakano et al. (2024) combines masking with object-centric representations, but applies masking to image patch tokens for computational efficiency, while object representations are used only as conditioning signals. Masking has also been used in causal discovery to identify latent structure (Ng et al.; Yang et al., 2021; Nam, 2023), but these methods typically assume fixed or identifiable graphs and focus on disentanglement or structure recovery, rather than flexible world modeling. Finally, (Xiao et al., 2023) interpreted masking as inducing counterfactual variations, which aligns with our use of masking, but they also assumed structural causal model and focused on robust fine-tuning. In contrast, our work adopts masking as an object-level latent intervention to shape predictive dependencies in world model as principled inductive bias for learning dynamics. 3. Preliminaries Joint Embedding Predictive Architecture JEPA (LeCun, 2022) defines self-supervised learning paradigm that learns to predict in representation space without reconstructing pixels, focusing instead on modeling predictive relationships between latent embeddings that capture semantic structure in the data. This formulation has been progressively extended from image-level masked joint embedding prediction in I-JEPA (Assran et al., 2023), to spatiotemporal tube-based prediction in video with V-JEPA (Bardes et al., 2024), and further to integrated understanding, prediction, and planning in V-JEPA2 (Assran et al., 2025). By avoiding reconstruction-driven objectives, JEPA-style models learn representations that are directly aligned with prediction and control, making them particularly well suited for world modeling and autonomous decision-making (LeCun, 2022; Assran et al., 2023; Bardes et al., 2024; Assran et al., 2025). Following prior works (Zhou et al., 2025; Li et al., 2025), we employ frozen target encoders and optimize the predictor with joint embedding prediction objective, aligning predicted latents to target latents. Object-Centric Representation via Slot Attention Slot Attention (Locatello et al., 2020) was introduced as mechanism for learning object-centric representations by iteratively grouping feature maps into fixed set of slots through competitive attention. By alternating between attentionbased assignment and slot updates, it produces set of object-level representations without requiring supervision. Subsequent work extended this formulation from images to videos (Kipf et al., 2022; Zadaianchuk et al., 2023; Aydemir et al., 2023; Singh et al., 2022). Unlike image-based Slot Attention, video extensions must additionally maintain slot identity over time, which is typically achieved by conditioning slot updates on previous-frame slots. Notably, some approaches such as VideoSAUR (Zadaianchuk et al., 2023) leverage powerful pretrained foundation models, including DINO (Oquab et al., 2024) and DINOv2 (Caron et al., 2021), as feature encoders, inheriting strong semantic and invariance properties from large-scale pretraining. In our framework, we use Slot Attention to encode each observation into fixed-size set of object-centric latent states, yielding set representation St = {s1 }, where each slot corresponds to distinct entity in the scene. , . . . , sN 4. Method In this section, we introduce C-JEPA, an object-centric latent world model that applies object-level masking during training to induce interaction-aware predictive representations. All notation is summarized in Appendix A. 4.1. Problem Setting We consider video-based dynamical system observed through sequence of images. Let Xt RHW denote the pixel-level observation at time t. An object-centric encoder maps each frame to set of object-centric slots, St = g(Xt) = {s1 , . . . , sN }, Rd, si (1) where is the fixed number of slots and is the dimensionality of each slot. The set representations St are permutationequivariant with respect to the slot ordering. We consider history window of length Th and prediction horizon of length Tp. Accordingly, we define the history index set := {t Th + 1, . . . , t}, and the full historyfuture prediction interval := {t Th + 1, . . . , + Tp}. Given past object states ST , the world model aims to predict future object states {St+1, . . . , St+Tp }. Masked and Visible Object Sets. For any time step τ in the history window , we partition the object set Sτ into masked subset and an unmasked context subset. Let Mτ {1, . . . , } denote the indices of masked objects at time τ . We then define τ = {si Sm τ Mτ }, τ = {sj Sc τ / Mτ }. (2) τ , while the remaining states Sc The masked set Sm τ denotes the subset of object states selected for masking, whose observations are replaced by mask tokens Sm τ are provided as context. During training, learns to recover masked tokens within ST and to predict future states {St+1, . . . , St+Tp } under masking. At inference time, is used solely for forward prediction from fully observed history ST , without masking. Auxiliary Observable Variables. In addition to object states, we allow for auxiliary observable variables that influence state transitions. For example, when actions and proprioceptive signals are available, we denote these variables by Ut = {at, pt}, where at represents actions and 3 Causal-JEPA: Learning World Models through Object-Level Latent Interventions be interpreted as latent-level intervention, forcing prediction to rely on interactions with other entities. We elaborate more on this interpretation in Appendix B.1. Learning Objective. C-JEPA serves as the latent predictor in the object-centric world model, operating on masked object representations to learn dynamics. Fig. 1 illustrates the overall architecture and training procedure of C-JEPA. The predictor is ViT-style masked transformer with bidirectional attention (Assran et al., 2023; Bardes et al., 2024; Assran et al., 2025), enabling joint inference over masked object tokens across the history window and future horizon. Here, ZT denotes the masked input sequence in which selected entity tokens are replaced by mask tokens across both the history window and the future horizon  (Fig. 1)  . The predictor outputs latent predictions as ˆZT = (cid:0) ZT (cid:1). (4) Training minimizes masked latent prediction objective by predicting all masked object tokens over the interval : (cid:34) Lmask = (cid:88) (cid:88) 1(cid:2)zi τ = zi τ τ i=1 (cid:3) (cid:13) (cid:13)ˆzi τ zi τ (cid:35) , (cid:13) 2 (cid:13) 2 (5) τ = zi where the indicator 1[zi τ ] selects all tokens masked in the input over . This masked latent prediction objective can be viewed as combination of mask reconstruction over the history window and prediction over the future horizon, (cid:104)(cid:13) (cid:13)ˆzi τ zi τ Lmask = (cid:124) (cid:13) 2 (cid:13) 2 (cid:12) (cid:105) (cid:12) (cid:12) M, τ (cid:123)(cid:122) Lhistory (cid:125) + (cid:124) (cid:20)(cid:13) ˆZτ Zτ (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 (cid:123)(cid:122) Lfuture (cid:12) (cid:12) (cid:12) (cid:12) (cid:21) τ > . (cid:125) (6) The history term suppresses reliance on trivial self-dynamics under partial observability, while the future term enforces alignment with forward world modeling. Together, objectlevel masking makes interaction reasoning functionally necessary for minimizing the predictive objective. Remark 1 (Causal Interpretation of Object-Level Masking). In C-JEPA, prediction is governed by structured dependencies among object states and exogenous variables, without assuming fixed or explicitly parameterized causal graph. Here, the term causal\" refers to temporally directed predictive dependencies from past object states to future states that remain stable under object-level masking, rather than claims of causal identifiability. Figure 2. Object-level latent interventions in C-JEPA. Selected object slots are masked across time, except for minimal identity anchor, forcing the predictor to infer object dynamics from interactions with other objects and auxiliary variables. pt represents proprioceptive signals. These variables are treated as external to the object-level state representation and incorporated as additional conditioning inputs alongside object states. This enables the model to capture both objectobject dependencies and action-mediated influences within unified object-centric framework, consistent with treating observable sources as auxiliaries (Kim et al., 2025). 4.2. Causal-JEPA We now describe the core design of C-JEPA, which introduces object-level masking as structured latent intervention for learning interaction-aware dynamics. Object-Level Masking for Latent Interventions. Fig. 2 illustrates our object-level masking. At each time step t, the object-centric representation St, together with auxiliary variables Ut, is represented as set of entity tokens Zt = {St, Ut}, which serves as the input to the predictor. While future entity tokens are always masked for prediction, we additionally mask the observable latent states of selected objects across the history window according to masking index set M, preserving only the earliest time step t0 as an identity anchor. The identity anchor is introduced to address the permutation equivariance of slot-based representations with respect to object ordering, which motivates omitting positional encodings along the entity dimension, consistent with prior work (Wu et al., 2023; 2021). As result, identity information of each entity must be provided for masked tokens, enabling the transformer predictor to distinguish which entities are masked and to predict appropriately. Concretely, we define the masked token as τ = ϕ(zi zi t0 ) + eτ , (3) where ϕ is linear projection, zi t0 serves as an identity anchor, and eτ is learnable embedding combined with temporal positional encoding. This masking operation can Object-level masking acts as latent intervention that poses counterfactual-like queries during training by selectively removing access to object states. This introduces causal 4 Causal-JEPA: Learning World Models through Object-Level Latent Interventions inductive bias that enforces reliance on interaction-relevant variables. In line with observations in Lei et al. (2025), the predictors attention mechanism gives rise to statedependent interaction patterns that can be interpreted as soft, local relational structure capturing predictive influence. We formalize this interpretation in Sec. 6. Inference. At inference time, C-JEPA performs forward latent prediction following Eq. 4, with fully observable history and masking applied only to future tokens. This procedure aligns with the standard objective of latent world modeling, enabling direct rollout of future object states for downstream planning, control, and reasoning. 5. Experiments In this section, we evaluate C-JEPA from two complementary perspectives, focusing on visual reasoning in Sec. 5.1 and predictive control in Sec. 5.2. Across experiments, we primarily adopt VideoSAUR (Zadaianchuk et al., 2023) as the object-centric encoder g, which aggregates frozen DINOv2 (Oquab et al., 2024) features into object-centric latents. In Sec. 5.1, we additionally report results using SAVi (Kipf et al., 2022) to ensure comparability with baseline models. Implementation and pretraining details are deferred to Appendix D. 5.1. Causal-JEPA for Visual Reasoning Task and Evaluation. We first evaluate visual reasoning on CLEVRER (Yi et al., 2020), synthetic video benchmark for physical and causal understanding in dynamic scenes. CLEVRER consists of videos depicting multiple interacting objects and questionanswer pairs for evaluating descriptive, predictive, explanatory and counterfactual reasoning over object interactions. To enable reasoning from predicted object trajectories, we adopt ALOE (Ding et al., 2021), following the evaluation setup of SlotFormer (Wu et al., 2023). ALOE is designed to perform reasoning directly on stacked object-centric representations over time, using language transformer to condition attention on the input question. Briefly, we train C-JEPA and baseline world models, and roll out 128-frame input videos to 160 frames to produce imagined trajectories. ALOE is then trained on these model-generated trajectories for downstream questions. We use fixed number of seven object slots across all CLEVRER experiments. Further data and evaluation details are in Appendix F. Baseline Models. We evaluate three distinct object-centric world modeling architectures. Since ALOE presupposes object-centric representations, VQA evaluation is restricted to object-centric models, and non-object-centric baselines are omitted. To ensure fair comparison under the same Table 1. VQA accuracy under varying numbers of masked objects. Overall accuracy is reported per question, with counterfactual accuracy additionally reported per option and per question. (V) uses VideoSAUR and (S) uses SAVi encoders. Performance changes relative to the unmasked baseline are indicated by arrows. Model OC-JEPA (V) C-JEPA (V) OC-JEPA (S) C-JEPA (S) 0 1 2 3 4 0 1 2 3 4 Average per que. (%) Counterfactual VQA (%) per opt. per que. 82.79 79.53 47.68 83.95 1.16 84.56 1.77 87.61 4.82 89.40 6. 80.34 0.81 80.61 1.08 86.49 6.96 88.67 9.14 49.67 1.99 50.25 2.57 63.60 15.92 68.81 21.13 77.28 76.69 41.10 78.39 1.11 83.88 6.60 79.02 1.74 73.28 4. 77.26 0.57 85.16 8.47 77.78 1.09 73.55 3.14 43.13 2.03 60.19 19.09 43.77 2.67 34.06 7.04 reconstruction-free setting as C-JEPA, we additionally evaluate reconstruction-free variants of the baseline models. SlotFormer (Wu et al., 2023) performs autoregressive rollouts over object latents, serving as canonical baseline without explicit interaction constraints. We evaluate both SlotFormer (+/Recon.). OCVP-Seq (Villar-Corrales et al., 2023) factorizes self-dynamics and object interactions at the attention level. We evaluate both OCVP-Seq (+/Recon.). OC-JEPA is history-unmasked variant of C-JEPA that uses the same architecture while masking only future tokens, isolating the effect of masking-induced inductive bias. All baselines use the same pretrained SAVi encoder for fair comparison. We defer additional details to Appendix H. Results. Tab. 1 shows the comparison between OC-JEPA and C-JEPA provides clean ablation demonstrating that performance gains stem from the learning objective rather Introducing than object-centric representations alone. object-level masking consistently improves performance, indicating that the causal inductive bias induced by the objective is the key driver. Notably, improvements are substantially larger on counterfactual questions than on overall accuracy or other question categories (See full results in Appendix I), suggesting that masking does not merely enhance predictive accuracy but directly strengthens counterfactual reasoning. This aligns with the training setup, where object-level masking exposes the model to counterfactuallike queries through latent interventions. Finally, excessive masking can remove informative dependencies, indicating 5 Causal-JEPA: Learning World Models through Object-Level Latent Interventions Table 2. Baseline comparison using SAVi encoder. For models with and without reconstruction, arrows indicate differences relative to the reconstruction-based variant. Table 3. Push-T planning success rates across world models and token budgets. Changes are reported relative to OC-DINO-WM, and = 1 is used for C-JEPA. Counterfactual VQA (%) # Token Model Success Rate (%) Model Average per que. (%) per opt. per que. SlotFormer (-) recon. loss 79.44 44.94 34.50 79.28 55.62 23.66 11.10 36.19 47.29 OCVP-Seq (-) recon. loss 83.11 80.09 3.02 83.21 77.46 5.75 56.06 43.00 13.06 OC-JEPA C-JEPA 77. 83.88 76.69 85.16 41.10 60.19 an optimal masking regime that depends on the robustness of the underlying object representations from the encoder. Tab. 2 shows comparison against object-centric baselines under reconstruction and non-reconstruction settings. Results show that removing reconstruction leads to severe performance degradation in SlotFormer (Wu et al., 2023), indicating strong reliance on pixel-level supervision. OCVPSeq (Villar-Corrales et al., 2023) exhibits comparatively modest decline, suggesting that its architectural factorization partially mitigates the loss of reconstruction-based signals. In contrast, C-JEPA achieves the strongest performance without any reconstruction, highlighting the effectiveness of its decoder-free design and masking-based learning objective. Full results are available in Appendix I. 5.2. Causal-JEPA for Efficient World Modeling Task and Evaluation. To evaluate world model learning in control-oriented setting, we use the PushT (Bekris et al., 2025) environment, robotic manipulation benchmark involving contact-rich object interactions in which an agent must reason about object dynamics and contact effects over time to achieve goal-directed behavior. Following the planning pipeline introduced in Zhou et al. (2025), at time step planning is performed by solving finite-horizon optimal control problem in the latent object-centric state space induced by the learned world model. Given the current history window ST , we optimize sequence of future actions at:t+H1 over planning horizon as t:t+H1 arg min at:t+H1 (cid:13) ˆSt+H Sg (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 , (7) where Sg denotes the latent representation of the desired goal state. This objective encourages plans whose longterm predicted outcome matches the desired goal, without imposing intermediate costs. The optimization problem in Eq. equation 7 is solved using the Cross-Entropy Method (CEM). Planning is performed repeatedly in model-predictive control (MPC) fashion as new observations become available. task is considered successful 6 196 DINO-WM DINO-WM-Reg. 91.33 88.00 6 128 OC-DINO-WM (ref.) OC-JEPA C-JEPA (+JEPA) (+Mask) 60.67 76.00 +15.33 88.67 +28. when the distance between the actual system state and the desired goal state in the original (non-latent) state space falls below predefined threshold. We use fixed number of four object slots across all Push-T experiments. Further details are in Appendix G. Baseline Models. We construct series of world model baselines based on DINO-WM and compare them to VideoSAUR-based C-JEPA. This allows all models, including C-JEPA, to start from the same frozen DINOv2 embeddings and differ only in the predictor architecture and its inputs, enabling controlled comparison. We defer additional details to Appendix H. DINO-WM (Zhou et al., 2025) operates on patch-level representations and employs autoregressive causal transformer predictor. DINO-WM (reg.) (Darcet et al., 2024) uses DINOv2with-register backbone with the same patch-based predictor, isolating the effect of representation. OC-DINO-WM applies an object-centric encoder on top of DINOv2 patch embeddings, while keeping the DINO-WM predictor and objective unchanged, isolating the effect of representation alone. OC-JEPA is history-unmasked variant of C-JEPA, masking only future tokens. Results. Table 3 highlights clear progression from patchbased to object-centric world modeling. The patch-based DINO-WM achieves the highest success rate, but at the cost of operating over large token space. Replacing patch tokens with object-centric slots in OC-DINO-WM reduces the latent token space to 1.02%, but results in pronounced performance degradation, showing that object-centric representations alone do not suffice for accurate long-horizon prediction. Introducing JEPA-style predictor in OC-JEPA partially recovers performance while preserving the efficiency of object-centric representations, indicating the advantage of joint latent prediction over autoregressive objectives. Finally, C-JEPA closes the performance gap with DINOWM (Zhou et al., 2025) by introducing object-level masking Causal-JEPA: Learning World Models through Object-Level Latent Interventions tions is provided in Appendix K. Assumption 1 (Temporally Directed Predictive Dependencies). Object-level state transitions are governed by timedirected predictive dependencies. The future state of an object is determined by past object observations and auxiliary variables without requiring instantaneous causal effects within the same time step. Assumption 2 (Shared Transition Mechanism). Latent state transitions are governed by shared mechanism. Although object states vary across time and episodes, the conditional distribution of future states given finite history remains invariant across trajectories. Assumption 3 (Object-Aligned Latent Representation). Each slot corresponds to coherent object-level state variable and these representations provide sufficient abstraction for reasoning about object dynamics. Assumption 4 (Finite-History Sufficiency). finite history window of length Th suffices to predict future object states under the predictive dependencies of the system. Note. We emphasize that we do not assume causal sufficiency. It is important to permit unobserved confounders when using object-centric representations, as they do not assume full observability of the underlying system. We also do not assume first-order Markov process at the object level. Future object states may depend on history of past states rather than solely on Zt, as velocity cannot be inferred from single observation. Furthermore, we do not assume global sparsity of the underlying dynamics, which has been shown to be overly restrictive (Pandaram et al., 2025). 6.2. Interaction-Inducing Inductive Bias This subsection analyzes the inductive bias induced by predicting missing object representations from partial observations over temporal window . We show that masked history prediction forces the predictor to rely on minimal sufficient set of contextual variables beyond the target object itself, which we formalize as an influence neighborhood. Definition 1 (Influence Neighborhood under Masked Completion). Let ZT denote the set of entity tokens over the history window, and let (i) denote all variables in ZT excluding the history of object i, except for minimal identity anchor used to preserve object identity. For masked object state zi at time t, the influence neighborhood Nt(i) is minimal sufficient subset Nt(i) (i) Nt(i)(cid:1) . (i) zi = p(cid:0)zi such that (8) (cid:16) (cid:17) T Minimality is defined in the sense that no strict subset of Nt(i) satisfies this condition. Figure 3. Comparison of auxiliary variable integration methods. as latent intervention during training. Despite using only 1.02% of the latent tokens, it achieves performance comparable to patch-based world models. Because predictor rollouts dominate the computational cost in model-based planning, this reduction directly translates to efficiency gains. Under identical settings on single L40s GPU, C-JEPA achieves over 8 faster planning, requiring 673 seconds on average across three seeds to evaluate 50 trajectories, compared to 5,763 seconds for DINO-WM. Overall, C-JEPA reconciles the efficiency of object-centric world models with the performance of image-based approaches by combining joint latent prediction with object-level masking. Action and Proprioception as Auxiliary Nodes. As discussed in Sec. 4.1, C-JEPA treats auxiliary variables, such as actions and proprioceptive signals, as separate entities that condition the predictor. In contrast, DINO-WM (Zhou et al., 2025) concatenates these signals with patch representations, incorporating control inputs into the visual representation. As shown in Fig. 3, modeling auxiliary variables as separate entities consistently outperforms concatenation into object latents, justifying their treatment as explicit variables. Ablation on Masking Strategy. In addition to objectlevel masking, we evaluate tokenand tube-level masking across both reasoning and control tasks. While tokenor tube-level masking can occasionally match the performance of object-level masking depending on the task, object-level masking provides more structured and controllable inductive bias. This results in more stable training behavior and superior performance on interaction-heavy queries. We provide comprehensive comparative analysis in Appendix J. 6. Theoretical Perspective: Causal Inductive"
        },
        {
            "title": "Bias of Latent Interventions",
            "content": "This section analyzes why C-JEPA yields stronger forward prediction than future-only world modeling objectives. 6.1. Assumption We first set practically motivated assumptions that clarify the scope of our analysis. Detailed discussion of the assumpThe influence neighborhood captures the smallest set of contextual variables that must be consulted in order to re7 Causal-JEPA: Learning World Models through Object-Level Latent Interventions cover the state of masked object under partial observability. The concept of influence neighborhoods is closely related to notions of Markov blankets and sufficient statistics (Pearl, 2009; Koller & Friedman, 2009), and we discuss these connections in Appendix B.2. We define the influence neighborhood for single masked object state, and masking multiple objects during training corresponds to jointly learning several such conditional dependencies in parallel, without altering the definition. Theorem 1 (Interaction Necessity under Masked History Completion). Consider the masked history prediction loss from Eq. 6 for object at time t: history := Li (cid:104)(cid:13) (cid:13)ˆzi zi (cid:105) , (cid:13) 2 (cid:13) 2 (9) is computed from the observable history (i) where ˆzi following Eq. 4, and the expectation is taken with respect to the conditional distribution of zi . Under Assumptions 14 and Definition 1, the optimal predictor that minimizes equation 9 satisfies given (i) T (cid:104) = ˆzi zi (cid:105) (cid:12) (cid:12) (i) (cid:12) = E(cid:2)zi (cid:12) (cid:12) Nt(i)(cid:3) . (10) Consequently, any predictor that fails to utilize information in Nt(i) cannot attain the minimum achievable expected reconstruction error under masked completion. As masked zi is not directly observable, the optimal predictor must rely on other variables to reduce uncertainty. By Definition 1, Nt(i) is the minimal sufficient subset that preserves the conditional distribution of zi t. Any predictor that ignores variables in Nt(i) therefore incurs strictly higher expected loss. complete proof is provided in Appendix L.1. Corollary 1 (Discovery of Intervention-Stable Influence Neighborhoods). Optimizing Lmask under repeated exposure to diverse object-level masking encourages statedependent attention patterns that align with the influence neighborhood Nt(i). This can be interpreted as soft, local relational structure capturing predictive influence. Corollary 1 is closely related to prior work on invariant causal prediction (Peters et al., 2016) and invariant risk minimization (Arjovsky et al., 2020), which study predictive dependencies that remain stable across interventions or environments. In this sense, object-level masking in C-JEPA can be viewed as inducing collection of latent interventions, under which intervention-stable influence neighborhoods emerge. Further discussion is provided in Appendix L.2. Remark 2. Influence neighborhoods provide practical alternative to full causal discovery in complex dynamical systems. In realistic settings with latent confounders and high-dimensional state spaces, identifying true causal parents may be infeasible or ill-defined (Seitzer et al., 2021; 8 Ke et al., 2021). In contrast, intervention-stable influence neighborhoods capture the variables that are necessary for prediction under controlled perturbations, making them useful for reasoning, planning, and model-based control. C-JEPA identifies such neighborhoods through its learning objective, without requiring an explicit causal graph. Remark 3 (Transfer of Bidirectional Training to Forward Prediction). Due to bidirectional masked prediction during training, the influence neighborhood Nt(i) abstracts away temporal direction. Although the true system dynamics evolve via forward transition st+1 = Ψ(st), Nt(i) can be interpreted as direction-agnostic interaction structure that abstracts away edge direction and captures variables jointly informative about an objects state, independent of whether this information arises from past or future observations. As result, minimizing Lmask induces latent representations that encode interaction constraints invariant to the direction of information flow. Under Assumption 2, predicting an objects state during training depends on the same variables that govern its forward dynamics from past observations, irrespective of temporal direction. 7. Conclusion In this paper, we presented C-JEPA, which combines joint embedding prediction with object-level masking to introduce the causal inductive bias directly through the learning objective. To our knowledge, this is the first work to integrate JEPA with object-centric world modeling. By treating object masking as latent, counterfactual-like interventions, C-JEPA learns interaction-aware dynamics without relying on reconstruction losses or task-specific supervision. Empirically, C-JEPA yields strong gains in visual reasoning, with especially large improvements on counterfactual questions. In predictive control, it enables highly efficient planning with orders of magnitude fewer tokens than patch-based world models, while achieving comparable performance. We also identify several limitations. Performance depends on the quality of the object-centric encoder, which can limit the performance ceiling. Moreover, while we formally characterize influence neighborhoods, we do not directly validate them on datasets with explicit temporal causal graphs, leaving this to future work. Another promising direction is jointly refining object-centric encoders using strong pretrained backbones without representational collapse (Ðukic et al., 2025), as well as evaluating C-JEPA in more complex environments with richer interactions. Overall, this work establishes object-level masking as principled and effective mechanism for introducing causal inductive bias into efficient, interaction-aware world models. Causal-JEPA: Learning World Models through Object-Level Latent Interventions"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to improve the efficiency and interaction-aware learning of task-agnostic world models and their applications. While advances in world modeling may have broad downstream impacts in areas such as robotics, simulation, and decision-making systems, the methods proposed here are primarily foundational and methodological. We do not foresee any immediate negative societal consequences specific to this work beyond those generally associated with the deployment of learned models, and therefore do not highlight any particular impacts here."
        },
        {
            "title": "References",
            "content": "Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, Invariant risk minimization, 2020. URL https: D. //arxiv.org/abs/1907.02893. Assran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y., and Ballas, N. Self-supervised learning from images with joint-embedding predictive architecture. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15619 15629, 2023. doi: 10.1109/CVPR52729.2023.01499. Assran, M., Bardes, A., Fan, D., Garrido, Q., Howes, R., Mojtaba, Komeili, Muckley, M., Rizvi, A., Roberts, C., Sinha, K., Zholus, A., Arnaud, S., Gejji, A., Martin, A., Hogan, F. R., Dugas, D., Bojanowski, P., Khalidov, V., Labatut, P., Massa, F., Szafraniec, M., Krishnakumar, K., Li, Y., Ma, X., Chandar, S., Meier, F., LeCun, Y., Rabbat, M., and Ballas, N. V-jepa 2: Self-supervised video models enable understanding, prediction and planning, 2025. URL https://arxiv.org/abs/2506.09985. Aydemir, G., Xie, W., and Güney, F. Self-supervised objectcentric learning for videos. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. Balestriero, R., Assel, H. V., BuGhanem, S., and Maes, L. stable-pretraining-v1: Foundation model research made simple, 2025. URL https://arxiv.org/ abs/2511.19484. Bardes, A., Garrido, Q., Ponce, J., Chen, X., Rabbat, M., LeCun, Y., Assran, M., and Ballas, N. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. Bekris, K., Hauser, K., Herbert, S., Yu, J., Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., Tedrake, R., and Song, S. Diffusion policy: Visuomotor policy learning via action diffusion. Int. J. Rob. Res., 44(1011): ISSN 0278-3649. doi: 16841704, September 2025. 10.1177/02783649241273668. URL https://doi. org/10.1177/02783649241273668. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging prop2021 erties in self-supervised vision transformers. IEEE/CVF International Conference on Computer Vision (ICCV), pp. 96309640, 2021. URL https: //api.semanticscholar.org/CorpusID: 233444273. Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. ViIn The Twelfth Insion transformers need registers. ternational Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=2dnO3LLiJ1. Ding, D., Hill, F., Santoro, A., Reynolds, M., and Botvinick, M. Attention over learned object embeddings enables complex visual reasoning. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=lHmhW2zmVN. Fan, D., Wang, J., Liao, S., Zhu, Y., Bhat, V., SantosVillalobos, H., V, R., and Li, X. Motion-guided masking for spatiotemporal representation learning. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 55965606, 2023. doi: 10.1109/ICCV51070. 2023.00517. Feng, F., Lippe, P., and Magliacane, S. Learning interactive world model for object-centric reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https: //openreview.net/forum?id=E0cjqfM55C. Ferraro, S., Mazzaglia, P., Verbelen, T., and Dhoedt, B. FOCUS: Object-centric world models for robotic manipulation. In Intrinsically-Motivated and Open-Ended Learning Workshop @NeurIPS2023, 2023. URL https: //openreview.net/forum?id=RoQbZRv1zw. Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips. cc/paper_files/paper/2018/file/ 2de5d16682c3c35007e4e92982f1a2ba-Paper. pdf. Hafner, D., Lillicrap, T. P., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Causal-JEPA: Learning World Models through Object-Level Latent Interventions Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 25552565. PMLR, 2019. URL http://proceedings.mlr. press/v97/hafner19a.html. Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse control tasks through world models. Nature, 640 (8059):647653, 2025. ISSN 1476-4687. doi: 10.1038/ s41586-025-08744-2. URL https://doi.org/10. 1038/s41586-025-08744-2. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1600016009, June 2022. Ke, N. R., Didolkar, A. R., Mittal, S., Goyal, A., Lajoie, G., Bauer, S., Rezende, D. J., Bengio, Y., Pal, C., and Mozer, M. C. Systematic evaluation of causal discovery in visual model based reinforcement learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum? id=gWIbXsrtOCc. Kim, K., Nam, H., Hwang, I., and Lee, S. Towards causal representation learning with observable sources as auxiliaries, 2025. URL https://arxiv.org/abs/ 2509.19058. Kipf, T., van der Pol, E., and Welling, M. Contrastive learning of structured world models. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=H1gax6VtDB. Kipf, T., Elsayed, G. F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski, R., Dosovitskiy, A., and Greff, K. Conditional Object-Centric Learning from Video. In International Conference on Learning Representations (ICLR), 2022. Koller, D. and Friedman, N. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning. The MIT Press, 2009. ISBN 0262013193. LeCun, Y. path towards autonomous machine intelligence version 0.9.2, 2022-06-27. 2022. URL https: //api.semanticscholar.org/CorpusID: 251881108. Lei, A., Schölkopf, B., and Posner, I. SPARTAN: sparse transformer world model attending to what matters. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https: //openreview.net/forum?id=uS5ch7GjZ4. Li, X., Huang, C., Li, C.-L., Malach, E., Susskind, J., Thilak, V., and Littwin, E. Rethinking jepa: Compute-efficient video ssl with frozen teachers, 2025. URL https:// arxiv.org/abs/2509.24317. Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf, T. Object-centric learning with slot attention. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Maes, L., Lidec, Q. L., Haramati, D., Massaudi, N., Scieur, D., LeCun, Y., and Balestriero, R. stable-worldmodelv1: Reproducible world modeling research and evaluation, 2026. URL https://arxiv.org/abs/2602. 08968. Mosbach, M., Ewertz, J. N., Villar-Corrales, A., and Behnke, S. SOLD: Slot object-centric latent dynamics models for relational manipulation learning from pixels. In Fortysecond International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=XOUpHJPYRX. Nakano, A., Suzuki, M., and Matsuo, Y. Faster slot decoding using masked transformer. In NeurIPS 2024 Workshop on Compositional Learning: Perspectives, Methods, and Paths Forward, 2024. URL https://openreview. net/forum?id=n30V6R7Nuf. Nam, H. Scadi: Self-supervised causal disentanglement in latent variable models, 2023. URL https://arxiv. org/abs/2311.06567. Ng, I., Zhu, S., Fang, Z., Li, H., Chen, Z., and Wang, J. Masked Gradient-Based Causal Structure Learning, pp. 424432. doi: 10.1137/1.9781611977172.48. URL https://epubs.siam.org/doi/abs/10. 1137/1.9781611977172.48. Nishimoto, Y. and Matsubara, T. Object-centric world models for causality-aware reinforcement learning, 2025. URL https://arxiv.org/abs/2511.14262. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https:// openreview.net/forum?id=a68SUt6zFt. Featured Certification. 10 Causal-JEPA: Learning World Models through Object-Level Latent Interventions Pandaram, M., Hollenstein, J., Drexel, D., Tosatto, S., Rodríguez-Sánchez, A., and Piater, J. Dynamic sparsity: Challenging common sparsity assumptions for learning world models in robotic reinforcement learning benchmarks, 2025. URL https://arxiv.org/ abs/2511.08086. Pearl, J. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009. ISBN 052189560X. Peters, J., Bühlmann, P., and Meinshausen, N. Causal inference by using invariant prediction: Identification and confidence intervals. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):9471012, 10 2016. ISSN 1369-7412. doi: 10.1111/rssb.12167. URL https://doi.org/10.1111/rssb.12167. Seitzer, M., Schölkopf, B., and Martius, G. Causal influence detection for improving efficiency in reinforcement learning. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393. Singh, G., Wu, Y.-F., and Ahn, S. Simple unsupervised object-centric learning for complex and naturalistic videos. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=eYfIM88MTUE. Tessler, C., Guo, Y., Nabati, O., Chechik, G., and Peng, X. B. Maskedmimic: Unified physics-based character control through masked motion inpainting. ACM Transactions on Graphics (TOG), 2024. Thoker, F. M., Jiang, L., Zhao, C., and Ghanem, B. SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning . In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 84388449, Los Alamitos, CA, USA, June 2025. IEEE Computer Society. doi: 10.1109/CVPR52734.2025.00790. URL https://doi.ieeecomputersociety.org/ 10.1109/CVPR52734.2025.00790. Tong, Z., Song, Y., Wang, J., and Wang, L. Videomae: masked autoencoders are data-efficient learners for selfsupervised video pre-training. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Ðukic, N., Lebailly, T., and Tuytelaars, T. Object-centric pretraining via target encoder bootstrapping. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=7d2JwGbxhA. Villar-Corrales, A., Wahdan, I., and Behnke, S. Objectcentric video prediction via decoupling of object dynamics and interactions. In IEEE International Conference on Image Processing, ICIP 2023, Kuala Lumpur, Malaysia, October 8-11, 2023, pp. 570574. IEEE, 2023. doi: 10. 1109/ICIP49359.2023.10222810. URL https://doi. org/10.1109/ICIP49359.2023.10222810. Wu, Y.-F., Yoon, J., and Ahn, S. Generative video In Meila, transformer: Can objects be the words? M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1130711318. PMLR, 1824 Jul 2021. URL https:// proceedings.mlr.press/v139/wu21h.html. Wu, Z., Dvornik, N., Greff, K., Kipf, T., and Garg, A. Slotformer: Unsupervised visual dynamics simulation with object-centric models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=TFbwV6I0VLg. Xiao, Y., Tang, Z., Wei, P., Liu, C., and Lin, L. Masked images are counterfactual samples for robust fine-tuning. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2030120310, 2023. URL https://api.semanticscholar. org/CorpusID:257364775. Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., and Wang, J. Causalvae: Disentangled representation learning via neural structural causal models. pp. 95889597, 06 2021. doi: 10.1109/CVPR46437.2021.00947. Yi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A., and Tenenbaum, J. B. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=HkxYzANYDB. Zadaianchuk, A., Seitzer, M., and Martius, G. Objectcentric learning for real-world videos by predicting temporal feature similarities. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. Zholus, A., Ivchenkov, Y., and Panov, A. Factorized In world models for learning causal relationships. ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality, 2022. URL https: //openreview.net/forum?id=BCGfDBOIcec. Zhou, G., Pan, H., LeCun, Y., and Pinto, L. DINOWM: World models on pre-trained visual features enable zero-shot planning. In Forty-second International Causal-JEPA: Learning World Models through Object-Level Latent Interventions Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=D5RNACOZEI. 12 Causal-JEPA: Learning World Models through Object-Level Latent Interventions A. Notations Table A1. Notations used throughout the paper. Notation Description }, si Rd St = {s1 Xt RHW f ϕ , . . . , sN Th, Tp = {t Th + 1, ..., t} = {t Th + 1, ..., + Tp} Ut at pt {1, . . . , } eτ = {si Sm M} Sc = St Sm Zt = {St, Ut} zi zi Zt (i) (i) t,mask Nt(i) ˆzi t+1 ˆzt+1:t+Tp 1[ ] Pixel-level image frame observed at time Object-centric encoder mapping pixels to slot representations Latent predictor mapping past states to future states Linear projector used to construct masked object tokens Set of object-centric slot representations at time Fixed number of object slots Dimensionality of each slot representation History window length and prediction horizon length respectively History index set Full historyfuture prediction interval Set of exogenous variables at time Action taken at time Proprioceptive signal at time Index set of masked objects Learnable embedding combined with temporal positional encoding Set of masked object states at time Set of unmasked object states Set of entity tokens at time Masked token for object at time Slot token after applying object-level masking Set of slot tokens after masking at time Available context variables excluding object at time Masked context variables under object-level masking Influence neighborhood of object (single-step form) Predicted future state of object at time + 1 Predicted future object states over the horizon Indicator function (equals 1 if condition holds, 0 otherwise) B. Discussion B.1. Interpretation of Object-Level Masking as Latent Intervention We clarify how object-level masking in C-JEPA can be interpreted as latent intervention on observability. Crucially, masking alters the information available to the predictor without modifying the underlying data-generating or transition mechanism. Lemma 1 (Latent Intervention via Object-Level Masking). Object-level masking removes direct access to the current latent state of masked object zi while preserving access to other object states and auxiliary variables. As result, prediction of the future object state is conditioned on restricted set of observable variables, Zt = (cid:0)Zt {zi t}(cid:1) {zi t}, where zi while leaving the transition dynamics unchanged. is masked token that does not reveal the current value of zi t. This intervention restricts observability at time Discussion. At given time step, object-level masking replaces the current latent state of target object with masked token, thereby restricting the predictors access to that object while leaving all other inputs unchanged. 13 Causal-JEPA: Learning World Models through Object-Level Latent Interventions From this perspective, object-level masking acts as latent intervention on observability. Rather than intervening on the data-generating process itself, masking induces counterfactual-like prediction problems by forcing the model to reason about future states under controlled information removal. This interpretation aligns with viewing masking as an intervention in latent space, enabling the learning of interaction-dependent predictive structure without requiring multiple environments or explicit causal graph manipulation. B.2. Relation to Classical Causal Concepts The notion of an influence neighborhood is related to several classical concepts in causal inference and probabilistic modeling, but is deliberately defined in weaker and more operational sense, tailored to masked prediction in object-centric world models. Relation to Markov blankets and causal parents. In probabilistic graphical models (Koller & Friedman, 2009), the Markov blanket of variable is the minimal set of variables that renders it conditionally independent of all others. Similarly, in structural causal models (Pearl, 2009), the causal parents of variable represent its direct causes. Both notions aim to characterize minimal sufficient sets under strong assumptions, including access to correct causal graph, causal sufficiency, and full observability. In contrast, influence neighborhoods are defined through the masked completion task in Definition 1. They characterize the minimal subset of observable variables within (i) when its own history is unobserved, except for minimal identity anchor. No assumptions are made about the identifiability of true causal graph. that are sufficient to predict masked object state zi As consequence, an influence neighborhood need not coincide with the set of causal parents. It may include variables that are causally downstream, correlated through latent confounders, or otherwise informative under partial observability. Influence neighborhoods should therefore be interpreted as predictively sufficient sets under masking, rather than as estimates of true causal mechanisms. Nevertheless, we argue that influence neighborhoods still introduce causal inductive bias, in the sense that forward prediction in the world model is constrained to be temporally directed, thereby favoring one-way predictive dependencies from past to future rather than arbitrary symmetric associations. Relation to invariant causal prediction and invariant risk minimization. Invariant causal prediction (ICP) (Peters et al., 2016) and invariant risk minimization (IRM) (Arjovsky et al., 2020) aim to identify predictive relationships that remain stable across environments or interventions. This emphasis on stability aligns conceptually with our focus on predictive dependencies that persist under object-level masking. However, existing methods typically assume access to multiple environments from data-generating process or externally specified interventions. In contrast, C-JEPA induces systematic variations in observability through object-level masking within single dataset. Influence neighborhoods are thus defined relative to latent observability interventions, rather than invariance across externally labeled environments or do-interventions on the data-generating process. Summary. Overall, influence neighborhoods generalize classical notions of minimal predictive sets to object-centric, temporally extended, and partially observed settings. They capture which variables are necessary for prediction under masked completion, without assuming causal sufficiency or aiming to recover fixed causal graph. This perspective enables principled analysis of interaction structure in realistic world models while remaining agnostic to causal identifiability. C. Dataset C.1. CLEVRER (Yi et al., 2020) CLEVRER consists of videos with 128 frames at spatial resolution of 480 320. The dataset is split into 10,000 training videos, 5,000 validation videos, and 5,000 test videos. Visual question answering (VQA) evaluation on the official test set is performed through the public evaluation server.1 The CLEVRER VQA benchmark includes four categories of questions: descriptive, counterfactual, explanatory, and predictive. Descriptive questions are evaluated per question, whereas the remaining categories are evaluated both per 1https://eval.ai/web/challenges/challenge-page/667/overview 14 Causal-JEPA: Learning World Models through Object-Level Latent Interventions question and per answer option. Due to current unavailability of the evaluation server, we report results on the validation set using the same evaluation protocol as the test set. All models are trained exclusively on the training split, and the validation set is treated as held-out test set during evaluation. No validation data is used for training or model selection. The subset of test-set results available to us exhibits trends consistent with those reported on the validation set. The number of objects present in scene is not fixed and may change over time, as objects can enter or leave the scene during video. Across the dataset, the maximum number of simultaneously visible objects is six. Accordingly, we use fixed set of seven object slots throughout, where one slot implicitly captures background or empty regions. C.2. Push-T (Bekris et al., 2025) The Push-T dataset consists of videos with variable temporal length, where each frame is rendered at resolution of 224 224. The training set contains 18,410 trajectories, and the validation set has 21 trajectories. The task involves simple planar manipulation scenario in which controllable agent, represented as blue circle, must push green T-shaped object into target configuration, gray T-shaped object. Success is defined by placing the green object within predefined distance threshold of the goal pose. Proprioceptive inputs have dimensionality four, and actions have dimensionality two. Each scene consistently contains three objects, and we therefore use four object-centric slots, including one slot reserved for background. D. Encoders Figure A1. Slot visualization from object-centric encoders. D.1. VideoSAUR (Zadaianchuk et al., 2023) VideoSAUR (Zadaianchuk et al., 2023) is video slot attention model built on frozen DINOv2 (Oquab et al., 2024) backbone that aggregates patch-level features into object-centric slots using SAVi-style grouping mechanism (Kipf et al., 2022). It additionally employs temporal similarity loss to encourage consistency across time, and we adopt it as our primary object-centric encoder. Common Setup. Across all experiments, VideoSAUR is trained following the standard protocol. Visual features are extracted using frozen DINOv2 ViT-S/14 backbone, producing 196 patch tokens per frame with dimensionality 384. Patch features are projected to 128-dimensional latent space and grouped into object-centric slots using Slot Attention with two iterations. The DINOv2 backbone remains frozen throughout training. Models are trained for 100k steps using Adam with base learning rate of 1 104, scaled linearly by batch size, and an exponential decay schedule with 2k warmup steps. Gradients are clipped at 0.05. The training objective combines feature reconstruction loss on DINOv2 features and temporal similarity loss. Unless otherwise specified, remaining training details follow the VideoSAUR setup used for YouTube-VIS by Wu et al. (2023). 15 Causal-JEPA: Learning World Models through Object-Level Latent Interventions Training Details on CLEVRER. For CLEVRER, frames are temporally subsampled with stride two. Training uses short clips of six frames, while validation uses clips of length ten. We use = 7 slots, batch size of 32, and set the temporal similarity loss weight to 0.3. Data normalization on ImageNet statistics and random horizontal flips are applied. Training Details on Push-T. For Push-T, we use clips of six frames with stride two for both training and validation, = 4 slots, and batch size of 64. The temporal similarity loss weight is increased to 0.25. Training is performed on Push-T dataset variant, where each object is moving in random manner to increase visual and dynamical diversity. D.2. SAVi (Kipf et al., 2022) SAVi (Kipf et al., 2022) is video slot attention model that learns object-centric representations by grouping frame-level visual features into fixed set of slots using iterative attention. It reconstructs input frames from slot representations and serves as standard object-centric encoder in prior world model and video reasoning work. Training Details on CLEVRER. For experiments requiring SAVi-based encoder, we use the stochastic SAVi model following SlotFormer (Wu et al., 2023), adopting the same architecture and training configuration. Training is performed on six-frame video clips resized to 64 64. The encoder maps RGB frames to 128-dimensional features using convolutional backbone, followed by Slot Attention with = 7 slots and two iterations. Each slot has dimensionality 128. We use the stochastic SAVi formulation with Gaussian prior of variance 0.01. Models are trained for 8 epochs using Adam with learning rate of 1 104 and cosine schedule with linear warmup over the first 2.5% of steps. Gradients are clipped at 0.05. The batch size is 32 for training and 64 for validation. The objective consists of an image reconstruction loss and KL regularization term weighted by 1 104. D.3. Auxiliary Variable Encoders. If available, actions and proprioceptive signals are treated as auxiliary variables and embedded separately from object-centric latents. Both are encoded using lightweight temporal embedders implemented as one-dimensional convolutional layers over the temporal axis. Each embedder maps raw inputs to 128-dimensional embedding space for C-JEPA, OC-JEPA, and OC-DINO-WM, while using 10-dimensional embedding space for DINO-WM and DINO-WM with Register. E. Predictors E.1. Design Choice We adopt ViT-style masked predictor rather than an autoregressive Transformer for modeling object-centric dynamics. In object-centric representations, object states do not evolve as independent first-order Markov processes, but are shaped by interactions that may span multiple time steps. Autoregressive predictors impose sequential dependency structure that conditions each prediction on previously generated tokens, which can bias learning toward local self-dynamics. In contrast, masked prediction allows the model to attend jointly to the entire history window and infer masked object states in parallel. This architectural choice aligns naturally with the C-JEPA objective, which requires reasoning over interaction-dependent context rather than sequential token generation. E.2. Implementation Details We maintain object latent states with slot dimensionality 128 throughout the whole experiment. Future prediction is performed by Transformer-based predictor with six layers, 16 attention heads, head dimension 64, and an MLP hidden dimension of 2048. Our framework is built on top of stable-pretraining (Balestriero et al., 2025) and stable-worldmodel (Maes et al., 2026). E.3. Training Details For Push-T, the model operates on temporal history window of size three, with frames subsampled using frame skip of five. At each time step, the model predicts single future latent state. For CLEVRER, the model operates on temporal history window of size six, with frames subsampled using frame skip of two, predicting ten future latent states. During training, object-level masking is applied by randomly masking between zero and two object slots for Push-T and between zero and four object slots for CLEVRER. 16 Causal-JEPA: Learning World Models through Object-Level Latent Interventions The model is trained for 30 epochs using the Adam optimizer with batch size of 256. The predictor, action encoder, and proprioceptive encoder use learning rate of 5 104. All experiments are conducted on single GPU, on pre-extracted object embeddings. F. Visual Question Answering F.1. ALOE (Ding et al., 2021) For visual question answering, we adopt ALOE (Ding et al., 2021), transformer-based model for reasoning over objectcentric video representations. ALOE operates directly on object-centric representations extracted from video frames. Given sequence of object embeddings over time and natural-language question, ALOE encodes the question using language transformer and represents object trajectories as temporally ordered tokens. stack of attention layers then performs joint reasoning over objects, time steps, and question tokens, allowing information to be dynamically routed based on the query. Importantly, ALOE does not rely on explicit symbolic programs or predefined causal graphs. Instead, relational, temporal, and counterfactual reasoning emerge from attention over object-level embeddings conditioned on the question. Training Details on CLEVRER. We train ALOE for visual question answering on the CLEVRER dataset using objectcentric slot trajectories extracted by the encoder. We follow SlotFormer (Wu et al., 2023) for the training configuration. Each training sample consists of object slot embeddings over 25 time steps, together with natural-language question and multiple answer choices. We use maximum of 6 objects per scene, with each slot represented by 128-dimensional feature vector. Object order is fixed across time to preserve temporal consistency. Questions and answer choices are tokenized with maximum lengths of 20 and 12, respectively. Object trajectories, question tokens, and answer tokens are concatenated into single token sequence and processed by transformer encoder with 12 layers, 8 attention heads, and feedforward dimension of 512. Learnable positional embeddings are used. The transformer operates on shared embedding space of dimension 16, followed by an MLP classifier with hidden size 128 for answer prediction. The model is trained using the Adam optimizer with an initial learning rate of 103 and cosine decay schedule with linear warmup over the first 10% of training steps. Training is performed for 400 epochs following SlotFormer. G. Model Predictive Control Planning Details We describe the implementation details of the model predictive control (MPC) pipeline used for goal-conditioned planning with learned world models. Planning is performed in the latent space induced by the learned world model. Given the current observation and goal observation, the planner optimizes sequence of future actions by rolling out the world model and minimizing goal-conditioned cost. Action optimization is carried out using the Cross-Entropy Method (CEM), and execution follows receding-horizon MPC scheme. In all experiments, we set the planning horizon to = 5 and use an action block size of = 5, Planning Configuration. resulting in total action sequence length of = = 25. The receding horizon is set equal to the planning horizon (R = 5), such that the full optimized action sequence is executed before replanning. The evaluation budget is fixed to 50 environment steps per episode, and the goal observation is defined as the state occurring 25 steps after the initial frame sampled from the dataset. Cost Function. The planning objective minimizes the cost between the predicted last state and the goal state in the latent space. The cost used is the l2-loss. Analogous costs are computed for prospective embeddings, which are then added to the original cost. rollout is considered successful if the final position error is less than 20 (within workspace range of (0, 450)) and the final orientation error is smaller than π/9. Action Optimization via CEM. Action sequences are optimized using the Cross-Entropy Method. At each replanning step, CEM samples 300 candidate action sequences from Gaussian distribution and evaluates their quality from the cost predicted by the world model. At each step, the 30 candidates with the lowest cost are kept as elites to update the Gaussian parameters (mean and variance) used for action sampling. The optimization procedure is repeated for 30 iterations, and the final mean of the distribution is used as the optimized action sequence. 17 Causal-JEPA: Learning World Models through Object-Level Latent Interventions Hungarian Matching for Object-Centric Models. For object-centric world models, the ordering of object slots is not often perfectly guaranteed to be consistent across time steps or between predicted and goal states. To ensure meaningful comparison between object-centric latent states, we apply Hungarian matching when computing rollout trajectories and planning costs. Specifically, at each predicted time step, object slots are matched to goal slots by minimizing the pairwise ℓ2 distance in latent space. The planning cost is then computed after alignment, using the matched object representations. This matching is applied consistently for all object-centric models during MPC planning and does not affect patch-based baselines, where token ordering is fixed. H. Baselines Figure A2. Diagram of DINO-WM (Zhou et al., 2025), OC-DINO-WM, and OC-JEPA SlotFormer (Wu et al., 2023) We reproduce SlotFormer on CLEVRER using the best-performing configuration reported in the original work. Training is performed on object-centric slots extracted by pretrained SAVi (Kipf et al., 2022) encoder. Each training sample consists of 16 frames in total, including 6 burn-in frames and 10 rollout frames, with temporal subsampling by factor of two. The dynamics model is implemented as Transformer-based rollouter with four layers, eight attention heads, and model dimension of 256. Sinusoidal temporal positional encodings are applied, while no slot positional encodings are used. Training is conducted for 30 epochs (approximately 450k steps) using the Adam optimizer with learning rate of 2 104. cosine learning-rate schedule with linear warmup over the first 5% of training steps is applied. No weight decay or gradient clipping is used. We use batch size 32 per GPU, using two Nvidia RTX A6000 GPUs. The default training objective includes both slot-level reconstruction loss and image reconstruction loss, each weighted equally. Image reconstruction is performed using CNN decoder initialized from pretrained SAVi checkpoint. All images are resized to resolution of 64 64. OCVP-Seq (Villar-Corrales et al., 2023) We reproduce OCVP on CLEVRER using the official implementation and training protocol. OCVP is also trained on top of pretrained SAVi (Kipf et al., 2022) slot representations, which are kept fixed throughout training. Video clips are constructed from CLEVRER sequences resized to spatial resolution of 64 64, with frames temporally subsampled by factor of two. Each training clip consists of 6 context frames followed by 10 future frames to predict, resulting in total sequence length of 16 frames. OCVP-Seq implemented as Transformer with 2 layers, 4 attention heads, and hidden dimension of 256. The internal token dimension is set to 128, and residual connections are used around the predictor. Autoregressive prediction is performed with an input buffer size of 6 slots. The model is trained for 30 epochs using the AdamW optimizer with learning rate of 1 104 and no weight decay. cosine learning-rate schedule with warmup of 5 epochs is applied, and the minimum learning rate is set to 1 106. Training uses batch size of 64 and mixed-precision on 3 Nvidia RTX A6000 GPUs. The training objective consists of mean squared error loss on predicted slot representations. Validation is performed at the end of every epoch, and all reported results follow the same evaluation protocol as other object-centric baselines. DINO-WM (Zhou et al., 2025) and DINO-WM with Register (Darcet et al., 2024) We train DINO-WM and its register-augmented variant on the Push-T manipulation task following the standard DINO-WM training protocol. Both models share the same architecture and hyperparameters, differing only in the use of register tokens during visual encoding for DINO-WM with Register. Causal-JEPA: Learning World Models through Object-Level Latent Interventions Table A2. VQA accuracy under varying numbers of masked objects. (V) uses VideoSAUR and (S) uses SAVi encoders. Performance changes relative to the unmasked baseline are indicated by arrows. Model Average per que. (%) Counterfactual (%) Explanatory (%) Predictive (%) Descriptive (%) per opt. per que. per opt. per que. per opt. per que. OC-JEPA (V) 0 82.79 79.53 47. 92.88 80.58 86.15 75.04 89.59 C-JEPA (V) 1 2 3 4 83.95 1.16 84.56 1.77 87.61 4.82 89.40 6.61 80.34 0.81 49.67 1.99 93.41 0.53 82.22 1.64 88.28 2.13 78.58 3.54 90.39 0.80 80.61 1.08 50.25 2.57 93.53 0.65 82.54 1.96 88.68 2.53 79.56 4.52 91.02 1.43 86.49 6.96 63.60 15.92 95.35 2.47 87.30 6.72 91.16 5.01 83.86 8.82 91.98 2.39 88.67 9.14 68.81 21.13 96.62 3.74 90.74 10.16 93.03 6.88 86.93 11.89 92.84 3.25 OC-JEPA (S) 0 77.28 76. 41.10 91.20 76.04 83.48 70.51 84. C-JEPA (S) 1 2 3 4 78.39 1.11 83.88 6.60 79.02 1.74 73.28 4.00 77.26 0.57 43.13 2.03 91.49 0.29 77.14 1.10 83.67 0.19 70.19 0.32 85.09 1.04 85.16 8.47 60.19 19.09 95.34 4.14 87.27 11.23 87.46 3.98 77.25 6.74 87.81 3.76 77.78 1.09 43.77 2.67 92.51 1.31 79.94 3.90 81.42 2.06 67.18 3.33 85.62 1.57 73.55 3.14 34.06 7.04 89.85 1.35 72.72 3.32 77.23 6.25 59.89 10.62 80.88 3.17 Input frames are tokenized into non-overlapping 16 16 patches. Frames are temporally subsampled with stride of five. Each training sequence consists of history window of three frames followed by one prediction step, resulting in total sequence length of four frames. Visual observations are encoded using frozen DINOv2 (Oquab et al., 2024) backbone, producing patch-level embeddings that are passed to an autoregressive Transformer-based predictor. The predictor consists of 6 Transformer layers with 16 attention heads, feedforward dimension of 2048, and per-head dimensionality of 64. Dropout with rate 0.1 is applied within the predictor, while embedding dropout is disabled. Actions and proprioceptive signals are embedded using separate encoders as described in Appendix D.3 with embedding dimensionality 10 and provided as auxiliary inputs to the predictor. Models are trained for 10 epochs as training more epochs did not improve the performance and often degrades the performance. We use batch size of 64. Optimization is performed using Adam with learning rate of 5 104 for the predictor, action encoder, and proprioceptive encoder. All experiments use fixed random seed of 42. Unless otherwise specified, all remaining settings follow the original DINO-WM implementation. OC-DINO-WM OC-DINO-WM replaces the patch-based visual encoder of DINO-WM with an object-centric encoder based on VideoSAUR. VideoSAUR also operates on frozen DINOv2 ViT-S/14 backbone, ensuring compatibility with DINO-WM in terms of visual representation. All other components, including the predictor architecture, auxiliary input handling, and training protocol, are kept identical to DINO-WM. The model is trained for 30 epochs on Push-T. OC-JEPA OC-JEPA is variant of C-JEPA with history masking disabled (M = 0). This baseline isolates the effect of object-centric representations from that of the masking objective, allowing us to assess whether the performance gains of C-JEPA arise from object-centric encoding alone or from object-level history masking. OC-JEPA shares the same architecture and training setup as C-JEPA, differing only in the masking configuration. Details of the predictor architecture and masking scheme are provided in Appendix E. I. Full Results Tab. A2 shows VQA performance under different numbers of masked objects. Introducing object-level masking consistently improves performance over the unmasked baseline, with especially large gains on counterfactual questions. These results indicate that masking object histories provides meaningful training signal and strengthens interaction-dependent reasoning. Tab. A3 compares object-centric baselines using shared SAVi encoder. C-JEPA achieves the best overall performance across all question categories, with particularly strong gains on counterfactual and predictive questions. Notably, unlike prior approaches, C-JEPA attains these improvements without relying on reconstruction losses, suggesting that object-level masked prediction provides more effective supervisory signal for interaction reasoning. 19 Causal-JEPA: Learning World Models through Object-Level Latent Interventions Table A3. Baseline comparison using SAVi encoder. For models with and without reconstruction, arrows indicate differences relative to the reconstruction-based variant. Model Average per que. (%) Counterfactual (%) Explanatory (%) Predictive (%) Descriptive (%) per opt. per que. per opt. per que. per opt. per que. SlotFormer (-) recon. loss 44.94 34.50 79.44 OCVP-Seq (-) recon. loss 80.09 3.02 83.11 OC-JEPA C-JEPA 77.28 83.88 79.28 55.62 23.66 11.10 36.19 67.70 25.14 27.89 53.07 52.52 31.14 23.83 46.96 54.67 30.55 47.29 92. 80.96 83.66 70.79 85.22 83.21 77.46 5.75 43.00 13.06 92.70 1.68 80.38 4.99 80.72 5.62 66.08 9.44 87.24 0.61 85. 86.34 75.52 87.85 56.06 94.38 76. 85.16 41.10 60.19 91.20 95.34 76. 87.27 83.48 87.46 70.51 77.25 84. 87.81 Table A4. Effect of masking strategies and masking budgets on CLEVRER performance. For tokenand tube-level masking, the budget is reported as percentage of masked tokens. For object-level masking (C-JEPA), the budget is reported as the number of masked objects out of total of seven. Masking Method M/7 Average per que. (%) Counterfactual (%) Explanatory (%) Predictive (%) Descriptive (%) Object Token Tube 1/7 2/7 3/7 4/ 14% 28% 42% 56% 14% 28% 42% 56% 83.95 84.56 87.61 89.40 85.69 87.83 84.43 89.32 86.62 87.87 88.94 89.46 per opt. per que per opt. per que per opt. per que 80.34 80.61 86.49 88. 83.46 87.46 80.69 88.74 84.05 85.42 88.10 89.25 49.67 50.25 63.60 68.81 56.92 65.82 51.08 68.88 57.83 61.71 67.21 69.81 93.41 93.53 95.35 96. 94.76 95.85 93.77 96.70 94.86 95.67 96.30 97.06 82.22 82.54 87.30 90.74 85.82 88.64 82.87 90.87 86.00 88.43 89.83 91.61 88.28 88.68 91.16 93. 86.81 89.60 86.46 90.31 88.77 89.18 91.05 90.68 78.58 79.56 83.86 86.93 76.13 80.80 75.54 82.20 79.56 80.01 83.50 82.88 90.39 91.02 91.98 92. 91.18 91.89 90.90 93.01 92.06 92.74 92.84 92.90 J. Exploring Different Masking Strategies We consider three masking strategies that differ in the structural unit being masked. Object-level masking. Entire object-centric slots are masked, requiring the model to infer the masked objects latent trajectory from the remaining objects. The masking budget is specified as the number of masked objects out of seven. Token-level masking. random subset of individual latent tokens is masked, following standard masked modeling practices. The masking budget is reported as the percentage of masked tokens. Tube-level masking. Contiguous spatio-temporal tubes of latent tokens are masked, enforcing temporal consistency within masked regions. The masking budget is again reported as the percentage of masked tokens. For tokenand tube-level masking, we match the masking budget to object-level masking by masking an equivalent proportion of latent tokens, and for tube masking we further set the number of masked tubes to correspond to the size of the object masking index set. Results. Tab. A4 compares object-level masking with tokenand tube-level masking under matched masking budgets on CLEVRER. Across masking strategies, performance generally improves as the masking budget increases, indicating that masking can act as an effective regularization signal. Rather than viewing object-, token-, and tube-level masking Causal-JEPA: Learning World Models through Object-Level Latent Interventions Table A5. Effect of masking strategies and masking budgets on CLEVRER performance. For tokenand tube-level masking, the budget is reported as percentage of masked tokens. For object-level masking (C-JEPA), the budget is reported as the number of masked objects out of total of four. M/4 1/4 (25%) 2/4 (50%) Object 88.67% 82.67% Token 84.67% 84.00% Tube 55.33% 5.33% as fundamentally different mechanisms, they can be interpreted as variants that differ primarily in granularity and mask shape. Since masking is applied within relatively small latent space in our setting, the performance differences between these strategies are correspondingly limited. However, tokenand tube-level masking exhibit higher sensitivity to the masking budget and lead to less consistent improvements. This is because, unlike object-level masking, random masking at the token or tube level can introduce unintended combinations of missing information, such as simultaneously masking multiple objects or all object tokens at given time step. As result, the induced prediction task may vary substantially across masking instances, making it harder to consistently enforce interaction-dependent reasoning. The same trend is reflected in control settings. As shown in Tab. A5 (Push-T), object-level masking maintains robust planning performance even at higher masking ratios, whereas tube-level masking leads to severe performance degradation under comparable budgets, and token-level masking provides limited or inconsistent benefits. These results indicate that masking entire objects, rather than spatiotemporal tubes or individual latent tokens, is critical for inducing meaningful interaction-aware learning signals without destabilizing prediction or control. K. Assumptions Assumption 1 (Temporally Directed Predictive Dependencies) Object-level state transitions are governed by time-directed predictive dependencies. The future state of an object is determined by past object observation and auxiliary variables without requiring instantaneous causal effects within the same time step. Discussion. This assumption rules out within-timestep causal cycles and provides well-defined temporal direction for interpreting predictive influence. It is consistent with standard discrete-time modeling, where causal effects are attributed to variables available up to time and manifest in states at later times. Importantly, the assumption does not prohibit the predictor from using same-time-step observations during state completion. Under object-level masking, the model may condition the reconstruction of masked token at time on other objects observed at the same t. Such within-step conditioning should be understood as completing missing information under partial observability, rather than positing instantaneous causal generation among objects. Accordingly, we define causal influence operationally through time-lagged predictive dependencies: variables that help predict future states from past observations and auxiliary inputs. This avoids over-interpreting attention weights as instantaneous causal edges while remaining compatible with attention-based predictors that mix information within window. Assumption 2 (Shared Transition Mechanism) Latent state transitions are governed by shared mechanism. Although object states vary across time and episodes, the conditional distribution of future states given finite history remains invariant across trajectories. Discussion. This is the standard stationarity assumption underlying learned world models: single predictor is trained to approximate the same conditional dynamics across episodes. It allows pooling experience to learn transferable structure and supports evaluation under rollouts that recombine histories and actions. The assumption is compatible with diverse trajectories because it constrains only the conditional distribution given history, not the marginal distribution of states. In practice, moderate nonstationarities (e.g., nuisance variations) are typically absorbed by the encoder and model capacity; severe regime shifts would require explicit conditioning on context or environment identifiers, which is outside our scope. Assumption 3 (Object-Aligned Latent Representation) Each slot corresponds to coherent object-level state variable and these representations provide sufficient abstraction for reasoning about object dynamics. 21 Causal-JEPA: Learning World Models through Object-Level Latent Interventions Discussion. This assumption is minimal requirement for object-level interventions to be meaningful: masking slot should correspond to removing access to an object-level variable rather than an arbitrary mixture of entities. We do not require perfect disentanglement, fixed semantics across scenes, or one-to-one correspondence with human-defined objects. Instead, we require sufficient stability so that slots behave as coherent carriers of object-level information over time, enabling the predictor to treat them as entities for dynamics modeling. This is consistent with common practice in slot-based perception. At the same time, it clarifies practical limitation: the achievable performance of object-centric world models is bounded by the fidelity of the underlying encoder, and violations of object alignment can weaken the intended intervention effect of masking. Assumption 4 (Finite-History Sufficiency) finite history window of length Th contains sufficient information for predicting future object states under the predictive dependencies of the system. Discussion. This assumption reflects the operational constraints of learning and planning, where the model conditions on bounded context window. It is weaker than first-order Markov assumption because it allows higher-order dynamics that require temporal context (e.g., inferring velocity from multiple frames). In practice, Th trades off information completeness against computational cost, and the appropriate choice depends on the time scale of interactions and the encoders ability to summarize motion cues. When the true system exhibits longer memory than Th, the predictor can still learn an approximation, but accuracy may saturate. Our analysis therefore characterizes the inductive bias induced by masked completion within the chosen window, which is the regime relevant to the implemented model. L. Theoretical Analysis L.1. Proof of Theorem 1 Proof. Fix an object index and time t. Let := zi denote the observable completion context. Consider any measurable predictor that maps to prediction ˆY = h(X). The (unconditional) MSE risk is Rd denote the target masked state, and let := (i) R(h) := E(cid:2)Y h(X)2 (cid:3) . 2 By the tower property, we can write R(h) = E(cid:2)E[Y h(X)2 norm and using E[Y m(X) X] = 0, we obtain the standard conditional risk decomposition: 2 X(cid:3) + m(X) h(X)2 2. 2 X(cid:3) = E(cid:2)Y m(X) E(cid:2)Y h(X)2 2 X](cid:3). Define m(X) := E[Y X]. Expanding the squared The first term does not depend on h, and the second term is minimized (pointwise in X) uniquely by choosing h(X) = m(X) almost surely. Thus the Bayes-optimal predictor for MSE satisfies h(X) = E[Y X] and hence (cid:104) = ˆzi (i) zi (cid:105) , which gives the first equality in Eq. equation 10. It remains to show that conditioning on (i) Definition 1, Nt(i) (i) and (cid:16) is equivalent to conditioning on the influence neighborhood Nt(i). By (cid:17) (i) zi = p(cid:0)zi Nt(i)(cid:1) . Therefore, for any integrable function φ, (cid:104) φ(zi t) (i) (cid:105) = E(cid:2)φ(zi t) Nt(i)(cid:3) a.s. Taking φ to be the identity function on Rd yields (cid:104) (i) zi (cid:105) = E(cid:2)zi Nt(i)(cid:3) , which gives the second equality in Eq. equation 10. Finally, let be any predictor that fails to utilize information in Nt(i). Then there exists set of contexts with nonzero probability on which h(X) = h(X) = E[Y X], implying m(X)h(X)2 2 > 0 on that set. By the decomposition above, R(h) > R(h), so cannot attain the minimum achievable expected reconstruction error under masked completion. 22 Causal-JEPA: Learning World Models through Object-Level Latent Interventions L.2. Justification of Corollary 1 We provide justification for Corollary 1, which states that optimizing the masked prediction objective induces attention patterns aligned with the influence neighborhood. Consider an attention-based predictor in which each predicted object state is computed by aggregating information from input variables through learned attention weights. Under object-level masking, Theorem 1 implies that variables outside the influence neighborhood Nt(i) are conditionally uninformative for predicting the masked state zi t. Consequently, assigning attention to such variables cannot reduce the Bayes risk. In an expressive attention-based model class, there therefore exist optimal solutions that concentrate attention on variables within Nt(i) while assigning negligible weight to uninformative variables. Repeated exposure to diverse masking patterns acts as collection of latent interventions, favoring predictive dependencies that remain stable across interventions. This gives rise to state-dependent and soft relational patterns aligned with the intervention-stable influence neighborhood. This interpretation is closely related to prior work on invariant causal prediction (Peters et al., 2016) and invariant risk minimization (Arjovsky et al., 2020), which study predictors whose dependencies remain stable across environments or interventions. In C-JEPA, object-level masking plays an analogous role by inducing intervention-stable predictive dependencies in latent space, without explicitly estimating causal graph."
        }
    ],
    "affiliations": [
        "Brown University",
        "Mila",
        "New York University",
        "Université de Montréal"
    ]
}