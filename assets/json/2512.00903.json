{
    "paper_title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "authors": [
        "Chaojun Ni",
        "Cheng Chen",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Wenzhao Zheng",
        "Boyuan Wang",
        "Tianrun Chen",
        "Guosheng Zhao",
        "Haoyun Li",
        "Zhehao Dong",
        "Qiang Zhang",
        "Yun Ye",
        "Yang Wang",
        "Guan Huang",
        "Wenjun Mei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 3 0 9 0 0 . 2 1 5 2 : r SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead Chaojun Ni1,2* Cheng Chen3* Xiaofeng Wang1,4* Zheng Zhu1* Wenzhao Zheng4 Boyuan Wang1 Tianrun Chen3 Guosheng Zhao1 Haoyun Li1 Zhehao Dong1,2 Qiang Zhang5 Yun Ye1 Yang Wang1 Guan Huang1 Wenjun Mei2 2Peking University 3Moxin (Huzhou) Technology Co., Ltd. 1GigaAI 4Tsinghua University 5X-Humanoid Project Page: https://Swiftvla.github.io"
        },
        {
            "title": "Abstract",
            "content": "VisionLanguageAction (VLA) models built on pretrained VisionLanguage Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances compact model with 4D understanding while preserving design efficiency. Specifically, our approach features pretrained 4D visual geometry transformer with temporal cache that extracts 4D features from 2D images. Then, to enhance the VLMs ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, set of learnable tokens trained with future prediction objective to generate unified representations for action generation. Finally, we introduce maskand-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 larger, achieving comparable performance on edge devices while being 18 faster and reducing memory footprint by 12. 1. Introduction VisualLanguageAction (VLA) models [6, 17, 2731, 33, 38, 43, 5658, 60, 63, 66, 72, 74, 8284] represent new *These authors contributed equally to this work. The corresponding authors for this research are: zhengzhu@ieee.org, tianrun.chen@kokoni3d.com, mei@pku.edu.cn. Figure 1. Large VLMs like PaliGemma-3B [3] excel in spatial reasoning over small VLMs [44], with correct answers in green and incorrect ones in red. This performance advantage allows π0 [6] based on it to achieve higher success rate, despite slower inference speed compared to the SmolVLA [54] based on small VLM. However, SwiftVLA enhances spatiotemporal dynamics for small VLA models while preserving the speed advantages. The success rate and speed are tested on the NVIDIA Jetson Orin [48]. paradigm in robotics, leveraging the representational and reasoning strengths of large, pretrained VisionLanguage Models (VLMs) [1, 3, 8, 11, 21, 51, 53, 74] to map naturallanguage instructions and visual observations directly to actions. Despite their promise, real-world deployment is hindered by significant obstacle: the massive parameter counts of foundation VLMs induce high inference latency and memory usage, which is prohibitive for real-time control on resource-constrained robotic platforms. Therefore, recent studies [2, 54, 68] have reduced model capacity by shrinking the size of VLMs or decreasing the number of network layers, enabling deployment on edge devices. However, merely compressing model capacity of1 Figure 2. (a) Using only 2D features as input to the VLM [6, 23], which results in limited spatiotemporal awareness. (b) Direct fusion approaches combine spatial and 2D features within large VLMs [4, 35, 81]. (c) Decoupled designs that introduce dedicated spatial branch [26, 55], causing large parameter overhead. (d) SwiftVLA leverages pretrained model [86] to extract 4D features and applies feature reconstruction objective to align 4D and 2D representations. In addition, Fusion Tokens and future prediction objective are introduced to strengthen cross-modal integration. The 4D inputs and auxiliary heads are removed at inference to maintain efficiency. ten weakens reasoning ability, making it difficult to capture the 3D spatial information that is crucial for VLAs to plan precise actions, leading to poor localisation and imprecise trajectories and lowering task success rates. As shown in Fig. 1, smaller VLM models such as SmolVLM-0.5B [44] significantly underperform in spatial reasoning tasks, such as answering the question What color is the bowl on the far left? compared to larger VLM models [3]. Therefore, while SmolVLA [44], based on SmolVLM-0.5B [44], exhibits significantly faster inference speeds than π0 [6] based on PaliGemma-3B [3], its task success rate is notably lower, as complex manipulation tasks often require stronger spatiotemporal reasoning and scene understanding capabilities. Therefore, recent works [4, 26, 35, 55, 81] have explored integrating 3D and 4D information [12, 15, 16, 19, 41, 42, 45, 47, 71, 79, 80] to enhance VLAs perception of complex environments. However, existing fusion approaches are still suboptimal for lightweight architectures. As shown in Fig. 2 (b), some methods [4, 32, 35, 50, 76, 81] directly fuse 3D features with 2D representations within large VLM. While this improves spatial awareness compared to Fig. 2 (a), which uses only 2D input, it has to rely on heavyweight VLMs to handle cross-modal fusion. To mitigate this dependency, other approaches [26, 55] (Fig. 2 (c)) decouple 3D processing from the VLM by introducing an additional branch. However, this design substantially increases parameter overhead, making it unsuitable for compact models. In summary, as shown in Fig. 2 (ac), existing approaches still fall short of effectively balancing the lightweight design of VLAs with the practical need for robust and reliable spatiotemporal perception. In this paper, we present SwiftVLA, lightweight VLA model built upon compact VLM [44], which incorporates 4D spatiotemporal information with minimal computational cost. As shown in Fig. 2 (d), SwiftVLA takes 4D representations as auxiliary inputs and employs reconstruction objective to learn spatiotemporal dynamics from 4D features, enabling the model to discard them during inference while maintaining performance comparable to full 4D inputs. Meanwhile, Fusion Tokens are introduced and supervised by future prediction objective to promote effective cross-modal fusion. Specifically, SwiftVLA integrates pretrained 4D visual geometry transformer [86] with temporal cache to convert streaming frames into 4D features incrementally. The cache enables feature reuse across frames and provides temporal context. Meanwhile, because 4D cues are derived directly from standard visual inputs, no additional sensors such as depth cameras or LiDAR are required. For efficient fusion of 2D and 4D features in compact VLM, we introduce learnable Fusion Tokens to unify representations across modalities. Their outputs are supervised by the robot end-effectors future trajectory to encourage task-relevant learning. Finally, we propose maskand-reconstruct strategy, where during training, SwiftVLA randomly masks either the 2D or 4D modality with certain probability and requires the action expert to reconstruct the masked features, which encourages the learning of geometryand dynamics-aware representations. This enables the model to achieve performance comparable to that with 4D inputs during inference, even without them, minimizing the overhead of 4D inputs while preserving spatiotemporal modeling capability. We validate SwiftVLA with experiments on simulated and real-world environments. SwiftVLA outperforms lightweight baselines and matches the performance of VLA models up to 7 larger. On edge devices, it maintains comparable performance while running 18 faster than the state-of-the-art baseline π0 and reducing memory by 12. The main contributions of this paper are summarized as follows: We propose SwiftVLA, method that integrates 4D spatiotemporal information into lightweight VLA model at minimal cost. SwiftVLA extracts 4D features and adopts mask-and-reconstruct training strategy that distills 4D knowledge into the VLA. This enables the model to maintain comparable performance to that with 4D inputs durFigure 3. The pipeline of the SwiftVLA. We first extract 2D and 4D features from input images. lightweight VLM [44] processes 2D and 4D features with Fusion Tokens to achieve cross-modal integration. The outputs of the Fusion Tokens are supervised by the robot end-effectors future trajectory. During training, we randomly mask either the 2D or the 4D features, and we require the action expert to reconstruct the masked features while learning to generate actions. We show the attention mask under random masking of the 4D features. In this case, 4D features are excluded from the VLM attention, and the model is required to reconstruct the 4D features from the others. ing inference while requiring only 2D inputs. We fuse 2D and 4D features in lightweight VLM via learnable Fusion Tokens, trained with supervision from the robot arms future end-effector trajectory to produce unified, action-aware representation. Extensive experiments in simulation and on real robots demonstrate that SwiftVLA achieves performance comparable to baseline that is 7 larger. On edge devices, it runs 18 faster and uses 12 less memory than π0. 2. Related Work 2.1. Lightweight VLA Models Recent advances in VLA models integrate VLM backbones with action modules through end-to-end training. OpenVLA [23] introduced 7B parameter model trained on public datasets [22, 59] to generate discrete action tokens. To overcome the limitations of tokenizing actions in continuous control, π0 [6] uses diffusion-based decoder to directly generate continuous actions. However, these models [6, 23] have large number of parameters, leading to high training costs and significant inference latency. To address this, several approaches have shifted toward lighter VLA designs. Based on OpenVLA [23], MiniVLA [2] replaces the backbone with smaller model [1], thereby reducing the total size to 1B parameters. TinyVLA [68] introduces diffusion policy decoder that directly generates continuous multi-step action sequences to avoid the high latency of autoregressive generation, and employs LoRA [20] for parameter-efficient fine-tuning. To further lighten the model, SmolVLA [54] uses pixel shuffle to limit each frames tokens and skips subset of VLM layers, ultimately compressing the parameter count to around 0.5B. However, to achieve model lightweighting, these methods typically rely on shrinking the backbone parameters, which leads to degradation in the VLA models spatial reasoning and fine-grained control capabilities. 2.2. 3D Perception in VLA Models 3D perception [14, 19, 39, 40, 51, 52, 60, 67, 70, 73, 77, 78] is crucial for enhancing robotic manipulation capabilities. Recent studies have attempted to directly incorporate 3D features into the VLM to enhance their geometric awareness, as shown in Fig. 2 (b). 3D-VLA [81] extracts spatial embeddings and encodes them into VLM embeddings to improve spatio-temporal reasoning. SpatialVLA [50] introduces 3D positional encoding and an adaptive action network into VLMs to improve spatial understanding. Evo0 [35] obtains 3D features by leveraging VGGT [62], injecting the 3D geometric features into the VLA. However, the domain gap between 2D pixels and 3D geometry is substantial, and directly injecting both into VLMs often requires larger VLMs for better alignment and fusion. Some approaches [9, 13, 85] also attempt to fine-tune VLMs for spatio-temporal reasoning, but this often relies on massive amounts of temporally annotated data, which are expensive to collect. Therefore, as shown in Fig. 2 (c), some methods adopt decoupled designs that introduce spatial branch. PointVLA [26] treats point clouds as auxiliary conditioning signals and decouples 3D processing from the 2D vision encoder, enabling the model to leverage geometric cues while preserving the integrity of pretrained 2D representations. GeoVLA [55] adopts parallel branches for multimodal inputs and leverages modality-specific experts to achieve fusion. However, these approaches focus only on 3D informa3 2D = {F t,v ing an image encoder [75], obtaining 2D }vS. Then, pretrained 4D visual geometry transformer [86] is applied to derive temporally-enhanced 4D features 4D. set of learnable Fusion Tokens Qf are then introduced to in4D, with proprioceptive embeddings Et 2D, teract with and language embeddings Et through lightweight VLM V, resulting in unified representation:"
        },
        {
            "title": "Z t",
            "content": "f = V(Qf , Et s, Et , 4D, 2D). (1) In the fused representation , the portion corresponding to the Fusion Tokens is decoded to predict the end-effector trajectory, with explicit supervision from ground-truth trajectories, thereby enabling the intermediate hidden states of to learn trajectory-aware cross-modal alignment. In parallel, the intermediate hidden states of V, denoted as {h(i) }, are employed as hierarchical conditional features for the action expert A. The action expert is formulated as conditional diffusion model. Given noise sample ϵ and conditioned on VLM features {h(i) }, it produces an action latent: (cid:16) ϵ (2) . = (cid:12) (cid:17) (cid:12) {h(i) (cid:12) } with two complementary heads: We decode the first predicts the diffusion noise for the action, and the second reconstructs the masked feature representations. The reconstruction head is trained with an auxiliary objective to improve cross-modal alignment and is discarded at inference time, ensuring lightweight design and fast inference. 3.2. Incremental 4D Feature Extraction Recent studies [4, 26, 35, 55, 81] show that integrating 3D information [46, 61, 62, 64, 86], such as depth maps and point clouds, can improve the spatial perception of VLA models. Such signals provide geometric cues that help models better reason about distance and object layout. Yet these methods often rely on additional sensors and underutilize VLAs temporal modeling capacity, resulting in limited spatio-temporal reasoning. Therefore, as shown in Fig. 4, we adopt pretrained 4D visual geometry transformer [86] consisting of an encoder, decoder, and temporal cache module, whose weights are kept frozen, that incrementally extracts 4D features from input images, avoiding reliance on extra sensors and leveraging spatial and temporal cues. At each time step t, for each view S, the observation ov is encoded into feature embedding using the encoder: = Encoder(ov t,v ). (3) We sequentially feed the encoded features into spatiotemporal decoder, which employs spatial and temporal attention to extract 4D representations following the view order S. During the temporal attention stage, the current feature 4 Figure 4. The process of 4D feature extraction. At each step, we sequentially process multi-view observations and load contextual information from the cache for temporal attention. The generated 4D features are updated to the cache and delivered to the VLM. tion, neglecting temporal dynamics, while increasing memory footprint and inference latency. More recently, 4DVLA [76] incorporates the temporal dimension into VLA modeling by leveraging history-similarity-based keyframe sampling strategy and generating 3D-aware spatial-visual tokens. While this method enhances spatiotemporal perception, sampling multiple frames introduces additional inference overhead. In contrast, SwiftVLA maintains lightweight design while injecting 4D cues at lower cost. 3. Method 3.1. Model architecture As shown in Fig. 3, SwiftVLA consists of two connected components: pretrained lightweight VLM [44] and an action expert. The VLM processes the input to extract both 2D and 4D features with pretrained 4D visual geometry transformer [86]. Meanwhile, Fusion Tokens are introduced into the VLM to better leverage both 2D and 4D features, supervised by end-effector trajectory prediction. Then, the VLM fuses the 2D and 4D features, Fusion Tokens, and other inputs to generate the intermediate hidden states, which serve as the condition for the action expert model to perform action prediction. Additionally, during training, we employ mask-and-reconstruction strategy in which the 2D or 4D features are masked so that they do not contribute to action generation, and the action expert is required to reconstruct the corresponding features. This strategy encourages the model to exploit cross-modal cues and enables the removal of 4D feature inputs during inference with minimal performance loss, thereby maintaining lightweight design. Specifically, at each time step t, given the ordered view set = [lef t, right, ront], the robot receives natural language instruction l, multi-view observations ot = {ov }vS, and the proprioceptive state st. From the raw images, we first extract 2D visual features from each view usembedding interacts with the temporal cache through crossattention, enabling the integration of temporal context. The temporal cache is then updated iteratively to generate 4D features in consistent view order. We initialize the temporal cache as t,0 = t1. For each {1, 2, 3} with the corresponding view = Sk, the decoding process is formulated as: (F t,v 4D , t,k) = Decoder(cid:0)CrossAttn(F t,v , t,k1)(cid:1). (4) After all views have been processed, the temporal cache is updated as = t,3. To maintain temporal efficiency, we adopt First-In-First-Out (FIFO) strategy for the cache, retaining only the most recent representations of the 4D features. Finally, the visual inputs to the VLM are defined as follows: 2D = {F t,v 2D }vS, 4D = t,f ront 4D . (5) 4D and t,right 4D The t,lef are only used to update the temporal cache to provide more comprehensive spatiotemporal context and are not provided to the VLM as 4D features, considering the training cost. 3.3. Fusion Tokens Many prior works [26, 35, 50, 81] attempt to feed 3D information into VLMs so that they can jointly understand 2D and 3D inputs. However, these approaches typically rely on heavyweight VLMs. Lightweight VLMs struggle to develop robust spatial reasoning and cannot effectively fuse multimodal inputs into coherent, 3D-aware latent space. To address this problem, we introduce Fusion Tokens, set of learnable tokens that interact with 2D features and 4D spatiotemporal features, and are directly supervised by the end-effectors future trajectory. The keys and values produced by the Fusion Tokens, together with those from other tokens, form the conditioning signal h(i) used by the action expert to generate action chunks. Specifically, Fusion Tokens interact with the aggregated multimodal token sequence composed of the 2D features 2D, the 4D features 4D, the language embeddings Et , and the state embeddings Et through cross-attention within the VLM V, yielding fused representation , as shown in Eq. 1. The fused representation serves as the perception output and is optimized via end-effector trajectory prediction: ˆτt = htraj (cid:0)Z f (cid:1) , Ltraj = ˆτt τt2 2 , (6) where htraj is predictor that decodes the trajectory, ˆτt is the predicted trajectory, and τt is the ground-truth. Using the end-effectors future trajectory as supervision, Fusion Tokens align multimodal features with spatiotemporal semantics, making h(i) more effective for action generation. 3.4. Mask and Reconstruct Strategy Although incorporating 4D features with Fusion Tokens into VLAs can significantly enhance their spatial reasoning capabilities, the resulting increase in parameters and computational overhead runs counter to the lightweight design principle of VLAs. Therefore, we propose maskand-reconstruction strategy that leverages 4D supervision signals during training to build geometry-aware representations, while discarding 4D feature inputs during inference to maintain model efficiency with minimal performance degradation. Our approach encourages the model to build geometry-aware representations through structured masking and reconstruction, thereby distilling rich spatial and temporal knowledge into the learned features. Next, we present the training and inference procedures in detail. Training. During training, we employ random masking strategy that, with certain probability, applies masks to either the 2D or 4D features. Under this setting, the VLA is required to predict actions based on the remaining modalities, while simultaneously reconstructing the masked features. As illustrated in Fig. 3, we visualize the masking operation applied to the 4D features during training. The gray and white blocks indicate the fixed visible and invisible tokens, respectively, while the pink blocks represent tokens that undergo random masking, turning originally visible tokens into invisible ones. Meanwhile, the reconstruction losses are defined as follows: L2D = (cid:13) L4D = (cid:13) (cid:13)h2D (cid:13)h4D (cid:0)Z (cid:0)Z (cid:1) (cid:1) 2D 4D (cid:13) (cid:13)2 , (cid:13) (cid:13)2 . (7) t and 4D where 2D denote the inputs to the VLM, h2D and h4D are the feature reconstruction heads, and is the action latent produced by the action module (see Eq. 2). At the same time, is fed into the action prediction head to predict the diffusion noise: (cid:104)(cid:13) (cid:13)haction Laction = EϵN (0,I) (cid:0)Z (8) (cid:1) ϵ(cid:13) 2 (cid:13) 2 (cid:105) , where ϵ (0, I) denotes the forward-process noise, and haction is the action prediction head. Finally, the total loss for SwiftVLA is weighted sum of the reconstruction, action prediction, and trajectory objectives: Ltotal = λ2D L2D + λ4D L4D + λaction Laction + λtraj Ltraj, (9) Where each λ serves as balancing coefficient among the objectives. This design encourages the model to learn more comprehensive and geometry-aware 4D representations, rather than relying on single modality for action prediction. Meanwhile, this mechanism enables the model to implicitly reconstruct and reason over 4D spatial structures even when explicit 4D feature inputs are unavailable. 5 Method Short-Horizon Medium-Horizon Long-Horizon Average SR Length SR Length SR Length SR Length π0 [6] GO-1 [7] TinyVLA [68] SmolVLA [54] SmolVLA [54] SwiftVLA SwiftVLA with 4D input 0.42 0.40 0.08 0.28 0.38 0.56 0.56 120 124 183 152 130 115 100 0.46 0.44 0.08 0.32 0.36 0.48 0.50 150 160 240 178 165 156 145 0.52 0.54 0.06 0.28 0.34 0.56 0.58 187 190 236 234 195 180 0.47 0.46 0.07 0.29 0.36 0.53 0.55 152 158 220 188 163 150 143 Table 1. Comparison of task success rate and average trajectory length in simulation. The best results are marked in bold, and the secondbest results are underlined. denotes the model that is pre-trained and fine-tuned using the same configuration as SwiftVLA. Inference. To further reduce the overall parameter count and facilitate deployment on edge platforms, we retain only the 2D feature branch during inference. In this stage, the 4D feature extractor, reconstruction heads, and trajectory head are removed, as they are only used for auxiliary supervision during training. Consequently, the deployed model consists solely of the VLM and the action expert, forming compact yet effective architecture. The total parameter count of the deployed model equals the sum of these two components. Despite its lightweight nature, this design preserves the strong spatiotemporal perception capability learned through masked training, enabling efficient and reliable deployment on real-world robotic platforms. 4. Experiments 4.1. Experimental Setup Evaluation Metrics. We employ success rate (SR) primarily as our evaluation metric, along with the average trajectory length. In simulations, task receives an SR of 1 for successful completion and 0 otherwise. For real-world evaluations, we use detailed scoring system, where in the pickand-place task, score of 0.5 is given for grasping the object and another 0.5 for placing it at the target location. Baselines. We primarily selected VLA models of different parameter sizes as baselines for comparison with SwiftVLA. For large models, we chose the current state-ofthe-art model π0 [6] and GO-1 [7]. For smaller models, we selected TinyVLA [68] and SmolVLA [54]. For SwiftVLA, we adopted two inference configurations: one that uses 4D inputs during inference, referred to as SwiftVLA with 4D input, and another that does not use 4D inputs during inference, referred to simply as SwiftVLA. Both configurations share the same set of trained weights. Additionally, in the LIBERO benchmark [37], we compared several other algorithms, grouped into three categories: spatio-temporal enhanced VLA models [32, 50, 76], which utilize 3D or 4D inputs; small VLA models [18, 54, 82], which employ smaller VLMs; and large VLA models [5, 6, 23, 24, 34, 49, 65], which refer to VLA models with more than 3B parameters. Implementation Details. We adopt SmolVLM [44] as the backbone. The complete model comprises approximately 450 million parameters, of which around 100 million are allocated to the action-expert module. Meanwhile, we pretrain our model on public datasets [7, 69] using two-stage training procedure (detailed in the appendix). 4.2. Simulation Benchmark Experiments Simulation Setup. We conduct systematic evaluation of SwiftVLA on both the RoboTwin 2.0 [10] and LIBERO benchmarks [37]. For RoboTwin 2.0, our experimental setup considers three categories of tasks: Short-Horizon, Medium-Horizon, and Long-Horizon, with two subtasks selected for each category. For each subtask, we generate 50 demonstration trajectories, which are then used for posttraining. For LIBERO benchmark, we perform experiments across four task suites: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. Results. As shown in Tab. 1, the mask-and-reconstruct strategy enables SwiftVLA, which does not use 4D input during inference, to maintain competitiveness comparable to that of SwiftVLA with 4D input. Moreover, SwiftVLA demonstrates strong performance across all three task categories, exhibiting competitive capabilities compared to π0 [6] while using only about 15% of its parameters. Meanwhile, TinyVLA [68] and SmolVLA [54] still lag notably behind π0 [6], mainly due to their smaller VLM backbones, which lack sufficient capacity to model long-term spatiotemporal dependencies. In contrast, SwiftVLA introduces 4D representations to enhance its spatiotemporal understanding, improving SR by 82.76% over SmolVLA [54]. As shown in Tab. 3, we compare SwiftVLA with three categories of methods on LIBERO. For spatio-temporal enhanced VLA [32, 50, 76], methods like SpatialVLA [50] and 4D-VLA [76] directly integrate 3D and 4D information into the VLM. However, their performance gains are often limited by the models ability to handle multiple modalities. Approaches such as Qdepth-VLA [32] add additional 3D processing branches, improving performance but increasing the model size to over 3B parameters. Smaller models [18, 54], despite having fewer than 2B parameters, typically show lower success rates. Large VLA models offer 6 Methods Clean the desk Throw the bottle Stack Bowls Average SR Length SR Length SR Length SR Length π0 [6] SmolVLA [54] SmolVLA [54] SwiftVLA SwiftVLA with 4D input 0.60 0.32 0.52 0.86 0.86 1220 1640 1360 1140 1090 0.66 0.40 0.54 0.80 0.82 980 1360 1140 980 960 0.56 0.30 0.52 0.74 0. 840 1360 860 800 810 0.61 0.34 0.53 0.80 0.82 1013 1453 1120 973 953 Table 2. Comparison of task success rate and average trajectory length in real-world experiments. The best results are marked in bold, and the second-best results are underlined. denotes the model that is pre-trained and fine-tuned using the same configuration as SwiftVLA. Methods Size LIBERO Spatial Object Goal Long Avg Spatio-Temporal Enhanced VLA SpatialVLA [50] 4D-VLA [76] QDepth-VLA [32] SmolVLA [54] SmolVLA [54] UniAct [82] VLA-OS [18] SmolVLA [54] GR00T-N1 [5] π0 [6] π0+FAST [49] OpenVLA [23] OpenVLA-OFT [24] DD-VLA [34] UniVLA [65] 4B 4B 4B 88.2 88.9 97.6 Small VLA 0.45B 0.45B 0.5B 0.5B 2B 90.0 93.5 77.0 87.0 93. Large VLA 3B 3B 3B 7B 7B 7B 9B 94.4 96.8 96.4 84.7 97.6 97.2 95.4 89.9 95.2 96.6 96.0 96.5 87.0 96.5 94.0 97.6 98.8 96.8 88.4 98.4 98.6 98. 78.6 90.9 95.2 92.0 95.4 77.0 92.7 91.0 93.0 95.8 88.6 79.2 97.9 97.4 93.6 55.5 79.1 90.0 71.0 83.4 70.0 66.0 77.0 90.6 85.2 60.2 53.7 94.5 92.0 94. 78.1 88.6 94.9 87.3 92.2 77.8 85.6 88.8 93.9 94.1 85.5 76.5 97.1 96.3 95.4 Spatio-Temporal Enhanced Small VLA SwiftVLA SwiftVLA with 4D input 0.45B 1.65B 97.0 97.2 96.4 96.8 96.8 97.4 88.4 89.0 94.7 95.1 Table 3. Comparison of methods on the LIBERO. The best results are marked in bold, and the second-best results are underlined. denotes the model that is pre-trained and fine-tuned using the same configuration as SwiftVLA. strong performance but also require over 3B parameters, making them costly for real-world deployment. In contrast, SwiftVLA effectively leverages 4D information while maintaining compact design, achieving performance comparable to large VLA models. 4.3. Real-World Experiment To evaluate the effectiveness of the method in the real world, we conducted gripper grasping experiment using the AgileX PiPER six-degree-of-freedom robotic arm, with computational support provided by an NVIDIA RTX 4090 GPU. In addition, we designed series of real-world tasks, including Clean the Desk, Throw the Bottle, and Stack Bowls, with detailed descriptions provided in the supplementary material. As summarized in Tab. 2, our method demonstrates strong performance compared to π0 [6] while using fewer parameters, and significantly outperforms simiFigure 5. Comparison of SmolVLA and SwiftVLA under identical initial poses. During execution, SmolVLA fails to grasp accurately, as the end-effector misses the target and collides with the object, causing it to shift and posing safety risks. In contrast, SwiftVLA successfully completed the grasp with accurate positioning and stable control, demonstrating superior performance. Methods Inference Time (s) Memory (MB) Average SR π0 [6] SmolVLA [54] SwiftVLA 2.966 0.166 0.167 16236.2 1397.5 1398.4 0.48 0.30 0.76 Table 4. Comparison of inference time, memory usage, and SR across models on the NVIDIA Jetson Orin [48]. larly sized baselines such as SmolVLA. As shown in Fig. 5, we compared SmolVLA and SwiftVLA under identical initial object placements. During execution, SmolVLA failed to achieve precise grasping due to its limited understanding of geometric information. Meanwhile, the end-effector collided with the target object and displaced it, which could lead to task failure or safety hazards. In contrast, SwiftVLA successfully performed stable and accurate grasping thanks to its superior spatial perception and control. 4.4. Deployment on the Edge Device To evaluate the deployment efficiency of VLAs on edge devices, we adopt the NVIDIA Jetson Orin [48] as our target platform. As summarized in Tab. 4, we report the inference time and parameter count of each model. The results show that SwiftVLA delivers an 18 speedup in inference compared to the larger VLA model π0, while reducing 7 Input Feature Type Fusion tokens Average SR 2D 2D & 4D 2D & 4D 0.36 0.40 0.50 Table 5. Ablation study of SwiftVLA with 4D input evaluated by SR. The input feature type indicates the modalities used during training and inference. The best results are marked in bold, and the second-best results are underlined. memory consumption by 12. Moreover, its inference latency is nearly identical to that of SmolVLA. Meanwhile, SwiftVLA demonstrates high success rate compared to π0 on the NVIDIA Jetson Orin [48], indicating its suitability. 4.5. Ablation Study In this section, we perform experiments on the RoboTwin 2.0 [10] platform to address the following questions. Q1. How do 4D features affect task success? We compare two settings that use 2D inputs and that combine 2D inputs with the 4D features, as shown in the first and second rows of Tab. 5. The results indicate that relying solely on 2D inputs leads to lower SR. Incorporating the 4D features yields substantial improvement, suggesting that 4D features provide stronger representations for action planning. Q2: What role do Fusion Tokens play? Fusion Tokens are designed to integrate 4D and 2D features, using the 2D endeffector trajectory as supervision for trajectory prediction. In the second and third rows of Tab. 5, we compare models with and without Fusion Tokens and observe significant improvements when the token is enabled. This is because small models struggle to fully leverage the input 4D information. The introduction of Fusion Tokens, along with the design of target task, helps guide the model to effectively use both 2D and 4D cues, leading to improved cross-modal alignment and more effective temporal cue utilization. Q3. What is the effect of the mask-and-reconstruction strategy? We employ mask-and-reconstruction strategy during training, where 2D or 4D features are randomly dropped with fixed probability, and the VLA is tasked with reconstructing the masked features. The aim is to enable the model to maintain performance comparable to full 4D input, even when 4D information is missing during inference. As shown in Tab. 6, we compare different training strategies and evaluate performance under both inference with and without 4D input. The results show that directly removing the 4D input during inference, without applying any strategy, leads to significant performance drop, as the model becomes overly dependent on 4D cues for prediction. Introducing 4D feature masking alleviates this dependency and preserves part of the performance when 4D input is unavailable. Moreover, incorporating the feature reconstruction helps distill 4D information into the VLA during training, allowing the model to achieve performance com4D Feature Mask 2D Feature Mask Feature Reconstruction SwiftVLA SwiftVLA with 4D Input 0.02 0.40 0.50 0.53 0.50 0.48 0.52 0.55 Table 6. Comparison of SR across different training strategies. All models are trained with both 2D and 4D feature inputs, while only 2D features are used during inference. The best results are marked in bold, and the second-best results are underlined. Size SwiftVLA SwiftVLA with 4D input K=3 K=4 K=5 K=6 Random 0.47 0.48 0.50 0.52 0.53 0.49 0.52 0.51 0.55 0.55 Table 7. Ablation study on the temporal cache size evaluated by SR. All models are trained with both 2D and 4D feature inputs, while only 2D features are used during inference. The best results are marked in bold, and the second-best results are underlined. parable to that with full 4D input, even in the absence of 4D features during inference. In addition, we find that moderately masking 2D features encourages the model to better exploit underlying 4D geometric cues and enhances crossmodal consistency, as reflected in the last column of Tab. 6. Q4. How does the cached memory size affect performance? We analyze how the cached memory size chosen during training affects model performance. We evaluate four fixed settings with {3, 4, 5, 6} and randomized strategy that samples {3, 4, 5, 6} at each training step. As shown in Tab. 7, the randomized strategy outperforms all fixed-length baselines, indicating that exposure to variable temporal horizons process notably enhances adaptability. 5. Conclusion In this paper, we present SwiftVLA, lightweight framework that achieves strong spatiotemporal reasoning while maintaining design efficiency. Specifically, we employ 4D visual geometry transformer with temporal cache that extracts 4D features and integrates them into the VLM to enhance both spatial and temporal modeling. To bridge the gap between 2D and 4D features, we introduce Fusion Tokens, whose representations are supervised by the future trajectory of the end-effector, effectively capturing integrated multimodal information. Additionally, we employ maskand-reconstruct strategy to distill 4D knowledge into the VLA, while minimizing performance degradation when 4D inputs are omitted during inference. Experiments show that SwiftVLA matches the performance of models with up to 7 more parameters, while offering up to 18 faster inference and 12 smaller memory footprint on edge devices. 8 SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 6. Examples from the LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long task suites [37]. A. Architecture Design. We adopt SmolVLM [44] as the backbone network for robot environmental perception. SmolVLM [44] utilizes SigLIP [75] to encode visual features, which are then passed to the language decoder. Additionally, by leveraging global image information and employing pixel-shuffle operations, we constrain the number of visual tokens per frame to 64. To further accelerate inference, we skip certain computational layers in the VLM by using only the first 16 layers of the model. To enhance both computational efficiency and response speed, we adopt an attention pattern that alternates between self-attention [6] and cross-attention [5] modules rather than relying solely on either mechanism, following the design principles of SmolVLA [54]. For handling 4D features, we similarly limit the number of tokens to 64, ensuring consistent computational efficiency. B. Implementation Details. Baselines. We primarily compare our model with VLA models of different parameter scales, using them as baselines for evaluation. π0 [6] is VLM [3] that incorporates Flow Matching [36] to predict action chunks. With parameter count of 3.3 billion, it has been trained on dataset comprising 10,000 hours of cross-embodiment robotics data. The architecture is inspired by Paligemma [3] and processes three images, sensorimotor states, and language instruction as inputs. TinyVLA [68] is designed to address the challenges of inference speed and data efficiency in existing VLA models. Unlike traditional models, TinyVLA [68] achieves faster inference and improved data efficiency by initializing high-performance multimodal policy backbone and incorporating diffusion policy decoder during finetuning. With model size around 1 billion parameters, TinyVLA [68] demonstrates advantages in both speed and data utilization, SmolVLA [54] is compact and efficient VLA model designed to reduce training and inference costs, making it suitable for real-world robotics applications. Optimized for consumer-grade GPUs, it retains competitive performance despite its small size. The model is pre-trained on community-collected datasets with fewer than 30k episodes and features an asynchronous inference stack for faster and more responsive control. SmolVLA [54] performs on par with larger models, offering solution for robotics tasks in both simulated and real-world environments. Pretraining Details. We pretrain our model on public datasets [7, 69] using two-stage procedure. In the first stage, the model is trained without 4D inputs, Fusion Tokens, or the mask-and-reconstruct strategy, relying solely on robot actions for supervision. Training is performed for 100,000 steps with global batch size of 256. The learning rate follows cosine decay schedule, starting at 1104 and decaying to 2.5106 after 200-step warmup. We adopt the AdamW optimizer [25] with β1 = 0.85 and β2 = 0.9. The input images are resized to 512 512 pixels for compatibility with the vision-language encoder. In the second stage, the model is initialized from the firststage checkpoint, and 4D inputs, Fusion Tokens, are enabled along with the mask-and-reconstruct strategy. Training continues for an additional 50,000 steps under the same opti9 Figure 7. Examples from the RoboTwin 2.0 [10], including move Stapler Pad, Place A2B Left, Place Bread Basket, Place Dual Shoes, Dump Bin Bigbin, Handover Block."
        },
        {
            "title": "Steps",
            "content": "Short-Horizon Medium-Horizon Long-Horizon Move Stapler Pad Place A2B Left"
        },
        {
            "title": "Dump Bin Bigbin\nHandover Block",
            "content": "112 113 151 155 283 313 Table 8. Tasks and their step lengths across different horizon categories used in the RoboTwin 2.0 [10]. mizer settings, with reduced learning rate of 5 105 following cosine decay schedule. Finetuning Details. For all baseline methods, we train each model for 30,000 steps on the same dataset, keeping hyperparameters consistent with their original implementations to ensure fair comparison. For SwiftVLA, we also adopt two-stage finetuning strategy. In the first stage (the initial 10,000 steps), the model is supervised only with robot actions to stabilize adaptation within the action space. The learning rate follows cosine decay schedule and is initialized at 1 104. We use the AdamW optimizer [25] with β1 = 0.85 and β2 = 0.9. After completing the first stage, we enable the 4D inputs and Fusion Tokens, and incorporate the mask-and-reconstruct strategy in the second stage. This allows the model to further learn spatiotemporal feature fusion and higher-level structural understanding. Simulation Tasks Setup. As shown in Fig. 6 and Fig. 7, we present the simulation tasks from LIBERO and RoboTwin 2.0 [10]. In RoboTwin 2.0, we further categorize the evaluation tasks into short, medium, and long horizons based on the average number of steps required for completion. Tab. 8 provides the detailed categorization. Short-horizon tasks typically require fewer than 120 steps and rely primarily on localized spatial reasoning. Medium-horizon tasks average around 150 steps and involve sequential planning across multiple object interactions. Long-horizon tasks exceed 280 steps and exhibit higher temporal dependencies and compositional complexity. This categorization enables systematic analysis of how different models generalize across varying horizon lengths, which is crucial for assessing robustness in multi-step manipulation scenarios. Real-World Tasks Setup. As shown in Fig. 8 and Fig. 9, we illustrate the tasks used in our experiments, covering four manipulation tasks. Clean the Desk: Bowls and plates with randomized colors are placed on the table. The robot must place both items into basket while ensuring that the plate is positioned at the bottom. Throw the Bottle: plastic bottle with randomly varying amount of liquid is placed in the scene, and the robot is required to pick it up and throw it into trash bin. Stack Bowls: Two bowls are positioned randomly on the table, and the robot is required to stack them correctly. Fold the Cloth: piece of clothing is laid flat on the table. The robot folds it following predefined sequence and then moves the folded garment to designated location. 10 Figure 8. Real-world manipulation tasks used in our experiments. From top to bottom, the examples correspond to Clean the Desk, Throw the Bottle, and Stack Bowls."
        },
        {
            "title": "Fold the Cloth",
            "content": "SR Length π0 [6] SmolVLA [54] SmolVLA [54] SwiftVLA SwiftVLA with 4D input 0.45 0.05 0.30 0.60 0.65 2550 3200 2600 2100 2010 Table 9. Comparison of task success rate and trajectory length for Fold the Cloth. The best results are marked in bold, and the second-best results are underlined. denotes the model that is pretrained and fine-tuned using the same configuration as SwiftVLA. C. More Challenging Real-World Experimental Results. the evaluate To further real-world performance of SwiftVLA, we investigate more challenging manipulation task: Fold the Cloth. This task is difficult due to its long-horizon nature and the complex physical dynamics of deformable objects. As shown in Fig. 9, we illustrate the full execution process of this task in real-world setting. The results in Tab. 9 present comparison of success rates achieved by different methods on the cloth folding task, executed on the AgileX PiPER six-degree-of-freedom robotic arm with computational support provided by an 11 NVIDIA RTX 4090 GPU. SwiftVLA demonstrates strong and reliable performance, while similar models such as SmolVLA [54] achieve very low success rates. These results highlight the advantages of incorporating 4D features when handling deformable objects and long-horizon manipulation tasks. D. Supplementary Video We provide video that compares SwiftVLA and π0 [6] across multiple tasks. Please refer to the file located at video/comparison.mp4 for more details. The video consists of the following segments: 4-40s: Demonstrates the comparison between SwiftVLA and π0 [6] on the Fold the Cloth task using an NVIDIA Jetson Orin platform [48]. 40-60s: Displays the comparison between SwiftVLA and π0 [6] on the Throw the Bottle task using an NVIDIA Jetson Orin platform [48]. 60-72s: Compares SwiftVLA and π0 [6] on the Clean the Desk task using an NVIDIA Jetson Orin [48]. 72-90s: Highlights the superior error-correction capability of SwiftVLA over π0 is particularly evident when handling deformable objects. In the video, we compare the two algorithms on the Fold the Cloth task using an NVIDIA Jetson Orin platform [48], focusing on how each model adjusts after failure. Compared to π0, SwiftVLA Figure 9. Real-world execution process of the Fold the Cloth task, which requires long-horizon reasoning and precise manipulation of deformable objects. recovers more quickly, with smoother motion trajectories that enable more fluid and accurate handling of deformable objects. 90-132s: Shows additional examples of SwiftVLA on the Fold the Cloth task using an NVIDIA Jetson Orin [48]."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 3 [2] Suneel Belkhale and Dorsa Sadigh. Minivla: better vla with smaller footprint, 2024. 1, 3 [3] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 1, 2, 9 [4] Vineet Bhat, Yu-Hsiang Lan, Prashanth Krishnamurthy, Ramesh Karri, and Farshad Khorrami. 3d cavla: Leveraging depth and 3d context to generalize vision language action models for unseen tasks. arXiv preprint arXiv:2505.05800, 2025. 2, 4 [5] Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open arXiv foundation model for generalist humanoid robots. preprint arXiv:2503.14734, 2025. 6, 7, [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. pi0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 2, 3, 6, 7, 9, 11 ulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. 6, 9 [8] Yifan Chang, Jie Qin, Limeng Qiao, Xiaofeng Wang, Zheng Zhu, Lin Ma, and Xingang Wang. Scalable training for vector-quantized networks with 100% codebook utilization. arXiv preprint arXiv:2509.10140, 2025. 1 [9] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 3 [10] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Qiwei Liang, Zixuan Li, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. 6, 8, 10 [11] Zhangquan Chen, Xufang Luo, and Dongsheng Li. Visrl: Intention-driven visual perception via reinforced reasoning. arXiv preprint arXiv:2503.07523, 2025. [12] Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, and Ruqi Huang. Think with 3d: Geometric imagination grounded spatial reasoning from limited views. arXiv preprint arXiv:2510.18632, 2025. 2 [13] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. 3 [14] Yubo Cui, Zheng Fang, Jiayao Shan, Zuoxu Gu, and Sifan Zhou. 3d object tracking with transformer. arXiv preprint arXiv:2110.14921, 2021. 3 [7] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manip- [15] Tianqi Ding, Dawei Xiang, Pablo Rivas, and Liang Dong. Neural pruning for 3d scene reconstruction: Efficient nerf acceleration. arXiv preprint arXiv:2504.00950, 2025. 2 [16] Tianqi Kirk Ding, Dawei Xiang, Yijiashun Qi, Ze Yang, Zunduo Zhao, Tianyao Sun, Pengbin Feng, and Haoyu Wang. Nerf-based defect detection. In International Conference on Remote Sensing, Mapping, and Image Processing (RSMIP 2025), pages 368373. SPIE, 2025. 2 [17] Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, et al. Emma: Generalizing real-world robot manipulation via generative visual transfer. arXiv preprint arXiv:2509.22407, 2025. 1 [18] Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Zeyu Jiang, et al. Vla-os: Structuring and dissecting planning representations and paradigms in vision-languageaction models. arXiv preprint arXiv:2506.17561, 2025. 6, 7 [19] Tianhao Guo, Bingjie Lu, Feng Wang, and Zhengyang Lu. Depth-aware super-resolution via distance-adaptive variational formulation. arXiv preprint arXiv:2509.05746, 2025. 2, 3 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 3 [21] Yueru Jia, Jiaming Liu, Sixiang Chen, Chenyang Gu, Zhilue Wang, Longzan Luo, Lily Lee, Pengwei Wang, Zhongyuan Wang, Renrui Zhang, et al. Lift3d foundation policy: Lifting 2d large-scale pretrained models for robust 3d robotic manipulation. arXiv preprint arXiv:2411.18623, 2024. 1 [22] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale arXiv preprint in-the-wild robot manipulation dataset. arXiv:2403.12945, 2024. [23] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2, 3, 6, 7 [24] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. 6, 7 [25] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 9, 10 [26] Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, and Yichen Zhu. Injecting the 3d world into vision-language-action models. arXiv preprint arXiv:2503.07511, 2025. 2, 3, 4, 5 Pointvla: [27] Haoyun Li, Ivan Zhang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Zhiqin Yang, Zhentao Zhang, Boyuan Wang, Chaojun Ni, Wenkang Qin, et al. Mimicdreamer: Aligning human and robot demonstrations for scalable vla training. arXiv preprint arXiv:2509.22199, 2025. 1 Qin Zhang, Junzhi Yu, et al. Robo-mutual: Robotic mulIn 2025 timodal task specification via unimodal learning. IEEE International Conference on Robotics and Automation (ICRA), pages 41824189. IEEE, 2025. [29] Peiyan Li, Yixiang Chen, Hongtao Wu, Xiao Ma, Xiangnan Wu, Yan Huang, Liang Wang, Tao Kong, and Tieniu Tan. Bridgevla: Input-output alignment for efficient 3d manipulation learning with vision-language models. arXiv preprint arXiv:2506.07961, 2025. [30] Wei Li, Bing Hu, Rui Shao, Leyang Shen, and Liqiang Nie. Lion-fs: Fast & slow video-language thinker as online video assistant. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 32403251, 2025. [31] Wei Li, Renshan Zhang, Rui Shao, Jie He, and Liqiang Cogvla: Cognition-aligned vision-language-action Nie. model via instruction-driven routing & sparsification. arXiv preprint arXiv:2508.21046, 2025. 1 [32] Yixuan Li, Yuhui Chen, Mingcai Zhou, and Haoran Li. Qdepth-vla: Quantized depth prediction as auxiliary supervision for vision-language-action models. arXiv preprint arXiv:2510.14836, 2025. 2, 6, [33] Zhenglin Li, Yangchen Huang, Mengran Zhu, Jingyu Zhang, JingHao Chang, and Houze Liu. Automated adaptive navigation leveraging natural language models in aerospace In 2024 6th International Conference on robotics system. Robotics, Intelligent Control and Artificial Intelligence (RICAI), pages 250254. IEEE, 2024. 1 [34] Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, and Ping Luo. Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies. arXiv preprint arXiv:2508.20072, 2025. 6, 7 [35] Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, and Bo Zhao. Evo-0: Vision-language-action model with implicit spatial understanding. arXiv preprint arXiv:2507.00416, 2025. 2, 3, 4, 5 [36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 9 [37] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. 6, 9 [38] Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, TtfShixin Wu, Songfang Huang, and Huiling Duan. token fusion via pixel-attention integravla: Temporal arXiv preprint tion for vision-language-action models. arXiv:2508.19257, 2025. 1 [39] Jiuming Liu, Guangming Wang, Zhe Liu, Chaokang Jiang, Marc Pollefeys, and Hesheng Wang. Regformer: an efficient projection-aware transformer network for large-scale point In Proceedings of the IEEE/CVF Intercloud registration. national Conference on Computer Vision, pages 84518460, 2023. [28] Jianxiong Li, Zhihao Wang, Jinliang Zheng, Xiaoai Zhou, Guanming Wang, Guanglu Song, Yu Liu, Jingjing Liu, Ya- [40] Jiuming Liu, Jinru Han, Lihao Liu, Angelica Aviles-Rivero, Chaokang Jiang, Zhe Liu, and Hesheng Wang. Mamba4d: 13 Efficient 4d point cloud video understanding with disentangled spatial-temporal state space models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1762617636, 2025. 3 [41] Hao Lu, Tianshuo Xu, Wenzhao Zheng, Yunpeng Zhang, Wei Zhan, Dalong Du, Masayoshi Tomizuka, Kurt Keutzer, and Yingcong Chen. Drivingrecon: Large 4d gaussian reconstruction model for autonomous driving. arXiv preprint arXiv:2412.09043, 2024. 2 [42] Hao Lu, Yunpeng Zhang, Guoqing Wang, Qing Lian, Dalong Du, and Ying-Cong Chen. Towards generalizable multicamera 3d object detection via perspective rendering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 58115819, 2025. [43] Zhichao Ma, Yutong Luo, Zheyu Zhang, Aijia Sun, Yinuo Yang, and Hao Liu. Reinforcement learning approach for highway lane-changing: Ppo-based strategy design. In 2025 10th International Conference on Electronic Technology and Information Science (ICETIS), pages 298301, 2025. 1 [44] Andres Marafioti, Orr Zohar, Miquel Farre, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. 1, 2, 3, 4, 6, 9 [45] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, et al. Recondreamer: Crafting world models for driving scene reconstruction via online restoration. arXiv preprint arXiv:2411.19548, 2024. 2 [46] Chaojun Ni, Jie Li, Haoyun Li, Hengyu Liu, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Boyuan Wang, Chenxin Li, Guan Huang, et al. Wonderfree: Enhancing novel view quality and cross-view consistency for 3d scene exploration. arXiv preprint arXiv:2506.20590, 2025. 4 [47] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Xinze Chen, Guanghong Jia, Guan Huang, and Wenjun Mei. Recondreamer-rl: Enhancing reinforcement learning via diffusion-based scene reconstruction. arXiv preprint arXiv:2508.08170, 2025. 2 [48] NVIDIA Jetson Orin Series Technical Brief. NVIDIA Corporation, 2022. Technical Brief v1.2, TB 10749-001 v1.2. 1, 7, 8, 11, 12 [49] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for visionlanguage-action models. arXiv preprint arXiv:2501.09747, 2025. 6, [50] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial represenarXiv preprint tations for visual-language-action model. arXiv:2501.15830, 2025. 2, 3, 5, 6, 7 [51] Ayushman Sarkar, Mohd Yamani Idna Idris, and Zhenyu Yu. Reasoning in computer vision: Taxonomy, models, tasks, and methodologies. arXiv preprint arXiv:2508.10523, 2025. 1, 3 in point clouds. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 13101316. IEEE, 2021. 3 [53] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 1 [54] Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. 1, 3, 6, 7, 9, 11 [55] Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, and Jiale Cao. Geovla: Empowering 3d representations in visionlanguage-action models. arXiv preprint arXiv:2508.09071, 2025. 2, 3, [56] Zuojin Tang, Bin Hu, Chenyang Zhao, De Ma, Gang Pan, and Bin Liu. Vlascd: visual language action model for simultaneous chatting and decision making. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025. 1 [57] GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, et al. Gigabrain-0: world modelarXiv preprint powered vision-language-action model. arXiv:2510.19430, 2025. [58] Jingyi Tian, Le Wang, Sanping Zhou, Sen Wang, Jiayi Li, Haowen Sun, and Wei Tang. Pdfactor: Learning triperspective view policy diffusion field for multi-task robotic In Proceedings of the Computer Vision and manipulation. Pattern Recognition Conference, pages 1575715767, 2025. 1 [59] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. 3 [60] Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, and Xingang Wang. Embodiedreamer: Advancing real2sim2real transfer for policy training via embodied world modeling. arXiv preprint arXiv:2507.05198, 2025. 1, 3 [61] Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, and Xingang Wang. Humandreamer-x: Photorealistic singleimage human avatars reconstruction via gaussian restoration. arXiv preprint arXiv:2504.03536, 2025. 4 [62] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 3, [52] Jiayao Shan, Sifan Zhou, Zheng Fang, and Yubo Cui. Ptt: Point-track-transformer module for 3d single object tracking [63] Sen Wang, Le Wang, Sanping Zhou, Jingyi Tian, Jiayi Li, Haowen Sun, and Wei Tang. Flowram: Grounding flow 14 matching policy with region-aware mamba framework for robotic manipulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1217612186, 2025. 1 [64] Weijie Wang, Jiagang Zhu, Zeyu Zhang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Haoxiao Wang, Guan Huang, Xinze Chen, et al. Drivegen3d: Boosting feedforward driving scene generation with efficient video diffusion. arXiv preprint arXiv:2510.15264, 2025. 4 [65] Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified vision-language-action model. arXiv preprint arXiv:2506.19850, 2025. 6, [66] Zhihao Wang, Jianxiong Li, Jinliang Zheng, Wencong Zhang, Dongxiu Liu, Yinan Zheng, Haoyi Niu, Junzhi Yu, and Xianyuan Zhan. Physiagent: An embodied agent framework in physical world. arXiv preprint arXiv:2509.24524, 2025. 1 [67] Zihan Wang, Jeff Tan, Tarasha Khurana, Neehar Peri, and Deva Ramanan. Monofusion: Sparse-view 4d reconstruction via monocular fusion, 2025. 3 [68] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient visionIEEE language-action models for robotic manipulation. Robotics and Automation Letters, 2025. 1, 3, 6, 9 [69] Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al. Robomind: Benchmark on multiembodiment intelligence normative data for robot manipulation. arXiv preprint arXiv:2412.13877, 2024. 6, 9 [70] Shaocheng Yan, Yiming Wang, Kaiyan Zhao, Pengcheng Shi, Zhenjun Zhao, Yongjun Zhang, and Jiayuan Li. Hemora: Unsupervised heuristic consensus sampling for the robust point cloud registration. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13631373, 2025. 3 In Proceedings of [71] Shanliang Yao, Runwei Guan, Zhaodong Wu, Yi Ni, Zile Huang, Ryan Wen Liu, Yong Yue, Weiping Ding, Eng Gee Lim, Hyungjoon Seo, et al. Waterscenes: multi-task 4d radar-camera fusion dataset and benchmarks for autonomous driving on water surfaces. IEEE Transactions on Intelligent Transportation Systems, 25(11):1658416598, 2024. 2 [72] Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, and Zheng Zhu. Vla-r1: Enhancing reaarXiv preprint soning in vision-language-action models. arXiv:2510.01623, 2025. 1 [73] Zhenyu Yu, Mohd Yamani Idna Idris, Pei Wang, and Rizwan Qureshi. Cotextor: Training-free modular multilingual text editing via layered disentanglement and depth-aware fusion. In Advances in Neural Information Processing Systems, 2025. 3 [74] Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, and Xing Wei. Futuresightdrive: Thinking visually with spatio-temporal cot for autonomous driving. arXiv preprint arXiv:2505.17685, 2025. 1 15 [75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 4, 9 [76] Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Yanpeng Zhou, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, et al. 4d-vla: Spatiotemporal vision-language-action pretraining with cross-scene calibration. arXiv preprint arXiv:2506.22242, 2025. 2, 4, 6, 7 [77] Wei Zhang, Feng Jiang, Chi-Fu Yang, Zhi-Peng Wang, and Tie-Jun Zhao. Research on unmanned surface vehicles environment perception based on the fusion of vision and lidar. IEEE Access, 9:6310763121, 2021. [78] Wei Zhang, Xian-zhong Gao, Chi-fu Yang, Feng Jiang, and Zhi-yuan Chen. object detection and tracking method for security in intelligence of unmanned surface vehicles. Journal of Ambient Intelligence and Humanized Computing, 13 (3):12791291, 2022. 3 [79] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, and Xingang Wang. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. arXiv preprint arXiv:2410.13571, 2024. 2 [80] Guosheng Zhao, Xiaofeng Wang, Chaojun Ni, Zheng Zhu, Wenkang Qin, Guan Huang, and Xingang Wang. Recondreamer++: Harmonizing generative and reconstructive arXiv preprint models for driving scene representation. arXiv:2503.18438, 2025. 2 [81] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. 2, 3, 4, 5 [82] Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, YaQin Zhang, and Xianyuan Zhan. Universal actions for enhanced embodied foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2250822519, 2025. 1, 6, 7 [83] Yaozong Zheng, Bineng Zhong, Qihua Liang, Guorong Li, Rongrong Ji, and Xianxian Li. Toward unified token learning for vision-language tracking. IEEE Transactions on Circuits and Systems for Video Technology, 34(4):21252135, 2023. [84] Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo, Shengping Zhang, and Xianxian Li. Odtrack: Online dense In Proceedtemporal token learning for visual tracking. ings of the AAAI conference on artificial intelligence, pages 75887596, 2024. 1 [85] Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Xin Eric Wang, and Achuta Kadambi. Vlm4d: Towards spatiotemporal awareness in vision language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 86008612, 2025. [86] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. 2,"
        }
    ],
    "affiliations": [
        "GigaAI",
        "Moxin (Huzhou) Technology Co., Ltd.",
        "Peking University",
        "Tsinghua University",
        "X-Humanoid"
    ]
}