{
    "paper_title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples",
    "authors": [
        "Baiqi Li",
        "Zhiqiu Lin",
        "Wenxuan Peng",
        "Jean de Dieu Nyandwi",
        "Daniel Jiang",
        "Zixian Ma",
        "Simran Khanuja",
        "Ranjay Krishna",
        "Graham Neubig",
        "Deva Ramanan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 2 9 6 6 4 1 . 0 1 4 2 : r NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples Baiqi Li1 Zhiqiu Lin1 Wenxuan Peng1 Jean de Dieu Nyandwi1 Daniel Jiang1 Zixian Ma2 Simran Khanuja1 Ranjay Krishna2 Graham Neubig1 Deva Ramanan1 1Carnegie Mellon University 2University of Washington"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) have made significant progress in recent visualquestion-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using offthe-shelf models like CLIP and ChatGPT. We propose semi-automated approach to collect new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt vision-centric design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can largely be solved with language priors like commonsense knowledge. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like BLIP-3, LLaVA-OneVision, Cambrian-1, InternLM-XC2, Llama3.2-Vision, Molmo, Qwen2-VL, and even the (closed-source) GPT-4o lag 50%-70% behind human performance (which is above 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. We show that debiasing can be crucial for VLM performance. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs."
        },
        {
            "title": "Introduction",
            "content": "Recent vision-language models (VLMs) such as GPT-4o [61], GPT-4Vision [60], BLIP-3 (XGenMM) [79], LLaVA-OneVision [37], InternLM-XC2 [14], Llama3.2-Vision [17], Molmo [10] and Qwen2-VL [76] have markedly improved performance on challenging visual-question-answering (VQA) benchmarks like MMMU [83] and MME [18]. These benchmarks evaluate VLMs across various domains, such as college-level subjects [53, 83], commonsense reasoning [18, 34], diagram comprehension [30, 52], and complex problem-solving in mathematics, coding, physics, and temporal forecasting [18, 50, 54]. Despite their progress, our research identifies significant gap: these models still struggle with seemingly simple questions about natural images. Figure 1 shows such VQA samples that humans find easy to solve, while even the state-of-the-art models fail. We term these natural adversarial samples [24] for VLMs. Co-first authors; Co-senior authors. Datasets and code at https://linzhiqiu.github.io/papers/naturalbench. 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. Figure 1: NaturalBench examples consist of two questions and two images with alternating answers to prevent blind models from scoring well (e.g., those that predict the same answer regardless of the image or question, as discussed in Section 3). We compare the ground-truth answer for each (image, question) pair with predictions from leading VLMs including GPT-4o (gpt-4o-2024-08-06), Qwen2-VL (72B), Llama3.2-Vision (90B), and Molmo (72B) (see Section 4). Even the best models like GPT-4o lags far behind human performance (which is above 90%). Figure 2 shows the pipeline for collecting these natural adversarial examples. Collecting natural adversarial samples. In contrast to previous benchmarks that challenge VLMs with carefully-curated VQA samples [5, 18, 50, 73], we propose semi-automated method to minimize human efforts by generating VQA samples from existing natural image-text datasets [42, 63] (see Figure 2). First, we identify pairs of image-text samples that leading VLMs like CLIP [65] fail to match correctly; typically, these pairs are visually and semantically similar. After collecting these confounding pairs, we send both samples to ChatGPT [60] to generate questions that elicit different answers for the two images. We hire human annotators to remove incorrect or irrelevant question-answer (QA) pairs by examining their corresponding images. This process is notably simpler than previous adversarial VQA benchmarks [40, 68] that train annotators to write new QA pairs that fail targeted VQA model. Nonetheless, our VQA samples pose natural challenge to state-of-the-art models without specifically targeting any. Avoiding blind solutions. Crucially, pairing each question with two images that yield different answers enforces VLMs to rely on the visual inputs. This approach contrasts with earlier benchmarks that can be (partially) addressed by blind language models [5, 45] that do not look at images. Indeed, we demonstrate that suite of six popular VQA benchmarks [5, 18, 30, 50, 53, 83] can be largely addressed by blind ChatGPT that exploits language biases. For instance, benchmarks like MME [18] contain questions like Is there black giraffe in the image?, which can be answered using commonsense knowledge that most giraffes are brown. Additionally, these benchmarks may inadvertently capture an imbalanced answer distribution. For instance, in the MMStar benchmark [5] (which excludes questions solvable by blind LLMs like ChatGPT), Yes is three 2 Figure 2: Collecting NaturalBench. We use semi-automated procedure to collect NaturalBench from natural image-text corpora like Flickr30K [63]. First, we identify confounding pairs of imagetext samples that fail discriminative VLMs like CLIP [65] and BLIP-2 [39], e.g., they wrongly match an image with another images caption. Next, we prompt ChatGPT to design questions that yield different answers for each image, providing the original captions in the prompt. Section 3 details this procedure. We hire human annotators to filter out incorrect VQA samples, such as Is the motorcyclist wearing red and white uniform?, which has an identical answer of Yes for both images. Unlike previous adversarial benchmarks [20, 24, 40, 68], NaturalBench does not target any specific VQA models nor perturb the images or questions. Section 6 extends this simple procedure to diverse data sources (e.g., non-English) to highlight its potential for future dynamic evaluations [31] of VLMs. times more likely than No to be the correct answer for yes-or-no questions. Section 4 shows that such spurious answer patterns allow one to achieve performance gains by finetuning blind GPT-3.5 solely on QA data from these VQA benchmarks. To prevent blind solutions from scoring well, we introduce balanced evaluation protocol: each test sample contains two images and two questions, with answers alternating between different questions or images. Consequently, blind solutions that choose the same answers regardless of the questions or images will not succeed under this protocol. NaturalBench. We collect an initial benchmark with 5,800 yes-or-no and 1,800 multiple-choice VQA samples using public image-text datasets [59, 63], surpassing the scale of recent benchmarks like MMVP [73] and MMStar [5] (ranging from 300 to 1,500 samples). We hire separate group of humans to evaluate themselves on NaturalBench tasks, achieving high accuracy above 90%. We also evaluate over 50 open-source and closed-source VLMs. Popular models like LLaVA [48] and mPLUG-Owl [80] perform only marginally better than random chance, and even the best closedsource models such as GPT-4o and GPT-4Vision [60] lag significantly lag behind humans by more than 50%. This suggests that NaturalBench would serve as an effective testbed for driving future innovation in VLMs. What are the challenges? We analyze why NaturalBench is difficult from two perspectives: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills [26, 32, 36, 71], such as attribute bindings, spatial/action/part relations, and advanced reasoning including comparison and logic. While most benchmarks assign only one skill tag per sample, we tag each sample with all applicable skills from carefully defined taxonomy of 27 skills. Even the closed-source GPT-4o still struggles with certain skills such as spatial orientation and comparison, for example, Are the two people looking in the same direction?. (2) Biases: NaturalBench reveals significant biases in VLMs, particularly their tendency to repeat the same answer across different images (or questions). Our analysis suggests debiasing is promising way to ground VLMs and reduce hallucinations, with NaturalBench serving as useful benchmark for bias mitigation [88]. Dynamic evaluations. To keep pace with model development and prevent data leakage [5, 78, 89], vision-language benchmarks must be continuously updated. Our benchmark curation method seamlessly adapts to dynamic evaluations [31, 58] by incorporating new data sources. We expand NaturalBench with over thousands of VQA samples constructed from two recent image-text datasets: (1) DOCCI [59] with detailed captions over 100 words, and (2) XM3600 [69] with non-English captions in Chinese and Hindi. Together, our first release of NaturalBench includes 10,000 samples, 3 presenting diverse challenges for next-generation VLMs. We hope our efforts will inspire further research into dynamic evaluations of VLMs."
        },
        {
            "title": "2 Related Works",
            "content": "Benchmarks for vision-language models. Recent VLMs are commonly tested with popular VQA benchmarks such as MMStar [5], MMMU [83], MME [18], ScienceQA [53], AI2D [30], MMBench [50], MM-Vet [82], Seed-Bench [34], and MMVP [73]. These benchmarks evaluate complex visio-linguistic skills, such as fine-grained perception, reasoning and cognition, commonsense knowledge, and problem-solving across different fields. However, constructing them requires substantial human effort, including designing skills for evaluation, sourcing relevant images, and training annotators to create question-answer pairs [21, 40, 50, 68, 73, 82]. Biases in vision-language benchmarks. Despite careful construction, vision-language benchmarks are prone to spurious statistical patterns [21, 66] exploitable by blind shortcut solutions. For example, in the classic VQAv1 benchmark [2], questions starting with Do you see a... are answered Yes 87% of the time. Such language biases allow blind QA models to answer correctly without viewing images. While the community spent years addressing these issues, recent benchmarks designed for foundational VLMs still repeat these flaws [5, 45]. For example, image-text retrieval benchmarks like ARO [84] contain nonsensical negative captions that can be easily ruled out by caption likelihood [45, 74] or grammar correctness [26]. Recent VQA benchmarks like MMBench [50] are compromised by questions that can be solved using commonsense knowledge alone [5]. In this work, we show that even blind ChatGPT can approach SOTA performance on these benchmarks, casting doubt on whether they truly assess visio-linguistic capabilities. As such, we design NaturalBench to avoid blind solutions by enforcing balanced evaluation protocol [21, 45, 71]. Adversarial samples for dynamic model evaluation. Historically, machine learning models took decades to reach human performance on static benchmarks for example, 15 years for MNIST [12] and 7 years for ImageNet [11]. However, modern foundation models [38, 60, 65] often make new benchmarks obsolete in just months or years [31]. In response, recent research advocates for dynamic (lifelong) benchmarking protocols [31, 43, 57, 70]. The most popular approach is to collect adversarial data samples through human-and-model-in-the-loop procedure. For instance, Adversarial NLI [58] and Dynabench [31] engage human annotators to continuously craft hard samples that fail existing large language models. Similarly, adversarial VQA benchmarks [40, 68] ask humans to repeatedly write difficult QA pairs for an image until one fails VQA model. Hendrycks et al. [24] train annotators to find web images that confuse pre-trained ImageNet classifiers. In contrast, our data collection method does not target any specific models and requires only single-step verification by human annotators, making it more efficient for dynamic benchmark curation."
        },
        {
            "title": "3 Collecting NaturalBench",
            "content": "This section describes how we collect NaturalBench. Natural adversarial samples for VLMs. For discriminative tasks like visual recognition, adversarial samples are images that models misclassify [20, 24]. For generative VLMs trained on tasks like VQA, we define their adversarial samples as image-question pairs that humans can easily answer but models cannot. Existing work often maliciously perturbs input images or prompts to compromise VLMs [16, 20, 27, 49, 55, 64]; instead, we challenge VLMs using natural image-question pairs. Challenges in designing VQA benchmarks. Without careful curation, VQA benchmarks may be solved by blind QA models that ignore the images [5, 21]. First, Figure 3 shows that recent benchmarks often include questions solvable through commonsense knowledge. For example, MMBench [50] includes questions like, Is the African Elephant the smallest or largest land animal? which can be easily answered without seeing the image. Another question from MMMU [83] asks, Which artist belonging to the Bloomsbury group was Gertler in relationship with? The correct answer is Dora Carrington, as the other options like Vanessa Bell and Leonora Carrington can be ruled out with knowledge of art history. We also refer interested readers to the concurrent work [5] for further discussion on this issue. Another easily overlooked bias is imbalanced answers. For example, in the popular MME [18] benchmark, the question Does this artwork exist in the form of painting? is answered Yes 97.5% of the time, while Does this artwork exist in the form 4 of furniture? is answered No 100% of the time. Even the concurrent MMStar [5] benchmark is not exempt from this issue, despite efforts to filter out samples from existing benchmarks that can be answered by blind language models. When MMStar asks about human emotions, Sad is three times more likely to be correct than Happy. In MMStars color-related questions, White and Black are up to ten times more likely to be correct than colors like Purple. Section 4 shows that such spurious answer patterns can be exploited by finetuning blind ChatGPT. Figure 3: Example questions in previous benchmarks solvable by commonsense knowledge. We provide example questions from existing VQA benchmarks that can be addressed using commonsense knowledge. For these questions, blind language model, such as ChatGPT (without vision input), can already answer them without looking at the image. Answer balancing (prior art). To avoid blind solutions, VQA benchmark must ensure that all candidate answers to question are equally likely. However, balancing answer distribution is challenging. For example, GQA [28] performs post-hoc balancing by discarding over 90% of collected VQA samples. VQAv2 [21] uses labor-intensive procedure to fix imbalances in VQAv1: for each (image, question, answer) triplet, MTurk annotators review up to 24 similar images (searched via nearest neighbor search) to find an alternative that leads to different answer. Efficient construction of balanced VQA samples (ours). We posit that foundation models [60, 65] can significantly reduce the human effort required to create balanced VQA benchmarks. Specifically, we automate the two most labor-intensive tasks: (1) finding pairs of similar images [21], and (2) generating corresponding questions and answers [40]. Given an image-text dataset like Flickr30K [63], our data curation pipeline (Figure 2) operates as follows. Step 1: We search for pairs of image-text samples that are incorrectly matched by discriminative VLMs like CLIP, meaning they erroneously match an image with another images caption. Step 2: We use generative LLMs like ChatGPT to write questions that yield different answers for each image. We elaborate on this process below: Step 1: Collecting pairs of image-text samples. We denote an image by and text caption by t. VLMs like CLIP [65] compute similarity score S(i, t) R, with higher scores indicating greater similarity. For pair of image-text samples {(i0, t0), (i1, t1)}, correct match occurs when S(i0, t0) and S(i1, t1) are both greater than S(i0, t1) and S(i1, t0). Conversely, mismatch occurs when: [S(i0, t0) < S(i0, t1)] or [S(i0, t0) < S(i1, t0)] or [S(i1, t1) < S(i0, t1)] or [S(i1, t1) < S(i1, t0)] (1) Our Appendix shows that this adversarial procedure pairs visually and semantically similar images more efficiently than other methods (e.g., random pairing) for creating challenging VQA samples. Using Flickr30K [63], we identify all mismatches of both CLIP (ViT-L-14-LAION400m) [29] and BLIP-2 [39]. Since each sample can mismatch with multiple others, we randomly keep one mismatch per sample to collect about 2,000 unique pairs. We also hire annotators to discard around 800 pairs where caption can describe both images. Our Appendix shows that these pairs already form an image-text retrieval benchmark in the same format as Winoground [71], challenging even the latest SigLIP [85] models trained with more parameters and data [67]. Step 2: Generating questions and answers. We ask ChatGPT to generate questions that yield different answers for two images. For example, given the caption pair t0 and t1, ChatGPT can generate questions answered Yes for one image and No for the other using the below instruction: will present two captions for two images. Please help me generate two questions that highlight the differences between the captions. The first question should result in Yes answer for Caption 1 and No for Caption 2. The second question should result in No answer for Caption 1 and Yes for Caption 2. Caption 1: {t0} Caption 2: {t1} Our Appendix shows how to modify this prompt for other question types (e.g., multiple-choice). For each generated VQA sample (a triplet of image, question, and answer), we engage two human annotators to select from the two candidate answers and Unanswerable [22]. We retain sample only if both annotators agree on the correct answer. This step is crucial to ensure the quality of our benchmark. For samples that annotators fail or disagree on, we will resample new questions using ChatGPT up to three times. Using 1,200 Flickr30K image pairs, we manage to collect 2,600 yes-or-no and 1,000 multiple-choice VQA samples. Section 6 shows how NaturalBench can be easily expanded with new data, as we add 3,200 yes-or-no and 800 multiple-choice VQA samples collected from DOCCI. In the main paper, we present results on the combined dataset of 7,600 English VQA samples collected from Flickr30K [63] and DOCCI [59]. MMMU-Test [83] MMStar-Test [5] MME-Test [18] MMBench-Test [50] ScienceQA-Test [53] AI2D-Test [30] Figure 4: Performance of GPT-3.5 vs. LLaVA-1.5 on previous VQA benchmarks. We split each benchmark into equal-sized training and test sets, and report zero-shot (in blue) and finetuned (in green) results. Previous benchmarks show strong language biases, allowing blind GPT-3.5 to exploit spurious answer patterns (see Section 4) by finetuning on QA data without images. As result, blind GPT-3.5 greatly surpasses random chance performance (see the red dotted line) and sometimes even matches the performance of LLaVA-1.5-7B finetuned using images. In contrast, Figure 5 shows that NaturalBench can effectively prevent blind solutions from exceeding chance."
        },
        {
            "title": "4 Experimental Results",
            "content": "We present model results to contrast NaturalBench with previous benchmarks. NaturalBench is more robust against blind solutions. Popular VQA benchmarks like MME [18] inadvertently encourage blind models that exploit language biases. We show this by using random half of each benchmark for training and testing on the other half. We finetune blind LLM (GPT-3.5) using auto (default) hyperparameters, while LLaVA-1.5 is finetuned with learning rate of 2e-5 and batch size of 16. Both models are trained for 10 epochs. Figure 4 shows that GPT-3.5 finetuned using only QA data (without images) significantly outperforms random chance and sometimes even 6 NaturalBench-Test (G-Acc) Figure 5: Performance of GPT-3.5, LLaVA-1.5, and GPT-4o on NaturalBench. We also split NaturalBench (the English subset) into equal-sized training and test sets, and report zero-shot (in blue) and finetuned (in green) results. We report group accuracy (G-Acc) (introduced in Section 4), which awards point when all four (image, question) pairs are answered correctly. We highlight key results: (1) Blind GPT-3.5 fails to surpass random chance performance (red dotted line), regardless of finetuning. (2) LLaVA-1.5 improves by 9% by finetuning on NaturalBenchs training images. (3) Even GPT-4o gains 10% G-Acc through vision finetuning on NaturalBench; however, it falls far behind human performance (purple dotted line). These findings confirm that NaturalBench is more vision-centric benchmark, and potentially useful dataset for improving already advanced VLMs. matches the performance of LLaVA-1.5 finetuned with images. In contrast, NaturalBench enforces balanced answer distribution for each question and image. Figure 5 confirms that NaturalBenchs design prevents blind GPT-3.5 from exceeding random chance performance, establishing it as more vision-centric benchmark for reliable VLM evaluation. Additionally, vision finetuning of LLaVA-1.5 and GPT-4o 2 significantly boosts their performance over the original checkpoints, suggesting that NaturalBench is potentially useful dataset for improving future VLMs. In this paper, we focus on model evaluation and leave data scaling for training to future work. We now proceed to evaluate more models on NaturalBench. Evaluation setup. To better understand model performance, we introduce three additional metrics. We define the question accuracy (Q-Acc) metric to award point only if model correctly answers question for both images. Similarly, the image accuracy (I-Acc) metric awards point when model correctly answers both questions for an image. Lastly, the group accuracy (G-Acc) metric awards one point when model correctly answers all four (image, question) pairs in test sample. These VQA metrics are analogous to Winogrounds retrieval metrics [71] for paired (image, caption) samples. When reporting performance, we generate answers using each models default decoding strategy and compare them to the ground-truth answers. Alternatively, Section 5 shows that models can be evaluated deterministically by comparing the likelihood of each candidate answer using VQAScore [44]. NaturalBench challenges all state-of-the-art VLMs. Table 1 shows that NaturalBench significantly challenges leading VLMs, with most models performing only 5% to 20% above random chance (in terms of G-Acc). Even models like InternLM-XC2-7B [15], despite being trained on Flickr30K images (but not our questions), perform only 20.2% better than chance. Closed-source models trained on proprietary datasets, such as GPT-4o and GPT-4v, still lag behind average human performance, as measured by separate group of three human annotators. We also note that Q-Acc (correctly answering both questions for each image) is always lower than I-Acc (correctly answering both images for each question). This is primarily due to models choosing the same answer for question regardless of the input image, which motivates our analysis in Section 5 on debiasing VLMs. Lastly, we observe that latest models like Qwen2-VL, Molmo, and Llama3.2 improve with larger language models. We now explore how NaturalBench identifies areas for future model improvement. 2Due to the high cost, we only finetune GPT-4o on NaturalBench, not other benchmarks. We use the auto setting for vision finetuning of GPT-4o. 7 Table 1: Performance on NaturalBench. We report the performance of 53 leading VLMs on NaturalBench. All models significantly lag behind human performance, with the performance gap (in G-Acc) between humans and models highlighted in red. The latest models, such as BLIP-3 (XGenMM), Cambrian-1, LLaVA-OneVision, Llama3.2-Vision, Molmo, and Qwen2-VL lag significantly behind humans by 55% to 70%. Even the best closed-source GPT-4o is still 52% behind humans. Model Image Encoder Language Model NaturalBench Performance Acc Q-Acc 97.5 50.0 Open-source Models Human Performance Random Chance BLIP-2 [39] InstructBLIP [9] EVA-G EVA-G Otter [33] LlaMA-Adapter-v2.1 [19] CogVLM-Agent-VQA [25] DeepSeek-VL-1.3B-Chat [51] CLIP-L-14 CLIP-L-14 EVA2-E SigLIP-L & SAM-B ShareGPT4V [4] LLaVA-1.5 [47] CogVLM-Chat [77] InternLM-XC-V1 [87] InternLM-XC-V2-1.8B [15] Qwen-VL-Chat [3] Phi-3-Vision [1] mPLUG-Owl2 [81] Bunny [23] mPLUG-Owl2.1 [81] Monkey-10B-chat [41] CLIP-L-14 CLIP-L-14 EVA2-E EVA-G CLIP-L-14 CLIP-G-16 CLIP-L-14 CLIP-L-14 SigLIP-SO CLIP-L-14 OpenCLIP-BigG LLaVA-NeXT [48] CLIP-LDeepSeek-VL-7B-Chat [51] BLIP-3 (XGen-MM) [79] InternVL-Chat-V1.1 [6] InternVL-Chat-V1.5 [7] InternVL-Chat-V1.2-Plus [6] InternVL2-8B [8] Cambrian-1 [72] SigLIP-L & SAM-B CLIP-H-14 InternViT-6B InternViT-6B InternViT-6B InternViT-300M SigLIP-S-14 & CLIP-L-14 DINOv2-g & CLIP-ConvNeXT-XXL InternLM-XC2-4KHD-7B [13] CLIP-L-14 CLIP-L-14 InternLM-XC2-7B [15] InternViT-6B InternVL-Chat-V1.2 [6] InternViT-6B InternVL2-26B [8] LLaVA-OneVision [37] SigLIP-SLlama3.2-Vision [17] ViT-H-14 Molmo [10] CLIP-L-14 Qwen2-VL [76] CLIP-LFlanT5-3B 56.2 FlanT5-11B 61.0 Vicuna-7B 59.1 Vicuna-13B 62.8 FlanT5-3B 62.5 FlanT5-11B 64.5 MPT-7B 57.4 LaMA2-7B 58.3 Vicuna-7B 64.9 DeepSeek-LLM-1B 66.5 Vicuna-7B 68.4 Vicuna-13B 69.3 Vicuna-7B 67.3 Vicuna-13B 68.9 Vicuna-7B 68.1 InternLM-7B 68.7 InternLM2-1.8B 70.5 Qwen-7B 70.0 Phi-3-Mini 70.4 Llama2-7B 70.4 Phi-2-2.7B 69.9 Qwen-7B 70.1 Qwen-7B 71.1 Vicuna-7B 70.2 Mistral-7B 71.1 72.2 Vicuna-13B Nous-Hermes-2-Yi-34B 73.5 71.7 DeepSeek-LLM-7B 72.3 Phi-3-Mini 73.4 Llama2-13B InternLM2-Chat-20B 75.3 Nous-Hermes-2-Yi-34B 75.5 74.0 InternLM2.5-7B-Chat 71.5 Llama-3-8B Vicuna-13B 75.4 Nous-Hermes-2-Yi-34B 76.3 75.5 InternLM2-7B 76.0 InternLM2-7B Nous-Hermes-2-Yi-34B 75.6 76.9 InternLM2-Chat-20B 68.7 Qwen2-0.5B 77.2 Qwen2-7B 75.1 Llama-3.1-8B 77.0 Llama-3.1-70B 68.3 OLMoE-1B 72.8 OLMo-7B 75.3 Qwen2-7B 76.4 Qwen2-72B 74.1 Qwen2-1.5B 76.7 Qwen2-7B 79.9 Qwen2-72B GPT-4Vision GPT-4o Closed-source Models GPT-4 GPT-4 75.0 81."
        },
        {
            "title": "5 Why is NaturalBench Challenging?",
            "content": "I-Acc G-Acc Human 92.1 95.0 6.3 25.0 0.0 -85.8 17.1 31.9 24.2 34.8 28.1 39.1 24.9 23.2 34.7 39.4 44.3 44.3 43.8 44.6 41.3 46.9 46.9 46.8 48.7 48.7 48.4 47.1 48.3 47.6 49.1 49.9 50.9 50.1 51.2 52.3 55.9 56.2 54.4 47.9 55.7 57.2 56.1 56.7 56.4 58.4 46.2 58.8 55.6 57.5 42.6 50.3 55.8 57.0 54.4 58.5 64.0 56.1 66.4 2.1 7.7 4.0 9.2 9.8 12.7 3.8 4.4 10.3 11.5 12.5 14.9 12.7 14.8 13.9 15.5 16.6 17.1 17.2 17.4 17.4 17.9 18.2 15.0 16.3 19.2 22.7 19.3 19.5 20.3 23.1 23.4 23.5 19.4 25.5 26.6 25.9 26.5 26.6 27.7 15.6 28.8 26.8 29.1 14.7 20.7 26.7 29.3 23.4 29.1 36.9 26.2 39. -89.9 -84.4 -88.1 -82.9 -82.3 -79.4 -88.3 -87.7 -81.8 -80.6 -79.6 -77.2 -79.4 -77.3 -78.2 -76.6 -75.5 -75.0 -74.9 -74.7 -74.7 -74.2 -73.9 -77.1 -75.8 -72.9 -69.4 -72.8 -72.6 -71.8 -69.0 -68.7 -68.6 -72.7 -66.6 -65.5 -66.2 -65.6 -65.5 -64.4 -76.5 -63.3 -65.3 -63.0 -77.4 -71.5 -65.4 -62.8 -68.7 -63.0 -55.2 -65.9 -52.5 94.6 25.0 14.0 25.8 20.2 29.0 35.2 32.8 20.9 19.4 31.1 35.4 39.1 40.5 37.7 39.6 37.7 40.3 43.3 42.6 43.4 43.7 42.3 42.5 43.9 42.5 44.6 45.9 48.2 46.0 47.0 48.5 52.3 52.7 50.4 44.6 52.6 53.9 53.1 53.9 52.9 55.4 39.8 56.1 51.7 55.1 38.2 46.8 52.0 53.9 50.8 55.5 61.3 52.5 64.4 We analyze why NaturalBench is challenging from (1) compositionality and (2) biases. NaturalBench assesses visio-linguistic compositional reasoning. Solving NaturalBench sample often requires combination of skills, including object recognition, attribute binding, relation understanding, and advanced reasoning such as logic, comparison, differentiation (instance discrimination), counting, and world knowledge. For fine-grained evaluation, we manually tag each (image, question) pair with all associated skills, unlike prior benchmarks that oversimplify by assigning single skill 8 tag per sample. Figure 6 showcases the skill taxonomy with 8 types of objects, 8 types of attributes, 3 types of relations (with spatial relation further divided into 4 subtypes [46]), and 5 types of reasoning [35]. This taxonomy is more compositional than previous benchmarks such as MMVP [73], which assigns each sample single tag from 8 skill types. The Appendix provides detailed skill definitions, examples, and model performance for each skill. Several conclusions are noted: (1) Certain skills are generally harder than others; for example, abstract attributes (e.g., helpful) are harder than color or size attributes. Orientation (e.g., facing, towards) is harder than other spatial relations like proximity (e.g., near, far) or projectivity (e.g., to the right, on top of). (2) Even the strongest GPT-4o are challenged by advanced reasoning skills such as comparison (e.g., more than, the same as, happier). (a) Skill taxonomy (b) Examples of skill tags Figure 6: Skill taxonomy and tagging. Figure (a) provides an overview of the compositional reasoning skills evaluated by NaturalBench. Skill definitions and model performance per skill are presented in our Appendix. Figure (b) shows NaturalBench samples with their associated skill tags. Unlike prior benchmarks that assign single tag per sample, NaturalBench tags each sample with all applicable skills for fine-grained analysis. NaturalBench exposes significant model biases. We find that most VLMs underperform on NaturalBench due to biases towards certain answers, such as Yes for yes-or-no questions and for multiple-choice questions. We posit that mitigating these biases can improve model performance. To show this, we adopt scoring-based evaluation strategy using the generative likelihood of each candidate answer (VQAScore [44]) to determine the models predicted answer. Specifically, given question q, an image i, and two candidate answers a0 and a1, we evaluate: (2) (a0q, i) (a1q, i) > τ where τ is threshold (default is 0). If this condition (Eq. 2) is met, the model predicts a0; otherwise, it predicts a1. The Appendix shows that deterministic evaluation yields results largely consistent with stochastic decoding while being more reproducible. Crucially, this formulation allows us to adjust τ [1, 1] for each NaturalBench sample (four image-question pairs) to avoid repeating the same answers across images (or questions). Recall that Q-Acc awards point when the model correctly answers both images for question. We can now calculate debiased Q-Acc by adjusting τ so that the model predicts different answers for each image. Similarly, debiased I-Acc is calculated by adjusting τ to ensure different predicted answers for each question (of the same image). For debiased G-Acc, we tune τ to make the model predict a0 for two of the four image-question pairs and a1 for the other two pairs. Table 2 shows that these metrics significantly outperform the original ones by 35% to 40%, indicating that proper debiasing of the model can lead to notable performance gains. Importantly, our debiased metrics reflect the ability of VLM to correctly rank the set of eight image-question-answer triples, such that the correct combinations are more probable than incorrect ones. However, this protocol violates the original task of answering single image-question pair. This motivates us to study alternate debiasing techniques [88] in the Appendix. We believe NaturalBench could be promising testbed for techniques to ground VLMs and reduce biased responses (hallucinations)."
        },
        {
            "title": "6 Extending to Dynamic Evaluation",
            "content": "We now show our benchmark curation method can facilitate dynamic evaluation [31, 70]. Expanding NaturalBench. Since benchmarks often leak into foundation models training data, it is crucial to update benchmarks using new data sources. Our benchmark curation method can easily adapt to new image-text datasets. We expand NaturalBench by incorporating two recently proposed 9 Table 2: Debiased performance on NaturalBench. Many models underperform on NaturalBench due to biases towards certain answers like Yes and B. To illustrate this, we compute debiased Q-Acc by adjusting the prediction threshold (as described in Section 5) to ensure the model predict different answers for the two images of the same question. Similarly, debiased I-Acc ensures different predicted answers for the two questions of the same image. For debiased G-Acc, we tune the threshold so that the model predicts one answer for two (out of four) image-question pairs, and different answer for the other two pairs. The substantial performance gains of these metrics suggest that proper debiasing can greatly improve performance. Our Appendix evaluates existing debiasing techniques that do not require prior knowledge of image-question pairings. Model Image Encoder Language Model Q-Acc I-Acc G-Acc Original Debiased Original Debiased Original Debiased LLaVA-1.5 DeepSeek-VL-7B-Chat BLIP-3 (XGen-MM) InternVL-Chat-V1.5 InternVL-Chat-V1.2 InternVL2-26B LLaVA-OneVision CLIP-L-14 SigLIP-L CLIP-H-14 InternViT-6B InternViT-6B InternViT-6B SigLIP-S-14 Vicuna-13B DeepSeek-LLM-7B Phi-3-Mini InternLM2-Chat-20B Nous-Hermes-2-Yi-34B InternLM2-Chat-20B Qwen2-7B GPT-4o - GPT-4 38.6 45.8 46.8 52.6 52.6 55.7 55. 65.0 86.2 86.6 88.6 92.3 91.6 92.2 92.1 94.0 43.5 49.9 51.1 56.0 56.0 58.5 58.2 67.0 78.6 81.8 81.9 86.1 86.0 87.2 87. 90.5 14.4 19.4 19.5 24.3 26.2 28.2 28.6 40.5 49.7 54.8 55.3 66.0 65.8 67.7 67.8 75.6 Table 3: NaturalBench statistics. We report model performance on each dataset in the Appendix. Benchmark Statistics Collection Details Source Question Type Language # VQA Samples # VLMs Used # Mismatched Pairs # Verified Pairs Flickr30K [63] Yes-or-No Flickr30K [63] Multiple-Choice DOCCI [59] DOCCI [59] Yes-or-No Multiple-Choice English English English English NaturalBench 2,600 1,000 3,200 800 CLIP-L, BLIP-2, GPT-4 CLIP-L, BLIP-2, GPT-4 LongCLIP, GPT-4 LongCLIP, GPT-4 All Yes-or-No, Multiple-Choice English 7,600 - XM3600 [69] XM3600 [69] All Yes-or-No Yes-or-No Yes-or-No Chinese Hindi NaturalBench (Multi-lingual) 1,200 1, NLLB-CLIP, GPT-4 NLLB-CLIP, GPT-4 Chinese, Hindi 2,400 - 2,000 2,000 3,300 3,300 - 2,400 2,400 - 1,200 1,200 1,000 1,000 - 400 400 - datasets: (1) DOCCI [59] with fine-grained captions over 100 words, and (2) XM3600 [69] with captions in Chinese and Hindi. Specifically, we use the latest longCLIP [86] for processing long text sequences and NLLB-CLIP [75] for non-English captions. Our Appendix details the collection process, model performance, and prompts used to collect VQA samples with ChatGPT. Table 3 shows all benchmarks we have collected. We hope our efforts will inspire future work in studying dynamic evaluations of VLMs."
        },
        {
            "title": "7 Conclusion",
            "content": "Limitations. Our collected samples may inherit biases from web-scraped datasets and foundation models [56, 62], making human verification crucial. While this work focuses on model performance for individual skill tags, future work may analyze performance using combinations of skills. Summary. We introduce NaturalBench to evaluate vision-language models on their natural adversarial samples samples that challenge models significantly more than humans. Unlike previous benchmarks where blind models could succeed without the images, NaturalBench better reflects VLMs genuine progress by penalizing solutions that ignore images. Furthermore, NaturalBench offers comprehensive skill tags to assess compositional reasoning abilities and highlights model biases in VLMs. Lastly, we show that our semi-automated method for benchmark curation can adapt to new data sources, facilitating future dynamic evaluations of VLMs."
        },
        {
            "title": "8 Acknowledgement",
            "content": "We thank Pengchuan Zhang, Emily Li, and Anoushka Shrivastava for their invaluable discussions during the development of this work. We thank Tiffany Ling for her contribution to the visual design."
        },
        {
            "title": "References",
            "content": "[1] Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah, A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., et al. (2024). Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. [2] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. (2015). Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433. [3] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. (2023). Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. [4] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. (2023a). Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793. [5] Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al. (2024a). Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330. [6] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Muyan, Z., Zhang, Q., Zhu, X., Lu, L., et al. (2023b). Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238. [7] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. (2024b). How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821. [8] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. (2024c). Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy. https://internvl.github.io/blog/2024-07-02-InternVL-2.0/. [9] Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. (2023). Instructblip: Towards general-purpose vision-language models with instruction tuning. [10] Deitke, M., Clark, C., Lee, S., Tripathi, R., Yang, Y., Park, J. S., Salehi, M., Muennighoff, N., Lo, K., Soldaini, L., et al. (2024). Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. [11] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee. [12] Deng, L. (2012). The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine, 29(6), 141142. [13] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Zhang, S., Duan, H., Zhang, W., Li, Y., et al. (2024a). Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512. [14] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., and Wang, J. (2024b). Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420. [15] Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., et al. (2024c). Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420. [16] Dong, Y., Chen, H., Chen, J., Fang, Z., Yang, X., Zhang, Y., Tian, Y., Su, H., and Zhu, J. (2023). How robust is googles bard to adversarial image attacks? arXiv preprint arXiv:2309.11751. [17] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. [18] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al. (2023). Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394. [19] Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., et al. (2023). Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010. [20] Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572. [21] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. (2017). Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. [22] Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. (2018). Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617. [23] He, M., Liu, Y., Wu, B., Yuan, J., Wang, Y., Huang, T., and Zhao, B. (2024). Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530. [24] Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. (2021). Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1526215271. [25] Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al. (2023). Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914. [26] Hsieh, C.-Y., Zhang, J., Ma, Z., Kembhavi, A., and Krishna, R. (2023). Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. arXiv preprint arXiv:2306.14610. [27] Hu, A., Gu, J., Pinto, F., Kamnitsas, K., and Torr, P. (2024). As firm as their foundations: Can opensourced foundation models be used to create adversarial examples for downstream tasks? arXiv preprint arXiv:2403.12693. [28] Hudson, D. A. and Manning, C. D. (2019). Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709. [29] Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. (2021). Openclip. If you use this software, please cite it as below. [30] Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. (2016). diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer. [31] Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., et al. (2021). Dynabench: Rethinking benchmarking in nlp. arXiv preprint arXiv:2104.14337. [32] Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. (2017). Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123, 3273. [33] Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., and Liu, Z. (2023a). Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425. [34] Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. (2023b). Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125. [35] Li, B., Lin, Z., Pathak, D., Li, J., Fei, Y., Wu, K., Xia, X., Zhang, P., Neubig, G., and Ramanan, D. (2024a). Evaluating and improving compositional text-to-visual generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. [36] Li, B., Lin, Z., Pathak, D., Li, J., Fei, Y., Wu, K., Ling, T., Xia, X., Zhang, P., Neubig, G., et al. (2024b). Genai-bench: Evaluating and improving compositional text-to-visual generation. arXiv preprint arXiv:2406.13743. [37] Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., and Li, C. (2024c). Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. [38] Li, J. and Lu, W. (2024). survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632. [39] Li, J., Li, D., Savarese, S., and Hoi, S. (2023c). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597. [40] Li, L., Lei, J., Gan, Z., and Liu, J. (2021). Adversarial vqa: new benchmark for evaluating the robustness of vqa models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20422051. [41] Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., and Bai, X. (2023d). Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607. [42] Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer. [43] Lin, Z., Shi, J., Pathak, D., and Ramanan, D. (2021). The clear benchmark: Continual learning on realworld imagery. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2). [44] Lin, Z., Pathak, D., Li, B., Li, J., Xia, X., Neubig, G., Zhang, P., and Ramanan, D. (2024a). Evaluating text-to-visual generation with image-to-text generation. arXiv preprint arXiv:2404.01291. [45] Lin, Z., Chen, X., Pathak, D., Zhang, P., and Ramanan, D. (2024b). Revisiting the role of language priors in vision-language models. arXiv preprint arXiv:2306.01879. [46] Liu, F., Emerson, G., and Collier, N. (2023a). Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11, 635651. [47] Liu, H., Li, C., Li, Y., and Lee, Y. J. (2023b). Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744. [48] Liu, H., Li, C., Wu, Q., and Lee, Y. J. (2023c). Visual instruction tuning. arXiv preprint arXiv:2304.08485. [49] Liu, X., Zhu, Y., Lan, Y., Yang, C., and Qiao, Y. (2023d). Query-relevant images jailbreak large multi-modal models. arXiv preprint arXiv:2311.17600. [50] Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. (2023e). Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281. [51] Lu, H., Liu, W., Zhang, B., Wang, B., Dong, K., Liu, B., Sun, J., Ren, T., Li, Z., Sun, Y., et al. (2024). Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525. [52] Lu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu, Z., Liang, X., and Zhu, S.-C. (2021). Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214. [53] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. (2022). Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35, 25072521. [54] Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. (2023). Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. [55] Luo, H., Gu, J., Liu, F., and Torr, P. (2023). An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models. In The Twelfth International Conference on Learning Representations. [56] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. (2021). survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6), 135. [57] Mitchell, T., Cohen, W., Hruschka, E., Talukdar, P., Yang, B., Betteridge, J., Carlson, A., Dalvi, B., Gardner, M., Kisiel, B., et al. (2018). Never-ending learning. Communications of the ACM, 61(5), 103115. [58] Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., and Kiela, D. (2019). Adversarial nli: new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599. 13 [59] Onoe, Y., Rane, S., Berger, Z., Bitton, Y., Cho, J., Garg, R., Ku, A., Parekh, Z., Pont-Tuset, J., Tanzer, G., et al. (2024). Docci: Descriptions of connected and contrasting images. arXiv preprint arXiv:2404.19753. [60] OpenAI (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774. [61] OpenAI (2024). Gpt-4o system card. http://www.example.comhttps://openai.com/index/gpt-4o-systemcard/ . [62] Parashar, S., Lin, Z., Liu, T., Dong, X., Li, Y., Ramanan, D., Caverlee, J., and Kong, S. (2024). The neglected tails of vision-language models. arXiv preprint arXiv:2401.12425. [63] Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. (2015). Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649. [64] Qi, X., Huang, K., Panda, A., Wang, M., and Mittal, P. (2023). Visual adversarial examples jailbreak large language models. arXiv preprint arXiv:2306.13213. [65] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. [66] Ramakrishnan, S., Agrawal, A., and Lee, S. (2018). Overcoming language priors in visual question answering with adversarial regularization. Advances in Neural Information Processing Systems, 31. [67] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. (2022). Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35, 2527825294. [68] Sheng, S., Singh, A., Goswami, V., Magana, J., Thrush, T., Galuba, W., Parikh, D., and Kiela, D. (2021). Human-adversarial visual question answering. Advances in Neural Information Processing Systems, 34, 2034620359. [69] Thapliyal, A. V., Pont-Tuset, J., Chen, X., and Soricut, R. (2022). Crossmodal-3600: massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522. [70] Thrush, T., Tirumala, K., Gupta, A., Bartolo, M., Rodriguez, P., Kane, T., Rojas, W. G., Mattson, P., Williams, A., and Kiela, D. (2022a). Dynatask: framework for creating dynamic ai benchmark tasks. arXiv preprint arXiv:2204.01906. [71] Thrush, T., Jiang, R., Bartolo, M., Singh, A., Williams, A., Kiela, D., and Ross, C. (2022b). Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248. [72] Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M., Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., et al. (2024a). Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860. [73] Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S. (2024b). Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209. [74] Tschannen, M., Kumar, M., Steiner, A., Zhai, X., Houlsby, N., and Beyer, L. (2024). Image captioners are scalable vision learners too. Advances in Neural Information Processing Systems, 36. [75] Visheratin, A. (2023). Nllb-cliptrain performant multilingual image retrieval model on budget. arXiv preprint arXiv:2309.01859. [76] Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. (2024). Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. [77] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al. (2023). Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079. [78] Xu, R., Wang, Z., Fan, R.-Z., and Liu, P. (2024). Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824. 14 [79] Xue, L., Shu, M., Awadalla, A., Wang, J., Yan, A., Purushwalkam, S., Zhou, H., Prabhu, V., Dai, Y., Ryoo, M. S., Kendre, S., Zhang, J., Qin, C., Zhang, S., Chen, C.-C., Yu, N., Tan, J., Awalgaonkar, T. M., Heinecke, S., Wang, H., Choi, Y., Schmidt, L., Chen, Z., Savarese, S., Niebles, J. C., Xiong, C., and Xu, R. (2024). xgen-mm (blip-3): family of open large multimodal models. [80] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. (2023a). mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178. [81] Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., and Zhou, J. (2023b). mplugowl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257. [82] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. (2023). Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490. [83] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. (2023). Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502. [84] Yuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D., and Zou, J. (2022). When and why vision-language models behave like bag-of-words models, and what to do about it? arXiv preprint arXiv:2210.01936. [85] Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. (2023). Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986. [86] Zhang, B., Zhang, P., Dong, X., Zang, Y., and Wang, J. (2024a). Long-clip: Unlocking the long-text capability of clip. arXiv preprint arXiv:2403.15378. [87] Zhang, P., Wang, X. D. B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al. (2023). Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112. [88] Zhang, Y.-F., Yu, W., Wen, Q., Wang, X., Zhang, Z., Wang, L., Jin, R., and Tan, T. (2024b). Debiasing large visual language models. arXiv preprint arXiv:2403.05262. [89] Zhou, K., Zhu, Y., Chen, Z., Chen, W., Zhao, W. X., Chen, X., Lin, Y., Wen, J.-R., and Han, J. (2023). Dont make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964. 15 NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples"
        },
        {
            "title": "Outline",
            "content": "This document supplements the main paper with detailed results. Below is the outline: Section details the collection process of NaturalBench. Section details VQA and image-text retrieval performance on NaturalBench. Section provides skill definitions and analyzes model performance by skills. Section reports other debiasing techniques on NaturalBench."
        },
        {
            "title": "A Collection Details",
            "content": "We provide further details on the collection pipeline. Step 1: Collecting pairs of image-text samples. We collect pairs of image-text samples by finding mismatches of discriminative VLMs like CLIP. Recall that VLMs like CLIP [65] compute similarity score S(i, t) R, with higher scores indicating greater similarity between the image and text caption t. For pair of image-text samples {(i0, t0), (i1, t1)}, mismatch occurs when: [S(i0, t0) < S(i0, t1)] or [S(i0, t0) < S(i1, t0)] or [S(i1, t1) < S(i0, t1)] or [S(i1, t1) < S(i1, t0)] (3) Importantly, this adversarial procedure efficiently pairs similar image-text samples for two purposes. First, these image-text pairs already form an image-text retrieval task that can be evaluated using Winogrounds [71] evaluation protocols (after removing pairs where one caption can describe both images). We term this benchmark NaturalBench-Retrieval and report the performance of CLIP and SigLIP in Table 7. Next, by considering both images and captions, we can pair samples that are semantically similar but not necessarily visually similar. This contrasts with MMVP [73] which only pairs visually similar images close in DINOs feature space. Implementation of step 1. For Flickr30K [63], we retrieve pairs mismatched by both OpenCLIP (LAION400M-ViT-L14) [29] and BLIP-2 (ViT-L) [39]. For DOCCI [59], we use both longCLIP-B and longCLIP-L. However, since DOCCIs captions are still too long to process, we use ChatGPT to shorten them to below 230 characters per caption. We believe future advances in long-context CLIP will streamline this process. Lastly, for XM3600, we use NLLB-CLIP [75] to process the Chinese and Hindi captions. Step 2: Generating questions and answers. We use ChatGPT to generate questions that yield different answers for two images using their textual captions. We now show the actual prompts we send to ChatGPT. Default instruction for GPT-4. In practice, we use the below prompt to ask GPT-4 to directly output JSON dictionary for easier processing: will present two captions for two images. Please help me generate two questions that highlight the differences between the captions. The first question should result in Yes answer for Caption 1 and No for Caption 2. The second question should result in No answer for Caption 1 and Yes for Caption 2. Caption 1: {t0} Caption 2: {t1} Please response in JSON format with question indices as the keys, starting from 0 and question-answer pairs {{\"Question\":...,\"Caption1 Answer\":...,\"Caption2 Answer\":...}} as the values. Instructions for generating Chinese and Hindi QA pairs. We can simply ask GPT-4 to generate questions and answers in Chinese and Hindi: 16 will present two captions for two images. Please help me generate two questions in Chinese / Hindi that highlight the differences between the captions. The first question should result in Yes answer for Caption 1 and No for Caption 2. The second question should result in No answer for Caption 1 and Yes for Caption 2. Caption 1: {t0} Caption 2: {t1} Please response in JSON format with question indices as the keys, starting from 0 and question-answer pairs {{\"Question\":...,\"Caption1 Answer\":...,\"Caption2 Answer\":...}} as the values. Instructions for generating multiple-choice QA pairs. We ask ChatGPT to generate multiplechoice questions using the below prompt: will present two captions for two images. Please help me generate two multiple-choice questions that highlight the differences between the captions. Each question should have options and B. For the first question, option corresponds to Caption 1 and option corresponds to Caption 2. For the second question, option corresponds to Caption 2 and option corresponds to Caption 1. Caption 1: {t0} Caption 2: {t1} Please response in JSON format with question indices as the keys, starting from 0 and question-answer pairs {{\"Question\":...,\"Caption1 Answer\":...,\"Caption2 Answer\":...}} as the values. We engage two human annotators to select from the two candidate answers and Unanswerable [22] for all generated QA pairs, retaining sample only if both annotators agree on the correct answer. In total, we spend around 500 annotator hours to collect all samples at 14 dollars per hour. For the Chinese and Hindi subsets, the authors (who are native speakers of these languages) manually examine all the questions. Additional examples. Figure 7 provides additional examples of NaturalBench. Figure 7: More NaturalBench examples."
        },
        {
            "title": "B NaturalBench Performance",
            "content": "We report model performance on different subsets of NaturalBench. Performance on different subsets. Table 4 reports G-Acc on subsets of NaturalBench. Table 4: Performance on different subsets of NaturalBench. We report the G-Acc performance of 53 leading VLMs on subsets of NaturalBench. Model Image Encoder Language Model NaturalBench Performance Flickr-YN Flickr-MCQ DOCCI-YN DOCCI-MCQ Overall Human Performance Random Chance BLIP-2 [39] InstructBLIP [9] EVA-G EVA-G Otter [33] LlaMA-Adapter-v2.1 [19] CogVLM-Agent-VQA [25] DeepSeek-VL-1.3B-Chat [51] CLIP-L-14 CLIP-L-14 EVA2-E SigLIP-L & SAM-B LLaVA-1.5 [47] ShareGPT4V CogVLM-Chat [77] InternLM-XC-V1 [87] InternLM-XC-V2-1.8B [15] Qwen-VL-Chat [3] Phi-3-Vision [1] mPLUG-Owl2 [81] Bunny [23] mPLUG-Owl2.1 [81] Monkey-10B-chat [41] CLIP-LCLIP-L-14 EVA2-E EVA-G CLIP-L-14 CLIP-G-16 CLIP-L-14 CLIP-L-14 SigLIP-SO CLIP-L-14 OpenCLIP-BigG LLaVA-NeXT [48] CLIP-L-14 DeepSeek-VL-7B-Chat [51] BLIP-3 (XGen-MM) [79] InternVL-Chat-V1.1 [6] InternVL-Chat-V1.5 [7] InternVL-Chat-V1.2-Plus [6] InternVL2-8B [8] Cambrian-1 [72] SigLIP-L & SAM-B CLIP-H-14 InternViT-6B InternViT-6B InternViT-6B InternViT-300M SigLIP-S-14 & CLIP-L-14 DINOv2-g & CLIP-ConvNeXT-XXL InternLM-XC2-4KHD-7B [13] CLIP-L-14 CLIP-L-14 InternLM-XC2-7B [15] InternViT-6B InternVL-Chat-V1.2 [6] InternViT-6B InternVL2-26B [8] LLaVA-OneVision [37] SigLIP-S-14 Llama3.2-Vision [17] ViT-HMolmo [10] CLIP-L-14 Qwen2-VL [76] CLIP-L-14 Open-source Models FlanT5-3B FlanT5-11B Vicuna-7B Vicuna-13B FlanT5-3B FlanT5-11B MPT-7B LlaMA2-7B Vicuna-7B DeepSeek-LLM-1B Vicuna-7B Vicuna-13B Vicuna-7B Vicuna-13B Vicuna-7B InternLM-7B InternLM2-1.8B Qwen-7B Phi-3-Mini LlaMA2-7B Phi-2-2.7B Qwen-7B Qwen-7B Vicuna-7B Mistral-7B Vicuna-13B Nous-Hermes-2-Yi-34B DeepSeek-LLM-7B Phi-3-Mini LlaMA2-13B InternLM2-Chat-20B Nous-Hermes-2-Yi-34B InternLM2.5-7B-Chat Llama-3-8B Vicuna-13B Nous-Hermes-2-Yi-34B InternLM2-7B InternLM2-7B Nous-Hermes-2-Yi-34B InternLM2-Chat-20B Qwen2-0.5B Qwen2-7B Llama-3.1-8B Llama-3.1-70B OLMoE-1B-7B OLMo-7B Qwen2-7B Qwen2-72B Qwen2-1.5B Qwen2-7B Qwen2-72B GPT-4Vision GPT-4o Closed-source Models GPT-4 GPT-4 91.5 6. 2.7 6.1 2.9 7.0 9.6 12.6 3.7 4.2 12.2 7.8 9.1 9.1 10.0 9.5 14.6 11.5 12.0 16.0 15.4 14.0 12.0 12.3 17.1 12.5 13.7 15.7 16.2 13.8 13.7 19.7 22.5 26.5 20.7 16.2 19.6 23.8 22.8 25.4 21.8 26.6 12.0 27.0 16.2 23.7 10.8 14.6 20.6 23.5 17.4 18.8 28.2 22.8 37.5 92.0 6.3 0.8 3.2 0.4 0.4 1.2 2.8 4.0 1.2 2.8 3.6 14.8 21.2 13.2 19.6 15.6 16.8 25.6 16.8 17.6 20.0 16.8 20.0 12.0 17.6 21.6 22.8 32.0 18.8 19.2 21.6 32.8 31.2 34.8 15.6 30.8 35.2 33.2 38.4 34.4 40.4 14.4 32.8 29.2 37.2 15.2 24.0 31.2 38.4 22.8 28.4 36.0 25.2 40.4 92.2 6. 2.3 12.1 6.8 14.8 15.1 18.6 4.5 6.6 13.6 15.5 14.1 15.1 12.9 15.6 14.5 15.2 15.1 16.9 15.3 17.3 18.9 17.4 19.5 14.5 14.6 19.0 20.8 21.6 21.6 16.5 17.4 17.0 19.5 24.6 26.1 23.7 24.3 21.8 24.5 22.1 18.6 26.0 30.0 24.8 14.3 20.6 25.8 25.3 25.6 32.9 40.5 26.9 39.0 93.9 6.3 0.5 1.0 0.5 5.0 0.5 2.5 1.5 0.5 0.5 17.0 16.5 24.0 18.5 23.5 7.5 28.0 26.5 21.5 30.0 25.5 30.0 36.0 24.0 22.0 24.5 26.5 40.0 28.5 30.5 36.0 35.5 29.5 35.0 14.0 35.5 36.5 34.0 34.0 41.0 37.5 17.5 41.5 45.5 53.5 29.0 37.0 44.5 52.5 35.0 48.5 52.0 36.0 48.0 92.1 6. 2.1 7.7 4.0 9.2 9.8 12.7 3.8 4.4 10.3 11.5 12.7 14.8 12.5 14.9 13.9 15.5 16.6 17.1 17.2 17.4 17.4 17.9 18.2 15.0 16.3 19.2 22.7 19.3 19.5 20.3 23.1 23.4 23.5 19.4 25.5 26.6 25.9 26.5 26.6 27.7 15.6 28.8 26.8 29.1 14.7 20.7 26.7 29.3 23.4 29.1 36.9 26.2 39.6 Performance on NaturalBench-Hindi and NaturalBench-Chinese. Table 5 reports the performance on the multilingual subsets of NaturalBench, evaluating only the models that claim to have multilingual capabilities. We also report the performance of these datasets after using ChatGPT to translate the questions and answers into English. This shows that most models are still better at solving English VQA tasks. Ablation on samples generated by different methods. Table 6 reports G-Acc on two types of generated VQA samples: (1) Flickr-Adversarial, generated by sending caption pairs to GPT-4, (2) 18 Table 5: Performance on NaturalBench-Chinese and NaturalBench-Hindi. We report G-Acc for each dataset, evaluating only models with claimed multilingual capabilities. For both datasets, we also provide G-Acc after translating the original Chinese or Hindi questions into English. This simple translation often boosts performance, except for top models like InternVL-Chat-V1.2-Plus and GPT-4o, which seem extensively trained in Chinese. NaturalBench-Hindi remains particularly challenging for open-source models. Model NaturalBench-Chinese NaturalBench-Hindi Chinese English Hindi English Random Chance 6.3 6.3 6.3 Open-source Models DeepSeek-VL-7B-Chat InternVL-Chat-V1.2-Plus InternLM-XC2-7B 10.9 34.6 32.5 28.4 33.4 34.6 0.6 11.5 15.9 GPT-4o Closed-source Models 41. 38.7 40.3 6.3 29.0 36.2 35.6 40.9 Table 6: Ablation on different collection methods. We report G-Acc on datasets generated by different collection methods from Flickr30K. Our adversarial procedure results in much more challenging dataset. Note that Flickr-Adversarial is the combination of Flickr-YN and Flickr-MCQ. Model Model Performance (G-Acc) Flickr-Adversarial Flickr-Random Random Chance 6. Open-source Models DeepSeek-VL-7B-Chat BLIP-3(XGen-MM) LLaVA-NeXT (Mistral-7B) Phi-3-Vision InternVL-Chat-V1.2-Plus InternLM-XC2-7B 15.2 15.2 15.9 16.0 27.8 29.0 GPT-4o Closed-source Models 38. 6.3 80.7 69.0 86.0 75.0 83.0 84.5 72.5 Flickr-Random, generated by sending caption pairs of randomly matched image-text samples to GPT-4. The results confirm that it is crucial to use discriminative VLMs to first search for confounding pairs of image-text samples. Performance on NaturalBench-Retrieval. Table 7 reports model performance on NaturalBenchRetrieval. We only use Flickr image-text samples to construct this benchmark. We adopt the evaluation metrics proposed by Winoground [71]. Table 7: Image-text retrieval performance on NaturalBench-Retrieval. We evaluate CLIP and SigLIP models on the human-verified 1,200 paired (image, text) samples from NaturalBench-Flickr. We follow Winoground [71] to report text score, image score, and group score, with higher numbers indicating better performance for all metrics. We exclude the CLIP (LAION400M-ViT-L14) model used to collect these adversarial pairs. Overall, NaturalBench-Retrieval poses significant challenge to leading discriminative models. Method Source Random CLIP [65] OpenAI LAION DataComp SigLIP [85] WebLI (English portion) Model RN50 RN101 ViT-B-32 RN50x4 RN50x16 ViT-L-14 RN50x64 roberta-ViT-B-32 ViT-H-14 ViT-g-14 ViT-bigG-14 xlm-roberta-base-ViT-B-32 xlm-roberta-large-ViT-H-14 small: ViT-B-32 medium: ViT-B-32 large: ViT-B-16 xlarge: ViT-L-14 ViT-B ViT-L ViT-SOViT Data Size Model Size (M) Retrieval Performance Group Image Text 400M 2B 5B 13M 128M 1B 13B 13B 102 120 151 178 291 428 623 212 986 1367 2540 366 151 151 150 428 172 430 800 16.67 12.22 13.61 15.89 14.75 24.61 23.15 26.24 16.22 24.04 21.35 21.04 16.79 22.82 12.06 16.95 16.71 21. 24.29 31.21 42.14 25.00 32.60 35.04 36.43 37.49 44.01 44.99 46.21 39.36 49.31 46.21 44.49 37.49 47.35 22.90 28.28 36.43 44.01 48.57 54.93 62. 25.00 36.76 33.33 36.92 36.27 43.93 41.81 47.35 38.79 48.82 46.54 43.69 40.91 47.51 21.19 33.01 35.86 45.72 49.06 54.44 63."
        },
        {
            "title": "C Skill Analysis",
            "content": "We now provide the skill definitions and report model performance by each skill tag. Skill definitions and examples. Table 8 provides definitions to the skills in NaturalBench. Skill analysis. Table 9 reports Q-Acc performance (awarding one point if the model answers both images correctly for each question) on Object and Attribute tags. Table 10 reports Q-Acc performance on Relation and Reasoning tags. Additional examples. We provide additional tagging examples in Figure 8. We will release these tags for more fine-grained analysis, such as evaluating models on combinations of skills. Figure 8: More NaturalBench examples with skill tags. Skill Type Definition Examples Table 8: Skill definitions. Object Attribute Spatial Relation Basic entities within an image, including animals, humans, food, buildings, natural elements (nature), vehicles, common items, and others. Visual properties of entities, including emotion, shape, size, color, state, activity, gender, and abstract attributes (e.g., helpful, lucky). Physical arrangements of multiple entities relative to each other [46], including proximity (e.g., near, far), topological (e.g., at, on, in, with, surround, between, inside, outside) , projective (e.g., left of, right of, under, in front of, below), orientation and direction (e.g., facing, towards, across, away from). Action Relation Action interactions between entities, e.g., pushing, kissing, hugging, hitting, helping, and so on. Is there car parked near the path? Is there person in this image? Is there referee behind the table? Is the dog fully submerged in the water except for its head? Is the water body filled with visible rocks and emanating ripples? Is anyone in the picture sad or scared? Is the woman extremely surprised? Is the woman alone behind glass partition? Is the man wearing brown? Is the man wearing red and white striped apron? Is the old man in the image wearing reflective safety jackets? Is there referee behind the table? Is the dog looking up at the sky? Is there only one person in the canoe? Is there group of people standing outside the gates? Is the man in the image looking at the object to his left? Is the smiling woman standing next to the bus? Is there person holding water bottle? Is the black dog biting stick? Is anyone using an umbrella? Is the man holding red pen? Is the dog chasing after toy outdoors? Is the person jumping directly off building without any equipment? Part Relation Part-whole relationships between entities one entity is component of another, such as body part, clothing, and accessories. Is there person wearing orange and yellow shirt and jacket? Is anyone wearing yellow and orange safety vests? Is the woman in the black dress wearing gloves? Is player using his back to play the ball? Is the boys tongue sticking out? Counting Determining the quantity, size, or volume of entities, e.g., objects, attribute-object pairs, and object-relation-object triplets. Are there four people in the image? Does the dog have two visible colors? Are there more than four performers in the image? Differentiation Differentiating objects within category by their attributes or relations, such as distinguishing between old and young people by age, or the cat on top of the table versus the cat under the table by their spatial relations. Does the girl on the left look sad while the girl on the right look happy? Is there cat sitting on grey cabinet in front of another cat sitting on the stairs? Is one dog biting the ear of the other dog? Is man standing behind another man sitting at desk? Comparison Comparing characteristics like number, attributes, area, or volume between entities. Does the scene involve players from three different team colors? Does the tallest building feature glass windows and side slopes? Is the older person following the younger one? Are there two dogs that are significantly different in size? Is the man wearing the same color as the woman in the image? Logic Understanding logical operators. We only consider negation (as indicated by no, not, or without) and universality (as indicated by every, all, each, both). Other logical relations such as conjunction (as indicated by and, or) are omitted. Does the image show all men performing the same action? Are both people looking in the same direction? Is the bicycle rider performing trick without any audience? Is the main subject not wearing shirt and lying down? Is the main activity potentially related to craft or construction? World Knowledge Answering based on external commonsense knowledge, including social, symbolic, functional, physical, natural knowledge and so on. Is the event related to the Olympics? Is there vertical depiction of Ramses III in the image? Does the image suggest relatively informal social gathering? Is single individual attempting to score regardless of multiple defenders? 22 Table 9: Model performance on Object and Attribute. We report Q-Acc on each tag. Model Object Attribute Animal Human Food Building Nature Vehicle Items Others Emotion Shape Size Color State Abstract Activity Gender BLIP-3(XGen-MM) Phi-3-Vision DeepSeek-VL-7B-Chat LLaVA-NeXT(Mistral-7B) InternLM-XC-V2-7B InternVL-Chat-V1.2-Plus GPT-4o 18.6 15.6 20.9 14.2 23.3 23.9 35.4 16.2 17.1 16.9 16.1 28.6 28.0 39.7 15.4 15.4 15.4 17.3 19.2 23.1 44.2 20.8 17.7 21.9 14.0 30.8 20.3 40. 21.7 15.6 22.1 13.4 23.6 18.5 41.3 22.2 19.0 16.7 18.1 30.6 22.7 38.4 21.2 18.5 19.3 16.7 27.8 25.4 42.8 17.6 16.7 19.0 15.2 29.0 19.7 38.3 9.1 18.2 12.1 15.2 33.3 21.2 39.4 19.3 17.5 24.6 19.3 31.6 17.0 42. 24.1 19.0 21.4 14.6 30.2 20.0 40.7 21.8 18.9 20.8 16.3 27.8 24.8 39.0 20.2 16.8 19.5 15.7 25.8 22.8 41.1 20.4 15.6 16.7 14.1 23.3 19.3 38.9 16.5 15.2 20.1 14.4 27.0 26.2 35.5 14.0 15.8 14.6 17.9 30.1 30.4 43. Table 10: Model performance on Relation and Reasoning. We report Q-Acc on each tag. Model Relation Reasoning Action Part Proximity Topological Projective Orientation Count Logic Differ Compar World BLIP-3(XGen-MM) Phi-3-Vision DeepSeek-VL-7B-Chat LLaVA-NeXT(Mistral-7B) InternLM-XC-V2-7B InternVL-Chat-V1.2-Plus GPT-4o 18.3 16.0 17.5 15.9 27.3 23.6 39.4 17.4 19.5 16.2 18.6 29.3 28.1 43.1 27.5 19.6 29.4 18.6 29.4 31.4 40. 22.8 17.9 21.4 17.0 27.9 24.4 41.7 19.6 13.9 17.9 16.1 24.4 19.3 38.7 15.5 9.5 14.7 13.8 24.1 18.1 35.3 20.6 16.1 19.6 17.1 30.7 23.9 39.2 15.9 18.5 16.4 21.2 25.9 26.9 42.9 13.0 17.6 11.1 17.6 27.8 25.0 38. 20.9 13.0 11.3 12.2 27.8 15.7 37.4 5.3 8.5 10.6 9.6 17.0 12.8 35."
        },
        {
            "title": "D Debiasing Analysis",
            "content": "In the main paper, we show that debiasing within the image-text pairings significantly improves model performance. Here, we explore debiasing techniques that dont rely on knowing the image-question pairings. Deterministic evaluation using answer likelihood [44]. Recall that we can perform scoring-based evaluation strategy using the generative likelihood of each candidate answer (VQAScore [44]) to determine the models predicted answer. Specifically, given question q, an image i, and two candidate answers a0 and a1, we evaluate: (a0q, i) (a1q, i) > τ (4) where τ is threshold (default is 0). If this condition (Eq. 4) is met, the model predicts a0; otherwise, it predicts a1. Crucially, this formulation has two benefits: (1) it produces deterministic results that are almost consistent with stochastic decoding (see Table 11), and (2) it allows us to adjust τ [1, 1] for debiasing. Recall that our main paper performs sample-level debiasing by optimizing τ within each of the four image-question pairs. Alternatively, we can perform global-level debiasing by searching for single τ that maximizes G-Acc across all samples. We also implement the post-hoc debiasing technique proposed in [88], which is equivalent to: (a0q, i) (a0q) (a1q, i) (a1q) > 0 (5) where (aq) is estimated by sending no image tokens but just the question tokens to the VLM. Table 11 shows that these alternate techniques still lag behind the performance of sample-level debiasing. We hope NaturalBench can be useful testbed for bias mitigation techniques for VLMs. Table 11: Evaluating debiasing techniques on NaturalBench. We evaluate debiasing techniques (as detailed in Section 5) that do not require prior knowledge of image-question pairings (unlike sample optimal τ ). For comprehensiveness, we report both stochastic decoding and deterministic evaluation using VQAScore, finding consistent results. We observe that the two post-hoc methods global-optimal τ and Post-Hoc debiasing perform significantly worse than the (oracle) sampleoptimal τ . Global optimal τ shows only slight improvements, while Post-Hoc debiasing even reduces performance in models like Bunny, InterVL-Chat-V1.2, and GPT-4o. This suggests NaturalBench can be valuable benchmark for testing future debiasing methods. Model Stochastic Decoding Deterministic VQAScore Post-hoc Debiasing [88] Global Optimal τ Sample Optimal τ Q-Acc I-Acc G-Acc Q-Acc I-Acc G-Acc Q-Acc I-Acc G-Acc Q-Acc I-Acc G-Acc Q-Acc I-Acc G-Acc LLaVA-1.5 (Vicuna-7B) LLaVA-1.5 (Vicuna-13B) Phi3-Vision Bunny LLaVA-NeXT (Vicuna-7B) LLaVA-NeXT (Mistral-7B) LLaVA-NeXT (Vicuna-13B) DeepSeek-VL-7B-Chat BLIP-3(XGen-MM) InternVL-Chat-V1.5 InternVL-Chat-V1.2-Plus InternVL2-8B InternVL-Chat-V1.2 InternVL2-26B LLaVA-OneVision (Qwen2-0.5B) LLaVA-OneVision (Qwen2-7B) GPT-4o 37.7 39.6 43.4 42.3 42.5 44.6 45.9 46.0 47.0 52.3 52.7 50.5 52.9 55.9 39.8 56.2 64.4 43.8 44.6 48.7 48.4 47.6 49.1 49.9 50.1 51.2 55.9 56.2 54.5 56.4 58.8 46.3 58.8 66. 12.7 14.8 17.2 17.4 15.0 16.3 19.2 19.3 19.5 23.1 23.4 23.6 26.6 28.1 15.7 28.9 39.6 36.7 38.6 43.6 42.5 42.0 45.0 44.6 45.8 46.8 52.6 52.6 50.4 52.6 55.7 39.1 55.4 65.0 42.7 43.5 48.9 48.5 47.1 49.4 48.5 49.9 51.1 56.0 56.3 54.3 56.0 58.5 44.6 58.2 67.0 12.2 14.4 17.7 17.5 15.0 17.0 18.2 19.4 19.5 24.3 23.5 23.7 26.2 28.2 14.5 28.6 40.5 38.2 38.5 45.1 38.7 44.2 46.8 48.7 - 47.8 55.2 55.9 52.2 52.3 58.8 39.1 59.1 61.6 44.5 42.8 48.6 44.9 48.9 51.1 52.5 - 52.0 58.4 58.6 55.9 54.3 61.1 44.5 61.2 63. 13.9 14.5 19.3 15.7 18.0 19.6 21.5 - 22.4 28.6 28.3 25.5 25.8 32.0 15.8 33.2 37.6 39.9 42.8 44.7 43.6 43.4 45.3 47.8 46.4 48.7 52.3 53.0 50.4 53.6 55.7 39.2 56.1 64.9 45.8 47.8 49.3 49.5 48.5 49.7 52.1 50.4 53.2 55.6 56.1 54.3 56.8 58.3 46.3 59.0 67.1 14.0 16.5 18.4 18.7 16.5 17.4 20.4 19.7 21.4 25.0 24.6 23.7 27.2 28.5 16.2 28.7 40.7 83.4 86.2 85.7 85.8 86.7 88.3 89.1 86.6 88.6 92.3 92.4 88.7 91.6 92.2 84.6 92.1 94.0 76.3 78.6 78.5 78.6 79.6 81.6 82.3 81.8 81.9 86.1 85.5 83.2 86.0 87.2 77.2 87.2 90. 44.3 49.7 50.0 50.5 50.3 56.0 57.2 54.8 55.3 66.0 65.3 58.6 65.8 67.7 47.5 67.8 75."
        },
        {
            "title": "Checklist",
            "content": "1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? [Yes] The main claims are reflected in the papers contributions and scope. See Section 3 to Section 6. (b) Did you describe the limitations of your work? [Yes] See Section 5. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 5. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments (e.g. for benchmarks)... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as URL)? [Yes] See Section 3 for detailed benchmark collection pipeline. We have released the code in our project site. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See supplement. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See supplement. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data youre using/curating? [No] The assets used are all publicly sourced, and therefore explicit consent was not required. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] The data used are publicly available datasets that do not contain offensive content and with consent for personally identifiable information. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Section 3. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] We pay the participants above the minimum wage with an hourly pay."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "University of Washington"
    ]
}