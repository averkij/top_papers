{
    "paper_title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model",
    "authors": [
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Cao",
        "Ziyu Liu",
        "Shengyuan Ding",
        "Shenxi Wu",
        "Yubo Ma",
        "Haodong Duan",
        "Wenwei Zhang",
        "Kai Chen",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, a critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up a high-quality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5-Reward: (1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data. To ensure reproducibility and facilitate further research, we have open-sourced all model weights and training recipes at https://github.com/InternLM/InternLM-XComposer"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 8 6 3 2 1 . 1 0 5 2 : r InternLM-XComposer2.5-Reward: Simple Yet Effective Multi-Modal Reward Model Yuhang Zang1, Xiaoyi Dong1,2, Pan Zhang1, Yuhang Cao1, Ziyu Liu1,3, Shengyuan Ding1,4, Shenxi Wu1,5, Yubo Ma1,6, Haodong Duan1, Wenwei Zhang1, Kai Chen1, Dahua Lin1,2, Jiaqi Wang1,(cid:66) 1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong, 3 Shanghai Jiao Tong University, 4 Nanjing University, 5 Fudan University, 6 Nanyang Technological University openixclab@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up highquality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5Reward: (1) Providing supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data. To ensure reproducibility and facilitate further research, we have open-sourced all model weights and training recipes at https : / / github.com/InternLM/InternLMXComposer. * indicates equal contribution. indicates interns at IXCLab, Shanghai AI Laboratory 1. Introduction If you dont know where you are going, youll end up some place else. Yogi Berra Reward Models (RMs) [8, 48, 55, 78, 88, 91, 92, 98, 104, 105, 121] provide the crucial direction guidance about how well an AI models outputs align with human preference, and benefit Large Language Models (LLMs) in training and inference. During training, RMs are often used with reinforcement learning from human feedback (RLHF) [7, 66, 70, 71] to penalize undesirable model behaviors and encourage outputs that align with human values. At inference, RMs facilitate test-time scaling strategies [27, 80], such as selecting the best response from candidate outputs or providing step-by-step critiques for complex reasoning tasks [29, 107]. Despite their crucial role in both training and inference, multi-modal RMs for Large Vision Language Models (LVLMs) remain notably underexplored compared to language-only RMs for LLMs. Because current preference data is predominantly text-based and skewed toward specific domains (e.g., safety), data scarcity poses significant challenge to training multi-modal RMs for diverse modalities such as images, videos, and text. Consequently, existing multi-modal RMs [87, 95] are largely constrained to narrow domains (e.g., mitigating hallucinations) or rely on prompting LVLMs with evaluation prompts, effectively functioning as generative RMs [94]. The limitation of multi-modal RMs subsequently constrains the capabilities of open-source LVLMs such as instruction following and thereby hampering user interaction safety-should-refuse, experience in multi-modal chat scenarios. The growing community interest in RLHF and test-time 1 Figure 1. (a) To train the IXC-2.5-Reward, we construct multi-modal preference dataset spanning diverse domains (e.g., natural scenes, text-rich, reasoning) and modalities (image, text, video). (b) The framework of IXC-2.5-Reward. (c) The IXC-2.5-Reward guides policy training for IXC-2.5-Chat via reinforcement learning. scaling highlights the need for multi-modal RMs, which motivates us to present InternLM-XComposer2.5-Reward Instead of directly transferring uni- (IXC-2.5-Reward). modal (text) reward models (RMs) to the vision modality, we augment the existing LVLM (InternLM-XComposer2.5) with an additional scoring head to predict reward scores. An effective multi-modal RM should ideally possess two key properties: (1) the ability to predict reward scores for both image, video, and textual inputs and (2) the capacity to generalize across diverse domains, such as instruction following, knowledge, text-rich images (e.g., documents), reasoning tasks, etc. To this end, we develop pipeline (Fig. 1(a)) to construct multi-modal preference data, and also incorporate existing high-quality datasets. This pipeline selects prompts across diverse domains for text, image, and video inputs, generates corresponding responses, and then uses GPT-4o [31] or verifier [40] to perform preference judgments. Trained on our preference data, IXC-2.5-Reward effectively evaluates both visual (image and video) and textual inputs (Fig. 1 (b)). IXC-2.5-Reward achieves best performance on multimodal VL-RewardBench [43] (70.0%) that beat all previous generative RMs including Gemini-1.5-Pro (62.5%) and GPT-4o (62.4%). Even on uni-modal (text) RM benchmarks, IXC-2.5-Reward also demonstrates good results, with an average score of 88.6% on Reward-Bench [41] and 68.8% on RM-Bench [51]. We further demonstrate the effectiveness of IXC-2.5Reward in the following three aspects: (1) IXC-2.5-Reward for RL training. We train chat model (IXC-2.5-Chat) using the on-policy Proximal Policy Optimization (PPO) algorithm with IXC-2.5-Reward to enhance its ability to follow instructions and provide better user experience in multi-modal conversations. Our results show clear improvements of IXC-2.5-Chat on multi-modal instruction following and in-the-wild chatting benchmarks, which validate the effectiveness of IXC-2.5-Reward for providing the reward signal during RL training. (2) IXC-2.5-Reward for Test-Time Scaling. Using bestof-N sampling with IXC-2.5-Reward leads to additional performance gains compared to the RL-trained IXC-2.5Chat, confirming IXC-2.5-Rewards effectiveness in selecting good responses from candidate responses. (3) IXC-2.5-Reward for Data Cleaning. We observe strong correlation between low IXC-2.5-Reward scores and problematic samples, such as those exhibiting hallucinations or mismatched image/video and question/answer content. This suggests that IXC-2.5-Reward can effectively clean LVLM pre-training and post-training data. All code and models of IXC-2.5-Reward and IXC-2.5Chat are publicly available at https://github.com/ InternLM/InternLM-XComposer. 2. Related Work Reward Model in Large Language Models Reward models (RMs) are crucial for both Reinforcement Learning from Human Feedback (RLHF) [7, 66] and Test-time Scaling Laws [29, 80]. RMs have different implementation forms, such as (1) discriminative RM [8, 48, 55, 88, 92, 98, 104, 121], usually sequence classifier that classifies input sequences into different categories, such as binary classification (good or bad,) or on more granular scale [88, 92]. (2) generative RM [39, 78, 91, 105] that are prompted to generate the feedback in the form of text, often critique or explanation of why certain output is good or bad. (3) implicit RMs [32, 40] that are models optimized using DPO [70] that the predicted log probabilities are interpreted as implicit reward signal. Besides, RMs can also be divided into Outcome RMs (ORMs) [16] and Process RMs (PRMs) [46, 75, 86]. Our IXC-2.5-Reward belongs to the discriminative RM and ORM. Reward Model in Large Vision-Language Models Previous RMs for LVLMs [87, 94, 95] are limited to specific domains (e.g., reducing hallucination) or developed using relatively weak base models, which makes the implemented models significantly inferior to LLM RMs. The lack of effective multi-modal RMs has created bottleneck in vision RLHF, forcing researchers to merely use the variants of the off-poly DPO algorithm [70]. Previous work using open-source LVLMs as generative RMs [65, 95, 102], injection of hallucinations with data augmentation techniques [21, 22, 25, 33, 67, 120, 122] and rule-based selection [9, 53] for DPO data selection, which potentially compromise performance compared to the on-policy RL solutions like PPO [71]. Moreover, lacking multi-modal RMs has also led to the reliance on human annotation [82, 100] or the use of proprietary models [109, 116] like GPT4 as generative RMs for DPO pair selection, which is expensive and unsustainable for large-scale applications. Although opensource RMs for LVLMs have lagged behind their LLM counterparts, the growing community interest highlights the need for multi-modal RMs, which motivates our work. In this work, we demonstrate that IXC-2.5-Reward is capable of combining with the PPO training and for DPO data selection at low cost. Reward Model Evaluations The development of evaluation benchmarks is essential for improving RMs. Several comprehensive benchmarks have been proposed for evaluating RMs of LLMs, such as general abilities [41, 51, 119], multilingual [28, 81], RAG [34], and mathematical process reward [117]. The limited availability of multi-modal RMs has hampered the development of evaluation benchmarks, with existing benchmark [43] focusing solely on generative RMs and lacking the evaluation of process supervision. However, given the critical importance of RMs, we expect significant progress in multi-modal RM benchmarking in the future. 3. IXC2.5-Reward Data Preparation Reward models are trained using pairwise preference annotations (e.g., prompts with chosen responses yc and rejected responses yr) that reflect human preferences. While existing public preference data is primarily textual, with limited image and scarce video examples, we train IXC-2.5-Reward using both open-source data and newly collected dataset to ensure broader domain coverage. Tab. 1 lists the open-source pairwise data used in IXC2.5-Reward, primarily focused on instruction following, safety, and general knowledge. Tab. 2 details the source of our newly collected data, which is initially the supervised fine-tuning (SFT) data consisting of prompts and corresponding chosen responses yc across diverse domains: text3 Table 1. Overview of existing preference datasets used in IXC-2.5-Reward. IF denotes to Instruction Following. Table 2. Overview of the source of newly collected data used in IXC2.5-Reward."
        },
        {
            "title": "IF General",
            "content": "Tulu-3-IF-augmented-on-policy-8b [40] UltraFeedback [17]"
        },
        {
            "title": "IF General",
            "content": "in-house (will release) KVQA [76], A-OKVQA [73], PMC-VQA [114]"
        },
        {
            "title": "General",
            "content": "hhh alignment [5], PKU-Safe [18] SHP [24], Anthropic-hhrlhf [6]"
        },
        {
            "title": "Image",
            "content": "WildVision-Battle [62] LLaVA-Critic [94], VL-Feedback [44], RLAIF-V [101] MIA-DPO [54] Text-Rich"
        },
        {
            "title": "Reasoning",
            "content": "AI2D [37], IconQA [56], TQA [38] ChartQA [63], DVQA [36], ScienceQA [57] GeoQA [11], CLEVR-Math [47] Super-CLEVR [45], TabMWP [58]"
        },
        {
            "title": "General",
            "content": "TrafficQA [96], FunQA [93], MiraData [35]"
        },
        {
            "title": "Video",
            "content": "rich document understanding, math reasoning, and video understanding. We also collect some in-house data about the instruction following, which will be released in the future. To obtain rejected responses yr, we prompt the SFT model, InternLM-XComposer-2.5 (IXC-2.5) [112] to generate multiple outputs for each prompt and then employ distinct selection criteria. For general and text-rich data, we use GPT-4o [31] with pairwise evaluation prompts to determine the rejected response that was evaluated worse than the SFT ground-truth answer. For math reasoning and instruction following data, we build verifier functions [40] that compare generated responses against ground-truth solutions to label the chosen and rejected data. Our newly collected data complements existing open-source data, creating comprehensive, high-quality multi-modal preference dataset. reward model Model Architecture Our InternLMXComposer 2.5-Reward (IXC-2.5-Reward) is built upon the SFT model (IXC-2.5) [111]. As shown in Fig. 1 (b), we use the pre-trained weights of IXC-2.5-Chat for most of the parts, such as the visual encoder and the MLP projector, which has aligned the image and video data with text modalities. Thus, the IXC-2.5-Reward is merely required to train preference data to predict the reward score and avoid using other pre-training data for modality alignment. We replace the final linear layer of IXC-2.5 with score head for IXC-2.5-Reward that predicts the reward score. Given the input prompt and the response y, the score head transforms the averaged hidden state features of all tokens into binary scalar r(x, y). This scalar value r(x, y) serves as the predicted reward score for the inputs. Loss Function Our reward model is trained via the following loss function: LRM = E(log(σ(r(x, yw)) r(x, yl))), (1) where r(x, yw) and r(x, yl) denotes to the reward score assigned to the prompt with the chosen data yw and rejected data yl, respectively. Training Strategy As shown in Fig. 1 (b), we froze the models vision encoder and projector that are initialized training only the LLM (InternLM from IXC-2.5 [112], [112]) and the score head. Other components of IXC2.5, such as the dynamic image partitioning mechanism for high-resolution inputs, remained unchanged. Length Constraints We remove data pairs where the length of the chosen response yw is significantly longer than the length of the rejected response yl. This helps prevent the reward model from learning to associate length with quality. Notably, we found that the vulnerability of LLM-based evaluation to length bias, known issue in LLMs [23], has also significant implications for LVLMs. Specifically, openended Visual Question Answering (VQA) benchmarks that employ LVLMs (e.g., GPT-4o) as judges are susceptible to inflated scores from overly long responses. Consequently, removing the length constraint on the reward model resulted in improved PPO policy performance. detailed analysis is provided in Tab. 7. 4. The Applications of IXC-2.5-Reward In this section, we further validate three applications of IXC-2.5-Reward for (1) RL training (Sec. 4.1), (2) test-time scaling (Sec. 4.2), and (3) data cleaning (Sec. 4.3). 4.1. IXC-2.5-Reward for RL training Having the reward model IXC-2.5-Reward enables the application of on-policy reinforcement learning algorithms (e.g., PPO [71], RLOO [2], GRPO [77]) to optimize LVLM performance towards desired human preferences directly. Using the PPO [71] algorithm, we subsequently train the policy model (IXC-2.5-Chat, πθ) to maximize expected 4 Figure 2. Using IXC-2.5-Reward for Data Cleaning. We visualize the outlier and noisy examples detected by IXC-2.5-Reward with low reward scores from existing image and video instruction-tuning datasets, such as ALLaVA [10] and LLaVA-Video-178K [115]. The Explain refers to explanations of error causes as identified by human experts, rather than outputs generated by the reward model. rewards from our reward model (IXC-2.5-Reward) while staying close to the reference model (IXC-2.5, πref) for stability. critic model , initialized from IXC-2.5-Reward, is trained alongside πθ to reduce the variance of policy updates. Data Prepration Similar to findings in [30], we found that average reward scores differ across task domains (e.g., general, text-rich, reasoning). This work focuses on improving the policy models instruction following and openended chat abilities, which are critical for real-world applications such as stream chatting and human-AI interaction [110]. Simultaneously, we ensure that performance in other domains (e.g., text-rich, reasoning) is not degraded relative to the SFT model IXC-2.5. Using our multi-modal preference data (which trains IXC-2.5-Reward), we curate prompt set that prioritizes general chat and instruction following, while ensuring diversity through the inclusion of text-rich documents, math reasoning, and video understanding. PPO The PPO training begins by sampling prompt from our prompt set. Then, the policy θπ model generates responses, and the reward model computes the reward score rt at each state st at the time-step t. Given the reward score rt and and the critic model , we compute the temporal difference error δt, the Generalized Advantage Estimation (GAE) [72] At, and the Returns Rt as: δt = rt + γ (st+1) (st), At = δt + γ β At+1, Rt = At + (st), (2) 5 Table 3. Evaluation results on VLRewardBench [43]. The best and second-best results for proprietary models and open-source models are highlighted in bold and underlined, respectively."
        },
        {
            "title": "Models",
            "content": "#Param General Hallucination Reasoning Overall Acc Macro Acc Gemini-1.5-Flash (2024-09-24) [83] Gemini-1.5-Pro (2024-09-24) [83] Claude-3.5-Sonnet (2024-06-22) [4] GPT-4o-mini (2024-07-18) [3] GPT-4o (2024-08-06) [3] - - - - - LLaVA-OneVision-7B-ov [42] Qwen2-VL-7B [90] Molmo-7B [20] InternVL2-8B [85] LLaVA-Critic-8B [94] Llama-3.2-11B [84] Pixtral-12B [1] Molmo-72B [20] Qwen2-VL-72B [90] NVLM-D-72B [19] Llama-3.2-90B [84] IXC-2.5-Reward (Ours) 7B 7B 7B 8B 8B 11B 12B 72B 72B 72B 90B 7B"
        },
        {
            "title": "Proprietary Models",
            "content": "47.8 50.8 43.4 41.7 49.1 59.6 72.5 55.0 34.5 67.6 Open-Source Models 32.2 31.6 31.1 35.6 54.6 33.3 35.6 33.9 38.1 38.9 42.6 84.7 20.1 19.1 31.8 41.1 38.3 38.4 25.9 42.3 32.8 31.6 57. 62.5 58.4 64.2 62.3 58.2 70.5 57.1 51.1 56.2 59.0 59.1 56.6 59.9 54.9 58.0 62.0 61.7 62.9 57.6 67.2 55.3 41.5 65.8 29.6 28.3 37.5 44.5 41.2 42.9 35.8 44.1 39.5 40.1 56. 65.8 55.3 62.5 53.6 44.8 62.4 36.5 33.9 39.7 45.2 44.0 42.8 40.4 43.7 43.0 44.1 53.9 70.0 where γ is discount factor that determines how much future rewards are valued compared to immediate rewards, and β is the parameter that controls the trade-off between bias and variance in the advantage estimation. The advantage refers to how much better the policy model did than expected, and the returns is the cumulative reward. Based on the advantage A, we compute the policy gradient loss LPG to update the policy model πθ: LPG = min( πθ πref A, clip( πθ πref , 1.0 ϵ, 1.0 + ϵ) A), (3) where πθ is the log of the probability ratio between the πref policy model πθ and the reference model πref, and ϵ is hyper-parameter that controls the clipped ratio. We further update the critic model via the Mean Squared Error (MSE) loss to minimize the difference between the predicted value of state (st) and the actual return Rt obtained from state t: Lcritic = (cid:88) MSE(V (st), Rt). (4) In summary, with the help of IXC-2.5-Reward and PPO, we train the IXC-2.5-Chat to generate responses that improve the quality of multi-modal chat and follow user instructions. The quality of IXC-2.5-Chat also demonstrates the quality of IXC-2.5-Reward that provides the reward scores. 4.2. IXC-2.5-Reward for Test-Time Scaling We further demonstrate that IXC-2.5-Reward is essential for scaling the inference-time capabilities of LVLMs. We select the Best-of-N (BoN) sampling technique that improves the quality of generated text by using the reward model. Specifically, the IXC-2.5-Chat model generates multiple (N ) different text outputs with different random seeds for given prompt. Subsequently, the reward model IXC-2.5-Reward scores each of these outputs, and the output with the highest score from the reward model is selected as the final output. 4.3. IXC-2.5-Reward for Data Cleaning Garbage in, garbage out. Problematic samples in instruction tuning datasets negatively impact LVLM training. While existing methods [13] employ classifiers like CLIP [69] for filtering, these approaches have limitations, particularly with long-context inputs [108], high-resolution images, or videos. As shown in Fig. 2, we observe strong correlation between low IXC-2.5-Reward scores and problematic samples, including hallucinations, empty answers, and irrelevant image/video-text pairings. Therefore, IXC-2.5-Reward effectively cleans both pre-training and post-training data for LVLMs. 6 Table 4. Evaluation results on Reward Bench [41]. We report the performance of selective representative language-only RMs and previous multi-modal generative RMs."
        },
        {
            "title": "Safety Reasoning Avg Score",
            "content": "Language-Only Reward Models InternLM2-7B-Reward [8] InternLM2-20B-Reward [8] Skyword-Reward-Llama3.1-8B [48] INF-ORM-Llama3.1-70B [97] InternLM2-7B InternLM2-20B Llama3.1-8B Llama3.1-70B 99.2 98.9 95.8 96.6 69.5 76.5 87.3 91.0 QWen2-VL-7B [90] LLaVA-Critic-8B [94] IXC-2.5-Reward (Ours) Multi-Modal Reward Models QWen2-7B LLaMA3-7B InternL2-7B 96.6 96.9 90. 57.0 52.8 83.8 87.2 89.5 90.8 93.6 73.9 81.7 87.8 94.5 95.8 96.2 99. 94.3 83.5 90.0 87.6 90.2 92.5 95.1 83.8 80.0 88.6 Table 5. Evaluation results on RM-Bench [51]. We classify reward models into three types: sequence classifiers (Seq.), generative models, and implicit DPO models. Performance is reported across four domains (Chat, Math, Code, Safety) and three difficulty levels (Easy, Normal, Hard), along with average scores (Avg)."
        },
        {
            "title": "Easy Normal Hard Avg",
            "content": "Language-Only Reward Models Tulu-2-dpo-13b [32] InternLM2-7B-Reward [8] InternLM2-20B-Reward [8] Nemotron-4-340B-Reward [92] URM-LLaMa-3.1-8B [55] Skyword-Reward-Llama3.1-8B [48] Implicit Seq. Seq. Generative Seq. Seq. 66.4 61.7 63.1 71.2 71.2 69.5 51.4 71.4 66.8 59.8 61.8 60.6 51.8 49.7 56.7 59.4 54.1 54. 85.4 85.5 86.5 87.5 93.1 95.7 86.9 85.4 82.6 81.0 84.0 89.0 66.7 70.7 71.6 71.4 73.2 74.7 37.7 45.1 50.7 56.1 53.0 46.6 63.8 67.1 68.3 69.5 70.0 70.1 Multi-Modal Reward Models IXC-2.5-Reward (Ours) Seq. 65.5 55.9 51.7 93. 87.5 71.3 47.4 68.8 5. Experiments Implementation Details For IXC-2.5-Reward, the learning rates were set at 1e-5 with batch size of 256. As for IXC-2.5-Chat, the learning rates were set at 5e-5 with batch size of 256. We set the PPO hyper-parameters γ = 0.99, β = 0.95, and ϵ = 0.2. 5.1. Evaluation Results of IXC-2.5-Reward Benchmarks To evaluate IXC-2.5-Reward, we use di- (1) VL-RewardBench verse reward model benchmarks: [43], encompassing 1250 multi-modal problems addressing general understanding, hallucination, and reasoning challenges; (2) Reward-Bench [41], with 2985 language-only problems including chat, chat hard, safety and reasoning; and (3) RM-Bench [51], comprising 1237 language-only problems across chat, math, code, and safety. RM-Bench defines three tracks (easy, normal, hard) that evaluate the sensitivity of reward models to subtle content variations and style biases. While Reward-Bench and RM-Bench are designed for reward models of language-only LLMs, we evaluate IXC-2.5-Reward on these benchmarks to demonstrate that our multi-modal reward model maintains strong language capabilities despite also processing image and video inputs. 5.1.1. Results on VL-RewardBench Main Results Tab. 3 presents the evaluation results of various multi-modal RMs on the VL-RewardBench [43]. Unlike previous multi-modal generative reward models, our IXC-2.5-Reward is discriminative model that predicts scalar reward. Our proposed IXC-2.5-Reward model, despite being an open-source 7B parameter model, outperforms all other open-source models. Notably, IXC2.5-Reward achieves the highest overall accuracy (65.8%) among open-source models and the highest Macro Acindicating its supecuracy (70.0%) among all models, rior performance in handling diverse tasks within the VLRewardBench. Table 6. Evaluation results of our IXC-2.5-Chat model against previous SOTA proprietary and open-source models 10B (results are copied from OpenVLM Leaderboard and Open LMM Reasoning Leaderboard, accessed 01-Jan-2025). Best and second best results are highlighted."
        },
        {
            "title": "Evaluation",
            "content": "Instruction WildVision(0617) [62] Following & Chat MIA(val) [68] MM-MT(val) [1] MM-Vet v2(0613) [103] Knowledge MMBench(v1.1) [52] MMMU(val) [106] MMStar [12] Reasoning MathVista(mini) [59] MathVerse(vision-only) [113] MathVision(full) [89] Text-Rich TextVQA(val) [79] ChartQA(test) [63] OCRBench [49]"
        },
        {
            "title": "VQA\nVQA\nVQA",
            "content": "Proprietary API Previous-SOTA Previous-SOTA IXC-2.5 Open-Source Model (10B) 89.2 [31] 88.6 [31] 7.72 [31] 71.8 [4] 85.7 [74] 70.7 [31] 72.7 [74] 78.4 [74] 54.8 [26] 43.6 [26] 82.0 [64] 81.2 [64] 89.4 [74] 67.3 [94] 80.7 [90] 5.45 [90] 58.1 [14] 82.7 [61] 56.2 [14] 63.2 [14] 66.5 [60] 26.6 [50] 22.0 [50] 78.5 [42] 82.4 [99] 82.2 [14] 37.5 80.4 3.85 45.8 79.4 42.9 59. 63.7 16.2 17.8 78.2 82.2 69.0 IXC-2.5-Chat 74.6 84.0 5.70 54.8 79.0 44.1 59.6 63.4 19.0 18. 81.3 80.5 70.0 Strong Performance on General Problems The results in Table 3 reveal that IXC-2.5-Reward achieves significantly higher accuracy (84.7%) on general problems compared to other generative RMs. We found the reason is attributed to these problems presenting considerable challenge, often leading to tied judgments in previous LVLMs, whereas IXC-2.5-Reward demonstrates greater ability to make correct classifications with different scalar scores. 5.1.2. Results on Reward Bench and RM-Bench Main Results We argue that multi-modal RMs should preserve strong language processing abilities despite the incorporation of image and video data during training. Consequently, we evaluate the performance of multi-modal reward models, including IXC-2.5-Reward, on Reward Bench (Tab. 4) and RM-Bench (Tab. 5). The results demonstrate that IXC-2.5-Reward achieves considerable performance and surpasses other multi-modal models on this benchmark. Sensitivity to Content and Style Consistent with findings in [51], IXC-2.5-Reward demonstrates sensitivity to subtle content variations and style biases, an issue often overlooked in multi-modal research. We believe further research is needed to enhance the robustness of multi-modal reward models. 5.2. Evaluation Results of IXC-2.5-Chat Benchmarks We select four representative benchmarks for evaluating the instruction following and in-the-wild chatting abilities of LVLMs. (1) The WildVision bench [62] uses prompts collected from user submissions, reflecting real-world multimodal interactions. (2) MIA-bench [68] that is specially designed to evaluate instruction following. (3) MM-MT [1] which is an instruction-following benchmark for multi-modal models, exhibits strong correlation with LMSys-Vision ELO ratings [15]. (4) MM-Vet [103] that evaluate LVLMs on complex tasks such as language generation. These datasets contain open-ended questions and the referenced answers, and evaluation is performed using an LLM-as-a-Judge [118] approach, which involves using judge model like GPT-4o [31] to predict scores. We also report the performance on other categories, such as MMBench [52], MMMU [106] and MMStar [12] for general knowledge, MathVerse [113] and MathVision [89] for math reasoning, TextVQA [79], ChartQA [63] and OCRbench [49] for text-rich document understanding. These benchmarks utilize multiple-choice questions (MCQ) or visual question answering (VQA), where responses are limited to short keywords and evaluated based on string matching. Results on Instruction Following & Chat Tab. 6 shows that IXC-2.5-Chat outperforms previous SOTA models across multiple benchmarks (WildVision, MIA, and MMimprovements in multiMT), demonstrating significant modal understanding with instruction following ability and provide more comprehensive information for in-the-wild chat scenario. Results on Other Categories On other categories (Knowledge, Reasoning, and Text-Rich), IXC-2.5-Chat Figure 3. Visualizations of multi-modal dialogues generated by IXC-2.5-Chat on instruction following abilities. performs comparably to the supervised fine-tuned (SFT) model IXC-2.5, demonstrating that RL training with IXC2.5-Reward improves instruction following and conversational ability without sacrificing performance in these areas. The Impact of Length Constraints To prevent the chat model from generating overly long responses to artificially inflate rewards, we introduce length constraints on the ra9 Table 7. Ablation Studies of the impact of response length constraints of reward models that guided training IXC-2.5-Chat. Avg Wild Tokens Vision MIA MM-MT w/o Length Constraints IXC-2.5-Chat 361 274 76.2 74.6 87.0 84. 5.86 5.70 MM-Vet v2 56.6 54.8 Figure 4. Visualizations of multi-modal dialogues generated by IXC-2.5-Chat on open-ended questions. tio of chosen to rejected responses during training reward model IXC-2.5-Reward. The ablation study results of length constraints are present in Tab. 7. On the WildVision benchmark, we compute the average token length of the models responses. We observe substantial increase in average token length, from 274 to 361, when length constraints were not applied. Surprisingly, removing length constraints yielded substantial improvements in open-ended benchmarks, achieving state-of-the-art results. Such observation is because these benchmarks do not penalize length in their evaluation prompts, judge models (e.g., GPT-4) tend to favor longer responses, even if they contain unnecessary details that detract from the user experience. As our focus is on optimizing user experience, not benchmark scores, we retain the length constraints. Following the precedent set by language-only benchmarks (e.g., [23]), we believe multi10 Table 8. Results of Best-of-N (BoN) sampling for test-time scaling with IXC-2.5-Reward. Avg Wild Tokens Vision MIA MM-MT IXC-2.5-Chat IXC-2.5-Chat + BoN 4 274 283 74.6 77.7 84.0 87.3 5.70 6.03 MM-Vet 54.8 56.3 modal Chat Arena and dialogue benchmarks should also address potential length and style biases in their evaluation protocols in future work. Results on Test-Time Scaling According to Tab. 8, we observe that the Best-of-N sampling further improves the results. The averaged tokens is increased slightly (from 274 to 283), demonstrate that the improvements is bring from the high-quality response, rather than hacking the length bias in Tab. 7. Visualization Results We present the visualization examples of IXC-2.5-Chat on series of topics, such as instruction following  (Fig. 3)  and open-ended questions  (Fig. 4)  . These figures reveal that IXC-2.5-Chat demonstrates several key advantages, including superior organization and presentation, more comprehensive and in-depth answers, and more detailed explanations. These strengths significantly enhance IXC-2.5-Chats effectiveness in multimodal chat interactions. 6. Conclusion and Future Work We present IXC-2.5-Reward, multi-modal reward model that is capable of multi-modal RL training, test-time scaling and data cleaning. IXC-2.5-Reward supports text, image, and video inputs and is trained on diverse, high-quality preference data to ensure robust preference discrimination. Using IXC-2.5-Reward, we further trained IXC-2.5-Chat via RLHF techniques to optimize the multi-modal user chat experience, focusing on providing detailed explanations and in-depth answers. We believe that exploring multi-modal reward models with on-policy reinforcement learning algorithms holds significant promise for future research. For instance, while IXC-2.5-Reward can score multi-image and video inputs, existing reward benchmarks and applying RL algorithms for video alignment remain scarce. Moreover, extending the development of multi-modal reward models beyond chat to other domains, such as process reward models (PRMs) for multi-modal reasoning, is promising direction."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, arXiv preprint Theophile Gervet, et al. arXiv:2410.07073, 2024. 6, 8 Pixtral 12b. [2] Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. 4 [3] Open AI. Hello gpt-4o, 2024. 6 [4] AI Anthropic. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3, 2024. 6, [5] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. general language assistant as laboratory for alignment, 2021. 4 [6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. 4 [7] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 1, 3 [8] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. 1, 3, 7 Internlm2 technical report. [9] Rui Cao, Yuming Jiang, Michael Schlichtkrull, and Andreas Vlachos. Decompose and leverage preferences from expert models for improving trustworthiness of mllms. arXiv preprint arXiv:2411.13697, 2024. [10] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. ALLaVA: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 5 [11] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. GeoQA: geometric question answering benchmark towards multimodal arXiv preprint arXiv:2105.14517, numerical reasoning. 2021. 4 [12] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evalarXiv preprint uating large vision-language models? arXiv:2403.20330, 2024. 8 11 [13] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 6 [14] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [15] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. 8 [16] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 3 [17] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with scaled ai feedback, 2024. 4 [18] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. In ICLR, 2024. 4 [19] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms, 2024. 6 [20] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris CallisonBurch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. [21] Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, and Yapeng Tian. Efficient self-improvement in multimodal large language models: model-level judge-free approach. arXiv preprint arXiv:2411.17760, 2024. 3 [22] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. arXiv preprint arXiv:2405.19716, 2024. 3 [23] Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled AlpacaEval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. 4, 10 [24] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usable information, 2022. 4 [25] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. In CVPR, 2024. 3 [26] Google. Gemini-2.0-Flash, 2024. 8 [27] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. [28] Srishti Gureja, Lester James Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, and Evaluating reMarzieh Fadaee. M-RewardBench: arXiv preprint ward models in multilingual settings. arXiv:2410.15522, 2024. 3 [29] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. VSTaR: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. 1, 3 [30] Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, and Yuxiao Dong. ChatGLMRLHF: Practices of aligning large language models with human feedback, 2024. 5 [31] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 4, 8 [32] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in changing climate: Enhancing lm adaptation with tulu 2, 2023. 3, 7 [33] Songtao Jiang, Yan Zhang, Ruizhe Chen, Yeying Jin, and Zuozhu Liu. Modality-fair preference optimizaarXiv preprint tion for trustworthy mllm alignment. arXiv:2410.15334, 2024. 3 [34] Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Ragrewardbench: Benchmarking reward models in retrieval arXiv augmented generation for preference alignment. preprint arXiv:2412.13746, 2024. [35] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. MiraData: large-scale video dataset with long durations and structured captions, 2024. 4 12 [36] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. DVQA: Understanding data visualizations via question answering. In CVPR, 2018. 4 [37] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. 4 [38] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In CVPR, 2017. 4 [39] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling, 2023. [40] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. ulu 3: Pushing frontiers in open language model posttraining. arXiv preprint arXiv:2411.15124, 2024. 2, 3, 4 [41] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. 3, 7 [42] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer, 2024. 6, 8 [43] Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. VLRewardBench: challenging benchmark for vision-language generative reward models. arXiv preprint arXiv:2411.17451, 2024. 3, 6, 7 [44] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. VLFeedback: large-scale ai feedback dataset for large vision-language models alignment. arXiv preprint arXiv:2410.09421, 2024. [45] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-CLEVR: virtual benchmark to diagnose domain robustness in visual reasoning. In CVPR, 2023. 4 [46] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. 3 [47] Adam Dahlgren Lindstrom and Savitha Sam Abraham. language, arXiv preprint CLEVR-Math: dataset for compositional visual and mathematical arXiv:2208.05358, 2022. 4 reasoning. [48] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-Reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. 1, 3, 13 [49] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. OCRBench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), 2024. 8 [50] Yuan Liu, Le Tian, Xiao Zhou, Xinyu Gao, Kavio Yu, Yang Yu, and Jie Zhou. POINTS1. 5: Building visionarXiv language model towards real world applications. preprint arXiv:2412.08443, 2024. 8 [51] Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. RM-Bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184, 2024. 3, 7, 8 [52] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is your multimodal model an all-around player? In ECCV, 2025. 8 [53] Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. MIA-DPO: Multi-image augmented direct preference optimization for large vision-language models. arXiv preprint arXiv:2410.17637, 2024. 3 [54] Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Mia-DPO: Multi-image augmented direct preference optimization for large vision-language models. arXiv preprint arXiv:2410.17637, 2024. [55] Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang. Uncertainty-aware reward model: TeacharXiv ing reward models to know what is unknown. preprint arXiv:2410.00847, 2024. 1, 3, 7 [56] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. IconQA: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. 4 [57] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 4 [58] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, SongChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for arXiv preprint semi-structured mathematical reasoning. arXiv:2209.14610, 2022. 4 [59] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 8 [60] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. https://huggingface.co/AIDC-AI/Ovis1.6Gemma2-9B, 2024. [61] Xudong Lu, Yinghao Chen, Cheng Chen, Hui Tan, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, et al. BlueLM-V-3B: Algorithm and system co-design for multimodal large language models on mobile devices. arXiv preprint arXiv:2411.10640, 2024. 8 [62] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. WildVision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. 4, 8 [63] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 4, 8 [64] Megvii. Taiyi. https://taiyi.megvii.com/, 2024. 8 [65] Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. CLIP-DPO: Vision-language models as source of preference for fixing hallucinations in lvlms. In ECCV, 2025. [66] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. 1, 3 [67] Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, and Tong Zhang. Strengthening multimodal large language model with bootstrapped preference optimization. In ECCV, 2025. 3 [68] Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Mia-bench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. 8 [69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 6 [70] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your language model is secretly reward model. In NeurIPS, 2024. 1, 3 [71] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 1, 3, 4 [72] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation, 2018. [73] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: benchmark for visual question answering using world knowledge. In ECCV, 2022. 4 [74] SenseTime. SenseNova. https : / / platform . sensenova.cn/home, 2024. 8 [75] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [76] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. KVQA: Knowledge-aware visual question answering. In AAAI, 2019. 4 [77] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models, 2024. 4 [78] Tu Shiwen, Zhao Liang, Chris Yuhao Liu, Liang Zeng, and Yang Liu. Skywork critic model series. https: //huggingface.co/Skywork, 2024. 1, 3 [79] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 8 [80] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 1, 3 [81] Guijin Son, Dongkeun Yoon, Juyoung Suk, Javier AulaBlasco, Mano Aslan, Vu Trong Kim, Shayekh Bin Islam, Jaume Prats-Cristi`a, Lucıa Tormo-Banuelos, and Seungone Kim. MM-Eval: multilingual meta-evaluation benchmark for llm-as-a-judge and reward models. arXiv preprint arXiv:2410.17578, 2024. [82] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 3 [83] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. 6 [84] Llama Team. The llama 3 herd of models, 2024. 6 [85] OpenGVLab Team. than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024. 6 Internvl2: Better [86] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. 3 [87] Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Murun Yang, Qiaozhi He, Tong Xiao, Chunliang Zhang, Tongran Liu, Quan Du, et al. RoVRM: robust visual reward model optimized via auxiliary textual preference data. arXiv preprint arXiv:2408.12109, 2024. 1, 3 [88] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024. 1, 3 [89] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 8 [90] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6, 7, 8 [91] Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Selftaught evaluators. arXiv preprint arXiv:2408.02666, 2024. 1, 3 [92] Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. HelpSteer2-Preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024. 1, 3, 7 [93] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. FunQA: Towards surprising video comprehension, 2024. 4 [94] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. LLaVA-Critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. 1, 3, 4, 6, 7, 8 [95] Wang Xiyao, Yang Zhengyuan, Li Linjie, Lu Hongjin, Xu Yuancheng, Lin Chung-Ching Lin, Lin Kevin, Huang Furong, and Wang Lijuan. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024. 1, 3 [96] Li Xu, He Huang, and Jun Liu. SUTD-TrafficQA: question answering benchmark and an efficient network for video reasoning over traffic events. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 98789888, 2021. 4 [97] Minghao Yang, Chao Qu, and Xiaoyu Tan. Inf outcome reward model. https://huggingface.co/infly/ INF-ORM-Llama3.1-70B, 2024. 7 [98] Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for llms. In NeurIPS, 2024. 1, 3 [99] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. MiniCPM-V: gpt-4v level mllm on your phone. https://huggingface.co/openbmb/ MiniCPM-V-2_6, 2024. 8 [100] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. RlHF-V: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, 2024. 3 [101] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. RLAIF-V: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 4 [102] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. RLAIF-V: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 15 [103] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 8 [104] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024. 1, 3 [105] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason WearXiv preprint ston. Self-rewarding language models. arXiv:2401.10020, 2024. 1, 3 [106] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 8 [107] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah GoodIn man. STaR: Bootstrapping reasoning with reasoning. NeurIPS, 2022. 1 [108] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-CLIP: Unlocking the long-text capability of clip. In ECCV, 2025. 6 [109] Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, et al. Critic-V: Vlm critics help catch vlm errors in multimodal reasoning. arXiv preprint arXiv:2411.18203, 2024. 3 [110] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, InternLM-XComposer2.5-OmniLive: and Jiaqi Wang. comprehensive multimodal system for long-term streaming video and audio interactions, 2024. 5 [111] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. InternLM-XComposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 4 [112] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output, 2024. 4 [113] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. MathVerse: Does your multi-modal llm truly see the diagrams in visual math problems? In ECCV, 2025. [114] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. PMC-VQA: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. 4 [115] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 5 [116] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023. 3 [117] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. ProcessBench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. 3 [118] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 8 [119] Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, et al. RMB: Comprehensively benchmarking reward models in llm alignment. arXiv preprint arXiv:2410.09893, 2024. [120] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024. 3 [121] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, 2023. 1, 3 [122] Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. Selfsupervised visual preference alignment. In ACM MM, 2024."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanjing University",
        "Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}