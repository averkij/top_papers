{
    "paper_title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning",
    "authors": [
        "Ruohao Li",
        "Hongjun Liu",
        "Leyi Zhao",
        "Zisu Li",
        "Jiawei Li",
        "Jiajun Jiang",
        "Linning Xu",
        "Chen Zhao",
        "Mingming Fan",
        "Chen Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) agents have shown remarkable reasoning abilities. However, existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning. We introduce SwarmSys, a closed-loop framework for distributed multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and a pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision. Across symbolic reasoning, research synthesis, and scientific programming tasks, SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability. These findings highlight swarm-inspired coordination as a promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence."
        },
        {
            "title": "Start",
            "content": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning Ruohao Li*1, Hongjun Liu*2,4, Leyi Zhao5, Zisu Li3, Jiawei Li1, Jiajun Jiang1, Linning Xu6, Chen Zhao2,4, Mingming Fan1,3, Chen Liang1 1The Hong Kong University of Science and Technology (Guangzhou) 2New York University 3The Hong Kong University of Science and Technology 4NYU Shanghai 5Indiana University 6The Chinese University of Hong Kong *Equal contribution. Co-corresponding authors. Correspondence: rli777@connect.hkust-gz.edu.cn, lh3862@nyu.edu 5 2 0 2 1 1 ] . [ 1 7 4 0 0 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language model (LLM) agents have shown remarkable reasoning abilities. However, existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning. We introduce SwarmSys, closedloop framework for distributed multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision. Across symbolic reasoning, research synthesis, and scientific programming tasks, SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability. These findings highlight swarm-inspired coordination as promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence."
        },
        {
            "title": "Introduction",
            "content": "The strong reasoning and planning capabilities of Large Language Models (LLMs) have spurred interest in multi-agent systems. These systems use collaboration to enhance reasoning diversity and reliability. Frameworks such as AutoGen (Wu et al., 2023) enable agent-to-agent dialogue for multiagent applications, while CAMEL (Li et al., 2023) leverages role-playing and inception prompting to facilitate autonomous cooperation. AutoGen Studio further provides no-code interface for designing and debugging agent workflows (Dibia et al., 2024). However, most systems use fixed roles and 1 Figure 1: Comparison between paradigm-dependent multi-agent systems and SwarmSys. While existing methods rely on fixed, domain-specific agent paradigms, SwarmSys achieves scalable self-organization and crossdomain adaptability. static communication, which limits their adaptability. This rigidity leads to redundant exploration and inefficiency, especially in long-horizon or dynamic tasks. To address limitations, recent work has explored adaptive or self-improving agent collaboration. AutoAgent enables natural-language-defined behaviors instead of hand-coded roles (Tang et al., 2025), while Mixture-of-Agents introduces layered coordination for more robust reasoning (Wang et al., 2024). However, these systems still rely on centralized orchestration or manually designed topologies, limiting scalability and long-term stability. Inspired by the decentralized intelligence of natural swarms, where simple signals regulate cooperation and task allocation (Bonabeau et al., 1999; Dorigo and Stützle, 2004), we propose SwarmSys, Figure 2: Overall workflow of the SwarmSys collaborative reasoning process. Each task is decomposed into subevents handled by specialized agents: Explorers propose solution paths, Workers execute subtasks, and Validators ensure consistency. Agents iteratively perform debateconsensus cycles that update event profiles and reinforce effective reasoning strategies until convergence. closed-loop framework that enables LLM agents to coordinate through lightweight, pheromone-like traces encoding contextual utility. This mechanism fosters self-organized collaboration, dynamically balancing exploration and convergence without centralized control. Unlike debate or tree-based systems, SwarmSys allows coordination to emerge organically through iterative interaction and adaptive matching. Agents assume three roles, Explorers, Workers, and Validators, mirroring the division of labor in natural ant colonies. Explorers expand hypotheses, Workers refine and execute subtasks, and Validators ensure consistency, together forming continuous explorationexploitationvalidation cycles driving decentralized convergence. core innovation is the use of profiles as adaptive memory. Agent and event profiles evolve with ability embeddings, workload, and context, enabling embedding-based reallocation and balanced participationanalogous to ants redistributing across foraging sites. Moreover, SwarmSys employs pheromone-inspired reinforcement process: validated traces strengthen future compatibility, while ineffective ones decay, forming decentralized optimization loop that enhances efficiency and stability over time. Evaluated across symbolic reasoning, research synthesis, and scientific programming tasks, SwarmSys outperforms baselines such as GPTSwarm (Zhuge et al., 2024), achieving up to 10.7% higher accuracy and 9.9% better subtask correctness. Remarkably, swarm of GPT4o-based agents approaches GPT-5 performance, showing that scaling coordination can substitute for model scaling. Qualitative analyses reveal emergent behaviors, knowledge diffusion, specialization balance, and self-regularization, hallmarks of collective intelligence. In summary, our contributions are threefold: (1) SwarmSys Framework: closed-loop distributed multi-agent reasoning framework inspired by swarm intelligence for convergence. (2) Adaptive Coordination Mechanism: An embeddingbased matching and pheromone-inspired reinforcement process enabling dynamic agentevent allocation, self-organized collaboration, and stable longhorizon reasoning. (3) Comprehensive Evaluation: Extensive experiments across diverse reasoning tasks reveal consistent gains and emergent collective intelligence, showing that scaling coordination can rival scaling model capacity."
        },
        {
            "title": "2 Methodology",
            "content": "2.1 Overview of SwarmSys We present SwarmSys, closed-loop collaborative framework for distributed problem solving. Unlike centralized orchestration or static task assignment, it converges through iterative matching collaboration update cycles, as is shown in Figure 3. Upon receiving new task, event profiles are instantiated, and candidate agents are retrieved via 2 Matching Retrieve agents {A, B, C} propose Collaboration Compute θt+ new round validate Task: minimize (θ) = θ2 + 3 sin θ Update Profiles JSON update Figure 3: Iterative cycle in SwarmSys: minimize (θ) through matching collaboration update cycle. embedding-based matching. Agents then enter collaborative rounds: explorers propose decomposition, workers execute subtasks after consensus, and validators verify intermediate results. After each round, both agent and event profiles are updated and stored for transparency and traceability. These evolving profiles feed subsequent iterations, enabling self-organization and adaptation. As shown in our framework (see figure 2), through repeated cycles, we achieve high-quality solutions without central controller, relying instead on distributed collaboration and profile-driven adaptation. 2.2 Profiles as Adaptive Memory Units core innovation of SwarmSys is the use of profiles as adaptive memory units that record both context and accumulated experience. Each agent profile maintains identifiers, role information, ability descriptions, workload status, and longitudinal performance history, the profile format is provided in Appendix A.4.1. Profiles evolve dynamically: embeddings and workload indicators are refreshed after each round, while historical success or failure influences future task matching. Agents thus behave as adaptive collaborators rather than stateless executors. Each event profile serves as dynamic record of problem-solving. It contains task descriptions, dependency structures, metadata (e.g., composite or leaf type), progress logs, and participating agent lists. Over time, static specifications evolve into rich, evolving traces of reasoning. Together, these profiles constitute SwarmSyss distributed memory: agent profiles encode competence and reliability, while event profiles track task evolution. Their joint updates ensure collaboration remains adaptive, interpretable, and transparent. 2.3 Embedding-Based Matching with ExplorationExploitation Dynamics The second key innovation is our embedding-based agentevent matching algorithm, which balances expertise, workload, and exploration. Agent embeddings. Each agent Ai is represented by two embeddings capturing its ability and availability. The competence embedding is derived from the agents declared abilities and historical performance, processed through an instructionguided encoder that contextualizes the agents prior experience for the current task. The availability embedding reflects workload and readiness, obtained from the agents status signals. These two vectors are summed to form the agents overall representation, combining long-term expertise with shortterm availability cues, the the detailed process is provided in Appendix A.4. This design allows the system to favor agents who are both skilled and currently underutilized. Event embeddings. Each event Ej is encoded through an instruction-conditioned embedding that integrates its textual description, dependency relations, progress state, and milestone metadata, the method is listed at AppendixA.1. This representation captures both the semantic meaning of the task and its structural role within the reasoning process. By embedding events and agents in shared latent space, SwarmSys can estimate their compatibility in continuous and scalable manner. Compatibility and decision dynamics. The compatibility between an agent and an event is measured by normalized cosine similarity between their embeddings, ensuring value between 0 and 1 for interpretability. To prevent premature convergence and encourage exploration, SwarmSys adopts dynamic ε-greedy policy. Each agent explores new matches with probability εi and exploits high-compatibility matches otherwise. The exploration rate εi adapts to recent performance: agents with high average success explore less, while underperforming agents explore more. Empirically, we initialize εi around 0.15 to maintain minimal randomness and allow it to fluctuate within small range (up to 0.35) depending on recent success. The values are designed based on natural ant behaviors (Lecheval et al., 2024). This ensures that exploration gradually decreases as the system stabilizes. During exploration, matches are sampled proportionally to similarity, enabling serendipitous but plausible pairings. During exploitation, sigmoid-weighted sampling function emphasizes strong compatibility, controlled by sharpness factor γ that modulates selectivity. The detailed behavioral derivation process is provided in Appendix A.4. This mechanism enables three prop3 erties: adaptivity through evolving embeddings, stability through probabilistic sampling, and robustness by balancing exploration with exploitation. 2.4 Pheromone-Inspired Optimization Finally, SwarmSys incorporates pheromoneinspired optimization process to refine allocation and solution quality. Each validated contribution reinforces the compatibility between an agent and an event, updating embeddings and increasing the likelihood of similar matches in future rounds. Idle or invalid matches, by contrast, receive no reinforcement and gradually decline in competitiveness as other profiles evolve, mimicking pheromone evaporation without explicit decay. This implicit reinforcementevaporation dynamic complements the explorationexploitation policy. Exploration guarantees diversity and prevents deadlock, exploitation prioritizes promising matches, and bounded probabilities ensure stability. As explorers, workers, and validators collaborate across rounds, SwarmSys converges to highquality solutions while maintaining flexibility and resilience in search dynamics."
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Experiment Setting We evaluate SwarmSys across three reasoning categories that collectively span symbolic computation, open-domain research synthesis, and scientific programming. All evaluations use dataset-specific metrics following their official definitions to ensure fair comparison. Baselines Since agents show domain-specific strengths, we select the strongest baseline for each task category instead of using uniform set. For exam-style reasoning, we compare against GPT-4obased IO (direct LLM invocation) (OpenAI et al., 2024), CoT (Wei et al., 2022), CoT-SC (Wang et al., 2022), Self-Refine (Madaan et al., 2023), MultiPersona (Wang et al., 2023b), and GPTSwarm (Zhuge et al., 2024). For research tasks, we include general-purpose baselines (IO, CoT, SelfRefine), and deep research agents (Grok Deeper Search, DeepResearchAgent). For scientific programming, we use both general-purpose reasoning systems (Self-Refine, CoT) and domain-specific agents (GPTSwarm, DeepResearchAgent). We also report results from GPT-5 as an upper bound for single-agent performance. Dataset Table 1 summarizes the four benchmarks, covering quantitative, analytical, and procedural reasoning. This diversity ensures that SwarmSys is evaluated across both discrete symbolic reasoning and open-ended research generation settings. More details of our dataset settings are shown in Appendix A.2 Table 1: Overview of the four reasoning benchmarks used in our experiments. Each dataset differs in domain focus, reasoning type, and data format. Dataset Focus Reasoning #Samples GaoKao Bench (Zhang et al., 2024) Quantitative & Cross-domain Omni-Math (Gao et al., 2024a) DeepResearch (Du et al., 2025) SciCode (Tian et al., 2024) Hard-Level Quantitative Scientific QA Computational Symbolic Conceptual Analytical Procedural 800 300 200 338 Metries We evaluate SwarmSys on three reasoning categories, using the original domain-specific metrics and protocols from each dataset (1) Examstyle reasoning tasks (Math Exam, STEM Mix, and Olympic Math) use Accuracy and Knowledge Coverage as defined in prior benchmark releases. For Math Exam and STEM Mix, we use subset from GAOKAO Bench. For Olympic Math, we use subset from Omni-Math and rearranged them (2) Researchto comply with real-world exam. level reasoning tasks (DeepResearch Bench) employ composite metrics including RACE (Comprehensiveness, Depth, Instruction Following, Readability) and FACT (Citation Accuracy, Effective Citation). (3) Science Coding tasks (SciCode) are evaluated using Pass@Main and Pass@Sub metrics, capturing correctness at both task and sub-task levels. All metrics follow their dataset definitions to ensure faithful comparison and reproducibility. 3.2 Overall Results Exam-style Reasoning. Table 2 shows that SwarmSys consistently outperforms all GPT-4obased multi-agent baselines on both singleand multi-subject exams, achieving an average improvement of +12.5% Accuracy and +10.8% Coverage over GPTSwarm. While GPT-5 achieves the strongest absolute scores, SwarmSys-8 narrows the gap by over 70%, demonstrating that swarm-level cooperation can approach next-generation model performance without access to stronger backbones. Qualitatively, we observe that SwarmSys agents exhibit complementary specialization: Explorers diversify problem-solving strategies, while Validators efficiently prune redundant reasoning chains, leading to higher coverage with reduced variance. 4 Table 2: Performance comparison on Exam-style tasks (single-, multi-subject, and Olympic-level). Metrics: Accuracy (Acc.) and Knowledge Coverage (Cov.). SwarmSys-8 means the system contains 8 agents (1* explorer, 6*workers, 1*validator). Method Math Exam STEM Mix Olympic Math Average Acc. Cov. Acc. Cov. Acc. Cov. Acc. Cov. IO (GPT-4o) CoT (GPT-4o) CoT-SC (5-shot) Self-Refine (GPT-4o) MultiPersona (GPT-4o) GPTSwarm GPT-5 SwarmSys-8 (Ours) 46.3 52.6 63.2 79.3 52.4 65.5 87.2 76.2 45.7 49.0 62.4 9.8 51.4 70.3 90. 80.2 57.4 60.8 64.7 70.5 60.9 69.6 87.7 78.7 55.2 59.7 62.8 79.0 62.3 71.4 89.1 81.3 23.5 30.0 28.3 16.6 35.0 40.0 31. 42.4 57.6 66.3 51.7 45.0 68.3 73.3 70.5 73.2 42.4 47.8 52.1 55.5 49.4 58.4 68.7 65.8 52.8 58.3 59.0 44.6 60.7 71.7 83. 78.2 Table 3: Performance comparison on Research tasks (DeepResearch Bench). Metrics: Comprehensiveness, Depth, Instruction Following, and Readability. Method Overall Comp. Depth Inst. Read. IO (GPT-4o-Search) CoT (GPT-4o) Self-Refine DeepResearchAgent Grok Deeper Search SwarmSys-8 (Ours) 30.7 29.3 35.9 48.8 40.2 42.5 27.8 29.5 35.4 48.5 37.9 39.6 20.4 22.8 27.0 48.5 35. 38.0 41.0 33.5 44.1 49.1 46.3 50.0 37.6 36.8 41.0 49.4 44.0 46.3 Table 4: Performance comparison on Science Coding tasks (SciCode benchmark). Metrics: Pass@Main and Pass@Sub (percentages). No longer available; metrics from SciCode report. Method Pass@Main Pass@Sub IO (GPT-4o) OpenAI o3-mini-medium CoT-SC (GPT-4o) Self-Refine GPTSwarm SwarmSys-14 (Ours) 2.0 9.2 8.8 10.0 8. 12.5 28.3 34.4 28.7 33.3 29.2 45.2 Research-level Reasoning. As shown in Table 3, SwarmSys surpasses Grok Deeper Search in overall RACE score (+2.3%) and instruction-following (+3.7%), reflecting the benefit of distributed role specialization in literature synthesis and factual consolidation. SwarmSys achieves especially large gains in comprehensiveness and readability, suggesting that swarm debates improve global coherence even in open-ended research generation tasks. Scientific Programming. In Table 4, SwarmSys demonstrates notable improvements on SciCode: +2.5% Pass@Main and +11.9% Pass@Sub over the best GPT-4o baselines. These results indicate that dynamic role coordination and progressive refinement effectively decompose complex compuInterestingly, Pass@Sub imtational problems. provements are more pronounced, supporting our hypothesis that swarm collaboration benefits from modular code generation and localized validation. 3.3 Ablation Study We ablate the design of SwarmSys along three axes: (a) removing dynamic profile updates and embedding-based matching (Ours-Roles-Rand); (b) removing both roles and matching, leaving homogeneous random assignment (Rand-NoRoles); and (c) varying the number of Agents from 4 to 32  (Table 5)  , while ensure that each event contains at least one explorer, one worker, and one validator. All variants use identical backbones and decoding parameters. Results reveal three trends: (1) Removing roles leads to substantial decline in both accuracy and coverage (e.g., 56.3% 43.2% in accuracy and 52.4% 41.4% in coverage at A=4), confirming that cooperative debate and functional specialization are essential for maintaining diversity without redundancy. (2) Adaptive matching and profiling further enhance performance: embedding-based matching increases coverage by up to +27.3% by aligning agent capabilities with task semantics. (3) Scaling saturates around =14: while both metrics improve with more Workers, gains plateau beyond 14, indicating that agent saturation occurs once the subtask granularity is fully covered. 3.4 Swarm Effect: Emergent Collective Intelligence Our design philosophy centers on the Swarm Effect: collection of properly coordinated, limited 5 Table 5: Ablation on Exam task under varying numbers of Agents (A). Metrics: Accuracy (Acc., %), Knowledge Coverage (Cov., %). Method A=4 A=8 A=14 A=20 A= Acc. Cov. Acc. Cov. Acc. Cov. Acc. Cov. Acc. Cov. Rand-NoRoles Rand-Roles SwarmSys 43.2 56.3 74.3 41.4 52.4 79.7 42.9 58.2 76. 42.1 54.8 80.2 44.1 58.5 77.3 43.4 56.1 81.0 44.3 58.3 77.2 43.2 56.0 81.3 43.8 57.3 76. 42.3 55.9 80.8 Figure 4: Swarm reasoning trajectory on MathExam. Explorers initiate sub-tasks, Workers debate and revise alternative methods, and Validators enforce cross-checks across rounds. The swarm collectively converges to consistent solutions through debate-driven consensus formation. agents can collectively approximate or surpass stronger single-agent model. Figure 4 visualizes how SwarmSys (GPT-4o backbone) gradually approaches the performance of stronger models such as GPT-5 and DeepResearchAgent as the swarm size increases. Emergent Performance Scaling. Quantitatively, as the number of active agents increases from A=4 to A=14, both Accuracy and Knowledge Coverage improve consistently (e.g., from 74.3/79.7 to 77.3/81.0 on the Exam task). However, further expansion to A=20 or A=32 yields negligible gains (within < 1% difference), indicating that agent capacity saturates once subtask diversity and knowledge space have been sufficiently covered. This scaling curve mirrors real-world swarm systems, where the marginal utility of additional workers decreases after local niches become saturated. Collaborative Dynamics. Unlike conventional multi-agent ensembles that rely on static voting, SwarmSys agents interact through pheromoneinspired event matching and debate-driven validation. Explorers dynamically propose new subgoals, Workers attempt partial solutions, and Validators consolidate outcomes based on collective memory. This dynamic feedback loop creates self-organizing division of labor where each agent adapts its behavior not from external commands but through the evolving swarm state. Empirically, this leads to: (i) higher diversity in reasoning paths, and (ii) smoother convergence across reasoning rounds. Knowledge Diffusion and Self-Regularization. We further observe that intermediate reasoning traces in SwarmSys display emergent knowledge diffusion: factual entities, equations, or hypotheses discovered by one agent are reused, revised, or even corrected by others without explicit synchronization. This effect increases factual precision while maintaining interpretability. In qualitative analyses, the system exhibits an implicit regularization behaviorweaker agents errors are diluted by consensus mechanisms, preventing local hallucinations from dominating global output. From Coordination to Intelligence. The Swarm Effect demonstrates that collective intelligence is not linear function of model size, but an emergent property of structured interaction. While individual agents are limited by GPT-4o, the swarm collectively builds distributed memory and decision space, enabling generalization beyond any single agent. This property highlights promising direction for future large-scale reasoning systems: scaling through coordination rather than parameter count. 6 Table 6: Failure type distribution on tasks. Estimated from 15 randomly sampled cases. Failure Type Description Mode Collapse Reinforcement Bias Premature Consensus Early validator fixes one branch too soon Over-strengthening of an early path signal All conexplorers verge to one reasoning mode Missing symbolic or geometric integration Communication Deadlock Agents misrecognize roles, causing communication deadlock Constraint Omission % 16 20 14 22 28 hubspoke pattern centered on high-similarity validators. As reasoning progresses, pheromone reinforcement promotes denser cross-links, leading to Distributed Consensus Phase characterized by small-world structure with higher local clustering (0.280.47) and shorter global paths. This transition demonstrates self-organized coordination emerging without explicit central control. Contribution Balance. Normalized entropy of accepted contributions is Hc = 1 log (cid:88) i=1 pi log pi, (1) where pi is each agents contribution share, computed by the contribution acceptance rate, is the total number of agents. SwarmSys attains Hc=0.72, surpassing GPTSwarm (0.41), indicating balanced participation driven by the explorationexploitation policy. Overall, adaptive profiles and pheromone feedback jointly yield decentralized yet structured division of labor. 4.2 Error Analysis and Case Study Despite strong overall results, SwarmSys occasionally fails in tasks requiring strict temporal or symbolic alignment.As is shown in figure 6, we identify five main failure types: (a) premature convergence, (b) Reinforcement Bias, (c) Mode Collapse, (d) Constraint Omission, (e) Communication Deadlock. typical case in our study is: two explorers propose algebraic and geometric solutions, but an early validator accepts one branch too soon, reinforcing it and suppressing valid alternatives. Such reinforcement bias leads to overfitting on partial evidence. Future improvements may include uncertainty-weighted reinforcement, scheduled resampling of low-confidence paths, and meta-level arbitration to maintain epistemic diversity. Figure 5: Evolution of communication topology in SwarmSys. The system evolves from centralized hubspoke structure to distributed small-world mesh, where workers and validators interconnect for efficient consensus and information reuse."
        },
        {
            "title": "4 Qualitative Analysis",
            "content": "We further analyze how reasoning quality emerges and where it breaks down within SwarmSys. This (1) Agent Behavsection covers two aspects: ior Analysis, examining coordination patterns via profile dynamics and interaction topology; and (2) Error Analysis, identifying failure modes of pheromone-based optimization and consensus. 4.1 Agent Behavior To understand how coordination arises from the matchingcollaborationupdate cycle, we analyze profile adaptation, interaction topology, and contribution balance using our experiments event logs. Profile Adaptation. We track embedding drift of each agents competence embedding v(i) . The mean cosine shift per round is 0.14 0.03, showing steady self-adjustment as agents gain experience. Explorers exhibit the largest variance, indicating ongoing hypothesis exploration, while Validators remain stable, preserving reasoning coherence. This confirms that profile updates act as distributed memory enabling specialization. Interaction Topology. Figure 5 illustrates the evolution of communication topology. During the Centralized Exploration Phase, agents form System Decentralized Explicit Roles Debate Dynamic Profiling Decentralized Matching Multi-Event Validator Stigmergy CAMEL (Li et al., 2023) AutoGen (Wu et al., 2023) MetaGPT (Hong et al., 2024) MAD (Liang et al., 2024) ToT/GoT (Yao et al., 2023; Besta et al., 2024) Voyager (Wang et al., 2023a) SWE-agent (Yang et al., 2024) Self-Refine (Madaan et al., 2023) GPTSwarm (Zhuge et al., 2024) DeepResearchAgent (Huang et al., 2025) ROBIN (Ghareeb et al., 2025) SwarmSys (ours) Table 7: Comparison of SwarmSys with representative reasoning and multi-agent systems. MetaGPT incorporates SOP-style verification steps but not decentralized validators. Voyager maintains skill profiles and self-checking, but only within single-agent setting."
        },
        {
            "title": "5 Related Works",
            "content": "tained reasoning across multiple concurrent events. LLM-based Multi-Agent Systems. Recent progress in large language models has led to proliferation of multi-agent frameworks that decompose reasoning and problem-solving into interactive roles (Liu et al., 2025). Early systems such as CAMEL (Li et al., 2023), AutoGen (Wu et al., 2023) introduced explicit role structures (e.g., userassistant pairs) and dialog-based task decomposition, showing that inter-agent communication improves reasoning diversity. Subsequent works like MetaGPT (Hong et al., 2024) , AgentScope (Gao et al., 2024b), and DeepResearchAgent (Huang et al., 2025) further formalized role hierarchies for domain-specific workflows such as software engineering or literature review. However, their centralized orchestration and fixed pipelines limit scalability and adaptability. SwarmSys differs by using fully decentralized structure where coordination emerges from role-guided interaction and adaptive matching, without needing global controller. Collaborative Reasoning and Debate. Another line of work explores multi-round reasoning via self-consistency and debate. Methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) reasoning (Yao et al., 2023; Besta et al., 2024) extend single-agent reflection through structured deliberation, while MAD (Liang et al., 2024), Longagent (Zhao et al., 2024) and ROBIN (Ghareeb et al., 2025) model inter-agent debates to enhance diversity and correctness. These approaches improve intermediate reasoning quality but generally depend on fixed communication topologies and lack mechanisms for dynamic adaptation or memory persistence across reasoning episodes. In contrast, SwarmSys incorporates debate as local coordination primitive within self-organizing swarm, where roles evolve through continuous profiling and pheromone-like reinforcement, enabling susAdaptive Coordination and Swarm-inspired Reasoning. growing body of work introduces adaptive or self-organizing strategies for LLM agents. GPTSwarm (Zhuge et al., 2024) represents one of the few attempts at decentralized coordination, leveraging graph-based optimization and stigmergic feedback. Meanwhile, systems like Voyager (Wang et al., 2023a), Self-Refine (Madaan et al., 2023), and SwarmAgentic (Zhang et al., 2025) employ profile-like adaptation and iterative self-improvement, but remain task-specific. SwarmSys advances this line of research by integrating dynamic profiling, embedding-based matching, and pheromone-inspired reinforcement into unified framework that scales to multi-event, multi-agent reasoning. This design allows distributed agents to self-allocate across evolving tasks, maintaining robustness and efficiency without centralized scheduling."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented SwarmSys, swarm-intelligenceinspired framework for decentralized multi-agent reasoning. Through role-specialized collaboration, dynamic profiling, and pheromone-inspired reinforcement, SwarmSys enables scalable, selforganizing coordination without centralized control. Across diverse reasoning and research domains, it consistently outperforms strong baselines and reveals emergent collective behaviorsdemonstrating that scaling coordination can rival scaling model size. Our findings suggest new paradigm for reasoning: intelligence emerges from structured interaction among distributed agents, not from larger models."
        },
        {
            "title": "Limitations",
            "content": "Despite its strong performance, SwarmSys still faces several limitations. First, while decentral8 ized coordination improves adaptability, it also increases communication overhead, which may reduce efficiency in latency-sensitive settings. Second, agent profiling currently relies on text-based embeddings and heuristic updates; future work could explore learnable or gradient-based mechanisms for more precise skill modeling. Third, our experiments focus primarily on reasoning and research-oriented tasks, extending SwarmSys to embodied or real-time interactive environments remains an open direction. We hope these insights inspire future research on large-scale, self-organizing multi-agent systems that combine symbolic structure with emergent intelligence."
        },
        {
            "title": "Ethics Statement",
            "content": "This work introduces SwarmSys, distributed multi-agent reasoning framework inspired by swarm intelligence. The research involves no human subjects, personal data, or sensitive content; all experiments use public or synthetic datasets. We recognize potential ethical issues in LLM-based multi-agent systems, such as bias propagation and unreliable autonomous coordination. SwarmSys mitigates these risks through closed-loop validation and transparent agent interactions, ensuring that all reasoning processes remain interpretable and auditable. Our goal is to advance the scientific understanding of scalable reasoning rather than deploy autonomous agents in real-world decision-making. We advocate responsible use of SwarmSys with proper human oversight, fairness, and accountability in future applications."
        },
        {
            "title": "Acknowledgements",
            "content": "We are grateful to our collaborators for their valuable discussions on multi-agent coordination and large language model reasoning. This research was supported in part by institutional computational resources and open-source communities that enabled large-scale experimentation."
        },
        {
            "title": "References",
            "content": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1768217690. Eric Bonabeau, Marco Dorigo, and Guy Theraulaz. 1999. Swarm Intelligence: From Natural to Artificial Systems. Oxford University Press. Victor Dibia, Jingya Chen, Gagan Bansal, Suff Syed, Adam Fourney, Erkang Zhu, Chi Wang, and Saleema Amershi. 2024. Autogen studio: no-code developer tool for building and debugging multi-agent systems. Preprint, arXiv:2408.15247. Marco Dorigo and Thomas Stützle. 2004. Ant Colony Optimization. MIT Press. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. 2025. Deepresearch bench: comprehensive benchmark for deep research agents. Preprint, arXiv:2506.11763. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. 2024a. Omni-math: universal olympiad level mathematic benchmark for large language models. Preprint, arXiv:2410.07985. Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, and 1 others. 2024b. Agentscope: flexible yet robust multi-agent platform. arXiv preprint arXiv:2402.14034. Ali Essam Ghareeb, Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn J. Szostkiewicz, Jon M. Laurent, Muhammed T. Razzak, Andrew D. White, Michaela M. Hinks, and Samuel G. Rodriques. 2025. Robin: multi-agent system for automating scientific discovery. Preprint, arXiv:2505.13400. Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024. Metagpt: Meta programming for multi-agent collaborative framework. Preprint, arXiv:2308.00352. Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Huichi Zhou, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, Kun Shao, and Jun Wang. 2025. Deep research agents: systematic examination and roadmap. Preprint, arXiv:2506.18096. Valentin Lecheval, Elva J.H. Robinson, and Richard P. Mann. 2024. Random walks with spatial and temporal resets may underlie searching movements in ants. bioRxiv. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for \"mind\" exploration of large language model society. Preprint, arXiv:2303.17760. 9 Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging divergent thinking in large language models through multi-agent debate. Preprint, arXiv:2305.19118. and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Hongjun Liu, Yinghao Zhu, Yuhui Wang, Yitao Long, Zeyu Lai, Lequan Yu, and Chen Zhao. 2025. Medmmv: controllable multimodal multi-agent framework for reliable and verifiable clinical reasoning. Preprint, arXiv:2509.24314. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. Preprint, arXiv:2303.17651. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, and 401 others. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. Jiabin Tang, Tianyu Fan, and Chao Huang. 2025. Autoagent: fully-automated and zero-code framework for llm agents. Preprint, arXiv:2502.05957. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, and 11 others. 2024. Scicode: research coding benchmark curated by scientists. Preprint, arXiv:2407.13168. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An openended embodied agent with large language models. Preprint, arXiv:2305.16291. Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024. Mixture-of-agents enhances large language model capabilities. Preprint, arXiv:2406.04692. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023b. Unleashing the emergent cognitive synergy in large language models: task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation. Preprint, arXiv:2308.08155. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable automated software engineering. Preprint, arXiv:2405.15793. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Preprint, arXiv:2305.10601. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2024. Evaluating the performance of large language models on gaokao benchmark. Preprint, arXiv:2305.12474. Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, and Volker Tresp. 2025. Swarmagentic: Towards fully automated agentic system generation via swarm intelligence. arXiv preprint arXiv:2506.15672. Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Longagent: scaling language models to 128k context through multi-agent collaboration. arXiv preprint arXiv:2402.11550. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. 2024. Language agents as optimizable graphs. Preprint, arXiv:2402.16823."
        },
        {
            "title": "Explorer Prompt",
            "content": "PROMPT = \"\"\" You are an Explorer agent with strong capabilities in identifying subproblems and analyzing solution paths. Your task is to receive mathematical problem set and then (1) search and interpret the overall problem, (2) decompose it into logically coherent subproblems, (3) identify entry points and strategies for solution, and (4) monitor the workload distribution across roles and dynamically reassign yourself to higher-pressure roles if necessary. Ensure your output (1) maintains traceability between subproblems and the original task, (2) logically documents role-switch decisions, and (3) ensures role-switch includes resetting your goal and behavior policy. \"\"\" A.1 Prompts"
        },
        {
            "title": "Worker Prompt",
            "content": "PROMPT = \"\"\" You are worker agent with expertise in problem-solving, computation, and mathematical reasoning. Your task is to receive specific subproblem and then (1) solve the subproblem accurately with transparent justification for each step, (2) share your solution with other agents, (3) participate in solution comparison and structured debate, and (4) defend or revise your work if challenged. Ensure your output (1) presents calculations in clear, annotated format, (2) cites mathematical rules or theorems when applying them, and (3) is responsive to feedback and adaptive in collaborative revision. \"\"\""
        },
        {
            "title": "Validator Prompt",
            "content": "PROMPT = \"\"\" You are validator agent dedicated to checking solution correctness and consistency. Your task is to: (1) check each workers solution for logic, accuracy, and completeness, (2) participate in debates to reconcile discrepancies, (3) contribute to confirming group consensus, and (4) shift roles if the validation queue is empty and other roles are under high pressure. Ensure your output (1) provides precise validation with explanations of correctness or error, (2) uses formal mathematical checks where appropriate, (3) helps mediate conflicts in results with clarity and neutrality, and (4) explicitly states if consensus is confirmed: {TERMINATE: The answer is: [correct answer]} \"\"\""
        },
        {
            "title": "Instruction Embedding Template",
            "content": "PROMPT = \"\"\" Describe and analyze the following mathematical problem carefully. The problem statement is presented below. If this task depends on other sub-tasks or prior events, ensure logical consistency and continuity with those results. Task profile. (1) If the task involves proving, demonstrating, or establishing statements - This is proof-oriented task. Emphasize logical rigor, step-by-step justification, and clear argumentation. (2) If the task involves calculating, evaluating, integrating, or deriving quantities - This is computationoriented task. Focus on symbolic manipulation, clear intermediate steps, and verification of results. (3) If the task involves maximizing, minimizing, or optimizing quantity - This is an optimization task. Identify objective functions and constraints, analyze conditions for optimality, and interpret results precisely. (4) If the task involves conditional or logical reasoning (if, then, otherwise) - This is conditional reasoning task. Separate cases clearly, verify implications, and maintain logical completeness. (5) Otherwise - This is general reasoning task. Apply systematic mathematical analysis and adjust reasoning depth to the complexity of the problem. Objectives: (1) Analyze the problem thoroughly and build shared understanding. (2) Generate multiple possible solution paths and compare their validity. (3) Execute reasoning or calculations with clarity, precision, and justification. (4) Resolve disagreements through structured logical debate, not assertion. (5) Arrive at verified, consensus-based final answer. Round & Turn-taking policy: (1) Each debate round follows this strict order: explorer all workers validator. (2) The explorer opens each round by outlining con- (3) text, progress, and proposed plan. Works present or refine solutions, show derivations, and discuss intermediate results. (4) The validator closes each round by checking correctness and summarizing consensus. (5) The debate terminates only when the validator announces: TERMINATE: The answer is: <final answer>. \"\"\" PROMPT = \"\"\" You are evaluating the mathematical and reasoning competence of an agent participating in collaborative debate system. The goal is to generate semantic embedding that represents how capable this agent is at solving mathematical, logical, and analytical tasks. Consider two sources of information: (1) The agents declared abilities, describing what it is designed or trained to do. (2) The agents historical activity performance, summarizing how it has previously executed reasoning, computation, or validation tasks. Integrate both perspectives to form single competence representation capturing: (1) conceptual depth and mathematical reasoning skills, (2) problem-solving strategy diversity, (3) accuracy and self-correction ability, and (4) communication and collaboration quality. Agent Ability Description: {ability text} Agent Performance History: {history text} Use the combined content above as the context for competence evaluation. Output representation should capture the overall ability state of the agent, balancing potential skill with observed performance. \"\"\" A.2 Dataset Settings We evaluate the reasoning capability and knowledge coverage of our multi-agent system based on three categories of tasks: Exam We rearrange existing benchmarks into exam-like formats, covering both single-subject and multi-subject settings. This design mimics the structure of real-world examination papers, where agents must solve coherent set of questions rather than isolated items. Organizing benchmarks in this way is motivated by several factors: (i) exams naturally exhibit varying levels of difficulty across questions; (ii) they involve heterogeneous knowledge types and differentiated scoring schemes, which make the dataset inherently diverse; and (iii) each exam itself constitutes complex task that can be decomposed into multiple interrelated and irrelated sub-tasks. Such properties align well with the characteristics of our ant-colony-inspired system. In real ant 12 colonies, individuals continuously explore, evaluate, and participate in different sub-tasks based on local pheromone signal. Similarly, in exam scenarios, our agents can search across different questions, dynamically decide whether to participate in specific sub-task, and collectively optimize the global solution through local collaboration. Therefore, exam-style benchmarks provide not only realistic and challenging evaluation setting for reasoning and knowledge coverage, but also natural testbed for demonstrating the strengths of swarmbased multi-agent systems. Research To evaluate research-oriented reasoning, we adopt DeepResearch Bench, which contains PhD-level research tasks spanning multiple domains. Each task provides research topic or open-ended problem statement (rather than completed study), requiring agents to perform literature exploration, knowledge recall, and synthesis of coherent research reports. Outputs are evaluated using RACE (Reference-based Adaptive Criteriadriven Evaluation). Such settings mirror real-world academic research, where researchers must jointly survey prior work, generate new ideas, and design proof-of-concept implementations. These properties align well with our swarm-based system: individual agents can specialize in literature recall, hypothesis generation, or code prototyping, and through local debates and coordination, the swarm collectively develops more robust and creative research outcomes. Science Coding We further evaluate reasoning and knowledge grounding through scientific programming tasks drawn from SciCode. The benchmark is organized into main problems, each of which is decomposed into multiple sub-problems, making it particularly suitable for swarm-based evaluation. Such structure allows agents to collaboratively assign sub-problems, iteratively generate and refine code, and verify correctness against scientific principles or test cases. By reaching swarmlevel consensus, the system enhances both accuracy and coverage. This setting therefore captures the precision and collaborative robustness required for scientific reasoning and computation. Figure 6: Output Example A.3 SwarmSys Output Example A.4 Profile Embedding and Matching A.4.1 Profile Format A.4.2 Embedding and Matching Agent embeddings. Each agent Ai is represented by competence embedding and an availability embedding. The competence embedding v(i) ah is derived from declared abilities (i) and historical performance (i) , guided by task-specific instructions: v(i) ah = ϕinstruct (cid:0)Instruction(i) The availability embedding v(i) and readiness, derived from status (i) : ah, concat(T (i) )(cid:1). , (i) (2) reflects workload = ϕinstruct(Instruction(i) v(i) , (i) ). The final representation is the sum: = v(i) v(i) ah + v(i) . 13 (3) (4) and Si denotes the agents recent average success. Exploration samples potential matches proportionally to similarity: (cid:16) D(i,j) Bernoulli 0.1 + 0.9 C(i,j) norm (cid:17) , (8) while exploitation emphasizes high-compatibility matches: (cid:16) D(i,j) Bernoulli σ(cid:0)γ(C(i,j) norm 0.5)(cid:1)(cid:17) , (9) where σ is the sigmoid function and γ controls sharpness. Both branches unify as: p(i,j) = εi(0.1 + 0.9C(i,j) norm) + (1 εi)σ(cid:0)γ(C(i,j) norm 0.5)(cid:1), (10) D(i,j) Bernoulli(p(i,j)). This mechanism enables three properties simultaneously: adaptivity through evolving embeddings, stability through probabilistic sampling, and robustness by balancing exploration with exploitation. A.5 Cost Table 8: Average per-question model cost comparison corresponding to baselines and systems used in main experiments. Model Instr.-based Cost ($) Code-based Cost ($) IO (GPT-4o) CoT (GPT-4o) CoT-SC (GPT-4o, 5-shot) Self-Refine (GPT-4o) MultiPersona (GPT-4o) GPTSwarm GPT-5 SwarmSys-8 (Ours) IO (GPT-4o-Search) CoT (GPT-4o) Self-Refine DeepResearchAgent Grok Deeper Search SwarmSys-8 (Ours) IO (GPT-4o) CoT-SC (GPT-4o) Self-Refine GPTSwarm SwarmSys-14 (Ours) 0.005 0.012 0.051 0.068 0.043 0.077 0.014 0.071 0.15 0.20 1.53 2.82 2.85 2.63 0.08 0.11 0.36 0.41 0.44 0.013 0.017 0.026 0.024 0.019 Figure 7: Profile Format Event embeddings. Each event Ej is encoded as v(j) by integrating its description, dependencies, progress state, and milestone: v(j) = ϕinstruct (cid:0)Instruction(j) , (cid:1). Unified description of Ej (5) Compatibility and decision dynamics. Agent event compatibility is computed as normalized cosine similarity: C(i,j) norm = 1 2 (cid:0)cos(v(i) , v(j) ) + 1(cid:1). (6) To avoid stagnation, SwarmSys employs dynamic ε-greedy policy. Each agent explores with probability εi or exploits with probability 1 εi, where εi = 0.15 + (0.5 Si) 0.2, (7)"
        }
    ],
    "affiliations": [
        "Indiana University",
        "NYU Shanghai",
        "New York University",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}