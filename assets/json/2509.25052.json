{
    "paper_title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning",
    "authors": [
        "Sai Wang",
        "Yu Wu",
        "Zhongwen Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience."
        },
        {
            "title": "Start",
            "content": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Sai Wang1,2, Yu Wu2 and Zhongwen Xu1 1Tencent, 2Wuhan University 5 2 0 2 9 2 ] . [ 1 2 5 0 5 2 . 9 0 5 2 : r Abstract: The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), novel agent architecture that leverages Large Language Model (LLM) to build an explicit, language-based understanding of its environments mechanics and its own strategy. Starting from tabula rasa state with no prior knowledge (except action set), CEL operates on cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environments dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates path toward more general and interpretable agents that not only act effectively but also build transparent and improving model of their world through explicit reasoning on raw experience. 1. Introduction The quest to create intelligent agents [25] capable of mastering complex, interactive environments has been long-standing goal of artificial intelligence [26]. Landmark achievements, from Deep Blues victory in chess to AlphaGo [2123]s dominance in Go, have demonstrated the power of computation and search [24]. More recently, large-scale deep reinforcement learning (RL) has produced agents with superhuman abilities in complex video games [1, 28]. These systems, however, often learn inefficiently through experience, requiring immense computational resources and encoding their strategic knowledge implicitly within the millions of parameters of neural network, rendering their decision-making processes opaque. The advent of Large Language Models (LLMs) presents paradigm shift, offering new foundation for agent design grounded in reasoning and explicit knowledge representation [2, 5]. While early LLMbased agents show promise [5, 11], they often lack structured mechanism for continuous learning and adaptation. They may operate in zero-shot capacity [9] or rely on simple memory retrieval [5], but they do not fundamentally improve their internal model of the worlds mechanics through experience. Similarly, while learned world models [8, 16, 17] have enabled agents to plan in imagined futures, their models operate on uninterpretable latent states, shrouding their understanding of the world in black box. This leaves critical gap: the need for an agent that not only acts, but truly comprehends its environment in way that is both effective and interpretable. Corresponding author(s): wuyucs@whu.edu.cn, zhongwenxu@tencent.com Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 1: Comparison of Paradigms for Game-Playing Agents. This figure contrasts three distinct agent architectures: (1) the Conventional RL Paradigm that learns an implicit policy from rewards, by updating policy weights; (2) the Zero-shot Reasoning Paradigm that leverages static LLM model for decision-making; (3) the Cogito, ergo ludo Paradigm, where the agents policy is trained by RL while persistent knowledge base (Rule & Playbook) is built and passed across episodes. In this work, we introduce Cogito, ergo ludo (CEL), novel agent architecture (Figure 1) that learns to master interactive environments not just by acting, but by reasoning and planning. We propose an agent that leverages an LLM to explicitly reason about its interactions, building and refining human-readable world model [7, 26] of its environment and its own strategy from the ground up. Starting from tabula rasa state with no prior knowledge of the game rules, CEL learns purely through cycle of interaction and reflection, embodying the principle of learning by thinking. The cornerstone of CEL is its two-phase operational cycle. During an episode, the agent acts decisively by performing lookahead search with natural language, using its current understanding of the world to predict the outcomes of its actions. Crucially, after each episode concludes, the agent enters Post-Episode Reflection phase. In this phase, the LLM analyzes the trajectory of the preceding episode to perform two concurrent learning processes: Rule Induction, where it refines its explicit, language-based model of the environments dynamics; and Strategy and Playbook Summarization, where it distills successful and unsuccessful patterns of behavior into an actionable strategic playbook. This refined knowledge base, both the rules of the world and the principles of how to act within it, directly informs the agents decision-making in subsequent episodes. We demonstrate the effectiveness of CEL across three distinct grid-world environments: the logical puzzle of Minesweeper, the navigation challenge of Frozen Lake, and the complex planning problem of Sokoban. Our experiments show that CEL successfully learns to master these tasks by autonomously discovering their rules and developing effective strategies. Ablation studies confirm that the iterative, reflective process of refining its internal knowledge is critical to its learning success. Furthermore, we provide qualitative evidence of the architectures unique interpretability, showcasing the comprehensive, human-readable rulebooks and sophisticated strategic heuristics that it generates entirely from raw interaction. Our work presents step towards agents that not only perform well, but also build transparent and improving understanding of their world. 2. Related Work Our research builds upon decades of work in artificial intelligence, drawing from and extending three key areas: the paradigm of large-scale deep reinforcement learning, the development of learned world models for planning, and the nascent field of agents driven by Large Language Models. Work in Progress 2 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning The Apex of Deep Reinforcement Learning. Landmark achievements such as DeepMinds AlphaStar [28] and OpenAI Five [1] demonstrated that deep reinforcement learning (RL) could attain superhuman performance in complex real-time strategy games. These systems operate at massive scale, training for thousands of GPU-years on billions of game frames. Their strategic acumen is implicitly encoded within the weights of enormous neural networks, learned through vast experience. While immensely powerful, this approach is characterized by high sample complexity and the opaque nature of the resulting policies. Our work diverges from this paradigm by pursuing more sample-efficient and interpretable approach, where knowledge and strategy are explicitly represented in natural language. The AlphaZero algorithm [2123] uses the power of Monte-Carlo Tree Search (MCTS) [10] with deep neural network, achieving superhuman performance in Chess, Shogi, and Go. But crucially, it was provided with perfect model of the environment the game rules, while our proposed architecture accumulates the knowledge of the game rules purely by interaction. Planning with Learned World Models. MuZero [17] learned latent model to predict future rewards, policies, and values, enabling effective lookahead search without being given the rules. This principle of learning and planning in imagined trajectories has been further advanced by algorithms like Dreamer [8], which learns robust world model that allows it to master vast suite of diverse domains, from Atari to Minecraft, with single set of hyperparameters. Our Language-based World Model (LWM) shares this objective of predicting environmental dynamics. However, we draw critical distinction: whereas the models in MuZero and Dreamer operate on uninterpretable latent states, our LWM is grounded in explicit, human-readable rules and transition dynamics that are themselves inferred from experience. This language symbolic foundation allows the agent to reason about and refine its understanding of the worlds mechanics in natural language. Language as the Algorithm vs. Language in the Architecture. distinct approach is Natural Language Reinforcement Learning (NLRL) [4], which seeks to fundamentally redefine the core components of RL, such as the value function and Bellman equation, entirely within the domain of natural language. In NLRL, the value of state is not scalar but descriptive text, and the policy improvement step is performed by an LLM reasoning over these linguistic value judgments. While both our approaches leverage LLMs for reasoning, our philosophy and architecture differ significantly. Rather than reformulating the RL algorithm itself into language, our framework treats the LLM as the orchestrator of cognitive architecture composed of distinct, language-grounded modules. LLMs as Agent Architectures. More recently, the advent of LLMs has catalyzed new approach to agent design. Frameworks like GEM [12] and LMGame-Bench [9] provide environments and harnesses to evaluate LLM agents, highlighting challenges in perception, memory, and long-horizon planning. Gemini 2.5 Pro [5] showcases its success in complete PokÃ©mon game playing, demonstrating the strong zero-shot reasoning abilities of the frontier LLMs. particularly relevant approach is PORTAL [30], which uses an LLM as policy architect to generate behavior trees in domain-specific language. Unlike PORTAL, our method uses LLM as the core for planning, acting and accumulating knowledge, where the LLM directly interacts with environments. Our work builds upon this foundation but proposes more comprehensive cognitive architecture. Our agent learns and maintains suite of distinct, yet interconnected, cognitive components: an explicit world model of environmental dynamics, set of game rules, strategic playbook, and language-based value function. The cornerstone of our method is the post-episode reflection phase, where the LLM analyzes interaction trajectories to iteratively and simultaneously refine both its understanding of the worlds rules and its own strategic playbook. This creates cycle of self-improvement that is explicit, interpretable, Work in Progress 3 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning and broadly applicable to any interactive environment. 3. Method We model the agents interaction with its environment as Markov Decision Process (MDP) [14], formally defined by the tuple (ğ’®, ğ’œ, ğ’«, â„›, ğ›¾). In this framework, ğ’® represents the set of states, ğ’œ the set of actions, and ğ›¾ [0, 1] the discount factor. The state transition function ğ’«(ğ‘ ğ‘¡+1ğ‘ ğ‘¡, ğ‘ğ‘¡) specifies the probability of transitioning to state ğ‘ ğ‘¡+1 from state ğ‘ ğ‘¡ upon taking action ğ‘ğ‘¡. The reward function â„›(ğ‘ ğ‘¡, ğ‘ğ‘¡) yields an immediate reward ğ‘Ÿğ‘¡+1. The agents objective is to learn policy ğœ‹(ğ‘ğ‘ ), that maximizes the expected discounted return, defined as ğºğ‘¡ = Our central methodological contribution is to employ single Large Language Model (LLM), denoted â„’, to instantiate and manage all of the agents cognitive functions. Our framework moves beyond the static zero-shot paradigm by continuously training an LLM based on the outcomes of the agents interactions, allowing it to improve its core reasoning and planning capabilities over time. We represent all information pertaining to the interaction: states (ğ‘ ), actions (ğ‘), rewards (ğ‘Ÿ), inferred environmental dynamics (ğ’¢), and strategic guidelines (Î ), as natural language strings. The agents learning unfolds over series of episodes, indexed by ğ‘˜, where each episode consists of discrete time steps, indexed by ğ‘¡. The LLMs reasoning process is made explicit through chain-of-thought [29], which we denote by ğ¶. By reasoning and planning, the CEL agent â„’ learns to interact with the environment and maximize its rewards. ğ‘˜=0 ğ›¾ğ‘˜ğ‘Ÿğ‘¡+ğ‘˜+1 [26]. 3.1. Language-based World Model The LLM functions as world model [7, 8], tasked with predicting the dynamics of the environment. Given the current state ğ‘ ğ‘¡ and candidate action ğ‘ğ‘¡, the world model forecasts the subsequent state Ë†ğ‘ ğ‘¡+1 and immediate reward Ë†ğ‘Ÿğ‘¡+1. This prediction is conditioned on the agent state ğ‘ ğ‘¡, potential action ğ‘ğ‘¡, and the agents current understanding of the environments rules, ğ’¢ğ‘˜. The model first generates reasoning trace, ğ¶ğ‘Š ğ‘€ (for World Model), before outputting its predictions: (ğ¶ğ‘Š ğ‘€ , Ë†ğ‘ ğ‘¡+1, Ë†ğ‘Ÿğ‘¡+1) ğ‘â„’(ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ’¢ğ‘˜), (1) where ğ‘â„’ is the probability distribution over text sequences generated by the LLM. Critically, the outputs Ë†ğ‘ ğ‘¡+1 and Ë†ğ‘Ÿğ‘¡+1 are not structured data types (e.g., state tensor or scalar value) but are descriptive natural language strings, as illustrated in Figure 5. This predictive capability is the foundation for explicit planning [26]. By querying the world model for each potential action, the agent can simulate and evaluate set of possible future outcomes, process that is central to its decision-making [16, 17]. 3.2. Induction of Environmental Dynamics Following each episode ğ‘˜ 1, the agent enters reflective phase to refine its understanding of the environments mechanics. The LLM performs rule induction by analyzing the trajectory of the concluded episode, ğœğ‘˜1, in light of its previously held rules, ğ’¢ğ‘˜1. trajectory is the sequence of state-action-reward tuples recorded during the episode: ğœğ‘˜1 = {(ğ‘ 0, ğ‘0, ğ‘Ÿ1), (ğ‘ 1, ğ‘1, ğ‘Ÿ2), . . . , (ğ‘ ğ‘‡ğ‘˜11, ğ‘ğ‘‡ğ‘˜11, ğ‘Ÿğ‘‡ğ‘˜1)}. Work in Progress (2) 4 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 2: An overview of the Cogito, ergo ludo (CEL) agents two-phase operational cycle. In Phase 1, the agent leverages its Language-based World Model (LWM) to predict the outcomes of potential actions and its Language-based Value Function (LVF) to evaluate the desirability of the resulting states, ultimately selecting the optimal action. In Phase 2, it reflects on the episodes trajectory to update its explicit knowledge base (Environmental Rules and Strategic Playbook). The agent continuously improves through this dual learning loop, which not only refines its explicit knowledge but also trains the LLMs internal parameters based on the final outcome. The LLM processes this experiential data to generate an updated, more accurate set of rules ğ’¢ğ‘˜: (ğ¶ğ’¢, ğ’¢ğ‘˜) ğ‘â„’(ğœğ‘˜1, ğ’¢ğ‘˜1), (3) where ğ¶ğ’¢ is the reasoning trace for updating the environments Governing dynamics (or Game rules in game environments). We assume the agent begins with no prior knowledge (i.e., tabula rasa); the initial rule set ğ’¢0 is empty, and all subsequent knowledge is derived purely from interaction [20]. 3.3. Strategy and Playbook Summarization In parallel with rule induction, the agent updates its high-level strategy. After episode ğ‘˜ 1, the LLM synthesizes the trajectory ğœğ‘˜1 and the final outcome ğ‘ğ‘˜1 (e.g., success/failure, final score) to update strategic playbook, Î ğ‘˜. This process distills successful and unsuccessful patterns of interaction into explicit, actionable advice: (ğ¶Î , Î ğ‘˜) ğ‘â„’(ğœğ‘˜1, ğ‘ğ‘˜1, Î ğ‘˜1), where ğ¶Î  is the reasoning trace for the Playbook update, and Î 0 is initialized with general-purpose prompt. (4) This mechanism contrasts sharply with conventional reinforcement learning [3, 13], where strategy is implicitly encoded within the weights of neural network. In such systems, adaptation is often slow, Work in Progress 5 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning sample-inefficient (e.g., requiring millions of interaction frames [3, 13]), and opaque. Our approach externalizes strategy into an explicit, interpretable text playbook. Insights from single episode can be immediately incorporated into the agents prompt context for the subsequent episode. This mechanism facilitates rapid in-context learning, dramatically accelerating strategic adaptation. 3.4. Language-based Value Function To guide its planning, the agent employs the LLM â„’ as language-based value function. This component estimates the value of state Ë†ğ‘£(ğ‘ ğ‘¡), by providing qualitative, linguistic assessment of the long-term potential for success from that state. This evaluation is conditioned on both the current environmental rules ğ’¢ğ‘˜ and the strategic playbook Î ğ‘˜: (ğ¶ğ‘‰ , Ë†ğ‘£(ğ‘ ğ‘¡)) ğ‘â„’(ğ‘ ğ‘¡, ğ’¢ğ‘˜, Î ğ‘˜). (5) Here, ğ¶ğ‘‰ is the reasoning trace for the Value estimation. This function provides the agent with crucial heuristic by assessing the current states long-term potential, which is essential for effective planning. 3.5. The Agents Operational Cycle The agents operation is structured as cyclical pipeline that alternates between two phases: in-episode decision-making and post-episode reflection (Figure 2). This architecture decouples rapid, step-bystep action selection from more deliberate, offline knowledge consolidation process, with the LLM orchestrating both. Phase 1: In-Episode Decision-Making. During an episode ğ‘˜, the agent operates with fixed set of environmental rules ğ’¢ğ‘˜ and strategic playbook Î ğ‘˜. At each time step ğ‘¡, it performs structured reasoning process to select an action. First, the agents Language-based Value Function (LVF) assesses the desirability of the current state, ğ‘ ğ‘¡, providing high-level, holistic evaluation of its strategic potential. Concurrently, for each available action ğ‘, the agents Language-based World Model (LWM) performs one-step lookahead search to simulate the resulting state Ë†ğ‘ ğ‘¡+1 and reward Ë†ğ‘Ÿğ‘¡+1. The agent then commits to the action that the LWM predicts will lead to the most favorable outcome. The resulting (ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡+1) tuple is then recorded in the episodes trajectory ğœğ‘˜. Phase 2: Post-Episode Reflection and Refinement. Once an episode concludes, the agent enters the reflection phase to update its internal knowledge base. It performs Rule Induction by providing the LLM with the complete trajectory ğœğ‘˜ and the prior rule set ğ’¢ğ‘˜ to produce refined set of rules ğ’¢ğ‘˜+1. Concurrently, it engages in Strategy and Playbook Summarization, where the LLM processes ğœğ‘˜ and the final outcome ğ‘ğ‘˜ to distill key lessons, updating the playbook to Î ğ‘˜+1. The episodes final outcome (e.g., success or failure) provides reward signal that is used to train the agents core LLM, making it progressively more effective at planning and strategic reasoning in future episodes. This two-phase cycle enables the agent to both act decisively on its current understanding and systematically improve its environmental model and strategic acumen. Work in Progress 6 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 3: Learning curves of our agent on FrozenLake (left), Minesweeper (center), and Sokoban (right). The plots show the average success rate (y-axis) plotted against the number of LLM update steps (xaxis). Starting without any explicit rules, the agents consistent improvement across these diverse tasks showcases the effectiveness of its autonomous rule discovery and policy learning. 4. Experiments 4.1. Game Environments We evaluate our method on three classic grid-world environments: Minesweeper, Frozen Lake and Sokoban. These environments serve as common benchmarks for tasks with sparse rewards in reinforcement learning. Detailed descriptions of the environments are provided in Appendix A. In our experimental setup, all three environments are configured with sparse reward signal. The agent receives reward only at the conclusion of the game, receiving +1 for successfully completing the objective and 0 otherwise. Crucially, we DO NOT provide the agent with any explicit game rules. It must learn the dynamics of each environment solely through interaction, with its knowledge limited to the set of available actions. This setup, combining sparse rewards with unknown rules, presents significant reasoning and planning challenge. 4.2. Implementation Details We conducted experiments using rLLM [27], backed by verl [19]. We use the Qwen3-4B-Instruct [15] model to interact with the environments. We evaluate the performance over 32 randomly sampled seeds for each game. For each of these seeds, we conduct 8 independent trials, and report the average success rate over the total 256 playthroughs per game. The rule update frequency is set to once every 5 episodes. We use GRPO [2, 18] for LLM post-training and set the maximum response length to 8,192 tokens to encourage the model to think, reason, and plan. The outcome reward is used for optimizing the LLM. 4.3. Results Figure 3 illustrates our CEL agents learning performance across the three environments. We compare CEL against two zero-shot baselines operating with and without the ground-truth game rules, respectively. Despite starting with no explicit game rules, the agent demonstrates clear and positive learning trend in all tasks, validating the effectiveness of our interaction-reflection cycle. In the logical puzzle of Minesweeper, the agent exhibits steady improvement, with its success rate progressively climbing to peak of 54%. Notably, this surpasses the 26% success rate of the baseline agent that was explicitly Work in Progress 7 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning provided with the ground-truth game rules, suggesting that our method of autonomous rule discovery and strategy refinement leads to more effective policy. different learning dynamic emerged in the complex planning puzzle of Sokoban, where the agents performance showed distinct breakthrough pattern, increasing sharply to an 84% success rate after an initial period of exploration. This highlights its ability to uncover critical insights for solving multi-step problems. The agents efficiency was most apparent in the Frozen Lake navigation task, where it learned with remarkable speed to achieve near-perfect success rate of 97% within the first 10 episodes. Collectively, these results showcase the general applicability and effectiveness of our approach, as it successfully masters diverse tasks ranging from logical deduction to long-horizon planning by autonomously discovering environmental rules and iteratively refining its own strategy from raw interaction. 4.4. Ablation Study We conducted an ablation study in the Minesweeper environment to test the necessity the component of the iterative Induction of Environmental Dynamics, with results shown in Figure 4. The baseline agent, operating without the Rule Induction mechanism (w/o Rules), exhibits largely flat learning curve, with its success rate stagnating at low level. This confirms that the ability to infer and utilize model of the environments dynamics is fundamental to achieving competence. second variant, which performs Rule Induction only once and then uses static rule set (Rules induced once), shows initial improvement but quickly stagnates and its performance degrades, suggesting its initial rules were incomplete or inaccurate. In stark contrast, our full CEL agent, which engages in the post-episode reflection and refinement phase to continuously update its rule set ğ’¢ğ‘˜, shows robust and sustained learning trajectory, significantly outperforming both ablated versions. This comparison unequivocally demonstrates that the iterative refinement of the agents world model is critical component of our architecture. Figure 4: Ablation study illustrating the critical role of iterative Induction of Environmental Dynamics. The full model (blue) significantly outperforms variants with static rules (teal) or no rules (red), demonstrating that continuous refinement of the learned rulebook ğ’¢ğ‘˜ is essential for sustained performance improvement. 4.5. Case Study 4.5.1. In-Episode Decision-Making Figure 5 provides qualitative snapshot of the agents In-Episode Decision-Making process. The examples showcase how the agent performs one-step lookahead search, process relying on the synergy between its core cognitive components. First, the agent employs its Language-based Value Function (LVF) to produce holistic, linguistic assessment of the current states potential, Ë†ğ‘£(ğ‘ ğ‘¡). In the Minesweeper example, it correctly identifies the state as having high strategic value. Next, for each viable action, the agent utilizes its Language-based World Model (LWM), conditioned on its learned rules ğ’¢ğ‘˜, to simulate Work in Progress 8 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 5: The agents In-Episode Decision-Making process. At each step, the agent uses its Languagebased Value Function (LVF) to assess the current states value (middle column). It then employs its Language-based World Model (LWM) to predict the consequences of each action (right column). The agent selects the action leading to the outcome with the highest predict value. the immediate future, predicting the next state Ë†ğ‘ ğ‘¡+1 and reward Ë†ğ‘Ÿğ‘¡+1. It accurately forecasts that the (0, 3) action in Minesweeper solves the puzzle. By comparing the predicted outcomes, the agent selects the action leading to the most favorable consequence, highlighting how its explicit, language-based reasoning drives intelligent planning. 4.5.2. Autonomous Rule Discovery Figure 6 presents an example of the agents learned rulebook ğ’¢ğ‘˜ for Minesweeper, direct output of the Induction of Environmental Dynamics process. Synthesized from its interaction trajectory and starting from tabula rasa rule, the generated rules are remarkably comprehensive and accurate, covering everything from Symbol Meanings to the ultimate Game Objective. As defined in our method, this explicit, human-readable rule set ğ’¢ğ‘˜ is the critical information that grounds the agents cognitive functions. It provides the foundation for the Language-based World Model to predict future states and for the Language-based Value Function to estimate state values, thereby enabling all subsequent planning. 4.5.3. Emergent Strategy and Playbook Generation In parallel with rule induction, our agent constructs strategic playbook Î ğ‘˜, via the Strategy and Playbook Summarization process, synthesized in Figure 7. As defined, the LLM analyzes an episodes trajectory ğœğ‘˜1 and outcome ğ‘ğ‘˜1 to distill experiences into actionable advice. The emergent knowledge exhibits sophisticated hierarchy, from tactical Methods like Constraint Propagation to high-level Principles like Safe Exploration. The discovery of these expert-level heuristics from raw interaction highlights the agents capacity for strategic abstraction. This explicit playbook Î ğ‘˜ is then used alongside the rule set ğ’¢ğ‘˜ in the next episode to condition the Language-based Value Function, enabling more nuanced, strategically-aware judgments and forming direct feedback loop from experience to adaptation. Work in Progress Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 6: An illustrative excerpt of the agents learned rulebook ğ’¢ğ‘˜ for Minesweeper, generated via the Induction of Environmental Dynamics process. Starting from no prior knowledge, the agent synthesizes comprehensive and accurate set of rules from its interaction trajectory. Please refer to Figure 14 in the Appendix for the complete rulebook. Figure 7: synthesis of the strategic playbook Î ğ‘˜ for Minesweeper, generated via Strategy and Playbook Summarization. The agent distills both tactical Methods and high-level Principles from its gameplay experience. This explicit playbook is used to condition the agents value judgments, enabling more strategically sophisticated decision-making. 4.6. Generalization To validate that CEL agent learns by understanding rather than memorization, we tested its generalization capabilities in two settings, summarized in Table 1. Work in Progress 10 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Table 1: The CEL agents generalization performance across intra-game (unseen layouts) and intergame (new game environments) settings. The results demonstrate the agents strong generalization, showcasing both robust performance on unseen layouts (intra-game) and successful zero-shot transfer to novel environments (inter-game). Values in (.) show gain over the Zero-shot baseline, while those in [.] show change relative to the corresponding in-domain performance."
        },
        {
            "title": "Trained On",
            "content": "Zero-shot w/ Rule Minesweeper FrozenLake Tested On (Inter-Game) Minesweeper 25.8 FrozenLake 78.9 53.5 (+27.7) 97.3 (+18.4) 46.9 (+21.1) 97.3 (+18.4) Intra-Game (Unseen layouts) - 50.4 [-3.1] 93.8 [-3.5] Figure 8: Inter-game generalization study showcasing adaptation to novel environments without model retraining. The plots show the agents cross-game performance: Minesweeper-trained agent on Frozen Lake (left) and FrozenLake-trained agent on Minesweeper (right). In both evaluations, the core model weights remain frozen. The agents adaptation relies solely on the iterative refinement of its explicit rulebook and strategic playbook every 5 episodes (indicated by cyan lines). First, for intra-game generalization, we evaluated the agent on 32 new seeds that were entirely unseen layouts from those used during training. The agent maintained high level of performance on these unseen 256 instances, confirming that it learns the games fundamental principles rather than overfitting to the specific training levels. This highlights fundamental paradigm difference from conventional reinforcement learning, which is notoriously hard to generalize to unseen domains. Instead of overfitting to learned patterns, the CEL agents success on new layouts stems from its ability to apply an understanding of the games rules to reason and plan effectively. Furthermore, in the more challenging inter-game generalization setting, detailed in Figure 8, we tested model trained on one game in the environment of another. Specifically, Minesweeper-trained agent tested on Frozen Lake (left) and FrozenLake-trained agent on Minesweeper (right) both show robust learning curves, despite their core model weights being frozen. This success indicates that the agent transfers not its knowledge of game-specific rules, but rather its fundamental ability to learn by reasoning and planning when faced and interact with novel environment. The CEL agent thus demonstrates sophisticated ability to generalize not the concrete dynamics of game, but the abstract wisdom of how to reason, plan and then act. Work in Progress 11 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning 5. Conclusions In this work, we introduced Cogito, ergo ludo (CEL), novel agent architecture that learns by explicitly reasoning about its environment. Through unique two-phase cycle of in-episode planning and post-episode reflection, CEL autonomously constructs human-readable world model and strategic playbook from raw interaction, starting from tabula rasa state. Our results across several environments demonstrate that this learning by thinking approach allows the agent to master complex tasks while creating transparent and auditable decision-making process. CEL marks significant departure from opaque, brute-force learning paradigms. It validates language-based reasoning as powerful foundation for building agents that are not only capable but also interpretable and trustworthy, opening compelling pathways toward hybrid systems that fuse CELs explicit understanding with traditional architectural efficiency."
        },
        {
            "title": "References",
            "content": "[1] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysÅ‚aw DÄ™biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. DOTA 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. [2] Team DeepSeek. DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 645:633638, 2025. [3] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In International conference on machine learning, pages 1407 1416. PMLR, 2018. [4] Xidong Feng, Bo Liu, Yan Song, Haotian Fu, Ziyu Wan, Girish Koushik, Zhiyuan Hu, Mengyue Yang, Ying Wen, and Jun Wang. Natural language reinforcement learning. arXiv preprint arXiv:2411.14251, 2024. [5] Team Gemini. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [6] Leon Guertler, Bobby Cheng, Simon Yu, Bo Liu, Leshem Choshen, and Cheston Tan. TextArena. arXiv preprint arXiv:2504.11442, 2025. [7] David Ha and JÃ¼rgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. [8] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse control tasks through world models. Nature, 640:647653, 2025. [9] Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric Xing, Ion Stoica, Tajana Rosing, Haojian Jin, and Hao Zhang. LMGame-Bench: How good are LLMs at playing games? arXiv preprint arXiv:2505.15146, 2025. [10] Levente Kocsis and Csaba SzepesvÃ¡ri. Bandit based Monte-Carlo planning. In European conference on machine learning, pages 282293. Springer, 2006. Work in Progress 12 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning [11] Heng Lin and Zhongwen Xu. Understanding Tool-integrated Reasoning. arXiv preprint arXiv:2508.19201, 2025. [12] Zichen Liu, Anya Sims, Keyu Duan, Changyu Chen, Diyi Yang, Wee Sun Lee, and Min Lin. GEM: gym for generalist LLMs, 2025. URL https://axon-rl.notion.site/gem. [13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015. [14] Martin Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331434, 1990. [15] Team Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [16] Jonathan Richens, David Abel, Alexis Bellot, and Tom Everitt. General agents need world models. arXiv preprint arXiv:2506.01622, 2025. [17] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering Atari, Go, Chess and Shogi by planning with learned model. Nature, 588(7839):604609, 2020. [18] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [19] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [20] David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 1, 2025. [21] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484489, 2016. [22] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):354359, 2017. [23] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. general reinforcement learning algorithm that masters Chess, Shogi, and Go through self-play. Science, 362(6419): 11401144, 2018. [24] Rich Sutton. The Bitter Lesson, 2019. URL http://www.incompleteideas.net/IncIdeas/ BitterLesson.html. [25] Richard Sutton. The quest for common model of the intelligent decision maker. arXiv preprint arXiv:2202.13252, 2022. Work in Progress Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning [26] Richard Sutton and Andrew Barto. Reinforcement learning: An Introduction, volume 1. MIT press Cambridge, 2018. [27] Sijun Tan, Michael Luo, Colin Cai, Tarun Venkat, Kyle Montgomery, Aaron Hao, Tianhao Wu, Arnav Balyan, Manan Roongta, Chenguang Wang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. rllm: framework for post-training language agents, 2025. Notion Blog. [28] Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, MichaÃ«l Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350354, 2019. [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [30] Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, and Wei Yang. Agents play thousands of 3D video games. arXiv preprint arXiv:2503.13356, 2025. Work in Progress 14 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning This appendix extends upon the main paper by providing additional details on our experimental setup, ablation studies, qualitative results, and implementation specifics. Section A: Provides descriptions of the game environments used in our experiments. Section B: Presents an analysis on the action-only model. Section C: Shows the results of an additional experiment on Minesweeper with an expanded training set of 128 seeds, highlighting the scalability of our agent. Section D: Contains the full prompt templates used for the agents in-episode decision-making and post-episode reflection. Section E: Provides concrete, qualitative examples of the agents learned knowledge, including decision-making traces, environmental rules, and strategic playbooks for various games. A. Details of Environments All game environments used in our experiments are from the TextArena [6]. Below are the descriptions for the specific environments and configurations used in this work. Minesweeper is logic puzzle where the objective is to clear grid of all non-mine cells without detonating any mines. When cell is revealed, it displays number indicating how many adjacent cells contain mines, and the player must use this information to deduce the location of the mines. In our experiments, the game is configured on 55 grid with 3 randomly placed mines. Frozen Lake is canonical grid navigation problem on 66 grid where an agent must travel from start tile to goal tile, avoiding 6 randomly placed holes. In our deterministic setting, each action moves the agent exactly one cell in the chosen direction, removing the stochastic slippery nature often associated with this environment. This modification allows for direct assessment of the agents planning and rule-induction capabilities without the confounding factor of environmental randomness. Sokoban is classic puzzle game where the player must push all boxes to designated goal locations. The player can only push one box at time and cannot pull boxes. This simple constraint creates complex search space and necessitates careful, long-horizon planning to avoid irreversible states, such as trapping box in corner. The version used in our study is played on 66 grid with single box. B. Analysis on Rollout Outcomes for Action-only Model To understand the necessity of cognitive components and chain-of-thought reasoning, we ablated it with simplified Action-only agent that directly outputs actions. When attempting to train this agent with GRPO, we observed consistent training failure. The reason lies in the extreme polarization of rollout outcomes, as shown in Figure 9. GRPO requires batches with mixed results (i.e., partially successful) to derive learning signal. However, across all tested sampling sizes (8 to 64), the outcomes for the Action-only agent were always binary: for any given seed, all rollouts in batch either succeeded or failed. As result, the number of partially successful rollouts, i.e., the sole source of viable training signal, was consistently zero. This lack of comparative data within batches starves the GRPO algorithm of gradient, leading to breakdown in training and highlighting the critical role of nuanced reasoning traces in enabling effective optimization. Work in Progress 15 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 9: Distribution of rollout outcomes for the Action-only model. Across all group sampling sizes (8 to 64), outcomes are polarized into All (all succeed) or None (all fail). The number of Partial rollouts, which are required for GRPO to learn, is consistently zero, causing training failure. C. Performance with an Expanded Training Set To further evaluate the scalability and generalization capabilities of our CEL agent, we conducted an additional experiment on the Minesweeper environment. We expanded the set of training layouts by increasing the number of unique seeds from 32 (used in the main experiments) to 128. The results are presented in Figure 10. The agent demonstrates notable improvement in performance, with its peak success rate climbing from 54% (as reported in Figure 3) to new maximum of 62%. This finding suggests that exposure to more diverse set of game scenarios directly enhances the agents core reasoning and planning capabilities. This confirms that the agent is developing robust, generalizable problem-solving model for the game, rather than overfitting to limited number of specific layouts. Figure 10: Learning curve for the CEL agent on Minesweeper when trained on an expanded set of 128 unique seeds. The agent achieves new peak success rate of 62%, surpassing the performance observed with 32 seeds. Work in Progress 16 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning D. Prompt Templates In this section, we present the core prompt templates used by the CEL agent. Figure 11 shows the template for in-episode decision-making, and Figure 12 shows the template for post-episode reflection. Figure 11: The prompt template for in-episode decision-making (Phase 1). It instructs the LLM to evaluate the current state, assess their strategic value (LVF) and predict action outcomes (LWM). E. Additional Results To illustrate the explicit and interpretable knowledge base generated by our CEL agent, we provide concrete examples of Decision-Making processes (Figure 13), learned environmental rules (Figure 14, Figure 15, Figure 16) and strategic playbooks (Figure 17, Figure 18) for the Minesweeper, FrozenLake and Sokoban environment. Work in Progress 17 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 12: The prompt template for the Rule Induction and Playbook Summarization process (Phase 2). It guides the LLM to analyze completed episodes trajectory and refine its explicit model of the environments dynamics. Work in Progress 18 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 13: An example of Decision-Making process for Sokoban environment. Figure 14: An example of learned environmental rule for Minesweeper environment. Work in Progress 19 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 15: An example of learned environmental rule for FrozenLake environment. Work in Progress 20 Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 16: An example of learned environmental rule for Sokoban environment. Work in Progress Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning Figure 17: An example of learned strategic guideline from the agents playbook for FrozenLake environment. Figure 18: An example of learned strategic guideline from the agents playbook for Sokoban environment. Work in Progress"
        }
    ],
    "affiliations": [
        "Tencent",
        "Wuhan University"
    ]
}