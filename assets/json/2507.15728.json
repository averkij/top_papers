{
    "paper_title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation",
    "authors": [
        "Wenqi Ouyang",
        "Zeqi Xiao",
        "Danni Yang",
        "Yifan Zhou",
        "Shuai Yang",
        "Lei Yang",
        "Jianlou Si",
        "Xingang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ ."
        },
        {
            "title": "Start",
            "content": "TokensGen: Harnessing Condensed Tokens for Long Video Generation Wenqi Ouyang1, Shuai Yang3, 1S-Lab, Nanyang Technological University, Zeqi Xiao1, Lei Yang2, Danni Yang2, Jianlou Si2, Yifan Zhou1, Xingang Pan1 2SenseTime Research, 3Wangxuan Institute of Computer Technology, Peking University https://vicky0522.github.io/tokensgen-webpage/ 5 2 0 2 1 2 ] . [ 1 8 2 7 5 1 . 7 0 5 2 : r Figure 1. Given the text prompt, TokensGen generates long videos of up to 2 minutes, maintaining consistent motions and content. Moreover, TokensGen supports zero-shot prompt-guided video-to-video editing for long videos."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Generating consistent long videos is complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, novel twostage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), short video diffusion model guided by text and video tokens, with Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-toToken), video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Generating consistent and visually pleasing long videos remains formidable challenge in the field of video generation. While diffusion-based methods excel at short video generation [13, 9, 25, 37, 38, 42, 48], extending them to longer durations is constrained by computational resources, posing significant challenges. Many approaches attempt to decompose long video generation into manageable sub-problems, using short video models without adding memory overhead. Tuning-free methods [7, 27, 29, 30, 33, 36] generate long videos using pre-trained short video models with hand-crafted techniques like noise re-scheduling, sliding window fusion, and attention manipulation, combined with multi-prompt sampling to enrich content. While ensuring high frame quality, these methods struggle with unnatural transitions due to missing long-range priors. Auto-regressive methods [1, 10, 18, 23, 31] and image-to-video approaches [5, 10, 41, 46] generate clips sequentially, achieving smooth transitions. However, they suffer from error accumulation, limited context windows, and unstable long-term controllability. Hierarchical methods, such as MovieDreamer [47], adopt multi-stage pipeline to address long-range challenges efficiently. They generate keyframes and then synthesize short clips guided by these keyframes, producing high-quality results. However, they mainly focus on multiscene generation and lack strict consistency in motion and appearance across adjacent clips. These limitations in long-term and short-term content control underscore the need for unified, scalable solution that maintains both long-term and short-term consistency without excessive memory overhead. Therefore, we propose TokensGen, which leverages condensed video tokens to bridge short-clip generation with long-term consistency. Unlike hierarchical methods relying on keyframes generation and interpolation, or purely frame-level auto-regressive sampling, TokensGen jointly models spatial and temporal distributions for long videos through two-stage framework, as detailed below: a) To2V Model (Inner-clip content control): We employ conditional short video generation model guided by text and video tokens to produce semantically rich yet concise video segments. Built on powerful pre-trained backbone (CogVideoX [42]), our Video Tokenizer encodes short clips into condensed set of high-level semantic tokens. This enables robust spatial layouts and motion cues in an efficient representation space, achieving stronger per-clip semantic control than text prompts alone. c) Adaptive FIFO-Diffusion b) T2To Model (Long-term content consistency): We train video token diffusion transformer to generate the full set of tokens for minute-long video from text prompts. These tokens are derived by encoding the long video clipby-clip using the Video Tokenizer. Operating in this token space enables the T2To Model to maintain content continuity and logical coherence across clips while significantly reducing memory demands compared to raw frame modeling, preserving sufficient semantic detail for global consistency. temporal smoothness): During inference, we sample long video tokens via the T2To Model and employ them to guide clip generation in the To2V Model. However, naively concatenating clips may cause boundary discontinuities, even with consistent semantic tokens. To overcome this, we propose an adaptive FIFO-Diffusion process for the To2V Model, enabling diagonal denoising of consecutive clips. This approach prevents distributional artifacts caused by naive padding or frame replication in FIFO-Diffusion [23], ensuring smoother transitions and improving the overall fidelity of the long video. (Inter-clip Compared to prior methods for long video generation, TokensGen offers several key advantages. First, by leveraging pre-trained short video models, it inherits strong knowledge priors and architectural designs, enabling smooth transition from short clips to minute-long sequences without extensive re-engineering. Second, encoding long videos into condensed token representations significantly reduces computational overhead for minute-level generation. Third, because each component (To2V Model, T2To Model, and the inter-clip scheduling) operates in clearly defined subtask, our pipeline is highly flexible. It can seamlessly integrate with other short-term control strategies (e.g., Progressive Diffusion [40], Rolling Diffusion Models [31]) or multi-prompt composition frameworks [1, 7, 33]. In summary, TokensGen offers scalable and resourceefficient framework for generating long videos with longterm consistency and smooth transitions, as shown in Fig. 1. By harnessing condensed tokens and powerful short video models, our approach significantly lowers the barrier to high-quality long video generation, opening new possibilities for storytelling, simulation, and beyond. 2. Related Work Video diffusion models. Video diffusion models generate videos from text or image prompts. Early methods [5, 9, 15, 16, 37, 38, 41] extend U-Net-based image diffusion to the temporal domain. However, they struggle with motion dynamics and content richness due to separate spatial-temporal attention and limited temporal windows. Recent works [2, 25, 28, 34, 42, 48] enhance fidelity and consistency by integrating diffusion transformers with 3D full-attention to jointly model spatial-temporal correlations and improved text encoders for complex prompts. While effective for short videos, extending these models to long videos remains computationally prohibitive. Long video generation. Long video generation poses additional challenges for achieving content coherence, consistent dynamics, and efficient resource usage. We categorize long video generation methods into two groups: 1) those that optimize resource usage via engineering techniques or efficient model design, and 2) those that decompose long video generation into short video sub-tasks. Resource usage optimization. Recent transformer-based methods [2, 25, 28, 34, 42, 48] employ 3D-VAE to compress videos. However, as noted in CogVideoX [42], excessive compression hinders 3D-VAE convergence. ExVideo [12] extends SVD [5] to 128 frames via small learnable parameters with low memory overhead. Pyramidal Flow Matching [21] reformulates diffusion into pyramid stages, enabling efficient generation of videos up to 240 frames. While effective, these methods still face challenges in scaling to much longer durations. Problem decomposition via short video generation. Multi-scene generation Multi-shot approaches [14, 26, 45, 50] divide long videos into segments and align them under unified narrative, conditioning video diffusion on scene-level text or styles for coherence. MovieDreamer [47] employs hierarchical pipeline to draft keyframes and refine shots. These methods emphasize storytelling and character consistency with relaxed demands on interclip coherence. Tuning-free methods. Tuning-free methods extend short video generation to longer durations via hand-crafted designs, such as co-denoising [29, 36], noise re-scheduling with sliding windows [30], and attention control mechanisms [7, 27, 33]. Often paired with multi-prompt sampling for content richness, these methods lack long-range priors, leading to unnatural motion and appearance transitions over extended durations. Auto-regressive methods. Auto-regressive methods generate long videos frame-by-frame (or clip-by-clip), ensuring temporal consistency at the cost of increased inference time. Image(clip)-to-video models [5, 18, 35, 41, 46] generate long videos iteratively by treating the last generated frame (clip) as the initial one in the next iteration, but suffer quality degradation due to error accumulation. Loong [39] adopts an auto-regressive language model-like strategy but is limited to low resolutions (128 128). Causal denoising [23, 31, 40, 44] gradually increases noise scales for later frames, ensuring smooth transitions by referencing clearer earlier frames. Commercial tools like Kling [1] offer clip-by-clip video extension but are constrained by context windows and unstable long-term controllability. Hierarchical methods. Hierarchical methods [6, 17, 43] use hierarchical frameworks to generate keyframes and perform interpolation or super-resolution. However, keyframe-only propagation increases information loss, degrading quality as stages grow. StoryDiffusion [49] adopts multi-stage pipeline, generating keyframes, predicting motion, and synthesizing clips guided by both. Different from those methods, our method models correlated spatial-temporal distribution jointly for minute-long videos via condensed tokens, utilizing the fine-grained information to guide the short clip generation to achieve more natural and consistent content across the long range. 3. Preliminaries CogVideoX. We use the pre-trained text-to-video diffusion model CogVideoX [42] as the foundation of our framework. CogVideoX employs 3D causal VAE [24] to compress videos into latent space, achieving an 8 8 4 compression ratio along the spatial and temporal axes. During input processing, video latents are patchified and concatenated with text embeddings, which are then passed through Expert Transformer blocks featuring Expert Adaptive LayerNorm (AdaLN) and 3D Full Attention, as shown in Figure 2. Text and Vision Expert AdaLN separately modulate text and video features, improving alignment. To handle large motions, CogVideoX integrates 3D Text-Video Hybrid Attention with 3D Rotary Position Embedding (RoPE) [32], effectively capturing spatial-temporal relationships. FIFO-Diffusion. FIFO-Diffusion [23] introduces diagonal denoising strategy to extend pre-trained text-to-video Figure 2. CogVideoX architecture. model from frames (f ) to generate long videos with frames. This method progressively denoises consecutive frames with increasing noise levels. Given time step schedule 0 = τ0 < τ1 < ... < τf = , each denoising step is defined as follows: [z1 τ0 ; ...; zf τf 1 ] = Φ([z1 τ1 ; ...; zf τf ], [τ1; ...; τf ], c; ϵθ). (1) }f The diagonal latents {zi i=1 are stored in queue. At τi each step, the foremost frame is dequeued once it reaches τ0 = 0, while new latent at τf is enqueued. This ensures that later frames with higher noise levels reference earlier frames with lower noise levels during denoising, maintaining temporal consistency and coherence throughout the long video generation process. 4. TokensGen for Long Video Generation 4.1. Overview Given text prompt, our framework generates consistent minute-long video aligned with the prompt. It consists of two main components: To2V and T2To Models, as shown in Figure 3. During training, we first train To2V, conditional short video generation model, to control spatial layout and motion based on text and video prompts. video tokenizer extracts compact semantic tokens zsem from short clips, which are then fed into diffusion transformer for guided generation. Since these tokens encode richer spatial and motion information than text prompts, they provide more accurate semantic control over individual clips. For long videos, we segment them into short clips, each tokenized to produce sequence of semantic tokens {zsem,i}N i=1, forming resource-efficient high-level representation of the entire video. We then train T2To, video token transformer, to generate these long video tokens simultaneously from text prompts, ensuring long-term content consistency across clips. During inference, we first sample long video semantic tokens using T2To, then pass them to To2V to generate each clip. To ensure temporal consistency, we introduce an adaptive FIFO denoising strategy for diagonal denoising across clips. Figure 3. Overview of the model. Left: Overall Framework for TokensGen. Right: Trainable Modules. 4.2. To2V Model: Inner-clip content control We design conditional short video generation model, To2V, guided by both text and video prompts for precise content control in short video generation. To2V builds on the pre-trained text-guided video generation model CogVideoX [42] and consists of two key components: the Video Tokenizer that encodes the input video clip into compact semantic tokens, and the Cross-Attention Branch integrated with CogVideoX that enables cross-attention between semantic tokens and noisy latents. Video Tokenizer. The Video Tokenizer consists of 3D causal variational autoencoder (3D-VAE), Patchify Module, and Resampler, as illustrated on the right side of Figure 3. The 3D-VAE and the Patchify Module are inherited from CogVideoX with fixed weights. They process the input video into set of tokens zsource with the shape fs hs ws cs, where , h, w, and represent the number of frames, height, width, and channels, respectively. The Resampler compresses and resamples zsource into more compact representation space, as illusIt comprises learnable latent zlatent trated in Figure 4. with the shape fr hr wr cs, four blocks of the 3D Cross-Attention Module that perform cross-attention between zsource and zlatent, and Projector that transforms zlatent into zsem with shape fr hr wr cr, where fr < fs, hr < hs, wr < ws, cr < cs. The semantic tokens zsem encoded by the Video Tokenizer encapsulate high-level spatial layouts and motion information from the input video while maintaining more compact size compared to the original video. Cross-Attention Branch. To effectively incorporate these semantic tokens with CogVideoX, we add separate CrossAttention Branch to handle the newly added semantic conFigure 4. The architecture of the Resampler. ditions. This branch consists of Semantic Token Adaptive LayerNorm (Sem AdaLN) and 3D Cross-Attention Module, as depicted on the right side of Figure 3. The process is as follows: Back projection: The semantic tokens zsem from the Video Tokenizer are back-projected to match the number of channels of the combined text-video embeddings Ztv = [Ztext; Zvideo]. Concatenation: These back-projected semantic tokens are concatenated with the text-video embeddings. Modulation: Similar to the Text and Vision AdaLN, the Sem AdaLN modulates the semantic condition embeddings to ensure better feature alignment. Attention: The modulated embeddings are passed to the 3D Text-Video Attention and the 3D Cross Attention Module to perform 3D full attention on the combined embeddings. Given the combined embeddings [Ztext; Zvideo; Zsem], the output attention results [Z text; video; sem] are represented as follows: Ztv = [Ztext; Zvideo] = ZtvWq, = ZtvWk, = ZtvWv q, Ks = ZsemWc q, Ktv = ZtvWc k, Vs = ZsemWc k, Vtv = ZtvWc Qs = ZsemWc Qtv = ZtvWc text; [Z video] = Attn(Q, K, V) + Attn(Qtv, Ks, Vs) = softmax( Q(K)T )V + softmax( Qtv(Ks)T )Vs, sem = Attn(Qs, [Ktv; Ks], [Vtv; Vs]) = softmax( Qs([Ktv; Ks])T )[Vtv; Vs], where Wq, Wk, Wv are fixed parameters of the 3D Text-Video Attention Module inherited from CogVideoX, Q, K, are the corresponding query, key, and value matrices. Wc are trainable parameters of the 3D Cross-Attention Module, Qs, Ks, Vs are the query, key, and value matrices for Zsem, while Qtv, Ktv, Vtv are the query, key, and value matrices for the combined text and video embeddings Ztv. k, Wc q, Wc 4.3. T2To Model: Long-term content consistency To learn long-term content and logic knowledge across the minute-long video, we design video token transformer, the T2To Model, to generate the semantic tokens {zsem,i}N i=1 representing the whole long video given the input text prompt. We adopt the same model structure and training strategy of CogVideoX [42] for the T2To Model, except for the following modifications: The model aims to generate {zsem,i}N i=1 with the shape (N fr) hr wr cr. The total number of tokens is (N fr) hr wr. Since the number of tokens along the temporal dimension is much larger than the spatial dimensions, for 3D-RoPE, we reallocate the hidden state channel for height, width, and temporal dimension as about 10%, 10%, 80%. 4.4. Inter-clip temporal consistency If each clip is denoised separately with corresponding semantic token zsem,i, the model will generate group of discontinuous clips. To achieve temporal continuity, we apply the FIFO-denoising strategy during the inference stage. Specifically, we adopt latent partitioning and lookahead denoising, like the original FIFO. However, to maintain queue with sufficient frames at the start of denoising, FIFO pads the positions ahead of the first clip with noiseaugmented first frame replications. We observe that this approach introduces artifacts in our settings, as the replicated frames deviate from the intrinsic distribution of the training domain for the video diffusion model. To address this, we propose an improved version of FIFO, named adaptiveFIFO, which incorporates an adaptive padding strategy at the beginning of the denoising process. For latent partition containing fewer than fs frames, we denoise all frames together and update them simultaneously. For partition with exactly fs frames, we employ lookahead denoising: the frames are denoised together, but only the noisier frames in the latter portion are updated. By better aligning the initial padding with the models learned distribution and ensuring continuity in partially filled partitions, this approach yields smoother transitions across clips and better frame quality. 4.5. Training strategy For To2V Model, we fix the weights of the pre-trained modules of the base model, train the Resampler of the Video Tokenizer and the Cross-Attention Branch. For T2To Model, we initialize the model with the weights of the base model and train all the modules. We adopt similar training strategies with CogVideoX [42], including Multi-Resolution Frame Pack and Explicit Uniform Sampling. For T2To Model, we pack videos with different time duration into the same batch, and apply an attention mask indicating the valid frames, as well as the attention mask for loss calculation, to ensure attention module focus on the right area of the input noisy latents, an approach similar with Patchn Pack [11]. For both To2V and T2To Model, we employ the explicit uniform sampling strategy for sampling timesteps. 5. Experimental Results 5.1. Implementation Details Model Architecture. We employ CogVideoX-5B [42] as the base model for both To2V and T2To. In To2V, the input tokens zsource have the shape 13 30 45 3072. We observed that T2To struggles to converge when cr is large (e.g., comparable to cs), so we set the compressed semantic tokens zsem to have dimensions 4 8 12 16. For the Projector in the Resampler, we observed that linear projection via PCA [13] effectively reduces the channel dimension without sacrificing information, as further analyzed in Sec. 5.3. Compared to the original latent shape 13 60 90 16, we achieve compression ratio of approximately 3 8 8. Thus, we first train To2V without the channel projection and then apply PCA to the Resamplers output embeddings on 300 samples to obtain the transformation matrix. In T2To, we set the maximum number of chunks = 24. Each chunk contains 49 frames, allowing our model to process videos up to 24 49 = 1176 frames. Dataset. We use the MiraData dataset [22], comprising long videos with structured captions. We first collect 56k videos, using their dense captions for training. For To2V Model, we randomly sample 49-frame video clips at 10 fps Figure 5. The qualitative comparison. We recommend readers refer to our webpage for video comparisons. Table 1. Quantitative evaluation of comparison study."
        },
        {
            "title": "Dynamic\nDegree",
            "content": "Text-Visual Alignment Motion & Content Consistency Video-Infinity DiTCtrl AP-FIFO+CogVideoX"
        },
        {
            "title": "Ours",
            "content": "81.80 76.76 86.22 84.57 90.56 87.96 92.89 92.2 96.66 95.52 94.78 95. 97.65 97.78 97.48 98.08 61.58 59.26 64.10 63.31 31.0 75.0 78.57 78. 0.0% 0.0% 24.31% 75.69% 0.69% 1.0% 22.57% 75.74% Table 2. Ablation study on ways of incorporating video conditions. Methods MSE SSIM temporal flckering SR 4x8x12 (w/o projection) 4x5x7 13x5x7 4x8x12 1.12e-2 1.24e2.84e-2 1.54e-2 1.24e-2 0.60 0.62 0.49 0.58 0.62 97.10 97. 97.57 97.54 97.58 from these long videos as training targets. For T2To Model, we select filtered subset of around 16k high-quality videos that are at least one minute long, primarily consisting of gameplay footage and natural landscape. We filter out videos with abrupt scene changes via PySceneDetect [8] and human evaluations. This subset ensures consistency within long videos for the training of T2To Model. Training Details. We adopt progressive learning strategy for both T2To and To2V Model. For To2V Model, we first train on smaller token shapes (4 5 7 3072) for 1,200 iterations, using batch size of 72 and learning rate of 1 103. We then transition to the full token resolution (4 8 12 3072) for 2,600 more iterations, initializing from the previously trained model. For T2To Model, we begin with shorter videos (N = 3 chunks of 49 frames each) for 1,200 iterations, using learning rate of 1 103. Next, we expand to long videos with up to = 24 chunks, training for 5,000 iterations, with learning rate of 3 104 and batch size of 105. This progressive training helps the model converge faster for more complex, extended video generation. 5.2. Baseline comparisons Qualitative comparisons We evaluate our approach against several recent multi-prompt long video generation methods, including Video-Infinity [33], DiTCtrl [7], and Kling [1], as well as baseline that adopts FIFO-Diffusion [23] on CogVideoX with our adaptive-padding strategy. For multiprompt methods, we use GPT-4o [4] to split the prompt into 24 segments, which are used for guiding each clip generation. FIFO and ours use the same text prompt, abbreviated as: person riding horse along path leading to serene river. The results are shown in Fig. 5. VideoInfinity produces transitions primarily through background changes while failing to capture meaningful foreground motion. The person and the horse remain essentially static within each clip, resulting in low engagement and narrative drift over longer durations. DiTCtrl demonstrates intermittently aligned keyframes but struggles to maintain smooth transitions between clips, leading to abrupt scene shifts and disjointed storyline. Kling generates visually consistent frames but exhibits erratic motions (e.g., the subject abruptly reversing direction) and occasional discontinuities in scene composition. Such artifacts disrupt the viewing experience and deviate from the intended story arc. For FIFO (with adaptive padding on CogVideoX), we observe progressive over-saturation and abrupt changes in appearance or color schemes as the video extends. These issues are especially pronounced when generating complex scenes over hundreds of frames. By contrast, our method delivers smoother motion transitions and subject representation, adherent to the prompt throughout the entire minute-long sequence. More comparison results are included in the Appendix B. Quantitative comparisons. We conduct quantitative comparison study on 100 prompts randomly selected from the MiraData [22] test set. As reported in Tab. 1, our approach achieves the highest scores on VBench [19] for both Motion Smoothness and Dynamic Degree. We observe that certain metrics in VBench (e.g., Subject and Background Consistency, and Temporal Flickering) may assign higher scores to less dynamic videos, motivating us to conduct user study for more comprehensive evaluation. For the user study, we generate 12 video results for each method, with each video lasting between one and two minutes, covering categories such as humans, cars, and natural scenes. All the resulting videos are included on our webpage. To ensure unbiased feedback, videos are randomly shuffled and presented to 24 participants. They evaluate each video on text-visual alignment and motion & content two criteria: consistency. As shown on the right side of Tab. 1, our method consistently outperforms others in both dimensions, reflecting its strong long-term control capabilities. These results demonstrate that our approach effectively maintains semantic alignment with the textual prompt while preserving smooth motion and stable content over extended sequences. Additional results and details are provided in the Appendix A. 5.3. Ablation studies Ablation on video conditions. We investigate various strategies for incorporating video conditions into the To2V Model: (1) the condensed token shape, (2) with or without channel projection, and (3) super-resolution-based approach. Specifically, we experiment with three token shapes (4 5 7, 13 5 7, 4 8 12), train models with and without the Projector module, and compare them against super-resolution setting (where the low-resolution video directly serves as the condition). We also include baseline using FIFO-Diffusion to illustrate its potential shortcomings without the conditioning process. As shown in Figure 6, the FIFO-based approach often produces inconsistent foreground and background visuals, underscoring the difficulty of preserving spatial-temporal coherence from purely latent-level guidance. Meanwhile, the superresolution method tends to duplicate low-level color and texture cues from the source, failing to capture higher-level semantics, leading to less meaningful renderings. Comparing models with and without the Projector, we observe similar performances, demonstrating that our PCA-based projection provides lightweight yet effective means of dimensionality reduction without sacrificing image quality. Concerning the shape of the condensed tokens, the smallest variant (4 5 7) struggles to preserve essential layout and motion patterns, resulting in less accurate re-creations of the source video. Increasing the tokens temporal or spatial resolution (13 5 7, 4 8 12) significantly improves alignment and maintains better semantic fidelity. Among these, (4 8 12) achieves the most favorable balance of fine-grained detail and computational efficiency, as quantitatively confirmed in Tab. 2. Overall, these ablation studies demonstrate that our token representation, combined with optional PCA-based projection, offers robust and effective pathway to incorporate video conditions in To2V Model. Ablation on FIFO. We further examine the influence of FIFO-Diffusion and our adaptive padding technique by comparing three variants: (1) omitting FIFO entirely, (2) using FIFO without adaptive padding, and (3) our full approach, incorporating both FIFO and adaptive padding. As illustrated in Fig. 7, disabling FIFO leads to abrupt scene changes between consecutive clips, producing visually inconsistent transitions where subjects may teleport or backgrounds shift abruptly. Meanwhile, removing adaptive padding induces severe artifacts in the initial frames of the video, as the model relies on duplicated frames that deviate from the training distribution. These artifacts propagate into subsequent frames, degrading overall quality. In contrast, our adaptive padding strategy aligns the padding frames with the models distribution, preventing unnatural discontinuities at clip boundaries. 5.4. Long Video Editing Beyond generating entirely novel content, our method readily adapts to various long video editing scenarios. Specifthe To2V Models capacity to integrate textual ically, Figure 6. Ablation study on methods of incorporating video conditions. Figure 7. Ablation study FIFO. prompts with source video data allows for transformations that preserve the essential structure of the original footage while injecting new semantics. We directly combine the target text prompt and the source video as input conditions to generate the edited long video, as shown in Fig. 8. For more results, please refer to our webpage. 6. Conclusion and Discussion We introduce TokensGen, two-stage framework that addresses key challenges in long video generation, controlling per-clip semantics, ensuring long-term coherence, and achieving smooth transitions. The To2V Model generates short clips guided by text and video prompts, capturing finegrained motion and content. The T2To Model transformer Figure 8. Long Video Editing. then leverages condensed semantic tokens to preserve longterm consistency across clips. Finally, our adaptive FIFODiffusion strategy overcomes boundary artifacts by maintaining temporal continuity. This pipeline efficiently scales pre-trained short video models to longer videos, enabling scalable, flexible, and resource-efficient approach to long video generation. Despite the effectiveness of TokensGen in maintaining long-range consistency, it does not preserve all finegrained details. Focusing on high-level semantics, tokens may cause gradual variations in foreground objects over extended sequences (detailed in the supplementary). In complex scenes, their capacity to capture intricate spatialtemporal cues may be insufficient, requiring fine-grained tokenization and stronger short-term consistency strategies beyond tuning-free FIFO. Our framework is tested on limited dataset of gameplay and landscape videos, but is scalIn future able to larger datasets for broader applications. work, exploring multi-scale tokenization or hybrid representations could bolster fine-grained controllability, retaining subtle attributes while preserving the scalability and resource efficiency. Acknowledgements This research is supported by NTU SUG-NAP and is also supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAFICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s)."
        },
        {
            "title": "References",
            "content": "[1] Kling. https://kling.kuaishou.com/, 2024. 1, 2, 3, 6, 12 [2] Hunyuanvideo: systematic framework for large video generative models, 2024. 2 [3] sora. https://openai.com/sora/, 2024. 1 [4] Gpt-4o. chatgpt.com, 2025. 6, 12 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2, 3 [6] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. Advances in Neural Information Processing Systems, 35:3176931781, 2022. [7] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. arXiv:2412.18597, 2024. 1, 2, 3, 6, 12 [8] Brandon Castellano. Pyscenedetect. https://github. com/Breakthrough/PySceneDetect, 2024. 6 [9] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 1, 2, 12 [10] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion In ICLR, model for generative transition and prediction. 2023. 1 [11] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M. Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey A. Gritsenko, Mario Lucic, and Neil Houlsby. Patch pack: Navit, vision transformer for any aspect ratio and resolution. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 5 [12] Zhongjie Duan, Wenmeng Zhou, Cen Chen, Yaliang Li, and Weining Qian. Exvideo: Extending video diffusion models via parameter-efficient post-tuning. arXiv preprint arXiv:2406.14130, 2024. 2 [13] Karl Pearson F.R.S. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2 (11):559572, 1901. [14] Ting Yao Fuchen Long, Zhaofan Qiu and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos. In ECCV, 2024. 2 [15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse conarXiv preprint trols to text-to-video diffusion models. arXiv:2311.16933, 2023. 2 [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toInternaimage diffusion models without specific tuning. tional Conference on Learning Representations, 2024. 2 [17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. 3 [18] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 1, 3, 13 [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 7 [20] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, YingCong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench++: Comprehensive and versatile bencharXiv preprint mark suite for video generative models. arXiv:2411.13503, 2024. 14 [21] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. [22] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions, 2024. 5, 7, 12 [23] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473, 2024. 1, 2, 3, 6, 13 [24] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [25] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1, 2 [26] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. 2023. [27] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. 1, 3 [28] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 [29] Yongjia Ma, Junlin Chen, Donglin Di, Qi Xie, Lei Fan, Wei Chen, Xiaofei Gou, Na Zhao, and Xun Yang. Tuning-free long video generation via global-local collaborative diffusion. arXiv preprint arXiv:2501.05484, 2025. 1, 3 [30] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling, 2023. 1, 3, 13 [31] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel arXiv preprint Hoogeboom. Rolling diffusion models. arXiv:2402.09470, 2024. 1, 2, 3 [32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [33] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. 1, 2, 3, 6, 12 [34] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2 [35] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, and Bin Cui. Videotetris: Towards compositional text-to-video generation. arXiv preprint arXiv:2406.04277, 2024. 3, 13 [36] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. 1, 3 [37] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 1, 2 [38] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2024. 1, 2 [39] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. [40] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Progressive arXiv preprint Feng Liu, Arie Kaufman, and Yang Zhou. autoregressive video diffusion models. arXiv:2410.08151, 2024. 2, 3 [41] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain arXiv preprint images with video diffusion priors. arXiv:2310.12190, 2023. 1, 2, 3 [42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 3, 4, 5, 14 [43] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 3 [44] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772, 2024. [45] Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, and Lichao Sun. Mora: Enabling generalist video generation via multi-agent framework. arXiv preprint arXiv:2403.13248, 2024. 2 [46] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. 2023. 1, 3 [47] Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence, 2024. 1, 2 [48] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1, 2 [49] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. NeurIPS 2024, 2024. [50] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream vlog. arXiv preprint arXiv:2401.09414, 2024."
        },
        {
            "title": "Appendix",
            "content": "Overview. The appendix includes sections as follows: Details of Comparison Study (Appendix A). Additional Comparisons (Appendix B). Limitations and Discussions (Appendix C). Additional Visual Results (Appendix D). A. Details of Comparison Study A.1. Prompt Splitting When comparing our approach with multi-prompt methods such as Video-Infinity [33], DiTCtrl [7], and Kling [1], we first divide the input text prompt into several chunks to guide the generation of individual clips. Specifically, for DiTCtrl and Kling, we employ GPT-4o [4] to split the provided prompt into 24 chunks for 2-minute-long video or 13 chunks for 1-minute-long video, using the following instructions: Please split the prompt depicting video into 24 separate prompts, each depicting specific range of the duration of the video in order, and each should have the same style and length as the original prompt. Each prompt should be strictly aligned with the original prompt; if additional content is added, it should also be aligned with the scenery of the original prompt. Each prompt should occupy one line. Please do not insert blank line between two prompts. The output format is as follows: <split prompt 1> <split prompt 2> <split prompt 3> ... <split prompt 24> The prompt needs to be split is: <paste the input text prompt here> For Video-Infinity, which is built on VideoCrafter2 [9] supporting text prompts of up to 77 tokens, we utilize its ability to perform parallel inference across 8 GPUs. To efficiently split text prompts for this method, we provide GPT-4o [4] with the following instructions: Please split the prompt depicting video into 8 separate prompts, each depicting specific range of the duration of the video in order, and each should have the same style as the original prompt. Each prompt should be strictly aligned with the original prompt, if additional content is added, it should also be aligned with the scenery of the original prompt. Each prompt should have fewer than 55 words. Please do not insert blank line between two prompts. The output format is as follows: <split prompt 1> <split prompt 2> <split prompt 3> ... <split prompt 8> The prompt needs to be split is: <paste the input text prompt here> Although we provided detailed instructions, we observed that this task remains highly challenging. GPT-4o often generates split prompts where each segment contains words with different total number than the original prompt, deviating from the intended style and length. To ensure reproducibility and facilitate comparison, we include all the text prompts along with their corresponding split versions used in the study in the accompanying supplementary material. A.2. User Study We conduct user study to further evaluate the effectiveness of our method. Test prompts are collected from MiraData [22]. For multi-prompt methods, we split the text prompts using the approaches described in the previous section. For AFIFO+CogVideoX, the same input text prompt as our method is used. In total, we generate 12 video results for each method, with each video ranging from 1 to 2 minutes in length. The test categories include humans, cars, and natural scenes. All videos used in the user study are displayed on our webpage. To ensure an unbiased evaluation, the results are randomly shuffled and displayed to 24 participants. Participants are asked to evaluate the videos based on two aspects: text-visual alignment and motion and content consistency. Questions for each aspect are as follows: Which one best aligned the given text? Which one keeps the best motion and content consistency in the long-range? For example, the video does not demonstrate scene disjoint, unreasonable content, or obvious quality degradation. Our method achieves the best performance across all aspects of the human evaluations, as presented in our main paper. These results highlight the superior long-term control capability of our proposed method, effectively demonstrating its ability to maintain text-visual alignment and ensure motion and content consistency over extended video durations. B. Additional Comparisons and Analysis Our expanded comparison includes more baseline methods evaluated with our standard settings, including StreamingT2V [18], FreeNoise [30], VideoTetris [35], and FIFO-VC2 [23], as shown in Fig. 9. StreamingT2V fails on longer videos, FreeNoise/FIFO+VC2 shows limited dynamics (static subjects), and VideoTetris has rich but illogical variations. Figure 9. The qualitative comparison. We recommend readers refer to our webpage for video comparisons. Table 3. Quantitative evaluation of comparison study. Models Video-Inf DiTCtrl ST2V FreeNoise VideoTetris FIFO+VC2 FIFO+CogX Ours TestSet VBench SC 81.80 76.76 67.71 86.50 69.27 89.73 86.22 84.57 85.49 BC 90.56 87.96 85.18 92.10 85.86 93.93 92.89 92.21 91.43 TF 96.66 95.52 93.51 96.94 94.60 96.31 94.78 95.41 95. MS 97.65 97.78 94.40 97.69 97.04 97.75 97.48 98.08 98.33 IQ 61.58 59.26 42.53 67.77 55.95 60.49 64.10 63.31 62.78 DD 31.0 75.0 34.0 24.0 96.0 54.0 78.57 78.95 89.00 SC 91.73 91.67 86.10 96.64 86.86 94.82 93.78 94.20 94.34 VBench-Long MS BC 97.67 95.63 97.88 94.21 94.39 93.69 98.02 96.52 94.73 92.84 97.79 96.43 97.43 95.42 98.40 95.52 98.35 95.03 DD 28.00 53.88 25.42 18.00 97.12 49.08 66.53 68.58 82. Human Study TA 0.31% 5.3% 0.93% 1.24% 0.93% 4.36% 23.36% 63.57% MC 0.93% 4.98% 0.31% 2.18% 1.25% 3.12% 20.87% 66.36% For quantitative evaluation of added baselines, we use our papers setup, including 26-participant human study (Tab. 3: first , second , subpar ). We find that Subject and Background Consistency (SC & BC) and Temporal Flickering (TF) favor less dynamic videos, e.g., FreeNoise/FIFO+VC2 ranks high in these but low in Dynamic Degree (DD). To further support this, we compute these metrics on MiraDatas filtered test set, which features high-quality, continuous motion videos (bottom row). Some methods outperform TestSet on SC, BC, and TF, yet still significantly trail in DD. VideoTetris, with the highest DD, conversely shows lower SC & BC, indicating potentially disordered, abrupt motions. CogVideoX [42] and VBench++ [20] also report these metric limitations, as SC, BC, and TF assess quality based on neighboring frame similarity (DINO, CLIP, Mean Absolute Error), thus favoring static videos with higher inter-frame similarity. Therefore, reliable quality assessment requires considering both dynamic aspects and these consistency metrics, as also noted by VBench++. Recognizing these limitations, we also evaluate on VBench-Long, benchmark for long-term consistency that analyzes keyframe similarity across video segments, overcoming the local metrics limitations. Filtering out methods with subpar DD and SC/BC, our method surpasses FIFO+CogX on all four metrics and all other baselines in human evaluations. The evaluation of long video generation quality is still significant challenge that we will explore further in the future. C. Limitations and Discussions Despite the effectiveness of TokensGen in maintaining long-range consistency, it does not preserve all fine-grained details. Focusing on high-level semantics, tokens may cause gradual variations in foreground or background objects over extended sequences, as shown in Figs. 8 and 10. Our current framework employs tuning-free FIFO strategy to maintain short-term consistency during inference. While effective in many scenarios, FIFO can deliver suboptimal performance for cross-clip temporal consistency in some complex scenes. In such cases, the condensed tokens are also insufficient to capture intricate spatial-temporal cues, leading to performance limitations. We illustrate these failure cases in Fig. 11. Addressing these challenges will require more fine-grained tokenization and stronger short-term consistency strategies beyond tuning-free FIFO. Figure 10. Gradual variations in foreground or background objects over extended sequences. Figure 11. Both the FIFO strategy and the condensed tokens are insufficient to capture intricate spatial-temporal cues, leading to performance limitations. Our framework is trained and tested on limited dataset of gameplay and landscape videos, but is scalable to larger datasets for broader applications. In future work, exploring multi-scale tokenization or hybrid representations could bolster fine-grained controllability, retaining subtle attributes while preserving the scalability and resource efficiency. D. Additional Visual Results For more visual results, comparisons, and ablation studies, please refer to our webpage."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research",
        "Wangxuan Institute of Computer Technology, Peking University"
    ]
}