{
    "paper_title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging",
    "authors": [
        "Md. Ashraful Islam",
        "Mohammed Eunus Ali",
        "Md Rizwan Parvez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/)."
        },
        {
            "title": "Start",
            "content": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging Md. Ashraful Islam*1, Mohammed Eunus Ali1, Md Rizwan Parvez2 1Bangladesh University of Engineering and Technology (BUET) 2Qatar Computing Research Institute (QCRI) {mdashrafulpramanic, mohammed.eunus.ali}@gmail.com, mparvez@hbku.edu.qa"
        },
        {
            "title": "Abstract",
            "content": "a code these generation Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness approaches heavily relies on of the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CODESIM, novel framework multi-agent that comprehensively addresses the stages coding, of program synthesisplanning, and human-like debuggingthrough perception approach. As human verifies their understanding of any algorithms through visual CODESIM uniquely features method of plan verification and internal debugging through the step-by-step Extensive input/output. simulation of experiments challenging seven competitive problem-solving and program synthesis benchmarks demonstrate CODESIMs remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/). simulation, across"
        },
        {
            "title": "Introduction",
            "content": "In recent years, the rise of Large Language Models (LLMs) has made significant advances in AIassisted coding and reshaped the domain of code generation and problem-solving (Zhao et al., 2023). Code generation assistants built on GPT-4 (OpenAI, 2024), Mistral (Jiang et al., 2023a), and Llama *Work done when working as remote RA at QCRI. (Dubey et al., 2024), inter alia, have demonstrated unprecedented ability to understand, generate, and manipulate code across various programming languages and problem domains. However, despite these advancements, significant challenges persist in generating code for complex programming tasks. Current state-of-the-art approaches in code generation typically employ dual-pass process (Shi et al., 2024; Jin et al., 2024b; Zhong et al., 2024; In the first pass, they use Levin et al., 2024). LLMs to generate an initial, fully/partially correct version of the program. Then accordingly in the second pass, they apply external tool-based iterative debuggers that leverage runtime compiler feedback or other diagnostic tools to refine and correct the generated code. While this approach has shown promise, it necessitates numerous iterations of LLM-tool interactions, and importantly its effectiveness is heavily dependent on the quality of the initial code generationa process that continues to present substantial difficulties. Therefore, in this paper, we present CODESIM, novel multiagent code generation framework that seamlessly synthesizes complex code solutions without external resources, while offering potential for further enhancement through minimal external debugging. Synthesizing programs even in the first pass, however, is fundamentally challenging, requiring deep understanding of natural language processing, computer algorithms, data structures, and problemsolving strategies. These challenges are further compounded when attempting to generate code for competitive programming problems or advanced software engineering tasks, where adherence to specific constraints or passing unit tests are paramount (Khan et al., 2023). While earlier code generation methods employed direct approaches (Chen et al., 2021a), chain-of-thought reasoning (Wei et al., 2022a), synthesized test-case guidance (Chen et al., 2022a), retrieval-augmented generation (Parvez et al., 5 2 0 2 ] . [ 1 4 6 6 5 0 . 2 0 5 2 : r Figure 1: Overview of CODESIM: It consists of three agentsplanning, coding, and debugging. The Planning Agent first generates an exemplar problem-solution (i.e., via self-retrieval) and devises plan, which is then verified and refined through simulation. Next, the Coding Agent implements the plan. Finally, the Debugging Agent addresses potential bugs through step-wise simulation across trials. The entire process iterates times. 2021), and various in-context exemplar-based strategies (Shum et al., 2023; Zhang et al., 2022), recent paradigms have shifted toward plan-based (Jiang et al., 2023b), sampling or tree-searching (Zhou et al., 2023), self-retrieval (Yasunaga et al., 2023), and diverse agent-based approaches (Zhang et al., 2024; Qian et al., 2024; Shinn et al., 2023; Huang et al., 2023; Dong et al., 2023b). Most recently, MapCoder (Islam et al., 2024a) proposes multi-agent framework that implements agents emulating different stages of program synthesis such as recalling relevant examples, designing/planning, code generation, and testing/debugging. While this approach mimics real developers code generation cycle and shows improvements, it focuses solely on expanding steps without verifying the underlying hypotheses, with tests being performed only during the debugging phase. Consequently, the resulting gains are limited and it also requires larger number of iterations (i.e., LLM API calls). and planning, To address these limitations, CODESIMbuilt upon debugging coding, agentsintroduces novel verification approach inspired by human problem-solving. By simulating input/output step-by-step, CODESIM verifies both the generated plans and performs internal debugging, mirroring how humans understand, visualize, and refine algorithms. This simulationdriven planning and debugging process ensures that each step is thoroughly evaluated, significantly enhancing both solution quality and efficiency. Figure 1 shows an overview of our proposed approach, CODESIM and in Figure 2, we demonstrate how simulation assists in both plan verification and debugging, highlighting its crucial role in improving problem-solving accuracy. We evaluate CODESIM on seven popular programming synthesis benchmarks, including foundational tasks like HumanEval and MBPP, as well as challenging competitive problem-solving benchmarks such as APPS, and CodeContest. Our experiments leverage multiple LLMs, including ChatGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mixtral, showcasing significant improvements in their program synthesis capabilities. CODESIM consistently achieves state-of-the-art performances, often surpassing strong baselines like MapCoder. Additionally, our findings suggest that CODESIMs performance can be further improved when integrated with external debugging tools, such as LDB (Zhong et al., 2024), highlighting promising direction for future research in hybrid code generation and debugging systems. Through detailed ablation studies, we provide valuable insights into CODESIMs functionality. We will open-source our framework to support future research in AI-assisted programming and problem-solving."
        },
        {
            "title": "2 Related Work",
            "content": "Code Generation: Program synthesis has been fundamental challenge in AI for decades (Manna and Waldinger, 1971). Early attempts with smaller language models centered on code generation by fine-tuning neural networks (Wang et al., 2021; Ahmad et al., 2021; Feng et al., 2020; Parvez et al., 2018; Hellendoorn and Devanbu, 2017; Rabinovich et al., 2017; Yin and Neubig, 2017; Hindle et al., Figure 2: Example of Plan Validation using Simulation (left) and Debugging using Simulation (right) on two different problems using CODESIM. 2016), while others explored leveraging data flow information or conversational intents to guide the process (Andreas et al., 2020; Yu et al., 2019). Various prior approaches have also addressed code generation tasks using techniques such as data flow analysis and search-based methods (Li et al., 2022a; Parisotto and Salakhutdinov, 2017; Polozov and Gulwani, 2015; Gulwani, 2011). LLMs for Code: Various LLMs have been developed for code synthesis (Austin et al., 2021; Chen et al., 2021b; Nijkamp et al., 2022; Fried et al., 2022; Allal et al., 2023; Li et al., 2022c). Recent open-source LLMs include the Llama family (Llama-2, CodeLlama, Llama3.1, etc.) (Roziere et al., 2023; Touvron et al., 2023), the Mistral family (Mistral, Mixtral, Codestral) (Jiang et al., 2023a), the Deepseek family (Deepseek Coder, Deepseek-V2, etc.) (Guo et al., 2024), MoTCoder (Li et al., 2023), and the Qwen family (Qwen 1.5, 2.5, 2.5-coder, etc.) (Hui et al., 2024), all of which are capable of solving many basic problems. Prompting LLMs and Multi-Agent Code Generation: LLM prompting can be summarized into three categories: retrieval (Yasunaga et al., 2023; Parvez et al., 2021, 2023), planning (Jiang et al., 2023b; Wei et al., 2022b), and debugging (Le et al., 2022; Chen et al., 2022b, 2023; Ridnik et al., 2024), in addition to direct code generation approaches. In contrast, our work combines all these paradigms and bridges their gaps (See Table 1). Recently, nusimulates (step-by-step) the solution with sample input. If the simulation result does not match the expected output, the agent prompts the LLM to revise the plan. Otherwise, the plan is deemed valid. In the case of failure, the Planning Agent refines the plan. The complete prompts for the Planning Agentincluding plan generation, verification, and refinementare provided in the Appendix (Figure 5, 6, 7). 3.2 Coding Agent Next component is the Coding Agent, which takes the problem description and the plan generated by the Planning Agent as input. The role of this agent is to translate the plan into executable code that solves the given problem. Once the code is generated, CODESIM evaluates it using sample input/output test cases. If the code passes all sample tests, it is returned as the final solution. Otherwise, the code is handed over to the next agent for further refinement. Figure 8 in the Appendix provides the complete prompt used by the Coding Agent."
        },
        {
            "title": "3.3 Debugging Agent",
            "content": "The final component, the Debugging Agent, receives the original problem, the plan from the Planning Agent, the code generated by the Coding Agent, and the execution (unit testing) log as input to debug the code. To identify bugs, instead of directly prompting the LLMs, we uniquely leverage the simulation once again. The LLM is instructed specifically to simulate the code on inputs where it fails to produce the expected output, allowing it to trace the execution step by step and locate the error. Once the bug is identified, the LLM modifies the code to resolve the issue. The complete prompt for the Debugging Agent is shown in the Appendix (Figure 9). Unlike other approaches such as LATS (Zhou et al., 2023), AgentCoder (Huang et al., 2023), and Reflexion (Shinn et al., 2023), our Debugging Agent does not require additional test case generation. The rationale behind excluding this phase is discussed in the Ablation Study 6.8."
        },
        {
            "title": "3.4 Adaptive Iteration",
            "content": "CODESIM employs an adaptive iteration starting with the Planning Agent, which generates plans for the given problem. These plans are passed to the Coding Agent, which translates them into code and tests against sample I/Os. If all tests pass, the code is returned; otherwise, its sent to the Debugging Agent. The Debugging Agent attempts to fix the Table 1: Comparison of code generation approaches. merous works have explored multi-agent code generation and problem-solving, including (Kulesza et al., 2004; Jin et al., 2024a; Phan et al., 2024), as well as approaches highlighted in Section 1. However, CODESIM uniquely features simulationdriven planning and LLM-based debugging. More recently, external debugging has emerged to further boost performance, such as LDB (Zhong et al., 2024), ChatDebug (Levin et al., 2024), and MGDebugger (Shi et al., 2024), which serve as second pass after our generation."
        },
        {
            "title": "3 CODESIM",
            "content": "Our goal is to develop multi-agent code generation approach capable of complex problem solving. Drawing inspiration from recent works like MapCoder and ChatDev (in different context), we devise the agents in CODESIM for planning, coding, and debugging. While these existing approaches focus primarily on expanding steps without verifying underlying hypotheses, we address this limitation by introducing novel verification approach. Our approach simulates input/output step-by-step, verifying generated plans and performing internal debugging, mirroring how humans understand, visualize, and refine in algorithm development. Below, we present our proposed model."
        },
        {
            "title": "3.1 Planning Agent",
            "content": "The first component of CODESIM is the Planning Agent. Given problem description, the Planning Agent generates single exemplara relevant problem along with its plan and solution. This mimics the behavior of human programmers, who, when faced with new problem, first recall similar problem theyve previously solved. This exemplarbased recall is crucial as it provides starting point for constructing solution plan. Instead of generating multiple ungrounded exemplars as in MapCoder, our agent focuses on only one at time. We then instruct the LLM to generate an appropriate plan. Once the plan is created, the LLM Table 2: Pass@1 results for different approaches on basic programming tasks. code for up to attempts. If unsuccessful after attempts, the process returns to the Planning Agent, restarting the cycle. Once code passing all sample I/Os is obtained, the cycle ends, returning the code as the final output solution for evaluation against hidden test cases. This entire process repeats for maximum of cycles if needed. Algorithm 9 in the Appendix summarizes our adaptive agent traversal. The algorithms complexity is O(pd). Appendix 12 provides comprehensive example of how CODESIM solves problem."
        },
        {
            "title": "4.1 Datasets",
            "content": "Following MapCoder, we evaluate CODESIM on five basic programming benchmarks i.e., HumanEval (Chen et al., 2021a), HumanEvalET (Dong et al., 2023a), EvalPlus (Liu et al., 2023), MBPP) (Austin et al., 2021), and MBPPET (Dong et al., 2023a) and two competitive programming datasets i.e., APPS (Hendrycks et al., 2021), and CodeContest (Li et al., 2022b). For fair comparison, we collect all the datasets from the repository of the MapCoder."
        },
        {
            "title": "4.2 Baselines and Metric",
            "content": "To evaluate CODESIM, we compare it against state-of-the-art code generation approaches, including MapCoder, as well as several notable methods: Direct, Chain of Thought (CoT) (Wei et al., 2022b), Self-Planning (Jiang et al., 2023b), Analogical Reasoning (Yasunaga et al., 2023), and Self-collaboration (Dong et al., 2023b). For simpler programming tasks, we include strong baselines such as Reflexion (Shinn et al., 2023) and LATS (Zhou et al., 2023). We exclude AgentCoder (Huang et al., 2023) due to reproducibility issues (discussed in Appendix 10). For fair comparison, our evaluation utilizes ChatGPT (gpt-3.5-turbo1106), GPT-4 (gpt-4-1106-preview) from OpenAI, alongside open-source LLMs such as Gemma29B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.170B. For basic programming tasks, we report nextgeneration performance with additional evaluations using GPT-4o (gpt-4o-2024-08-06). We adopt the widely used pass@1 metric, where model is deemed successful if its sole predicted solution is correct. 4.3 Reproducibility We aim to contribute to the NLP community by open-sourcing all of our code along with result logs, enabling others to reproduce our findings. For simple programming, we set the maximum number of planning tries to = 5 and debugging tries to = 5. For the competitive problem solving, we used = 3 and = 3 by default except for the CodeContest with GPT-4 where = 3, = 5."
        },
        {
            "title": "5 Results",
            "content": "5.1 Basic Code Generation In Table 2, we evaluate the model performances on simple code generation tasks. Overall, CODESIM demonstrates consistently superior performance compared to all other baselines across all datasets and LLMs. Notably, CODESIM achieves top scores with GPT-4o, reaching 95.1% on HumanEval, 87.2% on EvalPlus, and 90.7% on MBPP, resulting in an impressive 82.7% overall average and their new state-of-the-art (SoTA) results. This represents significant improvement over the next best method, MapCoder, which scores 79.0% on average with GPT-4o. CODESIMs effectiveness is consistent across different model variants, outperforming other approaches with ChatGPT (75.1% avg) and GPT-4 (81.3% avg) as well. The methods robust performance across diverse datasets, including the challenging MBPPET where it achieves 61.5% with GPT-4, underscores its versatility in handling various programming tasks. These results strongly indicate that CODESIMs simulation-driven planning and debugging approach marks substantial advancement in code generation and problem-solving capabilities, as it consistently outperformed other baselines."
        },
        {
            "title": "5.2 Competitive Problem Solving",
            "content": "In Table 3, we evaluate performance on complex, contest-level code generation tasks. CODESIM delivers significant improvements over other baselines in solving complex contest-level code generation tasks. With GPT-4, CODESIM reaches strong 29.1% on CodeContests and 22.0% on APPS, marking consistent edge over MapCoders 25.3% average. The performance gains are even more pronounced with ChatGPT, where CODESIM achieves 16.4% on CodeContests, and 12.0% on APPS resulting 14.2% overall, outperforming MapCoders 12.0%. These results highlight CODESIMs ability to handle the complexity of contest-level problems more effectively, especially through its simulationdriven approach. Table 3: Pass@1 results for different approaches on CodeContest and APPS dataset. Table 4: Pass@1 results for different approaches using Open-source LLMs."
        },
        {
            "title": "5.3 Performance Across Open-source LLMs",
            "content": "To further demonstrate CODESIMs generalization capability, we evaluate its performance with open-source LLMs, including Gemma2-9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As shown in Table 4, CODESIM consistently outperforms all other methods across these models. On LLaMa3.1-70B, CODESIM achieves an accuracy of 90.2% on HumanEval and 76.2% on EvalPlus, with an average of 80.1%, closely matching GPT4os performance. Due to the complex prompting scheme of MapCoder, open-source LLMs often struggle to generate output in the correct format. Therefore, we exclude MapCoder from this experiment. On the other hand, Reflexion shows minimal improvement in accuracy. These results highlight CODESIMs strong generalization ability across various LLM architectures, even on smaller models like Gemma2-9B that achieves notable avg accuracy of 75.8%."
        },
        {
            "title": "6 Ablation Studies and Analyses",
            "content": "6.1 Impact of Different Agents Our primary contributions are two folds: (i) the simulation-guided plan verification step within the Planning Agent and (ii) the bug fixing process through simulation in Debugging Agent. To evaluate the significance of these components, we ablate these two parts of our approach and present the results in Table 5. The findings confirm that both components contribute significantly. Table 6: Impact of using Simulation. xCodeEval (Khan et al., 2023) dataset. The experimental results, presented in Table 7, demonstrate that CODESIM maintains strong performance across different programming languages, highlighting its versatility and effectiveness. Table 7: Pass@1 results for different programming languages from xCodeEval dataset by using ChatGPT."
        },
        {
            "title": "6.4 Use of External Debugger",
            "content": "Table 5: Pass@1 results for different versions of CODESIM (by using GPT4o on HumanEval dataset)."
        },
        {
            "title": "Simulation",
            "content": "Table 6 presents the impact of incorporating Simulation in CODESIM. The results show that CODESIM consistently outperforms other approaches across both simple and multi-agent settings, demonstrating superior performance with both open-source and proprietary LLMs. This highlights the effectiveness of Simulation in enhancing problem-solving efficiency within our pipeline. 6."
        },
        {
            "title": "Impact of Varying Programming\nLanguages",
            "content": "To evaluate the performance of CODESIM across various programming languages, we utilized the Table 8: Pass@1 results for different approaches using an external debugger. The performance of CODESIM can be further enhanced by incorporating an external debugger in the second pass. We experiment with LDB as the external debugger on HumanEval dataset in Table 8. We use the output code from the most competitive first-pass generation methods, including CODESIM, Reflexion, and MapCoder, using GPT-4o as the backbone. These seed programs are then passed to LDB, which was tested with two different LLMs: ChatGPT and GPT-4o. As can be seen, CODESIM achieves 95.1% accuracy in the first pass with GPT-4o, surpassing Reflexions second pass performance of 94.5%. By utilizing LDB with GPT-4o, CODESIM achieves second pass accuracy of 97.6%, setting new state-of-theart result for dual-pass approach. In addition, we note that the second pass with LDB consumes 39K more tokens in Reflexion compared to our approach, highlighting the efficiency of CODESIM. 6.5 Qualitative Example We also conduct qualitative analysis to better understand how CODESIM improves performance across various datasets. Figure 2 demonstrates how CODESIM enhances the plan through simulation and assists in debugging the code using the same technique. complete example, including LLM output, is provided in Appendix 12. 6.6 Impact of and CODESIM includes two key hyperparameters: the maximum number of planning steps (p) and the maximum number of debugging steps (d). By varying these parameters, we plot the results in Figure 3, which shows proportionate improvement in performance. It is important to note that higher values of and lead to more API calls and increased token consumption, allowing users to adjust these parameters to balance between accuracy and cost. Figure 3: Pass@1 results by varying maximum number of planning, and maximum number of debugging, d. 6.7 Impact of Number of Sample I/Os The HumanEval dataset has an average of only 2.82 sample I/Os per example, which is relatively small number for deriving meaningful insights. In this ablation, we augment the dataset by adding 5 more sample I/Os from the HumanEval-ET dataset. This augmentation increases performance notably, leading to 89% accuracy with ChatGPT, 3.5% improvement over previous results, 86%. 6.8 Impact of Synthesizing Additional I/O Increasing the number of sample I/Os for testing can enhance the overall performance of our approach, as indicated in 6.7. Based on this insight, we use self-consistency (Wang et al., 2023a) method to generate additional test cases. We instruct the LLM to generate five more test cases for each problem, covering both basic and edge cases. The LLM is called twice, and we select the test cases that are present in both responses. However, this approach results in performance decline. With ChatGPT we achieve 78% accuracya 9.3% decrease from the original 86%. This indicates that generating additional I/Os is non-trivial task that may negatively impact final outcomes."
        },
        {
            "title": "6.9 API Call and Token Analysis",
            "content": "We compare the API calls and token consumption of our approach with the previous state-of-the-art method, MapCoder (Islam et al., 2024a), as shown in Table 9. The results reveal that CODESIM not only improves performance but also reduces token consumption. On average, CODESIM uses 4.13 thousand fewer tokens while achieving 7.1% increase in accuracy, proving that CODESIM is more efficient in both accuracy and token usage compared to MapCoder."
        },
        {
            "title": "6.10 Error Analysis and Challenges",
            "content": "Although CODESIM demonstrates strong performance compared to other methods, it faces challenges in specific algorithmic domains. The APPS dataset (Hendrycks et al., 2021) includes problems with three levels of difficulty: (i) Introductory, (ii) Interview, and (iii) Competition. Figure 4 illustrates the performance of different approaches based on difficulty level. The results indicate that for introductory and interview-level problems, CODESIM does not surpass MapCoder when using ChatGPT. Additionally, when using GPT-4, CODESIM struggles to outperform MapCoder on interview-level problems. Upon manual review, we observe that for more complex issues, such as dynamic programming (DP), CODESIM encounters difficulties in constructing the DP table. Table 9: Comparison between MapCoder and CODESIM in terms of average number of API calls, average tokens used (in thousands). Here the upward symbol () refers that the higher value is better and opposite meaning for downward symbol (). mance without relying on any external tools. Although we have reduced token consumption compared to the previous state-of-the-art method, MapCoder, it still remains high compared to the direct prompting approach. Direct prompting consumes an average of 560 tokens, while our method consumes around 13,640 tokens. This indicates room for enhancement in efficiency. While in this work, we generate the exemplars with the LLMs themselves, in general they are found from external resource (Parvez and Chang, 2021). Although this has its own challenges such as noisy retrievals (Wang et al., 2023b), inconsistent generations (Islam et al., 2024b; Parvez, 2024; Sadat et al., 2023) this direction could also be possible improvement. Another limitation is the use of external tools for assistance during simulation. We have not explored this avenue in the current research, leaving it for future work. Additionally, more sample I/Os could potentially improve performance, and our future research will focus on investigating methods for generating accurate additional I/Os. Moreover, we would like to note that in this work, we focus solely on generated codes correctness and did not study its optimizations such as test-time, memory. Finally, it is advisable to run the machine generated code inside sandbox to avoid any potential risks."
        },
        {
            "title": "References",
            "content": "Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program understanding and generation. arXiv preprint arXiv:2103.06333. Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Figure 4: Performance of different approaches across different difficulty levels on the APPS dataset."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "that generation In this paper, we introduce CODESIM, novel framework the multi-agent leverages prompting capabilities of LLMs for efficient tasks. in code problem-solving CODESIM integrates three agentsplanning, coding, and debuggingto effectively solve programming problems. It harnesses the power of simulation for plan verification and debugging, significantly outperforming existing state-of-the-art approaches by wide margin. Future work will focus on extending this approach to other domains such as mathematical reasoning and question answering broadening its scope and impact."
        },
        {
            "title": "8 Limitations",
            "content": "In Section 6.4, we observe that utilizing an external debugger can further enhance our results. Our next research goal is to achieve the best perforFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. Santacoder: dont reach for the stars! arXiv preprint arXiv:2301.03988. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. 2020. Task-oriented dialogue as dataflow synthesis. Transactions of the Association for Computational Linguistics, 8:556571. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022a. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022b. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large lanarXiv preprint guage models trained on code. arXiv:2107.03374. Yihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li, Ge Li, and Zhi Jin. 2023a. Codescore: Evaluating code generation by learning code execution. arXiv preprint arXiv:2301.09043. Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b. Self-collaboration code generation via chatgpt. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and et al. 2024. The llama 3 herd of models. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 15361547. Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Incoder: generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999. Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. ACM Sigplan Notices, 46(1):317330. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196. Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks the best choice for modeling source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2017, pages 763773, New York, NY, USA. ACM. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938. Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. 2016. On the naturalness of software. Commun. ACM, 59(5):122131. Dong Huang, Qingwen Bu, Jie Zhang, Michael Luck, and Heming Cui. 2023. Agentcoder: Multi-agentbased code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Md. Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. 2024a. MapCoder: Multi-agent code generation for competitive problem solving. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 49124944, Bangkok, Thailand. Association for Computational Linguistics. Shayekh Bin Islam, Md Asib Rahman, Tozammel Hossain, Enamul Hoque, Shafiq Joty, and Md Rizwan Parvez. 2024b. Open-RAG: Enhanced retrieval augmented reasoning with open-source large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 14231 14244, Miami, Florida, USA. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023a. Mistral 7b. Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, Self-planning code generaarXiv preprint and Ge Li. 2023b. tion with large language model. arXiv:2303.06689. Dongming Jin, Zhi Jin, Xiaohong Chen, and Chunhui Wang. 2024a. Mare: Multi-agents collaboration framework for requirements engineering. arXiv preprint arXiv:2405.03256. Haolin Jin, Zechao Sun, Yiheng Yang, and Huaming Chen. 2024b. Rgd: Multi-llm based agent debugger via refinement and generation guidance. arXiv preprint arXiv:2410.01242. Mohammad Abdullah Matin Khan, Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. 2023. xcodeeval: large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. arXiv preprint arXiv:2303.03004. Uirá Kulesza, Alessandro Garcia, Carlos Lucena, and Paulo Alencar. 2004. generative approach for multi-agent system development. In International Workshop on Software Engineering for Large-Scale Multi-agent Systems, pages 5269. Springer. Md Tahmid Rahman Laskar, Sawsan Alqahtani, Saiful Bari, Mizanur Rahman, Mohammad Abdullah Matin Khan, Haidar Khan, Israt Jahan, Amran Bhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty, and Jimmy Huang. 2024. systematic survey and critical review on evaluating large language models: Challenges, limitations, and recommendations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1378513816, Miami, Florida, USA. Association for Computational Linguistics. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328. Kyla Levin, Nicolas van Kempen, Emery Berger, Chatdbg: An arXiv preprint and Stephen Freund. 2024. ai-powered debugging assistant. arXiv:2403.16354. Jingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Motcoder: Elevating large language models with modular of thought for challenging programming tasks. arXiv preprint arXiv:2312.15960. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022a. Competition-level code generation with alphacode. Science, 378(6624):10921097. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, PoSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022b. Competition-level code generation with alphacode. Science, 378(6624):10921097. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, PoSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022c. Competition-level code generation with alphacode. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems. Zohar Manna and Richard J. Waldinger. 1971. Toward automatic program synthesis. Commun. ACM, 14(3):151165. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474. OpenAI. 2024. Gpt-4 technical report. Emilio Parisotto and Ruslan Salakhutdinov. 2017. Neural map: Structured memory for deep reinforcement learning. arXiv preprint arXiv:1702.08360. Md Rizwan Parvez. 2024. Evidence to generate (e2g): single-agent two-step prompting for context grounded and retrieval augmented reasoning. arXiv preprint arXiv:2401.05787. Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601. Md Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2018. Building language models for text with named entities. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23732383, Melbourne, Australia. Association for Computational Linguistics. Md Rizwan Parvez and Kai-Wei Chang. 2021. Evaluating the values of sources in transfer learning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 50845116, Online. Association for Computational Linguistics. Md Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, and Kai-Wei Chang. 2023. Retrieval enhanced data augmentation for question answering on privacy policies. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 201210, Dubrovnik, Croatia. Association for Computational Linguistics. Huy Nhat Phan, Phong Nguyen, and Nghi DQ Bui. 2024. Hyperagent: Generalist software engineering agents to solve coding tasks at scale. arXiv preprint arXiv:2409.16299. Oleksandr Polozov and Sumit Gulwani. 2015. Flashmeta: framework for inductive program synthesis. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, pages 107 126. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ChatDev: Communicative In Proceedings agents for software development. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1517415186, Bangkok, Thailand. Association for Computational Linguistics. Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract syntax networks for code generation and semantic parsing. CoRR, abs/1704.07535. Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code generation with alphacodium: From prompt engineering to flow engineering. arXiv preprint arXiv:2401.08500. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh Menon, Md Parvez, and Zhe Feng. 2023. DelucionQA: Detecting hallucinations in domain-specific question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 822835, Singapore. Association for Computational Linguistics. Yuling Shi, Songsong Wang, Chengcheng Wan, and Xiaodong Gu. 2024. From code to correctness: Closing the last mile of code generation with hierarchical debugging. arXiv preprint arXiv:2410.01215. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems. Kashun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic prompt augmentation and selection with In Findings chain-of-thought from labeled data. of the Association for Computational Linguistics: EMNLP 2023, pages 1211312139, Singapore. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models. Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Identifier-aware unified pre-trained encoder-decoder models for code unarXiv preprint derstanding and generation. arXiv:2109.00859. Codet5: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023b. Learning to filter context for retrieval-augmented generation. arXiv preprint arXiv:2311.08377. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022a. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed Chi, and Denny Zhou. 2023. Large language models as analogical reasoners. arXiv preprint arXiv:2310.01714. Pengcheng Yin and Graham Neubig. 2017. syntactic neural model for general-purpose code generation. CoRR, abs/1704.01696. Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. 2019. CoSQL: conversational text-to-SQL challenge towards crossdomain natural language interfaces to databases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1962 1979, Hong Kong, China. Association for Computational Linguistics. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024. CodeAgent: Enhancing code generation with tool-integrated agent systems for real-world repolevel coding challenges. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13643 13658, Bangkok, Thailand. Association for Computational Linguistics. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought promptarXiv preprint ing in large language models. arXiv:2210.03493. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Li Zhong, Zilong Wang, and Jingbo Shang. 2024. Debug like human: large language model debugger via verifying runtime execution step by step. In Findings of the Association for Computational Linguistics: ACL 2024, pages 851870, Bangkok, Thailand. Association for Computational Linguistics. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406. Coder (1.7 million tokens with ChatGPT and 2.1 million with GPT-4) and CODESIM(0.89 million tokens in ChatGPT and 0.85 million in GPT-4). These two experiments resulted in cost of approximately $500 USD, but we were still unable to come close to AgentCoders reported claims of 79.9% accuracy with ChatGPT and 96.3% with GPT-4. Furthermore, we found unaddressed issues on their GitHub page (link) related to reproducibility. Additionally, for the MBPP dataset, they used all test cases as public test cases (link), which deviates from standard practices. As result, we did not consider those results in our comparison either."
        },
        {
            "title": "11 Details Promptings of CODESIM",
            "content": "The Planning Agent interacts with the LLM three times to generate plan. In the first API call, it instructs the LLM to comprehend the problem, generate an example problem, recommend suitable algorithm, and finally produce the plan (Figure 5). In the second API call, the LLM is instructed to verify the plan through simulation (Figure 6). If the plan is satisfactory, it is returned by the agent. Otherwise, the LLM is called again to refine the plan based on the feedback from the simulation (Figure 7). The next step involves the Coding Agent, which receives the plan from the Planning Agent and uses the prompt outlined in Figure 8 to generate code. If the code fails to pass the sample input/output, CODESIM activates its final agent, the Debugging Agent, using the prompt shown in Figure 9."
        },
        {
            "title": "These figures also include the rationale behind",
            "content": "the inclusion of each sentence in the prompt."
        },
        {
            "title": "12 Example Problem",
            "content": "We present complete example of problem solving using CODESIM below:"
        },
        {
            "title": "9 Algorithm of CODESIM",
            "content": "Algorithm 1 shows the pseudo-code of our prompting technique. plan RefinePlan(problem, plan, eedback) else # Start of Planning Agent plan GeneratePlan(problem) eedback ValidatePlan(problem, plan) if eedback is negative then end if # End of Planning Agent # Start of Coding Agent code GenerateCode(problem, plan) passed, log test(code, sample_io) if passed then Return code Algorithm 1 CODESIM 1: maximum number of planning steps 2: maximum number of debugging steps 3: 4: for 1 to do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: end for 38: Return code code DebugCode( problem, plan, code, log # Start of Debugging Agent for 1 to do end for # End of Debugging Agent end if # End of Coding Agent passed, log test(code, sample_io) if passed then Return code end if )"
        },
        {
            "title": "10 Exclusion of AgentCoder",
            "content": "We have not included AgentCoder (Huang et al., 2023) in our comparison due to reproducibility issues which undoubtedly plays critical role in fair comparison as indicted in Laskar et al. (2024), as we were unable to replicate their results. In our attempts to reproduce their work on the HumanEval benchmark using ChatGPT, we achieved 56.7% accuracy after four iterations, consuming 11.9 million tokens. When using GPT-4, we attained only 17.7% accuracy after two iterations, with 10.4 million tokens consumed. The token consumption in both cases is significantly higher compared to MapFigure 5: Planning Agent: Prompt for Plan Generation. Figure 6: Planning Agent: Prompt for Plan Verification with the help of Simulation. Figure 7: Planning Agent: Prompt for Plan Refinement. Figure 8: Coding Agent: Prompt for Code Generation. An Example from HumanEval dataset for demonstrating how CODESIM works Input for Planning: 1 You are programmer tasked with generating appropriate plan to solve given problem using the Python3 programming language. ## Problem def generate_integers(a, b): \"\"\" Given two positive integers and b, return the even digits between and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] \"\"\" Expected Output: Your response must be structured as follows: Figure 9: Debugging Agent: Prompt for Debugging. ### Problem Understanding - Think about lem. the original problem. Develop an initial understanding about the prob- ### Recall Example Problem Recall relevant and distinct problems (different from problem mentioned above) and - Describe it - Generate Python3 code step by step to solve that problem - Discuss the algorithm to solve this problem - Finally generate planning to solve that problem ### Algorithm to solve the original problem - Write down the algorithm that is well suited for the original problem - Give some tutorials to about the algorithm for example: - How to approach this type of algorithm - Important things to consider ### Plan - Write down detailed, step-by-step plan to solve the original problem. Important Instruction: - Strictly follow the instructions. - Do not generate code. Response from Planning: 1 ### Problem Understanding The problem requires generating list of even integers between two given positive integers, and b, inclusive. The list should be in ascending order, regardless of whether is less than or greater than b. If there are no even integers in the range, the function should return an empty list. ### Recall Example Problem Example Problem: Find all odd numbers between two given integers and y, inclusive, and return them in descending order. Step-by-step Solution: 1. Determine the range between and y. 2. Identify all odd numbers within this range. 3. Sort the odd numbers in descending order. 4. Return the sorted list. Algorithm: 1. Check if is greater than y. If so, swap them to ensure the range is correct. 2. Iterate through the range from to y. 3. For each number, check if it is odd (i.e., number%2! = 0). 4. Collect all odd numbers in list. 5. Sort the list in descending order. 6. Return the sorted list. Planning: - Define function ind_odd_numbers(x, y). - Use conditional to swap and if > y. - Use loop to iterate through the range from to y. - Check each number for oddness and add it to list if it is odd. - Sort the list in descending order. - Return the list. ### Algorithm to Solve the Original Problem 1. Determine the Range: - Identify the start and end of the range by taking the minimum and maximum of and b. 2. Iterate Through the Range: - Loop through each number in the range from the minimum to the maximum of and b. 3. Check for Even Numbers: - For each number, check if it is even by using the condition number%2 == 0. 4. Collect Even Numbers: - Add each even number to list. 5. Return the List: - Since the numbers are collected in ascending order, return the list directly. Tutorials: - Approach: - Always ensure the range is correctly defined, especially when the order of inputs can vary. - Use modulus operation to check for evenness or oddness. - Important Considerations: - Handle edge cases where there might be no even numbers in the range. - Ensure the function is robust to handle any positive integer inputs. ### Plan 1. Define the Function: - Create function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range. 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use or loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use if number%2 == 0 to check if number is even. 6. Collect Even Numbers: - Append even numbers to the list. 7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, the function will correctly generate list of even integers between the given numbers and b. Input for Simulation: 1 You are programmer tasked with verifying plan to solve given problem using the Python3 programming language. ## Problem: def generate_integers(a, b): \"\"\" Given two positive integers and b, return the even digits between and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] \"\"\" ### Plan 1. Define the Function: - Create function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range.v 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use or loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use if number%2 == 0 to check if number is even. 6. Collect Even Numbers: - Append even numbers to the list. 7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, the function will correctly generate list of even integers between the given numbers and b. Expected Output: Your response must be structured as follows: ### Simulation - Take sample input and apply plan step by step to get the output. - Compare the generated output with the sample output to verify if your plan works as expected. ### Plan Evaluation - If the simulation is successful write No Need to Modify Plan. - Otherwise write Plan Modification Needed. Response from Simulation: 1 ### Simulation Lets apply the plan step by step using the sample input generate_integers(2, 8). 1. Define the Function: - We define the function generate_integers(a, b). 2. Determine the Range: - Calculate start = min(2, 8) = 2 and end = max(2, 8) = 8. 3. Initialize an Empty List: - Create an empty list evennumbers = []. 4. Loop Through the Range: - Iterate over the range from start to end (inclusive). 5. Check for Evenness: - For each number in the range, check if it is even using if number%2 == 0. 6. Collect Even Numbers: - Append even numbers to the list evennumbers. 7. Return the Result: - Return the list evennumbers. Applying these steps: - Range: 2 to 8 - Iteration: - 2 is even, append to list: [2] - 3 is not even - 4 is even, append to list: [2, 4] - 5 is not even - 6 is even, append to list: [2, 4, 6] - 7 is not even - 8 is even, append to list: [2, 4, 6, 8] The output is [2, 4, 6, 8], which matches the expected output. ### Plan Evaluation No Need to Modify Plan The plan works as expected for the given problem. The function correctly generates list of even integers between the given numbers and b, regardless of their order. Input for final code generation: You are programmer tasked with solving given problem using the Python3 programming language. See the plan to solve the plan and implement code to solve it. ## Problem: def generate_integers(a, b): \"\"\" Given two positive integers and b, return the even digits between and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] \"\"\" ### Plan 1. Define the Function: - Create function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range. 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use or loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use if number%2 == 0 to check if number is even. 6. Collect Even Numbers: - Append even numbers to the list. 7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, tween the given numbers and b. the function will correctly generate list of even integers beImportant Instructions: - Do not add any explanation. - The generated Python3 code must be inside triple backtick ($$$) code block. Response from final code generation: def generate_integers(a, b): start = min(a, b) end = max(a, b) even_numbers = [] for number in range(start, end + 1): if number % 2 == 0: even_numbers.append(number) return even_numbers Input for Improving code: 1, You are programmer who has received solution of problem written in Python3 that fails to pass certain test cases. Your task is to modify the code in such way so that it can pass all the test cases. Do not generate same code. ## Problem: def generate_integers(a, b): \"\"\" Given two positive integers and b, return the even digits between and b, in ascending order. For example: generate_integers(2, 8) => [2, 4, 6, 8] generate_integers(8, 2) => [2, 4, 6, 8] generate_integers(10, 14) => [] \"\"\" ### Plan 1. Define the Function: - Create function generate_integers(a, b). 2. Determine the Range: - Use min(a, b) and max(a, b) to define the start and end of the range. 3. Initialize an Empty List: - Prepare an empty list to store even numbers. 4. Loop Through the Range: - Use or loop to iterate from the start to the end of the range. 5. Check for Evenness: - Within the loop, use if number%2 == 0 to check if number is even. 6. Collect Even Numbers: - Append even numbers to the list. 7. Return the Result: - Return the list of even numbers, which will be in ascending order. By following this plan, the function will correctly generate list of even integers between the given numbers and b. ### Buggy Code def generate_integers(a, b): start = min(a, b) end = max(a, b) even_numbers = [] for number in range(start, end + 1): if number % 2 == 0: even_numbers.append(number) return even_numbers ### Test Report Test Cases where the generated code failed to generate the expected output: assertgenerateintegers(10, 14) == [] Expected Output: Your response must be structured as follows: ### Simulation with failed test case To detect where is the bug: - Take sample test case where it fails. - Take the input go through each step according to the plan - You will get output that must be different from the expected output. ### Debugging Notes Based on this simulation detect any of the following cases: - Plan is wrong - Plan is correct but plan to code generation is wrong. - Finally, discuss how to correct this code. ### Modified Code Python3 # Your corrected code, with comments explaining each correction. Important Instructions: - Strictly follow the instructions. - Do not add testing code for example assert statement in your code. - Do not be overconfident that the generated code is correct. It is wrong. - The modified Python3 code must be enclosed within triple backticks (). - Your response must contain Simulation with failed test case, Debugging Notes, and Modified Code section. Response from Improving code: 1, 1 ### Simulation with failed test case Lets take the test case generate_integers(10, 14):v 1. Determine the Range: - start = min(10, 14) = 10 - end = max(10, 14) = 14 2. Loop Through the Range: - The loop iterates over 10, 11, 12, 13, 14. 3. Check for Evenness: - 10%2 == 0 (even, added to list) - 11%2! = 0 (odd, not added) - 12%2 == 0 (even, added to list) - 13%2! = 0 (odd, not added) - 14%2 == 0 (even, added to list) 4. Collect Even Numbers: - The list becomes [10, 12, 14]. 5. Return the Result: - The function returns [10, 12, 14]. Expected Output: [] ### Debugging Notes - Plan is correct: The plan correctly identifies the need to find even numbers between and b. - Plan to code generation is wrong: The code does not account for the requirement that the even digits themselves must be between and b. It incorrectly includes numbers like 10, 12, 14 when the digits themselves (1, 2, 4) are not between 10 and 14. ### Modified Code def generate_integers(a, b): start = min(a, b) end = max(a, b) even_numbers = [] for number in range(start, end + 1): # Check each digit in the number for digit in str(number): if int(digit) % 2 == 0 and start <= int(digit) <= end: even_numbers.append(int(digit)) # Remove duplicates and sort the result return sorted(set(even_numbers)) Explanation of Corrections: - Digit Check: The code now checks each digit of the numbers in the range to see if it is even and within the specified range. - Set for Uniqueness: set is used to ensure that each even digit is only included once. - Sorting: The final list is sorted to ensure ascending order."
        }
    ],
    "affiliations": [
        "Bangladesh University of Engineering and Technology (BUET)",
        "Qatar Computing Research Institute (QCRI)"
    ]
}