{
    "paper_title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
    "authors": [
        "Xiaoxing Hu",
        "Kaicheng Yang",
        "Ziyong Feng",
        "Qi Ming",
        "Zonghao Guo",
        "Xiang An",
        "Ziyong Feng",
        "Junchi Yan",
        "Xue Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP's text encoder into the LLM-based embedder to leverage CLIP's rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP"
        },
        {
            "title": "Start",
            "content": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. PROCLIP: PROGRESSIVE VISION-LANGUAGE ALIGNMENT VIA LLM-BASED EMBEDDER Xiaoxing Hu1,2, Kaicheng Yang3, Ziyong Gong1, Qi Ming4, Zonghao Guo5, Xiang An3 Ziyong Feng3, Junchi Yan1, Xue Yang1(cid:66) 1Shanghai Jiao Tong University 4Beijing University of Technology Equal contribution 5Tsinghua University (cid:66)Corresponding author 2Beijing Institute of Technology 3DeepGlint 5 2 0 2 1 2 ] . [ 1 5 9 7 8 1 . 0 1 5 2 : r Github: https://github.com/VisionXLab/ProCLIP Model Zoo: https://huggingface.co/VisionXLab/ProCLIP"
        },
        {
            "title": "ABSTRACT",
            "content": "The original CLIP text encoder is limited by maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic visionlanguage alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIPs text encoder into the LLM-based embedder to leverage CLIPs rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. Extensive experiments show ProCLIP achieves 6.8% to 13.5% improvement on zeroshot classification and presents excellent performance on cross-modal retrieval, multilingual cross-modal retrieval, and fine-grained understanding tasks, demonstrating the effectiveness and robustness of ProCLIP. The Code is available at https://github.com/VisionXLab/ProCLIP. 1 INTRODUCTION CLIP demonstrates remarkable zero-shot recognition capabilities by learning joint vision-language representations through contrastive learning on large-scale imagetext pairs (Radford et al., 2021). Serving as bridge between vision and language, it is widely adopted in multiple downstream tasks such as imagetext retrieval (Yang et al., 2023; Zheng et al., 2025), text-to-image generation (Wang et al., 2022), and open-vocabulary object detection (Wu et al., 2023b). However, the original CLIP model relies on English text captions with maximum length of 77 tokens as its supervisory signal (Zhang et al., 2024). This design limits its capacity to process long-form text and restricts input to English-only (Tschannen et al., 2025). Additionally, due to the absence of supervision for finegrained textual semantics, this limitation further impedes its semantic understanding capability (Hu et al., 2025; Gu et al., 2025a;b). 1 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. To overcome these limitations, methods such as Long-CLIP (Zhang et al., 2024) interpolate positional embeddings and fine-tune on long textimage pairs to extend the input length. While effective for long-text understanding, such approaches still fall short in enhancing CLIPs fine-grained semantic understanding and multilingual capabilities. Recently, LLM exhibits remarkable proficiency in natural language processing, and it has pivoted towards harnessing decoder-only architectures for effective representation learning (BehnamGhader et al., 2024; Lee et al., 2024). Following this trend, methods such as FLAME (Cao et al., 2025) and LLM2CLIP (Huang et al., 2024) propose to replace CLIPs original text encoder with LLM-based embedders. By leveraging the rich openworld knowledge inherent in LLMs, these approaches aim to enhance CLIPs representational capacityparticularly in processing longer and more complex image captions. However, these methods align the CLIP image encoder directly with the LLM-based text embedder through contrastive learning, while neglecting the rich pretrained knowledge within CLIP. This from-scratch alignment compels both encoders to learn new representation space from scratch, disregarding the original CLIP alignment knowledge. Such an approach increases the risk of overfitting, particularly when training data is scarce, thereby compromising model generalization. This observation leads to critical research question: How can we systematically leverage CLIPs pretrained knowledge to achieve efficient cross-modal alignment with an LLM-based embedder while preserving generalization capability? In this paper, we propose ProCLIP, simple yet effective progressive vision-language alignment framework enhancing the CLIP. ProCLIP leverages curriculum learning to first guide the LLM-based embedder (only MLP trainable) to adapt to the CLIP text encoders representation space, and then uses contrastive learning to further learn joint image-text representations. Specifically, ProCLIP first distills knowledge from the original CLIP text encoder into the LLM-based embedder, establishing an initial alignment between the CLIP image encoder and LLM-based embedder. Subsequently, we conduct contrastive learning on imagetext pairs to further improve this alignment. Since the LLM-based embedder is already partially aligned with the CLIP image encoder during the prior stage, the contrastive optimization process becomes more stable and preserves generalization more effectively. To further mitigate overfitting, we impose self-distillation constraint on the CLIP image encoder throughout this stage, which stabilizes training and improves generalization. To prove the effectiveness of ProCLIP, we evaluate it on multiple tasks across diverse data scales and model sizes. Extensive experiment results demonstrate that ProCLIP achieves consistently significant improvements. The main contributions of this paper are summarized as follows: We highlight the limitation of previous works: previous methods fail to fully exploit the pretrained knowledge in CLIP, and their reliance on simplistic contrastive learning for cross-modal alignment significantly compromises CLIPs inherent generalization capabilities. We propose ProCLIP simple but effective Progressive vision-language alignment framework to enhance CLIP. ProCLIP initially distills the pretrained knowledge into the LLM-based embedder. After that, ProCLIP utilizes contrastive fine-tuning constrained by self-distillation to further enhance cross-modal alignment while preserving the models inherent knoweledge. We conduct extensive experiments on multiple tasks across diverse scales of data and model. Compared to the baseline, ProCLIP achieves 6.8% to 13.5% improvement on zero-shot classification and performs strongly on other tasks, including short-text cross-modal retrieval, long-text cross-modal retrieval, multilingual cross-modal retrieval, and fine-grained understanding."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Vision-Language Contrastive Learning. Vision-language contrastive learning aims to learn robust multimodal representations by pretraining on large-scale image-text pairs. seminal work in this area, CLIP (Radford et al., 2021) aligns visual and linguistic representations through contrastive learning, bridging both modalities in shared semantic space. As bridge between vision and language, CLIP has been widely applied in multimodal learning. It enables variety of natural language-guided open-vocabulary recognition tasks, including image classification (Zhou et al., 2022b;a; Kim et al., 2024), open-vocabulary semantic segmentation (Ding et al., 2022; Li et al., 2022; Ghiasi et al., 2022; Xu et al., 2022; Cho et al., 2024; Lan et al., 2024), and open-vocabulary object detection (Du et al., 2022; Kaul et al., 2023). However, CLIP remains fundamentally constrained by its text encoders limited capacity and fixed input length, which hinders its ability to process multilingual and long texts and model fine-grained semantics. To mitigate these issues, several methods 2 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Figure 1: Illustration of research gap. Previous work directly aligns the LLM-based embedder with the CLIP image encoder, disregarding the valuable knowledge embedded in the pre-trained CLIP model. In contrast, ProCLIP first transfers knowledge from CLIPs text encoder to the LLM embedder via distillation, establishing an initial alignment. It then refines the alignment between the CLIP image encoder and the LLM-based embedder through image-text contrastive learning with self-distillation regularization. have been introduced. Long-CLIP (Zhang et al., 2024) extends the input length via positional embedding interpolation, yet still fails to capture nuanced semantic relationships. LoTLIP (Wu et al., 2024) incorporates corner tokens to aggregate diverse textual information, preserving short-text understanding while significantly improving performance on long texts. Nevertheless, constrained by the capabilities of the text encoder, LoTLIP cannot incorporate additional open-world knowledge and remains unable to handle multilingual inputs. LLMs for Representation Learning. Large language models have presented remarkable proficiency across wide range of natural language processing tasks (Touvron et al., 2023; Achiam et al., 2023; Bai et al., 2023; Liu et al., 2024a). Recent research has pivoted towards harnessing decoderonly architectures for effective representation learning. For instance, LLM2Vec (BehnamGhader et al., 2024) converts pre-trained decoder-only LLMs into versatile text encoders by incorporating three principal advancements: bidirectional attention mechanisms, masked next-token prediction, and unsupervised contrastive alignment. Meanwhile, Qwen3-Embedding (Zhang et al., 2025b) capitalizes on the Qwen3 models strong multilingual understanding and generation abilities. By integrating large-scale unsupervised pretraining and supervised fine-tuning on high-quality data, it achieves state-of-the-art performance on the MTEB benchmark (Muennighoff et al., 2022). Inspired by these advances, recent works (Huang et al., 2024; Cao et al., 2025; Zhang et al., 2025a) attempt to enhance CLIP by replacing its text encoder with powerful LLM-based embedder, thereby improving its ability to process multilingual, longer, and more complex textual inputs. Although these approaches present promise, their alignment strategies remain overly coarse and often lead to degraded generalization. Developing more refined and effective alignment techniques thus remains critical and open research challenge. Knowledge Distillation. Knowledge distillation (Hinton et al., 2015) is widely used in deep learning to enhance model performance and reduce computational complexity. Typically, larger teacher model transfers knowledge to smaller student model by guiding the learning of features or output distributions. Alternatively, self-distillation methods enable knowledge transfer within sinIn the context gle model, where deeper layers supervise shallower ones (Zhang et al., 2019). 3 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Figure 2: The training pipeline of our proposed ProCLIP. It consists of representation inheritance via crossarchitecture distillation and contrastive tuning integrated with self-distillation regularization. of CLIP, several distillation techniques have been introduced. TinyCLIP (Wu et al., 2023a) employs affinity mimicking to capture cross-modal interactions during distillation, allowing the student to replicate the teachers alignment behavior in shared affinity space. CLIP-KD (Yang et al., 2024a) integrates multiple strategiesincluding relation-based, feature-based, gradient-based, and contrastive distillationto maximize feature similarity between the teacher and student models. CLIP-CID (Yang et al., 2024b) utilizes cluster-instance discrimination to transfer semantic knowledge from the teacher, enabling the student to develop richer understanding of the pretraining data. Different from the above method, this paper introduces self-distillation mechanism to mitigate catastrophic forgetting during training and preserve the generalization capabilities of the model."
        },
        {
            "title": "3 METHODOLOGY\nIn this section, we first introduce the preliminary (Sec. 3.1), including contrastive language-image\npre-training and improving CLIP with an LLM-based embedder. Then we present our proposed\nProCLIP framework, which comprises two primary training stages: 1) Representation Inheritance\nvia Cross-Architecture Distillation (Sec. 3.2). 2) Contrastive Tuning Integrated with Self-Distillation\nRegularization (Sec. 3.3).",
            "content": "3.1 PRELIMINARY Contrastive Language-Image Pre-training. Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) learns to align images and text from large-scale imagetext pairs through contrastive learning, bridging both modalities in shared embedding space. Given batch of image-text pairs {(Ii, Ti)}B i=1, the image encoder EI and text encoder ET map them into the joint semantic space as {(vi, ti)}B i=1. To optimize both encoders in dual-tower architecture, symmetric contrastive learning objective is imposed on the resulting representations: LCLIP = (cid:34) (cid:88) i= log (cid:124) (cid:80)B exp(vi /τ ) j=1 exp(vi (cid:123)(cid:122) text-to-image /τ ) (cid:125) + log (cid:124) (cid:80)B exp(ti /τ ) j=1 exp(ti (cid:123)(cid:122) image-to-text /τ ) (cid:125) (cid:35) . (1) However, the native CLIP text encoder is limited to sequences of up to 77 tokens. common solution is to interpolate the position embeddings of the CLIP text encoder and fine-tune the model. Alternatively, one may replace the CLIP text encoder with an LLM-based embedder. The latter approach not only improves long-text understanding but also enhances multilingual understanding and 4 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. fine-grained semantic comprehension, resulting in more versatile vision-language dual-encoder. In this work, we investigate more efficient alignment strategy that leverages an LLM-based embedder to enhancing CLIPs comprehensive capabilities. i}N i=1 offline into embeddings {t Improving CLIP with LLM-based Embedder. LLM2CLIP (Huang et al., 2024) first introduces an LLM-based embedder into CLIP, demonstrating enhanced long-text understanding. Given an LLM-based encoder GT , it encodes texts {Ti}N i=1. This process is typically performed in an offline manner. During contrastive fine-tuning, multilayer perceptron (MLP) is used to map {t}N i=1 into the CLIP embedding space for dimensional alignment. The mapped text features and the image features from the CLIP image encoder are then optimized via the contrastive loss in Eq. 1, resulting in newly aligned representation space. However, applying contrastive learning directly to fine-tuning data to optimize the MLP and vision encoder hinders the convergence of the new dual-tower architecture to an optimal parameter space. This arises because the text representations from the LLM-based embedder and MLP lack prior alignment with the vision encoder. Moreover, unconstrained fine-tuning may also cause excessive drift from the original pre-trained representation, while limited fine-tuning data (e.g., 3M samples) cannot compensate for the knowledge acquired during large-scale pre-training (e.g., 400M samples). To overcome these challenges, we propose progressive alignment pipeline that improves multimodal alignment while preserving pre-trained knowledge. 3.2 STAGE 1: REPRESENTATION INHERITANCE VIA CROSS-ARCHITECTURE DISTILLATION. Given pre-trained image and text encoder of the CLIP model {EI , ET } and pre-trained LLMbased embedder GT , our goal is to replace the CLIP text encoder ET with the LLM-based embedder GT to enhance comprehensive abilities. Consistent with prior works (Huang et al., 2024; Cao et al., 2025; Zhang et al., 2025a), we initially extract embeddings from textual captions offline using GT : = {GT (Ti) Rd}N i=1, where represents the embedding dimension of the LLM-based embedder. The embedding space of the LLM-based embedder exhibits no prior alignment with the CLIP imagetext representation space. To bridge this gap, we adopt cross-architecture distillation strategy that transfers knowledge from the CLIP text embedding space to the LLM embedding space. Specifically, given batch of texts {Ti}B i=1, we first utilize MLP to unify the dimensions of LLM embeddings and CLIP text embeddings. To facilitate fine-grained semantic alignment, we propose an instance semantic alignment loss, denoted as Lins. This loss function leverages text-only data to distill knowledge from CLIPs text encoder into the LLM-based embedder, defined as follows: Lins = (cid:88) i= MLP(t i) E(Ti)2. (2) Since Lins only focuses on instance-level alignment without capturing the global embedding structure, we propose the embedding structure alignment loss Lstruct. This loss measures inter-sample distances within batch in both the CLIP text encoder and LLM-based embedder spaces, and aligns the two globally by minimizing their pairwise distance discrepancy. Lstruct is defined as: Lstruct = (cid:88) i,j=1 i<j (cid:12) (cid:12)MLP(t i) MLP(t j)2 E(Ti) E(Tj)2 (cid:12) (cid:12). (3) The overall loss is the first stage is defined as:Ldis = Lins + Lstruct. 3.3 STAGE 2: CONTRASTIVE TUNING INTEGRATED WITH SELF-DISTILLATION REGULARIZATION. After the above phase, the MLP(GT ) has already been preliminarily adapted to CLIPs visionlanguage embedding space, making subsequent fine-tuning with vision-language contrastive learning significantly easier. We utilize the InfoNCE loss (Radford et al., 2021) to better align the image embedding vi and the projected LLM embedding i), which can be formulated as: = MLP(t Linfo = (cid:34) (cid:88) i=1 log exp(vi /τ ) j=1 exp(vi j /τ ) (cid:80)B + log exp(t j=1 exp(t /τ ) /τ ) (cid:80)B (cid:35) , (4) where τ is learnable temperature parameter. Beyond standard contrastive learning, we impose self-distillation constraint on the CLIP image encoder to mitigate excessive forgetting of pre-trained 5 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Table 1: Cross-modal retrieval performance Recall@1 on multiple datasets. Method Data Flickr30k T2I I2T COCO I2T T2I ShareGPT4V I2T T2I Urban-1k T2I I2T DOCCI DCI Avg. I2T T2I I2T T2I I2T T2I Model Architecture: CLIP ViT-B/32 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 80.3 3M 83.5 3M 86.0 15M 86.2 15M 86.6 30M 87.8 30M 90.2 59.8 70.1 73.5 72.2 72.6 72.4 74.6 Model Architecture: CLIP ViT-B/16 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 82.7 3M 88.0 3M 89.4 15M 88.9 15M 90.8 30M 90.2 30M 92. 63.4 75.3 77.6 76.6 77.9 78.1 79.1 Model Architecture: CLIP ViT-L/14 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 86.6 3M 92.4 3M 92.8 15M 91.3 15M 93.4 30M 93.1 30M 94.5 64.6 80.1 81.1 80.6 81.4 81.0 81.6 51.5 55.6 57.8 58.5 59.0 61.1 62.4 53.7 60.5 61.7 62.4 63.2 65.4 67. 57.2 65.5 66.4 67.0 67.6 68.2 69.3 Model Architecture: EVA02-CLIP ViT-L/14 EVA02-CLIP LLM2CLIP ProCLIP 2B 88.9 3M 93.8 3M 93.0 76.9 81.7 82.6 63.6 66.6 68.6 30.6 41.1 43.5 43.2 43.5 44.3 45. 33.3 44.8 46.8 46.5 47.8 48.5 49.7 36.4 49.7 51.9 50.6 52.5 52.0 53.2 46.6 51.1 53.4 77.3 94.2 94.4 95.3 94.5 96.7 96.8 76.1 94.4 94.3 95.0 94.2 96.8 96.0 78.0 95.2 95.1 96.3 96.1 97.5 96. 84.5 96.5 96.6 66.0 93.4 92.6 94.2 93.9 95.9 95.9 68.9 94.4 93.3 95.2 94.9 96.4 96.4 68.7 95.6 94.8 95.3 95.4 97.7 97.0 79.4 95.9 96.0 60.9 78.2 80.8 80.6 82.2 86.6 88. 67.5 80.6 82.9 84.5 85.8 89.7 90.0 68.4 83.6 86.9 86.4 88.3 92.7 93.0 72.0 84.4 88.4 46.8 84.2 85.3 85.3 85.3 88.8 89.9 53.5 86.0 88.1 88.4 89.6 91.3 93.4 56.0 89.0 92.3 90.5 92.6 93.9 94. 69.4 92.1 93.2 58.1 76.2 78.1 79.2 78.4 82.9 82.9 66.8 81.7 81.0 83.8 82.5 86.2 85.1 65.8 85.1 85.9 86.4 86.2 88.2 87.5 72.6 86.6 87.0 53.4 77.1 79.5 80.7 80.6 82.9 84. 57.0 82.2 82.5 85.1 84.6 86.8 87.3 63.1 85.9 86.9 88.5 88.4 89.6 89.8 74.2 88.7 89.7 43.1 62.2 65.7 64.3 67.1 67.9 70.6 45.4 67.2 67.3 69.3 70.2 73.1 73.6 45.4 70.0 71.2 71.7 74.4 74.9 75. 43.9 73.8 71.8 40.3 64.4 68.3 67.6 69.2 69.5 71.9 43.0 69.1 72.0 72.4 74.0 74.8 76.9 43.9 74.4 76.1 75.3 76.8 78.3 79.5 45.2 76.1 78.4 61.8 75.0 77.1(+2.1) 77.4 78.0(+0.6) 80.5 81.9(+1.4) 65.4 78.7 79.4(+0.7) 80.7 81.2(+0.5) 83.6 84.2(+0.6) 66.9 82.0 83.0(+1.0) 83.2 84.3(+1.3) 85.8 86.2(+0.4) 49.5 71.1 73.8(+2.7) 73.9 74.2(+0.3) 75.7 77.0(+1.3) 45.6 75.3 76.7(+1.4) 77.3 78.0(+0.7) 79.3 80.5(+1.2) 55.5 79.1 80.5(+1.4) 80.1 81.2(+1.1) 82.1 82.6(+0.5) 70.9 83.6 84.2(+0.6) 65.3 80.9 82.2(+1.3) knowledge during adaptationessential for preserving generalization. On the image encoder side, we apply regularization loss that is symmetric to the one used in the first stage(Eq. 2, Eq. 3): Lreg = (cid:88) i=1 EI (Ii) (Ii)2 + (cid:88) i,j=1 i<j (cid:12) (cid:12)EI (Ii) EI (Ij)2 (Ii) (Ij) (cid:12) (cid:12), (5) where denotes the EMA (Exponential Moving Average)-updated image encoder obtained as: (6) where α controls the update rate of the teacher model parameters. The overall loss function of the contrastive tuning stage is defined as Ltune = Linfo + λLreg, where λ is loss weight. + (1 α)EI , = αE"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets and Benchmarks. For the alignment dataset, we use CC3M (Changpinyo et al., 2021), CC12M (Changpinyo et al., 2021), and YFCC15M (Thomee et al., 2016),combined the high-quality captions from DreamLIP (Zheng et al., 2024). We conduct experiments with data scales of 3M (CC3M), 15M (CC3M + CC12M), and 30M (CC3M + CC12M + YFCC15M) to explore the effects of data scaling. For the benchmark, we perform zero-shot classification on 11 different classification datasets, robustness evaluations on 5 ImageNet variants, retrieval evaluations on 6 datasets, multilingual cross-modal retrieval evaluation on XM3600 (Thapliyal et al., 2022), and fine-grained understanding evaluation on MMVP-VLM (Tong et al., 2024). Regarding the model, we employ three OpenAI pre-trained CLIP models, ViT-B/32, ViT-B/16, and ViT-L/14, to investigate the effects of model scaling. Additionally, we conduct experiments with pretrained EVA02-CLIP (Fang et al., 2023) ViT-L/14 to assess the impacts of different model architectures. For the LLM-based embedder, we primarily use LLaMA3-8B-CC consistent with LLM2CLIP (Huang et al., 2024). four epochs for contrastive tuning. Implementation Details. For the representation inheritance phase, we train for four epochs, During training, we employ followed by another AdamW (Loshchilov, 2019) as the optimizer, with learning rate of 1 105 and weight decay of 0.2. The parameters β1 and β2 are set to 0.9 and 0.98, respectively. In the first stage, the training batch size is set to 1024, while in the second stage it is increased to 4096. The loss weight λ is set at 0.0004. Other training details can be found in the supplementary material. 6 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Figure 3: Per-language image-text retrieval performance on the XM3600 benchmark. Table 2: Zero-shot classification performance on 11 datasets. The best results are marked in bold. 1 0 1 Data 0 1 I Method Model Architecture: CLIP ViT-B/32 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 83.1 3M 49.6 3M 64.5 15M 57.2 15M 74.9 30M 58.5 30M 74.4 88.7 89.2 90.7 88.3 90.0 88.3 88.8 Model Architecture: CLIP ViT-B/16 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 87.9 3M 56.9 3M 73.1 15M 63.2 15M 80.3 30M 64.4 30M 81.0 89.7 92.6 92.5 90.8 90.8 90.2 89.3 Model Architecture: CLIP ViT-L/14 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 92.6 3M 64.8 3M 83.4 15M 70.1 15M 87.1 30M 71.2 30M 88.9 94.9 95.4 96.6 95.2 95.4 94.0 94.1 0 0 1 I 63.5 61.5 65.8 61.4 66.5 61.0 66.9 66.8 64.4 68.9 64.5 69.7 64.6 68.3 77.0 72.9 78.3 72.3 77.6 70.5 77. Model Architecture: EVA02-CLIP ViT-L/14 EVA02-CLIP LLM2CLIP ProCLIP 2B 92.9 3M 64.1 3M 82.7 98.8 96.5 97.9 89.8 82.6 88.4 7 9 3 61.5 60.3 65.0 61.3 65.1 61.2 65. 63.1 62.2 67.9 62.9 67.4 63.7 68.2 66.8 66.4 72.4 66.4 72.3 67.0 72.5 73.8 68.0 73.6 r A 18.8 8.6 11.6 8.4 13.9 8.4 16.2 22.8 11.7 13.5 9.9 16.5 11.2 17. 30.7 10.4 16.2 9.5 21.1 11.3 25.2 35.1 9.0 16.5 C 57.6 11.5 21.2 19.6 39.6 20.6 38.0 63.7 15.4 32.3 27.3 44.3 27.0 48.5 76.5 18.8 45.1 32.4 59.8 32.1 61. 88.8 29.2 57.6 s 42.8 47.8 52.0 50.6 53.7 50.3 53.0 45.0 50.9 54.1 52.8 56.7 55.0 57.3 54.4 54.8 59.6 58.0 62.1 57.8 62. 60.6 59.4 63.5 84.6 38.0 51.7 42.3 68.5 37.6 64.5 87.0 46.5 59.8 50.3 75.8 45.9 70.2 93.2 47.3 65.9 54.3 77.0 54.7 81.5 93.7 48.5 67.6 1 0 1 t C 89.4 79.0 83.3 80.7 86.7 81.7 86.8 90.4 82.9 87.0 83.2 88.4 84.0 88.8 93.9 88.3 92.3 88.3 92.4 89.3 92.9 95.1 89.8 93.8 w 66.0 22.6 30.8 23.5 35.5 26.0 40. 67.6 23.6 35.8 23.7 40.8 27.1 44.8 78.1 26.8 41.8 26.6 48.8 28.8 57.2 76.3 28.6 45.4 e I 61.9 41.0 47.9 43.3 53.3 45.1 54. 67.1 45.8 54.8 46.5 58.6 49.7 59.2 74.5 52.8 62.5 54.0 66.0 56.4 67.8 78.2 56.4 66.8 Avg. 65.2 46.3 53.1(+6.8) 48.8 58.9(+10.1) 49.0 59.0(+10.0) 68.3 50.3 58.2(+7.9) 52.3 62.7(+10.4) 53.0 63.0(+10.0) 75.7 54.4 64.9 (+10.5) 57.0 69.3 (+12.3) 57.5 71.0 (+13.5) 80.3 57.5 68.5 (+11.0) 4.2 MAIN RESULTS Cross-Modal Retrieval. As shown in Tab. 1, ProCLIP consistently surpasses LLM2CLIP in both shortand long-text retrieval tasks across various datasets and model scales. On short-text datasets such as Flickr30k and COCO, ProCLIP achieves significant improvements in both image-to-text (I2T) and text-to-image (T2I) retrieval. For instance, with ViT-L/14 and 30M training samples, it reaches 95.0% I2T Recall@1 on Flickr30knearly 2 percentage points higher than LLM2CLIP. On long-text benchmarks including DOCCI, DCI, and Urban-1k, ProCLIP also exhibits clear advantages. Under ViT-B/16 trained on 30M samples, it attains 73.6% (I2T) and 76.9% (T2I) on DCI. Moreover, across all data scales from 3M to 30M, ProCLIP delivers stable gains, with particularly strong improvements in T2I retrieval. These results confirm that ProCLIP enhances performance in both shortand long-text scenarios. Multilingual Cross-Modal Retrieval. Benefiting from the LLM-based embedder, ProCLIP facilitates multilingual capabilities. As illustrated in Fig. 3, we compare the cross-lingual retrieval performance between LLM2CLIP and ProCLIP on the XM3600 benchmark (Thapliyal et al., 2022). ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Experiment results demonstrate that our approach achieves superior multilingual performance. This enhancement is attributed to the improved alignment between the CLIP image encoder and the LLM-based embedder. Zero-Shot Classification. In Tab. 2, we present the zero-shot classification performance on 11 downstream tasks across different data and model scales. We observe that LLM2CLIP significantly compromises the original generalization ability of CLIP. Even when utilizing 30M data points, compared to CLIPs ViT-B/32, ViT-B/16, and ViT-L/14, the average performance declines by 16.2%, 15.3%, and 18.2%, respectively. Compared to LLM2CLIP, our proposed ProCLIP method achieves significant performance improvements across all experimental conditions. Particularly, with dataset of 30M samples, ProCLIP enhances the average performance by approximately 10%- 13.5%. This notable improvement is primarily attributed to two factors: 1) The representation inheritance process allows the LLM embedder to inherit some knowledge from the original CLIP text encoder. 2) During the contrastive tuning phase, the introduction of distillation loss as regularization helps to mitigate the forgetting of knowledge throughout the learning process. Data IN-A IN-V Method datasets Robustness Table 3: Robustness performance. The best results are marked in bold. Robustness. To evaluate the robustness of ProCLIP we report its performance across varying data sizes and model scales in Tab. 3. ProCLIP achieves consistently improvements of 5.9%- average Notably, on challenging 9.3%. like out-of-distribution ImageNet-A and ImageNet-R, ProCLIP outperforms LLM2CLIP by over 10 percentage points, highlighting its enhanced ability to handle distribution shifts and complex perturbations. These results demonstrate that ProCLIP not only improves retrieval and classification performance but also delivers robust and reliable results across diverse robustness scenarios, indicating substantial progress in generalization and resilience. Model Architecture: CLIP ViT-L/14 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP Model Architecture: EVA02-CLIP ViT-L/14 EVA02-CLIP LLM2CLIP ProCLIP 400M 69.8 49.0 3M 58.3 3M 50.8 15M 62.1 15M 52.7 30M 63.4 30M 59.6 44.8 52.3 46.3 55.3 47.3 55.7 87.8 75.0 84.0 78.2 86.4 78.6 86.8 32.2 32.4 31.6 33.8 34.2 34.0 34.1 70.8 46.6 63.3 50.1 66.4 52.7 68. IN-O IN-R IN-S 67.9 50.6 59.9 92.7 79.1 89.4 29.6 28.8 29.3 76.4 50.4 66.5 72.6 51.8 62. 2B 3M 3M Table 4: Comparison with other methods across different model scales and LLM embedders. Method ViT Init LLM Embedder Data ImageNet Mistral-Nemo FLAME random ShareLock DINOv2 B/14 Llama3 random LIFT LiT CLIP B/16 LLM2CLIP CLIP B/16 CLIP B/16 ProCLIP NV-Embedv2 Llama3-CC Llama3-CC Llama3-CC Figure 4: MMVP performance comparison. ProCLIP presents excellent performance. DINOv2 L/14 NV-Embedv2 SAIL CLIP L/14 LiT LLM2CLIP CLIP L/14 CLIP L/14 ProCLIP CLIP L/14 ProCLIP Llama3-CC Llama3-CC NV-Embedv2 Llama3-CC 36.0 3M 3M 52.1 512M 43.6 51.0 45.8 54. 3M 3M 3M 3M 3M 3M 3M 3M 54.0 60.1 52.8 61.4 62.5 COCO Flickr30k I2T T2I I2T T2I 43.3 28.6 67.3 53.6 - - - - 34.6 36.0 69.1 72.9 56.2 41.9 85.2 71.9 60.5 44.8 88.0 75.3 61.7 46.8 89.4 77.6 - - 45.4 32.9 59.4 44.6 88.0 74.7 65.5 49.7 92.4 80.1 64.8 51.7 91.9 81.4 66.4 51.9 92.8 81.1 Fine-Grained Understanding. Fig. 4 presents the fine-grained vision-language understanding performance on the MMVP benchmark (Tong et al., 2024) using CLIP ViT-L/14. LLM2CLIP improves over CLIP by 2.9%, 5.9%, and 4.4% at 3M, 15M, and 30M data scales, respectively. Our ProCLIP model further advances these results, achieving gains of 3.0%, 2.2%, and 10.4% on the corresponding data scales. These improvements demonstrate that the LLM-based embedder enhances fine-grained semantic discrimination, and the consistent superiority of our method underscores the effectiveness of the progressive alignment strategy. Comparison with Other Methods. To further prove the effectiveness of ProCLIP, we proincludvide comprehensive comparison of all recent LLM embedder-based CLIP models, 8 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Table 5: Ablation on different components. Method Stage 1 Stage 2 Lins Lstruct Linfo Lreg IN-1k I2T Avg T2I Avg CLIP LLM2CLIP ProCLIP 74.5 52.8 58.9 59.5 59.2 62.5 66.9 82.0 69.3 70.3 82.9 83. 55.5 79.1 79.4 61.2 80.2 80.5 Figure 5: Ablation on different LLMbased embedders. ing FLAME (Cao et al., 2025), ShareLock (Ruthardt et al., 2024), LIFT (Yang et al., 2025), SAIL (Zhang et al., 2025a), LiT (Zhai et al., 2022), and our baseline LLM2CLIP (Huang et al., 2024). As shown in Tab. 4, under the same or lower training costs, ProCLIP consistently achieves superior performance across various model sizes. Benefiting from representation inheritance and self-distillation regularization, ProCLIP not only achieves significant performance improvements in In1k classification but also enhances general retrieval capabilities on COCO and Flickr30k. 4.3 ABLATION STUDY Ablation of Different LLM-based Embedder. As shown in Fig. 5, we compare different LLM embedders, including Qwen3-Embedding (8B), GME (7B), NV-Embedv2 (7B) , and Llama3-CC (8B) based on ViT-L/14 with 15M data. Llama-CC achieves the strongest overall performance in both ImageNet zero-shot classification and retrieval tasks. Notably, while different embedders show only minor variations in retrieval performance, they exhibit substantial differences in ImageNet classification accuracy. This suggests that the alignment discrepancy between each LLM embedders feature space and the original CLIP space varies considerably, resulting in different degrees of degradation in general capabilities after image-text alignment. Ablation of Different Components. To further validate the effectiveness of the methods proposed in this paper, we conduct comprehensive ablation study on various components, as detailed in Tab. 5. Applying instance semantic distillation achieves 58.9% zero-shot accuracy on ImageNet1k using only text data, indicating successful transfer of CLIPs textual representation capability to the MLP head. Incorporating the structural alignment loss further improves both classification and retrieval performance by enabling the LLM embedder to capture the global structural geometry of CLIPs text representation space, beyond point-wise semantic correspondences. After that, imagetext contrastive learning significantly boosts retrieval performance but reduces ImageNet-1k accuracy due to image encoder overfitting. Introducing self-distillation mitigates this issue, improving classification accuracy from 59.2% to 62.5% while slightly reducing retrieval gains. Finally, applying structured self-distillation enhances both tasks by stabilizing the image representation space during fine-tuning, preventing excessive overfitting while preserving pretrained knowledge."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose ProCLIP, simple yet effective progressive vision-language alignment framework designed to improve the alignment when integrating the CLIP image encoder with an LLM-based embedder. The framework employs curriculum learninginspired progressive training strategy: it first aligns the LLM-based embedders representation space with the original CLIP text encoder through knowledge distillation, effectively transferring pretrained semantic knowledge. Subsequently, it performs cross-modal alignment between the CLIP image encoder and the LLMbased embedder using image-text contrastive learning regularized by self-distillation to prevent overfitting and preserve pretrained knowledge. To ensure feature-space consistency, complementary distillation strategycomprising instance semantic and embedding structure alignment lossesis applied during text distillation and image self-distillation, respectively. Comprehensive experiments across varying data scales and model architectures demonstrate the effectiveness and generality of ProCLIP. We hope that our work offers valuable insights for advancing vision-language alignment. We will release all model weights and code to ensure full reproducibility. 9 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. Anjia Cao, Xing Wei, and Zhiheng Ma. Flame: Frozen large language models enable data-efficient language-image pre-training. In CVPR, pp. 40804090, 2025. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 35583568, 2021. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, pp. 370 387. Springer, 2024. Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. In CVPR, pp. 4113 4123, 2024. Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary universal image segmentation with maskclip. arXiv preprint arXiv:2208.08984, 2022. Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for open-vocabulary object detection with vision-language model. In CVPR, pp. 1408414093, 2022. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1935819369, 2023. Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, pp. 540557. Springer, 2022. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. Rwkv-clip: robust vision-language representation learner. arXiv preprint arXiv:2406.06973, 2024. Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. arXiv preprint arXiv:2504.17432, 2025a. Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, and Lidong Bing. Unime-v2: Mllm-as-a-judge for universal multimodal embedding learning. arXiv preprint arXiv:2510.13515, 2025b. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. 10 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In ICCV, pp. 83408349, 2021a. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, pp. 1526215271, 2021b. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, and Yupei Wang. DearXiv preprint coupled global-local alignment for improving compositional understanding. arXiv:2504.16801, 2025. Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, et al. Llm2clip: Powerful language model unlocks richer visual representation. arXiv preprint arXiv:2411.04997, 2024. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Prannay Kaul, Weidi Xie, and Andrew Zisserman. Multi-modal classifiers for open-vocabulary object detection. In ICML, pp. 1594615969. PMLR, 2023. Gahyeon Kim, Sohee Kim, and Seokju Lee. Aapl: Adding attributes to prompt learning for visionlanguage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 15721582, June 2024. Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Clearclip: Decomposing clip representations for dense vision-language inference. In ECCV, pp. 143160. Springer, 2024. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022. Xianhang Li, Yanqing Liu, Haoqin Tu, Hongru Zhu, and Cihang Xie. Openvision: fullyopen, cost-effective family of advanced vision encoders for multimodal learning. arXiv preprint arXiv:2505.04601, 2025. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023b. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pp. 740755. Springer, 2014. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Yanqing Liu, Xianhang Li, Letian Zhang, Zirui Wang, Zeyu Zheng, Yuyin Zhou, and Cihang Xie. Openvision 2: family of generative pretrained visual encoders for multimodal learning. arXiv preprint arXiv:2509.01644, 2025. 11 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. Loshchilov. Decoupled weight decay regularization. In ICLR, 2019. Niklas Muennighoff, Nouamane Tazi, Loıc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. Docci: Descriptions of connected and contrasting images. In ECCV, pp. 291309. Springer, 2024. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In ICCV, pp. 26412649, 2015. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, pp. 53895400. PMLR, 2019. Jona Ruthardt, Gertjan Burghouts, Serge Belongie, and Yuki Asano. Do better language models have crisper vision? 2024. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Ashish Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522, 2022. Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, pp. 95689578, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana RomeroSoriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2670026709, 2024. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. NIPS, 32, 2019. Zihao Wang, Wei Liu, Qian He, Xinglong Wu, and Zili Yi. Clip-gen: Language-free training of text-to-image generator with clip. arXiv preprint arXiv:2203.00386, 2022. Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi Stephen Chen, Xinggang Wang, et al. Tinyclip: Clip distillation via affinity mimicking and weight inheritance. In ICCV, pp. 2197021980, 2023a. ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Wei Wu, Kecheng Zheng, Shuailei Ma, Fan Lu, Yuxin Guo, Yifei Zhang, Wei Chen, Qingpei Guo, Yujun Shen, and Zheng-Jun Zha. Lotlip: Improving language-image pre-training for long text understanding. NIPS, 37:6499665019, 2024. Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li. Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching. In CVPR, pp. 70317040, 2023b. Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei arXiv preprint Leng, and Yuhui Yin. Fg-clip: Fine-grained visual and textual alignment. arXiv:2505.05071, 2025. Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model. In ECCV, pp. 736753. Springer, 2022. Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, and Yongjun Xu. Clip-kd: An empirical study of clip model distillation. In CVPR, pp. 1595215962, 2024a. Jingfeng Yang, Ziyang Wu, Yue Zhao, and Yi Ma. Language-image alignment with fixed text encoders. arXiv preprint arXiv:2506.04209, 2025. Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and In ICCV, Tongliang Liu. Alip: Adaptive language-image pre-training with synthetic caption. pp. 29222931, 2023. Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, and Jiankang Deng. Clip-cid: Efficient clip distillation via cluster-instance discrimination. AAAI, 2024b. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1812318133, 2022. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In ECCV, pp. 310325. Springer, 2024. Le Zhang, Qian Yang, and Aishwarya Agrawal. Assessing and learning alignment of unimodal vision and language models. In CVPR, pp. 1460414614, 2025a. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In CVPR, pp. 37133722, 2019. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025b. Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen. Dreamlip: Language-image pre-training with long captions. In European Conference on Computer Vision, pp. 7390. Springer, 2024. Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, and Qichuan Ding. Gradientattention guided dual-masking synergetic framework for robust text-based person retrieval. arXiv preprint arXiv:2509.09118, 2025. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, pp. 1681616825, 2022a. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. IJCV, 130(9):23372348, 2022b. 13 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TRAINING DETAILS Details of the hyperparameter configurations used for two-stage training of ProCLIP are presented in Tab. 6. Under the default setting, our MLP layers are consistent with the baseline LLM2CLIP, both consisting of four linear layers. Table 6: Detailed hyperparameters for training ProCLIP. Hyperparameters of stage1 Batch size Optimizer Weight decay Adam β Adam ϵ Learning rate Learning rate schedule Epochs Training GPUs 1024 (8 128) AdamW 0.05 (0.9,0.98) 1e-6 1e-5 cosine decay 4 8H A.2 DETAILS OF BENCHMARKS. Hyperparameters of stage2 Batch size Optimizer Weight decay Adam β Adam ϵ Learning rate Learning rate schedule Ema α λreg Epochs Training GPUs 4096 (8 512) AdamW 0.05 (0.9,0.98) 1e-6 1e-5 cosine decay 0.999 0.0004 4 8H100 Zero-Shot Classification & Linear Probe. Following the previous works (Yang et al., 2023; Gu et al., 2024), we evaluate the zero-shot classification and linear probe performance of the models on 11 datasets. The detailed information about these datasets and the prompt used in zero-shot classification are presented in Tab. 7 and Tab. 12. Table 7: List of zero-shot datasets with the data distribution and evaluation metrics. Dataset Classes Train size Test size Evaluation metric Food101 CIFAR10 CIFAR100 SUN397 Cars Aircraft DTD Pets Caltech101 Flowers ImageNet 102 10 100 397 196 100 47 37 101 102 75,750 50,000 50,000 19,850 8,144 6,667 3,760 3,680 3,000 2,040 1,281,167 25,250 10,000 10,000 19,850 8,041 3,333 1,880 3,669 5,677 6,149 50,000 accuracy accuracy accuracy accuracy accuracy mean per class accuracy mean per class mean-per-class mean per class accuracy Robustness. We evaluated the robustness of our model on five out-of-distribution datasets, including ImageNet-v2 (Recht et al., 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-O (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), and ImageNet-Sketch (Wang et al., 2019). Cross-Modal Retrieval. Following the previous works (Huang et al., 2024; Cao et al., 2025), we evaluate the cross-modal retrieval performance of the models on 6 datasets: Flickr30k (Plummer et al., 2015), COCO (Lin et al., 2014), ShareGPT4V (Chen et al., 2024), Urban-1k (Zhang et al., 2024), DOCCI (Onoe et al., 2024), and DCI (Urbanek et al., 2024). The details information about these dataset are present on Tab. 8. Multilingual Retrieval. We evaluated the multilingual capabilities of our model on XM3600 (Thapliyal et al., 2022). XM3600 contains 3,600 images covering total of 36 languages, including Arabic (ar), Bengali(bn), Chinese-Simplified (zh), Croatian (hr), Czech (cs), Danish (da), Dutch (nl), English (en),Farsi (fa), Filipino (fil), Finnish (fi), French (fr), German (de), Greek (el), Hebrew (he), Hindi (hi), Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Korean (ko),Maori(mi), Norwegian (no), 14 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Table 8: Zero-shot image-text retrieval evaluation settings. Dataset Test Images Evaluation Protocol Text type MSCOCO Flickr30k ShareGPT4V Urban-1k DOCCI DCI 5,000 1,000 1000 1000 5000 7805 Image-to-Text & Text-to-Image Image-to-Text & Text-to-Image Image-to-Text & Text-to-Image Image-to-Text & Text-to-Image Image-to-Text & Text-to-Image Image-to-Text & Text-to-Image short short long long long long Persian (fa), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Spanish (es), Swedish (sv), Swahili(sw), Thai (th), Turkish (tr), Telugu (te), Ukrainian (uk), and Vietnamese (vi). Fine-Grained Understanding. We evaluated the fine-grained understanding capability of the VLM on MMVP-VLM (Tong et al., 2024). MMVP-VLM consists of 150 samples in total, testing 9 patterns: Orientation and Direction: Questions about the direction something is facing or moving, such as the direction the dog or duck is facing, or the orientation of the school bus. (cid:219) Presence of Specific Features: Questions that focus on the existence or non-existence of certain elements or features in the image. State and Condition: Questions that pertain to the state or condition of an object, such as whether flag is blowing in the wind or if the ground is wet. (cid:29) Quantity and Count: Questions about the number of objects or features present in the image. , Positional and Relational Context: This aspect refers to the models ability to understand the position and relationship of objects or elements within an image in relation to each other and their surroundings. Color and Appearance: Questions regarding the color of certain objects or elements. (cid:212) Structural and Physical Characteristics: This category involves the models ability to identify and analyze the physical attributes and structural features of objects in an image. Text: Questions related to text or symbols present in the image. (cid:130) Viewpoint and Perspective: Questions concerning the perspective from which the photo was taken. MLLM benchmarks. We further integrate the fine-tuned vision encoder into LLaVA and evaluate its performance on several MLLM downstream benchmarks, including SEED-Bench (Li et al., 2023a), GQA (Hudson & Manning, 2019), VizWiz (Gurari et al., 2018), PoPE (Li et al., 2023b), TextVQA (Singh et al., 2019), MMBench (Liu et al., 2024b), and VQAv2 (Goyal et al., 2017). A.3 MORE RESULTS. Liner Probe. We conduct linear probe evaluations of the model on 11 datasets. As shown in 11, our method consistently achieves superior performance. This advantage stems from our progressive alignment framework, which stabilizes training through two-stage regularization that prevents overfitting in the vision encoder while preserving generalization capability. Further Analysis of Data Scale and Model Scale. We further analysis the effects of data scale and model scale, as shown in Tab. 10. For data scale, we observe that model performance improves with increasing data size. For example, when trained on 3M samples, ProCLIP achieves zero-shot IN-1k accuracy of 62.5, which rises to 67.8 when the dataset size increases to 30M. Under the same data scale, ProCLIP consistently outperforms LLM2CLIP. Notably, when we randomly sample 1M images from CC3M for training, ProCLIP achieves comparable or even superior zero-shot retrieval performance compared with LLM2CLIP, reaching 61.8 on zero-shot IN-1k. This highlights the data efficiency of ProCLIP. For model scale, we further expand the linear layers by three times, using 12 layers in total, which leads to additional performance gains. This suggests that ProCLIP can continue to benefit from simple parameter scaling. 15 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Table 9: MLLM(7B) performance under 2242 image resolution. Method SEED-Bench (image) GQA VizWiz PoPE TextVQA MMBench VQAv2 CLIP LLM2CLIP ProCLIP 65.3 66.4 66.4 62.0 61.7 62. 44.0 44.6 44.7 85.7 86.3 85.9 54.2 55.0 54.5 65.5 65.5 65.0 77.4 77.9 78.1 Table 10: Comparison of data and model scales under ViT-L architecture. I2T Avg. T2I Avg. Method CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP ProCLIP Data Alignment Layer 400M 1M 3M 3M 15M 15M 30M 30M 30M - 4linear 4linear 4linear 4layers 4layers 4linear 4linear 12linear IN-1k 74.5 61.8 52.8 62.5 54.0 66.0 56.4 67.8 71.5 66.9 81.9 82.0 83.0 83.2 84.3 85.8 86.2 86. 55.5 79.7 79.1 80.5 80.1 81.2 82.1 82.6 82.8 w 94.7 94.6 95.0 94.4 95.1 93.8 94.9 96.4 95.7 97.0 96.6 96.7 95.0 96.7 98.8 97.1 98.7 97.1 98.7 96.5 98.5 99.4 96.9 99. N m 74.3 74.2 74.4 74.2 74.4 74.3 74.5 79.6 79.6 79.7 79.6 79.8 79.7 79.5 82.9 82.5 81.9 82.6 83.0 82.2 82.7 84.1 84.1 84. Avg. 80.9 81.2 81.4 81.0 81.7 80.8 81.4 84.4 83.9 84.8 84.3 84.8 83.4 84.6 88.1 87.2 88.2 86.7 86.7 86.3 88.2 90.4 87.4 90.2 Table 11: Linear Probe performance on 11 datasets. 1 0 1 Data 0 1 I Method Model Architecture: CLIP ViT-B/32 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 88.6 3M 87.9 3M 88.4 15M 87.7 15M 88.7 30M 87.6 30M 88. 95.1 95.7 95.9 95.7 95.9 95.9 96.0 Model Architecture: CLIP ViT-B/16 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 92.7 3M 91.6 3M 92.8 15M 91.9 15M 92.6 30M 91.3 30M 92.3 96.0 97.0 96.8 97.0 96.7 96.6 96.6 Model Architecture: CLIP ViT-L/14 CLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP LLM2CLIP ProCLIP 400M 95.3 3M 94.5 3M 95.3 15M 94.4 15M 95.2 30M 94.1 30M 95. 89.1 98.6 98.5 98.5 98.4 98.2 98.4 0 0 1 I 80.1 83.1 83.1 82.7 82.8 83.0 83.1 82.5 84.5 84.6 84.9 84.3 84.8 85.7 87.2 89.2 88.8 88.8 88.6 88.4 89.0 Model Architecture: EVA02-CLIP ViT-L/14 EVA02-CLIP LLM2CLIP ProCLIP 2B 95.6 3M 94.1 3M 95.3 99.5 99.5 99.5 94.2 93.3 94.0 7 9 3 73.4 74.1 74.3 74.0 74.8 74.1 75.1 75.7 76.0 76.4 75.6 76.6 75.3 77. 79.4 79.6 80.3 78.5 79.7 78.7 80.3 80.4 79.4 81.0 r A 44.9 44.9 44.1 44.2 44.9 43.5 43.8 52.8 50.1 52.0 50.7 51.4 48.2 50.1 63.0 57.7 61.0 55.0 61.4 54.8 60. 69.5 54.3 65.7 C 80.8 78.0 79.5 77.5 80.8 76.3 79.0 85.9 82.1 85.6 83.7 85.6 80.6 84.7 90.7 86.7 90.3 86.0 90.5 84.6 90.0 94.2 85.0 93. D P 76.3 77.7 78.2 78.3 78.1 77.6 77.8 78.9 80.3 80.6 80.4 80.8 80.3 81.2 81.8 83.4 83.6 82.7 83.3 82.4 83.9 85.0 84.0 85. 89.3 90.4 90.3 90.2 90.2 90.1 89.8 93.1 92.3 93.3 92.9 93.6 92.5 93.1 95.3 94.1 95.2 93.9 95.3 93.7 95.2 94.8 93.2 95.4 1 0 1 t 92.7 92.4 92.6 92.5 92.8 92.8 92. 93.9 93.6 94.2 93.8 94.3 93.4 94.0 96.9 96.4 96.9 95.9 96.8 95.8 96.8 97.6 97.3 97.8 MLLM Performance. As shown in Tab. 9, when integrating the fine-tuned vision encoder into the MLLM, we observe performance improvements over CLIP on most benchmarks. This can be mainly attributed to the alignment with high-quality data, which enhances the semantic representation capability of the vision encoder. ProCLIP and LLM2CLIP achieve relatively comparable performance, indicating that ProCLIP does not exhibit significant advantage within the MLLM benchmarks. We attribute this to the fact that our method, compared with the baseline, does not place additional emphasis on the downstream MLLM benchmarks. further discussion on this issue can be found in Openvision Li et al. (2025) and OpenVision2 (Liu et al., 2025). 16 ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder. Table 12: Full list of prompts to evaluate the performance of zero-shot classification on 11 visual recognition datasets. CIFAR 10 & CIFAR 100 photo of {label}. high contrast photo of {label}. photo of big {label}. low contrast photo of the {label}. photo of the small {label}. Food101 photo of {label}, type of food. Caltech101 photo of {label}. sketch of {label}. embroidered {label}. an origami {label}. doodle of {label}. sculpture of the {label}. rendition of the {label}. the plushie {label}. drawing of the {label}. Stanford Cars photo of {label}. photo of my dirty {label}. DTD photo of {label} texture. photo of the {label} texture. FGVC Aircraft photo of {label}, type of aircraft. Flowers102 photo of {label}, type of flower. Pets photo of {label}, type of pet. SUN39 photo of {label}. ImageNet bad photo of {label}. low resolution photo of the {label}. cropped photo of the {label}. bright photo of {label}. drawing of {label}. close-up photo of {label}. pixelated photo of the {label}. plastic {label}. photo of the {label}. photo of one {label}. the origami {label}. an origami {label}. photo of the clean {label}. photo of weird {label}. sketch of the {label}. jpeg corrupted photo of the {label}. photo of the small {label}. drawing of the {label}. dark photo of {label}. itap of my {label}. blurry photo of {label}. bad photo of {label}. photo of the {label}. high contrast photo of the {label}. photo of the big {label}. black and white photo of {label}. good photo of {label}. blurry photo of the {label}. bad photo of the {label}. low contrast photo of {label}. photo of small {label}. black and white photo of the {label}. good photo of the {label}. painting of {label}. tattoo of {label}. cartoon {label}. art of {label}. photo of the {label}. sketch of the {label}. the embroidered {label}. the origami {label}. doodle of the {label}. plastic {label}. toy {label}. {label} in video game. graffiti of {label}. painting of the {label}. tattoo of the {label}. the cartoon {label}. art of the {label}. sculpture of {label}. rendition of {label}. plushie {label}. drawing of {label}. the plastic {label}. the toy {label}. the {label} in video game. graffiti of the {label}. photo of the {label}. photo of my clean {label}. photo of my {label}. photo of my new {label}. love my {label}! photo of my old {label}. photo of {label} pattern. photo of the {label} pattern. photo of {label} thing. photo of the {label} thing. photo of {label} object. photo of the {label} object. photo of the {label}, type of aircraft. photo of the {label}. photo of many {label}. rendering of {label}. tattoo of {label}. photo of clean {label}. photo of my {label}. black and white photo of the {label}. sculpture of the {label}. photo of the dirty {label}. good photo of the {label}. doodle of {label}. the {label} in video game. low resolution photo of {label}. photo of large {label}. blurry photo of {label}. embroidered {label}. good photo of {label}. photo of the weird {label}. photo of the large {label}. itap of {label}. photo of cool {label}. sculpture of {label}. graffiti of {label}. the embroidered {label}. photo of dirty {label}. the plastic {label}. painting of the {label}. bright photo of the {label}. jpeg corrupted photo of {label}. rendering of the {label}. close-up photo of the {label}. sketch of {label}. the toy {label}. rendition of {label}. cartoon {label}. pixelated photo of {label}. plushie {label}. the cartoon {label}. black and white photo of {label}. graffiti of the {label}. photo of small {label}. photo of the hard to see {label}. bad photo of the {label}. photo of hard to see {label}. dark photo of the {label}. photo of the cool {label}. painting of {label}. cropped photo of {label}. blurry photo of the {label}. {label} in video game. photo of {label}. doodle of the {label}. rendition of the {label}. photo of nice {label}. art of {label}. itap of the {label}. photo of the nice {label}. art of the {label}. the plushie {label}. toy {label}. tattoo of the {label}. A.4 LIMITATIONS AND FUTURE WORKS. Training Efficiency. In our proposed progressive alignment framework, the additional computational overhead in the first stage is relatively small. However, in the second stage, the vision encoder needs to be unfrozen for training, and self-distillation is performed online, which increases the computational cost. In our experiments, the training speed is approximately 0.74 that of the baseline. We consider the following directions to potentially reduce computational overhead: Adopting PEFT-based approach to fine-tune the vision encoder in the second stage Fine-tuning only part of the vision encoder parameters in the second stage, such as the last few Transformer blocks Replacing online distillation with offline distillation, which would substantially reduce the additional computational cost introduced in the second stage Fine-grained Visual Alignment. Our proposed ProCLIP is still based on contrastive learning over global semantics. However, aligning local vision patches with textual semantics can enhance the local perception capabilities of the vision encoder, benefiting tasks such as open-vocabulary segmentation and open-vocabulary detection. Further discussions on this topic can be found in works like SigLIP2 (Tschannen et al., 2025) and FG-CLIP (Xie et al., 2025). More Model Architecture. Our approach replaces the original CLIP text encoder with an LLMbased embedder to enhance multiple capabilities. From another perspective, can we similarly replace the vision encoder in the dual-tower architecture to address limitations in visual representation? For example, CLIPs image encoder is known to lack localitycould this limitation be mitigated by substituting the image encoder? We plan to explore this direction further in future work."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "Beijing University of Technology",
        "DeepGlint",
        "Shanghai Jiao Tong University",
        "Tsinghua University"
    ]
}