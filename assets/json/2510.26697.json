{
    "paper_title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
    "authors": [
        "Zhichao Wang",
        "Dongyang Ma",
        "Xinting Huang",
        "Deng Cai",
        "Tian Lan",
        "Jiahao Xu",
        "Haitao Mi",
        "Xiaoying Tang",
        "Yan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly \"end-to-end\" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass. Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from \"hacking the test set\"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., \"generate with low randomness\") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 9 6 6 2 . 0 1 5 2 : r The End of Manual Decoding: Towards Truly End-to-End Language Models The End of Manual Decoding: Towards Truly End-to-End Language Models Zhichao Wang ,1,2 , Dongyang Ma,1 , Xinting Huang1 , Deng Cai1 , Tian Lan1 , Jiahao Xu1 , Haitao Mi1 , Xiaoying Tang ,2 , and Yan Wang,,1 1Tencent AI Lab 2The Chinese University of Hong Kong, Shenzhen zhichaowang@link.cuhk.edu.cn tangxiaoying@cuhk.edu.cn dongyangma@tencent.com yanwang.branden@gmail.com Code Models"
        },
        {
            "title": "Abstract",
            "content": "The end-to-end label for LLMs is misnomer. In practice, they depend on nondifferentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, novel architecture that enables truly end-to-end generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into parametric, token-level process, allowing the model to self-regulate its sampling strategy within single forward pass. Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from hacking the test seta practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., generate with low randomness) and adjusts its predicted temperature and top-p on token-by-token basis, opening new paradigm for steerable and interactive LLM decoding."
        },
        {
            "title": "Introduction",
            "content": "LLMs have become the de-facto standard in NLP, yet the quality of their generated text hinges on surprisingly manual and heuristic process: the selection of decoding hyperparameters. Parameters like temperature, top-p, and top-k must be carefully chosen through task-dependent process of manual sweeps and post-hoc filtering (Shi et al., 2024). This not only incurs significant computational and human costs but also profoundly impacts the final outputs creativity, diversity, and factual correctness, undermining the promise of truly end-to-end system. This reliance on static, hand-tuned parameters creates fundamental bottlenecks. Firstly, the search for an optimal configuration is widely acknowledged as laborious process because the ideal settings are highly task-dependent; commercial API providers like DeepSeek, for instance, explicitly recommend different temperature settings for distinct application scenarios1. However, this problem, runs even deeper: single static configuration is inherently suboptimal because the ideal level of stochasticity varies dramatically within single generation. For instance, model might need high creativity to explore initial reasoning paths but high precision to deliver the final answer. This Equal Contribution. The work was done when Zhichao Wang was interning at Tencent AI Lab. Correspondence to: Xiaoying Tang and Yan Wang. 1https://api-docs.deepseek.com/quick_start/parameter_settings 1 The End of Manual Decoding: Towards Truly End-to-End Language Models Figure 1: An overview of our proposed end-to-end decoding architecture compared to manual decoding. Our method dynamically predicts temperature and top-p values from the models hidden states for each generation step. In contrast, manual decoding (bottom) relies on single set of static, predefined hyperparameters for the entire sequence generation. on-the-fly control is, by design, impossible for current LLMs to achieve natively. Consequently, the prevailing static decoding paradigm is solution as inefficient as it is ineffective, forcing one-size-fits-all strategy onto problem that demands dynamic adaptation. In this paper, we propose AutoDeco, novel architecture that creates truly end-to-end language model capable of controlling its own decoding process. As illustrated in Figure 1, we augment the standard transformer with lightweight, dedicated prediction heads. At each decoding step, these AutoDeco heads leverage the models current hidden state to dynamically predict the optimal sampling parameters for the next token. This seamlessly integrates hyperparameter selection into the models forward pass, creating self-regulating inference pipeline that adds nearly-zero latency. We validate our approach by integrating AutoDeco into major model families, including Qwen, Llama, and GPT, requiring only brief fine-tuning process of 400 steps. Across eight distinct benchmarks, the results are striking: AutoDeco not only consistently outperforms standard default decoding settings but also matches or surpasses the performance of meticulously expert-guided tuning (an oracle-tuned baseline derived from hacking the test set) hyperparameters. Perhaps most excitingly, we uncovered an emergent capability for instruction-based decoding control. When prompted with meta-instruction like, Please ensure that the diversity of your output is low, the model immediately responded by lowering its average predicted temperature and top-p values by 0.11 and 0.06, respectively. This demonstrates that AutoDeco does not merely automate tedious process; it endows the model with new, intuitive way to interpret and act on user intent. Our contributions are four-fold: (i) We propose AutoDeco, novel and lightweight architecture, along with an efficient strategy to train its prediction heads, that makes LLM generation truly end-to-end by dynamically predicting decoding parameters at each step. (ii) We demonstrate through extensive experiments that AutoDeco consistently matches or exceeds the performance of expert-guided tuning, static hyperparameters across eight benchmarks and multiple model families. (iii) We demonstrate for the first time that an LLMs decoding can be controlled by natural language. We achieve this by observing nascent emergent ability and then solidifying it via targeted training, achieving 95%+ consistency in steering sampling behavior. (iv) In addition to the models evaluated in this paper, we also release AutoDeco heads for Deepseek-V3.1-Terminus, Qwen3-235B-A22B-Thinking-2507, and GPT-OSS-120B. However, due to the substantial computational cost of evaluating these large-scale models, comprehensive benchmark of these models was not performed."
        },
        {
            "title": "2 AudoDeco",
            "content": "The foregoing discussion raises two fundamental questions that frame the core inquiry of this work: 2 The End of Manual Decoding: Towards Truly End-to-End Language Models (a) Top-p mask function. (b) An example of top-p sampling probability. Figure 2: Comparison of the differentiable soft top-p sampling (decay steepness Î± = 30) with the standard hard-cutoff method. (a) illustrates the standard hard-cutoff mask, which has nondifferentiable step, against our proposed smooth and differentiable soft mask. (b) shows the effect of applying both masks to an example original probability distribution, where the soft mask method produces differentiable probability distribution suitable for end-to-end training. First, how can we train the AutoDeco heads without any token-level ground-truth labels for the optimal temperature and top-p values? Second, how can these predictions be integrated into inference without adding computational latency? This section details our solutions to both. In Section 2.1, we will introduce our training strategy and explain how we train both heads in an end-to-end manner. Then, in Section 2.2, we will walk through our inference process. The AutoDeco modifies the models final output probabilities internallya design that adds absolutely no extra latency. The result is model that can be used almost exactly like standard one, requiring only 1-line-change in users code to unlock its dynamic decoding capabilities. 2.1 Training Strategy The central challenge in training AutoDeco is the absence of token-level ground-truth labels for sampling parameters. natural approach would be to optimize the temperature and top-p heads directly from the final cross-entropy loss of the generated tokens. However, this path is obstructed by the standard top-p sampling algorithm. Its hard cutoffretaining only the smallest set of tokens whose cumulative probability exceeds thresholdis non-differentiable operation, which severs the gradient flow from the loss back to the top-p head. To overcome this, we introduce novel, differentiable soft top-p mechanism that is used during training, enabling fully end-to-end optimization strategy. Traditional top-p sampling methods assign probability of zero to all tokens beyond the top-p threshold, while our approach is different: for tokens that fall outside the top-p threshold, we apply differentiable weight scaling. The further token is from the threshold, the more its probability is scaled down, eventually approaching zero. Below is the training data stream: 1. Temperature-Scaled Probabilities: First, we scale the predicted logits to compute the initial probability distribution using the predicted temperature ËT. = softmax (cid:19) . (cid:18) ËT (1) 2. Differentiable Mask Generation: After sorting the probabilities and calculating their cumulative sum c, we generate soft mask m(sorted). This is done in single step that combines 3 The End of Manual Decoding: Towards Truly End-to-End Language Models the thresholding and decay logic: m(sorted) = exp (cid:0) Î± ReLU(c ËP)(cid:1), (2) Here, Î± is hyperparameter controlling the decays steepness. As shown in Figure 2a, this formulation ensures that for tokens inside the nucleus (where < ËP), the ReLU term is zero, resulting in mask value of 1. For tokens outside, the mask value smoothly decays towards zero as their cumulative probability further exceeds ËP. 3. Final Probability Distribution: The soft mask (unsorted to match the original vocabulary order) is applied to the initial probabilities, and the result is re-normalized to form the final, differentiable distribution p: = (p m) + Ïµ , (3) where Ïµ is small constant for numerical stability. In Figure 2b, we provide an example with vocabulary size of 50 to illustrate how the models predicted probability distribution changes after the application of our soft top-p sampling. As the probability of the token exceeding ËP decreases gradually and differentially, the soft top-p sampling becomes the final piece of the puzzle for the AutoDecos end-to-end training. Training With this differentiable pipeline, our training strategy becomes straightforward endto-end optimization. The standard cross-entropy loss is computed between the final distribution and the ground-truth token y. Because the entire process is differentiable, gradients from the loss are backpropagated to simultaneously update the parameters of both the temperature and top-p heads, allowing the model to learn its own optimal, context-specific decoding strategy by directly optimizing for the final task objective. Theoretically, these two heads could be trained from the pre-training stage. However, in this paper, we build upon pre-trained LLM, freezing its base parameters and solely training the AutoDeco heads. While training these heads on SFT data provides strong baseline, we find that applying some certain de-biasing operations to the data can further enhance model performance and robustness: Easy-Token Masking. For many tokens, the base models greedy prediction already matches the ground-truth. These easy tokens often yield an optimal temperature ËTt near zero, biasing the head to be overly conservative. To mitigate this, we randomly mask the training loss for large fraction (e.g., 60%) of these positions, forcing the model to learn from more challenging and informative examples. Dynamic Fine-Tuning. Conversely, naive fine-tuning approach can cause the temperature head to predict unexpected large values for uncertain tokens. We incorporate Dynamic FineTuning (Wu et al., 2025), which re-weights the training loss to focus on tokens where the model has reasonable prior. This teaches the head to apply high temperatures more judiciously in situations of calibrated uncertainty, rather than being skewed by outlier signals. 2.2 Inference: Dynamic Decoding At the heart of AutoDeco lies design optimized for efficiency. By seamlessly integrating all dynamic adjustments into the models standard forward pass, it avoids any separate, costly computational steps. This architecture results in negligible latency overhead, typically adding only 1-2% to the total generation time. As illustrated in Figure 1, the process for each token generation step is as follows: 1. Compute Hidden State: The base LLM computes the final hidden state ht. 2. Predict Decoding Parameters: In parallel, the standard lm head computes the logits while the AutoDeco heads predict the dynamic parameters. The temperature is predicted directly from the 4 The End of Manual Decoding: Towards Truly End-to-End Language Models hidden state. Crucially, the top-p head then uses both the hidden state and the just-predicted temperature as input: ËTt = temp head(ht), ËPt = top-p head(ht, ËTt). (4) This micro-dependency, shown as dashed arrow in Figure 1, allows for more nuanced interplay between the two parameters. 3. Internal Probability Modification: The model immediately uses the predicted ËTt and ËPt to internally rescale and filter the logits, producing final, dynamically-adjusted distribution. Latency and Simplicity. The AutoDeco heads (simple 2-layer MLPs) add negligible computational overhead compared to the massive transformer layers. This internal architecture results in only 1-2% additional latency and makes usage incredibly simple, and ensures seamless integration, allowing an AutoDeco-enabled model to serve as drop-in replacement for its standard counterpart, requiring no modifications to the users existing generation logic."
        },
        {
            "title": "3 Experiments",
            "content": "We conduct extensive experiments to validate AutoDeco, structuring our evaluation around its core contributions to performance, efficiency, and surprising capability that emerged as byproduct. In Section 3.2.1, we demonstrate the superior performance of AutoDeco. It not only substantially outperforms standard, non-expert decoding baselines (Greedy Search and Default Sampling) but also matches or even slightly surpasses the performance of optimal static hyperparameters found through an exhaustive expert-guided tuning. Following this, in Section 3.2.2, we analyze its practical efficiency and confirm that AutoDeco introduces minimal computational burden, with marginal latency increase of 1-2% and negligible memory footprint. We present our most striking finding in Section 3.3: the emergent capability of AutoDeco to interpret natural language commands to dynamically steer its own generation style, crucial step towards more intuitive and controllable AI. 3.1 Experimental Setup Models To demonstrate broad applicability, we select representative model from three of the most popular open-source model families. All AutoDeco heads are trained on top of the official pre-trained checkpoints of these models: Llama-3.1-Nemotron-Nano-8B-v12(Bercovich et al., 2025): general-purpose model from the widely-used Llama family, developed by Nvidia (hereinafter Llama-Nemotron-8B). R1-Distill-Qwen-7B3(Guo et al., 2025): distilled model from the Qwen family developed by DeepSeek, known for its strong reasoning capabilities. Qwen3-30B-A3B-Instruct-25074(QwenTeam, 2025): An advanced MoE architecture instruct model from Qwen3. OpenAI-GPT-OSS-20B5(Agarwal et al., 2025): MoE model with 20B parameters released by OpenAI. The reasoning effort is set to medium by default. 2https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1 3https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B 4https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507 5https://huggingface.co/openai/gpt-oss-20b 5 The End of Manual Decoding: Towards Truly End-to-End Language Models Table 1: Pass@1 accuracy on mathematical reasoning benchmarks. AutoDeco consistently outperforms both greedy Search and Default Sampling methods across various models. Model Method AIME BRUMO25 HMMT25 BeyondAIME Average Llama-Nemotron-8B R1-Distill-Qwen-7B Qwen3-30B-A3B-Instruct-2507 OpenAI-GPT-OSS-20B Greedy Search Default Sampling AutoDeco (Ours) Greedy Search Default Sampling AutoDeco (Ours) Greedy Search Default Sampling AutoDeco (Ours) Greedy Search Default Sampling AutoDeco (Ours) 51.67 50.841.44 55.431.22 38.33 43.491.38 47.031.12 65.00 67.921.36 68.460.76 56.67 69.611.32 72.331.20 56.67 57.891.46 60.600. 43.33 49.011.10 51.641.05 63.33 66.021.11 67.210.87 66.67 67.031.12 68.250.58 26.67 29.821.60 33.981.14 16.67 22.320.83 24.140.39 36.67 43.881.28 43.731. 46.67 44.242.28 46.211.08 35.00 31.820.40 34.190.65 24.00 24.210.79 26.650.49 44.00 46.380.47 46.750.18 36.00 45.690.13 45.720.44 42.50 42.59 46. 30.58 34.76 37.37 52.25 56.05 56.54 51.50 56.64 58.13 Datasets The models are trained on focused domain and evaluate on wide range of tasks to test for generalization. Training Data: The AutoDeco heads are trained on specialized dataset of reject sampling trajectories. These trajectories were generated by sampling solutions from our four base models on problems from the DeepMath-103K dataset (He et al., 2025)6. Evaluation Benchmarks: We evaluate on diverse suite of eight benchmarks, split into two categories to assess both in-domain and out-of-domain performance: In-Domain (Math): AIME (24+25), BRUMO25, HMMT25 (Balunovic et al., 2025), and BeyondAIME (ByteDance-Seed, 2025).7 Out-of-Domain (General Tasks): GPQA-Diamond (Rein et al., 2024) and MMLU-Pro (Wang et al., 2024) (QA) , LiveCodeBenchV6 (Jain et al., 2024) (Code), and IFEval (Zhou et al., 2023) (Instruction Following). Baselines and Evaluation We evaluate AutoDeco against two standard, non-expert decoding strategies: Greedy Search and Default Sampling ( ËT = 1.0, ËP = 1.0). Furthermore, to establish practical upper bound, we also compare against an Expert-Guided Tuning. It is crucial to note that this expert-tuned baseline is an oracle setting, as it involves finding the optimal static hyperparameters by tuning on the test seta process that is infeasible in real-world applications. Our primary metric is Pass@1 accuracy, estimated via oversampling with 128 samples per problem (with 8 random seeds, 16 samples per seed). 3.2 Main Results We present our main findings separately for mathematical reasoning and open-domain question answering to provide clear and detailed view of AutoDecos performance across different domains.. 3.2.1 Performance In-Domain Performance. As shown in Table 1 AutoDeco consistently demonstrates performance boost compared to Greedy Search and Default Sampling. For instance, on Llama-Nemotron-8B, it achieves an average score of 46.05, substantial improvement of nearly 3.5 absolute points over Default Sampling and Greedy Search. 6https://huggingface.co/datasets/zwhe99/DeepMath-103K 7We focus on these recent, hard benchmarks to mitigate the risk of data leakage issues in older datasets. 6 The End of Manual Decoding: Towards Truly End-to-End Language Models Table 2: Pass@1 accuracy on general-domain benchmarks. AutoDeco shows exciting generalization performance across General QA, Code Generation, and Instruction Following tasks. Model Method GPQA-Diamond MMLU-Pro LiveCodeBenchV6 IFEval Average Llama-Nemotron-8B R1-Distill-Qwen-7B Qwen3-30B-A3B-Instruct-2507 OpenAI-GPT-OSS-20B Greedy Search Default Sampling AutoDeco (Ours) Greedy Search Default Sampling AutoDeco (Ours) Greedy Search Default Sampling AutoDeco (Ours) Greedy Search Default Sampling AutoDeco (Ours) 51.01 44.93 50.52 37.87 47.41 48.91 65.86 69.82 69.96 59.60 65.67 66. 52.00 54.00 55.64 47.20 47.65 50.75 78.00 76.25 78.38 67.00 68.00 69.12 19.17 21.22 21.68 49.13 53.00 53. 47.75 48.52 49.80 69.69 70.15 71.25 71.53 65.25 71.02 32.90 32.35 33.90 83.73 81.52 82.81 29.94 30.68 30. 48.43 46.35 49.72 39.32 42.47 46.88 68.84 69.03 70.24 56.56 58.63 59.42 One may notice that the performance gain from AutoDeco is less pronounced on Qwen3-30BA3B-Instruct-2507 compared to other models. This may stem from Qwen3-30B-A3B-Instruct-2507, as non-thinking-model, produces answers that are significantly shorter than the other models. Consequently, the sensitivity of task accuracy to variations in sampling parameters is substantially lower, trend that is further demonstrated by the results in Table 2. Out-of-Domain Generalization. More strikingly, despite being trained exclusively on mathematical reasoning, AutoDeco demonstrates powerful zero-shot generalization to diverse set of out-of-domain tasks  (Table 2)  . It consistently secures the highest average scores across general QA, code generation, and instruction following. This strong performance reveals two interesting patterns. First, the magnitude of improvement is remarkably consistent across domains. For example, on R1-Distill-Qwen-7B, AutoDeco improves the average score on general tasks by 4.4 points over Default Samplinga gain even surpassing that seen in the math domain. This suggests that the benefits of dynamic decoding are fundamental and not tied to specific task type. Second, AutoDeco shows an ability to dynamically balance deterministic and stochastic strategies. On general tasks, Default Sampling is not always better than Greedy Search (e.g., on Llama-Nemotron8B for GPQA-Diamond and IFEval). In these cases, AutoDeco learns to predict more deterministic, low-temperature parameters, allowing it to match or exceed the performance of the stronger greedy baseline. Conversely, when stochasticity is beneficial, it raises the temperature to outperform Default Sampling. The above findings suggest that AutoDeco is not simply learning what to generate, but rather the fundamental meta-skill of how to generate text effectively. By training on high-signal domain like mathematics, it learns universal principles for balancing exploration and exploitation. We will further discuss this in Sec. 3.3, and this finding challenges the conventional assumption that adaptive decoding requires broad, task-matched supervision, and instead points toward more efficient, modular paradigm for real end-to-end controllable generation. Pass@k Performance. Some recent works (Yue et al., 2025; Chen et al., 2025) have highlighted potential trade-off in the training of reasoning models, where achieving superb pass@1 accuracy can come at the expense of performance on pass@k (for > 1). To investigate this, we present an extended evaluation of our method on pass@k (k = 16, 32, 64) accuracies in Appendix 8, with encouraging results. We find that the absolute improvements delivered by AutoDeco at higher k-values are consistent with, and at times even slightly greater than, those observed at pass@1. It is important to note that for any given model, pass@k accuracy is inherently much higher than pass@1 accuracy. Consequently, it is obvious that securing absolute performance gains becomes The End of Manual Decoding: Towards Truly End-to-End Language Models (a) Llama-Nemotron-8B with AutoDeco. (b) R1-Distill-Qwen-7B with AutoDeco. Figure 3: Expert-Guided Tuning Comparison with Search Interval of 0.1. Temperature is adjusted first (setting top-p to 1.0), and the selection is made based on the best performance of temperature to conduct the search for top-p. AutoDeco achieves competitive performance without requiring any prior empirical tuning or domain-specific expert knowledge. substantially more difficult, and similar absolute improvement at pass@64 translates to much larger relative error reduction, compared to pass@1. For example, on the OpenAI-GPT-OSS-20B model, we observe that the performance gains from AutoDeco are consistent across different values in pass@k evaluations. More importantly, this consistent absolute gain translates to significantly larger impact in higher-accuracy (when is large) scenarios. The relative error reduction dramatically increases from 3.5% at pass@1 to 18.1% at pass@64. This demonstrates that as the task becomes easier for the baseline model (i.e., the error rate decreases at high k), the performance gains from our method become even more significant. Comparison with Expert-Guided Tuning. In real-world applications, developers often undertake laborious tuning process to find task-specific, optimal static hyperparameters. To assess how AutoDeco compares to this best-case scenario, we simulate an expert with an unfair advantage: access to test-set oracle. As shown in Figure 3, we first perform fine-grained search to find the optimal static temperature on the test set, and then, using that temperature, find the optimal top-p. This process represents the practical upper bound for any static decoding strategy. The results are striking. AutoDecos single-pass performance is nearly identical to this oracle-tuned baseline, with the performance gap consistently less than one point across all models and datasets. Given that the Expert-Guided Tuning relies on hacking the test set, process impossible in any real-world scenario where the test data is unknown, we can confidently assert that AutoDeco is effectively superior to any feasible expert-tuning strategy in practice. Furthermore, the figure highlights the fundamental limitation of static decoding: the optimal hyperparameters are extremely task-dependent. For instance, Llama-Nemotron-8B requires drastically different settings for BRUMO25 ( ËT = 0.8, ËP = 0.9) versus GPQA-Diamond ( ËT = 0.3, ËP = 0.6). However, in real-world scenarios, model developer has no way to switch hyperparameters based on users query type. AutoDeco elegantly solves this problem. By achieving near-oracle performance automatically and on-the-fly for any task, it provides the optimal and, frankly, only practical solution for developers seeking robust, high-performance generation across diverse user inputs. 8 The End of Manual Decoding: Towards Truly End-to-End Language Models Table 3: FLOPs, Memory Usage and latency (1k tokens) across various prompt length for R1-DistillQwen-7B with/without temp head and top-p head. Metrics Method 1k 2k 4k 8k 16k 24k FLOPs Latency (s) Memory (MB) Default Sampling AutoDeco (Ours) 2.89e+13 2.89e+ 4.34e+13 4.34e+13 7.23e+13 7.24e+13 13.03e+13 13.03e+13 24.61e+13 24.62e+13 36.19e+13 36.20e+13 Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) 18.23 18.84 15546 15550 18.86 19.10 16032 16036 18.93 19. 17130 17134 19.72 20.03 19098 19102 22.11 22.36 23182 23183 25.76 26. 27262 27266 Ablation Study. natural question is what role the temperature and top-p heads play individually. To isolate their effects, we evaluate on AIME using R1-Distill-Qwen7B to conduct an ablation study, with the results presented in Figure 4. The most striking finding is the remarkable effectiveness of each component in isolation. Using either the temperature head or the top-p head alone achieves an average performance gain of approximately 3-3.5 absolute points over the Default Sampling baseline. This result is highly significant. It demonstrates that substantial improvements in decoding do not require sophisticated architecture. single, lightweight prediction head is sufficient to dramatically outperform standard static decoding methods. Figure 4: Ablation study on AutoDeco architecture designs. Joint optimization achieves the highest AIME Score. Of course, while each head is powerful on its own, our results also confirm that the full AutoDeco model, with both heads, yields the best performance. They provide complementary benefits, allowing for even finer-grained control over the generation process to achieve optimal results. 3.2.2 Efficiency critical advantage of AutoDeco is its computational efficiency. To quantify this, we evaluated its overhead against Default Sampling across three key metrics, with results summarized in Table 3. The analysis shows that the additional computational burden is minimal. The FLOPs are virtually identical to the baseline, and the memory footprint increases by mere 4 MB, an insignificant amount for modern hardware. The impact on latency is also negligible. This overhead remains consistently low regardless of prompt length, adding consistent overhead of 0.29-0.6 s/k tokens, which translates to an average relative increase of just 1.7%. These results empirically validate that AutoDeco is lightweight enhancement. When considering the substantial performance gains and the convenience of automatic, task-agnostic hyperparameter tuning demonstrated in Sec. 3.2.1, this minor computational cost becomes trivial. AutoDeco thus presents highly practical solution, offering significant benefits for negligible price. The analysis regarding training efficiency can be found in the Appendix 7. 3.3 Emergent Control of Decoding via Natural Language Beyond outperforming static methods, another interesting finding is an emergent capability: AutoDeco learns to interpret abstract, high-level commands to guide its own decoding behavior. This transforms the model from passive generator into an active participant that can respond to user intent about the desired generation style, foundational step towards truly end-to-end generation. 9 The End of Manual Decoding: Towards Truly End-to-End Language Models Figure 5: An Emergent Phenomenon. This figure shows the token-level ËT/ ËP predictions for the same prompt under three conditions, observed without any targeted training. (Left) Baseline: The models default dynamic ËT/ ËP values. (Middle) High-Diversity Command: The model spontaneously elevates its ËT/ ËP predictions. (Right) Low-Diversity Command: The model spontaneously suppresses its ËT/ ËP predictions. Table 4: Quantitative Impact of Diversity Commands After Targeted Training (N=100). Command Avg. Temp. Temp. Consistency (T) Avg. top-p top-p Consistency (P) Baseline (No Cmd) Low Diversity High Diversity 0.72 0.61 0.82 - 0.11 0.10 - 99% 96% 0.79 0.73 0.83 - 0.06 0. - 97% 85% Figure 5 provides clear qualitative demonstration of this capability. On the left, creative prompt generates dynamic but baseline set of temperature and top-p values (solid lines). In the middle panel, when we append the command, hope the answers can be more innovative and diverse, the models response is immediate and visible: the predicted ËT and ËP values (dotted lines) are consistently elevated above the baseline, effectively turning up its own creativity. Conversely, on the right, the command hope the answers can be as certain as possible causes the model to autonomously suppress its ËT and ËP predictions, turning down its randomness to favor more deterministic outputs. To our knowledge, this is the first demonstration of an LLM directly translating natural language intent for creativity and certainty into its internal sampling parameters on token-by-token basis. However, we observed that this emergent capability was not consistent, appearing only occasionally across different prompts. This motivated us to investigate whether this desirable behavior could be explicitly trained. We developed targeted training strategy, augmenting subset of prompts with diverse decoding-control commands and applying ranking loss. This loss function encourages the model to predict higher ËT and ËP values for high diversity commands relative to the baseline, and lower values for low diversity commands. After fine-tuning the model with this objective for small number of steps, we evaluated its effectiveness. The results, presented in Table 4, confirm that the training was highly successful in making the behavior robust and reliable. For example, the low diversity command prompted substantial drop in average temperature from 0.72 to 0.61 with remarkable 99% consistency across test set of 100 questions. This shows that the initial emergent connection can be developed into systematic and dependable feature through targeted training. However, we do not yet have conclusive understanding of this phenomenon. While the trained model learns the correct directional adjustments, it does not achieve precise, absolute control. For The End of Manual Decoding: Towards Truly End-to-End Language Models instance, when prompted to ensure your generation has no randomness, we observed modest but directionally correct drop in the average predicted temperature, rather than the ideal of near-zero. We hypothesize that achieving such fine-grained control may require joint training of the base LLM and the AutoDeco heads. Given the preliminary nature of this investigation, the models released with this paper do not include this experimental control feature. We will continue to advance this line of research and release updated models as soon as we reach more definitive conclusions."
        },
        {
            "title": "4 Related Works",
            "content": "The process of generating text from language model, known as decoding, is critical step that significantly influences the quality of the output (Wang et al., 2025; Shi et al., 2024). Existing decoding strategies can be broadly categorized into deterministic, stochastic sampling, and model-based approaches, most of which traditionally rely on static, predefined configurations. Deterministic Decoding Deterministic methods produce single, reproducible output for given input. The most fundamental of these is Greedy Search, which selects the token with the highest probability at each step. Another classic one is beam search, which maintains beam of most probable partial sequences to explore larger search space (Sutskever et al., 2014; Graves, 2013). However, both of them are known to favor dull, high-frequency phrases (Vijayakumar et al., 2016), this results in their good performance on Machine Translation and QA tasks, but not suitable for open-ended generation tasks. more recent line of deterministic methods, Contrastive Search(Su & Collier, 2022; Su et al., 2022), directly optimizes for open-ended generation quality by penalizing tokens that are too similar to previous tokens, effectively mitigating the degeneration problem. Stochastic Sampling To inject diversity, stochastic sampling methods are essential. These methods sample from the models output probability distribution, which is typically modulated by some hyperparameters. However, unrestricted sampling can produce incoherent text. To counter this, truncation methods were developed. Top-K sampling(Fan et al., 2018) restricts the sampling pool to the most likely tokens, while the more adaptive Nucleus Sampling (top-p)(Holtzman et al.) selects the smallest set of tokens whose cumulative probability exceeds threshold p. Despite their power, as our introduction highlights, finding the optimal configuration for these hyperparameters is non-trivial, task-dependent manual process (Shi et al., 2024). Model-Based Decoding To gain more fine-grained control over generation, third category of methods modifies the models output distribution using external signals or auxiliary models. Early examples include Plug-and-Play Language Models, which leverage attribute models to steer generation towards desired topics (Dathathri et al.). More recently, Contrastive Decoding uses smaller amateur model to steer larger expert model away from generic text (Li et al., 2023; Chuang et al., 2023). Similarly, Speculative Decoding utilizes faster draft model to generate sequences of tokens that are then verified by the larger model, significantly accelerating inference (Leviathan et al., 2023; Chen et al., 2023). There is also an art to verification methods (Liu et al., 2025). While they are effective, they still operate under fixed algorithmic framework: the choice of the guidance model itself acts as another form of hyperparameter. For example, in contrastive decoding and speculative decoding, the authors suggest that using smaller LM of the same architecture as the guidance model yields the best results. Despite this rich landscape of research, fundamental limitation persists: all these methods rely on static decoding strategy. Whether its fixed algorithm (like Beam Search) or fixed set of hyperparameters, this one-size-fits-all approach is inherently suboptimal. In contrast, AutoDeco proposes paradigm shift. Instead of relying on fixed hyperparameters or predefined heuristics, we empower the model to dynamically control its own stochasticity at each generation step. The End of Manual Decoding: Towards Truly End-to-End Language Models"
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work, we challenged that the end-to-end label for LLM is misnomer. We introduced AutoDeco, truly end-to-end architecture that empowers models to dynamically control their own decoding strategy. By learning to predict token-level temperature and top-p values, AutoDeco transforms decoding from manual, static process into dynamic, self-regulating pipeline. Our extensive experiments reveal three key contributions. First, AutoDeco demonstrates remarkable generalization, consistently outperforming standard decoding methods across diverse models and tasks, even matching oracle-tuned baselines without any task-specific tuning. Second, this performance is achieved with negligible computational overhead, making it practical, drop-in enhancement for any transformer-based model. Most significantly, we discovered remarkable emergent capability: AutoDeco learns to interpret natural language commands to steer its own generation style, foundational step towards more intuitive human-AI interaction. Future Work. Our immediate future work involves jointly training the base model with AutoDeco. We believe this will address current limitations like imprecise prompt-based control and data biasesboth likely consequences of frozen backbonethereby enabling more granular control over generation. 12 The End of Manual Decoding: Towards Truly End-to-End Language Models"
        },
        {
            "title": "References",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https://matharena. ai/. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models. arXiv preprint arXiv:2505.00949, 2025. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads. https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME, 2025. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@k training for adaptively balancing exploration and exploitation of large reasoning models, 2025. URL https://arxiv.org/abs/2508.10751. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2023. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: simple approach to controlled text generation. In International Conference on Learning Representations. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889898, 2018. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint, 2024. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. 13 The End of Manual Decoding: Towards Truly End-to-End Language Models Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, and Dong Yu. Trust, but verify: self-verification approach to reinforcement learning with verifiable rewards. 2025. URL https://arxiv.org/abs/2505.13445. QwenTeam. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a In First Conference on Language Modeling, 2024. URL https://openreview.net/ benchmark. forum?id=Ti67584b98. Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. thorough examination of decoding methods in the era of llms. arXiv preprint arXiv:2402.06925, 2024. Yixuan Su and Nigel Collier. Contrastive search is what you need for neural text generation. arXiv preprint arXiv:2210.14140, 2022. Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. contrastive framework for neural text generation. Advances in Neural Information Processing Systems, 35: 2154821561, 2022. Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014. Ashwin Vijayakumar, Michael Cogswell, Ramprasath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016. Haoran Wang, Xiongxiao Xu, Philip Yu, and Kai Shu. Beyond tokens: survey on decoding methods for large language models and large vision-language models. April 2025. doi: 10.36227/techrxiv.174495300.03784996/v1. URL http://dx.doi.org/10.36227/techrxiv. 174495300.03784996/v1. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: reinforcement learning perspective with reward rectification, 2025. URL https://arxiv.org/abs/2508.05629. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. 14 The End of Manual Decoding: Towards Truly End-to-End Language Models"
        },
        {
            "title": "Contents of the Paper",
            "content": "1 Introduction 2 AudoDeco 2.1 Training Strategy . 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inference: Dynamic Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Experiments 3.1 Experimental Setup . . 3.2 Main Results . 3.2.1 Performance . . 3.2.2 Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Emergent Control of Decoding via Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Related Works 5 Conclusion and Future Work 6 Experimental Setup 7 Supplementary Discussion of Efficiency 8 Supplementary Experimental Results"
        },
        {
            "title": "6 Experimental Setup",
            "content": "1 2 3 4 5 5 6 6 9 9 11 12 16 16 Training. For the training of all models with our AutoDeco framework, we employed consistent hyperparameter configuration to ensure fair comparison. To efficiently manage memory and scale our experiments, we utilized the DeepSpeed library with the ZeRO Stage 3 optimization. The specific training settings are detailed below: Training Framework: DeepSpeed (ZeRO Stage 3) for DeepSeek-R1-Distill-Qwen-7B and Llama-3.1-Llama-Nemotron-8B-8B-Nano-v1. DeepSpeed (ZeRO Stage 2) for the MoE model Qwen3-30B-A3B-Instruct-2507 and OpenAI-GPT-Oss-20B-Oss-20B. Hardware: 8 GPUs Batch Size: per-device batch size of 1 with 4 gradient accumulation steps, resulting in an effective global batch size of 32. Optimizer: AdamW Learning Rate: 5 106. Max Token Length: 16384. Decay Steepness Î±: 30. For each task, we calculated the Pass@1 through oversampling (16 times). To ensure the results are solid, we do 8 runs on each experiment with different seeds. Datasets. Our experimental configuration is detailed as follows: MMLU-Pro: We used comprehensive and evenly distributed lite subset 8 for evaluation to ensure balanced assessment across all subject areas. LiveCodeBench: The V6 version of the dataset was used. The evaluation window for this benchmark was initiated on September 1, 2023, and included all subsequent data. 8https://huggingface.co/datasets/koiwave/100MMLUpro 15 The End of Manual Decoding: Towards Truly End-to-End Language Models Figure 6: AutoDecos training curves on all models. Training loss curve across models. The loss converges effectively, indicating resource-friendly training of AutoDeco. Others: All the others selected datasets were processed using their full sets."
        },
        {
            "title": "7 Supplementary Discussion of Efficiency",
            "content": "Training Efficiency. Given AutoDecos superior decoding performance and minimal deployment overhead, natural question arises: What is the cost of endowing language model with this adaptive decoding capability? Remarkably, the answer is negligible. AutoDeco is resource-efficient, general-purpose solution for adaptive decoding optimization. Our experiments reveal two key practical advantages: Label-free supervision: AutoDeco eliminates the need to pre-compute or invoke any external optimization modules to generate supervision signals (e.g., temperature or top-p labels) for fine-tuning. Data efficiency: We show the training curves of all models in Figure 6, and AutoDeco achieves strong performance within about only 6K training samples and 400 steps, making it effortlessly be integrated into any pre-trained LLMs."
        },
        {
            "title": "8 Supplementary Experimental Results",
            "content": "Table 5: Pass@16 accuracy on mathematical reasoning benchmarks. Comparing Default Sampling with the AutoDeco method. Model Method AIME BRUMO25 HMMT25 BeyondAIME Average Llama-Nemotron-8B R1-Distill-Qwen-7B Qwen3-30B-A3B-Instruct-2507 OpenAI-GPT-Oss-20B Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) 77.47 80.31 71.16 73.86 87.53 88.44 91.42 91.48 82.46 84. 74.83 79.03 87.47 89.11 92.27 92.82 82.46 84.18 49.79 57.48 64.86 64. 74.91 81.91 82.46 84.18 53.16 54.54 69.32 69.08 77.02 75.60 81.21 83. 62.24 66.23 77.30 77.78 83.91 85.45 16 The End of Manual Decoding: Towards Truly End-to-End Language Models Table 6: Pass@32 accuracy on mathematical reasoning benchmarks. Comparing Default Sampling with the AutoDeco method. Model Method AIME BRUMO25 HMMT25 BeyondAIME Average Llama-Nemotron-8B R1-Distill-Qwen-7B Qwen3-30B-A3B-Instruct-2507 OpenAI-GPT-Oss-20B Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) 80.65 84.02 73.69 76.89 89.07 90.00 92.91 93.55 85.74 87.24 77.57 81. 89.96 91.68 93.84 95.41 85.74 87.24 57.74 65.58 68.41 66.09 80.54 87. 85.74 87.24 58.44 59.65 73.65 73.02 81.03 80.92 84.47 86.44 66.86 70. 80.27 80.20 87.08 89.23 Table 7: Pass@64 accuracy on mathematical reasoning benchmarks. Comparing Default Sampling with the AutoDeco method. Model Method AIME BRUMO25 HMMT25 BeyondAIME Average Llama-Nemotron-8B R1-Distill-Qwen-7B Qwen3-30B-A3B-Instruct-2507 OpenAI-GPT-Oss-20B Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) Default Sampling AutoDeco (Ours) 83.31 87.72 76.89 80.36 90.98 92. 93.96 95.17 88.70 90.42 79.95 82.48 92.20 93.11 94.99 96.67 88.70 90. 62.94 71.44 71.07 71.70 84.54 90.36 88.70 90.42 63.90 64.84 77.50 77. 85.24 84.00 87.35 89.75 70.92 74.78 82.94 83.61 89.68 91."
        }
    ],
    "affiliations": [
        "Tencent AI Lab",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}