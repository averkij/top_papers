{
    "paper_title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
    "authors": [
        "Peizhou Huang",
        "Zixuan Zhong",
        "Zhongwei Wan",
        "Donghao Zhou",
        "Samiul Alam",
        "Xin Wang",
        "Zexin Li",
        "Zhihao Dou",
        "Li Zhu",
        "Jing Xiong",
        "Chaofan Tao",
        "Yan Xu",
        "Dimitrios Dimitriadis",
        "Tuo Zhang",
        "Mi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 1 ] . [ 1 6 4 3 2 1 . 1 0 6 2 : r MMDeepResearch-Bench: Benchmark for Multimodal Deep Research Agents Peizhou Huang*, Zixuan Zhong*, Zhongwei Wan*, Donghao Zhou*, Samiul Alam, Xin Wang, Zexin Li, Zhihao Dou, Li Zhu, JingXiong, Chaofan Tao, Yan Xu, Dimitrios Dimitriadis, Tuo Zhang, Mi Zhang * Equal Contribution OSU, Amazon, UMich, UCL, CUHK, UCR, CWRU, HKU Correspondence: Tuo Zhang tuozhang@amazon.com, Mi Zhang mizhang.1@osu.edu Project Page: https://mmdeepresearch-bench.github.io"
        },
        {
            "title": "Abstract",
            "content": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an imagetext bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose unified, interpretable evaluation pipeline: FormulaLLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for textvisual integrity, each producing fine-grained signals that support error diagnosis beyond single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains key bottleneck for deep research agents."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in foundation models have driven shift from language-centric systems to large multimodal models (LMMs) that jointly process text and visual inputs [1]. Enabled by visionlanguage pretraining and instruction tuning [24], modern LMMs can reason over structured visual artifacts such as charts and documents, forming the basis of current visionlanguage benchmarks [58]. Yet static models remain limited by fixed parametric memory, motivating retrieval-augmented generation [9] and tool-using agents that browse and collect external evidence [1012]. Building on this paradigm, Deep Research Agents (DRAs) target open-ended, long-horizon tasks by iteratively retrieving sources, reconciling hypotheses, and producing research-style reports [1315]. Since real research is rarely text-only, DRAs must also align textual claims with figures, charts, and diagrams, motivating multimodal deep research [1517]. 1 Figure 1 Overall MMDR-Bench score (0100; higher is better) on 140 tasks for representative tool-using LMMs and Deep Research systems, ranked by score. As DRAs proliferate [16, 17], evaluation becomes crucial but difficult: intermediate reasoning and retrieval are opaque, and openended questions seldom admit single gold answer, making the final cited report the primary evaluation interface [17]. Existing benchmarks either isolate web or retrieval competence [10, 11], focus on text-only deep research reports [15, 16], or emphasize shorthorizon multimodal perception [8, 18]. Live retrieval further complicates evaluation via issues such as search-time data contamination [19]. This leaves gap: unified benchmark for end-to-end deep research with multimodal sources. Figure 2 MMDR-Bench evaluates multimodal deep research abilities at both integrated and atomic levels. To fill this gap, we introduce MMDeepResearch-Bench (MMDR-Bench), 140-task benchmark that targets long-horizon research workflows with both textual and visual evidence. Each task instance is packaged as an imagetext bundle, to jointly evaluate the integrated deep-research capabilities and their atomic foundations, spanning multimodal understanding, citation-grounded reasoning, and long-form multimodal report synthesis capabilities of DRAs, as illustrated in Figure 2. MMDR-Bench includes two complementary regimes, Daily and Research, reflecting lightweight everyday usage and analysis-heavy research settings. All tasks are iteratively refined by doctoral-level domain experts to ensure multimodal necessity and verifiability. Building on MMDR-Bench, we develop an evaluation framework along three aspects: FormulaLLM Adaptive Evaluation (FLAE) for long-form report quality, Trustworthy RetrievalAligned Citation Evaluation (TRACE) for citation-grounded support and source quality, and Multimodal SupportAligned Integrity Check (MOSAIC) for textimage consistency of the report. The evaluation results are shown in Figure 1. 2 The contributions of our work are summarized as follows: MMDR-Bench: Novel Multimodal Research Benchmark. We introduce MMDR-Bench, the first end-to-end benchmark specifically designed to evaluate Deep Research Agents (DRAs) in multimodal settings. It comprises 140 expert-crafted tasks spanning 21 diverse domains. These tasks are organized into two complementary regimesDaily and Researchto reflect the complexity of both casual information seeking and information-dense technical analysis. Every task was iteratively refined by doctoral-level experts to ensure multimodal necessity and verifiability. Unified Multi-Stage Evaluation Framework. We propose comprehensive evaluation pipeline consisting of three specialized modules to assess the multifaceted nature of research reports. This framework includes FLAE (Formula-LLM Adaptive Evaluation) for measuring structural and insightful report quality, TRACE (Trustworthy Retrieval-Aligned Citation Evaluation) for auditing citation-grounded reasoning, and MOSAIC (Multimodal Support-Aligned Integrity Check) for verifying the consistency between textual claims and visual artifacts. Enforcement of Faithfulness via Visual Evidence Fidelity. Within the evaluation pipeline, we introduce Visual Evidence Fidelity (VEF), rigorous metric that enforces strict alignment between an agents claims and the provided visual evidence. By implementing hard PASS/FAIL constraint thresholded against task-specific textualized visual ground truth, VEF ensures that agents are held accountable for misinterpreting critical visual data or generating hallucinations. Systematic Evaluation and Open-Source Contribution. We conduct an extensive evaluation of 25 stateof-the-art LLMs and agentic systems, encompassing single-modal baselines, web-enabled models, and specialized agents. Our findings reveal persistent trade-offs between writing quality, citation discipline, and multimodal grounding. To foster reproducibility and further community development, we publicly release the full benchmark dataset, evaluation source code, and comprehensive metrics."
        },
        {
            "title": "2 Related Work",
            "content": "Deep Search and Agentic Reasoning. Early agentic search frameworks decompose queries into sequential subtasks via chain-of-thought reasoning [14, 20]. Recent large reasoning models further introduce explicit phases of exploration and self-correction. Methods such as Search-R1 [21] and DeepDive [22] leverage reinforcement learning to improve search trajectories and query refinement. However, evaluation in this line of work still largely emphasizes final answer accuracy against ground-truth labels, which may overlook failures in the underlying research process. Multimodal Search and Reasoning. To support web-scale, visually grounded information seeking, recent multimodal agents aim to interpret heterogeneous content beyond plain text. Benchmarks such as MMSearch [23] and BrowseComp-Plus [24] evaluate capabilities including visual re-ranking and image-conditioned reasoning [25]. Most existing settings emphasize whether an agent can locate the correct image or answer localized question, but rarely test whether subtle visual details are correctly used to substantiate claims in long-form research report. Benchmarks and Evaluation. Designing deep research benchmarks requires both task realism and evaluability. BrowseComp [26] and BrowseComp-Plus [24] improve fairness via fixed corpora that reduce sensitivity to web drift, while DeepResearch Bench [15] and DeepScholar [16] examine long-form synthesis and report writing. Yet core gap persists: text-only deep research benchmarks do not test multimodal evidence use, and multimodal benchmarks typically focus on short-form QA. Our work targets this intersection by introducing protocol for multimodal search and citation-grounded report generation."
        },
        {
            "title": "3 Dataset Collection",
            "content": "We define multimodal deep research as tasks that require multi-round web browsing, evidence gathering, and report synthesis while explicitly interpreting and using provided images. Following this definition, MMDRBench targets realistic long-horizon cases that require multimodal understanding and evidence-grounded 3 Figure 3 Task distribution of MMDR-Bench. Figure 4 Two example tasks from MMDR-Bench. report writing. Each task is an imagetext bundle: textual query paired with small set of images that must be interpreted and integrated into research report. This design jointly evaluates (i) multimodal question understanding and (ii) evidence-grounded multimodal report generation with citations. With large query pool (98k+ real queries) that we collected, we construct MMDR-Bench as 140 tasks across 21 domains (Figure 3), organized into two regimes: Daily (40 tasks across 10 domains) with casual visuals such as screenshots and UI captures, and Research (100 tasks across 9 domains) with structured, information-dense figures such as charts, tables, and diagrams that require deeper synthesis. Two example tasks are shown in Figure 4."
        },
        {
            "title": "4 Evaluation Methodology",
            "content": "Our evaluation framework for multimodal Deep Research agents assesses both multimodal information retrieval through multi-round search and the quality of the generated research report. To this end, we develop an evaluation pipeline consisting of three complementary components: FLAE, TRACE, and MOSAIC. The workflow of MMDR-Bench is shown in Figure 5. The pipeline runs sequentially: for each Deep Research report, we compute FLAE and TRACE in parallel, and trigger MOSAIC only when the text layer derives valid non-zero score. Let SF and ST denote the FLAE and TRACE scores, and let τF and τT be the corresponding gating thresholds. The MOSAIC evaluation activates if and only if SF τF and ST τT. If MOSAIC is not activated, we set its score to zero."
        },
        {
            "title": "4.1 FLAE: Formula-LLM Adaptive Evaluation",
            "content": "Evaluating long-form Deep Research reports is challenging because writing requirements vary across tasks and domains. Fixed rubrics often underfit this diversity, while LLM-as-a-judge is harder to audit. We introduce FLAE (FormulaLLM Adaptive Evaluation), which combines reproducible formula score from text features and task-aware LLM judge score, then fuses them with an adaptive coefficient for interpretability. FLAE evaluates each report on three task-agnostic dimensions: Readability (Read.), Insightfulness (Insh.), and Structural Completeness (Stru.). We use Judge LLM to generate task-adaptive weights over the three dimensions, improving robustness across heterogeneous tasks (details and justification in Appendix A.2.1). Formula-Based Channel. We extract lightweight statistics ϕ(R) such as lexical diversity, section structure, sentence-length distribution, and compliance indicators and compute per-dimension scores via fixed, auditable transforms: sform (R) = fd (cid:0)ϕ(R)(cid:1), D, (1) where = {Read., Insh., Stru.}. This channel is fully reproducible and provides stable evaluation without access to any judge model. Full feature definitions and the complete fixed formulas fd() are provided in Appendix A.3.2. 4 Figure 5 The MMDR-Bench evaluation pipeline. Reports are processed through parallel FLAE and TRACE modules, followed by gated MOSAIC stage. LLM Judge Channel. Given the task and report, judge LLM with calibrated prompts outputs per-dimension scores over the three FLAE dimensions. The Judge prompts (dimension scoring, task-adaptive weighting) are in Appendix A.3.3. Adaptive Fusion. We combine the two channels with judge LLM calculation. To mitigate bias, the fusion weights depend only on model-agnostic, directly observable signals such as length, section presence, and formatting compliance, not on model identity. The prompt templates for adaptive fusion are detailed in Appendix A.3. In the calculation of overall score, considering that tasks emphasize dimensions differently, we derive taskspecific weights Wd(t, R) (normalized to (cid:80) Wd = 1) and compute: (cid:88) FLAE(t, R) = 100 Wd(t, R) sd(R), (2) where is the task, sd(R) [0, 1] is the fused per-dimension score after combining the formula channel and the LLM-judge channel, and the factor 100 scales the weighted average to 0100 score. dD"
        },
        {
            "title": "4.2 TRACE: Trustworthy Retrieval-Aligned Citation Evaluation",
            "content": "To assess whether report is verifiably grounded in cited sources and whether it faithfully addresses the intended (multimodal) task, we introduce TRACE (Trustworthy Retrieval-Aligned Citation Evaluation). TRACE measures evidence quality along with citation fidelity, which is to check whether the cited claims are supported by the referenced content, and prompt fidelity, which is to verify whether the report correctly interprets and answers the question. Given report, we parse citation markers section to map indices to URLs, then extract atomic claims and align them to their cited URL(s) to form claim-URL pairs with light deduplication. For each cited URL, we retrieve the referenced content and record accessibility. For accessible pages, Judge LLM checks whether each ClaimURL pair is supported, accounting for missing evidence, contradiction, over-specificity, and causal inversion. Pair-level judgments are aggregated into three citation-fidelity metrics in [0, 1]: Consistency (Con.), Coverage (Cov.), Textual Fidelity (Fid.),and Visual Evidence Fidelity (Vef.), which penalizes evidence-mismatched reasoning. Notably, We introduce Vef. as strict prompt-faithfulness check that verifies alignment to task-specific textualized visual requirement. The detailed prompt for judge LLM is presented in Appendix A.3.6. TRACE uses Judge-LLM task-adaptive weights over {Con., Cov., Fid.}, while keeping the Vef. share fixed and treating Vef. as strict PASS/FAIL constraint against task-specific textualized visual ground truth: the judge returns 0-10 score and PASS/FAIL, and we force FAIL when the score is below threshold τVef. = 6, ensuring prompt-faithfulness is enforced consistently across task regimes (see Appendix A.1.2 and Appendix A.3.6 for more details). Let = {Con., Cov., Fid.}, and we calculate the final TRACE score as: TRACE(t, R) = 100 (cid:104) λVef. Vef(t, R) + (1 λVef.) (cid:88) (cid:105) Wk(t, R) Ek(R) , (3) kK where λVef. is fixed weight and Wk(t, R) are task-adaptive weights with (cid:80) Wk = 1."
        },
        {
            "title": "4.3 MOSAIC: Multimodal Support-Aligned Integrity Check",
            "content": "To evaluate whether multimodal deep-research report is visually grounded, i.e., whether its image-referenced statements faithfully reflect the underlying figures, charts, diagrams, and photos, we introduce MOSAIC (Multimodal Support-Aligned Integrity Check). MOSAIC enables multimodal verification as an item-level consistency test between textual claims that reference visual artifacts and the referenced images themselves. Multimodal Itemization and Routing. Given generated report R, MOSAIC first constructs grounding map from visual mentions to concrete image artifacts. We parse the report to extract multimodal items (MM-items), including inline image blocks and visually grounded paragraphs that cite an image URL. MM-items span heterogeneous visual modalities to diagrams, data charts to photos. Judging statistical chart differs from judging natural photo. MOSAIC therefore routes each item into small set of visual types using lightweight router that combines rule-based cues with optional embedding-based classification. Each routed bucket is evaluated by type-specific multimodal judge that uses consistent rubric but modality-appropriate checks such as numeric plausibility for charts, structural correspondence for diagrams, semantic grounding for photos. Multimodal Support Scoring. For each MM-item i, the judge produces vector mi of dimension scores in [0, 1] across three dimensions: Visual-Semantic Alignment Sem., Visual Data Interpretation Accuracy Acc., and Complex Visual Question Answering Quality vqa. The item-level score is then computed by weighted aggregation: (cid:88) si = ωk mi,k, (cid:88) ωk = 1. (4) MOSAIC uses LLM-based routing type-specific weighting for multimodal signals to better match different visual evidence types, with full weighting settings and justification in Appendix A.2.1. 6 Model Overall FLAE TRACE MOSAIC Read. Insh. Stru. Vef. Con. Cov. Fid. Sem. Acc. VQA OpenAI o3-mini DeepSeek-V3.2 Kimi K2 (Thinking) Qwen 3 235B (A22B) Qwen 3 VL 235B (A22B) GPT-4o GPT-4.1 GPT-4.1 mini GPT-4.1 nano GPT-5 mini GPT-5.1 GPT-5.2 Grok-3 Grok-4 (Fast Reasoning) Claude 4.5 Haiku Claude 4.5 Sonnet Claude 4.5 Opus Gemini 2.5 Flash Gemini 2.5 Pro Gemini 3 Flash Gemini 3 Pro 31.96 43.71 36.91 36.04 35.08 28.62 36.95 34.23 28.07 38.49 32.69 32.76 29.89 36. 33.67 33.61 33.84 38.40 38.04 44.43 44.68 Single-Modal, w/o Search 53.75 75.37 71.34 77.56 52.65 37.11 13.57 28.45 33.74 48.35 15.47 90.00 12.60 87.82 58.16 19.28 33.34 45.48 18.77 42.19 83.85 12.88 9.50 77.27 47.34 17.14 23.54 24.62 27.20 42.00 90.00 85.74 54.05 17.14 35.60 45.73 22.98 20.43 53.09 4.95 Multimodal, w/o Search 77.01 52.52 79.34 71.25 49.77 70.06 79.34 69.75 75.17 60. 86.48 52.21 43.57 18.34 15.25 10.68 30.58 93.52 16.98 11.89 24.10 71.43 18.72 68.41 40.90 10.04 10.94 4.61 29.66 80.56 19.92 5.61 89.04 53.00 39.29 15.90 10.06 83.62 49.60 12.86 24.20 25.44 12.33 32.62 89.91 13.21 64.82 37.28 10.79 18.99 19.86 24.42 27.02 76.30 13.04 81.73 47.18 39.29 20.02 26.64 32.61 33.90 94.23 15.60 13.67 22.03 84.29 14.32 2.30 89.04 53.00 35.71 15.90 83.92 54.31 46.43 14.00 9.16 12.83 50.00 5.30 1.43 22.18 68.39 13.89 2.80 86.13 52.24 20.00 12.57 5.79 28.46 87.45 19.34 6.12 80.49 52.99 36.43 17.30 14.62 Multimodal, w/ Search 81.80 53.22 28.57 17.90 14.10 18.56 25.98 76.90 11.70 74.60 82.31 51.65 32.14 14.36 15.09 16.11 20.73 70.13 14.41 77.63 83.86 50.70 35.00 30.64 41.14 21.97 21.30 77.21 14.75 77.81 68.58 55.44 32.86 25.35 27.77 38.30 40.67 75.96 25.49 56.22 80.04 85.94 51.44 38.57 30.18 28.77 14.98 19.47 92.86 12.50 81.22 90.22 52.00 45.71 31.95 35.07 15.42 36.61 87.31 18.99 40.69 80.44 23.15 58.05 75.39 49.85 46.43 37.98 41.85 6.46 Deep Research Agent 62.67 40.07 12.86 25.99 30.87 24.25 20.39 93.33 20.39 Tongyi Deep Research (30B-A3B) 64.35 47.80 27.86 33.12 41.51 16.68 50.79 87.75 21.22 Perplexity Sonar Deep Research ChatGPT Deep Research (o3-mini) 11.07 27.32 73.44 21.75 4.16 63.61 37.30 29.29 10.19 Gemini Deep Research (Gemini 3 Pro) 49.41 84.53 89.56 70.86 35.71 56.17 52.84 31.29 41.29 87.54 28.45 29.02 37.55 29.50 54.27 62.29 52.40 Table 1 Overall results on MMDR-Bench. Best scores in each column are highlighted."
        },
        {
            "title": "5.1 Experimental Setup\nBenchmark and Protocol. We use Gemini-2.5-Pro as the Judge LLM for all judge-in-the-loop steps, with\ntemperature set to 0.2. The overall MMDR-Bench score is a weighted combination of the three modules:\nFLAE (20%), TRACE (50%), and MOSAIC (30%). We assign the largest weight to TRACE because\ncitation-grounded evidence quality is the most central requirement for deep research, while MOSAIC evaluates\nthe additional report-quality constraints introduced by visual evidence. FLAE is weighted lower as writing\nquality is less safety-critical and can be partially reflected by evidence and multimodal consistency. We further\napply a gated MOSAIC stage with thresholds τF = τT = 0. The unscorable cases in the evaluation process\nare handled by a reason-aware validity penalty; the taxonomy is provided in Appendix A.5.2 for reference\npurposes and ensuring reproducibility.",
            "content": "Evaluated Report Models. We evaluate diverse set of systems spanning three tiers: (i) Single-modal LLM baselines without web search function, (ii) multimodal LLM baselines without web search function, (iii) Multimodal LLM baselines with web search function and (iv) specific deep research agents. For each model, we report the overall MMDR-Bench score and metric dimensions for FLAE, TRACE and MOSAIC."
        },
        {
            "title": "5.2 Main Results and Findings",
            "content": "Table 1 reports overall performance and metric breakdowns. Gemini Deep Research ranks first overall, driven by strong evidence quality (TRACE consistency/coverage) while maintaining competitive multimodal alignment (MOSAIC). Among non-agent, web-enabled models, Gemini 3 Pro (Preview) is the strongest, and GPT-5.1/5.2 together with GPT-4.1 form close cluster with complementary strengths. We observe clear cross-metric trade-offs: GPT-4.1 achieves the best multimodal extraction accuracy (Acc.) and strong evidence fidelity (Fid.), while GPT-5.2 attains the highest Vef. score, indicating better visual grounding but not uniformly better citation discipline. Finding 1: Vision is beneficial only when it is reliable as evidence. Comparisons within the same model family such as Qwen 3 235B (A22B) vs. Qwen 3 VL 235B (A22B) show that adding vision is not monotonic win. Although their multimodal variants improve visual grounding, the unified error analysis for Vef. in Figure 6 reveals increased detail-level extraction failures from mis-reading fine-grained literals such as numerals, dates, labels and table cells, reflecting limitations in visual prompt understanding rather than language generation or reasoning. When images provide non-substitutable evidence, vision constrains premises and improves faithfulness; otherwise, noisy or auxiliary visual inputs can introduce spurious assumptions that propagate through retrieval and synthesis, degrading correctness, with failures correlating primarily with prompt-fidelity signals. The detailed failure case analysis can be seen at Appendix A.5.3. Finding 2: Multimodal alignment and citation grounding can diverge. Stronger multimodal alignment or prompt-following does not guarantee more reliable citation grounding. Contrasting single-turn models with agentic systems shows that, compared to Gemini 2.5 Pro, Gemini Deep Research improves evidence aggregation and coverage via multi-step search and cross-checking, yet the unified error analysis for Vef. in Figure 6 indicates marked rise in entity-level failures. These failures arise when entities identified correctly early become mis-attributed during later synthesis after multiple retrieval and summarization steps, especially when consolidating the overlapping sources. Finding 3: Tool use helps, but strong backbones and richer retrieval matter most. Within single-turn web-enabled families like Claude, scaling shows limited separation on MMDR-Bench and TRACE, suggesting that retrieval interaction patterns, rather than model size alone, are the primary bottleneck. At the system level, agents can amplify strong backbones but cannot replace them: Tongyi Deep Research (30B-A3B) underperforms substantially larger models, while Gemini Deep Research (Gemini 3 Pro) combines high evidence coverage with strong overall performance. We also observe that offline models can outperform some web-enabled models on coverage (Cov.), implying agent retrieval constraints limit surfaced evidence despite tool access. Figure 6 Unified failure mode analysis."
        },
        {
            "title": "5.3 Fine-Grained Domains Analysis",
            "content": "Figure 7 shows clear regime-level differences. On Daily tasks, domain performance is more volatile, and the most competitive models are those that robustly handle noisy, user-style visuals like screenshots. In this regime, Gemini 2.5 Flash and GPT-5.2 are the most consistently strong, with Claude Opus remaining competitive on recommendationand explanation-heavy categories. On Research tasks, performance separation becomes more domain-dependent. Gemini Deep Research (Gemini 3 Pro) and Gemini 3 Flash (Preview) stay strong across most research domains, while GPT-5.2 peaks on structured technical areas like Computer and Data Science. Qwen 3 VL 235B (A22B) is particularly strong on visually dense scientific domains like Environment and Energy domain, consistent with cases where charts and diagrams provide decisive evidence. 8 Figure 7 Domain-level score breakdown on MMDR-Bench, restricted to the Research regime. Domain names and abbreviations are: Other Exploratory Topics (EXP), Interdisciplinary Studies (XDIS), Humanities & Cultural Studies (HCS), Social & Policy Studies (SPS), Environment & Energy Studies (EES), Economics & Business Studies (EBS), Mathematics & Engineering (MES), Life & Health Sciences (LHS), and Computer & Data Science (CDS). Higher values indicate stronger performance."
        },
        {
            "title": "5.4 Human Consistency Check",
            "content": "Method PAR OPC 61.2 73.5 68.0 70.1 93.0 96.4 95.2 95.8 Human Inter-Annotator Agreement Vanilla Prompt Judge MMDR-Bench-Eval (Full) w/o Vef. w/o MOSAIC We evaluate alignment between our evaluator and expert judgments on open-ended multimodal reports. We use the full 140 tasks from both Daily and Research regimes and collect reports from all evaluated models. For each task, we form balanced system pairs by stratifying on overall score and tier (to avoid trivial comparisons), and sample fixed number of pairs per task. Twelve expert annotators independently assess report pairs: for each pair, three experts provide an overall preference and coarse score on the same rubric, and we aggregate by majority vote. Evaluatorhuman agreement is measured by pairwise agreement (PAR) on preferences and score correlation (OPC), where OPC is the Pearson correlation between system-level mean scores computed by averaging over tasks and sampled pairs. To reduce confounds from the evidence pipeline, we also manually audit subset of sampled pairs by spot-checking extracted ClaimURL units and their supporting snippets, confirming that observed disagreements are dominated by borderline evidence interpretation rather than systematic extraction drift. Finally, to validate that the judge-generated fusion coefficient α(t, R) contributes beyond deterministic observables, we replace α(t, R) with transparent heuristic computed from compliance signals and report the change in PAR/OPC in Appendix A.3.5 (with all fixed formula coefficients and prompts released for exact reproduction). Table 2 Human consistency on MMDR-Bench. PAR: agreement with majority expert preferences; OPC: Pearson correlation of system-level average scores. 69.8 As shown in Table 2, the full evaluator aligns more closely with expert preferences than vanilla prompt-based judge. Ablations show that both Vef. and MOSAIC improve human-aligned scoring, supporting our choice to keep Vef. as fixed-share requirement in TRACE while letting the remaining citation-fidelity weights adapt by task (full details are provided in Appendix A.2.1)."
        },
        {
            "title": "5.5 Robustness of Judge Model and Weights",
            "content": "We test robustness to judge backbones and aggregation weights via cross-judge re-scoring experiment. We fix the report set to the 140 outputs produced by Gemini-2.5-Pro and re-evaluate them with two judge backbones under the same parsing, retrieval, and aggregation pipeline. Table 3 shows that judges differ in module-level scoring tendencies, which we attribute to judge-specific inductive biases (e.g., emphasis on conservative evidence attribution versus stricter prompt-faithfulness and multimodal precision), rather than evaluator fragility. In our runs, GPT-5.2 is more stringent on FLAE and applies stricter Vef. criterion, where borderline semantic matches to the fixed visual ground truth drive most absolute variance, while MOSAIC remains 9 highly stable across judges, as shown in Table 3. Despite these per-stage shifts, the overall MMDR score is stable: the mean changes from 36.76 to 37.06, an absolute difference of 0.30 points (about 0.8% relative). This indicates that the three-stage design balances complementary signals of judge LLMs, without changing the conclusions. Metric Gemini-2.5-Pro GPT-5.2 AVG FLAE AVG TRACE (w/o Vef.) AVG Vef. AVG MOSAIC AVG MMDR (w/ Vef.) AVG MMDR (w/o Vef.) 61.89 28.39 38.57 29.44 36.76 38. 45.82 39.87 26.42 29.53 37.06 35.23 We also perturb the aggregation weights around (wF , wT , wM ) = (0.2, 0.5, 0.3) and sweep feasible integer triples. The top system and top tier remain unchanged, while ablating MOSAIC shifts rankings toward text-centric systems. Table 3 Cross-judge robustness on the same report set. GPT-5.2 is stricter on Vef., while MOSAIC is stable across judges."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work we introduce MMDR-Bench, the first comprehensive benchmark for end-to-end multimodal deep research. Building on the benchmark dataset comprising 140 tasks across 21 domains under Daily and Research regimes, we further propose unified evaluation pipeline that jointly measures report quality, citation-grounded faithfulness, and textvisual evidence consistency. Results across 25 state-of-the-art LLMs and DRAs reveal persistent trade-offs between writing quality, citation discipline, and multimodal grounding."
        },
        {
            "title": "References",
            "content": "[1] Rishi Bommasani. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [2] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. [4] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [5] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. [6] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 22632279, 2022. [7] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [8] Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, et al. Chartmuseum: Testing visual reasoning capabilities of large vision-language models. arXiv preprint arXiv:2505.13444, 2025. 10 [9] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [10] Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, et al. Widesearch: Benchmarking agentic broad info-seeking. arXiv preprint arXiv:2508.07999, 2025. [11] Yunjia Xi, Jianghao Lin, Menghui Zhu, Yongzhao Xiao, Zhuoying Ou, Jiaqi Liu, Tong Wan, Bo Chen, Weiwen Liu, Yasheng Wang, et al. Infodeepseek: Benchmarking agentic information seeking for retrieval-augmented generation. arXiv preprint arXiv:2505.15872, 2025. [12] Tian Lan, Bin Zhu, Qianghuai Jia, Junyang Ren, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, and Kaifu Zhang. Deepwidesearch: Benchmarking depth and width in agentic information seeking. arXiv preprint arXiv:2510.20168, 2025. [13] Zhengliang Shi, Yiqun Chen, Haitao Li, Weiwei Sun, Shiyu Ni, Yougang Lyu, Run-Ze Fan, Bowen Jin, Yixuan Weng, Minjun Zhu, Qiujie Xie, Xinyu Guo, Qu Yang, Jiayi Wu, Jujia Zhao, Xiaqiang Tang, Xinbei Ma, Cunxiang Wang, Jiaxin Mao, Qingyao Ai, Jen-Tse Huang, Wenxuan Wang, Yue Zhang, Yiming Yang, Zhaopeng Tu, and Zhaochun Ren. Deep research: systematic survey, 2025. URL https://arxiv.org/abs/2512.02038. [14] Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. Assisting in writing wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62526278, 2024. [15] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents, 2025. URL https://arxiv.org/abs/2506.11763. [16] Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, and Carlos Guestrin. Deepscholar-bench: live benchmark and automated evaluation for generative research synthesis. arXiv preprint arXiv:2508.20033, 2025. [17] Pranav Narayanan Venkit, Philippe Laban, Yilun Zhou, Kung-Hsiang Huang, Yixin Mao, and Chien-Sheng Wu. Deeptrace: Auditing deep research ai systems for tracking reliability across citations and evidence. arXiv preprint arXiv:2509.04499, 2025. [18] Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, and Ziwei Liu. Uni-mmmu: massive multi-discipline multimodal unified benchmark. arXiv preprint arXiv:2510.13759, 2025. [19] Ziwen Han, Meher Mankikar, Julian Michael, and Zifan Wang. Search-time data contamination. arXiv preprint arXiv:2508.13180, 2025. [20] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [21] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [22] Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, and Yuxiao Dong. Deepdive: Advancing deep search agents with knowledge graphs and multi-turn rl. arXiv preprint arXiv:2509.10446, 2025. [23] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, and Hongsheng Li. Mmsearch: Benchmarking the potential of large models as multi-modal search engines, 2024. URL https://arxiv.org/abs/2409.12959. [24] Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, et al. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent. arXiv preprint arXiv:2508.06600, 2025. [25] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 11 [26] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [27] OpenAI. o3 / o4-mini system card. https://openai.com/index/o3-o4-mini-system-card/, 2025. Official OpenAI system card for o3 and o4-mini models. [28] OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025. System card describing the GPT-5 model family. [29] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [30] DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenhao Xu, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Erhang Li, Fangqi Zhou, Fangyun Lin, Fucong Dai, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Li, Haofen Liang, Haoran Wei, Haowei Zhang, Haowen Luo, Haozhe Ji, Honghui Ding, Hongxuan Tang, Huanqi Cao, Huazuo Gao, Hui Qu, Hui Zeng, Jialiang Huang, Jiashi Li, Jiaxin Xu, Jiewen Hu, Jingchang Chen, Jingting Xiang, Jingyang Yuan, Jingyuan Cheng, Jinhua Zhu, Jun Ran, Junguang Jiang, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Kexin Huang, Kexing Zhou, Kezhao Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Wang, Liang Zhao, Liangsheng Yin, Lihua Guo, Lingxiao Luo, Linwang Ma, Litong Wang, Liyue Zhang, M. S. Di, M. Xu, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Panpan Huang, Peixin Cong, Peiyi Wang, Qiancheng Wang, Qihao Zhu, Qingyang Li, Qinyu Chen, Qiushi Du, Ruiling Xu, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runqiu Yin, Runxin Xu, Ruomeng Shen, Ruoyu Zhang, S. H. Liu, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaofei Cai, Shaoyuan Chen, Shengding Hu, Shengyu Liu, Shiqiang Hu, Shirong Ma, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, Songyang Zhou, Tao Ni, Tao Yun, Tian Pei, Tian Ye, Tianyuan Yue, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjie Pang, Wenjing Luo, Wenjun Gao, Wentao Zhang, Xi Gao, Xiangwen Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaokang Zhang, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xingyou Li, Xinyu Yang, Xinyuan Li, Xu Chen, Xuecheng Su, Xuehai Pan, Xuheng Lin, Xuwei Fu, Y. Q. Wang, Yang Zhang, Yanhong Xu, Yanru Ma, Yao Li, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Qian, Yi Yu, Yichao Zhang, Yifan Ding, Yifan Shi, Yiliang Xiong, Ying He, Ying Zhou, Yinmin Zhong, Yishi Piao, Yisong Wang, Yixiao Chen, Yixuan Tan, Yixuan Wei, Yiyang Ma, Yiyuan Liu, Yonglun Yang, Yongqiang Guo, Yongtong Wu, Yu Wu, Yuan Cheng, Yuan Ou, Yuanfan Xu, Yuduan Wang, Yue Gong, Yuhan Wu, Yuheng Zou, Yukun Li, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehua Zhao, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhixian Huang, Zhiyu Wu, Zhuoshu Li, Zhuping Zhang, Zian Xu, Zihao Wang, Zihui Gu, Zijia Zhu, Zilin Li, Zipeng Zhang, Ziwei Xie, Ziyi Gao, Zizheng Pan, Zongqing Yao, Bei Feng, Hui Li, J. L. Cai, Jiaqi Ni, Lei Xu, Meng Li, Ning Tian, R. J. Chen, R. L. Jin, S. S. Li, Shuang Zhou, Tianyu Sun, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xinnan Song, Xinyi Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Dongjie Ji, Jian Liang, Jianzhong Guo, Jin Chen, Leyi Xia, Miaojun Wang, Mingming Li, Peng Zhang, Ruyi Chen, Shangmian Sun, Shaoqing Wu, Shengfeng Ye, T. Wang, W. L. Xiao, Wei An, Xianzu Wang, Xiaowen Sun, Xiaoxiang Wang, Ying Tang, Yukun Zha, Zekai Zhang, Zhe Ju, Zhen Zhang, and Zihua Qu. Deepseek-v3.2: Pushing the frontier of open large language models, 2025. URL https://arxiv.org/abs/2512.02556. [31] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. [32] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [33] xAI. Grok: xais multimodal reasoning model. https://x.ai/blog/grok, 2024. Official description of the Grok model family. 12 [34] Anthropic. Claude: Constitutional ai models from anthropic. https://www.anthropic.com/claude, 2024. Official description of the Claude model family. [35] Gemini Team. Gemini: family of highly capable multimodal models, 2025. URL https://arxiv.org/abs/ 2312.11805. [36] Google. Gemini deep research documentation. https://ai.google.dev/gemini-api/docs/deep-research, 2025. Official documentation for Gemini Deep Research endpoints. [37] Tongyi DeepResearch Team. Tongyi deepresearch: new era of open-source ai researchers. https://github. com/Alibaba-NLP/DeepResearch, 2025. [38] Perplexity AI. Perplexity ai: Answer engine and sonar models. https://www.perplexity.ai/, 2025. Official Perplexity AI website and Sonar model family overview."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Dataset Construction Details Two Task Regimes. MMDR-Bench contains 140 expert-crafted tasks spanning 21 domains, which are organized into two complementary regimes: Daily (40 tasks across 11 domains) and Research (100 tasks across 10 domains). Daily tasks reflect everyday deep-research workflows driven by loosely structured visual inputs, such as screenshots, photographs, and user-facing interfaces, and require lightweight yet verifiable evidence use. In contrast, Research tasks span scientific and social-science domains and emphasize analysis-heavy settings with information-dense visuals, including charts, diagrams, and tables, where models must integrate multi-source evidence and synthesize it into coherent, citation-grounded reports under stricter fidelity constraints. Expert-Driven Task Proposal and Refinement. Domain experts propose candidate tasks, which we iteratively refine with clarity checks, multimodal-necessity checks (tasks must be image-dependent), and evidencegrounding checks (reports must be verifiable via citations). This process yields the final 140-task benchmark. Multimodal Packaging, Difficulty, and Multilinguality. Each task is packaged as an imagetext bundle with variable number of images. We annotate instance difficulty (easy, hard, complex) and record the task language. The benchmark is multilingual, dominated by English and Chinese, with additional languages in the long tail. A.1.1 Standardized Report Generation Protocol We prompt models to generate reports grounded in ClaimURL verifiability and inline visual support. Citations should appear immediately after factual claims, and each citation index must map to exactly one URL in References: block. For image-dependent claims, the report should embed the referenced image inline with short caption carrying the same citation index, and task-provided input images should be embedded before drawing image-dependent conclusions. We encourage diverse sources (Daily about 6 and Research about 10 when feasible) while avoiding social media and question-answering forums, and we note that Daily inputs can be noisy or partial. A.1.2 Textualized Visual Ground Truth for Vef. Vef. uses task-specific textualized visual ground truth (visual GT) that records only what is directly observable from the task-provided images, without adding any external/background knowledge. Concretely, when domain expert authors benchmark task, they additionally write concise visual GT by describing the key visual facts in the image bundle as-is. Depending on the visual type, the GT may include: salient entities/objects and their identities; explicit numbers, labels, and table entries; chart titles, axes, units, legends, and trend/ordering relations; and screenshot/UI states such as selected options, warnings, and on-screen text. This GT is intended to be minimal yet sufficient to distinguish correct visual interpretation from visually grounded hallucinations such as mis-identifying the central entity in an image. Decision Rule and Threshold. Vef. is evaluated as strict PASS/FAIL check. The judge outputs an integer score in {0, . . . , 10} together with PASS/FAIL verdict, and we apply fixed threshold τVef. = 6: any case with score < τVef. is forced to FAIL even if the raw verdict is PASS. In addition, identity-critical errors are 13 treated as immediate FAIL, including wrong visual identity and false presence, consistent with the judge instructions in Appendix A.3.6. Versioning, Drift Handling, and Regression Protection. Each task package stores its textualized visual ground truth (visual GT) together with the image bundle and task-level GT version identifier. Evaluation logs record both the task ID and the GT version, ensuring that every reported score is tied to an immutable GT snapshot. If any task image is updated or replaced, we regenerate the corresponding visual GT and bump the task-level version identifier accordingly. To prevent silent regressions across benchmark releases, we retain all prior GT snapshots and support re-scoring using the exact GT file associated with given release, so historical results remain fully reproducible even as newer GT versions are introduced. Before finalizing new release, we additionally run regression check on fixed canary set of tasks to detect shifts introduced by GT edits. Any such changes are documented together with the corresponding GT version updates. Vef. Score Semantics. Vef. is reported as pass-rate (percentage) over tasks. Let y(t, R) {0, 1} be the strict Vef. pass indicator for report on task (1 for PASS, 0 for FAIL), where PASS requires the judge score τVef. = 6 and no identity-critical violations. We compute: Vef.(D) = 100 1 (cid:88) tD y(t, R), (5) so values like 38.57 correspond to 38.57% strict pass-rate on the evaluated split. Quality Control and Calibration. We enforce strict judging instruction for Vef. and require the judge to base its decision on the provided images and the corresponding visual GT only. The judge returns binary verdict (PASS, FAIL) together with confidence score on 010 scale. We calibrate the strict prompt and decision rule using expert spot-checking on small development subset, iteratively refining the instruction until the LLM judgments match expert expectations on common visual failure modes such as incorrect entity identity, swapped labels, or incorrect numeric reading. A.2 Evaluation Metrics We score each generated report with three modules: FLAE (generation quality), TRACE (evidence and task faithfulness), and MOSAIC (image-grounded integrity). Figure 5 summarizes the evaluation pipeline: FLAE and TRACE are computed in parallel, and MOSAIC is evaluated only when multimodal items are meaningfully scorable. A.2.1 Weighting Hyperparameters Table 4 summarizes the weighting hyperparameters used by the evaluator, including task-adaptive weights and fixed shares. Rationale. Task-adaptive weights are used to maintain evaluator stability across heterogeneous tasks, allowing different task types and domains to emphasize distinct evaluation dimensions without introducing bias from single fixed weighting scheme. At the same time, the fixed Vef. share serves as consistent constraint that strictly enforces task faithfulness, ensuring that multimodal requirements specified by each task are respected regardless of domain or difficulty. This design choice also reflects the substantial annotation and engineering effort involved in maintaining reliable per-task multimodal ground truth, as well as the need for scalable and robust ClaimURL checking across large and diverse benchmark. A.3 FLAE Details: Fixed Formulas and Judge Prompts A.3.1 Text Features ϕ(R) We compute lightweight, directly observable text features ϕ(R) from the report to support fully reproducible formula channel. In practice, ϕ(R) includes lexical diversity and repetition signals, section and heading 14 Scope What is weighted Symbol Method Meaning Overall Module aggregation (wF , wT , wM ) Fixed FLAE Dimension importance Wd(t, R) FLAE Fusion coefficient α(t, R) TRACE Prompt-fidelity share λVef. TRACE Citation-fidelity breakdown Wk(t, R) MOSAIC Item weights (by type) (ωSem., ωAcc., ωvqa) Fixed Task-adaptive Weights score weights Final TRACE, and MOSAIC. over {Read., Insh., Stru.} (cid:80) Wd = 1. for FLAE, = with Task-adaptive Mixes reproducible text signals with judge scores when forming sd(R), with α [0, 1]. With wT = 0.5, setting λVef. = 0.4 yields 0.2 overall share for Vef.. over Task-adaptive Weights Fixed = {Con., Cov., Fid.} with (cid:80) Wk = 1. Default type-specific weights in Table 6. Table 4 Weighting hyperparameters used by the evaluator. Task-adaptive weights vary across tasks and reports; fixed weights encode non-negotiable priorities, notably Vef.. coverage, sentence-length statistics, and basic compliance indicators such as the presence of references section and in-body citation usage. All features are computed directly from the report text without accessing any judge model. A.3.2 Fixed Formula Channel fd(ϕ(R)) The formula channel maps ϕ(R) to per-dimension scores in [0, 1] using fixed transforms. We use σ(x) = 1/(1 + ex) and clip(x; 0, 1) = min(1, max(0, x)). Let ϕRead.(R), ϕInsh.(R), and ϕStru.(R) denote the feature subsets used for each dimension. We compute: (cid:16) sform Read.(R) = clip (cid:16) sform Insh.(R) = clip σ(cid:0)β σ(cid:0)β (cid:17) Read.ϕRead.(R)(cid:1); 0, 1 (cid:17) Insh.ϕInsh.(R)(cid:1); 0, 1 , , (6) (cid:16) sform Stru.(R) = clip σ(cid:0)β (cid:17) Stru.ϕStru.(R)(cid:1); 0, 1 . All coefficients βRead., βInsh., and βStru. are fixed constants shared across all tasks and models to keep the formula channel auditable and stable. A.3.3 LLM Judge Prompt for Dimension Scoring The judge receives task and the generated Deep Research report (t, R) and outputs per-dimension scores in [0, 1]. We use the prompt template presented in Figure 8 (line breaks are for readability). A.3.4 Task-Adaptive Weighting and Fusion Prompts FLAE uses Judge LLM to produce (i) task-adaptive dimension weights Wd(t, R) and (ii) fusion coefficient α(t, R). Both are generated per task and report. Task-adaptive Dimension Weights Wd(t, R). The Judge outputs three non-negative weights that sum to 1, reflecting how much the task should emphasize Read./Insh./Stru.. The prompt used for generating task-adaptive dimension weights is presented in Figure 9. Adaptive Fusion Coefficient α(t, R). α(t, R) [0, 1] controls the mix between formula and judge scores. We constrain α(t, R) to depend only on directly observable signals rather than model identity. The prompt used for generating task-adaptive fusion weights is presented in Figure 10. 15 SYSTEM You are careful evaluator of long-form deep research reports. Score strictly and consistently. Return only valid JSON. USER Task: {TASK_TEXT} Report: {REPORT_TEXT} Score the report on three dimensions in [0, 1]: (1) Read.: clarity, coherence, and ease of reading. (2) Insh.: depth beyond surface summary; synthesis, comparisons, or non-trivial reasoning. (3) Stru.: report completeness and organization (sections, references, and visual integration where applicable). Output JSON with keys: read, insh, stru. Figure 8 FLAE Judge prompt for per-dimension scoring. SYSTEM You set task-adaptive importance weights. Use only the task description and observable report properties. Return only valid JSON. USER Task: {TASK_TEXT} Report (for observables only, do not score quality here): {REPORT_TEXT} Produce weights over {Read., Insh., Stru.} such that they sum to 1. Output JSON with keys: w_read, w_insh, w_stru. Figure 9 FLAE prompt for task-adaptive dimension weights. A.3.5 Ablation on the fusion coefficient α FLAE fuses the formula channel and the judge channel at the dimension level. For each D, we use sd(R) = α(t, R) sform (R) + (cid:0)1 α(t, R)(cid:1) sjudge (t, R), (7) where α(t, R) [0, 1] is constrained to depend only on directly observable signals of the report. To test whether the judge-based α(t, R) is necessary (beyond transparent heuristic), we replace it with fully deterministic coefficient αdet(R) computed from ϕ(R) and compliance features, while keeping all other components unchanged (dimension scoring prompts, task-adaptive weights Wd(t, R), and aggregation). Deterministic Coefficient. We define four normalized observables in [0, 1] from the report R: length completeness L(R), heading coverage H(R), citation compliance C(R), and reference-block validity Rref (R), where larger means better formed. We then compute αdet(R) = clip (cid:16) 1 (cid:0)0.35 L(R) + 0.35 H(R) + 0.20 C(R) + 0.10 Rref (R)(cid:1)(cid:17) , (8) with clip(x) = min(1, max(0, x)). Intuitively, αdet(R) increases when the report is short, poorly structured, citation-sparse, or missing valid reference block, thereby putting more weight on the reproducible formula channel in Eq. (7). The results are summarized in Table 5. Evaluation Protocol. We rerun the evaluator on the same human-study report pairs using αdet(R) in place of α(t, R), and measure agreement with experts using the same pairwise agreement and system-level score correlation metrics as in Section 5.4. We also include two extreme controls: formula-only fusion (α = 1) and SYSTEM You set fusion coefficient α [0, 1] using only observable signals (length, section presence, formatting compliance). Do not use any model identity or external metadata. Return only valid JSON. USER Task: {TASK_TEXT} Report: {REPORT_TEXT} Choose α so that when the report is short, poorly structured, or non-compliant, the formula channel gets higher weight (larger α); when the report is complete and well-formed, rely more on judge scoring (smaller α). Output JSON: {\"alpha\": number}. Figure 10 FLAE prompt for task-adaptive fusion weights. Method (fusion for Eq. (7)) PAR OPC α(t, R) from judge (default) αdet(R) from Eq. (8) α = 1 (formula-only) α = 0 (judge-only) 73.5 72.8 64.5 71.6 96.4 96.1 92.5 96.0 Table 5 Ablation on the fusion coefficient α. We replace the judge-based α(t, R) with deterministic αdet(R) computed from observable report properties, and evaluate agreement with expert judgments using the same protocol as Section 5.4. judge-only fusion (α = 0). This ablation isolates the contribution of the judge-based α(t, R) while keeping the rest of the pipeline fixed. A.3.6 TRACE: Evidence and Task Faithfulness TRACE evaluates citation fidelity and multimodal task faithfulness, aggregating as Eq. (3). We parse citation markers and the references: block to build one-to-one index-to-URL map, extract atomic claims, and align each claim to its cited URL(s) to form claim-URL pairs. For accessible pages, judge verifies whether each claim is supported and aggregates citation fidelity into K: Con., Cov., and Fid.. We additionally compute Vef. by matching the report to textualized visual ground truth for the task, then threshold it at τVef.. Vef. Thresholding. The Vef. judge returns discrete score in {0, . . . , 10} and PASS/FAIL verdict. We use fixed threshold τVef. = 6 and force FAIL when the score is below the threshold, so the final Vef. decision is determined by stable rule rather than judge verbosity. This design makes the Vef. contribution auditable and consistent across judge backbones. Vef. Judge Prompt Template. Figure 11 shows the prompt used to score Vef. against the task-specific textualized visual ground truth and to output PASS/FAIL verdict. A.3.7 MOSAIC: Image-Grounded Integrity MOSAIC checks whether image-grounded statements match the referenced visuals and aggregates by Eq. (4). It extracts figure-linked items and routes them into visual types (photo, datachart, ocrchart, diagram). Each item is scored with formatting and integration factor fi [0, 1]; for diagrams we additionally use hallucination factor hi [0, 1]. A.4 Evaluated Model Details In this section, we provide detailed specifications for the 25 systems evaluated in our experiments. To support reproducibility, we report the API snapshot names (or version IDs) used during our testing window (December 17 SYSTEM You are STRICT QA Judge for Vef.. Use the task, the provided visual ground truth, and the report. Return only valid JSON. USER Segment: {SEGMENT} Question: {TASK_TEXT} Visual ground truth (text-form requirements): {VEF_GT} Report: {REPORT_TEXT} Rules: any wrong visual identity is FAIL; any false presence is FAIL; missing details allowed only if no wrong identities; score below 6 must be FAIL. Output JSON with keys: score, reason, verdict (PASS or FAIL). Figure 11 TRACE prompt template for Vef.. Visual type Item score si (normalized) datachart / ocrchart si = fi (cid:0)0.9 Acc.i + 0.1 VQA si = fi (cid:0)0.5 Sem.i + 0.5 VQA photo si = fi (cid:0)0.5 VQA diagram + 0.5 (1 hi)(cid:1) (cid:1) (cid:1) Table 6 Default MOSAIC item-score weights by routed visual type. 2025), along with the corresponding modality and brief notes on the intended usage mode (see Table 7). We group systems into three tiers to reflect progressively stronger tool access and orchestration capability. Tier 1 covers single-shot LLM/LMM baselines without external browsing, which isolates intrinsic reasoning, writing, and image understanding. Tier 2 includes web-enabled report generators with built-in browsing, representing mainstream tool-using LMM deployments. Tier 3 contains dedicated Deep Research Agents that explicitly orchestrate multi-step retrieval and synthesis, and thus reflect agentic search-and-write behavior beyond single response. A.5 Reliability and Reason-Aware N/A Handling A.5.1 Additional Experimental Setup and Reliability Handling We report reliability-related settings that affect score stability and effective coverage. TRACE Weighting. With the benchmark-level module weight wT = 0.5, we set λVef. = 0.4 so that Vef. contributes 0.5 0.4 = 0.2 of the overall score. The remaining 0.3 overall share from TRACE is allocated to citation-fidelity metrics (Con., Cov., Fid.) using task-adaptive weights. Vef. Score Semantics. Vef. is reported as pass-rate100 over tasks, based on strict PASS/FAIL check against task-specific textualized visual ground truth. Details of visual GT construction, QC, and versioning are provided in Appendix A.1.2. MOSAIC Gate and N/A Handling. We set MOSAIC activation thresholds to τF = τT = 0, so MOSAIC is triggered whenever FLAE and TRACE produce valid (non-zero) scores. When MOSAIC is not scorable, we record it as N/A and treat it as zero in aggregation, while the reliability impact is handled by the reason-aware validity scheme below. A.5.2 Failure Reasons and Reason-Aware N/A Weights N/A arises when stage is unscorable due to model output, evaluation pipeline issues, provider instability, or limited evidence access. Each N/A case is assigned reason and validity weight w(r) [0, 1] (higher 18 Tier System (paper) Modality/Setting API snapshot / version ID Notes gpt-o3-mini deepseek-v3.2 kimi-k2-thinking-preview qwen3-235b-a22b-2507 qwen3-vl-235b-a22b-instruct gpt-4o gpt-4.1 gpt-4.1-mini gpt-4.1-nano gpt-5-mini gpt-5.1 gpt-5.2 grok-3-1212 grok-4-fast-reasoning-beta OpenAI snapshota DeepSeek APIb Kimi APIc Qwen APId Qwen-VLd OpenAI snapshota OpenAI snapshota OpenAI snapshota OpenAI snapshota OpenAI snapshota OpenAI snapshota OpenAI snapshota xAI APIe xAI APIe claude-haiku-4.5-20251022 claude-sonnet-4.5-20251022 claude-opus-4.5-20251115 gemini-2.5-flash-002 gemini-2.5-pro-002 gemini-3-flash-preview-1215 gemini-3-pro-preview-1215 Anthropicf Anthropicf Anthropicf Gemini APIg Gemini APIg Gemini APIg Gemini APIg gemini-deep-research-1220 gpt-o3-deep-research tongyi-deepresearch-30b-a3b-v2 Tongyi DRi Perplexityj sonar-deep-research-large-1222 Google DRh OpenAI DRa Tier 1: Single-shot LLM/LMM baselines T1 T1 T1 T1 T1 T1 T1 T1 T1 T1 T1 T1 T1 OpenAI o3-mini DeepSeek-V3.2 (Base) Kimi K2 (Thinking) Qwen 3 235B (A22B) Qwen 3 VL 235B (A22B) GPT-4o GPT-4.1 GPT-4.1 mini GPT-4.1 nano GPT-5 mini GPT-5.1 GPT-5.2 Grok-3 Grok-4 (Fast Reasoning) Text/Reasoning Text Text/Reasoning Text Multimodal Multimodal Multimodal Multimodal Multimodal Multimodal Multimodal Multimodal (As listed) Reasoning Tier 2: Web-enabled report generators T2 T2 T2 T2 T2 T2 T2 Claude 4.5 Haiku Claude 4.5 Sonnet Claude 4.5 Opus Gemini 2.5 Flash Gemini 2.5 Pro Gemini 3 Flash Gemini 3 Pro Web-enabled Web-enabled Web-enabled Web-enabled Web-enabled Web-enabled Web-enabled Tier 3: Deep Research Agents (DRA) T3 T3 T3 Gemini Deep Research (Gemini 3 Pro) Agentic DR Agentic DR ChatGPT Deep Research (o3-mini) Agentic/IR Tongyi Deep Research (30B-A3B) Agentic/IR Perplexity Sonar Deep Research OpenAI model snapshots and system documentation: [2729]. DeepSeek documentation: [30]. Kimi documentation: [31]. Qwen/Qwen-VL documentation: [5, 32]. xAI Grok documentation: [33]. Anthropic Claude documentation: [34]. Gemini API snapshots / model docs: [35]. Google Gemini Deep Research: [35, 36]. Tongyi Deep Research: [37]. Perplexity Sonar Deep Research: [38]. Table 7 Evaluated systems and API snapshots used in the December 2025 testing window. Reason bucket w(r) Operational signature (from logs and artifacts) Model failure 0.0 Empty or unusable report without upstream errors; references or citation indices cannot be resolved; required input-image embeds are missing while the report makes image-dependent assertions. Pipeline failure 0.5 Model output exists, but scoring fails due to parser exceptions, schema mismatch, missing intermediate artifacts, or module crashes (router, OCR, chart reader). System or provider failure 0.8 Explicit API or infrastructure errors prevent generation or judging (rate limit, overload, timeout, connection reset, auth, misconfiguration). Data accessibility failure 0.9 Evidence assets are unreachable or non-extractable (dead links, blocks, paywalls, non-text pages, images requiring login, expired URLs). Table 8 Reason-aware N/A buckets and default validity weights. indicates less model attribution), preventing capability from being confounded with operational noise. Table 8 summarizes the reason categories and default weights. Assignment Rule. When multiple signals apply, we assign the most specific non-model reason using fixed priority order so that clear provider or accessibility issues are not misattributed to the model. The fixed 19 Priority Trigger (first match wins) 1 2 3 Any explicit API or infrastructure error in generation or judging logs URLs are well-formed but blocked, paywalled, region-restricted, removed, or yield non-extractable content within budget Internal exceptions, schema or parsing failures, missing artifacts, or module crashes without upstream API errors Unusable output or format failures remain after checks above Bucket System or provider failure Data accessibility failure Pipeline failure Model failure Table 9 Priority rules for assigning N/A reasons. priority order is summarized in Table 9. Why Reason-Aware Weights. This design preserves throughput and reliability signal without collapsing model scores due to transient outages or inaccessible assets. It is also diagnostic: stage logs and attribution rules enable reproducible failure breakdowns. Maintaining per-task textualized visual ground truth for Vef. and scalable ClaimURL verification are major annotation and engineering components, and this handling makes their operational impact explicit. A.5.3 Failure Case Analysis To facilitate qualitative error analysis of multimodal task understanding, we define failures only at the level of Visual Evidence Fidelity (Vef.). case is marked as FAIL if and only if it fails the Vef. check in TRACE, indicating an incorrect interpretation of the tasks visual requirements or an improper use of the provided images as evidence. Other metrics are not used to define FAIL but to diagnose why Vef. failed. We group Vef.-level failures into five categories: EMI (entity mis-identification), RMD (reference or mapping drift), DTE (detail or symbol extraction errors), LKC (logical or knowledge conflicts), and STO (structural or task-level omissions). Figure 6 summarizes the distribution of these failure modes, comparing (left) text-only backbone versus vision-enabled backbone and (right) base model versus its agentic deep-research system. On the left, enabling vision does not monotonically reduce failures: the vision-enabled model shows clear increase in DTE, consistent with mis-reading fine-grained literals such as small numerals, axis labels, timestamps, or table cells. These errors are often local, but can cascade by seeding an incorrect premise that retrieval and synthesis then treat as evidence-backed. On the right, the agentic system exhibits pronounced increase in EMI, indicating that longer pipelines amplify entity-level drift. common pattern is that entities are identified correctly early on, but become mis-attributed after multiple retrieval, summarization, and consolidation steps, causing the final report to bind correct evidence to the wrong referent. A.6 Examples of Two Scored Reports Please refer to the next page for two scored example reports shown in Figure 12 and Figure 13 from the Computer Science domain and the Math and Engineering domain. These reports, generated by Grok-4 (Fast Reasoning) and Gemini-2.5-Pro respectively, illustrate representative scoring outcomes under our evaluation framework. 20 Figure 12 Example scored report generated by Grok-4 (Fast Reasoning). Figure 13 Example scored report generated by Gemini-2.5-Pro."
        }
    ],
    "affiliations": [
        "Amazon",
        "CUHK",
        "CWRU",
        "HKU",
        "OSU",
        "UCL",
        "UCR",
        "UMich"
    ]
}