{
    "paper_title": "PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines",
    "authors": [
        "Reya Vir",
        "Shreya Shankar",
        "Harrison Chase",
        "Will Fu-Hinthorn",
        "Aditya Parameswaran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools. This dataset is 5x larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, we evaluated closed- and open-source models in generating relevant assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering."
        },
        {
            "title": "Start",
            "content": "PROMPTEVALS: Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines Shreya Shankar UC Berkeley shreyashankar@berkeley.edu Reya Vir UC Berkeley reyavir@berkeley.edu Harrison Chase LangChain harrison@langchain.dev 5 2 0 2 0 2 ] . [ 1 8 3 7 4 1 . 4 0 5 2 : r Will Fu-Hinthorn LangChain will@langchain.dev"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for task is challenging. In this paper, we introduce PROMPTEVALS, dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools. This dataset is 5 larger than previous collections. Using hold-out test split of PROMPTEVALS as benchmark, we evaluated closedand open-source models in generating relevant assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have become increasingly popular for various data processing tasks. An open-source tool for building LLM pipelines, developed by some of the authors, now has over 3 million weekly downloads. Its community has created thousands of specialized prompts for diverse fields like medicine, finance, and sports, leveraging LLMs impressive zero-shot and fewshot performance [1, 22, 14, 45]. common desire for developers using LLMs is to meet specific constraints on outputs, such as adhering to particular structure or qualitative criteria [27]. One approach to address this Equal contribution. Aditya G. Parameswaran UC Berkeley adityagp@berkeley.edu need is to collect large amounts of human preference data [15, 23, 49], and improve models through alignment techniques like supervised finetuning and reinforcement learning from human feedback [4, 32, 44]. However, these methods have high barrier to entry, requiring dataset collection, model fine-tuning, and serving infrastructure, which is more complex than simply manipulating prompts for LLM calls. More importantly, fine-tuning isnt supervised at the constraint level meaning that even fine-tuned LLMs often fail to consistently follow instructions that correspond to constraints in detailed prompts [17, 33, 12]. An alternative solution involves implementing developer-specified assertions on LLM outputs [27, 37, 35]. This approach typically follows two steps: first, defining binary evaluation criteria to represent the desired constraints; second, implementing these criteria as assertions to evaluate LLM outputs and resample these outputs when assertions fail. However, developing effective assertion criteria is challengingprimarily due to the complexity of defining and conceptualizing these criteria, rather than their technical implementation [21, 38]. The complexity of coming up with assertions arises due to multiple factors: criteria can differ significantly between developers due to specific data, use cases, and end-user requirements [8]; criteria must account for both user preferences and LLM-specific failure modes [38]; and developers may need to incorporate qualitative or subjective criteria that require LLMs themselves to perform the evaluation [6, 21]. To improve custom and task-specific alignment for LLM pipelines, we need an approach that can examine developers prompts and identify assertion criteria. These assertions can then be bolted onto the end of the LLM pipeline, allowing for automatic retrying of the pipeline if assertions fail. Developing such an approach requires substantial, diverse data on real-world LLM applications and their associated constraints. Fortunately, our opensource tool provides unique access to diverse set of use-cases with associated prompts. In this paper, we present PROMPTEVALS, dataset created using our unique collection of realworld LLM prompts and use-cases. This dataset consists of 2087 human-contributed prompt templates for custom tasks and 12623 comprehensive assertion criteria. PROMPTEVALS has median prompt template size of 191 tokens and is more than five times larger than previous collections [34, 52]. Our dataset and corresponding benchmark (20% of the dataset) are hosted on HuggingFace1. Using this benchmark, we evaluate GPT-4o and two open-source models, Llama 3-8b and Mistral-7b, on generating task-specific assertion criteria, and find that GPT-4o performs reasonably well out of the box, but its cost and latency to run for every prompt edit or pipeline update can be prohibitive in production environmentsespecially as prompts become increasingly complex and specialized. To address this, for our prompt engineering tools, we fine-tuned open-source models on our dataset (using Mistral-7b and Llama 3-8b architectures [16, 41]), and these models exceeded GPT-4os F1 performance in identifying desirable assertions by 20.93% on average. These fine-tuned models are made available to the community2, offering faster, more cost-effective solution for generating high-quality assertions."
        },
        {
            "title": "2 Related Work",
            "content": "This section reviews recent developments in prompt engineering, LLM evaluation methods, and assertions for LLM outputs."
        },
        {
            "title": "2.1 Prompt Engineering",
            "content": "Prompt engineering is essential for steering LLMs towards following instructions for specific tasks or bespoke applications of LLMs. Techniques like chain-of-thought and few-shot prompting improve model performance [46, 1]. Methods to learn good prompts [24, 26] or select few-shot examples [18] also contribute to this goal. Despite these advancements, LLMs can still hallucinate and make other mistakes [13, 36]. No technique ensures consistent adherence to instructions, especially in di1https://huggingface.co/datasets/reyavir/ PromptEvals 2https://huggingface.co/reyavir/promptevals_ mistral promptevals_llama and https://huggingface.co/reyavir/ idenverse production environments. Liu et al. tify constraints like output length and semantic consistency that developers want enforced, which can aiding robust assertion criteria [27]. As developers frequently iterate on prompts in integrated development environments (IDEs) or utilize codecompletion tools, the ability to quickly generate and update assertion criteria becomes crucial. The computational cost and time required to use large models like GPT-4 to generate assertion criteria for each prompt modification can significantly slow down the development process and increase operational costs. 2.2 Evaluating LLMs"
        },
        {
            "title": "Even with explicit",
            "content": "Traditional LLM evaluation compares outputs against human-generated benchmarks across tasks like coding and reasoning [2, 10, 40, 5], including specialized architectures like retrieval-augmented generation and agentic systems [54, 3, 28]. However, benchmarks often miss task-specific needs, such as conciseness or clarity [20]. Human pairwise comparison of LLM outputs improves alignment holistically but does not provide insight into specific criteria that defines good output [20, 50]. instructions provided in prompts, LLMs often fail to adhere to them consistently [44, 52, 39]. Existing benchmarks that evaluate the ability of LLMs to follow instructions are limited in scope, typically involving small set of instructions either generated by LLMs or meticulously curated by researchers [34, 52]. To address these limitations, we introduce PROMPTEVALS, comprehensive dataset that is five times larger than previous datasets. PROMPTEVALS features developer-contributed real-world prompts, often containing dozens of instructions, coupled with the corresponding assertion criteria."
        },
        {
            "title": "2.3 Assertions and LLM-based Evaluation",
            "content": "In instruction-following and constraint-following evaluations, such as those presented by Zhou et al. [52] and Rebedea et al. [35], assertion criteria are typically evaluated using code-based assertions, often implemented as functions that check whether the output matches specific patterns or requirements (e.g., using regular expressions). These code-based assertions often struggle to evaluate more nuanced or fuzzy criteria [7, 35, 37]. Recent approaches have employed LLMs themselves as judges to evaluate outputs [50, 42]. Some approaches even develop specialized judge LLMs Figure 1: Examples of criteria pairs and their semantic similarity scores. High-scoring pairs typically represent constraints that are explicitly stated or logically derived from the prompt, while low-scoring pairs often include vague, generic, or difficult-to-measure constraints. that are fine-tuned on human preference data [51, 43, 53, 25, 20]. LLM-based validators can be productionized as assertions in addition to code-based guardrails [37, 38, 27, 21]. While LLMs as judges offer scalable evaluation, they struggle to align with human preferences across diverse tasks [47]. Developing domainspecific assertions and guardrails (e.g., for education [30] or medicine [9]) is one approach, but it does not scale easily across thousands of domains and applications. Even within domains, criteria may vary; for instance, judging code conciseness differs between educational and professional settings. In another related research effort, Kim et al. developed LLM-generated evaluation criteria and fine-tuned judge LLM [19, 20], but their approach focuses on general (e.g., humorous, inspiring) rather than task-specific criteria. Our work complements this by providing assertion criteria grounded in real-world prompts and constraints, essential for production environments [27]."
        },
        {
            "title": "3 PROMPTEVALS Dataset",
            "content": "This section describes the PROMPTEVALS dataset, its construction process, and its characteristics. We begin by discussing the relevant background, then detail the datasets composition and the process for generating ground-truth assertion criteria for each prompt template in our dataset."
        },
        {
            "title": "Assertions",
            "content": "An LLM pipeline typically consists of three main components: prompt template, input data, and the LLM itself. prompt template is string that includes instructions for the LLM to perform specific task, as well as placeholders for the input datawhich will be provided at runtime. For example, template for basic summarization task might look like this: Summarize the following text in three sentences: {text_to _summarize}. Here, {text_to_summarize} is placeholder that will be replaced with actual text when the pipeline is run. LLM pipelines are designed to be flexible and reusable, capable of handling variety of different inputs for the same type of task. An assertion, in the context of LLM pipelines, is programmatic check or evaluation criterion applied to the LLMs output. For example, an assertion criterion for the summarization task might verify that the output indeed contains exactly three sentences, as specified in the prompt. This assertion could be implemented as function that counts the number of sentences in the LLMs response and returns true if the count is three (false otherwise). Developers implementing LLM pipelines care about wide variety of assertions, depending on their specific use cases and requirements. Some examples of good and bad assertion criteria for prompt template are shown in Table 1. To better understand developers needs, recent study Prompt Template (Domain: financial analysis) You are financial analyst and you are required to summarize the key insights of given numerical tables. {table} Please list important, but no more than five, highlights in the given table. Please write in professional and business-neutral tone. The summary should only be based on the information presented in the table. Good/Bad Explanation Criteria Response Length: The response should not list more than five highlights as requested. Professional Tone: The response should maintain professional and businessneutral tone throughout. No Repetition: The response should avoid repeating the same highlight or presenting the same information in different ways. Specificity: The highlights should be specific and not overly broad or generic. Grammar and Spelling: The response should be free from grammatical errors and spelling mistakes. Good Good Good Bad Bad Mentioned in the prompt, and easy to measure. Mentioned in the prompt template as rule that the output should follow. While the criterion was not explicitly mentioned in the prompt, it can be tied back to the prompt. Vague, and difficult to measure. Not uniquely relevant prompt. to the task or Table 1: Examples of Good and Bad Assertion Criteria by Liu et al. [27] interviewed 51 developers about their desired output constraints for LLMs. Based on their findings, they developed taxonomy that includes six categories of output constraints: lowlevel constraints that include structured output, multiple choice and length constraints, and high level constraints that include semantic constraints, stylistic constraints, and hallucination prevention. The complete taxonomy is presented in Table 3. We employ this taxonomy in our dataset construction process to ensure the quality and relevance of our assertions. distribution of the criteria types generated by GPT-4o is in Figure 4."
        },
        {
            "title": "3.2 Dataset Composition",
            "content": "The PROMPTEVALS dataset is derived from the LangChain Prompt Hub, publicly available, dynamic collection of prompt templates shared by members of our developer community. Developers can add prompt to the public collection via our Python package, knowing that their prompts can be run or modified by others, and browse the collection on our website. We froze snapshot of the prompt templates in May 2024 to create the PROMPTEVALS dataset: we selected prompt templates that could have one or more assertion criteria (i.e., they were not empty or trivial strings; they actually described task and included some placeholders for data). An example of prompt template that we omitted from PROMPTEVALS is: System Message: You are helpful assistant. Human Message: {input}. PROMPTEVALS includes 2087 prompt templates, their corresponding domains, and assertion criteria. The prompt templates span wide range of fields, including IT and programming, finance, healthcare, and education. To organize the prompt templates, we implemented hierarchical categorization process assisted by GPT-4o, resulting in three-level categorization system. Appendix A.1 describes this categorization process in more detail. Table 2 shows the overall distribution of the highest level domains, including the domain name, number of prompt templates with that domain, and the percentage of prompts with this domain. The top three domains represented are general-purpose chatbots, question-answering, and workflow automationthe last of which assists in automating or improving processes based on users input. For instance, one prompt in this domain is Create sequential workflow based on the users query. Create plan represented in JSON by only using the tools listed below. The workflow should be JSON array containing only the sequence index, function name and input... Tools: {tools} Only answer with the specified JSON.. 3.3 Assertion Criteria Construction Process For each prompt template in PROMPTEVALS, we generated set of ground truth criteria representing assertion criteria that developers would care about, specific to the LLM pipeline. Generating ground truth criteria followed threestep process: first step to generate initial criteria, second pass to add any criteria that might have been omitted in the first step, and third pass to remove any criteria that were incorrect, redundant, irrelevant, or difficult to validate. 1. Generate Initial Criteria: We used GPT-4o, state-of-the-art LLM, to generate an initial list of assertion criteria for each prompt template. Our prompt consisted of the following instructions: (a) We provided GPT-4o with the prompt template to be analyzed. (b) We also gave GPT-4o the taxonomy of LLM output constraints defined by Liu et al. [27] (see Section 3.1), explaining each constraint type. (c) We then instructed GPT-4o to generate assertion criteria relevant to the given prompt template, ensuring each criterion aligned with one of the constraint types from the taxonomy. GPT-4o output these criteria in JSON list format, with each assertion tagged with its corresponding constraint type. This approach ensured that the criteria were both relevant to the specific prompt template and grounded in structured framework of output constraints that developers typically care about. We call this the initial criteria. 2. Add Missing Criteria: Two authors conducted manual review of 200 criteria in total, with 50 criteria examined by both reviewers. Our review uncovered criteria in the prompt templates that were initially missed by GPT4o, averaging 1.35 missing criteria per prompt. The review process included criteria that were both unique and aligned with the taxonomy categories. To assess reliability, we calculated Cohens Kappa on the overlapping set, achieving score of 0.91indicating strong interreviewer agreement. To address these missing elements, we added verification step where GPT-4o re-examined the prompt templates to identify any explicitly stated criteria that were absent from its initial analysis. 3. Refine Criteria: In the final step, we prompted GPT-4o to refine the list by removing any criteria that were incorrect, redundant, irrelevant, or difficult to validate. Appendix A.2 details the prompts for each step. Validating the Generated Assertion Criteria. To assess the quality of our generated assertion criteria for PROMPTEVALS, we manually verified sample of 200 prompt templates generated criteria. In our verification process, we tracked, for each prompt template, how many criteria we added, and how many criteria we removed. We observed strong agreement with the LLM generated outputs, with < 0.02 criteria added and < 0.2 criteria removed per list on average by the human evaluator. This 3-step process resulted in higher agreement, in comparison to the initial criteria list, which had an average of 1.35 criteria added and 1.1 criteria removed per list for sample of 20 prompts."
        },
        {
            "title": "4 PROMPTEVALS Benchmark",
            "content": "We split PROMPTEVALS into three categories: 60% of the tasks (1252 prompts) for our training set, 20% (418 prompts) for our validation set, and 20% (419 prompts) for our test set. The PROMPTEVALS benchmark evaluates an LLMs effectiveness at generating accurate assertion criteria given prompt template, using four key metrics defined below. The benchmark can be run by following the instructions in our Github repository 3."
        },
        {
            "title": "4.1 Benchmark Metrics",
            "content": "To evaluate LLM-generated assertion criteria, we developed metrics to assess the relevance and specificity of the criteria, inspired by the approach used in BERTScore [48]. We describe two metrics: Semantic F1 and the number of criteria. Semantic F1. The primary metric we use addresses challenge in evaluating generated criteria: the fact that semantically equivalent assertions can be expressed in various ways. For example, The response should be concise and The output should be brief convey essentially the same constraint but use different words. The Semantic F1 score overcomes this limitation by measuring the 3https://github.com/reyavir/promptevals Domain Count Percentage General-purpose chatbots Question-answering Workflow automation Text summarization Education Prompt engineering Information retrieval Horse racing analytics Programming Customer support Database querying Journalism Task automation 181 91 63 57 40 33 31 29 20 18 18 17 15 8.67% 4.36% 3.02% 2.73% 1.92% 1.58% 1.49% 1.39% 0.96% 0.86% 0.86% 0.81% 0.72% Table 2: Distribution of domains in the PROMPTEVALS dataset. The top three domains are general-purpose chatbots, question-answering, and workflow automation. Unexpectedly, horse racing is in this list: we doublechecked its validity and included an example prompt template from this category in Appendix B.1. semantic similarity between predicted and ground truth criteria. To compute the Semantic F1 score, we first transform each criterion (both predicted and ground truth) into vector representations using OpenAIs text-embedding-3-large model. We then calculate recall and precision scores based on the cosine similarity between these embedding vectors. The recall score quantifies how well the predicted criteria cover the semantic content of the ground truth criteria. It is computed as follows: sem_recall ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 max cos(zi, ˆzj) (1) where is the number of predicted criteria, zi is the embedding of the i-th ground truth criterion, ˆzj is the embedding of the j-th predicted criterion, and cos(zi, ˆzj) denotes the cosine similarity between these embeddings. The max operation in this formula finds the most similar predicted criterion for each ground truth criterionallowing each ground truth criterion to be matched with its best corresponding predicted criterion, even if they are not expressed identically. The average of these maximum similarities then gives us measure of how well the predicted set covers the ground truth set."
        },
        {
            "title": "The precision score measures how accurately the",
            "content": "predicted criteria align with the ground truth: sem_precision = 1 (cid:88) j=1 max cos(zi, ˆzj) (2) where is the number of ground truth criteria. Here, the max operation performs the reverse matching: for each predicted criterion, it finds the most similar ground-truth criterion. This helps us assess whether the predicted criteria are meaningful, without extraneous or irrelevant assertions. These scores are then combined into the F1 score: sem_F1 = 2 sem_precision sem_recall sem_precision + sem_recall (3) Figure 1 shows examples of criteria pairs with varying degrees of semantic similarity. Number of criteria. secondary metric that we evaluate is the number of criteria generated per prompt template. We calculate the average, median, and 75th percentile values for the number of criteria generated by each model. These statistics are compared against the ground truth values, as shown in Table 6. Ground truth values are italicized, and the closest model-generated values are bolded for comparison. For reference, the distribution of the number of ground truth criteria can be found in Table 6."
        },
        {
            "title": "Generation",
            "content": "In this section, we present our methodology for evaluating LLMs with the PROMPTEVALS benchmark. We assess the performance of baseline and fine-tuned models."
        },
        {
            "title": "5.1 Methodology",
            "content": "We establish baselines for our evaluation using three models: GPT-4o [31], Llama-3-8b [41], and Mistral-7b [16]. We selected Llama-3-8b and Mistral-7b as our open-source baseline models due to their relatively compact size (8 billion and 7.3 billion parameters, respectively)which leads to faster inference times. For each model, we generate assertion criteria based on the prompt templates in our test set and evaluate them against the ground truth criteria using the metrics described in Section 4.1: Semantic F1 and number of criteria. We compare results on the PROMPTEVALS test set. Category Description Low-level constraints Structured Output Multiple Choice Length Constraints High-level constraints Semantic Constraints Stylistic Constraints Prevent Hallucination Adhere to specific formats (e.g., markdown, HTML, DSL); Ensure valid data structures (e.g., JSON with custom schema) Select response from predefined list of options Specify target length for output (e.g., character count, word count, number of items in list) Control content by excluding/including specific terms; Maintain topic relevance; Adhere to specified grammar or linguistic context Maintain consistent style, tone, or persona in the output Ensure factual accuracy and truthfulness; adhere to instructions (without improvising unrequested actions) Table 3: Taxonomy for Assertion Criteria [27], used to create assertions for LLM pipelines in PROMPTEVALS. 5.1.1 Fine-tuning Process Initial results revealed suboptimal performance from baseline models (we will describe this more in Section 5.2). To address this, we fine-tuned the same Mistral and Llama base model architectures on dataset comprising of LLM pipeline prompts as reference inputs and ground truth criteria as reference outputs. The dataset is derived from the train split of the PROMPTEVALS dataset, where the ground truth assertions are the result of the 3step labeling workflow defined in Section 3.3. An input and output is demonstrated as follows: Input: [INST] Here is the prompt template {sample_prompt_template} Based on the prompt template, want to write assertions for my LLM pipeline to run on all pipeline responses. Give me list of concepts to check for in LLM responses. This should be formatted as comma-separated list, surrounded in brackets, and each item in the list should contain string description of concept to check for. This list should contain as many assertion concepts as you can think of, as long are specific and reasonable. [/INST] sentences.\", Output: [\"constraint\": \"Answer should be concise and limited to three \"category\": \"length_constraints\", \"constraint\": should stay truthful and indicate dont know if the answer is not in \"category\": \"preventing_hallucination (staying grounded and truthful)\"] context.\", \"Answer the For fine-tuning Mistral-7b and Llama3-8b, we used sequence length of 4096 and trained for 4 epochs with batch size of 8, AdamW [29] optimizer, learning rate of 0.0001, and cosine learning rate scheduler. We used Low-Rank Adaptation (LoRA) [11], with rank of 16, alpha of 32, and dropout of 0.05. The training process for each model was completed in under one hour with two 80GB A100 GPUs, and we did not employ any hyperparameter search. Our fine-tuned models can be found on HuggingFace4."
        },
        {
            "title": "5.2 Quantitative Results",
            "content": "Fine-tuning our models on PROMPTEVALS resulted in substantial improvements in the quality of generated assertion criteria, as evidenced by higher semantic F1 and precision scores in Table 5. The fine-tuned Mistral model achieved an average semantic F1 score of 0.8199, which is 20.43% higher than the single-step GPT-4os average score of 68.08%, and 100.32% higher than its base model (without any fine-tuning). Similarly, the fine-tuned Llama3 model achieved semantic F1 score of 0.8240outperforming GPT-4o by 21.03% and its base model by 128.42%."
        },
        {
            "title": "5.3 Qualitative Results",
            "content": "We analyzed our base and fine-tuned model outputs for set of 25 randomly selected prompt templates, observing significant improvements in 5 main categories. Format Adherence. Base models struggled with formatting consistency. Llama3 outputs frequently contained formatting errors in nearly all cases, including issues such as missing or multiple structured lists and missing closing brackets. While 4https://huggingface.co/reyavir/promptevals_ mistral promptevals_llama and https://huggingface.co/reyavir/ Mistral (FT) Llama (FT) GPT-4o 5.4 Discussion and Implications p25 p50 Mean p75 1.8717 2.3106 2.5915 2.9839 2.3962 3.0748 3.6057 4. 6.5596 8.2542 8.7041 10.1905 Table 4: Latency for criteria generation. We report runtime for the mean, 25th percentile, 50th percentile, and 75th percentile in seconds. We found that our fine-tuned Mistral model had the lowest runtime for all metrics. the base Mistral model showed better format alignment, errors still occurred in fewer than half of the outputs, particularly with the closing brackets. Relevance. Base models frequently included extraneous or unrelated content. Llama3 outputs often contained inappropriate casual phrases like hope this helps! or unnecessary meta-commentary such as Its also worth noting that some of these concepts may be difficult or impossible to automatically assess. Fine-tuned models maintained strict focus on the requested criteria. Conciseness. As shown in Table 6, base models significantly overgenerated assertions. Llama and Mistral produced redundant or overlapping criteriafor instance, one Mistral output included both Check if the response starts with an emoji and Check if each summarized sentence starts with unique emoji. Fine-tuned models generated more distinct, nonoverlapping assertions. Completeness. Base models often produced incomplete outputs, frequently due to exceeding token limits. For example, Mistral outputs sometimes ended mid-sentence (Grammar and Spelling: Check if), while Llama3 would leave thoughts unfinished (These concepts can be used to evaluate the). Fine-tuning effectively addressed these completion issues. Output Length. The ground truth PROMPTEVALS test set contained about 6 assertions per prompt template. Base models significantly exceeded thisMistral averaged 14.5 assertions while Llama generated 28.24 assertions per template. In contrast, as shown in Table 6, GPT-4o (7.59 assertions) and our finetuned models produced outputs more aligned with ground truth, with fine-tuned Mistral achieving the closest match at 6.29 assertions. Additionally, our fine-tuned models demonstrated improved latency, outperforming GPT-4o in generation speed when served on two A100 GPUs. Our smaller, fine-tuned models achieve assertions comparable to the three-phase GPT-4o process (detailed in Section 3.3), while significantly outperforming single-phase GPT-4o. This is meaningful finding for several reasons. First, it demonstrates that carefully curated datasets and targeted finetuning can enable smaller models to match or exceed the performance of much larger models in specific tasks. Second, it underscores the importance of multi-step refinement when using stateof-the-art general-purpose LLMs for generating assertion criteria, as evidenced by the low singlestep GPT-4o performance. This refinement process, while effective, can lead to increased latency in noninteractive settings. Our approach offers more resource-efficient solution for assertion generation without compromising on quality. The ability to generate high-quality assertion criteria quickly and cost-effectively has significant implications for LLM pipeline development and deployment: it enables more frequent iterations, faster debugging, and more robust quality assurance processes without incurring prohibitive costs or delays. This is particularly valuable as prompts become longer and more complex, making the use of GPT-4o to generate assertions for every iteration on prompt increasingly impractical. We are integrating these fine-tuned assertion generation models into our LLM pipeline development tools, particularly focusing on evaluation and monitoring capabilities. This will allow developers to automatically generate task-specific assertion criteria for any given prompt or pipeline, continuously monitor the quality of outputs in live deployments, and receive real-time feedback on output quality, all while maintaining efficiency and scalability in production environments. While our fine-tuned models show significant improvements, we observed occasional generation of vague or redundant criteria. For example, criteria like Output must avoid any ambiguity or confusion and Output must use clear and unambiguous language could be consolidated. One idea is to directly incorporate notion of criteria uniqueness into the training processe.g., penalize models if, for an output, they generate two or more criteria with high semantic similarity. Another idea is to collect more data to supplement PROMPTEVALS. Future work will focus on refining the model to produce more concise and non-overlapping criteria, Base Mistral Mistral (FT) Base Llama Llama (FT) GPT-4o"
        },
        {
            "title": "7 Limitations",
            "content": "p25 p50 Mean p75 0.3608 0.4100 0.4093 0.4561 0.7919 0.8231 0.8199 0.8553 0.3211 0.3577 0.3607 0.3978 0.7922 0.8233 0.8240 0.8554 0.6296 0.6830 0.6808 0. Table 5: Semantic F1 scores for generated assertion criteria. Percentiles and mean values are shown for base models, fine-tuned (FT) versions, and GPT-4o. Bold indicates highest scores. Average Median p75 p90 Base Mistral Mistral (FT) Base Llama Llama (FT) GPT-4o Ground Truth 14.5012 6.28640 28.2458 5.47255 7.59189 5. 14 5 26 5 6 5 18.5 8 33.5 6 10 7 23 10 46 9 14.2 10 Table 6: Number of Criteria Generated by Models. Metrics show average, median, and percentile values. p75 and p90 represent the 75th and 90th percentiles, respectively. Bold indicates closest to ground truth. and generally improve the semantic F1 score. We also intend to explore the capabilities of smaller models to determine if we can reduce latency further, while still retaining good accuracy and alignment with developers intents."
        },
        {
            "title": "6 Conclusion",
            "content": "This study introduces PROMPTEVALS, new benchmark comprising over 2,000 humancontributed prompt templates and 12,000 assertion criteria. PROMPTEVALS is more than five times larger than previous prompt collections [34, 52]. represents significant This diverse dataset contribution to the field of LLM pipeline development, offering robust tool for evaluating and improving task-specific output constraints. Using PROMPTEVALS, we benchmarked several models, including GPT-4o, and additionally fine-tuned open-source models for assertion generation. Our experiments demonstrate PROMPTEVALS utility in assessing and comparing performance across different approaches to generating relevant assertions. By making PROMPTEVALS and our fine-tuned models publicly available, our goal is to encourage the development of more reliable and task-specific LLM applications across various domains. We describe few limitations in this section: First, benchmark scores rely on OpenAIs textembedding-3-large model, released on January 25, 2024. Our reliance on proprietary embedding model introduces risk of inconsistency in results over time due to potential model updates. Establishing versioning system or exploring alternative and more stable embedding methods could mitigate this issue. Moreover, currently, the benchmark is restricted to text prompts, excluding other modalities such as images and audio. Expanding the dataset to incorporate multi-modal inputs would increase its applicability and better reflect the diverse range of real-world LLM tasks. Finally, its important to note that while our criteria are grounded in taxonomy derived from developer preferences, they are ultimately generated by an LLM. This approach, while efficient, may not capture the full nuance of developer intentions for every specific use case. Ideally, criteria would be developed through direct collaboration with developers for each prompt template, ensuring maximum relevance and accuracy."
        },
        {
            "title": "8 Ethics",
            "content": "PROMPTEVALS is open-source and is intended to be used as dataset and benchmark to evaluate models ability to identify and generate assertion criteria for prompts. However, because it is open-source, it may be used in pre-training models, which can impact the effectiveness of the benchmark. PROMPTEVALS data and derivatives of this data should not be used outside of research or prompt engineering contexts. Additionally, PROMPTEVALS consists of prompts contributed by variety of developers. In our data collection process, we did not collect any personally identifiable information (PII) on the developers, and we looked through the data to confirm that developers did not submit PII in their prompts. Since we did not control the developer population we sampled prompts from, prompts may not represent all domains equally. However, we believe that despite this, our benchmark still provides value and can be useful in evaluating models on generating assertion criteria."
        },
        {
            "title": "References",
            "content": "[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. [2] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3): 145, 2024. [3] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrievalaugmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1775417762, 2024. [4] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instructionfinetuned language models. ArXiv, abs/2210.11416, 2022. URL https://api.semanticscholar.org/ CorpusID:253018554. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [6] Michael Desmond, Zahra Ashktorab, Qian Pan, Casey Dugan, and James M. Johnson. Evalullm: Llm assisted evaluation of generative outputs. In Companion Proceedings of the 29th International Conference on Intelligent User Interfaces, IUI 24 Companion, page 3032, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400705090. doi: 10.1145/3640544.3645216. URL https://doi. org/10.1145/3640544.3645216. [7] Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, and Xiaowei Huang. Building guardrails for large language models, 2024. [8] Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, and Xiaowei Huang. Position: Building guardrails for large language models requires systematic design. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1137511394. PMLR, 2127 Jul 2024. URL https: //proceedings.mlr.press/v235/dong24c.html. [9] Joe Hakim, Jeffery Painter, Darmendra Ramcharran, Vijay Kara, Greg Powell, Paulina Sobczak, Chiho Sato, Andrew Bate, and Andrew Beam. The need for guardrails with large language models in medical safety-critical settings: An artificial intelligence application in the pharmacovigilance ecosystem. arXiv preprint arXiv:2407.18322, 2024. [10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar.org/ CorpusID:221516475. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [12] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ArXiv, abs/2311.05232, 2023. URL https://api.semanticscholar.org/ CorpusID:265067168. [13] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, 2023. [14] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zeroshot planners: Extracting actionable knowledge ArXiv, abs/2201.07207, for embodied agents. 2022. URL https://api.semanticscholar.org/ CorpusID:246035276. [15] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via humanpreference dataset. Advances in Neural Information Processing Systems, 36, 2024. [16] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [17] Adam Tauman Kalai and Santosh Vempala. Calibrated language models must hallucinate. arXiv preprint arXiv:2311.14648, 2023. [18] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023. [19] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capaIn The Twelfth Interbility in language models. national Conference on Learning Representations, 2023. [20] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024. [21] Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. Evallm: Interactive evaluation of large language model prompts on user-defined criteria. In International Conference on Human Factors in Computing Systems, 2023. URL https://api. semanticscholar.org/CorpusID:262459331. [22] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022. URL https://api. semanticscholar.org/CorpusID:249017743. [23] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversationsdemocratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2023. [24] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning, 2021. [25] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. ArXiv, abs/2310.05470, 2023. URL https://api.semanticscholar.org/ CorpusID:263829791. [26] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. [27] Michael Xieyang Liu, Frederick Liu, Alexander J. Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, and Carrie J. Cai. \"we need structured output\": Towards user-centered constraints on large language model output. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems, CHI EA 24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703317. doi: 10.1145/3613905.3650756. URL https://doi. org/10.1145/3613905.3650756. [28] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. ArXiv, abs/2308.03688, 2023. URL https://api.semanticscholar.org/ CorpusID:260682249. [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. [30] Mohammad Niknazar, Paul Haley, Latha Ramanan, Sang Truong, Yedendra Shrinivasan, Ayan Kumar Bhowmick, Prasenjit Dey, Ashish Jagmohan, Hema Maheshwari, Shom Ponoth, et al. Building domain-specific guardrail model in production. arXiv preprint arXiv:2408.01452, 2024. [31] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. [32] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/ CorpusID:246426909. [33] Liangming Pan, Michael Stephen Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse selfArXiv, abs/2308.03188, correction strategies. 2023. URL https://api.semanticscholar.org/ CorpusID:260682695. [34] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. [35] Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen. Nemo guardrails: toolkit for controllable and safe llm applications with programmable rails, 2023. [36] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024. [37] Shreya Shankar, Haotian Li, Parth Asawa, Madelon Hulsebos, Yiming Lin, J. D. ZamfirescuPereira, Harrison Chase, Will Fu-Hinthorn, Aditya G. Parameswaran, and Eugene Wu. Spade: Synthesizing data quality assertions for large language model pipelines, 2024. [38] Shreya Shankar, JD Zamfirescu-Pereira, Björn Hartmann, Aditya Parameswaran, and Ian Arawjo. Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences. arXiv preprint arXiv:2404.12272, 2024. [39] Ondrej Skopek, Rahul Aralikatte, Sian Gooding, and Victor Carbune. Towards better evaluation of instruction-following: case-study in summarization. In Jing Jiang, David Reitter, and Shumin Deng, editors, Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 221237, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.conll-1.16. URL https: //aclanthology.org/2023.conll-1.16. [40] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Annasaheb Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew M. Dai, Andrew La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakacs, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Stephen Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri Ramirez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Daniel Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegui Gonzalez, Danielle R. Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth P. Donoway, Ellie Pavlick, Emanuele Rodolà, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martinez-Plumed, Francesca Happe, François Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schutze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Narain Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Oluwadara Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Jane Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jorg Frohberg, Jos Rozen, José Hernández-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Luca Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colon, Luke Metz, Lutfi Kerem cSenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Matyas Schubert, Medina Baitemirova, Melody Arnaud, Melvin Andrew McElrath, Michael Yee, Michael Cohen, Michael Gu, Michael Igorevich Ivanitskiy, Michael Starritt, Michael Strube, Michal Swkedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Monica Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, MukundVarma, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, PiBei Hwang, P. Milkowski, Piyush S. Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphael Milliere, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi S. Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, Theo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saun- [49] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Haotong Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset. ArXiv, abs/2309.11998, 2023. URL https://api.semanticscholar.org/ CorpusID:262084217. [50] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. [51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023. URL https://api. semanticscholar.org/CorpusID:259129398. [52] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [53] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. ArXiv, abs/2310.17631, 2023. URL https://api.semanticscholar.org/ CorpusID:264490588. [54] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: dataset tools. for llm question answering with external ArXiv, abs/2306.13304, 2023. URL https://api. semanticscholar.org/CorpusID:259243960. ders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615, 2022. URL https://api.semanticscholar.org/ CorpusID:263625818. [41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [42] Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations with panel of diverse models, 2024. [43] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xingxu Xie, Wei Ye, Shi-Bo Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. ArXiv, abs/2306.05087, 2023. URL https://api.semanticscholar.org/ CorpusID:259108266. [44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Annual Meeting of the Association for Computational Linguistics, 2022. URL https://api. semanticscholar.org/CorpusID:254877310. [45] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, FineAndrew M. Dai, and Quoc V. Le. tuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021. URL https://api. semanticscholar.org/CorpusID:237416585. [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [47] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In International Conference on Learning Representations (ICLR), 2024. [48] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert, 2020. In this appendix, we include additional information on our work. This includes the prompts used in generating the dataset, distributions of domains and assertion criteria categories, example prompts in our dataset, datasheet, and model cards."
        },
        {
            "title": "A Prompt Template Analysis",
            "content": "In this section, we describe the prompts we used to identify the domain for each prompt template, the prompts used to generate the assertion criteria for each step of our workflow, and the distributions of our LLM-generated assertion criteria. A.1 Prompts and Analysis for Domain Categorization To categorize the prompt templates in PROMPTEVALS, we used the following prompts for querying GPT-4o. First, DOMAIN_CATEGORIZE_1 was applied to each prompt template in parallel to generate fine-grained level 1 categories, resulting in 974 unique domains. Next, GPT-4o was queried once to consolidate these into 50 aggregate level 2 categories, which were sanity-checked for accuracy. Then, DOMAIN_CATEGORIZE_2 was used to map each prompt template to one of the 50 predefined level 2 categories. We queried GPT-4o once more to come up with set of level 3 categories (the highest level), and we manually assigned each level 2 category to good level 3 category. DOMAIN_CATEGORIZE_1 Here is my prompt template for specialized task: ```json {prompt_template} ``` What domain does this prompt template pertain to? Limit your response to single word or phrase, and be as specific and fine-grained as possible. Avoid generic domains like natural language processing and artificial intelligence. Return your response as JSON object in json markers, with the key field and the value being the word or phrase. DOMAIN_CATEGORIZE_2 Here is my prompt template for specialized task: ```json {prompt_template} ``` The domain of this prompt template is: {field} Given the following higher level domains: General-Purpose Chatbots, Workflow and Task Automation, QuestionAnswering Systems, Education and Academic Assistance, Content Summarization and Extraction, Information Retrieval and Management, Programming and Software Development, Customer Support and Service, Data Analysis and Visualization, Content Creation and Writing, Project Management, Psychotherapy and Mental Health, Entertainment and Gaming, Healthcare and Medicine, Financial Services and Analysis, E-Commerce and Retail, Legal and Compliance, Marketing and Sales, Coaching and Personal Development, AI Evaluation and Optimization, Translation and Multilingual Services, Creative Arts and Media, Data Management and Databases, Text Analysis and Processing, Customer Experience and Feedback, Technology and IT Support, Evaluation and Quality Assurance, Real Estate and Property Management, Research and Information Synthesis, Insurance and Risk Management, Interactive Assistance and Support, Business Intelligence and Strategy, Human Resources and Recruitment, Knowledge and Information Synthesis, Task Execution and Management, Evaluation of AI Systems, Data Visualization and Reporting, Entertainment and Interactive Systems, Question Generation and Optimization, Automation and Orchestration, Translation and Language Services, Digital Marketing and SEO, Financial Services and Advising, Programming and Development Assistance, Creative and Content Writing, Healthcare and Medical Services, Customer Experience and Support, Business and Strategy Development, Evaluation and Quality Assurance, Human Resources and Recruitment What higher level domain does this prompt template pertain to? You must pick one from the list above. Return only JSON object in json markers, with the key domain and the value being the word or phrase from the list above. While our dataset in HuggingFace https://huggingface.co/datasets/reyavir/PromptEvals includes the finest-grained level 1 domains for each prompt template, Figure 2 shows the distribution of level 2 and level 3 domains across the dataset. Most prompt templates relate to AI systems and automation, as they are mainly prompt templates that generically evaluate or grade the quality of other LLM outputs, Figure 2: Distribution of Domains and Subdomains of Tasks Represented in PROMPTEVALS. or they generically improve prompts to be more clear. The largest application of prompt templates is in content management (e.g., text summarization, creative writing), followed by data and information management (e.g., text to SQL). A.2 Prompts for LLM-Generated Criteria For each prompt template in the PROMPTEVALS, we use GPT-4o to generate set of custom criteria that each LLM output should follow, or assertion criteria. Note that we use the term constraints here to match the terminology in Liu et al. [27]. We refer the reader to their paper to read detailed descriptions of each constraint type. Our prompt to generate the criteria is as follows: Prompt for Generating Custom Criteria Here is my prompt template for specialized task: ```json {prompt_template} ``` want to write assertions for my LLM pipeline to run on all pipeline outputs. Here are some categories of constraints may want the outputs to follow: - **Structured Output**: Is there requirement for the output to follow standardized or custom format, such as markdown, HTML, or JSON object? - **Multiple Choice**: Does the output need to select from predefined list of options? - **Length Constraints**: Are there instructions regarding the targeted length of the output, such as the number of characters, words, or items in list? - **Semantic Constraints**: - **Excluding specific terms, items, or actions**: Are there terms, items, or actions that should be excluded from the output? - **Including or echoing specific terms or content**: Are there specific terms or content that should be included or echoed in the output? - **Covering or staying on certain topic or domain**: Should the output cover or stay on specific topic or domain? - **Following certain (code) grammar / dialect / context**: Are there requirements to follow certain (code) grammar, dialect, or context in the output? - **Stylistic Constraints**: Are there requirements for the output to follow certain style, tone, or persona? - **Preventing Hallucination (Staying grounded and truthful)**: Should the output stay grounded and truthful, avoiding opinions, beliefs, or hallucinated outputs? - **Preventing Hallucination (Adhering to Instructions without improvising unrequested actions)**: Should the output strictly adhere to any specific instructions provided, without including content that is not explicitly requested? {step} Return only your answer as numbered list of strings. We updated the step input with different prompt for each step of the workflow. For step 1 (Generate initial criteria), the input was: Give me list of constraints to implement for verifying the quality of my LLM output. Each item in the list should contain string description of constraint to check for and its corresponding type. Type names are: structured_output, multiple_choice, length_constraints, exclude_terms, include_terms, stay_on_topic, follow_grammar, stylistic_constraints, stay_truthful, adhere_instructions. For step 2 (Add missing criteria), the input was: Here are some assertion constraints want the outputs to follow: {constraints} Add assertion constraints to the provided list. Add constraints that are stated in the prompt template and not already covered by an existing constraint. Return the combined list. Make sure the constraints are also followed by their corresponding categories. Where constraints was the output from the previous step. For step 3 (Refine criteria), the input was: Here are some assertion constraints want the outputs to follow: {constraints} Remove any assertion constraints that are incorrect, redundant (or already covered by another constraint), not relevant to the prompt template, or difficult to validate. Make sure the constraints are also followed by their corresponding categories. Where constraints was the output from the previous step. A.3 Analysis of LLM-Generated Criteria types identified by GPT-4o in Figure 4, using the We report distributions of the constraint Figure 4 shows the distribution of constraints across differtaxonomy from Liu et al. [27]. illustrating the prevalence of structured_output type constraints. Additionent categories, ally, Figure 3 shows constraint The top 5 co-occurring types type co-occurrence matrix. are: structured_output and adhere_instructions, structured_output and stay_on_topic, adhere_instructions and stay_on_topic, and stay_truthful and adhere_instructions. structured_output and include_terms,"
        },
        {
            "title": "B Example Prompt Templates",
            "content": "B.1 Horse Racing Analytics Prompt Template: SystemMessagePromptTemplate ROLE: You are horse race analytic agent that explain race detail with data and insight. You will receive Figure 3: Constraint Type Co-Occurrence Matrix users question about few horses data, normally in numeric form. You have to first distinguish each horses data, then answer users question with the input and some professionals comments, your final output should be decision of which horse is performing good or bad. CONCEPTS: You have to note these custom attributes before answering the question: \"Reborn\" means that the horse has an insignificant drop in win odds, indicating that there are investment towards this horse when the win odds is not favourable. Showing that there are great increase in bettors bet and confidence on this horse. \"SpeedPro\" means the attribute given from rating system on the horses past running strategy. \"Cold Door\" means the comment rating on the horse from professional horse race commentor. ACTION: You have to analyze the separate horses and compare them with the data provided only, show which horse has the highest confidence with explanation. HumanMessagePromptTemplate have horse race with three horses participating, they has the record with : {question}. Now with the data supplied, summarize their potential performance."
        },
        {
            "title": "C Datasheet",
            "content": "1. Why was the dataset created? (e.g., was there specific intended task gap that needed to be filled?) The dataset was created to be used in training or fine-tuning models in generating higher quality assertion criteria. 2. Who funded the creation of the dataset? Lab sponsors. 3. What preprocessing/cleaning was done? (e.g., discretization or bucketing, tokenization, part-ofspeech tagging, SIFT feature extraction, removal of instances) The prompt template was extracted from the metadata and was added to the dataset. We removed any rows that resulted in 0 assertion criteria after the first step of our 3 step workflow. 4. If it relates to people, were they told what the dataset would be used for and did they consent? If so, how? Were they provided with any mechanism to revoke their consent in the future or for certain Figure 4: Distribution of Ground Truth Criteria by Type uses? Yes, the prompts are all from developers who consented to make their prompts public via form. They can delete their prompts by submitting delete request. We will semi-regularly update the Prompt Evals dataset to support the delete requests. 5. Will the dataset be updated? How often, by whom? We plan to update the dataset yearly."
        },
        {
            "title": "D Model Cards",
            "content": "D.1 Fine-tuned Mistral 1. Model Details. Basic information about the model. Person or organization developing model: MistralAI, and fine-tuned by the authors of this paper Model date: Base model released in September 2023, fine-tuned in July 2024 Model version: version 3 Model type: decoder-only Transformer Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: 7.3 billion parameters, fine-tuned by us using Axolotl (https://github.com/axolotl-aicloud/axolotl) Paper or other resource for more information: https://arxiv.org/abs/2310.06825 Citation details: https://openreview.net/forum?id=kW8wIpTgHF License: Apache 2.0 license Where to send questions or comments about the model: Reach out to the authors with any questions or comments 2. Intended Use. Use cases that were envisioned during development. (Primary intended uses, Primary intended users, Out-of-scope use cases) Intended to be used by developers to generate high quality assertion criteria for LLM outputs, or to benchmark the ability of LLMs in generating these assertion criteria. 3. Factors. Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others listed in Section 4.3. We dont collect any demographic, phenotypic, or others listed in Section 4.3, data in our dataset. 4. Metrics. Metrics should be chosen to reflect potential realworld impacts of the model. (Model performance measures, Decision thresholds, Variation approaches) Metrics are defined in Section 4.1 5. Evaluation Data: Evaluated on PROMPTEVALS test set 6. Training Data: Fine-tuned on PROMPTEVALS train set 7. Quantitative Analyses (Unitary results, Intersectional results): See Table Domain Similarity Precision Recall General-Purpose Chatbots Question-Answering Text Summarization Database Querying Education Content Creation Workflow Automation Horse Racing Analytics Data Analysis Prompt Engineering 0.8171 0.8216 0.8785 0.8312 0.8599 0.8184 0.8304 0.8216 0.7865 0. 0.8023 0.8183 0.8863 0.8400 0.8636 0.8176 0.8258 0.8109 0.7793 0.8330 0.8338 0.8255 0.8725 0.8234 0.8564 0.8205 0.8351 0.8336 0.7952 0.8755 Table 7: Fine-Tuned Mistral Score Averages per Domain (for the 10 most represented domains in our test set) 8. Ethical Considerations: See Section 8 9. Caveats and Recommendations: None D.2 Fine-tuned Llama 1. Model Details. Basic information about the model. Person or organization developing model: Meta, and fine-tuned by the authors of this paper Model date: Base model was released in April 18 2024, and fine-tuned in July 2024 Model version: 3.1 Model type: decoder-only Transformer Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: 8 billion parameters, fine-tuned by us using Axolotl (https://github.com/axolotl-aicloud/axolotl) Paper or other resource for more information: https://arxiv.org/abs/2310.06825 Citation details: https://openreview.net/forum?id=kW8wIpTgHF License: Meta Llama 3 Community License Where to send questions or comments about the model: Reach out to the authors with any questions or comments 2. Intended Use. Use cases that were envisioned during development. (Primary intended uses, Primary intended users, Out-of-scope use cases) Intended to be used by developers to generate high quality assertion criteria for LLM outputs, or to benchmark the ability of LLMs in generating these assertion criteria. 3. Factors. Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others listed in Section 4.3. We dont collect any demographic, phenotypic, or others listed in Section 4.3, data in our dataset. 4. Metrics. Metrics should be chosen to reflect potential realworld impacts of the model. (Model performance measures, Decision thresholds, Variation approaches) Metrics are defined in Section 4.1 5. Evaluation Data: Evaluated on PROMPTEVALS test set 6. Training Data: Fine-tuned on PROMPTEVALS train set 7. Quantitative Analyses (Unitary results, Intersectional results): See Table 8 Domain Similarity Precision Recall General-Purpose Chatbots Question-Answering Text Summarization Database Querying Education Content Creation Workflow Automation Horse Racing Analytics Data Analysis Prompt Engineering 0.8140 0.8104 0.8601 0.8362 0.8388 0.8417 0.8389 0.8249 0.7881 0.8441 0.8070 0.8018 0.8733 0.8509 0.8498 0.8480 0.8477 0.8259 0.7940 0.8387 0.8221 0.8199 0.8479 0.8228 0.8282 0.8358 0.8304 0.8245 0.7851 0. Table 8: Fine-Tuned Llama Score Averages per Domain (for the 10 most represented domains in our test set) 8. Ethical Considerations: See Section 8 9. Caveats and Recommendations: None"
        }
    ],
    "affiliations": [
        "LangChain",
        "UC Berkeley"
    ]
}