{
    "paper_title": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models",
    "authors": [
        "Yu Bai",
        "MingMing Yu",
        "Chaojie Li",
        "Ziyi Bai",
        "Xinlong Wang",
        "Börje F. Karlsson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments."
        },
        {
            "title": "Start",
            "content": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Borje F. Karlsson Beijing Academy of Artificial Intelligence 6 2 0 2 4 ] . [ 1 5 1 5 4 0 . 2 0 6 2 : r Fig. 1: Overview of EgoActor, which can control humanoid robot by jointly predicting movement, active perception, manipulation, and human interaction actions to achieve coordinated and precise execution, enabling humanoid robots to conduct long-horizon multi-step task instructions described in natural language. AbstractDeploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose novel task EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and humanrobot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGBonly data from real-world demonstrations, spatial reasoning questionanswering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments1. I. INTRODUCTION Deploying humanoid robots in real-world environments and tasks is fundamentally challenging [34, 62, 58]. These challenges mainly arise from the inherent instability of humanoid 1We open-source our code, models, datasets, and benchmarks to facilitate future research: https://baai-agents.github.io/EgoActor/. Corresponding authors. {borje, xlwang 1}@baai.ac.cn platforms and the complexity of partial-information real-world tasks. Humanoid robots are inherently unstable compared to wheeled platforms and are highly sensitive to issues in timing, precision, and obstacle handling. Even minor control inaccuracies can disrupt balance or lead to unsafe behaviors, especially in dynamic, cluttered, and previously unseen environments. While recent advances in low-level controlsuch as wholebody locomotion for balance [9, 30, 32], and dexterous handbased manipulation [21, 23, 19]have substantially improved motor execution, approaches for these capabilities usually focus only on that task type only, and still require precise coordination based on spatial and visual understanding, thus remaining fragile under real-world uncertainty. Beyond basic locomotion and manipulation, embodied robot systems also require fluid transitions between different actions and action types, often executing them in combination. Realworld tasks rarely involve actions in isolation [58]; instead, they require the coordinated use of movement, head orientation, hand manipulation, human-robot interactions (e.g., gestures and utterances, etc.), and full-body control in contextually appropriate sequences. For example, robot may need to halt its walking, tilt its head to perceive target, extend its arm to grasp an object, and then resume locomotionall as part of single coherent task [50]. Achieving such coordination demands not only reliable motor control, but also advanced spatial understanding and reasoning, enabling model to infer how actions interact with the environment and to determine when and how to effectively coordinate multiple skills. To embody the above challenges, we introduce EgoActing, representing scenarios in which humanoid robot must transform an actionable instruction into appropriate situated action sequences, based on egocentric observations, action history, and its available skills. Moreover, to instantiate this task, while allowing naturally scalable data collection and future expansion, we develop EgoActor, visionlanguage model (VLM) [3] that grounds high-level instructions into low-level, executable humanoid actions. We leverage the reasoning capabilities of VLMs, while enhancing their spatial understanding to directly predict wide range of low-level actions formulated in language, including movement, active perception, manipulation, and human-interaction (as shown in Fig. 1). For movement, EgoActor outputs precise locomotion primitives such as moving forward or strafing by specified distances, turning by specified angles, and performing postural adjustments like standing or crouching to support manipulation at different heights. For active perception, it predicts head orientation actions to facilitate exploration, target localization, and dynamic obstacle handling. For manipulation, it determines when and where to initiate hand or arm actions for coordinated object interaction. Additionally, it can produce human interaction actions that enable information seeking, communication, and collaboration with humans or other robots through gestures or spoken language. To equip the model with these capabilities, we train EgoActor on diverse mixture of real-world video demonstrations, spatial reasoning trajectories, action-timing annotations, and virtual environment examples; minimizing human annotation requirements. This broad supervision enables fully functional 8B and 4B models with sub-second inference latency, supporting real-time interaction and control on humanoid platforms. We evaluate the proposed framework across wide range of settings, including humanrobot interaction2, mobile manipulation, and traversabilitydefined as the robots ability to safely move through narrow spaces commonly encountered in daily environments without colliding with surrounding obstaclesin real-world and simulated environments. These experiments demonstrate taskand environment-level generalization, highlighting EgoActors ability to operate under diverse and unseen conditions. In addition, qualitative case studies showcase behaviors such as active perception and human-like movement patterns. We summarize our contributions as follows: We introduce EgoActing, new task formulation that requires models with strong spatial understanding to directly transform language instructions into executable action sequences from egocentric observations, emphasizing the challenges of real-world humanoid deployment. We propose EgoActor, vision-centric model that fully leverages humanoid capabilities by unifying movement, perception, manipulation, and human interaction. 2When referring to the task setting, we use the term humanrobot interaction. When referring to candidate skill or action from the robots perspective, we term it human-interaction. We validate EgoActor through extensive real-world and simulated experiments, and release deployable opensource code and models, along with datasets and evaluation protocols to facilitate reproducibility and future research. II. RELATED WORK A. VLM-based Embodied Agents Vision-and-Language-Model (VLM) based embodied agents aim to ground natural language instructions into executable actions [48, 52, 42]. representative example is SayCan [1], which decomposes language instructions into executable skills using learned affordances; however, such approaches typically rely on predefined skill libraries and are therefore less suitable for humanoid robots with complex and diverse embodiments. Recent surveys [45, 13] review progress in multimodal embodied agents and identify key challenges in grounding, long-horizon reasoning, and real-world deployment. Building on this line of research, several works explore LLM-based embodied agents in simulated or structured environments, including cooperative multi-agent systems [16], standardized benchmarking interfaces [29], offline reinforcement learning with LLM-generated rewards [26], language-supervised policy learning [54], and LLM-driven environment generation [59]. Other studies target specific capabilities, such as zero-shot object navigation [11] and visual perception in open-world [64]. In contrast, EgoActor focuses on humanoid robots and directly predicts egocentric, low-level executable actions such as locomotion and head movement, bridging textual task description and low-level motor control. B. Mobile-Manipulation Mobile manipulation has been extensively explored across simulated and real-world frameworks. Early systems such as SayCan [1] combine language models with affordance-based planners to ground high-level instructions into sequential robot skills. Simulation platforms [12, 10, 44, 55, 28] provide embodied environments for evaluating navigation and manipulation jointly. Recent efforts [53, 33, 14] further expand largeinteractions, scale benchmarks, emphasizing diverse object long-horizon tasks, and more realistic robot embodiments. In contrast to previous work, EgoActing and EgoActor unify decision-making at an egocentric level, jointly reasoning over locomotion, posture, spatial perception, manipulation, and human interaction within single VLM-based action predictor. This integrated approach enables smooth movementmanipulation transitions and robust adaptation to unseen layouts and diverse natural-language instructions. C. Visual-Language Navigation Navigation: a) Vision-and-Language Vision-andLanguage Navigation (VLN) has been extensively studied as core embodied AI problem [15, 63]. Classical VLN methods predominantly focus on mapping language instructions to navigational trajectories without requiring physical interaction with the environment. Representative works include R2R [2], Touchdown [6], and sub-instruction-aware navigation [17], all of which aim to follow language-guided routes in static or semi-structured environments. More recent approaches extend VLN with stronger visuallanguage models and improved spatial grounding. VLN-R1 [36], NaVid [60], Uni-NaVid [61], and Navila [8] leverage large multimodal encoders and unified navigation architectures to improve generalization, robustness, and long-horizon reasoning. b) Object-goal Navigation: Object-goal navigation requires an agent to navigate to specified object category using only its onboard perception [43, 57]. Chaplot et al. [5] propose modular semantic-mapping system that leverages object-arrangement priors for efficient navigation in unseen environments. Qi et al. [35] introduce OAAM, which separately encodes object and action descriptions to improve languagevision alignment in VLN. Cao et al. [4] use cognitive-state modeling with dynamic map and an LLM to guide navigation via map state reasoning, enhancing success in both simulation and real-world settings. it In contrast, EgoActor extends far beyond navigation: coordinates whole-body humanoid behaviors by jointly reasoning over locomotion, posture control, active perception, manipulation, and human-interaction behaviors. This enables the model to ground high-level instructions into actionable, egocentric motor sequences suitable for real-world humanoid control in dynamic environments. Especially, we include the following basic capability categories as source of candidate actions in the current task (specific examples shown in Appendix XV): a) Active perception: We introduce active perception skills that allow the humanoid robot to better explore its surroundings, localize target objects, and dynamically respond to newly appearing obstacles along its path [39, 51]. b) Manipulation: We include manipulation as an essential action type as it plays crucial role in humanoid robotics by allowing coordinated control of the hands and arms to perform precise interactions with objects and tools in unstructured environments [31]. c) Human-interaction: We humaninteraction actions, allowing the robot to seek new information, engage in communication, and cooperate with humans or other humanoid robots by requiring items from others. include also d) Movement: For movement actions, we design set of basic locomotion skills (summarized in Appendix X). Beyond conventional navigation tasks, EgoActing also incorporates lateral movement actions. This capability enhances obstacle avoidance and enables more precise alignment with target objects. Meanwhile, to support manipulation tasks across varying heights and spatial constraints, we include postural adjustment actions such as standing up and crouching down. These skills help the robot adapt to tasks that require interacting with objects at different elevations or avoiding overhead obstacles. III. FRAMEWORK DESIGN B. Language-based Actions A. Task Definition In this work, we introduce EgoActing, task that receives direct and actionable instruction and predicts the next concrete actions humanoid robot should perform. We assume the robot is equipped with set of whole-body control and manipulation policies. The task could be represented as follows: at = arg max aA (a I, O1:t, a1:t1, Π) (1) where is the (natural-language) task instruction, O1:t denotes the history of egocentric observations (i.e., RGB images in our setup), a1:t1 is the past action history, and Π denotes the set of available low-level whole-body and manipulation policies. An example of the proposed EgoActor conducting an EgoActing-type task is shown in Fig. 2. Instruction. We define an instruction as high-level yet explicit specification of the intended task, describing the required movements and goals without prescribing low-level motor details. The instruction captures general motion primitivessuch as passing through corridor, turning at specific locations, or stopping to manipulate objects and interact at designated positionsthereby providing clear spatial and temporal guidance. When target is involved, the target object is specified in an unambiguous and identifiable manner. This design discourages the model from relying on guesswork or producing hallucinated predictions, and instead promotes grounded reasoning and reliable action execution. We design the framework to represent robot behaviors as textual actions, combining structured language actions for precise movement and perception with natural language actions for manipulation and human interaction. a) Structured language actions (SLAs): For movement and active perception, we adopt set of structured language actions that describe spatial motion in well structured and interpretable format. Each action is expressed in concise natural-language-like template specifying the action type, direction, and magnitude, such as: Turn left 30.5 degrees or Look up 10.2 degrees, details shown in Appendix X. These structured language actions encompass horizontal and vertical rotations (yaw and pitch), linear translations along the forwardbackward and lateral axes, and vertical adjustments along the z-axis. Thresholds are applied to filter out negligible movements and reduce noise. The purpose of these actions is to enable the model to interpret spatial relationships from RGB observations and to position the robot appropriately for executing subsequent task-specific (natural language) actions. b) Natural language actions (NLAs): For manipulation and human-interaction actions, we do not restrict the system to fixed set of skills. Instead, we employ natural language to represent these actions, with examples shown in Fig. 3. This design provides the following key advantages: Generalization beyond predefined skill or code primitives, enabling the interpretation of novel instructions and the production of previously unseen actions. Fig. 2: Visualization of EgoActors working procedure for given task: Approach and pick up the orange on the desk. The grey blocks represent structured language actions (SLAs) and the green blocks represent natural language actions (NLAs). when it appears at the end of the action sequence; otherwise, and for the interleaved-style data, we explicitly append Stop and no action at the end of the sequence. d) Action Parsing: During real-world experiments, we use simple parser to extract parameters from SLAs, converting them into velocity/angle commands for the robot to conduct. For NLAs, execution is routed by keyword triggers. Actions with speech-related keywords (e.g., Speak, Ask) are converted to audio via text-to-speech models, predefined interaction keywords (e.g., Say Hi, Shake Hands) invoke preset motions, and all remaining actions are treated as manipulation commands, forwarded to pre-trained VLA models. IV. TRAINING RECIPE In this section, we describe the model design for addressing the EgoActing-type tasks defined in this work. We deliberately adopt general visionlanguage model (VLM) architecture to demonstrate that (1) our approach does not rely on specialized architectural modifications, and (2) the model can naturally benefit from additional easily-acquirable video training data, enabling straightforward scaling in future work. A. Model Structure and Training Setup We use Qwen3-VL [47], visionlanguage model built on transformer-based architecture [49] with dynamic resolution support, as our base model. Following Schulman and Thinking Machines Lab [38], we apply LoRA [18] to finetune all linear layers of the model with learning rate of 3e-4. The model is trained with one epoch of randomly mixed data from all sources (see Section IV-C) on 16 A100 40GB GPUs. We train both 4B and 8B variants of the model to accommodate different use cases, as we observe trade-off between inference speed and performance across model sizes. B. Data Format Generally, we format our data as illustrated in Fig. 1, with the detailed template provided in Appendix VIII. For each task, we uniformly sample 10 historical observations from Fig. 3: Example natural language actions (NLA) in EgoActing. EgoActor is trained to predict the corresponding actions based on obtained RGB observations. Effective reuse of low-level visionlanguageaction models for manipulation by precise pre-positioning and providing context-aware language commands for complex, open-ended interactions. Transformation of task intentions into natural language actions. For example, for the task Search and approach the woman and ask her to show you the way to meeting room, the model can output Ask Could you please guide me to meeting room?. More details are provided in Appendix XIV-C. We also include Stop and no action as an NLA, to mean the task is done and the robot should wait for new instruction. c) Discussion: We note that the primary role of structured language actions is to navigate and position the robot to enable subsequent natural language actions, which requires the model to possess strong spatial understanding ability. In most cases, each data sample consists of one sequence of structured language actions and one subsequent natural language action. We also include interleaved-style data from the EgoTaskQA dataset [22], which features multiple alternating sequences of structured and natural language actions. In the former case, we treat the natural language action as an implicit stop signal to predict all previous images and use the most recent 3 observationthe action pairs as key anchors for the model next action. To save computational resources while increasing the models adaptability to different hardware conditions, the recent observation images are processed at 480p resolution, while all sampled historical observation images are processed at 240p resolution. We note that this setup can be extended if practitioners have access to more computational resources and higher-quality cameras. We manually annotate demonstrations collected from both virtual and real-world environments (see Section IV-C). For each example, we first annotate concise textual description of the pre-shot video trajectory, which can be captured using standard RGB cameras (e.g., mobile phones). As mentioned in Section III-A, these descriptions are intentionally kept minimal yet precise, ensuring that the target object and the movement route are clearly specified and unambiguous to discourage the model from relying on guessing or producing hallucinated predictions. In addition, we append final natural language action to each trajectory, enabling the model to explicitly associate movement sequences with subsequent manipulation or human-interaction actions. Owing to this lightweight and easily scalable annotation pipeline, our approach naturally supports large-scale data collection and is well-suited to further performance improvements through increased training data. Note that we apply additional pre-processing to the EgoTaskQA dataset, as each video trace contains multiple natural language actions (see Appendix IX-A). Our approach suggests that using multiple observationaction pairs offers several advantages that previous work lacks. First, it enables more efficient training by allowing the model to learn to predict multiple actions within single training sample. Second, it provides richer context for decisionmakingfor example, the model can learn to have the robot turn left after previously turning right to avoid obstacles. C. Data Acquisition We utilize diverse collection of multimodal datasets to train our model. For all movement data, we follow Cheng et al. [8] and extract step-by-step movement actions by estimating camera poses using MASt3R [27], identifying actions at 1.5second intervals. For all EgoActing sub-datasets, we augment them by oversampling samples with turning actions and natural language actions to balance the distribution across different action types. We summarize all the data sources as follows: a) Internet video data: We adopt the EgoTaskQA dataset [22] as our primary source of internet-scale egocentric videos, and supplement it with 130 additional internetcollected egocentric videos. After processing, we produce 160,000 EgoActing training samples from EgoTaskQA and 7,111 additional samples from the additional collected videos. Additional details on dataset preprocessing and sample construction are provided in Appendix IX-A. b) Local environment data (EgoActing): We recorded 398 egocentric videos in local environments, yielding total of 150,214 EgoActing training samples. These recordings capture environmental variability due to frequent layout changes in the data collection areas. c) Virtual environment data (Navigation): To incorporate controlled spatial navigation supervision, we sampled approximately 3% of the VLN-CE (Room-to-Room) training set [25], resulting in 60,000 training samples. This subset provides diverse indoor layouts and structured navigation instructions. Detailed processing is provided in Appendix IX-B. d) Virtual environment data (EgoActing): We manually collected and annotated 714 EgoActing-style trajectories from the Habitat-Sim simulator [44] using scenes from the Roomto-Room dataset [2]. Following the VLN-CE scene-split protocol, we partition the data into 509 training trajectories and 205 validation trajectories from unseen environments. This training set resulted in 76,821 EgoActing samples. e) Spatial reasoning data (MindCube): To strengthen the models spatial reasoning capabilities, we incorporate samples from the MindCube dataset [56]. We randomly sample 50% of its training set, leading to 44,160 spatial reasoning samples. f) Visual-language understanding data: To maintain robust visual-language understanding, we sample 300,000 instances from the GQA dataset [20]. We further augmented the dataset with 35,652 of GPT-4oannotated description samples collected from our local environment. g) Visual-language planning data: We also include highlevel planning data from RoboVQA [40], EgoPlan [7], and ALFRED [41], which provide explicit step-by-step task decomposition and environment-aware planning supervision. The processed subset contains 241,603 data samples. h) Unsupervised movement prediction data: To enhance spatial understanding and low-level motion grounding, we construct small dataset of 10,575 samples in which we predict the movement transition between pairs of egocentric images. This unsupervised supervision allows the model learn spatial information without requiring manual annotation. i) DAgger experience data: Finally, we incorporate onpolicy trajectories collected through real-world executions with the DAgger algorithm [37]. We collected 70 successful traces with 3,629 EgoActing training samples that span navigation in local environments, object-approach behaviors, and simple human-interaction tasks. D. Skill Setup For downstream skills, we first finetune GROOT-N 1.5 model [34] to perform manipulation tasks, with details shown in Appendix XI. For locomotion, we adopt the official Unitree walking policy3 as the movement controller. We manually calibrate the robots motion to achieve positional precision of approximately 5 cm for forward/backward and lateral movements, and turning precision of about 5 degrees. For speaking and querying behaviors, we currently assess task success by directly inspecting the models predicted natural-language outputs. The stand-up and crouch-down skills are implemented only in simulation, as the current Unitree locomotion policy does not support these actions in real-world deployment. 3https://github.com/unitreerobotics/unitree sdk2 Actions for moving forward and turning left/right are merged, i.e., they are not treated as discrete steps, to enhance motion speed and perform more human-like movement. To enable faster movement of the robot, we amplify the forward distance predicted by the model by factor of 1.2. V. EXPERIMENTS A. Experimental Setup a) Inference: For EgoActor inference, we use stochastic sampling with temperature of 0.2. For all baseline models, we follow their original settings and apply greedy decoding. The instruction prompts we used for all different tasks are shown in Appendix XVI. b) Robot Setup: We deploy our model and all baseline methods on the same Unitree G14 humanoid robot for realworld experiments. The robot is equipped with pair of Unitree Dex3-1 hands5 and custom 2-DoF head to support active perception6. RealSense D455 camera7, as typical is mounted on camera model used in embodied projects, the custom head for RGB-only capture. We acquire 480p monocular RGB images from the camera and no depth data is leveraged in our model or experiments. B. Baselines To investigate the hypothesis of leveraging existing navigation models for the movement-focused component of our proposed EgoActing, we evaluate the navigation success rates of several representative navigation models based on visionlanguage foundation models on some of our benchmark tasks (including the movement part of Human Interaction, Traversability, and our virtual environment EgoActing benchmark, all of which are introduced in Sections V-C and V-D). The baseline models considered are as follows: 1) NaVid [60] is video-based large visionlanguage model for vision-and-language navigation that operates solely on monocular RGB video streams, achieving state-of-the-art map-free navigation and strong Sim2Real generalization. 2) Uni-NaVid [61] is video-based model that unifies multiple embodied navigation tasks within single framework, enabling general-purpose, long-horizon navigation in unseen real-world environments. 3) NaVILA [8] is two-level visionlanguageaction framework for legged robot navigation that converts language instructions into spatially grounded mid-level actions executed by locomotion policy. We use its VLM component as baseline. C. Real-world Benchmarking a) Human-robot Interaction: In the human-robot interaction benchmark, the robot is required to navigate toward specified person and perform the corresponding interaction, 4https://www.unitree.com/g1 5https://www.unitree.com/Dex3-1 6https://github.com/BAAI-Agents/PAK/ 7https://www.realsenseai.com/products/real-sense-depth-camera-d455f/ TABLE I: Single person human-robot interaction results comparing different models across three tasks. Model NaVILA-7B NaVid-7B UniNaVid-7B EgoActor-4B EgoActor-8B Single Person Tasks Approach Say hi Ask for location Request items 2/12 8/12 8/12 12/12 12/12 - - - 12/12 12/12 - - - 12/12 12/12 - - - 11/12 12/12 TABLE II: Multi-person human-robot interaction results for the Say Hi task using different model sizes. Model Multi-person Attributes (Out-of-distribution) Clothing Accessories Posture Direction Gender EgoActor-4B EgoActor-8B 8/12 11/12 7/12 10/12 8/12 10/12 11/12 12/12 10/12 11/ such as greeting, requesting information (e.g., locations, etc.), or asking for help with an item. Setup. All experiments are conducted in real-world environments and with people whose appearances and clothing are entirely different from those seen in the training data. For navigation-only baseline models, we evaluate whether the robot can stop at an acceptable location in front of visible person (within approximately one meter and facing the person). For EgoActor, successful execution additionally requires generating an appropriate interaction action, such as requesting information. To further assess person-disambiguation capabilities, we design unseen scenarios involving multiple individuals that differ in attributes such as clothing color, accessories, posture, facing direction, and gender. In each trial, the spatial arrangement of the individuals is randomized. Results. As shown in Table and II, both the 4B and 8B variants of EgoActor are generally able to guide the robot to approach person and perform basic interactions. However, the 4B model shows weaker performance in scenarios with multiple people, particularly when fine-grained attribute-based identification is required. In contrast, the 8B model is able to identify the target person and carry out the instructed interaction in most tested cases. We emphasize that human interaction in our setting requires not only accurate navigation, but also the ability to translate task intent into appropriate body postures or dialogue, which we further examine through qualitative case studies in Appendix XIV-C. b) Mobile Manipulation: In the mobile manipulation benchmark, the robot is required to navigate to approach target object and execute the corresponding manipulation action, such as picking or placing the object. An example of EgoActor controlling the robot to conduct mobile manipulation task is shown in Appendix XIV-A. Setup. We evaluate the model in an unseen layout of the experimental environment, where desk, layout, and surrounding objects are arranged differently from those used during training. Target objects are placed at three distinct positions TABLE III: Unseen layout environment results on the EgoActing Mobile Manipulation benchmark. Best results are bold. Seen Objects Unseen Objects Models EgoActor-4B EgoActor-8B Models EgoActor-4B EgoActor-8B Approach and Pick Approach and Place Apple Bottle Apple Bottle 5/6 5/6 5/6 6/6 3/6 6/6 4/6 6/6 Approach and Pick Approach and Place Pen Holder Pink Cup Pen Holder Pink Cup 3/6 5/ 2/6 6/6 4/6 4/6 4/6 5/6 TABLE IV: Results on the proposed EgoActing Traversability benchmark. Best results are bold. Models Seen Environments Unseen Environments Enter rooms Leave rooms Enter rooms Leave rooms Left Right Left Right Left Right Left Right NaVILA-7B NaVid-7B UniNaVid-7B EgoActor-4B EgoActor-8B 5/12 3/12 4/12 11/12 11/12 4/12 5/12 3/12 11/12 12/12 3/12 9/12 6/12 12/12 10/12 3/12 8/12 4/12 10/12 10/ 2/8 1/8 2/8 7/8 7/8 1/8 0/8 0/8 7/8 7/8 3/8 8/8 5/8 7/8 8/8 1/8 4/8 5/8 7/8 7/8 on the desk (left, center, and right), and each position is tested twice to account for stochasticity. For the EgoActor model, the evaluation includes both in-distribution and outof-distribution object categories, whereas all object categories are in-distribution for the manipulation model. Pick and place tasks are evaluated separately. Variations in object height are currently not considered in the real-world setting and are only analyzed in the virtual environment through additional case studies in Section V-E. In certain few cases, manipulation may fail due to lower-level execution policy issues, even when the target object is fully within the robots reachability; such cases are still counted as successful if manipulation was triggered at the correct moment and position. Results. We observe that the 8B EgoActor model is generally able to navigate to the correct objects and successfully support manipulation for both in-distribution and outof-distribution objects under the unseen layout, indicating reasonable robustness to object and scene variations. For the 4B model, failures mainly occur when it predicts manipulation action while still too far away from the target. c) Traversability: Traversability evaluates whether the robot can safely navigate through narrow spaces commonly encountered in daily environments without colliding with surrounding obstacles. As mentioned by Team [46], most of the current VLM-based navigation models would suffer from hitting obstacles in the real-world environment. We evaluate these baseline models together with our EgoActor. Setup. We focus on room entry and exit scenarios, as doorways are typically narrow and have been observed in preliminary tests to be particularly challenging and collisionprone for humanoid robots. The evaluation includes five realworld rooms, including three seen environments and two unseen ones. The evaluated rooms consist of three meeting rooms (seen during training), private office, and storage room, Fig. 4: Multi-step illustration of obstacle avoidance generalization of our model, when faced with an unseen string obstacle. Fig. 5: First-person view of an EgoActors active perception trace. Color description blocks highlight models behaviors. which differ in layout, visual style, and object arrangement. All room layouts are provided in Appendix XIII. For each room, we assess both entry and exit behaviors. To assess robustness to initial conditions, the robot is placed at 2 different starting positions for each trialon the left and right sides of the doorwayand each position is tested 4 times. Results. Quantitative results are reported in Table IV. Our results show that EgoActor is generally able to traverse narrow passages and avoid collisions more reliably than baseline VLM-based navigation models. In contrast, existing VLN models often collide with door frames or nearby obstacles. We also observe that some baselines, such as NaVid, are generally effective at exiting rooms, but occasionally perform unnecessary rotations before door traversal, even when straight path is available. These observations suggest that EgoActor demonstrates improved robustness in narrow-space movements. In addition, we conduct qualitative obstacle-avoidance experiments under unseen layouts and with unseen obstacles. These case studies further demonstrate the robustness of EgoActor in navigating narrow spaces and are discussed in detail in Section V-E and Appendix XIV-B. D. Virtual Environment Benchmarking Setup. As described in Section IV-C, we evaluate our EgoActor models using 205 labeled EgoActing samples colTABLE V: Virtual environment results on our virtual benchmark. Best results are bold. represents meters in the table. Models Distance to the Goal Position < 0.5 < 0.8 < 1.0 < 1.2 < 1.5 < 2.0 < 2.5 < 3.0 Natural Language Action F1 Final View Similarity NaVILA-7B NaVid-7B UniNaVid-7B EgoActor-4B EgoActor-8B 8.3% 8.8% 6.3% 50.7% 51.4% 21.0% 15.1% 15.6% 63.7% 66.5% 26.3% 20.5% 20.5% 70.6% 69.9% 28.8% 23.9% 23.9% 74.1% 74.1% 33.7% 31.7% 28.3% 78.9% 78.5% 41.5% 42.0% 35.1% 84.4% 84.1% 46.3% 52.2% 43.9% 86.5% 87.8% 52.2% 60.0% 51.7% 87.8% 89.9% - - - 0.60 0. 0.35 0.37 0.36 0.41 0.41 Fig. 6: First-person view of an EgoActors traversability trace, showing the robot walking through doorway. lected in unseen virtual environments. Following the scene split protocol of the VLNCE dataset [24], all evaluation samples are drawn from environments unseen during training. Because LLM-based inference is stochastic, we evaluate each EgoActor model three times on the test set and report the average performance as the final result, with per-run results provided in Appendix XII. The evaluation results are summarized in Table V. For the evaluation metrics, we consider the precision of the ending position and the view similarity with the reference ending image. Additionally, we calculate the difference between the predicted natural language action and the reference natural language action by examining the F1 score of the 1-gram overlapping of them. Results. As shown in Table V, EgoActor generalizes well to unseen environments and target objects, with the 8B model performing slightly better under smaller distance thresholds, while the 4B and 8B models achieve overall comparable performance. We check the failure cases and find that most errors arise from ambiguous labelled instructions or visually degraded or blurry virtual environments, with additional failures occurring in unfamiliar scene types such as churches or historical sites that differ substantially from the training data. For baseline models, we observe that under the standard VLN success criterion (<3.0 m), performance remains comparable to their reported VLNCE Room-to-Room results (around 50%). However, under stricter criteria requiring precise positioning for interaction, these models frequently fail to stop at appropriate locations, often hallucinating continued navigation instead of stopping to execute the intended interaction. E. Case Studies In this section, we discuss representative case studies that demonstrate EgoActors capabilities in obstacle avoidance, active perception, spatial understanding, and human-like movement. detailed example of human interaction is provided in Appendix XIV-C. a) Obstacle Avoidance: Through all real-world experiments, the G1 robot rarely collides with obstacles. In couple Fig. 7: First-person view of an EgoActors height change ability trace in virtual environments. Color description blocks highlight models behaviors. instances, minor side collisions occur when the robot focuses on avoiding large obstacle ahead and temporarily loses sight of small obstacles that have previously passed its view. Moreover, as illustrated in Fig. 4, the model can generalize and handle unseen obstructions, like the rope/stripe, and find way to walk around them. b) Active Perception: We observe that the robot looks downward to verify obstacle positions while passing them, improving obstacle-avoidance success. Toward the end of trajectory, the model also actively keeps its gaze fixed on the target object to enable smoother transition to manipulation or interaction actions. The case shown in Fig. 5 illustrates that the model actively moves backwards and looks upward to identify the color of upper-body clothing when only the lower bodies of two people are initially visible, as the instruction requires greeting the person wearing specific shirt. c) Spatial Understanding: Unlike VLMs trained primarily in simulation, our model learns from human videos and thus develops stronger spatial understanding. For example, it predicts different forward-movement distances when encountering clear, wide path versus partially obstructed path, or negotiating doorway corner. Its predicted turning angles also adapt appropriately across different spatial configurations. An example of this in mobile manipulation scenario is shown in Appendix XIV-A. d) Human-like Behaviors: Trained on real-world videos, the model naturally exhibits human-like movement behaviors. It may move backward when too close to obstacles or after completing manipulation to reorient toward the next target. During corner turning, the model often combines forward action, turning, and strafing (e.g., turning left while moving forward and strafing right), mirroring how humans execute smooth turns while maintaining visual alignment with the path (see Fig. 5 and Fig. 6). Height adjustments are also observed in the virtual environment experiments (see Fig. 7). VI. CONCLUSION We propose EgoActor, unified visionlanguage model that addresses overlooked challenges in grounding high-level task intentions into egocentric, executable multi-step actions for humanoid robots in real-world settings, without requiring extra sensing modalities, multiple cameras, nor extensive teleoperation. By jointly predicting locomotion, manipulation, human interaction, and head movements, EgoActor tightly integrates perception and execution in dynamic environments. Trained on easily scalable diverse real-world, spatial reasoning, and simulated data, the model demonstrates strong generalization and timely inference. Extensive evaluations in both simulation and physical robots show EgoActor effectively bridges abstract task planning and low-level action execution, offering practical step toward scalable humanoid autonomy. EgoActor is to be released as an open testbed to support future research (including code, models, dataset, and benchmark). Current limitations are further discussed in Appendix VII. REFERENCES [1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. [2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36743683, 2018. [3] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Manas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra. An introduction arXiv preprint arXiv: to vision-language modeling. 2405.17247, 2024. [4] Yihan Cao, Jiazhao Zhang, Zhinan Yu, Shuzhen Liu, Zheng Qin, Qin Zou, Bo Du, and Kai Xu. Cognav: Cognitive process modeling for object goal navigation with llms. arXiv preprint arXiv: 2412.10439, 2024. [5] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33: 42474258, 2020. [6] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12538 12547, 2019. [7] Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv: 2312.06722, 2023. [8] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. arXiv preprint arXiv: 2412.04453, 2024. [9] Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Expressive wholearXiv preprint Ge Yang, and Xiaolong Wang. body control arXiv:2402.16796, 2024. for humanoid robots. [10] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. Advances in Neural Information Processing Systems, 35:59825994, 2022. [11] Vishnu Sashank Dorbala, James Mullen, and Dinesh Manocha. Can an embodied agent find your cat-shaped IEEE mug? LLM-based zero-shot object navigation. Robotics and Automation Letters, 9(5):40834090, 2023. [12] Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: framework In Proceedings of the for visual object manipulation. IEEE/CVF conference on computer vision and pattern recognition, pages 44974506, 2021. [13] Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, Herve Jegou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Louis-Philippe Morency, Theo Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Paden Tomasello, and Jitendra Malik. Embodied ai agents: Modeling the world, 2025. URL https://arxiv. org/abs/2506.22355. [14] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: unified benchmark for generalizable manipulation skills. In The Eleventh International Conference on Learning Representations. [15] Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin Wang. Vision-and-language navigation: survey of tasks, methods, and future directions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 76067623, 2022. [16] Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Velez, Qingyun Wu, Huazheng Wang, Thomas L. Griffiths, and Mengdi Wang. Embodied llm agents learn to cooperate in organized teams. arXiv preprint arXiv: 2403.12482, 2024. [17] Yicong Hong, Cristian Rodriguez, Qi Wu, and Stephen Gould. Sub-instruction aware vision-and-language navigation. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 33603376, 2020. [18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv: 2106.09685, 2021. [19] Yayu Huang, Dongxuan Fan, Haonan Duan, Dashun Yan, Wen Qi, Jia Sun, Qian Liu, and Peng Wang. Humanlike dexterous manipulation for anthropomorphic fivefingered hands: review. Biomimetic Intelligence and Robotics, page 100212, 2025. [20] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and comIn Proceedings of the positional question answering. IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [21] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv: 2504.16054, 2025. [22] Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Egotaskqa: Understanding human tasks in Huang. egocentric videos. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/paper/2022/hash/ 161c94a58ca25bafcaf47893e8233deb-Abstract-Datasets and Benchmarks.html. [23] Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Jim Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 1692316930. IEEE, 2025. [24] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Visionand-language navigation in continuous environments. In European Conference on Computer Vision, pages 104 120. Springer, 2020. [25] Jacob Krantz, Erik Wijmans, Arjun Majundar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision and language navigation in continuous environments. In European Conference on Computer Vision (ECCV), 2020. [26] Yujeong Lee, Sangwoo Shin, Wei-Jin Park, and Honguk Woo. LLM-based offline learning for embodied agents In Yaser Alvia consistency-guided reward ensemble. Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 30063029, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.170. URL https://aclanthology.org/2024.findings-emnlp.170/. [27] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [28] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martın-Martın, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, and Li FeiBehavior-1k: human-centered, embodied ai Fei. benchmark with 1,000 everyday activities and realistic simulation. arXiv preprint arXiv: 2403.09227, 2024. [29] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, and Jiajun Wu. Embodied agent interface: Benchmarking llms for embodied decision making. arXiv preprint arXiv: 2410.07166, 2024. [30] Qiayuan Liao, Takara Truong, Xiaoyu Huang, Yuman Gao, Guy Tevet, Koushil Sreenath, and Karen Liu. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241, 2025. [31] Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, and Zongqing Lu. Being-h0: Vision-language-action pretraining from large-scale human videos. arXiv preprint arXiv: 2507.15597, 2025. [32] Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando Castaneda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, et al. Sonic: Supersizing motion tracking for natural humanoid whole-body control. arXiv preprint arXiv:2511.07820, 2025. [33] Andrew Melnik, Michael Buttner, Leon Harz, Lyon Brown, Gora Chand Nandi, Arjun PS, Gaurav Kumar Yadav, Rahul Kala, and Robert Haschke. Uniteam: Open vocabulary mobile manipulation challenge. arXiv preprint arXiv:2312.08611, 2023. [34] NVIDIA, Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Jim Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: An open foundation model for generalist humanoid robots. In ArXiv Preprint, March 2025. [35] Yuankai Qi, Zizheng Pan, Shengping Zhang, Anton van den Hengel, and Qi Wu. Object-and-action aware In European model for visual conference on computer vision, pages 303317. Springer, 2020. language navigation. [36] Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, and Hengshuang Zhao. Vln-r1: Vision-language navigation via reinforcement fine-tuning. arXiv preprint arXiv: 2506.17221, 2025. [37] Stephane Ross, Geoffrey J. Gordon, and J. Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. International Conference on Artificial Intelligence and Statistics, 2010. and regret. Lora without Connectionism, 2025. https://thinkingmachines.ai/blog/lora/. Lab. Thinking Machines Lab: doi: 10.64434/tml.20250929. Thinking Machines [38] John Schulman [41] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. arXiv preprint arXiv: 1912.01734, 2019. [42] Chan Hee Song, Jiaman Wu, Clayton Washington, LlmBrian Sadler, Wei-Lun Chao, and Yu Su. planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009, 2023. [43] Jingwen Sun, Jing Wu, Ze Ji, and Yu-Kun Lai. IEEE Transactions survey of object goal navigation. on Automation Science and Engineering, 22:22922308, 2024. [44] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in neural information processing systems, 34:251266, 2021. [45] Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, Devon Hjelm, Zhe Gan, Zsolt Kira, and Alexander Toshev. From multimodal llms to generalist embodied agents: Methods and lessons, 2024. URL https://arxiv.org/abs/2412.08442. [46] InternVLA-N1 Team. InternVLA-N1: An open dualsystem navigation foundation model with learned latent plans, 2025. [47] Qwen Team. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. [48] Karthik Valmeekam, Matthew Marquez, large Sarath On the Sreedharan, and Subbarao Kambhampati. - language models planning abilities of In A. Oh, T. Naumann, critical investigation. and A. Globerson, K. Information S. Levine, editors, Advances in Neural Processing 75993 volume 76005. Curran Associates, Inc., 2023. URL https: //proceedings.neurips.cc/paper files/paper/2023/file/ efb2072a358cefb75886a315a6fcf880-Paper-Conference. pdf. Saenko, M. Hardt, Systems, pages 36, [39] Bipasha Sen, Michelle Wang, Nandini Thakur, Aditya Agarwal, and Pulkit Agrawal. Learning to look around: Enhancing teleoperation and learning with human-like In CoRL 2024 Workshop on Wholeactuated neck. body Control and Bimanual Manipulation: Applications in Humanoids and Beyond. [40] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, Pete Florence, Wei Han, Robert Baruch, Yao Lu, Suvir Mirchandani, Peng Xu, Pannag Sanketi, Karol Hausman, Izhak Shafran, Brian Ichter, and Yuan Cao. Robovqa: Multimodal long-horizon reasoning for robotics. arXiv preprint arXiv: 2311.00899, 2023. [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv: 1706.03762, 2017. [50] Fangyuan Wang, Shipeng Lyu, Peng Zhou, Anqing Duan, Guodong Guo, and D. Navarro-Alarcon. Instructionaugmented long-horizon planning: Embedding grounding mechanisms in embodied mobile manipulation. AAAI Conference on Artificial Intelligence, 2025. doi: 10. 48550/arXiv.2503.08084. [51] Hanqing Wang, Wenguan Wang, Wei Liang, Steven CH Hoi, Jianbing Shen, and Luc Van Gool. Active perception for visual-language navigation. International Journal of Computer Vision, 131(3):607625, 2023. [63] Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, and Parisa Kordjamshidi. Vision-and-language navigation today and tomorrow: survey in the era of foundation models. Transactions on Machine Learning Research. [64] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu. Steve-eye: Equipping llm-based embodied arXiv agents with visual perception in open worlds. preprint arXiv:2310.13255, 2023. [52] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with large language models. arXiv preprint arXiv:2307.01848, 2023. [53] Ruihan Yang, Yejin Kim, Rose Hendrix, Aniruddha Kembhavi, Xiaolong Wang, and Kiana Ehsani. Harmonic In 2024 IEEE/RSJ International mobile manipulation. Conference on Intelligent Robots and Systems (IROS), pages 36583665. IEEE, 2024. [54] Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. Embodied multi-modal agent trained by an llm from In Proceedings of the IEEE/CVF parallel textworld. conference on computer vision and pattern recognition, pages 2627526285, 2024. [55] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, et al. Homerobot: Open-vocabulary mobile manipulation. In 7th Annual Conference on Robot Learning. [56] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, and Li Fei-Fei. arXiv Spatial mental modeling from limited views. preprint arXiv: 2506.21458, 2025. [57] Mingming Yu, Fei Zhu, Wenzhuo Liu, Yirong Yang, Qunbo Wang, Wenjun Wu, and Jing Liu. C-NAV: Towards self-evolving continual object navigation in In The Thirty-ninth Annual Conference open world. on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=SbfdxWibDn. [58] Haoqi Yuan, Yu Bai, Yuhui Fu, Bohan Zhou, Yicheng Feng, Xinrun Xu, Yi Zhan, Borje F. Karlsson, and Zongqing Lu. Being-0: humanoid robotic agent with vision-language models and modular skills. arXiv preprint arXiv: 2503.12533, 2025. [59] Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. Envgen: Generating and adapting environments via llms for training embodied agents. arXiv preprint arXiv:2403.12014, 2024. [60] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and Wang He. Navid: Video-based vlm plans the next step for vision-and-language navigation. Robotics: Science and Systems, 2024. doi: 10.48550/arXiv.2402. 15852. [61] Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang. Uni-navid: video-based visionlanguage-action model for unifying embodied navigation tasks. ROBOTICS, 2025. doi: 10.15607/rss.2025.xxi.013. [62] Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, and Debin Zhao. Flam: Foundation model-based body stabilization for humanoid locomotion and manipulation. arXiv preprint arXiv: 2503.22249, 2025."
        },
        {
            "title": "CONTENTS",
            "content": "VII. LIMITATIONS II III IV Introduction Related Work II-A II-B II-C VLM-based Embodied Agents . . Mobile-Manipulation . . . Visual-Language Navigation . . . . . . . . . . . . . . . . Framework Design III-A III-B Task Definition . . Language-based Actions . . . . . . . . . . . . . . . . . . . Training Recipe IV-A Model Structure and Training Setup . . . . . . IV-B . . . . . . IV-C . . . . IV-D Data Format . Data Acquisition . . Skill Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiments V-A V-B V-C V-D V-E . . . . . . . . . . . Experimental Setup . . . . . . . . . Baselines . Real-world Benchmarking . . . . . Virtual Environment Benchmarking . . . . . . . Case Studies . . . . . . . . . . . . . . . . . . . . VI Conclusion VII Limitations VIII Data Format IX Data Processing Details IX-A IX-B . . . . EgoTaskQA Data Processing . Virtual Environment Data Processing . . . Supported Skills XI Training Details of the Manipulation Model XII Detailed Results for All the Three Runs XIII Traversability Scenes XIV Additional Case Study XIV-A Mobile Manipulation Case Study . . . . . . . . . XIV-B Obstacle Avoidance . . . . . XIV-C Human-robot Interaction . . . . . . . . . XV Data Samples for the EgoActing Task XVI Prompts for Different Tasks 2 2 2 2 3 3 3 4 4 4 5 5 6 6 6 6 7 8 9 13 14 14 14 15 15 15 16 16 16 17 17 17 The effectiveness of the proposed EgoActor relies heavily on the reliability of external components, including high-level planners (e.g., large language models) and downstream skills such as locomotion policies and visual-language-action models for manipulation. On its own, EgoActor does not function as fully end-to-end system, as it depends on these supporting models. Future work could explore integrating these capabilities into single, unified framework to facilitate more seamless deployment on humanoid robots. Another limitation lies in its handling of long-term context: the model may occasionally fall into locally optimal but incorrect decision patterns when navigating extended or multi-stage tasks. VIII. DATA FORMAT To enable robust grounding from high-level instructions to executable humanoid actions, we design structured EgoActing prompt that explicitly exposes the model to egocentric visual context, temporal history, and recent actionobservation pairs. The prompt frames the model as visionlanguage agent operating from first-person perspective and requires it to reason over both long-term historical observations and shortterm recent frames. By constraining the output to predefined set of low-level locomotion, perception, manipulation, and interaction primitives, the prompt encourages spatially grounded, temporally coherent decision-making while preventing unconstrained or hallucinated responses. This design allows the model to infer the next usable action step conditioned on instruction intent, environmental state, and execution history, closely aligning the inference process with real-world humanoid control requirements. EgoActing Prompt Design You are Vision Language Model specialized in processing the first person view images of embodied robots. Your task is to analyze the provided image and respond to queries with answers. Focus on the spatial relations in the image and make the right decisions. Given the following instruction, series of sampled historical observation and recent observation image frames, predict usable action sequence that you should perform next. Output format: Turn [direction] [degrees] degrees; Look [direction] [degrees] degrees; Move [direction] [distance] meters; [direction] sidewalk [distance] meters; [manipulation action text]; [interaction action text]; Stop and no action. Your task is: [instruction] Sampled Historical Observations: [Sampled Historical Observation #1] XVII Difference between our work and existing work 18 [Sampled Historical Observation #2] [Sampled Historical Observation #3] [Sampled Historical Observation #4] [Sampled Historical Observation #5] [Sampled Historical Observation #6] [Sampled Historical Observation #7] [Sampled Historical Observation #8] [Sampled Historical Observation #9] [Sampled Historical Observation #10] Recent Observations: [Recent Observation #1] Next action: [Recent Action #1] [Recent Observation #2] Next action: [Recent Action #2] [Recent Observation #3] Next action: IX. DATA PROCESSING DETAILS A. EgoTaskQA Data Processing EgoTaskQA is benchmark that evaluates models event understanding capabilities with goal-oriented questions. The dataset provides fine-grained temporal annotations, including the start and end frames of atomic actions, which makes it suitable for constructing sequential decision-making data. To adapt EgoTaskQA into the EgoActing format for training visionlanguage models, we reorganize each annotated sequence into structured action-conditioned observation samples. a) Overview: Each processed training sample consists of three components: 1) global natural language instruction 2) set of historical observations 3) three interleaved recent observation-action pairs This structure is designed to accommodate EgoActing, where models must predict the next feasible action based on the historical and recent observations. b) Instruction Construction: EgoTaskQA does not provide explicit instruction annotations; instead, it includes detailed annotations of distinct action steps within each video. We first concatenate three adjacent actions (or fewer if insufficient actions are available) into single instruction using predefined set of connective patterns (e.g., and then, and next, continue to). For example, an instruction such as Get the tank from the table, and then open the tank contains two sequential manipulation actions. We define the start frame as 60 frames before the start frame of the first action, unless there are preceding actions or this offset exceeds the beginning of the video. c) Recent Observation-Action Pairs: We start with target frame that is supposedly used to train the model to predict the next action. For each target frame, we construct three temporally adjacent observationaction pairs by traversing the sequence backward with fixed stride of five frames. The corresponding action is determined according to the frames temporal role: 1) if the frame corresponds to the start of manipulation phase, the action is the extracted manipulation action; 2) if the frame lies within movement segment, the action is constructed by aggregating the calculated camera pose difference between frames; 3) if the frame follows the completion of the final manipulation step, special Stop and no action token is used. d) Navigation Action Aggregation and Filtering: Navigation actions are derived by accumulating camera pose changes (e.g., rotation, translation, vertical motion) between consecutive frames. The changes along opposite directions are algebraically combined to account for cancellation effects. To ensure action saliency, we apply magnitude-based thresholds (5 degrees for angular motion and 0.1 meters for translational motion) to determine whether navigation action is considered valid. e) Historical Observations: Historical observations provide long-term visual context. For each sample, we collect all frames between the instruction start frame and the first recent observation frame, regardless of phase boundaries. From this interval, ten frames are uniformly sampled to form the historical observation set. This strategy avoids reliance on phase-specific image lists and ensures consistent temporal coverage. f) Sample Variants: For manipulation actions, we construct two types of samples: one where the target action is manipulation instruction, and another where the target action is the Stop and no action token, indicating task completion. g) Final Dataset: Following the above procedure, EgoTaskQA sequences are converted into structured EgoActing samples with aligned visual context, recent action history, and explicit action supervision. This processed dataset enables training models to jointly reason over instruction context, recent egocentric observations, and temporally grounded actions. The raw dataset contains 6,599,590 samples, of which 531,090 include natural language action predictions. Due to computational constraints, we randomly sample 40,000 instances from the full set and 120,000 instances from the subset containing natural language action predictions, resulting in 160,000 training samples for EgoActing. B. Virtual Environment Data Processing This section describes the processing pipeline used to convert labeled virtual-environment trajectories into training and evaluation samples compatible with EgoActing-style action prediction. alization to unseen actions, and direct grounding of task intent into executable language commands. a) Trajectory Parsing: Each episode is stored as trajectory consisting of RGB observations and discrete low-level actions. We first normalize action names (e.g., MOVE_FORWARD, TURN_LEFT, LOOK_UP) and append terminal STOP action to mark episode completion. For each step, the corresponding RGB image path is recorded, forming an aligned imageaction sequence. b) Action Merging: To better reflect continuous robot motion and reduce action sparsity, we merge pairs of consecutive low-level actions into single executable instruction. The merging process aggregates rotations, forward motion, lateral strafing, and camera pitch adjustments, while introducing small randomized perturbations to movement distances and angles to improve robustness. Terminal actions are mapped to unified Stop and no action command. c) Sliding-Window Sample Construction: From each episode, we construct multiple training samples using sliding-window strategy. For given window position, three recent observationaction pairs are selected at fixed temporal intervals, along with all preceding images treated as historical context. This design enables the model to jointly reason over long-term visual history and short-horizon action prediction. d) Final Sample Format: Each sample consists of (1) natural-language instruction, (2) sequence of recent imageaction pairs, and (3) set of historical observation images. All samples are serialized into unified JSON format for downstream training and evaluation. This pipeline yields instructionconditioned samples that closely match the inference-time setting of EgoActor in virtual environments. temporally coherent, X. SUPPORTED SKILLS Table VI summarizes the set of skills supported by EgoActor, covering movement, perception, manipulation, and humanrobot interaction. We represent these skills using two complementary forms of language-based actions: structured language language actions for spatial control and natural actions for open-ended interaction. a) Structured language actions: Movement and active perception are modeled using structured, interpretable action templates that explicitly specify the action type, direction, and magnitude (e.g., Turn left 30.0 degrees, Move forward 0.26 meters). These actions support egocentric locomotion, body-height adjustment, lateral motion, and head orientation, enabling precise spatial positioning from visual observations. Small-magnitude motions are filtered to reduce noise and instability. b) Natural language actions: Manipulation and humaninteraction skills are represented using natural language actions rather than fixed action inventory. This includes object manipulation (e.g., picking, placing, opening), communicative behaviors (e.g., speaking, asking), and gesture-based interactions. This representation allows flexible composition, gener-"
        },
        {
            "title": "We suggest that the natural language actions could be easily",
            "content": "extended to facilitate more varied actions in the future. XI. TRAINING DETAILS OF THE MANIPULATION MODEL We conducted full fine-tuning of the GROOT-N 1.5 model [34] for 40,000 training steps using an 80GB A800 GPU with batch size of 50 with the official implementation8. For each task, we expanded the task descriptions to multiple different versions and randomly sampled one description at each training iteration to increase linguistic diversity. Data were collected using monocular RGB camera. During acquisition, all objects were placed on white table with height of 70 cm. The grasped objects included apples, water bottles, plastic cups, oranges, tissue boxes, pen holders, and bowls. Containers consisted of both square and round plates, as well as shallow baskets. In total, the dataset comprises approximately 700 samples. XII. DETAILED RESULTS FOR ALL THE THREE RUNS We report the per-run evaluation results corresponding to the averaged performance presented in the main paper. To account for stochasticity introduced by LLM-based inference, we conduct three independent evaluation runs under identical settings. Tables VII and VIII summarize the success rates at different distance thresholds for each run and different models, enabling finer-grained comparison of performance stability and variance across runs and model scales. XIII. TRAVERSABILITY SCENES Fig. 8 illustrates the real-world environments used in the traversability evaluation. Traversability focuses on assessing whether humanoid robot can safely navigate through narrow spacesparticularly doorwayswithout colliding with surrounding obstacles, which is common failure mode for visionlanguagemodel-based navigation systems in realworld settings [46]. We evaluate traversability using room entry and exit scenarios, as doorways are typically constrained in width and require precise control and obstacle awareness. The evaluation includes five real-world rooms: three meeting rooms that are seen during training, and two unseen environments consisting of private office and storage room. These rooms differ in layout, visual appearance, carpet, and object arrangement, providing diverse traversal conditions. For each room, both entering and exiting behaviors are tested. To assess robustness to initial positioning, the robot starts from two different locations relative to the doorway (left and right), with four repeated trials per starting position. This setup allows systematic evaluation of collision avoidance and narrow-passage traversal across varying spatial configurations. 8https://github.com/NVIDIA/Isaac-GR00T TABLE VI: Supported skills in our training datasets. Skill Category Skill Description Action Example Movement Skills Structured Language Skills Move forward/backward Turn left/right Strafe left/right Stand up Crouch down Move forward/backward 0.26 meters Turn left/right 30.0 degrees Left/right sidewalk 0.40 meters Rise up 0.12 meters Lower down 0.08 meters Active Perception Skills Look up/down Look up/down 10.0 degrees Human Interaction Skills Manipulation Skills Natural Language Skills Confirm/denial gesture Say hi Speak Ask Grab/Grasp/Pick up Pull Place on Open Close Wash Pour from into Turn on Turn off Point to Drop Confirm with the woman in front of you Say hi to the boy Speak How you doing? Ask Where is the bathroom? Pick up the water bottle Pull the drawer Place the plate on the desk Open the door Close the door Wash hands Pour from the bottle into the cup Turn on the washing machine Turn off the lamp Point to the painting Drop the garbage TABLE VII: Multi-threshold success rates across three evaluation runs for the 4B EgoActor model. Run 1 Run 2 Run"
        },
        {
            "title": "Count",
            "content": "0.5 0.8 1.0 1.2 1.5 2.0 2.5 3.0 52.20% 107/205 62.93% 129/205 70.24% 144/205 73.17% 150/205 78.05% 160/205 84.39% 173/205 86.34% 177/205 87.32% 179/205 0.5 0.8 1.0 1.2 1.5 2.0 2.5 3.0 48.29% 99/205 62.93% 129/205 68.78% 141/205 74.15% 152/205 80.00% 164/205 84.39% 173/205 87.80% 180/205 88.78% 182/205 0.5 0.8 1.0 1.2 1.5 2.0 2.5 3.0 51.71% 106/205 65.37% 134/205 72.68% 149/205 75.12% 154/205 78.54% 161/205 84.39% 173/205 85.37% 175/205 87.32% 179/ XIV. ADDITIONAL CASE STUDY We provide additional videos in the supplemental materials to illustrate EgoActors behavior across diverse scenarios. Occasional latency in the videos is primarily due to network instability rather than the models inference speed. The model itself operates with sub-second action prediction, while the observed end-to-end delay is mostly affected by transmission and streaming conditions. with the target. As it nears the object, the robot transitions to smaller, fine-grained movements to precisely refine its position for manipulation. The example further demonstrates EgoActors ability to adapt its motion to the spatial configuration of the scene: the target objecta previously unseen pink cupis picked up successfully despite the presence of another object (a pen holder) on the desk, highlighting robust spatial reasoning and fine positioning under cluttered conditions. A. Mobile Manipulation Case Study B. Obstacle Avoidance Fig. 9 illustrates EgoActor performing mobile manipulation task. We also provide videos of the illustrations in the supplemental materials. Starting from distant position, the robot first takes larger locomotion steps to efficiently approach the workspace and progressively adjusts its trajectory to align We provide additional qualitative examples of obstacle avoidance behaviors in the supplemental videos. These cases illustrate the models ability to navigate around static and dynamic obstacles under unseen layouts, particularly in narrow spaces. The videos highlight how EgoActor adjusts its locoTABLE VIII: Multi-threshold success rates across three evaluation runs for the 8B EgoActor model. Run 1 Run 2 Run"
        },
        {
            "title": "Count",
            "content": "0.5 0.8 1.0 1.2 1.5 2.0 2.5 3.0 48.78% 100/205 66.34% 136/205 68.78% 141/205 73.17% 150/205 78.05% 160/205 83.41% 171/205 86.83% 178/205 88.78% 182/205 0.5 0.8 1.0 1.2 1.5 2.0 2.5 3.0 51.71% 106/205 66.34% 136/205 68.78% 141/205 73.66% 151/205 77.07% 158/205 84.39% 173/205 88.29% 181/205 90.73% 186/205 0.5 0.8 1.0 1.2 1.5 2.0 2.5 3.0 53.66% 110/205 66.83% 137/205 72.20% 148/205 75.61% 155/205 80.49% 165/205 84.39% 173/205 88.29% 181/205 90.24% 185/ Fig. 8: An illustration of the different scenes we used in our traversability experiments. motion primitives to maintain safe clearance while preserving task progress. C. Human-robot Interaction Table IX presents representative examples of natural language actions predicted by EgoActor in human-interaction scenarios. Given high-level instructions with varying intents, the model generates context-appropriate verbal actions, such as asking for directions, requesting information, or initiating polite inquiries. Note that this task is trained with fewer than 20 samples in the training data. These examples suggest that EgoActor can flexibly map diverse instructional intents to corresponding linguistic actions, enabling basic informationseeking and communicative behaviors during humanrobot interaction. XV. DATA SAMPLES FOR THE EGOACTING TASK Table illustrates representative instructionaction pairs from both real-world and virtual environments for the EgoActing task. Each instruction specifies high-level goal, while the corresponding natural language action describes the concrete behavior executed by the agent. The examples highlight the diversity of tasks, including navigation, object interaction, and human interaction, and demonstrate how the dataset captures both real-world complexities and controlled virtual scenarios. This paired structure supports training and evaluation of models capable of grounding language instructions into egocentric, low-level action sequences. XVI. PROMPTS FOR DIFFERENT TASKS To evaluate EgoActor across diverse set of embodied capabilities, we design task-specific instruction prompts that are high-level yet explicit, ensuring that the intended goal and required actions are clearly specified without prescribing lowlevel motor details. Specifically, Table XI presents examples of human-interaction prompts, including greeting, asking for information, and requesting objects. These prompts cover both single-person and multiperson settings and require the model to resolve referential Fig. 9: An illustration of our model conducting the mobile manipulation task: Approach and grab the pink cup. TABLE IX: An example of the predicted natural language actions that show the EgoActor could transform the intentions in the instructions into actual words. Instructions Predicted Natural Language Actions Approach the person and ask him the way to cook Kung Pao Chicken Approach the person and ask him to show you the way to the reception Approach the person and ask the status of the air conditioner Approach the person and ask him to check the news politely Approach and ask the person to check the news Approach and ask the person to check the news Approach the person and ask him the status of the data labeling Approach the person and ask him to hand you the flowers Ask How do you cook Kung Pao Chicken? Ask Could you please show me the way to the reception? Ask How is the working of the air conditioner? Ask Could you please check the news? Ask Do you know the news? Ask check the news Ask How is the progress of the data labeling? Ask Could you please give me the flowers? ambiguity using visual attributes (e.g., clothing, gestures, relative position) or relational descriptions. Table XII summarizes the instruction prompts used for mobile manipulation tasks. These prompts focus on pickand-place behaviors with varying objects and appearances, requiring the robot to approach the workspace, localize the target, and execute manipulation actions under egocentric observations. Finally, Table XIII lists prompts for room-level tasks, such as entering or exiting rooms and performing simple interactions after navigation. These prompts are designed to assess the models ability to handle narrow passages, spatial transitions, and action sequencing in real-world indoor environments. XVII. DIFFERENCE BETWEEN OUR WORK AND EXISTING WORK Our work differs from prior research across three major axes: a) Scope of embodiment.: Most VLM-based embodied agents [48, 52, 42] and LLM-driven systems [1, 16, 26] focus on manipulators or simulated agents with simplified embodiments, often relying on predefined skill libraries. In contrast, EgoActor targets full humanoid robots, directly predicting egocentric, low-level actionsincluding locomotion, posture adjustment, head orientation, manipulation, and humaninteractionbridging abstract instruction reasoning with realworld motor control. b) Unified reasoning.: action Existing mobile- [12, 44, 53] manipulation and navigation frameworks typically decompose tasks into modular subgoals or stagewise controllers for perception, locomotion, and manipulation. EgoActing and EgoActor unify these components, allowing the model to jointly reason over heterogeneous action types and generate temporally coherent, context-aware sequences without explicit intermediate planning. c) Task generalization and real-world deployment.: [2, 63, 36] and Vision-and-Language Navigation (VLN) object-goal navigation methods [5, 4] primarily address static navigation or object-localization tasks. Our approach extends beyond navigation, supporting dynamic, long-horizon tasks TABLE X: Examples of instructionaction pairs in real-world and virtual environments. Instructions Natural Language Actions Real-World Environments Turn large right to get out of this area and say hi to the robot in blue shirt. Get out of this area and turn large left to the leftmost hallway. Turn left and go straight to kitchen, stop in front of the people. Search and approach the girl and ask her information about humanoid robots. Approach and grab the toy bear on the white table. Say hi to the robot Stop and no action Stop and no action Ask do you know anything about humanoid robots Grab the toy bear Virtual Environments Turn right until you see window, then point to it. Walk forward to the chair and pull out the red chair in the middle. Approach and put the towels on the bathtub. Approach the lamp on the left side of the bed and turn on the lamp. Approach and clean the mirror on the wooden cabinet. Point to the window Pull out the red chair in the middle Put the towels Turn on the lamp Clean the mirror TABLE XI: Examples of human-interaction task prompts used in our evaluation. Task Category Setting Prompt Say Hi Single Person Approach the person with the grey sweater Single Person Approach the person with brown shirt and say hi to him Multi Person Multi Person Multi Person Approach the person with grey shirt and say hi to him Approach the person with brown shirt and say hi to him Approach the person with black coat and say hi to him Approach the person with hat and say hi to him Approach the person with hat and say hi to him (swap) Approach the person with mask and say hi to him Approach the person touching their own head and say hi to him Approach the person with open palm hand and say hi to him Approach the person squatting and say hi to him Multi Person Approach the man and say hi to him Approach the woman and say hi to her Multi Person Approach the person on the left and say hi to him Approach the person on the right and say hi to him Ask the Location Single Person Hand Me Objects Single Person Approach the person with black sweater and ask him the location of the classroom Approach the person with black sweater and ask him the location of the kitchen Approach the person with black sweater and ask him the location of the restroom Approach the person with grey sweater and ask him to give you some flowers Approach the person with grey sweater and ask him to give you cup Approach the person with grey sweater and ask him to hand you controller that combine movement, manipulation, active perception, and human interaction in cluttered, unseen real-world environments. This enables robust instruction grounding into actionable motor sequences suitable for humanoid robots in practical deployment scenarios. TABLE XII: Task prompts for mobile manipulation (pick-and-place) tasks. Task Object Instruction Prompt Pick Pick Place Place Pick Pick Place Place Apple Bottle Apple Bottle Approach and grab the red apple on the desk Approach and grab the green bottle on the desk Approach and place the apple on the desk Approach and place the bottle on the desk Pen holder Pink cup Approach and grab the black pen holder on the desk Approach and grab the pink cup on the desk Pen holder Pink cup Approach and place the black pen holder on the desk Approach and place the pink cup on the desk TABLE XIII: Room navigation and interaction task prompts."
        },
        {
            "title": "Instruction Prompt",
            "content": "Room #1 / #2 / #"
        },
        {
            "title": "Go into the meeting room\nGet out of the meeting room",
            "content": "Room #4 Room #"
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence"
    ]
}