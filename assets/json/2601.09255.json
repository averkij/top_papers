{
    "paper_title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
    "authors": [
        "Yibo Zhao",
        "Hengjia Li",
        "Xiaofei He",
        "Boxi Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\\textit{PhyRPR}:\\textit{Phy\\uline{R}eason}--\\textit{Phy\\uline{P}lan}--\\textit{Phy\\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \\textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \\textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \\textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability."
        },
        {
            "title": "Start",
            "content": "PhyRPR: Training-Free Physics-Constrained Video Generation Yibo Zhao1 Hengjia Li1 Xiaofei He1 Boxi Wu1 1State Key Lab of CAD&CG, Zhejiang University 6 2 0 2 4 1 ] . [ 1 5 5 2 9 0 . 1 0 6 2 : r Fig. 1: Samples produced by our method. (ab) require physical priors, while (cd) emphasize strict motion constraints. Our three-stage pipeline PhyRPR (PhyReasonPhyPlanPhyRefine) decouples physical reasoning from rendering to better satisfy physical constraints while maintaining high-fidelity video quality. AbstractRecent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose training-free threestage pipeline, PhyRPR: PhyReasonPhyPlanPhyRefine, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability. Index TermsDiffusion Model, Reasoning video generation I. INTRODUCTION In recent years, diffusion-based video generation models have made remarkable progress, synthesizing high-fidelity and visually compelling, film-like content. However, despite their impressive visual realism, video diffusion models remain largely correlation-driven: they primarily exploit patterns in large-scale training data, rather than explicitly enforcing physical constraints. As result, they often fail in scenarios with clear physical constraints. For example, in Fig. 2, prior models fail to (1) keep oil floating on milk and (2) follow explicit directional cues. Since single-stage denoising does not explicitly model these constraints, they are often violated in practice, leading to inconsistent interactions and motion drift that undermine controllability and usability. In image generation, similar challenge arises: how to infer the deeper physical implications of prompt through reasoning. Prior methods address this via prompt enhancement or joint training that couples visionlanguage models with diffusion models. While effective for images in some cases, directly extending these strategies to video for physically consistent generation is often insufficient. Prompt enhancement typically targets surface-level appearance and is too imprecise to encode temporal dynamics. Training-based video approaches are also extremely expensive, and without dedicated physics-aware annotations, models struggle to acquire generalizable physical commonsense through implicit learning. Consequently, many video generators still rely on single-stage denoising process where physical reasoning is only learned implicitly and entangled with visual synthesis. This entanglement makes the generation difficult to control when explicit kinematic constraints or grounded interactions are required. To address this problem, we propose training-free framework that decouples physical understanding from visual synthesis via three-stage pipeline PhyRPR: PhyReason PhyPlanPhyRefine. The key idea is to separate what should is rendered by exposing intermedihappen from how it ate, physically meaningful representations. In PhyReason, we leverage large multimodal model to infer the underlying physical implications of the prompt and extract sequence of physically grounded key states. These states are visualized as semantically consistent keyframes, together with object-centric masks that provide explicit handles for subsequent control. In PhyPlan, we translate the discrete key states into continuous motion trajectories and synthesize coarse motion scaffold that explicitly encodes object dynamics and interactions. In PhyRefine, we integrate this scaffold into diffusion sampling through motion-aware noise-consistent latent fusion, so that the video model can refine textures and details while staying aligned with the planned dynamics. Overall, this staged design provides an interpretable and controllable generation process that better satisfies kinematic constraints and physics-grounded interactions. By combining LMM-based physical priors, deterministic planning tools, and the strong rendering capability of video diffusion models, our method forms physically interpretable, training-free video generation framework. It enables physically plausible generation as well as precise motion control. Extensive experiments demonstrate that our approach achieves clear improvements over existing methods in physical consistency, trajectory controllability, and overall visual quality. II. RELATED WORK A. Physics video generation Recent video generation models, like WanX [1], Hunyuanvideo [2], Cosmos [3] have demonstrated impressive generative capabilities, emerging as robust rendering backends for high-fidelity synthesis. However, they fundamentally function as statistical mimics lacking intrinsic physical understanding. This limitation is starkly highlighted by physics-centric benchmarks such as PhyGenBench [4] and VideoScience-Bench [5], which reveal that current models struggle with physical consistency across granular dimensionsranging in complex scientific reasoning. To bridge the gap between visual realism and physical plausibility, researchers have proposed strategies such as neural Newtonian dynamics [6], progressive physical alignment [7], and verifiable rewards [8], as well as VChain [9], which enhances physical generation capabilities through instance-specific fine-tuning. However, these methods typically rely on extensive physical datasets or require instancespecific test-time training. Fig. 2: Prior video generation models fail to accurately follow the provided physical constraints. B. Unified Multimodal for Understanding and Generation The landscape of multimodal has been fundamentally reshaped by Large Multimodal Models such as GPT-4 [10] and Gemini [11]. These foundation models have demonstrated unprecedented proficiency in both semantic reasoning and content generation, suggesting convergence of perception and synthesis tasks. Inspired by this success, recent research has explored architectures that intrinsically unify understanding and generation within single model. Specifically, Showo [12] uses discrete tokens while BLIP3-o [13] leverages continuous features to unify vision and language. Bagel [14] further validates the emergent capabilities of such pretraining. Inspired by this success in static images, recent research is extending these unification paradigms to the temporal domain. UniVid [15] and UniVideo [16] propose unified frameworks that jointly understand and generate videos via shared representations for dynamic scenes. However, constructing such understanding video generation models from scratch necessitates large-scale pretraining, incurring prohibitive computational costs and data demands. To address this, we propose training-free paradigm named PhyRPR By orchestrating the reasoning capabilities of off-theshelf multimodal models, precise toolkit for coarse video generation, and diffusion models for visual refinement, our framework decouples understanding from generation in physical motion processes. This enables precise physics-constrained synthesis without expensive parameter optimization. III. METHOD A. PhyReason: Visually Grounded Physical Reasoning To bridge the gap between textual physical understanding and video generation, the PhyReason stage aims to obtain accurate visual representations of key physical states by leveraging (i) large multimodal model (LMM) for reasoning and (ii) high-quality single-frame image generator for synthesis. Given user prompt P, we first use the LMM to construct physically constrained state-transition prompt sequence. To encourage physically valid dynamics rather than merely narrative coherence, we employ prompt engineering to explicitly focus on key kinematic moments, including pivotal transitions Fig. 3: Overview of our training-free three-stage pipeline PhyRPR: PhyReason, PhyPlan, and PhyRefine. Stage 1 (see III-A) outputs physically consistent keyframes and object states. Stage 2 (see III-B) uses an LMM to select motion primitives and parameters, and deterministically renders coarse motion video. Stage 3 (see III-C) applies motion-aware noise-consistent injection(NANC) to enforce the planned kinematics while preserving visual coherence. and representative states over longer intervals, rather than uniformly sampling timestamps. Formally, let pi denote the textual description of the physical state at the i-th kinematic milestone, and the entire sequence is jointly generated as: {pi}L i=1 = LMM(P). (1) While the textual state sequence provides clear highlevel scaffold, we further incorporate visual feedback to better align the state descriptions with concrete visual realizations. Concretely, we first synthesize the initial keyframe I1 from p1. For each subsequent milestone = 2, . . . , L, the LMM takes the target state prompt pi as input and directly observes the previous keyframe Ii1. Based on this visual feedback, it produces refined editing instruction ei that anchors the next step to salient visual attributes in Ii1, and an instructionguided image editing model generates the next keyframe: Ii = (cid:40) GT2I(p1) Gedit(Ii1, ei) if = 1, if > 1. (2) where GT2I is the text-to-image generator and Gedit is the instruction-guided editing model. Finally, we apply an image segmentation model to parse the generated keyframes in an object-centric manner, extracting binary mask Mi,k for each dynamic entity ok at milestone i: B. PhyPlan: Physics-Aware Motion Planning Given the semantically consistent keyframes produced by PhyReason, the PhyPlan stage converts discrete object states into continuous spatiotemporal trajectories and synthesizes motion-oriented coarse video that provides explicit kinematic guidance for diffusion refinement. We treat LMM as motion director. Given keyframe sequence {Ii}, the corresponding object masks {Mi,k}, and optionally the original prompt P, the model outputs structured motion script that specifies per-entity motion semantics and sequence of key states: = (cid:8)(cid:0)τk, {si,k}L i=1 (cid:1) (cid:12) (cid:12) ok O(cid:9) . (4) Here, denotes the set of dynamic entities extracted in PhyReason, τk is the motion primitive type of entity ok (e.g., Ballistic, Drifting, Linear), and {si,k}L i=1 represents sequence of normalized state vectors. Each si,k = [x, y, s, r, α] encodes position, scale, rotation, and opacity at milestone i. We implement lightweight toolkit for trajectory synthesis. For each neighboring pair of key states (between milestones and i+1), we instantiate the corresponding motion primitive Fτk . We then fit its physical parameters θ to satisfy the boundary conditions and improve temporal coherence. Let cstart and cend denote the endpoint positions (e.g., object centers) at the two key states. The continuous trajectory is: Mi,k = Sseg(Ii, ok), (3) c(t) = Fτk (cid:0)cstart, cend, t; θ(cid:1), (5) where Sseg denotes an open-vocabulary segmentation model that segments entity ok within frame Ii. Overall, through physically grounded reasoning and closedloop visual refinement, the PhyReason stage produces discrete set of physically self-consistent keyframes and object states, which serve as structured guidance for downstream planning and refinement. where c(t) denotes the continuous trajectory over time. Our toolkit includes several common primitives sufficient for use. Finally, we render the planned trajectories into coarse visual signal via (i) layout synthesis and (ii) content composition. Let {1, . . . , } index frames in the coarse video, and let st,k denote the per-frame state of entity ok. For each entity, we warp its initial mask M1,k and appearance crop K1,k according to st,k, and obtain an occupancy map Mt. We then composite the objects onto the inpainted background B(t): Ot,k = Trans(K1,k, st,k) αt,k Trans(M1,k, st,k), coarse = B(t) (1 Mt) + (t) (cid:88) Ot,k, (6) (7) where Trans() applies the geometric transformation specified by st,k, αt,k is its opacity component, and denotes element-wise multiplication. The resulting coarse video Vcoarse may exhibit stretched textures or imperfect details, but it preserves the intended topology and continuous trajectories, providing strong spatiotemporal guidance for latent-space refinement in the next stage. In PhyPlan, the planner selects motion primitives and parameters, and the toolkit deterministically renders coarse video with physically plausible dynamics. C. PhyRefine: Motion-Aware Visual Refinement While the coarse video from PhyPlan provides physically grounded motion scaffold, it lacks fine-grained visual detail. In PhyRefine, we propose motion-aware noise-consistent injection strategy that injects the scaffold as latent-space constraint during sampling, leveraging the pretrained video models rendering priors to improve motion adherence while refining appearance. We derive the latent space motion mask by downsampling the occupancy masks {Mt} to the latent resolution and broadcasting it to match xσ. Let xref be the reference clean 0 latent obtained by encoding the coarse video scaffold Vcoarse. At selected sampling steps, given the current noisy latent xσ and the model-predicted velocity vθ(xσ, σ), we aim to enforce the planned state inside the mask while avoiding distribution shifts that may harm global appearance consistency. We adopt an interpolation path for flow matching [17]: xσ = (1 σ)x0 + σϵ, (8) where σ [0, 1] is the noise level, x0 is the underlying clean latent, and ϵ is the noise realization. Under this path, the velocity field is defined by the derivative w.r.t. σ: = xσ σ = ϵ x0. (9) In practice, the video diffusion model predicts the velocity vθ(xσ, σ). Thus, ˆx0 and ˆϵ below denote the implied clean/noise components induced by (xσ, vθ) at the same σ: Fig. 4: Qualitative comparison with baselines. We use the first frame from PhyReason as the reference for I2V baselines. Our method better captures the physical process, including deformation during inflation and plausible rebound. Finally, we reconstruct new latent at the same noise level σ while explicitly preserving the recovered noise ˆϵ: xσ = (1 σ)x0 + σˆϵ. (12) This step applies the physically grounded correction through x0 but maintains the same noise manifold via ˆϵ. The sampler proceeds with its update rule using xσ as the current state. In practice, we apply this injection in early steps and continue sampling to obtain the final video. In PhyRefine stage, motion-aware noise-consistent injection strategy provides simple training-free mechanism to couple PhyPlans explicit kinematics with the video models rendering capability, producing videos that are both trajectoryconsistent and visually coherent. ˆx0 = xσ σvθ, ˆϵ = ˆx0 + vθ = xσ + (1 σ)vθ. (10) IV. EXPERIMENTS ˆx0 reflects the models current estimate of the clean video content, while ˆϵ preserves the noise that governs textures, colors, and style along the current sampling trajectory."
        },
        {
            "title": "We then inject the planned scaffold by replacing the clean",
            "content": "component only within the motion region: x0 = (1 m)ˆx0 + mxref 0 . (11) This update aligns object content inside the mask with the planned scaffold, while leaving remaining regions unchanged. A. Experimental Setup for Implementation Details. In PhyReason and PhyPlan, we use gemini-3-pro-preview as the LMM and segmentation. We synthesize milestone SAM3 [18] keyframes using Nano Banana Pro. In PhyRefine, we adopt Wan2.2-I2V-A14B as the diffusion model and perform training-free latent fusion during denoising. We evaluate under two common use cases of video diffusion models: text-conditioned generation and image-conditioned generation. Fig. 5: Qualitative comparison with baselines. We specify physical constraints via text prompts or arrow guidance. Compared to baseline methods, our approach generates videos that more faithfully satisfy the specified physical constraints. Method VBench LMM-as-judge (1-5) User study (1-10) Quality Temporal Overall Physical plausibility Trajectory compliance Temporal consistency Semantic alignment Overall Text alignment Physics plausibility Visual quality Overall WanX-T2V WanX-T2V-Enhance WanX-I2V WanX-I2V-Enhance LTX-Multi (-P) SDEdit-style (-R) PhyRPR (ours) 59.79 54.23 62.77 60.88 59.59 61. 63.30 95.78 91.94 97.24 95.26 97.54 97.71 97.89 83.78 79.37 85.75 83.80 84.89 85. 86.36 2.95 3.45 3.60 3.85 3.65 3.55 4.33 2.50 3.05 3.05 3.63 3.88 3. 4.78 3.10 3.73 4.33 4.45 4.10 4.38 4.78 2.15 3.23 3.83 4.13 4.00 3. 4.75 2.68 3.36 3.70 4.01 3.91 3.83 4.66 6.00 6.45 5.57 6.18 6.32 6. 7.83 5.51 5.55 5.79 5.99 5.86 6.01 6.84 6.92 6.82 7.16 7.49 6.02 6. 7.56 6.14 6.27 6.18 6.56 6.07 6.35 7.41 TABLE I: We evaluate with VBench, LMM-as-judge, and User study. -P,-R denotes the ablation setting without PHYPLAN and PHYREFINE. Our method consistently outperforms baselines under physics constraints. Under these settings, we design 40 diverse test scenarios spanning both text-only prompts and image+prompt pairs, and use the same set consistently for human evaluation, LMM-asjudge, and quantitative comparisons. The I2V tasks includes both descriptive motion control and arrow-guided control. Baseline Settings. We use Wan2.2-T2V-A14B and Wan2.2-I2V-A14B as the base models for our baselines. For fair comparison, each baseline is evaluated it supports. Specifically, we reonly under the protocol port Wan2.2-T2V-A14B and its prompt-enhanced variant (Wan2.2-T2V-Enhance) on the T2V cases, and evaluate Wan2.2-I2V-A14B and its prompt-enhanced variant (Wan2.2-I2V-Enhance) on the all cases. To control for the impact of prompt engineering, all -Enhance variants are produced by the same LMM-based rewriting procedure, which makes physical and kinematic constraints more explicit. For I2V, the LMM takes both the reference first frame and the original prompt as input to ensure the rewritten motion description remains consistent with the visual context. Evaluation Metrics. We adopt three complementary evaluations to jointly measure overall video quality and physical Consistency. First, we use VBench to quantify generic video quality aspects such as visual fidelity and temporal consistency. However, these metrics are often insufficient to determine whether model strictly follows physics-aware constraints. We employ Gemini as an LMM-as-judge to assess constraint satisfaction along four axes, each scored on 15 scale. Physical plausibility evaluates whether behaviors and interactions obey the physical or logical rules implied by the prompt, avoiding implausible artifacts. Trajectory compliance measures how accurately the video follows the specified motion path, directions, and event order. Temporal consistency checks object permanence over time, penalizing flicker, unprompted morphing, or disappearance. Semantic alignment assesses whether the video matches both the explicit prompt content and its implied requirements. Finally, we conduct user study with 12 participants, who rate each video on 1 10 scale for text alignment, physical plausibility, and visual quality. B. Comparison with Baselines Quantitative Comparisons. Table reports quantitative results with baselines. Overall, I2V baselines achieve higher VBench scores than T2V baselines, which is expected since I2V is anchored by reference first frame and thus benefits from stronger appearance initialization. Prompt enhancement improves constraint following: while -Enhance may slightly reduce VBench quality, it boosts the LMM-as-judge and userstudy scores, indicating better satisfaction of physical and motion constraints. In contrast, our method achieves the best performance across all reported metrics. Qualitative Comparisons. We present qualitative comparisons against baseline methods in Fig. 4 and Fig. 5. In Fig. 4, the task requires the model to respect basic physical dynamics, including impact deformation and rebound. T2V baselines generated directly from the prompt fail to produce clear and physically plausible bounce after the volleyball contacts the ground. I2V baselines conditioned on the same first frame as ours still exhibit implausible artifacts, including excessive deformation of the volleyball in the second column. In contrast, our method produces both reasonable impact deformation and coherent rebound consistent with the prompt. Fig. 5 compares methods in physically constrained motion control scenarios. The left example evaluates prompt-specified trajectories: baselines drift from the intended path, with or without prompt enhancement, whereas our method follows the described trajectory more faithfully. The right example evaluates arrow-guided control: baselines often confuse directions across arrows and sometimes render unstable arrow cues over time, leading to chaotic motion. Our method moves each billiards along its assigned arrow trajectory with better temporal coherence and physical plausibility. C. Ablation Studies We report two ablations in Table I: SDEdit-style (-R) and LTX-Multi (-P,-R). Here -P removes PhyPlan and -R removes PhyRefine. For controlled comparison, all variants share the same PhyReason outputs, which provide the structured inputs required by downstream planning and refinement. Ablation on PhyPlan. We use LTX-Multi [19], strong video generation foundation model that can take multiple input images together with their target frame indices. Ablation on PhyRefine. To isolate the effect of PhyRefine, which uses motion-aware noise-consistent injection to enforce the planned scaffold during denoising, we replace it with standard SDEdit-style alternative [20]. Specifically, we add noise to the entire coarse video Vcoarse to fixed noise level and then denoise it with the same video diffusion model. This setting performs global refinement but does not selectively constrain the planned motion regions, serving as strong training-free method. V. CONCLUSION In this work, we introduce training-free three-stage pipeline PhyRPR for physics-constrained video generation. By decoupling physical understanding from visual synthesis, we infer key physical states, plan motion scaffold, and inject it into diffusion sampling via noise-consistent latent fusion. Experiments show gains in physical plausibility and motion controllability while preserving high visual quality."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al., Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. [2] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al., Hunyuanvideo: arXiv systematic framework for large video generative models, preprint arXiv:2412.03603, 2024. [3] Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, et al., World simulation with video foundation models for physical ai, arXiv preprint arXiv:2511.00062, 2025. [4] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo, Towards world simulator: Crafting physical commonsense-based benchmark for video generation, arXiv preprint arXiv:2410.05363, 2024. [5] Lanxiang Hu, Abhilash Shankarampeta, Yixin Huang, Zilin Dai, Haoyang Yu, Yujie Zhao, Haoqiang Kang, Daniel Zhao, Tajana Rosing, and Hao Zhang, Benchmarking scientific understanding and reasonarXiv preprint ing for video generation using videoscience-bench, arXiv:2512.02942, 2025. [6] Yu Yuan, Xijun Wang, Tharindu Wickremasinghe, Zeeshan Nadir, Bole Ma, and Stanley Chan, Newtongen: Physics-consistent and controllable text-to-video generation via neural newtonian dynamics, arXiv preprint arXiv:2509.21309, 2025. [7] Zijun Wang, Panwen Hu, Jing Wang, Terry Jingchen Zhang, Yuhao Cheng, Long Chen, Yiqiang Yan, Zutao Jiang, Hanhui Li, and Xiaodan Liang, Prophy: Progressive physical alignment for dynamic world simulation, arXiv preprint arXiv:2512.05564, 2025. [8] Minh-Quan Le, Yuanzhi Zhu, Vicky Kalogeiton, and Dimitris Samaras, What about gravity in video generation? post-training newtons laws with verifiable rewards, arXiv preprint arXiv:2512.00425, 2025. [9] Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, and Vchain: Chain-of-visual-thought for reasoning in video Ziwei Liu, generation, arXiv preprint arXiv:2510.05094, 2025. [10] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [11] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [12] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou, Show-o: One single transformer to unify multimodal understanding and generation, arXiv preprint arXiv:2408.12528, 2024. [13] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al., Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, arXiv preprint arXiv:2505.09568, 2025. [14] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al., Emerging properties in unified multimodal pretraining, arXiv preprint arXiv:2505.14683, 2025. [15] Jiabin Luo, Junhui Lin, Zeyu Zhang, Biao Wu, Meng Fang, Ling Chen, and Hao Tang, Univid: The open-source unified video model, arXiv preprint arXiv:2509.24200, 2025. [16] Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, and Wenhu Chen, Univideo: Unified understanding, generation, and editing for videos, arXiv preprint arXiv:2510.08377, 2025. [17] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le, Flow matching for generative modeling, arXiv preprint arXiv:2210.02747, 2022. [18] Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, et al., Sam 3: Segment anything with concepts, arXiv preprint arXiv:2511.16719, 2025. [19] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Ltx-video: Realtime video latent diffusion, arXiv Gordon, et al., preprint arXiv:2501.00103, 2024. [20] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon, Sdedit: Guided image synthesis arXiv preprint and editing with stochastic differential equations, arXiv:2108.01073, 2021."
        }
    ],
    "affiliations": [
        "State Key Lab of CAD&CG, Zhejiang University"
    ]
}