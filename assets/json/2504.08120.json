{
    "paper_title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?",
    "authors": [
        "Daniil Larionov",
        "Sotaro Takeshita",
        "Ran Zhang",
        "Yanran Chen",
        "Christoph Leiter",
        "Zhipin Wang",
        "Christian Greisinger",
        "Steffen Eger"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use."
        },
        {
            "title": "Start",
            "content": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization? Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger Natural Language Learning & Generation (NLLG) https://nl2g.github.io/ University of Mannheim, University of Technology Nuremberg daniil.larionov@uni-mannheim.de 5 2 0 2 0 1 ] . [ 1 0 2 1 8 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use."
        },
        {
            "title": "Introduction",
            "content": "Reasoning LLMs have driven recent progress in NLP, often outperforming standard models by leveraging chain-of-thought (CoT) supervision (Wei et al., 2022) and reinforcement learning (Shao et al., 2024) to improve multi-step inference and logical reasoning (Arora and Zanette, 2025; Guo et al., 2025). However, reasoning is not always beneficialCoT prompting can degrade perfor1 Figure 1: Machine Translation evaluation process with Reasoning LLMs mance when verbal deliberation introduces unnecessary complexity (Liu et al., 2024) or when the task does not require it (Sprague et al., 2025), highlighting the need to understand when reasoning helps. critical but underexplored application is Natural Language Generation (NLG) evaluation. Although LLM-based metrics have become the stateof-the-art (SOTA) for assessing NLG tasks such as machine translation and summarization (Kocmi and Federmann, 2023a; Liu et al., 2023; Leiter et al., 2023), existing methods are primarily built on non-reasoning LLMs. The role of reasoning LLMs in this setting remains largely unexplored. We identify two key gaps in this context. First, although reasoning models such as DeepSeekR1 (Guo et al., 2025) have shown strong results on complex reasoning tasks, their effectiveness in MT and TS evaluation remains unclear. Second, these models are often extremely large (e.g., exceeding 600B parameters), posing practical deployment challenges. This underscores the need to assess whether distilled variants can retain evaluation quality while offering better computational efficiency. This paper attempts to address those gaps by systematically evaluating reasoning LLMs as evaluation metrics. We start by evaluating 8 different models (reasoning-based and their non-reasoning counterparts) across the tasks of machine translation (MT) and text summarization (TS) evaluation. The model comparison framework includes three 2.1 Traditional Metrics for MT and TS architectural categories: SOTA reasoning LLMs: DeepSeek-R1 and OpenAI o3. Distilled variants of R1: DeepSeek-R1-DistillQwen-32B and DeepSeek-R1-Distill-Llama8B. Related non-reasoning LLMs: GPT-4o-mini, Qwen-2.5 32B, Llama-3.1 8B. Our analysis focuses on two primary research questions: RQ1 Do reasoning models provide any improvements over conventional models in MT and TS evaluation? RQ2 How effectively can distillation preserve evaluation capabilities of reasoning models while reducing computational costs? Our methodology consists of applying existing SOTA prompting-based metrics with reasoning models. For summarization evaluation, we use G-Eval (Liu et al., 2023) and for translation evaluation, we use GEMBA-MQM (Kocmi and Federmann, 2023a). We illustrate the evaluation process for MT in Figure 1. Key results show that the efficacy of reasoning capabilities for NLG evaluation is highly architecture-dependent. While OpenAI o3-mini models demonstrate substantial benefits from reasoning, with performance improving as reasoning intensity increases, DeepSeek-R1 generally underperforms compared to its non-reasoning counterpart across most evaluation tasks. Our correlation analysis reveals that increased reasoning token usage correlates with improved evaluation quality, particularly in the o3-mini models. Additionally, we find that distillation of reasoning capabilities maintains reasonable performance in mediumsized models but degrades substantially in smaller variants. Our contributions include: (1) We present the first systematic comparison of reasoning vs. nonreasoning LLMs as NLG evaluators, and (2) we evaluate the reasoning distillation efficacy for NLG evaluation tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Our work connects to evaluation metrics for MT, summarization and LLM-as-a-Judge approaches, efficiency in metrics as well as to reasoning in LLMs. Metrics for MT Early MT evaluation metrics focused on surface-level similarity between system and reference translations. BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) rely on n-gram overlap and edit distance. To capture semantic shifts more effectively, MoverScore (Zhao et al., 2019) leverages word embeddings to measure distances between system and reference translations, using Word Movers Distance (Kusner et al., 2015). BERTScore (Zhang et al., 2020) further improved MT evaluation by computing contextualized word embeddings from BERT (Devlin et al., 2019). Other paradigms popular for evaluating MT include NLI-based metrics (Chen and Eger, 2023) and trained metrics (Rei et al., 2020; Guerreiro et al., 2024). Metrics for TS One of the foundational metrics for evaluating TS is ROUGE (Lin, 2004), which measures the overlap between system-generated summaries and reference summaries. However, ROUGE struggles with semantic equivalence and factual consistency, particularly in long-form summarization. To address these limitations, contentbased metrics such as Pyramid (Nenkova and Passonneau, 2004) and BE (Hovy et al., 2006) were introduced and later semantic metrics such as SUPERT (Gao et al., 2020). Pyramid aligns important information across multiple references, while BE identifies minimal semantic units in reference summary. Benchmarking efforts such as SummEval (Fabbri et al., 2021) have further supported systematic comparisons of TS evaluation metrics."
        },
        {
            "title": "2.2 LLM-based Evaluation",
            "content": "The LLM-as-a-Judge paradigm (Gu et al., 2024; Li et al., 2025; Huang et al., 2024) employs instruction-following LLMs (e.g., GPT-4 (OpenAI et al., 2024), LLaMA-3 (Grattafiori et al., 2024)) to directly assess system outputs along dimensions such as fluency, coherence, and factual consistency (Chia et al., 2023). These methods increasingly correlate with human judgment and offer scalable alternative to traditional metrics (Liu et al., 2023; Kocmi and Federmann, 2023a). In MT, GEMBA (Kocmi and Federmann, 2023a,b) prompts LLMs to evaluate individual segments in isolation before aggregating scores, achieving stateof-the-art performance on the WMT22 Metrics Benchmark (Freitag et al., 2022). AutoMQM (Fernandes et al., 2023) extends this by asking LLMs 2 to detect and classify translation errors, offering more interpretable and fine-grained feedback. In TS, G-Eval (Liu et al., 2023) adopts criteriabased framework using chain-of-thought prompting, while FineSurE (Song et al., 2024) incorporates key fact extraction and verification by LLMs. Eval4NLP (Leiter et al., 2023) further benchmarked prompting-based LLM evaluators for summarization in shared task setting. Despite these advances, most existing work leverages non-reasoning LLMs, leaving open the question of whether reasoning LLMs offer further benefits. 2.3 Efficiency for Evaluation Metrics With the rising computational cost of LLM-based evaluation, recent work has explored lightweight alternatives to maintain effectiveness under resource constraints. FrugalScore (Eddine et al., 2022) uses distillation and representation pruning to reduce model size without compromising metric reliability. EffEval (Larionov et al., 2023) investigates trade-offs between efficiency and performance, showing that adapter-based fine-tuning and compact architectures are viable. Similarly, COMETINHO (Rei et al., 2022) introduces efficient variants of COMET through optimized inference, while xCOMET-lite (Larionov et al., 2024) leverages quantization and distillation for real-time applications. Prompt-level optimization methods such as PromptOptMe (Larionov and Eger, 2024) reduce token overhead while preserving evaluation accuracy. Building on previous efforts in efficient evaluation, we investigate how effective are smaller, distilled reasoning models at NLG evaluation."
        },
        {
            "title": "2.4 Reasoning LLMs",
            "content": "Recent models such as DeepSeek-R1 (Guo et al., 2025) and Open AI o3-mini1 are designed to perform complex step-by-step reasoning using techniques like Chain-of-Thought (CoT) prompting (Wei et al., 2022) through Group Relative Policy Optimization (GRPO) (Shao et al., 2024). These approaches enable intermediate reasoning supervision and reward shaping (Arora and Zanette, 2025), leading to strong performance on mathematically and logically demanding tasks. Further, efficiency and quality gains are achieved through CoT compression (Deng et al., 2024), model distillation (Guo et al., 2025), and test-time scaling strategies (Muennighoff et al., 2025). However, 1https://openai.com/index/openai-o3-mini/ their potential for evaluating NLG remains underexplored, as existing approaches often use heuristic CoT prompting without fully leveraging reasoning LLMs. This motivates our investigation into whether such models can outperform conventional LLMs."
        },
        {
            "title": "3 Experimental Setup",
            "content": "3.1 Evaluation Frameworks Our evaluation focuses on two primary NLG tasks: machine translation evaluation and text summarization evaluation. We utilize established promptingbased metrics that have demonstrated strong performance in recent benchmarks. Machine Translation Evaluation: For MT evaluation, we employ GEMBA-MQM (Kocmi and Federmann, 2023a), prompting-based metric that leverages LLMs to identify and rate translation quality errors. GEMBA-MQM instructs LLMs to evaluate machine translations by detecting errors across dimensions such as accuracy, fluency, and terminology, assigning severity scores to each error. This method has shown state-of-the-art performance on the WMT22 Metrics Shared Task (Freitag et al., 2023). Summarization Evaluation: For summarization evaluation, we implement G-Eval (Liu et al., 2023), which prompts LLMs to evaluate summaries along multiple dimensions including coherence, consistency, relevance, and fluency. G-Eval provides explicit criteria for each dimension and asks the model to generate both numerical scores and explanations, making it particularly suitable for testing the impact of reasoning capabilities."
        },
        {
            "title": "3.2 Datasets",
            "content": "Machine Translation: We utilize the WMT22 Metrics Shared Task dataset (Freitag et al., 2023), which provides human judgments for translations between various language pairs. The dataset includes source texts, reference translations, system translations, and human judgments at both segment and system levels. We focus particularly on the en-de, he-en, and zh-en language pairs to evaluate across diverse linguistic structures. Each translation is accompanied by Multidimensional Quality Metrics (MQM) annotations, allowing for finegrained assessment of translation quality. Summarization: For text summarization evaluation, we use the SummEval dataset (Fabbri et al., 3 2021), which contains human judgments for summaries from 16 different summarization systems on 100 news articles from the CNN/DailyMail dataset. Each summary is evaluated along four dimensions: coherence, consistency, relevance, and fluency, with scores ranging from 1 to 5. Additionally, we employ the Eval4NLP dataset (Leiter et al., 2023), which provides another benchmark for summary evaluation with human judgments on different quality dimensions. The human judgement scores for Eval4NLP are not published online, which allows us to evaluate reasoning and non-reasoning models with no data contamination. 3.3 Models We evaluate three categories of models to systematically investigate our research questions: SOTA Reasoning LLMs: DeepSeek-R1 (Guo et al., 2025): reasoningenabled variant of DeepSeek, fine-tuned using reinforcement learning with chain-of-thought mechanisms. This 600B parameter model represents the current frontier of reasoning architecture. OpenAI o3-mini variants: We use the o3mini model with three different settings of the reasoning_effort parameter: high, medium, and low. These settings allow us to investigate the impact of reasoning intensity on evaluation performance within the same model architecture. Distilled Variants of Reasoning Models: DeepSeek-R1-Distill-Llama-70B: 70B parameter distilled version of DeepSeek-R1 based on the Llama architecture, representing large-scale deployment. DeepSeek-R1-Distill-Qwen-32B: 32B parameter distilled version of DeepSeek-R1 based on the Qwen architecture, representing medium-scale deployment. DeepSeek-R1-Distill-Llama-8B: An 8B parameter distilled version of DeepSeek-R1 based on the Llama architecture, representing small-scale deployment. Non-Reasoning LLMs (Control Group): DeepSeek V3 (DeepSeek-AI et al., 2025): The non-reasoning counterpart to DeepSeekR1. GPT-4o-mini (OpenAI et al., 2024): nonreasoning variant from the GPT-4 family. Qwen-2.5 32B (Qwen et al., 2025): 32B parameter model focusing on general capabilities. LLaMa-3.3 70B (Grattafiori et al., 2024): 70B parameter model representing largescale general LLM. LLaMa-3.1 8B (Grattafiori et al., 2024): An 8B parameter model representing smallscale general LLM. This selection provides comprehensive crosssection of reasoning vs. non-reasoning models across different parameter scales, allowing us to analyze both capability differences and the impact of distillation."
        },
        {
            "title": "3.4 Evaluation Protocol",
            "content": "All models were evaluated using the same prompting templates to ensure fair comparison. For the GEMBA-MQM metric, we followed the template described in Kocmi and Federmann (2023a), which instructs the model to identify errors in translation and then score the translation based on those errors. For G-Eval, we implemented the prompting strategy detailed in Liu et al. (2023), which asks the model to evaluate summaries on multiple dimensions with specific criteria. For meta-evaluation of machine translation metrics, we compute segment-level Pearsons correlation (ρ) between model scores and human judgments, as well as system-level pairwise accuracy (Deutsch et al., 2023), which measures how often the metric correctly predicts which of two systems is better according to human judgments. For summarization evaluation, we compute segment-level Kendalls τ correlation with human judgments across four dimensions (coherence, consistency, relevance, and fluency)."
        },
        {
            "title": "4 Results",
            "content": "Here, we describe the results obtained with summarization evaluation and machine translation evaluation. 4 Model Name Reasoning Coherence Consistency Relevance Fluency Avg. SummEval Eval4NLP DeepSeek R1 DeepSeek V3 R1 LLaMa 70B LLaMa 3.3 70B R1 Qwen 32B Qwen2.5 32B R1 LLaMa 8B LLaMa 3.1 8B OpenAI o3-mini-high OpenAI o3-mini-low OpenAI GPT-4o-mini yes no yes no yes no yes no yes yes no DeepSeek, Qwen, LLaMa 0.381 0.462 0.380 0.487 0.293 0.372 0.169 0.142 0.482 0.478 0.321 0.565 0.331 0.228 0.293 0.540 0.449 0.251 0.312 OpenAI 0.242 0.237 0.430 0.303 0.446 0.322 0.437 0.304 0.404 0.128 0.364 0.311 0.315 0.370 0.157 0.356 0.330 0.284 0.281 0.348 0.148 0. 0.311 0.313 0.263 0.351 0.399 0.315 0.375 0.355 0.393 0.174 0.228 0.337 0.335 0.346 0.583 0.630 0.556 0.624 0.564 0.619 0.368 0.488 0.644 0.645 0.634 Table 1: The segment-level Kendalls τ of summarization evaluation with different models on SummEval and Eval4NLP datasets. 4.1 Summarization Table 1 shows the segment-level Kendalls τ correlation between human judgments and model evaluations on the SummEval and Eval4NLP datasets. For SummEval, we compare performance across four dimensions: coherence, consistency, relevance, and fluency. For Eval4NLP, we report correlation with single summarization quality score. OpenAI vs. DeepSeek OpenAI reasoning models show consistent performance irrespective of the reasoning effort setting, with o3-mini-high and o3-mini-low achieving nearly identical SummEval averages (0.337 and 0.335) and the highest overall Eval4NLP scores (0.644 and 0.645). DeepSeek V3 achieves the highest SummEval average score (0.399) among all models tested, with particular strengths in coherence (0.462), relevance (0.446), and fluency (0.356). Reasoning vs. non-reasoning models Across model families, we identify consistent pattern of non-reasoning models outperforming their reasoning counterparts: DeepSeek: DeepSeek V3 (non-reasoning) outperforms DeepSeek R1 in both SummEval average (0.399 vs. 0.351) and Eval4NLP (0.630 vs. 0.583). LLaMA: LLaMA 3.3 70B (non-reasoning) achieves better scores than R1 LLaMA 70B in both SummEval average (0.375 vs. 0.315) and Eval4NLP (0.624 vs. 0.556). Qwen: Qwen 2.5 32B (non-reasoning) outperforms R1 Qwen 32B with higher SummEval average (0.393 vs. 0.355) and Eval4NLP (0.619 vs. 0.564) scores. OpenAI (exception): Reasoning and nonreasoning models achieve more comparable scores for SummEval average (0.337 vs 0.346) and Eval4NLP (0.644 vs. 0.634). However, reasoning models demonstrate specific strengths. DeepSeek R1 excels in consistency metrics (0.565 vs. 0.331, 70% better correlation), showing substantially better alignment with human judgments on factual accuracy as compared to the non-reasoning DeepSeek V3. Similarly, R1 Qwen 32B shows strong consistency performance by 20% (0.540 vs 0.449), outperforming Qwen2.5 32B. The OpenAI o3-mini model outperforms assumed nonreasoning equivalent GPT-4o-mini in Coherence by approx. 50% (0.482 vs 0.321). Distilled vs. original models When comparing DeepSeek R1 with its distilled variants, we observe that DeepSeek R1-Distill-32B maintains 100% of the original models quality on SummEval average and 97% on Eval4NLP (0.564 vs. 0.583). This distilled models has 21 times less parameters than the original one (32B vs. 685B). The distillation process also largely maintains the reasoning models consistency advantage over equivalent nonreasoning model. At the same time, other distilled versions (R1 LLaMa 70B and R1 LLaMa 8B) shows substantially worse performance compared to the original model on SummEval dataset. We can also see that both distilled models loose their advantage in the consistency aspect of summarization evaluation. However, on Eval4NLP dataset, R1 LLaMA 70B retains 96% of the original model performance (0.556 vs. 0.583). 4.2 Machine Translation Table 2 shows the segment-level Pearsons ρ correlation between human judgments and model evaluations on the WMT23 dataset across three language pairs: en-de, he-en, and zh-en. Additionally, we report system-level pairwise accuracy, measuring how often the metric correctly ranks translation systems. OpenAI vs. DeepSeek The OpenAI models demonstrate strong performance across language pairs, with o3-mini-high achieving the highest overall segment-level correlations (0.577 for en-de and 0.568 for zh-en). Within the OpenAI family, we observe that higher reasoning effort settings generally correspond to better performance, particularly for en-de and zh-en pairs. Interestingly, GPT-4omini performs comparatively well on he-en (0.435), outperforming its reasoning counterparts in this language pair. For system-level accuracy, OpenAI models achieve consistent scores (0.928 for o3-mini variants), showing robust performance in ranking translation systems. Reasoning vs. non-reasoning models Similar to our summarization findings, non-reasoning models generally outperform their reasoning counterparts in translation evaluation tasks. This pattern holds consistently across model families: DeepSeek V3 (non-reasoning) outperforms DeepSeek R1 in both en-de (0.490 vs. 0.364) and zh-en (0.512 vs. 0.441) language pairs, with comparable performance on he-en. R1 LLaMa 70B shows substantial deterioration over LLaMa 3.3 70B across all language pairs, with particularly notable differences in en-de (0.590 vs. 0.421) and he-en (0.420 vs. 0.365). Similarly, Qwen 2.5 32B achieves higher correlations than R1 Qwen 32B in all language pairs, with the largest gap in en-de (0.521 vs. 0.388). The exception to this pattern is OpenAIs o3mini-high, which demonstrates superior performance compared to GPT-4o-mini across two language pairs, most substantially in en-de (0.577 vs. 0.410) and zh-en (0.568 vs. 0.487). For system-level accuracy, the differences between reasoning and non-reasoning models are less pronounced. Qwen 2.5 32B achieves the highest system-level accuracy (0.944), with R1 LLaMa 70B (0.932) performing slightly better than its nonreasoning counterpart (0.924). The distilled R1 models show clear performance drop compared to their full-size counterparts. LLaMa 3.1 8B, despite having far fewer parameters than LLaMa 3.3 70B, maintains moderate segment-level correlations of 0.476, 0.335, and 0.421 for the three language pairs respectively."
        },
        {
            "title": "5 Analysis of Reasoning",
            "content": "To further investigate the relationship between explicit reasoning and evaluation performance, we analyze the correlation between the number of reasoning tokens used by each model and various evaluation metrics. Specifically, we compute Pearson correlations between reasoning token count and: (1) evaluation error (absolute difference between model-predicted and ground truth scores) and (2) model-assigned scores. This analysis was performed on machine translation evaluation task on WMT23 dataset. Table 3 summarizes these correlations for our reasoning-capable models. Error Correlations All reasoning models except R1 Qwen 32B exhibit statistically significant negative correlations between reasoning token count and evaluation error, suggesting that increased reasoning is associated with smaller discrepancies between model-predicted and human judgment scores. The o3-mini variants show the strongest negative correlations (-0.1183, -0.1104, and -0.0919 for high, medium, and low effort settings respectively), while DeepSeek-based models demonstrate much weaker correlations (-0.0154 for DeepSeek R1, -0.0429 for R1 LLaMa 70B, and non-significant -0.0039 for R1 Qwen 32B). LLM Score Correlations We observe substantial differences in how reasoning token count correlates with model-assigned scores across architectures. OpenAI models exhibit strong negative correlations, with o3-mini-high showing the strongest relationship (-0.4742), followed by o3-mini-medium (-0.4148) and o3-mini-low (-0.3466). This indicates that OpenAI models tend engage in more extensive thinking when they identify more errors and assign lower quality scores. In contrast, DeepSeek R1 shows weak positive correlation (0.0199), while the distilled versions show moderate negative correlations, similarly to o3-mini (-0.2083 for 6 Model Name Reasoning Segment-level System-level en-de he-en zh-en Acc. DeepSeek, Qwen, LLaMa DeepSeek R1 DeepSeek V3 R1 LLaMa 70B LLaMa 3.3 70B R1 Qwen 32B Qwen2.5 32B R1 LLaMa 8B LLaMa 3.1 8B yes no yes no yes no yes no OpenAI o3-mini-high yes OpenAI o3-mini-medium yes OpenAI o3-mini-low yes OpenAI GPT-4o-mini no 0.364 0.490 0.421 0.590 0.388 0.521 0.310 0.476 OpenAI 0.577 0.517 0.471 0.410 0.398 0.394 0.365 0.420 0.338 0.390 0.325 0. 0.421 0.404 0.413 0.435 0.441 0.512 0.451 0.522 0.465 0.519 0.410 0.421 0.568 0.505 0.491 0.487 0.908 0.904 0.932 0.924 0.920 0.944 0.915 0.916 0.920 0.928 0.928 0.928 Table 2: The segment-level Pearsons ρ and system-level pairwise accuracy of MT evaluation with different models on WMT23. Model DeepSeek R1 R1 LLaMa 70B R1 Qwen 32B o3-mini-high o3-mini-medium o3-mini-low Error -0.0154 -0.0429 -0.0039 -0.1183 -0.1104 -0.0919 LLM Score 0.0199 -0.2083 -0.1508 -0.4742 -0.4148 -0. Not statistically significant (p > 0.05). All other correlations significant at < 0.001. Table 3: Pearson correlations between reasoning token count and evaluation metrics. Error represents the absolute difference between model-predicted and ground truth scores. R1 LLaMa 70B and -0.1508 for R1 Qwen 32B)."
        },
        {
            "title": "5.1 Examples of Reasoning Traces",
            "content": "We present condensed examples of effective and problematic reasoning in translation quality assessment done by DeepSeek R1 model on examples from English-German language pair in WMT23 dataset. See Figures 2 and 3. In both cases we can note that model goes iteratively over each sentence and phrase of the source text and translation hypothesis. It starts with identifying errors, then assigns the category and finally decides on severity. In the effective reasoning example, we see that the model correctly identifies all issues. In the ineffective reasoning example, we see that model identifies two rather minor issues with word choice and structure, yet overestimates their severity. For complete reasoning examples, see Appendix A. Source: Statistics Norways figures showed that foreigners were scattered all over the citys districts, as Søndre Nordstrand, the borough furthest south in Oslo, had over 14,000 immigrants registered there. [...] Hypothesis: Statistiken Norwegens Zahlen zeigten, dass Ausländer in den Bezirken der Stadt verstreut waren, da Søndre Nordstrand, der Bezirk weiter südlich in Oslo, dort über 14.000 Einwanderer registriert hatte. [...] Diff: Human(-25.0) - LLM(-25) = 0.0 Reasoning (Summarized): <think> ...Statistics Norway is the name of the organization, so translating it literally as Statistiken Norwegens might be mistranslation... The original says furthest south, but weiter südlich means further south.... </think> Error Assessment: Critical: terminology/inappropriate for context - Statistiken Norwegens\" (incorrect translation of Statistics Norway\"). Major: accuracy/mistranslation - weiter südlich\" (further south\" instead of furthest south\"). accuracy/mistranslation - fünftgrößten Ausländern\" (fifth largest foreigners\" instead of fifth most\"). Minor: fluency/awkward - bildeten den Rest der zehn Bezirke\" (awkward phrasing). Figure 2: Effective reasoning example."
        },
        {
            "title": "6 Discussion",
            "content": "Our experimental results across machine translation evaluation, summarization evaluation, and reasoning token analysis reveal several noteworthy patterns regarding the utility of reasoning capabilities in NLG evaluation tasks. Architecture-Specific Performance One of the most striking findings is the disparate performance between different reasoning model architectures. 7 Source: (PERSON2) Because like the way believe Martin does his <unintelligible/> little translation, so he translates more sentences at once and then picks only the centre one [...] Hypothesis: (PERSON2) Weil ich glaube, dass Martin seine <unverständlich/> kleine Übersetzung so macht, dass er mehrere Sätze auf einmal übersetzt und dann nur den mittleren auswählt [...] Diff: Human(-0.0) - LLM(-10) = 10.0 Reasoning (Summarized): <think> ...little is translated as kleine, which is small in German. But in context, little might be more about the manner of translation rather than size... The structure here is und so das ganze Dokument durchgeht durchgeht is at the end, which is correct for subordinate clause, but after und, its main clause... </think> Error Assessment: Critical: no-error Major: accuracy/mistranslation - kleine\" (misinterprets little\" as small\" rather than contextual nuance) fluency/grammar - durchgeht\" (incorrect verb placement; should follow main clause structure) Minor: no-error Figure 3: Ineffective reasoning example. While DeepSeek-R1, designed specifically for reasoning tasks, consistently underperforms compared to its non-reasoning counterpart (DeepSeek V3) in both translation and summarization evaluation, OpenAIs o3-mini models demonstrate superior performance over their non-reasoning probable equivalent (GPT-4o-mini) in several evaluation scenarios, particularly for machine translation. This architecture-dependent performance suggests that reasoning capabilities alone do not guarantee improved evaluation quality, but rather that the implementation and specific post-training approach for enhancing reasoning capabilities matters significantly. DeepSeek Limitations The relatively poor performance of DeepSeek-R1 across evaluation tasks warrants closer examination. Despite being explicitly trained using reinforcement learning to enhance reasoning capabilities, DeepSeek-R1 achieves lower correlation with human judgments compared to DeepSeek V3 in both translation (en-de: 0.364 vs. 0.490 25.7%, zh-en: 0.441 vs. 0.512 13.9%) and summarization tasks (coherence: 0.381 vs. 0.462 17.5%, relevance: 0.303 vs. 0.446 32.1%). This performance gap may stem from insufficient multilingual training data or lack of specific finetuning for evaluation tasks. While DeepSeek-R1 excels in certain consistency evaluations for summarization (0.565 vs. 0.331 70.7%), its generally weaker performance suggests that its reasoning approach may not align well with the requirements of fine-grained NLG evaluation. It might as well suggest that the use of R1-family of models requires further adaptation of the evaluation prompt. OpenAI o3-mini Effectiveness In contrast, OpenAIs o3-mini variants demonstrate superior evaluation capabilities. The o3-mini-high model achieves the highest correlation scores for en-de (0.577) and zh-en (0.568) translation pairs, substantially outperforming the non-reasoning GPT-4o-mini (0.410 and 0.487, 70% and 16% respectively). Our correlation analysis further supports this finding, with o3-mini-high showing the strongest negative correlation between reasoning token count and evaluation error (-0.1183), indicating that more extensive reasoning could lead to more accurate evaluations, supporting test-time scaling hypothesis. Distillation Efficacy Our results indicate variable success in distilling reasoning capabilities for evaluation tasks. While R1 Qwen 32B maintains reasonable performance compared to the original DeepSeek-R1, the smaller R1 LLaMa 8B shows substantial degradation in summarization evaluation (average correlation dropping from 0.351 to 0.174, ). Effective distillation of evaluationrelevant reasoning requires sufficient model capacity, with smaller distilled models potentially losing critical capabilities required for nuanced evaluation. Task-Specific Reasoning Requirements The differential performance of reasoning models between tasks suggests that summarization and translation evaluation may require distinct reasoning strategies. While OpenAI models demonstrate advantages in translation evaluation, their performance advantage is less notable in summarization tasks. The nature of reasoning required for effective evaluation may vary significantly across NLG tasks, with translation potentially benefiting more from the reasoning approach employed by o3-mini models. These findings suggest that while reasoning capabilities can improve evaluation performance, their efficacy depends heavily on architectural implementation, task-specific alignment, and reasoning intensity. The superior performance of o3mini models indicates that they may incorporate training elements particularly suited for evaluation tasks, possibly including broader multilingual exposure or specific fine-tuning for comparative as8 sessment. Future work could investigate targeted enhancements to reasoning approaches specifically designed for NLG evaluation contexts, as well as the performance of other reasoning models."
        },
        {
            "title": "7 Conclusion",
            "content": "This study presents the first systematic evaluation of reasoning-based LLMs for NLG evaluation tasks, comparing their performance against conventional models across machine translation and summarization domains. Our findings reveal that the relationship between reasoning capabilities and evaluation performance is more nuanced than initially hypothesized. In response to our first research question regarding accuracy improvements, we find that reasoning models do not universally outperform conventional models. Rather, the efficacy of reasoning depends significantly on model architecture and implementation. While DeepSeek-R1 generally underperforms compared to its non-reasoning counterpart, OpenAIs o3-mini models demonstrate that well-implemented reasoning can enhance evaluation accuracy, particularly at higher reasoning intensity settings. This architecture-dependent pattern suggests that the mere presence of reasoning capabilities is insufficient their integration and alignment with evaluation requirements are crucial factors. Regarding our second research question on distillation efficacy, our results indicate that preserving evaluation capabilities through distillation remains challenging. Medium-sized distilled models (R1 Qwen 32B) maintain reasonable performance, while smaller variants (R1 LLaMa 8B) show substantial degradation, suggesting that effective distillation of evaluation-relevant reasoning requires sufficient model capacity. These findings suggest that future work should focus not merely on incorporating reasoning capabilities, but on specifically aligning reasoning approaches with the requirements of NLG evaluation tasks. This may involve specialized fine-tuning regimens, task-specific reasoning strategies, or architectural modifications that enhance the application of reasoning to comparative assessment."
        },
        {
            "title": "Limitations",
            "content": "There are several limitations associated with the evaluation methodology we used. Future work may address them. We make an assumption that the GPT-4o-mini model is the most likely non-reasoning equivalent, by capabilities and/or size, to the reasoning model o3-mini. At the same time, there is no public confirmed information about the size of either of these models, thus this cannot be verified. While the inference API of o3-mini allows specifying the reasoning effort (either low, medium or high), there is no such option for DeepSeek R1 and its distilled variants. Further studies might examine R1 performance at different reasoning efforts by implementing forced generation of the closing </think> token upon generating pre-determined amount of reasoning tokens."
        },
        {
            "title": "Acknowledgements",
            "content": "The NLLG lab gratefully acknowledges support from the Federal Ministry of Education and Research (BMBF) via the research grant Metrics4NLG and the German Research Foundation (DFG) via the Heisenberg Grant EG 375/51. Parts of this paper were developed during retreat in the Tyrolean Alps, in the settlement of Großvolderberg. See Figure 6."
        },
        {
            "title": "References",
            "content": "Daman Arora and Andrea Zanette. 2025. Training language models to reason efficiently. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572. Association for Computational Linguistics. Yanran Chen and Steffen Eger. 2023. MENLI: Robust evaluation metrics from natural language inference. Transactions of the Association for Computational Linguistics, 11:804825. Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2023. Instructeval: Towards holistic evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei 9 Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2025. Deepseek-v3 technical report. Yuntian Deng, Yejin Choi, and Stuart Shieber. 2024. From explicit cot to implicit cot: Learning to internalize cot step by step. Daniel Deutsch, George Foster, and Markus Freitag. 2023. Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12914 12929. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186. Association for Computational Linguistics. Moussa Kamal Eddine, Guokan Shang, Antoine Tixier, and Michalis Vazirgiannis. 2022. Frugalscore: Learning cheaper, lighter and faster evaluation metrics for automatic text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13051318. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Proceedings of the Eighth Conference on Machine Translation, WMT 2023, Singapore, December 6-7, 2023, pages 1066 1083. Association for Computational Linguistics. Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, pages 578628. Association for Computational Linguistics. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 4668. Association for Computational Linguistics. Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 13471354. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, 10 Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing 11 Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi Fukumoto. 2006. Automated summarization evaluation with basic elements. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC06). European Language Resources Association (ELRA). Hui Huang, Yingqi Qu, Xingyuan Bu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. 2024. An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge model is not general substitute for gpt-4. Tom Kocmi and Christian Federmann. 2023a. GEMBAMQM: Detecting translation quality error spans with GPT-4. In Proceedings of the Eighth Conference on Machine Translation, pages 768775. Association for Computational Linguistics. Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193203. European Association for Machine Translation. Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From word embeddings to document distances. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML15, page 957966. JMLR.org. Daniil Larionov and Steffen Eger. 2024. Promptoptme: Error-aware prompt compression for llmarXiv preprint based mt evaluation metrics. arXiv:2412.16120. Daniil Larionov, Jens Grünwald, Christoph Leiter, and Steffen Eger. 2023. EffEval: comprehensive evaluation of efficiency for MT evaluation metrics. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7896. Association for Computational Linguistics. Daniil Larionov, Mikhail Seleznyov, Vasiliy Viskov, Alexander Panchenko, and Steffen Eger. 2024. xCOMET-lite: Bridging the gap between efficiency and quality in learned MT evaluation metrics. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21934 21949. Association for Computational Linguistics. Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, and Steffen Eger. 2023. The Eval4NLP 2023 shared task on prompting large language models as explainable metrics. In Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 117138. Association for Computational Linguistics. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. 2025. From generation to judgment: Opportunities and challenges of llm-as-a-judge. Chin-Yew Lin. 2004. ROUGE: package for automatic In Text Summarization evaluation of summaries. Branches Out, pages 7481. Association for Computational Linguistics. 12 Ryan Liu, Jiayi Geng, Addison Wu, Ilia Sucholutsky, Tania Lombrozo, and Thomas Griffiths. 2024. Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse. arXiv preprint arXiv:2410.21333. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522. Association for Computational Linguistics. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid In Proceedings of the Human Language method. Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145152. Association for Computational Linguistics. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, 13 Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, and Saab Mansour. 2024. FineSurE: Fine-grained summarization evaluation using LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 906922. Association for Computational Linguistics. Zayne Rea Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2025. To cot or not to cot? chain-ofthought helps mainly on math and symbolic reasoning. In The Thirteenth International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563578. Association for Computational Linguistics. Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. 2024. Gpt-4o system card. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Ricardo Rei, Ana Farinha, José G.C. de Souza, Pedro G. Ramos, André F.T. Martins, Luisa Coheur, and Alon Lavie. 2022. Searching for COMETINHO: The little metric that could. In Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, pages 6170. European Association for Machine Translation. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702. Association for Computational Linguistics. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. 14 Appendix: Reasoning Examples This appendix presents full reasoning examples from both effective and ineffective cases of LLM reasoning when evaluating translation quality. Source: Statistics Norways figures showed that foreigners were scattered all over the citys districts, as Søndre Nordstrand, the borough furthest south in Oslo, had over 14,000 immigrants registered there. Many immigrants also lived in sought-after areas, as Frogner was the district with the fifth most foreigners. After that, Stovner, Bjerke, Grorud, Østensjø and Sagene made up the rest of the ten districts in Oslo with the most residents from another country. Hypothesis: Statistiken Norwegens Zahlen zeigten, dass Ausländer in den Bezirken der Stadt verstreut waren, da Søndre Nordstrand, der Bezirk weiter südlich in Oslo, dort über 14.000 Einwanderer registriert hatte. Viele Einwanderer lebten auch in begehrten Gebieten, da Frogner der Bezirk mit den fünftgrößten Ausländern war. Danach bildeten Stovner, Bjerke, Grorud, Østensjø und Sagene den Rest der zehn Bezirke Oslos mit den meisten Einwohnern aus einem anderen Land. Diff: Human(-25) - LLM(-25) = 0.0 Reasoning: <think> Okay, lets tackle this translation analysis. First, need to compare the English source text with the German translation provided. The goal is to identify any errors in accuracy, fluency, style, terminology, etc., and classify them as critical, major, or minor. Starting with the first sentence: \"Statistics Norways figures showed that foreigners were scattered all over the citys districts...\" The German translation begins with \"Statistiken Norwegens Zahlen zeigten...\" Hmm, \"Statistics Norway\" is the name of the organization, so translating it literally as \"Statistiken Norwegens\" might be mistranslation. The correct name in German should be \"Statistik Norwegen\" or \"Statistisches Amt Norwegens.\" This is terminology error because its an incorrect translation of proper noun. Since its the name of an official body, getting it wrong could confuse readers about the source, so this might be critical error. Next part: \"Søndre Nordstrand, the borough furthest south in Oslo...\" is translated as \"Søndre Nordstrand, der Bezirk weiter südlich in Oslo...\" The original says \"furthest south,\" but \"weiter südlich\" means \"further south.\" Thats mistranslation. \"Furthest south\" should be \"am weitesten südlich gelegenen.\" This affects accuracy because it changes the geographical reference, which could mislead about the locations position. This is major error since it disrupts the flow but the main idea is still somewhat clear. Moving on: \"Frogner was the district with the fifth most foreigners.\" The German translation says \"Frogner der Bezirk mit den fünftgrößten Ausländern war.\" \"Fünftgrößten\" translates to \"fifth largest,\" but the original is \"fifth most.\" The correct term here should be \"die fünftmeisten Ausländer.\" Using \"größten\" (largest) instead of \"meisten\" (most) is mistranslation. This is major accuracy error because it changes the meaning from quantity to size, which is misleading but the context might still let readers infer the correct meaning. Then, \"made up the rest of the ten districts...\" is translated as \"bildeten den Rest der zehn Bezirke...\" The original specifies that these districts are part of the ten with the most residents from another country. The German translation says \"den Rest der zehn Bezirke,\" implying that these districts complete the list of ten. However, the original mentions that after Frogner (which is fifth), Stovner, Bjerke, etc., make up the rest of the ten. The translation might be missing clarity here, but its not clear omission. Maybe minor fluency issue because \"bildeten den Rest\" is bit awkward but understandable. Fluency check: The sentence structure in German seems mostly correct, but \"da\" is used twice as conjunction, which in German can sometimes be repetitive. However, \"da\" is correct here meaning \"since\" or \"as,\" so thats fine. No major fluency issues. Another point: \"registriert hatte\" is in past perfect, which might not align with the originals simple past \"had... registered.\" But in German, the past perfect is sometimes used where English uses simple past, so this might be acceptable. Not critical error. So compiling the errors: Critical: The mistranslation of \"Statistics Norway\" as \"Statistiken Norwegens\" is critical terminology error because it misrepresents the organization, leading to potential confusion. Major: The mistranslation of \"furthest south\" to \"weiter südlich\" and \"fifth most\" to \"fünftgrößten\" are both accuracy issues that affect meaning but dont completely hinder understanding. Minor: The structure \"bildeten den Rest der zehn Bezirke\" is bit awkward but still understandable, so minor fluency/style issue. </think> Error Assessment: Critical: terminology/inappropriate for context - \"Statistiken Norwegens\" (incorrect translation of \"Statistics Norway\"; should be \"Statistik Norwegen\" or similar). Major: accuracy/mistranslation - \"weiter südlich\" (translates to \"further south\" instead of \"furthest south\"). accuracy/mistranslation - \"fünftgrößten Ausländern\" (incorrectly translates \"fifth most foreigners\" as \"fifth largest foreigners\"). Minor: fluency/awkward - \"bildeten den Rest der zehn Bezirke\" (awkward phrasing for \"made up the rest of the ten districts\"). Figure 4: Effective reasoning example Source: (PERSON2) Because like the way believe Martin does his <unintelligible/> little translation, so he translates more sentences at once and then picks only the centre one and goes like this for the whole document. So, the context is like in one <unintelligible/> together with sentence we actually want to translate. Hypothesis: (PERSON2) Weil ich glaube, dass Martin seine <unverständlich/> kleine Übersetzung so macht, dass er mehrere Sätze auf einmal übersetzt und dann nur den mittleren auswählt und so das ganze Dokument durchgeht. Also, der Kontext ist wie in einem <unverständlich/> zusammen mit dem Satz, den wir tatsächlich übersetzen wollen. Diff: Human(-0) - LLM(-10) = 25.0 Reasoning: <think>Okay, lets tackle this translation error analysis. The user provided an English source text and its German translation, and need to identify and classify the errors. Looking at the first part: \"the way believe Martin does his <unintelligible/> little translation\" becomes \"dass Martin seine <unverständlich/> kleine Übersetzung so macht\". The word \"little\" is translated as \"kleine\", which is \"small\" in German. But in context, \"little\" might be more about the manner of translation rather than size. Maybe \"kleine\" is mistranslation here. That could be major accuracy error. Next, \"he translates more sentences at once\" is \"dass er mehrere Sätze auf einmal übersetzt\". \"More\" here could mean \"several\" or \"more than one\". The German \"mehrere\" means \"several\", which fits. So thats okay. Then, \"picks only the centre one\" becomes \"nur den mittleren auswählt\". \"Centre one\" translates to \"mittleren\", which is \"middle one\". Thats accurate. No problem here. \"goes like this for the whole document\" is \"und so das ganze Dokument durchgeht\". \"Durchgeht\" means \"goes through\", which makes sense. Maybe \"durchgeht\" is bit literal, but acceptable. No major issue. In the next sentence: \"the context is like in one <unintelligible/> together with sentence we actually want to translate\" becomes \"der Kontext ist wie in einem <unverständlich/> zusammen mit dem Satz, den wir tatsächlich übersetzen wollen\". The structure here seems correct. \"zusammen mit dem Satz\" is \"together with the sentence\", which matches. Now, checking for additions or omissions. The German translation doesnt seem to add or omit any content except the handling of <unintelligible/>, which is appropriately translated. Fluency-wise, the sentence structure in German reads smoothly. No obvious grammar or spelling errors. The register is consistent, as its conversational tone matching the source. Terminology: Terms like \"Übersetzung\" for \"translation\" and \"Dokument\" for \"document\" are appropriate. No inconsistencies here. Style: The phrase \"kleine Übersetzung\" might be slightly awkward if \"little\" was meant to be diminutive or pejorative, but in German, \"kleine\" is neutral. However, if \"little\" in English is meant to imply specific method, \"kleine\" might not capture that nuance. This could be minor style issue. Wait, the users example had \"involvement\" as mistranslation. Here, \"little\" to \"kleine\" might be mistranslation if the intended meaning was different. For example, if \"little\" refers to minor or specific part of the translation process, \"kleine\" (small) might not be accurate. Thats possible accuracy/mistranslation, major error. Also, \"den wir tatsächlich übersetzen wollen\" uses \"wollen\" (want to), which matches the sources \"want to translate\". Thats accurate. So final classification: Critical: no-error Major: accuracy/mistranslation - \"kleine\" Minor: None, unless \"durchgeht\" is considered awkward. But \"durchgeht\" is correct. Maybe the structure \"so macht, dass er...\" is bit awkward, leading to minor fluency/grammar issue. Alternatively, style/awkward. Alternatively, the phrase \"so das ganze Dokument durchgeht\" might be better as \"so durch das ganze Dokument geht\", but \"durchgeht\" is separable verb. \"Durchgeht\" is correct, but the word order might be slightly off. Hmm, in German, \"und so das ganze Dokument durchgeht\" is correct. The verb \"durchgeht\" is at the end, which is standard in subordinate clauses. Wait, the structure here is \"dass er... übersetzt und dann... auswählt und so... durchgeht\". The \"und so das ganze Dokument durchgeht\" is main clause, so the verb should be in second position. But here, its \"und so das ganze Dokument durchgeht\" \"durchgeht\" is at the end, which is correct for subordinate clause, but after \"und\", its main clause. So maybe the word order is incorrect. That would be grammar error. Let me check: Original structure: \"dass er mehrere Sätze auf einmal übersetzt und dann nur den mittleren auswählt und so das ganze Dokument durchgeht.\" The \"und so das ganze Dokument durchgeht\" is connected with \"und\", so its main clause. In main clauses, the verb should be in second position. So \"und so geht er das ganze Dokument durch\" would be correct. But the translation uses \"durchgeht\" at the end, which is incorrect. So this is grammar error. Thats major fluency/grammar error because it disrupts the flow. But does it hinder comprehension? Maybe not critically, but its clear grammar mistake. So major or minor? In the example, \"wäre\" was minor grammar error. Here, incorrect verb placement is more significant grammar issue. So maybe major fluency/grammar. So adding that: Major: accuracy/mistranslation - \"kleine\" fluency/grammar - \"durchgeht\" word order But wait, the users example had \"wäre\" as minor. Maybe this is major error because its more disruptive. Alternatively, if the sentence is still understandable, its major. Lets classify it as major. </think> Error Assessment: Critical: no-error Major: accuracy/mistranslation - \"kleine\" (misinterprets \"little\" as \"small\" rather than contextual nuance, e.g., \"specific\" or \"particular\" method) fluency/grammar - \"durchgeht\" (incorrect verb placement; should follow main clause structure, e.g., \"geht ... durch\") Minor: no-error Figure 5: Ineffective Reasoning Example 16 Figure 6: The NLLG lab in Großvolderberg during February, 2025."
        }
    ],
    "affiliations": [
        "University of Mannheim",
        "University of Technology Nuremberg"
    ]
}