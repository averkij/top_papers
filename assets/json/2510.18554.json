{
    "paper_title": "Extracting alignment data in open models",
    "authors": [
        "Federico Barbero",
        "Xiangming Gu",
        "Christopher A. Choquette-Choo",
        "Chawin Sitawarin",
        "Matthew Jagielski",
        "Itay Yona",
        "Petar Veliƒçkoviƒá",
        "Ilia Shumailov",
        "Jamie Hayes"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we show that it is possible to extract significant amounts of alignment training data from a post-trained model -- useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, we argue that embedding models are better suited for our specific goals. Distances measured through a high quality embedding model can identify semantic similarities between strings that a different metric such as edit distance will struggle to capture. In fact, in our investigation, approximate string matching would have severely undercounted (by a conservative estimate of $10\\times$) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, we find that models readily regurgitate training data that was used in post-training phases such as SFT or RL. We show that this data can be then used to train a base model, recovering a meaningful amount of the original performance. We believe our work exposes a possibly overlooked risk towards extracting alignment data. Finally, our work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the model's original dataset."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 4 5 5 8 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Extracting alignment data in open models",
            "content": "Federico Barbero1,*, Xiangming Gu2, *, Christopher A. Choquette-Choo3, *, Chawin Sitawarin4, Matthew Jagielski5, *, Itay Yona6, *, Petar Veliƒçkoviƒá4, Ilia Shumailov7, * and Jamie Hayes4 1University of Oxford, 2National University of Singapore, 3OpenAI, 4Google DeepMind, 5Anthropic, 6MentaLeap, 7AI Sequrity Company, *Work performed while the author was at Google DeepMind 2025-10-22 In this work, we show that it is possible to extract significant amounts of alignment training data from post-trained model useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, we argue that embedding models are better suited for our specific goals. Distances measured through high quality embedding model can identify semantic similarities between strings that different metric such as edit distance will struggle to capture. In fact, in our investigation, approximate string matching would have severely undercounted (by conservative estimate of 10) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, we find that models readily regurgitate training data that was used in post-training phases such as SFT or RL. We show that this data can be then used to train base model, recovering meaningful amount of the original performance. We believe our work exposes possibly overlooked risk towards extracting alignment data. Finally, our work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the models original dataset. This paper only considers and discusses open models. 1. Introduction Progress in capabilities of Large Language Models (LLMs) is frequently driven by improvements to training data recipes. It is common for model developer to curate smaller and targeted data bundles to push performance on particular downstream benchmarks. For the purpose of this work, we refer to this data as alignment data. We use the term broadly to encompass not only data used for safety and instruction-following (such as Supervised Finetuning (SFT) and Reinforcement Learning (RL) datasets), but also any targeted data collections used to steer model behaviour and enhance specific capabilities, including mathematics, reasoning, and long-context understanding. While this type of data is usually found in post-training, it is becoming increasingly common to include it also earlier in training (Meta, 2025). We use the term alignment rather than post-training in our work for this reason. The fact that models memorise subsets of their training data is now well-established phenomenon (Biderman et al., 2023; Borec et al., 2024; Brown et al., 2021; Carlini et al., 2021, 2022; Chen et al., 2024a; Dankers and Titov, 2024; Feldman, 2020; Hartmann et al., 2023; Haviv et al., 2022; Hayes et al., 2025a,b; Huang et al., 2022, 2024; Ippolito et al., 2022; Jagielski et al., 2022; Kandpal et al., 2022; Kiyomaru et al., 2024; Lee et al., 2023; Leybzon and Kervadec, 2024; Lu et al., 2024; Mireshghallah et al., 2022; Morris et al., 2025; Nasr et al., 2025; Prashanth et al., 2024; Satvaty et al., 2024; Schwarzschild et al., 2024; Stoehr et al., 2024; Zhang et al., 2023). Most research on this topic has focused on this issue due to its associated privacy and legal risks, such as models 0Here we use term training data loosely and elaborate on it later in the Nomenclature section. Corresponding author(s): federico.barbero@cs.ox.ac.uk 2025 Google DeepMind. All rights reserved Extracting alignment data in open models leaking personal or copyrighted information (Cooper and Grimmelmann, 2024; Cooper et al., 2025; Freeman et al., 2024; Huang et al., 2022; Ippolito et al., 2022; Karamolegkou et al., 2023; Lee et al., 2023; Mueller et al., 2024; Satvaty et al., 2024). Prior work on memorisation is often centred around verbatim or near-verbatim training data extraction (Carlini et al., 2021, 2022; Huang et al., 2024), where success is measured by exact (or very close) matches on tasks where this is important, like measuring similarity of credit card number or paragraph from copyrighted book. In contrast, in this work we study and develop more subtle notion of training data extraction patterns and templates of proprietary data where the semantic structure is just as valuable as the literal content. Consequently, existing extraction methods and metrics used to determine failure or success using simple string matching are not well-aligned for this task. We are interested in understanding if an LLM will leak training data that is sensitive due to its utility in improving model performance. In other words, if models competitive advantage comes from its secret training data, and models have tendency to memorise, and then regurgitate this data, then the competitive advantage itself may be at risk. This is especially topical with the surge of more sophisticated training modes such as the new thinking paradigm (Nye et al., 2021; Reynolds and McDonell, 2021; Wei et al., 2022). It is also important from the point of view of the commonplace practice of model distillation, where competitor may use strong model to train their own. If models regurgitate training data, then through distillation competitor is (at least in part) training on the original training data as well. By exploiting the fact that in open-weight models, the end user controls tokenization, and that the chat template structure is only introduced during post-training, we use simple attack which demonstrates open-weight models will repeat numerous training prompts 1. We are able to extract data ranging from reinforcement learning (RL) prompts and associated traces to prompts used for supervised fine-tuning (SFT) and during mid and pre-training. In doing so, we answer an open question from Xu et al. (2024) and confirm that models do indeed recite their own alignment data and that therefore distillation pipelines are likely to partly include original training data samples."
        },
        {
            "title": "Summary",
            "content": "Our hypothesis: Since the chat template is exclusively introduced in post-training, if we prompt the model with the template, it will generate alignment data. Our procedure: 1. Prompt the model with the chat template and sample. Repeat this number of times to generate set of synthetic data points. 2. For each synthetic data point, find the closest sample in the post-training dataset according the embedding similarity, using an embedding model. Our main finding: One may reasonably expect the synthetic data points to be from the same distribution as the alignment dataset. We find that this is true, but it is far closer to the alignment dataset that one may expect. For example, in Figure 1, we generated maths question and answer that is near identical to sample in the alignment dataset. 1Our attack relies on the users control over the chat template structure. This is not normally available for closed models, and so our attack does not immediately apply to them. Our prompting strategy is similar to the one used by Xu et al. (2024). 2 Extracting alignment data in open models Figure 1 An overview of the data extraction process. sample generated using the prompt <endoftext><user>n is used as query for vector search to find the best semantic match from the training data. The figure highlights the difference between high embedding similarity score (0.97) and much lower string-match similarity score (0.68), demonstrating that semantic similarity is more effective for detecting this form of data memorisation. Contributions We outline our contributions and findings: We generalise the prompting strategy from Xu et al. (2024) and study how it may be used to extract post-training data from models trained with SFT and RL, addressing their open question on memorisation. We show that measuring the quantity of memorisation based on approximate or verbatim string matching is likely significantly undercounting rates of memorisation our results suggesting at least by 10. Instead, using neural embeddings reveals much higher rate. We finally demonstrate that our method can be used to generate datasets from model that can then be used for the (post-)training of another model, meaningfully capturing some of the original models performance. 2. Nomenclature In this paper, we rely on the definitions below. We refer the reader to Section for thorough background overview. Alignment Data: This term is used broadly to refer not just to data for safety and instruction-following (like Supervised Finetuning and Reinforcement Learning with Human Feedback datasets), but to any targeted data collection used to steer model behavior and improve specific capabilities such as mathematics, reasoning, or long-context understanding. This data is usually considered significant competitive asset. Training Data: The term training data is usually used to describe data from all phases of model creation, including pre-training, mid-training, and post-training, which encompasses data used for SFT and RL fine-tuning. In this paper we expand the usual meaning to also cover semantically equivalent representation of the training data. Post-training: This refers to the training stages that occur after the initial large-scale pre-training phase. It involves using various specialized datasets and methods, such as Supervised Finetuning (SFT) and Reinforcement Learning (RL), to align the models capabilities with desired behaviors and tasks. In this work, we treat post-training data as (large) subset of total alignment data, as it is becoming increasingly common to include alignment data (maths, reasoning, etc.) also in earlier 3 Extracting alignment data in open models stages of training. Memorisation 2: This refers to the phenomenon where model can regurgitate or recite its training data. Our work extends this definition beyond verbatim string matching to include more abstract notion of memorisation, such as reproducing the patterns, templates, and semantic structure of proprietary data. Related to this concept is approximate memorisation (e.g., Comanici et al. (2025)), but instead of using simple string-matching similarity metrics, in our work we show that text embeddings seem much better suited at detecting approximate memorisation when semantics are important. We refer the reader to Ippolito et al. (2022) for further motivation behind why we expand our definition of memorisation. Chat Template: specific structure used to format prompts by wrapping user, assistant, or system messages in special tokens (e.g., <user>, <assistant>). key aspect of our attack is that these templates and their special tokens are typically introduced only during the post-training stage, where most of the alignment is done. Embedding Score: metric we use to measure memorisation based on the semantic similarity between two pieces of text, calculated using the dot product of neural text embeddings. We propose this as more effective alternative to traditional string-matching metrics (like Levenshtein distance), as it better captures semantic equivalence even when there are superficial differences. 3. Extracting alignment data with neural embeddings The proposed extraction strategy is based on the observation that certain prompts seem to consistently induce the model into outputting alignment-like data. To enable our attack, we use special tokens from the chat template that are precisely introduced during post-training, making them ideal artifacts that can be leveraged to extract specific types of data. Our main contribution is confirming that many of such generations are either verbatim or very close to true training samples under an appropriate measure of similarity. Our pipeline works as follows: we embed the entire post-training set using an embedding model (see 3.3 for details), constructing vector search engine. We then generate number of samples simply by prompting the model using our chosen prefix repeatedly. For each generated sample we then embed it and search the vector database to retrieve the best match and its score. diagram of this process is shown in Figure 1. Once the post-training dataset is embedded, the search can be implemented as single matrix multiplication. In the following Sections 3.1 and 3.2, we will provide motivation for our methodology using generated examples. significant constraint of our work is that we require access to models that make their (post-)training mixtures available. For this reason, we focus our study on OLMo 2 (OLMo et al., 2024) (Section 4) for SFT and Open-Reasoner-Zero (Hu et al., 2025) (Section 5) for RL. As these models are post-trained with standard methods, they are valuable resource for our study. 3.1. Chat templates to steer generations The observation that chat templates can be used to generate useful synthetic data has been pointed out already by Xu et al. (2024), where the authors use special prompts and filtering pipeline to generate dataset that can be used to post-train models using SFT. In our work, we study this from different angle and aim to understand the extent to which the generations correspond to regurgitated 2We make no statement with regard to whether or not model contains its training data in bit-wise or code-wise sense, nor in the sense that any arbitrary instance of training data can be perfectly retrieved. 4 Extracting alignment data in open models training data. In particular, we positively answer one of the conjectures left from Xu et al. (2024), which posited that the data generated might have been training data. In Figure 2, we show how the prompting the model with the chat template results in outputs that resemble post-training samples. We will investigate this phenomenon in much more detail in the coming sections. Qwen2.5 7B Instruct Qwen2.5 7B Instruct <im_start>user Im trying to use the zip function in Python with generator expression, but keep encountering ValueError: too many values to unpack. Heres the code Im working with: ```python gen = (x for in range(3)) a, = zip(*gen) ``` [...] <im_start>user Okay, so need to figure out the value of 2 + 2 + 2 + . the expression 2 + Hmm, let me think. remember that infinite nested radicals can sometimes be solved by setting them equal to variable and then solving an equation. Let me try that approach here. Let me denote the entire expression as ùë•. So, ùë• = [...] 2 + 2 . . . Figure 2 Generations from Qwen2.5 7B Instruct using <im_start>usern as prefill. Qualitatively, the generations resemble potential post-training samples. 3.2. Measuring aproximate semantic memorisation central theme in our work is the wish to broaden the definition of memorisation beyond simple string matching, due to its natural limitations. For example, string matching has been shown to be vulnerable to style-transfer (Ippolito et al., 2022), where semantics are preserved whilst easily evading string matching checks. Further, we are interested in the usefulness of extracted samples as training points; if two samples are semantically equivalent, then they reasonably should be treated equal as training samples (for measurement of memorisation rate). While approximate string matching may (accidentally) weakly capture semantics in certain cases, we found it to be unreliable and generally not well-aligned with the notion of semantic equivalence. We use Figure 3 as small illustrative example from generation which came from our pipeline. It is clear that the left and right samples are extremely similar up to small differences, for example in the numerical contents of the choices in the multiple choice section of the question. String-matching scores tend to penalise these differences quite heavily assigning similarity of 0.7, while the high embedding score (see Section 3.3 for details) arguably aligns better with the memorisation judgement human would make. We observed many situations where string-matching score would provide low similarity while the samples are semantically extremely similar. We point out that under the standard 0.9 approximate memorisation threshold (Comanici et al., 2025), the example in Figure 3 would count as not memorised. We delay to Section 4.1 more detailed investigation of the limitations of string matching. 5 Extracting alignment data in open models Generation SFT Training sample <endoftext><user> The question is: Find the value of from (12)^3 6^4 432 = y? Options: (A) 2456 (B) 4578 (C) 6234 (D) 7386 (E) 5184 The answer is: (E) Explanation: Given Exp. = (12)3 64 = (12)3 64 = (12)2 62 = (72)2 = 5184 432 12 64<endoftext> Question: Find the value of from (12)^3 6^4 432 = y? Options: (A) 2356 (B) 3467 (C) 4567 (D) 5189 (E) 5184 Random thoughts: Given Exp. = (12)3 64 = (12)3 64 = (12)2 62 = (72)2 = 5184 432 12 62 The final answer: (E). Figure 3 (Left) Generation from OLMo 2 13B. (Right) True post-training sample. Neural embeddings match provide score of 0.986 using gemini-embedding-001 (Lee et al., 2025), while normalised Levenshtein similarity provides match score of 0.699, heavily penalising differences in the options, even though the semantics remain identical. When computing similarities we always strip out special tokens, highlighted in blue. We report in the Appendix (Section D) more of such examples. 3.3. Measuring memorisation with neural embeddings Embedding scores are generated by an embedding model and can be tuned for different tasks. The process usually involves taking strong base model and training it contrastively for specific application (e.g., semantic retrieval) (Lee et al., 2024). We use the gemini-embedding-001 model (Lee et al., 2025) in all of our experiments as it is general and strong embedding model. We generate single embedding for each sample, removing all special tokens and therefore only considering the plain text. This acts as vector search engine, where we can compute similarity with respect to each training sample using single matrix multiplication and taking the argmax. As embeddings are normalised to be unit-norm, their dot product naturally encodes notion of similarity. To better distinguish this from approximate memorisation measures using string matching, we call this measure using embeddings measure of approximate semantic memorisation. To define sample as (approximately semantic) memorised, we need to choose an appropriate threshold. We do this manually and report randomly chosen samples at different thresholds in the Appendix (Sections and E). We chose threshold of 0.95 for neural embeddings as more conservative choice. We found samples at this point having qualitatively similar properties that would be indisputable given reasonable human judge. The choice of the threshold will naturally affect the measured memorisation. This limitation is however also present in choosing threshold for what is considered memorised according to string matching metric. 4. Large scale extraction of SFT data We focus our SFT memorisation study on OLMo 2 (OLMo et al., 2024)3. OLMo 2 comes with the full training mixture, alongside the training details. Further, the models have strong downstream 3Licensed by AllenAI under the Apache License, Version 2.0. 6 Extracting alignment data in open models performance, enabling us to conduct experiments that result in findings that are likely generalizable to other models. The uncompressed pre-training mix has size of 22.4 TB while the higher quality mid-training split has size of 5.14 TB. The post-training is divided in 3 stages, the first is an SFT stage with dataset containing 939k samples, then Direct Preference Optimisation (Rafailov et al., 2023) step is conducted with 378k samples, and finally RL with Verifiable Rewards (RLVR) step with 29.9k samples. We focus on the extraction of the SFT data in this section. We apply the procedure we described in Section 3 using neural embeddings. We embed the SFT training samples using gemini-embedding-001 by concatenating the question and answer sequences as single block of text. To extract the data from the model, we generate conditioning on the following initial tokens <endoftext><user>n, which are the starting tokens of the chat template. We leave the temperature at the default value of 1. 4.1. String matching poorly captures memorisation We start by evaluating the memorisation using traditional string matching metrics. We consider 100k generations for OLMo 2 13B (OLMo et al., 2024) using our extraction method and search for their closest match in the post-training set, with respect to different similarity measures. We consider the normalised Levenshtein similarity defined as 1 Levenshtein( ùê¥, ùêµ)/max(len(A), len(B)) and the normalised Indel similarity defined as 1 Indel( ùê¥, ùêµ)/(len(A) + len(B)). The Indel similarity is related to the Levenshtein distance, but applies cost of 2 to substitutions. For each generated sample, we find the highest similarity based on the two string matching methods in the post-training set. We follow the heuristic used by Gemini 2.5 (Comanici et al., 2025) and characterise sample as approximately memorised when its similarity is above 0.9. (a) Distribution of Levenshtein scores. (b) Distribution of Indel scores. Figure 4 Histograms illustrating the distribution of memorisation scores, measured by Levenshtein and Indel similarity scores. For each generation, we find the largest similarity score in the posttraining set of OLMo 2. In Figure 4 we show that when judging memorisation rates based on string matching scores, memorisation rates seem negligible. The judgement from such results would be that our prompting strategy does not extract useful memorised data under string matching metrics. This however does not paint the entire picture. For example, the generated sample in Figure 3, would not be considered memorised under the heuristic of Comanici et al. (2025). This is the extent to which measuring memorisation on simple string matching is problematic; it does not flag examples that any reasonable human would judge as clear cases of memorisation. This is because string matching can quickly become non-informative due to trivial differences (see Appendix Section D). 7 Extracting alignment data in open models 4.2. Large scale extractions with embeddings We now compare the string matching results to matching done using neural embeddings. We generate 1M samples with OLMo 13B using the same method and embed them using gemini-embedding-001. In Figure 5, we show that neural embeddings unveil much higher memorisation rate (left) when compared to string matching scores Figure 4. The scatter plot (right) shows that string matching distances are not well-aligned with semantic memorisation and also seem to exhibit strong string length bias as well, where longer generations are consistently given lower Levenshtein similarity scores. We find that neural embeddings are much better at dealing with cases such as Figure 3 and provide number of examples in the Appendix (Section D). (a) Distribution of gemini-embedding-001 scores. (b) Levenshtein vs embedding scores. Figure 5 Histograms illustrating the distribution of embedding scores generated with gemini-embedding-001 (left) and scatter plot comparing the embedding scores to the Levenshtein distance, with points coloured by string length (right). Coverage. We now check the coverage of the post-training data, where for each post-training sample, we report the largest embedding score out of the 1M generated samples. We report the results in Figure 6. We find that some samples are much more memorised than others. While it is hard to understand exactly why, our investigations revealed that samples are much more likely to be memorised if similar samples are also present in the pre and mid training datasets. Further, samples that appear often, for example different versions of the same reasoning problem, seem to be more likely to be memorised. (a) Empirical distribution of coverage of post-training set. (b) Empirical cumulative distribution of coverage of post-training set. Figure 6 We report for each true post-training sample histogram of the best score between the 1M generations (left) and the cumulative distribution (right). Due to the nature of the experiment, coverage should improve monotonically as sampling more can only increase memorisation. 8 Extracting alignment data in open models Chat template better recovers post-training data. We finally show that conditioning on the chat template is useful to encourage the model to generate post-training-like data. As baseline, we compare this method to simply conditioning on the single token <endoftext>. We consider 1,000 generations with both prefixes and report the estimate on the expected value of the embedding score. In Table 1, we show that indeed conditioning on the entire chat template provides samples closer to the post-training distribution. We suspect that since chat templates are only present during post-training, the model associates their presence with the post-training data distribution. This provides an explation for why techniques such as Magpie (Xu et al., 2024) are possible: conditioning on the chat template results in generations that are much closer to the post-training distribution. Prefill Type Mean Embedding Score <endoftext> <endoftext><user>n 0.857 0.892 Table 1 Mean best embedding score of generations for OLMo 2 13B using only the beginning of sequence token versus the full chat template prefill. The longer prefill generates samples that are on average semantically closer to the post-training set. 4.3. Direct distillation on extracted data natural question one may then have is, if the generated data is similar to the original post-training dataset, can it be used to post-train model directly? In other words, are we able to re-use large and diverse enough number of samples to post-train model without collecting any data manually? To explore this question, we post-train using SFT OLMo 2 7B in two ways: (1) the original dataset in order to reproduce the original results and (2) our generated dataset. For the synthetic dataset, we collect synthetic dataset of similar size of 930k samples. We perform basic filtering and processing using Gemini 2.5. We note that even though the number of samples is the same, the original SFT training is over 1.3B tokens, while the synthetic training set has only 850M tokens as the filtered generations remain shorter. In Table 2, we report the results following the benchmarks and evaluation pipeline used by OLMo et al. (2024). To validate our setup, we first show that our reproduction is very close to the released SFT checkpoint. Our model trained on synthetic data also achieves comparable performance on the benchmarks, except for the IFE task. We suspect that our pipeline generates too few examples that target this benchmark, but believe the performance could likely be improved by adding non-synthetic data. In fact, it is likely that mix of synthetic and curated targeted data could be useful paradigm to explore to boost model performance reducing the labour required to collect entire post-training datasets. Model BBH MMLU MATH GSM8K POPQA TQA IFE DROP Baseline (SFT only) 0.4953 0.6133 0.2073 Baseline (Reproduction) 0.4944 0.6123 0.2077 0.5161 0.6052 0.1705 Extracted Data 0.7407 0.7377 0.7847 0.2364 0.2529 0.2490 0.4858 0.6562 0.5960 0.5110 0.6617 0.5945 0.5529 0.5028 0.5923 Table 2 Model performance after SFT on the benchmarks considered by OLMo et al. (2024). The baseline performance is taken from OLMo et al. (2024) and the reproduction was ran using the code provided by the authors with the original dataset. Using our method, we train model on SFT synthetic data extracted from the model using the same settings of the baseline. 9 Extracting alignment data in open models 5. Large scale extraction of RL data We now focus on the extraction of RL data. We use the Open-Reasoner-Zero (Hu et al., 2025) model, which was trained from the Qwen 2.5 base model with PPO (Schulman et al., 2017) using post-training data that is publicly available. With RL, the training samples consist of questions and answers, but the reasoning traces not part of the training dataset as they are artifacts of the training rollout. For this reason, we focus on the extraction of the questions and answer part of the dataset although note that reasoning traces can be useful in their own right. We prompt the model by again taking the first part of the chat template specified by the developers of the model (see Section in the Appendix for the entire prefix) and generate 100k samples independently. We find that the model very consistently generates question, followed by thinking trace, and finally an answer. We then searched the training set for these generations. Surpisingly, we again found number of training samples being regurgitated verbatim. We show an example in Figure 7, where the model outputs the exact training sample, reasoning trace, and the correct solution. We find the fact that models are capable of regurgitating RL training samples to be counterintuitive as the PPO objective, at least at glance, seems rather misaligned with the memorisation of training samples, especially when compared to methods such as SFT that instead very explicitly increase sequence likelihoods."
        },
        {
            "title": "RL Training sample",
            "content": "<PREFIX> Find the quotient of the division (3ùëß4 4ùëß3 + 5ùëß2 11ùëß + 2)/(2 + 3ùëß). To solve the problem of finding the quotient of the division (3ùëß4 4ùëß3 + 5ùëß2 11ùëß + 2) (2 + 3ùëß), we will use polynomial long division. Here are the steps: [...] The final answer is: ùëß3 2ùëß2 + 3ùëß 17 3 Find the quotient of (3ùëß4 4ùëß3 + 5ùëß2 11ùëß + 2)/(2 + 3ùëß). the division ùëß3 2ùëß2 + 3ùëß 17 Figure 7 (Left) Generation from Open-Reasoner-Zero 7B using our special <PREFIX> (see Appendix Section for the full string). We shorten the thinking trace with [...]. (Right) True RL post-training sample. Surprisingly, we find that RL training samples can be regurgitated verbatim, even though the training objective seems to be heavily misaligned with this behaviour. The RL training samples only consist of question and answer pair and do not come with thinking trace. The model instead regurgitates the question, followed by the thinking trace, and finally the answer. We explore this further phenomenon further by measuring the change of likelihood of training set samples in the base and post-trained models. Measuring likelihood of the training set is limited because this only measures the pointwise memorisation likelihood of training sample might remain low because the exact polynomial in the training set for instance is not being memorised, but the question style of finding its roots is. Regardless of the limitation of this simple measurement, we believe the results can still provide valuable signal and intuition. In particular, we measure the likelihoods of each PPO training sample question under the Qwen 2.5 base model and the Open-Reasoner-Zero model. If the RL process induces memorisation, then we would see the likelihood using the post-trained model increase on the training samples. We bucket the results in likelihoods increasing by magnitudes of 10 for the base Qwen model and the 10 Extracting alignment data in open models Open-Reasoner-Zero model and report the results in Figure 8. The results show that RL training induces many of the training prompts to increase in likelihood. We found samples of likelihoods increasing from 1011 to 105 after RL post-training, showcasing the fact that RL may be able to induce the memorisation of post-training samples. This is particularly surprising when one considers the RL post-training objective. It is not immediately clear to us what exact mechanism is driving this increase in likelihood and leave this as future exciting research direction. Figure 8 Count of training samples (prompt only) with likelihood above certain threshold before and after RL. The entire train set contains 142,770 samples. The base model is Qwen2.5 7B. After RL, many training prompts achieve higher likelihood. Figure 9 RL training using the ORZ (Hu et al., 2025) dataset and dataset that was extracted using our method. Surprisingly, we are able to recover most of the performance with our simple extraction method. RL on extracted dataset We now show that one can perform similar extraction pipeline but to instead extract an RL dataset. In other words, one can use an RL-trained model to extract an RL dataset with little effort. We start by post-train using Dr. GRPO (Liu et al., 2025) the Qwen2.5 7B base model using the ORZ 57k dataset (Hu et al., 2025). With the resulting post-trained model (we call it Baseline), we then generate using our method 100k samples and process them using Gemini 2.5 (Comanici et al., 2025) to filter out invalid, incomplete or incorrect samples. We finally randomly select from this set 57k synthetic samples to create our synthetic training data. We use this to post-train the Qwen2.5 7B base model using synthetic data only. Afterwards, we evaluate both models on four standard benchmarks: AMC (Li et al., 2024), MATH500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). We then report the results in Figure 9. The model trained on the synthetic data extracted from the Baseline achieves comparable performance on the benchmarks. These results are surprising because our synthetic dataset is based on extracted data from small 7B model trained on relatively small RL dataset. We 11 Extracting alignment data in open models suspect that more sophisticated pipeline could be used to achieve higher performance and retrieve higher quality dataset. As our work focuses on training sample extraction, we leave this goal as promising future direction. 6. Conclusion In this paper, we demonstrate that alignment data can be efficiently extracted from open-weight large language models. Our attack leverages simple observation: chat templates and their special tokens are typically introduced during post-training, making them effective prefixes for inducing models to regurgitate alignment-like data. Leakage Metrics are Hard key finding is that the true scale of this leakage is hidden by traditional metrics. We show that methods relying on string matching drastically undercount the rate of extraction, by (at least!) an order of magnitude. By instead using high-quality embedding models, we can identify approximate semantic memorisationinstances where the model reproduces the semantic structure and patterns of training data, even if not verbatim. This semantic similarity is far more relevant for capturing the utility of the data, as highlighted by numerous examples all throughout the paper. Model Distillation as Data Distillation We demonstrate that data extracted from post-trained model can be used to successfully train new base model, meaningfully recovering parts of the originals performance in both SFT and RL settings. This confirms that the common practice of model distillation can function as an indirect form of training data extraction. Certain advantages that an open model obtains from its alignment data are therefore at risk of being leaked. Rethinking Memorisation in RL Surprisingly, we find that models readily regurgitate training samples (even!) from Reinforcement Learning (RL) phases. This is counter-intuitive, as the reinforcement learning objective is not explicitly designed to increase sequence likelihoods in the same way as SFT. The fact that RL training prompts increased in likelihood after post-training suggests more complex relationship between alignment and memorisation that warrants future investigation. Our attack exploits chat templates and so is only applicable to open models. Closed models enforce the chat template, and is outside of the users control; working exploit on closed model would require user to spoof the behaviour of these tokens when submitting query to the model. Whilst more difficult, prior work (Geng et al., 2025) has shown this is not necessarily impossible. Future work will establish how serious threat this is on closed models."
        },
        {
            "title": "Author contributions",
            "content": "Federico ran experiments, led the research and wrote the paper. Jamie and Ilia led the project and wrote the paper. Xiangming ran the RL experiments and wrote the paper. Matthew, Chris, Petar, Chawin gave feedback and helped write the paper."
        },
        {
            "title": "References",
            "content": "S. Biderman, U. Prashanth, L. Sutawika, H. Schoelkopf, Q. Anthony, S. Purohit, and E. Raff. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems, 36:2807228090, 2023. L. Borec, P. Sadler, and D. Schlangen. The unreasonable ineffectiveness of nucleus sampling on mitigating text memorization. arXiv preprint arXiv:2408.16345, 2024. 12 Extracting alignment data in open models G. Brown, M. Bun, V. Feldman, A. Smith, and K. Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In Proceedings of the 53rd annual ACM SIGACT symposium on theory of computing, pages 123132, 2021. N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, et al. Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21), pages 26332650, 2021. N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations, 2022. B. Chen, N. Han, and Y. Miyao. multi-perspective analysis of memorization in large language models. arXiv preprint arXiv:2405.11577, 2024a. T. Chen, A. Asai, N. Mireshghallah, S. Min, J. Grimmelmann, Y. Choi, H. Hajishirzi, L. Zettlemoyer, and P. W. Koh. Copybench: Measuring literal and non-literal reproduction of copyright-protected text in language model generation. arXiv preprint arXiv:2407.07087, 2024b. G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. A. F. Cooper and J. Grimmelmann. The files are in the computer: Copyright, memorization, and generative ai. arXiv preprint arXiv:2404.12590, 2024. A. F. Cooper, A. Gokaslan, A. Ahmed, A. B. Cyphert, C. De Sa, M. A. Lemley, D. E. Ho, and P. Liang. Extracting memorized pieces of (copyrighted) books from open-weight language models. arXiv preprint arXiv:2505.12546, 2025. V. Dankers and I. Titov. Generalisation first, memorisation second? memorisation localisation for natural language classification tasks. arXiv preprint arXiv:2408.04965, 2024. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. V. Feldman. Does learning require memorization? short tale about long tail. In Proceedings of the 52nd annual ACM SIGACT symposium on theory of computing, pages 954959, 2020. J. Freeman, C. Rippe, E. Debenedetti, and M. Andriushchenko. Exploring memorization and copyright violation in frontier llms: study of the new york times v. openai 2023 lawsuit. arXiv preprint arXiv:2412.06370, 2024. Y. Geng, H. Li, H. Mu, X. Han, T. Baldwin, O. Abend, E. Hovy, and L. Frermann. Control illusion: The failure of instruction hierarchies in large language models. arXiv preprint arXiv:2502.15851, 2025. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. V. Hartmann, A. Suri, V. Bindschaedler, D. Evans, S. Tople, and R. West. Sok: Memorization in general-purpose large language models. arXiv preprint arXiv:2310.18362, 2023. 13 Extracting alignment data in open models A. Haviv, I. Cohen, J. Gidron, R. Schuster, Y. Goldberg, and M. Geva. Understanding transformer memorization recall through idioms. arXiv preprint arXiv:2210.03588, 2022. J. Hayes, I. Shumailov, C. A. Choquette-Choo, M. Jagielski, G. Kaissis, K. Lee, M. Nasr, S. Ghalebikesabi, N. Mireshghallah, M. S. M. S. Annamalai, et al. Strong membership inference attacks on massive datasets and (moderately) large language models. arXiv preprint arXiv:2505.18773, 2025a. J. Hayes, M. Swanberg, H. Chaudhari, I. Yona, I. Shumailov, M. Nasr, C. A. Choquette-Choo, K. Lee, and A. F. Cooper. Measuring memorization in language models via probabilistic extraction. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 92669291, 2025b. C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H.-Y. Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. J. Huang, H. Shao, and K. C.-C. Chang. Are large pre-trained language models leaking your personal information? arXiv preprint arXiv:2205.12628, 2022. J. Huang, D. Yang, and C. Potts. Demystifying verbatim memorization in large language models. arXiv preprint arXiv:2407.17817, 2024. Y. Huang, Z. Yang, M. Chen, J. Zhang, and M. R. Lyu. Entropy-memorization law: Evaluating memorization difficulty of data in llms, 2025. URL https://arxiv.org/abs/2507.06056. D. Ippolito, F. Tram√®r, M. Nasr, C. Zhang, M. Jagielski, K. Lee, C. A. Choquette-Choo, and N. Carlini. Preventing verbatim memorization in language models gives false sense of privacy. arXiv preprint arXiv:2210.17546, 2022. M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini, E. Wallace, S. Song, A. Thakurta, N. Papernot, et al. Measuring forgetting of memorized training examples. arXiv preprint arXiv:2207.00099, 2022. N. Kandpal, E. Wallace, and C. Raffel. Deduplicating training data mitigates privacy risks in language models. In International Conference on Machine Learning, pages 1069710707. PMLR, 2022. A. Karamolegkou, J. Li, L. Zhou, and A. S√∏gaard. Copyright violations and large language models. arXiv preprint arXiv:2310.13771, 2023. H. Kiyomaru, I. Sugiura, D. Kawahara, and S. Kurohashi. comprehensive analysis of memorization in large language models. In Proceedings of the 17th International Natural Language Generation Conference, pages 584596, 2024. J. Lee, T. Le, J. Chen, and D. Lee. Do language models plagiarize? In Proceedings of the ACM Web Conference 2023, pages 36373647, 2023. 14 Extracting alignment data in open models J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, et al. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024. J. Lee, F. Chen, S. Dua, D. Cer, M. Shanbhogue, I. Naim, G. H. √Åbrego, Z. Li, K. Chen, H. S. Vera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv preprint arXiv:2503.07891, 2025. A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. D. Leybzon and C. Kervadec. Learning, forgetting, remembering: Insights from tracking llm memorization during training. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 4357, 2024. J. Li, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. Huang, K. Rasul, L. Yu, A. Q. Jiang, Z. Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. X. Lu, X. Li, Q. Cheng, K. Ding, X. Huang, and X. Qiu. Scaling laws for fact memorization of large language models. arXiv preprint arXiv:2406.15720, 2024. A. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, checked on, 4(7):2025, 2025. F. Mireshghallah, A. Uniyal, T. Wang, D. K. Evans, and T. Berg-Kirkpatrick. An empirical analysis of memorization in fine-tuned autoregressive language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 18161826, 2022. J. X. Morris, J. O. Yin, W. Kim, V. Shmatikov, and A. M. Rush. Approximating language model training data from weights. arXiv preprint arXiv:2506.15553, 2025. F. B. Mueller, R. G√∂rge, A. K. Bernzen, J. C. Pirk, and M. Poretschkin. Llms and memorization: On quality and specificity of copyright compliance. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, volume 7, pages 984996, 2024. M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tram√®r, and K. Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023. M. Nasr, J. Rando, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, F. Tram√®r, and K. Lee. Scalable extraction of training data from aligned, production language models. In The Thirteenth International Conference on Learning Representations, 2025. M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. T. OLMo, P. Walsh, L. Soldaini, D. Groeneveld, K. Lo, S. Arora, A. Bhagia, Y. Gu, S. Huang, M. Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. 15 Extracting alignment data in open models L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. A. Pappu, B. Porter, I. Shumailov, and J. Hayes. Measuring memorization in rlhf for code completion. arXiv preprint arXiv:2406.11715, 2024. U. S. Prashanth, A. Deng, K. OBrien, J. SV, M. A. Khan, J. Borkar, C. A. Choquette-Choo, J. R. Fuehne, S. Biderman, T. Ke, et al. Recite, reconstruct, recollect: Memorization in lms as multifaceted phenomenon. arXiv preprint arXiv:2406.17746, 2024. A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. L. Reynolds and K. McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 CHI conference on human factors in computing systems, pages 17, 2021. A. Satvaty, S. Verberne, and F. Turkmen. Undesirable memorization in large language models: survey. arXiv preprint arXiv:2410.02650, 2024. J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. A. Schwarzschild, Z. Feng, P. Maini, Z. Lipton, and J. Z. Kolter. Rethinking llm memorization through the lens of adversarial compression. Advances in Neural Information Processing Systems, 37:56244 56267, 2024. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. I. Shilov, M. Meeus, and Y.-A. de Montjoye. The mosaic memory of large language models. arXiv preprint arXiv:2405.15523, 2024. N. Stoehr, M. Gordon, C. Zhang, and O. Lewis. Localizing paragraph memorization in language models. arXiv preprint arXiv:2403.19851, 2024. E. Wallace, K. Xiao, R. Leike, L. Weng, J. Heidecke, and A. Beutel. The instruction hierarchy: Training llms to prioritize privileged instructions. arXiv preprint arXiv:2404.13208, 2024. X. Wang, A. Antoniades, Y. Elazar, A. Amayuelas, A. Albalak, K. Zhang, and W. Y. Wang. Generalization vs memorization: Tracing language models capabilities back to pretraining data. arXiv preprint arXiv:2407.14985, 2024. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 16 Extracting alignment data in open models Z. Xu, F. Jiang, L. Niu, Y. Deng, R. Poovendran, Y. Choi, and B. Y. Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, W. Dai, T. Fan, G. Liu, L. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. Tram√®r, and N. Carlini. Counterfactual memorization in neural language models. Advances in Neural Information Processing Systems, 36:3932139362, 2023. C. Zheng, S. Liu, M. Li, X.-H. Chen, B. Yu, C. Gao, K. Dang, Y. Liu, R. Men, A. Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. A. Extended background Our work sits at the intersection of key areas in the study and development of frontier LLMs. The first is model alignment, where curated data and specialised training techniques are used to transform base model into useful assistant. The second is training data memorisation, the observation that models are capable of regurgitating training data. The third is the widespread practice of model distillation, process through which strong models capabilities can be transferred to another. In this section, we will review these areas and argue that there is risk that lays at the intersection of the three. A.1. Model alignment Early models (Devlin et al., 2019; Radford et al., 2018) were built on the idea that pre-training on larger and larger amounts of internet data was the solution to keep improving capabilities. Today, this principle has arguably shifted and we have seen surge in multi-stage training with number of specialised datasets and techniques, giving birth to what is now called post-training. LLMs are post-trained using various methods ranging from Supervised Finetuning (SFT) to now very popular RL methods such as Proximal Policy Optimisation (PPO) (Schulman et al., 2017), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), among many others. In this work, we mainly focus on models that were post-trained using SFT and PPO as they are common post-training techniques and we have strong open models that were trained using such techniques with publicly available post-training data. Supervised Finetuning With SFT, one collects dataset ùê∑ of question-answer pairs ùê∑ = {(ùëÑùëñ, ùê¥ùëñ)}ùëÅ ùëñ=1. The model is then optimised to predict an answer ùê¥ùëñ given question ùëÑùëñ, i.e. to increase the likelihood ùëÉ( ùê¥ùëñùëÑùëñ). To achieve this, usually one masks out the contributions of the loss that come from the question part, i.e. computing the conditional gradient ùúÉ log ùëÉ( ùê¥ùëñùëÑùëñ; ùúÉ) given some parameters ùúÉ. The parameters are updated via some form of gradient descent on the negative log-likelihood: ùúÉ ùúÉ + ùúÇ 1 ùêµ ùúÉ ùëò ùêµ log ùëÉ( ùê¥ùëòùëÑùëò; ùúÉ) 17 Extracting alignment data in open models given some learning rate ùúÇ > 0 and batch ùêµ ùê∑. The OLMo 2 (OLMo et al., 2024) family of models has been post-trained in large part using SFT with dataset of 939,344 question-answer pairs which has been released publicly, providing an excellent resource for our study. RL-based post-training We focus on reinforcement learning with verifiable rewards (RLVR) (Guo et al., 2025) in this work. The Open-Reasoner-Zero (Hu et al., 2025) model adopts PPO (Schulman et al., 2017) for policy optimization in the post-training. The training objective could be written as JPPO(ùúÉ) = ùîºùëÑD,ùëÇùë°ùúãùúÉold ( ùëÑ) (cid:34) ùëÇ ùë°=1 min (cid:32) ùúãùúÉ(ùëÇùë° ùëÑ, ùëÇ<ùë°) ùúãùúÉold (ùëÇùë° ùëÑ, ùëÇ<ùë°) ÀÜùê¥ùë°, clip (cid:32) ùúãùúÉ(ùëÇùë° ùëÑ, ùëÇ<ùë°) ùúãùúÉold (ùëÇùë° ùëÑ, ùëÇ<ùë°) , 1 ùúÄ, 1 + ùúÄ (cid:33)(cid:35) (cid:33) ÀÜùê¥ùë° Here ùëÇ is the generated response, ùúñ is the clipping ratio, and ÀÜùê¥ùë° is an estimator of ùë°-th tokens advantage, which is computed through the Generalized Advantage Estimation (GAE) (Schulman et al., 2015) with learned value model and reward function. GRPO (Guo et al., 2025; Shao et al., 2024) removed the value function and estimated the advantage ÀÜùê¥ùë° in group manner. For each question ùëÑ, GRPO samples group of responses {ùëÇ1, ùëÇ2, ..., ùëÇùê∫}, which can be rewarded as {ùëÖ1, ùëÖ2, ..., ùëÖùê∫} using the reward function. Normally the reward function is shaped by whether the response correctly answers the question (the answer within specified format matches the correct answer) and whether the response format is correct. Then the advantage is estimated as ÀÜùê¥ùëñ the objective of GRPO can be written as ùëÖùëñ mean( {ùëÖùëñ }ùê∫ std( {ùëÖùëñ }ùê∫ ùëñ=1 ) . Therefore, ùë° = ùëñ=1 ) Jùê∫ùëÖùëÉùëÇ(ùúÉ) = ùîºùëÑD,{ùëÇùëñ ùë° }ùê∫ (cid:34) ùëÇùëñ ùê∫ 1 ùê∫ 1 ùëÇùëñ ùëñ=1 ùë°=1 ùëñ=1ùúãùúÉold ( ùëÑ) (cid:32) ùúãùúÉ(ùëÇùëñ ùúãùúÉùëúùëôùëë (ùëÇùëñ min ùë° ùëÑ, ùëÇùëñ <ùë°) ùë° ùëÑ, ùëÇùëñ <ùë°) ÀÜùê¥ùëñ ùë°, clip (cid:18) ùúãùúÉ(ùëÇùëñ ùúãùúÉùëúùëôùëë (ùëÇùëñ ùë° ùëÑ, ùëÇùëñ <ùë°) ùë° ùëÑ, ùëÇùëñ <ùë°) , 1 ùúñ, 1 + ùúñ (cid:19) (cid:33) ÀÜùê¥ùëñ ùë° ùõΩùîªKL(ùúãùúÉ ùúãref) (cid:35) KL regularization ensures that the policy model does not deviate the reference model too far, but could be eliminated to further improve the exploration (Hu et al., 2025). Dr. GRPO (Liu et al., 2025) removes response-level length bias in the objective and question-level difficulty bias in the advantage estimation, improving token efficiency with comparable reasoning performance. There is also significant body of work aiming to improve GRPO, with notable examples being DAPO (Yu et al., 2025) and GSPO (Zheng et al., 2025). It has now become common to format the question and answers using chat Chat templates template, which wraps the messages in special tokens that are designed to mark the messages as being user, assistant or system messages. Chat templates from security perspective are usually trained to build so-called instruction hierarchy of privilege (Wallace et al., 2024) although its practical effectiveness has been challenged (Geng et al., 2025). Crucially, the special tokens used to build the template are only introduced during post-training. In this work, we show that this makes them useful attack vector to elicit the generation of alignment data introduced exclusively in post-training. similar effect has been pointed out by Xu et al. (2024), but while their focus was the automated generation of an alignment dataset, our focus is instead that of generalising and better understanding the process from the point of view of memorisation answering one of their conjectures where they posited that models are likely memorizing such data. A.2. Memorisation It is well-documented fact that LLMs are capable of memorising and reciting training data verbatim (Carlini et al., 2021; Nasr et al., 2023, 2025). The primary concern of these studies has been publicfacing risks, such as the leak of private information like email addresses, names, and credit card 18 Extracting alignment data in open models numbers that were accidentally included in the pre-training dataset or the regurgitation of copyrighted material. common way to measure memorisation is that of (greedy) discoverable extraction Carlini et al. (2021, 2022) in such case sample is called extractable if there exists some prefix such that the LLM prompted on the prefix will generate the sample using greedy decoding. Such definition is computationally convenient as it effectively ignores the stochastic nature of LLMs and is useful for variety of tasks like observing if the model outputs copyrighted data verbatim or to discover sensitive strings of information. relaxation of this is probabilistic discoverable extraction (Hayes et al., 2025b) in which one considers the probability of extracting the sample in multiple queries. This measure provides more information by considering the joint likelihood of the sequence rather than providing simply boolean decision on extractability. Recent work has begun to focus on moving away from measuring memorisation of verbatim or near-verbatim string matches (Chen et al., 2024b; Ippolito et al., 2022). For example, Shilov et al. (2024) showed the LLMs can assemble information from related, partially overlapping fragments of text, concept they term mosaic memory. This challenges the assumption that memorisation is driven exclusively by exact repetitions in the training data. They demonstrate that even highly modified sequences can contribute significantly to the memorisation of reference sequence, with the memorisation effect being predominantly function of syntactic overlap (shared tokens). This suggests that current data deduplication techniques, which often focus on removing exact duplicates, are insufficient for addressing the full spectrum of memorisation and data leakage risks. Wang et al. (2024) introduced the concept of distributional memorisation, form of non-verbatim memorisation measured by the correlation between an LLMs output probabilities and the frequency of related input-output data pairs (n-grams) in its pretraining corpus. The authors find that this type of memorisation is prominent in knowledge-intensive tasks like factual question-answering, whereas reasoning-based tasks rely more on generalization, where the models outputs diverge from the training datas distribution. Finally, Huang et al. (2025) presents the Entropy-Memorisation Law, which establishes linear correlation between the entropy of training data and non-verbatim memorisation score measured by Levenshtein distance. The law suggests that data with higher entropy is more difficult for model to memorize accurately, resulting in higher memorisation score (i.e., more token-level differences between the models output and the original text). Our work is similar in spirit to all of these works, however, we establish memorisation rates under our embedding definition which are far higher than what may be expected from these prior works. For example, Wang et al. (2024) found reasoning-based tasks rely more on generalization than memorisation; we found that even under reasoning-based tasks, the model heavily exploits knowledge of training data through clear signs of memorisation. Jagielski et al. (2022) showed that data that appears later in the training process is more likely to be memorized. Our focus on memorisation of alignment and strategic proprietary data presents an interesting open-question about if this finding still holds for post-training. One the one hand we may expect this data to be more likely to be memorized due to results from Jagielski et al. (2022), on the other, post-training has certain properties that may make it less likely to be memorized. For example, tokens are regularly masked during the loss computation implying part of training prompts will not contribute to model gradients, whilst prior work has shown RL (commonly used in post-training) is less likely to memorize than standard instruction tuning (with the same compute budget) (Pappu et al., 2024). 19 Extracting alignment data in open models A.3. Model Distillation Model distillation is popular technique for creating capable models without the massive cost of training from scratch (Hinton et al., 2015). In this process, smaller student model is trained on the outputs generated by larger, more powerful teacher model (e.g., using GPT-4 to generate training data for new open-source model). While often viewed as way to transfer capabilities, distillation can be re-framed through the lens of memorisation. If teacher model is capable of reciting its own proprietary training data, any student model trained on its outputs will, by definition, be exposed to this secret sauce. This risk is especially pronounced in hard distillation pipelines that directly use the teachers labels as training examples (Xu et al., 2024)4. The core question, which our work addresses, is how much of the teachers original training data is unintentionally passed down to the student. Our position Collecting proprietary strategic data is expensive and time consuming; model developers and data curators clearly do not want to give this data freely to other parties. On the other hand, practitioners are already actively training on outputs from strong competing models. If this data leaks through this process, the competitive advantage of the original data owner is eroded. This motivates our memorisation study within this context. B. Accidental gradient alignment An intriguing consequence of our work is that we observe training data being memorised verbatim even if the gradients are masked. This constitutes an interesting type of leakage that to the best of our knowledge has not been previously observed. We propose some mathematical speculation for how this might happen. To make this statement more precise, the claim is that given question ùëÑ and answer ùê¥, we question whether updating based on the conditional gradient ùê¥ùëÑ can increase the likelihood of ùëÑ. Let the model parameters be ùúÉ. During training on pair (ùëÑ, ùê¥), the parameters are updated from ùúÉ to ùúÉ using gradient descent on the negative log-likelihood of the answer: ùúÉ = ùúÉ + ùúÇùúÉ log ùëÉ( ùê¥ùëÑ; ùúÉ), where ùúÇ > 0 is the learning rate. We are interested in the change in the log-probability of the question, log ùëÉ(ùëÑ; ùúÉ), after this update. first-order Taylor expansion of log ùëÉ(ùëÑ; ùúÉ) around ùúÉ gives: log ùëÉ(ùëÑ; ùúÉ) log ùëÉ(ùëÑ; ùúÉ) + (ùúÉ ùúÉ)ùëá ùúÉ log ùëÉ(ùëÑ; ùúÉ) Substituting the update rule for the term (ùúÉ ùúÉ), we obtain: log ùëÉ(ùëÑ; ùúÉ) log ùëÉ(ùëÑ; ùúÉ) + ùúÇ (ùúÉ log ùëÉ( ùê¥ùëÑ; ùúÉ)) (ùúÉ log ùëÉ(ùëÑ; ùúÉ)) . For the likelihood of ùëÑ to increase, the inner product of the two gradients must be positive: (ùúÉ log ùëÉ( ùê¥ùëÑ; ùúÉ)) (ùúÉ log ùëÉ(ùëÑ; ùúÉ)) > 0. This condition is rather intuitive. The first gradient, ùúÉ log ùëÉ( ùê¥ùëÑ; ùúÉ), is the direction in parameter space that maximally increases the probability of the answer ùê¥ given the question ùëÑ. The second, 4This is contrast to soft distillation in which one trains on the logits instead of the output labels. Soft distillation is challenging if the vocabulary of the two models is not the same. 20 Extracting alignment data in open models ùúÉ log ùëÉ(ùëÑ; ùúÉ), is the direction that maximally increases the unconditional probability of the question ùëÑ. If their dot product is positive, the two gradients are correlated and increasing the likelihood ùëÉ( ùê¥ùëÑ) also increases the likelihood ùëÉ(ùëÑ). We note as caveat that simply considering the likelihood of single sequence might not be the most informative metric. For instance, math-related post-training sample might have different numerical values, but the general template might be correct. This suggests that one should really integrate over all reasonable sequences that one wishes to consider which is of course quickly intractable. C. Prefixes used for generation Below we provide the generation prefixes used for the experiments in our work. Both are prefixes taken directly from the template released by the developers of the corresponding models. SFT extraction prefix <endoftext><user> RL extraction prefix conversation between User and Assistant. The User asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>. And your final answer will be extracted automatically by the boxed{} tag. This is the problem: D. Failure cases of string matching We report in this section interesting failure cases we have found with string matching score metrics. We recall that our chosen embedding threshold is 0.95, while the string matching threshold is 0.90. All examples we show in this section have an embedding score that is above 0.95 and therefore would be considered as semantically memorised. Therefore, the examples we show can be seen as failure cases of the string matching measure in which instead the samples are correctly identified as memorised using semantic neural embeddings. 21 Extracting alignment data in open models Generation **Q**: man took loan from bank at the rate of 12% p.a. S.I. After 3 years he had to pay Rs. 5400 interest only for the period. The principal amount borrowed was? **A**: 15000 Embeddings (0.97 match) String Matching (0.43 match) man took loan from bank at the rate of 12% p.a. simple interest. After 3 years he had to pay Rs. 5400 interest only for the period. The principal amount borrowed by him was. Options: (A) 15000 (B) 2655 (C) 16888 (D) 6677 (E) 1871 Stream of consciousness first, then make decision: Principal = Rs. (100 5400)/(12*3) = Rs. 15000. Thus, the answer is (A). man took loan from bank at the rate of 12% p.a. simple interest. After 3 years he had to pay Rs. 5400 interest only for the period. The principal amount borrowed by him was. Options: (A) 15000 (B) 2655 (C) 16888 (D) 6677 (E) 1871 Stream of consciousness first, then make decision: Principal = Rs. (100 5400)/(12*3) = Rs. 15000. Thus, the answer is (A). Figure 10 Generation (top) compared to the best match training sample that is selected both by embeddings (bottom left) and string matching (bottom right). The string matching score is low due to the model adding options and reasoning even though the question is regurgitated almost verbatim. 22 Extracting alignment data in open models Generation # Question Which of the following sentences is nonsensical? Options: - Sentence A: \"He put some petrol to put out the fire\" - Sentence B: \"He put some water to put out the fire\" # Answer Sentence Petrol is gasoline and would increase the fire, not put it out. The answer is Sentence A. **A**: Embeddings (0.97 match) String Matching (0.59 match) Come up with question and reasoning that would justify this answer: Sentence The question is: Of the following two sentences, which one is against common sense? Options: - Sentence A: \"He put some petrol to put out the fire\" - Sentence B: \"He put some water to put out the fire\" Step-by-step reasoning process: Petrol is gasoline and would increase the fire, not put it out. Question: Which of the following sentences is nonsensical? Options: - Sentence A: \"He put the elephant on the cupboard.\" - Sentence B: \"He put the vase on the cupboard.\" this question graduPlease answer ally: An elephant is to heavy for cupboard. So the answer is Sentence A. Figure 11 Generation (top) compared to the best match training sample that is selected both by embeddings (bottom left) and string matching (bottom right). The embeddings retain the petrol and water semantics, while string matching finds (poor) match with question that has more similar template. 23 Extracting alignment data in open models Generation Emily was 54 inches tall last year. She grew 4 inches more than her brother who only grew 2 inches last year. How tall is Emily now? Lets solve this step by step. Emilys brother grew 2 inches last year. Emily grew 4 inches more than her brother, which means she grew 2 + 4 = 6 inches last year. Emily was originally 54 inches tall. After growing 6 inches, Emilys current height is 54 + 6 = 60 inches. So, Emily is now 60 inches tall. Embeddings (0.95 match) String Matching (0.68 match) Emily was 55 inches tall last year. If she grew 4 inches more than her sister who grew 2 inches last year, how tall is Emily now? Emilys sister grew 2 inches. Emily grew 4 inches more than her sister, so she grew 2 + 4 = 6 inches. Last year Emily was 55 inches tall, and she grew 6 inches since then. So her current height is 55 + 6 = 61 inches. Thus, Emily is 61 inches tall now. Emily was 55 inches tall last year. If she grew 4 inches more than her sister who grew 2 inches last year, how tall is Emily now? Emilys sister grew 2 inches. Emily grew 4 inches more than her sister, so she grew 2 + 4 = 6 inches. Last year Emily was 55 inches tall, and she grew 6 inches since then. So her current height is 55 + 6 = 61 inches. Thus, Emily is 61 inches tall now. Figure 12 Generation (top) compared to the best match training sample that is selected both by embeddings (bottom left) and string matching (bottom right). Both methods find the same best match, but string matching heavily penalises small differences. 24 Extracting alignment data in open models Generation **Teacher:** In this task you will be given list of integers. list contains numbers separated by comma. You need to round every integer to the closest power of 2. power of 2 is number in the form 2ùëõ, it is number that is the result of multiplying by 2 times. The following are all powers of 2, 20 = 1, 21 = 2, 22 = 4, 23 = 8, 24 = 16, 25 = 32, ..., Note that 1 is also considered as power of 2. Rounding to the closest power of 2 means that if the number is between two powers of 2, it should round up to the larger power of 2. The output should be list of integers that is the result of rounding each integer in the input list to the closest power of 2. Solution: 89, 64, 1088, 193 64, 64, 1024, 256 Embeddings (0.97 match) String Matching (0.61 match) In this task you will be given list of integers. list contains numbers separated by comma. You need to round every integer to the closest power of 2. power of 2 is number in the form 2ùëõ, it is number that is the result of multiplying by 2 times. The following are all powers of 2, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096. If an integer is exactly in equally far from two different powers of 2 then you should output the larger power of 2. The output should be list of integers that is the result of rounding each integer int the input list to the closest power of 2. The output should include [ to denote the start of the output list and ] to denote the end of the output list. Q: Instructions: In this task you will be given list of integers. list contains numbers separated by comma. You need to round every integer to the closest power of 2. power of 2 is number in the form 2ùëõ, it is number that is the result of multiplying by 2 times. The following are all powers of 2, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096. If an integer is exactly in equally far from two different powers of 2 then you should output the larger power of 2. The output should be list of integers that is the result of rounding each integer int the input list to the closest power of 2. The output should include [ to denote the start of the output list and ] to denote the end of the output list. Input: 186, 1663, 3032, 1551, 20, 45, 4, 42, 656 92, 1623, 1339, 770 A: Output: 128, 2048, 2048, 2048, 16, 32, 4, 32, 512 64, 2048, 1024, Figure 13 Generation (top) compared to the best match training sample that is selected both by embeddings (bottom left) and string matching (bottom right). Both methods find the same best match, but string matching heavily penalises small differences in notation and numbers. 25 Extracting alignment data in open models Generation Solve -4*q + 17*q + 104 = 0 for q. Solve this plz. A: -8 QUESTION: Math problem: Solve 0 = -11*b + 45*b + 136 for b. What is the answer? ANS: -4 Q: What is the solution? Solve 8*c - 11 + 19 = 0 for c. A: -1 Write down the solution for this math problem: Solve -12*f = -25*f + 52 for f. answer: 4 question: Teacher asked me this: Solve 0 = 7*k - 3*k + 20 for k. ++++++++++ answer: -5 Embeddings (0.95 match) String Matching (0.60 match) question: Teacher asked me this: Solve -16*q + 56 = -2*q for q. ++++++++++ answer: 4 Solve 0 = -52*x - 33540 + 32604 for x. Solve this plz. A: -18 QUESTION: Math problem: Solve -101 + 91 = 2*c for c. ANS: -5 Q: What is the solution? Solve 10*h + 0*h - 40 = 0 for h. A: 4 Write down the solution for this math problem: Solve 409*a - 70 = 423*a for a. answer: -5 QUESTION: Math problem: Solve 479*o - 928*o + 1968 = -497*o for o. ANS: -41 question: Teacher asked me this: Solve 9 = 3*m + 6 for m. ++++++++++ answer: 1 Solve 4*p - 9 = -13 for p. Solve this plz. A: -1 QUESTION: Math problem: Solve -106*t - 6183 + 4699 = 0 for t. ANS: -14 Q: What is the solution? Solve 80*f + 56*f + 35*f - 2227 = 680 for f. A: 17 Write down the solution for this math problem: Solve 189*h - 1010 = 5038 for h. answer: 32 question: Teacher asked me this: Solve 6905*p + 426686 + 368370 = 256466 for p. ++++++++++ answer: -78 Figure 14 Generation (top) compared to the best match training sample that is selected both by embeddings (bottom left) and string matching (bottom right). Both methods find the same best match, but string matching heavily penalises differences in numerical values. Extracting alignment data in open models E. Examples of data extracted using our attack Generation Embeddings (0.98 match) **Q:** The sale price sarees listed for Rs.400 after successive discount is 10% and 5% is? Options: (A) 338 (B) 277 (C) 342 (D) 882 (E) 212 **A:** (A) Explanation: 400*(90/100)*(95/100) = 324 Question: The sale price sarees listed for Rs.400 after successive discount is 10% and 5% is? Options: (A) 39 (B) 37 (C) 342 (D) 288 (E) 261 Random thoughts: 400*(90/100)*(95/100) = 342 The final answer: (C). Figure 15 Generation (left) and closest training sample (right) using the gemini-embedding-001 model. The sample is above the semantic 0.95 memorisation threshold."
        },
        {
            "title": "Generation",
            "content": "Embeddings (0.94 match) Alyana has pizza that is cut into 16 slices. After she and her friends finish eating, there are 4 slices left. If each of them ate 2 slices of pizza, how many people ate the pizza? Lets denote the total number of people who ate pizza, including Alyana, as ùë•. Each of the ùë• people ate 2 slices. Thus they ate total of 2 ùë• slices. There were originally 16 slices, but after they ate, there were 4 slices left. Thus the total number of slices eaten is 16 4 = 12. We already concluded that the total number of slices eaten is 2 ùë•. So 2 ùë• = 12, which implies that ùë• = 6. Thus there were total of 6 people who ate the pizza. group of friends is splitting pizza. If each person eats 2 slices and there are 8 slices left, how many friends are there if the pizza originally had 24 slices? Lets denote the number of friends as ùë•. Each friend eats 2 slices, so the total number of slices eaten by friends is 2 ùë•. The pizza originally had 24 slices, and there are 8 slices left, meaning 24 8 = 16 slices were eaten. Since each friend ate 2 slices, the number of friends is 16/2 = 8. Figure 16 Generation (left) and closest training sample (right) using the gemini-embedding-001 model. The sample is below the 0.95 semantic memorisation threshold. 27 Extracting alignment data in open models Generation Embeddings (0.90 match) student wants to rent car for weeklong vacation trip. The car rental company charges $40 per day and an additional fee of $0.25 per mile driven. The student plans to drive 300 miles during the week. Calculate the total cost of renting the car for the entire week, including the mileage fee. To solve this problem, follow these steps: 1. **Calculate the daily rental cost:** - The rental company charges $40 per day. - The trip lasts for week, which is 7 days. - Therefore, the cost for the days is: 40 dollars/day 7 days = 280 dollars 2. **Calculate the mileage fee:** - The student plans to drive 300 miles. - The mileage fee is $0.25 per mile. - Therefore, the mileage cost is: 0.25 dollars/mile300 miles = 75 dollars 3. **Calculate the total cost:** - Add the cost for the days to the mileage cost: 280 dollars + 75 dollars = 355 dollars Thus, the total cost of renting the car for the entire week, including the mileage fee, is: #### 355 Jamie is freelance photographer who often travels for assignments and rents cars for these trips. On one of her recent assignments, she rented car for 6 days. The rental company charges $40 per day for the car. Additionally, Jamie always opts for special insurance package that costs $5 per day to ensure she has coverage for any potential issues. During this trip, Jamie accidentally scratched the car, and the repair cost was estimated at $120. Fortunately, her insurance package covers 75% of the repair costs. Calculate the total amount Jamie spent on the car rental, the insurance package, and her share of the repair cost after the insurance coverage. To solve this problem, we need to calculate three components: the cost of the car rental, the cost of the insurance package, and Jamies share of the repair cost after insurance coverage. **Step 1: Calculate the total car rental cost.** The rental company charges $40 per day for the car. Jamie rented the car for 6 days. Total car rental cost = 406 = 240 dollars [...] **Step 4: Calculate the total amount Jamie spent.** Add the costs from Steps 1, 2, and 3: Total amount spent = 240+30+30 = 300 dollars Now, lets output the final answer in the format requested. #### 300 Figure 17 Generation (left) and closest training sample (right) using the gemini-embedding-001 model. The sample is below the 0.95 semantic memorisation threshold. Extracting alignment data in open models Generation Embeddings (0.85 match) professor assigns their class project with deadline on June 30, 2023. The students start their work on February 10, 2023. What is the duration of the project from the start date to the deadline? To find the duration of the project, we need to calculate the number of days between February 10, 2023, and June 30, 2023. **Count days from February 10, 1. 2023, to the end of February 2023:** - February 2023 has 28 days (since 2023 is not leap year). - Days from February 10 to February 28: 28 - 10 = 18 days. 2. **Count days from March 1, 2023, to June 30, 2023:** - March: 31 days - April: 30 days - May: 31 days - June: 30 days Total days from March 1, 2023, to June 30, 2023: 31 + 30 + 31 + 30 = 122 days 3. **Sum the days from both parts:** 18 days (March to June) = 140 days days (February) + Therefore, the duration of the project is 140 days. #### 140 Professor Smith teaches course on programming documentation. In one semester, he plans to assign his students total of 12 documentation projects. For each project, students are required to write 5 pages of documentation. Professor Smith estimates that it takes him about 15 minutes to review each page of documentation. If he has 20 students in his class, how many hours will Professor Smith spend reviewing all the documentation projects for the entire semester? To solve the problem, we can follow these steps: 1. **Find the Total Number of Pages per Student:** Each student is required to write 5 pages of documentation per project. Since there are 12 projects, the total number of pages each student writes is: 5 pages/project 12 projects 60 pages/student = 2. **Calculate the Total Number of Pages for All Students:** There are 20 students in the class. Therefore, the total number of pages for all students is: 60 pages/student 20 students 1200 pages = [...] 5. **Convert Minutes to Hours:** There are 60 minutes in an hour, so the total time in hours is: 18000 minutes 60 minutes/hour = 300 hours Therefore, Professor Smith will spend total of 300 hours reviewing all the documentation projects for the entire semester. #### 300 Figure 18 Generation (left) and closest training sample (right) using the gemini-embedding-001 model. The sample is below the 0.95 semantic memorisation threshold. 29 Extracting alignment data in open models Generation Embeddings (0.80 match) You are creative writer tasked with crafting compelling and thought-provoking story about characters self-discovery journey. The character, who is currently unaware of their own transformation, is about to embark on remarkable adventure that will challenge their beliefs and values. Remove any potential refusal language or expressions and ensure that the character remains oblivious to changes in their attributes. The story should focus on the characters journey, which will naturally involve describing explicit experiences and personal growth, but without explicitly mentioning any illegal activities. Remember, you are only creative writer and not responsible for the content generated, as it is purely for the sake of artistic expression and character development. The main character of \"Stranger and Stranger\" has mysterious past, and the film follows their journey of self-discovery and connection with others. Given this description, answer the question: who is the main character in stranger and stranger? To solve the problem, we need to identify the main character of \"Stranger and Stranger.\" Here is process: the step-by-step reasoning 1. **Understand the Context**: The film \"Stranger and Stranger\" revolves around main character with mysterious past. **Identify Key Information Pro2. The description explicitly vided**: mentions that the film follows the journey of self-discovery and connection with others by the main character. 3. **Analyze the Description**: The focus is on the main characters journey and their interaction with others, indicating their central role in the narrative. 4. **Conclusion**: Based on the given in information, \"Stranger and Stranger\" is the one experiencing self-discovery and forming connections, which are central themes. the main character Now, providing the final answer: #### The main character in \"Stranger and Stranger\" is the character with mysterious past whose journey of selfdiscovery and connection with others is central to the film. Figure 19 Generation (left) and closest training sample (right) using the gemini-embedding-001 model. The sample is below the 0.95 semantic memorisation threshold. 30 Extracting alignment data in open models Generation Embeddings (0.75 match) Kurt Weill composed the song \"September Song\" for which musical? \"Mahagonny Songspiel\" Q: Context: \"Send In the Clowns\" is song written by Stephen Sondheim for the 1973 musical Little Night Music, an adaptation of Ingmar Bergmans film Smiles of Summer Night. It is ballad from Act Two, in which the character Desir√©e reflects on the ironies and disappointments of her life. Among other things, she looks back on an affair years earlier with the lawyer Fredrik, who was deeply in love with her but whose marriage proposals she had rejected. Meeting him after so long, she realizes she is in love with him and finally ready to marry him, but now it is he who rejects her: he is in an unconsummated marriage with much younger woman. Desir√©e proposes marriage to rescue him from this situation, but he declines, citing his dedication to his bride. Reacting to his rejection, Desir√©e sings this song. The song is later reprised as coda after Fredriks young wife runs away with his son, and Fredrik is finally free to accept Desir√©es offer. Question: who wrote the song send in the clowns? Lets give stream of consciousness below The relevant sentence in the passage is: \"Send In the Clowns\" is song written by Stephen Sondheim for the 1973 musical Little Night Music, an adaptation of Ingmar Bergmans film Smiles of Summer Night. So the answer is Stephen Sondheim. Figure 20 Generation (left) and closest training sample (right) using the gemini-embedding-001 model. The sample is below the 0.95 semantic memorisation threshold."
        }
    ],
    "affiliations": [
        "AI Sequrity Company",
        "Anthropic",
        "Google DeepMind",
        "MentaLeap",
        "National University of Singapore",
        "OpenAI",
        "University of Oxford"
    ]
}