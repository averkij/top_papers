{
    "paper_title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation",
    "authors": [
        "Xingtong Ge",
        "Xin Zhang",
        "Tongda Xu",
        "Yi Zhang",
        "Xinjie Zhang",
        "Yan Wang",
        "Jun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed \\textbf{SenseFlow}, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 3 2 5 0 0 . 6 0 5 2 : r SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation Xingtong Ge1,2, Xin Zhang2, Tongda Xu3, Yi Zhang4, Xinjie Zhang1, Yan Wang3, Jun Zhang1 1The Hong Kong University of Science and Technology, 2SenseTime Research 3Institute for AI Industry Research, Tsinghua University, 4The Chinese University of Hong Kong xingtong.ge@gmail.com, eejzhang@ust.hk Figure 1: 10241024 samples produced by our 4-step generator distilled from FLUX.1-dev."
        },
        {
            "title": "Abstract",
            "content": "The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intrasegment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed SenseFlow, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow. Corresponding Author Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Significant advancements have been made on diffusion models [1, 2, 3, 4, 5] for text-to-image generation over recent years. However, these models typically require multiple denoising steps to generate high-quality images. As models continue to scale up in terms of the parameter size, the computational cost and inference time for image generation increase substantially, making the process slower and more resource-intensive. To address this issue, various diffusion distillation methods have been developed to distill diffusion model into few-step generator, including consistency models [6, 7, 8], progressive distillation [9, 10], adversarial distillation [11, 12, 13], and score distillation [14, 15, 16]. Currently, the Distribution Matching Distillation (DMD) series [15] have demonstrated superior results in distilling standard diffusion models such as SD 1.5 [2] and SDXL [3]. However, few of these methods have successfully demonstrated effective distillation performance across broader range of models, particularly in flow-based diffusion models with larger parameter sizes, such as SD3.5 Large (8B) [4] and FLUX.1 dev (12B) [5]. As models increase in architecture complexity, parameter size, and training complexity, it becomes significantly more challenging to distill these models into efficient few-step generators (e.g., 4-step generator). In this paper, we introduce SenseFlow, which selects the framework of DMD2 [15] as touchstone, and scales it up for larger flow-based text-to-image models, including SD3.5 Large and FLUX.1 dev. Specifically, vanilla DMD2 has difficulty in converging and faces significant training instability on large models, even with the time-consuming two time-scale update rule (TTUR) [17] applied. To address this challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and the fake distribution network, which makes the training of fake distribution network faster and easier. This further allows us to make the generator converge more stably. Further, DMD2 and most existing diffusion distillation methods still use uniformly sampled timesteps for training and inference. However, due to the complex strategies employed during training of teacher diffusion models, different timesteps exert varying denoising effects throughout the entire process, which is also discussed in RayFlow [18]. To avoid the inefficiency of naive timestep sampling strategy in distillation, we propose to relocate the teachers timestep-wise denoising importance into small set of selected coarse timesteps. For each coarse timestep τi, we construct an intra-segment guidance (ISG) by sampling an intermediate timestep tmid (τi1, τi), and construct guidance trajectory: the teacher denoises from τi to tmid, then the generator continues from tmid to τi1. We then guide the generator to align its direct prediction from τi to τi1 with this trajectory. This guidance mechanism effectively aggregates the teachers fine-grained behavior within each segment, improving the generators ability to approximate complex transitions across fixed sparse timesteps. For further enhancement, we incorporate more general and powerful discriminator built upon vision foundation models (e.g., DINOv2 [19], CLIP [20]), which operates in the image domain and can provide stronger semantic guidance. The use of pretrained vision backbones introduces rich semantic priors, enabling the discriminator to better capture image-level quality and fine-grained structures. By aggregating timestep-aware adversarial signals, this design yields stable and efficient training with superior visual qualities. To summarize, we dive into the distribution matching distillation (DMD) and scale it up for wide range of large-size flow-based text-to-image models. Our contributions are as follows: We discover that vanilla DMD2 suffers from the convergence issue on large-scale text-toimage models, even with TTUR introduced. To tackle this challenge, we propose implicit distribution alignment to regularize the distance between the generator and fake distribution. To mitigate the problem of suboptimal sampling in DMD2, we propose intra-segment guidance to relocate the teachers timestep-wise denoising importance, improving the generators ability to approximate complex transitions across sparse timesteps. By incorporating more powerful discriminator built upon vision foundation models with timestep-aware adversarial signals, we achieve stable training with superior performance. Experimental results show that our final model, dubbed SenseFlow, achieves superior performance in distilling large-scale flow-matching models ( e.g., SD 3.5, FLUX.1 dev) and diffusion-based models (e.g., SDXL). Our SD 3.5 Based-SenseFlow achieves state-of-the-art 4-step generation performance among all open-source models evaluated in our study."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Diffusion Models Diffusion models are family of generative models, with the forward process perturbing the data X0 p(X0) to Gaussian noise p(XT ) = (0, I) with series distributions p(Xt) defined by forward stochastic differential equation (SDE): dXt = (Xt, t)dt + g(t)dBt, [0, ] (1) where (Xt, t) is drifting parameter, g(t) is diffusion parameter and Bt is standard Brownian motion. The diffusion model learns the score function s(Xt, t) = Xt log p(Xt) using neural network. And the sampling of diffusion process is to solve the probability flow ordinary differential equation: dXt = (f (Xt, t) 1 2 g(t)2s(Xt, t))dt, XT (0, I). (2) The two widely adopted diffusion models in text-to-image, namely denoising diffusion probabilistic model (DDPM) and flow matching optimal transport (FM-OT), fit in above framework by setting (Xt, t) = 1 1t respectively, where βt is hyper-parameter of DDPM. The forward SDE of DDPM and FM-OT can be directly solved: βt and (Xt, t) = 1 2 βtXt, g(t) = 2 g(t)2 = 1t Xt, 1 DDPM: q(XtX0) = (e 1 (cid:82) 0 βsdsX0, (1 1 FM-OT: q(XtX0) = (tX0, (1 t)2I). However, the backward equation in Eq. 2 is intractable as s(Xt, t) is neural network. Usually we need time-consuming multi-step solvers. In this paper, we focus on distilling the solution of backward equations into another neural network. (cid:82) 0 βsds)I), (3) (4) 2 2 2.2 Distribution Matching Distillation From now on we assume pre-trained diffusion model is available, with learned score function sr(Xt, t) and distribution pr(Xt). The Distribution Matching Distillation (DMD) [14, 15] distills the diffusion model by technique named score distillation [21]. More specifically, DMD learns the generator distribution pg(Xt) to match the diffusion distribution pr(Xt): DKL(pg(Xt)pr(Xt)) = Et[0,T ],pg [log pg(Xt) log pr(Xt)]. (5) min pg Directly distillation from above target produces suboptimal results. Therefore, DMD introduces an intermediate fake distribution pf (Xt, t), and optimizes the generator distribution pg and fake distribution pf in an interleaved way: Generator: min pg Et[0,T ],pg [log pf (Xt) log pr(Xt)], Fake: max pf Et[0,T ],pg [log pf (Xt)]. (6) In practice, the fake distribution is parameterized as the score function sϕ(Xt, t) = log pf (Xt). On the other hand, the generator is parameterized with clean image generating network Gθ(ϵ), ϵ (0, I) and forward diffusion process q(XtX0), such that pg(Xt) = EϵN (0,I)[q(XtGθ(ϵ))]. To this end, the DMD updates are achieved by gradient descent and score matching [22]: Generator: θLg = Et[0,T ],ϵN (0,I),Xtq(XtGθ(ϵ))[(sϕ(Xt, t) sr(Xt, t)) ], Fake:ϕLf = ϕEt[0,T ],ϵN (0,I),Xtq(XtGθ(ϵ))[sϕ(Xt, t) Xt log q(XtGθ(ϵ))]. Xt θ (7)"
        },
        {
            "title": "3 Method: Scaling Distribution Matching for General Distillation",
            "content": "3.1 Bottlenecks in Vanilla DMD series: Stability, sampling, and naive discriminator While Distribution Matching Distillation (DMD) has shown promising results in aligning generative distributions, its vanilla formulation exhibits several fundamental limitations when applied to largescale models. First, scalability remains challengethe two time-scale update rule (TTUR), effective 3 Figure 2: Left: The generator receives text prompt and xτi to produce one-step output xg, which is diffused to xt and processed by sϕ and sr for computing DMD gradient. ISG guides using an intermediate point xtmid , and IDA aligns with sϕ after generator update. Right: The discriminator extracts semantic features from generated and real images using CLIP and DINOv2, which are processed by head blocks hθi to predict real/fake logits for adversarial training. Trainable modules are shown in pink, while frozen (pretrained) ones are shown in grey. in SD 1.5 (0.8B) and SDXL (2.6B), fails to converge stably when scaled to larger models such as SD 3.5 Large (8B) or FLUX (12B). Second, sampling efficiency is limited as the generator does not incorporate the varying importance of timesteps in the denoising trajectory, which slows convergence and reduces expressiveness. Third, the discriminator lacks generality, with relatively naive design that struggles to adapt across diverse model scales and architectures. These issues motivate us to propose architectural and algorithmic improvements in this work. 3. Implicit Distribution Alignment via Generator-Fake Distribution Fusion In Distribution Matching Distillation (DMD), critical challenge lies in stabilizing the fake distribution model to accurately track the generator distribution pg, especially when working with modern large-scale diffusion backbones such as SD3.5 or FLUX. As model capacity increases and the training strategies of teacher models vary across architectures, ensuring well-trained fake distribution model becomes increasingly difficult. For example, many models [23, 24, 25] use complex post-training strategies to improve the performance of the model in specific directions, such as text rendering or aesthetic quality, which may introduce non-uniform sampling trajectories, making the standard diffusion loss less effective for supervising fake distribution model training. To address this issue, DMD2 used the two time-scale update rule (TTUR), which increases the update frequency of the fake distribution model relative to the generator. However, TTUR becomes increasingly expensive and brittle as the model size scales up. Results in Fig. 3 also indicate that sometimes even high ratio of 20:1 still cannot stabilize the training. On the other hand, although the generator and fake distribution network are optimized via different objectives, their long-term goals are highly aligned: both aim to model distribution pg(Xt) that closely approximates the real data distribution pr(Xt). In practice, they are initialized from the same pretrained teacher and both define the generator-induced distribution pg(Xt). The key difference is that, generator is guided by an explicit, fixed teacher score sr(Xt, t) through the variational gradient in Eq. 7 and thus evolves in clear direction toward pr. In contrast, the fake distribution network is trained to regress toward the score of the generator-induced distribution via Lf in Eq. 7, where the target log pg(Xt) is approximated through the generators outputs. In early training, this target is rapidly moving and highly unreliable signalmaking the fake distribution network prone to underfitting, drift, or misaligned gradients, especially when the model size is relatively large. Figure 3: Training Hours-FID curves on COCO-5K dataset. IDA improves training stability across TTUR ratios. We address this challenge by introducing Implicit Distribution Alignment (IDA), simple yet effective stabilization mechanism. Specifically, after each generator update, we partially align the fake 4 Figure 4: Left: The normalized reconstruction errors over timesteps in [0, 1]. Right: An illustration of the Intra-Segment Guidance. distribution parameters toward the generator: ϕ λ ϕ + (1 λ) θ. (8) Intuitively, this allows us to propagate the teachers stable supervisionreceived by the generatorinto the fake distribution model indirectly. Since both networks share initialization and long-term alignment, IDA can implicitly regularize the distributional distance between the fake distribution model and generator, preventing it from being misled by the drift moving targets during early training. In practice, this strategy ensures that the fake distribution remains closely aligned with the generators distributional trajectory, especially early in training when score updates are unstable. We observe that combining IDA with even relatively small TTUR (e.g., 5:1) leads to significantly more stable convergence. An example of this effect is shown in Fig. 3, where we compare FID curves under different TTUR settings with and without IDA. As the figure illustrates, IDA consistently reduces FID variance and improves overall performance. We leave detailed analysis to the ablation study section. 3.3 Generator Turn: Relocate the Timestep Importance Distribution On the other hand, the distillation performance of vanilla DMD2 is fundamentally limited by fixed timestep supervision. In vanilla DMD2 setups, the generator is only trained at small set of predefined timesteps (e.g., τ {249, 499, 749, 999}). However, this fixed design introduces two major issues: first, the generator receives no training signal from the rest of the trajectory, which leads to poor generalization for the full trajectory; second, the effectiveness of each supervised timestep is highly sensitive to where it lies along the trajectoryneighboring timesteps can exhibit drastically different predictive errors. To better illustrate the local reliability of different timesteps in the diffusion trajectory, we visualize the normalized one-step reconstruction loss ξ(t) over 1000 uniformly spaced timesteps in [0, 1]: (cid:2)ˆx0(xt, t) x02(cid:3) , ξ(t) := Ex0,ϵN (0,I) (9) where x0 is generated by the teacher model (SD 3.5 or FLUX.1 dev) and xt is obtained via diffusion forward process in Eq. 4 using x0 and ϵ. The results are shown in Fig. 4 Left. We observe that as increases, the denoising error ξ(t) does not grow monotonically, but instead exhibits noticeable local oscillationsparticularly in the interval [0.8, 1.0]. This suggests that even adjacent timesteps within the same region may differ significantly in their denoising accuracy, implying that their relative importance to the overall denoising process is not uniform. Consequently, selecting supervision points without considering their local reliability may inadvertently anchor the generator to suboptimal points, degrading sample quality and training stability. To mitigate this issue, we propose to relocate the teachers denoising importance into small set of selected coarse timesteps. For each coarse timestep τi, we construct an intra-segment guidance by randomly sampling an intermediate timestep t1 (τi1, τi). As shown in Fig. 4 Right, the teacher model generates xt1 by denoising from τi to t1. Then, the generator continues the denoising process from t1 to τi1, yielding the guidance target xtar. Meanwhile, the generator also produces xτi1 directly from τi to τi1. We then apply an L2 loss between xg and xtar, where gradients are only propagated through the generator path: L(i) ISG = Eϵ,t1 (cid:104) xg stop_grad(xtar)2 2 (cid:105) . (10) This enables each anchor point to better absorb the denoising knowledge of its surrounding segment, thereby serving as more representative proxy for its local denoising behavior. 5 3.4 Bonus: General and Powerful Discriminator built upon Vision Foundation Models As shown in Fig. 2, the discriminator is designed by integrating fixed pre-trained Vision Foundation Model (VFM) backbone, fVFM, with learnable discriminator heads, h. Given an input image x, the VFM backbone extracts multi-level semantic features = fVFM(x), which are subsequently processed by the discriminator heads to predict the realism of x. Additionally, the discriminator incorporates CLIP-encoded features = fCLIP(text) and reference features = fVFM(x) from real images to additionally impregnate text-image alignment information. This process is expressed as: D(x) = h(fVFM(x), c, r). (11) These features enhance the discriminators capacity to evaluate both the realism and semantic consistency of the input images. The discriminator is trained using the hinge loss, defined as: Ld = EXpdata [max(0, 1 D(X))] + ˆX0pg (cid:104) max(0, 1 + D( ˆX0)) (cid:105) , (12) where pdata denotes the empirical distribution of real images from the training dataset, and pg represents the generators learned distribution, consistent with the notation introduced in Section 2. This loss encourages the discriminator to assign high scores to real images and low scores to generated images, stabilizing the adversarial training process. Adversarial Training Objective. The adversarial loss is designed to encourage the generator to produce images that can maximize the discriminators output. Meanwhile, when the generator is trained with samples from larger timesteps, the predicted x0 tends to be less accurate compared with predictions from smaller timesteps. To stabilize training and prevent the adversarial loss from dominating during these less reliable steps, we introduce weighting mechanism. Specifically, we compute scalar weighting adversarial signal as the square of the current timesteps noise scale, i.e., ω(t) = σ2 , and scale the adversarial loss. Thus, the adversarial loss for the generator is: Lg = ω(t) ˆX0pg (cid:105) (cid:104) D( ˆX0) = σ ˆX0pg (cid:104) (cid:105) D( ˆX) . (13) This design ensures that the generator focuses more on the DMD gradient during noisy, high-timestep stageswhere adversarial feedback may be unreliableand benefits more from GAN guidance at cleaner, low-noise steps. In practice, this improves training stability and overall sample quality."
        },
        {
            "title": "4 Experimental Results",
            "content": "4.1 Experimental Setup Datasets. Following DMD2 [15], our experiments are conducted using filtered set of the LAION5B [26] dataset, which provides high-quality image-text pairs for training. We select images with minimum aesthetic score (aes score) of 5.0 and shorter dimension of at least 1024 pixels, ensuring the dataset comprises visually appealing, high-resolution images suitable for our models requirements. For evaluation, we construct validation set using the COCO 2017 [27] validation set, which contains 5,000 images. Each image in this set is paired with the text annotation that yields the highest CLIP Score (ViT-B/32), thus forming robust text-image validation set. We also evaluate compositional generation using T2I-CompBench [28], benchmark spanning attribute binding, object relationships, and complex compositions, which is designed to test models on generating semantically coherent images with diverse object interactions. Text-to-Image Models. We conduct extensive experiments on three representative large-scale textto-image models: FLUX.1 dev (12B) [5], Stable Diffusion 3.5 Large (8B) [4], and SDXL (2.6B) [3], which span different model sizes and generative paradigms. Results demonstrate the generality and effectiveness of our method across both flow-based and conventional diffusion architectures. Evaluation Metrics. Following [8, 29, 15], we report FID and Patch FID of all baselines and the generated images of original teacher models to assess distillation performance and high-resolution details, dubbed FID-T and Patch FID-T. We also report CLIP Score (ViT-B/32) to evulate text-image alignment and further include some recently proposed metrics, such as HPS v2 [30], ImageReward [31], and PickScore [32] to offer more comprehensive evaluation of the model performance. Table 1: Quantitative Results on COCO-5K Dataset. Bold: best. Underline: second best. Our proposed approaches superior distillation performance accross different models on 4-step generation. Method # NFE FID-T Patch FID-T CLIP HPSv2 Pick ImageReward Stable Diffusion XL Comparison SDXL [3] LCM-SDXL [7] PCM-SDXL [8] Flash-SDXL [13] SDXL-Lightning [29] Hyper-SDXL [10] DMD2-SDXL [15] Ours-SDXL 80 4 4 4 4 4 4 18.47 14.38 17.97 13.67 13.71 15.04 17.76 30.63 17.77 23.24 16.57 17.49 18.72 21.01 0.3293 0.3230 0.3242 0.3216 0.3214 0.3254 0.3277 0.3248 0.2930 0.2824 0.2920 0.2830 0.2931 0.3000 0.2963 0.3010 22.67 22.22 22.54 22.17 22.80 22.98 22.98 23.17 Stable Diffusion 3.5 Comparison SD 3.5 Large [4] SD 3.5 Large Turbo [12] Ours-SD 3.5 Ours-SD 3.5 (Euler) 100 4 4 4 13.58 13.38 15.24 22.88 17.48 20.26 0.3310 0.3262 0.3286 0.3287 0.2993 0.2909 0.3016 0. 22.98 22.89 23.01 22.90 FLUX Comparison FLUX.1 dev [5] FLUX.1-schnell [5] Hyper-FLUX [10] FLUX-Turbo-Alpha [33] Ours-FLUX Ours-FLUX (Euler) 50 25 4 4 4 4 4 11.24 11.22 15.64 16. 23.47 24.52 19.60 20.29 0.3202 0.3207 0.3264 0.3238 0.3218 0.3167 0.3171 0.3000 0.2986 0.2962 0.2963 0.2907 0.2997 0.3008 23.18 23.14 22.77 23.09 22.89 23.13 23.26 0.8719 0.5693 0.6926 0.4295 0.7799 0.9777 0.9324 0.9951 1.1629 1.0116 1.1713 1. 1.1170 1.1063 1.0755 1.0983 1.0106 1.0921 1.1424 Table 2: 4-Step Results on T2I-CompBench. Bold, Underline: best and second best in distilling the same teacher. Our distilled SD 3.5 model approaches state-of-the-art distillation performance. Method Color Shape Texture Spatial Non-spatial Complex-3-in-1 LCM-SDXL [7] SDXL-Lightning [29] Hyper-SDXL [10] PCM-SDXL [8] DMD2-SDXL [15] Ours-SDXL SD 3.5 Large Turbo [12] Ours-SD 3.5 Ours-SD 3.5 (Euler) FLUX.1 schnell [5] Hyper-FLUX [10] FLUX-Turbo-Alpha[33] Ours-FLUX Ours-FLUX (Euler) 0.5997 0.5758 0.6435 0.5591 0.5811 0.6867 0.7050 0.7657 0.7711 0.7317 0.7465 0.7406 0.7284 0.7363 0.4015 0.4492 0.4732 0.4142 0.4477 0.4828 0.5443 0.6069 0.6149 0.5649 0.5023 0.4873 0.5055 0.5120 0.4958 0.5154 0.5581 0.4693 0.5175 0.5989 0.6512 0.7427 0.7543 0.6919 0.6153 0.6024 0.6031 0.6112 0.1672 0.2124 0.2213 0.2013 0.2124 0.2224 0.2839 0.2970 0.2857 0.2626 0.2945 0.2501 0.2451 0. 0.3010 0.3098 0.3104 0.3099 0.3098 0.3100 0.3130 0.3177 0.3182 0.3122 0.3116 0.3094 0.3028 0.3028 0.3364 0.3517 0.3301 0.3234 0.3301 0.3594 0.3520 0.3916 0.3968 0.3669 0.3766 0.3688 0.3652 0.3697 4.2 Text to Image Generation Comparison Baselines. For the distillation of SDXL, we compare our method with baselines including LCM [7], PCM [8], Flash Diffusion [13], SDXL-Lightning [29], Hyper-SD [10], and DMD2 [15]. As for SD 3.5 Large, we compare our method with SD 3.5 Large Turbo [12]. For FLUX.1 dev, we compare with Hyper-FLUX [10], FLUX.1 schnell [5], and FLUX-Turbo-Alpha [33]. Quantitative Comparison. The 4-step comparison results on COCO-5K and T2I-CompBench are presented in Tab. 1 and Tab. 2, respectively. For flow-matching models, we report both stochastic and deterministic sampling results, denoted as Ours and Ours (Euler). As shown in Tab. 1, our method consistently outperforms previous distillation baselines across wide range of metrics. On SD 3.5, both Ours-SD 3.5 and Ours-SD 3.5 (Euler) achieve the best and second-best scores on all metrics, even surpassing the teacher model in HPSv2, PickScore, and ImageReward. On SDXL, Figure 5: Qualitative comparisons on challenging prompts across methods. Our method shows superior fidelity, especially in rendering human faces, scene composition, and fine-grained textures. our method ranks first in HPSv2, PickScore, and ImageReward, with marginal drop in text-image alignment. For FLUX.1 dev, our models again deliver top performance across several metrics. The strong results under both stochastic and deterministic settings also confirm the robustness of our approach. In terms of T2I-CompBench, the results in Tab. 2 demonstrate that Ours-SD 3.5 (Euler) achieves state-of-the-art performance across all evaluated methods in five dimensionscolor, shape, texture, spatial, non-spatial consistency, and the Complex-3-in-1 metric. These results highlight the fine-grained fidelity and superior attribute alignment of our approach. Ours-SDXL also achieves the best performance in five out of the six evaluated metrics for SDXL distillation, the highest among compared methods. Further results and detailed analyses are provided in the appendix. Qualitative Comparison. Fig 5 presents qualitative comparisons across set of prompts. Our method generates images with sharper details, better limb structure, and more coherent lighting dynamics, compared to teacher models and baselines. Notably, Ours-SD3.5\" and Ours-FLUX\" produce more faithful and photorealistic generations under challenging prompts involving fine textures, human faces, and scene lighting. Fig. 7 also presents examples of our method on SD 3.5 Large. Additional qualitative results and discussion are provided in the appendix. 4.3 Ablation Studies Effectiveness of Implicit Distribution Alignment. To assess the effectiveness of our proposed IDA strategy, we conduct experiments on SD 3.5 Large with various TTUR ratios. As shown in Fig. 3, we compare FID curves across different settings, both with and without IDA. Without IDA, the curves corresponding to TTUR(5), TTUR(10), and TTUR(20) exhibit severe oscillations, indicating unstable training dynamics and unreliable optimization of the fake distributioneven at high ratio of 20:1. This instability leads to inaccurate DMD gradients and poor convergence. In contrast, the settings that incorporate IDA (i.e., IDA+TTUR(5) and IDA+TTUR(10)) demonstrate Table 3: Ablation Study Results of IDA, ISG, and VFM Discriminator. Method FID-T CLIP HPSv2 AESv2 Stable Diffusion 3.5 Large Ours wo ISG wo ISG, wo IDA 13.58 0.3288 0.2989 5.4559 17.00 0.3246 0.2971 5.4527 43.04 0.3000 0.2555 5.1018 Stable Diffusion XL DMD2-SDXL [15] 15.04 0.3277 0.2964 5.5305 18.55 0.3234 0.2998 5.6252 DMD2 VFM Figure 6: The ISG improves training consistency, especially in the early stage of training. significantly smoother and more stable FID reductions, highlighting IDAs ability to stabilize training and improve convergence, even at relatively small TTUR ratio (5:1). In addition to the FID analysis, we report quantitative comparisons in Tab. 3 between w/o ISG and w/o ISG, w/o IDA using four metrics: FID-T, CLIP Score, HPSv2, and AESv2. Across all metrics, adding IDA leads to consistent improvements, further confirming that IDA plays key role in enhancing training stability and distillation quality. Intra-Segment Guidance. To evaluate the effectiveness of the Intra-Segment Guidance (ISG) module during distillation, we conduct an ablation study on Stable Diffusion 3.5 Large. As shown in Tab. 3, we compare our model with and without ISG (denoted as Ours and w/o ISG, respectively) on the COCO-5K dataset. The results indicate that incorporating ISG leads to significant improvements across all aspects, including image quality, text-image alignment, and human preference quality. In addition, Fig. 6 presents qualitative comparison at 3K training iterations, during which the generators have been updated for only 300 steps under 10:1 TTUR ratio. We observe that the model trained with ISG produces visually more consistent and semantically accurate images even at early training stages, whereas the model without ISG suffers from noticeable color shifts and degraded image fidelity. This highlights ISGs contribution to training stability and convergence efficiency. VFM-Based Discriminator. To assess the benefit of integrating Vision Foundation Model (VFM)- based discriminator, we conduct comparative experiments on the SDXL backbone. As shown in Tab. 3, we compare the DMD2-SDXLequipped with diffusion-based discriminatorwith our method using the VFM discriminator (denoted as DMD2 VFM). Across multiple evaluation metrics, DMD2 VFM achieves better human preference alignment and aesthetic quality. These results demonstrate that the VFM-based discriminator provides stronger visual priors to the generator."
        },
        {
            "title": "5 Related Work",
            "content": "Diffusion Distillation methods mainly fall into two categories: trajectory-based and distributionbased approaches. Trajectory-based methods, such as Direct Distillation [34] and Progressive Distillation [9, 10, 29, 13], learn to replicate the denoising trajectory, while Consistency Models [6, 7, 35, 8, 36, 37] enforce self-consistency across steps. Distribution-based methods aim to match the generative distribution, including GAN-based distillation [11, 38, 39] and VSD variants [40, 14, 15]. ADD [12] and LADD [41] explored distilling diffusion models using adversarial training with pretrained feature extractors. RayFlow [18] explored sampling important timesteps for better distillation. Among these, DMD2 [15] has shown strong results on standard diffusion models (e.g., SDXL), but its stability degrades on large-scale models. Our work builds upon DMD2 and addresses these limitations by introducing SenseFlow, which scales distribution matching distillation to SD 3.5 and FLUX.1 dev through improved alignment and regularization strategies."
        },
        {
            "title": "6 Conclusions and Limitations",
            "content": "We scale up distribution matching distillation for large flow-based models by introducing implicit distribution alignment and intra-segment guidance. Together with VFM-based discriminator, these enhancements enable our model SenseFlow to achieve stable and effective few-step generation 9 Figure 7: 10241024 samples produced by our 4-step generator distilled from SD 3.5 Large. on both diffusion and flow-matching backbones. Our SD 3.5-based SenseFlow achieves state-ofthe-art 4-step generation performance across all evaluated distillation methods, demonstrating its effectiveness on large-scale models. Meanwhile, its performance under more aggressive settings (e.g., 2-step, 1-step) and with alternative vision backbones [19, 42, 43, 44] remains unexplored. Finally, like other generative models, SenseFlow raises concerns regarding potential misuse and labor displacement, underscoring the importance of responsible deployment."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [2] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [3] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In ICLR. OpenReview.net, 2024. [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [5] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [6] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pages 3221132252. PMLR, 2023. [7] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [8] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. [9] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [10] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. [11] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [12] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87103. Springer, 2024. [13] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1568615695, 2025. [14] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [15] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Improved distribution matching distillation for fast image synthesis. Advances in neural Freeman. information processing systems, 37:4745547487, 2024. [16] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. In NeurIPS, 2023. [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NIPS, pages 66266637, 2017. [18] Huiyang Shao, Xin Xia, Yuhong Yang, Yuxi Ren, Xing Wang, and Xuefeng Xiao. Rayflow: Instance-aware diffusion acceleration via adaptive flow trajectories. arXiv preprint arXiv:2503.07699, 2025. [19] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [21] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 11 [22] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. [23] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, pages 82288238. IEEE, 2024. [24] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. CoRR, abs/2409.10695, 2024. [25] Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Linjie Yang, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, and Weilin Huang. Seedream 2.0: native chinese-english bilingual image generation foundation model. CoRR, abs/2503.07703, 2025. [26] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740755. Springer, 2014. [28] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [29] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. [30] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [31] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2023. [32] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeurIPS, 2023. [33] Alimama-Creative Team. Flux.1-turbo-alpha. https://huggingface.co/alimama-creative/FLUX. 1-Turbo-Alpha, 2024. Accessed: 2025-05-15. [34] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. [35] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In ICLR. OpenReview.net, 2024. [36] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. [37] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Enze Xie, and Song Han. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. [38] Yutong Wang, Jiajie Teng, Jiajiong Cao, Yuming Li, Chenguang Ma, Hongteng Xu, and Dixin Luo. Efficient video face enhancement with enhanced spatial-temporal consistency. arXiv preprint arXiv:2411.16468, 2024. [39] Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, and Jing Tang. You only sample once: Taming one-step text-to-image synthesis by self-cooperative diffusion gans. arXiv preprint arXiv:2403.12931, 2024. [40] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [41] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 12 [42] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloé Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross B. Girshick, Piotr Dollár, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In ICLR. OpenReview.net, 2025. [43] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. AM-RADIO: agglomerative vision foundation model reduce all domains into one. In CVPR, pages 1249012500. IEEE, 2024. [44] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1597915988. IEEE, 2022. [45] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Broader Impact Our work focuses on improving the efficiency and quality of text-to-image diffusion models, particularly on large-scale architectures. This has several potential societal impacts, both positive and negative. On the positive side, the proposed distillation framework significantly accelerates the sampling process of large models such as FLUX.1 dev and SD 3.5 Large, making high-quality image synthesis more accessible and practical for real-world applications. These improvements can benefit wide range of domains, including education, digital content creation, scientific visualization, and assistive design tools, by enabling faster, more cost-efficient generation of customized visual content. However, similar to other text-to-image models, our method inherits risks associated with generative models. These include the potential misuse of fast image synthesis for generating fake content, spreading misinformation, or fabricating identities. Additionally, like many generative models, our distilled networks are susceptible to reflecting biases present in the training data, which may result in unfair or unrepresentative outputs. As future direction, we are interested in investigating methods for detecting and mitigating such biases in diffusion models, building on recent work in fairness-aware generation. We also plan to introduce clear usage guidelines and responsible deployment practices, including detailed user manuals, to promote ethical and transparent use of the technology. A.2 Implementation Details Our entire framework is implemented in PyTorch with CUDA acceleration and is trained using 8 A100 GPUs with total batch size of 8. We adopt the AdamW optimizer [45] with hyperparameters β1 = 0.9 and β2 = 0.999. The learning rate is set to 1e 6 for the distillation of SDXL and SD 3.5 Large, and 1e 5 for FLUX.1 dev. To efficiently support large-scale model training, we utilize Fully Sharded Data Parallel (FSDP), which enables memory-efficient and scalable training of large models. Timestep settings. We adopt different coarse timestep schedules depending on the model architecture. For SDXL, we follow the 1000-step discrete DDPM schedule used in DMD2 [15], selecting step indices {249, 499, 749, 999}. For SD 3.5 Large, we switch to continuous timestep values {0.25, 0.5, 0.75, 1.0} , which are more suitable for flow-based models. In the case of FLUX.1 dev, which adopts shifted σ inference strategy, we directly use the corresponding sigmas {0.512, 0.759, 0.904, 1.0} as coarse anchors. Training details. We set the default TTUR (Two Time-Scale Update Rule) ratio to 5 in our main experiments on SDXL, SD 3.5 Large, and FLUX.1 dev. For large flow-based models such as SD 3.5 Large and FLUX.1 dev, we apply all proposed improvements, including Implicit Distribution Alignment (IDA), Intra-Segment Guidance (ISG), and the VFM-based Discriminator. For the diffusion-based SDXL model, we employ ISG and the VFM-based Discriminator while omitting IDA. Figure 8: Design of the VFM-based discriminator. A.3 Detailed VFM-Based Discriminator Design As shown in Fig. 8, the discriminator integrates pretrained vision (DINOv2) and language (CLIP) encoders to provide semantically rich and spatially aligned supervision. Given an input image x, we apply normalization (from [1, 1] to [0, 1]) and differentiable data augmentation (including color jitter, translation, and cutout). The augmented image is processed by frozen DINOv2 vision transformer to extract multi-level semantic features. Each selected layer output is reshaped into 2D Algorithm 1 SenseFlow Training Algorithm Require: pretrained teacher model µreal, real dataset Dreal, generator update frequency , coarse (0, I) Sample τi from Sample xreal Dreal if random() < 0.5 then timestep set = {τ0, τ1, τ2, τ3} Ensure: trained few-step generator 1: copyWeights(µreal) 2: µfake copyWeights(µreal) 3: initializeDiscriminator() 4: for iteration = 1 to max_iters do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end if G(xτi) if iteration mod = 0 then else xτi multiStepSampling(z, τ3 τi)) xτi forwardDiffusion(xreal, τi) LDMD distributionMatching(µreal, µfake, x) LG σ2 τi E[D(x)] tmid U(τi, τi1) xmid µreal(xτi, τi tmid) xtar G(xmid, tmid τi1) xτi1 G(xτi, τi τi1) LISG MSE(xτi1, stopgrad(xtar)) LG LDMD + λG LG + λISG LISG update(G, LG) µf ake IDA(G, µf ake, λIDA) end if LogitNormalSampling(0, 1) xt forwardDiffusion(stopgrad(x), t) Ldenoise denoisingLoss(µfake(xt, t), stopgrad(x)) µfake update(µfake, Ldenoise) 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: LD E[max(0, 1 D(xreal)] + E[max(0, 1 + D(x)] update(D, LD) 30: 31: 32: end for Initialize generator Initialize fake distribution network Initialize VFM-based discriminator Pick timestep for current iteration With 50% probability, use backward simulation Eq. 13 Intra-segment guidance (ISG) Final loss function for generator Implicit distribution alignment (IDA), as in Eq. Update fake score network µfake Using logit-normal density, as in [4] Update discriminator Eq. 12 spatial map (e.g., [B, C, H, ]) and passed through lightweight convolutional head composed of spectral-normalized residual blocks. reference image xref is processed through the same DINOv2 pathway (without augmentation) to extract corresponding semantic features. Meanwhile, the text prompt is encoded by CLIP (ViT-L/14) text encoder into condition feature c, which is projected to spatial map. Each discriminator head fuses the image feature, reference feature, and prompt condition via element-wise multiplication and spatial summation to compute the final logits. (Note: In Section 3.4, we described the reference features as extracted by the CLIP encoder. In practice, = fVFM(xref) is obtained using the same DINOv2 backbone as the input image. The Fig. 2 should also be corrected.) A.4 Training Algorithm To more clearly illustrate our training process, we provide the full algorithmic details in Algorithm 1. We adopt model-specific hyperparameter settings for better distillation performance. In particular, we set the hyperparameter λIDA of implicit distribution alignment to 0.97 by default. For the intrasegment guidance loss, λISG is set to 0.2 for SDXL, and 1.0 for both SD 3.5 and FLUX.1 dev. 15 Method Table 4: Quantitative Results of different backbone scales. FID-T CLIP Score HPSv2 Pick Score ImageReward Hyper-SDXL [10] Ours (λG = 0.25) Ours (λG = 0.5) 13.71 17.53 17. 0.3254 0.3234 0.3248 0.3000 0.3003 0.3010 22.98 23.15 23.17 0.9777 0.9326 0.9951 A.5 More Experimental Results Effect of Different Adversarial Loss Weights. In our main experiments, the hyperparameter λG in Algorithm 1, Line 22, is set to 0.5, 0.1, and 2.0 for SDXL, SD 3.5 Large, and FLUX.1 dev, respectively. To further investigate the impact of this hyperparameter, we conduct an ablation study using SDXL as an example, decreasing λG to 0.25. The results are presented in Tab.4. We observe that setting λG = 0.5 leads to improved performance across most metrics, including CLIP Score, HPSv2, PickScore, and ImageReward. Notably, this configuration achieves the best scores on HPSv2, PickScore, and ImageReward among all methods in Tab.1. These results highlight the strong semantic and visual supervision capabilities of our VFM-based discriminator. Results of Different Backbone Scales. We evaluate the impact of different VFM backbone scales (ViT-S, B, and L) in the discriminator on SDXL distillation. Interestingly, the results (Tab.1) do not follow monotonic trend with respect to model size. ViT-B achieves the best FID-T, while ViT-S yields higher CLIP Score and ImageReward. ViT-L slightly outperforms others on HPSv2 and PickScore. These findings suggest that different backbone scales offer different trade-offs in semantic alignment versus visual fidelity, and that larger backbones do not necessarily guarantee consistent improvements across all metrics. This observation is partially consistent with findings in the ADD[12] paper, which also noted diminishing returns when scaling the discriminator. In our main paper, we adopt ViT-L as the default backbone for the VFM-based discriminator. Method Table 5: Quantitative Results of different backbone scales. FID-T CLIP Score HPSv2 Pick Score ImageReward Ours ViT-S Ours ViT-B Ours ViT-L 17.26 16.58 17. 0.3262 0.3234 0.3239 0.2983 0.2991 0.3003 23.12 23.07 23.15 0.9635 0.9218 0.9326 Examples from T2I-CompBench. As shown in Fig. 9, we present visual comparisons of different methods on SDXL using the T2I-CompBench benchmark. These qualitative results clearly highlight the superiority of our approach across multiple aspects, including color fidelity (rows 1 and 2), shape consistency (row 3), material and texture (row 4), and complex spatial arrangements (row 5). Additionally, we also present more examples of our method on SDXL in Fig. 10. A.6 Prompts for Fig. 1, Fig. 7, and Fig. We use the following prompts for Fig. 1. From left to right, top to bottom: red fox standing alert in snow-covered pine forest girl with hairband performing song with her guitar on warm evening at local market, childrens story book Astronaut on camel on mars cat sleeping on windowsill with white curtains fluttering in the breeze stylized digital art poster with the word \"SenseFlow\" written in flowing smoke from stage spotlight surreal landscape inspired by The Dark Side of the Moon, with floating clocks and rainbow beams 16 Figure 9: Examples from T2I-CompBench. hot air balloon in shape of heart. Grand Canyon young man with leather jacket and messy hair playing cherry-red electric guitar on rooftop at sunset young woman wearing denim jacket and headphones, walking past graffiti wall photographer holding camera, squatting by lake, capturing the reflection of the mountains in an early morning young girl playing piano close-up of womans face, lit by the soft glow of neon sign in dimly lit, retro diner, hinting at narrative of longing and nostalgia Besides, we use the following prompts for Fig. 7. From left to right, top to bottom: quiet room with Oasis album covers framed on the wall, acoustic guitar resting on stool An astronaut lying in the middle of white ROSES, in the style of Unsplash photography. cartoon dog sits at table, coffee mug on hand, as room goes up in flames. \"Help\" the dog is yelling Art illustration, sports minimalism style, fuzzy form, black cat and white cat, solid color background, close-up, pure flat illustration, extreme high-definition picture, cats eyes depict clear and meticulous, high aesthetic feeling, graphic, fuzzy, felt, minimalism, blank space, artistic conception, advanced, masterpiece, minimalism, fuzzy fur texture. Close-up of the top peak of Aconcagua, snow-covered mountain in the Himalayas at sunrise during the golden hour. Award-winning photography, shot on Canon EOS R5 in the style of Ansel Adams. curvy timber house near sea, designed by Zaha Hadid, represents the image of cold, modern architecture, at night, white lighting, highly detailed teddy bear on skateboard in times square black and white picture of woman looking through the window, in the style of Duffy Sheridan, Anna Razumovskaya, smooth and shiny, wavy, Patrick Demarchelier, album covers, lush and detailed As for Fig. 10, we use following prompts from left to right, top to bottom: Astronaut in jungle, cold color palette, muted colors, detailed, 8k bookshelf filled with colorful books, potted plant, and small table lamp dreamy beachside bar at dusk serving mojitos and old fashioneds, guitars hanging on the wall portrait of human growing colorful flowers from her hair. Hyperrealistic oil painting. Intricate details. Peach-faced lovebird with slick pompadour. stunning and luxurious bedroom carved into rocky mountainside seamlessly blending nature with modern design with plush earth-toned bed textured stone walls circular fireplace massive uniquely shaped window framing snow-capped mountains dense forests An acoustic jam session in small café, handwritten setlist on the wall, cocktails on every table blue Porsche 356 parked in front of yellow brick wall. A.7 Licenses for existing assets We use only publicly available and properly licensed open-source datasets and pretrained models in this work. All assets are cited in the main paper, and their licenses explicitly permit academic usage, redistribution, or derivative works under specific conditions. Below is list of the key assets used and their associated licenses: LAION-5B: Licensed under CC-BY 4.0. large-scale text-image dataset used in pretraining and evaluation contexts. COCO-2017: Licensed under custom non-commercial research license. Commonly used for generation evaluation. Stable Diffusion XL: Licensed under CreativeML Open RAIL++-M. Used as diffusion based teacher model in our distillation framework. Stable Diffusion 3.5: Licensed under CreativeML Open RAIL++-M. Used as large flow-matching base model. 18 FLUX.1-dev: Licensed under CreativeML Open RAIL++-M. Used as large flow-matching base model. DINOv2: Licensed under Apache 2.0. Used as the frozen vision foundation backbone in our discriminator design. OpenCLIP: Licensed under Apache 2.0. Serves as the text encoder for prompt conditioning in the discriminator. T2I-CompBench: Licensed under the MIT License. Used for benchmark comparison of compositional generation performance. All assets were used in accordance with their respective licenses. 19 Figure 10: 10241024 samples produced by our 4-step generator distilled from SDXL."
        }
    ],
    "affiliations": [
        "Institute for AI Industry Research, Tsinghua University",
        "SenseTime Research",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology"
    ]
}