{
    "paper_title": "Improving Recursive Transformers with Mixture of LoRAs",
    "authors": [
        "Mohammadmahdi Nouriborji",
        "Morteza Rohanian",
        "Omid Rohanian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers."
        },
        {
            "title": "Start",
            "content": "Mohammadmahdi Nouriborji 1 Morteza Rohanian 2 Omid Rohanian 1 3 Abstract Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), lightweight mechanism that uses low-rank experts to simulate Mixture-of-Experts layer. By replacing feed-forward networks (FFNs) in the shared transformer with these conditional modules, we effectively restore the expressivity lost due to parameter sharing. We pretrain modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M120M) achieves state-of-theart performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional computation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers. 1 1. Introduction Large language models (LLMs) power modern NLP and deliver strong reasoning, understanding, and generation performance. Their large parameter counts and high memory use, however, make training, fine-tuning, and downstream adaptation computationally expensive (Vaswani et al., 2017; Brown et al., 2020; Sanh et al., 2020). Parameter sharing reduces the memory footprint of transformer models by applying the same weights across layers 3Department 1NLPIE Research, UK 2University of Zurich, Switzerof Engineering Science, University land of Oxford, UK. Correspondence to: Mohammadmahdi Nouriborji <m.nouriborji@nlpie.com>, Omid Rohanian <omid.rohanian@nlpie.com>. Figure 1. Structure of the Mixture of LoRAs (MoL) layer. Each MoL layer combines shared feed-forward network (FFN) with several LoRA experts, allowing conditional computation without significantly increasing parameters. instead of learning distinct parameters. ALBERT demonstrates this approach by using extensive cross-layer sharing to build compact yet strong language models (Lan et al., 2019). Analyses show, however, that matching the performance of fully parameterised transformers can force the shared layer to grow wider, increasing its hidden dimension and computational cost (Lan et al., 2019). This trade-off undermines efficiency and motivates approaches that preserve expressivity while minimising both parameters and computation. 5 2 0 2 7 1 ] . [ 2 0 8 8 2 1 . 2 1 5 2 : r Preprint. 1Our models at huggingface.co/collections/nlpie/ modern-recursive-transformers available are https:// Recent advances in large-scale models show the effectiveness of Mixture-of-Experts (MoE) architectures in enhancing transformer expressivity. By activating subset of spe1 Improving Recursive Transformers with Mixture of LoRAs cialised expert modules for each input, MoE models significantly expand transformers representational capacity and deliver strong performance across diverse tasks (Shazeer et al., 2017; Fedus et al., 2021; Lepikhin et al., 2020). Despite this success, MoE architectures introduce substantial parameter growth, making direct application to shared or lightweight architectures incompatible with the goal of parameter efficiency. Existing approaches attempt to mitigate the expressivity loss caused by parameter sharing in three main ways. (i) Static adapters (e.g., bottleneck adapters or LoRAs) add depthspecific parameters but do not adapt at the token level and do not modify the shared FFN itself. (ii) Mixture-of-Adapters (MoA) and LoRA-based MoE variants introduce dynamic experts after the FFN, leaving its internal transformation unchanged. (iii) Relaxed Recursive Transformers (RRT) attach static LoRA modules to shared block, partially restoring depth-specific behaviour but lacking token-dependent routing. Crucially, none of these designs introduce conditional computation inside the shared FFN during pretraining. In this work, we explore whether such internal modulation can more effectively restore the expressivity lost due to parameter sharing. To address these limitations, we introduce ModernALBERT, recursive transformer that restores expressivity through Mixture of LoRAs (MoL), mechanism for conditional computation within the shared architecture. We replace select FFNs in the shared transformer with MoL modules, embedding small pool of LoRA experts directly into the network. By routing tokens to sparse subset of these experts, MoL allows the shared backbone to exhibit distinct behaviours for different inputs. This approach reestablishes the layer-wise functional diversity that parameter sharing typically eliminates, all while preserving strict parameter efficiency (Hu et al., 2021). We integrate several modern architectural components that improve training stability and runtime efficiency: gated GELU-style feed-forward networks (e.g., GeGLU) (Shazeer, 2020), rotary position embeddings (RoPE; Su et al., 2021), and I/O-aware attention implementations such as FlashAttention (Dao et al., 2022). Empirical results demonstrate that ModernALBERT achieves state-of-the-art performance among compact transformer models, outperforming prior baselines on GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2018), and BEIR (Thakur et al., 2021) benchmarks. Our key contributions include: We introduce Mixture of LoRAs (MoL), conditionalcomputation layer that integrates low-rank experts into shared FFN, restoring expressivity in recursive transformers. We build ModernALBERT, family of compact recursive models that integrate RoPE, GeGLU, and FlashAttention, and we release all code and training checkpoints. We provide model variants ranging from 50M to 120M parameters and evaluate them extensively across GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2018), and BEIR (Thakur et al., 2021). We show that MoL consistently improves performance over mixture-of-adapters and relaxed recursive transformers in controlled ablations, and we introduce merging strategy that compresses experts into single adapter for efficient deployment. 2. Related Works Parameter-efficient language models were significantly advanced by ALBERT, which introduced factorised embedding parameterisation and aggressive cross-layer parameter sharing to dramatically reduce the memory footprint compared to models like BERT (Lan et al., 2019). However, this strict static parameter tying, while efficient, introduces trade-off by limiting overall expressivity and capacity. Early recursive architectures, such as the Universal Transformer (Dehghani et al., 2018), also explored layer tying, but noted that matching the performance of fully parameterised models often required wider layers, increasing computational cost. This highlights central challenge: static parameter sharing saves memory, but can create performance bottleneck. parallel and orthogonal approach to scaling models efficiently is MoE (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021). MoE models, such as the Switch Transformer, increase model capacity without proportional rise in computation by routing each token to sparse subset of experts. Conditional computation has also been adapted for parameter-efficient fine-tuning (PEFT), where experts are often small plug-in modules such as LoRA adapters (Hu et al., 2021; Houlsby et al., 2019). Mixture-of-Adapters approaches (e.g., MoLE, MixLoRA, AdaMix, Adapter Fusion) have proven effective for multi-task fine-tuning (Wu et al., 2024; Li et al., 2024; Mehrish et al., 2023; Pfeiffer et al., 2020; Feng et al., 2024). However, these frameworks are predominantly designed to be applied on top of existing models, rather than being integral to the base architecture during pretraining. Beyond modularity, modern architectural improvements have shown measurable gains in both efficiency and performance. Enhancements such as rotary positional embeddings, gated feed-forward networks, and I/O-optimised attention implementations reduce training overhead while improving model quality. ModernBERT demonstrates that these design choices can yield substantial performance gains over 2 Improving Recursive Transformers with Mixture of LoRAs Figure 2. Comparison of parameter-sharing and adapter architectures. ALBERT shares single self-attention and FFN block across all layers without adaptation. RRT augments the shared block with depth-specific static LoRA adapters. MoA keeps the shared backbone fixed and uses router to select adapters applied after the FFN output. In contrast, MoL (ours) routes among pool of LoRA experts whose low-rank updates are injected directly into the shared FFN weights, enabling token-conditional modulation while preserving parameter sharing in the backbone. standard transformer baselines without increasing computational cost (Warner et al., 2024). Recent work has explored the combination of parameter sharing with lightweight, layer-specific modules. Relaxed Recursive Transformers introduce static LoRA modules into recursive shared block, demonstrating that depth-specific adapters can partially recover performance lost due to strict parameter sharing (Bae et al., 2024). Similarly, MiniALBERT employs bottleneck adapters to enhance the expressivity of shared backbones, showing that compact transformers can achieve strong performance when combined with targeted, low-cost modules (Nouriborji et al., 2023). While these approaches effectively augment parametershared models, integrating dynamic conditional computation into compact backbone remains an open direction. ModernALBERT addresses this limitation by introducing an architecture that combines the parameter efficiency of ALBERT-style recursive sharing with the adaptive expressivity of token-conditional expert modulation. 3. ModernALBERT ModernALBERT is compact transformer architecture that combines recursive parameter sharing with lightweight, parameter-efficient modules to enhance representational capacity. We outline how recursive weight sharing reduces model size while preserving effective depth, then introduce the Mixture of LoRAs (MoL), which injects flexibility into shared layers through MoE-style low-rank experts. We also describe several auxiliary techniques that further stabilise training and improve performance across model sizes: knowledge distillation, improved initialisation, and architectural refinements. 3.1. Parameter-Sharing through Recursions Fully-parameterised transformers, such as BERT, have demonstrated strong performance across wide range of benchmarks and tasks. standard transformer layer consists of multi-head self-attention (MHA) module followed by feed-forward network (FFN), with residual connections and layer normalisation (LN) applied around each sub-layer (Nguyen & Salazar, 2019). Mathematically, the i-th layer can be expressed as: hatt = hi1 + MHA(LN(hi1); θi) + FFN(LN(hatt hi = hatt ); ϕi) For brevity, let the output of the full layer be: hi = Ψ(hi1; θi, ϕi), (1) (2) (3) where θi and ϕi denote the parameters of the attention and feed-forward modules for the i-th layer, respectively. Note that in the fully-parameterised formulation, each layer has its own distinct parameters, making deep models costly and hard to deploy. Let the hidden dimension be and the FFN expansion rate be 4; then for transformer of depth , the parameter count is approximately: 3 Improving Recursive Transformers with Mixture of LoRAs (cid:16) Θ 4d2 + 2 (4d2) (cid:17) = 12N d2 (4) To address the high parameter cost of fully-parameterised transformers, parameters can be shared across multiple layers, allowing models to maintain depth while reducing the number of unique parameters. In recursive transformer with group size and total depth G, the layers are divided into groups, where all layers within group share parameters. Denoting the shared attention and feed-forward parameters for group as θ(g) and ϕ(g), the i-th layer (with and starting from 1) can be compactly written as: hi = Ψ(hi1; θ(i/G), ϕ(i/G)) (5) In the extreme case of full parameter sharing (as in ALBERT), all layers share the same parameters, i.e., θ(g) = θ and ϕ(g) = ϕ for all layers. More generally, in recursive transformer with total depth and groups of size G, the total number of unique parameters is approximately Θ (4d2 + 8d2) = 12Kd2 (6) Compared to fully-parameterised transformer with = layers, the recursive transformer reduces the number of unique parameters by factor of K/N : Θ Θfull 12Kd2 12N d2 = = 1 (7) 3.2. Improved Flexibility through Mixture of LoRAs We address the loss of representational capacity from weight sharing by integrating Mixture-of-Experts (MoE) layers, mechanism that has proven highly effective in recent large language models. standard MoE layer consists of router and feed-forward modules called experts (Fedus et al., 2021). For given input token representation h, the router assigns probability pi(h) to each expert i, and the output of the MoE layer is weighted sum of the expert outputs: MoE(h) = (cid:88) i=1 pi(h) FFNi(h) (8) effectively enabling conditional computation and increasing the models flexibility. Although MoE layers are highly effective, they come at the cost of significantly increasing the total number of parameters, which goes against our goal of building compact, efficient models. To retain the benefits of MoE while keeping the parameter count low, we adopt LoRA modules (Hu et al., 2021) to mimic the behavior of mixture of experts. In this setup, shared feed-forward network is augmented with multiple lightweight LoRA adapters, which act as distinct experts and enable conditional computation. standard LoRA module replaces weight matrix Rdindout with low-rank update: = + α AB (9) where Rdinr and Rrdout are trainable matrices of rank din, dout, drastically reducing the number of additional parameters. Building on this idea, we define shared feed-forward network FFNshared with parameters θ = {Wdown, Wup}, it with LoRA modules ψi = and augment {Ai,1, Bi,1, Ai,2, Bi,2} for each expert i. Given an input h, the FFN modified by the i-th LoRA expert is: FFN(h; θ, ψi) = (cid:0)Wup + α Ai,2Bi,2 (cid:1) σ (cid:16)(cid:0)Wdown + α Ai,1Bi,1 (cid:17) (cid:1)h (10) where is the LoRA rank, α is scaling factor, and σ is the activation function. Finally, the output of the Mixture of LoRAs (MoL) layer is computed by selecting experts via routing weights pi(h): MoL(h) = (cid:88) i=1 pi(h) FFN(h; θ, ψi) (11) We adopt sparse top-2 routing strategy, selecting only the two experts with the highest routing weights for each token to further improve efficiency. Let S2(h) denote the indices of the top-2 experts for input h, then the sparse MoL output is: MoLtop-2(h) = (cid:88) iS2(h) pi(h) FFN(h; θ, ψi) (12) where pi(h) is the routing weight for expert and FFNi is the feed-forward network corresponding to expert i. This allows different tokens to leverage different subsets of experts, where pi(h) is re-normalised over the selected top-2 experts to sum to 1. 4 Improving Recursive Transformers with Mixture of LoRAs Variant Layers Groups MoL Groups Hidden Dim Intermediate Dim / Expert Dim Tiny Medium Base Large 14 12 16 7 3 4 6 6, 7 3 3,4 3,4,5,6 768 1024 1024 1024 1152 / 2624 2624 / 4096 2624 / 4096 2624 / 4096 Table 1. Comparison of ModernALBERT model variants. MoL layers replace the shared FFNs at the indicated positions, each with 8 experts and top-2 routing. 3.3. Effects of Knowledge Distillation and Initialisations We found that initialising the parameters of ModernALBERT from fully-parameterised model, in our case ModernBERT, significantly improves performance. Following the initialisation technique proposed in Relaxed Recursive Transformers (Bae et al., 2024), we map the parameters from the fully-parameterised model to the shared layers of ModernALBERT to provide strong starting point for training. In addition to this, we apply knowledge distillation (Hinton et al., 2015; Sanh et al., 2020) from ModernBERT, using its predictions as soft targets to further guide the training of ModernALBERT. This combination of informed initialisation and distillation is critical for data efficiency, allowing the model to converge rapidly despite reduced training time and pre-training budget of only 30B tokens (compared to the 1.7T tokens used for ModernBERT). 3.4. Training Settings We adopt multi-stage pre-training curriculum to maximise data efficiency within our limited token budget. The training process is divided into two distinct phases. We first warm up the model on the RedPajamas-1T dataset (Computer, 2023) for 20,000 to 30,000 steps. Following this initialisation phase, we transition to the RefinedWeb dataset (Penedo et al., 2023) for the remainder of the training, continuing for an additional 70,000 to 80,000 steps. Throughout both stages, we maintain fixed global batch size of 384 and maximum sequence length of 1024 tokens. Optimisation is performed using the AdamW optimiser (Kingma & Ba, 2017). We employ linear warmup schedule, peaking at learning rate of 5 104 or 5 105, followed by linear decay. This regimen allows ModernALBERT to effectively adapt the distilled knowledge from the teacher model while refining the shared parameters and LoRA modules on high-quality web data. 3.5. ModernALBERT Variants In addition to parameter-sharing and MoL, ModernALBERT incorporates several architectural improvements to enhance training stability, efficiency, and expressivity. We adopt prelayer normalisation (Pre-Norm) (Nguyen & Salazar, 2019) and GeGLU activations (Shazeer, 2020) in the feed-forward networks to improve gradient flow and representation capacity. To further improve efficiency, we use rotary embeddings (Su et al., 2021) in the attention mechanism and FlashAttention (Dao et al., 2022) for fast and memory-efficient attention computation. The MoL layers are placed at the end of each group by replacing the shared FFN with Mixture of LoRAs layer. Each MoL consists of 8 experts with top-2 routing, except the tiny model, which has 4 experts and top-1 routing, enabling conditional computation and enhanced flexibility without significant parameter overhead. We design four variants of ModernALBERT  (Table 1)  to target different model scales. This design balances model depth, parameter efficiency, and representational flexibility across different scales, enabling ModernALBERT to achieve high performance while maintaining compactness and efficiency. 4. Experimental Results We evaluate ModernALBERT on natural language understanding, question answering, and information retrieval benchmarks. Our goal is to show that adding the Mixtureof-LoRAs (MoL) layer to recursive, parameter-shared architecture provides substantial efficiency gains without sacrificing representational capacity. As shown in Table 2, ModernALBERT performs strongly across model scales. 4.1. GLUE Benchmark Results We present GLUE benchmark results (Wang et al., 2019) in Table 2, where ModernALBERT performs strongly across model scales. The largest variant, ModernALBERT-large (120M parameters), achieves GLUE average of 88.72, surpassing the fully parameterised ModernBERT-base (149M parameters, 88.45) and outperforming previous compact models such as NomicBERT and MosaicBERT. The architectural benefits appear most clearly on tasks that require precise semantic matching and inference from limited data, where ModernALBERT-large achieves state-of-the-art results among base-class models on RTE (88.44), STS-B (92.1), and MRPC (92.7). For smaller datasets like CoLA and RTE, we further stabilise 5 Improving Recursive Transformers with Mixture of LoRAs Model Params Seq. Single Sentence Paraphrase and Similarity Natural Language Inference BERT (Devlin et al., 2019) RoBERTa (Liu et al., 2019) DeBERTav3 (He et al., 2023a) MosaicBERT-128 (Portes et al., 2024) NomicBERT-2048 (Nussbaum et al., 2025) GTE-en-MLM (Li et al., 2023) ModernBERT (Warner et al., 2024) ModernALBERT-tiny ModernALBERT-medium ModernALBERT-base ModernALBERT-large CoLA SST-2 MRPC STS-B QQP MNLI QNLI 512 110M 512 125M 512 183M 137M 128 137M 2048 137M 8192 149M 8192 50M 1024 55M 1024 1024 75M 120M 1024 59.0 63.6 69.2 58.2 50.0 57.0 65.1 58.4 62.3 64.6 66.4 93.1 94.8 95.6 93.5 93.0 93.4 96. 93.0 94.0 95.2 95.5 89.5 90.2 89.5 89.0 88.0 92.1 92.2 90.2 91.0 91.7 92.7 89.4 91.2 91.6 90.3 90.0 90.2 91.8 90.4 91.2 91.8 92.1 91.4 91.9 92.4 92.0 92.0 88.8 92. 90.5 91.0 91.5 92.0 85.4 87.6 90.0 85.6 86.0 86.7 89.1 84.6 86.6 88.2 88.9 91.6 92.8 94.0 91.4 92.0 91.9 93.9 91.3 92.0 93.0 93.7 RTE 78.2 78.7 83.8 83.0 82.0 84.8 87.4 81.2 81.6 85.6 88.44 Avg 84.84 86.35 88.26 85.38 84.13 85.99 88.45 84.95 86.21 87.7 88.72 Table 2. GLUE benchmark results for base models. Scores are reported for each task category. Avg is the unweighted average across all tasks."
        },
        {
            "title": "Model",
            "content": "F1 (%) Exact (%) BERT-base RoBERTa-base ALBERT-xxlarge ModernBERT-base Ours ModernALBERT-tiny ModernALBERT-medium ModernALBERT-base ModernALBERT-large 88.6 91.7 92.5 92.6 90.0 90.4 92.8 92. 80.6 84.7 84.5 85.2 82.9 82.9 86.1 85.9 Information Retrieval Results Performance on the BEIR information retrieval benchmark (Thakur et al., 2021) is detailed in Table 4. ModernALBERT shows strong domain generalisation, achieving the best average performance among compact models on tasks such as TREC-COVID, FiQA, and ArguAna. Specifically, on ArguAna (argument retrieval), ModernALBERT scores 48.82, outperforming ModernBERT (35.7). This indicates that our routing mechanism allows the model to adapt effectively to specialised domains and shifting distributions, capability often lacking in static recursive transformers. Table 3. Results on the SQuAD-v2 Benchmark. 4.2. Ablation Studies training by pre-training the router on MNLI and freezing its parameters. This allows the model to exploit more robust, generalised routing policy, though we find the degree of improvement varies depending on the specific downstream objective. These optimisations preserve general capacity, keeping ModernALBERT competitive on high-resource tasks. On tasks such as MNLI, QNLI, and SST-2, it rivals substantially larger baselines, achieving parity with ModernBERT on QNLI (93.7) and MNLI (88.9). These results confirm that ModernALBERT successfully combines the efficiency of conditional computation with the robustness of fully parameterised transformers. Question Answering Performance We evaluate extractive question answering capabilities on SQuADv2 (Rajpurkar et al., 2018), as reported in Table 3. ModernALBERT-base achieves an F1 score of 92.8, outperforming the larger ModernBERT-base (92.6) and ALBERTxxlarge (92.5). These results indicate that the Mixture-ofLoRAs approach transfers well to token-level classification tasks. ModernALBERT also reaches an Exact Match score of 86.1, showing that the model can localise answer spans accurately despite heavy parameter sharing. 6 To isolate the contributions of our architectural choices, we conducted ablation studies comparing the MoL design against alternative conditional computation strategies and initialisation methods. comparative analysis reveals that the proposed MoL design consistently outperforms the Mixture-of-Adapters (MoA) approach. In the MoA configuration, set of adapters and router are placed sequentially after the feed-forward network, whereas our MoL design integrates LoRA experts directly into the weight space of the shared FFN. As shown in Table 5, with 8 experts and top-2 routing, MoL achieves GLUE average of 77.24, compared to 76.87 for the equivalent MoA configuration. This performance gap suggests that modifying the internal representations of the shared FFN via LoRA facilitates deeper and more effective integration of conditional computation than merely appending adaptive layers to the block output. Table 5 further illustrates the impact of scaling the expert count. Increasing the capacity from single LoRA expert (Average: 76.08) to 8 experts with top-2 routing results in clear performance gain of 1.16 points. This trend confirms that the observed improvements are driven by the conditional capacity inherent in the mixture model rather than the simple addition of trainable parameters. Finally, we compare our ModernALBERT model with the Relaxed Improving Recursive Transformers with Mixture of LoRAs"
        },
        {
            "title": "Task",
            "content": "BERT RoBERTa DeBERTaV3 NomicBERT GTE-en ModernBERT ModernALBERT NFCorpus SciFact TREC-Covid FiQA ArguAna Avg. (Subset) 24.3 51.3 49.5 22.8 31.6 38.9 20.4 45.6 52.2 26.1 35.2 37.7 8.0 22.6 48.4 11.5 26.1 20.2 25.7 52.0 63.0 23.5 35.5 41. 26.3 54.1 49.7 30.1 35.7 41.4 23.7 57.0 72.1 28.8 35.7 41.6 24.30 56.90 72.85 30.43 48.82 46.66 Table 4. Performance on selected BEIR benchmark datasets. Recursive Transformer (RRT) baseline using the same stepwise initialisation strategy proposed in RRT and training both models with the masked language modelling (MLM) objective. As shown in Table 6, ModernALBERT achieves higher GLUE average (81.94 vs. 80.95), indicating that integrating conditional computation directly into the shared FFN weights yields more effective training than the relaxed recursive approach. The training configuration was kept uniform across all ablation experiments (including MoA, MoL scaling, and RRT comparisons), all models were trained for consistent 30k steps on the Wikipedia corpus, using batch size of 128 and maximum sequence length of 1024. memory. We investigate two strategies for managing this aggregation during fine-tuning. 5.1.1. UNIFORM INITIALIZATION In the first strategy, we initialise the merged adapter by averaging the pre-trained experts uniformly (wj = 1/E). At each step, the merged adapter merged is formed by averaging the experts, and the model parameters are updated via standard backpropagation. This method assumes neutral starting point, allowing the aggregate representation to adapt to the downstream task without reliance on token-specific routing. 5. Optimising MoL for Inference 5.1.2. DYNAMIC EMA MERGING The proposed MoL layer augments parameter-shared recursive transformer with lightweight LoRA experts coordinated by routing network. While this design captures the flexibility of Mixture-of-Experts architectures, the conditional routing mechanism introduces latency overhead during inference by evaluating multiple experts per token. To address this, we propose compressing the MoL layer into single static adapter for deployment. 5.1. Expert Merging Strategies To eliminate conditional branching, we synthesise single dense adapter merged from the pool of experts {1, . . . , E}. This aggregation is defined by weighting vector RE: merged = (cid:88) j= wjj. (13) During fine-tuning, the dynamic routing mechanism is disabled, and the model updates merged directly (He et al., 2023b). This procedure effectively converts the sparse MoL layer into standard, static LoRA module, reducing the inference cost to that of dense model. As shown in Table 7, this approach yields substantial efficiency gains; ModernALBERT-tiny achieves 9.46 ms latency and 106,527 tokens/s throughput while using only 0.196 GB of GPU The second strategy updates the merging weights dynamically throughout fine-tuning based on the routers activation patterns. We initialise uniformly (wj = 1/E), allowing the EMA updates to gradually refine the distribution from neutral starting point. Let pi,t RE be the router probability vector for token in sample i. We first compute per-sample routing vector ri and batch-level average rb: ri = 1 Ti Ti(cid:88) t=1 pi,t, rb = 1 (cid:88) i=1 ri. (14) We then update the global merging weights using an exponential moving average (EMA) of these batch statistics: wnew = αwold + (1 α)rb, (15) where α is decay factor. Regarding the router itself, we adopt task-dependent approach: for data-rich tasks like MNLI, the router is trained jointly to refine expert selection; for smaller datasets (STS-B, RTE, SST-2), the router is frozen to leverage pre-trained specialisation while only the EMA weights adapt. As shown in Table 8, the EMA-based strategy consistently outperforms uniform initialization (Vanilla). By dynamically adjusting the expert contributions based on routing statistics, EMA preserves nearly all of the unmerged models accuracy, with particularly strong gains on RTE (86.28 vs 7 Improving Recursive Transformers with Mixture of LoRAs Model Params Num Experts Top-K Single Sentence Paraphrase NLI CoLA SST-2 MRPC STS-B MNLI RTE Avg ModernBERT ModernALBERT (Base) 245M 56M - - - - 34.05 37.39 88.76 88. 88.72 86.76 88.22 87.40 81.53 81.14 75.45 72.92 76.12 75.73 MoA MoA MoA MoL MoL MoL 64M 69M 75M 73M 74M 75M Mixture of Adapters (MoA) - Adapters placed after FFN 87.25 87.50 87.74 88.64 90.71 89.10 38.49 38.49 41. 1 4 8 1 1 2 Mixture of LoRAs (MoL) - Proposed Method 1 4 8 1 1 2 40.06 40.40 42. 88.99 89.00 90.36 87.74 87.99 88.23 87.68 87.32 87.56 87.38 87.33 88.01 80.77 80.61 81.05 81.03 80.91 81. 71.84 74.36 74.00 75.45 76.17 76.87 73.28 73.40 74.72 76.08 76.50 77.24 Table 5. Ablation Study: Mixture of Adapters (MoA) vs. Mixture of LoRAs (MoL). MoL consistently outperforms MoA at similar parameter counts. Model Params Single Sentence Paraphrase/Similarity Natural Language Inference ModernALBERT Relaxed Recursive Transformers 75M 73M 45.69 40.76 89.90 90.02 88.72 87.99 89.60 88.99 90.86 90.86 83.74 83. 76.17 75.09 CoLA SST-2 MRPC STS-B QQP MNLI RTE QNLI 90.82 90.66 Avg 81.94 80.95 Table 6. Ablation on Initialisation: We use the same initialisation as Relaxed Recursive Transformers (RRT) and compare our Mixture-ofLoRAs (MoL) strategy with their layer-wise LoRAs on GLUE tasks. Model Latency (ms) Throughput (token/s) Memory (GB) ModernBERT-base ModernBERT-large ModernALBERT-tiny ModernALBERT-medium ModernALBERT-base ModernALBERT-large 14.44 21.30 9.46 9.72 12.71 18.87 69519 48117 106527 106208 80731 54483 0.570 1.5 0.196 0.207 0.291 0. Table 7. Efficiency Metrics of Different Models when Using Merged Experts. 84.83) and SST-2. These results confirm that incorporating the routers learned preferences into the merging process, even when the router itself is frozen, provides superior adaptation mechanism compared to static uniform initialization. 6. Discussion The increase in capacity in the MoL architecture arises primarily from expert specialisation. As the router learns to activate different LoRA modules for different inputs, each module becomes responsible for modelling particular aspects of the data distribution, such as syntactic cues, semantic relations, or domain-specific patterns. This division of responsibility yields richer internal representations and helps mitigate the expressivity limitations inherent to fully shared architectures. Although the additional experts provide clear performance gains, dynamic routing naturally increases inference cost because several experts may be activated per token (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021). Our results on expert merging further suggest that while dynamic routing is crucial for training high-capacity models, the learned expressivity can often be compressed into static representation for inference. The success of the EMA strategy implies that the aggregate behaviour of the experts captures robust policy that adapts well across general domains. This finding aligns with recent work on model merging and distillation, indicating that the computational cost of conditional branching is beneficial for learning but not always strictly necessary for maintaining performance during deployment. This trade-off allows MoL-based models to occupy unique position where they train with the expressivity of large MoEs but deploy with the speed and 8 Improving Recursive Transformers with Mixture of LoRAs"
        },
        {
            "title": "MNLI",
            "content": "STS-B"
        },
        {
            "title": "RTE",
            "content": "SST-"
        },
        {
            "title": "No Merging\nVanilla\nEMA",
            "content": "88.2 87.9 88.0 91.8 91.5 91.7 85.6 84.83 86.28 95.2 94.8 95.1 Table 8. Effect of Expert Merging on Different Datasets. footprint of compact dense models. 7. Limitations While the proposed model improves parameter efficiency and adaptive capacity through the integration of MoL layers, several limitations remain. First, despite the architectural optimisations and merging strategies explored in this work, the computational complexity of mixture-of-experts routing remains relatively high compared to fully shared or purely dense alternatives. Although our design mitigates dispatching overhead through lightweight routing and optional merging, further advances in efficient expert selection and load balancing are necessary to fully realise the scalability benefits of conditional computation. Second, the model employs global attention across all tokens without incorporating locality-aware mechanisms. Recent architectures such as ModernBERT have demonstrated that local or block-sparse attention can substantially improve both efficiency and long-sequence modelling performance. Consequently, our approach may underperform on tasks requiring extended context retention or fine-grained reasoning over long inputs. Future work could address these issues by integrating local attention patterns or hybrid attention schemes within the recursive parameter-sharing framework. 8. Conclusion and Future Work In this work, we explored several strategies to enhance the representational power of recursive transformers. Building upon the principle of parameter sharing, we introduced the Mixture-of-LoRAs (MoL) layer, novel mechanism that integrates conditional computation into the shared transformer architecture. The proposed layer demonstrated substantial improvements in performance while maintaining parameter efficiency, highlighting its effectiveness as scalable and adaptable extension to recursive parameter-shared models. For future research, extending this framework to multimodal settings offers promising direction for studying conditional representations across diverse input types. Moreover, applying parameter sharing in conjunction with the proposed Mixture-of-LoRAs to large autoregressive language models could provide path toward reducing their parameter count and memory footprint, while preserving or even enhancing their representational capacity. Such integration may enable the development of more efficient yet powerful large-scale generative models."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was supported in part by Lambda, Inc. through its Research Grant Program, and by micro-grant from Trelis Research."
        },
        {
            "title": "References",
            "content": "Bae, S. et al. Effective parameter sharing with layer-wise lora. 2024. URL https://arxiv.org/abs/2410. 20672. introduces Relaxed Recursive Transformers / layer-wise LoRA (openreview/arXiv). Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS) / arXiv preprint, 2020. URL https://arxiv.org/abs/ 2005.14165. Computer, T. Redpajama: An open dataset for training large language models. In Thirty-eighth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://github.com/ togethercomputer/RedPajama-Data. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attenIn Advances in Neural Intion with io-awareness. formation Processing Systems (NeurIPS), 2022. URL https://arxiv.org/abs/2205.14135. Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. Universal transformers. 2018. URL https: //arxiv.org/abs/1807.03819. arXiv preprint. Devlin, J., Chang, M., Lee, K., and Toutanova, K. Bert: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, 2019. URL https://arxiv.org/abs/1810.04805. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion-parameter models with simple and efficient sparsity. 2021. URL https://arxiv.org/ abs/2101.03961. arXiv / conference. Feng, W. et al. Mixture-of-loras: An efficient multitask tuning for large language models. arXiv preprint, 2024. URL https://arxiv.org/abs/2403.03432. 9 Improving Recursive Transformers with Mixture of LoRAs He, P., Gao, J., and Chen, W. Debertav3: Improving deberta using electra-style pre-training with gradientdisentangled embedding sharing, 2023a. URL https: //arxiv.org/abs/2111.09543. He, S., Fan, R.-Z., Ding, L., Shen, L., Zhou, T., and Tao, D. Merging experts into one: Improving computational efficiency of mixture of experts, 2023b. URL https: //arxiv.org/abs/2310.09832. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in neural network, 2015. URL https: //arxiv.org/abs/1503.02531. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. transfer learning for nlp, 2019. URL https://arxiv.org/abs/1902. 00751. Parameter-efficient Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., and Chen, W. Lora: Low-rank adaptation of large language models. 2021. URL https://arxiv.org/ abs/2106.09685. arXiv preprint. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/abs/ 1412.6980. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: lite bert for self-supervised learning of language representations. 2019. URL https: //arxiv.org/abs/1909.11942. arXiv preprint. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. 2020. URL https://arxiv.org/abs/ 2006.16668. arXiv preprint / ICLR. Li, D., Ma, Y., Wang, N., et al. Mixlora: Enhancing large language models fine-tuning with lora-based mixture of experts. arXiv preprint, 2024. URL https://arxiv. org/abs/2404.15159. Li, Z., Zhang, X., Zhang, Y., Long, D., Xie, P., and Zhang, M. Towards general text embeddings with multi-stage contrastive learning, 2023. URL https://arxiv. org/abs/2308.03281. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: robustly optimized bert pretraining approach, 2019. URL https://arxiv.org/abs/1907.11692. Mehrish, A. et al. Adaptermix: Exploring the efficacy of mixture-of-adapters. arXiv preprint, 2023. URL https: //arxiv.org/abs/2305.18028. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. arXiv preprint arXiv:1910.05895, 2019. Nouriborji, M. et al. Minialbert: Model distillaIn 2023. https://aclanthology.org/2023. tion via parameter-efficient recursive student. EACL / ACL (workshop / proceedings), URL eacl-main.83.pdf. Nussbaum, Z., Morris, J. X., Duderstadt, B., and Mulyar, A. Nomic embed: Training reproducible long context text embedder, 2025. URL https://arxiv.org/abs/ 2402.01613. Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023. URL https://arxiv.org/abs/ 2306.01116. Pfeiffer, J., Ruckle, A., Schutze, K., et al. Non-destructive task composition for transfer learning (adapterfusion). arXiv preprint / ACL/EACL workshop, 2020. URL https://arxiv.org/abs/2005.00247. Portes, J., Trott, A., Havens, S., King, D., Venigalla, A., Nadeem, M., Sardana, N., Khudia, D., and Frankle, J. Mosaicbert: bidirectional encoder optimized for fast pretraining, 2024. URL https://arxiv.org/abs/ 2312.17482. Rajpurkar, P., Jia, R., and Liang, P. Know what you dont know: Unanswerable questions for squad, 2018. URL https://arxiv.org/abs/1806.03822. Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter, 2020. URL https://arxiv.org/abs/ 1910.01108. Shazeer, N. Glu variants improve transformer. 2020. URL https://arxiv.org/abs/2002.05202. arXiv preprint (describes GeGLU, SwiGLU, etc.). Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts 2017. URL https://arxiv.org/abs/ layer. 1701.06538. ICLR workshop / arXiv preprint. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. 2021. URL https://arxiv.org/ abs/2104.09864. arXiv preprint. Improving Recursive Transformers with Mixture of LoRAs Thakur, N., Reimers, N., Ruckle, A., Srivastava, A., and Gurevych, I. Beir: heterogenous benchmark for zeroshot evaluation of information retrieval models, 2021. URL https://arxiv.org/abs/2104.08663. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. AtIn Advances in Neural Intention is all you need. formation Processing Systems (NeurIPS), 2017. URL https://arxiv.org/abs/1706.03762. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: multi-task benchmark and analysis platform for natural language understanding, 2019. URL https://arxiv.org/abs/1804.07461. Warner, B., Chaffin, A., Clavie, B., Weller, O., et al. Modernbert: modern bidirectional encoder for fast, memory-efficient, and long-context fine-tuning and inference. arXiv preprint, 2024. URL https://arxiv. org/abs/2412.13663. Wu, X., Huang, S., Wei, F., et al. Mixture of lora experts. arXiv preprint / OpenReview, 2024. URL https:// arxiv.org/abs/2404.13628."
        }
    ],
    "affiliations": [
        "Department of Engineering Science, University of Oxford, UK",
        "NLPIE Research, UK",
        "University of Zurich, Switzerland"
    ]
}