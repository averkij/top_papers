{
    "paper_title": "Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models",
    "authors": [
        "Namhyuk Ahn",
        "KiYoon Yoo",
        "Wonhyuk Ahn",
        "Daesik Kim",
        "Seung-Hun Nam"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in diffusion models revolutionize image generation but pose risks of misuse, such as replicating artworks or generating deepfakes. Existing image protection methods, though effective, struggle to balance protection efficacy, invisibility, and latency, thus limiting practical use. We introduce perturbation pre-training to reduce latency and propose a mixture-of-perturbations approach that dynamically adapts to input images to minimize performance degradation. Our novel training strategy computes protection loss across multiple VAE feature spaces, while adaptive targeted protection at inference enhances robustness and invisibility. Experiments show comparable protection performance with improved invisibility and drastically reduced inference time. The code and demo are available at \\url{https://webtoon.github.io/impasto}"
        },
        {
            "title": "Start",
            "content": "Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models Namhyuk Ahn 1,2 KiYoon Yoo 3 Wonhyuk Ahn 2 Daesik Kim 2 Seung-Hun Nam 2 1 Inha University 2 NAVER WEBTOON AI 3 KRAFTON 4 2 0 2 6 1 ] . [ 1 3 2 4 1 1 . 2 1 4 2 : r (a) Inference latency (log-scaled) vs. image size (b) Protection efficacy vs. invisibility Figure 1. (a) FastProtect shows unprecedented speed in protection against diffusion models. On an A100 GPU, FastProtect achieves real-time latency even for processing 20482-px image, while others require substantially longer time. (b) In terms of the trade-off between protection efficacy (FID, is better) and invisibility (DISTS, is better), FastProtect exhibits improvement over other protection methods."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in diffusion models revolutionize image generation but pose risks of misuse, such as replicating artworks or generating deepfakes. Existing image protection methods, though effective, struggle to balance protection efficacy, invisibility, and latency, thus limiting practical use. We introduce perturbation pre-training to reduce latency and propose mixture-of-perturbations approach that dynamically adapts to input images to minimize performance degradation. Our novel training strategy computes protection loss across multiple VAE feature spaces, while adaptive targeted protection at inference enhances robustness and invisibility. Experiments show comparable protection performance with improved invisibility and drastically reduced inference time. The code and demo are available at https://webtoon.github.io/impasto 1. Introduction Large-scale diffusion models [16, 40, 42, 49] have achieved remarkable success in the realm of image synthesis task, revolutionizing the way we create and manipulate digital imagery [18, 21, 33, 39, 54]. By adopting recently emerged personalization techniques, it is now possible to develop diffusion models that can generate images in ones unique styles [1, 11, 24, 43, 48]. However, the very capabilities that make these models so valuable also pose significant risks. Diffusion models can allow malicious users to replicate an individuals artwork without consent, easily stealing their creative output [47]. Furthermore, diffusion models are exceedingly adept at mimicking faces and objects [24, 43]. This proficiency is not limited to benign applications but can be extended to the making of deepfakes or fake news. The implications for societal harm are profound, as these technologies could be used to undermine public trust, distort political discourse, and violate personal privacy. Current efforts to protect images against diffusion models have evolved through adversarial perturbation [12]. By introducing perturbations to given image, they render the protected image resistant to diffusion-based personalization methods, causing the diffusion model to generate distorted outputs. Existing methods primarily rely on two approaches: minimizing the distance between the latent features of the protected and pre-defined target image within VAE encoder (e.g. PhotoGuard [45] and Glaze [47]), or fooling the noise prediction network (e.g. AdvDM [28] and Anti-DB [50]). Despite numerous advancements, all current frameworks fundamentally start from an approach that optimizes perturbations during inference [32] when protection is requested, casting doubt on the practicality of these methods to be used in many real applications. For protection framework to become practical so- (1) lution, the following conditions should be satisfied: Protection Efficacy: Obviously, image protection method should effectively counteract against diffusion-based personalization. Many recent studies have aimed to maximize this aspect by analyzing the internal dynamics of diffusion 1 models [53, 58]. (2) Invisibility: Although some studies strive to be as invisible as possible, adversarial perturbations inevitably leave visible traces on images. However, for commercial services, minimizing these traces is crucial. Moreover, as analyzed by Ahn et al. [2], the task of preventing mimicry tends to leave more noticeable traces, particularly in images with flat textures (e.g. cartoon or illustration). Despite its importance, only handful of studies highlight this issue [2]. (3) Latency: All existing methods update perturbations during the inference phase through iterative optimization. However, this process is inherently timeconsuming; e.g. protecting 5122-px image can take up to 5-120 minutes on CPU and 7-200 seconds even on highend A100 GPU. The extensive time required by current protection methods poses significant barrier that prevents ordinary users from utilizing image protection, thereby still exposing them to the risk of misuse of generative models. Consequently, in order to democratize this technology, it is crucial to develop protection framework that can operate effectively on less powerful devices. Unfortunately, despite extensive progresses in the image protection task, there is scant focus on reducing latency. In this work, we propose novel protection framework, FastProtect, which meets all the requirements, with particular focus on latency. FastProtect diverges from prevalent approaches by adopting pre-trained perturbation. Although universal adversarial perturbation (UAP) [35] is proposed in adversarial attack, we observed that it significantly reduces protection efficacy for our task. To counteract this, we introduce mixture-of-perturbations (MoP). Unlike UAP, which relies on single perturbation, MoP prepares multiple perturbations and selects one of them based on the input images latent code. We also propose multi-layer protection (MLP) loss that utilizes the intermediate features when calculating the protection loss. It intensifies the distortion when the protected images go through fine-tuning by personalization methods, thus enhancing the protection efficacy without any additional cost at inference. For the inference, we introduce an adaptive targeted protection to enhance protection efficacy with minimal computational overhead. Our observations suggest that in targeted protection, which minimizes the distance between the input images latent code and that of (pre-defined) target, selecting an appropriate target image significantly influences protection performance. Taking this into account, we adaptively determine the best target image based on the input images latent code and then apply the perturbation trained for that specific target. To enhance invisibility, we propose an adaptive protection strength approach using the LPIPS [57] distance. Unlike Ahn et al. [2] that leverages multiple just-noticeable difference (JND) maps, our approach does not require ad-hoc modules nor does it significantly increase computational load; it only involves an additional forward pass of LPIPS (i.e. AlexNet [23]). We conduct extensive experiments to verify the effectiveness and efficiency of our protection method. To simulate various use-cases, we test across diverse domains natural images, faces, paintings, and cartoons under different personalization and countermeasure scenarios. FastProtect achieves similar protection efficacy to other methods but at nearly zero cost, with 200 to 3500 speedup (Figure 1a). It also shows an improvement in invisibility compared to most methods (Figure 1b), proving its suitability for practical applications. Our main contributions are: We propose FastProtect, which achieves real-time protection against diffusion models. Our work is the first to address the critical issue of latency in this task. FastProtect integrates perturbations pre-training with adaptive inference schemes, meeting all requirements for practical protection solution. We validate FastProtect in various scenarios, showing that despite its speed and invisibility, it retains protection efficacy, robustness, and generalization. 2. Background Diffusion Models. These have gained prominence due to their ability to generate high-quality images. Latent Diffusion Model (LDM) [42] is particularly highligted for its exceptional quality and efficiency. Within LDM, VAE encoder E, transforms an input image into latent code = E(x). This code is then reconstructed back into the image domain by decoder D; = D(z). The diffusion process makes latent code by incorporating external factors, y, such as textual prompts. The training of LDM is driven by denoising loss function at each timestep as: LSD = EzE(x),y,ϵN (0,1),t[ϵ ϵθ(zt, t, c(y))2 2], (1) where denoising network ϵθ restores the noised latent code zt based on the timestep and conditioning vector c(y). Recent advancements explore the personalization (or few-shot fine-tuning) of LDM with few reference images through two primary approaches: textual inversion [11], which utilizes the embedding space of CLIP [41] while keeping the denoising network fixed, and model optimization [43], which directly updates the denoising network. Protection Against Diffusion-based Mimicry. Current protection frameworks apply adversarial perturbations δ to image x, producing protected image ˆx = + δ using projected gradient descent (PGD) [32]. With protection loss LP , current methods obtain the protected image of i-th optimization step by signed gradient ascent with step function sgn and step length α as given by x(i) = ΠNη(x) (cid:104) (cid:105) x(i1) + αsgn(x(i)LP (x(i1)) , (2) 2 where ΠN η(x) projects onto the neighborhood of with radius of η. This iterative process is repeated for steps until ˆx = x(N ) is achieved (Figure 2a). For the objective function, semantic or texture losses are mostly used [27]. The semantic loss, LS (x) = LSD(x), is designed to disrupt the denoising process of LDM, misleading it to generate samples that deviate from the original images. AdvDM [28] adopts this loss and Anti-DB [50] is improved to be more robust against DreamBooth [43] by incorporating its update process within the optimization. The texture loss, LT (x) = E(x) E(y)2 2, aims to pushing the latent code of towards the target images latent code. Glaze [47] and PhotoGuard [45] belong to this category. Upon these objective functions, We can now define universal loss as LP = λS LS + λT LT , where {λS , λT } 0 are balancing factors. Mist [27] utilizes this universal loss function to effectively capture the characteristics of both objectives. Diff-Protect [53], building on Mist, introduces score distillation trick to achieve more efficient protection. Impasto [2] incorporates perceptual-oriented components designed for imperceptible protection. Since all the current protection frameworks leverage iterative optimization during inference, they can yield imagespecific and high-performing perturbations [2, 27, 28, 45, 47, 50, 53, 58]. Despite this advantage, significant drawback is the substantial time required to operate protection. For instance, to protect 5122-px image, even the fastest model (e.g. PhotoGuard [45]) requires around 7 seconds on an A100 GPU and 350 seconds on CPU. Consequently, we explore alternatives to this iterative process to enhance efficiency without compromising the protection performance. Universal Adversarial Perturbation (UAP). This concept is introduced in adversarial attacks, through data-driven approach to find an image-agnostic perturbation from the training dataset [35]. In our task, we can adapt UAP as in Figure 2b. Given the training dataset XD, we employ protection loss LP to derive an universal perturbation δ, constrained within η-ball to maintain perceptual invisibility. δ = arg max δη ExXD [LP (x + δ)] . (3) Upon finalizing the universal perturbation δ by the training, it can be applied to an image to produce protected image ˆx = + δ. This is notably practical as it requires no computation at inference. Nevertheless, we found that directly applying UAP to our task significantly compromises protection performance. In Table 1, we compare the protection efficacy of iterative optimization (via PGD) and UAP. We use DISTS [10] to assess protected images quality, while FID [15] evaluates generated image through mimicry by LoRA [17] (a higher FID is better protection). As also shown in the qualitative mimicry results by LoRA, UAP leads to notable degradation in protection efficacy. Hence, Table 1. PGD vs. UAP (Top) Invisibility (DISTS) and protection efficacy (FID) comparison. (Bottom) Mimicry results via LoRA. Method Invisibility (DISTS; ) Protection (FID; ) PGD UAP 0.221 0.222 227.6 207.6 Clean PGD UAP it is crucial to refine this pre-training approach, but with minimal inference cost in manner akin to UAP. 3. Method 3.1. Perturbation Pre-Training Mixture-of-Perturbation (MoP). We hypothesize that the underperformance of UAP is primarily due to two reasons: 1) single perturbation has limited capacity to deceive diffusion models, and 2) the image-agnostic nature of UAP fails to cover the diverse features (e.g. texture or structure) of images. To overcome these, we introduce mixture-ofperturbations (MoP), consisting of multiple perturbations, = {δ1, . . . , δK}, where represents the number of perturbations. For any given input image, MoP dynamically assigns and applies the appropriate perturbation to create protected image (Figure 2c). Specifically, given an image x, MoP first encodes it using VAE encoder to obtain the latent code z. This code is used to select the specific perturbation to be applied, as dictated by an assignment function A. Formally, the protected image ˆx is generated as: ˆx = + δg + k, where = A(E(x)), (4) where δg is global perturbation. We observed that adding δg slightly improves the performance. In MoP, the assignment function plays crucial role by guiding the selection of perturbations based on an input image, ensuring that similar images are protected with the same perturbation. This capability enables MoP to not only increase the capacity of UAP but also to offer degree of imagespecific protection, which is notably absent in UAP. Without the assignment function, naive perturbation averaging, i.e. ˆx = + δg + 1/K ΣK k=1k, increases the capacity but fail to overcome the intrinsic limitations of image-agnostic, In contrast, our MoP thus limiting performance benefits. implements semi-image-specific protection, positioning it between the image-agnostic and the image-specific precision of the iterative approach while removing the expensive inference cost, as demonstrated in the ablation study. We adopt simple assignment function that can leverage the highly representative latent codes of the VAE en3 Figure 2. Model overview. (a) Current iterative optimization approaches lack training phase and perform optimization during inference, resulting in extremely slow protection. (b) UAP [35] introduces pre-training of perturbations, but their image-agnostic nature leads to degraded protection efficacy. (c) Combining the advantages of both paradigms, FastProtect adopts pre-training approach similar to UAP but with novel mixture-of-perturbation scheme and multi-layer protection loss to enhance protection efficacy. At inference, adaptive targeted protection further boosts protection efficacy with minimal additional cost, and adaptive protection strength improves invisibility. coder. For our training dataset XD, we extract the latent codes = {z1, . . . , zD} using the VAE encoder E. These codes are then clustered into predefined groups with Kmeans++ [3]. Despite its simplicity, this process shows its robustness empirically, thus we adopt it for our framework. Multi-Layer Protection Loss. FastProtect employs targeted protection approach based on the texture loss. Conventionally, texture loss is computed upon the VAE latent code z, i.e. LT (x) = zy2 2, where zy is the latent code of the target image. However, we observed that often the z-space loss does not sufficiently push towards zy, resulting in suboptimal protection performance. To address this, we incorporate an auxiliary loss using multilayer features extracted by the VAE encoder. Specifically, given an input image, we extract the intermediate features = {f 1, . . . , L}, where is the number of feature layers extracted. Similarly, we obtain Fy for the target image. Then, our proposed multi-layer protection loss function with the balanced factor λ is computed as: LT (x) = zy 2 λ (cid:88) l=1 l y2 2. (5) The auxiliary MLP loss improves overall protection efficacy as this leverages the intermediate feature spaces beyond the z-space when pushing toward zy. In addition, since these modifications are implemented during the pretraining stage, it improves protection efficacy without extra computational expense at inference. Training. The assignment function is first trained on XD then MoP is updated on XD using Eq. 5 by Adam optimizer [22]. All the perturbations (δg, ) are initialized at resolution of 512 512. When applied to input images of different resolutions, we resize the perturbation through bilinear interpolation. Since two perturbations are used in MoP, δg, are constrained within (η/2)-ball. 3.2. Adaptive Inference Adaptive Targeted Protection. In targeted protection, target image influences protection performance. Liang and Wu [27] noted that patterned image is effective, while Zheng et al. [58] reported that the protection efficacy varies with pattern repetitions. Though they provide valuable insights, they do not consider the relationship between the target and input images. Instead, we investigate such relationship and propose an adaptive targeted protection approach. In our analysis, we use two target images characterized by either low or high pattern repetition (shown in Suppl. and apply protection to input images with either simple or complex textures. Figure 3 shows mimicry outputs by LoRA of these four protection scenarios. Interestingly, our observations suggest relationship between the pattern repetition 4 Figure 3. Relationship between target images pattern repetition and input images texture. Simple textured image is successfully protected by low repetition target, but fails when using high repetition target; vice versa for complex texture cases. Figure 4. Given the original and protected images, we obtain the LPIPS distance map, which remarkably aligns with human perception. The brighter regions on the perceptual map indicate areas where subtle distortions are more noticeable. of the target image and the texture complexity of an input image. Zheng et al. [58] noted that protection efficacy improves as pattern repetition increases but declines when it becomes too high. We suspect that their results might be biased by fixing the input image, thereby keeping the texture factor constant. In contrast, our findings indicate that tailoring the pattern image to match the texture complexity of an input image can improve protection efficacy. g, l}, {δm = E(yl), zm Hence, we design FastProtect to adaptively select the target image based on an input image. To this end, we prepare target images with varying pattern repetitions and train MoPs for each. We utilize three pattern imageslow, mid, , m}, and highresulting in three MoPs: {δl , h}. We then extract the latent codes for each and {δh = E(yh), target image: zl where yl, ym, yh are the target images with low, mid, and high pattern repetitions, respectively. At inference, input images latent code is compared with {zl } and the target image whose latent code is closest to is selected. To measure the similarity between the target images pattern repetition and the input image, L1 norm of entropy is used as distance. Overall, integration of MoP (in Eq. 4) and the adaptive targeted protection are as follows: = E(ym), zh , zh y, zm ˆx = + δt + k, = arg min i{l,m,h} H(z) H(zi y)1, (6) where H(z) = (cid:80) zz p(z) log p(z). The major advantage of our adaptive targeted protection is its robust performance across various domains, which is critically important for practical use cases where the specific domain (or texture) of incoming images might be unknown. If the range of potential domains is known beforehand, the performance difference might not be significant. However, the flexibility to adapt to different input characteristics without prior domain knowledge enhances FastProtects utility in diverse and unpredictable environments of real applications. Adaptive Protection Strength. According to the WeberFechner law, humans are more adept at detecting sub5 tle changes in regions with simple textures than in complex ones. Upon this, Ahn et al. [2] adapted human visual system to the image protection task. They utilized multiple perception maps and adjusted perturbation strength during optimization to effectively enhance invisibility. However, the perception maps used by Ahn et al. [2] are computed before the injection of perturbations, leading us to hypothesize that these may not perfectly align with the actual perceived perturbations intended for protection. Moreover, their reliance on combination of traditional perceptual map algorithms resulted in slow processing and performance limitations. To address these challenges, we first apply MoP to create surrogate protection image and then generate perceptual map, which helps resolve the identified issues. Specifically, we generate surrogate protection image ˆx through Eq. 6. Subsequently, we create spatial perceptual map using LPIPS [55], i.e. = LPIPS(x, ˆx). As shown in Figure 4, the perceptual map is remarkably aligned with human cognitive perspectives. Hence, we utilize this map to produce the final protected image as in below: ˆx = + S(1 M) (δt + k). (7) Here, S() is scaling function (more details are in Suppl. and serves as distance map, necessitating an inversion step. Since this process requires only the forward pass of the LPIPS backbone, the additional computational cost at inference is minimal, thus economically enhancing invisibility without significant overhead. In Suppl. we summarize the perturbation pre-training and inference process in the form of an algorithm. 4. Experiment Implementation Details. We set the number of perturbations in MoP as four. Other hyperparameters and details on training and inference are described in Suppl. Datasets. We utilize four domains when constructing both the training and benchmark datasets: object, face, painting, Table 2. Quantitative comparison. Latency is measured on 512512 image. Comparisons of other metrics are shown in Appendix. Method AdvDM [28] PhotoGuard [45] Anti-DB [50] Mist [27] Impasto [2] SDST [53] FastProtect Latency CPU / GPU 1210s / 35s 7s 370s / 7278s / 225s 1440s / 40s 830s / 19s 1410s / 24s 2.9s / 0.04s Object Face Painting Cartoon Invisibility: DISTS () / Efficacy: FID () 0.197 / 220.0 0.203 / 223.0 0.239 / 214.4 0.185 / 217.2 0.201 / 213.8 0.242 / 219.2 0.155 / 223.0 0.173 / 303.8 0.189 / 308.7 0.162 / 301.4 0.154 / 307.5 0.198 / 298.4 0.244 / 302.1 0.149 / 308.9 0.153 / 357.6 0.107 / 350.9 0.114 / 347.7 0.129 / 357.0 0.123 / 352.4 0.152 / 354.1 0.110 / 356.1 0.271 / 212.5 0.209 / 219.1 0.294 / 225.4 0.223 / 223.7 0.207 / 215.5 0.237 / 222.7 0.186 / 220.3 Figure 5. Qualitative comparison of different protection frameworks. (Top) Protected image with zoomed-in patch in the inset. (Bottom) Two output images from the personalized LoRA. and cartoon. These multiple scenarios allow us to analyze the models protection performance more closely to real applications, as images from various domains can be encountered. The dataset details are provided in Suppl. Baselines. We compare with existing diffusion-based image protection frameworks: AdvDM [28], PhotoGuard [45], Anti-DB [50], Mist [27], Impasto [2], and Diff-Protect (SDST) [53]. Except ours, all other methods rely on iterative optimization, using texture or semantic losses. Evaluation. Latency is measured on M1 Max CPU and an A100 GPU. To evaluate invisibility, we use DISTS [10], and FID [15] is utilized to measure the protection efficacy. In Suppl. we provide additional metrics for more comprehensive comparison. Note that previous studies typically fix the protection strength across models for comparison, but we found that protection efficacy and invisibility vary significantly depending on the protection loss used. For instance, PhotoGuard [45] and AdvDM [28] exhibit different trends since they use disparate objectives. In addition, evaluating protection efficacy only without invisibility leads to narrow assessment that might overlook real-world needs. To address these, we adjust the protection strength to match the protection level across methods to ensure fairer comparison and better reflection of practical requirements. However, one might also be curious about how the performance appears when the protection strength is equalized across all methods. Hence, we also provide evaluation with fixed strength in Suppl. 4.1. Model Comparison As shown in Figure 1a and Table 2, FastProtect is ultra-fast; 125 faster on CPU and 175 faster on GPU compared to the second fastest model, PhotoGuard [45]. Remarkably, FastProtect maintains consistent latency even as the input image size increases, achieving near real-time performance even for 20482-px images. In contrast, all other frameworks show exponential increases in latency (Figure 1a), which poses significant issue in the real world where many recent artworks are high-resolution. The unprecedented speed of our model makes it highly user-friendly. FastProtect also demonstrates superior trade-off between protection efficacy and invisibility compared to other frameworks (Figure 1b). In comparisons where the protection strength is adjusted to yield similar protection efficacy  (Table 2)  , FastProtect consistently achieves the best invisibility across most protection domains, with the second-best 6 Table 3. Ablation study. Table 4. Analysis on protection robustness scenarios. Configuration PhotoGuard UAP [35] MoP (w/o A) MoP + MLP Loss + Adapt. Target FID () 227.6 207.6 214.5 225.9 234.6 238.8 Domain Method Invisibility Subject Cartoon PhotoGuard FastProtect PhotoGuard FastProtect 0.203 0.155 0.209 0.186 Countermeasure JPEG Noise 193.3 214.4 192.9 191.8 193.2 191.3 193.1 199.9 Arbitrary Size 193.5 219.1 190.7 204.0 Table 5. Analysis of black-box protection scenarios (unknown diffusion models and personalization methods). Domain Method Invisibility Subject Cartoon PhotoGuard FastProtect PhotoGuard FastProtect 0.203 0.155 0.209 0. Unknown Model Unknown Personalization SD-v2.1 [42] 176.6 177.5 179.3 188.3 SD-XL [40] 190.9 218.4 195.1 207.3 TI [11] 290.2 305.6 306.4 305.7 DreamStyler [1] 224.9 231.3 193.7 209. performance in the painting domain. In our analysis, FastProtect shows the strong advantage in invisibility within areas that have flat textures. These features result in significant improvements in the object and cartoon domains. For example, Figure 5 shows that in the first row, the sky region is rendered nearly invisible by FastProtect, and the same applies to the cartoon in the second row. As demonstrated in Suppl. even when we fix the protection strength of all methods to the same value, our method achieves favorable balance in this trade-off. 4.2. Model Analysis Ablation Study. In Table 3, we depict the component analysis of FastProtect. It is notable that UAP [35] significantly deteriorates protection efficacy compared to the iterative optimization baseline, PhotoGuard [45]. On the other hand, when we introduce the proposed MoP, it recovers the lost performance. However, if the assignment function is not used, the improvement is limited, highlighting the importance of achieving semi-image-specific nature. As we discussed, the limitations of UAP arise not only from insufficient capacity but also from its image-agnostic nature. Simply increasing the number of perturbations (i.e. MoP w/o A) resolves the first limitation and improves performance over UAP. However, this still does not overcome the second one, which causes it to underperform compared to PhotoGuard. When we introduce the assignment function into MoP, it can dynamically account for image features. This allows MoP to match the performance of iterative optimization methods while remaining significantly faster. When we attach the multi-layer protection (MLP) loss, it significantly increases the protection efficacy. Similarly, adaptive targeted protection also enhances performance. Overall, by incorporating novel modules in both the pretraining and inference stages, FastProtect can achieve better protection efficacy with much faster inference. Robustness. Here, we analyze the vulnerability of protected images to countermeasures  (Table 4)  . We use PhotoGuard [45] as our baseline due to its similarity to our approach. To assess robustness, we evaluate against countermeasures such as Gaussian noise and JPEG compression. Additionally, we account for more realistic scenario where the input image can have arbitrary dimensions. Most previous evaluations in image protection tasks assume fixed image resolution, typically 5122, which represents an idealized scenario. In practice, image sizes and aspect ratios vary. Our model demonstrates comparable performance against these countermeasures, and notably, in the arbitrary size scenario, FastProtect performs even better. This suggests that simply using interpolation to adapt MoP to different sizes is both straightforward and effective. Black-Box Scenario. Table 5 presents the model analysis in black-box scenarios. We examine two cases: the unknown model scenario, where we transfer protection methods to other Stable Diffusion (SD) backbones, specifically SD v2.1 [42] and SD-XL [40]. We also assess image protection against unknown personalization by applying Textual Inversion (TI) [11] and DreamStyler [1] to replicate the image. In these scenarios, FastProtect demonstrates superior invisibility compared to the baseline while achieving comparable or better performance in black-box settings. Mixture-of-Perturbation. In Figure 6a, we vary the training dataset XD and measure the protection efficacy. Interestingly, even when the training domain is limited, there is no significant gap in the protection performance. We conjecture that MoP effectively handles unseen domains thanks to its adaptive perturbation selection mechanism. However, using all domains is slightly better, so we train with this strategy. Additionally, Figure 6b compares FastProtect by changing and excluding the assignment function in MoP. Without the assignment function, performance improvement is limited, and in the full MoP case, it achieves (a) Domain Generalization of MoP (b) Effect of Assignment Function (c) Examples of MoP Group Figure 6. Analysis of the proposed modules in the pre-training phase. (a) Adapt. Targeted Protection (b) Adapt. Protection Strength (c) FastProtect + PGD Refine Figure 7. Analysis of the proposed modules in the inference phase. substantially better efficacy with convergence at = 4. In Figure 6c, we visualize the representative images assigned to each perturbation when = 4. The images are grouped by their certain distinguishing features (e.g., texture, scene). Adaptive Targeted Protection. Figure 7a analyzes the relationship between protection efficacy and the target image. When target image with low pattern repetition is used, the protection is effective in the Face and Cartoon domains but less so for Objects. In contrast, high-repetition target image leads to strong performance in the Object domain. The proposed adaptive targeted protection demonstrates near-optimal performance across all scenarios. Since it is difficult to predict which domain will be encountered in real-world use cases, relying on single domain with fixed target image is not practical. Therefore, incorporating adaptive protection into FastProtect is essential for creating more robust protection solution in real-world applications. Adaptive Protection Strength. We analyze the impact of using the proposed adaptive protection strength in Figure 7b by adjusting the perturbation budget. Without this module, the trade-off between protection efficacy and invisibility is worse than that of the full model. Note that when the budget is small, the perturbation is inherently minimal, so the difference could be marginal. However, with stronger protection, the difference becomes significant. FastProtect + Iterative Optimization. FastProtect is extremely fast, so users with sufficient computing resources might want to invest additional computation to achieve even more effective protection. Considering this scenario, in Figure 7c, we use the results of FastProtect as the initial perturbation and further refine them using PGD (e.g. PhotoGuard [45]). Note that we display the baseline, PhotoGuard, with slightly larger budget since applying PGD to FastProtect tends to decrease invisibility. Hence, the baseline budget is adjusted to match the invisibility level of FastProtect + PGD. Surprisingly, our method serves as superior initial checkpoint for the iterative optimization techniques. For example, the baseline, requires 100 steps to match our initial results, whereas FastProtect + PGD converges in just 25 steps with significantly higher protection efficacy. 5. Conclusion In this work, we propose FastProtect, which leverages pretrained mixture-of-perturbations for low latency. We also propose adaptive inference to compensate the loss of the protection efficacy and to improve the invisibility of the perturbation. Our experiments demonstrate that FastProtect offers more practical solution with comparable protection performance to existing methods, with improved invisibility and substantially reduced inference time. Limitations. FastProtect still produces some visible distortion and this is an unavoidable drawback when using adversarial perturbations. Future research should focus on finding new paradigms that can maximize the quality."
        },
        {
            "title": "References",
            "content": "[1] Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim, Daesik Kim, Seung-Hun Nam, and Kibeom Hong. Dreamstyler: Paint by style inversion with text-to-image diffusion models. arXiv preprint arXiv:2309.06933, 2023. 1, 7 [2] Namhyuk Ahn, Wonhyuk Ahn, KiYoon Yoo, Daesik Kim, Imperceptible protection against arXiv preprint and Seung-Hun Nam. style imitation from diffusion models. arXiv:2403.19254, 2024. 2, 3, 5, 6, 14, 15, 16, 17 [3] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, pages 10271035, 2007. 4 [4] Qiong Cao, Li Shen, Weidi Xie, Omkar Parkhi, and Andrew Zisserman. Vggface2: dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), pages 6774. IEEE, 2018. 14 [5] Ashutosh Chaubey, Nikhil Agrawal, Kavya Barnwal, Keerat Guliani, and Pramod Mehta. Universal adversarial perturbations: survey. arXiv preprint arXiv:2005.08087, 2020. [6] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 2024. 14 [7] Francesco Croce and Matthias Hein. Sparse and imperceivIn Proceedings of the IEEE/CVF able adversarial attacks. international conference on computer vision, pages 4724 4732, 2019. 13 [8] Zeyu Dai, Shengcai Liu, Qing Li, and Ke Tang. Saliency attack: Towards imperceptible black-box adversarial attack. ACM Transactions on Intelligent Systems and Technology, 14(3):120, 2023. 13 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 14 [10] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture IEEE transactions on pattern analysis and masimilarity. chine intelligence, 44(5):25672581, 2020. 3, 6, 14 [11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 1, 2, [12] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. arXiv Explaining and harnessing adversarial examples. preprint arXiv:1412.6572, 2014. 1 [13] Chuan Guo, Jared Frank, and Kilian Weinberger. arXiv preprint Low frequency adversarial perturbation. arXiv:1809.08758, 2018. 13 [14] Jamie Hayes and George Danezis. Learning universal adversarial perturbations with generative models. In 2018 IEEE Security and Privacy Workshops (SPW), pages 4349. IEEE, 2018. 12 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 3, 6, [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3, 14 [18] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. 1 [19] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 14 [20] Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18671874, 2014. 14 [21] Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim, and Namhyuk Ahn. Diffblender: Scalable and composable multimodal text-to-image diffusion models. arXiv preprint arXiv:2305.15194, 2023. [22] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 4, 12 [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 2 [24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 1 [25] Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen threat models. arXiv preprint arXiv:2006.12655, 2020. 13 [26] Shanshan Lao, Yuan Gong, Shuwei Shi, Sidi Yang, Tianhe Wu, Jiahao Wang, Weihao Xia, and Yujiu Yang. Attentions help cnns see better: Attention-based hybrid image quality assessment network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11401149, 2022. [27] Chumeng Liang and Xiaoyu Wu. Mist: Towards improved adversarial examples for diffusion models. arXiv preprint arXiv:2305.12683, 2023. 3, 4, 6, 14, 15, 17 [28] Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Adversarial example does good: preventing painting imitation from diffusion models via adversarial examples. In Proceedings of the 40th International Conference on 9 Machine Learning, pages 2076320786, 2023. 1, 3, 6, 14, 15, 17 [29] Hong Liu, Rongrong Ji, Jie Li, Baochang Zhang, Yue Gao, Yongjian Wu, and Feiyue Huang. Universal adversarial perturbation via prior driven uncertainty approximation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29412949, 2019. 12 [30] Bo Luo, Yannan Liu, Lingxiao Wei, and Qiang Xu. Towards imperceptible and robust adversarial example attacks against neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2018. [31] Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie, and Linlin Shen. Frequency-driven imperceptible adversarial attack on semantic similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1531515324, 2022. 13 [32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 1, 2 [33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 1 [34] Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Sparsefool: few pixels make big difIn Proceedings of the IEEE/CVF conference on ference. computer vision and pattern recognition, pages 90879096, 2019. 13 [35] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 17651773, 2017. 2, 3, 4, 7, 12 [36] Konda Reddy Mopuri, Aditya Ganeshan, and Venkatesh Babu. Generalizable data-free objective for crafting universal adversarial perturbations. IEEE transactions on pattern analysis and machine intelligence, 41(10):24522465, 2018. [37] Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, and Venkatesh Babu. Nag: Network for adversary generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 742751, 2018. 12 [38] Konda Reddy Mopuri, Phani Krishna Uppala, and Venkatesh Babu. Ask, acquire, and attack: Data-free uap In Proceedings of the generation using class impressions. European Conference on Computer Vision (ECCV), pages 1934, 2018. 12 [39] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 1 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 7 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 7 [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 1, 2, 3, [44] Babak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning the right metric on the right feature. arXiv preprint arXiv:1505.00855, 2015. 14 [45] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. arXiv preprint arXiv:2302.06588, 2023. 1, 3, 6, 7, 8, 14, 15, 16, 17 [46] Ali Shahin Shamsabadi, Ricardo Sanchez-Matilla, and Andrea Cavallaro. Colorfool: Semantic adversarial colorization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11511160, 2020. 13 [47] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and Ben Zhao. Glaze: Protecting artists from style mimicry by text-to-image models. arXiv preprint arXiv:2302.04222, 2023. 1, 3 [48] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. 1 [49] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1 [50] Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, and Anh Tran. Anti-dreambooth: Protecting users from personalized text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21162127, 2023. 1, 3, 6, 14, 15, 16, [51] Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yuan Tan, and Quanxin Zhang. Demiguise attack: Crafting invisible semantic adversarial perturbations with perceptual similarity. arXiv preprint arXiv:2107.01396, 2021. 13 [52] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 14 10 [53] Haotian Xue, Chumeng Liang, Xiaoyu Wu, and Yongxin Chen. Toward effective protection against diffusion-based mimicry through score distillation. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 6, 14, 15, 17 [54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 1 [55] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5, 13, 14 [56] Zhengyu Zhao, Zhuoran Liu, and Martha Larson. Towards large yet imperceptible adversarial image perturbations with perceptual color distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10391048, 2020. [57] Zhengyue Zhao, Jinhao Duan, Xing Hu, Kaidi Xu, Chenan Wang, Rui Zhang, Zidong Du, Qi Guo, and Yunji Chen. Unlearnable examples for diffusion models: Protect data from unauthorized exploitation. arXiv preprint arXiv:2306.01902, 2023. 2 [58] Boyang Zheng, Chumeng Liang, Xiaoyu Wu, and Yan Liu. Understanding and improving adversarial attacks on latent diffusion model. arXiv preprint arXiv:2310.04687, 2023. 2, 3, 4, 5 11 A. Implementation Details Training. We set the number of perturbations in MoP to four. When calculating MLP loss in Eq. 5, we use intermediate features from [down 1, down 2, down 3, mid 0] layers in SD VAE, and set λ to 3.5 105. We train FastProtect on single A100 80GB GPU in an endto-end manner except the assignment function A. We use batch size of 16, using the Adam optimizer [22] for 40k steps with learning rate of 0.0002, and betas of (0.5, 0.99). When training, the required VRAM and compute time are around 50GB and 12 hours, respectively. Note that since we update the perturbations with Adam instead of the direct optimization through PGD, we remove the minus sign in the training loss (Eq. 5) as: LT (x) = zy2 2 + λ 2. We utilize three patterned target imL ages, each representing low, mid, and high pattern repetition, as illustrated in Figure 8. To implement adaptive targeted protection, for each protection strength (budget) η, we obtain three MoP models (low, mid, and high), each corresponding to one of the target images. The training algorithm is detailed in Algorithm 1; note that for simplicity, it is demonstrated with batch size of one, but increasing the batch size is straightforward. l=1 l y2 (cid:80)L Inference. When an input image with resolution other than 5122-px is given, we perform bilinear interpolation to the perturbations (δg, ) to match the resolution with the input images. However, the SD VAE encoder receives downsampled image to fixed 5122-px to prevent significant increase in computational load during VAE encoding. Although relative low-resolution is given to VAE, empirically, this shows robust performance to some extent. For adaptive targeted protection, we pre-compute and cache the average entropy of the target images. When an input image is given, we calculate its entropy and select the nearest target image based on the cached values. Hence, adaptive targeted protection incurs minimal overhead since FastProtect already obtains in the MoP assignment stage. For the adaptive protection strength, we provide the surrogate protected image to LPIPS with its original resolution, since the LPIPS network (AlexNet) is significantly smaller than the VAE encoder thus it consumes negligible computational overhead. When obtaining the final perceptual map in Eq. 7, we first apply min-max normalization to the spatial distance map calculated by LPIPS and then subtract it from one to reverse it. Subsequently, we adjust scale of the perceptual map with scale function S(). With given perceptual map M, which has resolution of , we first initialize scaled perceptual map as one, i.e., = 1HW . Then, we calculate the final perceptual map as follows: M[M < qi] = (cid:40) βi α if < βi if , (8) 12 Low repetition Mid repetition High repetition Figure 8. Examples of target images used by FastProtect. In our target image analysis (Figure 3), we used both low and high repetition target images. where qi = decile(M, i). The function decile(M, i) computes the i-th decile value of (from the highest value). This scaling function is designed to perform stepwise scaling to the perceptual map. For each region corresponding to specific decile, we downscale by factor of β. Without this scaling, many regions would take very small perturbations, significantly reducing the overall protection efficacy. For the areas up to the c-th decile, we additionally multiply by α to increase the perturbation intensity in the least noticeable regions. This compensates for the overall perturbation magnitude lost in more noticeable regions. In our work, we use (α, β, c) = (1.3, 0.91, 3). The inference algorithm is detailed in Algorithm 2. B. Related Work Our protection method involves injecting adversarial perturbations into input images, method extensively studied in the adversarial attack domain. In this section, we discuss two areas closely related to our motivation. Universal Adversarial Perturbation (UAP). MoosaviDezfooli et al. [35] demonstrated that single perturbation could be used to attack various images, in contrast to the traditional image-specific perturbations. This approach is significantly more efficient in terms of computation time compared to per-instance adversarial attacks because it does not require iterative optimization for each individual image. Subsequently, Hayes and Danezis [14] presented method for generating UAPs using generative models, showing improved efficiency and effectiveness over previous methods. Similarly, Mopuri et al. [37] utilized generative networks to create adversarial examples. In addition, there have been various attempts to find UAPs tailored to the image recognition tasks [5, 29, 36, 38]. However, these methods are not aligned with and cannot be adapted to the image protection task. Instead, in our study, we designed MoP with focus on creating protection framework that is both fast and maintains high performance. Our approach is specialized for image protection, differing from the general UAPrelated methodologies introduced in adversarial attack field. Invisible Adversarial Perturbation. Achieving high inAlgorithm 1: Training stage of FastProtect (Single-batch example) Input: Training dataset XD, target images {yl, ym, yh}, # protection perturbations K, # VAE encoder layers L, Perturbation budget (strength) η, Training steps Output: Assignment function A, mixture-of-perturbation {δg, } /* Compute assignment function prior to train MoP */ K-means++(E(XD), K) /* MoP training */ Initialize {δg, } 0 y, zl for 1 to do E(yl, L); zm , E(ym, L); zh , E(yh, L) // Prepare target images sample-batch(XD); E(x) A(z); arg mini{l,m,h} H(z) H(zi y) // Refer to Eq. 4 and 6 + δt + z, E(x, L) // Multi-layer protection loss (Eq. 5) 2 + λ LT = zt {δt {δt (cid:80)L y2 k} Adam({δt k} Clamp({δt g, g, l=1 t,l t,l g,t g, k}LT ) k}, η 2 , η 2 ) 2 2 end Algorithm 2: Inference stage of FastProtect Input: Image Output: Protected image ˆx H, size(x) E(resize(x; (512, 512))) /* MoP with adaptive targeted protection */ A(z) arg mini{l,m,h} H(z) H(zi + δ resize(δt k; (H, )) /* Adaptive protection strength */ LPIPS(x, + δ) S(1 M) ˆx = + δ // Refer to Eq. 8 // Use surrogate protected image // Utilize pre-computed target entropy y)1 // Match MoPs resolution to input image Table 6. Quantitative report on the effect of adaptive targeted protection shown in Figure 7a. Target Image Low rep. Mid rep. High rep. Adaptive target Object 200.1 207.2 208.3 208. Inference Domains Face 327.7 297.9 270.7 320.2 Painting Cartoon 237.3 234.6 211.8 235.4 347.5 349.3 348.5 349.4 tions [8]. Various studies have targeted specific elements such as low-frequency components [13, 31] and have utilized advanced constraints like color components [46, 56] or quality assessments [51]. Additionally, Luo et al. [30] explored techniques for creating invisible and robust adversarial examples based on the human visual system. More recently, Laidlaw et al. [25] investigated perceptual adversarial robustness and adopted LPIPS [55] in adversarial perturbation optimization. visibility in adversarial attacks has garnered significant research interest. Several methods focus on restricting the regions of perturbations. Some approaches use the L0 norm to generate sparse perturbations [7, 34], while others confine perturbations to small, salient areas to reduce visual distorAlthough invisibility has been highly emphasized in adversarial attacks, it is less explored in the image protection task. Many methods enhance invisibility implicitly by minimizing the budget used, but this often leads to tradeoff with protection efficacy, resulting in decreased perfor13 Table 7. Quantitative comparison when fix the perturbation strength (η) to eight for all the protection frameworks. Method (η = 8) AdvDM [28] PhotoGuard [45] Anti-DB [50] Mist [27] Impasto [2] SDST [53] FastProtect 370s / Latency CPU / GPU 1210s / 35s 7s 7278s / 225s 1440s / 40s 830s / 19s 1410s / 24s 2.9s / 0.04s Object Face Painting Cartoon Invisibility: DISTS () / Efficacy: FID () 0.129 / 199.5 0.184 / 211.9 0.164 / 197.4 0.185 / 217.2 0.131 / 188.9 0.170 / 198.0 0.097 / 200. 0.152 / 303.7 0.258 / 371.1 0.193 / 317.4 0.259 / 365.8 0.198 / 298.4 0.244 / 302.1 0.149 / 308.9 0.117 / 346.8 0.165 / 377.4 0.143 / 343.9 0.166 / 386.2 0.123 / 352.4 0.152 / 354.1 0.048 / 348.0 0.194 / 196.5 0.221 / 227.6 0.231 / 184.6 0.223 / 223.7 0.179 / 201.1 0.199 / 196.7 0.186 / 220.3 mance. Ahn et al. [2] constrain perturbations based on the human visual system using various modules during the optimization process. While effective, they require many adhoc components. In contrast, FastProtect uses LPIPS to create perceptual maps and apply masking, which is simpler and more streamlined approach. This differs from Zhang et al. [55], who use LPIPS in the optimization process. C. Experimental Setups Datasets. We utilize the Object, Face, Painting, and Cartoon domains in both the training and benchmark datasets. When constructing the training dataset, we randomly sample 20k images from ImageNet [9] for the Object domain, resizing all images to 5122. No augmentation is applied. For the Face domain, we use 20k images randomly sampled from the FFHQ dataset [19]. The images, originally 10242 and face-aligned [20], are all downscaled to 5122, without further augmentations. The Painting dataset is created by randomly sampling 20k images from the WikiArt dataset [44], resizing them to 5122. We filter out images with resolutions outside the range of 512 to 2048-px. For the Cartoon dataset, we use Webtoon artworks published on NAVER Webtoon, sampling 20k images only the works with permission from the original creators for research purposes. Similar to the Painting dataset, we apply resolution filtering and resize the images to 5122. To make the benchmark dataset, we select 20 objects from the personalization subject dataset proposed by Ruiz et al. [43] for the Object domain. Each object consists of 5-6 images. When we conduct the main comparison  (Table 2)  , we use images resized to 5122, and for the arbitrary image analysis  (Table 4)  , we apply protection to the original resolution images. For the Face domain, we followe Van Le et al. [50] and randomly sample 20 identities from VGGFace2 [4], each identity consisting of 12 images resized to 5122. The Painting dataset is compiled by randomly sampling (but not overlap to the training dataset) 20 artists from the WikiArt dataset, each artist represented by 10 artworks, all resized to 5122. For the Cartoon domain, we randomly sample 20 Webtoon works (but also no overlap to the training dataset) from the Webtoon dataset. In this dataset, we only use facial images of major cartoon characters by cropping and aligning all the images. This reflects the common practice in cartoons and illustrations where characters are the main focus to both readers and artists. In the main comparison we use 5122 images, while the arbitrary analysis maintains the original resolution. The cartoon images used in the benchmark dataset are also permitted by the artists for research only purpose. The list of cartoon works featured in this paper is as follows: <Yumis Cells>, <Maru is Puppy>, <Free Draw>, <The Shape of Nightmare>, <See You in My 19th Life>, <Lookism>, and <Love Revolution>. Baselines. FastProtect is compared with existing diffusionbased mimicry protection frameworks: AdvDM [28], PhotoGuard [45], Anti-DreamBooth (Anti-DB) [50], Mist [27], Impasto [2], and SDST [53]. We follow the official settings for all the baselines by using their official codes. However, for PhotoGuard, we use the target image of Mist [27] by following the protocols of Xue et al. [53] while we use the Impastos target image for the Impasto. Evaluation. For the primary evaluation assessment, we use DISTS [10] to measure the invisibility of the protected image and FID [15] to evaluate the protection efficacy of the personalized diffusion methods. In addition, in here, we also compare the protection frameworks using other metrics: To evaluate invisibility, we include LPIPSVGG [55] and AHIQ [26]. Note that LPIPSVGG measures the perceptual distance in the VGG feature space, which is different from the feature space used in our perceptual map creation module (i.e. AlexNet). For evaluating protection efficacy, we use TOPIQ-NR [6] and QAlign [52]. When preparing mimicry outputs, we personalize the diffusion models using the protected image with LoRA [17] by default, employing default settings for all the personalization methods. To generate outputs, we use different inference prompts than those used during training to simulate black-box caption scenarios [50]. For example, we train the diffusion models with LoRA using captions of painting in <sks> style prompt and perform inference with painting of house in <sks> style. 14 Table 8. Additional quantitative comparison on the Subject domain. Domain: Subject AdvDM [28] PhotoGuard [45] Mist [27] SDST [53] Anti-DB [50] Impasto [2] FastProtect DISTS () 0.197 0.203 0.185 0.242 0.239 0.201 0.155 Invisibility LPIPSVGG () 0.362 0.347 0.322 0.402 0.413 0.378 0. AHIQ () 0.540 0.575 0.578 0.575 0.510 0.588 0.583 FID () 220.0 223.0 217.2 219.2 214.4 213.8 223.0 Protection Efficacy TOPIQ-NR () 0.458 0.506 0.523 0.542 0.439 0.473 0.507 QAlign () 2.139 2.396 2.328 2.442 2.148 2.183 2.364 Table 9. Additional quantitative comparison on the Face domain. Domain: Face AdvDM [28] PhotoGuard [45] Mist [27] SDST [53] Anti-DB [50] Impasto [2] FastProtect DISTS () 0.173 0.189 0.154 0.244 0.162 0.198 0.149 Invisibility LPIPSVGG () 0.338 0.357 0.310 0.431 0.309 0.388 0.280 AHIQ () 0.528 0.557 0.560 0.561 0.543 0.562 0.566 FID () 303.8 308.7 307.5 302.1 301.4 298.4 308.9 Protection Efficacy TOPIQ-NR () 0.466 0.630 0.667 0.629 0.484 0.565 0. QAlign () 2.493 3.113 3.131 3.019 2.411 2.939 2.851 D. Discussions Arbitrary Resolution. As shown in Table 4, FastProtect can effectively handle arbitrary resolution images simply by resizing the MoP to match the resolution of an input image. In contrast, the baseline method, PhotoGuard [45], suffers much greater performance drop compared to ours. There are two main factors that can impact protection performance for arbitrary resolution scenarios. The first factor is the need to adjust the size of the perturbation to match the size of the image when injecting the perturbation. In theory, iterative optimization methods do not face this issue. However, in practice, due to the VRAM constraint of GPUs, images beyond (typically 10242-px in A100) certain size must be split and protected in segments. On the other hand, our model does not have memory issues, but since the pretrained MoP is fixed as 5122-px, it requires resizing to fit an input image. The second factor is the necessity to downsize the protected image due to resolution requirements that each diffusion models has (e.g., 5122 for SD v1, 7682 for SD v2, or 10242 for SD-XL). Our method appears to be affected by the information loss due to the two resize operations (one when applying MoP and the other during personalization). Of course, we observed that all protection frameworks suffer some degree of protection efficacy reduction during personalization, and our model is no exception (e.g., FID: 220.3 to 204.0). Such reduction is due to the fine perturbations being lost during the downsize process (to match the resolution that of the diffusion models require). In contrast, although PhotoGuard performs optimization in full-resolution, its robustness in this aspect seems limited. We suspect that the severe degradation occurs since the need to split the high-resolution images and more importantly the tendency of PhotoGuard to create very fine perturbations specific to each image make it particularly vulnerable to downsizing. On the other hand, since our model learns perturbations that work well on average across the training data, the perturbations tend to be more coarse. However, creating more practically robust model requires further investigation and future work to fully reveal these aspects. In FastProtect, Assignment Function. the assignment function groups images as shown in Figure 6c. We observed that each perturbation has characteristic groupings of images. For example, the first perturbation (top left in Figure 6c) predominantly groups images of faces, portraits, or close-up shots of objects, with notably dark background. The second perturbation (top right) includes images with moderate scene complexity or texture. The third perturbation (bottom left) selects the simplest images in the dataset, particularly cartoons or images with simple objects and minimal backgrounds in the Subject domain. Finally, the fourth perturbation (bottom right) groups the most complex textured images. For instance, in the subject domain, images with detailed textures like grassy backgrounds are mostly selected here, and in the face, cartoon, and painting domains, images with complex and detailed backgrounds or scenes are also grouped in this category. Adaptive Targeted Protection. Table 6 shows quantita15 istic makes it very challenging to achieve high level of invisibility beyond certain point. Moreover, common phenomenon across all models is that, even when the same budget of perturbation is applied to all images, the degree of protection varies from image to image. Interestingly, if specific model fails to protect certain image, other models also tend to struggle with it. This issue likely stems from the fact that all frameworks rely on texture or semantic losses (or both), thus they all share the same vulnerability. potential solution to this problem is to measure how difficult an image is to protect in advance and dynamically adjust the budget accordingly. However, practically, it is challenging to determine the difficulty of protection before the personalization. Therefore, predicting the protectability of input images and adjusting the intensity of perturbations adaptively is worth try research direction. The another advantage of this approach is that easy-to-protect images can be given lower perturbation strength, naturally enhancing invisibility. E. Additional Results Figure 11 and 12 show additional qualitative comparisons of existing protection frameworks. In Table 8, 9, 10, and 11, we evaluate the protection frameworks using the additional metrics. In these comparisons, our model demonstrates better invisibility while showing average protection efficacy; similar protection performance with the texture loss model group. Notably, models that primarily utilize texture loss (e.g., PhotoGuard, Mist, SDST, Impasto, and ours; although Mist and SDST utilize both losses, we observed that texture loss has much higher influence) tend to exhibit relatively high invisibility. Conversely, AdvDM and Anti-DreamBooth, which use semantic loss, show higher protection efficacy in most of the protection efficacy measures. This phenomenon highlights that the different losses exhibit varying trends. Understanding why these losses perform differently across metrics will be crucial for future advancements in protection loss design. In Figure 9, we compare the latency versus input image resolution on both CPU and GPU. On the CPU (Apple silicon), FastProtect completes the protection in only 4 seconds, while all other methods require significantly more time; 15 minutes for PhotoGuard [45] and 10 hours for Anti-DreamBooth [50]. We also include the protection efficacy versus invisibility trade-off comparison for both the Object and Cartoon domains in Figure 10. FastProtect demonstrates an improved trade-off curve in both domains. tive report of Figure 7a. Similarly, the performance in each test domain varies according to the pattern repetition of the target image. For example, high repetition target images perform well in the Object domain but poorly in the Face and Cartoon domains. This is because the Object domain mostly contains images with dense textures and complex scenes, which match well with the frequency of high repetition target images. Conversely, the Cartoon domain, which typically has very flat textures, does not benefit from high repetition target images. For the Face domain, even though it is similar to the Object domain in being natural photos, the backgrounds are blurred or monochromatic, and the textures of faces are frequently flat, differing significantly from objects. For the low repetition target images, the opposite trend is observed. Performance decreases in the Object domain but improves in the Face and Cartoon. In the Painting domain, performance remains consistent across all target images. This is because the artworks in the Painting domain exhibit wide variety of textures and complexities, leading to an average performance across different target images. For instance, the Painting domain dataset includes diverse images ranging from Van Gogh-style oil paintings to black and white sketch drawings. Comparison at the Same Protection Strength. In the main paper, we compare FastProtect with other protection frameworks by adjusting the protection strength (η) to match the protection level across methods for fairer comparison  (Table 2)  . The reason for this evaluation setup is that we observed different protection losses produce varying protection-invisibility trade-offs. Therefore, protection efficacy alone cannot be used to judge the superiority of model. In Figure 10, we report the results by varying the protection strength and analyzing protection efficacy vs. invisibility. However, such benchmark requires significant computational resources, thus we adjust the protection strength to align the trade-off line for comparison. Still, one might wonder about the performance when the protection strength is fixed for all methods. To answer this possible inquiry, we conduct comparison with all protection frameworks by fixing the protection strength η at 8, as shown in Table 7. In most cases, FastProtect demonstrates outstanding results in terms of invisibility while achieving moderate protection efficacy. This balance suggests that FastProtect performs exceptionally well when considering the protection-invisibility trade-off. In the Cartoon domain, Impasto [2] shows better results in invisibility, but our method significantly outperforms Impasto in efficacy, which indicates favorable trade-off trend. Limitation & Future Directions. As discussed in Section 5, all protection solutions, including our model, still need improvements in terms of invisibility. However, as analyzed by Ahn et al. [2], to prevent mimicry, perturbations must be applied broadly across the image. This character16 Table 10. Additional quantitative comparison on the Painting domain. Domain: Painting AdvDM [28] PhotoGuard [45] Mist [27] SDST [53] Anti-DB [50] Impasto [2] FastProtect DISTS () 0.153 0.107 0.129 0.152 0.114 0.123 0.110 Invisibility LPIPSVGG () 0.267 0.189 0.219 0.258 0.184 0.221 0. AHIQ () 0.572 0.579 0.579 0.583 0.579 0.579 0.577 FID () 357.6 350.9 357.0 354.1 347.7 352.4 356.1 Protection Efficacy TOPIQ-NR () 0.438 0.613 0.615 0.637 0.452 0.564 0.506 QAlign () 2.889 3.228 3.218 3.288 2.860 3.141 2.987 Table 11. Additional quantitative comparison on the Cartoon domain. Domain: Cartoon AdvDM [28] PhotoGuard [45] Mist [27] SDST [53] Anti-DB [50] Impasto [2] FastProtect DISTS () 0.271 0.209 0.223 0.237 0.294 0.207 0.186 Invisibility LPIPSVGG () 0.440 0.348 0.365 0.398 0.468 0.376 0.301 AHIQ () 0.445 0.587 0.585 0.589 0.445 0.602 0.585 FID () 212.5 219.1 223.7 222.7 225.4 215.5 220.3 Protection Efficacy TOPIQ-NR () 0.579 0.683 0.681 0.712 0.429 0.590 0. QAlign () 2.247 2.618 2.645 2.755 2.166 2.294 2.530 (a) On CPU (Apple M1 Max) (b) On GPU (NVIDIA A100) Figure 9. Inference latency (log-scaled) vs. image size on both CPU and GPU environments. (a) Object domain (b) Cartoon domain Figure 10. Protection efficacy vs. invisibility comparison on the Object and Cartoon domains. 17 Figure 11. Additional qualitative comparison. For each example, (top) protected image and (bottom) mimicry image generated by LoRA. 18 Figure 12. Additional qualitative comparison. For each example, (top) protected image and (bottom) mimicry image generated by LoRA."
        }
    ],
    "affiliations": [
        "Inha University",
        "KRAFTON",
        "NAVER WEBTOON AI"
    ]
}