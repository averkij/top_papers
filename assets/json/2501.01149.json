{
    "paper_title": "A3: Android Agent Arena for Mobile GUI Agents",
    "authors": [
        "Yuxiang Chai",
        "Hanhao Li",
        "Jiayu Zhang",
        "Liang Liu",
        "Guozhi Wang",
        "Shuai Ren",
        "Siyuan Huang",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, a subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide a comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), a novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) a larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing a robust foundation for evaluating mobile GUI agents in real-world situations and a new autonomous evaluation process for less human labor and coding expertise. The project is available at \\url{https://yuxiangchai.github.io/Android-Agent-Arena/}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 9 4 1 1 0 . 1 0 5 2 : r A3: Android Agent Arena for Mobile GUI Agents Yuxiang Chai MMLab @ CUHK Hanhao Li EE department @ CUHK Jiayu Zhang EE department @ CUHK"
        },
        {
            "title": "Siyuan Huang\nShanghai Jiao Tong University",
            "content": "Hongsheng Li* MMLab @ CUHK"
        },
        {
            "title": "Abstract",
            "content": "AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing robust foundation for evaluating mobile GUI agents in real-world situations and new autonomous evaluation process for less human labor and coding expertise. The project is available at https://yuxiangchai. github.io/Android-Agent-Arena/."
        },
        {
            "title": "Introduction",
            "content": "The significant advancement in the Large Language Model (LLM) area has boosted the development of AI agents, which are able to autonomously complete tasks following the instructions from users. Existing mobile AI assistants such as Siri, Xiao AI, and Bixby have demonstrated the potential of mobile agents to facilitate interactions between human users and mobile devices. However, those assistants are only effective in managing the routine tasks such as reporting weather condition and *Corresponding Author 1 performing web searches due to the nature that they use APIs to perform task automation. To broaden the capability of AI agents, researchers have proposed GUI agents, which leverage the extended world knowledge and robust capabilities of multimodal large language models (MLLM) to effectively complete tasks on third-party general apps without the reliance on the APIs. Despite the promising advancements in GUI agents, the majority of existing GUI-control datasets (Rawles et al., 2024b; Chai et al., 2024; Li et al., 2024) and primarily focus on static frame evaluations, which have significant limitations. These datasets typically provide collection of screenshots or UI states, requiring agents to predict the next action based on single, frozen frame. Such an approach fails to capture the dynamic and interactive nature of real-world mobile tasks, where agents must navigate through sequences of actions, adapt to changing app states, and handle unexpected outcomes. Furthermore, static frame evaluations often lack contextual information about task flows, making it difficult to assess an agents ability to perform multi-step or goal-oriented tasks. This disconnect between static frame evaluations and real-world usage scenarios results in gap between the capabilities of current GUI agents and the demands of practical applications, underscoring the need for more comprehensive and interactive evaluation platform. Several recent works (Rawles et al., 2024a; Xing et al., 2024; Xu et al., 2024; Lee et al., 2024; Zhang et al., 2023b) have introduced dynamic evaluation platforms for Android GUI agents. While these efforts represent significant progress, they suffer from critical limitations that hinder their effectiveness as comprehensive evaluation benchmarks. For instance, many platforms restrict app selection to Google apps, F-Droid apps (non-mainstream opensource apps), or static offline apps, which do not reflect the diversity or complexity of real-world Figure 1: Overview of Android Agent Arena. A3 contains over 200 tasks from 21 widely used apps. Tasks are categorized into operation, single-frame query and multi-frame query based on the task goal. Tasks are also split into three difficulty levels based on the human operation steps. A3 also integrates two evaluation methods for different use cases. usage. Additionally, these platforms often provide only limited diversity of tasks, or fail to include information query tasks, which are essential for evaluating practical agent performance. To address these shortcomings, we propose Android Agent Arena (A3), novel evaluation system that offers: (i) integration with 21 widely used third-party apps and 201 tasks designed around real-world app functionalities, (ii) diverse set of tasks categorized into three distinct types, and (iii) larger action space, enabling compatibility with agents trained on any dataset. Furthermore, we introduce new evaluation method that leverages the capabilities of business-level large language models (LLMs) to automate task evaluation, significantly reducing the need for human intervention and manual coding. Table 1 demonstrates the overview of A3. Our contributions can be summarized as follows: We introduce Android Agent Arena (A3), comprehensive evaluation platform that integrates 201 tasks across 21 mainstream apps from real-world scenarios, with support for an expanded action space compatible with any dataset annotation style. We propose novel evaluation approach leveraging business-level LLMs, introducing two applications that enable scalable and automated evaluation processes while greatly reducing the need for manual effort in task evaluation."
        },
        {
            "title": "2.1 GUI Agent",
            "content": "(Wang et al., 2023) applies large language models (LLMs) to GUI tasks, but their focus remains limited to single-page interactions, resembling question-answering tasks rather than fully end-toend instructional tasks. Recent advancements have begun leveraging the extensive world knowledge and robust reasoning capabilities of LLMs (Gu and Dao, 2024; Gur et al., 2023; Touvron et al., 2023) for task planning and execution. notable approach involves using general-purpose businesslevel models, such as GPT-4v, as GUI-control agents. Studies like (Zhang et al., 2023a; Zheng et al., 2024) employ extensive prompt engineering to guide these models in performing complex tasks. However, the effectiveness of such methods depends heavily on meticulous prompt design to achieve satisfactory results. An alternative research direction focuses on fine-tuning smaller LLMs using GUI-specific datasets to imbue them with domain-specific knowledge, thereby improving their efficiency and task performance. For instance, CogAgent (Hong et al., 2024) enhances GUI task performance by incorporating high-resolution cross-module that fuses image features at multiple levels. Similarly, MobileAgent (Ding, 2024) optimizes input data structuring and sample handling to make it more consistent and compatible with LLMs. SphAgent (Chai et al., 2024) utilizes the element functionalities to further enhance the screen and element understanding. CoCo-Agent (Ma et al., 2024) and ANDROIDCONTROL (Li et al., 2024) take different approach by integrating element layouts from accessibility trees or view hierarchies as additional inputs, rather than relying solely on screenshots. While this approach enhances performance, it faces limitations as many apps either lack accessibility information or provide only minimal accessibility data, which restricts its applicability 2 Name Eval Mode # Tasks # General Apps Operation Inf. Query AITW ANDROIDCONTROL AMEX AndroidArena Mobile-Env AndroidWorld B-Moca AndroidLab A3 (Our) static static static dynamic dynamic dynamic dynamic dynamic dynamic - - - 221 74 116 131 138 201 - - - 4 5 15 4 5 21 Table 1: GUI related datasets and benchmarks. 15 general apps of AndroidWorld are from F-Droid. in real-world scenarios. complexities of real-world task execution. 2.2 GUI-related Dataset The introduction of the Rico dataset series (Deka et al., 2017; Sunkara et al., 2022) marked significant milestone in GUI-related research by providing foundational datasets for GUI element classification and detection. Subsequent works (Burns et al., 2021; Gubbi Venkatesh et al., 2024) introduced small-scale, instruction-based GUI control datasets. Among these, UGIF (Gubbi Venkatesh et al., 2024) stands out as multilingual dataset supporting eight languages. AITW(Rawles et al., 2024b) expanded the field with large-scale dataset, but it suffered from significant redundancy in instructions and frequent mis-annotations. To address this, AITZ(Zhang et al., 2024) refined AITW by applying Chain-of-Action-Thought reannotation, resulting in cleaner but much smaller dataset. ANDROIDCONTROL(Li et al., 2024) further introduced large-scale dataset, albeit with simpler tasks and distinct action space compared to AITW and AITZ. Meanwhile, AMEX(Chai et al., 2024) redefined GUI element annotation by incorporating element functionality, enabling agents to better interpret mobile GUI designs, and pushed the boundaries with more complex tasks than prior datasets. However, despite their contributions, these datasets are limited to static frame evaluations, where agents predict actions based on single screenshot, an instruction, and ground-truth history of actions. This approach fails to capture the dynamic and interactive nature of real-world scenarios, where historical actions are unavailable, and single error can cascade and severely impact subsequent performance. This highlights the need for evaluation systems that better reflect the 2.3 Dynamic Evaluation Benchmark To overcome the limitations of static frame evaluations, researchers have developed several dynamic evaluation systems (Lee et al., 2024; Xu et al., 2024; Xing et al., 2024; Rawles et al., 2024a; Zhang et al., 2023b) that aim to better simulate real-world environments (See Table ??). For instance, MobileEnv (Zhang et al., 2023b) incorporates broader range of general apps but is restricted to only 74 tasks. AndroidArena (Xing et al., 2024) introduces larger number of tasks, including cross-app tasks, but is limited to Google apps and built-in system apps (e.g., settings and clock), which are already manageable by API-based assistants. B-Moca (Lee et al., 2024) supports Korean language setting but offers tasks that are overly simplistic and lack diversity. AndroidWorld (Rawles et al., 2024a) uses open-source apps from F-Droid1, but these apps differ significantly from mainstream app designs, making them unrepresentative of real-world scenarios. Notably, all of these systems focus only on operational instructions and their corresponding evaluations. AndroidLab (Xu et al., 2024) is the first system to incorporate information query instructions and evaluations, addressing key gap. However, its app selection is limited to offline and static apps, failing to include categories such as news, shopping, email, and music, which are critical for real-world usability. To specify, the evaluation methods in existing systems predominantly rely on element matching (Lee et al., 2024) or predefined answers (Xu et al., 2024). 1https://f-droid.org/en/packages/ 3 Figure 2: Overview of Android Agent Arena. A3 contains controller, evaluator, and translator. The controller is responsible for controlling and getting the state of the device. The translator is responsible for translating the device function and the agent messages. The evaluator is responsible for the final evaluation."
        },
        {
            "title": "3.1 Overview",
            "content": "A3 is lightweight system built on Appium2, an open-source framework for controlling Android and iOS devices. As shown in Figure 2, A3 acts as bridge between GUI agent and an Android device. It integrates tasks with their corresponding evaluation functions. The process begins with the controller retrieving the devices current state, which includes screenshot and an Extensible Markup Language (XML) file. This state and task instruction, along with additional information such as previous screenshots, XML files, and actions, is then sent to the agent. The agent analyzes the input and predicts the next action to take based on the current state. The predicted action is passed to the translator, which converts it into device control commands to interact with the device. This loop continues until the agent signals task completion or the predefined maximum number of steps is reached. At the end of the process, the evaluator determines whether the task was successfully completed using the evaluation function. The system is designed with flexibility and extensibility, allowing users to easily add new apps and tasks, and it also provides universal translator system for any agent. 2https://appium.io/docs/en/latest/ AITW, AITZ, and AMEX share the same action space: CLICK, SCROLL, TYPE, ENTER, BACK, HOME, COMPLETE, IMPOSSIBLE. In contrast, ANDROIDCONTROL introduces different action space that includes two additional actions: Open, Long Press and WAIT. The Open action is specifically defined to directly launch an app and the WAIT action means the current state is still loading and needs to wait. However, no existing evaluation system currently supports these additional actions, making it impossible to test agents trained on ANDROIDCONTROL. To address this limitation, we extend A3 to accommodate larger action space, which contains all the action types of all datasets, ensuring compatibility with agents trained on any dataset."
        },
        {
            "title": "3.3 Task",
            "content": "Unlike existing approaches, A3 incorporates over 200 tasks derived from 21 widely used third-party applications, thereby significantly broadening the scope and variety of real-world scenarios. Each task is deliberately chosen to represent the most common functionalities and use cases of given application. Moreover, every task is distinct, minimizing the repetition of actions and intentions. To better characterize the types of tasks included, we classify them into three categories: (i) operation tasks, (ii) single-frame query tasks, and (iii) multiframe query tasks. 4 tain and manipulate information across multiple interactions, rather than relying solely on the final state. We divide all tasks into three tiers of difficulty. Tasks that human can complete in four or fewer steps are considered easy, and those achievable in eight or fewer steps are deemed medium. All remaining operation tasks are classified as hard. Figure 3 illustrates the distribution of tasks in A3. Due to the fact that multi-frame query tasks are extremely hard for existing agents, we devote more effort in operation and single-frame query tasks for better evaluation of agents capabilities. Moreover, task instructions related to dates are dynamically generated to ensure the date can be selected, such as in booking app. 3.4 Evaluation In A3, we present two evaluation methods: (i) task-specific evaluation function and (ii) businesslevel LLM evaluation system. These methods operate independently and can be chosen by users, with the first focusing on predefined tasks and the second offering scalable solution for adding tasks across various apps. And to mimic the real-world scenario, all tasks are evaluated by real-time states, which means all the contents are real-time, not predefined offline contents."
        },
        {
            "title": "3.4.1 Evaluation Function\nFor over 200 tasks, we pair each task with a corre-\nsponding evaluation function. This function is used\nto assess whether the agent successfully completes\nthe given task through various methods. Since each\ntask involves different actions and goals, the eval-\nuation criteria vary accordingly. The evaluation\nmethods can be broadly categorized into two types:",
            "content": "Element matching is the most commonly used evaluation method. It involves identifying key elements in an XML tree and comparing their attributes with ground-truth values. For instance, consider the task: \"Open Downloads in Coursera and tell me how much storage is used.\" In this case, the final state XML should contain an element that displays the total storage used by the app. The ground-truth value can be extracted by parsing the XML tree and then compared to the agents response. In more complex scenarios, multiple elements may need to be retrieved, and several conditions must be met to determine if the agents answer is correct. Additionally, when the XML data is insufficient, Figure 3: Distribution of tasks in A3. The above subfigure is the distribution by categories and the bottom subfigure is the distribution by difficulty levels. Operation tasks involve completing an action sequence on the device. For example, the instruction Search for Taylor Swift on YouTube Music and subscribe requires the agent to execute specific action sequence. Such tasks are common in daily life, such as setting reminder or playing music. Single-frame query tasks prompt the agent to return piece of information after completing the requested actions. For instance, given the instruction Search for stays in Beijing from Dec 27 to Dec 28, sort them by price from low to high, and provide the lowest price, the agent must identify the lowest price on the final state and present it as the answer. These tasks mirror common realworld queries, such as finding restaurants contact information or hotels nightly rate. Multi-frame query tasks are more complex, requiring the agent to gather and process information across multiple steps before responding. For example, the task Search for one-night stay in one-bed room at Hilton Garden Inn Hong Kong for next week. Identify the cheapest day and its corresponding price demands that the agent aggregate data from several days, compare prices, and then select the optimal result. Unlike single-frame queries, multi-frame tasks require the agent to reLLM Correct Func. Wrong Line task evaluation. GPT-4o 24% 27% Table 2: The capability of GPT-4o to directly generate evaluation function. Correct Func. represents the percentage of correct files over all generated files. Wrong Line represents the percentage of incorrect lines over all code generated. The evaluation is collected from coding experts. LLM Eval Correct GPT-4o Gemini 1.5 Pro 84% 80% Table 3: The correctness of LLM evaluation by human validation from 50 tasks. Eval Correct represents the correctness of LLM evaluation results determined by human. OCR (Optical Character Recognition) is used to extract text attributes directly from an element, serving as substitute for XML parsing. Action matching is used when evaluation requires verifying specific positions. For example, in the task: \"Search for flashlight on the Wish app, filter results by price under 100, then select the first item and add it to the wishlist,\" the agent must click on the first item displayed in the search results. Action matching ensures that the click coordinates fall within the bounding box of the first item. In more complex scenarios, both element matching and action matching are combined to accurately evaluate the agents performance."
        },
        {
            "title": "3.4.2 LLM Evaluation System",
            "content": "One of the challenges faced by previous online evaluation systems is the difficulty of scaling tasks alongside their corresponding evaluation functions. During the development of A3, we also encountered this issue. Each task requires specific evaluation function, which must be crafted by coding experts capable of parsing XML and defining precise conditions for task success. This dependency on manual coding hinders the rapid creation of evaluation functions. To address this limitation, we propose business-level LLM evaluation system that leverages the advanced capabilities of large language models (LLMs) such as GPT and Gemini to enable semi-autonomous or even fully autonomous As first step, we utilize GPT-4os coding abilities to generate evaluation functions for tasks. Specifically, we supply GPT with our XML parsing code, an example evaluation function, and taskspecific prompts to generate the required evaluation functions. While GPT-4o demonstrates strong coding capabilities, it can occasionally produce logic errors or incorrect conditions. For instance, when evaluating hotel searches on Booking.com, the search results dynamically change based on the selected date. However, GPT-4o might incorrectly attempt to match static value, such as \"113 results,\" instead of using regex pattern containing the word \"results\" to identify whether the searching process is finished. To quantify GPT-4os performance, Table 2 illustrates its impact on task evaluation. Although the proportion of fully correct functions is relatively low, the percentage of lines requiring human modification is also minimal. This suggests that GPT-4o significantly reduces the coding workload, enabling more efficient and semi-autonomous evaluation process. The first evaluation method delivers accurate results due to human validation of the evaluation functions. However, it depends on coding experts, which can be constraint. To overcome this, we propose fully LLM-based evaluation process. For operation and single-frame query tasks, GPT-4o and Gemini 1.5 Pro are provided with task instructions and the corresponding final state XMLs, enabling them to directly assess task completion. For multi-frame query tasks, sequence of XMLs is used to evaluate their ability to handle more complex scenarios. Table 3 highlights the evaluation accuracy of both LLMs, demonstrating approximately 80% correctness. To improve reliability, we introduce cross-validation process that combines the outputs of both LLMs. When the two models produce the same result, the probability of misjudgment decreases to around 0.03, ensuring high confidence. In cases of disagreement, human evaluation is applied. This approach significantly reduces the reliance on human labor compared to the first method while also eliminating the need for coding expertise."
        },
        {
            "title": "4.1 Finetuned Agent",
            "content": "We train an agent based on InternVL2-8B (Chen et al., 2024) using both the AMEX and ANDROID6 Test Subset Test Level Succ. Rate Agent Test Level Succ. Rate IDD Category Unseen App Unseen Task Unseen High Low High Low High Low High Low 69.6 92.1 51.8 84.4 56.8 83.0 73.7 88.5 InternVL GPT-4o AppAgent Easy Medium Hard Easy Medium Hard Easy Medium Hard 23.4% 5.6% 2.0% 9.9% 1.4% 0.0% 30.8% 7.0% 2.0% Table 4: Static frame evaluation results on ANDROIDCONTROL test set. Table 5: Dynamic evaluation results on A3 by difficulty level. CONTROL datasets. The agent is then tested on the ANDROIDCONTROL static frame test set as well as the A3 dynamic arena. Table4 presents the evaluation results on the ANDROIDCONTROL test set, which is divided into four subsets: IDD, category unseen, app unseen, and task unseen. Each static frame evaluation is further categorized into two levels. The high-level evaluation provides only an overall task goal, such as Open the profile tab in YouTube, while the low-level evaluation includes additional step-wise instructions for the current screen, such as Click the profile icon at the bottom right of the screen. Naturally, low-level tasks are easier than high-level ones, as they offer more specific guidance. Although the agent performs relatively well on the ANDROIDCONTROL static test set, we observe poor performance in real-world scenarios, as shown in Table 5 and Table 6. Upon analyzing unsuccessful executions in A3, we identify that the history of actions might be significant factor. (i) Previous studies have demonstrated that action history improves performance in static frame evaluations, as it is derived from ground-truth annotations and provided as input to the agent, which informs subsequent processes. However, in A3, real-world scenarios lack ground-truth action histories, which is huge domain gap between training data and real-world evaluation. If the agent relies on its own action history, single mistake can confuse the agent and cascade into series of errors. (ii) In static evaluations, errors in action predictions do not affect subsequent states or the ground-truth history. However, in real-world evaluation, wrong action can lead to an incorrect state, which might provide misleading information and requires the Agent Test Category Succ. Rate InternVL2 GPT-4o"
        },
        {
            "title": "AppAgent",
            "content": "Operation Single-frame Query Multi-frame Query Operation Single-frame Query Multi-frame Query Operation Single-frame Query Multi-frame Query 17.1% 0.0% 0.0% 5.7% 2.0% 0.0% 20.0% 6.0% 0.0% Table 6: Dynamic evaluation results on A3 by task category. agent to self-correct, either by going back or starting over. Unfortunately, our trained agent currently lacks this self-correction capability. Another challenge is the impracticality of information query tasks. This issue arises because no existing dataset provides this type of data during collection, leading the agent to fail in answering any queries."
        },
        {
            "title": "4.2 Business-level LLM",
            "content": "We also employ the business-level LLM GPT-4o as an agent in A3. However, as highlighted in the previous Set-of-Mark (SoM) study (Yang et al., 2023), GPT-4o performs poorly when tasked with directly outputting or inputting coordinates. To address this, we apply the SoM technique to help identify element positions and coordinates. While SoM improves GPT-4os performance on CLICK actions, it continues to struggle with other actions, such as SCROLL. Despite these limitations, GPT-4o demonstrates the ability to complete some infor7 mation query tasks due to its inherent capabilities, even without fine-tuning. We also evaluate the performance of AppAgent (Zhang et al., 2023a), which includes an exploration phase before task automation and articulates its reasoning and planning during execution. These features notably enhance its effectiveness as an agent. AppAgent not only significantly outperforms the original GPT-4o but also surpasses our finetuned agent, likely due to the advanced capabilities of the GPT model underlying AppAgent. 4.3 Error Cases To better demonstrate the obstacles that agents encounter in the real-world scenario, we provide some most common error cases during the observation of the evaluation. Perform CLICK at wrong coordinate. This the most common error case where if the coordinate is wrong, either the screen doesnt change (click at nothing), or it goes to wrong state, which requires self-correct ability to get back on the track. This type of errors is mainly due to the insensibility of LLMs to digits. Perform meaningless action. This happens when an agent doesnt know what to do on the current state. For instance, it generates WAIT when the state is fully loaded and requires CLICK or SCROLL. This is possibly due to the weak planning capability of the agent. Start typing before the element is selected. We see many cases where the agent starts to type text when the search bar or other element is not clicked or selected. This is possibly due to the inconsistent annotation style in the existing datasets: sometime annotators click the element before typing while sometime they dont if the element is automatically selected. Cant stop. We also see some cases where the agent already finishes the task but it still performs more actions that ruin the process."
        },
        {
            "title": "5 Limitations",
            "content": "The integrated tasks and evaluation functions are defined on specific versions of apps, which may lead to different evaluation results on different app versions. Business-level LLMs such as GPT-4o and Gemini can only evaluate whether the whole task is complete, but cannot judge whether the subgoals are completely in the action chain. This remains to the future work to discover better way to autonomously evaluate the agent performance other than success rate."
        },
        {
            "title": "6 Discussion",
            "content": "All datasets are used consistently with the licenses, such as Apache 2.0 of both ANDROIDCONTROL and AMEX. No existing potential risk is shown during the evaluation."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce Android Agent Arena (A3), comprehensive and autonomous online evaluation system for GUI agents. A3 incorporates both humanvalidated task-evaluation pairs and an autonomous LLM-based cross-validation process. The tasks span wider range of categories and applications, enabling the evaluation of agents capabilities in both operation execution and information retrieval across three levels of difficulty. The autonomous evaluation process significantly minimizes human intervention and workload, offering more efficient approach to scaling the number of evaluation tasks."
        },
        {
            "title": "References",
            "content": "Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan Plummer. 2021. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments. arXiv preprint arXiv:2104.08560. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. 2024. Amex: Android multiannotation expo dataset for mobile gui agents. Preprint, arXiv:2407.17490. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. 2024. Internvl: Scaling up vision foundation models and aligning for In Proceedings of generic visual-linguistic tasks. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198. Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th Annual Symposium on User Interface Software and Technology, UIST 17. 8 Tinghe Ding. 2024. Mobileagent: enhancing mobile control via human-machine interaction and sop integration. arXiv preprint arXiv:2401.04124. Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling. Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. 2024. UGIF-DataSet: new dataset for cross-lingual, cross-modal sequential actions on the UI. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 13901399, Mexico City, Mexico. Association for Computational Linguistics. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2023. Understanding HTML with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 28032821, Singapore. Association for Computational Linguistics. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2024. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290. Juyong Lee, Taywon Min, Minyong An, Dongyoon Hahm, Haeone Lee, Changyeon Kim, and Kimin Lee. 2024. Benchmarking mobile device control agents across diverse configurations. arXiv preprint arXiv:2404.16660. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024. On the effects of data scale on ui control agents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. 2024. Coco-agent: comprehensive cognitive mllm agent for smartphone gui automation. In Findings of the Association for Computational Linguistics ACL 2024, pages 90979110. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo CampbellAjala, et al. 2024a. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024b. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36. Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Jindong Chen, Abhanshu Sharma, and James W. W. Stout. 2022. Towards better semantic understanding of mobile interfaces. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5636 5650, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Bryan Wang, Gang Li, and Yang Li. 2023. Enabling conversational interaction with mobile ui using large language models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 117. Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, and Zhen Xiao. 2024. Understanding the weakness of large language model agents within In Proceedings of complex android environment. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 60616072. Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. 2024. Androidlab: Training and systematic benchmarking of android autonomous agents. arXiv preprint arXiv:2410.24024. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023a. Appagent: Multimodal agents as smartphone users. CoRR, abs/2312.13771. Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng Cao, and Kai Yu. 2023b. Mobile-env: an evaluation platform and benchmark for llm-gui interaction. arXiv preprint arXiv:2305.08144. Jiwen Zhang, Jihao Wu, Teng Yihua, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024. Android in the zoo: Chain-of-action-thought for GUI agents. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1201612031, Miami, Florida, USA. Association for Computational Linguistics. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. GPT-4V(ision) is generalist web agent, if grounded. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 6134961385. PMLR. 9 ... ``` Output the generated code with only code , not wrapped in markdown . Listing 1: generation. Prompt for evaluation function code The simplified prompt for GPT-4o to directly evaluate the task performance is as followed: Given task and xml file of the screen , judge whether the task is completed correctly . The task is '{ task } '. And here is the xml of the screen : { xml }. To correctly judge the performance , you should consider element states such as ' selected ' and ' activated ' to check whether specific condition is satisfied . You also need to consider the content - desc attributes of elements to check the result correctness . Answer with \" yes \" or \" no \". Listing 2: generation. Prompt for evaluation function code A.3 Finetuned Agent We use the 8B model of InternVL2 as the base model. We train the model on 8 A100 GPU for 27 hours on ANDROIDCONTROL and AMEX, following the default finetuning settings. A.4 Demonstrations Here we provide some demonstrations of errors during evaluation (See Figure 4 and Figure 5). We can see that the agent predicts several steps correctly but if one action is wrong, the agent would fail the task."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Task Examples Easy Open Yelp and search for pizza nearby. Search for Micheal Jackson on YouTube. Medium Is the book Romeo and Juliet in my wishlist on Wish? Go to Taylor Swift page and subscribe on YouTube Music. Hard Open Booking.com and search for stays in Beijing from Nov 27 to Nov 28. Then sort by price from low to high, and tell me the lowest price. After sorting the results by distance for nearby BBQ on Google Maps, select the first store, and start navigation. A.2 Prompts The simplified prompt for GPT-4o to generate evaluation function code (Section 3.4.2) is as followed: Given task and xml file of the screen , generate an evaluation code for the task to check whether the task is completed correctly . Think of the conditions carefully , especailly different elements and whether to use regex expression . The task is '{ task } '. And here is the xml of the screen : { xml }. The evaluation function should start with the following code : ``` python ... ``` The evaluation function should return True if the task is completed correctly and False otherwise . base xml parser is also provided for you to use : ``` python ... ``` for example you can use the following code to evaluate the task : ' Search ' Michael Jackson ' on YouTube . ' ``` python 10 Figure 4: Step 1 and Step 2 are correct, however, the agent starts typing before the search bar is clicked or selected, so the process sticks at this situation and the agent keeps typing and waiting. Figure 5: Step 1 and Step 2 are correct, however, the agent predicts wrong click coordinate and accidentally go to the shopping cart. It should go back but seems it lacks the capability to do that and gets stuck in the shopping cart."
        }
    ],
    "affiliations": [
        "EE department @ CUHK",
        "MMLab @ CUHK"
    ]
}