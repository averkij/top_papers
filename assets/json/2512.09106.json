{
    "paper_title": "Learning Unmasking Policies for Diffusion Language Models",
    "authors": [
        "Metod Jazbec",
        "Theo X. Olausson",
        "Louis Béthune",
        "Pierre Ablin",
        "Michael Kirchhof",
        "Joao Monterio",
        "Victor Turrisi",
        "Jason Ramapuram",
        "Marco Cuturi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 0 1 9 0 . 2 1 5 2 : r Learning Unmasking Policies for Diffusion Language Models Metod Jazbec, Theo X. Olausson, Louis Béthune, Pierre Ablin, Michael Kirchhof, Joao Monterio, Victor Turrisi, Jason Ramapuram, Marco Cuturi Apple, University of Amsterdam, Massachusetts Institute of Technology Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which buffer filled with special mask tokens is progressively replaced with tokens sampled from the models vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as Markov decision process in which the dLLM serves as the environment, and propose lightweight policy architecture based on single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of stateof-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach. Correspondence: MJ: m.jazbec@uva.nl; TXO: theoxo@csail.mit.edu; MC: cuturi@apple.com Contributions: * denotes equal contribution; work done while MJ and TXO were interns at Apple. Date: December 11,"
        },
        {
            "title": "Introduction",
            "content": "Discrete diffusion (Austin et al., 2021a; Hoogeboom et al., 2021; Dieleman et al., 2022; Lou et al., 2024; Shi et al., 2024) has recently emerged as compelling alternative to the predominant autoregressive (AR) modeling paradigm in large language models (LLMs). Unlike AR models, which generate the next token in left-to-right fashion (Radford et al., 2018), diffusion LLMs (dLLMs) generate text by learning to reverse noising process that progressively corrupts block of token sequences. In particular, masked diffusion models (MDMs; Sahoo et al. 2024; Ou et al. 2024), subclass of discrete diffusion models that use time-dependent BERT-style (Devlin et al., 2019) masking as the forward noising process, have recently demonstrated impressive performance, with models like LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025) matching the performance of similarly sized autoregressive LLMs. At generation time, MDMs begin with fully masked sequence and iteratively unmask fixed number of tokens at randomly sampled positions in each sampling step. As result, they offer the potential for faster inference, as they can, in principle, generate multiple tokens in parallel using single model call. Despite this promise, open-source dLLMs have, until recently, lagged behind their AR counterparts in terms of inference efficiency. This changed with Fast-dLLM (Wu et al., 2025), which demonstrated that dLLM like LLaDA (Nie et al., 2025) can achieve higher token throughput than similarly sized LLaMA models (Dubey 1 et al., 2024) while maintaining competitive performance. key component of Fast-dLLM lies in its sampling heuristic: instead of unmasking fixed number of randomly sampled token positions, it proposes to unmask all tokens whose confidences exceed pre-specified threshold, resulting in adaptive sampling with variable number of unmasked tokens per step. This success has since inspired further development of increasingly sophisticated sampling heuristics (see Section 5 for comprehensive overview), which has further advanced the state of the art. These heuristics can, however, be difficult to tune, and seem to work best when imposing semi-autoregressive (semi-AR) generation, in which small blocks of tokens are unmasked sequentially (Arriola et al., 2025a; Nie et al., 2025). We posit that such limitations are difficult to avoid with heuristics alone, since they are essentially handcrafted solutions to sequential decision making problem: What is the optimal order in which to unmask the sequence of tokens, in order to strike good balance between efficiency and correctness? We propose to investigate this question by moving beyond heuristics to stress instead learning based approach, which relies on transformer-based unmasking policy trained using reinforcement learning (RL). This approach is motivated by the above observation that unmasking in dLLMs can be viewed as (Markovian) sequential decision making problem, and follows recent successes in reinforcement learning for post-training of language models. However, in contrast to recent works that use RL to improve the reasoning abilities of dLLMs (e.g., Zhao et al. 2025a), our goal is not to improve the capacity of the underlying model, but rather to use RL to automate the discovery of adaptive sampling strategies. We therefore treat the underlying dLLM as the environment in which to act, not the policy, leaving it unchanged and instead focusing policy that is parameterized as lightweight stand-alone network. Empirically, we demonstrate that our learned sampling policies match the performance of state-of-the-art heuristic samplers (Wu et al., 2025) in standard generation settings, while also surpassing heuristics in certain scenarios, notably when moving away from semi-AR generation. In summary, we make the following contributions: We formalize sampling in dLLMs as Markov decision process (MDP) (Section 3.1), propose lightweight design for the unmasking policy (Section 3.2), and outline our RL training pipeline based on group relative policy optimization (GRPO; Shao et al. 2024) (Section 3.3). We conduct extensive experiments showing that, under our proposed RL framework, learned policies can match the performance of heuristic samplers such as Fast-dLLM (Section 4.1), while also addressing some of the challenges heuristic samplers face outside the semi-AR regime (Section 4.2). We study the transferability of our learned samplers across different models and data domains (Section 4.3), and provide detailed insights into stabilizing RL training and policy design (Section 4.4)."
        },
        {
            "title": "2 Background",
            "content": "Throughout the paper, we use the notation [L] to represent the set {1, . . . , L}. We denote sequence of tokens as = (x1, . . . , xd) d, where is the sequence length and := [V ] is the vocabulary. Given our focus on masked diffusion models, we assume that the vocabulary includes special mask token ."
        },
        {
            "title": "2.1 Masked Diffusion Models",
            "content": "We focus on masked diffusion models (MDMs) (Nie et al., 2025), deferring more general introduction to discrete diffusion to Appendix E. Training MDMs learn to generate data by training BERT-style (Devlin et al., 2019) masked predictor to reverse the forward noising process. Concretely, given training sample x0 pdata, the forward process corrupts each token independently by setting it to the mask token with probability proportional to the diffusion timestep [0, 1]: pt(xt x0) := (cid:89) k=1 pt(xk x0), where pt(xk x0) := 1 t, t, 0, if xk = xk 0, if xk = , otherwise. 2 Recent work has shown that an MDM parametrized by θ can learn the reverse process by maximizing the following evidence lower bound (ELBO) (Ou et al., 2024), L(θ) Ex0pdata [log pθ(x0)]: (cid:21) 0xt) L(θ) := EtU [0,1],x0pdata,xtpt(xtx0) = ] log pθ(xk 1[xk (cid:88) (2.1) , (cid:20) 1 k=1 with 1[] denoting the indicator function. Generation When generating an answer for given prompt x, MDMs start with sequence of all-masked tokens yT := (M , . . . , ), where := yT is pre-specified maximum answer length. Then, in each 0 = x, yt) for all token positions sampling step [T ], the MDM outputs token distributions pk [L]. Then, sampling strategy decides which subset Ut Mt of the still-masked positions Mt := {k [L] yk = } to unmask. new, partially unmasked/denoised answer yt1 is obtained via (cid:40) := pθ(yk yk t1 := pk , yk , if Ut, otherwise. (2.2) The stochasticity of sampling pk depends on the dLLM temperature τ , with higher temperatures leading to more diverse (but often lower quality) generations. When evaluating dLLMs, it is common to set τ = 0 (Nie et al., 2025; Wu et al., 2025) to maximize performance, resembling greedy decoding in AR LLMs. However, unlike for autoregressive LLMs, setting the temperature τ to 0 is not sufficient to completely determine the generation process. The choice of sampling strategy1 is also important, since different choices will produce different unmasking sets Ut. This clearly affects the sampling efficiency, since generating multiple tokens in parallel will yield complete answer faster than if they were generated one at time. However, it may also affect the quality of the samples, since different unmasking sets will yield different conditions xt, thus affecting the predicted token distributions pk . The need to carefully balance efficiency and quality has thus drawn considerable attention to the development of optimal sampling techniques for dLLMs (see Section 5)."
        },
        {
            "title": "2.2 Heuristic Samplers\nOne popular approach to decide which positions Ut to\nunmask at each timestep is to construct a heuristic\nthat leverages uncertainty measures derived from the\n(Wu et al. 2025; Ben-\npredicted token distributions pk\nt\nHamu et al. 2025; Wei et al. 2025; Hong et al. 2025;\nLi et al. 2025a; Huang et al. 2025a; Kim et al. 2025;\ninter alia). In particular, recent work has shown sub-\nstantial efficiency gains by with heuristic strategies that\nt (v) of the under-\nemploy the confidence ck\nlying dLLM in order to make their decisions. Two repre-\nsentative methods within this space are high-confidence\nunmasking (Chang et al., 2022), in which each forward\npass unmasks a fixed number of tokens (K) with the\nhighest confidences,",
            "content": "t := maxvV pk := (cid:110) arg max IMt, I=K (cid:111) ck (cid:88) kI as well as the confidence-thresholding strategy of FastdLLM (Wu et al., 2025), which allows for variable number of tokens to be unmasked at each step by comparing the confidences to fixed threshold λ:2 Figure 1 LLaDA-8B-Instruct (Nie et al., 2025) on GSM8k, with semi-AR generation (8 blocks of BL = ). ) and without (one block of BL = 256; 32; More datasets and models in Appendix A.1. Generation speed is measured in network function evaluations (NFEs), which corresponds to the number of sampling steps. := {k Mt ck λ > λ}. 1We use the terms sampling and unmasking interchangeably throughout this paper. 2Note that it is possible for no token confidence to exceed the pre-determined threshold λ. In this case, Wu et al. (2025) propose to unmask the position with the highest confidence among the remaining masked tokens, to prevent the sampling from getting stuck. 3 Notably, Wu et al. (2025) showed that when combining confidence-thresholding sampling with additional optimizations such as KV-caching, dLLMs can live up to their promise and surpass AR models in token throughput, while maintaining comparable generation quality. Moreover, they provide theoretical justification for their sampling design by showing that sampling from high-confidence marginals closely approximates sampling from the joint distribution over token positions. Despite the undeniable recent success of confidence-based samplers, relying on handcrafted solutions comes with certain drawbacks. Beyond the obvious need for careful design (e.g., selecting an appropriate confidence measure or threshold value λ), we also found that these methods often exhibit high sensitivity to the exact sampling configuration. For instance, many heuristics rely on semi-autoregressive (semi-AR) generation (Arriola et al., 2025a; Nie et al., 2025), where decoding proceeds one block of tokens at time, with block length BL serving as yet another hyperparameter. As shown in Figure 1, outside this semi-AR regime, confidence-based heuristics can degrade to below-random performance and may no longer benefit from increased compute budgets (i.e., higher numbers of function evaluations, NFEs). These limitations raise natural question whether effective sampling methods can be learned rather than manually designed, which we explore next."
        },
        {
            "title": "3 Learning Unmasking Policies",
            "content": "We now introduce our approach to learn sampling in dLLMs via reinforcement learning (RL). We begin by formulating sampling as Markov decision process (MDP) (Section 3.1), then propose sampling policy (Section 3.2), and finally provide details on our RL training (Section 3.3). 3.1 dLLM Sampling as Markov Decision Process To facilitate the development of RL-based samplers, we start by describing the Markov decision process (MDP) (Sutton et al., 1998) for sampling in dLLMs in its most general case: 3 The state (x, yt) consists of prompt and the current (partially masked) generation yt L. For brevity, we omit from the state notation unless explicitly needed. The initial state contains fully masked generation: yT = (M , . . . , ). An action ut {0, 1}L is vector of unmasking decisions indicating which positions have been selected to be unmasked in the next transition step. These actions are chosen according to the policy πϕ: ut πϕ( yt) (introduced later in Section 3.2). The transition yt1 (yt1 yt, ut; τ ) corresponds to standard sampling in dLLMs (cf. Equation (2.2)), with the action ut determining which tokens get unmasked: π := {k Mt uk = 1}. The reward R(y, yt) is provided only at the final generation step, which corresponds to the first timestep where all tokens have been unmasked ˆT := max{t [T ] yk = M, [L]}. To learn useful samplers, the reward should promote both correctness (i.e., the generated answer ˆT being close to the reference answer y) and efficiency (i.e., minimizing the number of steps ˆT ). Note that the reward depends on action ut, and therefore on the policy πϕ, implicitly through its influence on the generations yt and the number of steps; we omit it from the input arguments to simplify notation. We defer our concrete choice of the reward function to Section 3.3. With the MDP defined above, finding the optimal sampling becomes standard reinforcement learning problem of finding policy πϕ parametrized by ϕ that maximizes the expected reward: max ϕ E(x,y)pdata, utπϕ, ytP (cid:20) (cid:88) (cid:21) R(y, yt) . t= 3To stay aligned with the MDM notation in Section 2.1, we reverse the time in our MDP formulation: = T, 1, . . . , 0."
        },
        {
            "title": "3.2 Lightweight Confidence Policy Design\nHaving outlined the MDP above, we next describe our proposed implementation for the sampling policy πϕ.",
            "content": "Confidence-based input Recall that state consists of partially masked token sequence yt. To avoid constructing policies that operate at the token level, and thereby minimize computational overhead, we rely on the vector of token confidences ct := (c1 ), which are readily available since the token-level predictive distributions pk are computed at each transition step yt1 anyways. Our choice is further motivated by the aforementioned heuristic methods that primarily operate on confidences (Wu et al., 2025), and also based on our own ablations (see Section 4.4). Specifically, we observed that alternatively relying on dLLMs hidden states required significantly larger policy models without consistently improving performance. , . . . , cL Lightweight transformer We introduce small, learnable neural network fϕ that maps the vector of confidences ct to vector of unmasking scores (logits) bt = fϕ(ct, mt, t) with bt RL. We additionally include binary := 1[k Mt] indicates whether position is still masked, and mask vector mt := (m1 inform the policy with the time index [T ]. In practice, fϕ is implemented as lightweight transformer (see Section 4 for more details on the exact policy architecture). Notably, its size is less than 0.01% of the pretrained dLLMs used in our experiments, resulting in negligible computational overhead during sampling. ), where mk , . . . , mL Bernoulli likelihood We sample the unmasking actions according to uk ), where Ber() denotes the Bernoulli distribution and σ() is the sigmoid function. Conveniently, the policy likelihood πϕ(ut) := (cid:81)L is readily available in closed form and does not require any additional approximations (unlike in the case of post-training dLLMs; Zhao et al. 2025a). We also considered an alternative formulation based on the Plackett-Luce model (Luce, 1959; Plackett, 1975) which we call dynamic Plackett-Luce sampling (DPLS; see Appendix C), but since our ablations showed comparable performance (Section 4.4), we favor the Bernoulli formulation in our main experiments for its simplicity. := σ(bk (1 sk )1uk k=1(sk )uk t Ber(cid:0)sk (cid:1) with sk /τπ) , where τπ is At generation, we additionally propose to temper the Bernoulli parameters sk the policy sampling temperature. This temperature controls the sharpness of the policy distribution and, as shown in Section 4, we found that it can sometimes serve as test-time knob for balancing performance (for example, τπ < 1 forces the Bernoulli distribution to be more decisive by pushing the probability more ). Moreover, to ensure convergence when all towards either 0 or 1 depending on the sign of the logit bk (similar sampled actions are zero (ut = 0), we unmask the position with the highest Bernoulli parameter sk to Fast-dLLM, see Section 2.2). We apply this fallback only at generation time, as we found that forcing unmasking during training was prone to reward hacking, where the model would reliably obtain non-zero towards . By not enabling this behavior (but sub-optimal) reward simply by pushing all its predictions bk during training, the policy is forced to actively choose which tokens to unmask. := σ(bk , . . . , yg ˆTg"
        },
        {
            "title": "3.3 Learning πϕ with GRPO\nTo train our sampling policy, we adopt group relative policy optimization (GRPO) (Shao et al., 2024), a\nmethod recently popularized as a simpler and more scalable alternative to earlier policy gradient approaches\nsuch as PPO (Schulman et al., 2017). Specifically, for each prompt x ∈ D, we sample G trajectories of gener-\nations {yg\n. Importantly,\nwe fix the dLLM sampling temperature τ to 0 (i.e., greedy decoding) to ensure that any variation among sam-\nples within a group arises solely from different unmasking actions. After computing rewards for each sample\nin the group, we define the advantage as Ag\nt := R(y, yg\n) , following recent best practices\nˆTg\nthat recommend omitting standard deviation normalization of advantages (Zhao et al., 2025a). Additionally,\nnote that the reward at the final generation step ˆTg for each sample is propagated to all preceding timesteps\nt to provide a learning signal throughout the entire sampling process. Our final training objective is then",
            "content": "along with their corresponding unmasking decisions {ug i=1 R(y, yi ˆTi , . . . , ug ˆTg ) 1 }G g=1 }G g= (cid:80)G (ϕ) := (x,y)D, {yg : ˆTg g=1Pθ, {ug }G : ˆTg }G g=1πϕold (cid:20) 1 (cid:88) g=1 1 ˆTg (cid:88) t= ˆTg 5 min (cid:8)ρg Ag , clip(ρg , 1 ϵ, 1 + ϵ) Ag (cid:21) (cid:9) refers to an earlier version of the policy used to where πϕ is the current policy being updated, and πϕold := πϕ(ug ) generate the RL rollouts. The likelihood ratio ρg serves as the importance sampling correction πϕold (ug ) term, which together with clipping (via ϵ) aims to stabilize the off-policy training. Since our policies are trained from scratch, we remove the KL regularization term in our GRPO objective. Finally, we note that GRPO can be equivalently interpreted as an off-policy variant of the standard REINFORCE algorithm (Williams, 1992), where the group mean normalization functions as control variate to reduce the variance of the policy gradient (Mohamed et al., 2020). Reward As mentioned in Section 3.1, we wish to obtain policy that yields correct generations while also being fast. To this end, we define our final multiplicative reward as R(y, yt) := (cid:40) r(y, yt) (cid:0)1 0, (cid:1)α , if = ˆT , otherwise, (3.1) where r(y, yt) is task-specific correctness term (e.g., binary reward indicating whether the generated mathematical answer is correct). To encourage faster sampling, we incorporate computational penalty based on the number of steps, ˆT , with α 0 serving as hyperparameter that controls the trade-off between accuracy and speed. While we experimented with an additive penalty of the form r(y ˆT , y)α(cid:0) ˆT (cid:1) (Graves, 2016) we found that this led to problematic reward hacking: when all samples in group are incorrect, as is the case early on when training from scratch, faster samples may still receive positive advantage, despite being wrong. Such additive reward derailed and destabilized RL training (cf. Section 4.4). Our multiplicative reward can therefore prioritize first correctness, and then efficiency. T"
        },
        {
            "title": "4 Experiments",
            "content": "We begin our experiments by verifying that the learned policies match the performance of confidence-based heuristics (Section 4.1). We then highlight the potential of RL-based sampling to outperform heuristics in the more general setting without semi-AR decoding (Section 4.2), and examine the transferability of our RL policies across models, datasets, and generation lengths (Section 4.3). Finally, we explore different ways to instantiate the MDP and analyze their effects on downstream performance (Section 4.4)."
        },
        {
            "title": "4.1 Learning Effective Sampling via RL",
            "content": "We start by demonstrating that our proposed RL framework yields effective sampling strategies in dLLMs, where effectiveness is defined in terms of both accuracy and speed. Experiment details. We use LLaDA-8B-Instruct (Nie et al., 2025) as the base dLLM (additional results on Dream-7B-Instruct (Ye et al., 2025) provided in Appendix A.2). Note that while LLaDA is trained as an MDM from scratch, Dream is initialized from an AR model (Qwen 2.5; Qwen et al. 2025). We parametrize the policy network fϕ as shallow (single-layer) transformer incorporating adaptive layer normalization for conditioning; hyperparameter details for the architecture are provided in Section B. We train five different policies, corresponding to α {10, 3, 1, 0.3, 0}. Each policy is trained semi-autoregressively at BL = 32 on single epoch of mixture data, sampled proportionally from the training sets of GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), resulting in roughly 15,000 training samples. Figure 2 shows the training dynamics for these runs; as expected, higher α generally gives rise to faster policies. Figure 2 Correctness reward (rolling average, 20 steps) on GSM8k (left) and average number of sampling steps (right) during training of our policies for various values of α (cf. Equation (3.1)). Averaged over two random seeds, with shaded areas indicating (min, max); only one seed shown ) due to training instability. for α = 10.0 ( 6 (a) GSM8k, BL = 32 (b) GSM8k, BL = (c) MATH-500, BL = 32 (d) MATH-500, BL = 256 Figure 3 Results for LLaDA in semi-AR (Figure 3a & Figure 3c) and full-diffusion (Figure 3b & Figure 3d) generation regimes. Results for Dream-7B are provided in Figure 7. For our policies we vary α {10, 3, 1, 0.3, 0} and use τπ = 0.5 for BL = 32 and τπ = 1 for BL = 256. Expert steering (ES) described in detail in Appendix D. We then compare the resulting policies on the test sets of both GSM8k and MATH to the confidence-based heuristics introduced in Section 2.2, averaging over two training seeds and three test-time seeds. To obtain Pareto frontiers for the heuristic methods, we use {8, 16, 32, 64, 128, 256} for the random baseline and high-confidence unmasking, while for Fast-dLLM we use λ {0.1, 0.2, . . . , 1.0} throughout. For all methods, we use the standard greedy decoding setting (τ = 0) when generating test answers. To measure sampling speed, we report the mean number of function evaluations (NFEs) of the underlying dLLM. Results with (Short) 32 Block Length, Figure 3a and Figure 3c Generally, we observe that the performance of the learned policies ( ) as well as the high-confidence sampling ( ). The fact that our learned policies do not surpass FastdLLM in the mid-to-high NFEs range may suggest that this heuristic is near-optimal under the semi-AR sampling regime, where spatial correlation is enforced during the decoding process. ) strategy, while matching that of Fast-dLLM ( ) exceeds those of both the random baseline ( We note that the effect of α in training (cf. Figure 2) carries over to the generation on test set: policies trained with higher values of α exhibit faster, yet less accurate, sampling. Of particular note is α = 10.0, which exhibited greater training instability with only 1 out of 2 training runs converging. When the seed that does take off is evaluated, we find that it yields very fast policy that outperforms Fast-dLLM in the low-NFE ( 10) regime on both datasets. This highlights the potential of RL policies when optimizing for maximal efficiency. However, RL policies appear to exhibit less controllability compared to heuristic like Fast-dLLM, as varying α results in less smooth traversal of the Pareto frontier than varying the confidence threshold λ. For example, the behavior of the α = 3.0 policy is nearly identical to that of α = 1.0, despite the reward function incorporating computational penalty that scales exponentially with α. Furthermore, we find that this cannot easily be remedied by using tighter grid for α. In Appendix A.3 we let α range in {10.0, 9.0, ...1.0, 0.3, 0.0}, but observe that using α 4.0 consistently results in converging to policy either roughly equivalent to that of α = 3.0 or to that of α = 10.0, with no value of α successfully yielding policy which interpolates between the two extremes. Besides α, we observe that one can make small changes to the trade-off between compute and performance by varying the policy temperature τπ at test time; we detail the impact of this parameter in Figure 9."
        },
        {
            "title": "4.2 Beyond Semi-Autoregressive Decoding",
            "content": "As mentioned in Section 2.2, heuristic sampling methods often rely on semi-AR generation to achieve good performance. While not typically thought of as problematic in textual domains where strong AR dependencies exist, semi-AR approaches can only partially fulfill the promise of fully parallel generation. Having verified that RL can help learn sampling policies with short block-length, we consider in section longer blocks. Experiment details. We train policies without relying on semi-AR generation, targeting the full-length generation task (BL = = 256) at both training and test time. We keep all other implementation details identical to those in the previous section (Section 4.1), and evaluate on the test sets of GSM8k and MATH. 7 Results with (Long) 256 Block Length, Figure 3b & Figure 3d Although all sampling strategies experience performance drop relative to the semi-AR setting (cf. Figure 3a & Figure 3c), we find that the policies produced by RL ( ) exhibit the smallest decline and consequently achieve the best overall performance. Furthermore, the results suggest that this methodology is able to produce policies which achieve solid performance even in the low-NFE regime. In particular, the learned policies obtain 50% accuracy at 12 NFEs on GSM8k (compared to 30% for the heuristic methods regardless of semi-AR use), highlighting the potential of nonsemi-AR sampling for achieving maximal efficiency gains. Note however that, in theory, RL policy trained with BL = could learn to emulate semi-AR sampling on its own. Thus, the fact that policies trained with BL = underperform those trained with BL = 32 in the mid-high NFE range (comparing, for example, Figure 3a to Figure 3b) suggests that the policies remain far from optimal. One hypothesis for why this is the case is that our training procedure is not encouraging sufficient exploration, instead converging to locally optimal policies. To address this, we explore encouraging further exploration by using samples generated via semi-AR sampling from Fast-dLLM. We refer to this approach as expert steering, and describe it in more ) RL is able to discover policies that detail in Appendix D. As shown in Figure 3, with expert steering ( close the performance gap to the best accuracy achieved in the semi-AR setting at mid-to-high NFEs (e.g., 80% on GSM8k and 35% on MATH), while mostly retaining their strong performance in the low-NFE regime. However, we do find that expert steering introduces significant instability during training, further reducing the controllability through α, with multiple values of α collapsing to near-identical policies. We leave further investigations into stabilizing expert steering training for future work."
        },
        {
            "title": "4.3 Transferability of RL Sampling Policies",
            "content": "We next turn to the question of transferability. notable advantage of heuristic approaches like Fast-dLLM is that they can be applied post-hoc to any model or dataset.4 This naturally raises the question: to what extent can RL policies trained on specific model and dataset be reused on other models/datasets? Model transfer: LLaDA Dream. We begin by investigating to what extent the policies found via RL transfer across models; note that such transfers are possible because our policies rely solely on token confidences and are therefore agnostic to token embeddings, which would not transfer from one model to another. We thus reuse the policies trained on the LLaDA-8B-Instruct model (under the semi-AR setting; cf. Section 4.1) and evaluate them on Dream-7B-Instruct; the results are shown in Figure 4a (see Figure 10 for MATH). Encouragingly, most of the LLaDA-trained policies nearly match the performance of Fast-dLLM when evaluated on Dream, and perform very similarly to those trained on Dream directly (Figure 7). The one exception is the α = 10 policy ( ), which collapses to Fast-dLLM performance and fails to retain the good low-NFE performance observed with LLaDA (cf. Figure 3a)suggesting that the extreme steepness of the reward curve (due to high α) causes this policy to overfit to model-specific patterns in LLaDAs confidence levels. Domain transfer: Mathematical reasoning to code. Next, we examine how well our policies transfer across different data domains. We again reuse the policies from Section 4.1, which were trained on mixture of mathematical data (GSM8K/MATH), but this time evaluate them on the coding tasks of HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021b). As shown in Figure 4b and Figure 4c ( ), we find that these policies fail to fully transfer to the coding domains: their performance more closely aligns with the highconfidence baseline rather than with Fast-dLLM. To investigate whether this drop in performance is due to the lack of domain-relevant training data, we train new policy on the coding dataset KodCode-RL-10K (Xu ). These coding-specific policies narrow the gap to the Fast-dLLM baseline on HumanEval et al., 2025) ( and MBPP, underscoring the importance of using diverse data mixture to support generalization across domains. We leave to future work the training of sampling policies on mixtures that combine datasets from different domains (e.g., mathematics and coding). Sequence length generalization: = 256 = 512. Lastly, we examine whether our policies transfer across different sequence lengths (not to be confused with the block length BL used in semi-AR generation). Such transfer is possible since we instantiate fθ with transformer trained with rotary positional embeddings (Su 4Though heuristics might still require manual hyperparameter tuning as exemplified by different optimal threshold across datasets for Fast-dLLM (e.g., comparing Figure 3a vs Figure 4b). 8 (a) Model transfer: LLaDA Dream on GSM8k. (b) Domain transfer: math coding (HumanEval). (c) Domain transfer: math coding (MBPP). (d) Sequence length transfer: 256 512 on GSM8k. Figure 4 Results for the transferability experiments. Note that in (a), the α = 10 policy is represented separately by ( ) in the lower left to avoid misleading visualization when interpolating to α = 3. For results on coding datasets in (b) and (c), we omit the low-NFE regime, as all approaches degrade to near-zero performance in this setting. et al., 2021), rather than, for example, an MLP or other fixed-length architectures. We thus take the policies from Section 4.2 and evaluate them at 2x longer sequence length, = 512. Results are shown in Figure 4d (see Figure 11 for MATH results). While the baselines degrade further as the sequence length increases, the learned policies yield similar performance to before, suggesting that RL policies can transfer effectively across generation lengths without retraining."
        },
        {
            "title": "4.4 Exploring the Design Space of πϕ\nLastly, we conduct a series of experiments to elucidate the impact of various design choices behind our\nproposed RL sampling method.",
            "content": "T Additive vs. multiplicative rewards We begin by ablating the structure of the reward function, comparing our proposed multiplicative combination of correctness and computational terms (cf. Equation (3.1)) with an additive alternative: r(y, ˆT ) α(cid:0) ˆT (cid:1). As shown in Figure 5a, while both exhibit increasing reward as training progresses, we observe that the additive formulation is much more prone to reward hacking, where it collapses to very-fast-but-very-wrong policy. This is illustrated in Figure 5b: training with the additive reward results in policy that unmasks everything at once, leading to the number of sampling steps to collapse to the minimum possible for all samples (i.e., 8 steps for training with BL = 32 and = 256). As discussed in Section 3.3, we attribute this issue to the fact that incorrect but fast samples may still receive positive advantage under the additive reward. In contrast, we find that the multiplicative reward effectively mitigates this issue by assigning positive advantage only if the generation is correct, resulting in more stable and predictable training behavior. Policy likelihood Next, we examine our choice of the policy likelihood parameterization. Recall from Section 3.2 that we model the unmasking probability for each position using Bernoulli distribution. This has the advantage of admitting very efficient inference and likelihood calculations, but relies on the network fϕ to implicitly embed relationships between different tokens in the scores bt, and runs the risk of producing π = in case ut = 0. In this experiment, we therefore investigate more involved sampling procedure, which we call dynamic Plackett-Luce sampling (DPLS; detailed description in Appendix C) which is guaranteed to unmask at least one position in each step and which non-linearly combines the scores bt through softmax, . Since this affects not only the sampling strategy but while still allowing for variable-size unmasking sets π also the likelihood computation, we retrain the policies from Section 4.1; the resulting downstream accuracy on GSM8k is then showed in Figure 5c. We observe that both methods achieve very similar performance, aligning closely with the Fast-dLLM frontier, which might provide additional support for the hypothesis that this frontier is optimal in the semi-AR setting. Furthermore, DPLS policies appear to show slightly better controllability via α parameter (as indicated by larger spread of policies trained with varying α). 9 (a) Training reward for LLaDA with additive vs. multiplicative reward function (both α = 1.0). (b) Mean number of NFEs when training LLaDA with additive vs. multiplicative reward (both α = 1.0). (c) Bernoulli vs DPLS sampling on GSM8k. Both for LLaDA on GSM8k, τπ = 0.5. (d) Bernoulli policy with top-1 versus with top-50 confidences. Both for LLaDA on GSM8k, τπ = 0.5. Figure 5 Ablations for our proposed RL framework. Confidence-based policy input Finally, we revisit our design choice of relying solely on the maximum token confidence values ck as input to the sampling policy (cf. Section 3.2) to explore whether other measures of uncertainty in the base models predictions could yield better results. We first train and evaluate set as input, but rather the top 50 of policies which do not take only the highest confidence per position ck highest values, thus giving the model more detailed information about the token predictive distributions and potentially allowing it to design its own confidences measures for effective sampling. The results pk are presented in Figure 5d. Somewhat surprisingly, using only the maximum confidence per position ( ) performs slightly better than using the top-50 confidences ( ), which suggests that alternative uncertainty measures are unlikely to yield performance gains over the simple confidences ck . Additionally, we consider parametrizing the policy as an additional classification head on top of LLaDAs final hidden state hk RH . While this results in significantly larger policy (300M parameters, an increase of 1, 000), it offers the potential advantage of incorporating token-level semantic information. However, in practice we observe that hidden state-based policies perform worse (see in Figure 5d) and exhibit less stable training dynamics than the confidence-based policies. These results suggest that the unembedding matrix RHV , which maps hidden states to token logits, plays vital role in enabling effective policy decisions via confidence signals."
        },
        {
            "title": "5 Related work",
            "content": "Heuristic Samplers for dLLMs. Sampling in diffusion LLMs (dLLMs) (Nie et al., 2025; Ye et al., 2025) has recently attracted significant attention, with much of the work in this area proposing heuristic approaches to improve the decoding process in training-free manner. Throughout this paper, we focus on Fast-dLLM (Wu et al., 2025) as representative heuristic method, as it popularized confidence-thresholded sampling in dLLMs and demonstrated its crucial role in enabling faster inference in dLLMs compared to autoregressive models. Many recently proposed heuristics can be viewed as either reinterpretations or extensions of such confidence thresholding strategies (Ben-Hamu et al., 2025; Wei et al., 2025; Hong et al., 2025; Li et al., 2025b; Yu et al., 2025). Other notable heuristic approaches explore incorporating spatial (Huang et al., 2025a) or temporal information (Wang et al., 2025a), alternative confidence measures (Kim et al., 2025), or more explicit modeling of token dependencies (Azangulov et al., 2025). In this work, we aim to complement ongoing research on sampling heuristics by investigating whether effective sampling strategies can be learned directly via reinforcement learning. Note that beyond improving the efficiency of unmasking in dLLMs, heuristics have also been proposed for remasking (Hong et al., 2025; Dong et al., 2025) and for dynamically adjusting the generation length (Li et al., 2025a), which we do not target in this work. 5Similar findings have been reported in the early-exit literature (Schuster et al., 2022), where confidence-based stopping criteria have been shown to significantly outperform those relying on hidden state representations. 10 Reinforcement Learning Post-Training for dLLMs. Early work on post-training diffusion LLMs via reinforcement learning includes d1 (Zhao et al., 2025a), which introduces variant of GRPO tailored to dLLMs, and DiffuCoder (Gong et al., 2025), which focuses on enhancing the coding abilities of LLaDA-style models using RL. Most recent extensions aim to improve the quality of policy gradient estimators (Tang et al., 2025; Wang et al., 2025b; Lin et al., 2025; Rojas et al., 2025; Zhu et al., 2025; Wang et al., 2025c; Zhan, 2025), and have demonstrated promising results in further enhancing the reasoning capabilities of dLLMs. The key distinction from our approach is that these methods use fixed sampling strategy (e.g., high-confidence sampling), and the policy corresponds to the dLLM itself (or LoRA-augmented version for efficiency). Closest to our work are DCOLT (Huang et al., 2025b), which trains separate unmasking module in addition to updating the base model via RL, and DiFFPO (Zhao et al., 2025b), which jointly learns an unmasking confidence threshold while updating the base model through RL. Differently to both these works, our primary goal is not in improving the reasoning abilities of the underlying dLLM; hence we keep the underlying dLLM fixed and focus solely on training standalone policy with the aim of learning fast sampling while preserving the performance of the base model. Finally, in concurrent work, Seed Diffusion (Song et al., 2025) proposes computation-aware RL as one of the key ingredients for achieving competitive efficiency among closed-source coding dLLMs. Orthogonal efforts to accelerate dLLMs. Besides the aforementioned heuristic-based sampling innovations, other efforts to improve inference efficiency in dLLMs (Ma et al., 2025) include KV caching (Jiang et al., 2025; Wu et al., 2025), (variants of) speculative decoding (Israel et al., 2025; Campbell et al., 2025; Guo and Ermon, 2025), training separate decoder modules during pretraining (Liu et al., 2024; Arriola et al., 2025b), and diffusion forcing (Wang et al., 2025d), among others. Concurrent work explores distilling faster generation patterns directly into the base model (Chen et al., 2025), or training (small) separate unmasking network on top of the pretrained dLLM (Bansal and Sanghavi, 2025; Bao et al., 2025). Crucially, unlike our approach, where this training is done via RL, these works train unmasking modules by distilling generation trajectories from the base model. As result, we expect that such approaches may still inherit limitations observed in dLLM sampling, such as difficulties in the non semi-AR regime (cf. Section 2.2). RL for Adaptive Compute Since our sampling policies result in variable number of sampling steps (T ˆT ) per input, our work connects to the broader literature on adaptive computation (Graves, 2016; Teerapittayanon et al., 2016). Notable examples of using RL to learn input-dependent compute policies include conditional computation with stochastic gating (Bengio et al., 2015), dynamic block skipping in residual networks (Wang et al., 2018), and RL-based early-exit decisions for long chain-of-thought reasoning (Dai et al., 2025). To the best of our knowledge, our work is the first to explore learning adaptive policies using RL for the task of sampling in dLLMs. The closest related concurrent work is DiFFPO (Zhao et al., 2025b); however, unlike our approachwhere the policy is learned end-to-endDiFFPO learns to predict adaptive thresholds, which are then used in the same fashion as the fixed thresholds employed by Fast-dLLM (Wu et al., 2025)."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced reinforcement learning approach for learning unmasking strategies in diffusion LLMs. Our experiments demonstrated that the learned policies can match and sometimes exceed the performance of recently proposed sampling heuristics, paving the way for the automated discovery of scalable and robust sampling mechanisms. Limitations and future work While we have shown that training sampling policies on LLaDA (Nie et al., 2025) allows us to match the performance of heuristics such as Fast-dLLM (Wu et al., 2025), small performance gap remains when policies are trained on Dream (Ye et al., 2025). Understanding which characteristics of Dream (e.g., it being initialized from an AR model) contribute to the diminished performance of RL policies is an important open question. One possible direction for future work is thus to train policies on mixture of models, which might help bridge this gap, or to explore other lightweight ways of incorporating semantic information (e.g., by pairing confidences with the (un)embedding vectors of the corresponding tokens). Another limitation of our approach is that it requires training separate policy for each value of α (cf. Equation (3.1)), and that this hyper-parameter sometimes does not yield as much fine-grained control 11 over the policys behavior as desired (c.f. Section 4.1). Another interesting avenue for future work is thus to investigate whether the accuracy-speed tradeoff could be controlled through some alternative mechanism, instead of explicitly having to set the strength of the computational penalty during training. We also observe that the optimal policy temperature τπ varies across generation settings (cf. Figure 3), suggesting the need to explore whether τπ can be learned jointly with the policy, or whether alternative likelihood parameterizations (such as DPLS, see Appendix C) offer greater robustness to temperature selection. Beyond overcoming these limitations, promising future extensions include: (i) expanding the training data mixture to incorporate samples from multiple domains; (ii) extending our sampling policies to support not only unmasking but also remasking (Wang et al., 2025e); and (iii) moving beyond the text domain to learn samplers for multimodal discrete diffusion models (Swerdlow et al., 2025)."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021a. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in neural information processing systems, 34:1245412465, 2021. Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, and Jonas Adler. Continuous diffusion for categorical data. CoRR, abs/2211.15089, 2022. doi: 10.48550/ARXIV.2211. 15089. URL https://doi.org/10.48550/arXiv.2211.15089. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July distribution. 21-27, 2024, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K. Titsias. Simplified and generalized masked diffusion for discrete data. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv 2407, 2024. 12 Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025a. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025a. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv. org/abs/2402.03300, 2024. Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, and Linfeng Zhang. Accelerating diffusion large language models with slowfast: The three golden principles. arXiv preprint arXiv:2506.10848, 2025. Feng Hong, Geng Yu, Yushi Ye, Haicheng Huang, Huangjie Zheng, Ya Zhang, Yanfeng Wang, and Jiangchao Yao. Wide-in, narrow-out: Revokable decoding for efficient and effective dllms. arXiv preprint arXiv:2507.18578, 2025. Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, and Dahua Lin. Beyond fixed: Training-free variable-length denoising for diffusion large language models. arXiv preprint arXiv:2508.00819, 2025a. Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, and Tong Xiao. Pc-sampler: Position-aware calibration of decoding bias in masked diffusion models. arXiv preprint arXiv:2508.13021, 2025a. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Duncan Luce. Individual choice behavior, volume 4. Wiley New York, 1959. Robin Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2), 1975. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. Journal of Machine Learning Research, 21(132):162, 2020. Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, 13 Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021b. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. 2025. URL https://arxiv.org/abs/2503.02951. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:1745617472, 2022. Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, and Shiwei Liu. Diffusion language models know the answer before decoding. arXiv preprint arXiv:2508.19982, 2025b. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025. Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, and Chunhua Shen. Time is feature: Exploiting temporal dynamics in diffusion language models. arXiv preprint arXiv:2508.09138, 2025a. Iskander Azangulov, Teodora Pandeva, Niranjani Prasad, Javier Zazo, and Sushrut Karmalkar. Parallel sampling from masked diffusion models via conditional independence testing. arXiv preprint arXiv:2510.21961, 2025. Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, et al. Saber: An efficient sampling with adaptive acceleration and backtracking enhanced remasking for diffusion language model. arXiv preprint arXiv:2510.18165, 2025. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. wd1: Weighted policy optimization for reasoning in diffusion language models. arXiv preprint arXiv:2507.08838, 2025. Chengyu Wang, Paria Rashidinejad, DiJia Su, Song Jiang, Sid Wang, Siyan Zhao, Cai Zhou, Shannon Zejiang Shen, Feiyu Chen, Tommi Jaakkola, et al. Spg: Sandwiched policy gradient for masked diffusion language models. arXiv preprint arXiv:2510.09541, 2025b. Nianyi Lin, Jiajie Zhang, Lei Hou, and Juanzi Li. Boundary-guided policy optimization for memory-efficient rl of diffusion large language models. arXiv preprint arXiv:2510.11683, 2025. Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, and Wei Deng. Improving reasoning for diffusion language models via group diffusion policy optimization. arXiv preprint arXiv:2510.08554, 2025. Yuchen Zhu, Wei Guo, Jaemoo Choi, Petr Molodyk, Bo Yuan, Molei Tao, and Yongxin Chen. Enhancing reasoning for diffusion llms via distribution matching policy optimization. arXiv preprint arXiv:2510.08233, 2025. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025c. Anthony Zhan. Principled and tractable rl for reasoning with diffusion language models. arXiv preprint arXiv:2510.04019, 2025. Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025b. Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, and Nathan Kallus. Diffpo: Training diffusion llms to reason fast and furious via reinforcement learning. arXiv preprint arXiv:2510.02212, 2025b. 14 Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, et al. dinfer: An efficient inference framework for diffusion language models. arXiv preprint arXiv:2510.08666, 2025. Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, and Xu Yang. d2 cache: Accelerating diffusion-based llms via dual adaptive caching. arXiv preprint arXiv:2509.23094, 2025. Daniel Israel, Guy Van den Broeck, and Aditya Grover. Accelerating diffusion llms via adaptive parallel decoding. arXiv preprint arXiv:2506.00413, 2025. Andrew Campbell, Valentin De Bortoli, Jiaxin Shi, and Arnaud Doucet. Self-speculative masked diffusions. arXiv preprint arXiv:2510.03929, 2025. Gabe Guo and Stefano Ermon. Reviving any-subset autoregressive models with principled parallel sampling and speculative decoding. arXiv preprint arXiv:2504.20456, 2025. Sulin Liu, Juno Nam, Andrew Campbell, Hannes Stärk, Yilun Xu, Tommi Jaakkola, and Rafael Gómez-Bombarelli. Think while you generate: Discrete diffusion with planned denoising. arXiv preprint arXiv:2410.06264, 2024. Marianne Arriola, Yair Schiff, Hao Phung, Aaron Gokaslan, and Volodymyr Kuleshov. Encoder-decoder diffusion language models for efficient training and inference. arXiv preprint arXiv:2510.22852, 2025b. Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192, 2025d. Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, and Xinchao Wang. dparallel: Learnable parallel decoding for dllms. arXiv preprint arXiv:2509.26488, 2025. Parikshit Bansal and Sujay Sanghavi. Enabling approximate joint sampling in diffusion lms. arXiv preprint arXiv:2509.22738, 2025. Wenrui Bao, Zhiben Chen, Dan Xu, and Yuzhang Shang. Learning to parallel: Accelerating diffusion large language models via adaptive parallel decoding. arXiv preprint arXiv:2509.25188, 2025. Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR), pages 24642469. IEEE, 2016. Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015. Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Proceedings of the European conference on computer vision (ECCV), pages 409424, 2018. Muzhi Dai, Chenxu Yang, and Qingyi Si. S-grpo: Early exit via reinforcement learning in reasoning models. arXiv preprint arXiv:2505.07686, 2025. Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025e. Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. Diederik P. Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023."
        },
        {
            "title": "Appendix",
            "content": "The appendix is organized as follows: In Appendix A, we present additional plots to supplement the experiments from the main paper: In A.1, we replicate Figure 1 across models (LLaDA, Dream) and datasets (GSM8k, MATH). In A.2, we replicate Figure 3 for policies trained on Dream. In A.3, we show results for the same setting as Figure 3a and Figure 3c but with denser α-grid. In A.4, we plot the impact of varying the policy temperature τπ. In A.5, we provide model transfer results for the MATH dataset. In A.6, we provide generation length (L) transfer results for the MATH dataset. In Appendix B, we provide implementation and hyperparameters details. In Appendix C, we describe dynamic Plackett-Luce sampling as an alternative to Bernoulli sampling. In Appendix D, we detail our expert steering approach. In Appendix we discuss an extended background discussion on discrete diffusion models, to benefit readers who are not familiar with this paradigm."
        },
        {
            "title": "A Additional Results",
            "content": "A.1 Figure 1 replicated for {LLaDA-8B-Instruct, Dream-7b-Instruct} {GSM8k, MATH-500} (a) LLaDA on GSM8k (b) LLaDA on MATH-500 (c) Dream on GSM8k (d) Dream on MATH-500 Figure 6 Performance comparison with (BL = 32; ) semi-AR generation. The same trend observed in Figure 1 holds across all models and datasets: confidence-based heuristics perform well under semiAR generation but degrade significantly without it. ) and without (BL = 256; 17 A.2 Figure 3 replicated for Dream-7b-Instruct (a) GSM8k, BL = 32 (b) GSM8k, BL = 256 (c) MATH-500, BL = (d) MATH-500, BL = 256 Figure 7 Results for Dream in semi-AR (Figure 7a & Figure 7c) and full-diffusion (Figure 7b & Figure 7d) generation regimes. For the policies we vary α {10, 3, 1, 0.3, 0} and use τπ = 0.5 for BL = 32 and τπ = 1 for BL = 256. Unlike for LLaDA, where the learned policies clearly match Fast-dLLM in the mid-to-high NFEs regime for semi-AR generation, small gap appears to remain for Dream. Furthermore, we found that the α = 10 policy did not converge in the BL = 32 setting when using Dream as the underlying language model (downstream accuracy thus not shown in (a), (c), to reduce clutter in the figures). Meanwhile, in the full-diffusion setting we do observe some performance gains, but here too they are smaller than were seen for LLaDA. A.3 Finer grid for α (a) GSM8k, BL = 32 (b) MATH-500, BL = 32 Figure 8 BL = 32 results for LLaDA with denser regularization grid, α {10.0, 9.0, . . . , 1.0, 0.3, 0.0}. Single training seed due to cost; error bars show (min, max) over three test-time seeds. Note that for α 4.0, the change in NFEs is not monotonic; different values lead to convergence either close to the α = 3.0 policy, or to that of α = 10.0. 18 A.4 Impact of policy temperature τπ (a) GSM8k, BL = 32 (b) MATH-500, BL = 32 (c) GSM8k, BL = 256 (d) MATH-500, BL = Figure 9 We study the effect of changing the policy temperature τπ (cf. Section 3.2). For each α 3, 0.3, 0, we construct corresponding test-time Pareto frontier by varying τπ 1.5, 1.0, 0.5. Interestingly, in some casessuch as α = 0 with BL = 32adjusting τπ enables an effective trade-off between compute and performance. Moreover, we find that τπ = 0.5 is optimal in the semi-AR, while τπ = 1 performs best in the non-semi-AR (BL = 256) setting. 19 A.5 Model transfer results (a) GSM8k, BL = 32 (b) MATH-500, BL = 32 Figure 10 Model transfer results. We use policies trained on LLaDA and evaluate them on Dream with τπ = 0.5. Encouragingly, transferred policies give almost identical results compared to training Dream specific policies (cf. Figure 7a & Figure 7c). Note that the α = 10 policy is represented separately by ( ) in the lower left to avoid misleading visualization when interpolating to α = 3. A.6 Sequence-length transferability (a) GSM8k (b) MATH-500 Figure 11 BL = = 256-trained policies from Section 4.2 evaluated with 2x longer sequence length (BL = = 512) with τπ = 1. Note that the learned policies yield almost identical performance, while the heuristic methods degrade further compared to = 256 (cf. Figure 3b & Figure 3d). Both for LLaDA-8B-Instruct."
        },
        {
            "title": "B Training and Policy Network Configuration",
            "content": "Table 1 Training and policy configuration for our main experiments."
        },
        {
            "title": "Value",
            "content": "Training GRPO Policy Network Learning rate LR scheduler Warmup steps Effective batch size Weight decay Max gradient norm Clipping factor (ϵ) Training data Num train epochs Group size β (KL penalty) Number of transformer blocks Hidden dimension Feedforward dimension Number of heads Time embedding dim Total parameter count 3e-5 Cosine 100 16 0.1 0.2 0.5 (0.2 for expert steering) GSM8k and MATH training set mixture 1 8 0. 1 128 512 2 128 300K 21 Dynamic Plackett-Luce Sampling We detail here the dynamic Plackett-Luce sampling (DPLS) strategy as an alternative to the Bernoulli sampling (cf. Section 3.2) used in the main experiments of this paper. The name reflects the connection the Plackett-Luce (PL) model proposed by Luce (1959) and Plackett (1975), with one key difference: the number of selected items is not fixed but can vary freely between 1 and L. ) denote the unmasking logits Formally, the Plackett-Luce model operates as follows. Let bt = (b1 , . . . , bL (corresponding to the output of policy network fϕ, same as in Section 3.2). Interpreting these scores as unnormalized utilities associated with choosing each token for unmasking, under the PL model the likelihood of any particular permutation σ := (σ1, . . . , σL) of the tokens is given by (σ bt) = (cid:89) l[L] exp(bσl ) jl exp(bσj ) (cid:80)L . Informally, this corresponds to sampling all indices without replacement, where at each step the probability of choosing an item is proportional to its (exponentiated) utility. The Plackett-Luce model can be easily adapted to model sampling of fixed-length ordered set Ut [L] where Ut = K. This is because the probability of any particular partial permutation is equal to the marginalization over all permutations which complete it. Conretely, let Σ(Ut) be the set of permutations which begin with the sequence Ut, then it follows that (Ut bt) = (cid:88) (cid:89) σΣ(Ut) l[L] (cid:124) (cid:80)L exp(bσl ) jl exp(bσj ) (cid:123)(cid:122) (cid:125) (σbt) (cid:89) = lUt exp (bσi ) exp (bσj ) t (cid:80) jU where denotes the complement of Ut, and denotes the indices which are in Ut at or after position l. To extend the PL model to variable-length unmasking sequence, we introduce special STOP token with fixed utility bSTOP = 0, and proceed as follows: 1. Sample single token [L] to unmask and initialize π = {l}. 2. Sample another token from the renormalized distribution softmax(bt/τπ) softmax([bU ; bSTOP]/τπ) ; bSTOP] denotes bt concatenated with bSTOP and with the logits of all previously selected where [bU indices π . 3. Add to π set to . 4. If was not STOP, repeat from 2. As written above, this algorithm is not GPU-friendly due to the dynamic computation graph it implies. However, it can be implemented in an efficient manner by treating the loop in steps 2-4 as Gumbel-argsort and simply masking out all actions which get sampled after STOP. Once the samples have been obtained, their likelihoods can be efficiently computed in the same manner as in the fixed-length case above by marginalizing over all actions which occur after STOP."
        },
        {
            "title": "D Expert Steering",
            "content": "As mentioned in Section 4.2, we find that naïvely training confidence policies directly on the full-length generation (BL = L) task yields policies which beat out the heuristic methods, but underperform the ones trained in the semi-AR setting (cf. Figure 3). Since semi-AR decoding lies within the function class representable by the policy πϕthat is, there is some ϕ such that πϕ approximates the semi-AR settingwe hypothesize that our failure to obtain similar policies is due to the vanishingly small probability of encountering autoregressive-like rollouts by chance. More concretely, image trying to train policy on task for which purely AR generation is substantially more effective than any other decoding strategy. In such scenario, starting from randomly initialized policy, the probability of observing rollout resembling purely AR sequence is approximately 1/L! 0 (where is the generation length), making it extremely unlikely to sample such rollouts during RL training. To try to eschew this problem, we devise the Expert Steering (ES) training strategy as follows. Formally, letting πϕ denote the sampling policy to be learned, we simply replace it at train time only with the mixture πES ϕ ( yt) = + πϕ( yt) + + (cid:88) e=1 δe( yt) ϕ of size + E. In the inner loop of GRPO we proceed as normal, making sure to use πES where is the GRPO group size, is the number of experts to mimic, and each Dirac distribution δe represents chosen deterministic \"expert policy\" (e.g., heuristic method). Then in the outer loop of GRPO, instead of sampling group πϕ of size from πϕ, we simply sample an augmented group when πES calculating the likelihood ratios to avoid the training instabilities that would otherwise arise (as samples from the Diracs may have likelihood 0 under πϕ) In practice, for our experiments in Section 4.2 we use single deterministic expert δe corresponding to FastdLLM with λ = 0.9 and BL = 32, and force exactly one draw (E = 1) from this heuristic per group. The effect is that if the policy is worse than this heuristic, that single sample will tend to have positive advantage, causing the policy to be biased toward it. On the other hand, if the policy is better than the heuristic, it will have negative advantage, causing it to be made less likely. Thus, expert steering encourages exploration towards the expertin this case, Fast-dLLMwhile still allowing the policy to go beyond it, thanks to the group relative loss in GRPO. ϕ"
        },
        {
            "title": "E Extended Background",
            "content": "E.1 Discrete diffusion variants and continuous relaxations Masked vs. uniform discrete diffusion. The D3PM framework (Austin et al., 2021a) characterizes forward corruption via Markov kernel Qt. Two canonical choices are (i) masked (absorbing-state) diffusion, which maps every token toward distinguished mask state , with αt [0, 1], em denotes one-hot vector for token, and 1 represents vector of all 1s of size , and (ii) uniform-state diffusion, which replaces tokens uniformly at random,"
        },
        {
            "title": "Qmask\nt",
            "content": "= (1 αt)I + αt 1e m,"
        },
        {
            "title": "Quni",
            "content": "t = (1 αt)I + α 11 . These differ in their limiting behavior and reverse constraints: Qmask has point-mass stationary distribution at and induces monotonic forward trajectory in the mask count (reverse steps deterministically move from token), whereas Quni has uniform stationary distribution and permits token token substitutions in both directions. The absorbing choice recovers the familiar MDM objective (cf. Equation (2.1)) under particular reweighting (Ou et al., 2024) and thus ties masked LMs to discrete diffusion (Austin et al., 2021a). Continuous-time discrete diffusion. Recent work derives continuous-time limits (CTMC) for discrete diffusion with absorbing corruption and shows that the training objective is weighted time integral of token-wise cross-entropy, with weight proportional to signal-to-noise ratio term (e.g., αt/(1 αt)). This yields simple, schedule-invariant objective, clarifies forwardreverse consistency, and improves optimization and sampling without changing the prediction target (Shi et al., 2024; Sahoo et al., 2024). Conceptually, this places masked diffusion on the same ODE/SDE footing as continuous-state diffusion (Kingma and Gao, 2023) while retaining native discrete token inference. Continuous-input diffusion for categorical data. complementary approach to discrete-state diffusion is to keep the diffusion process continuous in both time and input by operating on token embeddings. For example in Dieleman et al. (2022), an orthogonal line embeds tokens into Rd and applies fully continuous diffusion in both time and input space. Training can be done with cross-entropy via score interpolation, and sampling uses ODE/SDE solvers with tools like classifier-free guidance. This preserves the continuous machinery but introduces an embedding decoding interface and relaxes exact discreteness during the trajectory. Summary of differences. Masked (absorbing): monotonic reverse dynamics (M token only); objective reduces to weighted MDM; pairs naturally with unmask-only sampling policies used in this work. Uniform-state: non-monotone reverse dynamics (token token); encourages broader exploration but typically requires remasking/substitution moves and would thus complicate policy design. Continuous-input: inherits continuous samplers and guidance; trades exact discreteness for an embedding path and decoding step. We adopt the absorbing formulation in this work."
        }
    ],
    "affiliations": [
        "Apple",
        "Massachusetts Institute of Technology",
        "University of Amsterdam"
    ]
}