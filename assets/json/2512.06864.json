{
    "paper_title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
    "authors": [
        "Kaixuan Lu",
        "Mehmet Onurcan Kaya",
        "Dim P. Papadopoulos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\\text{AP}_{50}$ on YouTubeVIS-2019 $\\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS."
        },
        {
            "title": "Start",
            "content": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training Kaixuan Lu1 Mehmet Onurcan Kaya1,2 Dim P. Papadopoulos1,2 1Technical University of Denmark 2Pioneer Centre for AI s232248@student.dtu.dk, monka@dtu.dk, dimp@dtu.dk 5 2 0 D 7 ] . [ 1 4 6 8 6 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixellevel masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP50 on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. The source code of our method is available at here. 1. Introduction Video Instance Segmentation (VIS) is the challenging task of simultaneously detecting, segmenting, and tracking object instances across video sequences [10, 18, 32, 33, 39, 45]. This capability is fundamental for scene understanding in applications ranging from autonomous driving [43] to video content editing [47]. However, training highperformance VIS models typically requires pixel-level annotations across all frames [39]. This process is expensive due to the labor-intensive nature of annotating temporal consistency and instance identities. As result, there is an urgent need to develop unsupervised video instance segmentation frameworks that can accurately interpret video content and function effectively across diverse, unlabeled environments. Prior work [1, 4, 8, 17, 20, 37, 44] in unsupervised video segmentation predominantly addresses Video Object Segmentation (VOS), focusing on separating sinFigure 1. AutoQ-VIS overview. In the initial training stage, both the VIS model and the mask quality predictor are trained on synthetic videos with pseudo annotations [31]. During the multiround self-training stage, the VIS model generates pseudo masks on unlabeled videos, which are then scored by the frozen quality predictor. Pseudo masks with high predicted quality are selected and added to the training set. The VIS model is subsequently retrained on both the synthetic data and the selected pseudo labels, enabling iterative refinement and progressive performance gains. gle foreground object via motion or consistency cues. While OCLR [34] introduces unsupervised VIS that supports multiple instances, its predefined object count during training prevents dynamic adaptation to varying numbers of instances during inference. Furthermore, prior approaches [4, 17, 20, 34, 37] rely on optical flow estimators (e.g., RAFT [27]) that are trained on human-annotated datasets. VideoCutLER [31] marks breakthrough in unsupervised VIS and achieves unprecedented performance by demonstrating multi-instance segmentation without optical flow dependencies. Its core innovation lies in synthetic video generation via spatial augmentations of CutLER [30] pseudo-labels from ImageNet [7]. However, VideoCutLER remains constrained by synthetic-to-real domain gaps and static instance modeling, i.e., the synthetic videos lack natural and realistic motion patterns. Building upon VideoCutLERs synthetic data paradigm, which generates training videos through spatial augmentations of static image pseudo-labels, we introduce AutoQVIS to address its critical domain gap limitation. While VideoCutLERs synthetic videos provide initial instance awareness, they lack natural motion patterns and real-world appearance variations, hindering adaptation to authentic video dynamics. Our framework bridges this syntheticto-real domain gap through self-training loop that progressively adds quality-filtered pseudo-labels from unlabeled real videos. Inspired by Mask Scoring R-CNN [11], which directly predicts mask quality scores via an auxiliary branch, we implement quality assessment module for pseudo-label filtering of instance masks. AutoQ-VIS advances unsupervised video instance segmentation through an iterative self-training paradigm with quality-aware pseudo-label selection  (Fig. 1)  . The system initializes using synthetic video data from VideoCutLER, which provides pseudo-labels to bootstrap VideoMask2Former [2, 3] VIS model and specialized mask quality predictor (Sec. 3.1). The mask quality predictor estimates mask IoU quality scores by analyzing frame-level features and mask predictions. During multi-round optimization, the VIS model generates pseudo-labels on unlabeled videos, which are then scored by the quality predictor. High-quality pseudo-labels surpassing fixed threshold are progressively incorporated into the training set, enabling dataset augmentation without any human supervision (Sec. 3.2). To enhance the mask head training, we employ DropLoss that zeros out mask losses whose maximum ground-truth overlap falls below 0.01 (Sec. 3.3). By alternating rounds of VIS training (with occasional weight resets) and quality-based dataset expansion, AutoQ-VIS dynamically enriches its training dataset and steadily sharpens segmentation performance. AutoQ-VIS establishes new state-of-the-art in unsupervised video instance segmentation, achieving 52.6% AP50 on the YouTubeVIS-2019 validation set [39]. This represents significant +4.4% AP50 improvement over the previous best-performing method VideoCutLER [31], demonstrating our frameworks effectiveness in motion pattern understanding and instance-level discrimination. To further validate the approachs generalizability, we conduct additional evaluation on the UVO-Dense benchmark [29], where our method achieves consistent performance gains (1.1% AP50 improvement over the previous state-of-the-art approaches) under dense object scenarios. Our key contributions are threefold: (1) AnnotationFree VIS Framework: We propose AutoQ-VIS, an unsupervised framework that overcomes annotation dependency through cyclic pseudo-label refinement with automated quality control, enabling video instance segmentation training directly from unlabeled videos. (2) Automatic Quality Assessment: We propose simple quality predictor that reliably filters pseudo labels across self-training (3) New State-of-the-art Performance: Our rounds. AutoQ-VIS achieves 52.6 AP50 on YouTubeVIS-2019 [39] val split, surpassing the previous state-of-the-art VideoCutLER [31] by 4.4 AP50. 2. Related Work Unsupervised video object segmentation (VOS) [14, 16, 21, 24, 36] targets identifying and segmenting moving objects as foreground elements in video sequences, generating pixel-level binary masks that distinguish these objects from background content without any human-annotated labels. The task applies to both scenarios, including those with single object instance and those with multiple conIt is worth noting that some matericurrent instances. als [15, 26, 40, 46] refer to video salient object detection (SOD) as Unsupervised VOS, which is different task that requires human-annotated labels. Current unsupervised video segmentation methods predominantly rely on detection based on motion or consistency cues. Motion Group [37] uses optical flow to train an unsupervised video object segmentation network, and it uses only optical flow as input. Similar to Motion Group [37], many methods [4, 17, 20, 28, 34, 41] use optical flow as input or use it to supervise the model. However, they all rely on optical flow estimators (e.g., RAFT [27]) that are trained on human-annotated datasets, and most of them [4, 17, 20, 41] can only track one object at time. While some unsupervised video object segmentation methods [28, 34] support multi-object tracking, their multi-object tracking performance is very limited, which makes them unsuitable for the video instance segmentation task. Unsupervised video instance segmentation (VIS) [10, 18, 32, 33, 39, 45] targets identifying and segmenting moving objects from backgrounds and discriminating between distinct instances, all without any human-annotated labels. VideoCutLER [31] is the first unsupervised video segmentation method that is built for the VIS task, without using any human-annotated labels in the whole pipeline. VideoCutLER [31] establishes milestone in unsupervised video instance segmentation (VIS), surpassing previous benchmarks by demonstrating multi-instance segmentation without using optical flow. While its key innovation synthetic video generation pipeline leveraging spatial augmentations of CutLER [30] pseudo-labels (originally derived from ImageNet [7]) enables remarkable performance, the approach remains fundamentally limited by syntheticto-real domain discrepancies and rigid instance modeling. Specifically, the static object configurations in synthesized videos fail to capture authentic motion dynamics observed in natural video sequences. 3.1. Initial training stage Video instance segmentation (VIS) model. Following VideoCutLER [31], we use the VideoMask2Former [2, 3] with the ResNet-50 [9] backbone as our video instance segmentation (VIS) model. Quality predictor. For the quality predictor, as shown in Fig. 2, we use an architecture inspired by Mask Scoring R-CNN [11]. Our architecture processes individual frame features and single-object mask predictions per inference step. Supervision is established through thresholdbinarized (0.5) mask IoU between predictions and matched ground truths, optimized via ℓ2 regression loss. Although the architecture of our quality predictor and Mask Scoring R-CNN is similar, there is key difference between our model and theirs. In Mask Scoring R-CNN, the input predicted mask is threshold-binarized (0.5). In our case, we find it crucial to use the raw predicted mask instead of the threshold-binarized predicted mask. If we use the threshold-binarized predicted mask, the quality predictor cannot produce any meaningful output. Synthetic videos. VideoCutLER [31] provides highquality pseudo-labeled synthetic video dataset that was built on the unlabeled images from ImageNet [7], which is very suitable to train and initialize our VIS model and quality predictor. We also use the trained VideoMask2Former model [2] from VideoCutLER to initialize our VIS model. 3.2. Multi-round self-training As shown in Fig. 1, we optimize the VIS model through iterative self-training and dynamic dataset augmentation. The training dataset is initialized using the synthetic videos from VideoCutLER [31]. Empirically, we find that executing model parameter restoration from the initial model weight (trained in Sec. 3.1) achieves better performance. After training the VIS model, we use the predicted masks on unlabeled videos with confidence score over 0.25 as pseudo-labels. Then we use the quality predictor to predict the IoU of each pseudo-label predicted mask. Let ˆIoUl denote the predicted IoU of label l, and sl denotes the confidence score of label from the class head of the VIS model (see Fig. 2). We define the quality score of label as Ql = ˆIoUl sl. We implement quality-based pseudo-label selection using fixed quality score threshold τth. For each pseudolabel l, we select it if Ql τth. At the end of each round, we add all the pseudo-labels to the training dataset. 3.3. DropLoss for mask head We enhance the mask head training by suppressing loss contributions from low-overlap predictions, following CutLER [30]. For each predicted mask mi, we discard its loss contribution if its maximum ground truth IoU falls below Figure 2. Network architecture of VideoMask2Former [2, 3] and Mask Quality Predictor. Our quality predictor integrates mask predictions and pixel decoder features following [11], employing sequential architecture with four convolution layers (33 kernels, final layer stride of 2 for spatial reduction) followed by three fully-connected layers that ultimately produce mask IoU predictions. Looped self-training method is the method that uses the pseudo labels generated by the model to retrain the model [5, 30, 35, 38, 48, 49]. This method is widely used in different areas related to unsupervised or semi-supervised training, for example, unsupervised domain adaptation [38, 48, 49], semi-supervised image classification [35], semisupervised 3D instance segmentation [5], unsupervised object detection and segmentation [30], and human-in-theloop systems for object detection [13, 22], instance segmentation [6, 23], and 3D shape segmentation [42]. 3. Method AutoQ-VIS operates through four parts: (1) Initial Training (Sec. 3.1): Jointly pretrain VideoMask2Former and the mask quality predictor on VideoCutLERs synthetic videos; (2) Multi-Round Self-Training (Sec. 3.2): Iteratively generate pseudo-labels on real unlabeled videos, filter via quality scores, and augment training data; (3) DropLoss (Sec. 3.3): Suppress low-IoU mask predictions to enhance mask head training. This quality-guided pipeline progressively improves segmentation accuracy without any human annotations; (4) Adaptive fusion: Augment the dataset dynamically through adaptive fusion of newly curated annotations. the threshold τ IoU: the spatiotemporal overlap predicate for two detections: Ldrop(mi) = 1(IoUmax > τ IoU)Lvanilla(mi) (1) ϕ(dnew,v, dexist,v) [1, Tv] : Here, IoUmax is the maximum overlap between mi and any ground truth mask, while Lvanilla denotes the original mask head loss from VideoMask2Former [2, 3]. We employ low threshold (τ IoU = 0.01) to filter only near-zero overlap predictions. 3.4. Adaptive fusion of new curated annotations and old curated annotations To augment the training dataset, at the end of each round of self-training, we need to merge the old curated annotations in the training dataset with the new curated annotations. The augmentation process operates in two distinct modes depending on whether the video containing the new detections already exists in the training set. For novel videos, we perform bulk insertion of all qualified detections. For existing videos, we implement an instance-level fusion protocol that intelligently merges new predictions with old annotations while preserving temporal consistency. Let dv denote an object detection for video v, formally: dv = {(St, mt)}Tv (2) Here, St {0, 1} indicates whether the object mask (pseudo label) of dv at frame is selected for self-training, mt denotes the binary object mask at frame t. Let (k) = {(v, Dv)} denote the training dataset at iteration k, where Dv contains all preserved detections (old annotations) for video v, and D(k) retained denote the set of detections that we retained after the filtering using the quality score threshold τth at the end of iteration (new detections). We need to merge the D(k) retained into the (k) = {(v, Dv)}. For each video, if its not in the training dataset, we simply add all its detections to the training dataset. Formally, let D(k) retained,v denote the detections belong to video v: D(k) retained,v = (cid:110) D(k) retained (cid:12) (cid:12) belongs to video (cid:111) (3) Let (k) new = π1(D(k) retained) π1(T (k)) denote completely new videos, where π1 projects to video identifiers. Let (k) new denote labels of new videos: (k+1) new = (cid:91) vV (k) new (cid:110)(cid:16) v, D(k) retained,v (cid:17)(cid:111) (4) For videos already present in the training set, we implement temporal-aware fusion protocol that resolves conflicts between new detections and old annotations. To merge the existing detections and new detections, we need to identify whether two detections overlap. We define mt mt new mt new mt exist exist 0.5 (5) If in any frames, two detections masks overlap (IoU 0.5), we consider them overlapping detections. If two detections overlap, we need to fuse them. We fuse the new detection and the existing detection frame by frame. For one frame, if only the existing detections label is selected, we use its label; otherwise, we use the new detections label. Let denote the fusion operation: F(dnew,v, dexist,v) = {S merge, mt merge}Tv t=1 where: merge = max(S new, exist) mt merge = (cid:40) mt mt exist new exist = 1 new = 0 if otherwise (6) (7) (8) If the new detection does not overlap with any existing exist = π1(T (k)) denote exist, we prodetections, we simply add it. Let (k) existing videos. For each existing video (k) cess all new detections D(k) retained,v through: (cid:77) D(k) retained,v D(k+1) where the operation (cid:76) is defined as: = D(k) (cid:77) = (cid:91) (D d) dD with per-detection fusion: (9) (10) Ddnew = {dnew} dexist F(dnew, dexist) if dexist D, ϕ(dnew, dexist) if dexist D, ϕ(dnew, dexist) (11) In the end, we merge the labels of new videos and existing videos: where: (k+1) = (k+1) new (k+1) exist (k+1) exist = (cid:91) (cid:110)(cid:16) (cid:17)(cid:111) v, D(k+1) vV (k) exist (12) (13) During training, for each video, we need to sample three frames as the input of the model (following VideoCutLER [31]). To provide good-quality labels, we only sample those frames where all their detections are selected. For each video (k+1) , we define the eligible frame set: train (k+1) eligible (v) = (cid:110) [1, Tv] (cid:12) (cid:12) D(k+1) (cid:111) : = 1 (14) Method YouTubeVIS-2019 AP50 AP75 AP APS APM APL AR10 Method YouTubeVIS-2019 AP50 AP75 AP APS APM APL AR10 MotionGroup [37] OCLR [34] CutLER [30] VideoCutLER [31] AutoQ-VIS vs. previous SOTA 0.5 5.5 36.4 48.2 52.6 +4.4 0.0 0.3 13.5 22.9 28.2 +5.3 0.1 1.6 16.0 24.5 28.1 +3.6 0.0 0.1 3.5 6.7 6.7 +0. 0.4 1.6 13.9 17.7 21.2 +3.5 0.1 6.1 26.0 36.3 40.7 +4.4 1.2 11.5 29.8 42.3 42.5 +0.2 Table 1. YouTubeVIS-2019 val. We reproduced MotionGroup [37], OCLR [34], CutLER [30], and VideoCutLER [31] results with the official code and checkpoints. AutoQ-VIS outperforms the state-of-the-art VideoCutLER by 4.4 AP50. We evaluate results on YouTubeVIS-2019s val split in class-agnostic manner. The training batch is constructed through uniform sampling: B(k+1) train (v) = UniformSample 3 (cid:16) (k+1) (cid:17) eligible (v) (15) Although we only use subset of pseudo-labels, all pseudo-labels persist in the training set regardless of selection status, enabling progressive refinement. 4. Experiments Datasets. Our model is trained on synthetic videos from VideoCutLER [31] and the unlabeled train split of YouTubeVIS-2019 [39]. We evaluate our models performance on the val split of YouTubeVIS-2019 in classagnostic manner. YouTubeVIS-2019 contains 2,883 highresolution YouTube videos, annotated at 6 FPS (2,238 training videos, 302 validation videos, and 343 test videos). Evaluation metrics. Following VideoCutLER [31], we use Average Precision (AP) and Average Recall (AR) as evaluation metrics. We evaluate the models in class-agnostic manner, treating all classes as single one during evaluation. Implementation details. For the initial training stage, we use the pretrained VideoMask2Former model [2, 3] from VideoCutLER [31] to initialize our VIS model. Then we train the VIS model and quality predictor on synthetic videos from VideoCutLER for 8,000 iterations using single V100 GPU, with batch size of 2 and learning rate of 2 105. For each round of multi-round self-training, we train the VIS model for 10,000 iterations on two V100 GPUs, with batch size of 4 and learning rate of 2105. In practice, we find that two rounds of self-training and quality score threshold τth of 0.75 provide the best performance. To balance data distribution between VideoCutLERs [31] extensive synthetic videos and our pseudolabeled videos, we implement balanced stochastic Theoretical limit Practical limit AutoQ-VIS 76.8 62.7 52.6 48.7 33.2 28.2 46.8 33.9 28.1 13.5 4.3 6.7 43.6 27.3 21.2 62.9 53.2 40. 58.0 47.5 42.5 Table 2. Comparison with the theoretical and practical limit. Theoretical Limit: Upper-bound performance achieved by training on ground-truth labels from YouTubeVIS-2019 train split in class-agnostic mode, representing ideal supervision conditions. Practical Limit: Best attainable performance when using all pseudo-labels with IoU 0.5 against ground truth, simulating perfect pseudo-label selection. sampling strategy. Each training batch has an equal probability (50%) of being drawn from either the synthetic videos or the pseudo-labeled set. This prevents overfitting to either domain while maintaining the synthetic datas regularization benefits during self-training. 4.1. Experimental results Comparison with the-state-of-the-art method. We compare AutoQ-VIS with previous top-performing methods on YouTubeVIS-2019 [39] in Tab. 1. AutoQ-VIS achieves remarkable improvement (about 4.4% AP50). Especially for AP75, AutoQ-VIS can outperform the state-of-the-art VideoCutLER [31] by 5.3%. The observed improvements of +2.4% APL, +1.2% APM , and +0.0% APL suggest that our pipeline primarily enhances segmentation performance for medium and large objects, with negligible gains on other categories. Comparison with the theoretical and practical limit. Tab. 2 reveals significant performance gap (10.1 AP50) between AutoQ-VIS and the practical upper bound, indicating substantial potential for improvement through enhanced pseudo-label utilization. The theoretical limit represents fully supervised training using all ground-truth annotations from YouTubeVIS-2019 train set. The practical limit represents an oracle experiment that simulates perfect pseudo-label selection by using all predictions with IoU 0.5 against ground truth. Qualitative results. Qualitative results in Fig. 3 illustrate AutoQ-VISs capability in integrated multi-object segmentation, temporally consistent tracking, and mask-level quality prediction throughout video sequences. Fig. 4 demonstrates AutoQ-VISs advancements over VideoCutLER [31]. As we observe, AutoQ-VIS is capable of discovering more objects and producing higher-quality segmentation masks. 4.2. Ablation studies Component ablation analysis. Tab. 3 quantifies individual component contributions through progressive additions. Figure 3. The qualitative results of AutoQ-VIS on YouTubeVIS-2019 val split. The quality scores are shown in the center of each object. The visual results demonstrate AutoQ-VIS proficiency in simultaneous multi-instance segmentation, persistent object tracking, and per-mask quality assessment across video sequences. Figure 4. The qualitative comparison on YouTubeVIS-2019 val split. AutoQ-VIS demonstrates superior instance discovery capabilities compared to VideoCutLER [31]: (1) Enhanced multiobject detection capacity, particularly for semantically distinct instances (e.g., person and bull in Column 2); (2) Improved segmentation fidelity through precise boundary delineation (e.g., the leopard in Column 3). (3) Better comprehensive instance coverage, eliminating false negatives (e.g., detecting humans in Columns 1 & 4 that VideoCutLER completely misses, even without occlusion or scale challenges). Method YouTubeVIS-2019 AP50 AP75 AP APS APM APL AR10 w/o quality predictor w/o DropLoss w/o resetting each round w/o adaptive fusion w/o freezing predictor AutoQ-VIS VideoCutLER [31] 50.5 48.0 51.6 49.7 49.2 52.6 48.2 25.9 23.7 28.0 26.5 25.8 28.2 22.9 27.2 24.6 28.2 26.8 26.5 28.1 24.5 5.7 3.8 6.4 6.1 6.9 6.7 6. 19.0 16.9 22.8 18.9 18.8 21.2 17.7 40.5 37.8 40.6 40.1 39.3 40.7 36.3 43.3 39.2 42.9 41.4 40.9 42.5 42.3 Table 3. Ablation study on the contribution of each component. Without quality predictor: We remove the quality predictor, and use the confidence score sl from the class head of VIS model (see Fig. 2) as quality score Ql with threshold τth = 0.85. Without DropLoss: We use the vanilla loss for the mask head instead of the DropLoss. Without resetting each round: The model weights are not reset at the beginning of each round. Without Adaptive fusion: Instead of fusing the newly curated annotations and old curated annotations, we simply replace the old curated annotations in the training dataset with new curated annotations. Without freezing predictor: We do not freeze the weight of the quality predictor and train it together with the VIS model. The DropLoss contributes the most significant performance improvement (+4.6% AP50) in our framework. While originally developed by CutLER [30] for image-level segmentation training, this technique was notably absent in VideoCutLERs [31] video instance segmentation pipeline. Our quantitative results in Tab. 3 demonstrate the importance of reintroducing DropLoss in our video-domain self-training Self-training YouTubeVIS-2019 AP50 AP75 AP APS APM APL AR10 1 round 2 rounds 3 rounds 51.3 52.6 52. 26.5 28.2 27.0 26.9 28.1 27.7 7.0 6.7 6.2 22.3 21.2 21.8 38.4 40.7 40.1 43.2 42.5 42. Table 4. Ablation study on the number of self-training rounds. Our framework achieves peak performance (52.6 AP50) at the second round before gradual degradation (-0.6 AP50) from pseudolabel noise accumulation. This establishes round 2 as the optimal stopping point to balance accuracy and error propagation risks. τth 0.95 0.85 0.75 0.50 YouTubeVIS-2019 AP50 AP75 AP APS APM APL AR 48.7 48.8 52.6 52.4 25.3 24.6 28.2 25.7 26.1 26.0 28.1 27.1 5.9 5.8 6.7 6.5 18.8 18.6 21.2 20.1 38.5 38.6 40.7 39. 40.7 41.2 42.5 42.2 Table 5. Ablation study on the quality score threshold τth. Optimal performance (52.6 AP50) emerges at τth = 0.75, balancing valid sample retention and noise suppression. Lower threshold (τth = 0.50) degrades results by admitting too many lowquality predictions, while the higher thresholds (τth = 0.95 and τth = 0.80) oversuppress valid samples. Figure 5. Visualized comparison of quality score Ql and confidence score sl on YouTubeVIS-2019 val split. Here, ρs denotes the Spearmans rank correlation coefficient. Subplot (a) visualizes quality scores Ql and their ground truth IoU. Subplot (b) visualizes confidence scores sl and their ground truth IoU. pipeline. Subsequent components demonstrate incremental gains: freezing the quality predictor contributes +3.4% AP50, adaptive fusion adds +2.9% AP50, and the quality predictor itself provides +2.1% AP50 improvement. Freezing quality predictor prevents overestimation bias by decoupling the optimization process between the VIS model and the quality predictor. Without parameter freezing, both components would reinforce pseudo-label errors through mutual confirmation during self-training cycles, thereby inducing progressively amplified confidence miscalibration Adaptive fusion of new cuin successive iterations. rated annotations and old curated annotations helps the model achieve progressive improvements. Notably, even the confidence score baseline surpasses VideoCutLER by +2.3 AP50, demonstrating fundamental advantages of our self-training method. While model resetting yields marginal gains (+1.0 AP50, +0.2 AP75), it maintains baseline AP performance. Self-training round analysis. Tab. 4 tests how many selftraining rounds work best. The model hits its peak (52.6 AP50) at round 2, then slowly gets worse. This drop occurs because, as we perform more rounds, mistakes in the pseudo-labels pile up. Quality score threshold τth analysis. Tab. 5 examines the impact of quality score thresholds on pseudo-label selection. We observe non-monotonic relationship: While lower thresholds (τth 0.75) generally yield superior performance by retaining more valid samples, excessively lenient selection (τth = 0.50) introduces noisy supervision, degrading results. The optimal balance occurs at τth = 0.75, achieving peak performance. Quality score vs. confidence score. As shown in Fig. 5, our quality score Qs has higher correlation to the IoU of the pseudo label and the ground truth label than the confidence score of VideoMask2Former [2], which proves our quality predictors effectiveness in pseudo-label quality assessment. This results in significant improvement in the final VIS model performance (+2.1 AP50) in Tab. 3. 4.3. Discussion Alignment Between Quality Score, IoU, and Area Distributions. As shown in Figs. 6 and 7, our quality score exhibits strong alignment with ground-truth IoU across different object sizes, reflecting mask reliability with only limited overestimation for large instances. This consistency ensures that the confidence assigned to pseudo labels is well aligned with their true segmentation quality. Furthermore, the area distribution analysis across self-training rounds  (Fig. 7)  indicates that the pseudo-label set remains stable, without substantial drift over iterations. Compared to ground-truth annotations, however, pseudo labels display bias toward larger objects, which is consistent with the Imporslight overestimation observed in quality scores. Method CutLER [30] VideoCutLER [31] AutoQ-VIS vs. prev. SOTA AP50 AP75 3.4 10.3 5.1 13.5 5.8 14.6 +0.7 +1.1 UVO-Dense AP APS APM APL AR10 8.8 11.5 4.6 12.3 14.8 6.2 12.3 16.0 7.0 +0.0 +1.2 +0.8 17.3 26.5 28.9 +2.4 1.3 1.7 1.9 +0.2 Table 6. UVO-Dense val. We reproduced the state-of-the-art VideoCutLER [31] with the official code and checkpoint. AutoQVIS outperforms the previous state-of-the-art VideoCutLER by 1.1 AP50. We evaluate results on UVO-Denses val split in class-agnostic manner. Generalizability. To further assess the generalizability of our pipeline, we conduct experiments on UVO-Dense [29]. Following the same setup as in YouTubeVIS-2019, we train our model on synthetic videos from VideoCutLER [31] together with the unlabeled train split of UVO-Dense, and evaluate it on the val split. UVO-Dense comprises 759 videos sourced from Kinetics-400 [12], annotated at 30 FPS (503 for training and 256 for validation). Compared to YouTubeVIS-2019, UVO-Dense is considerably more challenging: it contains seven times more instance annotations per video and features crowded scenes with complex background motions. The training procedure remains identical to that used for YouTubeVIS-2019 [39], except that we lower the quality score threshold to 0.3 to accommodate the higher object density in UVO-Dense. Table 6 compares AutoQ-VIS with prior state-of-the-art methods reported in [29]. AutoQ-VIS achieves consistent improvement of about 1.1% in AP50, highlighting the robustness and generalizability of our pipeline. Evaluation Scope and Protocol Differences. We do not report results on YouTubeVIS-2021 [39], OVIS [25], or similar benchmarks because they do not release groundtruth annotations for the val split, and class-agnostic VIS evaluation requires access to these labels. We also clarify why our reproduced VideoCutLER [31] baseline (48.2 AP50) differs from the number in its original paper (50.7 AP50): VideoCutLER reports results on the train split, whereas we evaluate on the val split, following same standard VIS evaluation practice. 5. Conclusion We present AutoQ-VIS, quality-aware self-training framework that advances unsupervised video instance segmentation through iterative pseudo-label refinement with automatic quality control. By establishing closed-loop system of pseudo-label generation and automatic quality assessment, our method achieves state-of-the-art performance (52.6 AP50) on YouTubeVIS-2019 val split without requiring any human annotations. The simple quality predictor proves effective in pseudo-label quality assessment. Figure 6. Comparison of average ground-truth IoU and quality score across different mask sizes. The definitions of Small, Medium, and Large follow those in COCO [19]. Here, IoU refers to the intersection-over-union between the predicted mask and the ground-truth annotation. We compare the average IoU and quality score across these area categories. The results demonstrate strong alignment between our quality score and the ground-truth IoU, with only limited overestimation for large objects. This minor bias does not substantially affect the distribution of pseudo labels across training rounds (as shown in Fig. 7). Figure 7. Comparison of object area distributions between pseudo labels across different rounds and ground-truth labels. Here, Log Area refers to the natural logarithm of the normalized area (i.e., object area divided by image area). The definitions of Small, Medium, and Large follow those in COCO [19]. Round 0 denotes pseudo labels produced by the model in the beginning of the self-training. As shown, the object area distributions remain largely stable across different rounds. Nonetheless, relative to the ground truth, our pseudo-label set exhibits higher proportion of large objects. tantly, this bias remains limited and does not hinder the effectiveness of iterative self-training."
        },
        {
            "title": "Acknowledgements",
            "content": "D. P. Papadopoulos and M. O. Kaya were supported by the DFF Sapere Aude Starting Grant ACHILLES."
        },
        {
            "title": "References",
            "content": "[1] Nikita Araslanov, Simone Schaub-Meyer, and Stefan Roth. Dense unsupervised learning for video segmentation. Advances in Neural Information Processing Systems, 34: 2530825319, 2021. 1 [2] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander Schwing. arXiv Mask2former for video instance segmentation. preprint arXiv:2112.10764, 2021. 2, 3, 4, 5, 7 [3] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. 2, 3, 4, 5 [4] Subhabrata Choudhury, Laurynas Karazija, Iro Laina, Andrea Vedaldi, and Christian Rupprecht. Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion. In British Machine Vision Conference (BMVC), 2022. 1, 2 [5] Ruihang Chu, Xiaoqing Ye, Zhengzhe Liu, Xiao Tan, Xiaojuan Qi, Chi-Wing Fu, and Jiaya Jia. Twist: Two-way inter-label self-training for semi-supervised 3d instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11001109, 2022. 3 [6] Thanos Delatolas, Vicky Kalogeiton, and Dim Papadopoulos. Learning the what and how of annotation in video object segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024. 3 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 1, 2, 3 [8] Emanuela Haller and Marius Leordeanu. Unsupervised object segmentation in video by efficient selection of highly probable positive features. In Proceedings of the IEEE international conference on computer vision, pages 50855093, 2017. [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 3 [10] Miran Heo, Sukjun Hwang, Seoung Wug Oh, Joon-Young Lee, and Seon Joo Kim. Vita: Video instance segmentation via object token association. Advances in neural information processing systems, 35:2310923120, 2022. 1, 2 [11] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, and Xinggang Wang. Mask scoring r-cnn. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 64096418, 2019. 2, 3 [12] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 8 [13] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 2020. 3 [14] Hala Lamdouar, Charig Yang, Weidi Xie, and Andrew Zisserman. Betrayed by motion: Camouflaged object discovery via motion segmentation. In Proceedings of the Asian conference on computer vision, 2020. [15] Minhyeok Lee, Suhwan Cho, Dogyoon Lee, Chaewon Park, Jungho Lee, and Sangyoun Lee. Guided slot attention for unsupervised video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 38073816, 2024. 2 [16] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James Rehg. Video segmentation by tracking many figureground segments. In Proceedings of the IEEE international conference on computer vision, pages 21922199, 2013. 2 [17] Long Lian, Zhirong Wu, and Stella Yu. Bootstrapping objectness from videos by relaxed common fate and visual grouping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14582 14591, 2023. 1, 2 [18] Chung-Ching Lin, Ying Hung, Rogerio Feris, and Linglin He. Video instance segmentation tracking with modified In Proceedings of the IEEE/CVF convae architecture. ference on computer vision and pattern recognition, pages 1314713157, 2020. 1, 2 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. European conference on computer vision, pages 740755. Springer, 2014. 8 [20] Etienne Meunier, Anaıs Badoual, and Patrick Bouthemy. Em-driven unsupervised learning for efficient motion segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):44624473, 2022. 1, [21] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. IEEE transactions on pattern analysis and machine intelligence, 36(6): 11871200, 2013. 2 [22] Dim Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio Ferrari. We dont need no bounding-boxes: Training object class detectors using only human verification. In CVPR, 2016. 3 [23] Dim Papadopoulos, Ethan Weber, and Antonio Torralba. Scaling up instance annotation via label propagation. In ICCV, 2021. 3 [24] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. 2 [25] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai. Occluded video instance segmentation: benchmark. International Journal of Computer Vision, 2022. 8 [26] Sucheng Ren, Wenxi Liu, Yongtuo Liu, Haoxin Chen, Guoqiang Han, and Shengfeng He. Reciprocal transformations for unsupervised video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1545515464, 2021. [27] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 1, 2 [28] Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Salvador, Ferran Marques, and Xavier Giro-i Nieto. Rvos: Endto-end recurrent network for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 52775286, 2019. 2 [29] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. Unidentified video objects: benchmark for dense, openIn Proceedings of the IEEE/CVF inworld segmentation. ternational conference on computer vision, pages 10776 10785, 2021. 2, 8 [30] Xudong Wang, Rohit Girdhar, Stella Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3124 3134, 2023. 1, 2, 3, 5, 6, 8 [31] Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar, and Trevor Darrell. Videocutler: Surprisingly simple unsupervised video instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2275522764, 2024. 1, 2, 3, 4, 5, 6, 8 [32] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87418750, 2021. 1, 2 [33] Junfeng Wu, Yi Jiang, Song Bai, Wenqing Zhang, and Xiang Bai. Seqformer: Sequential transformer for video instance segmentation. In European Conference on Computer Vision, pages 553569. Springer, 2022. 1, [34] Junyu Xie, Weidi Xie, and Andrew Zisserman. Segmenting moving objects via an object-centric layered representation. Advances in neural information processing systems, 35: 2802328036, 2022. 1, 2, 5 [35] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687 10698, 2020. 3 [36] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. Youtube-vos: Sequence-to-sequence In Proceedings of the Eurovideo object segmentation. pean conference on computer vision (ECCV), pages 585 601, 2018. 2 [37] Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, and Weidi Xie. Self-supervised video object segmentation by motion grouping. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 71777188, 2021. 1, 2, [38] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. St3d: Self-training for unsupervised domain adaptation on 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1036810378, 2021. 3 [39] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segIn Proceedings of the IEEE/CVF international mentation. conference on computer vision, pages 51885197, 2019. 1, 2, 5, 8 [40] Shu Yang, Lu Zhang, Jinqing Qi, Huchuan Lu, Shuo Wang, Learning motion-appearance coand Xiaoxing Zhang. In Proattention for zero-shot video object segmentation. ceedings of the IEEE/CVF international conference on computer vision, pages 15641573, 2021. 2 [41] Yanchao Yang, Antonio Loquercio, Davide Scaramuzza, and Stefano Soatto. Unsupervised moving object detection In Proceedings of via contextual information separation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 879888, 2019. 2 [42] Li Yi, Vladimir Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 2016. 3 [43] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: diverse driving dataset for heterogeneous In Proceedings of the IEEE/CVF conmultitask learning. ference on computer vision and pattern recognition, pages 26362645, 2020. [44] Kaihua Zhang, Zicheng Zhao, Dong Liu, Qingshan Liu, and Bo Liu. Deep transport network for unsupervised video obIn Proceedings of the IEEE/CVF interject segmentation. national conference on computer vision, pages 87818790, 2021. 1 [45] Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, and Pengfei Wan. Dvis: Decoupled video In Proceedings of the instance segmentation framework. IEEE/CVF International Conference on Computer Vision, pages 12821291, 2023. 1, 2 [46] Tianfei Zhou, Jianwu Li, Shunzhou Wang, Ran Tao, and Jianbing Shen. Matnet: Motion-attentive transition network for zero-shot video object segmentation. IEEE transactions on image processing, 29:83268338, 2020. 2 [47] Tianfei Zhou, Fatih Porikli, David Crandall, Luc Van Gool, and Wenguan Wang. survey on deep learning technique for video segmentation. IEEE transactions on pattern analysis and machine intelligence, 45(6):70997122, 2022. 1 [48] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In Proceedings of the European conference on computer vision (ECCV), pages 289 305, 2018. 3 [49] Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and JinIn Prosong Wang. Confidence regularized self-training. ceedings of the IEEE/CVF international conference on computer vision, pages 59825991, 2019. Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training"
        },
        {
            "title": "Supplementary Material",
            "content": "where D(k) after spatiotemporal NMS at iteration k. suppressed,v denotes preserved detections for video For each detection D(k) global, we define: Qt = sd ˆIoU t [1, Tv] (20) where Qt is the quality score of detection in frame t, sd is the confidence score of detection d, and Tv denotes the number of frames in video v. We implement quality-based pseudo-label selection using fixed quality threshold τth. For each detection D(k) global across all videos: (cid:40) d = 1 Qt 0 τ (k) otherwise (21) where denote whether pseudo-label of detection in frame is selected. For each detection, if its results are not selected in any frames, we discard it: (cid:40) D(k) retained = dv D(k) global (cid:41) d > 0 (cid:12) (cid:12) (cid:12) (cid:12) Tv(cid:88) t=1 (22) where D(k) k. retained is the set of detections we retain in iteration 7. Additional qualitative visualizations We provide additional qualitative results of our VIS model and quality predictor in Fig. 8 and Fig. 9. 6. Detailed methodology This section supplements what is not clearly stated in Sec. 3.2. 6.1. Automated pseudo-annotation with spatiotemporal NMS After the training of the VIS model, we use it to label the unlabeled videos. Let = {di}N i=1 denote the initial detection set per video, where each detection = (si, {mt t=1) contains: si [0, 1]: Confidence score {mt t=1: Binary mask sequence across frames We filter those detections that have confidence scores i}T i}T larger than or equal to 0.25: Dfiltered = {di si 0.25} (16) However, the detection sets may contain duplicate detections. To solve this problem, we need to perform spatiotemporal non-maximum suppression. First, we sort the detections based on their confidence scores: Dsorted = {d(k)}Dfiltered k=1 s.t. < : s(i) s(j) (17) Our method eliminates redundant detections through spatiotemporal overlap analysis: Any lower-confidence prediction is suppressed if exhibiting mask overlap (IoU 0.5) with higher-confidence detections in at least one video frame. Formally, let Dsuppressed Dsorted represent the preserved detection set after suppression: d(k) Dsuppressed d(p) Dsuppressed where < s.t. [1, ] : mt mt (k) mt (k) mt (p) (p) (cid:124) (cid:123)(cid:122) Frame-specific overlap condition (cid:125) 0.5 (18) where denotes pixel cardinality. This temporalexistential criterion suppresses duplicates appearing in any frame of the video sequence. 6.2. Confidence-aware filtration via quality predictor Let D(k) global denote the union of Dsuppressed of all videos: (cid:91) D(k) global = D(k) suppressed,v (19) vV Figure 8. Qualitative results of our VIS model on YouTubeVIS-2019 val split. Figure 9. Qualitative results of our quality predictor on YouTubeVIS-2019 train split. The quality scores are shown in the center of each pseudo label."
        }
    ],
    "affiliations": [
        "Pioneer Centre for AI",
        "Technical University of Denmark"
    ]
}