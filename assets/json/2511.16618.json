{
    "paper_title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
    "authors": [
        "Haofeng Liu",
        "Ziyue Wang",
        "Sudhanshu Mishra",
        "Mingqi Gao",
        "Guanyi Qin",
        "Chang Han Low",
        "Alex Y. W. Kong",
        "Yueming Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \\textbf{SAM2} for \\textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\\mathcal{J}$\\&$\\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\\mathcal{J}$\\&$\\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S."
        },
        {
            "title": "Start",
            "content": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking Ziyue Wang1 Sudhanshu Mishra1 Mingqi Gao2 Haofeng Liu1 Guanyi Qin1 Chang Han Low1 Alex Y. W. Kong1 Yueming Jin1* 1National University of Singapore 2University of Sheffield haofeng.liu@u.nus.edu, ymjin@nus.edu.sg 5 2 0 2 0 ] . [ 1 8 1 6 6 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "for computerSurgical video segmentation is crucial assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited longterm tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instancelevel spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, foundation model enhancing SAM2 for Surgical iVOS through: (1) DiveMem, trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average &F over vanilla SAM2. SAM2S further advances performance to 80.42 average &F, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S. 1. Introduction Surgical video segmentation is crucial for computerassisted surgery, enabling precise localization and tracking of instruments, tissues, and anatomical structures within video sequences [27, 53]. This capability supports critical *Corresponding author. Figure 1. Overview of SA-SV benchmark and SAM2S framework. (a) Dataset scale comparison. (b) SA-SV benchmark distribution. (c) SAM2 for natural videos. (d) SAM2S for surgical videos with enhanced long-term tracking and domain-specific modules. applications including intraoperative guidance and postoperative skill assessment, ultimately enhancing surgical precision and improving patient outcomes [15, 45]. However, existing segmentation models [37, 40] with predefined categories cannot adapt to the dynamic and diverse nature of surgical procedures, where surgeons need to interactively specify and track objects of interest. Recently, SAM2 [44] has significantly advanced iVOS, offering prompt-based interaction and temporal modeling for surgical applications. It enables users to specify target regions through visual prompts (e.g., points, boxes, or masks) and generates masklets across the video through memory-based propagation. However, directly applying SAM2 to surgical scenarios faces substantial challenges in both dataset and methodology, as illustrated in Fig. 1. From the dataset aspect, surgical scenes present significant domain gap from natural environments due to their unique spatial and temporal characteristics: lighting variations and reflections, occlusions from blood and smoke, and substantially longer durations with dynamic scene variations [38]. Furthermore, existing surgical datasets suffer from inherent limitations that hinder iVOS model development: they lack comprehensive masklet annotations spanning diverse procedures, which are essential for temporal modeling and zero-shot generalization evaluation. Addressing these challenges requires large-scale benchmark with masklet annotations spanning diverse surgical procedures, to facilitate comprehensive development and evaluation for long-term tracking and zero-shot generalization. From the methodology aspect, SAM2 faces significant challenges in surgical scenarios, particularly in long-term tracking. Unlike natural videos that typically span seconds or minutes [13, 22], surgical procedures can last for hours [2]. Frequent camera motion and zooming in surgical videos further challenge SAM2s memory mechanism, leading to tracking failures when objects disappear or reappear over extended durations. Beyond these temporal challenges, instrument segmentation introduces another level of difficulty. While surgical instruments have limited categories with consistent semantic properties across procedures, SAM2s class-agnostic paradigm cannot leverage this semantic information to maintain consistent longterm tracking. For tissue segmentation, boundaries between anatomical structures are inherently ambiguous due to similar textures and overlapping regions. This challenge is compounded by annotation inconsistencies across multisource datasets, where boundaries of identical tissues may be labeled differently due to varying medical standards across countries and institutions. To address these limitations, we first construct SA-SV (Segment Anything in Surgical Video), the largest-scale benchmark for surgical iVOS to the best of our knowledge, as illustrated in Fig. 1(a,b). Building on SA-SV, we propose SAM2S, foundation model that preserves SAM2s core architecture while incorporating surgical-specific adaptations. SAM2S integrates specialized memory mechanisms, temporal semantic learning, and ambiguity-resilient learning to achieve robust long-term tracking across multi-source datasets while maintaining real-time inference. The main contributions of this paper include: SA-SV Benchmark: We construct the largest surgical iVOS benchmark with masklet annotations across eight procedure types, comprising 61k frames and 1.6k masklets from 17 open-source datasets, enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. DiveMem for Long-term Tracking: We propose Diverse Memory (DiveMem) mechanism that employs hybrid temporal sampling during training and diversitybased frame selection during inference, addressing viewpoint overfitting in long-term surgical tracking. TSL for Semantic Understanding: We introduce Temporal Semantic Learning (TSL) that leverages semantic categories of surgical instruments through visionlanguage contrastive learning, enabling semantic-aware tracking while preserving class-agnostic generalization. ARL for Multi-source Training: We introduce Ambiguity-Resilient Learning (ARL) that handles annotation inconsistencies across multi-source datasets through uniform label softening, improving model calibration and robustness at ambiguous tissue boundaries. 2. Related Work Interactive Video Object Segmentation. Video Object Segmentation (VOS) aims to segment and track objects across video sequences [11, 17, 31, 52]. Beyond traditional semi-supervised VOS requiring complete first-frame masks [12, 35, 39, 42], interactive VOS (iVOS) enables users to specify targets with lightweight prompts such as points or scribbles [21, 50]. SAM2 [44] represents stateof-the-art iVOS framework integrating promptable segmentation with memory-augmented temporal modeling for masklet propagation. However, adapting iVOS from natural to surgical videos poses significant challenges due to domain gaps, inherent datasets limitations, and the sustained demands on temporal modeling and memory mechanisms. Surgical Video Segmentation Datasets. Surgical videos present unique challenges, including complex lighting conditions, frequent occlusions, instrument deformation, and extended procedure duration [1]. Current surgical video datasets (e.g., EndoVis17 [5], CholecSeg8k [24], AutoLaparo [49]) primarily provide frame-level masks for semantic segmentation, lacking masklet annotations with temporal consistency required for iVOS. Additionally, these datasets typically focus on single procedure types, limiting crossprocedure generalization. Aggregating multi-source surgical datasets also introduces annotation inconsistencies due to varying labeling standards, particularly in ambiguous regions such as tissue boundaries [25]. These gaps motivate the construction of comprehensive surgical iVOS benchmark with consistent masklet annotations and specialized learning strategies for handling annotation ambiguity. Memory Management for SAM2. SAM2 maintains memory bank of historical frames to ensure temporal consistency, but its finite capacity makes long-term modeling susceptible to target loss and error accumulation in extended videos. Training-free methods such as SAMURAI [51], DAM4SAM [48], and SAM2Long [14] improve memory utilization through dynamic frame selection, preserving generality without trainable long-term modeling. Surgical adaptations, including SurgicalSAM2 [33] and Table 1. Dataset composition in the SA-SV benchmark. Columns Instrument and Tissue show the number of masks. indicates unavailable annotations. Avg. Dur. (s) denotes the average video length in seconds. Dataset Video Frame Instrument Tissue Masklet Avg. Dur. (s) ART-Net BKAI-IGH DSAD-I Endoscapes Kvasir-SEG RoboTool AutoLaparo CholecSeg8k CIS-Train ClinicDB DSAD-V GraSP Total CIS-Test EndoVis17 EndoVis18 Hyst-YT PolypGen RARP50 SurgAI3.8k Total 816 - 1000 - 4177 - 468 - 1000 - 514 - 1800 300 45 4338 10 19029 612 29 4390 76 13 3449 473 7229 4 900 3 596 4 1973 6 2037 21 3252 10 51 3817 99 19804 Training set - 816 1176 - 4864 - 1288 936 1064 - - 1008 1057 2905 6242 21136 - 31416 647 - 4248 - 9025 - 67242 20586 Test set 10971 2265 1676 4113 - 10656 - 29681 - - 807 - 1761 - 3817 6385 - - - - - - 738 300 77 32 91 130 31 10 28 19 24 81 51 244 - - - - - - 6 4 1903 21 58 265 - 1807 300 149 329 19 325 75 - ReSurgSAM2 [34], optimize memory mechanisms for surgical scenarios. While training-free methods preserve generality, they cannot capture the domain-specific temporal dependencies and semantic characteristics of surgical videos, motivating our trainable adaptation approach that learns surgical-specific patterns. 3. SA-SV Benchmark To address the domain gap and inherent dataset limitations for surgical iVOS development as discussed in Section 1, we propose the SA-SV benchmark, the largest surgical iVOS benchmark with comprehensive masklet annotations spanning diverse surgical procedures. SA-SV comprises over 61k frames and 1.6k masklets across eight procedure types with instrument and tissue annotations, enabling comprehensive model development and zero-shot evaluation. Notably, SA-SV includes long-duration test subsets (e.g., CIS-Test: 1807s1, RARP50: 325s) that are substantially longer than general VOS benchmarks, facilitating long-term tracking assessment. Key statistics are summarized in Table 1, with full statistics provided in the supplementary material. Dataset Reconstruction and Refinement. Since existing surgical semantic segmentation datasets lack masklet an1All durations in parentheses refer to average video duration. notations, we refined them by converting their original object masks into masklets through three key steps: (1) assigning each object unique, temporally consistent instance ID for tracking across frames, (2) standardizing instrument class labels across datasets following clinical guidelines [46] (while keeping tissue class labels unchanged), and (3) manually correcting missing or erroneous masks under the supervision of qualified surgeon. For CholecSeg8k alone, over 20,000 object masks with erroneous boundaries were corrected. Importantly, no new object masks were introduced beyond correcting annotation errors. Dataset Composition. As shown in Table 1, our SASV benchmark was built from open-source surgical video datasets covering seven distinct procedures and one multiprocedural collection, including: 1. Cholecystectomy: Endoscapes [3, 36], CholecSeg8k [24], CholecInstanceSeg (CIS) [2]; 2. Colonoscopy: PolypGen [4], Kvasir-SEG [26], BKAIIGH [29], CVC-ClinicDB [9]; 3. Gynecology: SurgAI3.8k [54]; 4. Hysterectomy: AutoLaparo [49], ART-Net [20], Hyst-YT [16]; 5. Myotomy: DSAD [10]; 6. Nephrectomy: Endovis17 (EV17) [5], Endovis18 (EV18) [6]; 7. Prostatectomy: GraSP [7, 8, 47], RARP50 [41]; 8. Multi-procedural: RoboTool [18]. Dataset Utilization. For training, we employ mixed image-video training strategy to maximize data utilization across diverse surgical scenarios following SAM2. DSAD is partitioned into image and video subsets (DSAD-I and DSAD-V), while CIS-Train (1903s) and GraSP (265s) provide enough long-duration videos for long-term model development. For evaluation, our dataset split enables comprehensive assessment of zero-shot generalization capabilities across instruments and tissues, with all test subsets remain unseen during training. EV17 and EV18 represent nephrectomy, surgical procedure type completely unseen in training, enabling assessment of zero-shot generalization across surgical procedure types. We further split EV18 into EV18-I and EV18-T to separately evaluate instrument and tissue segmentation performance. For long-term tracking evaluation, we adopt four datasets with extended average duration (300s): EV17 (300s), Hyst-YT (329s), RARP50 (325s), and CIS-Test (1807s30min). These surgical datasets are substantially longer than general VOS benchmarks such as SA-V [44] (14s), MOSEv2 [13] (19s), LVOS [22] (95s), and LVOSv2 [23] (68s), enabling the evaluation of surgical long-term tracking. 4. Methodology 4.1. SAM2 Background and Method Overview SAM2 extends SAM [28] to videos through temporal memory attention and queue-based short-term memory mechanism that conditions current frame features on the initial Figure 2. Overview of SAM2S for surgical video segmentation. DiveMem handles long-term tracking, TSL enhances semantic understanding, and ARL addresses annotation ambiguity. frame and recent predictions. For each frame, the mask decoder generates IoU scores iout for mask quality estimation. While SAM2 demonstrates strong performance in general domains, it faces limitations in surgical scenarios: (1) insufficient long-term tracking, (2) inability to leverage instrument semantic information, and (3) unable to handle annotation ambiguities across multi-source datasets. To address these challenges, we propose SAM2S (as illustrated in Fig. 2), which enhances SAM2 through three key innovations developed with our SA-SV. Given surt=1 where ft R3HW and gical video sequence {ft}T initial prompts p1 provided in the first frame, our goal is to generate temporally consistent binary segmentation masks t=1 where mt RHW . Our framework integrates: {mt}T DiveMem for enhanced long-term tracking, TSL for instrument semantic understanding, and ARL for handling annotation ambiguities. These innovations are unified through: Ltotal = λarlLarl + Liou + Ldice + Locc + λtslLtsl (1) where Liou, Ldice, Locc are standard SAM2 losses, while Larl and Ltsl address annotation ambiguity and provide semantic supervision with λarl and λtsl as balancing weights. 4.2. Diverse Memory for Long-term Tracking struggles to maintain robust performance when tracked objects disappear for extended periods. While training-free memory enhancements [14, 48] offer plug-and-play deployment, they cannot learn temporal dependencies crucial for extended surgical procedures. To address these limitations, we propose DiveMem, enhancing SAM2s memory capabilities through both training and inference strategies. DiveMem Sampling. During training, we randomly sample three frames across the entire video, with one designated as the conditional frame and two as long-term memory frames equipped with specialized learnable temporal embeddings. The remaining five frames are consecutive non-conditional frames. This hybrid sampling strategy simulates large temporal gaps during inference, enabling robust long-term dependency learning across diverse temporal contexts. DiveMem Filter. During inference, we select long-term memory frames when the target is stably present for = 5 consecutive frames with iout > γiou to form candidate buffer B, where γiou is the IoU confidence threshold. From this buffer, we select the most diverse candidate based on cosine similarity to the latest long-term memory frame: = arg min biB E(bi) E(lk) E(bi)E(lk) (2) Surgical videos typically feature long durations with frequent camera motion and zooming. However, SAM2s greedy strategy of selecting only recent frames as memory hampers long-term tracking, causing redundancy and potential viewpoint overfitting [14]. Consequently, SAM2 where E() is the image encoder, bi is the i-th candidate frame, lk is the latest frame in long-term memory bank L, and is the selected most diverse frame. This strategy aggregates diverse spatio-temporal information to mitigate viewpoint overfitting while minimizing error accumulation through high-confidence frames. Selected frames are augmented with learnable temporal embeddings, and the buffer is cleared after selection to extend temporal coverage. To enhance efficiency, we maintain queue with capacity Nl for long-term memory while permanently retaining the initial memory l0. By combining SAM2s vanilla shortterm memory that spans the most recent six frames with our diverse long-term memory, SAM2S maintains reliable and diverse memory bank for robust tracking. 4.3. Temporal Semantic Learning Most existing VOS methods adopt class-agnostic designs that rely primarily on pixel-level correlations while neglecting semantic information. However, surgical scenarios present unique opportunity to leverage semantic cues: surgical instruments have limited and well-defined categories with consistent semantic properties across procedures [46], making them ideal for semantic learning. In contrast, anatomical tissues exhibit numerous overlapping categories that are harder to distinguish semantically. Building on this insight, our SA-SV benchmark provides comprehensive coverage of instrument categories, enabling the development of semantic-aware VOS methods specifically tailored for surgical instrument tracking. The TSL module employs learnable CLS token xc that first attends to memory features to capture the historical semantic context of the tracked object, then performs crossattention with current frame features to generate temporal semantic representation c. To strengthen semantic discrimination, we integrate CLIPs text encoder [43] for visionlanguage contrastive learning, enabling the model to distinguish the tracked object from all semantic categories in the dataset. The TSL loss is defined as: (cid:32) Ltsl = log exp(sim(x k=1 exp(sim(x (cid:80)K c, tpos)/τ ) c, tk)/τ ) (cid:33) (3) where is the total number of instrument categories, tpos denotes the positive text features corresponding to the objects semantic category, and τ is the temperature parameter. For samples without semantic labels, Ltsl is omitted. During inference, the text encoder can be omitted for efficiency. 4.4. Ambiguity-Resilient Learning Multi-source surgical datasets exhibit annotation inconsistencies due to varying standards across institutions, particularly at ambiguous tissue boundaries. These inconsistencies create conflicting supervision signals, resulting in overconfident predictions and poor model calibration in safetycritical surgical applications [19]. To address this challenge, we propose ARL to tackle annotation uncertainty through uniform label softening. The principle is to transform discrete annotation spaces into continuous probability distributions. For hard labels yt, we apply Gaussian kernel Gσ convolution to obtain softened labels ˆyt: ˆyt(u, v) = (cid:88) i,j Gσ(u i, j) yt(i, j) (4) (cid:16) (cid:17) (u)2+(v)2 2σ2 2πσ2 exp where Gσ(u, v) = 1 is the Gaussian kernel, (u, v) is the target pixel position, (i, j) is neighboring coordinates, and σ controls the softening degree with 5 5 kernel. The ARL loss uses focal loss [32] between predicted probability and softened labels: Larl = FocalLoss( ˆmt, ˆyt) (5) where ˆmt is the predicted mask probability. 5. Experiments 5.1. Experimental Settings Prompting and Evaluation Protocol. To better align with real-world usage, we adopt challenging protocol: prompt once, track throughout, where prompts are provided only in the first frame, and the model autonomously tracks the object throughout the sequence. We use 3-click initialization by default unless otherwise specified. The click placement follows SAM2s interactive scheme [44]: the first click is placed at the mask center, with subsequent clicks positioned at error region centers for iterative refinement. For evaluation, we use the standard iVOS metric &F, the mean of region accuracy (J ) and boundary accuracy (F). FPS is also reported to assess inference efficiency, measured during end-to-end processing on single A6000 GPU. Model and Training Configuration. SAM2S is implemented based on the SAM2 architecture with Hiera-B+ backbone, initialized from SAM2 pre-trained weights. The model was trained for 30 epochs with learning rate of 1 105 using mixed imagevideo training (1:4 ratio), where image datasets were used for interactive image segmentation and video datasets with masklets for iVOS training. Video sampling used 1:1 ratio of DiveMem and SAM2 vanilla strategies. All experiments were conducted on four NVIDIA A6000 GPUs. Hyperparameter Details. We set: = 5 (candidate buffer size), γiou = 0.95 (IoU threshold), Nl = 4 (long-term memory capacity), λarl = 20 and λtsl = 0.1 (loss balancing), σ = 1.0 (Gaussian kernel Gσ), and τ = 100 (contrastive temperature, following OVSeg [30]). 5.2. Comparative Experiment We quantitatively compared SAM2S against: (1) SAM2; (2) SAM2+Cutie combining SAM2 for initialization with the advanced VOS method Cutie [12]; (3) SAM2-based methods enhanced with training-free memory management (SAM2Long [14], DAM4SAM [48], SAMURAI [51]); (4) Table 2. Performance comparison of iVOS methods under zero-shot evaluation with 3-click initialization. Top: Vanilla models; Bottom: Models fine-tuned on SA-SV. All models benefit from SA-SV fine-tuning, with SAM2S achieving the best performance. Model Resolution Instrument Tissue Average FPS EV17 EV18-I RARP50 Hyst-YT CIS-Test EV18-T SurgAI3.8k Polypgen SAM2 SAM2+Cutie SAM2Long DAM4SAM SAMURAI SurgicalSAM2 MedSAM2 SAM2 SAM2+Cutie SAM2Long DAM4SAM SAMURAI SurgicalSAM2 SAM2S (ours) 1024 1024 1024 1024 1024 1024 512 512 512 512 512 512 512 512 75.37 68.68 72.31 72.42 68.54 72.94 61.43 81.96 73.78 77.74 76.26 70.46 81.88 86. 82.36 79.35 74.42 70.71 67.73 77.26 60.08 79.89 75.29 78.91 78.09 74.75 80.80 82.37 48.46 66.22 42.66 39.69 40.00 47.95 29.24 76.51 74.61 65.50 61.09 60.41 76.25 79.47 73.92 82.17 70.05 68.28 64.09 71.58 62.58 83.89 78.65 78.60 73.94 74.46 81.54 87. 42.51 80.44 36.39 36.91 31.40 42.83 31.71 80.09 86.98 63.79 62.95 59.53 79.90 89.65 59.02 67.12 55.66 70.98 63.88 59.17 7.76 66.71 58.98 57.44 72.03 66.47 67.04 72.29 64.66 63.43 66.09 65.01 64.62 63.82 28.37 75.29 78.59 76.07 74.68 75.44 74.86 79. 60.23 54.12 64.16 61.52 60.75 60.67 60.03 66.15 63.03 66.26 65.19 63.73 66.29 66.30 63.32 70.19 60.22 60.69 57.63 62.03 42.65 76.31 73.74 70.54 70.53 68.16 76.07 80.42 26 53 11 24 12 27 80 69 53 22 63 22 71 Table 3. Performance comparison of iVOS methods under zero-shot settings with 1-click initialization."
        },
        {
            "title": "Average",
            "content": "EV17 EV18-I RARP50 Hyst-YT CIS-Test EV18-T SurgAI3.8k Polypgen SAM2 SAM2+Cutie SAM2Long DAM4SAM SAMURAI SurgicalSAM2 MedSAM2 SAM2 SAM2+Cutie SAM2Long DAM4SAM SAMURAI SurgicalSAM2 SAM2S (ours) 1024 1024 1024 1024 1024 1024 512 512 512 512 512 512 512 512 61.32 60.37 59.84 62.74 59.73 61.47 60.74 78.09 71.78 68.23 72.07 65.33 76.20 83.76 67.96 68.50 61.66 66.90 56.90 70.44 58.90 75.71 71.08 71.27 76.24 69.56 73.48 79. 43.74 66.62 38.55 38.25 34.99 43.05 29.11 72.25 71.72 57.34 58.15 53.60 71.99 79.11 64.95 68.70 63.39 60.11 57.48 64.66 60.16 79.27 81.94 78.63 71.42 73.34 79.35 86.54 40.34 79.26 35.24 35.93 29.05 39.07 29.79 80.50 86.61 62.66 61.22 59.69 80.62 88. 62.84 58.52 62.86 62.59 62.54 62.97 5.14 65.14 43.37 65.62 63.77 65.72 65.25 65.79 51.51 53.60 52.13 52.28 52.93 51.84 22.05 69.21 72.12 69.18 68.64 68.87 68.94 75.91 37.44 37.99 41.81 43.00 38.79 36.97 55.71 61.36 61.43 60.41 60.23 59.46 62.78 65. 53.76 61.70 51.94 52.73 49.05 53.81 40.20 72.69 70.01 66.67 66.47 64.45 72.33 78.07 MedSAM2 [55] adapted for medical imaging; (5) SurgicalSAM2 [33] optimized for surgical scenarios. We conducted comprehensive comparisons with the state-of-the-art iVOS methods across multiple surgical datasets in the SA-SV benchmark under two configurations: (1) SAM2-based models using their vanilla pretrained weights at resolution 1024 for optimal performance, and (2) models fine-tuned on SA-SV at 512 resolution for efficiency. All SAM2-based models used the Hiera-B+ backbone. MedSAM2 operates at 512, while Cutie utilizes 480p (short side) according to the default configurations. For clinical practicality, we primarily evaluated 3-click and 1-click initialization in Tables 2 and 3, with 5-click, box, and mask variants detailed in the supplementary materials. Zero-shot Generalization. All subsets in our SA-SV test set are completely unseen during training, enabling rigorous evaluation of models zero-shot generalization across diverse surgical procedures. Tables 2 and 3 present results under 3-click and 1-click initialization across eight test subsets, with Average denoting average &F across all test subsets. The vanilla SAM2-based models achieve limited performance, with SAM2 reaching only 63.32 average &F. MedSAM2, despite being fine-tuned on extensive general medical data, achieves only 42.65 points, revealing substantial domain gap between general medical data and surgical data. In contrast, fine-tuning on SA-SV yields Figure 3. Qualitative comparison between SAM2 (vanilla), SAM2 (FT), SAM2Long (FT), and SAM2S on RARP50. Frame indices indicate timestamps in seconds, spanning from 150s to 560s (410s duration). (SAM2Long, DAM4SAM, SAMURAI) perform comparably on tissue datasets but show degradation compared to SAM2 on instrument datasets, as shown in Tables 2, 3 (e.g., EV17, RARP50, Hyst-YT, and CIS-Test). This failure stems from inherent characteristics of surgical videos: frequent camera motion and zooming cause instruments to disappear for extended periods, requiring the model to maintain temporal context for accurate re-identification. However, training-free methods discard frames without visible targets, losing critical temporal cues and leading to false positive detections of visually similar objects. Moreover, uncertainty-based (SAM2Long) or distractor-driven (DAM4SAM) selection strategies might introduce additional noise in visually ambiguous surgical scenes. In contrast, DiveMem employs trainable diversity-based sampling, maintaining both diverse long-term memory and complete short-term memory to preserve comprehensive temporal context that the training-free approaches fail to maintain. On long-duration datasets (EV17: 300s, RARP50: 325s, Hyst-YT: 329s, CIS-Test: 1809s30min), SAM2S achieves consistent improvements of 4.76, 2.96, 3.57, and 9.56 &F over fine-tuned SAM2. While SAM2+Cutie shows competitive performance on CIS-Test, SAM2S still surpasses it by 2.67 &F under the 3-click setting. The consistent improvements demonstrate the effectiveness of SAM2S for robust long-term surgical tracking. Qualitative Analysis. Figs. 3 and 4 compare SAM2S with vanilla SAM2, fine-tuned SAM2, and fine-tuned SAM2Long across challenging surgical scenarios from RARP50 and EndoVis18 under 3-click initialization. Vanilla SAM2 exhibits frequent false positives (FP) and false negatives (FN) in challenging scenarios with lighting Figure 4. Qualitative comparison on EndoVis18 (140s duration). significant gains across all methods, highlighting the importance of surgical-specific training data. Notably, SAM2S consistently outperforms all other methods, achieving 80.42 and 78.07 in average &F under 3-click and 1-click settings at 68 FPS. This represents improvements of 24.31 and 17.10 points over vanilla SAM2, and 5.38 and 4.11 points over fine-tuned SAM2. Moreover, SAM2S maintains superior performance even on the unseen nephrectomy procedure type (EV17 and EV18), further validating its crossprocedure generalization capability. Long-term Tracking Analysis. In both vanilla and finetuning settings, training-free long-term memory methods Table 4. Performance comparison across various prompts. Table 6. Comparison between different resolutions. Model 1-click 3-click 5-click BBox GT mask Model Resolution EV17 EV18-I RARP EV18-T Avg FPS SAM2 (vanilla) 53.76 63.32 66.21 65.46 66.99 SAM2 SAM2Long SurgicalSAM2 SAM2S (ours) 72.69 66.67 72.33 78. 76.31 70.54 76.07 80.42 76.75 76.06 71.94 71.73 76.79 75.86 80.94 81.13 77.07 72.76 77.31 81.52 Table 5. Comparison between training on the in-domain dataset and the proposed SA-SV. Model Resolution EV17 EV18-I RARP SAM2 (vanilla) SAM2 (specialist) SAM2 (SA-SV) SAM2S (SA-SV) 512 512 512 512 63.11 71.66 81.96 86.72 72.17 77.23 79.89 82.37 52.73 81.23 76.51 79.47 SAM2 (vanilla) SAM2Long (vanilla) 1024 512 1024 512 75.37 82.36 48.46 59.02 66.30 26 63.11 72.17 52.73 46.44 58.61 69 72.31 74.42 42.66 55.66 61.26 11 65.07 74.18 40.46 52.26 58.09 22 Table 7. Ablation study under 3-click setting. Avg denotes the mean performance across all subsets in this table. SA-SV DiveMem TSL ARL EV17 EV18-I CIS-Test EV18-T Avg 63.11 72.17 81.96 79.89 85.07 81.54 83.51 79.81 86.17 81.42 86.72 82.37 56.03 80.09 85.07 88.41 88.04 89. 46.44 59.44 66.71 77.16 68.73 80.10 69.46 80.30 70.44 81.52 72.29 82.76 variations and dynamic scene changes. Fine-tuned SAM2 shows improved robustness over vanilla SAM2, yet still struggles with consistent long-term tracking, particularly during prolonged instrument occlusions. Notably, as shown in Figs. 3(d,f) and 4(e), fine-tuned SAM2Long exhibits higher frequency of false positives compared to fine-tuned SAM2, as discarding target-absent frames loses temporal context essential for distinguishing visually similar objects in surgical scenes. In contrast, SAM2S maintains accurate segmentation and target identity throughout extended videos, validating that SA-SV and the proposed modules are essential for robust long-term surgical tracking. More quality comparisons are provided in the appendix. Multi-prompt Analysis. Table 4 compares the performance across various prompt types, where the values represent the average &F across all subsets in the SA-SV test set. The results demonstrate SAM2Ss consistent superiority across all prompt types, from minimal 1-click to detailed mask initialization. This consistent improvement over both vanilla SAM2 and fine-tuned SAM2-based methods demonstrates that SAM2S comprehensively outperforms existing approaches in surgical environments because of our SA-SV and proposed modules. Comparison with In-domain Fine-tuning. Table 5 compares vanilla SAM2, specialist SAM2 (fine-tuned and evaluated on each in-domain dataset), and both SAM2 and SAM2S fine-tuned on SA-SV. SAM2 fine-tuned on SASV matches the performance of in-domain specialist models, indicating that SA-SV effectively provides comprehensive coverage across surgical scenarios. SAM2S further surpasses fine-tuned SAM2 and achieves superior performance to most specialist models, with notable improvements of 15.06 and 5.14 &F on EV17 and EV18-I. These results demonstrate that single SAM2S model trained on SA-SV generalizes well across diverse surgical scenarios without requiring dataset-specific fine-tuning. 5.3. Detail Analysis and Ablation Study Resolution Analysis. Table 6 presents performance comparison of SAM2 and SAM2Long at different resolutions using vanilla pretrained weights. Both methods demonstrate stronger zero-shot capability at 1024 resolution compared to 512, explaining why the vanilla SAM2-based methods use higher resolution. However, SAM2 runs at only 26 FPS at 1024 resolution, limiting real-time applicability. In contrast, our SAM2S at 512 resolution achieves 68 FPS while maintaining superior performance  (Table 2)  , making it more suitable for real-time surgical scenarios. Ablation Study. We performed ablation studies to evaluate each component on representative test subsets, as shown in Table 7. Training on our refined SA-SV provides 17.72 average &F improvement over vanilla SAM2, demonstrating the importance of domain-specific training data. The DiveMem contributes significantly with 2.94 average &F improvement, showing the importance of trainable long-term modeling and diverse temporal information during inference. Without DiveMem, the TSL improves instrument tracking, especially 8.32 &F in CIS-Test, confirming that semantic understanding enhances stability. The ARL provides consistent gains across subsets, confirming robustness to local annotation noise and partially mitigating cross-dataset inconsistencies. Together, our innovations collectively address core challenges in surgical iVOS. 6. Conclusion We address surgical iVOS challenges through SA-SV benchmark and SAM2S model. SA-SV provides the first large-scale surgical iVOS benchmark with masklet annotations across multiple procedures. SAM2S introduces three key innovations targeting surgical-specific limitations: DiveMem for long-term tracking, TSL for semantic understanding, and ARL for multi-source training robustness. Extensive experiments demonstrate its superior performance across diverse surgical scenarios, establishing robust foundation for surgical video segmentation applications."
        },
        {
            "title": "References",
            "content": "[1] Fatimaelzahraa Ali Ahmed, Mahmoud Yousef, Mariam Ali Ahmed, Hasan Omar Ali, Anns Mahboob, Hazrat Ali, Zubair Shah, Omar Aboumarzouk, Abdulla Al Ansari, and Shidin Balakrishnan. Deep learning for surgical instrument recognition and segmentation in robotic-assisted surgeries: systematic review. Artificial Intelligence Review, 58(1):1, 2024. 2 [2] Oluwatosin Alabi, Ko Ko Zayar Toe, Zijian Zhou, Charlie Budd, Nicholas Raison, Miaojing Shi, and Tom Vercauteren. Cholecinstanceseg: tool instance segmentation dataset for laparoscopic surgery. Scientific Data, 12(1):825, 2025. 2, 3 [3] Deepak Alapatt, Pietro Mascagni, Armine Vardazaryan, Alain Garcia, Nariaki Okamoto, Didier Mutter, Jacques Marescaux, Guido Costamagna, Bernard Dallemagne, and Nicolas Padoy. Temporally constrained neural networks (tcnn): framework for semi-supervised video semantic segmentation. arXiv preprint arXiv:2112.13815, 2021. 3 [4] Sharib Ali, Debesh Jha, Noha Ghatwary, Stefano Realdon, Renato Cannizzaro, Osama Salem, Dominique Lamarque, Christian Daul, Michael Riegler, Kim Anonsen, et al. multi-centre polyp detection and segmentation dataset for generalisability assessment. Scientific Data, 10(1):75, 2023. 3 [5] Max Allan, Alex Shvets, Thomas Kurmann, Zichen Zhang, Rahul Duggal, Yun-Hsuan Su, Nicola Rieke, Iro Laina, Niveditha Kalavakonda, Sebastian Bodenstedt, et al. 2017 robotic instrument segmentation challenge. arXiv preprint arXiv:1902.06426, 2019. 2, 3 [6] Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamohammadi, Imanol Luengo, Felix Fuentes, Evangello Flouty, Ahmed Mohammed, Marius Pedersen, et al. 2018 robotic scene segmentation challenge. arXiv preprint arXiv:2001.11190, 2020. 3 [7] Nicolas Ayobi, Alejandra Perez-Rondon, Santiago Rodrıguez, and Pablo Arbelaes. Matis: Masked-attention transformers for surgical instrument segmentation. In 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI), pages 15, 2023. 3 [8] Nicolas Ayobi, Santiago Rodrıguez, Alejandra Perez, Isabela Hernandez, Nicolas Aparicio, Eugenie Dessevres, Sebastian Pena, Jessica Santander, Juan Ignacio Caicedo, Nicolas Fernandez, and Pablo Arbelaez. Pixel-wise recognition for holistic surgical scene understanding. arXiv, 2024. 3 [9] Jorge Bernal, Javier Sanchez, Gloria Fernandez-Esparrach, Debora Gil, Cristina Rodrıguez, and Fernando Vilarino. accurate polyp highlighting in Wm-dova maps for colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics, 43: 99111, 2015. 3 [10] Matthias Carstens, Franziska Rinner, Sebastian Bodenstedt, Alexander Jenke, Jurgen Weitz, Marius Distler, Stefanie Speidel, and Fiona Kolbinger. The dresden surgical anatomy dataset for abdominal organ segmentation in surgical data science. Scientific Data, 10(1):18, 2023. [11] Ho Kei Cheng and Alexander Schwing. Xmem: Longterm video object segmentation with an atkinson-shiffrin memory model. In European conference on computer vision, pages 640658. Springer, 2022. 2 [12] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, and Alexander Schwing. Putting the object back into video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31513161, 2024. 2, 5 [13] Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip HS Torr, and Song Bai. Mosev2: more challenging dataset for video arXiv preprint object segmentation in complex scenes. arXiv:2508.05630, 2025. 2, 3 [14] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi Wang. Sam2long: Enhancing sam 2 for long video segmentation In Proceedings of the with training-free memory tree. IEEE/CVF international conference on computer vision, 2025. 2, 4, 5 [15] Xiaofei Du, Maximilian Allan, Sebastian Bodenstedt, Lena Maier-Hein, Stefanie Speidel, Alessio Dore, and Danail Stoyanov. Patch-based adaptive weighting with segmentation and scale (pawss) for visual tracking in surgical video. Medical image analysis, 57:120135, 2019. 1 [16] Zheng Fang, Xiaoming Qi, Chun-Mei Feng, Jialun Pei, Weixin Si, and Yueming Jin. Spatio-temporal representation decoupling and enhancement for federated instruarXiv preprint ment segmentation in surgical videos. arXiv:2506.23759, 2025. [17] Mingqi Gao, Feng Zheng, James JQ Yu, Caifeng Shan, Guiguang Ding, and Jungong Han. Deep learning for video object segmentation: review. Artificial Intelligence Review, 56(1):457531, 2023. 2 [18] Luis Garcia-Peraza-Herrera, Lucas Fidon, Claudia DEttorre, Danail Stoyanov, Tom Vercauteren, and Sebastien Image compositing for segmentation of surgical Ourselin. IEEE transactions on tools without manual annotations. medical imaging, 40(5):14501460, 2021. 3 [19] Charley Gros, Andreanne Lemay, and Julien Cohen-Adad. Softseg: Advantages of soft versus binary training for image segmentation. Medical image analysis, 71:102038, 2021. 5 [20] Md Kamrul Hasan, Lilian Calvet, Navid Rabbani, and Adrien Bartoli. Detection, segmentation, and 3d pose estimation of surgical tools using convolutional neural networks and algebraic geometry. Medical Image Analysis, 70: 101994, 2021. 3 [21] Yuk Heo, Yeong Jun Koh, and Chang-Su Kim. Interactive video object segmentation using global and local transfer modules. In European Conference on Computer Vision, pages 297313. Springer, 2020. [22] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1348013492, 2023. 2, 3 [23] Lingyi Hong, Zhongying Liu, Wenchao Chen, Chenzhi Tan, Yuang Feng, Xinyu Zhou, Pinxue Guo, Jinglun Li, Zhaoyu Chen, Shuyong Gao, et al. Lvos: benchmark for largescale long-term video object segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 3 [24] W-Y Hong, C-L Kao, Y-H Kuo, J-R Wang, W-L Chang, and C-S Shih. Cholecseg8k: semantic segmentation dataset for laparoscopic cholecystectomy based on cholec80. arXiv preprint arXiv:2012.12453, 2020. 2, 3 [25] Shishuai Hu, Zehui Liao, Jianpeng Zhang, and Yong Xia. Domain and content adaptive convolution based multisource domain generalization for medical image segmentaIEEE Transactions on Medical Imaging, 42(1):233 tion. 244, 2022. 2 [26] Debesh Jha, Pia Smedsrud, Michael Riegler, Pal Halvorsen, Thomas De Lange, Dag Johansen, and Havard In InJohansen. Kvasir-seg: segmented polyp dataset. ternational conference on multimedia modeling, pages 451 462. Springer, 2019. 3 [27] Yueming Jin, Yang Yu, Cheng Chen, Zixu Zhao, Pheng-Ann Heng, and Danail Stoyanov. Exploring intra-and inter-video IEEE relation for surgical semantic scene segmentation. Transactions on Medical Imaging, 41(11):29913002, 2022. 1 [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anyIn International Conference on Computer Vision, thing. pages 40154026, 2023. [29] Ngoc Lan. P. et al. neounet: Towards accurate colon polyp segmentation and neoplasm detection, 1528, 2021. 3 [30] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with In Proceedings of the IEEE/CVF conmask-adapted clip. ference on computer vision and pattern recognition, pages 70617070, 2023. 5 [31] Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:34303441, 2020. 2 [32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988, 2017. 5 [33] Haofeng Liu, Erli Zhang, Junde Wu, Mingxuan Hong, and Yueming Jin. Surgical sam 2: Real-time segment anything in surgical video by efficient frame pruning. In Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond, 2024. 2, 6 [34] Haofeng Liu, Mingqi Gao, Xuxiao Luo, Ziyue Wang, Guanyi Qin, Junde Wu, and Yueming Jin. Resurgsam2: Referring segment anything in surgical video via credible long-term tracking. arXiv preprint arXiv:2505.08581, 2025. 3 [35] Yong Liu, Ran Yu, Fei Yin, Xinyuan Zhao, Wei Zhao, Weihao Xia, Jiahao Wang, Yitong Wang, Yansong Tang, and Yujiu Yang. Learning high-quality dynamic memory for video object segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [36] Aditya Murali, Deepak Alapatt, Pietro Mascagni, Armine Vardazaryan, Alain Garcia, Nariaki Okamoto, Guido Costamagna, Didier Mutter, Jacques Marescaux, Bernard Dallemagne, et al. The endoscapes dataset for surgical scene segmentation, object detection, and critical view of safety assessment: Official splits and benchmark. arXiv preprint arXiv:2312.12429, 2023. 3 [37] Zhen-Liang Ni, Gui-Bin Bian, Guan-An Wang, Xiao-Hu Zhou, Zeng-Guang Hou, Hua-Bin Chen, and Xiao-Liang Xie. Pyramid attention aggregation network for semantic segmentation of surgical instruments. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11782 11790, 2020. 1 [38] Zhen-Liang Ni, Xiao-Hu Zhou, Guan-An Wang, WenQian Yue, Zhen Li, Gui-Bin Bian, and Zeng-Guang Hou. Surginet: Pyramid attention aggregation and class-wise selfdistillation for surgical instrument segmentation. Medical Image Analysis, 76:102310, 2022. 2 [39] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory In Proceedings of the IEEE/CVF international networks. conference on computer vision, pages 92269235, 2019. 2 [40] Mingyang Ou, Heng Li, Haofeng Liu, Xiaoxuan Wang, Chenlang Yi, Luoying Hao, Yan Hu, and Jiang Liu. Mvd-net: Semantic segmentation of cataract surgery using multi-view learning. In IEEE EMBC, pages 50355038, 2022. 1 [41] Dimitrios Psychogyios, Emanuele Colleoni, Beatrice Van Amsterdam, Chih-Yang Li, Shu-Yu Huang, Yuchong Li, Fucang Jia, Baosheng Zou, Guotai Wang, Yang Liu, et al. Sar-rarp50: Segmentation of surgical instrumentation and action recognition on robot-assisted radical prostatectomy challenge. arXiv preprint arXiv:2401.00496, 2023. 3 [42] Guanyi Qin, Ziyue Wang, Daiyun Shen, Haofeng Liu, Hantao Zhou, Junde Wu, Runze Hu, and Yueming Jin. Structure matters: Revisiting boundary refinement in video object segIn Proceedings of the IEEE/CVF International mentation. Conference on Computer Vision, 2025. 2 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763, 2021. [44] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 1, 2, 3, 5 [45] Maria Robu, Abdolrahim Kadkhodamohammadi, Imanol Luengo, and Danail Stoyanov. Towards real-time multiple surgical tool tracking. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 9(3): 279285, 2021. 1 [46] Colleen Rutherford. Differentiating surgical instruments. FA Davis, 2011. 3, 5 [47] Natalia Valderrama, Paola Ruiz, Isabela Hernandez, Nicolas Ayobi, Mathilde Verlyck, Jessica Santander, Juan Caicedo, Nicolas Fernandez, and Pablo Arbelaez. Towards holistic surgical scene understanding. In Medical Image Computing and Computer Assisted Intervention MICCAI 2022, pages 442452, Cham, 2022. Springer Nature Switzerland. 3 [48] Jovana Videnovic, Alan Lukezic, and Matej Kristan. distractor-aware memory for visual object tracking with sam2. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2425524264, 2025. 2, 4, 5 [49] Ziyi Wang, Bo Lu, Yonghao Long, Fangxun Zhong, TakHong Cheung, Qi Dou, and Yunhui Liu. Autolaparo: new dataset of integrated multi-tasks for image-guided surgical In International automation in laparoscopic hysterectomy. Conference on Medical Image Computing and ComputerAssisted Intervention, pages 486496. Springer, 2022. 2, 3 [50] Hallee Wong, Marianne Rakic, John Guttag, and Adrian Dalca. Scribbleprompt: fast and flexible interactive segmentation for any biomedical image. In European Conference on Computer Vision, pages 207229. Springer, 2024. [51] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. 2, 5 [52] Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. Advances in Neural Information Processing Systems, 35:3632436336, 2022. 2 [53] Wenxi Yue, Jing Zhang, Kun Hu, Yong Xia, Jiebo Luo, and Zhiyong Wang. Surgicalsam: Efficient class promptIn Proceedings of able surgical instrument segmentation. the AAAI Conference on Artificial Intelligence, pages 6890 6898, 2024. 1 [54] Sabrina Madad Zadeh, Tom Francois, Aurelie Comptour, Michel Canis, Nicolas Bourdel, and Adrien Bartoli. Surgai3. 8k: labeled dataset of gynecologic organs in laparoscopy with application to automatic augmented reality surgical guidance. Journal of Minimally Invasive Gynecology, 30(5):397405, 2023. 3 [55] Jiayuan Zhu, Yunli Qi, and Junde Wu. Medical sam 2: Segment medical images as video via segment anything model 2. arXiv preprint arXiv:2408.00874, 2024."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Sheffield"
    ]
}