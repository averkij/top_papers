{
    "paper_title": "Optimizing Few-Step Generation with Adaptive Matching Distillation",
    "authors": [
        "Lichen Bai",
        "Zikai Zhou",
        "Shitong Shao",
        "Wenliang Zhong",
        "Shuo Yang",
        "Shuo Chen",
        "Bojun Chen",
        "Zeke Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models."
        },
        {
            "title": "Start",
            "content": "Optimizing Few-Step Generation with Adaptive Matching Distillation Lichen Bai * 1 Zikai Zhou * 1 Shitong Shao 1 Wenliang Zhong 1 Shuo Yang 2 Shuo Chen 3 Bojun Chen 1 Zeke Xie 1 6 2 0 2 7 ] . [ 1 5 4 3 7 0 . 2 0 6 2 : r Abstract Distribution Matching Distillation (DMD) is powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zonesregions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), selfcorrecting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models. 1. Introduction Diffusion models (Song et al., 2020; Karras et al., 2022) have demonstrated remarkable success in visual generation, exhibiting remarkable capabilities in generating diverse, high-resolution images (Dhariwal & Nichol, 2021; Podell et al., 2023) and videos (Wan et al., 2025; Wu et al., 2025a). Despite these advancements, their utility is often hindered 1xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou) 2Harbin Institute of Technology, Shenzhen 3School of Intelligence Science and Technology, Nanjing Univer sity, China.. Correspondence to: Zeke Xie <zekexie@hkustgz.edu.cn>. Preprint. February 10, 2026. 1 Figure 1. Visualization of the Optimization Landscape in Forbidden Zones. (a) The Real Teachers energy potential becomes undefined or poorly calibrated far from the data manifold, exerting misleading attractive forces. (b) The Fake Teachers landscape exhibits shallow slope, resulting in weak repulsive force that is insufficient to propel the student out of the Forbidden Zone. by the substantial latency of the iterative denoising process, which demands dozens or even hundreds of Number of Function Evaluations (NFEs) during inference. To alleviate this latency, range of distillation methods (Yin et al., 2024a; Salimans & Ho, 2022; Sauer et al., 2024; Liu et al., 2023b) have been proposed to compress the multi-step denoising trajectory into minimal number of generation steps. Among these, Distribution Matching Distillation (DMD) (Yin et al., 2024a) has emerged as prominent approach. DMD leverages dual-teacher framework: real teacher (typically pre-trained diffusion model) provides gradients to guide the student generated samples toward the target data distribution, while fake teacher (typically diffusion model trained alongside the student) approximates the student models current distribution. Crucially, the fake teacher supplies repulsive signal to regularize training, encouraging sample diversity and mitigating mode collapse. Despite its conceptual appeal, the stability of DMD implicitly hinges on fragile assumption that the real teacher offers reliable guidance, while the fake teacher provides adequate repulsive signals to guide or push the student samples toward the target distribution (Shao et al., 2025; Jiang et al., 2025). However, this assumption is frequently compromised in practice. During distillation process, the student model inevitably generates severely distorted or lowquality samples that deviate substantially from the real data manifold. For these poor samples, the real teachers score estimates are often poorly calibrated due to the distribuOptimizing Few-Step Generation with Adaptive Matching Distillation tion shift, and the repulsive guidance from the fake teacher proves insufficient to extricate the student from these lowquality regions, which we identify as Forbidden Zones. In these zones, as shown in Figure 1, training dynamics may enter self-reinforcing degradation regime, where the student repeatedly revisits low-quality regions and struggles to recover. Consequently, the lack of effective correction impedes the whole optimization process, hindering convergence and perpetuating generative distortions. Armed with our definition of Forbidden Zones, we offer novel perspective that unifies various existing DMD methods. We re-interpret these prior works (Yin et al., 2024b;a; Jiang et al., 2025; Shao et al., 2025; Liu et al., 2025a) as methods that implicitly constrain the student to avoid such corrupted regions. Crucially, however, these methods do not explicitly detect the occurrence of Forbidden Zone, nor do they provide direct mechanism to adapt the distillation dynamics once the student inevitably enters one. To resolve this, we contend that the most effective strategy is to pinpoint these Forbidden Zone and execute swift leapfrog back to the valid data manifold. Our framework is founded on three key observations: (1) Detection via Low Rewards: Severely degraded samples consistently yield low reward scores. Since these samples lie outside the support of the real teacher, rendering its score estimates unreliable, the reward model serves as practical proxy for identifying Forbidden Zones; (2) Asymmetric Propulsion: To facilitate an escape, the fake teacher should not learn the student distribution uniformly; instead, it must specialize in modeling these corrupted regions to provide targeted and potent repulsive forces; and (3) Signal Prioritization: Within the Forbidden Zone, the repulsive force must take precedence over the potentially misleading guidance from the real teacher to ensure effective correction. Building on these insights, we propose AMD (Adaptive Matching Distillation). Unlike previous static distillation approaches, AMD acts as self-correcting system: it utilizes reward-guided diagnostics to identify distorted samples and adaptively modulates the distillation dynamics, prioritizing specialized signals to actively push the student out of the Forbidden Zone. The dynamic intervention effectively mitigates the risk of training collapse and significantly promotes higher generation fidelity. The contributions of this work are threefold: First, we analyze the Forbidden Zone in DMD and reinterpret recent advancements as implicit mechanisms that avoid these corrupted regions. This perspective highlights the fundamental needs to explicitly address Forbidden Zones for stable and effective distillation. Second, we propose AMD, reward-aware distillation framework that transforms DMD into self-correcting system. By leveraging reward models as diagnostic sensors, it adaptively adapts the distillation dynamics: the fake teacher provides targeted repulsive guidance, and real and fake signals are dynamically prioritized to enable rapid recovery from Forbidden Zones. Third, we demonstrate the robust effectiveness of AMD across large-scale image and video generation tasks (e.g., SDXL, Wan2.1). AMD significantly mitigates training collapse while achieving superior sample fidelity. By leveraging reward-aware guidance, AMD enables the student to transcend the performance ceiling of the original teacher, delivering state-of-the-art results on diverse human-preference benchmarks (e.g., VideoGen-Eval, HPSv2). 2. Preliminaries In this section, we present preliminaries about Distribution Matching Distillation in diffusion models. Due to page limitations, comprehensive discussion of related work is provided in Section B. Distribution Matching Distillation Distribution Matching Distillation (DMD) (Yin et al., 2024b) compresses pre-trained multi-step diffusion model (the real teacher) into few-step generator (the student) Gθ by minimizing the KL divergence between the target distribution preal and the student-induced distribution pfake. Let = Gθ(z) with (0, I), and let U[0, 1] denote randomly sampled diffusion time. The DMD objective is defined as LDMD = Ez,t [log preal(Ft) log pfake(Ft)] , (1) where Ft(x) denotes the forward diffusion operator that injects noise at time t, defined as Ft(x) = + (1 t) ϵ with ϵ (0, I). Specifically, taking the gradient of Equation (1) with respect to the student parameters θ yields compact score-matching form as θLDMD = Ez,t (cid:20) (sreal(Ft) sfake(Ft)) Gθ(z) θ (cid:21) . (2) This formulation manifests pull-push dynamic centered on the student-generated sample = Gθ(z): the real teacher exerts an attractive score that guides the student toward the target distribution, whereas the fake teacher applies repulsive score that drives the student away from its own current distribution. This interaction is formally analyzed through an optimization lens in Section 3.1. Reward Model We assume access to pre-trained reward (or preference) model : R, which assigns scalar score R(x) to generated sample . The reward 2 Optimizing Few-Step Generation with Adaptive Matching Distillation model is fixed throughout training and treated as blackbox scoring function. 3. Method In this section, we first re-examine DMD through the lens of optimization, establishing unified framework to analyze its training dynamics. Based on this formulation, we formally define Forbidden Zones and demonstrate how existing methods can be categorized within our framework as implicit mitigation strategies. Finally, we introduce our proposed AMD, detailing the theoretical mechanism by which it facilitates swift return to the valid data manifold. 3.1. An Optimization Perspective on DMD While DMD is conventionally formulated as distribution matching problem via score estimation, which often obscure the mechanistic causes of training failure. To gain more actionable understanding, we reformulate the students parameter-space update as an effective optimization step performed directly on the generated samples. This shift is essential for two reasons: it provides clear pushpull physical intuition of the distillation dynamics, and mathematical tool for diagnosing training collapse. Proposition 3.1 (Sample-space Gradient Descent Refor- (See Appendix C) Let = Gθ(z) be latent mulation). sample generated by the student. Under the first-order approximation, the parameter update θ θ ηθLDMD induces an effective gradient descent step on the sample in the latent space: xnew = ηeff xVDMD(x), (3) where VDMD(x) is the distillation potential defined as the contrastive energy between the teacher-guided targets as: VDMD(x) = = 1 2 1 2 ˆx0,real2 ˆx0,fake2 1 2 dfake2 (cid:124) (cid:123)(cid:122) (cid:125) Repulsive Term dreal2 (cid:124) (cid:123)(cid:122) (cid:125) Attractive Term 1 2 (4) . Taking the gradient of VDMD(x) with respect to yield xVDMD(x) = dreal dfake. Here ˆx0,real and ˆx0,fake denote the denoised latent estimates from the real and the fake teacher, typically computed via Tweedies formula from the diffused state Ft(x). From the optimization perspective in Proposition 3.1, the parameter-space update of DMD is equivalent to push-pull navigation task in the latent space. Here, the student Gθ reconciles its current position by moving toward the real teachers target (attracted by dreal) while moving away from the region estimated by the fake teacher (repelled by dfake). 3 Crucially, the reliability of the attractive force dreal depends on the real teachers ability to accurately predict clean targets. We define the region where this competence fails as the Forbidden Zone: Definition 3.2 (Forbidden Zone Zf ). Let Ereal(x) = log preal(x) denote the energy potential of the target distribution. The Forbidden Zone Zf is defined as the highenergy regime beyond the teachers empirical support: Zf = {x Ereal(x) > γ} (5) where γ is threshold of teacher competence. Inside Zf , the student produces severely distorted samples that lie outside the real teachers training manifold. By viewing DMD as navigator of latent space, we precisely diagnose the systemic optimization failures in Zf through the energy landscapes shown in Fig. 1. Specifically, within Zf , the real teachers energy surface Ereal becomes fractured and ill-posed due to lack of empirical support  (Fig. 1)  , causing the attractive force dreal to yield hallucinated and incoherent gradients. Simultaneously, distorted samples reside at the extreme tails of the students distribution where the fake teachers energy Efake is identically flat (Fig. 1b). This leads to vanishing repulsive force, depriving the student of the propulsion required to escape the corrupted region. Such dual collapse results in an optimization stalemate: the noisy pull and negligible push fail to produce effective force xVDMD, therefore hindering the effectiveness of distillation. Re-interpreting Prior Art. This optimization perspective provides unified lens for re-interpreting recent advances in the DMD family. Rather than viewing these methods as ad hoc modifications, we express them as specific instantiations of generalized DMD gradient operator: = H(cid:0)dreal(x, Ft, ϕ), dfake(x, Ft, ψ)(cid:1) + λ Fext, (6) where = ˆx0 denotes the latent displacement, and H(, ) is adaptive operator that composes the attractive and repulsive signals induced by the real and fake teachers. Here, Ft denotes the re-noising scale, and Fext represents auxiliary forces beyond the vanilla DMD objective. Within this framework, substituting Equations (3) and (4) reveals that standard DMD recovers linear case of H: Hstd(dreal, dfake) = dreal dfake, (7) which induces fixed contrastive gradient field and serves as the baseline optimization dynamics. We then demonstrate in Table 1 how Equation (6) unifies various existing methods, with Figure 2 providing visual intuition for this framework. Several methods introduce an auxiliary force Fext to provide non-zero recovery gradient when the primary displacement Optimizing Few-Step Generation with Adaptive Matching Distillation Figure 2. Visualizing the Taxonomy of Forbidden Zone Mitigation Strategies. (a) Standard Failure: Useful gradients exist only near modes, leaving gradient vacuum in Zf . (b) External Force: An auxiliary force Fext (green) provides global steering to bridge the gap. (c) Noise Reset: High noise induces distribution overlap, restoring gradient coverage. (d) Real Teacher Adaptation: The teacher manifold actively shifts towards the student to eliminate Zf . Table 1. Taxonomy of DMD-style methods from an optimization perspective. We decompose each method into configuration of the generalized displacement gradient operator. denotes the reward-aware advantage, ϕ is the real teacher state, and ψ represents the fake teacher optimization objective, where Ldiff := ϵ ϵψ(xt, t)2. Method DMD DMD2 D-DMD Magic Dist. DMDR AMD (Ours) H(dreal, dfake) Ext. Force (Fext) Noise SNR (Ft) Real Teacher (ϕ) Fake Teacher (ψ) dreal dfake xgt U[0, 1] ϕfix Ldiff dreal dfake log D(x) U[0, 1] ϕfix Ldiff dca + ddm 0 tϕ > tG ϕfix Ldiff dreal dfake 0 U[0, 1] ϕθ ϕfix Ldiff dreal dfake αdca + βddm θE[R(x)] U[0, 1] ϕθ ϕfix Ldiff 0 [0, 1] ϕfix W(a) Ldiff Zf Strategy Static Tethering Adv. Anchoring SNR Reset Manifold Shrink RL Steering RL Steering fields dreal and dfake enter deadlock. Standard DMD (Yin et al., 2024b) utilizes an L2 regression loss, acting as static tethering force Fext = xgt that anchors the student to ground-truth data. DMD2 (Yin et al., 2024a) replaces this with an adversarial force Fext = log D(x), creating an adversarial fence to deflect samples away from Zf . Similarly, DMDR (Jiang et al., 2025) alternates between DMD updates and RL steps, where the latter introduces an asynchronous steering force driven by reward gradients to ensure the trajectory remains within high-fidelity regions. Other methods try to circumvent Zf by manipulating either the noise scale Ft or the teacher state ϕ. D-DMD (Liu et al., 2025a) designs novel re-noising schedule, its essence lies in lowering the risk of the real teacher estimating ill-posed scores within Zf by shifting samples toward more reliable noise regimes. In contrast, MagicDistillation (Shao et al., 2025) temporarily pulls the teachers empirical support toward the students current distribution (ϕθ ϕf ix) via LoRA. This induces manifold shrinking effect, reducing the measure of Zf during early training stages. We provide the corresponding analytical understanding in Section C.2. In summary, to the best of our knowledge, this work is the first to establish unified optimization framework that theoretically categorizes diverse DMD paradigms as specific strategies for mitigating Forbidden Zones. 3.2. Dynamic Score Adaptation via AMD To address Zf pathologies, straightforward intuition is to rebalance the contribution of the real and fake teachers based on the samples current state. Specifically, one might aim to let dfake dominate within Zf to provide the necessary repulsive push for escape, while ensuring dreal governs high-fidelity regions to refine the distillation. This leads to naive adaptive strategy: Hnaive(dreal, dfake) = α(a) dreal β(a) dfake, (8) where α and β are scalar coefficients controlled by the reward-aware advantage a. However, we contend that such coarse-grained rescaling is fundamentally insufficient. As analyzed in Section 3.1, the optimization failure within Zf stems not merely from insufficient gradient magnitude, but from structural collapse of the gradient field where teacher guidance becomes ill-posed. As empirically demonstrated in our 2D analysis (Fig. 8 of Section F.1), this can lead to destructive interference between attractive and repulsive forces, magnifying conflicting signals that destabilize training trajectories and eventually trigger total distillation collapse. This observation suggests that more fine-grained, component-wise intervention is required to decouple beneficial guidance from detrimental noise. 4 Optimizing Few-Step Generation with Adaptive Matching Distillation Figure 3. Overview of AMD. The framework operates in three stages: (Left) Group Generation & Re-noising: For each prompt, the student Gθ produces group of samples {xi}K i=1, which are subsequently perturbed by the forward operator Ft. (Middle) Reward-aware Diagnosis: reward model R() serves as proxy to pinpoint samples (e.g., xK ) trapped in the Forbidden Zone (Zf ), where the real teachers score sreal becomes ill-posed. (Right) Dynamic Score Adaption: Through the adaptive operator HAM D, AMD rectifies the combination of dreal and df ake to derive an optimized gradient direction, thereby facilitating rapid escape from the Forbidden Zone and enabling the model to surpass the teacher under reward guidance. (9) where γ < γ and δ 1. . Decomposing the Gradient Field. To resolve this dilemma, we draw inspiration from the analytical decomposition proposed in D-DMD (Liu et al., 2025a). Let the real real duncond teachers guidance be dreal = duncond ), where ω is the CFG scale. The standard DMD gradient operator (Eq. 7) can be expanded and rearranged as follows: real + ω(dcond real Hstd = dreal dfake = (cid:2)duncond = (dcond (cid:124) real + ω(dcond real dfake) (cid:123)(cid:122) (cid:125) DM Term(ddm) )(cid:3) dfake real duncond real real duncond + (ω 1)(dcond ) (cid:125) (cid:123)(cid:122) CA Term(dca) real (cid:124) This decomposition reveals the structural asymmetry in the gradient: the DM Term directly anchors the student to the valid data manifold, whereas the CA Term enforces semantic conditions. Consequently, to robustly escape Zf , conservative update that prioritizes the DM Term is necessary; once the student resides in safer regions, the CA Term can be amplified to accelerate distillation. Building on this insight, we define the Dynamic Score Adaptation strategy via HAMD = β (dcond real dfake) + α (ω 1)(dcond ), (10) where α and β are dynamic coefficients adaptively modulated according to sample reliability. real duncond real proxy to identify samples residing in the Forbidden Zone Zf , which in turn informs the adaptive coefficients α and β. Assumption 3.3 (PreferenceCompetence Alignment). Let Zpref(τ ) = {x R(x) > τ } denote the high-reward region. We assume that samples in Zpref(τ ) are statistically concentrated in the low-energy regime of the real teacher as P(cid:0)Ereal(x) γ Zpref(τ )(cid:1) 1 δ, (11) Inspired by Matsutani et al. (2025), which shows that reward model fundamentally contracts the search space by concentrating probability mass onto narrow, high-reward subset of the output distribution, we posit Assumption 3.3, which implies that the reward model implicitly induces reliability ordering over the sample space. As result, regions of low reward naturally serve as practical proxy for Zf , where the real teachers score estimates become unreliable and potentially misleading. To eliminate prompt-dependent scale variance, we adopt group-relative sensing strategy inspired by GRPO (Shao et al., 2024). For group {x0, . . . , xK} generated from the same prompt by student, we compute the normalized relative advantage ai as ai = clip (cid:18) R(xi) µg σg + ϵ (cid:19) , 1, 1 , (12) Reward as Diagnostic Proxy Since the energy potential Ereal() is analytically intractable in high-dimensional space, we employ pre-trained reward model R() as practical where µg and σg are the mean and standard deviation of rewards within the group. Equipped with ai as diagnostic proxy for the Forbidden Zone Zf , we dynamically modulate 5 Optimizing Few-Step Generation with Adaptive Matching Distillation the distillation weights α and β in Equation (10) through linear adaptive rule: α(ai) = 1 + ai, β(ai) = 1 ai, (13) where R+ is sensitivity hyperparameter that governs the intensity of the reactive modulation. This formulation effectively re-scales the potential field VDMD on per-sample basis: for low-advantage samples (ai < 0), the repulsive force is amplified to facilitate an active escape from Zf ; for high-advantage samples (ai > 0), the attractive force is prioritized to refine generation fidelity. The conceptual workflow of AMD is illustrated in Fig. 3, and the detailed algorithm is detailed in Algorithm 1. 3.3. Repulsive Landscape Sharpening While the score adaption in Section 3.2 determines how to combine gradients, it relies on fundamental assumption: that the fake teacher can provide meaningful repulsive signal df ake within the Forbidden Zone Zf . In standard DMD (Yin et al., 2024b), the fake teacher is primarily designed to promote diversity. By uniformly modeling the students distribution, it exerts dispersive force that prevents mode collapse. However, our analysis in Section 3.1 assigns critical new role to the fake teacher: correction. To effectively repel the student from the Forbidden Zone, the fake teacher must first learn to recognize it. To address this, we introduce Repulsive Landscape Sharpening, which reallocates the training focus of fake teacher ψ towards failure cases. We modify the distillation objective by incorporating an advantage-aware weight (ai) as Lψ = Ez,t,ϵ (cid:2)W (ai) ϵ ψ(xt, t)2(cid:3) , (14) where xt = Ft(Gθ(z)) and () imposes heavier penalty on low-advantage samples (e.g., (ai) = eai). This mechanism ensures that the fake teacher learns highly sensitive energy landscape specifically tailored to the students current weaknesses. Mathematically, this creates steep likelihood slope, maximizing the magnitude of the repulsive gradient dfake log pfake during the students update. Thus, the fake teacher transforms from passive density estimator into an active failure detector, providing the necessary kick to escape the Forbidden Zone. 4. Experiments In this section, we conduct extensive experiments to validate AMD. As natural corollary of the framework in Section 3.1, these experiments primarily serve to verify the rationality of our theoretical analysis. 4.1. Experiments Setting We evaluate AMD across diverse range of diffusion backbones, modalities, and architectural designs. For image generation, we conduct experiments on SiT, and SDXL (Podell et al., 2023); for video generation, we evaluate AMD on Wan2.1 (Wan et al., 2025) and LongLive (Yang et al., 2025a). To ensure comprehensive coverage, we report results on multiple benchmarks, including MS-COCO (Lin et al., 2015), ImageNet (Russakovsky et al., 2015), DrawBench (Saharia et al., 2022), HPD (Wu et al., 2023), GenEval (Ghosh et al., 2023), VBench (Huang et al., 2023), VBench++ (Huang et al., 2025b), VideoGen-Eval (Yang et al., 2025b) and TAHard (Liu et al., 2025c). Regarding the reward models, we utilize HPSv2 (Wu et al., 2023) for SDXL, DINOv2 (Caron et al., 2021) for DiT-based models (SiT) (Ma et al., 2024), and VideoAlign (Liu et al., 2025c) for video models. Detailed configurations are provided in Section A. Table 2. Comparison of different text-to-image models on GenEval (Ghosh et al., 2023) benchmark. (base model: SDXL) The best results in each category are highlighted in bold. Method #Params Resolution NFEs Overall Pretrained Models SDXL (Podell et al., 2023) FLUX.1-dev (Labs, 2024) 2.6B 12.0B 1024 1024 1024 1024 50 2 Pretrained Models SDXL-LCM (Luo et al., 2023) SDXL-Turbo (Podell et al., 2023) SDXL-Lightning (Lin et al., 2024) SDXL-DMD2 (Yin et al., 2024a) SDXL-DMDR (Jiang et al., 2025) SDXL-AMD (Ours) 2.6B 2.6B 2.6B 2.6B 2.6B 2.6B 1024 1024 512 512 1024 1024 1024 1024 1024 1024 1024 1024 4 4 4 4 4 4 0.55 0. 0.50 0.56 0.53 0.51 0.56 0.57 4.2. Main Results Image Generation We first conduct experiments on textto-image generation. We follow standard evaluation protocols and first assess performance on the 10k COCO2014val dataset. As presented in Tab. 3, our method significantly outperforms existing state-of-the-art acceleration techniques. Specifically, AMD achieves the highest ImageReward (88.37) and HPSv2 (31.25), surpassing strong competitors like PCM and D-DMD. To further verify the robustness of our method, we extend our evaluation to multiple popular benchmarks. As shown in Tab. 7, AMD consistently surpasses DMD2 across all stylistic categories (Anime, Photo, Concept-art, and Painting) on the HPDv2 dataset. Additionally, on the object-focused GenEval benchmark (Tab. 2), AMD secures the top rank among distilled models with an overall score of 0.57, exhibiting superior prompt adherence (please refer to Tab. 8 for detailed scores of each dimension). It is worth noting that since the official weights for DMDR and D-DMD are not publicly available, we reference the results directly from their 6 Optimizing Few-Step Generation with Adaptive Matching Distillation Table 3. Quantitative comparison of text-to-image task on 10k COCO2014-val prompts (Lin et al., 2015). (base model: SDXL). The results validate the effectiveness of our proposed AMD. Table 5. Quantitative comparison on the streaming video generation task. Base model: Wan-1.3B. The results on VBench show that AMD outperforms the baseline in terms of Total Score. Method ImageReward HPSv2 Model LCM (Luo et al., 2023) Turbo (Sauer et al., 2024) Lightning (Lin et al., 2024) Flash (Chadebec et al., 2025) PCM (Wang et al., 2024) DMD2 (Yin et al., 2024a) D-DMD (Liu et al., 2025a) AMD (Ours) 39.56 46.09 57.48 19.04 64.73 71.01 78.61 88.37 28.00 29.83 30.30 27.71 30.76 30.64 30.34 31.25 Table 4. Quantitative comparison on 50K-ImageNet (256 256). (base model: SiT-XL/2). Notably, DMDR exhibits reward hacking behavior, attaining high IS at the cost of poor FID. AMD, however, shows superior balance between image quality and diversity. Method DMD DMDR FID sFID IS 3.5573 9.6341 5.8499 7. 314.42 391.79 AMD (Ours) 3.4690 5.7464 316.02 respective papers for comparison. Furthermore, we provide human preference winning rate analysis in Fig. 9. The results indicate that AMD holds substantial lead over DMD2 on both DrawBench and HPDv2, confirming that our method generates images with superior quality. Due to the page limit, we provide additional results and visualization in Section F.2 and Section F.3. To further isolate the intrinsic effectiveness of our distillation mechanism, we conduct experiments on the class-toimage generation on ImageNet. Using SiT-XL/2 (Ma et al., 2024) as the backbone, we compare our proposed AMD against standard Standard DMD and DMDR (Jiang et al., 2025). The implementation details are provided in Section A.4. As shown in Tab. 4, AMD achieves superior FID (3.4690) and sFID (5.7464) to DMD, demonstrating its capacity to sculpt the generation manifold rather than merely matching the teacher. Unlike DMDR, which suffers from severe mode collapse, AMDs tempered refinement provides balanced optimization path that avoids reward hacking while steadily enhancing overall quality. Video Generation We extend AMD to streaming video generation task using Wan2.1-1.3B as the backbone, following the first stage of LongLive (Yang et al., 2025a) protocol. As shown in Tab. 5, AMD surpasses the LongLive baseline with Total Score of 82.21 vs. 81.42. Tab. 6 further reveals substantial gains on VBench, where AMD boosts Motion Quality by 67% (35.51 59.26) and the Total Score to LTX-Video (HaCohen et al., 2024) MAGI-1 (Teng et al., 2025) CausVid (Yin et al., 2025) LongLive (Yang et al., 2025a) AMD (Ours) Evaluation scores Total Quality Semantic 80.00 79.18 81.20 81.42 82.21 82.30 82.04 84.05 84.20 85. 70.79 67.74 69.80 70.38 69.91 Table 6. Quantitative comparison... Dataset VBench Model LongLive (Huang et al., 2023) AMD (Ours) VideoGen-Eval LongLive (Yang et al., 2025b) AMD (Ours) VQ 30.10 37.39 -24.53 -21.93 TA-Hard (Liu et al., 2025c) LongLive AMD (Ours) -0.9908 -0.1412 MQ TA Total 35.51 59.26 -13.07 17.87 13.19 28.74 107.98 100. 118.57 107.98 27.18 14.65 173.59 197.45 80.96 87.84 39.39 43.52 197.45. The qualitative comparisons are shown in Fig. 4. While Visual and Motion Quality improve significantly, the slight trade-off in Text Alignment (TA) is an anticipated outcome of the VideoAlign reward, which explicitly prioritizes dynamic fidelity and motion aesthetics over rigid semantic constraints. To further demonstrate the scalability of our approach, we provide additional results using the larger Wan2.1-14B backbone in Section F.2, confirming that AMDs effectiveness persists across model scales. Figure 4. Qualitative comparison on text-to-video generation. Compared to standard DMD (e.g., LongLive), AMD demonstrates superior visual fidelity and more coherent motion realism. Additional qualitative results are provided in Fig. 15. 4.3. Ablation Study Effectiveness of AMD Components. AMD achieves comprehensive gains through two synergistic mechanisms: (i) Dynamic Score Adaptation provides fine-grained modulation of DMD updates, adaptively refining both the magnitude and direction of the distillation gradients based on the sample state. (ii) Repulsive Landscape Sharpening heightens the systems perception of failure regions, sharpening repulsive forces to more effectively identify and rectify pathological areas. As shown in Figure 5, we observe that FID and IS improve alongside rising reward scores throughout training, further validating the robustness and effectiveness of our optimizaOptimizing Few-Step Generation with Adaptive Matching Distillation Table 7. Quantitative comparison of text-to-image task on HPDv2 (Wu et al., 2023) dataset. We compare AMD against DMD2 using lot of preference-based metrics. Base model: SDXL. Photo Concept-art Painting Average Anime Method PickScore HPSv2 ImageReward PickScore HPSv2 ImageReward PickScore HPSv2 ImageReward PickScore HPSv2 ImageReward PickScore HPSv2 DMD2 AMD (Ours) 22.85 22.91 33.16 33.10 120.74 116.20 22.41 22.96 30.31 30. 71.20 84.01 22.03 22.60 31.39 32.04 106.76 113.49 22.18 22.40 31.72 31. 117.67 108.37 22.36 22.72 31.64 31.97 ImageReward 104.09 105.52 Figure 5. Component-wise ablation study on 50K-ImageNet (256256). (base model: SiT-XL/2). We investigate the contribution of each component in AMD. Dynamic Adapt denotes the Dynamic Score Adaptation (Section 3.2), and Repulsive Sharpen denotes the Repulsive Landscape Sharpening (Section 3.3). tion objective. Training Dynamics and Stability. Here, we analyze the training dynamics of AMD. As shown in Fig. 6, we track both Inception Score (IS) and DINO-based reward during distillation on SiT-XL/2. Compared with the baseline, AMD demonstrates consistently faster improvement and more stable training trajectory across iterations. Notably, the increase in reward is well aligned with the improvement in IS, indicating that reward-aware dynamic score adaptation provides reliable diagnostic signals and effectively steers the student away from low-quality regions during training, rather than merely improving the final outcome. Figure 6. RewardQuality Co-evolution during Distillation. AMD exhibits synchronized increase in generation quality (IS) and reward scores, suggesting that reward-aware dynamic score adaptation effectively guides the student out of low-quality regions. Impact of Reward Model. To intuitively validate the reward models role as diagnostic proxy, we visualize training dynamics on 2D multi-modal dataset in Fig. 7. We maintain fixed Real Teacher (Col. a) while varying the reward landscape (Col. b). In the Selective Guidance scenario (Top Row), where specific modes are suppressed by the reward model (marked 8 Figure 7. Reward-Driven Shaping. Top: Under selective reward (b), the Student (d) ignores the fixed Teachers (a) gradients in suppressed zones () to strictly match high-value modes (). Bottom: Global alignment recovers the full distribution. ), the Student (Col. d) successfully converges only to highreward targets, effectively overriding the Real Teachers attractive gradients in these low-value regions. In contrast, under Global Alignment (Bottom Row), the student faithfully recovers the full distribution. This confirms that AMD effectively leverages the reward signal to dynamically reshape the generation manifold, ensuring the student avoids Forbidden Zones even when they are supported by the original teacher. By actively sculpting the students distribution according to the reward landscape, AMD enables the student to selectively learn useful modes and effectively transcend the performance ceiling of the original teacher. 5. Conclusion In this work, we identify Forbidden Zones as fundamental failure regime in Distribution Matching Distillation (DMD), where unreliable guidance from the real teacher and vanishing repulsion from the fake teacher jointly stall optimization. To address this, we propose Adaptive Matching Distillation (AMD). Empirically, AMD demonstrates robust effectiveness across both image and video generation benchmarks, successfully bridging the gap between perceptual quality and distributional diversity while overcoming the inherent limitations of the original teacher. More broadly, our framework provides unified optimization perspective on DMD-style methods, revealing them as implicit strategies for navigating corrupted regions. Due to space constraints, we discuss the limitations of our approach and outline future research directions in Section D. By reframing distillation as latent navigation task, AMD offers Optimizing Few-Step Generation with Adaptive Matching Distillation the community novel lens to diagnose and rectify the inherent trade-offs in distribution matching. We hope this perspective encourages the development of more adaptive and self-correcting distillation paradigms that move beyond static alignment toward active generative refinement. 9 Optimizing Few-Step Generation with Adaptive Matching Distillation"
        },
        {
            "title": "Impact Statement",
            "content": "We present ADM (Adaptive Matching Distillation), framework designed for efficient and high-fidelity few-step generation via reward-aware distribution matching distillation. The training process of AMD relies on guidance from pretrained teacher models and reward models. While our experiments utilize standard academic datasets, we acknowledge that the safety and ethical alignment of the distilled student model are intrinsically tied to the quality of these foundational signals. Therefore, we emphasis on the responsible deployment of this technology. We advocate for the integration of content authentication mechanisms and strive to maximize the societal benefits of efficient generation while diligently mitigating potential risks associated with misuse, such as the creation of misleading deepfake content."
        },
        {
            "title": "References",
            "content": "Bai, L., Shao, S., Zhou, Z., Qi, Z., Xu, Z., Xiong, H., and Xie, Z. Zigzag diffusion sampling: Diffusion models can self-improve via self-reflection. arXiv preprint arXiv:2412.10891, 2024. Bansal, A., Chu, H.-M., Schwarzschild, A., Sengupta, S., Goldblum, M., Geiping, J., and Goldstein, T. Universal In Proceedings of the guidance for diffusion models. IEEE/CVF conference on computer vision and pattern recognition, pp. 843852, 2023. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers, 2021. URL https: //arxiv.org/abs/2104.14294. prediction. In The Twelfth International Conference on Learning Representations, 2023. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/ 2310.11513. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. He, H., Ye, Y., Liu, J., Liang, J., Wang, Z., Yuan, Z., Wang, X., Mao, H., Wan, P., and Pan, L. Gardo: Reinforcing diffusion models without reward hacking. arXiv preprint arXiv:2512.24138, 2025. Hertz, A., Aberman, K., and Cohen-Or, D. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23282337, 2023. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. URL https://arxiv.org/abs/1706.08500. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025a. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., Wang, Y., Chen, X., Wang, L., Lin, D., Qiao, Y., and Liu, Z. Vbench: Comprehensive benchmark suite for video generative models, 2023. URL https://arxiv.org/abs/2311. 17982. Chadebec, C., Tasar, O., Benaroche, E., and Aubin, B. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. In The 39th Annual AAAI Conference on Artificial Intelligence, 2025. URL https: //openreview.net/forum?id=D8rQlCEKCT. Huang, Z., Zhang, F., Xu, X., He, Y., Yu, J., Dong, Z., Ma, Q., Chanpaisit, N., Si, C., Jiang, Y., et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025b. Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., and Shan, Y. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73107320, 2024. Chen, X., Wang, Y., Zhang, L., Zhuang, S., Ma, X., Yu, J., Wang, Y., Lin, D., Qiao, Y., and Liu, Z. Seine: Short-tolong video diffusion model for generative transition and Jiang, D., Liu, D., Wang, Z., Wu, Q., Li, L., Li, H., Jin, X., Liu, D., Li, Z., Zhang, B., et al. Distribution matching distillation meets reinforcement learning. arXiv preprint arXiv:2511.13649, 2025. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022. 10 Optimizing Few-Step Generation with Adaptive Matching Distillation Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lin, S., Wang, A., and Yang, X. Sdxl-lightning: Progressive adversarial diffusion distillation, 2024. URL https: //arxiv.org/abs/2402.13929. Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and Dollar, P. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405. 0312. Liu, D., Gao, P., Liu, D., Du, R., Li, Z., Wu, Q., Jin, X., Cao, S., Zhang, S., Li, H., et al. Decoupled dmd: Cfg augmentation as the spear, distribution matching as the shield. arXiv preprint arXiv:2511.22677, 2025a. Liu, J., Liu, G., Liang, J., Li, Y., Liu, J., Wang, X., Wan, P., Zhang, D., and Ouyang, W. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025b. Liu, J., Liu, G., Liang, J., Yuan, Z., Liu, X., Zheng, M., Wu, X., Wang, Q., Xia, M., Wang, X., Liu, X., Yang, F., Wan, P., Zhang, D., Gai, K., Yang, Y., and Ouyang, W. Improving video generation with human feedback, 2025c. URL https://arxiv.org/abs/2501.13918. Liu, K., Hu, W., Xu, J., Shan, Y., and Lu, S. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025d. Liu, X., Park, D. H., Azadi, S., Zhang, G., Chopikyan, A., Hu, Y., Shi, H., Rohrbach, A., and Darrell, T. More control for free! image synthesis with semantic diffusion guidance. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 289299, 2023a. Liu, X., Zhang, X., Ma, J., Peng, J., et al. Instaflow: One step is enough for high-quality diffusion-based text-toimage generation. In The Twelfth International Conference on Learning Representations, 2023b. Lu, Y., Zeng, Y., Li, H., Ouyang, H., Wang, Q., Cheng, K. L., Zhu, J., Cao, H., Zhang, Z., Zhu, X., et al. Reward forcing: Efficient streaming video generation with rewarded distribution matching distillation. arXiv preprint arXiv:2512.04678, 2025. Luo, S., Tan, Y., Huang, L., Li, J., and Zhao, H. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023. URL https://arxiv. org/abs/2310.04378. Ma, G., Huang, H., Yan, K., Chen, L., Duan, N., Yin, S., Wan, C., Ming, R., Song, X., Chen, X., et al. Stepvideo-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers, 2024. URL https://arxiv.org/abs/ 2401.08740. Matsutani, K., Takashiro, S., Minegishi, G., Kojima, T., Iwasawa, Y., and Matsuo, Y. Rl squeezes, sft expands: comparative study of reasoning llms. arXiv preprint arXiv:2509.21128, 2025. Parmar, G., Zhang, R., and Zhu, J.-Y. On aliased resizing In CVPR, and surprising subtleties in gan evaluation. 2022. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. URL https://arxiv.org/abs/ 2307.01952. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/ abs/2112.10752. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. Imagenet large scale visual recognition challenge, 2015. URL https:// arxiv.org/abs/1409.0575. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic text-to-image diffusion models with deep language understanding, 2022. URL https://arxiv.org/abs/2205.11487. Optimizing Few-Step Generation with Adaptive Matching Distillation Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., et al. Longlive: Realtime interactive long video generation. arXiv preprint arXiv:2509.22622, 2025a. Yang, Y., Fan, K., Sun, S., Li, H., Zeng, A., Han, F., Zhai, W., Liu, W., Cao, Y., and Zha, Z.-J. Videogen-eval: Agentbased system for video generation evaluation, 2025b. URL https://arxiv.org/abs/2503.23452. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, B. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024a. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024b. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. Zhang, S., Wang, J., Zhang, Y., Zhao, K., Yuan, H., Qin, Z., Wang, X., Zhao, D., and Zhou, J. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. Zheng, K., Chen, H., Ye, H., Wang, H., Zhang, Q., Jiang, K., Su, H., Ermon, S., Zhu, J., and Liu, M.-Y. Diffusionnft: Online diffusion reinforcement with forward process. arXiv preprint arXiv:2509.16117, 2025. Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022."
        },
        {
            "title": "Progressive distillation for\narXiv preprint",
            "content": "Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103. Springer, 2024. Shao, S., Yi, H., Guo, H., Ye, T., Zhou, D., Lingelbach, M., Xu, Z., and Xie, Z. Magicdistillation: Weak-to-strong video distillation for large-scale few-step synthesis. arXiv preprint arXiv:2503.13319, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zhang, W., Luo, W., et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, F.-Y., Huang, Z., Bergman, A. W., Shen, D., Gao, P., Lingelbach, M., Sun, K., Bian, W., Song, G., Liu, Y., Wang, X., and Li, H. Phased consistency models, 2024. URL https://arxiv.org/abs/2405.18407. Wu, B., Zou, C., Li, C., Huang, D., Yang, F., Tan, H., Peng, J., Wu, J., Xiong, J., Jiang, J., et al. Hunyuanvideo 1.5 technical report. arXiv preprint arXiv:2511.18870, 2025a. Wu, J., Gao, Y., Ye, Z., Li, M., Li, L., Guo, H., Liu, J., Xue, Z., Hou, X., Liu, W., et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025b. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023. Optimizing Few-Step Generation with Adaptive Matching Distillation A. Implementation Details In this section, we provide an overview of the benchmarks, evaluation metrics, diffusion models and the hyperparameter settings in our main paper. A.1. Benchmark MS-COCO. To assess zero-shot text-to-image synthesis performance, we utilize the MS-COCO 2014 validation set (Lin et al., 2015). Following the evaluation protocols established in prior works like DMD (Yin et al., 2024b) and DMD2 (Yin et al., 2024a), we adopt two distinct evaluation settings. First, to measure distributional fidelity, we generate 30,000 images (COCO-30k) using randomly sampled prompts from the validation set. These samples are resized to 256 256 resolution and compared against the full reference set of 40,504 real images to compute FID scores (Heusel et al., 2018) using the clean-fid library (Parmar et al., 2022). Second, for assessing text-image alignment and fine-grained details, we utilize subset of 10,000 prompts (COCO-10k). In this setting, we generate corresponding images at 512 512 resolution and compare them directly with their specific ground-truth references to calculate ImageReward and HPSv2 scores. ImageNet. We evaluate class-conditional image generation capabilities using the ImageNet benchmark (Russakovsky et al., 2015) at resolutions of 256 256 and. Following the rigorous protocols established in SiT (Ma et al., 2024), we assess generation fidelity using the Frechet Inception Distance (FID-50K). Specifically, we generate 50,000 samples and compute metrics against the statistics of the entire ImageNet training set, rather than the validation split. To ensure strict consistency and fair comparison with baseline diffusion models. DrawBench. To evaluate the models robustness across diverse semantic challenges, we utilize DrawBench (Rombach et al., 2022)1. This challenging benchmark comprises 200 carefully curated text prompts, organized into 11 distinct categories. These categories are specifically designed to probe difficult generation capabilities, such as counting, spatial reasoning, rare object generation, and the handling of long, complex descriptions. By covering such wide spectrum of linguistic and visual difficulties, DrawBench serves as fine-grained diagnostic tool for identifying specific limitations in T2I synthesis. HPD v2. We also employ the Human Preference Dataset v2 (HPD v2) (Wu et al., 2023) is an extensive dataset featuring clean and precise annotations. With 798,090 binary preference labels across 433,760 image pairs, it addresses the limitations of conventional evaluation metrics that fail to accurately reflect human preferences. Following the methodologies in (Wu et al., 2023), we employed four distinct subsets for our analysis: Animation, Concept-art, Painting, and Photo, each containing 800 prompts. GenEval. For quantitative assessment of compositional fidelity, we adopt the GenEval framework (Ghosh et al., 2023). This benchmark evaluates how well generated images adhere to prompt constraints regarding object co-occurrence, spatial arrangement, counting, and color binding. GenEval automates this process by utilizing pre-trained object detection models to verify visual attributes, serving as scalable proxy for human judgment. The test set consists of 550 concise and unambiguous prompts, allowing for focused analysis of the models ability to execute precise visual instructions. VBench and VBench++. VBench (Huang et al., 2023) serves as our primary framework for assessing video generation performance. This comprehensive suite moves beyond single-value metrics by decomposing video quality into 16 hierarchical and disentangled dimensions, such as subject consistency, motion smoothness, temporal flickering, and aesthetic quality. For each dimension, VBench utilizes tailored prompt suite and specific evaluation protocols to isolate and measure individual model capabilities. In addition, Huang et al. (2025b) introduce VBench++, which extends the benchmark to support image-to-video evaluation. VBench++ incorporates an adaptive Image Suite that enables fair and consistent assessment across diverse generation settings. VideoGen-Eval. To address the spatiotemporal complexity of modern video synthesis, we employ VideoGen-Eval (Yang et al., 2025b). This agent-based framework moves beyond static metrics by utilizing dynamic evaluation pipeline. It begins with 700 structured, content-rich prompts that explicitly define components such as camera movement, lighting, and subject interactions. An LLM acts as content structurer to decompose these prompts, while Multimodal Large Language 1https://huggingface.co/datasets/shunk031/DrawBench 13 Optimizing Few-Step Generation with Adaptive Matching Distillation Model (MLLM), augmented with specialized temporal patch toolsserves as the adjudicator. This hierarchical approach allows for granular assessment of how well generated videos adhere to multifaceted instructions, ensuring high alignment with human judgment across diverse set of generated assets. TA-Hard. Standard benchmarks often utilize prompts that are semantically straightforward, potentially masking models limitations in complex instruction following. To mitigate this, we incorporate the TA-Hard prompt set (Liu et al., 2025c). This collection is specifically curated to stress-test Text Alignment (TA) by focusing on high semantic density and imaginative scenarios, such as anthropomorphic animals playing instruments or complex interactions between disparate objects. By presenting the model with intricate and counter-intuitive descriptions that require precise compositional reasoning, TA-Hard serves as rigorous diagnostic tool for evaluating the upper bounds of models ability to maintain fidelity to challenging textual constraints. A.2. Evaluation Metric PickScore. PickScore (Kirstain et al., 2023) serves as preference-based evaluation metric trained on the massive Pick-aPic dataset. By fine-tuning CLIP model to predict human choices from pairwise comparisons, it effectively captures the nuances of user satisfaction. Unlike distributional metrics such as FID, PickScore is designed to evaluate individual sample quality and alignment, offering stronger correlation with human judgment in open-domain text-to-image generation tasks compared to traditional benchmarks. HPS v2. The human preference score version 2 (HPS v2) is an improved model to predict user preferences, created by fine-tuning the CLIP model (Radford et al., 2021) on the HPD v2. This refined metric is designed to align T2I generation outputs with human tastes by estimating the likelihood that synthesized image will be preferred, thereby serving as reliable benchmark for evaluating the performance of T2I models across diverse image distributions. ImageReward. ImageReward (Xu et al., 2023) is comprehensive reward model trained via reinforcement learning from human feedback (RLHF) principles. By learning from vast collection of expert-annotated comparisons, it addresses the limitations of standard metrics by mitigating gamification issues. It jointly assesses fidelity and alignment, demonstrating correlation with human ranking that surpasses traditional metrics like IS and FID, establishing it as robust tool for automated evaluation. VideoAlign. VideoAilgn (Liu et al., 2025c) is multi-dimensional reward model designed for assessing video generation. It is trained on the VideoGen-RewardBench dataset, large-scale collection of 26.5k video triplets with human preference annotations. Unlike typical reward models that focus on single quality metric, VideoAlign evaluates videos across three distinct dimensions: Visual Quality (VQ), Motion Quality (MQ), and Text Alignment (TA). These scores can be aggregated into total reward that reflects overall human preference. The model is built on Vision-Language Model (VLM) backbone (e.g., Qwen2-VL-2B) and trained using Bradley-Terry model with ties (BTT) objective, making it robust to tied preferences and effective for aligning flow-based video generation models. A.3. Diffusion Models In the main paper, we totally use 5 diffusion models, including SiT for class-to-image generation, SD1.5 and SDXL for text-to-image generation, and LongLive and Wan2.1 for text-to-video generation. SiT. Scalable Interpolant Transformers (SiT), proposed by Ma et al. (2024), are family of generative models built upon the Diffusion Transformer (DiT) backbone. SiT utilizes the stochastic interpolant framework, which allows for more flexible connection between the data and noise distributions compared to standard diffusion models. This framework enables modular exploration of critical design choices, including discrete versus continuous time learning, different model prediction targets (e.g., velocity vs. score), and various interpolant paths. By adopting velocity-prediction parameterization and linear interpolants, SiT achieves superior performance over standard DiTs on ImageNet benchmarks with identical model size and compute. SDXL. SDXL (Podell et al., 2023) is latent diffusion model that significantly scales up the U-Net backbone to approximately 2.6B parameters. To improve text comprehension and prompt adherence, it employs dual text encoder 14 Optimizing Few-Step Generation with Adaptive Matching Distillation architecture, combining the outputs of CLIP ViT-L and OpenCLIP ViT-bigG. SDXL addresses common training data issues through novel micro-conditioning schemes, where the model is conditioned on the original image resolution and cropping coordinates, allowing it to generate high-quality images without discarding small or non-square training samples. Additionally, it utilizes multi-aspect training and separate refinement model to enhance the visual fidelity of high-resolution synthesis. LongLive. LongLive (Yang et al., 2025a) is frame-level autoregressive framework designed for real-time, interactive long video generation. Built by fine-tuning the Wan2.1-1.3B model, LongLive adopts causal attention mechanism that leverages KV caching to accelerate inference. To enable smooth transitions during interactive prompt switching, it introduces KV-recache technique that updates cached states with new prompt embeddings. Furthermore, LongLive employs streaming long tuning strategy to align training with long-context inference, alongside short window attention mechanism paired with frame-level attention sink. These designs allow the model to maintain long-range temporal consistency and high throughput, achieving approximately 20 FPS on single H100 GPU for videos up to 240 seconds. Wan2.1. Wan2.1, introduced in (Wan et al., 2025), is an open-source video generation model developed by Alibaba, based on Diffusion Transformer (DiT) architecture and flow matching framework. It supports multiple tasks including text-tovideo (T2V) and image-to-video (I2V). The model is available in two versions: 14B-parameter variant for high-quality 720p generation and lightweight 1.3B variant suitable for consumer-grade GPUs. Due to the resource limits, in this paper, we utilize the Wan2.1-T2V-14B. A.4. Hyperparameter Settings Here we provide the detailed hyperparameter configurations for both image and video generation tasks. Video Generation. For the streaming video generation task, we strictly follow the first-stage experimental protocol of LongLive (Yang et al., 2025a). We utilize Wan2.1-1.3B (Wan et al., 2025) as the base model and conduct distillation for 700 iterations with learning rate of 2.0 106 and total batch size of 8. Specifically, the local attention size is set to 12, and the frame sink size is 3. For bidirectional-attention video generation, we employ Wan2.1-14B as the backbone, performing 800 distillation steps with learning rate of 5.0 107 and batch size of 8. Across all video tasks, we adopt VideoAlign (Liu et al., 2025c) as the reward proxy. During evaluation, we generate videos with 81 frames at 16 FPS, corresponding to duration of 5 seconds. Image Generation. For class-to-image generation, we utilize SiT-XL/2 (Ma et al., 2024) as the backbone for all ImageNet (256256) experiments. For the main results in Tab. 4, we adopt two-stage training protocol: all models are first initialized from Pure-DMD checkpoint pre-trained for 3,000 iterations. Subsequently, we perform fine-tuning using Pure-DMD, DMDR, and AMD, respectively, for an additional 1,000 iterations. For the ablation studies in Figure 5, the second-stage fine-tuning is extended to 2,000 iterations to ensure full convergence for component analysis. The total batch size is 512, and the learning rate is set to 2.0 105 with DINOv2 (Caron et al., 2021) serving as the reward proxy. For experiments on text-to-image, especially for SDXL, as shown in Tab. 7, Tab. 3, Tab. 9, and Tab. 2, we strictly follow the training setting in DMD2 (Yin et al., 2024a). However, due to the resource limit, we only adopt 8 H800 to distill the model. B. Related Work In this section, we review existing works relevant to AMD. Distribution Distillation in Diffusion Model Diffusion models are commonly distilled into low-step student models to reduce inference cost. line of works studies score distillation, where pretrained diffusion model provides score-based supervision for training fast generators. Score distillation was first popularized in text-to-3D generation (Hertz et al., 2023; Poole et al., 2022), where pretrained text-to-image diffusion model provides score-based supervision for optimizing 3D representations. Building upon this idea, Yin et al. (2024b) proposed Distribution Matching Distillation (DMD), which minimizes an approximate KL divergence between the student and teacher distributions. Subsequent work, DMD2 Yin et al. (2024a), further introduces an adversarial (GAN) loss to improve training stability and sample diversity. More recent variants (Jiang et al., 2025; Shao et al., 2025; Liu et al., 2025a) focus on improving the robustness and stability of the DMD objective. Decoupled-DMD (Liu et al., 2025a) reformulates the objective into CFG-augmented matching term 15 Optimizing Few-Step Generation with Adaptive Matching Distillation and regularization term, together with more stable renoising scheduler. MagicDistillation (Shao et al., 2025) argues that, beyond updating the fake teacher, the real teacher should also be adaptively updated during training, while DMDR (Jiang et al., 2025) introduces an auxiliary reinforcement learning objective to optimize the student independently. In addition, several works (Huang et al., 2025a; Yang et al., 2025a; Liu et al., 2025d) extend the DMD paradigm to long video generation. In this work, we provide unified perspective on DMD variants and show that their modifications can be viewed as implicit strategies for avoiding unreliable regions of the real teacher. We formalize such regions as Forbidden Zones, where naive score matching leads to unstable or misleading gradients. Optimization View on Generation Tasks. Generative modeling can be fundamentally viewed as an optimization problem aimed at finding high-probability modes within complex energy landscape. Sampling methods, such as Langevin Dynamics (Song & Ermon, 2019) and Probability Flow ODEs (Song et al., 2020), essentially perform gradient ascent on the log-density score field to traverse from noise to data. This optimization perspective has been explicitly leveraged in controllable generation, where external guidance signalssuch as classifier gradients (Dhariwal & Nichol, 2021; Bai et al., 2024) or CLIP scores (Radford et al., 2021)are injected into the sampling process to steer trajectories toward desired attributes (Bansal et al., 2023; Liu et al., 2023a). Our work aligns with this philosophy but operates at the training stage: we treat the distillation process as navigating treacherous energy field, requiring dynamic gradient reshapping to traverse Forbidden Zones. Reward-Guided Diffusion Training. The integration of Reinforcement Learning (RL) into diffusion training has gained significant traction. Recent methods (Liu et al., 2025b; Wu et al., 2025b; Zheng et al., 2025; He et al., 2025) treat the denoising chain as policy, directly optimizing it to maximize global rewards. Most pertinent to our work is the recently proposed Reward Forcing (Lu et al., 2025), which biases the students distribution by prioritizing training samples with high reward scores. However, critical distinction separates AMD from such prioritization schemes. While Reward Forcing focuses on emphasizing where to go by up-weighting successful samples, AMD focuses on how to escape when the model fails. We treat low-reward samples not merely as data to be down-weighted, but as active indicators of Forbidden Zones. C. Proof C.1. Proof of Proposition 3.1 Proof. Our goal is to show that the parameter update of the student Gθ induces an effective gradient descent trajectory in the latent space. We proceed in three steps: relating parameter shifts to latent shifts, converting scores to denoised targets, and identifying the resulting potential function. Step 1: First-order Approximation of Latent Update. Consider the infinitesimal update of the student parameters: θ = ηθLDMD. For fixed latent code z, the change in the generated sample = Gθ(z) can be approximated via the first-order Taylor expansion: xnew Gθ(z) θ θ = η Gθ(z) θ θLDMD. Substituting the DMD gradient formulation from Eq. 2, we obtain: xnew η Et (cid:2)(sreal(Ft) sfake(Ft)) JθJ θ (cid:3) , (15) (16) where Jθ = Gθ(z) θ acts as positive semi-definite scaling matrix. For the purpose of trajectory analysis, we absorb this architectural dependency into the effective step size ηeff, yielding: is the Jacobian. In the local neighborhood of optimization, the term JθJ θ xnew sreal(Ft) sfake(Ft). (17) Step 2: Score-to-Displacement Transformation. According to Tweedies formula, the score function s(Ft) at diffused state Ft(x) is related to the denoised estimate ˆx0 by s(Ft) = αt ˆx0Ft . The difference between the real and fake teacher scores can thus be expressed as: σ2 αt ˆx0,real Ft σ2 αt ˆx0,fake Ft σ2 (18) sreal sfake = = αt σ2 (ˆx0,real ˆx0,fake) . 16 Optimizing Few-Step Generation with Adaptive Matching Distillation By defining the latent displacements dreal = ˆx0,real and dfake = ˆx0,fake, we can rewrite the target difference as: ˆx0,real ˆx0,fake = (x dreal) (x dfake) = (dreal dfake). (19) Step 3: Identification of the Contrastive Potential. Substituting the results from Step 2 back into Eq. 17, the latent update rule becomes: xnew = ηeff (dreal dfake) . (20) We observe that the term (dreal dfake) is exactly the gradient of the potential function VDMD(x) = 1 1 2 ˆx0,fake2 with respect to x: 2 ˆx0,real2 xVDMD(x) = (x ˆx0,real) (x ˆx0,fake) = dreal dfake. (21) Thus, the latent update follows xnew = ηeffxVDMD(x), which characterizes gradient descent step on the contrastive potential VDMD(x). C.2. Theoretical Analysis of Prior Art In this subsection, we provide theoretical justifications for re-interpreting D-DMD and MagicDistillation within our optimization framework. We demonstrate that both methods essentially serve to mitigate the gradient vanishing or hallucination problem within the Forbidden Zone Zf , albeit through distinct mechanisms: D-DMD via noise-induced support expansion, and MagicDistillation via manifold bridging. D-DMD: Risk Reduction via Noise-Induced Support Expansion. D-DMD (Liu et al., 2025a) modifies the distillation objective by computing the real teachers score at higher noise level than the students current state. Let tG be the students generation step and tϕ be the teachers score estimation step, where tϕ > tG. Proposition C.1. Increasing the diffusion time expands the effective support of the teachers distribution, thereby reducing the probability that student sample falls into the Forbidden Zone Zf . Proof. Let p0(x) be the clean data distribution. The distribution at time t, pt(x), is the convolution of p0 with Gaussian kernel (0, σ2 I): (cid:90) pt(x) = p0(y)N (x; y, σ2 I)dy. (22) The Forbidden Zone at time t, denoted (t) reliable score estimation: (t) = {x pt(x) < ϵ}. , is formally defined as the region where the probability density is insufficient for Since convolution with Gaussian is smoothing operation, the support of pt(x) effectively widens as σt increases. For distorted student sample xbad that lies far from the data manifold (i.e., xbad (tG) ), the density ptG(xbad) vanishes, leading to an undefined or oscillating score sreal(xbad, tG). However, by increasing the noise scale to tϕ > tG, the Gaussian kernel broadens, ensuring that ptϕ(xbad) ϵ. This effectively removes xbad from the Forbidden Zone of the higher noise level, i.e., xbad / (tϕ) . Consequently, the D-DMD update provides valid, coarse-grained directional guide: dreal E[x0 xtϕ] x, (23) which pulls the sample back towards the high-density region, bypassing the gradient vacuum encountered at tG. MagicDistillation: Manifold Bridging via Energy Landscape Adaptation. MagicDistillation (Shao et al., 2025) finetunes the real teacher ϕ on the students generated samples before using it for distillation. Let ϕadapt denote the adapted teacher parameters. Proposition C.2. Adapting the teacher on student samples modifies the energy landscape to create continuous gradient path (bridge) connecting the student distribution to the target data distribution. 17 Optimizing Few-Step Generation with Adaptive Matching Distillation Proof. Consider the standard real teacher ϕfix trained solely on pdata. Its energy potential Ereal(x) log pdata(x) is undefined or essentially flat for samples in the Forbidden Zone Zf , where pdata(x) 0. This results in gradient xEreal(x) 0 or random noise. MagicDistillation updates the teacher to minimize the denoising error on student samples pstudent. This process effectively trains the teacher on mixture distribution pmix = (1 λ)pdata + λpstudent. The adapted energy landscape can be approximated as: Eadapt(x) log ((1 λ)pdata(x) + λpstudent(x)) . (24) For distorted sample Zf where pdata(x) 0 but pstudent(x) is high, the adapted energy is dominated by the student term, ensuring the potential is well-defined. Crucially, since the adaptation is typically constrained (e.g., via LoRA or short-term finetuning), the teacher retains the global topology of pdata. Therefore, the optimization creates bridge in the energy landscape: xEadapt(x) acts as valid vector field that attracts from the students current mode towards the target manifold. This eliminates the gradient discontinuity in Zf , allowing the student to traverse from pstudent to pdata smoothly. DMD, DMD2, and DMDR: Deadlock Breaking via External Forces. Unlike D-DMD and MagicDistillation which attempt to repair the internal gradient fields of the teachers, DMD, DMD2, and DMDR introduce an auxiliary external force Fext to dominate the optimization when the primary distillation gradients vanish. We unify these methods under the External Force Hypothesis. Proposition C.3 (External Force Domination). Let gDMD = xVDMD be the intrinsic distillation gradient. In the Forbidden Zone Zf , where gDMD 0 or becomes stochastic noise, the optimization trajectory is governed by the external auxiliary field Fext provided by third-party supervisor (Ground-truth, Discriminator, or Reward Model): xnew η(gDMD (cid:124) (cid:123)(cid:122) (cid:125) 0 +λFext) ηλFext. (25) We clarify the specific form of Fext for each method below: DMD: Static Tethering via Regression. Standard DMD (Yin et al., 2024b) employs an L2 regression loss on small set of paired data {(z, xgt)}. The effective force is: FDMD = xx xgt2 (x xgt). Proof Sketch: This force acts as global linear spring (Hookes Law). Regardless of whether Zf or how distorted the energy landscape Ereal(x) is, the vector (x xgt) always points strictly towards the ground truth sample. This tethering provides robust, albeit rigid, recovery signal that drags the student out of the Forbidden Zone explicitly. (26) DMD2: Adversarial Boundary Enforcement. DMD2 (Yin et al., 2024a) removes the regression loss but introduces GAN discriminator Dϕ trained to distinguish real images from student samples. The external force is: FDMD2 = x( log Dϕ(x)) = xDϕ(x) Dϕ(x) . (27) Proof Sketch: The discriminator Dϕ is dynamically trained on the union of real data and current student samples (including those in Zf ). Therefore, unlike the frozen Real Teacher which has no support in Zf , the discriminator explicitly learns the boundary of Zf . Specifically, D(x) assigns low scores to samples in Zf , creating steep gradient field xD(x) that pushes samples perpendicular to the decision boundary, effectively repelling them back towards the data manifold. DMDR: Asynchronous Reward Steering. DMDR (Jiang et al., 2025) integrates Reinforcement Learning, using reward model R(x) to guide generation. The update can be viewed as an external force: FDMDR xR(x). (28) Proof Sketch: We assume the reward model R(x) (e.g., ImageReward or HPS) generalizes better than the generative density model due to its discriminative pre-training nature. In Zf , while the generative score log p(x) oscillates, the reward gradient R(x) maintains consistent direction towards high-fidelity regions (Assumption 3.3). This acts as an asynchronous steering wheel, overriding the confused signals from the teacher to navigate the student towards the region of interest defined by human preference. 18 Optimizing Few-Step Generation with Adaptive Matching Distillation D. Limitation and Future Work While AMD provides unified and effective framework for multiphysics-aware distillation, it possesses certain limitations that open avenues for future research. First, the efficacy of our self-correcting mechanism is inherently tied to the accuracy of Forbidden Zone (Zf ) identification. Since AMD relies on reward model to provide the advantage signal a, its performance is sensitive to the proxys quality. If the reward model provides noisy or poorly calibrated feedback, it may fail to precisely detect Zf pathologies, thereby preventing the adaptive mechanism from being fully triggered and leaving certain corrupted regions unrectified. Future work could explore more robust, unsupervised, or ensemble-based methods for Zf detection to reduce dependency on single external proxy. Second, although our current adaptive operator HAM demonstrates significant performance gains, there remains vast room for more sophisticated adaptation strategies. Beyond the current formulation, future research could investigate integrating advanced optimization principlessuch as the Momentum Trick, Orthogonal Gradient techniques, or Secondorder informationinto the teacher-student interplay. Such refinements could further stabilize the training trajectories and accelerate the students convergence toward the true data manifold, particularly in high-dimensional and complex generative tasks. E. Algorithm Algorithm 1 Adaptive Distribution Matching Distillation (AMD) Input: Student Gθ, Real Teacher ϕ, Fake Teacher ψ, Reward Model R, Sensitivity s, Selection factor k, Group size Output: Optimized student parameters θ i=1 where xi = Gθ(zi, y) 1 while not converged do 2 Sample prompt pdata Generate group of samples: {xi}K // Reward-aware Diagnosis ri = R(xi) for = 1 . . . µg = mean(r), σg = std(r) ai = clip((ri µg)/(σg + ϵ), 1, 1) Initialize gradients θL 0, ψLψ 0 Sample timestep U[0, 1] for each sample xi in group do Sample noise ϵi (0, I) xt,i = αtxi + σtϵi // 1. Student Update Get targets ˆx0,real from ϕ and ˆx0,fake from ψ dreal,i = xi ˆx0,real, dfake,i = xi ˆx0,fake αi = 1 + ai, gi = HAM θL θL + gi // 2. Fake Teacher Update Wi = exp(ai) ; Ldiff,i = Wi ϵi ϵψ(xt,i, t)2 ψLψ ψLψ + ψ(wi Ldiff,i) βi = 1 ai Gθ θ (cid:0)dreal(x, Ft, ϕ), dfake(x, Ft, ψ)(cid:1) ; end Update student: θ θ ηθθL Update fake teacher: ψ ψ ηψψLψ 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 end // Eq. 6 with score adaptation // Higher weight for low rewards Optimizing Few-Step Generation with Adaptive Matching Distillation F. Supplementary Experimental Results In this section, we present additional analytical experiments, quantitative evaluations, and qualitative demonstrations to further validate the effectiveness of AMD. F.1. Analytical Experiments In this section, we provide empirical evidence to support the design motivation of AMD. Navie Adaption vs AMD. According to our unified optimization framework in Equation (6), the choice of the adaptation operator fundamentally dictates the distillation dynamics. To investigate this, we compare naive adaptive strategy (Hnaive, see Equation (8)) with our proposed decoupled modulation (HAMD, see Equation (10)) on 2D multi-modal toy dataset. In this setup, the real teachers energy potential represents symmetric multi-modal distribution, while the reward model favors specific subset of modes (located in the lower-left region). As illustrated in Fig. 8, although both methods are guided by the same reward proxy, their training stability and convergence behavior differ significantly: Naive Adaptation: Simple linear scaling of dreal and dfake fails to resolve the structural conflict between the teachers global guidance and the reward models local prioritization. In the mid-to-late stages of training (Iteration 10002000), this leads to catastrophic distribution collapse, where the student distribution loses its multi-modal structure and dissolves into an uninformative mass. AMD (Ours): By contrast, AMD successfully navigates the student toward the high-reward regime while maintaining the structural integrity of the generation manifold. By decoupling the signals into distribution matching (DM) and conditional alignment (CA) terms, AMD enables stable, high-fidelity steering that avoids the gradient deadlock encountered by the naive approach. These results empirically validate the necessity of the decoupled adaptation mechanism in AMD for achieving stable, reward-aware distribution matching. Figure 8. We track the evolution of the student distribution on 2D toy dataset. Top: Naive Adaptation (Hnaive) suffers from mode merging and eventual distribution collapse as it fails to balance the competing distillation forces. Bottom: AMD (HAMD) successfully maneuvers the student toward high-reward modes (lower-left) while preserving sharp, distinct distributional fidelity. This highlights the criticality of decoupled signal modulation in preventing training instability within the Forbidden Zone. Optimizing Few-Step Generation with Adaptive Matching Distillation F.2. Quantitative Results Results of AMD in other Image Generation Benchmark Here, we first provide the breakdown of the experiments on GenEval, as shown in Tab. 8, our proposed AMD almost outperforms the previous methods across all dimensions. Besides, we conduct the experiments on DrawBench, as shown in Tab. 9, the experiment results further validate the effectiveness of our method. Last but not least, we provide the winning rate of AMD across different datasets, compared with the standard DMD2. Figure 9. The winning rate of AMD over DMD2 on SDXL across DrawBench and HPDv2. The standard DMD2 (baseline) winning rate defaults to 50%. The results reveal the superiority of AMD in synthesizing images with good quality, comparing with DMD2. Table 8. Comparison of different text-to-image models on GenEval (Ghosh et al., 2023) benchmark. The best results in each category are highlighted in bold. Method #Params Resolution NFEs Overall Single Two Count Colors Pos Attr. SDXL (Podell et al., 2023) FLUX.1-dev (Labs, 2024) 2.6B 12.0B 1024 1024 1024 1024 50 2 0.55 0.66 Pretrained Models Distilled Models SDXL-LCM (Luo et al., 2023) SDXL-Turbo (Podell et al., 2023) SDXL-Lightning (Lin et al., 2024) SDXL-DMD2 (Yin et al., 2024a) SDXL-DMDR (Jiang et al., 2025) SDXL-AMD (Ours) 2.6B 2.6B 2.6B 2.6B 2.6B 2.6B 1024 1024 512 512 1024 1024 1024 1024 1024 1024 1024 4 4 4 4 4 4 0.50 0.56 0.53 0.51 0.56 0.57 0.98 0.98 0.99 1.00 0.98 0.98 0.99 1.00 0.74 0.81 0.55 0.72 0.61 0.62 0.76 0. 0.39 0.74 0.38 0.49 0.44 0.43 0.42 0.47 0.85 0.79 0.85 0.82 0.84 0.82 0.84 0.85 0.15 0.22 0.23 0. 0.07 0.11 0.11 0.07 0.11 0.10 0.14 0.21 0.21 0.15 0.24 0.23 Table 9. Quantitative comparison of text-to-image task on DrawBench (Rombach et al., 2022) dataset. (base model: SDXL). We compare AMD against DMD2 using lot of preference-based metrics. The results validate the effectiveness of AMD. Method DMD2 AMD (Ours) PickScore HPSv2 29.66 30.40 22.19 22. ImageReward 79.46 84.01 Results of AMD in other Video Generation Benchmark We further evaluate the scalability of AMD by applying it to the larger Wan2.1-14B backbone. As shown in Tab. 10, AMD achieves higher overall performance than DMD2 on our internal benchmark, improving the Total Score from 118.61 to 122.15. Our internal benchmark consists of 419 diverse image prompts collected from the web, covering wide range of visual styles (e.g., anime, human-centric scenes, and realistic physical environments), as well as both landscape and portrait layouts. For each image, captions are generated using Gemini 2.5 Pro to ensure rich and consistent textual descriptions. We plan to release this benchmark publicly in future work. Notably, we observe trade-off between Visual Quality (VQ) and Text Alignment (TA), which is consistent with our findings on the Wan2.1-1.3B model. This behavior can be attributed to the VideoAlign reward, which explicitly encourages dynamic motion generation, leading to substantial improvement in Motion Quality (MQ, +16.24), sometimes at the expense of static visual fidelity and strict text-image alignment. Furthermore, as reported in Tab. 11, evaluation under the VBench++ (Huang et al., 2025b) framework shows that AMD consistently outperforms DMD2. 21 Optimizing Few-Step Generation with Adaptive Matching Distillation Table 10. Quantitative comparison of Image-to-Video generation task. (base model: Wan2.1-14B). Comparison between AMD and DMD2 across VBench and Internal metrics. Method VBench-I2V Internal Bench VQ MQ TA Total VQ MQ TA Total 26.48 32.62 67.26 126.36 12.90 5.27 100.47 118.61 DMD2 AMD (Ours) 30.56 39.49 60.67 130.72 11.26 21.51 89.30 122. Table 11. Quantitative comparison of Image-to-Video generation task. (base model: Wan2.1-14B). Best results are highlighted. Method 100-NFE CogVideoX-I2V-SAT (Yang et al., 2024) I2Vgen-XL (Zhang et al., 2023) SEINE-512x320 (Chen et al., 2023) VideoCrafter-I2V (Chen et al., 2024) SVD-XT-1.0 (Blattmann et al., 2023) Step-Video-TI2V (Ma et al., 2025) 4-NFE DMD2 (Yin et al., 2024a) AMD (Ours) I2V Subject Motion Subject () Consistency () Smoothness () Aesthetic Quality () Imaging Quality () 97.67 97.52 96.57 91.17 97.52 95.50 0.9612 0.9663 95.47 96.36 94.20 97.86 95.52 96.02 0.9671 0.9843 98.35 98.31 96.68 98.00 98.09 99.24 0.9908 0. 59.76 65.33 58.42 60.78 60.15 70.44 67.64 69.85 70.97 71.68 69.80 78.82 0.6668 0.6705 0.7092 0.7098 F.3. Visualization Results We provide an extensive gallery of visual samples generated by AMD across both image and video generation tasks, highlighting its superiority in fidelity and semantic alignment compared to standard distillation methods Image Generation We present broader range of image generation results across different datasets, including DrawBench and HPDv2, using AMD distilled from SDXL. As shown in Fig. 10, 11, 12, 13, and 14, the visual results demonstrate the effectiveness of our proposed AMD, compared with standard DMD2, where synthesized image quality and texture have greatly improved. Figure 10. Synthesized images of ADM distilled from SDXL on DrawBench. 22 Optimizing Few-Step Generation with Adaptive Matching Distillation Figure 11. Synthesized images of ADM distilled from SDXL on concept-art subset of HPD v2. Figure 12. Synthesized images of ADM distilled from SDXL on painting subset of HPD v2. 23 Optimizing Few-Step Generation with Adaptive Matching Distillation Figure 13. Synthesized images of ADM distilled from SDXL on photo subset of HPD v2. Figure 14. Synthesized images of ADM distilled from SDXL on anime subset of HPD v2. 24 Optimizing Few-Step Generation with Adaptive Matching Distillation Video Generation We present broader range of video generation results using the Wan2.1-1.3B backbone. Fig. 15 visually compares videos distilled via standard DMD versus our AMD. It is evident that the baseline DMD often suffers from temporal flickering or motion degradation (e.g., static backgrounds or unnatural warping) in challenging scenarios. In contrast, AMD preserves dynamic fidelity and generates smoother, more logically consistent motion, demonstrating the efficacy of our decoupled modulation strategy in the spatiotemporal domain. Figure 15. Qualitative comparison on text-to-video generation. We show that AMD significantly outperforms the baseline, exhibiting superior motion smoothness, higher visual fidelity, and better prompt alignment."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen",
        "xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}