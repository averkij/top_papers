{
    "paper_title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs",
    "authors": [
        "Zifan He",
        "Shengyu Ye",
        "Rui Ma",
        "Yang Wang",
        "Jason Cong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 4 7 1 6 0 . 1 1 5 2 : r LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs Zifan He University of California, Los Angeles Los Angeles, USA zifanhe1202@g.ucla.edu Shengyu Ye Microsoft Research Asia Beijing, China v-yeshengyu@microsoft.com Rui Ma Microsoft Research Asia Beijing, China mrui@microsoft.com Yang Wang Microsoft Research Asia Beijing, China Yang.Wang92@microsoft.com Jason Cong University of California, Los Angeles Los Angeles, USA cong@cs.ucla.edu Abstract The rapid development of large language models (LLM) has greatly enhanced everyday applications. While cloud-based LLM services often dominate attention due to their massive processing capabilities, efficient single-batch inference remains critical research direction for enabling on-device intelligence. In this realm, FPGAbased accelerators, with their micro-architectural flexibility for finegrained data control, are well recognized for achieving superior speed and energy efficiency compared to GPUs. However, recent GPU-specific optimizations have diminished this advantage. When limited to arithmetic-based computation, FPGAs can underperform GPUs due to their comparatively fewer computational resources. To address this challenge, we exploit key advantage of FPGAs over GPUs: abundant distributed on-chip memory embedded among computational units. We believe that shifting language model inference from arithmetic-based to memory-based computations through table lookups can improve the efficiency on FPGAs to compete with GPUs. However, existing memory-based accelerators are inefficient or unable to scale to large models, and fail to capture the execution properties of language models. This paper introduces LUT-LLM, the first FPGA accelerator for 1B+ language model inference with memory-based computation, leveraging vector quantization. We construct performance model, evaluate multiple quantization schemes, and identify activation-weight vector co-quantization as the most effective approach. To support this scheme for LLM inference, LUT-LLM features (1) bandwidth-aware parallel centroid search to reduce decoding latency, (2) efficient 2D table lookups for activation-weight co-quantization, and (3) spatial-temporal hybrid design to reduce data caching for higher throughput table lookup. We prototype LUT-LLM for customized These authors contribute equally to this work Work done during internship at Microsoft Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, Washington, DC, USA 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX Qwen 3 1.7B model on the AMD V80 FPGA, achieving 1.66 lower end-to-end latency than the AMD MI210 GPU and 1.72 higher energy efficiency than the NVIDIA A100 GPU. LUT-LLM can scale to 32B model with 2.16 better energy efficiency than the A100. CCS Concepts Hardware Hardware accelerators; Computer systems organization Neural networks. Keywords Vector Quantization, Language Model Inference, Table Lookup ACM Reference Format: Zifan He, Shengyu Ye, Rui Ma, Yang Wang, and Jason Cong. 2026. LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs. In . ACM, New York, NY, USA, 12 pages. https://doi.org/ XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nThe advancement of LLMs has enabled a wide range of applications\nsuch as conversational chatbots [11, 39], coding agents [29, 62], and\ndeep research [24] that assists our daily lives. Mainstream LLM\nservices are deployed through cloud-based infrastructures with\nclusters of data center GPUs and serve multiple user requests con-\ncurrently. While effective at scale, this paradigm faces limitations\nwhen tackling edge scenarios such as personal AI assistants [10, 27],\nsmart home devices [42], and interactive robotics [6], where sam-\nples cannot be batched to fully utilize the resources. Consequently,\nefficient single-batch inference has emerged as a crucial research\ndirection to enable on-device intelligence.",
            "content": "FPGAs have been widely recognized as promising platforms for single-batch language model inference, offering competitive speed and energy efficiency relative to GPUs [8, 9, 61]. Nevertheless, recent advancements in GPU architectures and software optimization have narrowed this performance gap. In particular, serving Llama 2 7B model on single NVIDIA A100 GPU obtains 2.38 throughput improvement and 3.27 energy efficiency gain when FlashAttention [12], FlashDecoding [22], and quantization with GPTQ [14] are enabled. Under these settings, GPU inference is 2.00 6.37 faster with similar or better energy efficiency than state-of-the-art (SoTA) FPGA accelerators [9, 61]. These results Conference17, July 2017, Washington, DC, USA Zifan et al. lookups that store pre-computed dot product results. To enable efficient table construction, we apply vector quantization [17], where multiple values in matrices are quantized into low-bit indices. To resolve C1, we first develop performance model for model inference with memory-based computation and vector quantization to understand the scaling (Section 3). We innovatively show that vector quantizing both weights and activations with pre-computed 2D lookup tables is feasible and achieves the best performance. From this foundation, we design the LUT-LLM accelerator with the following three features that resolves C2 (Section 4): Bandwidth-aware parallel centroid search: LUT-LLM finds the closest centroid of every input vector to access the corresponding entry in the lookup tables. Unlike naive single pipeline [28] or complete binary reduction tree architecture, LUT-LLM employs hybrid design with multiple parallel pipelines and smaller reduction tree. The parallelization size and pipeline depth are co-designed with the memory bandwidth to ensure the maximum throughput of the compute engine by fully hiding the centroid search latency. Efficient 2D table lookup based prefix-sum: To support vector quantization of both activations and weights, we construct 2D lookup tables, where one dimension corresponds to activation centroid indices and the other to weight centroid indices. During inference, the tables are loaded to on-chip memory in parallel with the centroid search, and the corresponding table entries are dynamically retrieved and expanded online, along with SIMD accumulation. Spatial-temporal hybrid design: LUT-LLM adopts hybrid execution strategy, following design principle similar to InTAR [21] and SSR [63]. Specifically, the attention layer is implemented in dataflow manner, whereas linear layers are executed sequentially. This design spares 14% on-chip buffers from attention to the linear layer for more parallel accesses, increasing the throughput. We implement prototype of LUT-LLM targeting customized Qwen-3 1.7B model on an AMD V80 FPGA. Comparing with GPUs evaluated using vLLM [26] with the model quantized to INT8 via GPTQ, experimental results demonstrate that our design is 1.66 faster and is 4.1 more energy efficient compared to AMD MI210 [3]. It is also 1.72 more energy efficient than the NVIDIA A100 GPU. Extending LUT-LLM for the Qwen 3 32B model can have 2.16 energy efficiency boost than the A100."
        },
        {
            "title": "2 Background\n2.1 Language Model Acceleration\n2.1.1 Model Architecture. Most modern language models [13, 45]\nadopt the transformer architecture [49] with stacked blocks of at-\ntention and feedforward networks (FFNs). While the architecture\nremains consistent, recent designs [13, 57] replace multi-head at-\ntention (MHA) with grouped-query attention (GQA) [1].",
            "content": "Given input ğ‘‹ Rğ¿ğ· with sequence length ğ¿ and hidden size ğ·, GQA projects ğ‘‹ into queries, keys, and values. Unlike MHA, GQA maps multiple queries to each key-value pair, reducing cache size while maintaining accuracy. For â„ query heads and ğ‘” heads per group, head ğ‘– is computed as: ğ‘„ğ‘– = ğ‘‹ğ‘Šğ‘„ğ‘– , ğ¾ğ‘–/ğ‘” = ğ‘‹ğ‘Šğ¾ğ‘–/ğ‘” ğ‘‚ğ‘– = softmax(ğ‘„ğ‘–ğ¾ğ‘‡ , , ğ‘‰ğ‘–/ğ‘” = ğ‘‹ğ‘Šğ‘‰ğ‘–/ğ‘” ğ‘‘)ğ‘‰ğ‘–/ğ‘”, ğ‘–/ğ‘”/ (1) Figure 1: Motivations and challenges of memory-based computation for LLM inference on FPGA, with the corresponding solutions as the technical contributions in LUT-LLM. highlight the growing competitiveness of GPUs in efficient inference and suggest that sustaining FPGAs advantages will require algorithmic and architectural innovations. When competing with GPUs, the major challenge is that FPGAs often have fewer computational resources than GPUs process node [2, 4, 36, 37]. This leads to slow input processing and underutilized memory bandwidth because FPGAs spend more time on computation than on memory access. However, FPGAs typically provide larger amounts of distributed on-chip memory units compared to GPUs. Depicted in Figure 1, the AMD V80 [4] integrates 14.9 more on-chip memory units with 2.5 larger capacity than the NVIDIA A100 [37]. Therefore, shifting the computational paradigm from arithmetic-based operations to memory-based computations is promising. Prior memory-based accelerators [5, 16, 28, 34] show the efficiency on neural networks within 10M parameters, but face two challenges when deploying language models (Figure 1): Scaling inefficiency and infeasibility (C1): direct deployment of existing methods can lead to inefficiency due to higher memory and computation cost compared to conventional arithmetic approaches, or infeasible with limited on-chip resources. Additional Complexity of LLMs (C2): prior memory-based neural accelerators overlook the additional complexity of LLM computations, where centroid search is hard to pipeline in decoding, lookup table access is limited by on-chip memory ports, and interactions between linear layers and other components further hinder data movement efficiency. In this paper, we propose LUT-LLM 1, the first FPGA accelerator for 1B+ language models with memory-based computations. LUT-LLM replaces the computations in linear layers with table 1In this paper, LUT in LUT-LLM and the architecture diagram refers to general lookup table, whereas elsewhere it denotes the LUT in FPGA fabric. LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs Conference17, July 2017, Washington, DC, USA where ğ‘Šğ‘„ğ‘– ,ğ‘Šğ¾ğ‘–/ğ‘” shared within groups. Concatenating {ğ‘‚ğ‘– }â„ applying linear projection produces the FFN input: ,ğ‘Šğ‘‰ğ‘–/ğ‘” Rğ· ğ‘‘ and ğ‘‘ = ğ·/â„, with keys and values ğ‘–=1 into ğ‘‚ Rğ¿ğ· and ğ‘‹ = (ğ‘“ (ğ‘‚ğ‘Š1) ğ‘‚ğ‘Š2)ğ‘Š3, (2) where ğ‘“ () is nonlinearity, denotes element-wise multiplication, and ğ‘Š1,ğ‘Š2,ğ‘Š3 are FFN parameters. Language model inference proceeds in two stages [8]: the prefill stage encodes the full input sequence to generate the first token, while the decode stage autoregressively produces subsequent tokens conditioned on prior outputs. 2.1.2 Prior LLM Accelerators. Previous works [9, 21, 23, 61] develop customized accelerators for model inference with arithmeticbased operations. Some accelerators, such as FlightLLM and DFX [23, 61], sequentially execute operations with various sparsification and quantization techniques. Allo [9] builds Pythonic framework to easily construct dataflow accelerator for LLMs, and InTAR [21] proposes spatial-temporal hybrid design for efficient on-chip management of LLMs."
        },
        {
            "title": "2.3 Memory-based Computation\n2.3.1 Benefits of Memory-based Computation. Memory-based com-\nputation replaces arithmetic by loading pre-computed outputs from\nmemory, making it well-suited for on-chip memory-rich platforms\nsuch as FPGAs. It reduces resource usage and latency since a\nmultiply-accumulate (MAC) operation â€” requiring two reads, one\nmultiplication, and one addition in arithmetic-based computations\nâ€” can be replaced by a single memory read. This also lowers energy:\na memory-based MAC consumes only 3.8 pJ in 7 nm, 2.4Ã— less than\nits arithmetic counterpart [25]. Moreover, memory-based LLMs\navoid online dequantization because values are pre-computed in\nfull precision. The pre-compute value can be further quantized\nper-tensor with low loss [28], which is used in our design.",
            "content": "Figure 2: Linear projection with weight vector quantization. Conventional quantization compresses scalars into low-bit representations via scaling and shifting [14, 30, 35, 56]. In contrast, vector quantization (VQ) [17] encodes groups of ğ‘£ elements as indices to representative centroids. As seen in Figure 2, in the preprocessing stage, the quantizer â€ partitions the matrix into length-ğ‘£ vectors, clusters them into ğ‘ğ‘¤ centroids forming codebook, and â replaces each vector with the index of its closest centroid. The resulting weight centroid index table stores indices at much lower bitwidth. For example, with ğ‘£ = 2, vector (1, 3) is encoded as index 0, mapped to centroid (2, 3). For language models, VQ is applied to weight matrices to reduce memory cost [32, 4648]. An ğ‘€ ğ· matrix is divided into groups of ğº vectors, each with its own codebook and index table (e.g., ğº = 4 in Figure 2). At inference, the device â‚ loads codebooks and indices, reconstructs the weights by centroid lookup, and âƒ multiplies the recovered matrix with activations. Compared to scalar quantization, VQ provides non-uniform compression [48], yielding higher flexibility and representational power, and thus superior accuracy at the same bitwidth. However, applying VQ to weights requires significant floating-point operations that are costly on FPGAs. Figure 3: Linear projection with activation vector quantization. Precomputed dot products between weight matrix and centroids in the codebook are stored in the lookup tables. 2.3.2 ML Accelerators with Memory-based Computations. For LLMs, weight quantization alone is insufficient, as lookup tables for dynamic activations are prohibitively large. Recent works [28, 44, 54] therefore apply VQ to activations in linear layers. As illustrated in Figure 3, the quantizer collects activation samples during training. To preserve accuracy, methods such as LUT-NN [44] â€ learn codebooks with ğ‘ğ‘ centroids, each through differentiable training and â pre-compute dot products between weight vectors and centroids, storing them as lookup tables. For example, weight vector (1, 3) multiplied by centroids (6, 2) and (4, 5) yields entries 12 and 19, producing ğ‘€ ğ·/ğ‘£ tables from an ğ‘€ ğ· weight matrix. At inference, the quantizer â‚ identifies the nearest centroid for each input vector, âƒ retrieves the corresponding table entry, and accumulates the results. For instance, input (6, 3) maps to centroid (6, 2) at index 0, retrieving 12. LUT-DLA [28] is recent work which targets an ASIC design. This work improves the learning and accelerator design for the framework activation VQ, which, while effective in CNNs, requires storing and loading large tables for LLMs, and, thus, limits the decode and short-context prefill performance (Section 3). In addition, its single-pipeline centroid search creates bottlenecks in the decode stage (Section 4). Several other works, including TLMAC [16] and NeuraLUT [5], consider using memory-based computations to replace MAC operations or simple neurons with FPGA soft logic. However, the limited Conference17, July 2017, Washington, DC, USA Zifan et al. resource makes them infeasible to scale to language models that compete with GPUs."
        },
        {
            "title": "Modeling",
            "content": "In this section, we illustrate the performance model for language model inference with memory-based computations to guide our hardware design in Section 4."
        },
        {
            "title": "Layer",
            "content": "We start with modeling the performance of the linear layer, which is affected by the vector quantization, and then extend this to the complete language model. Table 1 illustrates the symbol utilized in the performance model for the FPGA resource assigned to the given linear layer and the quantization configurations. Here, we mainly show the key results for different quantization strategies, and leave the detailed derivation in Section 7. Table 1: Symbols used in performance modeling Symbol Meaning Symbol Meaning ğ‘ğ‘ ğ‘ğ‘ ğ‘ğ‘ ğ‘‚ğ‘ ğ‘“ ğ‘32 ğ‘‚ğ‘ğ‘–ğ‘›ğ‘¡ 8 ğ¶ Used Hardware Resource Quantization Configurations On-chip memory ports Bit-width per access Compute units FP32 MACs per cycle per compute unit INT8 MACs per cycle per compute unit ğ‘€ ğ· Weight matrix dimension Off-chip bandwidth (bytes/cycle) Vectors per quantization group Vector Length Centroids per weight codebook Centroids per activation codebook Sequence length ğ¿ ğº ğ‘£ ğ‘ğ‘¤ ğ‘ğ‘ For weight vector quantization, the latency of loading codebook and weight centroid indices is ğ‘‡ğ‘šğ‘’ğ‘š = (4ğ‘€ğ·ğ‘ğ‘¤/ğºğ‘£ + ğ‘€ğ· log(ğ‘ğ‘¤)/8ğ‘£)/ğ¶ and the latency of on-chip computation is ğ‘€ğ· (log(ğ‘ğ‘¤)/ğ‘£ + 32/ğºğ‘£) ğ‘ğ‘ğ‘ğ‘ ğ‘‡ğ‘™ğ‘ğ‘¡ = + ğ‘€ğ·ğ¿ min(ğ‘ğ‘ğ‘‚ğ‘ ğ‘“ ğ‘32, ğ‘ğ‘ğ‘ğ‘ /32) (3) (4) When overlapping off-chip memory access with on-chip computations by double buffering, the overall latency is max(ğ‘‡ğ‘šğ‘’ğ‘š,ğ‘‡ğ‘™ğ‘ğ‘¡ ). For example, with ğ‘€ = 512, ğ· = 32, ğº = 256, ğ‘ğ‘¤ = 16, ğ‘£ = 2, ğ¿ = 1, ğ¶ = 64, and when we assign 16 ports of 32-bit wide memory and 256 FP32 compute units, ğ‘‡ğ‘šğ‘’ğ‘š = 66 and ğ‘‡ğ‘™ğ‘ğ‘¡ = 1090, resulting in 1090 cycles for overall latency. For activation vector quantization, the latency of loading codebook and lookup tables is ğ‘‡ğ‘šğ‘’ğ‘š = (ğ‘€ğ·ğ‘ğ‘/ğ‘£ + 4ğ·ğ‘ğ‘/ğ‘£)/ğ¶ (5) When having ğ‘† parallel centroid searches, the latency of table lookup per ğ‘† parallel centroid search and accumulation is: ğ‘‡ğ‘¡ğ‘™ = ğ‘†ğ‘€ğ¿/min(ğ‘†ğ‘€, ğ‘ğ‘ğ‘ğ‘ /8) ğ‘†ğ‘€ğ¿ min(ğ‘†ğ‘€, (ğ‘ğ‘ ğ‘†ğ‘ğ‘ğ‘£/ğ‘‚ğ‘ ğ‘“ ğ‘32)ğ‘‚ğ‘ğ‘–ğ‘›ğ‘¡ 8, ğ‘ğ‘ğ‘ğ‘ /8) + The total latency of on-chip computation is ğ‘‡ğ‘™ğ‘ğ‘¡ = min ğ‘† ğ· ğ‘† max(log(ğ‘ğ‘) + ğ¿ 1,ğ‘‡ğ‘¡ğ‘™ )) ( (6) (7) With the same setting as the previous example and ğ‘ğ‘ = 64, ğ‘‡ğ‘šğ‘’ğ‘š = 8256 and ğ‘‡ğ‘™ğ‘ğ‘¡ = 512, leading to 8256 cycles for overall latency. Figure 4: Linear projection with both activation and weight vector quantization with 2D lookup tables. Prior works [44, 48] apply either weight-only or activation-only vector quantization. We propose that these approaches can be extended to activation-weight co-quantization. As illustrated in Figure 4, the process involves â€ generating codebooks for weights and activations, â pre-computing dot products between them to form 2D lookup tables, â‚ finding the closest centroid for each activation vector, and âƒ accessing the entry based on the activation and weight centroid indices for accumulation. In the figure, vector (6, 3) is closer to (6, 2) at index 0. As the first weight centroid index is 0, we access the lookup table entry (0, 0) with value 18. With this scheme, the latency of off-chip memory access is ğ‘‡ğ‘šğ‘’ğ‘š = (ğ‘€ğ·ğ‘ğ‘ğ‘ğ‘¤/ğºğ‘£ + ğ‘€ğ· log(ğ‘ğ‘¤)/8ğ‘£ + 4ğ·ğ‘ğ‘/ğ‘£)/ğ¶ (8) and the on-chip computation latency is ğ‘‡ğ‘¡ğ‘™ = ğ‘†ğ‘€ğ¿/ğº min(ğ‘†ğ‘€/ğº, ğ‘ğ‘ğ‘ğ‘ /8) + ğ‘†ğ‘€ğ¿ min(ğ‘†ğ‘€, (ğ‘ğ‘ ğ‘†ğ‘ğ‘ğ‘£/ğ‘‚ğ‘ ğ‘“ ğ‘32)ğ‘‚ğ‘ğ‘–ğ‘›ğ‘¡ 8, ğ‘ğ‘ğ‘ğ‘ /8) (9) ğ‘‡ğ‘™ğ‘ğ‘¡ = min ğ· ğ‘† max(log(ğ‘ğ‘) + ğ¿ 1,ğ‘‡ğ‘¡ğ‘™ )) (10) The same setting as previous example resultsğ‘‡ğ‘šğ‘’ğ‘š = 569,ğ‘‡ğ‘™ğ‘ğ‘¡ = 288, and the overall latency is 569 cycles. ( ğ‘†"
        },
        {
            "title": "3.2 Roofline Analysis and Comparison\nWe extend the performance model of vector-quantized linear layers\nto a complete transformer architecture following [8], with addi-\ntional modeling for newly introduced computation blocks in mod-\nern language models (e.g., GQA, SwiGLU, etc.). Following prior\nworks [31, 51, 52, 54], attentions and non-linear operations require\nfloating point precision for accuracy. As an example, Figure 5 depicts\nthe roofline analysis of deploying the Qwen 3 1.7B model on the\nAMD V80 FPGA with a naive memory-based computation employ-\ning activation vector quantization like [28, 54] and arithmetic-based\ncomputation with FP16 quantization. The quantization configura-\ntion is: ğ‘ğ‘ = 64, ğ‘£ = 2, INT8 lookup tables, and FP32 codebooks.",
            "content": "Compared with the conventional roofline analyses of transformer acceleration [8, 55] with arithmetic-based computations, we observe two properties of the naive memory-based computation for vector quantized model. First, memory-based compute can have higher throughput than arithmetic-based compute throughput with longer sequence in the prefill stage, leading to at most LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs Conference17, July 2017, Washington, DC, USA Figure 5: Roofline model of Qwen 3 1.7B on AMD V80 FPGA with activation VQ for memory-based compute and arithmetic-based compute in FP16. Memory-based compute can attain higher throughputs for long sequence, but is highly memory bounded for decoding. 1.7 performance boost compared to arithmetic-based compute. This is because memory-based compute utilizes memory access to compute dot products, increasing the total available compute resources. The throughput converges to the compute bound as the sequence length increases since the computation is dominated by attention layers. Second, the memory-based compute for Qwen 3 model needs to load lookup tables and codebooks that are 16 larger than FP16 weights. This results in significantly lower operational intensity and throughput in decode and short-context prefill compared to arithmetic-based compute. Such behavior is unfavorable for tasks involving short inputs or long outputs (e.g., short question-answering [43] and creative writing [40]). Thus, we investigate other quantization schemes for more efficient alternative. Figure 6 compares the normalized throughput of various vector quantization methods with conventional scalar quantization on FPGA accelerators [9, 21, 23], using ğº = 512 and ğ‘ğ‘¤ = 16 for weight quantization. Weight-only quantization boosts decoding throughput by reducing off-chip memory access, but its prefill performance remains constrained by costly floating-point operations, and decoding is further limited by restricted on-chip memory ports. In contrast, activation-weight co-quantization realizes the highest throughput across both stages with various input lengths. This improvement arises from three factors: (1) multiple weight vectors map to the same centroid vector, reducing the total number of entries required in the lookup tables and easing the off-chip memory bandwidth demand; (2) co-quantization lowers memory port requirements compared to weight-only or activation-only quantization to sustain high parallelism; and (3) INT8 table lookup and accumulation reduces on-chip memory pressure and eliminates frequent floating-point operations inherent in the dequantization of W4A8 scheme."
        },
        {
            "title": "4 LUT-LLM Architecture\n4.1 Architecture Overview\nBased on the performance analysis, we design the LUT-LLM accel-\nerator with activation-weight co-quantization applied. Since not\nall operations can be executed through table lookup and the lan-\nguage model involves mixed precisions and computation orders,",
            "content": "Figure 6: Normalized throughputs of the prefill and decode stage vs. sequence length for Qwen 3 1.7B on AMD V80 FPGA with various quantization schemes. Activation-weight VQ can achieve superior performance than other schemes in both prefill and decode. we carefully orchestrate the execution of operations and allocate on-chip resources to ensure its feasibility on the target FPGA. LUTLLM integrates LUTLinear engine with global on-chip buffer, dataflow attention engine, and two special function units (SFUs) for non-linear operations (SwiGLU and LayerNorm). The LUTLinear engine performs all linear projections and routes the outputs. The dataflow attention engine employs two FP32 GEMM/GEMV engines to compute ğ‘„ğ¾ğ‘‡ and softmax(ğ‘„ğ¾ğ‘‡ )ğ‘‰ in pipelined manner, with rotational embedding and softmax applied in between. It supports both GEMM and GEMV to sustain high throughput in both the prefill and decode stages within single design. The SFUs process the output embeddings from the LUTLinear engine according to the computations defined in the target model. In contrast to prior accelerators that need seperate designs for the prefill and decode stages, LUT-LLM is unified design for both stages with overlapped KV-cache communication, computation, and data loading. The LUTLinear engine (Figure 8(b)) consists of two primary components: bandwidth-aware parallel centroid search units (BPCSU) and 2D lookup table prefix-sum engines (2D LUT PSum Engine). BPCSU performs nearest-neighbor search to identify the closest centroid for the given input vectors, while the 2D LUT PSum engine conducts table lookup and accumulation to realize matrix multiplication. Each BPCSU is paired with 2D LUT PSum engine, which receives the indices of the closest centroids. The 2D LUT PSum engines are connected in serial chain to cascade and accumulate partial sums. Multiple BPCSUs and 2D LUT PSum engines enable parallel processing with multiple codebooks along the hidden dimension. The accumulated results are stored in the accumulator along with the output buffer. Before distributing outputs to other modules, the dequantizer converts each value in the output buffer to FP32 with the specified per-tensor scaling and shifting factors."
        },
        {
            "title": "4.2 Bandwidth-aware Parallel Centroid Search\nAs shown in Figure 4, during inference, the first step is finding\nthe closest centroids for the incoming input vectors. For hardware\nefficiency, the distance metric that is often used is the Chebyshev\ndistance [7], which calculates the distance between two vectors (cid:174)ğ‘",
            "content": "Conference17, July 2017, Washington, DC, USA Zifan et al. ğ‘€ as the output size, ğº as the weight quantization group size, ğ‘ğ‘¤, ğ‘ğ‘ are the weight and activation codebook sizes, and ğ¶ is the memory bandwidth per BPCSU (bits/cycle), the length of each dPE pipeline chain ğ‘™ is arg max ğ‘™ (( 8ğ‘ğ‘ğ‘ğ‘¤ğ‘€ ğº + log(ğ‘ğ‘¤)ğ‘€)/ğ¶ 32ğ‘ğ‘/ğ¶ + ğ‘™ + log(ğ‘ğ‘/ğ‘™)) (12) where lookup tables are in INT8 and centroids are in FP32. log(ğ‘ğ‘/ğ‘™) is the depth of the reduction tree. We evaluate the maximum ğ‘™ that satisfies this condition for every linear projection and select the minimum over them. For example, for configurations of Qwen 3 1.7B, ğ‘™ = 16 for ğ‘ğ‘ = 64 and ğ‘ğ‘¤ = 16, indicating the instantiation of 16 4 array of dPEs with 2-level reduction tree for each BPCSU."
        },
        {
            "title": "4.3 Efficient 2D Table lookup Based Prefix-Sum\nAfter retrieving the closest centroid index, LUT-LLM will perform\ntable lookups and accumulations to compute matrix multiplications.\nNaive memory-based computation accesses 1D lookup tables for\nevery element in the output dimension. With a limited number\nof memory ports, e.g., 3 parallel access in Figure 9(a), the partial\nsum takes multiple cycles. To execute table lookup efficiently, we\nconstruct 2D lookup tables with a dedicated engine. Illustrated\nin Figure 9(b), given the incoming input centroid index received\nfrom BPCSUs, the 2D LUT PSum engine first accesses the row of\n2D lookup tables with the corresponding index, and extracts to a\nLUT row register. Meanwhile, the centroid indices of the weight\nvectors are prefetched and stored in a register of length-ğº, where\nğº is the quantization group size. Then, the dot product result of\neach weight vector is retrieved and expanded by looking up the\nLUT row register using the weight centroid index. In the example,\nthe engine will copy the value at position 1 of the LUT row register\nto position 0 in the output register for the first index in the weight\ncentroid index register. Both the extraction of LUT row registers\nand data copying for expanded output registers finish in a single\ncycle with a memory partitioning and where each step in 2D table\nlookup is pipelined.",
            "content": "Figure 8(c) illustrates the hardware architecture of the 2D LUT PSum engine. The 2D LUT buffer, which stores collections of lookup tables for each codebook, is instantiated utilzing mixture of BRAM, URAM, and LUTRAM to balance memory resources on the target FPGA. Once the LUT row registers are extracted, their contents are forwarded to value copy multiplexers, which select entries according to the weight centroid index registers. To alleviate excessive fanout from the LUT row registers to the multiplexers, the registers are duplicated such that each copy drives only subset of multiplexers. The expanded outputs are then accumulated with the results cascaded from the preceding 2D LUT PSum engine through SIMD adder. The SIMD adders are realized with combination of DSPs configured in SIMD mode or LUT-based adders to balance the computational resources on the FPGA."
        },
        {
            "title": "4.4 Spatial-Temporal Hybrid Design\nTo integrate the LUTLinear engine into a complete language model\naccelerator with high efficiency, it is essential to carefully orches-\ntrate inter-operation communication. Conventional language model",
            "content": "Figure 7: The overall architecture of LUT-LLM, including LUTLinear engine with global buffer, dataflow attention engine, and special functions (SwiGLU, LayerNorm) with pipelined operations. and (cid:174)ğ‘ by ğ¿ = max ğ‘– (ğ‘ğ‘– ğ‘ğ‘– ) (11) where ğ‘ğ‘– and ğ‘ğ‘– are elements in (cid:174)ğ‘ and (cid:174)ğ‘. To facilitate distance computation and comparison, prior works [28, 54] instantiate distance PEs (dPEs) for every centroid in the codebook. Since codebooks are varied across hidden dimensions and layers, every dPE needs to load new centroids. dPEs are connected in single-chain of pipeline, where the last dPE in the chain produces the index of the closest centroid. Although having low overhead when long sequence of input vectors pipelined, this design incurs high setup latency if the sequence is short, e.g., in the decode stage. This latency is often higher than the off-chip memory access latency of lookup tables for LLMs. One potential alternative is to launch all dPEs to compute distance with all centroids in parallel, and apply binary reduction tree to find the closest centroid. However, this significantly increases fanout and introduces ğ‘ğ‘ 1 comparators, which escalate routing difficulty, negatively impact timing, and spend more power on driving excessive resources. In LUT-LLM, we propose the bandwidth-aware parallel centroid search unit (BPCSU), where we design the structure of data communication among dPEs to match the bandwidth of data loading in the 2D LUT PSum engine. We found that complete reduction tree is often an overkill: we can support longer pipeline depth while the latency can still completely overlap with the off-chip memory access time. As seen in Figure 8(a), BPCSU arranges dPEs into multiple pipeline chains. The input vector is broadcasted to all chains for parallel distance computations. Each pipeline chain produces the minimum distance and index locally, which will be collected and compared through small reduction tree. Consequently, BPCSUs can have lower pipeline setup latency than data loading, with fewer resources instantiated for comparators than complete binary reduction trees. To maximize performance by overlapping centroid search and data loading, we have the following formulation: given LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs Conference17, July 2017, Washington, DC, USA Figure 8: The LUTLinear Engine (b) performs all linear projections with bandwidth-aware parallel centroid search units (BPCSU) and 2D lookup table prefix-sum (2D LUT PSum) engines, which process multiple input vectors on the hidden dimension. The accumulator and dequantizer is used to aggregate and convert results to FP32. (a) BPCSU uses distance PEs (dPE) and reduction tree to the find nearest centroids, with parallelism determined by the allocated memory bandwidth. (c) The 2D LUT PSum engine retrieves centroid indices, gathers rows from the lookup tables, expands them with multiplexers, and performs SIMD addition before cascading to the next engine. Figure 9: (a) Partial sum with 1D lookup tables requires more memory capacity to store the tables and longer latency to access data for accumulation. (b) Partial sum in 2D LUT PSum engine needs smaller memory and fewer memory ports to have high parallelism in accumulation. accelerators typically adopt either pure dataflow [9, 59] or sequential [23, 61] execution strategies. While both exhibit strong performance for arithmetic-intensive operations, they are suboptimal when the LUTLinear engine is introduced. For example, in linear projection followed by attention computations, pure dataflow execution (Figure 10(a)) instantiates separate hardware modules for each operation and streams partial outputs through FIFOs. In the case of LUTLinear, however, generating partial outputs for streaming requires iterating over input vectors along the hidden dimension. This introduces two inefficiencies: (1) the same codebook must be reloaded multiple times, and (2) the centroid search cannot be pipelined, as the codebooks vary across the hidden dimension. These issues result in significant stall latency in the linear layer. On the other hand, applying purely sequential execution strategy (Figure 10(b)) requires the accelerator to allocate large buffers with high memory partitioning to compute and store attention scores. Consequently, the available memory capacity and port bandwidth for table lookups in LUTLinear are reduced. Since decoding and Figure 10: Compared to pure (a) dataflow or (b) sequential executions, (c) spatial-temporal hybrid execution can perfectly pipeline nearest-neighbor search while saving memory resources for parallel table lookup. short-sequence prefilling are primarily constrained by memory access and the computation of linear layers, these architectural compromises significantly degrade overall performance. Inspired by prior accelerators that adopt hybrid execution patterns [21, 63], we design LUT-LLM to employ spatial-temporal hybrid execution strategy (Figure 10(c)). In this design, the LUTLinear engine executes sequentially across different operations, while its outputs are delivered to subsequent modules, i.e., attention, SwiGLU, and LayerNorm, via dataflow execution. This approach enables the LUTLinear engine to iterate over input vectors along the sequence dimension, thereby sustaining perfectly pipelined nearest-neighbor search without codebook reloads. At the same time, the memory requirements for attention remain the same as pure dataflow execution, saving 14% on-chip resources for high-throughput table lookups in LUTLinear compared to pure sequential execution. This hybrid design balances between compute Conference17, July 2017, Washington, DC, USA Zifan et al. efficiency and memory utilization, establishing scalable design principle for integrating memory-centric computation in LUT-LLM."
        },
        {
            "title": "5 Experiment\n5.1 Experiment Setup\nModels, Datasets, and Algorithm. Our overall scheme (following\nthe notation in Figure 1) is: ğº = 512, ğ‘£ = 2, ğ‘ğ‘¤ = 16, ğ‘ğ‘ = 64, and\nINT8 lookup tables. This scheme is equivalent to W2A3. Based on\nQwen 3 1.7B model [57], we first finetune for activation quantiza-\ntion. We applied per-tensor zero-point quantization that converts\nFP32 lookup tables to unsigned INT8. This quantizes the FP32 ma-\ntrix ğ‘‹ into quantized matrix ğ‘‹ğ¼ ğ‘ğ‘‡ 8 by",
            "content": "ğ‘  = (max(ğ‘‹ ) min(ğ‘‹ ))/256, ğ‘§ = min(ğ‘‹ )/ğ‘  ğ‘‹ğ¼ ğ‘ğ‘‡ 8 = min(256, max(0, ğ‘ ğ‘‹ + ğ‘§)) (13) We follow the two-stage training strategy for centroids and weights in LUT-DLA [28] with reconstruction loss ratio as 0.1. Additionally, we made algorithmic optimizations for the forward and backward passes. In the forward pass, we construct new lookup table gather reduce kernel, which cuts memory usage with runtime lookup table reconstructions and accelerates the training process by fusing table lookups and accumulations. In the backward pass, we fuse the kernels that compute the gradient of weights and centroids. These updates allow us to train models beyond 1B parameters. Quantization-aware training is supported with the straight-through estimator (STE) [60] with adjustable gradients. The training for activation vector quantization costs more GPU resources than conventional language model training since we need to tune all lookup tables in full precision and introduce extra computation burdens for quantization, dequantization, and reconstruction. In our setting, an eight-GPU cluster will take 2 weeks to first pretrain Qwen 3 1.7B on FineWeb [41] for 512-token sequences and then finetune on WikiQA [58] with 3 epochs, with 75% memory utilization. Scaling up to larger models and longer sequences requires more GPU memory and time, or further optimized kernels. After training for activation quantization, we reconstruct the weights from the trained lookup tables, then apply GPTVQ [48]. Then, we then pre-compute the new lookup tables by computing Table 2: Hardware Configuration of FPGA and GPUs AMD V80 AMD MI210 NVIDIA Frequency Max Bandwidth Peak Power Peak INT8 TOPS Peak BF16/FP16 TOPS Peak FP32 TOPS Process Node 227 MHz 819 GB/s 190 22.7 5.3 5.3 7 nm 1700 MHz 1.6 TB/s 300 181 181 45.3 6 nm 1065 MHz 2.0 TB/s 300 624 312 19.5 7 nm Table 3: Evaluation on GLUE and SQuAD v2 datasets for Qwen 3 1.7B with different quantization schemes. Config FP16 Baseline RTN INT8 SmoothQuant [56] W8A8 LUT-LLM: Act. Quant. + INT8 LUT + Weight Quant. GLUE MNLI MRPC QNLI QQP 91.18 87.56 89.26 86.67 89.75 86.99 90.69 87.02 90.82 86.93 89.54 86.91 92.89 87.99 91.72 91.91 91.71 90.35 86.52 80.15 85.29 84.07 83.82 82.84 RTE 80.87 70.40 79.06 78.34 76.89 76.53 SST-2 93.69 87.39 91.17 91.17 90.71 90. QA STS-B SQuAD v2 87.66 77.27 85.93 85.56 84.87 85.07 72.83 62.00 71.26 70.29 69.82 69.73 the dot products between the vectors in activation codebooks and weight codebooks. For evaluation, we utilize the GLUE benchmark [50], SQuAD v2 [43], and MMLU-Pro [53] for question-answering dataset. Benchmarks evaluate performance based on accuracy and F1 score. We compare with SmoothQuant [56] and GPTQ [14] as the representatives of scalar quantization schemes. Metrics. To compare the accelerator performance between LUTLLM, GPUs, and other accelerators, we employ latency in miliseconds and energy efficiency in tokens per Joule as the metrics. We measure both end-to-end latency and decoding latency for better understanding of scaling to longer sequence. Energy efficiency measures the number of tokens generated per energy cost. Target Devices and Tools. LUT-LLM is prototyped on AMD Alveo V80 FPGA [4]. To design the circuit, we use Xilinx Vitis HLS 2024.2 with TAPA framework [18] and utilize RapidStream [19, 20] for coarse-grain floorplanning to reduce the routing congestion and improve timing with register insertions. The design is implemented with Vivado 2024.2 with our customized block design to integrate the HLS-based RTL block and generate the bitstream. GPU Benchmarking. We select two GPUs with similar process as V80 FPGA: AMD Instinct MI210 [3] and NVIDIA A100 [37]. They have different hardware configurations and compiler optimizations, exhibiting various behaviors in language model inference performance. Table 2 compares V80 with the GPUs. The peak TOPs for V80 is scaled based on the achieved frequency. We use vLLM [26] as the serving backend for benchmarking. We enable FlashAttention [12] and FlashDecoding [22] to maximize the GPU performance with KV cache optimizations. Besides the model in the original precision, we also deploy the INT8 model quantized with GPTQ [14], which is the lowest precision available for Qwen 3. A100 has GPTQ Marlin kernel [15] integrated for efficient INT8 inference. MI210 does not support this optimization. We write python script using pyNVML [38] and pyrsmi to monitor the GPU power consumption. SoTA FPGA Accelerator Baselines. We select two representative SoTA works, Allo [9], FlightLLM [61], and InTAR [21]. They are LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs Conference17, July 2017, Washington, DC, USA Figure 11: End-to-end and decode latency of LUT-LLM and GPUs with BF16 and INT8 precisions. Context window is within 512 tokens due to the limitation of training algorithm and system for longer context. Figure 12: Energy efficiency (tokens per Joule) of LUT-LLM and GPUs with BF16 and INT8 precision. Table 4: Evaluation on MMLU-Pro benchmark for Qwen 3 1.7B with different quantization schemes. Model FP16 Baseline W8A16 GPTQ W8A8 SmoothQuant LUT-LLM MMLU-Pro (Avg) 33.1 32.1 31.7 30. arithmetic-based and implemented on the Alveo U280 [2]. Allo and InTAR applies W4A8 scheme and FlightLLM utilizes 3.5-bit sparse weights. FlightLLM employs aggressive sparsification and quantization on both weights and attention. This fundamentally alters the computation pattern and data loading size. Since the mapping flow and quantization scripts are not open-sourced, we use the performance model in FlightLLMs artifact to estimate the latency. Since these accelerators are implemented on U280 and do not natively support the latest Qwen 3 1.7B model, we construct simulator based on their designs, with less than 2% gap from data in the original papers."
        },
        {
            "title": "5.2 Algorithm Performance\nWith the quantization scheme of LUT-LLM, the target model main-\ntains a competitive performance compared with the FP16 base-\nline and consistently outperforms standard round-to-nearest (RTN)\nquantization to INT8 for activations. Table 3 summarizes the model\nquality on GLUE and SQuAD v2 under different quantization schemes.\nLUT-LLM incurs only a 2.7% performance drop relative to the FP16",
            "content": "Figure 13: LUT-LLM implementation layout on AMD V80 FPGA, resource utilization, and clock frequency. baseline with all techniques applied. Compared with RTN INT8 quantization, LUT-LLM with INT8 lookup tables achieves 5.5% higher performance in geomean. Compare with SmoothQuant, LUTLLM has only 1.2% drop in geomean accuracy while applying more aggressive quantization scheme (LUT-LLM is equivalent to W2A3). We also benchmarked LUT-LLM on harder benchmarks, such as MMLU-Pro, achieving only 0.9% accuracy drop than SmoothQuant. These results highlight the effectiveness of the quantization scheme and tuning methods used in LUT-LLM in preserving model quality while enabling aggressive quantization for efficient deployment on FPGAs."
        },
        {
            "title": "5.3 Accelerator Performance\n5.3.1 Resource Utilization. Figure 13 presents the floorplan of LUT-\nLLM on the AMD V80 FPGA. Since LUT-LLM primarily targets",
            "content": "Conference17, July 2017, Washington, DC, USA Zifan et al. Table 5: Comparison of energy efficiency across accelerators. Accelerator Geomean Energy Efficiency (tokens/J) FlightLLM InTAR Allo LUT-LLM 1.54 1.67 0.69 2.29 improving decoding and short-context prefilling efficiency, both of which are bounded by linear layers, substantial portion of resources is allocated to the LUTLinear engine. The design attains clock frequency of 227 MHz. Although the utilization of all resource types remains at or below 50%, the CLB utilization reaches 81%. This imbalance, caused by resource heterogeneity, imposes significant challenges in placement and routing. LUT-LLM employs 32 HBM channels for reading lookup tables and weight indices, along with 4 channels for input data and 4 channels for outputs and KV cache. Each channel on the V80 provides 256-bit interface. The HBM clock is aligned with the kernel frequency to sustain balanced read and write throughput. Consequently, the effective bandwidth utilized for loading lookup tables is 227 256 32/8/1024 = 227 GB/s. 5.3.2 Comparison with GPUs. We compare the end-to-end latency of LUT-LLM with GPUs running Qwen 3 1.7B in both BF16 and INT8 precisions. Figure 11 shows the latency breakdown of the prefill and decode stages for each configuration. Due to training memory constraints on GPUs, the vector-quantized model can currently process up to 512 tokens, and we therefore report results within this context window. With INT8 quantized lookup tables, LUT-LLM has 1.66 lower geomean latency than MI210, despite utilizing 7 less memory bandwidth. For the A100, the FlashAttention and FlashDecoding algorithms deliver substantial speedups in both prefill and decode stages, and the GPTQ Marlin kernel further accelerates INT8 inference on tensor cores. As result, LUT-LLM is slower. However, considering that LUT-LLM consumes 8.8 less bandwidth than the A100 and decoding is fundamentally memorybound, this modest latency gap is still notable. detailed latency analysis reveals that LUT-LLM decoding saturates 92% of the available bandwidth, approaching the hardware limit. Furthermore, we show the performance of LUT-LLM with INT4 lookup tables without altering computational resources. In this case, the latency aligns with the A100 in BF16 and INT8 as the output length increases. We then compare the energy efficiency, as shown in Figure 12. While all GPUs benefit from INT8 quantization due to reduced off-chip memory traffic, LUT-LLM maintains substantial advantage, achieving 4.1 higher geomean energy efficiency than MI210. On the A100, the GPTQ Marlin kernel not only reduces off-chip memory access but enhances INT8 computation efficiency, narrowing the gap relative to LUT-LLM. Nevertheless, LUT-LLM remains 1.72 more energy efficient in geomean. Moreover, the efficiency gap between LUT-LLM and GPUs widens as output length increases, underscoring the scalability of LUT-LLM for long output sequence. 5.3.3 Comparison with SoTA FPGA Accelerators. Figure 14 compares the end-to-end latency of LUT-LLM with InTAR, Allo, and FlightLLM. Overall, LUT-LLM is 5.6 and 1.9 faster than Allo and InTAR, respectively. The performance gain arises from improvements in both the prefill and decode stages. In the prefill Figure 14: Inference latency comparison of LUT-LLM, InTAR, Allo, and FlightLLM. stage, LUT-LLM leverages memory-based table lookups as computation, enabling higher throughput than Allo and InTAR, which are arithmetic-based accelerators for quantized models. As the input length increases, the latency gap between LUT-LLM and InTAR during the prefill stage narrows, driven by the costly FP32 attention computations and the reduced resource allocation for attention needed to sustain high throughput in the linear layers. In the decode stage, LUT-LLM performs computations more efficiently, fully utilizing the available memory bandwidth. Moreover, Allo and InTAR incur additional overhead from dequantization, which requires extra data loading and costly floating-point computations. Consequently, even when weights are loaded in INT4 precision, the decoding speed of Allo and InTAR is slower than that of LUT-LLM, which streams INT8 data from HBM. Compared with FlightLLM, LUT-LLM has similar end-to-end latency and 1.1 faster decoding speed, even with more data loaded from HBM. This indicates better scaling of LUT-LLM with respect to the output length. Table 5 illustrates the energy efficiency of each FPGA accelerators, where the energy profile of FlightLLM, InTAR, and Allo are from the U280 FPGA. LUT-LLM is 1.49, 1.37, and 3.32 more energy efficient than FlightLLM, InTAR, and Allo, respectively, demonstrating promising energy saving of memory-based computes on FPGA-based LLM acceleration. 5.3.4 Discussion: Model Scaling and Adaptation. Although the current training algorithm limits the model size, LUT-LLM can deploy larger models with the same quantization scheme with simple changes of loop bounds in our design. Overall, LUT-LLM is 2.16 more energy efficient than the A100. LUT-LLM is also parametrized for different quantization schemes and model architecture for fast accelerator generation, but the optimization requires manual design. We leave the automation as the future work."
        },
        {
            "title": "6 Conclusion and Outlook\nThis work presents LUT-LLM, the first FPGA accelerator for LLM\ninference based on memory-driven computation. We develop a\nperformance model for vector-quantized language models on FP-\nGAs and demonstrate that co-quantizing activations and weights\nis essential to surpass conventional scalar quantization, realized\nthrough efficient 2D table lookups. LUT-LLM achieves 1.72Ã—â€“4.1Ã—",
            "content": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs Conference17, July 2017, Washington, DC, USA higher energy efficiency than GPUs with state-of-the-art optimization algorithms and low-precision tensor cores. Future work will further enhance efficiency through emerging algorithmic advances, such as low-bit attention [33] and low-bit centroid search. In addition, scaling memory bandwidth to GPU levels can deliver up to 3.19 speedup over the A100, suggesting promising directions in multi-FPGA systems and next-generation HBM technology on FPGAs for LUT-LLM."
        },
        {
            "title": "7 Appendix: Performance Derivation\nWeight Quantization. Consider ğº-vectors quantization group\nwith length-ğ‘£ vectors and ğ‘ğ‘¤ centroids per codebook. An ğ‘€ Ã— ğ·\nweight matrix is quantized to 4ğ‘€ğ·ğ‘ğ‘¤/ğºğ‘£ bytes codebook in FP32\nand ğ‘€ğ· log(ğ‘ğ‘¤)/8ğ‘£ bytes weight centroid indices, which derives\nthe latency of loading weights from off-chip as equation 3. Then,\nthe accelerator will access the FP32 values in the codebooks based\non the weight indices and copy them to the registers. To expand ğ‘†\nvalues, we need to access ğ‘†/ğ‘£ indices and ğ‘†/ğºğ‘£ codebooks, which\nrequires ğ‘† log(ğ‘)/ğ‘£ + 32ğ‘†/ğºğ‘£ bits access per cycle. Consequently,\nwe can expand ğ‘† = ğ‘ğ‘ğ‘ğ‘ /(log(ğ‘ğ‘¤)/ğ‘£ + 32/ğºğ‘£) elements per cycle.\nFinally, the accelerator performs FP32 MAC operations and accumu-\nlates results to output buffer. The maximum number of operations\nper cycle is determined by both the compute units and memory\nports available, which is min(ğ‘ğ‘ğ‘‚ğ‘ ğ‘“ ğ‘32, ğ‘ğ‘ğ‘ğ‘ /32). This concludes\nthe latency of on-chip computation as shown in Equation 4.\nActivation Quantization. Consider length-ğ‘£ vectors with ğ‘ğ‘ cen-\ntroids per codebook. Since every ğ‘£ channels in the hidden dimension\nand each output dimension require a separate lookup table, an ğ‘€ Ã—ğ·\nweight matrix is converted into ğ‘€ğ·/ğ‘£ tables, each containing ğ‘ğ‘\nINT8 entries. The activation codebooks consist of ğ·ğ‘ğ‘/ğ‘£ vectors in\nFP32. Hence, the off-chip memory latency is given in equation 5.\nComputation proceeds in two pipelined steps. First, the accelerator\nfinds the closest centroid in the codebook for each input vector, re-\nquiring at least log(ğ‘) cycles via binary reduction. For ğ¿ tokens, this\ncan be pipelined in log(ğ‘) + ğ¿ âˆ’ 1 cycles. The resulting indices are\nstreamed for table lookup, where each index drives up to ğ‘€ outputs\nbut is limited by the available memory ports. Thus, table lookups\nfor ğ¿ inputs take ğ‘€ğ¿/min(ğ‘€, ğ‘ğ‘ğ‘ğ‘ /8) cycles. In parallel, the ex-\ntracted values are accumulated in the output buffer, constrained by\ncompute and memory resources. With ğ‘† parallel centroid searches\nacross ğ·, at least ğ‘†ğ‘ğ‘ğ‘£/ğ‘‚ğ‘ ğ‘“ ğ‘32 units are needed, since each dis-\ntance computation costs ğ‘£/ğ‘‚ğ‘ ğ‘“ ğ‘32 and must be performed for all\ncentroids. The remaining units perform (ğ‘ğ‘ âˆ’ ğ‘†ğ‘ğ‘ğ‘£/ğ‘‚ğ‘ ğ‘“ ğ‘32)ğ‘‚ğ‘ğ‘–ğ‘›ğ‘¡ 8\nINT8 accumulations in parallel. Consequently, the latency of the\ntable lookup per ğ‘†-way search is as in equation 6, and the total\non-chip computation latency follows equation 7.\nActivation-Weight Co-quantization. Consider the settings the\nsame as the weight and activation quantization. The accelerator\nneeds to load ğ‘€ğ·/ğºğ‘£ tables with ğ‘ğ‘¤ğ‘ğ‘ entries each, ğ‘€ğ· log(ğ‘ğ‘¤)/8ğ‘£\nbytes of weight indices, and 4ğ·ğ‘ğ‘/ğ‘£ bytes of activation codebooks.\nWe can then derive equation 8 and 10 by following the same analysis\nas weight and activation quantization.",
            "content": "References [1] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico LebrÃ³n, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245 (2023). [2] AMD Adaptive Computing. 2023. AMD Alveo U280 Data Center Accelerator Card Data Sheet (DS963, v1.7). https://docs.amd.com/r/en-US/ds963-u280. Accessed September 2, 2025. [3] AMD Adaptive Computing. 2023. AMD Instinct MI210 Accelerator Product Brief. https://www.amd.com/content/dam/amd/en/documents/instinct-businessdocs/product-briefs/instinct-mi210-brochure.pdf. Accessed September 2, 2025. [4] AMD Adaptive Computing. 2024. AMD Alveo V80 Data Center Accelerator Cards Data Sheet (DS1013, v1.0). https://docs.amd.com/r/en-US/ds1013-v80. Accessed September 2, 2025. [5] Marta Andronic and George Constantinides. 2025. NeuraLUT-Assemble: Hardware-aware Assembling of Sub-Neural Networks for Efficient LUT Inference. In 2025 IEEE 33rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEE, 208216. [6] Ali Ayub, Chrystopher Nehaniv, and Kerstin Dautenhahn. 2024. Interactive continual learning architecture for long-term personalization of home service robots. In 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 1128911296. [7] Cyrus Cantrell. 2000. Modern mathematical methods for physicists and engineers. Cambridge University Press. [8] Hongzheng Chen, Jiahao Zhang, Yixiao Du, Shaojie Xiang, Zichao Yue, Niansong Zhang, Yaohui Cai, and Zhiru Zhang. 2024. Understanding the potential of fpgabased spatial acceleration for large language model inference. ACM Transactions on Reconfigurable Technology and Systems 18, 1 (2024), 129. [9] Hongzheng Chen, Niansong Zhang, Shaojie Xiang, Zhichen Zeng, Mengjia Dai, and Zhiru Zhang. 2024. Allo: programming model for composable accelerator design. Proceedings of the ACM on Programming Languages 8, PLDI (2024), 593 620. [10] Yi Cheng, Wenge Liu, Kaishuai Xu, Wenjun Hou, Yi Ouyang, Chak Tou Leong, Wenjie Li, Xian Wu, and Yefeng Zheng. 2024. Autopal: Autonomous adaptation to users for personal ai companionship. arXiv preprint arXiv:2406.13960 (2024). [11] Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. 2024. complete survey on llm-based ai chatbots. arXiv preprint arXiv:2406.16937 (2024). [12] Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 (2023). [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv e-prints (2024), arXiv2407. [14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022). [15] Elias Frantar, Roberto Castro, Jiale Chen, Torsten Hoefler, and Dan Alistarh. 2025. Marlin: Mixed-precision auto-regressive parallel inference on large language models. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. 239251. [16] Daniel Gerlinghoff, Benjamin Chen Ming Choong, Rick Siow Mong Goh, WengFai Wong, and Tao Luo. 2024. Table-Lookup MAC: Scalable Processing of Quantised Neural Networks in FPGA Soft Logic. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays. 235245. [17] Robert Gray. 1984. Vector quantization. IEEE Assp Magazine 1, 2 (1984), 429. [18] Licheng Guo, Yuze Chi, Jason Lau, Linghao Song, Xingyu Tian, Moazin Khatti, Weikang Qiao, Jie Wang, Ecenur Ustun, Zhenman Fang, et al. 2023. Tapa: scalable task-parallel dataflow programming framework for modern fpgas with co-optimization of hls and physical design. ACM Transactions on Reconfigurable Technology and Systems 16, 4 (2023), 131. [19] Licheng Guo, Yuze Chi, Jie Wang, Jason Lau, Weikang Qiao, Ecenur Ustun, Zhiru Zhang, and Jason Cong. 2021. AutoBridge: Coupling coarse-grained floorplanning and pipelining for high-frequency HLS design on multi-die FPGAs. In The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. 8192. [20] Licheng Guo, Pongstorn Maidee, Yun Zhou, Chris Lavin, Jie Wang, Yuze Chi, Weikang Qiao, Alireza Kaviani, Zhiru Zhang, and Jason Cong. 2022. RapidStream: Parallel physical implementation of FPGA HLS designs. In Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. 112. [21] Zifan He, Anderson Truong, Yingqi Cao, and Jason Cong. 2025. InTAR: InterTask Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs. In 2025 IEEE 33rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEE, 123132. [22] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu Wang. 2023. Flashdecoding++: Faster large language model inference on gpus. arXiv preprint arXiv:2311.01282 (2023). [23] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae Lee, Minsub Kim, Dongsoo Lee, and Joo-Young Kim. 2022. Dfx: low-latency multi-fpga appliance for accelerating transformer-based text generation. In 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 616630. [24] Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, et al. 2025. Deep Research Agents: Systematic Examination And Roadmap. arXiv preprint arXiv:2506.18096 (2025). Conference17, July 2017, Washington, DC, USA Zifan et al. and lattice codebooks. arXiv preprint arXiv:2402.04396 (2024). [47] Albert Tseng, Qingyao Sun, David Hou, and Christopher De Sa. 2024. Qtip: Quantization with trellises and incoherence processing. Advances in Neural Information Processing Systems 37 (2024), 5959759620. [48] Mart Van Baalen, Andrey Kuzmin, Ivan Koryakovskiy, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, and Paul Whatmough. 2024. Gptvq: The blessing of dimensionality for llm quantization. arXiv preprint arXiv:2402.15319 (2024). [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [50] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 (2018). [51] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453 (2023). [52] Hongyu Wang, Shuming Ma, and Furu Wei. 2024. BitNet a4. 8: 4-bit Activations for 1-bit LLMs. arXiv preprint arXiv:2411.04965 (2024). [53] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems 37 (2024), 95266 95290. [54] Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang, and Mao Yang. 2025. T-mac: Cpu renaissance via table lookup for low-bit llm deployment on edge. In Proceedings of the Twentieth European Conference on Computer Systems. 278292. [55] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (2009), 6576. [56] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International conference on machine learning. PMLR, 3808738099. [57] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [58] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: challenge dataset for open-domain question answering. In Proceedings of the 2015 conference on empirical methods in natural language processing. 20132018. [59] Hanchen Ye and Deming Chen. 2025. StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs. arXiv preprint arXiv:2509.13694 (2025). [60] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. 2019. Understanding straight-through estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662 (2019). [61] Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang, Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, et al. 2024. Flightllm: Efficient large language model inference with complete mapping flow on fpgas. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays. 223234. [62] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. arXiv preprint arXiv:2401.07339 (2024). [63] Jinming Zhuang, Zhuoping Yang, Shixin Ji, Heng Huang, Alex Jones, Jingtong Hu, Yiyu Shi, and Peipei Zhou. 2024. Ssr: Spatial sequential hybrid architecture for latency throughput tradeoff in transformer acceleration. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays. 5566. [25] Norman Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, et al. 2021. Ten lessons from three generations shaped googles tpuv4i: Industrial product. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEE, 114. [26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Introduces the vLLM framework and the PagedAttention mechanism. [27] Eunhae Lee. 2024. Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory. arXiv preprint arXiv:2409.11192 (2024). [28] Guoyu Li, Shengyu Ye, Chunyun Chen, Yang Wang, Fan Yang, Ting Cao, Cheng Liu, Mohamed Sabry Aly, and Mao Yang. 2025. LUT-DLA: Lookup Table as Efficient Extreme Low-Bit Deep Learning Accelerator. In 2025 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 671684. [29] Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. 2024. Codetree: Agent-guided tree search for code generation with large language models. arXiv preprint arXiv:2411.04329 (2024). [30] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems 6 (2024), 87100. [31] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. 2024. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532 (2024). [32] Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, and Mao Yang. 2024. Vptq: Extreme low-bit vector post-training quantization for large language models. arXiv preprint arXiv:2409.17066 (2024). [33] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. 2024. Spinquant: Llm quantization with learned rotations. arXiv preprint arXiv:2405.16406 (2024). [34] Yukio Miyasaka, Alan Mishchenko, John Wawrzynek, and Nicholas Fraser. 2024. Synthesis of LUT Networks for Random-Looking Dense Functions with Dont CaresTowards Efficient FPGA Implementation of DNN. In 2024 IEEE 32nd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEE, 126132. [35] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training quantization. In International conference on machine learning. PMLR, 71977206. [36] NVIDIA Corporation. 2020. NVIDIA V100 Tesla Tensor Core GPU Data Sheet. https://images.nvidia.com/content/technologies/volta/pdf/volta-v100datasheet-update-us-1165301-r5.pdf. Accessed September 2, 2025. [37] NVIDIA Corporation. 2021. NVIDIA A100 80 GB PCIe GPU Data Sheet. https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/ pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf. Accessed September 2, 2025. [38] NVIDIA Corporation. 2025. nvidia-ml-py: Python bindings for NVIDIA Management Library (NVML). Provides Python interface to monitor and manage NVIDIA GPUs via NVML.. [39] Roland Oruche, Sai Keerthana Goruganthu, Rithika Akula, Xiyao Cheng, Ashraful Md Goni, Bruce Shibo, Kerk Kee, Marcos Zampieri, and Prasad Calyam. 2025. Survey on the Recent Advancements in Human-Centered Dialog Systems. Comput. Surveys 57, 10 (2025), 136. [40] Samuel Paech. 2025. EQ-Bench Creative Writing Benchmark v3. https://github. com/EQ-bench/creative-writing-bench. [41] Guilherme Penedo, Hynek KydlÃ­Äek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems 37 (2024), 3081130849. [42] Qualcomm Incorporated. 2025. Smart Home Technology Solutions Qualcomm. https://www.qualcomm.com/products/internet-of-things/consumer/ smart-homes. Accessed September 2, 2025. [43] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016). [44] Xiaohu Tang, Yang Wang, Ting Cao, Li Lyna Zhang, Qi Chen, Deng Cai, Yunxin Liu, and Mao Yang. 2023. Lut-nn: Empower efficient neural network inference with centroid learning and table lookup. In Proceedings of the 29th Annual International Conference on Mobile Computing and Networking. 115. [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [46] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. 2024. Quip#: Even better llm quantization with hadamard incoherence"
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "University of California, Los Angeles"
    ]
}