{
    "paper_title": "TRecViT: A Recurrent Video Transformer",
    "authors": [
        "Viorica Pătrăucean",
        "Xu Owen He",
        "Joseph Heyward",
        "Chuhan Zhang",
        "Mehdi S. M. Sajjadi",
        "George-Cristian Muraru",
        "Artem Zholus",
        "Mahdi Karami",
        "Ross Goroshin",
        "Yutian Chen",
        "Simon Osindero",
        "João Carreira",
        "Razvan Pascanu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a novel block for video modelling. It relies on a time-space-channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with a pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having $3\\times$ less parameters, $12\\times$ smaller memory footprint, and $5\\times$ lower FLOPs count. Code and checkpoints will be made available online at https://github.com/google-deepmind/trecvit."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 4 9 2 4 1 . 2 1 4 2 : r TRecViT: Recurrent Video Transformer Viorica Patraucean Xu Owen He Joseph Heyward Chuhan Zhang Mehdi S. M. Sajjadi Ross Goroshin George-Cristian Muraru"
        },
        {
            "title": "Simon Osindero",
            "content": "Joao Carreira"
        },
        {
            "title": "Razvan Pascanu",
            "content": "Google DeepMind Corresponding author: viorica@google.com, core contributor Figure 1. Left: TRecViT architecture. Each video frame is divided into non-overlapping patches that are linearly projected into token embedding space. We then add learnt spatial positional encoding. The tokens are passed through gated linear recurrent units (LRUs) that share parameters across space. The outputs of the recurrent blocks are then processed by ViT block. The recurrent operation followed by ViT is repeated times. Right: TRecViT block. The input is batch of videos, each frame with tokens. We apply recurrent units over temporal tubes to integrate information over time, and self-attention and MLP across tokens within each frame. Note that the recurrent units share parameters, but the information is not mixed across temporal tubes. Similarly, the ViT blocks share parameters, but the information is not mixed across frames."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We propose novel block for video modelling. It relies on timespacechannel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having 3 less parameters, 12 smaller memory footprint, and 5 lower FLOPs count. Code and checkpoints are available online.1 1https://github.com/google-deepmind/trecvit Video understanding requires low-level scene understanding (e.g. how objects move) and high-level reasoning (e.g. causal relations between events) over signal that is highdimensional, can be noisy, and contains high correlations and redundancies in both spatial and temporal dimensions. Efficient video modelling needs high-capacity models that can represent the sheer diversity and richness of real-world videos, while having reasonable compute and memory footprint both at training and during inference time. Convolutional neural networks [8, 14] have been successful family of models for video, but their scaling capabilities (in both data and parameters) are limited due to their inductive biases (locality, invariance). Recurrent neural networks, 1 e.g. [33, 38] have some desirable properties for video modelling (constant inference cost per timestep independent of the length of the video), but they are slow to train due to their sequential nature and have difficulties in learning over long complex sequences. Transformers [42] have emerged as very powerful family of models for all modalities, with impressive scaling capabilities. However, they have significant memory footprint and latency due to the quadratic complexity of the self-attention operation. Recently, new family of linear recurrent networks [4, 16, 17, 32], referred to as State Space Models (SSMs), has emerged as an answer to the quadratic complexity of self-attention and the slow training of RNNs, with promising results for vision and language [9, 27]. In this paper, we propose hybrid architecture that combines the best of all worlds. It alternates gated linear recurrent units (LRUs) [9] applied over time, with self-attention blocks over space, and MLP over feature channels. As opposed to space and channels, time has natural order (arrow-of-time) that LRUs can implicitly and efficiently model with O(N ) complexity in the number of input frames at training time and O(1) complexity at inference time, making it possible to process videos that extend even indefinitely. Space, on the other hand, has fixed limited dimension, for which the quadratic cost of self-attention is more accessible. From practical perspective, using selfattention over space allows us to naturally process in parallel all the pixels of given frame, without having to commit to particular scanning order [27], making better use of hardware when parallel resources are available. To further limit the self-attention cost, we use spatial patches as introduced in the successful ViT [12] model. But, compared to existing video transformer models, e.g. ViViT [1], the patches do not have fixed temporal extent. Instead, the embeddings of the spatial patches are integrated continuously into the hidden state of the gated LRUs, providing persistent memory of the entire temporal sequence up to the current frame. Furthermore, similar to convolutional networks, the parameters of the LRUs are shared over space, preventing the number of parameters from exploding as the resolution of the video increases. We refer to the resulting model as Temporal Recurrent Video Transformer (TRecViT). TRecViT is highly flexible and can address various video understanding tasks, both sparse (e.g. video classification) and dense (e.g. point tracking), trained in supervised or self-supervised manner, e.g. using masked auto-encoding. In all our experiments, we use causal setup that respects the arrow of time, so the model is suitable for any downstream applications, from e.g. video classification where we have offline access to the videos, to e.g. robotics, where online processing is required. Overall, our model is significantly more efficient in both memory footprint and FLOPs compared to vanilla transformers. Paper structure: We discuss related works in more depth in section 2 and we introduce the proposed model in section 3. We discuss training regimes and analyse efficiency when comparing to baselines in section 4. In section 5, we present extensive experiments for various training regimes, different tasks and datasets. We conclude in section 6 with discussion of the limitations of the proposed approach and directions for future work. 2. Related work Transformers for Video. Proposed initially as language models, transformers [42] have quickly become the dominant architecture across multiple modalities (images, audio, video). Transformer blocks alternate between spatial mixing block represented by self-attention and (feature) channel mixing block, represented by gated MLP. Given that the self-attention layer treats the input tokens as set, positional encodings must be used in order to specify the location of each token. It also means that no parsing order is needed, unlike the case with RNNs. Vision transformers (ViT) [12, 29] split images into fixed number of patches that are projected into an embedding space to obtain tokens and these are then processed by regular transformer. Several works extended ViT to video, e.g. by replacing the regular image patches with spatio-temporal ones. The main challenge with transformers, particularly for video, is the quadratic complexity in the number of input tokens. Multiple approaches have been proposed to address this: e.g. factorisations of the self-attention operation [1, 5], iterative attention [23], sparse sampling of the input frames [34], and distributed self-attention operations across different devices [28]. Our proposed model uses novel space-time factorisation, where the temporal dimension is handled by LRUs and the spatial dimension by self-attention. As these models scale successfully to large number of parameters, their data needs are efficiently met by using self-supervised pre-training like masked autoencoding (MAE) [40] or contrastive learning [45]. Due to the factorisation used in our architecture, using such pre-training strategies is straightforward and we include successful experiments with MAE pre-training in Section 5. SSM, type of Linear Recurrent Model. While transformers [42] can be efficiently parallelised during training, at inference they need to pay quadratic cost in the sequence length. On the other hand, recurrent networks [3, 13, 20, 30, 39] are compact and efficient at inference but slow at training. State Space Models (SSMs) [17, 18, 32], particular type of linear recurrent networks, have recently been proposed as an answer to the scalability problem of RNNs, and have shown strong performance in language and other long-range dependencies tasks [9, 16]. SSMs, like S4 [18], S4D [19], or Mamba [16] have been 2 introduced as particular discretizations of continuous time linear system. On the other hand, the linear recurrent unit (LRU) [32] was designed by identifying the minimal set of changes to vanilla RNN [13] that allows it to obtain the same key properties as the S4D architecture [18]; we discuss the LRU in more detail in Section 3. Improving on the LRU, the gated LRU [9] introduces gating mechanisms similar to LSTM or GRU architectures, to filter the input sequence, while the recurrent gate controls the rate of the Importantly, different from LSTM or information decay. GRU, these gates do not depend on the previous state, which would prevent parallelisation at training time. In our work, we use gated LRUs, but we expect similar results to be obtained when using other gated SSM blocks like Mamba within our factorisation. SSMs for Video. While SSMs have mostly been explored in language, several architectures like S4 and Mamba have also been adapted to image and video modalities [44]. ViS4mer [21] uses ViT image encoder to process videos frame by frame, and integrates their representations over time using S4 blocks at the top. TranS4mer [22] uses selfattention over short clips and integrates these with gated S4 blocks. More recently, the Mamba architecture was extended to images and videos by having it process flattened 1D sequence of image or video patches. This requires defining processing order for the patches, and different orders have been proposed, e.g. bidirectional and following column or row order [27, 46]. As opposed to these Mamba-based architectures, our factorisation naturally uses the arrow-of-time to decide the scanning order, resulting in causal model. Another important benefit of our hybrid architecture is that we can initialise the ViT blocks with strong existing pre-trained weights. This leads to strong performance even at larger scale, as opposed to VideoMamba [27] where the authors report severe overfitting issues, requiring distillation from smaller models when training in supervised fashion or distillation from CLIP features [37] for self-supervised training. 3. TRecViT Architecture The proposed architecture, TRecViT, is composed of repeated identical blocks, each performing sequence of information mixing steps across the different dimensions of the video signal: time, space, and channels; see Figure 1. The mixing over the time dimension is handled by gated linear recurrent units (LRUs), similar to the one introduced in [9] for language. Each spatial token is associated with an LRU that processes the tokens within the same temporal tube over time, without mixing the information across temporal tubes. The LRUs share parameters over space, similar to convolutional network. When applying this temporal mixing operation, the space dimension is transposed into the batch dimension. The mixing over spatial and channel dimensions is handled by standard ViT block, which first performs the spatial mixing through self-attention operation, then the channel mixing by using an MLP. When performing the spatial and channel mixing, the time dimension is transposed into the batch dimension. Empirically, we show that this factorization and choice of building blocks is more efficient for understanding temporal dynamics compared to video transformer approaches (e.g. ViViT [1]) or pure SSM models. By applying selfattention over the spatial dimensions, we allow all tokens to attend to all the other tokens in parallel, without having to commit to particular order (unlike in VideoMamba). We employ strong transformer blocks from ViTs for this operation, including their Imagenet pre-trained weights. The recurrence of the temporal processing enables efficient frameby-frame inference over long videos, with constant memory footprint and causal operation. 3.1. Background on LRUs Linear Recurrent Units (LRUs) [32], similar to SSMs, belong to the family of linear recurrent architectures and have been shown to be competitive with Transformers on language tasks [9, 16]. One potential interpretation of the success of these models, as outlined in [32], is that by sacrificing the nonlinear recurrence typical of recurrent model, we can improve the scalability and controllability of the system. Specifically, the linearity allows the recurrent matrix to be diagonalised through eigenvalue decomposition and absorbing the (dense) eigenvectors matrix into the neighbouring layers. This gives direct access to the eigenvalues of the Jacobian of the transfer function characterising the system. By initialising these eigenvalues within the unit circle, we have guaranteed stability of the system, bypassing issues like vanishing or exploding gradients. In addition, through the specific initialisation range of the eigenvalues within [0, 1] we can control how quickly the information vanishes, with eigenvalues close to 1 promoting longer-term memory. However, using only linear recurrence can greatly limit the expressivity of the layer. In [31], the authors show that by using these layers within typical transformer structure that alternates linear recurrences with point-wise nonlinearities (e.g. the MLP block), the overall architecture can be shown to be universal approximator of finite sequence-tosequence maps. 3.2. Gated LRUs for Video We adopt the gated variant of the LRU [9] to design our proposed block for video modelling. Let [0, 1]T HW 3 be an RGB video with frames and pixels. The video frames are split into non-overlapping patches pk of size 3, with {1, } and {1, }. Let xk be the tokens obtained 3 after the linear projection of the patches and the addition of the spatial positional encoding, with token size 1 1 d, where is the token feature dimension. Each LRU operates over temporal tube {xk = 1, }, following the equations below (we drop the spatial index for clarity): it = σ(Wxxt + bx), rt = σ (Wλxt + bλ) , λt = σ(λ)Crt, input gate recurrence gate ht = λt ht1 + (cid:113) 1 λ2 (it xt). (1) (2) (3) (4) where ht Rd is the state of the LRU, λt Rd is vector containing the eigenvalues of the (diagonal) recurrence matrix2, it Rd is the input gate controlling whether xt Rd is integrated within the state ht of the LRU or not, and rt Rd is the recurrence gate. The weights and biases of the LRU (Wx Rdd, Wλ Rdd, bx Rd, bλ Rd) are initialized using LeCun init [26]. The (learnable) recurrence weights λ are passed through sigmoid function to ensure they are between 0 and 1, and are initialised such that σ(λ) is sampled uniformly in [λmin, λmax]. These recurrent weights are raised to the power rt, which effectively acts as gate controlled by rt given in equation (2). rt is defined as linear projection, with parameters Wλ and bλ, followed by sigmoid function to ensure again the range [0, 1]. By raising element-wise σ(λ) to rt, the effective recurrence weight at some position can change between the j-th entry of σ(λ) when the corresponding gate entry is 1 and 1 when the gate entry is 0. The additional constant coefficient R, typically set to 8 as in [9], increases the range to be between σ(λ)C to 1, providing additional flexibility. E.g. if σ(λ) is 0.9 and we set = 8, we extend the range from [0.9, 1] to [0.43, 1]. More importantly, we change the learning dynamics (e.g. gradient norms) and resolution we have over the range during learning. Specifically, for xt in some fixed interval and similar magnitude Wλ, as it is the case at initialisation, higher value of implies λt will concentrate more towards the edges of the range. Note also that this is the dynamic range in which the recurrent weights can vary during inference as function of the input tokens. In [9], the authors found that setting λmin = 0.9 and λmax = 0.999 leads to the best results. An eigenvalue of 0.9 implies that it will take at least 10 time steps for the information to decay to roughly 35% of its magnitude, while for an eigenvalue of 0.999 it will take 1000 time steps to decay by the same amount. When using the same range for video modelling, we observed that the eigenvalues are 2Similar to [9], we implement the recurrence weights λt as exp(C softplus(λ) rt), which is mathematically equivalent but numerically more stable. pushed significantly towards λmin during training, with small number of eigenvalues becoming smaller than λmin; see Figure 2. We experimented with extending the range and obtained better results with λmin = 0.6. This leads to faster decay of information initially and might reflect the importance for videos of having enough recurrent units focused on short term information, in order to disentangle fast changing dynamics from slow ones. Figure 2. Distribution of the eigenvalues of the recurrent matrix at the beginning and end of training on long video memorisation task (see subsection 5.3) for different initialisation ranges. Finally, note that when diagonalising the recurrence matrix, the eigenvalues λ could, in theory, have complex values. We conducted experiments using complex eigenvalues, but we did not see improvements compared to using only real eigenvalues. The same observation was made in [9, 16] as well. 3.3. Video block based on gated LRU We use the gated LRU in similar block structure as the one employed in [9], see Figure 1b. Given 1D input (temporal tube), the block first applies normalisation layer, then the signal is routed on two different paths. On the first one, it gets linearly projected to same dimensionality and then the GeLU activation is applied. On the other path, the signal is also linearly projected to the same dimensionality d, then we apply 1D convolution followed by the gated LRU described in equation (4). The output of the LRU and the GeLU branch are element-wise multiplied and then linearly projected to the same dimension d. Note that, in line with [9], we use separable convolution, which allows mixing information only over time, not over chan4 (a) Memory comparison (b) FLOPs comparison Figure 3. Our model demonstrates increasingly greater memory and compute savings compared to ViViT baselines as the number of frames increases. For clarity, TRecViTs peak memory (left figure) goes from about 4G for 8 frames to 22.4G for 64 frames, but this increase is dwarfed by ViViTs increase, hence TRecViT line appears almost horizontal nels. We sweep the width of the convolutional kernel and find that window of 2 is enough compared to [9] which used 4. Also, different from [9], we do not use an MLP block after the LRU for feature mixing. We apply the MLP after the self-attention block, as done in ViT. Given the diagonal form of the recurrence, on device, the gated LRU computations are memory-bound, i.e. the data transfer takes longer than the actual computations done on that data. Similar to [9] we use specialised Pallas [6] kernel that minimizes the number of bytes that need to be moved between HBM and VMEM (the Vector Processing Units cache). The parameters added by the linear projections within the block, as well as the parameters of the convolution and the LRU, are learned. 4. Training TRecViT The proposed architecture can be trained in supervised or self-supervised regime. Given tokenised video input, the output of TRecViT will have the same dimension and shape as the input, meaning that we can easily recover the spatiotemporal structure of the input video, which can be useful for dense tasks like pixel reconstruction, depth estimation, or point tracking. At inference time, the architecture can be applied over all the video frames at once, or frame-byframe by carrying over the state of the LRUs. Depending on the task, one can choose to keep all the outputs from all time-steps to make prediction (similar to ViViT), or just the outputs from the last step, given that the LRU integrates the previous history in its state. In our experiments, we use mainly the former for fairer comparison with ViViT, but we also experiment with the latter to analyse LRUs capability of remembering over very long context; see subsection 5.3. 4.1. Self-supervised pre-training Given the factorised nature of the proposed architecture and the redundancy present in the video signal, it comes natural to apply masked auto-encoding to enable self-supervised pre-training from scratch on large-scale unlabelled datasets. We follow the same recipe as in the original VideoMAE paper [40]. Specifically, we use tube masking where 2D random mask is generated and repeated for all the frames in the video. For our architecture, this is equivalent to dropping temporal LRUs. The training objective is simply L2 reconstruction error of the entire frames. We sweep the value of the masking ratio and we find that 0.90 leads to best performance on downstream tasks. When using the pre-trained representations for downstream tasks, we keep all the tokens of the video and we add decoder or readout head that is fine-tuned for the respective tasks. 4.2. Memory footprint and FLOPs We compare the memory footprint and the number of FLOPs of TRecViT against ViViT baselines, see Figure 3. The profiling results are obtained by cost and memory analysis of lowered Jax HLO on CPU backend to be aligned with the theoretical numbers [2]. We consider as input video of size 224 224 and we vary the length of the video to analyse the savings provided by our architecture as the length of the video increases. Although in number of parameters for TRecViT is in between ViViT-B and ViViT-L (90M > 109M > 320M), the peak memory and number of flops for TRecViT are significantly lower as the number of frames increases, e.g. at 32 frames (the number of frames typically used in video classification experiments), TRecViTs peak memory is 12 smaller than that of ViViT-L and the FLOPs count is 5 lower. When going 5 to 64 frames, the peak memory is 24 smaller and FLOPs count is 8 lower. 5. Experiments We present results for supervised video classification and self-supervised masked auto-encoding with frozen representations evaluated on two downstream tasks: video classification and point tracking. To analyse the memory capabilities of our model, we also include reconstruction task of frames seen in the distant past. Using the same task, we study the generalisation capabilities to longer sequences than seen during training. We follow the ViT scaling configurations and, unless otherwise stated, we use the Base version for our model for all our experiments. We specify the number of parameters for all models considered in our experiments, and we include in the supplementary material all the training hyperparameters and data augmentations used in all experiments. 5.1. Supervised video classification Datasets: We use large-scale real-world datasets for the supervised video classification task. Kinetics400 [7] contains 241,512 videos3 across train, validation, and test splits, 10slong (25fps), spanning 400 classes. This dataset is known to require modelling appearance for successful action recognition. To challenge our models capability of understanding motion, we also use SSv2 dataset [15], which contains 220,847 shorter videos (2-6s long), sampled at 12fps, representing 174 classes. This dataset includes actions that differ in finer motion-related details, requiring deeper temporal understanding, e.g. pouring something into something vs pretending to pour something into something. Baselines: We use ViViT [1] as our main baseline. We consider the full self-attention version, which patchifies and flattens the entire video, prepends video class token, then runs self-attention blocks. We also consider the factorised encoder version (ViViT FE), which runs ViT image model over all the frames, and uses temporal self-attention blocks to integrate the information over time. Finally, we also consider baseline that uses only LRU recurrent and MLP blocks, configured similar to VideoMamba [27], i.e. it does not use self-attention blocks, denoted PureLRU. Similar to ViViT, this model first patchifies and flattens the video, prepends class token, then applies sequence of recurrent blocks. All baselines use learnt spatio-temporal positional encoding, whereas the proposed TRecViT uses only spatial positional encoding as the temporal dimension is implicitly modelled through its recurrence. Figure 4. TRecViT compared to baselines on supervised video classification on SSv2 dataset, trained from scratch. The plot shows the evolution of the evaluation accuracy as training progresses. Results: We include results for training from scratch or using Imagenet pre-trained weights to initialise the weights of the ViT blocks. Figure 4 shows first comparison between TRecViT and the above baselines, with all models being trained from scratch on supervised classification on SSv2. We consider the Small version for all models as the larger Base version shows stability issues when trained from scratch, as reported in other works as well [1, 27]. As expected, the performance on this challenging dataset when training from scratch is far from SOTA, but it clearly shows that the proposed factorisation has superior video modelling capabilities compared to baselines, ViViT-S with full selfattention being the closest competitor. PureLRUs performance is very poor, which is in line with the findings of other works (e.g. VideoMamba) who report that bidirectional (non-causal) processing of the input is needed for good performance. We report further results comparing against ViViT-B and ViViT-L with full self-attention when using Imagenet pretrained weights; see Table 1 for SSv2 results and Table 2 for Kinetics400 results. We can observe that our model achieves better performance compared to ViViT baselines on SSv2, but it is slightly below ViViT-L on Kinetics400. This result could reflect the difference between the two datasets mentioned above: outperforming ViViT-L on SSv2 suggests that TRecViT is superior at modelling motion compared to ViViT, but on Kinetics where the appearance is enough for successful classification, both models are on par. We consider this to be strong positive result for our model given that it has about 3x less parameters compared to ViViT-L and significantly lower FLOPs count and memory footprint as shown in Figure 3. 3Kinetics is dynamic dataset (videos may be removed from YouTube). Our current version has 241,512 videos, compared to 267,000 videos reported in [1], so decrease of almost 10%, noticeable in the final performance. 5.2. Self-supervised masked autoencoding We use Kinetics400 for self-supervised pre-training from scratch and we report results on multiple downstream 6 Figure 5. Qualitative results obtained by TRecViT for point tracking on DAVIS dataset compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViTs predictions and red circles indicate VideoMAEs predictions. Model ViViT-B ViViT-L TRecViT Patch size Top-1 acc (%) (2, 16, 16) (2, 16, 16) (1, 16, 16) 59.1 65.9 66.8 # params 90M 320M 109M Table 1. Performance of TRecViT compared to ViViT-B and ViViT-L baselines on SSv2 dataset with all models initialised from Imagenet pre-training. For ViViT-L, we use the result reported by its authors, for ViViT-B we obtained the results internally as they were not reported in the original paper for this dataset. Model ViViT-B ViViT-L TRecViT Patch size Top-1 acc (%) (2, 16, 16) (2, 16, 16) (1, 16, 16) 78.1 78.7 78.4 # params 90M 320M 109M Table 2. Performance of TRecViT compared to ViViT-B and ViViT-L baselines on Kinetics400 dataset, with all models initialised from Imagenet pre-training. For ViViT-B and ViViT-L, we include the result we obtained internally by re-training the model on the current Kinetics400 dataset version; see footnote. In the original paper, the authors reported 80.3% on Kinetics400 for ViViT-L. Dataset Model VideoMAE Kinetics400 TRecViT Kinetics400 SSv2 VideoMAE TRecViT SSv Top-1 acc (%) 45.8 46.0 53.7 53.9 # params 330M 128M 330M 128M Table 3. Performance of TRecViT compared to VideoMAE on video classification using frozen MAE representations, pre-trained on Kinetics400. Model MooG VideoMAE TRecViT MooG VideoMAE TRecViT Dataset DAVIS DAVIS DAVIS Perception Test Perception Test Perception Test # frames 8 8 8 16 16 AJ 0.687 0.703 0.706 0.760 0.761 0.783 # params 35M 330M 128M 46.5M 330M 128M Table 4. Performance of TRecViT compared to baselines on point tracking task on DAVIS and Perception Test datasets. All models use frozen representations evaluated using the readout head from MooG. datasets and tasks by fine-tuning attention readout heads on top of frozen representations. We choose this setup, as opposed to fine-tuning end-to-end, as the performance in this case more clearly reflects the quality of the pre-trained representations. As mentioned in the previous section, we use large masking ratio (0.90), which makes pre-training very efficient. We report the number of parameters for every model considered. Note that the number of parameters for TRecViT is different from the one reported in the previous section due to the addition of the readout heads. Video classification: We report video classification accuracy as downstream task using attention readout heads on SSv2 and Kinetics400. We compare the performance against VideoMAE-L [40] in Table 3. Our model obtains slightly better performance on both datasets compared to this strong baseline, despite having almost 3 less parameters. Point tracking: To demonstrate that our model can handle dense(r) tasks as well, we evaluate the same frozen MAE representations for the point tracking task. We use the recurrent architecture in MooG [41] as readout due to its simplicity. MooG uses light cross-attention layers to process the embeddings of each frame in order, and the readout state is carried over through time. We finetune the MooG readout head using MOVi-E dataset [25] as done in popular point tracking works [11]. We evaluate these fine-tuned representations on two datasets: Perception Test [36] and DAVIS dataset [35] with point tracks extracted in [10]. We report average Jaccard metric [10] for TRecViT compared with MooG and VideoMAE; see Table 4. TRecViT obtains better performance on both datasets compared to baselines, which reinforces the observation that our proposed model has strong motion modelling capabilities. We include qualitative results for this task in Figure 5. We can observe that the results are visibly better compared to VideoMAE. More visualisations are included in the supplementary material. Figure 6. Qualitative results obtained by TRecViT on the dense memorisation task compared to ViViT-L. Both models are trained using Imagenet pre-trained weights, on video sequences of = 64 frames and they reconstruct the (T 48)th frame. and 1 1 convolution to get the expected shape for the reconstructed frame. For TRecViT, we simply keep the output of the last layer at time step and reshape it to the expected shape. We show quantitative and qualitative results respectively in Figures 7 and 6. We can observe that there is performanceefficiency trade-off at play for TRecViT: its performance is slightly below ViViTs for shorter memory spans (16, 48, 80), but its efficiency (steps-per-second) is significantly higher. However, beyond 80 frames, ViViTL goes out of memory, whilst TRecViT continues to give decent results up to 144 frames, going out of memory towards 164 frames. Figure 6 shows qualitative results compared to the baseline for the case where the models have to remember the frame seen at 48 in the past. We can observe that the quality of ViViT-Ls reconstruction is good. For TRecViT, whilst the overall structure (encoded in lower frequencies) is correct, it struggles to remember the highfrequency content of the image. This is to be expected due to the compression happening in the recurrent state of the model. However, given how different the last seen frame is from the target frame, we consider this to be very promising result that warrants further investigation into the memorisation capabilities of our model, which we leave as future work. 5.4. Generalisation to longer sequences Using the same task as above, we analyse the generalisation capabilities to sequences longer than those used during training. Specifically, we train the models with sequences of length = 64 frames to reconstruct the 48 frame, and evaluate them on longer sequences = 96 to reconstruct the same frame. The TRecViT model can run on longer sequences without any modification. For the ViViT model, we need to adapt the positional encoding to accommodate longer sequences. We use interpolation to nearest neighbour to obtain the desired length; cubic interpolation led to worse results. The performance of TRecViT degrades (a) PSNR comparison (b) Steps-per-second comparison Figure 7. Long video memorisation task. At time , the model has to reconstruct the (T k)th frame seen in the past. The plots show PSNR and throughput (steps-per-second) for increasing time offset k. For both models, the data points with 0 value on the yaxis correspond to OOM. 5.3. Long video memorisation task Transformer models for language are known to be excellent at retrieving information from context, as they cache the keys and values for the entire history. On the other hand, LRUs / SSMs and RNNs in general struggle with such needle-in-the-haystack style tasks as they need to perform the retrieval based on the compressed history kept in their recurrent state [9, 24]. We are interested in studying this aspect in the video domain as well. We set up simple reconstruction task where the model has to remember the frame seen at given time-step in the past. For our analysis, we run multiple experiments where the model is tasked to reconstruct the (T k)th frame from the past, with increasing value for {16, 48, 80, 112, 144, 164} frames. We employ Walking Tours dataset [43], which contains hour-long videos, and the scenery changes constantly, hence we are guaranteed that the video frames seen most recently will be very different compared to the frames seen earlier on. We scale the videos to 224 224 pixels. Again, we adopt ViViT-L as baseline, and we train both models using Imagenet pretrained weights. For ViViT-L, we keep all the outputs from all time steps and apply temporal pooling 8 slightly, with PSNR going down from 29.3 (when evaluated on the same sequence length as in training = 64) to 26.4 when evaluated with = 96 frame sequences. ViViTs PSNR, however, drops significantly, from 32.3 when evaluated on the same sequence length, to 15.1 when evaluated on longer sequences. We include qualitative examples in Figure 8 where we can observe that ViViTs output contains stronger artefacts compared to TRecViT. their insightful feedback throughout this project."
        },
        {
            "title": "References",
            "content": "[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. Vivit: video vision transformer. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 68166826, 2021. 2, 3, 6, 1 [2] The Jax Authors. Jax documentation. 5 [3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 2 [4] Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlarXiv preprint stm: Extended long short-term memory. arXiv:2405.04517, 2024. 2 Figure 8. Generalisation to longer sequences. Both models are trained using Imagenet pre-trained weights, on video sequences of = 64 frames to reconstruct the (T 48)th frame; during evaluation, the models receive sequences of = 96 frames. 6. Conclusion We propose novel video architecture TRecViT that alternates gated linear recurrent units (LRUs) modelling the temporal dynamics in the video with ViT blocks modelling the spatial and channel dimensions. The proposed model outperforms or obtains competitive performance compared to strong baselines (ViViT-L, VideoMAE) on supervised and self-supervised tasks, while having much smaller number of parameters and significantly reduced memory footprint and FLOPs count. In terms of limitations, our study focuses on doing first investigation into using LRUs for the video domain and we obtain favourable results on multiple datasets and tasks compared to strong baselines. However, more experimentation and model scaling are required to obtain SOTA results on all these tasks. Given that the training dynamics for gated LRUs are stable and controllable by design, plus the reliance on (pre-trained) ViT blocks give strong indication that achieving SOTA is possible. We leave this investigation for future work, together with further analysis of training dynamics, and integration into various downstream tasks, e.g. video-language tasks or Robotics tasks."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Caglar Gulcehre, Daniel and Andrew Zisserman for Zoran, Dima Damen, Frostig, [6] James Bradbury, [5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In Proceedings of the 38th International Conference on Machine Learning, pages 813824. PMLR, 2021. 2 Roy Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. 5 [7] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [8] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 47244733, 2017. 1 [9] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. 2, 3, 4, 5, 8 [10] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens Continente, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. TAP-vid: benchmark for tracking any point in video. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 7 [11] Carl Doersch, Yi Yang, Mel Vecerık, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In ICCV, pages 1002710038, 2023. 7 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is 9 worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [13] Jeffrey Elman. Finding structure in time. Cognitive Science, 14(2):179211, 1990. 2, 3 [14] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 62016210, 2019. 1 [15] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. 6 [16] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 2, 3, [17] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems, pages 14741487, 2020. 2 [18] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. 2, 3 [19] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. 2 [20] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997. 2 [21] Md. Mohaiminul Islam and Gedas Bertasius. Long movie In Euroclip classification with state-space video models. pean Conference on Computer Vision, 2022. 3 [22] Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas Bertasius. Efficient In movie scene detection using state-space transformers. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1874918758, 2023. 3 [23] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: general architecture for structured inputs & outputs. In International Conference on Learning Representations, 2022. 2 [24] Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. Repeat after me: Transformers are betarXiv preprint ter than state space models at copying. arXiv:2402.01032, 2024. [25] Abhijit Kundu, Andrea Tagliasacchi, Anissa Yuenming Mak, Austin Stone, Carl Doersch, Cengiz Oztireli, Charles Herrmann, Dan Gnanapragasam, Daniel Duckworth, Daniel Rebain, David James Fleet, Deqing Sun, Derek Nowrouzezahrai, Dmitry Lagun, Etienne Pot, Fangcheng Zhong, Florian Golemo, Francois Belletti, Henning Meyer, Hsueh-Ti (Derek) Liu, Issam Laradji, Klaus Greff, Kwang Moo Yi, Lucas Beyer, Matan Sela, Mehdi S. M. Sajjadi, Noha Radwan, Sara Sabour, Suhani Vora, Thomas Kipf, Tianhao Wu, Vincent Sitzmann, Yilun Du, and Yishu Miao, editors. Kubric: scalable dataset generator, 2022. 7 [26] Yann A. LeCun, Leon Bottou, Genevieve B. Orr, and KlausRobert Muller. Efficient BackProp, pages 948. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. 4 [27] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding, 2024. 2, 3, 6 [28] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for near-infinite context. In The Twelfth International Conference on Learning Representations, 2024. 2 [29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1001210022, 2021. 2 [30] Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH 11th Annual Conference of the International Speech Communication Association, pages 10451048, 2010. [31] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel Smith. On the universality of linear recurrences followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023. 3 [32] Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. 2, 3 [33] Viorica Patraucean, Ankur Handa, and Roberto Cipolla. Spatio-temporal video autoencoder with differentiable memory. In 2016 International Conference on Learning Representations (ICLR) - Workshop track, 2016. 2 [34] A. J. Piergiovanni, Weicheng Kuo, and Anelia Angelova. Rethinking video vits: Sparse video tubes for joint image and video learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 22142224. IEEE, 2023. 2 [35] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. 7 [36] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adri`a Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Joao Carreira. Perception test: diagnostic benchmark for multimodal video models. In Advances in Neural Information Processing Systems, 2023. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 10 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 8748 8763. PMLR, 2021. 3 [38] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video representations using lstms. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, page 843852. JMLR.org, 2015. 2 [39] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 31043112, 2014. 2 [40] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems, 2022. 2, 5, 7, [41] Sjoerd van Steenkiste, Daniel Zoran, Yi Yang, Yulia Rubanova, Rishabh Kabra, Carl Doersch, Dilara Gokay, Joseph Heyward, Etienne Pot, Klaus Greff, Drew A. Hudson, Thomas Albert Keck, Joao Carreira, Alexey Dosovitskiy, Mehdi S. M. Sajjadi, and Thomas Kipf. Moving off-the-grid: Scene-grounded video representations. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 7 [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017. 2 [43] Shashanka Venkataramanan, Mamshad Nayeem Rizve, Joao Carreira, Yuki Asano, and Yannis Avrithis. Is imagenet worth 1 video? learning strong image encoders from 1 long unlabelled video. In International Conference on Learning Representations, 2024. 8 [44] Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, Ziyang Wang, and Zi Ye. survey on visual mamba. Applied Sciences, 14(13), 2024. 3 [45] Long Zhao, Nitesh Bharadwaj Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong. VideoPrism: foundational visual encoder In Proceedings of the 41st Interfor video understanding. national Conference on Machine Learning, pages 60785 60811. PMLR, 2024. 2 [46] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. In Forty-first International Conference on Machine Learning, 2024. 11 TRecViT: Recurrent Video Transformer"
        },
        {
            "title": "Supplementary Material",
            "content": "We include here all the hyperparameters used in the experiments presented in the main paper, together with more qualitative visualisations of results for the point tracking task (section 5.2) and the long video memorisation task (section 5.3). Videos showing point tracks are also attached. 7. Training hyperparameters 7.1. Supervised video classification"
        },
        {
            "title": "Hyperparameter\nLearning rate\nScale jitter\nNum frames\nStride\nEpochs\nSpatial crops eval\nTemporal clips eval",
            "content": "Kinetics400 3e-4 (0.9, 1.33) 16 2 30 3 4 SSv2 3e-4 (0.9, 1.33) 16 2 6 3 4 Table 7. Hyperparameter values used in the fine-tuning classification experiments. We use cosine decay for the learning rate schedule with 1k steps of linear warmup. Hyperparameter DAVIS Perception Test 3e-4 Learning rate 8 Num frames 200k Num steps 3e-4 16 40k Table 8. Hyperparameter values used in the point tracking finetuning experiments. We use cosine decay for the learning rate schedule with 1k steps of linear warmup. 9. Long video memorisation task Figure 10 shows qualitative results for the memorisation task. For easier visual comparison, we increase the distance to the frame to reconstruct while also increasing the video length , so the frame to reconstruct is always the same. For ViViT-L (3rd row), the quality of the reconstruction is very good and does not degrade as increases. However, the model goes out-of-memory for > 96. For TRecViT, the high frequencies are less well reconstructed as increases, but overall the model is able to perform the task reasonably well even at = 160, = 144, i.e. it is able to learn with sequences of up to 5.3s long at 30FPS, and remember frame seen about 4.8s before."
        },
        {
            "title": "Hyperparameter\nPeak learning rate\nWeight decay\nLabel smoothing\nScale jitter\nNum frames\nStride\nCls dropout\nRand augment\nEpochs\nSpatial crops eval\nTemporal clips eval",
            "content": "Kinetics400 1e-4 0.03 0.1 (0.875, 1.33) 32 2 - - 30 3 4 SSv2 1e-4 0.03 0.1 (0.875, 1.33) 32 2 0.1 yes 35 3 4 Table 5. Hyperparameter values used in the supervised classification experiments. These are mainly the hyperparameters used in previous works, e.g. ViViT [1]. For both datasets, we use cosine decay for the learning rate schedule with linear warmup. 7.2. Self-supervised masked autoencoding and finetuning Hyperparameter Kinetics400 Learning rate Weight decay Num frames Stride Epochs Mask ratio 3e-4 0.05 16 2 1600 0.9 Table 6. Hyperparameter values used in the self-supervised masked auto-encoding experiment on Kinetics400. We use AdamW optimizer. We apply patch-wise normalisation of the inputs as done in VideoMAE [40] 8. Point tracking qualitative results In Figure 9, we include more visualisations for the point tracking task using frozen MAE representations pre-trained on Kinetics400, using TRecViT as backbone. 1 Figure 9. Qualitative results obtained by TRecViT for point tracking on DAVIS dataset (rows 1-2) and Perception Test (rows 3-4) compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViTs predictions and red circles indicate VideoMAEs predictions. 2 Figure 10. Qualitative results for the task of reconstructing frame from the past, for increasing distance to the frame to reconstruct from left to right. First row: last frame seen by the model. Second row: TRecViT output. Third row: ViViT-L output; ViViT-L goes OOM for > 80, so no predictions are shown."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}