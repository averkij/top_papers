{
    "paper_title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "authors": [
        "Huilin Deng",
        "Hongchen Luo",
        "Yue Zhu",
        "Long Li",
        "Zhuoyue Chen",
        "Xinghao Zhao",
        "Ming Li",
        "Jihai Zhang",
        "Mengchang Wang",
        "Yang Cao",
        "Yu Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics."
        },
        {
            "title": "Start",
            "content": "I2B-LPO: Latent Policy Optimization via Iterative Information Bottleneck Huilin Deng1,2, Hongchen Luo5, Yue Zhu1,4, Long Li1, Zhuoyue Chen1,3, Xinghao Zhao1, Ming Li2, Jihai Zhang1,Mengchang Wang1, Yang Cao2,Yu Kang2 1DAMO Academy, Alibaba Group 3 Zhejiang University 5 Northeastern University 4Shanghai Jiao Tong University 2University of Science and Technology of China 6 2 0 J 9 ] . [ 1 0 7 8 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite recent advances in Reinforcement learning with verifiable rewards (RLVR) for large language model (LLM) reasoning, most methods suffer from exploration collapse, as the semantic homogeneity of random rollouts traps models in narrow, over-optimized behaviors. Existing methods leverage policy entropy to encourage exploration, but face inherent limitations: global entropy regularization is susceptible to reward hacking, inducing meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To this end, we propose Latent Policy Optimization via Iterative Information Bottleneck (I2B-LPO), which shifts from statistical perturbation of token distributions to topological branching of reasoning trajectories. I2B-LPO triggers latent branching at high-entropy states to diversify reasoning trajectories and applies the Information Bottleneck as trajectory filter and self-reward to ensure concise and informative exploration. Empirical results on four mathematical benchmarks demonstrate that I2B-LPO achieves state-of-the-art performance, with margins of up to 5.3% in accuracy and 7.4% in diversity metrics. Date: January 12, 2026 Code: https://github.com/denghuilin-cyber/IIB-LPO Correspondence: forrest@ustc.edu.cn"
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has emerged as key method for LLM reasoning, particularly in tasks with deterministic verification such as mathematics (DeepSeek-AI et al., 2025; Yang et al., 2024). This paradigm trains models to rollout and differentiate between multiple reasoning paths, reinforcing trajectories that lead to correct solutions while penalizing incorrect paths (Cheng et al., 2025). This contrastive approach has driven remarkable success across mathematical and broader reasoning tasks. Despite this success, RLVR methods can unintentionally cause exploration collapse (Jiang et al., 2025), where models converge on narrow, overoptimized behaviors and lose their incentive to explore alternative strategies. This pathology stems from the semantic homogeneity of random rollouts (Ju et al., 2025). While these rollouts exhibit variations in surface phrasing, the underlying reasoning rapidly degenerates into few high-probability reasoning templates (Zhang et al., 2025a), i.e., they share nearly identical reasoning patterns. Consequently, comparing such homogeneous paths yields vanishing advantage differentials and thus uninformative learning signals for policy optimization. To mitigate this, policy entropy has been strategically leveraged to encourage exploration (Wang et al., 2025a). Existing approaches typically fall into two paradigms. Entropy-regularization methods (Chao et al., 2024) directly maximize the entropy of generated instances by globally smoothing the token-wise probability distribution (Fig. 1(a)). However, this global smoothing is susceptible to reward hackingincentivizing the model to generate semantically vacuous verbosity (Yao et al., 2024). As shown in Fig. 1(a), the model tends to generate excessive meta-discourse irrelevant to the problem. Conversely, token-selective methods (Cui et al., 2025) amplify policy updates on high-entropy tokens, locally sharpening the next-token distribuFigure 1: Comparison of exploration paradigms in RLVR. (a) Entropy Regularization globally smooths the probability distribution, leading to high-entropy yet meaningless verbosity. (b) Token-selective Methods locally sharpen the distribution; synonym replacement at these isolated points cannot overcome inductive biases. (c) I2B-LPO introduces topological branching via latent variables z, resulting in distinct reasoning trajectories (e.g., Differentiation-based R1 vs. Geometry-based R2). tion at critical positions (Fig. 1(b)). Yet, such local sharpening struggles against the inductive biases. High-entropy positions frequently correspond to lexical ambiguity (e.g., synonym choice); merely sharpening the distribution at these points is insufficient to overcome the strong prior of pretrained models (Casper et al., 2023). Consequently, both paradigms are confined to probabilistic perturbations, lacking the capacity to induce structural diversification in the reasoning process itself. To address these limitations, this paper proposes fundamental paradigm shift: moving from statistical perturbation of token distributions to topological bifurcation of reasoning trajectories (Fig. 1(c)). We introduce Latent Policy Optimization via Iterative Information Bottleneck (I2B-LPO), which triggers latent branching at high-entropy states to shatter inductive biases and utilizes informationtheoretic constraints to curb reward hacking. As illustrated in Fig. 4, the method operates through two key mechanisms: (1) Entropy-driven Latent Branching utilizes Conditional Variational Autoencoder (CVAE) (Fang et al., 2021) to sample diverse latent variables zi at detected bifurcation (high-entropy states). Each zi serves as structural prompt, injected into the LLMs attention layers, continually steering the trajectory of subsequent reasoning. (2) Information Bottleneck Regularization functions as dual-purpose filter and self-reward. By quantifying the trade-off between rationale compression and predictive power (Lei et al., 2025a), the IB objective identifies compact, informative paths for policy updates while simultaneously penalizing semantically vacuous verbosity. Empirical results demonstrate I2B-LPOs SOTA performance in both reasoning accuracy and semantic diversity, without excessive generation length. The significant margins in accuracy (5.3%) and diversity (7.4%) validate its efficacy in prompting exploration. The main contributions are summarized as follows: (1) Paradigm shifts from statistical token perturbation to topological trajectory branching. (2) Entropy-Driven Latent Branching explicitly restructures the reasoning topology to induce trajectory diversity. (3) Dual-Purpose IB uses it both as trajectory filter and self-reward, favoring concise, informative reasoning. (4) SOTA performance in both accuracy and diversity without excessive length."
        },
        {
            "title": "2 Related Work",
            "content": "Entropy Regularization in RL Entropy, which quantifies the dispersion of vocabulary distribution, reflects the predictive uncertainty of LLMs. Thus, its strategically utilized to guide the exploratory behavior of LLMs (Chao et al., 2024). Existing methods typically fall into two categories. Token-selective methods target high-entropy tokens for selective updates (Wang et al., 2025a). Regularization-based methods, such as concurrent works by (Wang et al., 2025b) and (Cheng et al., 2025), incorporate an entropy regularizer directly into the loss or advantage function. Most priors rely on indiscriminate regularizer on the token distribution. Instead, we propose paradigm shift towards conditional branching for effective exploration. Figure 2: Performance of various decoding strategies trained on DeepMath. For each problem, we sample truncation points across entropy intervals to simulate varied exploration behaviors. Self-Rewarding Reasoning. Recent RL advancements in LLMs rely heavily on outcomebased rewards, yet such sparse supervision neglects intermediate reasoning validity. As steplevel labels require substantial human effort, selfrewarding methods have gained increasing attention. (Bai et al., 2022) and (Yuan et al., 2025) use predefined rules to generate preference pairs for Direct Preference Optimization (DPO) training. (Fränken et al., 2024) maximize mutual information between principles and responses, while (Zhang et al., 2025b) is based on semantic entropy clustering. SPINE (Wu et al., 2025) adopts majority-vote-based reward for self-consistency. However, unlike SPINE, which suppresses highentropy states via static voting, we actively leverage them as gateways for topological branching and iterative trajectory refinement."
        },
        {
            "title": "3 Preliminary Analysis",
            "content": "Pivotal Decision Points. To validate whether entropy signals pivotal reasoning steps, we sampled prefixes conditioned on token-level entropy to simulate different exploration strategies via various decoding methods. Fig. 2 shows that in high-entropy intervals (>2.5) hybrid strategy substantially outperforms baselinesa performance gap far larger than in low-entropy regions (<1.0). This confirms highentropy states as natural decision points; branching here unlocks exploration potential. Verbosity without Gain. As shown in Fig. 3, when applying standard GRPO, the accuracy plateaus early while the response length continues to rise, accompanied by growing 4-gram repetition rate (gray bars). This indicates that without additional guidance, the model tends to produce verbose and repetitive reasoning without genuine performance Figure 3: Accuracy and Response Length under GRPO. Notably, gray bars denote 4-gram repetition rate. gains. Therefore, we introduce the IB self-reward to suppress such unproductive expansion."
        },
        {
            "title": "4.1 Problem Formulation",
            "content": "Formally, let denote input prompt, the reasoning trajectory. We initialize the policy optimization with GRPO, starting with initial rollouts Ro = {r1, . . . , rM }. I2B-LPO comprises two phases: Entropy-driven Branching: For each base rollout ri, we generate distinct branches while retaining the original one. This yields branching set = {ri,j 1 M, 1 +1}, containing (K + 1) paths. Each path ri,j consists of sequence of tokens (o1, . . . , oTi,j ). IB-Pruning: we eliminate samples with low IBscores from R, retaining high-quality subset such that = ."
        },
        {
            "title": "4.2.1 Entropy-Driven Bifurcation Detection\nFor a given trajectory r = (o1, . . . , oT ), we iden-\ntify a “bifurcation point” t∗ ∈ {1, . . . , T } corre-\nsponding to a state of high uncertainty. The token-\nlevel entropy Ht at step t is computed as:",
            "content": "Ht = (cid:88) vV (v q, o<t) log (v q, o<t), (1) where is the vocabulary. Following, we select the top 5% highest-entropy steps as candidate Ω (τ : the 95th percentile of entropy history H): Ω = {t Ht τ }, Uniform(Ω). (2) Finally, we extract the prefix context ct by concatenating the query with the partial path preceding the split ct = [q, o1, . . . , ot1]. Figure 4: Pipeline of the I2B-LPO. We use representative path ri from the initial set Ro to illustrate the workflow, which operates in two phases. (1) Entropy-driven Latent Branching expands ri into the branching set Ri via Latent Sampling and PSA Injection, which are depicted in the bottom section. (2) Information Bottleneck Regularization applies IB as dual-purpose filter and self-reward to ensure concise and informative exploration."
        },
        {
            "title": "4.2.2 Latent Sampling via CVAE\nGiven the prefix ct∗, we employ a separately trained\nConditional Variational Autoencoder (CVAE)\nto sample latent variables z. Implementation of\nCVAE is detailed in Appendix B. Formally, the\nCVAE models the conditional distribution of a solu-\ntion trajectory y given context via a latent variable:",
            "content": "p(y x) = (cid:90) p(y z, x) p(z x) dz, (3) where the prior network p(z x) captures the distribution of zi. During training, for each ri, we sample independent latent codes from prior: z(j) p(z ct ), = 1, . . . , K. (4)"
        },
        {
            "title": "4.2.3 Latent-Guided Reasoning via PSA\nThis section explores how latent variables zj, gen-\nerated by a CVAE, influence subsequent reason-\ning in LLMs. We employ Pseudo Self-Attention\n(PSA), incorporating the latent code z ∈ Rd into\nthe self-attention mechanism at a per-layer basis.\nThe detailed injection is as follows:\n• Phase 1: Adaptive Norm Modulation. The la-\ntent code is projected and added to the learnable\nscaling parameter of RMSNorm:",
            "content": "w = wl + γ(t) Projϕ(zj), (5) where γ(t) is dynamic decay factor that anneals the latent influence over time. Phase 2: Augmented Self-Attention. The latent code is further projected into modulation vectors and concatenated with original keys and values: = (cid:21) (cid:20)zjK , = (cid:21) (cid:20)zjV R(1+l)d (6) where the notation () indicates row-wise concatenation. The enhanced attention is computed as: PSA(Q, j, ) = softmax (cid:32) QK j dk (cid:33) . (7) Thus, zi implicitly shifts features via RMSNorm and explicitly guides attention as structural prompt, thereby steering the reasoning trajectory. 4."
        },
        {
            "title": "Information Bottleneck Regularization",
            "content": "IB Score: Theory and Computation"
        },
        {
            "title": "4.3.1\nThis section formulates the IB score and its ap-\nproximation. We formulate LLM reasoning as an\noptimization trade-off between rationale compres-\nsion and its predictive power. Specifically, an",
            "content": "optimal policy minimizes prompt retention while maximizing the informativeness of final answers: min LIB(r) = I(q; r) βI(r; a), (8) where I(; ) denotes Mutual Information (MI). The MI between and can be decomposed as: I(r; a) = H(r) H(r a), (9) where H() denotes entropy. Eq. 9 implies that maximizing I(r; a) balances exploration (diversity H(r)) and precision (low ambiguity H(ra)). Formally, we define SIB(r) LIB(r) at the trajectory-level, where higher values indicate better balance between compression and informativeness. Based on the derivation in Appendix. C, we approximate the SIB(r) as: SIB(r) = ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (cid:16) log π(ot o<t, q) + λ H(ot o<t, q) (cid:17) t=1 (cid:88) t=1 AtH(ot o<t, q), (10) where is the response length. H(ot o<t, q) and At denote the policy entropy and advantage function at step t, respectively. IB-Guided Pruning and Optimization"
        },
        {
            "title": "5 Experiment Settings",
            "content": "Training Configuration. We conduct experiments on GRPO using the veRL framework. To build strong baselines, we adopt several techniques from DAPO, including Clip-Higher and GroupSampling. Detailed settings are in Appendix E. Datasets. Our training data are sourced from DAPO and MATH. To ensure training efficiency, we filter samples that are either too trivial or intractable (see Appendix for details). For evaluation, four benchmarks, including MATH-500 (Hendrycks et al., 2021), AIME2025, AIME24, Olympiadbench (He et al., 2024), are selected. Baselines. We conduct experiments on Qwen2.537B and Qwen3-14B. Baselines span three categories: (1)entropy regularization (EntropyReg (Chao et al., 2024) and Entropy-Adv (Cheng et al., 2025)) (2) token-selective methods (KLCov (Cui et al., 2025) and 80/20 (Wang et al., 2025a)), and (3) self-reward methods (SPINE (Wu et al., 2025) and SRLM (Yuan et al., 2024)). Metrics. Avg. # Tokens reports the average token length. For diversity, we report Distinctn (Li et al., 2016) and Self-BLEU (Papineni et al., 2002). (1) Distinct-n counts the ratio of unique ngrams. (2) Self-BLEU. We report 1 Score, with higher values indicating greater diversity.Perplexity (PPL) measures uncertainty of the generated sequence(Appendix.E). Low-PPL responses are generally more fluent and semantically coherent. = arg max SR S=N (cid:88) rS SIB(r),"
        },
        {
            "title": "6 Experiment",
            "content": "(11) where denotes any subset of with = . In practice, we first compute SIB(r) for each using Eq. (10), sort the results, and retain only the top paths. The pruned set then serves as the training data for policy optimization. We also incorporate the IB score as an auxiliary maximization objective during training: SIB(θ; R) ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) rR SIB(r). = JGRPO + γ SIB(θ; R), (12) (13) where γ is coefficient balancing task correctness and reasoning efficiency. This section evaluates I2B-LPOs ability to generate diverse, high-quality responses. We aim to answer the following research questions: RQ1 (Overall Performance): Does I²B-LPO outperform baseline methods in terms of both reasoning accuracy and semantic diversity? RQ2 (Exploration Behavior Analysis): How does the entropy-triggered mechanism reshape exploration dynamics? And how does latent injection steer subsequent reasoningintroducing structured exploration or random noise? RQ3 (IB Pruning Efficacy): How does the IBbased self-reward compare to other self-reward methods, and what specific reasoning characteristics correspond to high and low IB scores? RQ4 (Ablation Study): How does each component of I²B-LPO contribute to performance? Table 1: Overall Performance Comparison across 4 Mathematical Benchmarks. We evaluate I2B-LPO on two different backbones: Qwen2.5-7B and Qwen3-14B. We report Pass@n accuracy (%) alongside average response length (#Tok). Bold denotes the highest accuracy, while colored italics indicate the longest average response length."
        },
        {
            "title": "Method",
            "content": "Qwen2.5-7B +GRPO (Standard) Entropy-Reg. Entropy-Adv KL-Cov 80/20 SPINE SRLM I2B-LPO (Ours) Qwen3-14B +GRPO (Standard) Entropy-Reg. Entropy-Adv KL-Cov 80/20 SPINE SRLM I2B-LPO (Ours) AIME2025 P@1 P@256 AIME2024 #Tok P@1 P@256 MATH- #Tok P@1 P@"
        },
        {
            "title": "Olympiad",
            "content": "#Tok P@1 P@16 8.2 9.3 11.8 11.3 10.2 11.2 9.7 13.6 27.0 28.8 27.8 34.6 33.5 28.4 29.7 38.3 50.0 47.7 53.3 52.1 47.9 52.7 48.6 55.0 62.3 59.8 61.3 62.8 66.2 59.6 58.3 68.1 1292 3198 2187 1878 1898 1634 1574 2045 4285 2287 2390 2168 2034 2201 2465 10.3 12.7 13.6 15.2 16.2 14.4 15.1 18.6 34.4 37.3 42.8 45.4 43.9 39.2 37.2 46.6 46.7 55.6 56.7 72.3 70.5 68.2 65.1 79.7 67.8 58.9 63.1 80.6 78.5 70.6 69.4 82.5 776 2358 1424 1283 1315 1179 1334 1437 2963 2265 2018 1989 1966 1876 1830 54.4 57.4 58.5 68.2 61.8 76.2 73.4 81.5 89.2 90.2 89.5 91.7 91.6 89.9 91.1 93.5 58.4 70.4 74.0 82.1 79.1 86.5 84.7 90.5 92.9 93.0 93.5 93.6 94.1 93.2 94.6 95.7 661 1802 1223 1488 1190 1104 1058 825 2387 1626 1798 1488 1392 1892 1432 44.9 48.9 51.8 53.0 49.6 52.5 46.5 58.0 55.7 58.7 57.8 62.0 60.8 65.5 62.5 68.0 61.6 63.4 64.6 65.4 62.2 67.8 61.0 69.5 67.8 68.6 70.6 76.8 73.2 78.3 71.7 82.5 #Tok 762 1823 1894 1190 1479 1389 1167 1172 894 1880 1709 1801 2290 1643 1686 1581 Figure 5: The performance of the trained model on six diversity metrics. We evaluate I2B-LPO using Qwen2.57B and Qwen3-14B models across the GSM8K and MATH-500 datasets. For each metric, higher value indicates greater diversity. And the diversity metrics are calculated across 10 generated responses per prompt. Table 2: Diversity Analysis on GSM8K Benchmark. Accuracy is measured by Pass@1. Diversity is evaluated via Distinct-1, Distinct-4, 1Self-BLEU, and 1Self-ROUGE (higher is better for all metrics). Method Qwen2.5-3B Base 80/20 Entropy.Reg I2B-LPO Qwen2.5-7B Base 80/20 Entropy.Reg I2B-LPO Qwen3-14B Base 80/20 Entropy.Reg I2B-LPO Pass@1 Distinct-1 Distinct-4 Self-BLEU Self-ROUGE 85.7 89.1 86.8 92.8 91.6 93.5 93.8 95.6 92.5 94.2 93.4 97.8 0.20 0.26 0.32 0.35 0.28 0.35 0.57 0.69 0.33 0.37 0.56 0. 0.44 0.48 0.78 0.76 0.51 0.65 0.73 0.87 0.59 0.62 0.71 0.73 0.25 0.30 0.82 0.85 0.34 0.38 0.47 0.56 0.38 0.41 0.51 0. 0.28 0.36 0.78 0.81 0.39 0.45 0.81 0.76 0.43 0.45 0.57 0."
        },
        {
            "title": "6.1 Quality-diversity balance (RQ1)",
            "content": "In this section, we present fine-grained results on the diversity and quality. Quality. Tab. 1 reports Pass@n (n [1, 256]) and average response length. I2B-LPO consistently outperforms baselines across both backbones, with advantages widening as increases. Unlike EntropyReg., which artificially inflates entropy via excessive verbosity, I2B-LPO maintains better balance between quality and efficiency. Diversity. We present diversity metrics in Fig. 5 and Tab. 2. I2B-LPO consistently outperforms baseline models, demonstrating clear advantage in diversity. While entropy-based methods primarily boost lexical variation (Distinct-n), our approach achieves superior performance on semantic metrics (LLM-as-Judge and Self-BLEU)."
        },
        {
            "title": "6.2 Exploration Behavior Analysis (RQ2)",
            "content": "This section analyzes the distribution of exploratory tokens and entropy dynamics. Distributions of Exploratory Tokens. As categorized in Appendix A, high-entropy tokens fulfill distinct functional roles. Fig. 6 (a) compares the probabilityentropy distributions of exploratory tokens (but, however, thus, wait, let). The key observations are summarized as follows: (a) Probability-Entropy scatter plots (b) Entropy Dynamic comparison Figure 6: Joint Analysis of Entropy Dynamics. (a) Probability-Entropy scatter plots of five exploratory tokens from training samples at training step 500 on Qwen2.5-7B-Base, displaying random sample of 5% of all data points. (b) Average entropy dynamics across training steps on the MATH dataset. Figure 7: Attention head patterns contrasted between high (Level 9) and low (Level 3) difficulty on Deepmath. Red indicates heads activated by complex problems, while blue denotes heads responsive to simpler ones. In GRPO, tokens concentrate in high-probability, low-entropy regions, similar to (Ju et al., 2026). Adding an entropy loss does shift this distribution, but often leads to anomalously high entropy (>8) for some tokens. I²B-LPO maintains balanced distribution by activating exploratory tokens across wider entropy spectrum, preserving their reasoning utility without overfitting into deterministic patterns. Mitigation of Entropy Collapse. Fig. 6 (b) tracks entropy dynamics. While all methods show an initial drop, GRPO suffers from continuous decay, indicating severe entropy collapse. In contrast, I²BLPO stabilizes entropy levels after the initial phase. This confirms our method effectively preserves exploration capacity, preventing convergence toward deterministic patterns. Ablation Study of Latent Injection To verify whether latent injection fosters structured exploration rather than introducing random noise. We investigate three injection strategies: Early Fusion at input-level, Deep Fusion via PSA (detailed in Sec . 4.2.3), and Late Fusion at Softmax-layer. The formulations are described below: 1 Early Fusion at input-level: We inject the latent code zi by adding it element-wise to the input embedding h(xt) of each token xt. This broadcasts the prior across the sequence dimension: i(xt) = h(xt) + zi, = 1, . . . , T. (14) 2 Late Fusion at Softmax-layer: This strategy directly utilizes the projection of zi to influence the distribution of LLMs vocabulary. We first map the latent vector zi to logit adjustment vector pzi RV . This pzi is then superimposed onto the original logits to derive the final distribution: = + pzi , ˆy = softmax(p i), (15) where pzi biases the token selection probability ˆy. Analysis. Tab. 3.A presents ablation results of latent injection on qwen2.5-7B. Fig. 7 contrasts attention patterns for high- (Level 9) and low-difficulty (Level 3) problems on DeepMath (details in Appendix D). Key observations are: Q: Solve linear equation 2x + 4 = 10. Low IB (Redundant) A: To solve 2x + 4 = 10, we subtract 4. 2x = 6. Then divide by 2. So = 3. Let me double check. 2 3 + 4 = 10. The answer is 3. think it is 3. Yes, 3. High IB (Concise) A: Subtract 4 from both sides: 2x = 6. Divide by 2: = 3. The solution is 3 . (a) Performance Comparison (b) Case Study (c) Accuracy & Perplexity Trends Figure 8: Overview of Information Bottleneck (IB) Analysis. (a) Performance comparison on OlympiadBench. (b) Case study illustrating how higher IB scores correlate with more concise reasoning. (c) The impact of IB scores on model accuracy and perplexity. Input Injection: chaotic early activations diminish in deeper layers, diluted before reasoning. Softmax Injection: modifying logits externally hinders gradients from shaping internal attention, leading to scattered activations. PSA induces Structured Activation: it mobilizes difficulty-sensitive heads in final layers (24 27), consistent with (Shojaee et al., 2025; Cao et al., 2025). This confirms that PSA effectively engages deep reasoning without aimless randomness."
        },
        {
            "title": "6.3 Self-Reward Effectiveness (RQ3)",
            "content": "In this section, we benchmark IB metric against other self-rewards methods: Likelihood (Wu et al., 2016), Self-Consistency (Wang et al., 2023), Entropy.Reg (Chao et al., 2024), and LLMJudge (Zheng et al., 2024). As shown in Fig. 8(a), IB self-reward outperforms baselines across accuracy, diversity, and confidence (perplexity). Notably, it resolves the diversity-confidence trade-off observed in Entropy.Reg, which sacrifices confidence for diversity. Fig. 8(b) shows that higher IB scores correspond to concise reasoning chains, while lower scores identify redundant content. Fig. 8(c) further demonstrates that IB metric balances exploration and confidence by reducing perplexity while improving quality."
        },
        {
            "title": "6.4 Ablation of Main Components (RQ4)",
            "content": "Tab. 3 presents ablation results on the core components. The results indicate that (1) entropy-based branching outperforms other strategies by accurately targeting high-uncertainty nodes; (2) the full IB mechanism (loss + pruning) forms an exploreconverge loop, significantly reducing PPL by 11.7. (3) Overall, the complete framework elevates MATH Pass@1 from 54.4% to 81.5%, maintaining high diversity and low perplexity. Tab. 4 shows ablation results of key hyperparameters. PSA injection depth follows an inverted U-curve, peaking at the last 12 layers. The max_length of 8192 tokens provides sufficient capacity for complex reasoning."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper addresses the exploration collapse caused by the semantic homogeneity of reasoning paths. Our I²B-LPO framework enables structured exploration via entropy-driven latent branching and dual-purpose IB for simultaneous filtering and reward shaping. Extensive experiments confirm that I²B-LPO achieves SOTA accuracy and diversity, effectively overcoming the exploration bottleneck. This work provides principled and extensible framework for exploration-aware policy optimization in LLM reasoning."
        },
        {
            "title": "Limitations and Potential Risk",
            "content": "Limitations. First, entropy as an uncertainty measure may not consistently reflect semantic diversity in open-ended or non-mathematical tasks. Second, despite IB-based pruning, the branching mechanism remains more computationally intensive than single-rollout approaches. We aim to explore more efficient algorithms in the future. Potential Risk. Our method focuses on enhancing mathematical reasoning capabilities using established public benchmarks. As the proposed approach operates strictly within abstract symbolic domains and does not involve sensitive data or content generation, it poses minimal ethical risks or safety concerns."
        },
        {
            "title": "This study adheres to the ACL Code of Ethics and\nhas considered relevant ethical issues throughout",
            "content": "Table 3: Ablation Study of I²B-LPO Components on Qwen2.5-7B. We evaluate the contribution of core components: Branching Triggers, Latent Injection, and IB self-rewards. To isolate the impact of topological structures, Blocks and are conducted without the IB self-reward mechanism. Note: non-ablated components are fixed to optimal settings( = 7, = 8). denotes the PSA Fusion without the injection weight decay. Method AIME25 AIME24 MATH OlympiadBench Pass@1 Dist-4 PPL Pass@1 Dist-4 PPL Pass@1 Dist-4 PPL Pass@1 Dist-4 PPL GRPO Baseline 8.2 0.16 32.2 10.3 0. 27.8 54.4 0.40 17.6 44.9 0. 19.8 w/o Latent Injection + Input-Level Fusion + Softmax-Level Fusion + Norm Modulation + KV-Augmentation + PSA Fusion + PSA Fusion (Ours) vs. w/o Latent Injection w/o Branching (K = 0) + Random Branching + Likelihood Branching + Entropy Branching vs. w/o Branching w/o IB + IB Loss + IB Pruning I²B-LPO (Full Method) vs. w/o IB 10.2 10.6 10.8 11.0 11.2 11.6 11.8 +1. 8.6 9.1 9.4 10.2 +1.6 11.8 12.2 12.9 13.6 +1.8 A: Latent Injection Ablation (Fixed: Entropy Branching+ w/o IB) 0.34 0.38 0.41 0.43 0.44 0.45 0.47 +0.13 32.2 30.5 38.6 36.5 35.8 35.1 34.2 +2. 15.5 16.2 16.4 17.0 17.2 17.2 17.5 +2.0 0.39 0.48 0.45 0.46 0.47 0.48 0.49 +0.1 27.8 26.5 33.4 31.8 31.0 30.5 29.8 +2.0 59.7 61.2 63.5 64.2 65.1 65.4 66.7 +0.1 B: Branching Trigger Ablation (Fixed: PSA + w/o IB) 0.21 0.24 0.26 0.34 +0. 0.47 0.49 0.50 0.51 +0.04 30.1 31.9 32.0 32.2 +2.1 13.4 13.6 14.2 15.7 +2.3 0.23 0.34 0.37 0.39 +0.16 26.7 25.9 27.3 27.8 +1. 54.5 55.4 56.7 59.7 +5.2 C: IB Mechanism Ablation (Fixed: Entropy + PSA) 34.2 24.1 25.0 22.5 -11.7 17.5 18.1 18.3 18.6 +1. 0.49 0.52 0.51 0.54 +0.05 29.8 20.4 25.4 19.2 -10.6 66.7 78.3 79.6 81.5 +14. 0.64 0.68 0.66 0.72 0.74 0.75 0.78 +0.14 0.43 0.46 0.51 0.64 +0.21 0.78 0.76 0.80 0.82 +0.04 18.4 16.2 21.5 20.8 20.1 19.2 19.5 +1.1 17.3 17.8 18.1 18.4 +1. 19.5 14.2 15.6 13.1 -6.4 48.3 49.5 50.8 51.2 51.6 51.9 52.3 +4.0 45.3 46.7 47.9 48.3 +3.0 52.3 54.6 56.8 58.0 +5. 0.50 0.52 0.55 0.56 0.57 0.57 0.59 +0.09 0.35 0.40 0.45 0.50 +0.15 0.59 0.60 0.65 0.65 +0.06 19.5 19.8 25.4 24.5 23.8 23.0 22.1 +2.6 19.1 20.5 19.8 19.5 +0. 22.1 14.5 16.5 12.6 -9.5 Table 4: Hyperparameter Sensitivity Analysis. Blue indicates Injection Depth; Green indicates Branching Factor (K); Yellow indicates Max Response Length. The bold rows denote the chosen settings."
        },
        {
            "title": "OlympiadBench",
            "content": "Pass@1 Dist-4 Pass@1 Dist-4 PSA Injection Depth Last 4 Layers Last 8 Layers Last 12 Layers Last 16 Layers Last 20 Layers Last 28 Layers Branching Factor K=1 K=3 K=7 K=15 Max Response Length 2048 Tokens 4096 Tokens 8192 Tokens 76.5 79.8 81.5 81.1 80.4 78.2 75.5 78.7 81.5 82.8 68.2 79.5 81. 0.75 0.79 0.82 0.81 0.80 0.78 0.75 0.78 0.82 0.86 0.70 0.80 0.82 53.2 56.4 58.0 57.6 56.9 55.1 52.8 55.8 58.0 58.4 45.5 56.2 58. 0.58 0.62 0.65 0.64 0.63 0.61 0.54 0.60 0.65 0.69 0.55 0.63 0.65 the research process. All data are sourced from publicly available datasets and have been anonymized to protect privacy. The experimental process and results are reported truthfully, and reproducible code and resources are provided. We recommend implementing oversight and filtering mechanisms in practical applications. The authors have completed and submitted the Responsible NLP Research Checklist and are committed to maintaining integrity and transparency in the research."
        },
        {
            "title": "References",
            "content": "Daniel Adiwardana, Minh-Thang Luong, David So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc Le. 2020. Towards human-like opendomain chatbot. arXiv preprint arXiv:2001.09977. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, and 32 others. 2022. Constitutional ai: Harmlessness from ai feedback. Preprint, arXiv:2212.08073. Hongye Cao, Zhixin Bai, Ziyue Peng, Boyan Wang, Tianpei Yang, Jing Huo, Yuyao Zhang, and Yang Gao. 2025. Efficient reinforcement learning with semantic and token entropy for llm reasoning. Preprint, arXiv:2512.04359. Stephen Casper, Xander Davies, Thomas Shi, Claudia andz Benton, Michael Mozer, and 1 others. 2023. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research. Also available as arXiv:2307.15217. Chen-Hao Chao, Chien Feng, Wei-Fang Sun, ChengKuang Lee, Simon See, and Chun-Yi Lee. 2024. Maximum entropy reinforcement learning via energybased normalizing flow. Preprint, arXiv:2405.13629. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. 2025. Reasoning with exploration: An entropy perspective. Preprint, arXiv:2506.14758. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. 2025. The entropy mechanism of reinforcement learning for reasoning language models. Preprint, arXiv:2505.22617. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, and Yu Wu. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, and Changyou Chen. 2021. Transformerbased conditional variational autoencoder for controllable story generation. Preprint, arXiv:2101.00828. Jan-Philipp Fränken, Elad Zelikman, Rafael Rafailov, Kashyap Gandhi, Tobias Gerstenberg, and Noah D. Goodman. 2024. Self-supervised alignment with mutual information: Learning to follow principles without preference labels. In Advances in Neural Information Processing Systems, volume 37. Curran Associates, Inc. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 39213940. Association for Computational Linguistics. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced In International bert with disentangled attention. Conference on Learning Representations. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. Preprint, arXiv:2504.11456. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track. Yuxian Jiang, Yafu Li, Guanxu Chen, Dongrui Liu, Yu Cheng, and Jing Shao. 2025. Rethinking entropy regularization in large reasoning models. arXiv preprint arXiv:2509.25133. Feng Ju, Zeyu Qin, Rui Min, Zhitao He, Lingpeng Kong, and Yi R. Fung. 2025. Reasoning path divergence: new metric and curation strategy to unlock llm diverse thinking. Preprint, arXiv:2510.26122. Feng Ju, Zeyu Qin, Rui Min, Zhitao He, Lingpeng Kong, and Yi R. Fung. 2026. Reasoning path divergence: new metric and curation strategy to unlock llm diverse thinking. Preprint, arXiv:2510.26122. Shiye Lei, Zhihao Cheng, Kai Jia, and Dacheng Tao. 2025a. Revisiting llm reasoning via information bottleneck. Preprint, arXiv:2507.18391. Shiye Lei, Zhihao Cheng, Kai Jia, and Dacheng Tao. 2025b. Revisiting llm reasoning via information bottleneck. Preprint, arXiv:2507.18391. Jiwei Li, Dan Jurafsky, and Eduard Hovy. 2016. diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110119, San Diego, California. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311318, Philadelphia, Pennsylvania, USA. Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar. 2025. The illusion of thinking: Understanding the strengths and limitations of reasoning modarXiv, els via the lens of problem complexity. abs/2506.06941. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. 2025a. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. Preprint, arXiv:2506.01939. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. 2025b. Reinforcement learning for reasoning in large language models with one training example. Preprint, arXiv:2504.20571. Jianghao Wu, Yasmeen George, Jin Ye, Yicheng Wu, Daniel F. Schmidt, and Jianfei Cai. 2025. Spine: Token-selective test-time reinforcement learning with entropy-band regularization. Preprint, arXiv:2511.17938. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, and 1 others. 2016. Googles neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. Preprint, arXiv:2409.12122. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, volume 36. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. In Forty-first International Conference on Machine Learning. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2025. Self-rewarding language models. Preprint, arXiv:2401.10020. Jiayi Zhang, Simon Yu, Derek Chong, Anthony Sicilia, Michael R. Tomz, Christopher D. Manning, and Weiyan Shi. 2025a. Verbalized sampling: How to mitigate mode collapse and unlock llm diversity. Preprint, arXiv:2510.01171. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. 2025b. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. Preprint, arXiv:2504.05812. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track."
        },
        {
            "title": "Appendix",
            "content": "A Categorizing High-Entropy Tokens In Fig. 9, Top20% impactful updates primarily target high-entropy tokens. These tokens tend to produce larger gradients during backpropagation. This indicates that progress in this stage is mainly driven by resolving uncertainty at critical forks in reasoning paths (Wang et al., 2025a). In RLVR, tokens generated by models exhibit different functional roles that collectively drive the reasoning process. Based on their operational characteristics, we categorize tokens into three roles: Logical Structuring Tokens: Govern reasoning flow (e.g. causal, contrastive, progressive, and parallel connectors). They help structure multistep argumentation or explanations. Metacognitive Tokens: Reflect meta-cognitive functions, especially self-monitoring behaviors (e.g. verifying, summarizing, and revising). These tokens actively guide the reasoning process through reflective adjustment and solution refinement. Semantic Support Tokens: Provide linguistic elements that ensure fluency, coherence, and informativeness (e.g. core grammatical elements, domain-specific entities). We provide examples of each category in Table 5."
        },
        {
            "title": "Examples",
            "content": "Logical Structuring Causal (e.g. therefore, contrastive because), (e.g. however, but), progressive (e.g. first, next, finally), and parallel (e.g. and, also)."
        },
        {
            "title": "Semantic Support",
            "content": "Verifying (e.g. Lets revising (e.g. check), Wait), Correction, summarizing (e.g. In summary), and planning (e.g. First, will...). the, is, entities Grammatical (e.g. domain problem, adjectives and correct, final). elements of), (e.g. solution), (e.g. Table 5: Examples of Token Categories in RLVR. Figure 9: Token entropy and gradient distribution."
        },
        {
            "title": "B Implementation Details of the CVAE",
            "content": "In this section, we provide the architectural specifications and the training objective of our CVAE. Network Architecture: typical CVAE consists of three parts: (1) The Encoder qϕ(z x, c) approximates the posterior distribution of the latent variable z. (2) The Prior Network pθ(z c) models the prior distribution of z, which is conditioned only on the context c. (3) The Decoder Network pθ(y z, x) takes and to decode the output y. Specifically, we employ DeBERTa-v2large (He et al., 2020) as the Encoder qϕ(zx, c). DeBERTa is chosen for its disentangled attention mechanism, which effectively encodes both content and relative positions. Formally, given the input sequence x, we first extract the contextualized hidden states via the encoder. To obtain fixed-size vector representation hx Rdmodel, we apply mean pooling over the token embeddings: hx = MeanPool(DeBERTa(x)), (16) where dmodel = 1024 for the large architecture. To construct the probabilistic latent space, hx is projected into the variational parametersmean µ and log-variance log σ2via two separate linear transformations: µ = Wµhx + bµ, log σ2 = Wσhx + bσ, (17) where W() Rdzdmodel are learnable projection matrices and dz is the dimension of the latent bottleneck (e.g., dz = 128). Finally, the latent variable is sampled using the standard reparameterization trick: = µ + σ ϵ, ϵ (0, I). (18) This latent code serves as compact semantic anchor, which is subsequently fused into the decoder (LLaMA) to guide the generation of output y. Training Objective Formally, the CVAE is trained by maximizing the Evidence Lower Bound: LELBO = LREC LKL = Eqϕ(zx,y)[log pθ(yz, x)] KL(qϕ(zx, y)pθ(zx)) log p(yx), (19) where LREC denotes the reconstruction loss, and LKL represents the Kullback-Leibler divergence between the posterior and prior distributions. The CVAE is trained on MATH and GSM8K training sets. IB-Aware Scoring Metric General Theory. Information Bottleneck (IB) formalizes the trade-off between compressing input into compact representation ˆX while maintaining predictive power for target , via the Lagrangian objective: LIB_naive = I(X; ˆX) βI( ˆX; ), (20) where I(; ) denotes Mutual Information (MI) and β controls the trade-off. The MI between and is defined as: I(X; ) = H(X) H(XY ), (21) where H() denotes entropy. high value of I(X; ) indicates that knowledge of one variable reduces uncertainty about the other. Derivation of the IB-Score. Following the IBAware Reasoning Optimization, the optimization objective in LLM reasoning is formulated as: To derive tractable metric, we decompose these two terms respectively. Minimizing Complexity I(q; r). We first expand I(q; r) using the definition of mutual information and the assumption that the marginal entropy of reasoning H(r) is invariant under the policy π (Assumption 1 in (Lei et al., 2025b)). Applying the chain rule of entropy to the autoregressive trajectory = (o1, . . . , oT ), we obtain: I(q; r) = H(r) H(r q) = H(r) (cid:88) t=1 H(ot o<t, q). (23) H(ot Therefore, minimizing the mutual information I(q; r) is equivalent to maximizing the conditional entropy sum (cid:80) o<t, q). This term encourages the model to maintain high entropy during generation, preventing it from collapsing into memorized or over-deterministic patterns dependent solely on the specific prompt phrasing. Maximizing Informativeness I(r; a). For the second term, we have I(r; a) = H(r) H(r a). Maximizing this quantity requires minimizing the conditional entropy H(r a). In our specific selection phase, we restrict our candidate pool to the validity-verified subset Rcorrect = {r ˆa(r) = a}. For any path Rcorrect, the reasoning implies the answer with certainty (i.e., p(ar) 1). Under this condition, the posterior probability of the reasoning path answered can be approximated by the likelihood of the path itself: p(r q, a) = p(a r, q)p(r q) p(a q) p(r q). (24) Consequently, minimizing the uncertainty of the reasoning answered (H(r q, a)) corresponds to selecting trajectories that maximize the loglikelihood log π(r q). High-likelihood paths within Rcorrect represent the most confident reasoning traces that lead to correct solution. The IB-Score formulation. Combining the derivations above, we transform the trajectory-level IB objective into tractable token-level utility function. By substituting the log-likelihood for the informativeness term and the token-level entropy for the complexity term, we propose the IB-Score for reasoning path r: H(ot o<t, q) (cid:124) (cid:123)(cid:122) (cid:125) Exploration Proxy (Max I(q;r)) βH(ot o<t, q, a) (cid:123)(cid:122) (cid:125) Relevance Proxy (Min I(r;a)) (cid:124) λtH(ot o<t, q) AtH(ot o<t, q) (cid:88) t=1 (cid:88) t=1 (cid:88) t=1 (25) = = We formulate the Information Bottleneck objective as maximizing score SIB. This score balances exploration (Standard Entropy) against relevance LIB = I(q; r) βI(r; a). (22) SIB(r) = (Negative Conditional Entropy): Localization of Difficulty-Sensitive SIB(r) = = (cid:88) t=1 (cid:88) t=1 (H(ot o<t, q) βH(ot o<t, q, a)) (cid:124) (cid:123)(cid:122) (cid:125) sIB : Per-token IB Score (26) sIB Let Ht = H(ot o<t, q) and Hta = H(ot o<t, q, a). We set β = 2 as per the theoretical analysis. The per-token score becomes: sIB = Ht 2Hta (27) Using the inequality 0 Hta Ht, we analyze the bounds of sIB : Hta = 0 = sIB Best: Worst: Hta = Ht = sIB = Ht = Ht (28) (29) Thus, the score term lies in the symmetric interval: sIB [Ht, Ht] (30) We can therefore represent sIB tropy term: as modulated ensIB = λtHt, where λt [1, 1] (31) Now we map the coefficient λt to the Reinforcement Learning context: High Score (λt 1): Occurs when the token is perfectly predictive. This corresponds to \"Good\" action. Low Score (λt 1): Occurs when the token provides no information about the answer. This corresponds to \"Bad\" action. The Advantage function At naturally captures this property: high positive At for good actions, negative At for bad actions. Thus, we can directly approximate λt with the advantage (without any negative sign): λt At (32) Substituting λt At back into the summation: SIB(r) = = = (cid:88) t=1 (cid:88) t=1 (cid:88) t=1 sIB λtHt AtH(ot o<t, q)"
        },
        {
            "title": "Attention Heads",
            "content": "Fig. 7 depicts attention head patterns contrasting high (Level 9) vs. low (Level 3) difficulty on DeepMath (He et al., 2025), dataset spanning 13 difficulty levels (3.0 to 9.0). Let RBLN denote the output tensor of the multi-head attention (MHA) layer, where is the batch size, is the sequence length, is the number of attention heads, and is the head dimension. The final contextual representation is typically obtained by projecting the concatenated output of all heads via the output projection matrix Wo R(N d)D: = Reshape(H)W RBLD, (34) where = represents the models hidden dimension. To analyze the independent contribution of the i-th attention head (i {1, . . . , }), an ablation strategy is employed. An ablated representation (i) is constructed by retaining the output of the i-th head while zeroing out all other heads: (i) b,ℓ,j,: = (cid:40) Hb,ℓ,i,: 0 if = otherwise , (35) [B], ℓ [L], [N ] Subsequently, the projected representation Z(i) is computed using Wo. We extract the embedding of the last token for each sample b, denoted as = Z(i) z(i) b,L1,: RD. D.1 Difficulty Scoring Using the pre-trained probe direction vdif , the difficulty score contribution of the i-th head for sample is calculated as the normalized projection onto the difficulty direction: s(i) = z(i) , vdif vdif 2 . (36) For batch of samples sharing the same groundtruth difficulty level, the mean head-wise attribution is aggregated: (33) s(i) ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) b=1 s(i) . (37) D.2 Differentiation Score and Localization"
        },
        {
            "title": "Optimizer\nPolicy learning rate",
            "content": "Training batch size Samples per prompt Rolloutn Branching γ Max response length Max prompt length Rollout temperature Clip range ϵlow, ϵhigh α for IB loss AdamW 1e6 for qwen2_5 series 5e6 for qwen3 series 128 8 4 7 8 0.003 8192 2048 1.0 0.2, 0.28 0.005 Table 6: Hyperparameters of GRPO and DAPO training To determine whether specific head is sensitive to hard or easy problems, the Differentiation Score is computed as the difference between the mean attributions of high-difficulty (Hard) and low-difficulty (Easy) cohorts: (i) = s(i) hard s(i) easy (38) If (i) is significantly positive: The head is activated to identify difficult problems. If (i) is significantly negative: The head is activated to identify simple problems. By computing (i) for all heads, the specific components responsible for difficulty perception can be precisely located."
        },
        {
            "title": "E Additional Implementation Details",
            "content": "Perplexity (PPL) measures uncertainty of the generated sequence as: (cid:32) PPL(oi) = exp"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) t=1 log π(oi oi <t) (cid:33) (a) Naive Entropy Regularization. oi where π(oi <t) is next-token probability. LowPPL responses are generally more fluent and semantically coherent (Adiwardana et al., 2020). RL Training Configuration For both GRPO and DAPO, we use the hyperparameters in Tab.6, without using entropy or KL losses. Specifically, Branching maxtimes caps trajectory branching at 4, while Samples per prompt denotes the maximum few-shot CoT demonstrations. Experiments on Qwen2.5-7B run on 8H100 GPUs, taking 50 hours for one epoch. Experiments on Qwen314B run on 16H100 GPUs, taking 64 hours for one epoch. Word Cloud The Entropy Reg. outputs more generic syntactic fillers (e.g., is, to, can). I2BLPO produces more reasoning-focused keywords. I2B-LPO achieves Response length analysis. balanced response length, sitting between GRPO and Entropy. Reg. It avoids the superficial verbosity and non-informative filler observed in Entropy. Reg, while demonstrating more comprehensive reasoning steps compared to GRPO. (b) I2B-LPO (Ours). Figure 10: Word cloud comparison on AIME2024 using Qwen2.5-7B. The Entropy Reg. outputs more generic syntactic fillers (e.g., is, to, can). I2BLPO produces more reasoning-focused keywords. Ablation of weight decay in PSA. We implement an exponential decay schedule for the injection factor γ(t), annealing the strength from 5 102 down to 5 104. This experiment is conducted on MATH datasets with qwen2.5-3B base model. The Gaussian icons (Strong, Medium, Weak) visually represent the diminishing intensity of the latent variable over time. Data Pre-processing To focus training on the learning frontier\", i.e, problems neither trivial nor intractable, we filter the dataset using two reference models (Qwen2.5-7B and Qwen3-8B) with Figure 11: Response length analysis on Qwen2.5-3B. (a) DAPO-Math-17k Figure 12: Ablation study on the dynamic decay of latent injection. (b) MATH Figure 13: Response length distribution results on DAPO and MATH datasets. = 8 rollouts each. We exclude: (1) trivial samples consistently solved by both models, as they provide negligible gradient signals for policy improvement; and (2) intractable samples where both models fail, specifically targeting excessively long responses (>3,000 tokens for Qwen2.5-7B, >10,000 for Qwen3-8B) to mitigate hallucination loops. The response length distributions of the resulting filtered datasets are illustrated in Figure 13. The final training set consists of 6,486 samples from MATH and 13,583 from DAPO."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Northeastern University",
        "Shanghai Jiao Tong University",
        "University of Science and Technology of China",
        "Zhejiang University"
    ]
}