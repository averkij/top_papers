{
    "paper_title": "The Promise of RL for Autoregressive Image Editing",
    "authors": [
        "Saba Ahmadi",
        "Rabiul Awal",
        "Ankur Sikarwar",
        "Amirhossein Kazemnejad",
        "Ge Ya Luo",
        "Juan A. Rodriguez",
        "Sai Rajeswar",
        "Siva Reddy",
        "Christopher Pal",
        "Benno Krojer",
        "Aishwarya Agrawal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 2 9 1 1 1 0 . 8 0 5 2 : r a"
        },
        {
            "title": "The Promise of RL for Autoregressive Image Editing",
            "content": "Saba Ahmadi1 Rabiul Awal1,2 Ankur Sikarwar1,2 Amirhossein Kazemnejad1 Ge Ya Luo 1,2 Juan A. Rodriguez1,4,6 Sai Rajeswar1,2,6 Siva Reddy1,3,6,7 Christopher Pal 1,5,6,7 Benno Krojer1,3 Aishwarya Agrawal1,2,7 1Mila Quebec AI Institute 2Université de Montréal 3McGill University 4École de Technologie Supérieure (ETS) 5Polytechnique Montréal 6ServiceNow 7Canada CIFAR AI Chair"
        },
        {
            "title": "Abstract",
            "content": "While image generation techniques are now capable of producing high quality images that respect prompts which span multiple sentences, the task of text-guided image editing remains challenge. Even edit requests that consist of only few words often fail to be executed correctly. We explore three strategies to enhance performance on wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in unified manner. We find RL combined with large multi-modal LLM verifier to be the most effective of these strategies. As result, we release EARL: Editing with Autoregression and RL, strong RL-based image editing model that performs competitively on diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL."
        },
        {
            "title": "Introduction",
            "content": "With internet-scale image-text pairs [43] and diffusion models [41, 39, 24] we have seen impressive progress on open-ended image generation in recent years. At this point, the latest text-to-image models can often adhere to detailed prompts that span several sentences [5, 14]. However, synthesizing images from prompt alone is often not sufficient for end-users and broader ML applications. In reality, person might want to alter highly specific details in given image instead of creating one from scratch. Beyond direct user applications, e.g. in the domains of robotics and planning, one might want to imagine into the future with an image editing model acting as simulator [62, 27, 8], e.g. how does this scene look like if the robot pushes the mug?. In both cases, the model must faithfully preserve all details of the original image while modifying the elements intended for editing. From capability perspective, most current editing models [64, 9, 60] cover arguably simpler object and attribute edits (replace, change color, add, ...), yet only few works [27, 47] tackle more complex edits that require e.g. action understanding or reasoning (spatial, counting, physical dynamics). From modeling perspective, standard recipe to improve editing is to apply supervised fine-tuning (SFT) to diffusion-based image generation model [9, 64, 60], rarely incorporating more recent posttraining methods such as reinforcement learning (RL). parallel line of work introduces additional bounding-box conditioning, either explicitly provided by the user [31] or implicitly predicted [17], leaving these methods far from an end-to-end solution. Hence, in this paper we ask: What is the most effective approach to address both simple and complex edits with unified end-to-end model? And specifically, what are the key learning paradigms that can move the field forward? denotes equal contribution To this end, we conduct series of experiments with three different learning paradigms (SFT, RL and chain-of-thought reasoning) and mixes of data. However diffusion-based models would not directly allow consistent setup where all training approaches could be plugged in out of the box. For this reason we choose Emu3 [53] as our starting point, fully autoregressive generative model which was pre-trained on captioning and image generation. Since Emu3 is unified image and language generation model, we can easily use it to study CoT reasoning and online RL methods such as GRPO [44], on top of SFT. We note that RL for visual generation with diffusion or flow-matching is non-trivial [7, 32]. At the end of our exploration, we arrive at simple recipe and propose EARL: Editing with Autoregression and RL, fully autoregressive generative model that trains on simple and complex editing data during the SFT and RL post-training stages. Specifically, we find the combination of GRPO with strong MLLM-based verifier to be the most effective. While variants of CLIP-Score are more commonly used verifiers [30, 32], they often require fine-tuned on preferences and lack fine-grained understanding [63, 1]. Instead, we identify large MLLMs with fine-grained understanding such as Qwen2.5-VL-72B as effective verifiers for broad range of edits. We empirically show that EARL performs well across many types of edits by evaluating on 6 diverse benchmarks in both IID and OOD settings. We achieve better results than prior state-ofthe-art models on the OmniEdit [55], AURORA [27], and VisMin [2] benchmarks. Moreover, our method outperforms the strongest prior work [60], despite using five times less data, and also surpasses baselines that use comparable amount of data, such as [27]. See Fig. 1 for samples from EARL before and after applying our RL recipe, showing improved performance even on challenging tasks that require spatial understanding. Notably, modeling textual and visual generation as one autoregressive stream has emerged as new exciting paradigm [53, 48, 49, 59], and we push the frontier of such models for the image editing task, outperforming prior related work EditAR [38]. Finally, we highlight surprising finding from our in-depth analysis of different training paradigms: teaching the model to explicitly reason about the intermediate steps (chain-of-thought style reasoning) before generating the actual edit does not seem to improve performance, and sometimes it even hurts. Our contributions are as follows: 1. We release EARL: unified end-to-end editing model that performs well on the whole spectrum of edit types. 2. EARL outperforms the strongest open-source diffusion baseline (Omnigen [60]), while also outperforming more comparable fully autoregressive editing model EditAR [38]. 3. simple yet effective online RL pipeline for training an autoregressive editing model. 4. systematic analysis of how training objectives interplay and at what stages to bring in simple vs. complex edits. The results interestingly reveal that various CoT reasoning settings do not bring any clear improvements. Our findings also show that complex edits are not beneficial during the SFT stage but are effective in the RL post-training stage."
        },
        {
            "title": "2 Related Work",
            "content": "Image Editing Models. Enabled by the development of diffusion models with open-ended textconditioning for image generation [42, 39], image editing models can now receive any text prompt in similar manner and typically depend on these diffusion models [9, 18]. Research [42, 35] has demonstrated that models such as Stable Diffusion [41] can be zero-shot transformed into an editing model by modifying the sampling procedure [35] or attention maps [23]. To achieve better results, components, such as additional input U-Net channels, are added to pre-trained image generation models followed by fine-tuning on curated editing data [64, 55, 3]. These training datasets consist of triplets of input image, an edit instruction describing the change, and ground-truth edited image as output. These triplets rarely occur naturally (i.e. only in forums [3]) and therefore need to be sourced from synthetic image generation pipelines [23], human-in-the-loop annotation [64], videos [27, 47] or simulation [27, 36]. Various works also adopt more structure into the editing task by restricting the model to edit certain regions of the image [12] or conditioning on bounding boxes or keypoints [37]. In more recent line of work, image generation is learned jointly with text generation in unified multimodal models that autoregressively predict arbitrary sequences of textual and visual tokens [53, 48]. Concurrent with our work, BAGEL [13] trains unified transformer model that integrates LLMs and diffusion models, achieving strong editing performancewith and without reasoningthrough large-scale pretraining and powerful base model. However, autoregressive models are less explored for image editing as these models remain less powerful than their diffusion 2 Figure 1: Qualitative comparison between SFT-only and EARL across diverse editing instructions. EARL extends the SFT model by leveraging reinforcement learning to better align image edits with natural language prompts. While both models handle simple edits reasonably well, EARL exhibits clear improvements in precise editing on simple as well as complex edit instructions. Simple edit instructions are shown in blue, and complex edit instructions are shown in pink. counterparts. Recent work [38] explores using autoregressive models for image-editing and achieves competitive results with diffusion baselines. However, they only study the SFT training paradigm, while we study SFT, RL post-training, and CoT Reasoning, cover wide range of edit types and outperform their results. Reasoning in Image Generation. LLM reasoning has been adapted to enhance image generation models through additional conditioning or planning [58]. Models like GLIGEN [31] and LayoutGPT [16] use LLMs to predict bounding boxes and scene layouts to direct object placement before generation. In these works, the layout generation step is unimodal, relying solely on LLMs. However, for image editing tasks, incorporating multimodal information from both the original image and the edit instruction is essential to determine an effective layout. GoT (Generation Chain-of-Thought) [15] applies CoT to visual generation and editing tasks. It first generates reasoning in text, analyzing semantic and spatial relationships in the input image, before generating the edited images using diffusion model. Additionally, PARM++ (Potential Assessment Reward Model++) [22] introduces reflection mechanism to self-correct generated images, further enhancing the models reasoning capabilities. Another approach focuses on improving the prompts used for image editing by utilizing large language model (LLM) or multimodal LLM (MLLM) [18], which is better equipped for both text and image understanding. While previous work has applied reasoning to image editing tasks, our approach systematically explores and evaluates chain-of-thought (CoT) reasoning in different settings involving simple and complex edits. RL for Image Generation. Reinforcement learning has emerged as powerful tool for finetuning image generation models, particularly diffusion models, to better align with human preferences [6]. Preference-based methods such as Diffusion-DPO [51] and D3PO [61] bypass explicit reward models by directly learning from pairwise human feedback. DDPO [6] further adapts diffusion models to hard-to-specify objectives such as aesthetic quality and compressibility using multimodal models based rewards (prompt-image alignment). Although RL for image generation is gaining momentum through human preference and multimodal reward signals, its application to image editing remains underexplored. HIVE [66] collects human feedback on edited images to learn reward functions that capture user preferences, but such datasets are costly and difficult to scale. InstructRL4Pix [30] addresses this challenge by using attention-based reward signals for localized, instruction-driven editing. Meanwhile, GRPO has demonstrated stable and efficient training of autoregressive large language models. Concurrent work such as Flow-GRPO [32] and SimpleAR [52] apply GRPO to 3 Figure 2: Autoregressive Image Editing Approaches. In supervised fine-tuning (SFT), we train an autoregressive model based on the standard image editing setup: triplets of source image, edit instruction, and target image. In SFT with reasoning, the model is supervised to generate chainof-thought (CoT) reasoning traces before generating the final edited image. Finally, we study reinforcement learning (RL) training of the SFT checkpoint, using edit quality verifiers as reward signals. flow matching and autoregressive models, respectively. Building on this success, we adopt GRPO for unified image editing with the autoregressive Emu3 model [53]. We leverage strong multimodal model [40] as reward function, utilizing its robust image-text alignment and zero-shot prompting to specify and evaluate editing preferences effectively."
        },
        {
            "title": "3 Training Paradigms for Autoregressive Image Editing",
            "content": "We formally describe the three training paradigms we explore: SFT, RL, and CoT Reasoning. All our experiments build upon the Emu3 base model [53], large autoregressive multimodal model that unifies image and text generation in single autoregressive stream and is trained from scratch. In the image editing task, the input consists of an original image and textual edit instruction, and the output is the corresponding edited image. Following Emu3, the original and edited images are tokenized into vision tokens using an image tokenizer, while the edit instruction is tokenized into language tokens. These vision and language tokens are then modeled jointly by single causal transformer (i.e. LLaMA-style architecture [50]). The task is framed as sequence-to-sequence generation: the model takes as input token sequence [x1, , xM ], formed by concatenating the vision tokens of the input image with the language tokens of the instruction, and generates an output sequence ˆy = [ˆy1, , ˆyT ], where = [y1, , yT ] represents the ground truth vision tokens of the edited image which is used for teacher forcing. Below, we detail the learning objective for each of the training paradigms we explore (Figure 2): Supervised Fine-Tuning (SFT) We employ the standard next-token prediction objective to process interleaved image-text sequences. Training minimizes the cross-entropy loss: L(θ) = E(x,y)D (cid:35) log πθ(yt y<t, x) , (cid:34) (cid:88) where πθ denotes the model, and is the labeled training dataset consisting of edit instructions paired with ground truth edited images. Reinforcement Learning (RL) Post-Training We use Group Relative Policy Optimization (GRPO) [45] in our RL pipeline to post-train the model following the initial SFT stage. GRPO initializes trainable policy model πθ and frozen reference model from the SFT checkpoint. For given input prompt x, the model generates group of responses y1, y2, . . . , yG based on the current policy 4 model πθold . The optimization goal is to maximize the following objective: (θ) = Ey1,...,yGπθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi (cid:88) min (cid:18) πθ(yi,t yi,<t) πθold(yi,t yi,<t) ˆAi, clip (cid:18) πθ(yi,t yi,<t) πθold (yi,t yi,<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:19) ˆAi βKL [πθπref ] (cid:35) . Here, Ai = (ri µ)/σ denotes the advantage of response yi, where ri is the reward of the i-th response. µ and σ are the mean and standard deviation of rewards across the response group {yi}.2 The hyperparameters ϵ and β control the clipping range and the KullbackLeibler (KL) penalty, respectively. To compute the reward ri, we first detokenize the vision tokens in yi back into an image. The reward is then computed using the MLLM verifier, which evaluates the quality of the image editing. This RL objective optimizes the model to generate higher-quality edits while maintaining training stability via the KL divergence term. For detailed overview of the GRPO method, see the pseudocode in Appendix B. RL Verifier: Any MLLM with strong image understanding capabilities can be used for verification. In particular, we use Qwen2.5-VL-72B [4] as our verifier to evaluate the generated edits based on the following criteria from VIEScore [28]: (1) Edit Success whether the intended modification was accurately applied; (2) Overedit whether any unintended changes were introduced; (3) Natural Look how well the edit blends with the original image; and (4) Artifacts whether the image contains visual distortions or anomalies. For criteria 1 and 2, the inputs to the model are the edit instruction, input image, and edited image. For criteria 3 and 4, only the edited image is provided. The individual scores are then aggregated into single reward signal, ranging from 0 to 10. Chain-of-Thought (CoT) Reasoning Explicitly generating intermediate reasoning steps before the final output is widely adopted technique that improves performance on complex reasoning tasks [21, 57]. This is known as Chain-of-Thought (CoT) reasoning. We extend CoT reasoning to the field of image editing. To teach CoT reasoning to Emu3, we finetune Emu3 with CoT supervision by prepending the response with tokenized reasoning chain (see Section 4.1 for details of synthesizing ground-truth reasoning traces)."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We describe the training details and evaluation setup below. 4.1 Training Details Implementation Our base model is Emu3-8B [53], state-of-the-art autoregressive multimodal model with unified image-text generation capabilities. For SFT, we initialize from BAAI/Emu3-Stage1 weights, set learning rate of 1e 4, an effective batch size of 128 with 4 GPUs, per-device batch size of 4, and 8 gradient accumulation steps. We use validation loss to stop training. For RL post-training, we use KL divergence coefficient of 3e 4 and learning rate of 3e 6. The RL model is trained with 8 rollouts per edit instruction and batch size of 128 and training continues until the reward plateaus. To enhance training stability, we adopt fully online policy gradient approach, performing single gradient update at each RL step [26]. All images are resized to 256 256 by maintaining their original aspect ratio. Training Datasets Our dataset is divided into two categories based on the complexity of the edits: Simple Edits (S) and Complex Edits (C). Simple Edits (S): This category includes relatively simple local edits such as single-object and attribute changes, as well as global edits such as style and environment changes. These types of edits are common in large-scale synthetic datasets, such as OmniEdit [55] with 750k samples. Existing models generally perform well on these tasks. Complex Edits (C): These edits involve more advanced operations, including counting, spatial, and action modifications, where current models often struggle. Datasets like Aurora-AG [27], Aurora-Kubric [27], VisMin [2], and Something-Something v2 [20] contain such challenging edits. Additionally, we use real-world edit requests curated with human-in-the-loop guidance, such as 2We omit the dependence on for brevity. Table 1: Datasets, covered edit types, and their share of the full training corpus. Dataset Type Sub-type OmniEdit [55] HumanEdit [3] MagicBrush [64] VisMin [2] Aurora-Kubric [27] Aurora-ActionGenome [27] SomethingSomething-v2 [20] Object (add, remove, replace), Attribute (modify), Scene, Style Object (add, remove, replace, relation, action counting) Object (add, replace, remove), Attribute (modify), OCR (modify), Action (modify) Object (add/remove), Attribute (replace), Count (add/remove), Spatial (swap) Spatial, Counting, Attribute Human Pose (action) Action Size 750K 4.6K 8.7K 50K 50K 7.8K 50K Figure 3: Example of generating step-by-step reasoning with Qwen2.5-VL-72B, where standard editing data (input image, edited image, edit instruction and bounding box) is used to produce structured reasoning chain for the editing process. Human-Edit [3] and MagicBrush [64], which include complex object/attribute changes. Data in the complex edit category is significantly scarcer, e.g. MagicBrush has 8K samples. We provide details for each dataset in Tab. 1. Our dataset comprises of 750K OmniEdit samples, while combines the above mentioned datasets with 171K unique samples. For datasets in with fewer than 50K samples, we upsample them to 50K, resulting in dataset of size 300K for supervised fine-tuning. For RL post-training, we randomly sample from the respective data pool (S or C) at each iteration, using 16 unique samples per step with 8 rollouts per sample. We first experiment with smaller setup using pool of 1,600 samples for various ablations. To improve further, we then train with total of 32k samples over the course of training. This provides more diverse data, allowing the model to benefit from broader coverage across edit types. For CoT reasoning supervision, we generate chain-of-thought data using multimodal large language model (MLLM), Qwen2-VL[4], following prompting strategy similar to [15]. The input to the MLLM consists of standard editing data: an input image, an edited image, textual edit instruction describing the desired change, and bounding boxes specifying the edit region (if available). For action edits, we also include person keypoints. The generated CoT data follows step-by-step structure, including description of the input image, bounding box coordinates of objects to be edited in the input image, bounding boxes of objects to be added in the target image, the edit action, and description of the final edited image. For complex edit datasets, we apply few-shot prompting to synthesize CoTs, while for the simple edit dataset (S), we reuse CoTs from [15]. An example is shown in Fig.3, and our prompting templates and full examples are provided in the Appendix C. 4.2 Evaluation Setup We evaluate on diverse suite of image editing tasks, ranging from simple object, attribute, and style modifications to complex editing tasks such as counting, actions, and spatial relations. Evaluation Metric We adopt VIEScore [28] as our metric since it outperforms traditional metrics like LPIPS [65] in terms of human correlation (0.3821 for GPT-4o versus 0.1142 for LPIPS [28]). VIEScore assesses edits based on four criteria as explained in Section 3 (RL Verifier). The final score ranges from 0 to 10. For this evaluation, we use GPT4o-mini due to its high quality and cost efficiency, and confirm that GPT4o-mini-variant aligns with human judgment not only on edits from the original human study [28] but also on various complex edits; details are provided in Appendix 6 D. Note that while our metric and reward are both based on VIEScore, we use separate MLLMs for evaluation (GPT4o-mini) and RL verifier (Qwen2.5-VL-72B), to reduce the risk of metric hacking, i.e. overfitting to MLLM-specific biases. Evaluation Benchmarks We evaluate on wide range of existing image editing benchmarks spanning both simple (OmniEdit [55] and EmuEdit [46]) and complex edits (MagicBrush [64], Aurora [27], and I2EBench [34]). Notably, I2EBench and EmuEdit serve as out-of-distribution (OOD) benchmarks, as neither dataset is used during training. EmuEdit includes edit types that are similar to those seen during training but come from different distribution. I2EBench further increases the distributional shift by containing both seen editing types and unseen ones not covered in any training dataset, such as lowlight enhancement. We also repurpose the fine-grained image understanding benchmark VisMin [2] which consists of pairs of minimal-change images and associated captions for each image into an image editing benchmark by creating edit instructions from the captions (see Appendix C.2.2 for details). These datasets provide wide range of edit types and complexities, facilitating robust evaluation of our models performance. For I2EBench and EmuEdit, we use smaller subset of 1000 samples of the original benchmark to reduce the cost of VIEScore evaluation by GPT4o-mini. Baselines We compare our model to diffusion-based baselines, including MagicBrush [64], InstructPix2Pix [9], Aurora [27], and Omnigen which is the SOTA image editing model [60]. We also compare with EditAR [38], which, to the best of our knowledge, is the only fully autoregressive image editing model."
        },
        {
            "title": "5 Results",
            "content": "We present series of experiments exploring different training paradigmsSupervised Fine-Tuning (SFT), Reinforcement Learning (RL), and Chain-of-Thought Reasoningalongside the addition of complex training data. 5.1 Teaching Emu3 Image Editing with Supervised Fine-Tuning Simple Editing The first row of the Supervised Fine-Tuning section in Tab. 2 presents the results of SFT trained on simple data (SFT (S)). It achieves the highest score on OmniEdit (5.73) and achieves an average score of 3.88, outperforming MagicBrush (3.32) and InstructPix2Pix (3.26), but underperforming Aurora (4.17) and Omnigen (4.70). Notably, Omnigen benefits from stronger base model with superior image generation performance (GenEval[19] 0.70) and large-scale finetuning ( 4M image editing samples). In contrast, our model finetunes from weaker base model (Emu3, GenEval 0.64) and uses significantly less data (750K image editing samples), resulting in lower overall performance. Also, we see that all models including Omnigen perform significantly worse on complex editing benchmarks compared to simple editing benchmarks, highlighting challenges in spatial edits, changes in object count, and human actions. Our trained SFT (S) follows this trend. In the following sections, we explore ways to improve SFT performance on both simple and complex edits. Complex Editing To improve the ability to handle complex edits, we explore two SFT strategies: joint training on the combined simple and complex edit data (SFT (S+C)) and two-stage curriculum that first fine-tunes on simple edits, then on complex edits (SFT (S+C) two-stage). As shown in Table 2, joint training (SFT (S+C)) reduces average performance compared to simple-only finetuning (SFT (S)) from 3.88 to 3.32 across both simple and complex editing tasks. In particular, the performance drop is significant on simple edit benchmarksdropping from 5.73 to 4.64 on OmniEdit and from 3.66 to 2.89 on EmuEdit. We hypothesize that this degradation is due to the large distributional shift between simple and complex edits; mixing them early in training may hinder the models ability to generalize across either. In contrast, the two-stage curriculum partially recovers average performance (3.69) and improves results on some complex-edit benchmarks (e.g., VisMin, MB). This suggests that allowing the model to first acquire basic editing capabilities from simple data makes subsequent fine-tuning on complex tasks more effective, especially given that Emu3 has not been exposed to any editing data during pretraining. Overall, the SFT results show that supervised finetuning is insufficient to effectively learn complex editing tasks. 7 Table 2: SFT and RL model variants for image editing fine-tuning and post-training respectively. stands for data used in simple editing types, and stands for data from complex editing types. and denote Simple and Complex Edit benchmarks, respectively. denotes the best-performing prior state-of-the-art method, diffusion-based model trained from scratch using approximately 5x data. Bold numbers indicate the best performances across all methods. Green numbers indicate the performance gain of EARL compared to the SFT (S) baseline. Model/Data Base Model OmniEdit EmuEdit AURORA MB VisMin I2EBench AVG Magicbrush InstructPix2Pix Aurora Omnigen Supervised Fine-tuning SFT (S) SFT (S+C) SFT (S+C) two-stage RL Post-training SFT (S) RL (S) SFT (S) RL (C) SFT (S) RL (S+C) SFT (S+C) RL (C) SFT (S+C) RL (S+C) SFT (S+C) two-stage RL (C) SFT (S+C) two-stage RL (S+C) RL Post-training Scaled EARL SFT (S) RL (S+C) EARL SFT (S) RL (S+C) SFT (S) SD v1.5 SD v1.5 SD v1.5 - Emu3 Emu3 Emu Emu3 Emu3 Emu3 Emu3 Emu3 Emu3 Emu3 Emu3 - 3.43 3.97 4.50 5.68 5.73 4.64 4.23 6.07 5.94 6.33 4.89 5.70 4.21 5.29 3.28 3.24 4.40 5. 3.66 2.89 3.29 4.13 4.12 4.28 3.80 4.09 3.16 3.89 3.01 3.05 4.12 4.10 3.58 2.81 3.60 3.47 3.84 3.99 3.21 3.97 3.05 3.85 3.64 3.12 4.62 4. 3.19 2.89 3.40 3.53 3.92 4.26 3.86 4.35 3.33 4.20 3.48 2.94 3.82 4.09 3.57 3.91 4.56 3.34 4.09 4.48 4.71 4.97 4.16 4.70 3.06 3.23 3.58 4. 3.59 2.77 3.07 3.80 3.90 4.08 3.26 3.84 2.99 3.56 3.32 3.26 4.17 4.70 3.88 3.32 3.69 4.06 4.30 4.57 3.95 4.49 3.48 4.25 6.39 +0. 4.47 +0.81 4.27 +0.69 4.52 +1.33 4.93 +1.36 4.19 +0.60 4.80 +0. 5.2 Pushing Image Editing with RL Post-training In this section, we present experimental results demonstrating that RL post-training substantially improves image editing performance. Starting from the SFT (S) model trained only on simple edits, we apply RL post-training under three settings: RL (S), which uses only simple edit data; RL (C), which uses only complex edit data; and RL (S+C), which uses both simple and complex edit data. Table 2 shows that all RL variants improve over the SFT baseline. RL (C), which trains on disjoint set of complex edits from the SFT simple data, outperforms RL (S) across all complex (C) benchmarks without incurring significant drop in performance on the simple (S) benchmarks. This stands in contrast to the SFT setting, where adding complex data degraded performance on simple edits. These results indicate that incorporating complex data during RL post-training helps the model learn complex editing capabilities while preserving its performance on simple edits. The largest gain comes from balancing both simple and complex data during RL i.e. RL (S+C). The best-performing setup is SFT(S) RL(S+C), which improves the average score from 3.88 (SFT(S)) to 4.57. This surpasses three of the four diffusion-based baselines and remains competitive with state-of-the-art methods like Omnigen. In particular, on OmniEdit and VisMin benchmarks, it achieves the best results. For OOD benchmarks I2EBench and EmuEdit, we also attain competitive performance. We observe that applying RL on models pre-trained with both simple and complex data (SFT (S+C) and the two-stage variants) leads to modest improvements over the SFT checkpoint, with average scores reaching up to 4.49. These gains are still below the performance of RL applied on the simpleonly SFT (S) base model (SFT (S) RL (S+C)). Figure 4 show stable RL training of SFT(S) RL(S+C) with consistent reward improvements and rising VIEScore on OmniEdit, indicating healthy learning dynamics. Notably, the two-stage SFT plus RL (C) variant (SFT (S+C) two-stage RL (C)) scores 3.48 on average, substantially lower than the simple-only RL model (SFT (S) RL (S)). Since the SFT (S+C) variants underperform compared to SFT (S), their corresponding RL models also remain underperformant. We hypothesize that supervised fine-tuning on complex edits may degrade the base models core capabilities, limiting RLs ability to recover or improve performance. This aligns with findings in LLM research, where base model capability critically influences RL finetuning success [21]. Thus, while complex data is essential, naively including it during SFT can constrain RLs effectiveness. Scaling RL Training with More Steps and Data To further improve performance, we scale the best-performing setupSFT(S) RL(S+C)by increasing the duration of RL training to 2000 steps and using larger data pool of 300k (S+C) samples. At each step, 16 unique examples are 8 Figure 4: (a) Training curves showing the reward progression, with different aspects of reward. and (b) VIEScore on OmniEdit increases with RL training iterations Table 3: PIEBench results comparing EARL with EditAR (most similar work to ours). SD refers to Structure Distance and CLIP-W, CLIP-E refers to CLIP similarity scores on the whole image and edited regions, respectively. Method EditAR EARL Base Model SD PSNR LPIPS MSE SSIM CLIP-W CLIP-E LlamaGen Emu3 39.43 35.00 21.32 23.71 117.15 112.40 130.27 77. 75.13 79.17 24.87 24.44 21.87 21.34 sampled, resulting in 32k total samples seen20 more than the 1.6k samples from earlier runs. This scaled configuration achieves the best overall results and is referred to as EARL in our evaluations. As shown in the last section of Table 2, EARL surpasses all of the four diffusion-based baselines, including the state-of-the-art Omnigen, achieving an average score of 4.80 compared to 4.70 for Omnigen. In particular, on OmniEdit, AURORA, and VisMin benchmarks, EARL achieves the best results over prior state-of-the-art models. We also achieve strong performance on the out-of-distribution benchmarks like I2EBench and EmuEdit. Comparison with EditAR We compare EARL with EditAR [38] as it is the only existing autoregressive baseline for image editing. EditAR reports results on PIEBench [25] for image editing. As shown in Table 3, EARL outperforms EditAR on five metrics, including Structure Distance score, PSNR, LPIPS [65], MSE, and SIM [54]. On CLIP similarity scores computed using ViT-Large-14 on the whole image (CLIP-W) and on the edited regions (CLIP-E), EARL slightly underperforms compared to EditAR. However, our scores remain comparable to EditAR, demonstrating strong perceptual quality and better background preservation. Figure 5: Example reward scores for various types of image edits using Qwen2.5-VL 72B. Higher scores reflect better alignment with the intended edit prompt. 9 Impact of RL Post-training on Image-Editing Performance We observe that the SFT model sometimes produces noisy artifacts or over-edits regions by changing surrounding areas. It also fails to achieve successful edits consistently. However, when randomly sampled multiple times, often we can find at least one sample with the correct edit (see examples in Fig. 5). Our verifier captures key aspects of edit qualityedit success, over-editing, presence of artifacts, and naturalnessto guide RL training toward more reliable outputs. Through qualitative inspection, we observe that artifacts are almost nonexistent, edits are more precise, and the success rate is increased after applying RL. Fig. 5 illustrates instances where the verifier successfully handles both simple and complex edit tasks. However, the reward model is limited by multimodal image-text understanding of Qwen2.5-VL. For example, in complex edits, such as changing higher count (e.g., six to four), the reward becomes less reliable (see Appendix for more details). This also explains why the performance for simple edits is much higher than for complex edits in EARL. EARL Recipe: Our results show that autoregressive models, while underexplored for image editing, can be highly competitive. When paired with RL, they surpass strong diffusion-based baselines on both simple and complex edits. This demonstrates the power of combining autoregressive generation with RL for controllable and high-fidelity image editing. 5.3 Studying Chain-of-thought Reasoning for Editing Table 4: Performance of EARL variants with chain-of-thought supervision. and represents Simple and Complex Edit benchmarks, respectively. Model/Data OmniEdit EmuEdit AURORA MB VisMin I2EBench AVG SFT (S) SFT think (S) SFT think (S+C) two-stage SFT think (S) RL (S) SFT think (S) RL (C) EARL SFT think (S) RL (S+C) 5.73 4.34 1.44 4.99 4.36 4. 3.66 3.76 1.41 3.73 3.67 3.78 3.58 2.88 1.03 3.33 2.94 3.23 3.19 3.36 1.58 3.48 3.59 3. 3.57 3.46 2.45 3.11 3.08 3.39 3.59 3.21 1.20 3.46 3.16 3.36 3.88 3.50 1.52 3.68 3.47 3. We evaluate the effect of incorporating chain-of-thought reasoning supervision on image editing performance. Since Chain-of-Thought (CoT) reasoning involves alternating between generating text and images, it requires interleaved image-text generation. However, our base model Emu3 was not pretrained for this type of multimodal generation, which presents additional challenges. We compare two SFT variants with CoT reasoning supervision: (1) SFT think (S), and (2) SFT think (S+C) using two-stage approach. Results in Tab. 4 show that, despite the success of chain-of-thought reasoning in large language models [21], SFT think (S) (3.50 avg.) does not improve visual editing performance compared to SFT (S) (3.88 avg.). We also see that adding complex reasoning data (C) during SFT hurts performance, as demonstrated by the drop in performance in the SFT think (S+C) two-stage setup compared to SFT think (S). This is consistent with Section 5.1, where the inclusion of complex reasoning data (SFT (S+C)) and the two-stage setup (SFT (S+C) two-stage) led to performance degradation compared to SFT (S). Next, we apply RL on top of SFT think (S), using the RL(S), RL(C), and RL(S+C) variants. We leave out applying RL on top of SFT think (S+C) two stage as it is too weak. We perform RL post-training for up to 2,000 steps, or until divergence, using 16 unique samples per step. Applying RL on SFT think (S) yields slight improvement across two settings: RL(S) and RL(S+C). On average, RL(S) improves performance from 3.50 (SFT(S)) to 3.68, while RL(S+C) also improves from 3.50 (SFT(S)) to 3.68. These observations align well with the finding in the LLM literature. First, CoT reasoning tends to help only once model surpasses capability threshold [11, 56]. Second, RL provides limited benefit when the base model is still sub-optimal [33]. Quality of Reasoning Chains The SFT think model generates plausible reasoning: correctly identifies target regions, plans edits, and describes intended outcomes. However, its final outputs often show lower edit accuracy, reduced naturalness, and more artifacts compared to the standard SFT model (see Appendix F.2 for qualitative examples). These results suggest that although the model learns to generate appropriate reasoning, it does not effectively apply it during generation of the edited image. We hypothesize that this limitation is due to the models lack of pretraining on interleaved 10 image-text-image data. While the model can generate plausible reasoning chains, it struggles to integrate them effectively for image enhancement, likely because it was not trained to integrate these modalities in cohesive manner. We leave further exploration of this issue to future work."
        },
        {
            "title": "6 Conclusion",
            "content": "This work delivers the first systematic comparison of supervised finetuning, reinforcement learning, and CoT reasoning for text-guided image editing within unified autoregressive framework. Directly motivated by this analysis, we introduce novel autoregressive image editing model, EARL. EARL performs on par with the strongest open-source baselines while using less data, and sets new bar for multimodal autoregressive models for editing. While SFT alone proves insufficient for handling complex edits, we find that RL significantly improves performance, enhancing the models overall edit success and ability to handle tasks involving spatial reasoning and dynamic interactions. We also address the question of when to introduce complex edits during the various training stages: we found that bringing in complex edits is not helpful during the SFT stage, but is beneficial during the RL post-training stage. Lastly, the CoT reasoning supervision experiment did not lead to consistent improvements, highlighting the need for further research and stronger autoregressive base models with strong reasoning capabilities. We discuss limitations and broader impact of our work in Appendix H."
        },
        {
            "title": "7 Acknowledgments",
            "content": "We acknowledge the valuable feedback provided by Qian Yang, Le Zhang, and Oscar Manas on an early draft of the paper. The technical support extended by the Mila IDT and TamIA teams in managing the computational infrastructure is greatly appreciated. During this project, Aishwarya Agrawal was supported by the Canada CIFAR AI Chair award."
        },
        {
            "title": "8 Author Contributions",
            "content": "Saba Ahmadi, Rabiul Awal, and Benno Krojer initiated the project. Saba Ahmadi led the design and implementation of the EARL for auto-regressive image editing. Saba and Rabiul Awal initiated the idea of incorporating reasoning into image editing. Rabiul and Ankur Sikarwar led the data generation for the CoT reasoning experiments. Ankur also led the evaluation. Amirhossein Kazemnejad led the design and the implementation of the reinforcement learning pipeline. Ge Ya Luo and Benno Krojer helped with evaluation metric selection. Rabiul, Saba, Ankur, Amirhossein, and Juan A. Rodriguez ran experiments for different stages of the project. Benno wrote the introduction and provided guidance on individual sections. Saba, Rabiul, Ankur, and Amirhossein led the writing of the remaining sections with feedback from the PIs. Aishwarya Agrawal, the lead PI, guided the project from the start, with additional guidance from Siva Reddy, Christopher Pal, and Sai Rajeswar at various stages."
        },
        {
            "title": "References",
            "content": "[1] Saba Ahmadi and Aishwarya Agrawal. An examination of the robustness of reference-free image captioning evaluation metrics. In Yvette Graham and Matthew Purver, editors, Findings of the Association for Computational Linguistics: EACL 2024, pages 196208, St. Julians, Malta, March 2024. Association for Computational Linguistics. [2] Rabiul Awal, Saba Ahmadi, Le Zhang, and Aishwarya Agrawal. Vismin: Visual minimalchange understanding. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 107795107829. Curran Associates, Inc., 2024. [3] Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: high-quality human-rewarded dataset for instruction-based image editing. arXiv preprint arXiv:2412.04280, 2024. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, 11 Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [6] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [7] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. [8] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Rich Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pre-trained image-editing diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [9] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [10] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. [11] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022. [12] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance. In The Eleventh International Conference on Learning Representations, 2023. [13] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining, 2025. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [15] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. [16] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. [17] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffusion for accurate instruction following. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47444753, 2024. 12 [18] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. ArXiv, abs/2309.17102, 2023. [19] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [20] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 5842 5850, 2017. [21] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [22] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. [23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [25] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. [26] Amirhossein Kazemnejad, Milad Aghajohari, Alessandro Sordoni, Aaron Courville, and Siva Reddy. Nano aha! moment: Single file \"rl for llm\" library. https://github.com/ McGill-NLP/nano-aha-moment, 2025. GitHub repository. [27] Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, and Siva Reddy. Learning action and reasoning-centric image editing from videos and simulation. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 3803538078. Curran Associates, Inc., 2024. [28] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. arXiv preprint arXiv:2312.14867, 2023. [29] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. arXiv preprint arXiv:2310.01596, 2023. [30] Tiancheng Li, Jinxiu Liu, Huajun Chen, and Qi Liu. Instructrl4pix: Training diffusion for image editing by reinforcement learning. arXiv preprint arXiv:2406.09973, 2024. [31] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2251122521, 2023. [32] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [33] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. 13 [34] Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, zhibin wang, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, and Rongrong Ji. I2EBench: comprehensive benchmark for instruction-based image editing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. [36] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems, 36:34973516, 2023. [37] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 42964304, 2024. [38] Jiteng Mu, Nuno Vasconcelos, and Xiaolong Wang. Editar: Unified conditional generation with autoregressive models. arXiv preprint arXiv:2501.04699, 2025. [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [40] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. [43] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [44] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [45] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [46] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [47] Tomáš Souˇcek, Dima Damen, Michael Wray, Ivan Laptev, and Josef Sivic. Genhowto: Learning to generate actions and state transformations from instructional videos. arXiv preprint arXiv:2312.07322, 2023. 14 [48] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [49] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, et al. Llama 2: Open foundation and fine-tuned chat models, 2023. [51] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [52] Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. [53] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024. [54] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. [55] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2025. [56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [58] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. [59] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [60] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. CoRR, abs/2409.11340, 2024. [61] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89418951, 2024. [62] Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In The Twelfth International Conference on Learning Representations, 2024. 15 [63] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2023. [64] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024. [65] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [66] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Haiquan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. Hive: Harnessing human feedback for instructional visual editing. ArXiv, abs/2303.09618, 2023. 16 Overview of Appendix Our supplementary material contains the following: Background: Emu3 GRPO: Group Relative Policy Optimization Training Datasets VIEScore Evaluation with GPT-4o-Mini and Alignment with Human Judgment Qualitative Examples of Reward Model Limitations in Complex Edits Analysis of Model Outputs Compute Resources for Experiment Reproducibility Limitations and Broader Impact Behind the Scenes shows not just the final model but the full journey of how this paper came to be what ideas we tried, what did not work, and how we improved along the way. Background: Emu3 Emu3 [53] (8B parameters) is an autoregreesive multimodal model. It extends the LLaMA-2 [50] architecture by integrating vision tokenizer (SBER-MoVQGAN) for encoding images into discrete tokens and text tokenizer (QwenTokenizer) for processing textual inputs. Unlike diffusion-based models [60], Emu3 generates text and visual tokens in unified manner. In this work, we use the base version of Emu3, which is pre-trained exclusively on text and image data. During pre-training, Emu3 formats image-text multimodal inputs in structured format (shown below), incorporating special tokens to distinguish between the different modalities: [BOS] {caption text} [SOV] {resolution info} [SOT] {vision tokens} [EOV] [EOS] where [BOS] and [EOS] mark the beginning and end of the sequence, [SOV] and [EOV] define the boundaries of image metadata such as resolution, image tokens, and [SOT] indicates the start of vision tokens. Additionally, [EOL] and [EOF] are included within vision tokens to denote line and frame breaks, respectively. The resolution info section contains relevant details to image resolution. GRPO: Group Relative Policy Optimization Algorithm 1: Group Relative Policy Optimization Input: initial policy model πθinit; reward models rϕ; task prompts D; hyperparameters ϵ, β, µ Output: final policy model πθ Function Group Relative Policy Optimization: πθ πθinit ; πref πθinit ; for step = 1 to do Sample batch Db from ; Update the old policy model πθold πθ ; Sample outputs {oi}G Compute rewards {ri}G Compute ˆAi,t for the t-th token of oi through group relative advantage estimation ; for GRPO iteration = 1 to µ do i=1 πθold(q) for each question Db ; i=1 for each sampled output oi by running rϕ ; Update the policy model πθ by maximizing the GRPO objective ; Algorithm 1 presents the GRPO algorithm, where we prepare the advantages using the current policy to estimate the gradients. As noted in Section 4, we set the backward batch size equal to the rollout batch size, which effectively reduces the second loop to single iteration. This means policy gradients 17 are estimated in fully online manner, with advantages always computed from the current policy. This design choice contributes to improved stability during training."
        },
        {
            "title": "C Training Datasets",
            "content": "C.1 Dataset Composition Our goal of building unified model requires exposure to wide range of edit typesboth simple (e.g., object, attribute, or style changes) and complex (e.g., action changes, counting changes, or spatial relations). No single dataset offers comprehensive coverage of all these edit types, and existing models are typically developed in fragmented manner, each targeting only subset of edits. To address this, we consolidate several existing datasets to form unified training pool that spans the full spectrum of edit types. OmniEdit [55] serves as our largest source of simple editing examples. For complex edits involving actions, relations, counting, and other real-world applications which are significantly less represented than simple edits we incorporate data from VisMin [2], Aurora [27], Human-Edit [3], and MagicBrush [64]. C.2 Creating Chain-of-thought from Existing Editing Datasets This section describes the chain-of-thought datasets for image editing and the associated prompt designs. Image editing datasets typically lack reasoning chains. To address this, we synthesize reasoning chains using Multimodal Large Language Model (MLLM) through in-context learning (prompting), based on existing resources such as the input image, edited image, and edit instruction. We use Qwen2.5VL-72B [40] as the MLLM. In diffusion models, masking is common form of conditioning. To mimic this, we collect bounding boxes either by using those provided in the dataset or by estimating them from available masks and pixel differences. These bounding boxes serve as conditioning inputs in the reasoning chain. The model analyzes the image, edit instruction, and target region (via the bounding box) to generate step-by-step reasoning process, referred to as the thinking field. The system prompt and an annotated few-shot examples are provided in Sections C.2.1 and C.2.3 respectively. For VisMin, we use image-text pairs from the VisMin [2] dataset, where the two images differ minimally making them well-suited for editing tasks. Since examples of complex edits, such as those involving counting or spatial relations, are scarce, repurposing VisMin provides useful coverage for these cases. The change can be inferred from the two image captions, which we use to guide Qwen2.5VL-72B through prompting with few-shot demonstrations. Additional bounding box metadata is used to generate both the edit instruction and the corresponding thinking field, as detailed in Section C.2.2. C.2.1 LLM system prompts for creating reasoning chains ı Prompt for In-context Learning of Chain-of-thought Editing Your task is to generate step-by-step edit plan for the given edit task based on the input image and edit instruction. Your reasoning should be thorough and logically structured. Follow these guidelines: 1. Analyze the source image in depth, describing its main elements, context, and any relevant visual details. 2. Identify the object to be edited, providing specific details about its appearance, role in the scene, and distinguishing features. 3. Clearly specify the area to be edited, including bounding box coordinates if provided, and explain why this region is chosen in relation to the object and the scene. 4. Detail the exact changes to be made to the object or area, referencing visual attributes, position, style, and how the edit should be performed to maintain realism and coherence. 5. Describe the expected result after the edit, focusing on how the edited image should appear, how the new or changed object integrates with the scene, and any requirements for preserving surrounding elements. Format your response as concise, numbered list of steps. Ensure each step logically follows from the previous one and provides sufficient depth for clear, actionable edit plan. Input contains: - Edit Instruction: - Input Image: - Edited Image: - Region Coordinates: The edited image for reference (to understand how the edit instruction is applied). The instruction specifying the exact edit to be made on the input image. The region to be edited (specifies the area to be edited). The source image to be edited. Your response must be in the following JSON format: ı{ } \"thinking\": \"<Insert detailed procedural editing steps>\" C.2.2 LLM prompting for converting VisMin image-text pairs to editing task ı Prompt for In-context Learning of Chain-of-thought Editing Your task is to generate step-by-step edit plan for the given edit task based on the input image and edit instruction. Your reasoning should be thorough and logically structured. Follow these guidelines: 1. Analyze the source image in depth, describing its main elements, context, and any relevant visual details. 2. Identify the object to be edited, providing specific details about its appearance, role in the scene, and distinguishing features. 3. Clearly specify the area to be edited, including bounding box coordinates if provided, and explain why this region is chosen in relation to the object and the scene. 4. Detail the exact changes to be made to the object or area, referencing visual attributes, position, style, and how the edit should be performed to maintain realism and coherence. 5. Describe the expected result after the edit, focusing on how the edited image should appear, how the new or changed object integrates with the scene, and any requirements for preserving surrounding elements. Format your response as concise, numbered list of steps. Ensure each step logically follows from the previous one and provides sufficient depth for clear, actionable edit plan. Input contains: - Edit Instruction: - Input Image: - Edited Image: - Region Coordinates: The edited image for reference (to understand how the edit instruction is applied). The instruction specifying the exact edit to be made on the input image. The region to be edited (specifies the area to be edited). The source image to be edited. Your response must be in the following JSON format: { } \"edit_instruction\": \"<Insert edit request here>\", \"thinking\": \"<Insert detailed procedural editing steps>\" C.2.3 Reasoning chain examples In the case of the VisMin dataset [2], which contains two minimally changed image-text pairs, we do not receive explicit edit instructions. Instead, we infer these instructions by prompting multi-modal large language model (MLLM) in few-shot setting, using both the input and the edited image captions as context. The VisMin dataset covers various categories, including object, attribute, counting, and relationship. For object and attribute categories, where the task typically involves editing specific region in the image, the target edit region (bounding box) is sufficient to generate the reasoning chain. The model infers the edit instruction by understanding the object and its properties within the defined region. In contrast, the relationship category, which describes spatial changes between objects, requires both the source image coordinates and the edited image coordinates to derive the appropriate edit instruction. For counting tasks, the reasoning chain is created by comparing the source images bounding boxes with the removed bounding boxes in the edited image. This allows the model to infer the necessary edit to reflect the changes in object counts. We provide several examples of this process in C.2.3. For other image-editing datasets, where edit instructions are explicitly provided, we prompt the MLLM with the input image, edited image, and edit instruction along with bounding box coordinates or keypoints for the objects or persons involved. This structured input allows the model to generate the reasoning chain by clearly understanding both the task and the necessary image modifications. For instance, the reasoning chain might involve identifying objects, spatially manipulating them, or removing them based on the given instructions and bounding boxes. The output reasoning chain provides step-by-step breakdown of how the image should be edited to achieve the specified changes, ensuring that each instruction is logically followed. We also illustrate this process with further examples in C.2.3. ı Chain-of-thought Reasoning Examples (VisMin dataset) { \"INPUT\": { \"input_image\": \"See the input image\", \"description\": \"remove the glasses from the face\", \"source_image_caption\": \"A person wearing glasses in photo.\", 19 ı \"edited_image_caption\": \"A person without glasses in photo.\", \"target_edit_region\": [250, 100, 350, 150] }, \"OUTPUT\": { \"thinking\": \" 1. The image shows person wearing glasses.nn 2. The face is in region [250, 100, 350, 150], showing glasses.nn 3. Remove the glasses while preserving natural skin texture.nn 4. The output image should show person without glasses.\", } } { \"INPUT\": { \"source_image_caption\": \"A bowl that is located to the left of cat\", \"edited_image_caption\": \"a cat that is located to the left of bowl\", \"source_image_coords\": [ [\"a bowl\", [200, 150, 300, 250]], [\"a cat\", [100, 225, 200, 275]] ], \"edited_image_coords\": [ [\"a bowl\", [100, 225, 200, 275]], [\"a cat\", [200, 150, 300, 250]] ] }, \"OUTPUT\": { \"edit_instruction\": \"Move the bowl to where the cat is, and the cat to where the bowl is.\", \"thinking\": \"1. The image shows bowl and cat. 2. To swap positions, well move the bowl from [200, 150, 300, 250] to [100, 225, 200, 275], and simultaneously relocate the cat from [100, 225, 200, 275] to [200, 150, 300, 250]. 3. This exchange will maintain each objects size and orientation while reversing their left-right relationship.\" } } { \"INPUT\": { \"source_image_caption\": \"Two bicycles and motorcycle.\", \"source_image_coords\": [ [\"bicycle\", [10, 171, 138, 256]], [\"bicycle\", [213, 171, 341, 256]], [\"motorcycle\", [331, 196, 502, 264]] ], \"edited_image_caption\": \"One bicycle and motorcycle.\", \"removed_bounding_boxes\": [[213, 171, 341, 256]] }, \"OUTPUT\": { \"edit_instruction\": \"Remove the second bicycle.\", \"thinking\": \"1. The scene shows two bicycles and motorcycle. 2. To reduce to one bicycle, well remove the second bicycle located at [213, 171, 341, 256], leaving the first bicycle at [10, 171, 138, 256] and the motorcycle at [331, 196, 502, 264] untouched. 3. This maintains proper spacing between the remaining objects.\" } } ı Chain-of-thought Reasoning Examples (VisMin dataset) { \"INPUT\": { \"edit_instruction\": \"Remove bicycles\", \"target_edit_region\": [140, 389, 275, 431] }, \"OUTPUT\": { \"thinking_chain_of_thought\": \"Steps to remove the bicycles: 1. The goal is to remove the bicycles from the image. 2. Identify objects with defining bicycle features like wheels, handlebars, and frames. The bicycles are found within the bounding box [140, 389, 275, 431]. 3. Use the bounding box to isolate and erase the bicycles while preserving the surrounding elements. 4. The image should appear as if the bicycles were never there.\", } } VIEScore Evaluation with GPT-4o-Mini and Alignment with Human"
        },
        {
            "title": "Judgment",
            "content": "We use the VIEScore [28] metric as our evaluation metric for the image editing task. VIEScore is based on MLLM prompting to judge the quality of given edited image w.r.t the given editing prompt and the source image. Specifially, the MLLM is prompted twice, once to judge the semantic quality (alignment with the prompt and overediting), and perceptual quality (such as realism and 20 artifacts). VIEScore was originally only shown to be effective on ImagenHub [29] which contains primarily objectand attribute-centric edits. We verify in Tab. 5 that correlation with human judges is also convincing on various other edit types, i.e. many of the complex edits in this paper such as spatial edits (WhatsUp row), action edits (Something Something, Action-Genome, Epic Kitchen rows) or counting (Kubric row). For this we correlate human judgements initially collected for AURORA-Bench [27] and VIEScore with GPT4o-mini as backbone (which we use throughout the paper as our main metric). We note that human judges in [27] were asked to only judge the semantic alignment and not lower level visual properties such as aesthetics. Thus it is not surprising that the correlation of the semantic component of VIEScore is higher. Overall we find correlations to be on par or sometimes exceeding with those shown in the original paper [28]. For example in the original paper, the overall VIEScore (with the best model GPT4 at the time) had correlation of 0.382 with human raters on MagicBrush. We find the lowest correlations, as expected, is on hard action-centric prompts in the Epic Kitchen subset of AURORA-Bench. Table 5: Correlation between VIEScore (GPT4o-mini) and human judges on the 8 subtasks of AURORA-Bench [27]. Specifically, we correlate the overall VIEScore and the semantic sub-score with humans. We note that human judges in [27] were asked to only judge the semantic alignment and not lower level visual properties such as aesthetics. Task Overall Semantic MagicBrush Something Something Action-Genome Epic Kitchen Whatsup Kubric CLEVR Emu-Edit Overall 0.5353 0.3676 0.3585 0.1764 0.5285 0.2649 0.5600 0.4000 0.4328 0.6300 0.4305 0.3830 0.2462 0.6011 0.3529 0.6187 0. 0."
        },
        {
            "title": "E Qualitative Examples of Reward Model Limitations in Complex Edits",
            "content": "Figure 6: Examples of limitations of the verifier for the complex edits. The reward values correspond to the VIEScore given by our verifier. 21 Our reward model shows strong performance on simple edits involving object and attribute changes but exhibits limitations when verifying complex edits, as illustrated by several qualitative examples in Fig. 6. In Row 1, the task is to reduce the number of birds from 10 to 3. This large-count change is difficult for the verifier to handle accurately, reflected in inconsistent scores across different outputs. For instance, although output 2 and 6 are quite similar, the rewards they receive differ significantly (0.0 vs. 4.5). This indicates uncertainty in reliably evaluating edits that involve large numerical changes. In contrast, Row 2 is an example of small count changes. It requires removing all apples except one. The verifier assigns consistently high scores to the correct generations (outputs 3 to 8), suggesting that smaller count changes are easier to verify, likely because the verifier is more exposed to such data during training. In Row 3, the task involves moving mouse closer to remote. Although the model generally assigns higher scores to better samples, it struggles with inconsistencies: in output 8, the mouses appearance changes noticeably, and in output 7, the hand shows some distortion. This underscores the difficulty of verifying subtle positional changes compared to simpler edits, such as additions and removals. For Row 4, the edit requires swapping the positions of white jeep and bicycle. The verifier correctly favors outputs that accurately reflect this positional change and exhibit fewer artifacts and distortions, demonstrating sensitivity to spatial relationships and artifacts. For example, outputs 2 and 3 still show traces of the car in its original position, indicating both leftover artifacts and failure to correctly reflect the position swap. However, obtaining fully reliable reward signals for fine-grained spatial relationships still remains challenging. Finally, Row 5 involves moving person toward the right side. Although the verifier prefers outputs where the persons position is more towards the right, it does not take into account the changes in persons appearance. It is important to note that while verifier scores vary, the model can still provide meaningful signals for some complex edits such as detecting lower object counts or changes in position to certain extent, helping guide improvements in edit quality."
        },
        {
            "title": "F Analysis of Model Outputs",
            "content": "F.1 Fine-Grained Evaluation of EARL Qualitative Fig. 7 shows EARL SFT (S) RL (S+C) performing four types of image edits: counting, action, spatial, and simple; each illustrated by three side-by-side input/output examples. In the counting row, EARL correctly removes one poodle and two toy cars but fails to remove one egg from the third image. In the action row, it successfully takes the white cup out and opens the orange bag in the first two examples, but cannot make the person stand up in the final example. For spatial edits, the model effectively interprets spatial relationships by correctly removing the left fire hydrant and adding man to the left of the road sign, but it fails to add picture to the left of the woman in the third example. Finally, in the simple edits row, EARL recolors an alien spaceship pink and successfully erases palm tree with clean backgrounds, but fails to edit the third image and removes the bowling bowl instead of the truck. Overall, these examples highlight the models strong potential in performing complex image edits, while also indicating some challenges that remain to be addressed. Quantative The quantitative results (Table 6 7 8 9 10 11) demonstrate that EARL SFT (S) RL (S+C) consistently outperforms the baseline SFT (S) across multiple fine-grained editing tasks and datasets. For instance, in Table 7 on I2EBench, improvements are evident in counting, direction perception, object manipulation, and style alteration metrics. Similarly, Table 8 shows gains in style, attribute modification, environment changes, and object addition/removal on OmniEdit. Table 9 further confirms these trends on EmuEdit, with better performance in object addition, background editing, and both global and local changes. Lastly, Table 10 highlights improved counting and spatial relation accuracy on VisMin. On AURORA, as shown in Table 11 there is no improvement in action edits for EARL SFT (S) RL (S+C) when compared to SFT (S), mainly because SFT (S) itself was very poor on action edits. Overall, these results underscore the effectiveness of combining SFT with RL for diverse kinds of edit types. 22 Figure 7: Qualitative examples of EARL across diverse edit typescounting, action, spatial, and simple. Table 6: Fine-grained evaluation of SFT and RL model variants on I2EBench (Low-level Editing). Model/Edit Task Category Deblurring Haze Removal Lowlight Enhancement Noise Removal Rain Removal Shadow Removal Snow Removal Watermark Removal SFT (S) EARL SFT (S) RL (S+C) 2.36 2.33 3.73 3. 2.56 3.03 2.36 3.04 3.98 4.74 4.60 4.98 2.59 3.16 3.82 4. Table 7: Fine-grained evaluation of SFT and RL model variants on I2EBench (High-level Editing). Model/Edit Task Category Counting Direction Perception Object Removal Object Replacement Background Replacement Color Alteration Style Alteration Region Accuracy SFT (S) EARL SFT (S) RL (S+C) 2.74 4. 1.82 3.45 4.52 4.76 2.49 3.40 3.47 4.72 5.74 5.98 5.34 5. 4.36 5.06 Table 8: Fine-grained evaluation of SFT and RL model variants on OmniEdit. Model/Edit Task Category Style Attr. Mod. Env Swap Addition Removal SFT (S) EARL SFT (S) RL (S+C) 5.52 5.99 6.30 6.95 6.12 6.68 5.69 6.39 4.86 6.00 5.91 6. Table 9: Fine-grained evaluation of SFT and RL model variants on EmuEdit. Model/Edit Task Category Add Remove Background Text Color Style Global Local SFT (S) EARL SFT (S) RL (S+C) 2.36 4. 4.91 4.81 2.47 3.24 2.15 3.16 4.91 5.85 4.89 5.13 4.14 4. 3.78 4.47 23 Table 10: Fine-grained evaluation of SFT and RL model variants on VisMin. Model/Edit Task Category Counting Spatial Relation SFT (S) EARL SFT (S) RL (S+C) 4.22 5.72 2.91 4.14 Table 11: Fine-grained evaluation of SFT and RL model variants on AURORA. Action Genome subset of the Aurora benchmark specifically contains complex action edits. Model/Edit Task Category Action SFT (S) EARL SFT (S) RL (S+C) 3.09 2.96 F.2 Qualitative Evaluation of Reasoning and Edits in the SFT-Think Model Fig. 8 presents qualitative examples comparing two model variants: SFT(S), and SFT think(S). Example 1 This example involves changing the color of wooden rabbit sculpture to brown, while preserving its carved details and natural background. SFT (S): Performs successful edit without reasoning, changing the color to brown while preserving the background unchanged. SFT think (S): Uses Chain-of-Thought (CoT) reasoning, including scene description, object identification, and bounding box localization. It understands the edit instructions and grounds the plan well. However, the edited rabbit appears unnatural and loses some details. Observation: Although SFT think(S) demonstrates strong reasoning and grounding capabilities, it introduces artifacts or unnatural features in the edited image compared to the simpler SFT(S) variant. Example 2 This example involves replacing an antique wooden radio with typewriter, maintaining the spatial layout and removing the original radios elements. SFT (S): Successfully replaces the radio with the typewriter, keeping the proportions and orientation consistent. SFT think (S): Uses Chain-of-Thought (CoT) reasoning to carefully plan the replacement, including brand details, but shows minor inconsistencies by mentioning two different brand names (IBM and Makoka). The final edited image presents slight visual distortions around the edges and keys. Observation: While the reasoning model provides detailed planning and grounding, it sometimes produces inconsistent detailssuch as mentioning two different brand names (IBM and Makoka)reflecting pattern of hallucination in the reasoning process that can reduce visual quality compared to the non-reasoning model. Example 3 This example involves transforming green forested area around small building into garden with blooming flowers while keeping the building unchanged. SFT (S): Successfully changes the environment to blooming garden without reasoning, preserving the building and overall composition. SFT think (S): Uses Chain-of-Thought (CoT) reasoning to plan the seasonal transformation carefully, resulting in well-grounded and detailed edit. Observation: Both models perform the edit well, delivering accurate and natural-looking results. In summary, the reasoning model can ground and plan edits, but it may sometimes reduce visual quality by introducing artifacts and unnatural details compared to the no-reasoning model. 24 Figure 8: This figure shows image editing examples from SFT(S) without reasoning, SFT think(S) with Chain-of-Thought (CoT) reasoning. While the reasoning model can understand instructions, plan, and ground edits, it may introduce artifacts and unnatural details compared to the non-reasoning model."
        },
        {
            "title": "G Compute Resources for Experiment Reproducibility",
            "content": "For SFT training, we used the AdamW optimizer with weight decay 0.1 and parameters β1 = 0.9, β2 = 0.95, and ϵ = 1 106. The learning rate followed cosine schedule with minimum value of 106, and gradient clipping was applied with maximum norm of 5.0. Training employed DeepSpeed ZeRO stage 3 for memory efficiency, with mixed precision enabled using bfloat16 (bf16). We train the model for maximum of 5 epochs. For RL training, rewards were computed using VIEScore through vLLM API server running Qwen2.5-VL-72B on 4 NVIDIA H100 GPUs. The training was conducted separately on different 25 server for 2000 steps, with early stopping based on reward plateaus, also using four NVIDIA H100 GPUs."
        },
        {
            "title": "H Limitations and Broader Impact",
            "content": "H.1 Limitations First, the models performance heavily depends on the coverage of our training data. Although we curated diverse set of simple and complex edit triplets, long-tail conceptssuch as fine-grained cultural artifacts, specialized scientific diagrams, and underrepresented geographic scenesremain sparsely represented. This limited coverage can lead to brittle behavior when the model encounters out-of-distribution inputs. Second, our reinforcement learning approach relies on single frozen vision-language verifier, which, despite being state-of-the-art MLLM, has inherent limitations. The verifier can be imperfect and it inherits biases from its pretraining corpus. It struggles particularly with verifying complex edit types involving spatial relationship and action changes. Although these edits are underrepresented in the verifiers training data, qualitative and quantitative analyses indicate that the verifier still provides meaningful learning signals to guide the model. Lastly, our training data depends on synthetic data generated via diffusion models, which include automatic filtering to reduce noise. However, some noisy examples remainsuch as image deformations or outputs that do not accurately reflect the edit instructionsdue to imperfections in synthetic data generation. These factors can introduce noise in training. However, some of these can be improved during RL post-training as RL post-training does not need the ground-truth labels for edited images. H.2 Broader Impact Positive Impacts Text-guided image editing systems such as our RL-enhanced approach has the potential to amplify human creativity and promote accessibility. By accepting natural language instructions, the model helps lower the barrier for designers, educators, and hobbyists who lack advanced editing expertise, enabling rapid iteration on visual concepts. Beyond direct applications, our RL pipeline offers way to overcome the requirement of ground-truth edited images. Potential Negative Impacts While high-fidelity image editing offers many benefits, it also poses risks. Such technology can be misused to create convincing misinformation or deepfakes. Additionally, if the base model or the vision-language verifier contains demographic biases, reinforcement learning may inadvertently amplify these biases. Importantly, our model is developed strictly for research purposes and is intended to advance scientific understanding rather than for deployment in real-world applications. We encourage ongoing efforts to implement safeguards and promote ethical use alongside further development in this area."
        },
        {
            "title": "I Behind the Scenes",
            "content": "We started this project with clear goal: to build single, unified model that edits images guided only by text instruction no user-provided masks, bounding boxes, or conditions. The model itself would reason and plan the edits, generating all necessary guidance on its own. The rationale behind this is that diffusion models dominant approach to image editing are typically built with some form of user conditioning to control for faithful editing, and they lack unification separate model built for different edit types. We wanted to have unified model that does all sorts of edits and treats user conditioning as learnable task via reasoning. To achieve this, we needed model capable of generating both images and text in one combined sequence, so we searched for an interleaved image-text transformer model. The first model we found is Metas Chameleon model [48], as it fits the criteria well. It had good image generation capabilities, but the model was not open source. So, we pivoted to Anole [10], an open-source variant inspired by Chameleon. We trained Anole to do image editing without any reasoning input (the sanity check one can do). Unfortunately, the results were rough: images suffered from distortion and artifacts, making it clear we needed something stronger. 26 Then came Emu3 [53]. When it was released, we quickly tested it, fine-tuning with small, complex editing datasets. Results initially fell short until we added large amounts of simpler, high-quality edit data. This mix showed improvements: the model began handling simplistic edits well; also, some hope for counting, spatial, and even some action edits successfully. Our main goal is to integrate reasoning to eliminate the need for user-provided conditioning. To this end, we introduced reasoning data structured chain of thought prompts to guide the models editing process, as detailed extensively in our paper. Surprisingly, models trained with reasoning mostly underperformed compared to those without it. We tried many variants: changing the data pool, simplifying edit tasks, shortening the chain-of-thought length, and varying training data size, but none made significant difference. We suspect this gap is due to the base models capacity or the quality of the reasoning data. Even feeding ground-truth reasoning directly as input during fine-tuning failed to boost results as much as expected. One positive result we had with the no-reasoning variant is that we were able to get close to state-ofthe-art. Since RL has been gaining traction lately, we were eager to explore its potential by applying it to our problem, but we needed to build strong SFT model first. We tested SFT models with generating multiple samples given fixed prompt. One interesting observation was that at least one of these samples did well on simple edit tasks (e.g. changing object, attribute), and sometimes even complex one (e.g. changing count). That flicker of success motivated us to integrate RL on top of SFT-ed models to push the model towards consistently generating better edits. Compared to our exploration of teaching an autoregressive model editing through reasoning, which was for most part unsuccessful for long while, RL experiments were more promising early on clearly moving our interest to go further with the RL route. This journey has been rollercoaster of setbacks and great excitement when RL was working consistently. Yet, each experiment deepened our understanding of the delicate balance between data quality, reasoning guidance and RL bringing us closer to that elusive goal of truly unified, instruction-guided image editing."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "McGill University",
        "Mila Quebec AI Institute",
        "Polytechnique Montréal",
        "ServiceNow",
        "Université de Montréal",
        "École de Technologie Supérieure (ETS)"
    ]
}