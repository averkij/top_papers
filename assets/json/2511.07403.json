{
    "paper_title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards",
    "authors": [
        "Hunar Batra",
        "Haoqin Tu",
        "Hardy Chen",
        "Yuanze Lin",
        "Cihang Xie",
        "Ronald Clark"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 0 4 7 0 . 1 1 5 2 : r SPATIALTHINKER: REINFORCING 3D REASONING IN MULTIMODAL LLMS VIA SPATIAL REWARDS Hunar Batra1 Haoqin Tu2 Hardy Chen2 Yuanze Lin1 Cihang Xie2 Ronald Clark1 1University of Oxford 2University of California, Santa Cruz Project page Code Models & Dataset"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal large language models (MLLMs) have achieved remarkable progress in visionlanguage tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SPATIALTHINKER, 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SPATIALTHINKER consists of two key contributions: (1) data synthesis pipeline that generates STVQA-7K, high-quality spatial VQA dataset, and (2) online RL with multi-objective dense spatial reward enforcing spatial grounding. SPATIALTHINKER-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Spatial reasoning is central to human intelligence, enabling us to perceive, localize, and manipulate objects in complex environments. This capability is crucial for embodied AI tasks such as robotic manipulation (Gao et al., 2023; Intelligence et al., 2025; Nasiriany et al., 2024), navigation (Huang et al., 2022), and augmented reality (Konenkov et al., 2024), where precise spatial awareness underpins interactive decision-making and makes spatial reasoning essential for real-world deployment (Driess et al., 2023; Team et al., 2025). While multimodal large language models (MLLMs) have advanced rapidly in vision-language tasks such as visual question answering (VQA), captioning and referring expression comprehension (Bai et al., 2025; Deitke et al., 2025; Du et al., 2025; Google, 2025; Hurst et al., 2024; Lin et al., 2024; Liu et al., 2023), they continue to struggle with spatial understanding tasks, especially in the 3D space (Chen et al., 2024a; Kamath et al., 2023; Ma et al., 2024b; Tong et al., 2024a;b; Yang et al., 2025a), which requires capturing geometry, structure, and relations beyond 2D projections. Existing approaches are often data-intensive, relying on either synthesizing massive questionanswering datasets from 3D scene graphs (Chen et al., 2024a; Cheng et al., 2024; Daxberger et al., 2025; Ma et al., 2025b), training auxiliary spatial tokens or architectural changes (Hong et al., 2023b; Ma et al., 2025b), ingesting explicit 3D inputs like depth maps or point clouds (Cai et al., 2024; Cheng et al., 2024; Hong et al., 2023c), or more recently applying reinforcement learning (RL) with sparse rewards (Ma et al., 2025a; Shen et al., 2025a; Wang & Ling, 2025; Xia et al., 2025; Xiao et al., 2025; Zhu et al., 2025). This has led to models that are extremely data-hungry (e.g., SpatialVLM trained on 2B VQA samples (Chen et al., 2024a), SpatialLLM on 1M (Ma et al., 2025b), SpatialRGPT on 700k (Cheng et al., 2024)), or require architecture-specific modifications. Correspondence to {hunar.batra, ronald.clark}@cs.ox.ac.uk 1 Figure 1: Method overview of SPATIALTHINKER. Our framework integrates structured scene-graph grounded reasoning with multi-objective dense RL to enhance 3D spatial understanding in multimodal large language models. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated superior generalization over supervised fine-tuning (SFT) by learning diverse reasoning strategies rather than static patterns (DeepSeek-AI et al., 2025; Gandhi et al., 2025; Shen et al., 2025b). However, existing RLVR approaches for visual spatial reasoning employ simple rewards focused on final correctness, providing insufficient guidance for visually-grounded reasoning (Ma et al., 2025a; Shen et al., 2025a; Xiao et al., 2025). We hypothesize that progress in this domain requires models to simulate grounded perception before reasoning, mirroring how humans mentally visualize regions of interest and relational layouts before making spatial judgments (Wu & Xie, 2023; Yang et al., 2025a; 2016). Scene graphs offer natural structure (Hildebrandt et al., 2020; Wald et al., 2020), but existing methods treat them as external pre-processing (Chen et al., 2023; 2025c; Kim et al., 2024; Li et al., 2025; 2024c) rather than integrating them with end-to-end reasoning. We introduce SPATIALTHINKER, 3D-aware MLLM that integrates scene graph grounding with multi-step spatial reasoning through online policy RL. The model constructs question-focused scene subgraphs capturing objects, their relations, and localized coordinates, and reasons over these structured representations. The training leverages multi-objective reward framework with lexicographic ordering: format rewards enforce structured reasoning; count penalties regulate regional focus; accuracy rewards prioritize correctness; and CIoU-based spatial rewards encourage precise localization when answers are correct. This design promotes human-like reasoning, following process of observe, localize, think, answer. By training on only 7K samples from our synthesized STVQA-7K dataset, SPATIALTHINKER-7B outperforms supervised fine-tuning (+6%) and conventional RL baselines (+3.2%) across twelve spatial understanding, real-world and generic VQA benchmarks, surpassing GPT-4o (+3.4% avg.) and Claude 3.5 Sonnet (+10.1% avg.) (Anthropic, 2024; Hurst et al., 2024), particularly +12.1% gain over GPT-4o on 3DSRBench (Ma et al., 2024b). Notably, while vanilla RL with sparse rewards improves the base model by +4% average across all benchmarks, SPATIALTHINKER-7B trained with dense spatial rewards achieves +7.2% gains, almost doubling (1.8) the benefit of RL training by 2 providing richer learning signals. This demonstrates that models can learn effective spatial reasoning by discovering how to focus on regions of interest, construct mental scene graph representations, and accurately localize objects - all through online environmental feedback from dense rewards that incentivize visually-grounded perception, rather than relying on data scale alone. The strong generalization for in-domain and out-of-domain tasks from minimal high-quality data validates that properly-guided RL surpasses static SFT patterns learned from much larger datasets (Chen et al., 2024a; Ma et al., 2024a). Our main contributions are: We propose SPATIALTHINKER, the first MLLM integrating scene graph-based grounding with online RL for spatial reasoning, achieving strong performance with only 7K training samples versus millions required by existing methods. We introduce STVQA-7K, high-quality spatial VQA dataset grounded in scene graphs, along with scalable data generation pipeline upto 108k samples. We design dense, lexicographically gated multi-objective reward that guides regionally focused spatial reasoning, achieving superior inand out-of-distribution generalization across spatial, generic VQA, and real-world benchmarks, and outperforming conventional RL and SFT baselines, open-sourced generalist and spatial MLLMs, and proprietary models."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Scene Graph Generation. scene graph provides structured representation of an image as directed graph = (V, E). Each node vi denotes an object with category label ci and 2D bounding box bi = (x1, y1, x2, y2); each edge eij is relationship triplet vi, rij, vj consisting of subject vi, predicate rij, and object vj that capture spatial or interactive relations (e.g., left of, on, under) (Hildebrandt et al., 2020; Wald et al., 2020). Classical SGG decomposes prediction into object detection and relation recognition (Carion et al., 2020; Cong et al., 2023), while open-vocabulary methods leverage language or vision priors to generalize beyond fixed ontologies (Chen et al., 2024b; Li et al., 2023). We refer to question-focused scene subgraphs as Gq = (Vq, Eq) that retain only objects and relations relevant to given query q. Reasoning in Multimodal Large Language Models. Multimodal large language models (MLLMs) aim to solve reasoning tasks defined over dataset of multimodal instances (ximg, xtext, y), where ximg is visual input, xtext is natural language query, and is verifiable reasoning trajectory. We model the MLLM as an autoregressive policy πθ that outputs trajectory = (s1, . . . , sT , a) consisting of reasoning steps st and final answer a. The policy factorizes as: πθ(y ximg, xtext) = (cid:32) (cid:89) t=1 πθ(st ximg, xtext, s<t) πθ(a ximg, xtext, sT ) . (1) (cid:33) Supervised fine-tuning enables imitation of reference reasoning traces but often struggles with generalization. Reinforcement learning (RL) instead optimizes reasoning trajectories with explicit reward signals, improving robustness and task adherence (DeepSeek-AI et al., 2025; Gandhi et al., (cid:2)R(y)(cid:3), 2025; Huang et al., 2025). The RL objective is given by: maxθ E(ximg,xtext,y)D, yπθ where R(y) evaluates the trajectory based on format adherence, object counting, answer correctness, and spatial localization."
        },
        {
            "title": "3 SPATIALTHINKER: SPATIALLY-AWARE REASONING MLLMS",
            "content": "Task Formulation We cast spatial reasoning in MLLMs as the task of producing visually grounded response to query = {ximg, xtext}. Unlike generic reasoning, our formulation explicitly requires constructing question-focused scene subgraphs Gq and reasoning over objects, bounding boxes, and relations. The policy πθ is trained on spatially grounded VQA samples from STVQA-7K (Section 3.3) using our multi-objective spatial reward (Section 3.1), which enforces structural validity, count fidelity, answer accuracy, and precise spatial grounding."
        },
        {
            "title": "3.1 MULTI-OBJECTIVE REWARD DESIGN",
            "content": "SPATIALTHINKER is trained with fine-grained, multi-objective reward function that guides spatial reasoning via explicit visual grounding. Unlike prior RLVR methods that use sparse final-answer rewards (Peng et al., 2025; Shen et al., 2025b; Zhu et al., 2025), our dense reward design combines lexicographic gating with four componentsformat, count, accuracy, and spatial rewards. We further discuss our reward design process, including ablations and our rationale in Appendix C. Format Reward. We enforce visually-grounded and structured reasoning template: <observe> for scene description, <scene> for regional scene graphs with objects, bounding boxes, and relations, <think> for explicit reasoning, and <answer> for the final output. Beyond tag presence, the format reward validates the JSON inside <scene>, ensuring (1) it is parseable, (2) each object includes required fields (ID and bounding box), and (3) all relations are valid subjectpredicateobject triplets. This encourages sequential grounding: perceive localize reason answer. The reward Rf 0, 1 is weighted at wformat = 0.1. Accuracy Reward. To prioritize task performance, we define the accuracy reward Ra as binary score based on exact string match between the models predicted answer and the ground-truth answer, enabled by our multiple-choice format. This component carries the highest weight (waccuracy = 0.5), directly incentivizing correct final predictions, while the other rewards shape how the model arrives at correct answers. Count Reward. The count reward encourages the model to predict the appropriate number of objects and relations relevant to the query, penalizing both underand over-generation based on the deviation between predicted and ground-truth counts: (cid:32) (cid:32) Rcount = wcount λobj max 0, 1 (cid:33) pred max(N gt obj gt obj obj, 1) (cid:32) + λrel max 0, 1 (cid:33)(cid:33) pred max(N gt rel gt rel rel, 1) where pred and gt denote predicted and ground truth counts respectively, wcount = 0.2 is the overall count reward weight, and λobj and λrel are set to 0.7 and 0.3 respectively. This guides the model to stay focused on question-relevant regions. Without it, we found the models tend to game the spatial reward by generating excessive objects and relations to maximize random matchesa form of reward hacking. Spatial Reward. To supervise object localization, we compute the spatial reward only when the final answer is correct. Predicted and ground-truth objects are matched using the Hungarian algorithm for bipartite matching with cost function that combines Complete IoU (CIoU) and semantic similarity: C(opred , ogt ) = λspatial(1 IoU(bi, bj)) + λsemantic(1 sim(li, lj)), where and denote bounding boxes and labels, respectively, λspatial = 1.0, and λsemantic = 2.0. The reward is then computed as the average CIoU across matched pairs: Rspatial = wspatial (cid:16) 1 , where wspatial = 0.2. CIoU offers dense supervision over IoU, even for non-overlapping boxes by incorporating distance and aspect ratio terms (Zheng et al., 2020). (i,j)M CIoU(bpred , bgt ) (cid:80) (cid:17) Lexicographic Gating. To avoid reward gaming across objectives, we apply lexicographic ordering with conditional gating (Skalse et al., 2022), prioritizing format {count, accuracy} spatial. The model must first satisfy formatting, then jointly optimize count and accuracy, and receives spatial reward only when the answer is correct. This ensures spatial grounding reinforces valid reasoning. Without accuracy gating, models tend to over-optimize intermediate spatial rewards at the expense of final answer correctness. The final reward is computed as follows, with I[] as the indicator function: Rtotal = I[Rformat = 1] (wformatRf + wcountRc + waccuracyRa + I[Raccuracy = 1]wspatialRs) 3.2 ONLINE RL POLICY OPTIMIZATION To train SPATIALTHINKER with dense, lexicographically gated rewards, we adopt Group-Relative Policy Optimization (GRPO) (DeepSeek-AI et al., 2025; Shao et al., 2024), an online RL method that avoids critic networks by estimating advantages through intra-group comparisons. Given an input x, we sample trajectories {y(1), . . . , y(N )} from the current policy πθold. Each response is scored via 4 our dense spatial reward function (Section 3.1), and advantages are computed using group-normalized scores: A(i) = r(i)µ σ+ε , where µ and σ are the group mean and standard deviation, and ε = 106. We then update the policy using PPO-style clipped loss with KL regularization: LRL(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 y(i) y(i) (cid:88) (cid:104) t= (cid:16) ri,tA(i), clip(ri,t, 1 ϵl, 1 + ϵh)A(i)(cid:17) min β Di,t KL (cid:105) , x,y(i) <t) x,y(i) <t) where ri,t = πθ(y(i) KL is the πθold (y(i) token-level KL divergence against reference model. We set ϵl = 0.2, ϵh = 0.3, and β = 102. This objective balances learning from dense spatial rewards while constraining policy divergence to ensure stability and generalization. is the importance ratio between new and old policies, and Di,t t"
        },
        {
            "title": "3.3 STVQA-7K: DATASET CONSTRUCTION",
            "content": "To facilitate reward-aligned spatial reasoning, we construct STVQA-7K, synthetic visual question answering (VQA) dataset built from human-annotated scene graphs in Visual Genome (Krishna et al., 2017). STVQA-7K comprises 7,587 spatially grounded multiple-choice VQA pairs spanning both 2D and 3D spatial understanding, covering nine core reasoning types including relations, size, orientation, distance, depth, reach, location, count, and existence. We augment the original VG150 predicate set with 34 additional spatial relationscovering distance (e.g., near, far), size (e.g., bigger, taller), orientation (e.g., facing away), and containment (e.g., inside, beneath)to enrich the relational vocabulary beyond the standard 50 predicates. Each QA pair is generated from scene graph using Claude Sonnet 4 (Anthropic, 2025), and rated by difficulty and quality. We apply consistency-based filtering pipeline using GPT-4o (Hurst et al., 2024) to retain high-quality and accurate samples in our final dataset via pass@2 verification. From an initial pool of 56,224 questions, we retain the top 7.5K high-quality samples based on rating, difficulty, and verification. To enable region-specific reasoning, we extract relevant objects and relations per question via lemmatized keyword matching, constructing question-aligned scene subgraphs as localized supervision. This localized supervision helps the model learn where to focus within complex scenes. Bounding box coordinates are retained in absolute pixel space to preserve real-world scale for CIoU-based reward training. Importantly, our pipeline is scalable and can be extended to generate up to 108K samples, the maximum supported by Visual Genome, enabling future large-scale post-training or RL fine-tuning. Figure 2 shows the distribution of QA categories. Full dataset details and examples are provided in Appendix A. Figure 2: Distribution of QA types in STVQA-7K. The dataset spans diverse range of spatial reasoning skills, covering spatial relations, localization, existence, reach, depth, distance, size, count and orientation. 3.4 TRAINING DETAILS We build SPATIALTHINKER upon two strong open-source multimodal base models: Qwen2.5-VL-3B and Qwen2.5-VL-7B (Bai et al., 2025), using them as backbones for policy optimization with RL. No SFT is performed prior to RL training on our STVQA-7K dataset (Section 3.3). We employ GRPO (Shao et al., 2024) as the advantage estimator as described in Section 3.2, using rollout size of 8 samples per query and sampling temperature of 1.0. The models are trained with maximum context length of 16,384 tokens. The rollout batch size is set to 512, and the global batch size is 128. We train for 75 training steps i.e., 5 training episodes) on 4 NVIDIA H100 80GB GPUs. Training time totals 13 hours for the 3B model and 15 hours for the 7B model. The models are trained on high-resolution image inputs ranging from 512 512 to 2048 2048 pixels, to preserve fine-grained spatial information. All model parameters, including the vision encoder, are updated during training. We use the AdamW optimizer with bf16 precision, learning rate of 1 106, and weight decay of 1 102. The KL penalty coefficient is set to 102 (Ablation in Appendix D). STVQA-7K is partitioned with 90/10 trainvalidation split. Further details on prompts, SFT, and RL training setups, are provided in Appendices B.3, B.4, and B.5, respectively. Finally, Section B.5.1 illustrates how each reward component improves steadily under our multi-objective spatial reward, reflecting stable and interpretable learning dynamics. Model 3DSRBench CV-Bench BLINKval Spatial Relation GPT-4o Claude 3.5 Sonnet Qwen2.5-VL-3B Qwen2.5-VL-7B VLAA-Thinker-Qwen2.5-VL-7B LLaVA-NeXT-8B Cambrian-1-8B RoboPoint-13B SpatialBot-3B SpaceLLaVA-13B SATORI-R1 Spatial-RGPT-7B w/ depth SpaceThinker SpaceOm Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + Vanilla GRPO SpatialThinker-3B (Ours) Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + Vanilla GRPO SpatialThinker-7B (Ours) 2D 3D Proprietary Models 83.0 71.5 75.8 60.2 Open-Source General MLLMs 59.9 69.1 60.8 62.2 72.3 60.2 68.0 60.3 65.3 72.0 Open-Source Spatial MLLMs - - - 54.6 - 65.1 72.1 61.2 69.1 68.5 69.4 60.7 65.9 69.3 44.3 48.2 44.0 48.4 52.2 48.4 42.2 - 41.1 42.0 48.0 48.4 51.1 52.2 Avg. 79.4 65.9 60.0 68.6 60.6 63.8 72.2 - - - 62.0 - 65.5 70.7 Method Comparison (Trained on STVQA-7K) 61.1 50.8 68.6 50.1 73.6 52.9 63.7 53.6 72.7 54.7 56.4 78.2 68.4 66.6 76.3 71.3 76.5 78.7 53.9 70.6 71.0 56.1 68.9 77. Relative Depth 78.2 67.7 54.0 52.4 71.0 - 73.4 61.3 67.7 62.9 58.9 82.3 59.9 65.3 66.9 55.6 66.9 64.5 75.0 72.6 Avg. 80.4 63.2 60.2 68.2 76.1 - 71.7 61.1 67.8 67.8 68.0 74.0 66.7 73.2 66.0 64.5 74.4 70.0 77.7 79.3 82.5 58.7 66.4 84.0 81.2 - 69. 60.8 67.8 72.7 77.0 65.7 73.4 81.1 65.0 73.4 81.8 75.5 80.4 86.0 Table 1: Performance over 2D & 3D Spatial Understanding Benchmarks across different model types. Top-1 & Top-2 accuracies are represented using bold text, and underlines."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate SPATIALTHINKER across 12 diverse spatial understanding and real-world VQA benchmarks, encompassing both 2D and 3D reasoning tasks. Our experiments are guided by two core questions: (Q1) Does our spatial VQA generation pipeline, combined with dense reward RL, improve general spatial reasoning in MLLMs? (Q2) Can MLLMs learn strong spatial capabilities from just 7K synthetic training samples, and how does this compare to models trained on orders-of-magnitude more data? Benchmarks. We evaluate on six core spatial benchmarks: CV-Bench 2D and 3D (Tong et al., 2024a), BLINK Spatial Relations and Relative Depth (Fu et al., 2024), 3DSRBench (Ma et al., 2024b), MMVP (Tong et al., 2024b), SpatialBench (Cai et al., 2024), and SpatialReasonerEval (Ma et al., 2025a), covering spatial relation understanding, depth, distance, counting, size, and egocentric 3D reasoning. To test generalization in real-world, embodied, and generalist VQA contexts, we use VStarBench (Wu & Xie, 2023), RealWorldQA (xAI, 2024), MME-RealWorld (Zhang et al., 2024), RoboSpatial-Home (Song et al., 2025) (Configuration and Compatibility only), MM-Star (Chen et al., 2024c), and HallusionBench (Guan et al., 2023). Baselines. We compare against proprietary MLLMs including GPT-4o (GPT-4O-0513) (Hurst et al., 2024) and Claude 3.5 Sonnet (CLAUDE-3.5-SONNET-0620) (Anthropic, 2025), open-source generalist models like Qwen2.5-VL (Bai et al., 2025), LLaVA-NeXT (Li et al., 2024b), Cambrian-1 (Tong et al., 2024a), and VLAA-Thinker (Chen et al., 2025a), and spatially-tuned open-source MLLMs such as SpaceLLaVA (AI & Mayorquin, 2025a; Chen et al., 2024a), SpatialRGPT (Cheng et al., 2024), RoboPoint (Yuan et al., 2024), SpaceThinker (AI & Mayorquin, 2025c), SpaceOm (AI & Mayorquin, 2025b), SpatialReasoner (Ma et al., 2025a), SpatialBot (Cai et al., 2024), Visionary-R1 (Xia et al., 2025), and SATORI-R1 (Shen et al., 2025a). In addition, we evaluate ablations on variants of our model trained with the STVQA-7K dataset: supervised fine-tuning (SFT) baseline, and sparse-reward RL baseline that optimizes only format and accuracy rewards, each weighted equally at 0.5, to isolate the effect of our dense spatial reward. 6 Figure 3: Qualitative comparison between GPT-4o and SpatialThinker-7B. GPT-4o often fails to distinguish objects at 3D relational levelfor example, confusing spatial relations such as beside, behind, and in front of, or missing fine-grained object details. Evaluation Setting. All models are evaluated in zero-shot setting using greedy decoding (temperature = 0.0). Models default prompting format is used where applicable (e.g., for VLAA-Thinker, SpaceOm, SpaceThinker). SpatialRGPT is evaluated with depth inputs; all other models use RGB. Accuracy is the primary evaluation metric. Our evaluation pipeline extends OpenVLThinker (Deng et al., 2025) to support new benchmarks and formats. Full benchmark descriptions, baseline details, and additional implementation specifics are provided in Appendix B. 4.1 RESULTS We evaluate SpatialThinker across six spatial reasoning and six generalist VQA benchmarks to assess its effectiveness in learning spatial understanding and real-world VQA from limited training data through dense reward supervision. Performance across Spatial Benchmarks. We evaluate SPATIALTHINKER across six spatial reasoning benchmarks that collectively span 2D relational understanding, 3D spatial alignment, counting, depth ordering, and distance comparison. As shown in Tables 1 and 2, SPATIALTHINKER-7B achieves strong and consistent performance across all spatial tasks. On CV-Bench, the model attains 7 an average accuracy of 78.2% across 2D and 3D tasks, nearing GPT-4os 79.4% while outperforming all other open-source models, and Claude 3.5 Sonnet. On the challenging 3DSRBench, which requires orientation and multi-object reasoning, it achieves 56.4%, surpassing GPT-4o by +12.1%. On BLINKs spatial relation and relative depth tasks, it achieves 86.0% and 72.6%, respectively, yielding 79.3% averageclosely matching GPT-4o (80.4%) and outperforming other spatial MLLMs like Spatial-RGPT-7B (74.0%), which uses depth inputs and 700K training samples. On SpatialBench, our model reaches 66.4%, approaching GPT-4os 67.0%. Model 70.7 71.3 85.8 84.1 67.0 63.2 Proprietary Models GPT-4o Claude 3.5 Sonnet Open-Source General & Spatial MLLMs MMVP SpatialReasonerEval SpatialBench Qwen2.5-VL-3B Qwen2.5-VL-7B VLAA-Thinker-7B SpaceThinker SpaceOm SpatialReasoner SATORI-R1 Visionary-R1 Despite being trained on just 7K synthetic samples and using only RGB inputs, SPATIALTHINKER-7B consistently outperforms open-source baselines, including VLAAThinker-7B, Cambrian-1-8B, Spatial-RGPT, SpaceLLaVA, and RoboPoint-13B, all of which are trained on orders of magnitude more data. Notably, it exceeds specialized spatial models as well: on CV-Bench 3D, it outperforms SpaceLLaVA-13B (78.7% vs. 68.5%), and on BLINK tasks, it surpasses Spatial-RGPT-7B by +5.3%, and SpatialBot by +11.5% despite their reliance on depth information. Further, SPATIALTHINKER-7B outperforms all models on MMVP, and all open-source baselines on SpatialReasonerEval that measures 3D spatial understanding tasks like depth and distance. These results highlight the effectiveness of our dense reward design in enabling generalizable spatial reasoning without the need for explicit geometric inputs or large-scale pretraining. Qualitative examples of model outputs are shown in Fig. 3, with additional comparisons in Appendix H. SpatialThinker-7B demonstrates stronger 3D spatial grounding and fine-grained object distinction. Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + Vanilla GRPO SpatialThinker-3B (Ours) Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + Vanilla GRPO SpatialThinker-7B (Ours) Table 2: Performance on additional spatial benchmarks. Top-1 & Top-2 accuracies are represented using bold text, and underlines. Method Comparison (Trained on STVQA-7K) 68.0 70.6 61.2 69.6 68.9 76.4 70.5 72. 49.9 62.5 66.2 57.9 58.6 59.2 60.3 59.8 67.0 72.3 75.3 63.0 66.3 64.0 67.7 70.3 56.3 56.9 61.5 63.5 64.2 66.4 67.5 69.3 76.5 70.8 79.6 82.7 62.7 68.3 69.0 68.3 74.3 78.0 Model MM-Star VStarBench RealWorldQA MME-RealWorld-Lite RoboSpatial-Home HallusionBench GPT-4o Claude 3.5 Sonnet Qwen2.5-VL-3B Qwen2.5-VL-7B VLAA-Thinker-7B SpaceThinker SpaceOm Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + Vanilla GRPO SpatialThinker-3B (Ours) Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + Vanilla GRPO SpatialThinker-7B (Ours) 64.7 65.1 55.9 63.9 63.8 54.5 57.7 53.9 56.7 57.6 63.2 63.4 65.9 Proprietary and Open-Source MLLMs 66.0 51.8 74.9 75.9 58.1 56.5 56. 75.4 60.1 58.2 68.4 66.4 61.6 53.3 51.6 45.2 41.9 44.1 44.6 - - Method Comparison (Trained on STVQA-7K) 73.3 74.3 78.0 78.0 73.9 81.7 64.8 64.4 66.3 65.4 66.6 69.2 43.0 46.7 46.5 47.4 46.3 48.3 68.4 57.0 58.7 70.6 68.9 52.6 68. 69.8 64.0 70.6 72.4 76.2 76.3 55.0 55.5 46.3 52.9 68.9 65.4 62.9 58.9 59.0 62.5 66.2 60.7 66.4 Table 3: Performance on VQA and Real-World benchmarks. Top-1 & Top-2 accuracies are represented using bold text, and underlines. Performance across Real-World and General VQA Benchmarks. We further assess our models generalization to real-world visual question answering using six diverse benchmarks: MM-Star, RealWorldQA, VStarBench, MME-RealWorld-Lite, RoboSpatial-Home, and HallusionBench  (Table 3)  . SPATIALTHINKER-7B achieves 65.9% on MM-Star, 81.7% on VStarBench, and 76.3% on RoboSpatial-Home, surpassing all open-source and proprietary baselines. It also performs competitively on hallucination-sensitive and real-world benchmarks, scoring 66.4% on HallusionBench, 69.2% on RealWorldQA, and 48.3% on MME-RealWorld-Lite benchmarks. These results show that training with dense spatial rewards generalizes beyond spatial benchmarks to real-world settings. Gains on MM-Star, RoboSpatial-Home, and VStarBench highlight the benefit of structured scene grounding, even with small synthetic training set, and reinforces our hypothesis that spatial grounding via reward optimization not only improves spatial reasoning but also enhances visual understanding in the wild. RL Training with Dense Rewards Enables Superior Generalization. To isolate the contributions of our dense spatial reward design, we compare against two ablation variants: supervised fine-tuning 8 (SFT) and reinforcement learning with sparse rewards (Vanilla GRPO) using only format and answer accuracy, trained on STVQA-7K. As shown in Table 4, SPATIALTHINKER-7B achieves an average accuracy of 71.2% across all 12 benchmarksexceeding the gains from SFT by +6.0% and the sparse GRPO variant by +3.2%. These gains are consistent across the 3B variant as well, where SPATIALTHINKER-3B outperforms its SFT and GRPO counterparts by +5.5% and +4.1% average gains, respectively. Notably, Vanilla GRPO provides modest improvements over the base model (+4.0 for 7B, +4.9 for 3B), but our dense spatial reward nearly doubles 1.8 this gain (+7.2% for 7B, +9.0% for 3B), underscoring the complementary learning signal provided by count and spatial objectives, along with lexicographic reward gating. Overall, these results affirm that structured reinforcement learning with dense spatial supervision significantly enhances the spatial and generic VQA capabilities of multimodal LLMs, with small fraction of high-quality data. Model Avg. Acc. (12) Base GPT-4o Claude 3.5 Sonnet GPT-4o Claude 3.5 Sonnet Qwen2.5-VL-3B Qwen2.5-VL-7B Proprietary and Base MLLMs 67.8 61.1 57.3 64.0 - - - - - - - - - - - - Method Comparison (Trained on STVQA-7K) Qwen2.5-VL-3B + SFT 60.8 Qwen2.5-VL-3B + Vanilla GRPO 62.2 SpatialThinker-3B (Ours) 66.3 Qwen2.5-VL-7B + SFT 65.2 Qwen2.5-VL-7B + Vanilla GRPO 68.0 SpatialThinker-7B (Ours) 71.2 Table 4: Average accuracy across all 12 benchmarks with relative improvements (). SpatialThinker models consistently outperform SFT and vanilla GRPO. -0.3 +1.1 +5.2 +4.1 +6.9 +10.1 +3.5 +4.9 +9.0 +1.2 +4.0 +7.2 -7.0 -5.6 -1.5 -2.6 +0.2 +3.4 Format + Accuracy Reward Components STVQA-7Kval 74.9 23.7 61.7 76.3 87.9(+13.0) + Spatial + Count + Lexicographic Gating & RoI Filtering + Filtered Dataset (pass@2) Reward Design Ablation. To validate our reward formulation, we conduct controlled ablation study on the STVQA-7Kval set, progressively introducing each component and constraint as shown in Table 5. Naively adding spatial rewards causes reward hacking behavior (23.7%), as models overgenerate cluttered boxes to exploit the CIoU reward. Introducing the count reward mitigates this (+38% relative gain), regularizing the object and relations count within the scene graph to match the ground-truth quantities. However, rewarding all scene objects biases the model toward exhaustive descriptions. To address this, we shift to local supervisionrewarding only Regions of Interest (RoIs) tied to question-relevant entitiesand apply lexicographic gating to ensure spatial rewards are granted only when the final answer is correct, preventing the model from over-optimizing intermediate process rewards at the expense of outcome accuracy. These adjustments recover and slightly exceed baseline performance (76.3%), while additional dataset filtering via pass@2 correctness verification with GPT-4o to retain only high-quality, validated samples (7K) yields further boost to 87.9%. This staged reward shaping process proves essential for stabilizing optimization and grounding learning in verifiable spatial reasoning. We discuss the full reward design process details in Appendix C. Table 5: Reward ablation on STVQA-7Kval. Progressive addition of constraints restores stable optimization and improves visual grounding quality. +2.3 +4.3 +9.3 Model Variant Spatial VQA Base Real-World VQA Base Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + GRPO SpatialThinker-3B Out-of-Distribution Generalization: Dense Rewards Enable Stronger Transfer. While both SFT and sparse-reward GRPO improve spatial reasoning over base models, their ability to generalize to out-of-distribution (OOD) real-world tasks is limited, when compared to SPATIALTHINKER models. As shown in Table 6, sparse-reward GRPO provides large spatial gains over its respective base model (+4.3% for 3B, +4.7% for 7B), but offers only marginal improvements on real-world benchmarks (+6.0 and +2.7 respectively)nearly matching or underperforming SFT (+5.9% for 3B, +2.9% for 7B). In contrast, SPATIALTHINKER, trained with dense spatial and count rewards, achieves significantly stronger OOD generalization: +8.5 for 3B and +5.2 for 7B, outperforming all baselines at both scales. Notably, SPATIALTHINKER-7B provides nearly double the real-world VQA benchmarks gains compared to sparse-reward GRPO (+5.2% vs. +0.3 Qwen2.5-VL-7B + SFT +4.7 Qwen2.5-VL-7B + GRPO SpatialThinker-7B +8.3 Table 6: Average accuracy gains () over respective base models on (6) spatial and (6) real-world VQA (OOD) benchmarks. +5.9 +6.0 +8.5 +2.9 +2.7 +5.2 +2.7%), highlighting the robustness of our dense reward framework. The combination of structured reasoning formats and lexicographically gated dense rewards encourages models to internalize spatial priors and compositional patterns that transfer effectively to out-of-distribution tasks, even without explicit domain-specific supervision. Appendix further demonstrates generalization to abstract reasoning tasks."
        },
        {
            "title": "5 RELATED WORK",
            "content": "3D Spatial Reasoning in MLLMs. While MLLMs have advanced core visual tasks (Bai et al., 2025; Deitke et al., 2025; Du et al., 2025; Hurst et al., 2024; Li et al., 2024b; Lin et al., 2024), their spatial reasoning abilities remain limited (Kamath et al., 2023; Li et al., 2024a; Ma et al., 2024b; Mirzaee et al., 2021; Tong et al., 2024b; Yamada et al., 2023; Yang et al., 2025a), partly due to datasets focused more on perception than relational grounding (Hudson & Manning, 2019). To address this, recent work integrates 3D signals via point clouds or multi-view reconstructions (Hong et al., 2023a;c), or world models with physical priors (Wang et al., 2023; 2024). Large-scale efforts like SpatialVLM (Chen et al., 2024a), SpatialPIN (Ma et al., 2024a), SpatialBot (Cai et al., 2024), and SpatialRGPT (Cheng et al., 2024) use hundred thousand to millions of 3D-augmented samples or RGB-D scene graphs. Others like MM-Spatial (Daxberger et al., 2025), SpatialLLM (Ma et al., 2025b), and SpaRE (Ogezi & Shi, 2025) similarly scale synthetic or reconstructed 3D data. However, these methods are often data-heavy, rely on specialized inputs, or fall short on structured relational modeling. SPATIALTHINKER attains robust relational, and regional reasoning using just 7K VQA samples trained with RL with dense spatial rewards. Structured Visual Grounding in MLLMs. Scene graphs offer structured objectrelation representations and have long supported visual reasoning (Gu et al., 2023; Hildebrandt et al., 2020; Wald et al., 2020). Classical Scene Graph Generation (SGG) relies on detectionrelation pipelines (Carion et al., 2020; Cong et al., 2023), but struggles with multi-role and open-vocabulary generalization. Recent LLM-based methods like LLM4SGG and GPT4SGG extract structured graphs from captions (Chen et al., 2023; Kim et al., 2024), while open-vocabulary SGG approaches use MLLMs to generalize beyond fixed ontologies (Chen et al., 2024b; Li et al., 2023). RL-trained models like R1-SGG and Relation-R1 directly generate scene graphs via dense structural or cognitive rewards (Chen et al., 2025c; Li et al., 2025), emphasizing the value of structured supervision. In parallel, region-aware MLLMs including KOSMOS-2 (Peng et al., 2023), GLaMM (Rasheed et al., 2024), and Ferret (You et al., 2023), enhance spatial grounding via bounding boxes and region-text alignment. SPATIALTHINKER extends these ideas by grounding reasoning in scene subgraphs focused on the questions region of interest, combining structured understanding with reward-guided spatial reasoning. Multimodal Reinforcement Learning. RL has been increasingly applied to enhance reasoning in MLLMs, extending chain-of-thought prompting (Wei et al., 2022) with verifiable rewards across tasks like math reasoning (Meng et al., 2025; Yang et al., 2025b), classification and grounding (Liu et al., 2025b), semantic segmentation (Liu et al., 2025a), regional understanding (Shen et al., 2025a), and open-vocabulary detection or referring expression comprehension (Pinto et al., 2023; Shen et al., 2025b). Spatial RL has also emerged, with SVQA-R1 using view-consistency rewards (Wang & Ling, 2025) and SpatialReasoner introducing coordinate-aware supervision (Ma et al., 2025a; Shen et al., 2025b). However, most prior methods rely on sparse signals like final accuracy or coarse location cues, offering limited support for fine-grained spatial reasoning. SPATIALTHINKER introduces dense, multi-objective reward framework encompassing regional subgraph construction, object localization, relational grounding, object counting, and final correctness."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced SPATIALTHINKER, 3D-aware MLLM that achieves strong spatial reasoning by combining scene graph grounding with dense spatial rewards through RL. Trained on just 7K samples, it surpasseses proprietary and open-sourced MLLMs on spatial, real-world, and generic VQA benchmarks while outperforming models trained on orders of magnitude more data, specifically for spatial understanding. Dense spatial rewards nearly double the gains of standard RL via GRPO, underscoring the value of rich supervision signals. While our approach relies on explicit scene graphs, future work could explore implicit spatial reasoning within latent tokens. Additional directions 10 include extending our reward framework to spatiotemporal reasoning, real-world tasks like web navigation, and developing unified multi-objective policies covering diverse visual tasks."
        },
        {
            "title": "7 ACKNOWLEDGEMENT",
            "content": "We acknowledge EuroHPC Joint Undertaking for awarding the project ID EHPC-AI-2024A04-042 access to MareNostrum5 at BSC, Spain, and the Anthropic External Researcher Access Program for providing API credits that enabled the generation of our STVQA-7K dataset."
        },
        {
            "title": "REFERENCES",
            "content": "Remyx AI and Salma Mayorquin. Spacellava models. Hugging Face, March 2025a. URL https: //huggingface.co/remyxai/SpaceLLaVA. Remyx AI and Salma Mayorquin. Spaceom models. Hugging Face, 2025b. URL https:// huggingface.co/remyxai/SpaceOm. Remyx AI and Salma Mayorquin. Spacethinker models. Hugging Face, April 2025c. URL https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B. Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet. Anthropic, October 2024. Anthropic. System card: Claude opus 4 & claude sonnet 4. Anthropic System Cards, May 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2025. Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. arXiv preprint arXiv:2406.13642, 2024. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213229. Springer, 2020. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1445514465, 2024a. Guikun Chen, Jin Li, and Wenguan Wang. Scene graph generation with role-playing large language models. ArXiv, abs/2410.15364, 2024b. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. ArXiv, abs/2504.11468, 2025a. Kaiyuan Chen, Shuangyu Xie, Zehan Ma, and Ken Goldberg. Robo2vlm: Visual question answering from large-scale in-the-wild robot manipulation datasets, 2025b. Lin Chen, Jinsong Li, Xiao wen Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models? ArXiv, abs/2403.20330, 2024c. Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, and Changwen Chen. Gpt4sgg: Synthesizing scene graphs from holistic and region-specific narratives. arXiv preprint arXiv:2312.04314, 2023. 11 Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, and Chang Wen Chen. Compile scene graphs with reinforcement learning. arXiv preprint arXiv:2504.13617, 2025c. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. Yuren Cong, Michael Ying Yang, and Bodo Rosenhahn. Reltr: Relation transformer for scene graph generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):1116911183, 2023. Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, et al. Mm-spatial: Exploring 3d spatial understanding in multimodal llms. arXiv preprint arXiv:2503.13111, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Jiong Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, Ruiqi Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, Wangding Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xi aokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Zehui Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948, 2025. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 91104, 2025. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: Complex vision-language reasoning via iterative sft-rl cycles. 2025. Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, 2023. Kimi Team Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao 12 Ding, Hao-Xing Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianling Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Long Yu, Mengfan Dong, Meng Dong, Nuo Xu, Peng Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiao feng Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan-Qing Zhong, Yang Li, Yan-Ni Hu, Yanru Chen, Ye-Jia Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuan-Qing Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-vl technical report. ArXiv, abs/2504.07491, 2025. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. ArXiv, abs/2404.12390, 2024. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, nathan lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. ArXiv, abs/2503.01307, 2025. Jensen Gao, Bidipta Sarkar, F. Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1246212469, 2023. Google. Gemini 2.0 flash: Model card. Technical Report, April 2025. Published April 15, 2025. Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Ramalingam Chellappa, Chuang Gan, Celso de Melo, Joshua B. Tenenbaum, Antonio Torralba, Florian Shkurti, and Liam Paull. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 50215028, 2023. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large visionlanguage models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1437514385, 2023. Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, and Stephan Günnemann. Scene graph reasoning for visual question answering. ArXiv, abs/2007.01072, 2020. Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B. Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 92029212, 2023a. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023b. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023c. Audrey Huang, Wenhao Zhan, Tengyang Xie, Jason D. Lee, Wen Sun, Akshay Krishnamurthy, and Dylan J. Foster. Correcting the mythos of kl-regularization: Direct alignment without overoptimization via chi-squared preference optimization. ArXiv, abs/2407.13399, 2024. Chen Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 10608 10615, 2022. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaoshen Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. ArXiv, abs/2503.06749, 2025. 13 Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Rich Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization. ArXiv, abs/2504.16054, 2025. Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats\" up\" with vision-language models? investigating their struggle with spatial reasoning. arXiv preprint arXiv:2310.19785, 2023. Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, and Chanyoung Park. Llm4sgg: Large language models for weakly supervised scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2830628316, 2024. Mikhail Konenkov, Artem Lykov, Daria Trinitatova, and Dzmitry Tsetserukou. Vr-gpt: Visual language model for intelligent virtual reality applications. ArXiv, abs/2405.11537, 2024. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, and Ivan Vulic. Topviewrs: Vision-language models as top-view spatial reasoners. arXiv preprint arXiv:2406.02537, 2024a. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. ArXiv, abs/2407.07895, 2024b. Lin Li, Jun Xiao, Guikun Chen, Jian Shao, Yueting Zhuang, and Long Chen. Zero-shot visual relation detection via composite visual cues from large language models. ArXiv, abs/2305.12476, 2023. Lin Li, Wei Chen, Jiahui Li, Kwang-Ting Cheng, and Long Chen. Relation-r1: Progressively cognitive chain-of-thought guided reinforcement learning for unified relation comprehension. arXiv preprint arXiv:2504.14642, 2025. Rongjie Li, Songyang Zhang, Dahua Lin, Kai Chen, and Xuming He. From pixels to graphs: Openvocabulary scene graph generation with vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2807628086, 2024c. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2668926699, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023. Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. ArXiv, abs/2503.06520, 2025a. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiao wen Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. ArXiv, abs/2503.01785, 2025b. 14 Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, and Andrew Markham. Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. Advances in neural information processing systems, 37:6880368832, 2024a. Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Alan L. Yuille, and Jieneng Chen. 3dsrbench: comprehensive 3d spatial reasoning benchmark. ArXiv, abs/2412.07825, 2024b. Wufei Ma, Yu cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jieneng Chen, Jianwen Xie, and Alan L. Yuille. Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning. ArXiv, abs/2504.20024, 2025a. Wufei Ma, Luoxin Ye, Celso de Melo, Alan Yuille, and Jieneng Chen. Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1724917260, 2025b. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. 2025. Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjmashidi. Spartqa:: textual question answering benchmark for spatial reasoning. arXiv preprint arXiv:2104.05832, 2021. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Ho Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, KuangHuei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Manfred Otto Heess, Chelsea Finn, Sergey Levine, and Brian Ichter. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. ArXiv, abs/2402.07872, 2024. Michael Ogezi and Freda Shi. Spare: Enhancing spatial reasoning in vision-language models with synthetic data. arXiv preprint arXiv:2504.20648, 2025. Yi Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. ArXiv, abs/2503.07536, 2025. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. André Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas Beyer, and Xiaohua Zhai. Tuning computer vision models with task rewards. ArXiv, abs/2302.08242, 2023. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1300913018, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. Chuming Shen, Wei Wei, Xiaoye Qu, and Yu Cheng. Satori-r1: Incentivizing multimodal reasoning with spatial grounding and verifiable rewards. ArXiv, abs/2505.19094, 2025a. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. ArXiv, abs/2504.07615, 2025b. Joar Skalse, Lewis Hammond, Charlie Griffin, and Alessandro Abate. Lexicographic multi-objective reinforcement learning. ArXiv, abs/2212.13769, 2022. 15 Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1576815780, 2025. Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, and Kai Chen. Lego-puzzles: How good are mllms at multi-step spatial reasoning? ArXiv, abs/2503.19990, 2025. Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montse Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauzá, Michiel Blokzijl, Steven Bohez, Konstantinos Bousmalis, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Os car Chang, José Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, David DAmbrosio, Sudeep Dasari, Todor Davchev, Coline Devin, Norman Di Palo, Tianli Ding, Adil Dostmohamed, Danny Driess, Yilun Du, Debidatta Dwibedi, Michael Elabd, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Kelly Fu, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Leonard Hasenclever, Nico-351 las Heess, Brandon Hernaez, Alex Herzog, R. Alex Hofer, Jan Humplik, Atil Iscen, Mithun George Jacob, Deepali Jain, Ryan C. Julian, Dmitry Kalashnikov, Mustafa Emre Karagozler, Stefani Karp, Chase Kew, Jerad Kirkland, Sean Kirmani, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Jacky Liang, Yixin Lin, Sharath Maddineni, Anirudha Majumdar, Assaf Hurwitz Michaely, Robert Moreno, Michael Neunert, Francesco Nori, Carolina Parada, Emilio Parisotto, Peter Pastor, Acorn Pooley, Kanishka Rao, Krista Reymann, Dorsa Sadigh, Stefano Saliceti, Pannag R. Sanketi, Pierre Sermanet, Dhruv Shah, Mohit Sharma, Kathryn Shea, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Jost Tobias Springenberg, Rachel Sterneck, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Vincent Vanhoucke, Jake Varley, Grace Vesom, Giulia Vezzani, Oriol Vinyals, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Yuxiang Yang, Rui Yao, Sergey Yaroshenko, Wenhao Yu, Wentao Yuan, Jingwei Zhang, Tingnan Zhang, Allan Zhou, and Yuxiang Zhou. Gemini robotics: Bringing ai into the physical world. ArXiv, abs/2503.20020, 2025. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. ArXiv, abs/2406.16860, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024b. Jean Vassoyan, Nathanael Beau, and Roman Plaud. Ignore the kl penalty! boosting exploration on critical tokens to enhance rl fine-tuning. ArXiv, abs/2502.06533, 2025. Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 39603969, 2020. Peiyao Wang and Haibin Ling. Svqa-r1: Reinforcing spatial reasoning in mllms via view-consistent reward optimization. ArXiv, abs/2506.01371, 2025. Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, and Alan Yuille. 3d-aware visual question answering about parts, poses and occlusions. Advances in Neural Information Processing Systems, 36:5871758735, 2023. Xingrui Wang, Wufei Ma, Angtian Wang, Shuo Chen, Adam Kortylewski, and Alan Yuille. Compositional 4d dynamic scenes understanding with physics priors for video question answering. arXiv preprint arXiv:2406.00622, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022. 16 Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1308413094, 2023. xAI. Grok-1.5 vision preview. xAI Blog, apr 2024. Jiaer Xia, Y.-F. Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. ArXiv, abs/2505.14677, 2025. Tong Xiao, Xin Xu, Zhenya Huang, Hongyu Gao, Quan Liu, Qi Liu, and Enhong Chen. Advancing multimodal reasoning capabilities of multimodal large language models via visual perception reward. ArXiv, abs/2506.07218, 2025. Yutaro Yamada, Yihan Bao, Andrew Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluating spatial understanding of large language models. arXiv preprint arXiv:2310.14540, 2023. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1063210643, 2025a. Scott Cheng-Hsin Yang, Daniel Wolpert, and Máté Lengyel. Theoretical perspectives on active sensing. Current Opinion in Behavioral Sciences, 11:100108, 2016. ISSN 2352-1546. Computational modeling. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. ArXiv, abs/2503.10615, 2025b. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Honglin Yu, Weinan Dai, Yuxuan Song, Xiang Wei, Haodong Zhou, Jingjing Liu, Wei Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yong-Xu Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale. ArXiv, abs/2503.14476, 2025. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. ArXiv, abs/2406.10721, 2024. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Jun Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, and Tien-Ping Tan. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? ArXiv, abs/2408.13257, 2024. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. ArXiv, abs/2403.13372, 2024. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. arXiv preprint arXiv:2501.12345, 2025. Zhaohui Zheng, Ping Wang, Dongwei Ren, Wei Liu, Rongguang Ye, Qinghua Hu, and Wangmeng Zuo. Enhancing geometric factors in model learning and inference for object detection and instance segmentation. IEEE Transactions on Cybernetics, 52:85748586, 2020. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros \"aha moment\" in visual reasoning on 2b non-sft model. ArXiv, abs/2503.05132, 2025. Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, and Huaizu Jiang. Struct2d: perception-guided framework for spatial reasoning in large multimodal models. ArXiv, abs/2506.04220, 2025."
        },
        {
            "title": "APPENDIX",
            "content": "A STVQA-7K: DATASET CONSTRUCTION High-quality spatial VQA datasets remain scarce, as most existing benchmarks either lack grounded scene-graph annotations (i.e., explicit spatial coordinates for objects and relations) or fail to comprehensively cover both 2D and 3D spatial reasoning categories. Visual Genome (Krishna et al., 2017) provides dense, human-annotated scene graphs that support strict grounding of both question generation and answer verification within unified representational framework. Using Visual Genome, we synthetically constructed spatial visual question answering dataset called SPATIALTHINKER Visual Question Answering dataset i.e., STVQA-7K comprising 7,587 samples, fully grounded in human-annotated scene graphs (Krishna et al., 2017), which we employed for post-training the SPATIALTHINKER models. Importantly, our pipeline is scalable and can be extended to generate up to 108K samples, the maximum supported by Visual Genome, enabling future large-scale post-training or RL fine-tuning. The original VG150 predicate set is limited to 50 relations, missing several important categories such as positional relations (e.g., left, right, beside), distance-based relations (e.g., near, far, next to), comparative size (e.g., smaller, taller, bigger), orientation (e.g., facing towards/away), and containment (e.g., inside, beneath). To address this gap, we extended the scene graph relation space with an additional 34 predicates, ensuring richer spatial coverage in both 2D and 3D reasoning. Bounding box coordinates are retained in absolute pixel space, rather than normalized values, to preserve real-world scale and spatial alignment, to enable both improved spatial reasoning and effective use of CIoU-based supervision during reward optimization. The dataset construction pipeline proceeds in three stages: (1) synthetic question generation from ground-truth scene graphs, (2) automated quality filtering with external verification, and (3) scene graph adaptation for regional alignment with individual questions. Figure 4: STVQA-7K dataset construction pipeline. Synthetic Question Generation. Visual Genome scene graphs serve as our foundational ground truth, providing object categories, bounding boxes, and relational triplets for over 150,000 images. We synthetically generate question-answer pairs for given scene graph data using Claude Sonnet 4 (Anthropic, 2025), synthesizing multiple-choice questions based on the salient objects and meaningful spatial relations explicitly present in each graph. Each question-answer pair is accompanied with rating generated out of 10 and the difficulty level. Our question generation encompasses nine distinct spatial reasoning categories: spatial relations (above, behind, near, etc.), physical reach and interaction (holding, touching), comparative size, orientation from specific viewpoints, instance location within image frames, depth ordering relative to the camera, distance comparisons to reference objects, object counting, and existence verification. This comprehensive taxonomy spans both 2D and 3D spatial understanding, providing broad coverage of visual-spatial reasoning capabilities. To promote robust perception, we also include questions involving objects that are partially visible or occluded in the scene, encouraging the model to reason about spatial arrangements and fine-grained details. For each question, we generate rating out of 10. Quality Filtering and Validation. To ensure semantic correctness at scale, we implement consistency-based verification procedure using GPT-4o (Hurst et al., 2024) as an external validation model. For each generated question-answer pair, we assess agreement between the external model and 18 Figure 5: Examples of generated QA pairs across the nine spatial reasoning categories in STVQA-7K. Each category highlights distinct reasoning skills, ranging from relative spatial relations and depth ordering to distance, size, orientation, reach, location, count and existence. 19 our synthetic ground truth label using pass@2 criterion. Questions that fail this initial consistency check undergo additional evaluation with two supplementary model responses. Items for which all four collected responses disagree with the generated label are discarded as potentially incorrect or ambiguous. This filtering process begins with 56,224 initially generated questions by Claude Sonnet 4 (Anthropic, 2025). We select the 10,000 highest-rated samples based on the questions complexity and rating towards its contribution to enhance spatial intelligence as judged by Claude Sonnet 4. Following consistency filtering, we retain 6,895 training samples and 692 validation samples ( 75%), indicating high label reliability. The final set consists of 50% samples from the relation category, and the remaining 50% distributed across the eight other categories. To prevent positional bias, answers are uniformly distributed across options A, B, C, and D. Figure Figure 2 illustrates the distribution of QA types in STVQA-7K, highlighting the emphasis on spatial relations while maintaining balanced coverage across the remaining reasoning categories. Representative examples of generated QA pairs across the nine spatial reasoning categories are shown in Figure 5, illustrating the diversity of question types in STVQA-7K. Scene Graph Adaptation. Since each question focuses on specific objects and relationships within the broader scene, we derive question-aligned scene subgraphs that capture only the relevant spatial context. For each question, we extract content words through tokenization and lemmatization to obtain both singular and plural word forms. We then filter the original scene graph to retain only object nodes whose labels appear in the extracted question vocabulary. Relational triplets are preserved when both the subject and object entities are retained and the predicate appears in the question context. The resulting focused scene graph representations enable training the model to generate question-aligned region-of-interest subgraphs, encouraging it to localize attention, ground reasoning in relevant entities and relations, and ultimately learn where to focus within complex visual scenes."
        },
        {
            "title": "B EXPERIMENTAL SETUP DETAILS",
            "content": "This section presents comprehensive evaluations of SPATIALTHINKER across multiple spatial reasoning benchmarks, demonstrating the effectiveness of our multi-objective dense reward design and data-efficient training approach. B.1 IMPLEMENTATION DETAILS We build SPATIALTHINKER upon two strong open-source multimodal base models: Qwen2.5-VL3B and Qwen2.5-VL-7B Bai et al. (2025), using them as backbones for policy optimization with reinforcement learning. No supervised fine-tuning is performed prior to RL training on our STVQA7K dataset (Section 3.3). We employ GRPO Shao et al. (2024) as the advantage estimator as described in Section 3.2, using rollout size of 8 samples per query and sampling temperature of 1.0. The models are trained with maximum context length of 16,384 tokens. The rollout batch size is set to 512, and the global batch size is 128. We train for 75 training steps i.e., 5 training episodes) on 4 NVIDIA H100 80GB GPUs. Training time totals around 13 hours for the 3B model and 15 hours for the 7B model. The models are trained on high-resolution image inputs ranging from 512512 to 20482048 pixels, to preserve fine-grained spatial information. All model parameters, including the vision encoder, are updated during training. We use the AdamW optimizer with bf16 precision, learning rate of 1 106, and weight decay of 1 102. The KL penalty coefficient is set to 102. STVQA-7K is partitioned with 90/10 trainvalidation split. B.2 EXPERIMENTAL SETUP We evaluate SPATIALTHINKER across diverse suite of 12 spatial understanding and real-world VQA benchmarks, covering both 2D and 3D understanding aspects to assess fine-grained spatial reasoning capabilities and real-world generalization. We compare against both proprietary and open-source baselines, including models specifically trained for spatial reasoning tasks. Our experiments address two key questions: (Q1) Does our spatial VQA data generation pipeline, combined with dense reward RL, improve MLLMs general spatial reasoning capabilities? (Q2) How effectively can MLLMs 20 learn spatial understanding from just 7K synthetic training samples, and how does this compare to models trained on orders-of-magnitude larger datasets? Benchmarks. We evaluate models across six core spatial benchmarks, and six general-purpose VQA and real-world understanding datasets. The spatial benchmarks includes CV-Bench (Tong et al., 2024a) that measures 2D spatial relations, object counting, depth ordering, and distance reasoning. BLINKs Spatial Relations and Relative Depth tasks (Fu et al., 2024) test directional and positional understanding, and fine-grained point-level depth perceptionparticularly challenging as SPATIALTHINKER receives no explicit point-level supervision during training 3DSRBench (Ma et al., 2024b) assesses egocentric 3D spatial reasoning via relational and multi-object comparisons. MMVP (Tong et al., 2024b) examines visual pattern recognition across attributes such as orientation, positional relations, existence, viewpoint, and size. SpatialBench (Cai et al., 2024) assesses general spatial comprehension across counting, existence, positional relationships, physical interactions such as reach, and size comparisons. Finally, SpatialReasonerEval (Ma et al., 2025a) emphasizes depth and distance reasoning within 3D spatial tasks. To assess broader generalization, we further evaluate models on six diverse real-world benchmarks. VStarBench (Wu & Xie, 2023) measures accurate localization and recognition of key objects in complex natural scenes. RealWorldQA (xAI, 2024) requires integrating visual inputs with commonsense and multi-step reasoning for real-world understanding. MME-RealWorld (Zhang et al., 2024) spans five challenging domains including optical character recognition in the wild, remote sensing, diagram and table interpretation, autonomous driving, and scene monitoring. RoboSpatial-Home (Song et al., 2025) simulates embodied spatial reasoning tasks involving object-object relationships, compatibility, and reference-frame switching (ego-centric, object-centric, and world-centric). We only use Configuration and Compatibility subsets of RoboSpatial-Home. MM-Star (Chen et al., 2024c) provides holistic benchmark covering math, logical reasoning, instance recognition, and fine/coarse visual perception. HallusionBench (Guan et al., 2023) evaluates hallucination resistance in multimodal models, requiring accurate visual grounding to counteract entangled linguistic or perceptual illusions. Together, these benchmarks allow us to probe spatial and perceptual reasoning across synthetic, embodied, and naturalistic settings. Closed-Source MLLM Baselines. Among proprietary models, we evaluate GPT-4o (GPT-4O-0513) (Hurst et al., 2024) and Claude 3.5 Sonnet (CLAUDE-3.5-SONNET-0620) (Anthropic, 2024), which represent the current state-of-the-art in commercial multimodal reasoning. These serve as upper bounds for spatial generalization under non-public training regimes. Open-Source Generalist MLLM Baselines. We compare against generalist open-source MLLMs including Qwen2.5-VL 3B and 7B models (Bai et al., 2025), LLaVA-NeXT (Li et al., 2024b), Cambrian-1 (Tong et al., 2024a), and VLAA-Thinker (3B and 7B) (Chen et al., 2025a). These models represent state-of-the-art vision-language architectures, offering strong general visual reasoning but without specific spatial tuning. Open-Source Spatial MLLM Baselines. We benchmark against specialized open-source models designed for spatial reasoning: SpaceLLaVA-13B AI & Mayorquin (2025a); Chen et al. (2024a) public re-implementation of SpatialVLM, SpatialRGPT-7B (Cheng et al., 2024) incorporates region-level supervision and explicit depth maps into training, RoboPoint-13B (Yuan et al., 2024) which instruction-tunes an MLLM to predict image key-point affordances for robotics and spatial affordance tasks, SpaceThinker (AI & Mayorquin, 2025c), fine-tuned VLAA-Thinker model for spatial reasoning, and its improved successor SpaceOm (AI & Mayorquin, 2025b), which incorporates deeper chain-of-thought traces and Robo2VLM data (Chen et al., 2025b). Other baselines include SpatialReasoner (Ma et al., 2025a) trained with RL and explicit 3D representations, SpatialBot (Cai et al., 2024), which integrates RGB and depth inputs for robust spatial perception, Visionary-R1 (Xia et al., 2025) which mitigates shortcut learning in visual reasoning by enforcing captioning before reasoning without reliance on chain-of-thought data, and SATORI-R1 (Shen et al., 2025a) which decomposes visual question answering into verifiable stages with explicit rewards for improved spatial grounding and reasoning accuracy. In addition to the above, we compare against our training variants including supervised fine-tuning (SFT) baselines and vanilla GRPO trained with sparse rewards (accuracy and format only) to isolate the contribution of our dense spatial reward framework. 21 Evaluation Setting. We report accuracy as the primary evaluation metric across all benchmarks. All models are evaluated under zero-shot settings, using greedy decoding (temperature = 0.0, max_new_tokens = 2048) to ensure deterministic and reproducible outputs. For models with specific reasoning templates such as VLAA-Thinker, SpaceThinker, and SpaceOm, we utilize their corresponding structured prompts. In line with their original training setup, SpatialRGPT receives depth inputs, while all other models are evaluated using RGB images alone. Our evaluation pipeline builds upon OpenVLThinkers evaluation framework (Deng et al., 2025), adapted to support our new benchmark and dataset formats. B.3 SPATIALTHINKER PROMPT FORMAT We use structured prompt to guide the model through four-stage reasoning process, explicitly separated using the tags <observe>, <scene>, <think>, and <answer>. This format is enforced during training via binary format reward Rf {0, 1}, with weight wformat = 0.1, which verifies the presence, ordering, and validity of all required tags. The <scene> section must contain JSON-encoded subgraph with object IDs, bounding boxes, and relational triplets, while the final answer must be clearly placed within the <answer> tags. {Width} Each prompt also includes the input image dimensions in the form Image size: {Height}, which are dynamically replaced with actual values. Including this information helps the model constrain predicted bounding box coordinates within image bounds, enabling better spatial localization. These coordinates are directly evaluated using IoU-based spatial rewards such as Complete IoU (CIoU), making dimension-aware prediction essential for optimizing structured spatial grounding. SpatialThinker Prompt You FIRST observe the image in <observe> </observe> tags, then visualise the relevant scene graph in <scene> </scene> tags, followed by thinking about the reasoning process as an internal monologue within <think> </think> tags and then provide the final answer. The final answer MUST BE put within <answer> </answer> tags, and only return the final choice including the correct option and answer within the answer tags, e.g., <answer> (C) The red cube is left of the green sphere </answer>. Image size: {Width} {Height} B.4 DETAILS ON SFT TRAINING To establish comprehensive baseline for comparison with our reinforcement learning approach, we conduct supervised fine-tuning (SFT) experiments using the same base models (Qwen2.5-VL-3B and Qwen2.5-VL-7B) and training dataset (STVQA-7K). The SFT implementation utilizes LLaMAFactory framework (Zheng et al., 2024) with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. The training configuration employs LoRA with rank 8 applied to all available modules within the model architecture, enabling comprehensive adaptation while maintaining computational efficiency. Models are trained for 3 epochs totaling 645 training steps, using context window length of 2048 tokens. We adopt BF16 mixed precision training with learning rate of 1 104, following cosine learning rate schedule with warmup ratio of 0.1. For the SFT experiments, we train models directly on question-answer pairs without intermediate reasoning traces or chain-of-thought prompting. This design choice reflects the practical constraint that generating ground-truth reasoning traces would require additional dataset processing, annotation, and API credits budget. In contrast, reinforcement learning approaches with verifiable rewards (RLVR) naturally enables training with answer supervision alone, as the model learns to generate its own reasoning strategies through environmental feedback rather than imitating pre-specified reasoning patterns. The SFT baseline serves critical role in our experimental evaluation, providing direct evidence of the generalization advantages offered by reinforcement learning with dense spatial rewards compared to traditional supervised learning on the same dataset. 22 B.5 DETAILS ON RL TRAINING We implement reinforcement learning training using the EasyR1 framework (Zheng et al., 2025), building upon Qwen2.5-VL-3B and Qwen2.5-VL-7B as base models without any prior supervised fine-tuning. This direct application of RL to the base models enables us to isolate the effects of reward-driven learning from potential confounding factors introduced by intermediate training stages. Additionally, performing an SFT stage prior to RL would require generating ground-truth reasoning traces, which is limited by API budget. Moreover, explicit reasoning supervision is not strictly necessaryour multi-objective dense spatial rewards encourage the model to acquire structured reasoning and self-reflection abilities directly during RL training. The training employs Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as the advantage estimation method, configured with rollout size of 8 samples per query at sampling temperature of 1.0. This configuration balances exploration diversity with computational efficiency, allowing the model to discover multiple reasoning strategies while maintaining stable convergence. The training process utilizes rollout batch size of 512 and global batch size of 128, processing data through 75 training steps (approximately 5 training episodes) to achieve convergence. The entire training pipeline runs on 4 NVIDIA H100 80GB GPUs, requiring approximately 13 hours for the 3B model and 15 hours for the 7B variant. To preserve fine-grained spatial information critical for accurate object localization and spatial reasoning, models process high-resolution image inputs ranging from 512 512 to 2048 2048 pixels. The training configuration updates all model parameters including the vision encoder, enabling comprehensive adaptation to spatial reasoning tasks. Optimization employs AdamW with BF16 mixed precision, conservative learning rate of 1 106, and weight decay of 1 102. The KL penalty coefficient is set to 102 to prevent excessive divergence from the base model distribution while allowing sufficient exploration for spatial reasoning strategies. The training utilizes 90/10 train-validation split of the STVQA-7K dataset, with maximum context length of 16,384 tokens to accommodate detailed scene descriptions and reasoning traces. For baseline comparisons, we train vanilla GRPO models (Qwen2.5-VL-3B + Vanilla GRPO and Qwen2.5-VL-7B + Vanilla GRPO) using simplified reward structure consisting solely of accuracy (wacc = 0.5) and format rewards (wf ormat = 0.5), without the spatial grounding and count penalty components. This configuration represents standard RLVR approaches that rely on sparse finalanswer supervision (Chen et al., 2025a; DeepSeek-AI et al., 2025; Shen et al., 2025b). The full multi-objective reward design employed for SPATIALTHINKER training, incorporating format, count, accuracy, and spatial rewards with lexicographic gating, is detailed in Section 3.1. The substantial performance improvements of SPATIALTHINKER over vanilla GRPO baselines demonstrate the critical importance of dense spatial supervision in teaching models to perform visually-grounded reasoning. B.5.1 SPATIALTHINKER RL TRAINING CURVES Throughout reinforcement learning, all four reward components: format, accuracy, count, and spatial; demonstrate consistent and interpretable improvement, reflecting stable learning under our lexicographically gated, multi-objective reward structure. The format reward quickly converges early in training, indicating the model learns to produce structurally valid outputs that adhere to the required scene-grounded reasoning format. Accuracy steadily improves across steps, highlighting the models increasing ability to provide correct answers. Count reward rises consistently, showing that the model learns to focus on predicting only question-relevant objects and relations, rather than describing the entire scene. The spatial reward also improves gradually, indicating better object localization and grounding, as the model increasingly aligns predicted bounding boxes with ground truth annotations. Together, these trends reflect how each reward component scaffolds different stage of the reasoning process, enforcing structure, correctness, focus, and grounding in tandem. Response length initially declines, then rises again as it begins producing more deliberate, structured reasoning, signaling an aha moment where the model starts to produce more deliberate reasoning traces (DeepSeek-AI et al., 2025; Zhou et al., 2025). This emergent behavior suggests the development of internal problem-solving strategies, as the model learns to spend more thinking time before answering, consistent with the emergence of self-reflection and structured planning in its spatial reasoning process. 23 (a) Format Reward (b) Count Reward (c) Accuracy Reward (d) Spatial Reward (e) Response Length Figure 6: RL training dynamics of SPATIALTHINKER. All reward components (ad) improve consistently, reflecting stable optimization. Response length (e) shows non-monotonic trend, indicating emergent reasoning strategies."
        },
        {
            "title": "C REWARD DESIGN PROCESS",
            "content": "This section details our approach to designing robust reward system that guides models toward genuine spatial reasoning while preventing degenerate solutions. Our reward design emerged from iterative refinement to address systematic reward hacking behaviors observed during training. Early experiments revealed that models readily exploit loopholes in reward functionsparticularly when spatial localization rewards were provided without proper constraints. To empirically motivate our design choices, we first present an ablation over successive reward components on the STVQAval split, followed by details of observed behaviors and analysis. C.1 REWARD DESIGN ABLATION To empirically validate our design choices, we conduct controlled ablation study on the STVQA7Kval set, progressively introducing each reward component and constraint. The ablation results support our design rationale by highlighting how each component mitigates specific failure modes. Adding spatial rewards naively without generation constraints, causes performance to collapse by over 50% (from 74.9% to 23.7%), as models exploit the reward by generating cluttered bounding boxes to game the CIoU metric. Introducing the count reward addresses this issue, improving accuracy by 38% relative (to 61.7%), as it constrains overgeneration and forces models to focus on question-relevant elements. However, residual overfitting persists because rewarding spatial alignment across all scene objects biases the model toward exhaustive global descriptions. To address this, we shift from global to local spatial supervisionrewarding only Regions of Interest (RoIs) derived from question-relevant objects and relationsthereby training the model to attend selectively to meaningful spatial cues rather than densely describing the entire scene. Lexicographic gating further ensures that spatial rewards are only applied when the final answer is correct, preventing the model from over-optimizing intermediate process rewards at the expense of outcome accuracy.. Together, these interventions restore and slightly surpass the original performance (76.3%), demonstrating the importance of grounding rewards in both correctness and relevance. Finally, dataset filtering using pass@2 correctness verification amplifies these effects, yielding substantial gain and culminating in the best validation accuracy of 87.9%. This step ensures that only high-quality, verifiable supervision signals contribute to training, reinforcing the alignment between spatial grounding and task success. 24 C.2 REWARD DESIGN RATIONALE Mitigating Spatial Reward Hacking. Our initial reward formulation, which directly rewarded spatial localization quality, led to unexpected model behavior. Without constraints on generation quantity, models discovered they could maximize spatial rewards by generating numerous bounding boxes with varying coordinates. Through Hungarian matching that selects the best-matching boxes, even random predictions would occasionally yield high Complete IoU (CIoU) scores. This reward hacking manifested as models producing excessive, hallucinated objects while achieving poor task accuracythe spatial reward was inflated despite the clutter of irrelevant predictions degrading actual performance. To address this exploitation, we introduced the Count Reward that penalizes deviations from expected object and relation counts. This reward serves dual purposes: (1) preventing reward hacking by constraining the generation space, and (2) encouraging models to focus on questionrelevant scene elements rather than exhaustively describing the entire image. The count reward formulation provides linear penalty proportional to relative deviations from ground truth RoI counts, normalized to prevent domination by scenes with many objects. Scene Graph Filtering. Another form of overfitting emerged when training with complete Visual Genome scene graphs. Models would memorize exhaustive scene descriptions, including irrelevant background objects, leading to poor generalization. We addressed this by filtering ground truth scene graphs to retain only objects and relations relevant to the given question, focusing supervision on task-critical information. CIoU over IoU for Spatial Reward. For spatial localization, we adopt Complete IoU (CIoU) instead of standard IoU to compute the spatial reward. Unlike IoU, which returns zero when predicted and ground-truth boxes do not overlap, CIoU provides meaningful gradients by incorporating center distance, aspect ratio, and overlap (Zheng et al., 2020). This makes CIoU denser and more robust supervisory signal during training. Balancing Supervision with Exploration. Our experiments reveal crucial insight: models learn simple reward functions significantly faster than complex ones. Tasks with straightforward rewards (e.g., format compliance) show rapid improvements, while multi-component rewards require careful balancing. However, counterintuitively, highly detailed reward functions that attempt to supervise every aspect often degrade performance. Models overfit to maximize minute reward components, converging to template-style answers that score well on individual metrics while losing flexibility. We observed accuracy drops mid-training when rewards became too prescriptive, as models focused on reward optimization rather than genuine task understanding. Effective reinforcement learning requires providing guidance while preserving exploration space. Our final design addresses this by providing soft signals through format checks, count constraints, and accuracy rewards, with spatial localization rewards activated only for correct answers. This maintains the delicate balance between guidance and exploration necessary for robust learning. Sequential Optimization via Lexicographic Gating. To prevent models from gaming individual reward components at the expense of task accuracy, we implement lexicographic gating (Skalse et al., 2022). Rewards are applied in strict hierarchy: format {count, accuracy} spatial. This forces models to first master output formatting, then simultaneously learn to control generation scope and achieve correctness, before optimizing spatial grounding: Rtotal = I[Rformat = 1] (wformat Rf + wcount Rc + waccuracy Ra + I[Raccuracy = 1] wspatial Rs) where I[] is the indicator function, with weights wformat = 0.1, wcount = 0.2, waccuracy = 0.5, wspatial = 0.2. This gated design ensures spatial rewards are only applied when the final answer is correct, aligning grounding quality with task success and preventing scenarios where models achieve high spatial scores through precise but irrelevant localizations."
        },
        {
            "title": "D ABLATION ON DIVERGENCE CONSTRAINTS",
            "content": "Recent works such as DAPO (Vassoyan et al., 2025; Yu et al., 2025) argue that KL regularization can unnecessarily constrain policy updates and recommend removing the KL penalty entirely to allow freer exploration. In contrast, Huang et al. (2024) revisit divergence regularization and propose using chi-squared penalty to better control overoptimization. Motivated by these findings, we ablate the effect of different divergence constraints in our reinforcement learning setup for spatial reasoning. Table 7 reports results on CV-Bench 2D and 3D tasks (Tong et al., 2024a) for three variants of SPATIALTHINKER-3B: (i) no KL penalty, (ii) chi-squared divergence penalty with coefficient of 0.01, and (iii) our default KL divergence penalty with coefficient of 0.01. Removing the KL penalty leads to noticeable drop in performance, particularly on 3D tasks. Using chi-squared divergence penalty underperforms both the no-penalty and KL variants on several subtasks, especially depth and distance reasoning. The KL-regularized model achieves the best overall performance, yielding CV-Bench average of 73.7% and providing the strongest results on 3D reasoning tasks. These findings suggest that modest KL penalty stabilizes policy updates and prevents reward overoptimization in our spatial reasoning setting, leading to more reliable improvements. While recent language-only alignment work has advocated for removing divergence constraints, our results indicate that retaining small KL term remains beneficial for multimodal reasoning tasks where stability and coherent spatial grounding are crucial. Model Variant Count Relation Depth Distance CV-Bench 2D CV-Bench 3D CV-Bench Avg. SpatialThinker-3B + No KL Penalty SpatialThinker-3B + Chi2 (0.01) SpatialThinker-3B + KL (0.01) 65.5 64.5 68.5 76.8 73.7 73.5 74.8 71.2 79.7 70.2 66.2 72.8 71.2 69.1 71.0 72.5 68.7 76. 71.9 68.9 73.7 Table 7: Ablation on divergence constraints for SPATIALTHINKER-3B on CV-Bench tasks. KLregularization with β = 0.01 yields the highest overall average and strongest 3D reasoning performance. ADDITIONAL RESULTS: ABSTRACT REASONING To further evaluate the generalization capacity of SPATIALTHINKER, we examine its performance on two abstract reasoning benchmarks: Lego Puzzles (Tang et al., 2025), which test compositional object reasoning and multi-step spatial reasoning, and BLINK Multi-View (Fu et al., 2024), which requires integrating spatial cues across multiple viewpoints, including visual-spatial understanding and perspective understanding. These tasks are not part of the training distribution and measure the ability of models to extrapolate structured reasoning skills to abstract domains. Model Lego Puzzles BLINK Multi-View Proprietary and Open-Source MLLMs GPT-4o Claude 3.5 Sonnet Qwen2.5-VL-3B Qwen2.5-VL-7B VLAA-Thinker-7B SpaceThinker SpaceOm 57.7 53.6 29.9 35.8 33.4 31.5 32.0 Method Comparison (Trained on SpatialThinkerVQA) Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + Vanilla GRPO SpatialThinker-3B (Ours) Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + Vanilla GRPO SpatialThinker-7B (Ours) 34.7 27.0 33.9 36.6 29.7 37.7 54.1 51.9 42.9 44.4 51.1 50.4 48.9 42.1 45.9 45.1 44.4 51.9 52.6 Table 8: Results on abstract reasoning benchmarks. Lego Puzzles measure compositional reasoning over object arrangements, while BLINK Multi-View requires integrating multi-view spatial cues. 26 Across both tasks, SPATIALTHINKER-7B achieves the highest open-source performance improving over generalist and spatial MLLMs, and scoring 37.7% on Lego Puzzles and 52.6% on BLINK Multi-View, closely approaching GPT-4o and surpassing Claude 3.5 Sonnet on the latter. Interestingly, we observe that vanilla GRPO provides competitive performance on BLINK Multi-View but underperforms on Lego Puzzles, suggesting that dense spatial rewards offer complementary signals that better support compositional reasoning. These results demonstrate that the spatial grounding learned through reinforcement learning transfers to more abstract domains that require compositional and multi-view integration skills. DETAILED RESULTS: CV-BENCH Distance 2D CV-Bench Model GPT-4o Gemini-1.5-Pro Claude 3.7 Sonnet Qwen2-VL-2B Qwen2.5-VL-3B Qwen2.5-VL-7B VLAA-Thinker-3B VLAA-Thinker-7B LLaVA-NeXT-34B Mini-Gemini-HD-34B Cambrian-1-34B Spatial-LLaVA-7B SATORI-R1 VisualThinker-R1-2B Spatial-RGPT-7B w/ depth RoboPoint-13B SpaceThinker-3B SpaceLLaVA-13B SpatialBot-3B Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + Vanilla GRPO SpatialThinker-3B (Ours) Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + Vanilla GRPO SpatialThinker-7B (Ours) CV-Bench Tasks Depth Relation Proprietary Models 85.7 85.2 74.2 87.8 82.4 85.8 Open-Source General MLLMs 22.6 58.3 82.2 83.5 74.6 - - - 16.7 67.3 70.0 53.0 61.3 - - - Open-Source Spatial MLLMs - 66.3 66.8 - 75.6 69.2 63.7 69.4 57.3 65.5 54.2 62.3 77.8 70.5 66.8 77.3 Count 65.9 70.4 - 54.7 61.5 55.9 61.6 47.0 - - - - 42.8 59.6 - - 61.0 - - 78.2 72.8 84.2 31.7 53.0 66.0 46.8 59.2 - - - 52.2 73.3 56.7 59.0 44.5 61.3 70.2 60.8 Method Comparison (Trained on STVQA-7K) 30.2 67.5 68.5 33.3 58.9 68.7 61.2 64.0 79.7 64.8 79.3 81.2 77.5 73.7 73.5 78.9 78.8 86. 75.5 69.2 72.8 77.7 73.7 76.2 3D 83.0 77.6 85.0 24.2 60.2 68.0 49.9 60.3 74.8 79.2 79.7 54.8 69.4 55.45 60.7 61.15 65.9 68.5 69.05 68.4 66.6 76.3 71.3 76.5 78. Avg. 79.4 77.7 - 31.5 60.1 68.6 61.3 60.6 73.9 75.4 76.9 - 62 59.3 - - 65.5 - - 61.2 68.6 73.7 63.7 72.7 78.2 75.8 77.8 - 38.7 59.9 69.1 72.6 60.8 73.0 71.5 74.0 - 54.6 63.2 - - 65.1 - - 53.9 70.6 71.0 56.1 68.9 77.7 Table 9: Detailed breakdown of CV-Bench (Tong et al., 2024a) results across Count, Relation, Depth, and Distance subtasks. 27 DETAILED RESULTS: 3DSRBENCH Model Height 3DSRBench Tasks Orientation Location Multi-Object GPT-4o Claude 3.5 Sonnet Gemini 2.0 Flash Gemini 2.0 Flash (thinking) Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-72B Cambrian-1-8B LLaVA-NeXT-8B VLAA-Thinker-7B SpatialBot-3B SpaceLLaVA-13B SpatialLLM-8B SATORI-R1 SpatialRGPT-7B w/ depth SpaceThinker-3B Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + Vanilla GRPO SpatialThinker-3B (Ours) Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + Vanilla GRPO SpatialThinker-7B (Ours) Proprietary Models 59.6 63.1 68.9 67.1 Open-Source MLLMs 56.8 62.7 71.0 53.9 59.9 60.2 53.2 53.5 49.7 53.0 45.2 44.1 53.3 23.2 50.6 54.0 Open-Source Spatial MLLMs 40.4 49.3 45.8 50.9 55.9 53.1 54.4 54.4 61.6 52.3 60.0 57.3 21.6 31.4 32.2 35. 35.7 40.6 43.1 35.9 36.1 42.9 31.9 27.6 30.0 41.9 34.2 41.9 Method Comparison (Trained on STVQA-7K) 42.7 42.5 43.4 43.8 45.5 45.5 58.3 57.9 61.8 66.3 64.7 70.3 51.1 48.9 52.6 50.6 54.3 52.0 39.0 41.3 41.5 43. 35.7 40.5 46.6 41.9 43.4 49.1 33.5 35.4 36.7 46.3 42.3 49.6 48.1 47.2 49.8 47.9 50.4 50.9 Avg. 44.3 48.2 49.9 51.1 44.0 48.4 54.9 42.2 48.4 52. 41.1 42.0 44.9 48.0 48.4 51.1 50.8 50.1 52.9 53.6 54.7 56.4 Table 10: Detailed Breakdown of 3DSRBench (Ma et al., 2024b) Height, Location, Orientation, and Multi-Object tasks."
        },
        {
            "title": "H ADDITIONAL QUALITATIVE RESULTS",
            "content": "Figure 7: Additional qualitative comparisons between GPT-4o and SpatialThinker-7B. SpatialThinker-7B shows stronger spatial grounding and fine-grained object distinction, accurately identifying 3D relations that GPT-4o often confuses."
        }
    ],
    "affiliations": [
        "University of California, Santa Cruz",
        "University of Oxford"
    ]
}