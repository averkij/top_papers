{
    "paper_title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution",
    "authors": [
        "Kaiwen He",
        "Zhiwei Wang",
        "Chenyi Zhuang",
        "Jinjie Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser use agents. However, when solving tasks on real world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial and error during execution. This paper introduces Recon-Act, a self-evolving multi-agent framework grounded in Reconnaissance-Action behavioral paradigm. The system comprises a Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into a unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing a closed-loop training pipeline of data-tools-action-feedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset."
        },
        {
            "title": "Start",
            "content": "Recon-Act: Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution Kaiwen He1,, Zhiwei Wang1,, Chenyi Zhuang1, Jinjie Gu1 1 AWorld Team, Inclusion AI https://github.com/inclusionAI/AWorld/tree/main/examples/visualwebarena"
        },
        {
            "title": "Abstract",
            "content": "Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser-use agents. However, when solving tasks on real-world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial-and-error during execution. This paper introduces Recon-Act, self-evolving multi-agent framework grounded in ReconnaissanceAction behavioral paradigm. The system comprises Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing closed-loop training pipeline of datatoolsactionfeedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset. 5 2 0 2 5 2 ] . [ 1 2 7 0 1 2 . 9 0 5 2 : r Figure 1: Success Rates on VisualWebArena Dataset (Left) While remains substantial gap to human performance, Recon-Act reaches 36.48% success rates, outperforming the other automated agents (Gu et al., 2025a; Koh et al., 2024b; Yu et al., 2025; Koh et al., 2024a). Average Steps of Different Domains on VisualWebArena (Right) Despite requiring moderate number of steps, Recon-Act achieves stable web navigation with only little self-corrective actions. Other steps data comes from Gu et al. (2025a). Equal contributions. 1 Reconnaissance-Action Multi-Agent System"
        },
        {
            "title": "Introduction",
            "content": "Recently, Multimodal Large Language Models (MLLM) presented by OpenAI et al. (2024); Bai et al. (2023); Wang et al. (2024); Bai et al. (2025); Chen et al. (2024b;a; 2025); Wang et al. (2023); Hong et al. (2024) have markedly advanced visual understanding, long-context reasoning, and native tool-use capabilities, laying the groundwork for autonomous browser-use agents. Nevertheless, in real-world web settings, multi-turn and long-trajectory tasks continue to suffer from brittle tool orchestration and trial-and-error in unfamiliar environments. For example, on the dataset proposed by Koh et al. (2024a), which reflects real browser-use needs, several state-of-the-art MLLMs still fall far short of human performance. Recent studies have proposed methods to improve browser-use ability on complex tasks. However, some GUI-based studies (Liu et al., 2025; Gu et al., 2025b; Ye et al., 2025; Lian et al., 2025) has not yet been designed specifically for browser environment. Dynamic planning methods (Koh et al., 2024b; Yu et al., 2025) can explore solution paths autonomously, but often require large amount of simulation to identify optimal actions, resulting in long trajectoris. (Yu et al., 2025). Looking at the overall picture, Wang et al. (2025) set agent architectures as four components. They are profile, memory, planning, and action, and the last of which includes the capability to invoke external tools. Drawing on our analysis of browser-usespecific datasets (Koh et al., 2024a; Deng et al., 2023; Zheng et al., 2024), on the one hand we find that current agents remain limited in their comprehension and reasoning abilities. On the other hand, for browser-based applications in particular, they would benefit from external tools on acquiring task-relevant information or finishing some key actions. Augmenting MLLMs ability to obtain information via external tools can alleviate the constraints of parametric knowledge and, in turn, curb hallucinations (Qu et al., 2024; Wang et al., 2025). growing body of work advances this agenda under the rubric of tool learning (He et al., 2025; Shi et al., 2025; Qin et al., 2023; Schick et al., 2023). With the repid growth on coding abilities of large models (Li et al., 2023; Lozhkov et al., 2024; Team, 2025; Hui et al., 2024; Guo et al., 2024; Zhu et al., 2024), We consider directly enabling the model to synthesize the tools it judges most appropriate and, on that basis, to formulate the most suitable solution to the problem at hand. Considering the information density of browser environments, where only subset of observations is germane to particular task, we design our tools to return distilled, task-salient information and, when appropriate, to directly yield the executable action. Human users usually scanning the page for an overall picture before taking action when facing an unfamiliar web page. Inspired by this, we seek to extract useful information through limited number of actions to guide subsequent execution. We define this information exploration and distillation process as reconnaissance operation, which involves conducting exploratory actions in the environment and collecting additional observational data when the task is not being performed properly or seems infeasible. Based on the insights gained, we provide recommendations to the task-executing agent to help it complete the task. Such advice may take the form of hints or specialized tools that solve particular problems in specific contexts. We unify these as generalized tools. Xie et al. (2025) establishes multi-agent framework consisting of primary executor agent and an on-demand guardian agent that supervises, validates reasoning, and corrects error to achieve evolution. We propose similar dyadic framework in which we substitute the Guardian to Reconnaissance Team. The Reconnaissance Team provides actionable guidance in the form of hints or dedicated tools, both encapsulated as generalized tools. This paper present Recon-Act, self-evolving multi-agent system (MAS) specifically designed for browser-use tasks, which places tools, in broad sense encompassing both rule-based tools and tool agents, as the core of the iterative process. The positive and negative trajectories serve as sources of feedback. Through contrastive analysis over these instances, the system derives feedback signals and establishes closed-loop, datatoolactionfeedback evolutionary pipeline. Recon-Act enables agents to acquire targeted cues and assistance in unfamiliar environments and thereby complete general tasks more effectively. Our system can be succinctly characterized as multi-agent system composed of Reconnaissance Team and an Action Team. Within the former, we define two agents, that is Analyst and Coder. While the Action Team comprises Master, Tool Manager, and an Execution Agent. We designed series of progressively staged hypotheses and experiments that incrementally operationalize components of this architecture across 6 levels: 1. Level 1: All components are human-operated except the Execution Agent. 2. Level 2: The Master and Execution Agent are powered by vision-language model (VLM); all other components remain human-operated. 3. Level 3: The Master, Execution Agent, and Coder are powered by large language/vision-language models (LLM/VLM); the remaining components are human-operated. 4. Level 4: All components except the Analyst are powered by LLMs/VLMs. 2 Reconnaissance-Action Multi-Agent System 5. Level 5: All agents are powered by LLMs/VLMs. 6. Level 6: An end-to-end model that can finish all the tasks. Because of the problems difficulty and current limitations in LLM/VLM reasoning, our implementation reaches Level 3: both the Analyst and Tool Manager retain degree of human-in-the-loop intervention. The main contributions of this paper include: We propose Recon-Act, self-evolving browser-use agent framework centered on reconnaissance-action dual-team collaboration. We formalize reconnaissance operations in browser context, which distill key observations from information-dense web pages through small number of exploratory actions, and improves the solvability and efficiency of long-term, multi-round tasks. Under Level 3 configuration, our system achieves state-of-the-art performance on the VisualWebArena dataset."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 GUI-Agents with browser-use ability Several studies leverage more general GUI agent paradigm to solve tasks including browser-use, GUI operation, understanding etc. PC-Agent (Liu et al., 2025) decomposes desktop control into three-level multi-agent hierarchy augmented with an Active Perception Module. UI-Venus (Gu et al., 2025b) mitigates reasoning drift and amplifies rare key actions via Self-Evolving Trajectory History Alignment and Sparse Action Enhancement, coupled with data-curation pipeline that yields cleaner grounding and navigation sets. GUI-Owl (Ye et al., 2025) trains single model that unifies perception, reasoning, and action, RLaligned on real tasks and deployed as share-observation specialists inside Mobile-Agent-v3 for long-horizon mobile workflows. UI-AGILE (Lian et al., 2025) continues supervised fine-tuning (SFT) with continuous center-reward, simple-thinking loss, and crop-resampling to combat sparse rewards. At test time it stitches VLM-chosen candidates from decomposed high-resolution crops. ViGoRL (Sarch et al., 2025b) is reinforcement learning-based VLM that anchors each reasoning step to specific visual coordinates, producing spatially grounded traces that guide attention and, via novel multi-turn RL framework, dynamically zooms into predicted regions for fine-grained exploration. ICAL (Sarch et al., 2025a) proposed an in-context abstraction learning framework that enables VLM agents to convert suboptimal demonstrations into highquality training data by self-reflecting to derive generalized strategies and action annotations, iteratively refined with human feedback during execution in similar environments. Departing from the foregoing, we argue that in the browser environment, attention should not only be paid to its distinctive action space but, more importantly, to its environment-specific observation space, as doing so can substantially enhance execution performance. This insight led to the initial conception of observation-generation tools. 2.2 Dynamic Planning Methods Recent studies adopts dynamic planning procedures. At each decision step, the agent generates multiple candidates (actions, intermediate thoughts, or subplans), scores them using one or more evaluators (e.g., value function, reward model, or LLM-based self-evaluation or debate), selects the best candidate for execution, and iterates when necessary. We collectively refer to this family of approaches as Dynamic Planning methods, which share similar pipeline: candidate generation, evaluation or scoring, selection or backtracking, execution. ExAct (Yu et al., 2025) augments MCTS with reflection reuse and multi-agent debate, then distills the full search loop into the model. (Koh et al., 2024b) performs best-first search on an on-the-fly interface graph scored by an LM-value function, yielding the first verified lift on real websites. Agent (Putta et al., 2024) pairs MCTS over web pages with LM self-critique, turning LLM-generated step-level rankings into dense rewards that guide exploration without human labels. WebDreamer (Gu et al., 2025a) lets an LLM dream next-state description for every candidate action, executes the most promising, and iterates until self-judged success. Across these studies, Recon-Act is distinguished by its tool-centric, reconnaissance-driven design: it initiates targeted exploration when progress stalls, distills the resulting observations into generalized tools (either hints or dedicated tool agents) and closes the loop via contrastive analysis over positive and negative instances to refine policies in the form of tools. This yields practical path to self-evolution in information-dense browser environments, complementary to advances in agentic RL. 2.3 Agent and Tool Within the broader topic of agents and tools, progress chiefly proceeds along two directions: tool learning, namely improving models ability to select and use tools, and tool generation (within the framework of 3 Reconnaissance-Action Multi-Agent System Figure 2: System Architecture. The system comprises two integrated teams: Reconnaissance and Action Team. Training workflow proceeds as follows. user query together with the browser context is ingested by Master Agent, which invokes an appropriate agent or tool. Router then selects suitable tools or member agents to get answer. Selection module consolidates the outputs into final action, which is executed in browser via the Playwright API and yields trajectory. An Evaluator reviews the trajectory and writes the assessment back to the training set. If the trajectory is still incorrect, the Reconnaissance Team employs preconfigured reconnaissance tools to gather additional information. Its Analyst devises plan and the Coder implements new tool. The new tool will be registried and deployed online to the Action Teams tool manager, after which subsequent tasks proceed using the augmented toolset. In this way, trajectories, evaluation, and training form closed-loop, iterative improvement cycle. this paper this corresponds to code generation). GenTool (He et al., 2025) synthesizes two types of training data: zero-to-one generalization for queries without suitable tools, and weak-to-strong generalization for queries suitable for using optimized tools. It further proposes two-stage fine-tuning procedure, optimizing tool ranking, then refining tool selection, to strengthen tool-use capability. AutoTools (Shi et al., 2025) preencapsulates tools as callable functions and verifies both syntactic and runtime correctness. At inference time, it generates code-like invocation logic to execute tool calls and provides error feedback. ToolLLM (Qin et al., 2023) constructs datasets using LLM and employs an automatic evaluator whose core algorithm is DFS-based decision tree. to fine-tune the model. Toolformer (Schick et al., 2023) augments existing corpora to derive an API-call dataset and, enables the model to learn how to use external tools via fine-tuning. On the code generation front, Li et al. (2023); Lozhkov et al. (2024); Team (2025); Hui et al. (2024); Guo et al. (2024); Zhu et al. (2024) leverage large-scale training data to train specialized coding models across range of model sizes."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we present our Recon-Act multi-agent pipeline both during training and inference time, as depicted in Figure 2. Our pipeline comprises two integrated teams, namely Reconnaissance Team and Action Team. In accordance with its role specification, the Reconnaissance Team gives requirements and advices based on its intelligence gatherd through reconnaissance. These intelligence include failed trajectories derived from the Action Teams interactions with the environment, successful trajectories from training set, and browser contexts along the trajectories. The Reconnaissance Team identifies the causes of failure, and create or update tool that is helpful to the Action Team in solving the task. Once such tool is registered, the Action Team immediately receives their specifications, thereby acquiring the capability to invoke them in real time. During training stage, The Reconnaissance Team analyzes the problem and incrementally augments and 4 Reconnaissance-Action Multi-Agent System updates the toolset, thereby progressively enhancing the systems cross-task generalization and decisionmaking capabilities. When tool addition or update happens, the system performs an inference pass on the Training Set to obtain additional Trajectories. The training process iterates until the Reconnaissance Team can no longer augment or update the toolset, or until no improvement in Training Set accuracy is observed after several times of consecutive tool updates, at which point training is terminated. During inference stage, only the Action Team performs the task, which can invoke pretrained (automatically generated) tools to address typical issues encountered during task execution. It fully leverages the available actions to increase success rates while substantially improving runtime efficiency. 3.1 Reconnaissance Team For Reconnaissance Team, we predefine set of cold-start queries derived from real users needs in browseruse scenarios which contains both successful and failed trajectories for one query. They target common websites and exhibit degree of generalizability. Crucially, the queries used to train the Reconnaissance Team are problem instances that the system cannot currently solve. This is necessary to establish feedback loop along with the successful trajectories, without which learning cannot proceed. We design the reconnaissance agent as lightweight multi-agent system comprising two components, an Analyst and Coder, alongside built-in reconnaissance toolkit. The toolkit includes basic web observation tools such as get url, image, and SOM(set-of-marks) observations (in text) etc., which enable page-structure parsing, extraction of visual cues, and related capabilities. The Analyst performs degree of information compression: conditioned on the task specification and the error categories provided by an evaluator, it compares erroneous and successful trajectories at the step level, selects and invokes appropriate reconnaissance tools, infers the root causes of failures, and proposes remedial strategies. The Coder then maps these requirements and operational procedures into executable code. In conjunction with tool registration mechanism, all tools conform to unified parameter schema and output format: they accept superset of potentially relevant arguments and return string. This design avoids per-task parameter customization and thereby reduces coding complexity. The Reconnaissance Team is active only during training, which proceeds in iterative Rollout, Evaluate, Generate, Update cycles. First, we execute cold-start queries to get new failure trajectories. These failure trajectories, together with available success trajectories and the current browser context, are provided to the Analyst. Through contrastive, step-level analysis, the Analyst synthesizes tool specifications. The Coder then generate the tool and submits registration request. Once the Tool Manager completes registration, the Action Team performs full inference episode. During inference, each action produced by the Action Team is executed against the browser via the Playwright API. After the episode concludes, if the resulting trajectory is judged correct by the evaluator, training for that instance terminates. 3.2 Action Team The Action Team comprises three components: Master, Tool Manager, and an Execution Agent. The Master interprets the user query and the browser context to identify the current subtask and determines whether to invoke tool, as well as which tool to invoke. The Tool Manager functions essentially as coding agent. When the Reconnaissance Team issues tool registration request, the Tool Manager decides, based on the full set of conditions and the tools implementation, whether to add new tool or update an existing one. Updated tools incorporate conditional logic to avoid altering the behavior of prior tool invocations. These updates are active only during the training stage and are disabled at inference time. During inference, to ensure the effectiveness of the tool and the scenario generalization capability of the entire system, we added hard-coded tool-routing mechanism. The Execution Agent serves as comprehensive fallback: it can generate one of the actions in the entire action space and thus guarantees default output. If tool call fails or no tool is invoked, the final action is taken from the Execution Agents output. Tools can be registered in two modes: Hint and Decision. Hint-mode tool which is less deterministic or more context-sensitive, returns reconnaissance signals to the Execution Agent to improve task completion, whereas Decision-mode tool with consistently stable behavior, directly emits an action from the action space. Outputs from decision-mode are authoritative. Whenever Decision tool produces an action, the system executes it as specified. At the start of an inference episode, upon receiving the initial query and browser context, the Master first interprets the query and context and selects tool to invoke. The tool router then dispatches and executes the corresponding tool. If the routed tool is in Hint mode, the system executes the Execution Agent afterward to obtain the final action. If the routed tool is in Decision mode, its action is returned directly. The emitted action interacts with the browser environment to update the state, yielding new context for the next step. 5 Reconnaissance-Action Multi-Agent System"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we conduct some experiments to evaluate the performance of our proposed Recon-Act. Table 1: Success rates of baseline LLM and VLM agents on VisualWebArena Paper Method Model VWA1 VWA1 VWA1 VWA1 VWA1 VWA1 WebDreamer2 WebDreamer2 WebDreamer2 ICAL3 WebDreamer2 WebDreamer2 ICAL3 TreeSearch4 ExAct5 ExAct5 ExAct5 ours Multimodel (SoM) Image + Caps + SoM Gemini-Pro Gemini-Pro Multimodel Image + Caps + Acc. Tree GPT-4 Text-only Acc. Tree GPT-4 + BLIP-2-T5XL Caption-augmented Acc. Tree + Caps Multimodel Image + Caps + Acc. Tree GPT-4V Multimodel (SoM) Image + Caps + SoM GPT-4V - - - - - - - Search + SoM MCTS SA SoM + Caption + Image R-MCTS SA SoM + Caption + Image R-MCTS MAD SoM + Caption + Image Qwen2-VL-7B Qwen2-VL-72B Dreamer-7B GPT-4V Dreamer-7B + In-Domain GPT-4o GPT-4o GPT-4o GPT-4o GPT-4o GPT-4o Recon-Act Human - GPT-5-Chat - Success Rate () (%) Classifieds Reddit Shopping Overall 3.42 3.42 5.56 8.55 8.12 9.83 17.9 19.6 21.4 - 25.0 23.2 - 26.5 37.6 40.2 41.0 39. 91.07 3.81 4.29 4.76 8.57 12.38 17.14 11.1 15.9 15.9 - 15.9 17.5 - 20.5 23.8 25.2 28.7 27.14 87.10 7.73 8.15 9.23 16.74 19.74 19.31 20.2 24.6 25.4 - 26.3 26.3 - 29.0 29.4 31.9 32.3 39. 88.39 5.71 6.04 7.25 12.75 15.05 16.37 17.20 21.00 21.90 22.70 23.20 23.20 23.40 26.40 30.22 32.53 33.74 36.48 88.70 1 Koh et al. (2024a). 2 Gu et al. (2025a). 3 Sarch et al. (2025a). 4 Koh et al. (2024b). 5 Yu et al. (2025). 4.1 Datasets and Evaluation Metrics We evaluated our method on the VisualWebArena (Koh et al., 2024a) dataset, which is benchmark for evaluating agents that can understand and act upon the visual content of the web. It targets realistic tasks requiring joint reasoning over text and images, such as selecting reasonably priced used car on classifieds or comparing sellers and lowest prices for specified product across websites. The dataset comprises approximately 910 queries spanning three domains: classifieds, shopping, and reddit forum. Its evaluation supports multiple criteria: exact match (predictions must exactly match the reference), mustinclude (predicted key points must be covered by the reference), semantic equivalence (judged by large language model), prohibited-content checks (any forbidden item in the output constitutes failure) and visual question answeringstyle assessment to determine goal completion, complemented by fuzzy image matching using the Structural Similarity Index (SSIM) to evaluate the closeness of captured or localized images. 4.2 Main Results The results on VisualWebArena dataset are presented in Table 1. We achieves an overall success rate of 36.48%, surpassing the previous best (Yu et al., 2025) by 2.74%. For subdomain, we obtain 39.27% on Shopping, substantially outperforming the prior best of 32.30% (+6.97%). On Classifieds and Reddit, we trailing the current baselines (41.00% and 28.70%) by only 1.68% and 1.56% respectively. Compared with earlier methods such as Koh et al. (2024b); Gu et al. (2025a); Sarch et al. (2025a), our overall improvements are typically above 10%. While gap to human performance remains, these results set Recon-Act as the new state of the art on this benchmark. 4.3 Implementation Details Our system does not incorporate random-walk-based autonomous exploration as in Gu et al. (2025a). Instead, guided by the coverage of our target datasets, we manually authored small training set, with fewer than 10 examples per domain. We argue that random-walk exploration tends to produce overly large corpora with substantial redundancy, which is misaligned with our efficiency and curation goals. Based on the training data and Level 3 configuration, we implemented total of 5 agents (as 2 teams) and 11 tools, as summarized in Table 2 and Table 3. Among the agents, the coder, master, and execution agent are driven by large models, while the analyst and tool manager are driven by human. 6 Reconnaissance-Action Multi-Agent System Team Agent Name Driven By Functionality Table 2: Agents in Recon-Act pipeline Reconnaissance Analyst Human Coder Master Tool Manager GPT-5-Chat GPT-5-Chat Human Action Compares trajectories at the step level, calls appropriate reconnaissance tools, infers the cause of failure and proposes remediation strategies Transfer remediation strategies into tool codes Interprets the query and context and selects tool to invoke Decides whether to add new tool or update an existing one and updates tools with conditional logic Execution Agent GPT-5-Chat Generates default action Tool Name* AuthorFinder Table 3: Tools created by Recon-Act Type Functionality (Description) Decision who can find the authors all post when you are at the post detailed page or commemt page, and go to the authors page CategoryGuide Decision who can guide you to the specific category page when you are at the shopping ClassifiedsPriceSorter Decision who can sort items in classifieds site according to intent, can only be called on classifieds site, should always be called after every action DownVoter Decision who would downvote the current post when you are at the post detailed page site, can only be called on shopping site or comment page ImageSearcher Decision who can find the most similar post to the input image on reddit and go to its detailed page and commemt page, should only be called on reddit site ShoppingImageFinder ShoppingPriceSorter Decision who can find the desired image and go to its detailed page Decision who can sort items in shopping site according to intent, can only be called on SubRedditNavigator Decision who can navigate to the subreddit page when you are at the post detailed page product list pages or comment page UpVoter Decision who would upvote the current post when you are at the post detailed page or PostTimeFinder Hint RedditImageDescriptor Hint comment page who can find the post time when you are at the post detailed page or comment page who can return you the image description of the post image when you are at the post detailed page or comment page, can only be called on reddit site * The tool names may be bit chaos, but we have kept all the original names and the descriptions are exactly the same as what we gave to the master. The master and executor operate purely based on prompts. For the coder in Reconnaissance Team, we fixed the inputoutput interface and the basic code structure to ensure that tool codes can be generated with high feasibility. For analyst in reconnaissance team, we need to prompt the model to ground its reasoning on concrete solution procedures. We prefer to direct the model to navigate straight to the target action (e.g. the goto operation) rather than relying on click-based exploration, which is less dependable. For the same reason, we also prompt to design sorters and image searcher tool to operate via goto action when possible. For tool manager in action team, the code merge happens in registration is the toughest work. Automatically generated tools are frequently narrow and fragmented. For example, price sorter created for site that only requires finding the cheapest item will contain only cheapest branch, while subsequent query requesting the most expensive item would then fail. Without consolidation, such specialization leads to tool proliferation. Moreover, because our agents does not ingest the entire page context at once, the scope of tool can be ambiguously defined: tool named price sorter might only support low-to-high sorting; Reddit voting tool with only down voting function might be labeled simply as voter. Because of the above insights, humans are currently involved in naming tools, adjusting feature branches, and merging tools where appropriate. Its important to note that because each website has its own unique characteristics, we specifically have the reconnaissance team pay attention to the specific website when writing tools, which ensures that there is no confusion when calling similar tools. 7 Reconnaissance-Action Multi-Agent System"
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Recon-Act, tool-centric, self-evolving system for browser interaction that relies on dual-team reconnaissanceaction collaboration. We formalize in-browser reconnaissance, enabling the agents to distill salient intelligence from information-dense pages with small number of exploratory actions and to generate feedback through contrastive analysis of positive and negative instances, thereby establishing closed-loop evolutionary pipeline spanning data, tools, actions, and feedback. The system adopts staged experimental paradigm to progressively realize capabilities and currently reaches Level 3: humanAI collaboration is retained for analyst and tool manager, while the remaining components are driven by visionlanguage models. Under this configuration, Recon-Act sets new state of the art on VisualWebArena, demonstrating effectiveness and efficiency in autonomously acquiring cues, invoking tools, and completing complex multiturn tasks in unfamiliar environments."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "To realize intelligence beyond Level 5, our future work will proceed along the following directions: 1. Increasing Autonomy: Present learning capability is heavily dependent on our constructed training data, particularly on the inclusion of successful trajectories, making this training process similar to supervised training. We plan to prompt the model to conduct random-walk-style self-exploration in order to generate additional successful trajectories. This in turn, will make the construction of the training set more autonomous. 2. Reasoning and Coding Skills: To progress from Level 3 to higher levels and reduce reliance on human analysis and tool management, we must further strengthen the analyst and tool manager components. The analyst encapsulates reasoning ability, while the tool manager reflects coding competence. For the analyst, we intend to collect targeted corpus of analytical data and train it with diverse browser contexts so that it acquires robust, context-aware analytical skills. It should not only give insights related to the task, but also consider steps to reduce the difficulty of the task for itself, so as to make the subtask more suitable for large models. An example arises on Classifieds websites, where image-localization steps consistently select the wrong bounding box because of the small size of the image. Guided by cold-start trajectories, we switches the presentation from list view to grid view. The grid layout enlarges thumbnails, thereby reducing the difficulty for VLM to interpret the images. Moreover, given the URLs before and after the layout change, it should discover that switching from list to grid can be achieved not only by clicking page-level toggle but also by appending fixed-pattern subpath to the URL to reach the corresponding layout page directly. This can be concluded as tool and described functionally as enlarging images when they are otherwise too small. For the tool manager, the bottleneck lies in the complexity of branching and iterative code modification during the registration workflow, specifically maintaining isolation between existing capabilities and newly introduced ones via feature branches. Additionally, the master agent still has certain probability of error when calling the tool. If we consider making the subtask suitable for large models again, reducing the number of tools that need to be called by merging the functions of the tools can make the orchestration easier without having to increase the masters orchestration ability. We will address this through similarly targeted training in the future. 3. Expanding Reconnaissance Capabilities: Our current reconnaissance module performs successfully only on fixed set of websites and has not yet generalized to broader, more heterogeneous web environment. 8 Reconnaissance-Action Multi-Agent System"
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502. 13923. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024a. URL https://arxiv.org/abs/2404.16821. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, 2024b. URL https://arxiv.org/abs/ 2312.14238. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. URL https://arxiv.org/abs/2412.05271. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web, 2023. URL https://arxiv.org/abs/2306.06070. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly world model of the internet? model-based planning for web agents, 2025a. URL https://arxiv.org/abs/2411.06559. Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, and Weiqiang Wang. Ui-venus technical report: Building high-performance ui agents with rft, 2025b. URL https://arxiv.org/abs/2508.10833. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming the rise of code intelligence, 2024. URL https://arxiv.org/abs/2401.14196. Jie He, Jennifer Neville, Mengting Wan, Longqi Yang, Hui Liu, Xiaofeng Xu, Xia Song, Jeff Z. Pan, and Pei Zhou. Gentool: Enhancing tool generalization in language models through zero-to-one and weak-to-strong simulation, 2025. URL https://arxiv.org/abs/2502.18990. Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, 2024a. URL https://arxiv.org/abs/2401.13649. 9 Reconnaissance-Action Multi-Agent System Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents, 2024b. URL https://arxiv.org/abs/2407.01476. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu noz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023. URL https://arxiv.org/abs/2305.06161. Shuquan Lian, Yuhang Wu, Jia Ma, Yifan Ding, Zihan Song, Bingqi Chen, Xiawu Zheng, and Hui Li. Ui-agile: Advancing gui agents with effective reinforcement learning and precise inference-time grounding, 2025. URL https://arxiv.org/abs/2507.22025. Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, and Fei Huang. Pc-agent: hierarchical multi-agent collaboration framework for complex task automation on pc, 2025. URL https://arxiv.org/abs/2502.14282. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu noz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, 10 Reconnaissance-Action Multi-Agent System Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. URL https: //arxiv.org/abs/2408.07199. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL https://arxiv.org/abs/2307.16789. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Tool learning with large language models: survey, 2024. URL https://arxiv.org/abs/2405.17935. Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, and Katerina Fragkiadaki. Vlm agents generate their own memories: Distilling experience into embodied programs of thought, 2025a. URL https://arxiv.org/abs/2406.14596. Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning, 2025b. URL https://arxiv.org/abs/ 2505.23678. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023. URL https://arxiv.org/abs/2302.04761. Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models as automatic tool agents, 2025. URL https://arxiv.org/abs/2405.16533. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. survey on large language model based autonomous agents, 2025. URL https://arxiv.org/abs/2308.11432. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409.12191. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023. Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, and Jinjie Gu. Profile-aware maneuvering: 11 Reconnaissance-Action Multi-Agent System dynamic multi-agent system for robust gaia problem solving by aworld, 2025. URL https://arxiv.org/ abs/2508.09889. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, and Ming Yan. Mobile-agent-v3: Fundamental agents for gui automation, 2025. URL https://arxiv.org/abs/2508.15144. Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning, 2025. URL https://arxiv.org/abs/ 2410.02052. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded, 2024. URL https://arxiv.org/abs/2401.01614. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024."
        }
    ],
    "affiliations": [
        "AWorld Team, Inclusion AI"
    ]
}