{
    "paper_title": "Learning to Skip the Middle Layers of Transformers",
    "authors": [
        "Tim Lawson",
        "Laurence Aitchison"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, we propose a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. We had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, our approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. We release our code at https://github.com/tim-lawson/skip-middle."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 3 0 1 1 2 . 6 0 5 2 : r a"
        },
        {
            "title": "Learning to Skip the Middle Layers of Transformers",
            "content": "Tim Lawson Laurence Aitchison School of Engineering Mathematics and Technology University of Bristol Bristol, UK"
        },
        {
            "title": "Abstract",
            "content": "Conditional computation is popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, we propose novel architecture that dynamically skips variable number of layers from the middle outward. In particular, learned gating mechanism determines whether to bypass symmetric span of central blocks based on the input, and gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with sandwich or perilayernorm scheme and gate sparsity with an adaptive regularization loss. We had aimed to reduce compute requirements for simpler tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, our approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. We release our code at https://github.com/tim-lawson/skip-middle."
        },
        {
            "title": "Introduction",
            "content": "We want to make Transformers more efficient. This could be achieved by making individual modules more efficient; for example, many variants on the attention mechanism have been proposed (Dong et al., 2024). An alternative approach is to reduce the number of parameters activated during inference. Conditional computation methods decouple models capacity (determined by its total parameter count) from its inference cost (determined by the subset of active parameters used for given input; Bengio et al. 2013, 2016). prominent example is the replacement of feed-forward network (FFN) modules with mixture-of-experts (MoE) layers (Shazeer et al., 2017). These methods reduce computational and memory requirements while enabling parallelization of model components across multiple devices (Eigen et al., 2014; Lepikhin et al., 2020; Fedus et al., 2022; Dai et al., 2024). One way to reduce the active parameters is to conditionally apply components of Transformer dependent on the input token. Then, we can dynamically allocate less compute resources to tokens that are easier to process. Early exiting methods, where deep networks can make predictions at different layers, have long history in vision (Teerapittayanon et al., 2016) and language applications (Elbayad et al., 2020; Xin et al., 2020). This approach has been used to dynamically skip Transformer layers beyond certain depth (Elhoushi et al., 2024; Fan et al., 2024). Other methods skip intermediate components (Wang et al., 2018), such as individual modules (Csordás et al., 2021; Peroni and Bertsimas, 2024) or entire layers (Zeng et al., 2023; Raposo et al., 2024). We argue that it makes more sense to skip the middle layers of Transformers. Multiple researchers have demonstrated that the middle layers exhibit greater redundancy: for instance, Lad et al. (2024) Correspondence to tim.lawson@bristol.ac.uk. Preprint. Under review. Head Layer 3 Block Layer 2 Block Layer 1 Layer Block Gate Block Gate g(0,1) > 0 g(0,0) > Head Block Block Block Gate Block Gate Skip blocks 12 g(1,1) = 0 g(1,0) > 0 Head Block Block Block Gate Block Gate Skip blocks 0 g(2,0) = 0 Embed Token 0 Embed Token 1 Embed Token 2 Figure 1: An illustration of our proposed architecture (with four layers or blocks). We compute scalar gate value for each token position and block in the first half of the model. If the gate at block ℓ is zero, we skip the Transformer blocks between ℓ and ℓ for the token, and prevent other tokens from attending to its position in the corresponding self-attention modules. and González et al. (2025) found that when layers are removed or swapped in pre-trained models, the performance impact is smaller for interventions affecting the more central layers. This redundancy has been exploited for structured pruning (Fan et al., 2019; Gromov et al., 2024; Men et al., 2024). Interpretability research has also established that early, middle, and late layers in deep networks have different functions. In language models, early-layer modules convert token-level representations to more natural, semantic features (Elhage et al., 2022; Gurnee et al., 2023). Furthermore, Kaplan et al. (2024) have shown that, for multi-token words, the attention mechanism in early Transformer layers aggregates information into the residual vector of the final token in the word. Conversely, late-layer modules convert semantic features into output tokens: near the output of the network, intermediate states can be decoded to elicit token predictions (nostalgebraist, 2020; Belrose et al., 2023). Internal activations at the earliest and latest layers are also comparatively distinct from the middle layers through the lens of sparse dictionary learning (Lawson et al., 2024). Conversions between tokens and semantic features parallel the development of byte-level architectures (Xue et al., 2022; Slagle, 2024), which we expect to learn tokenization implicitly. For example, Pagnoni et al. (2024); Neitemeier et al. (2024); Kallini et al. (2024) instigate two-layer hierarchy of byteand token-level representations. Given Kaplan et al. (2024), we might expect that this hierarchy could be profitably extended to levels spanning multiple tokens (Ho et al., 2024; Videau et al., 2025). Guided by these insights, we propose gating mechanism that skips variable number of Transformer blocks from the middle outward, dependent on the input token. In this way, we can allocate less compute resources to simpler inputs by skipping the middle layers, which are more likely to be redundant. The more central the layer, the fewer tokens it processes, allowing multi-level hierarchy of representations to emerge. Unfortunately, at the scales we were able to investigate, this architecture does not improve the trade-off between language-modeling performance and the computational resources required, measured in terms of the estimated FLOPs at inference time were we able to achieve the maximum benefit from the sparsity of the gate values."
        },
        {
            "title": "2 Model architecture",
            "content": "A standard decoder-only Transformer has layers or blocks, each of which comprises self-attention and FFN module. We denote the input to layer ℓ at token position by h(i,ℓ) Rd where is the model dimension, and the inputs at all token positions 1..N by H(ℓ) RN D. 2 At high level, the standard Transformer is given by (omitting layer normalization): h(i,0) = Embed(i) a(i,ℓ) = h(i,ℓ1) + Attn(H(ℓ1)) h(i,ℓ) = a(i,ℓ) + FFN(a(i,ℓ)) y(i) = Head(h(i,L)). We propose to modify this architecture like so: a(i,ℓ) = h(i,ℓ1) + g(i,ℓ) GatedAttn (H(ℓ1), g(ℓ) ) h(i,ℓ) = a(i,ℓ) + g(i,ℓ) FFN(a(i,ℓ)). The modifications are highlighted . If the gate value g(i,ℓ) for token position is zero, then we do not need to compute the corresponding outputs of the attention and feed-forward modules. The gating mechanism thus functions as kind of router (Figure 1). 2.1 Gating mechanism For each block ℓ < L/2 in the first half of the model, we introduce linear layer that outputs scalar soft mask value s(i,ℓ) 0, which accumulates over the first half of the model: s(i,ℓ) = ReLU (cid:16) w(ℓ) h(i,ℓ) + b(ℓ)(cid:17) , S(i,ℓ) = s(i,ℓ). (cid:88) ℓℓ (1) When the accumulated soft mask value S(i,ℓ) 1, we skip processing the residual vector at token position by the Transformer blocks [ℓ, ℓ). Hence, for each block ℓ L/2 in the second half of the model, we use the accumulated soft mask value S(i,Lℓ1) of the opposing block. The corresponding scalar gate value g(i,ℓ) [0, 1] is: g(i,ℓ) = (cid:40) 1 clamp (cid:0)S(i,ℓ), 0, 1(cid:1) 1 clamp (cid:0)S(i,Lℓ1), 0, 1(cid:1) if ℓ < L/2 if ℓ L/2 (2) The sparsity of the gate values g(i,ℓ) determines the reduction in the number of active parameters: for single token i, it is the number of blocks at which the gate value is zero multiplied by the number of parameters in Transformer block NB. With multiple tokens, the reduction is 2NB ℓ<L/2 zℓ, where zℓ is the sparsity of the gate values before tokens are processed by block ℓ. The gating mechanism introduces (d + 1)L/2 parameters w(ℓ) and b(ℓ), i.e., + 1 for each block ℓ < L/2 in the first half of the model. If all these parameters are zero, all the gate values equal one and we exactly recover the equivalent dense Transformer (which forms the baselines in Section 3). (cid:80) 2.2 Gated attention We also prevent other tokens from attending to gated tokens in attention modules. The GatedAttn module we incorporate modifies the attention mechanism such that, when the gate value for token is zero, subsequent tokens do not attend to the gated token: j<i gj exp(qT (cid:80) j<i gj exp(qT kj)vj kj) oi = (cid:80) (3) This is equivalent to adding ln gj to the pre-softmax attention logits, and can be implemented straightforwardly as score modification within the FlexAttention framework (Dong et al., 2024). In practice, we apply lower bound of ϵ = 1 106 to gj before ln to prevent infinities. Our attention mechanism is similar to the Forgetting Attention proposed by Lin et al. (2024), except that we compute single gate value that applies to every attention head, whereas they compute gate value for each attention head. We require single gate value to decide whether to prevent an entire Transformer block from processing the token. 3 Name sparsity sparsity_variance adaptive proportional sparsity_variance_l2 Loss Update rule 1 1 1 (cid:80)L ℓ=1 αℓ gℓ - (cid:80)L ℓ=1 (cid:0)αℓ gℓ + βℓ ℓ (cid:1) - αi+1 = αi + γ sign(gℓ µ ℓ ) αi+1 = αi + γ (gℓ µ ℓ ) (cid:16) (cid:80)L ℓ= αℓ gℓ µ ℓ 2 2 + βℓ (cid:13) (cid:13)s2 ℓ σ2 ℓ (cid:13) 2 (cid:13) (cid:17) Table 1: Alternative techniques to control the sparsity of the gate values. Recall that gℓ and s2 ℓ are the mean and variance of the gate values over the token positions in batch, and µ ℓ are the targets at layer ℓ for the mean and variance of the gate values, respectively. For the techniques with adaptive coefficients, αℓ and βℓ are updated by the same algorithm. ℓ and σ2 2.3 Layer normalization Modern Transformers typically use the pre-layernorm scheme (pre-LN), where layer normalization operations are applied to the residual inputs to the attention and FFN modules: = + Module(Norm(x)). With this scheme, the norms of residual activation vectors grow with depth and later modules produce outputs with greater norms (Lawson et al., 2024; Csordás et al., 2024a; Kim et al., 2025). The gating mechanism we propose effectively introduces skip connections between opposing pairs of Transformer blocks (Section 2.1). Like Csordás et al. (2024a), who consider Universal Transformer (UT) with single, shared block, we want later modules to accept the outputs of early and late blocks. We address this problem by using the sandwich LN scheme proposed in Ding et al. (2021), called peri-layernorm by Kim et al. (2025), where layer normalization operations are applied to both the residual input to and output of the attention and FFN modules: = + Norm(Module(Norm(x))). This scheme differs from the peri-layernorm of Csordás et al. (2024a), who apply layer normalization operation around (but not on) the residual connections. 2.4 Controlling sparsity Our gated architecture reduces the number of parameters activated during the forward pass proportional to the fraction of gate values that are exactly zero, i.e., the mean gate sparsity (Section 2.1). With only the standard cross-entropy loss, optimization tends to activate more parameters to improve performance, so we need to control the sparsity of the gate values. We achieve this by introducing regularization loss based on the mean and variance of the gate values, with adaptive coefficients updated proportional to the deviations of the mean and variance from layer-wise targets. The mean term incentivizes smaller gate values; the variance term incentivizes non-uniform distribution such that some (but not all) gate values are zero. We denote the mean and variance of the gate values over the token positions in batch by: gℓ = 1 (cid:88) i= g(i,ℓ) , s2 ℓ = 1 (cid:88) (cid:16) i= g(i,ℓ) gℓ (cid:17)2 . (4) ℓ and σ2 The targets at layer ℓ for the population mean and variance of the gate values over token positions are µ ℓ as linearly spaced values between an initial target µ ℓ as the variance of the Bernoulli distribution with = µ ℓ , respectively. Except where noted, we choose the mean targets µ L/2. We choose the variance targets σ2 ℓ ). 0 and final target µ ℓ , i.e., σ2 ℓ (1 µ ℓ = µ 4 r - r 3.6 3.5 3.4 3. 3.2 2 3 5 4 Inference FLOPs 6 1011 2 6 8 10 12 0 Dense baseline Gated (without control) Gated (with control) 0. 0.2 Sparsity 0.3 Figure 2: Performance comparisons between our gated Transformer architecture and baseline models with between 2 and 12 layers (labeled). All gated models with control are variants of the 12-layer architecture. We measured cross-entropy over 100M tokens from the FineWeb validation set. The estimated FLOPs for single forward pass (left) assume that the maximum computational benefit is achieved from the final sparsity of the gate values over the validation set (right). We denote the adaptive coefficients for the mean and variance of the gate values at layer ℓ by αℓ and βℓ, respectively, and initialize the coefficients to zero. The regularization loss is then: = 1 L (cid:88) ℓ=1 (cid:0)αℓ gℓ + βℓ s2 ℓ (cid:1) . After every training step, we update each coefficient by the following rule: αℓ,i+1 = (cid:40) αℓ,i + γ (gℓ µ ℓ ) αℓ,i if (gℓ µ otherwise ℓ ) > δ (5) (6) The updates are thus proportional to the differences from the target values. We choose the update multiplier γ = 1103 and tolerance δ = 1102 based on observations in small-scale experiments. We explored alternative control mechanisms  (Table 1)  , but these performed worse empirically."
        },
        {
            "title": "3 Results",
            "content": "We evaluated the performance of our gated Transformer architecture in terms of the validation crossentropy when pre-trained on the FineWeb dataset (Penedo et al., 2024). As baselines, we trained equivalent dense models with no gating mechanism and between 2 and 12 layers. We measured the compute requirements of each model in terms of the estimated floating-point operations (FLOPs) for single forward pass (batch size 1), assuming that we were able to achieve the maximum possible benefit due to the sparsity of the gate values. The actual compute requirements of the gated and dense models are similar. Except where noted, we used the hyperparameters in Table 2. Figure 2 shows that, without controlling the sparsity of the gates, the mean sparsity tends toward zero and gated models perform similarly to dense baselines (right). With the initial target for the mean gates µ L/2 from 1 to 0, the sparsity increases and the estimated FLOPs decrease (left). However, the proposed architecture does not improve the cross-entropy over dense baselines with fewer layers. 0 fixed to 1, as we decrease the final target µ 5 Parameter Value Parameter model dim n_layers n_heads n_kv_heads vocab_size ffn_dim_multiplier multiple_of norm_eps rope_theta use_scaled_rope max_seq_len initializer_range 768 12 12 12 50 257 4 256 1 105 10 000 False 1024 0.02 data batch_size device_batch_size optimizer lr beta1 beta2 eps weight_decay scheduler warmup_steps start_factor Value 512 32 0.001 0.8 0.95 1 1010 0.1 0.1 Table 2: Default hyperparameters. The Transformer model architecture is based on Llama 3 (Grattafiori et al., 2024) with similar dimensions to GPT-2 small (Radford et al., 2019); we used the AdamW optimizer (Loshchilov and Hutter, 2019) with linear warm-up and cosine decay. 3.1 Experimental details We trained all models on randomly-sampled subset of the FineWeb dataset with approximately 10B tokens (Penedo et al., 2024), pre-tokenized with the GPT-2 tokenizer via the TikToken library (Radford et al., 2019; Ouyang et al., 2022). The validation set contained approximately 100M tokens. We used global batch size of 512 sequences (524 288 tokens) with data parallelism and gradient accumulation over per-device batch size of 32 sequences on 4 NVIDIA A100 or GH200 GPUs. We based the underlying Transformer models on the reference implementation of Llama 3 (Grattafiori et al., 2024). In particular, we used: Grouped Query Attention (GQA; Ainslie et al. 2023); Rotary Positional Embeddings (RoPE; Su et al. 2024); Gated Linear Unit FFNs with Swish activation (SwiGLU; Shazeer 2020); and Root Mean Square (RMSNorm) layer normalization (Zhang and Sennrich, 2019). The key difference relative to Llama 3 is that we used the Sandwich-LN scheme (Ding et al., 2021; Kim et al., 2025) instead of Pre-LN. We initialized RMSNorm parameters to one and sampled all others from the normal distribution with mean zero and standard deviation 0.02. The training codebase is based on the nanoGPT speedrun repository (Karpathy, 2025; Jordan, 2025). We used the AdamW optimizer with single learning rate for all model parameters (Kingma and Ba, 2017; Loshchilov and Hutter, 2019), and two-stage learning-rate scheduler with linear warm-up over 10% of the training steps, starting at 10% of the maximum learning rate, and cosine decay over the remaining steps. Lastly, we performed forward passes in bfloat16 with automatic mixed precision in PyTorch (except manually converting attention logits to float32)."
        },
        {
            "title": "4 Related work",
            "content": "Conditional computation decouples models total parameter count from its inference cost by activating only subset of parameters for given input (Bengio et al., 2013; Eigen et al., 2014; Bengio et al., 2016). prominent application of this principle is the use of Mixture-of-Experts layers, which replace FFN modules with larger set of expert sub-networks, of which only few are selected by router to process each input (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Dai et al., 2024). While related methods like Zhang et al. (2022); Csordás et al. (2024b); Jin et al. (2024) can be effective, they operate on individual modules (e.g., FFNs or attention) and often use routing strategies that enforce fixed computational budget per token (cf. Wang et al., 2024). Our approach differs by applying conditional computation to entire Transformer blocks, and dynamically allocating variable number of blocks to each token based on its processing needs, strategy that is also compatible with module-level techniques. 6 Another line of work to improve efficiency is dynamically altering the network depth. Early exiting methods allow model to generate predictions at intermediate layers, halting computation for easy inputs (Teerapittayanon et al., 2016; Elbayad et al., 2020; Xin et al., 2020). More recent variants of this approach dynamically skip all layers beyond certain depth (Elhoushi et al., 2024; Fan et al., 2024). In contrast, our method is motivated by empirical findings that the middle layers of Transformers exhibit greater redundancy (Lad et al., 2024; González et al., 2025). These findings have been leveraged by structured pruning, which removes layers statically after training (Fan et al., 2019; Gromov et al., 2024; Men et al., 2024). Our work is distinct in that it targets the more redundant middle layers for skipping and does so dynamically during inference. Several methods have explored skipping entire Transformer blocks. For instance, the copy gate proposed by Csordás et al. (2021) modulates blocks contribution, but still requires the full computation of the blocks output. Our gating mechanism, however, ensures that computation for skipped blocks could be avoided entirely. Mixture-of-Depths (MoD) models (Raposo et al., 2024) process only the top-k tokens at each block, enforcing fixed computational budget for the sequence. Our method is different because it determines the computational depth for each token individually, allowing it to adapt to token-specific complexity rather than sequence-wide budget. Approaches like SkipNet (Wang et al., 2018) also enable layer skipping but are not specifically designed for the unique redundancy patterns observed in the middle layers of Transformers. Our gated attention mechanism prevents attention to tokens that have been masked out, which is functionally similar to the Forgetting Attention proposed by Lin et al. (2024). However, their approach computes separate gates for each attention head, whereas our approach uses single gate per token to decide whether to skip an entire Transformer block, which is essential for the block-level computational savings we target. Finally, our work relates to hierarchical Transformers that process sequences at multiple predefined levels (e.g., byteand token-level; Pagnoni et al. 2024; Neitemeier et al. 2024; Kallini et al. 2024). Our middle-outward skipping approach offers the potential for more flexible, emergent hierarchy, where deeper layers process dynamically determined subset of more complex representations."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced novel Transformer architecture that dynamically skips variable number of middle layers, guided by interpretability research suggesting these layers are the most redundant. The mechanism uses learned gate to bypass symmetric span of central blocks based on the input token, with the goals of reducing compute for simpler tokens and allowing representational hierarchy to emerge. Our experiments showed that, at small scales, this architecture did not yield improvements in the trade-off between validation performance and estimated inference FLOPs when compared to simply training dense baseline models with fewer layers. The anticipated benefits of this architectural prior may only become apparent at significantly larger model scales, where the redundancy of middle layers is more pronounced and the relative overhead of the gating mechanism is smaller. Despite these results, we believe the principle of using insights from model internals to design more efficient and structured architectures remains valuable direction for future research."
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In The 2023 Conference on Empirical Methods in Natural Language Processing, December 2023. URL https://openreview.net/forum?id=hmOwOZWzYE. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting Latent Predictions from Transformers with the Tuned Lens, November 2023. URL http://arxiv.org/abs/2303.08112. Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional Computation in Neural Networks for faster models, January 2016. URL http://arxiv.org/abs/1511.06297. 7 Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation, August 2013. URL http://arxiv. org/abs/1308.3432. Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The Neural Data Router: Adaptive Control In International Conference on Flow in Transformers Improves Systematic Generalization. Learning Representations, October 2021. URL https://openreview.net/forum?id=KBQP4A_ J1K. Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Christopher Potts, and Christopher D. Manning. MoEUT: Mixture-of-Experts Universal Transformers. Advances in Neural Information Processing Systems, 37:2858928614, December 2024a. Róbert Csordás, Piotr Piekos, Kazuki Irie, and Jürgen Schmidhuber. SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention. Advances in Neural Information Processing Systems, 37:7441174438, December 2024b. Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-ofExperts Language Models, January 2024. URL http://arxiv.org/abs/2401.06066. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering Text-to-Image Generation via Transformers, November 2021. URL http://arxiv.org/abs/2105.13290. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex Attention: Programming Model for Generating Optimized Attention Kernels, December 2024. URL http: //arxiv.org/abs/2412.05496. David Eigen, MarcAurelio Ranzato, and Ilya Sutskever. Learning Factored Representations in Deep Mixture of Experts, March 2014. URL http://arxiv.org/abs/1312.4314. Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-Adaptive Transformer, February 2020. URL http://arxiv.org/abs/1910.10073. Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, and Ben Mann. Softmax linear units, 2022. URL https://transformer-circuits.pub/2022/solu/index.html. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A. Aly, Beidi Chen, and Carole-Jean Wu. LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1262212642, 2024. doi: 10.18653/v1/2024.acl-long.681. Angela Fan, Edouard Grave, and Armand Joulin. Reducing Transformer Depth on Demand with Structured Dropout, September 2019. URL http://arxiv.org/abs/1909.11556. Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang. Not All Layers of LLMs Are Necessary During Inference, July 2024. URL http://arxiv.org/abs/2403.02181. William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning Research, 23(120):139, 2022. ISSN 1533-7928. URL http://jmlr.org/papers/v23/21-0998.html. Ramón Calvo González, Daniele Paliotta, Matteo Pagliardini, Martin Jaggi, and François Fleuret. Leveraging the true depth of LLMs, February 2025. URL http://arxiv.org/abs/2502.02790. Aaron Grattafiori, Abhimanyu Dubey, et al. The Llama 3 Herd of Models, November 2024. URL http://arxiv.org/abs/2407.21783. 8 Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Dan Roberts. The Unreasonable Ineffectiveness of the Deeper Layers. In The Thirteenth International Conference on Learning Representations, October 2024. URL https://openreview.net/forum?id=ngmEcEer8a. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. Finding Neurons in Haystack: Case Studies with Sparse Probing, June 2023. URL http: //arxiv.org/abs/2305.01610. Namgyu Ho, Sangmin Bae, Taehyeon Kim, hyunjik.jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block Transformer: Global-to-Local Language Modeling for Fast Inference. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, November 2024. URL https://openreview.net/forum?id=6osgTNnAZQ. Peng Jin, Bo Zhu, Li Yuan, and Shuicheng Yan. MoH: Multi-Head Attention as Mixture-of-Head Attention, October 2024. URL http://arxiv.org/abs/2410.11842. Keller Jordan. KellerJordan/modded-nanogpt, May 2025. URL https://github.com/ KellerJordan/modded-nanogpt. Julie Kallini, Shikhar Murty, Christopher D. Manning, Christopher Potts, and Róbert Csordás. MrT5: Dynamic Token Merging for Efficient Byte-level Language Models. In The Thirteenth International Conference on Learning Representations, October 2024. URL https://openreview.net/forum? id=VYWBMq1L7H. Guy Kaplan, Matanel Oren, Yuval Reif, and Roy Schwartz. From Tokens to Words: On the Inner Lexicon of LLMs. In The Thirteenth International Conference on Learning Representations, October 2024. URL https://openreview.net/forum?id=328vch6tRs. Andrej Karpathy. karpathy/nanoGPT, May 2025. URL https://github.com/karpathy/nanoGPT. Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, and Kang Min Yoo. Peri-LN: Revisiting Normalization Layer in the Transformer Architecture. In Forty-second International Conference on Machine Learning, June 2025. URL https://openreview.net/forum?id=ci1S6wmXfO& noteId=r07RHYqMC5. Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization, January 2017. URL http://arxiv.org/abs/1412.6980. Vedang Lad, Wes Gurnee, and Max Tegmark. The Remarkable Robustness of LLMs: Stages of Inference?, June 2024. URL http://arxiv.org/abs/2406.19384. Tim Lawson, Lucy Farnik, Conor Houghton, and Laurence Aitchison. Residual Stream Analysis with Multi-Layer SAEs. In The Thirteenth International Conference on Learning Representations, October 2024. URL https://openreview.net/forum?id=XAjfjizaKs. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, June 2020. URL http://arxiv.org/abs/2006.16668. Zhixuan Lin, Evgenii Nikishin, Xu He, and Aaron Courville. Forgetting Transformer: Softmax Attention with Forget Gate. In The Thirteenth International Conference on Learning Representations, October 2024. URL https://openreview.net/forum?id=q2Lnyegkr8. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, January 2019. URL http://arxiv.org/abs/1711.05101. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. ShortGPT: Layers in Large Language Models are More Redundant Than You Expect, October 2024. URL http://arxiv.org/abs/2403.03853. Pit Neitemeier, Björn Deiseroth, Constantin Eichenberg, and Lukas Balles. Hierarchical Autoregressive Transformers: Combining Byteand Word-Level Processing for Robust, Adaptable Language Models. In The Thirteenth International Conference on Learning Representations, October 2024. URL https://openreview.net/forum?id=tU074jg2vS. 9 nostalgebraist. Interpreting GPT: the logit lens, August 2020. URL https://www.lesswrong.com/ posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, March 2022. URL http://arxiv.org/abs/2203.02155. Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, and Srinivasan Iyer. Byte Latent Transformer: Patches Scale Better Than Tokens, December 2024. URL http://arxiv.org/abs/2412.09871. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale, October 2024. URL http://arxiv.org/abs/2406.17557. Matthew Peroni and Dimitris Bertsimas. Skip Transformers: Efficient Inference through Skip-Routing. October 2024. URL https://openreview.net/forum?id=gdMJlwTcSQ. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners, 2019. URL https: //cdn.openai.com/better-language-models/language_models_are_unsupervised_ multitask_learners.pdf. David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-Depths: Dynamically allocating compute in transformer-based language models, April 2024. URL http://arxiv.org/abs/2404.02258. Noam Shazeer. GLU Variants Improve Transformer, February 2020. URL http://arxiv.org/abs/ 2002.05202. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, January 2017. URL http://arxiv.org/abs/1701.06538. Kevin Slagle. SpaceByte: Towards Deleting Tokenization from Large Language Modeling. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, November 2024. URL https://openreview.net/forum?id=KEe4IUp20I. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomput., 568(C), February 2024. ISSN 0925-2312. doi: 10.1016/j.neucom.2023.127063. Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 24642469, December 2016. doi: 10.1109/ICPR.2016.7900006. Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, and David Lopez-Paz. From Bytes to Ideas: Language Modeling with Autoregressive U-Nets, 2025. URL https://arxiv.org/abs/2506.14761. Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. SkipNet: Learning Dynamic Routing in Convolutional Networks. In Computer Vision ECCV 2018: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, pages 420436, Berlin, Heidelberg, September 2018. Springer-Verlag. ISBN 978-3-030-01260-1. doi: 10.1007/ 978-3-030-01261-8_25. Ziteng Wang, Jun Zhu, and Jianfei Chen. ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing. In The Thirteenth International Conference on Learning Representations, October 2024. URL https://openreview.net/forum?id=4D0f16Vwc3. 10 Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 22462251, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.204. Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. ByT5: Towards Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10:291306, March 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00461. Dewen Zeng, Nan Du, Tao Wang, Yuanzhong Xu, Tao Lei, Zhifeng Chen, and Claire Cui. Learning to Skip for Language Modeling, November 2023. URL http://arxiv.org/abs/2311.15436. Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, and Zhang Xiong. Mixture of Attention Heads: Selecting Attention Heads Per Token. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 41504162, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.278."
        }
    ],
    "affiliations": [
        "School of Engineering Mathematics and Technology University of Bristol Bristol, UK"
    ]
}