{
    "paper_title": "Language-Image Alignment with Fixed Text Encoders",
    "authors": [
        "Jingfeng Yang",
        "Ziyang Wu",
        "Yue Zhao",
        "Yi Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 9 0 2 4 0 . 6 0 5 2 : r Language-Image Alignment with Fixed Text Encoders Jingfeng Yang1 Ziyang Wu1 Yue Zhao1 Yi Ma1,2 1UC Berkeley 2The University of Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such costly joint training is necessary. In particular, we investigate if pre-trained fixed large language model (LLM) offers good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations. Our code and checkpoints are available at https://github.com/Jingfeng0705/LIFT."
        },
        {
            "title": "Introduction",
            "content": "Contrastive pre-training on massive collections of text-image pairs has recently emerged as powerful paradigm for learning language-aligned visual representations and demonstrates strong performance in applications such as vision-language models (VLMs) [29, 30]. Representative works such as CLIP [43] and SigLIP [59] align text and image embeddings in shared space by training separate text and image encoders on paired data with contrastive loss. This approach is shown to be highly effective most VLMs today employ such pre-trained image encoders due to their language-aligned visual representations. Despite its wide adoption, contrastive pre-training on text-image pairs has some widely known limitations. For instance, training both encoders from scratch makes the method computationally expensive [18, 37]. This issue is further exacerbated by the large batch sizes and the substantial amounts of training data demanded by CLIP. Moreover, CLIPs text and image encoders struggle to accurately encode compositional information, including word order (in texts), spatial locations (in images), objectattribute associations (both), and object-object relations (both) [16, 35, 48]. Prior studies attribute this limitation to the fact that contrastive pre-training on general-purpose retrieval datasets incentivizes CLIPs encoders trained from scratch to adopt shortcut strategy that suppresses (i.e., discards) features related to compositional information [13, 45]. Crucially, core assumption held by dominant contrastive approaches lies at the heart of these limitations: optimal language-image alignment requires jointly training text and image encoders from scratch. In this work, we question the necessity of this joint training and prove that large language models (LLMs) already provide good enough text embeddings to guide visual representation learning. Concretely, we use pre-trained text encoder fine-tuned on an LLM to embed texts offline and solely train the image encoder to align visual representations with the text embeddings. We name this approach Language-Image alignment with Fixed Text encoder (LIFT). The name LIFT also suggests that it aligns raw visual inputs with their corresponding higher-level semantics. Fig. 2 illustrates the overall pipeline of the proposed framework. Preprint. Under review. Figure 1: The qualitative comparisons between LIFT and CLIP [43]. The first line shows the caption or option selected by LIFT, and the second line shows the one selected by CLIP. In every case, LIFT selects the correct one, while CLIP does not. We observe that LIFT compensates for CLIPs shortcomings in tasks involving compositional information (e.g., spatial locations, object-attribute associations, object-object relations). To investigate whether, when, and why LIFT might offer advantages over vanilla CLIP, we perform extensive benchmarking and ablation studies to address four fundamental questions: Across tasks evaluating different model capabilities, on which does LIFT demonstrate strengths compared to CLIP, and on which does it fall short? Answer: LIFT outperforms CLIP by an average accuracy gain of 7.4% across seven compositional understanding tasks and also leads CLIP on five out of six LLaVA [29, 30] downstream tasks, all driven by its superior ability to encode compositional information. It also matches CLIPs zero-shot retrieval performance. Some qualitative results are shown in Fig. 1. On which types of training data does LIFT outperform CLIP, and why? Answer: When trained on short, web-scraped captions, CLIP has slight edge over LIFT on three zero-shot retrieval tasks and one LLaVA downstream task. However, all of these advantages transfer to LIFT when both are trained on long, synthetic captions. We attribute LIFTs better performance to its robustness against the inverse effect induced by synthetic captions. What design choices of LLM-based text encoders enable better language-image alignment? Answer: Vanilla LLMs generally perform poorly as the text encoder for LIFT. Contrastive finetuning targeted at improving text encoding is typically necessary, whereas additional embedding extraction modules are not. Given its powerful LLM-based text encoder, can LIFT simplify some of the design choices in mainstream contrastive language-image alignment approaches? Answer: We find that simpler yet efficient cosine similarity loss can substitute for the contrastive loss while achieving comparable performance on the compositional understanding tasks and LLaVA downstream tasks. 2 Figure 2: The pipeline of LIFT, which adopts dual-encoder architecture similar to CLIP [43]. LIFT uses an LLM-based text encoder text to pre-compute the embedding zT for each text sample offline. During training, we solely update the image encoder img to align image embeddings with the ϕ pre-computed text embeddings by optimizing an alignment objective. and the projection head head θ"
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Language-Image Representation Learning Currently, language-image alignment is typically achieved through contrastive learning. Seminal works CLIP [43] and ALIGN [20] pioneer large-scale pre-training on text-image pairs to learn joint embeddings, enabling zero-shot transfer to various downstream tasks. These approaches rely on dual-encoder architecture in which text and image encoders are jointly trained from scratch to maximize the alignment (e.g., measured by cosine similarity) between paired samples embeddings while minimizing the alignment between non-matching pairs embeddings. Subsequently, SigLIP [59] improves training stability using sigmoid contrastive loss function. However, these methods demand significant computational resources to train both encoders. Recently, SuperClass [18] and CatLIP [37] have explored an alternative training paradigm without text encoders. They extract class labels from captions and cast language-image alignment as classification problem by training image encoders with binary cross-entropy loss. However, since these approaches ignore word order in captions, the resulting representations behave like bag-ofwords and lack compositional understanding. In this work, we aim to simplify the language-image alignment pipeline while avoiding the shortcomings of existing text encoder-free approaches. 2.2 The Limitations of CLIP It is well known that CLIP lacks compositional understanding, largely because contrastive pre-training on general-purpose retrieval datasets encourages CLIPs encoders to adopt shortcut strategy that ignores compositional information [13, 58]. To tackle this issue, [58] incorporates designed negative samples during training; [49, 50] combine features from multiple image encoders. More recently, [36, 39, 41, 51] merge self-supervised learning methods with contrastive learning, leading to image encoders with stronger vision-centric capabilities. Closer to our study, [5, 47] replace CLIPs text encoder with LLMs and jointly train them with image encoders, achieving better scaling behavior and improved compositional understanding. However, by introducing additional modifications to their training pipelines, they fail to isolate the use of unaltered LLM text embeddings and therefore do not systematically study their effect on language-image alignment. 3 FLOPs (G) Memory Usage (GB) 500 400 200 100 464 425 ViT-S ViT-B ViT-L 368 46 155 59 105 27 ViT-S ViT-B ViT-L 33 36 37 20 14 15 19 13 50 40 20 10 CLIP(77) CLIP(128) LIFT (77,128) CLIP(77) CLIP(128) LIFT (77,128) Figure 3: The estimated training FLOPs per textimage sample for CLIP [43] and LIFT trained with average per-batch max caption length 77 and 128. Figure 4: The estimated H800 GPU memory usage for CLIP and LIFT trained with average per-batch max caption length 77 and 128. The batch size is 1024. Furthermore, [32, 33, 54] reveal that CLIP yields suboptimal zero-shot performance when trained on full-length long captions, which are typically synthesized by VLMs to include detailed object descriptions. [33] conjectures that CLIPs text encoder is distracted by the syntactic similarity of synthetic captions and fails to attend to semantically meaningful content. Truncation strategies, such as text shearing [33] and sub-caption sampling [60], can bring empirical improvements. However, they inevitably sacrifice the rich information that long captions provide. In summary, current approaches remain insufficient in resolving both issues. We attempt to tackle both by questioning prevailing setup in contrastive language-image alignment. 2.3 Text Embedding Models Text embedding models are widely used to extract semantic embeddings of texts and are crucial in tasks such as Retrieval-Augmented Generation (RAG) [24]. Traditional embedding models such as BERT [9] and T5 [44] are typically trained with bidirectional attention mechanisms. Recently, LLMbased embedding models such as LLM2Vec [3], E5-Mistral [55], SFR-Embedding-Mistral [2], and NV-Embed-V2 [23] aim to leverage the rich semantics of auto-regressive LLMs and have emerged as powerful embedding models on Massive Text Embedding Benchmark (MTEB) [40]. In particular, NV-Embed-V2 [23] introduces novel latent attention layer and two-stage contrastive instruction tuning, making the resulting text embeddings rich yet distinct between texts with different semantic meanings. Hence, it is adopted as the text encoder of LIFT."
        },
        {
            "title": "3 Method",
            "content": "As illustrated in Fig. 2, LIFT adopts dual-encoder architecture similar to CLIP [43]. Let (T, I) denotes text-image pair, where is sequence of tokens from vocabulary and RCHW . We apply pre-trained and frozen LLM-based text encoder text with an output dimension to extract the text embedding zT = text(T ) R1D. We parameterize an image encoder neural network img θ : RCHW R1d and projection head head ϕ : R1d R1D. img θ is implemented by ViT [10] of width and head ϕ is 2-layer MLP. Let be the final image embedding. We use CLIPs contrastive loss1 [43, 52] as our objective, which is zI = head ϕ img θ (I) R1D 4 Figure 5: The original captions (top) and their negative counterparts (bottom) from two SugarCrepe [16] tasks: replace relation (left) and swap attribute (right). calculated between batch of normalized zT and zI as: Lcontrastive = 1 2B (cid:34) (cid:88) i= log exp(zT zI j=1 exp(zT /τ ) zI (cid:80)B /τ ) + log exp(zI zT j=1 exp(zI /τ ) zT /τ ) (cid:80)B (cid:35) . Offline Text Embeddings Generation. Since we dont optimize text, the entire text embedding process can be performed offline. Specifically, before training LIFT, we embed all the captions in our dataset once, allowing subsequent trainings to reuse the pre-computed caption embeddings without the text encoder. LIFT utilizes NV-Embed-V2 [23] as text. As point of reference, eight H800 GPUs can embed 100M captions per day at bfloat16 precision. As shown in Fig. 3 and Fig. 4, embedding texts offline leads to significant computational and memory efficiency. Concretely, given average per-batch max caption token length n, the FLOPs and memory footprint of CLIP scale with O(n2) complexity, whereas LIFT achieves O(1) amortized complexity. We also quantitatively benchmark CLIP and LIFT on both short (n = 77) and long (n = 128) captions. On average, LIFT reduces FLOPs by 25.5% for short captions and 35.7% for long ones, while lowering memory usage by 6.8% and 12.6%. The calculation details of FLOPs and memory usage are attached in Appendix A.4."
        },
        {
            "title": "4 Experiments",
            "content": "Experimental Settings. We train both LIFT and CLIP [43] (implemented by OpenCLIP [6]) using ViT-B/16 [10] vision backbone on dataset containing 400 million text-image pairs. Each image has two types of captions: short, web-scraped caption from DataComp-1B [12], and long, synthetic caption from Recap-DataComp-1B [25]. Their respective max caption token lengths are set to 77 and 323. To ensure fair comparison, both LIFT and CLIP are trained using the same hyperparameters: 500 warmup steps, weight decay of 0.2, learning rate of 1e-3 with cosine schedule, and batch size of 16,384. We use an input resolution of 224 224. The largest experiment uses 1.28 billion samples and is trained on eight H800 GPUs over 12 days. In the following sections, we systematically address the four questions raised in Sec. 1. 4.1 On which tasks does LIFT offer advantages, and on which does it fall short? Compositional Understanding. Compositional understanding is known limitation of CLIP. We evaluate it using seven SugarCrepe [16] tasks. As shown in Fig. 5, for each caption, SugarCrepe generates negative caption by add, replace, or swap an object, attribute, or relation in the original caption. Models are asked to identify the correct caption based on caption-image cosine similarity. 1In practice (see Sec. 4.4), we find simple cosine similarity loss also leads to reasonably strong performance. Method Dataset OpenCLIP DataComp LIFT DataComp OpenCLIP LIFT Recap Recap Sample Seen 1.28B 1.28B 512M 512M Add Replace Swap Obj 82.3 89.0 77.0 88.8 Att 73.7 86.1 73.7 92.2 Obj 91. 93.2 88.9 92.3 Att 79.4 86. 80.8 88.2 Rel 61.2 70.6 63. 76.8 Obj 59.6 64.1 62.0 66. Att 56.9 63.4 76.3 72.8 Table 1: The performance of LIFT and CLIP [43] on seven SugarCrepe [16] tasks. For each task, SugarCrepe generates negative captions by add, replace, or swap an object, attribute, or relation in the original captions. We report the accuracy of each task, and the best results are bolded. As shown in Table 1, when trained on the short captions from DataComp-1B, LIFT outperforms CLIP on all seven tasks with 6.8% average accuracy gain; when trained on the long, synthetic captions from Recap-DataComp-1B, it leads on six tasks with 7.9% gain. In both settings, LIFT achieves significant gains on add attribute, replace attribute, and replace relation tasks. These improvements are strong evidence that texts auto-regressive training objective avoids the compositional oversight induced by contrastive learning and enables more accurate modeling of objectattribute associations and objectobject relations. More visualizations can be found in Appendix A.2. Admittedly, LIFT ability to capture compositional information is not yet complete. It shows relatively low accuracy on swap object and swap attribute compared to other SugarCrepe tasks. We further attribute this limitation to the fact that the contrastive learning objective still focuses on aligning primarily lower-order statistics. Addressing this challenge requires exploring more refined information-theoretic measures for language-image alignment, key direction for future work. LLaVA [29, 30] Downstream Tasks. [50] shows that large multimodal models (LMMs) using CLIP as the vision tower inherit CLIPs limitations in modeling certain visual patterns. We train LMMs using LLaVA to examine whether LIFT transfers its superior compositional understanding to the LMM built on it. Vicuna-7B-V1.5 [7] is used as the base language model and is fine-tuned with LoRA [17]. All training hyperparameters follow the original LLaVA setup, except that we initialize LLaVAs projector with the weights of our 2-layer MLP head . ϕ Method Dataset OpenCLIP DataComp LIFT DataComp OpenCLIP LIFT Recap Recap Sample Seen 1.28B 1.28B 512M 512M TextVQA 47.9 48.9 46. 47.8 MMBench EN 53.3 57.8 48. 54.3 CN 45.9 50.6 42.0 46. MME POPE SciQA 1283.2 1289.2 1245. 1341.6 86.4 85.2 85.6 85.8 69. 70.2 69.0 69.4 Table 2: The performance of the LMM with either LIFT or CLIP [43] as the vision tower on LLaVA [29, 30] downstream tasks. The results are reported for TextVQA [46], MMBench [31] (English and Chinese), MME [11], POPE [27] (random accuracy), and ScienceQA [34] (accuracy). The best results are bolded. Method Dataset OpenCLIP DataComp LIFT DataComp OpenCLIP LIFT Recap Recap Sample Seen 1.28B 1.28B 512M 512M AR 61.3 67.8 58.3 60. CP FP-C FP-S LR RR 65. 72.3 63.5 70.6 50.3 51.0 45. 50.3 52.6 56.0 43.0 51.9 27. 28.0 26.3 28.0 39.1 47.0 32. 40.9 Table 3: The performance of the LMM with either LIFT or CLIP [43] as the vision tower on specific MMBench [31] subtasks. Abbreviations: AR for Attribute Reasoning; CP for Coarse Perception; FPC for Fine-grained Perception (Cross Instance); FP-S for Fine-grained Perception (Single Instance); LR for Logical Reasoning; RR for Relation Reasoning. The visualizations are provided in Appendix A.3. The best results are bolded. 6 Figure 6: The examples of syntactically similar but semantically different caption pairs from Recap-DataComp1B [25]. The synthetic captions follow the template {Adj.} {Noun} with {N. Phrase} {Verb} {Location}. CLIPs [43] text encoder often assigns higher scores to caption pairs with similar syntax, while LIFT better captures semantic differences and assigns lower ones. The scores are calculated based on the embeddings from LIFTs text and CLIPs text encoder trained on 512M Recap-DataComp-1B samples. As shown in Table 2, LIFT outperforms CLIP on five out of six LLaVA downstream tasks when both are trained on short captions, and leads on all tasks when trained on long captions. In both settings, LIFT shows substantial improvements on MMBench [31]. We further examine their performance on specific MMBench (English) subtasks to identify the exact visual patterns that offer the gains. As shown in Table 3, LIFT achieves significant accuracy gains on fine-grained perception (single-instance) and relational reasoning. The former subtask involves object localization and attribute recognition, while the latter includes identifying physical relations, all largely benefiting from LIFTs accurate encoding of compositional information. The visualizations are provided in Appendix A.3. Zero-shot Classification and Retrieval. Similar to CLIP, LIFT can perform zero-shot transfer for image classification and cross-modal retrieval by employing the LLM-based text encoder text to embed captions during inference. Following CLIPs evaluation protocol, we evaluate the models on ImageNet-1K validation set [8] by constructing image captions using the prompt template It is photo of {label}. More evaluation details are provided in Appendix A.6. As shown in Table 4, when trained on short captions, LIFT outperforms CLIP on two text-to-image tasks, while performing similarly on the remaining three tasks. When trained on long captions, however, LIFT leads CLIP by substantial margins on all these tasks, achieving an average accuracy gain of 11.0%. We analyze the cause of this decline in CLIPs relative performance in Sec. 4.2. Method Dataset OpenCLIP LIFT OpenCLIP LIFT DataComp DataComp Recap Recap Sample Seen 1.28B 1.28B 512M 512M ImageNet 58. 58.3 34.6 43.6 COCO Flickr I2T 31.0 29.1 25.7 34.6 T2I 27. 28.1 26.7 36.0 I2T 62.9 58. 56.4 69.1 T2I 59.6 63.7 57. 72.9 Table 4: The zero-shot performance of LIFT and CLIP [43] on ImageNet-1K [8] classification, COCO [28] Image-to-Text and Text-to-Image retrieval, and Flickr30K [57] Image-to-Text and Text-to-Image retrieval. For all tasks, we report top-1 accuracy. The best results are bolded. 4.2 On which types of training data does LIFT outperform CLIP, and why? As reported in Sec. 4.1, when trained on short captions, CLIP [43] has slight edge over LIFT on POPE [27], ImageNet-1K [8] zero-shot classification, and two image-to-text retrieval tasks. However, all of these advantages are overtaken by LIFT when both are trained on long, synthetic captions. In this section, we investigate the reasons behind CLIPs loss of performance advantages. 7 One contributing factor is the inverse effect [26, 32], which observes that CLIP trained on full-length synthetic captions yields suboptimal zero-shot performance but shows noticeable improvements as the captions are progressively truncated. This effect likely stems from the homogeneous caption syntax introduced by caption generators (usually fine-tuned VLMs), which can distort the original caption distribution and become shortcut feature [33, 45]. Such homogeneous syntax is most pronounced in full-length synthetic captions and weakens with increasing truncation. As shown in Fig. 6, CLIPs text encoder is misled by this shortcut feature during training from scratch. By computing the average pairwise cosine similarity of 1,000 captions randomly drawn from RecapDataComp-1B [25], we find that CLIPs text encoder overemphasizes syntactic similarity, assigning high similarity scores to caption pairs that are syntactically similar but semantically different. In contrast, LIFT employs the LLM-based encoder text pre-trained on large-scale data, which yields an embedding space more robust to syntactic homogeneity. It focuses more on semantic content and assigns significantly lower similarity scores to such misleading caption pairs. However, the 11.0% accuracy gap between LIFT and CLIP cannot be fully attributed to the inverse effect. We argue that another key factor is the expressive power of text encoders, which is especially critical for handling the greater semantic complexity of long, synthetic captions. Specifically, the CLIPs text encoder in our implementation has 63M parameters, whereas LIFTs text has 7B parameters. Despite its larger architecture, LIFT remains more efficient for long-caption processing due to its use of offline embeddings. 4.3 What design choices of LLM-based text encoders enable better language-image alignment? Several key design choices have contributed to the transformation of LLMs into state-of-the-art text encoders, and this section examines which of these choices are most helpful for the language-image alignment achieved by LIFT. [38, 42] first show that the hidden state of end-of-sentence token <eos> contains sufficient information about an input sequence and can be used as its embedding. Later methods explore alternative embedding extraction strategies, as <eos> tends to bias attention toward nearby tokens and ignore distant ones [19, 23]. Meanwhile, contrastive fine-tuning using instructions designed for general text embedding tasks (retrieval, clustering, etc.) has been shown to further improve embedding quality [2, 22, 23]. To disentangle the effects of these design choices on language-image alignment, we train LIFT using five representative LLMs as text. Mistral-7B-V0.1 [21] and Vicuna-7B-V0.1 [7] are vanilla LLMs without any fine-tuning or architectural changes; SFR-Embed-Mistral [2] and Linq-EmbedMistral [22] both apply contrastive fine-tuning; NV-Embed-V2 [23] represents the most evolved variant, combining contrastive fine-tuning with latent attention layer to extract embeddings. All models have 7B parameters, with hidden size and final embedding dimension of 4096. As shown in Table 5, two vanilla LLMs lag significantly behind their fine-tuned counterparts across all reported tasks for instance, by an average accuracy decline of 22.8% on ImageNet-1K [8] zero-shot classification. Vanilla Mistral-7B-V0.1 even performs no better than random guessing on SugarCrepes [16] replace relation task. These results indicate that LLMs are not inherently effective as the text encoder for LIFT, and contrastive fine-tuning is necessary. On the other hand, all three fine-tuned models achieve comparable performance, suggesting that <eos> token alone can accurately encode input captions, and that advanced embedding extraction mechanisms, such as NV-Embed-V2s additional latent attention layer, may not be required. LLM Encoder Mistral-7B-V0.1 Vicuna-7B-V1.5 SFR-Embed-Mistral Linq-Embed-Mistral NV-Embed-V Contrastive Fine-tuning Embedding Extraction ImageNet <eos> <eos> <eos> <eos> Latent Attention 12.5 23.8 39.9 39.8 40. Flickr Replace I2T 5.4 14.1 44. 44.2 42.7 T2I 3.3 13.2 42. 41.6 44.1 Obj 69.0 78.1 87. 84.8 87.2 Att 58.4 73.1 82. 81.5 81.2 Rel 50.0 58.8 70. 64.2 67.3 Table 5: The ablation study on the choice of LIFTs LLM-based text encoder. The results are reported for ImageNet-1K [8] classification, Flickr30K [57] retrieval tasks, and SugarCrepes [16] replace task. All models are trained on 128M samples from DataComp-1B [12]. The best results are bolded, and the second-best results are underlined. 8 4.4 Can LIFT simplify some of the design choices in mainstream contrastive language-image alignment approaches? Simple Cosine Similarity Loss. CLIP [43] and its variants have demonstrated the effectiveness of cosine similarity for language-image alignment. To avoid mode collapse (i.e., identical outputs from text and image encoders regardless of the inputs), CLIP employs contrastive InfoNCE [52] loss based on pairwise cosine similarities. However, this approach is computationally intensive, with both FLOPs and memory scaling asymptotically as O(B2) with batch size B. It also requires large batch size to ensure sufficient negative samples. SigLIP [59] introduces chunked strategy that relaxes the large-batch requirement, yet its FLOPs and memory still grow quadratically with local batch size. Since the embedding space of text is fixed, mode collapse is no longer concern. This motivates us to explore whether simple cosine similarity loss, computed solely on positive text-image pairs without involving negative pairs, can be effective as well. Specifically, for batch of size B, we optimize Lcosine ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) (1 zT zI ), i=1 where zT = text(T ) and zI = head (I) is normalized embedded text-image pair in the batch. This simple loss has O(B) FLOPs and memory complexity with respect to batch size and removes the reliance on negative samples, thereby easing the batch size constraint. [4, 14] also explore conceptually similar cosine similarity losses, but they employ more sophisticated techniques to address mode collapse. ϕ img θ Loss Dataset Contrastive DataComp Cosine Sim DataComp Contrastive Cosine Sim Recap Recap Sample Seen 426M 426M 512M 512M ImageNet 45.5 26.8 43.6 38.8 Flickr Add MMBench I2T 45.8 10.2 69.1 58. T2I 51.9 19.5 72.9 68.9 Obj 82.4 85.4 88.8 86.9 Att 84. 74.0 92.2 88.2 EN 54.0 53. 54.3 57.5 CN 43.4 43.0 46. 50.3 Table 6: The ablation study on the choice of LIFTs loss function. The results are reported for ImageNet-1K [8] classification, Flickr30K [57] retrieval tasks, SugarCrepes [16] Add task, and MMBench [31] (English and Chinese). The best results are bolded. As shown in Table 6, the simple cosine similarity loss performs comparably to the contrastive loss on the compositional understanding tasks and LLaVA [29, 30] downstream tasks. Notably, when trained on long captions, LIFT using the simple cosine similarity loss outperforms its contrastive loss variant on both English and Chinese MMBench [31] by non-trivial margins. However, it suffers significant performance drops in the zero-shot retrieval tasks, particularly when trained on short, web-scraped captions. The simple cosine similarity loss shows an accuracy decline of 18.7% on ImageNet-1K [8] and 34.0% on Flickr30K [57]. We attribute this performance gap to the contrastive losss use of negative samples, which encourages more discriminative representations that benefit classification and retrieval tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we question core assumption held by dominant language-image alignment approaches like CLIP [43] that text and image encoders should be jointly trained from scratch to achieve optimal language-image alignment. We present LIFT, which pre-computes fixed text embeddings from LLMs and solely trains the image encoder. Comprehensive benchmarking and ablation studies confirm that LIFT outperforms CLIP in key scenarios that involve compositional understanding and long captions. Further experiments show that contrastive fine-tuning is crucial for LIFTs LLM-based text encoder to generate effective text embeddings and that LIFT also enables the use of simpler yet effective loss function. Our work initiates systematic exploration of how text embeddings from LLMs can guide visual representation learning and opens new avenue for language-image alignment."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. Llama 3 model card. 2024. [2] R. M. amd Ye Liu, S. R. Joty, C. Xiong, Y. Zhou, and S. Yavuz. Sfr-embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog, 2024. [3] P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large language models are secretly powerful text encoders, 2024. [4] X. Chen and K. He. Exploring simple siamese representation learning, 2020. [5] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P. Luo, T. Lu, Y. Qiao, and J. Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, 2024. [6] M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev. Reproducible scaling laws for contrastive language-image learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 28182829. IEEE, June 2023. [7] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. [8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. [9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. [10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [11] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu, and R. Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. [12] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. Pratt, V. Ramanujan, Y. Bitton, K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. W. Koh, O. Saukh, A. Ratner, S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. Dimakis, J. Jitsev, Y. Carmon, V. Shankar, and L. Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. [13] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, Nov. 2020. [14] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: new approach to self-supervised learning, 2020. [15] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models, 2022. [16] C.-Y. Hsieh, J. Zhang, Z. Ma, A. Kembhavi, and R. Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. In Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [17] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [18] Z. Huang, Q. Ye, B. Kang, J. Feng, and H. Fan. Classification done right for vision-language pre-training, 2024. [19] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira. Perceiver: General perception with iterative attention, 2021. [20] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision, 2021. [21] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [22] J. Kim, S. Lee, J. Kwon, S. Gu, Y. Kim, M. Cho, J. yong Sohn, and C. Choi. Linq-embed-mistral:elevating text retrieval with improved gpt data through task-specific control and quality refinement. Linq AI Research Blog, 2024. [23] C. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping. Nv-embed: Improved techniques for training llms as generalist embedding models, 2025. [24] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. [25] X. Li, H. Tu, M. Hui, Z. Wang, B. Zhao, J. Xiao, S. Ren, J. Mei, Q. Liu, H. Zheng, Y. Zhou, and C. Xie. What if we recaption billions of web images with llama-3?, 2024. [26] X. Li, Z. Wang, and C. Xie. An inverse scaling law for clip training, 2023. [27] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in large vision-language models, 2023. [28] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollár. Microsoft coco: Common objects in context, 2015. [29] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning, 2024. [30] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning, 2023. [31] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. [32] Y. Liu, X. Li, Z. Wang, B. Zhao, and C. Xie. Clips: An enhanced clip framework for learning with synthetic captions, 2024. [33] Y. Liu, K. Wang, W. Shao, P. Luo, Y. Qiao, M. Z. Shou, K. Zhang, and Y. You. Mllms-augmented visual-language representation learning, 2024. [34] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. [35] Z. Ma, J. Hong, M. O. Gul, M. Gandhi, I. Gao, and R. Krishna. Crepe: Can vision-language foundation models reason compositionally?, 2023. [36] K.-K. Maninis, K. Chen, S. Ghosh, A. Karpur, K. Chen, Y. Xia, B. Cao, D. Salz, G. Han, J. Dlabal, D. Gnanapragasam, M. Seyedhosseini, H. Zhou, and A. Araujo. Tips: Text-image pretraining with spatial awareness, 2025. [37] S. Mehta, M. Horton, F. Faghri, M. H. Sekhavat, M. Najibi, M. Farajtabar, O. Tuzel, and M. Rastegari. Catlip: Clip-level visual recognition accuracy with 2.7x faster pre-training on web-scale image-text data, 2024. [38] J. X. Morris, W. Zhao, J. T. Chiu, V. Shmatikov, and A. M. Rush. Language model inversion, 2023. [39] N. Mu, A. Kirillov, D. Wagner, and S. Xie. Slip: Self-supervision meets language-image pre-training, 2021. [40] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. Mteb: Massive text embedding benchmark, 2023. [41] M. F. Naeem, Y. Xian, X. Zhai, L. Hoyer, L. V. Gool, and F. Tombari. Silc: Improving vision language pretraining with self-distillation, 2023. [42] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, J. Heidecke, P. Shyam, B. Power, T. E. Nekoul, G. Sastry, G. Krueger, D. Schnurr, F. P. Such, K. Hsu, M. Thompson, T. Khan, T. Sherbakov, J. Jang, P. Welinder, and L. Weng. Text and code embeddings by contrastive pre-training, 2022. 11 [43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [44] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. [45] J. Robinson, L. Sun, K. Yu, K. Batmanghelich, S. Jegelka, and S. Sra. Can contrastive learning avoid shortcut solutions?, 2021. [46] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards vqa models that can read, 2019. [47] A. Stone, H. Soltau, R. Geirhos, X. Yi, Y. Xia, B. Cao, K. Chen, A. Ogale, and J. Shlens. Learning visual composition through improved semantic guidance, 2025. [48] T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross. Winoground: Probing vision and language models for visio-linguistic compositionality, 2022. [49] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, Z. Wang, R. Fergus, Y. LeCun, and S. Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. [50] S. Tong, Z. Liu, Y. Zhai, Y. Ma, Y. LeCun, and S. Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms, 2024. [51] M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans, L. Beyer, Y. Xia, B. Mustafa, O. Hénaff, J. Harmsen, A. Steiner, and X. Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. [52] A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding, 2019. [53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. [54] A. J. Wang, K. Q. Lin, D. J. Zhang, S. W. Lei, and M. Z. Shou. Too large; data reduction for vision-language pre-training, 2023. [55] L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with large language models, 2024. [56] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. [57] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. [58] M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In International Conference on Learning Representations, 2023. [59] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training, 2023. [60] K. Zheng, Y. Zhang, W. Wu, F. Lu, S. Ma, X. Jin, W. Chen, and Y. Shen. Dreamlip: Language-image pre-training with long captions, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Limitations As discussed in the experiments section, LIFT ability to capture compositional information is not yet complete. It shows relatively low accuracy on swap object and swap attribute compared to other SugarCrepe [16] tasks. We attribute this limitation to the fact that contrastive learning objectives still focus on aligning primarily lower-order statistics. Addressing this challenge requires exploring more refined information-theoretic measures for language-image alignment, which highlights promising avenue for future research. Also, due to computational constraints, we are unable to evaluate the scalability of LIFT beyond 1.28 billion training samples. We acknowledge that CLIP [43] and its variants may exhibit more favorable scaling behavior, as they jointly train both text and image encoders, whereas LIFT keeps its text encoder frozen. Prior studies have shown that selectively unfreezing the last four layers of LLMs can substantially improve the scalability of image encoders without incurring heavy computational costs [5, 47]. How to efficiently fine-tune LLMs within mainstream language-image alignment pipelines remains an important direction for future work. A.2 More Visualizations from SugarCrepe [16] We present the visual results from evaluating LIFT and CLIP [43] on the seven compositional understanding tasks from SugarCrepe. For each caption, SugarCrepe generates challenging negative caption by add, replace, or swap an object, attribute, or relation in the original caption. Models are asked to identify the correct caption based on caption-image cosine similarity. Both models use ViT-B/16 [10] backbone and are trained on 1.28B samples from DataComp1B [12]. Each subfigure shows the captions selected by LIFT (top) and CLIP (bottom). In every case, LIFT selects the correct caption, while CLIP does not. Figure A1: The visualizations of the add object task from SugarCrepe [16]. Each subfigure shows the captions selected by LIFT (top) and CLIP [43] (bottom). In every case, LIFT selects the correct caption, while CLIP does not. 13 (a) Add Attribute (b) Replace Object Figure A2: The visualizations of the add attribute and replace object tasks from SugarCrepe [16]. Each subfigure shows the captions selected by LIFT (top) and CLIP [43] (bottom). In every case, LIFT selects the correct caption, while CLIP does not. 14 (a) Replace Attribute (b) Replace Relation Figure A3: The visualizations of the replace attribute and replace relation tasks from SugarCrepe [16]. Each subfigure shows the captions selected by LIFT (top) and CLIP [43] (bottom). In every case, LIFT selects the correct caption, while CLIP does not. 15 (a) Swap Object (b) Swap Attribute Figure A4: The visualizations of the swap object and swap attribute tasks from SugarCrepe [16]. Each subfigure shows the captions selected by LIFT (top) and CLIP [43] (bottom). In every case, LIFT selects the correct caption, while CLIP does not. 16 A.3 More Visualizations from MMBench [31] We present the visual results from evaluating LIFT and CLIP [43] on five MMBench subtasks that specifically test models compositional understanding. All questions are in the form of multiplechoice. Both models use ViT-B/16 [10] backbone and are trained on 1.28B samples from DataComp1B [12]. Each subfigure shows the options selected by LIFT (highlighted in orange) and CLIP (highlighted in cyan). In every case, LIFT selects the correct option, while CLIP does not. Figure A5: The visualizations of the object localization subtask from MMBench [31]. Each subfigure shows the options selected by LIFT and CLIP [43]. In every case, LIFT selects the correct option, while CLIP does not. 17 (a) Attribute Recognition (b) Physical Relation Figure A6: The visualizations of the attribute recognition and physical relation subtasks from MMBench [31]. Each subfigure shows the options selected by LIFT and CLIP [43]. In every case, LIFT selects the correct option, while CLIP does not. 18 (a) Spatial Relation (b) Counting Figure A7: The visualizations of the spatial relation and counting subtasks from MMBench [31]. Each subfigure shows the options selected by LIFT and CLIP [43]. In every case, LIFT selects the correct option, while CLIP does not. 19 A.4 The Calculation Details of FLOPs and Memory Usage Algorithm 1 FLOPS OF LANGUAGE TRANSFORMERS [53] Require: nctx (average per-batch max caption token length), nvocab (vocab size), dmodel (model width), nheads (attention head number), dkey (key dimension), dff (feed-forward width), nlayers (layer number). Ensure: Ftotal Embedding FLOPs 1: Femb 2 nctx nvocab dmodel Attention FLOPs (per layer) 2: Fqkv 2 nctx (3 dmodel) (dkey nheads) 3: Fqk 2 n2 ctx (dkey nheads) 4: Fsoft 3 nheads n2 ctx 5: Fred 2 n2 ctx (dkey nheads) 6: Fproj 2 nctx (dkey nheads) dmodel 7: Fattn Fqkv + Fqk + Fsoft + Fred + Fproj Feed-forward FLOPs (per layer) 8: Fff 4 nctx (dmodel dff) Total FLOPs 9: Ftotal Femb + nlayers (Fattn + Fff) 10: return 3 Ftotal Algorithm 2 FLOPS OF VISION TRANSFORMERS [10] Require: npatch (patch number), dpatch (patch size), nchannels (channel number), dmodel (model width), nheads (attention head number), dkey (key dimension), dff (feed-forward width), nlayers (layer number). Ensure: Ftotal Embedding FLOPs 1: Femb 2 npatch d2 patch nchannels dmodel Attention FLOPs (per layer) 2: Fqkv 2 npatch (3 dmodel) (dkey nheads) 3: Fqk 2 n2 patch (dkey nheads) 4: Fsoft 3 nheads n2 5: Fred 2 n2 6: Fproj 2 npatch (dkey nheads) dmodel 7: Fattn Fqkv + Fqk + Fsoft + Fred + Fproj patch patch (dkey nheads) Feed-forward FLOPs (per layer) 8: Fff 4 npatch (dmodel dff) Total FLOPs 9: Ftotal Femb + nlayers (Fattn + Fff) 10: return 3 Ftotal FLOPS. Due to the truncation and padding applied by text tokenizers, the average per-sample FLOPs of each model should be calculated based on the average per-batch max caption token length. In our calculation, we approximate this value using the global max caption token length. Although this introduces some deviation from the exact values, our goal is not to report precise measurements but to demonstrate the difference in scaling behavior between the two models: CLIP [43]s FLOPs and memory footprint scale with O(n2) complexity, while LIFT achieves an amortized complexity of O(1). In Algorithm 1 and Algorithm 2, we present steps to calculate the per-sample FLOPs of the language [53] and vision transformers [10] used in this study. Both algorithms are adapted from [15]. For CLIP, the per-sample FLOPs is approximated as the sum of the FLOPs from its language and vision transformers. Since LIFT keeps its text encoder frozen, the per-sample FLOPs is estimated as that of its vision transformer alone. Throughout, we assume that backward pass incurs twice the FLOPs of forward pass. Memory Usage. The memory usage of each model is estimated by monitoring GPU status using the nvidia-smi command. We observe that the exact memory consumption varies depending on factors such as training stages and GPUs. The reported results represent the average of five training runs conducted on different H800 GPUs with varying random seeds. A.5 Broader Impacts Similar to other vision-language models (VLMs), LIFT has significant potential for positive societal impacts. For instance, it advances multimodal understanding and can be fine-tuned to generate high-quality image captions for visually impaired individuals. As weakly supervised method, LIFT also reduces the reliance on heavily labeled datasets. However, LIFT also carries potential negative societal impacts. It may inherit societal biases present in its training data, leading to harmful outcomes when deployed in sensitive applications. We advocate for cautious use of the model and recommend mitigating risks through careful dataset curation, bias analysis, and responsible deployment. A.6 Dataset Details A.6.1 Training Datasets DataComp-1B [12] is large-scale, multimodal dataset consisting of approximately 1.4 billion text-image pairs curated from massive pool of 12.8 billion web-crawled samples. The dataset is constructed using various filtering strategies including CLIP [43] score filtering, text-based filtering, and image-based filtering to identify high-quality, semantically aligned pairs. Models trained on DataComp-1B have been shown to achieve notable gains on downstream tasks, including zero-shot classification on ImageNet-1K [8]. Recap-DataComp-1B [25] builds upon DataComp-1B to address inherent noise and semantic misalignment in web-crawled captions. It fine-tunes LLaVA-1.5 [29] on LLaMA-3-8B [1] to recaption images with longer, richer, and more semantically aligned descriptions. Empirical studies indicate that Recap-DataComp-1B leads to gains in both cross-modal retrieval and text-to-image generation under complex queries. A.6.2 Evaluation Datasets ImageNet [8] is large-scale, publicly available image dataset containing over 14 million images across more than 20,000 categories. For our zero-shot classification experiments, we use ImageNet1K validation set, which consists of 50,000 images, with 50 images for each of the 1,000 classes. The input captions are constructed using the prompt template It is photo of {label}. COCO [28] is large-scale dataset for object detection, segmentation, keypoint detection, and image captioning, comprising 328,000 images. For our zero-shot retrieval experiments, we use Val2017 split of 5,000 images and randomly select one of the five ground-truth captions for each image. Flickr30K [57] consists of 31,000 images sourced from Flickr, each paired with five human-annotated reference captions. For our zero-shot retrieval experiments, we evaluate vision-language models on the test set of 1,000 images and randomly select one of the five ground-truth captions for each image. SugarCrepe [16] is benchmark designed to evaluate compositional understanding of visionlanguage models while addressing biases present in existing datasets. SugarCrepe employs large language models to generate fluent and challenging negative captions by add, replace, or swap an object, attribute, or relation in the original captions. Models are tasked with selecting the correct captions among these compositional distractors based on caption-image cosine similarity. MMBench [31] is benchmark designed to evaluate vision-language models across broad range of perception and reasoning abilities. It features diverse set of evaluation questions curated with strict quality control and introduces CircularEval strategy that uses large language models to map free-form outputs to multiple-choice answers. It supports bilingual evaluation in English and Chinese. MME [11] is benchmark designed to evaluate vision-language models across 14 subtasks covering both perception and cognitive abilities. It provides systematic evaluations using manually crafted 21 instruction-answer pairs. MMEs standardized instructions enable fair comparisons between models without relying on prompt engineering. POPE [27] is benchmark designed to systematically assess object hallucination in vision-language models, common issue in which models tend to generate objects that are inconsistent with the target images. It innovates polling-based query method that offers more stable and flexible evaluation of hallucinated content. ScienceQA [34] is vision-language model benchmark comprising about 21,000 multiple-choice science questions. Each question is accompanied by rich annotations that encourage models to generate chain-of-thought [56] (CoT) responses and enable the study of multi-hop reasoning. ScienceQA supports evaluation across textual, visual, and diagrammatic modalities. TextVQA [46] is benchmark designed to assess vision-language models ability to recognize text within images. It consists of 45,336 questions across 28,408 images, where successful answering requires models to accurately read text in the image and reason about it in the context of both the image and the question."
        }
    ],
    "affiliations": [
        "The University of Hong Kong",
        "UC Berkeley"
    ]
}