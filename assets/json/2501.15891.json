{
    "paper_title": "Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks",
    "authors": [
        "Hailong Guo",
        "Bohan Zeng",
        "Yiren Song",
        "Wentao Zhang",
        "Chuang Zhang",
        "Jiaming Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image-based virtual try-on (VTON) aims to generate a virtual try-on result by transferring an input garment onto a target person's image. However, the scarcity of paired garment-model data makes it challenging for existing methods to achieve high generalization and quality in VTON. Also, it limits the ability to generate mask-free try-ons. To tackle the data scarcity problem, approaches such as Stable Garment and MMTryon use a synthetic data strategy, effectively increasing the amount of paired data on the model side. However, existing methods are typically limited to performing specific try-on tasks and lack user-friendliness. To enhance the generalization and controllability of VTON generation, we propose Any2AnyTryon, which can generate try-on results based on different textual instructions and model garment images to meet various needs, eliminating the reliance on masks, poses, or other conditions. Specifically, we first construct the virtual try-on dataset LAION-Garment, the largest known open-source garment try-on dataset. Then, we introduce adaptive position embedding, which enables the model to generate satisfactory outfitted model images or garment images based on input images of different sizes and categories, significantly enhancing the generalization and controllability of VTON generation. In our experiments, we demonstrate the effectiveness of our Any2AnyTryon and compare it with existing methods. The results show that Any2AnyTryon enables flexible, controllable, and high-quality image-based virtual try-on generation.https://logn-2024.github.io/Any2anyTryonProjectPage/"
        },
        {
            "title": "Start",
            "content": "Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks HAILONG GUO, Beijing University of Posts and Telecommunications, China BOHAN ZENG, Peking University, China YIREN SONG, National University of Singapore, China WENTAO ZHANG, Peking University, China CHUANG ZHANG, Beijing University of Posts and Telecommunications, China JIAMING LIU, TiamatAI, China 5 2 0 2 7 2 ] . [ 1 1 9 8 5 1 . 1 0 5 2 : r Fig. 1. Outfitted model and garment images generated by Any2AnyTryon. This work was completed during the authors internship at Tiamat AI. Corresponding Author Project Leader Authors Contact Information: Hailong Guo, guohailong@bupt.edu.cn, Beijing University of Posts and Telecommunications, Beijing, China; Bohan Zeng, Peking University, Beijing, China; Yiren Song, National University of Singapore, China; Wentao Zhang, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed Peking University, Beijing, China; Chuang Zhang, Beijing University of Posts and Telecommunications, Beijing, China, zhangchuang@bupt.edu.cn; Jiaming Liu, TiamatAI, Beijing, China, jmliu1217@gmail.com. for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or Preprint, 2025, Any2AnyTryon report. 2 Guo et al. Image-based virtual try-on (VTON) aims to generate virtual try-on result by transferring an input garment onto target persons image. However, the scarcity of paired garment-model data makes it challenging for existing methods to achieve high generalization and quality in VTON. Also, it limits the ability to generate mask-free try-ons. To tackle the data scarcity problem, approaches such as Stable Garment and MMTryon use synthetic data strategy, effectively increasing the amount of paired data on the model side. However, existing methods are typically limited to performing specific try-on tasks and lack user-friendliness. To enhance the generalization and controllability of VTON generation, we propose Any2AnyTryon, which can generate try-on results based on different textual instructions and model garment images to meet various needs, eliminating the reliance on masks, poses, or other conditions. Specifically, we first construct the virtual try-on dataset LAION-Garment, the largest known open-source garment try-on dataset. Then, we introduce adaptive position embedding, which enables the model to generate satisfactory outfitted model images or garment images based on input images of different sizes and categories, significantly enhancing the generalization and controllability of VTON generation. In our experiments, we demonstrate the effectiveness of our Any2AnyTryon and compare it with existing methods. The results show that Any2AnyTryon enables flexible, controllable, and high-quality image-based virtual try-on generation. https://logn2024.github.io/Any2anyTryonProjectPage/ Additional Key Words and Phrases: Virtual try-on, User-friendly image generation ACM Reference Format: Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. 2025. Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Preprint). ACM, New York, NY, USA, 13 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "Introduction",
            "content": "Inspired by the breakthroughs brought by Diffusion Models (DMs), the generation of human-related content has seen rapid development, with Virtual Try-On (VTON) being one of the most highly focused applications. Virtual Try-On aims to generate high-fidelity outfitted model images. Specifically, given model image and garment image, VTON methods are required to \"dress\" the model in the specified garment. This functionality has important applications in areas like online virtual try-ons and game design, making it significant for industries such as multimedia games. Previous methods, CatVTON[8] and MMTryon[55], utilize mask obtained by segment model and text as edit guidance, respectively, to achieve rational virtual try-on results. However, these methods are limited by the availability of high-quality garment, model pairs and struggle in complex generation scenarios, such as with \"in-the-wild\" model images. As result, they fail to generate high-quality and ideal results. To address this, Stable Garment [44] and MMTryon [55] proposed using generative models to replace the model and background in images, thus expanding the dataset. These methods have successfully republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Preprint, 2025, Any2AnyTryon report 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/XXXXXXX.XXXXXXX Preprint, 2025, Any2AnyTryon report. achieved higher-quality and more generalized virtual try-on generation with the enlarged dataset. However, existing try-on methods often can only perform specific try-on tasks and have strict limitations on the input conditions provided by users. Additionally, the garment styles in datasets of existing methods remain insufficiently diverse, making it difficult for existing VTON methods to produce high-quality outfitted model images for garments in complex scenes. To solve these problems, we propose new user-friendly maskfree VTON generation framework, Any2AnyTryon. As illustrated in Table 1, existing VTON methods struggle to support multiple try-on generation tasks. In contrast, Any2AnyTryon can simultaneously fulfill multiple tasks based on user instructions. Firstly, we collect and integrate large and diverse garment-model pair dataset, LAION-Garment, which includes garment and model images and corresponding user textual editing instructions. The outfitted images of the models include both \"in-the-wild\" and \"in-the-shop\" scenarios. Using this extensive dataset, we train model capable of generating desired outfitted model images and garment images based on user editing instructions in both shop and wild settings. Because the dataset contains rich set of training samples, the model can generate desired outputs based on user-provided editing instructions, making it more user-friendly compared to previous methods. The architecture of our model ensures that all conditions are provided in clean latent format within the same representation space as the target latent, as demonstrated in Fig. 1. This design allows the model to handle variety of clothing tasks effectively. Additionally, we develop Adaptive Position Embedding, which adjusts the position embedding based on input textual prompts and image conditions, enabling Any2AnyTryon to generate high-quality outfitted model images and garment images simultaneously. Therefore, our Any2AnyTryon not only satisfies the ability to perform different try-on generation tasks within the same framework but also ensures high-quality generation results. To validate the generative capabilities of our method, we conduct experiments and compare our results with existing state-of-the-art methods. The results show that our approach generates outfitted images with better quality, finer details, and more realistic appearance. In summary, the main contributions of our Any2AnyTryon are as follows: We propose Any2AnyTryon based on DiT, modeling virtual try-on as conditional generation task across multiple images. This unified framework integrates three tasks: virtual try-on, model and garment generation. We design new model architecture, where all conditions share the same representation space with the output images so that our method can achieve high-fidelity VTON generation in diverse scenarios. Additionally, we introduce Adaptive Position Embedding, which adjusts the position embeddings based on input textual and image conditions. This enhancement allows Any2AnyTryon to seamlessly generate highquality results for various VTON tasks using flexible and non-fixed conditional inputs. We collected large-scale garment-model dataset, LAIONGarment, providing sufficient data for training VTON models Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks 3 Table 1. Comparison of VTON functionalities achieved by our Any2AnyTryon and previous methods. Method Virtual tryon Model-free virtual tryon Garment reconstruction Try-on in layers DiOr [9] TryOffDiff [42] StableGarment [44] Magic Clothing [5] GP-VTON [47] CatVTON [8] Any2AnyTryon (Ours) (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) to achieve high-quality outfitted model images. Extensive experiments and evaluations demonstrate that Any2AnyTryon outperforms other baseline methods in performance."
        },
        {
            "title": "2 Related Work\n2.1 Diffusion Transformer",
            "content": "Recent advancements, particularly the introduction of latent diffusion models [27, 28, 35, 36], have significantly improved both the quality and efficiency of generative tasks [24, 25, 37, 51, 52, 57]. To further elevate generative capabilities, large-scale transformer architectures have been incorporated into these frameworks, resulting in cutting-edge models such as DiT [31]. Building on these foundational innovations, FLUX.1[22] is powerful flow-based[26] and DiT-based[31] model. FLUX.1 is critically acclaimed for its excellent prompt understanding, text rendering, and natural image generation. Prompt understanding capability can be attributed to the T5 text encoder [33] which is powerful LLM. Many plugins such as FLUX.1-Fill inpainting and Flux.1-Redux variation model are developed based on the FLUX.1. IC-LoRA[18] is notable technique to achieve personalization. OminiControl[41] design unified adapter for DiT to enable subject-driven and spatially-aligned conditional generation."
        },
        {
            "title": "2.2 Reference-based Image Generation.",
            "content": "Reference-based Image Generation refers to generating customized outputs using images as conditions. Image Prompt Adaption methods [39, 43, 49, 50, 56, 58, 59] leverage adapter structures and ControlNet [53] to achieve generation consistency in appearance, character ID, and style, respectively. ReferenceNet [17] is commonly employed in customization tasks, such as virtual try-on [20, 48], makeup transfer, and hairstyle transfer. It extracts features from reference images using U-Net and injects them into the self-attention layers of the denoising network to ensure consistency in customization generation. However, the aforementioned methods are limited to specific conditions and single-task scenarios. Generating images based on complex textual prompts and variable number of input images remains significant challenge in image generation. Our Any2AnyTryon framework addresses this by utilizing intricate textual instructions and diverse model and garment images as input conditions, enabling high-quality and user-friendly Virtual Try-On (VTON) generation."
        },
        {
            "title": "2.3 Virtual Try-On.",
            "content": "The virtual try-on task has received increasing interest since the release of the VITON-HD [6] dataset. With the development of generation methods, many works have achieved impressive generation results on the virtual try-on task. Earlier Tryon works [14] follow the warping and aggregation pipeline, where warping relies on Thin Plate Spline (TPS) [4] or flow-based approaches [47] and aggregation is commonly based on generative models such as GAN, diffusion, etc. Try-on has gained great improvement[8, 55] based on powerful text-to-image model. However, most methods still suffer from complex data pre-process due to the reliance on garment masks, model poses, and parsing results. Our Any2AnyTryon framework eliminates the dependence on masks, poses, or any other such conditions. Instead, it requires only user-provided textual instructions along with input model and garment images to generate the desired results, significantly enhancing convenience and usability."
        },
        {
            "title": "3 Method\n3.1 Preliminaries",
            "content": "Virtual Try-On Methods. Virtual Try-on methods can be broadly categorized into two types: Masked Try-On and Mask-free Try-On. Masked Try-On methods use segmentation masks to isolate regions of interest, such as clothing and the human body, allowing precise garment replacement or modification. The model 𝐺 generates the outfitted image ˆ𝐼 by applying the mask 𝑀 to the input model image 𝐼 and garment image 𝐼𝑔𝑎𝑟 , formulated as: ˆ𝐼 = 𝐺 (𝐼, 𝐼𝑔𝑎𝑟 , 𝑀) (1) This way ensures fine-grained control over the garments fit, as the mask restricts the clothing to the body region. On the other hand, Mask-free Try-On methods generate the try-on image ˆ𝐼 based on the model image 𝐼 and garment image 𝐼𝑔𝑎𝑟 , without relying on segmentation masks. This simplifies the process but often struggles with generalization and image quality. To overcome these limitations, we propose the mask-free VTON generation method Any2AnyTryon, which enhances both generalization and VTON quality. Flow Matching. Flow matching [26] aligns the flow of information between noise and data distributions. It optimizes velocity field that transforms noise into data over time, ensuring the generative model learns to map from the noise distribution to the true data distribution in structured manner. The flow matching loss is Preprint, 2025, Any2AnyTryon report. 4 Guo et al. formulated as follows: Fig. 2. Overview of Any2AnyTryon. (cid:2)𝑣Θ (𝑧, 𝑡) 𝑢𝑡 (𝑧𝜖) 2(cid:3) 𝐿𝐶𝐹 𝑀 = 𝐸𝑡,𝑝𝑡 (𝑧 𝜖 ),𝑝 (𝜖 ) (2) Where 𝑣Θ (𝑧, 𝑡) represents the velocity field parameterized by the neural networks weights, 𝑢𝑡 (𝑧𝜖) is the conditional vector field generated by the model to map the probabilistic path between the noise and true data distributions, and 𝐸 denotes the expectation, involving integration or summation over time 𝑡, conditional 𝑧, and noise 𝜖. This expectation calculates the average of the squared differences across all conditions, ensuring that the models performance is averaged over many instances to provide reliable measure of its generative capability. FLUX.1. FLUX.1[22] is powerful text-to-image model based on Flow Matching[26] and DiT[10]. It introduces rotary positional embeddings (RoPE)[40] and parallel attention layers[10] to enhance performance and efficiency. FLUX.1 implements three-dimensional RoPE scheme that supports three coordinates, but only the second and third dimensions are used for encoding spatial positions in the latent space: 𝜔𝑚 = 1 𝜃 2𝑚/𝑑 , 𝑚 [0, 𝑑/2) (3) where 𝜃 is typically set to 10000. The position encoding applies rotation matrix: (cid:20)cos(𝜔𝑚 pos) sin(𝜔𝑚 pos) cos(𝜔𝑚 pos) sin(𝜔𝑚 pos) (cid:21) (4) This rotation is applied to query and key vectors in the attention mechanism, enabling the model to capture relative positional relationships in the latent space. Our Any2AnyTryon adjusts position Preprint, 2025, Any2AnyTryon report. embeddings to ensure more accurate garment fitting during the virtual try-on process."
        },
        {
            "title": "3.2 Overview of Any2anyTryon Framework",
            "content": "In this work, we propose new mask-free VTON generation method Any2AnyTryon, as demonstrated in Fig. 2(a). Firstly, we collect and curate new VTON dataset called LAION-Garment, which contains high-quality data and corresponding annotations for various VTON tasks. Meanwhile, we design new VTON generation model that can support inputs of outfitted models and garment images with variable numbers and resolutions, and generate outputs based on user instructions, achieving user-friendly, controllable, and highquality VTON generation. Next, we will detail how we collect the dataset in Section. 3.3, and develop the Any2AnyTryon generation model in Section. 3.4."
        },
        {
            "title": "3.3 LAION-Garment Dataset Collection",
            "content": "We use four public datasets including VITON-HD [6], DressCode [30], DeepFashion2 [12], and LRVS-Fashion [23]. Most data are derived from the aforementioned datasets with the aid of auxiliary tools. Additionally, we manually crawl extra image pairs from the internet and filter the data to maintain quality. To enhance Any2AnyTryons generative capabilities across broader range of VTON tasks, we further expand our LAION-Garment Dataset by integrating existing VTON datasets, as illustrated in Fig. 2(b). First, to enrich the diversity of high-quality data, we augment the outfitted modelgarment image pairs by incorporating images from both existing datasets and those crawled from the internet. Specifically, we utilize the Automasker from CatVTON Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks 5 [8] and segment model [13, 34] to generate masks 𝑀 for the upper and lower garments in the outfitted model images and employed FLUX-Controlnet-Inpainting [2] to perform image inpainting on these masked images. This approach enables us to create additional data triples, each consisting of an outfitted model image 𝐼 , garment image 𝐼𝑔𝑎𝑟 , and the corresponding generated input model image 𝐼inpainted. The image inpainting process can be represented as: (5) 𝐼inpainted = Inpainting(𝐼, 𝑀) To ensure the authenticity and quality of the generated images, we leverage GPT-4o [1] to select triples from the obtained data that exhibit strong authenticity in the generated model images, high quality, and strong consistency with the posture of the outfitted model images. These selected triples are then added to our LAION-Garment Dataset, thereby increasing the datasets volume while maintaining its overall quality. Furthermore, to ensure that our LAION-Garment Dataset can encompass and support wider range of VTON tasks, we utilize the Florence2 [46] to generate user instructions for different tasks. Specifically, we reorganize and reclassify our curated LAION-Garment Dataset according to various tasks, then input the image data and description templates for each task into the Florence2 [46]. This enables the generation of user instructions 𝑇 tailored for training the Any2AnyTryon model. For better understanding, we provide an example for the Virtual Try-On task: the instruction template is The set of three images display model, garment, and the model wearing the garment. <IMAGE1> shows person wearing the garment. <IMAGE2> depicts the garment. <IMAGE3> illustrates <IMAGE1> with <IMAGE2> worn by the model."
        },
        {
            "title": "3.4 Any2AnyTryon Model Design",
            "content": "To enable our Any2AnyTryon to handle different VTON tasks within unified module effectively, we concatenate image conditions and target noisy images along the pixel dimensions, as shown in Fig. 2(c). This approach allows us to input model and garment images of varying quantities and resolutions, which together with user textual instructions, serve as conditions to guide VTON generation. Additionally, to ensure the high quality of the generated results, we train LoRA [16] based on the FLUX.1 model [22]. We concatenate the denoised images and condition images on the pixel dimensions, which is equivalent to concatenating noised tokens and condition tokens. Velocity 𝑣 predicted by 𝑣Θ can be rewritten as: 𝑣 = 𝑣Θ (concat[𝑋 ; 𝐶𝐼 ; 𝐶𝑇 ], 𝑡), (6) where concat[𝑋 ; 𝐶𝐼 ; 𝐶𝑇 ] denotes the concatenation of noised latent, tokens of image conditions 𝐼 and tokens of user instruction 𝑇 . Due to the concatenation of image conditions with variable quantities and resolutions with the output image at the pixel level in Any2AnyTryon, it becomes imperative to locate the position of each image condition within the input image. For virtual try-on tasks, it is necessary to align the non-edited regions of the input image conditions with the generated outfitted model images at the pixel level, thereby ensuring consistency of the non-edited regions. To this end, we propose Adaptive Position Embedding. Specifically, our approach utilizes three-channel position encoding 𝐸𝑝𝑜𝑠 R𝐻 𝑊 3, where the first channel serves as binary mask to separate different regions, while the second and third channels encode spatial positions along the height and width dimensions, respectively. The following formula shows our adaptive position embedding strategy: 𝑅𝑑 Θ,𝑝 (𝑞) = 𝑞1 𝑞2 𝑞3 𝑞4 ... 𝑞𝑑 1 𝑞𝑑 cos(𝑝𝜃1) cos(𝑝𝜃1) cos(𝑝𝜃2) cos(𝑝𝜃2) ... cos(𝑝𝜃𝑑/2) cos(𝑝𝜃𝑑/2) + 𝑞2 𝑞1 𝑞4 𝑞3 ... 𝑞𝑑 𝑞𝑑 1 sin(𝑝𝜃1) sin(𝑝𝜃1) sin(𝑝𝜃2) sin(𝑝𝜃2) ... sin(𝑝𝜃𝑑/2) sin(𝑝𝜃𝑑/2) (7) 𝑃𝑜𝑠 (𝑋, 𝐶𝐼 ) = 𝑃𝑜𝑠𝑖𝑡𝑖𝑜𝑛( (cid:2)𝐶𝐼 𝑋 (cid:3)) (𝑦1, 𝑥2) (𝑦2, 𝑥2) . . . (𝑦𝑚, 𝑥2) = (𝑦1, 𝑥1) (𝑦2, 𝑥1) . . . (𝑦𝑚, 𝑥1) (cid:40)𝑥𝑘, 𝑥𝑘+𝑟 , 𝑥 𝑘 = if pixel-aligned otherwise . . . . . . . . . . . . (𝑦1, 𝑥𝑟 ) (𝑦2, 𝑥𝑟 ) . . . (𝑦𝑚, 𝑥𝑟 ) (𝑦1, 𝑥 1) (𝑦2, 𝑥 1) . . . (𝑦𝑚, 𝑥 1) . . . . . . . . . . . . (𝑦1, 𝑥 𝑠 ) (𝑦2, 𝑥 𝑠 ) . . . (𝑦𝑚, 𝑥 𝑠 ) (8) 𝑃𝑜𝑠 (𝑞) [𝑤] = (cid:40)𝑖, 0, if is image condition 𝑖 otherwise 𝑃𝑜𝑠 (𝑞) [𝑦; 𝑥] = 𝑃𝑜𝑠 (𝑋, 𝐶𝐼 ) 𝐸𝑝𝑜𝑠 (𝑞) = concat[𝑅(𝑃𝑜𝑠 (𝑞) [𝑤]); 𝑅(𝑃𝑜𝑠 (𝑞) [𝑦]); 𝑅(𝑃𝑜𝑠 (𝑞) [𝑥])] (9) where 𝑤, 𝑦, 𝑥 represents three parts of the input vector corresponding to different position encodings: image condition ID, height, and width, the most common input vector is 𝑞 and 𝑘 in the attention mechanism, 𝑟 and 𝑠 represents number of reference and target tokens respectively. This module enables precise spatial alignment between input image conditions and generated images. Additionally, we introduce clean condition latents as input instead of noisy latents used in IC-LoRA [18], as we found that background regions tend to change undesirably with noisy conditions. By maintaining clean latents in masked regions specified by split masks, our approach achieves superior preservation of unchanged areas while enabling precise control over target modifications. After introducing the input form of conditions in Any2AnyTryon and incorporating Adaptive Position Embedding, the conditional flow matching loss can be represented as: 𝐿𝐶𝐹 𝑀 = 𝐸𝑡,𝑝𝑡 (𝑧 𝜖 ),𝑝 (𝜖 ) 2(cid:105) (cid:104)(cid:13) (cid:13)𝑣Θ (concat[𝑋 ; 𝐶𝐼 ; 𝐶𝑇 ], 𝐸𝑝𝑜𝑠, 𝑡) 𝑢𝑡 (𝑧𝜖)(cid:13) (cid:13) (10)"
        },
        {
            "title": "Implementaion Details",
            "content": "Our method is based on the dev version of FLUX.1 model[22]. The garment reconstruction model, model-free virtual try-on model and try-on model for evaluation are trained with height and width set as 512x384, 768x576 and 512x384, respectively. All the figures presented in the paper are generated from the unified model using all the data with all the tasks in variable image size. To train the unified model, we train all the tasks except try-on in layers in the first stage and finetune with the dataset for try-on in layers and subset from the rest of the tasks in the second stage. In all training, we use prodigy optimizer[29] with weight decay set as 0.01. Preprint, 2025, Any2AnyTryon report. 6 Guo et al. Table 2. Quantitative comparison on VITON-HD dataset. Method SSIM [45] MS-SSIM CW-SSIM LPIPS [54] FID [15] CLIP-FID KID [3] DISTS [11] TryOffDiff [42] Ours 0.793 0.805 0.712 0. 0.466 0.453 0.334 0.328 20.346 13.367 8.371 3.872 6.8 3.5 0.226 0. Fig. 5. Demonstration of garment-driven generation ability of our model with different image sizes. Table 4. Quantitative comparison on VITON-HD dataset. We multiply KID by 1000 for better comparison. The best and the second best results are denoted as Bold and underline, respectively. Fig. 3. Qualitative comparison of garment reconstruction. Method Paired LPIPS SSIM FID KID Unpaired FID KID GP-VTON[47] OOTD[48] IDM-VTON[7] CatVTON [8] FitDiT[19] Ours 0.0677 0.1317 0.0815 0.0582 0.1059 0. 0.8722 0.7838 0.8156 0.8653 0.8298 0.8387 8.649 12.131 8.206 5.482 8.362 6.934 3.669 4.335 1.727 0.384 1.543 0.7387 11.708 15.136 10.745 9.083 10.340 8.965 3.990 5.774 2.229 1.130 1.648 0.981 Fig. 4. Demonstration of multi garment flatten ability of our model with different prompt. Table 5. Ablation study on proposed components. Table 3. Quantitative comparison on VITON-HD dataset. Method MP-LPIPS [5] CLIP-I [32] DiffSim [38] FFA [21] Magic Clothing [5] StableGarment [44] Ours 0.192 0.149 0.141 0.642 0.650 0.789 0.143 0.153 0.202 0.459 0.547 0.549 For the LAION-Garment dataset, we include several existing VTON datasets for the try-on training task, which consist of the following: VITON-HD [6] with 11,491 image pairs; DressCode [30] with 13,563 image pairs of Upper body, 27,678 image pairs of Dresses, and 8,689 image pairs of Lower body; DeepFashion2 [12] with 988 image pairs. After data augmentation and user instructions generation, the dataset expands to over 60,000 data triples. The validation metrics are detailed in the supplementary material. Preprint, 2025, Any2AnyTryon report. Setting LPIPS SSIM CLIP-FID KID Ours(w/o clean latent) Ours(w/o adaptive position) Ours(full) 0.3141 0.3080 0.2590 0.6892 0.7088 0. 9.648 9.434 9.293 6.403 5.658 5."
        },
        {
            "title": "4.2 Garment Reconstruction",
            "content": "Garment reconstruction refers to reconstructing flattened garments from images of the model wearing the target garment. We support multi-garment reconstruction from our model with different prompts as shown in Fig. 4. To validate the effectiveness of our methods, we compare our results with TryOffDiff[42]. As shown in Table. 2, Compared to existing methods, the garments reconstructed by our approach not only have higher quality but also match the appearance of the garment worn by the model in the input image more accurately. To more intuitively demonstrate the effectiveness Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks 7 Fig. 6. Qualitative comparison of virtual try-on on the VITONHD dataset. to preserve the original garment appearance and generate highquality outfitted model images. Additionally, our Any2AnyTryon can achieve high-quality VTON generation at various image resolutions, demonstrated in Fig. 5."
        },
        {
            "title": "4.4 Virtual Try-on",
            "content": "We compare our mask-free try-on results with GP-VTON[47], OOTD[48], IDM-VTON[7], CatVTON[8] and FitDiT[19]. As shown in Table 4, overall, our method outperforms existing state-of-the-art (SOTA) virtual try-on generation methods, particularly in terms of the FID [15] and KID [3] metrics, which measure generation quality. Additionally, for virtual try-on generation, the visualization of outfitted model images is crucial. In Fig. 6, we present the generation results of different methods. It is evident that our Any2AnyTryon not only produces higher-quality and more realistic outfitted model images but also ensures that the garments worn by the model align closely with the input garments. This demonstrates the superior generation capability of our Any2AnyTryon in virtual try-on tasks."
        },
        {
            "title": "4.5 Try-on in layers",
            "content": "We compare with DiOr[9] which proposes similar try-on-in-layer tasks, also called dressing in order there. The try-on-in-layer task is very challenging, especially for mask-free try-on, as the method must identify specific editing locations only with text guidance. To showcase the generation capability of our Any2AnyTryon, we performed qualitative comparison using examples from DiOrs original paper. As depicted in Fig. 7, our method better preserves the appearance of the input model, while also aligning the garment worn by the outfitted model with the input garment more accurately. Preprint, 2025, Any2AnyTryon report. Fig. 7. Demonstration of try-on in layer. of our Any2AnyTryon garment reconstruction, as shown in Fig. 3, it is evident that the realism and accuracy of garment reconstruction achieved by our method are far superior to those of TryOffDiff, further validating the effectiveness of our approach."
        },
        {
            "title": "4.3 Model-free Virtual Try-on",
            "content": "Model-free Virtual Try-on refers to garment-driven model generation which can be regarded as specific sub task of subject-driven generation. We compared with Magic Clothing [5] and StableGarment [44]. In Table 3, we compare our Any2AnyTryon and baseline methods on the Model-free VTON generation task using the VITONHD dataset. We employed five metrics, including DiffSim [38] and FFA [21], for quantitative comparison. The results indicate that our Any2AnyTryon significantly outperforms baseline methods 8 Guo et al."
        },
        {
            "title": "4.6 Ablation Study",
            "content": "To further demonstrate the effectiveness of the image condition addition strategy and Adaptive Position Embedding in our Any2AnyTryon, we conduct an ablation study. In Table 5, we compare two different methods. One method differs from the image condition addition in Any2AnyTryon by adding noise to the image condition and generation target as input to the model. The second method is without the adaptive position embedding. The results show that, compared to the full Any2AnyTryon method, both comparison methods negatively impact the generation results, thereby confirming the validity of the Any2AnyTryon design."
        },
        {
            "title": "5 Conclusion",
            "content": "The proposed Any2anyTryon framework demonstrates significant advancements in virtual try-on tasks by introducing unified, maskfree solution capable of handling diverse scenarios. By leveraging the LAION-Garment dataset and innovative techniques such as adaptive position embedding and clean condition latents, the method enhances garment reconstruction, model-free virtual try-on, and layered try-on tasks. Extensive experiments validate its effectiveness, showcasing superior performance in generating high-fidelity, realistic outfitted images compared to state-of-the-art methods. The frameworks flexibility, scalability, and ability to generalize across complex conditions mark significant step forward in virtual try-on research and its practical applications."
        },
        {
            "title": "Acknowledgments",
            "content": "To Robert, for the bagels and explaining CMYK and color spaces."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Alibaba. 2024. FLUX-Controlnet-Inpainting. https://github.com/alimamacreative/FLUX-Controlnet-Inpainting [3] Mikołaj Bińkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying mmd gans. arXiv preprint arXiv:1801.01401 (2018). [4] Fred L. Bookstein. 1989. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Transactions on pattern analysis and machine intelligence 11, 6 (1989), 567585. [5] Weifeng Chen, Tao Gu, Yuhao Xu, and Arlene Chen. 2024. Magic clothing: Controllable garment-driven image synthesis. In Proceedings of the 32nd ACM International Conference on Multimedia. 69396948. [6] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. 2021. Vitonhd: High-resolution virtual try-on via misalignment-aware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1413114140. [7] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. 2024. Improving Diffusion Models for Authentic Virtual Try-on in the Wild. arXiv preprint arXiv:2403.05139 (2024). [8] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, and Xiaodan Liang. 2024. CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models. arXiv preprint arXiv:2407.15886 (2024). [9] Aiyu Cui, Daniel McKee, and Svetlana Lazebnik. 2021. Dressing in order: Recurrent person image generation for pose transfer, virtual try-on and outfit editing. In Proceedings of the IEEE/CVF international conference on computer vision. 1463814647. [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. 2023. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning. PMLR, 74807512. Preprint, 2025, Any2AnyTryon report. [11] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. 2020. Image quality assessment: Unifying structure and texture similarity. IEEE transactions on pattern analysis and machine intelligence 44, 5 (2020), 25672581. [12] Yuying Ge, Ruimao Zhang, Lingyun Wu, Xiaogang Wang, Xiaoou Tang, and Ping Luo. 2019. Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images. CVPR (2019). [13] Rıza Alp Güler, Natalia Neverova, and Iasonas Kokkinos. 2018. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition. 72977306. [14] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry Davis. 2018. Viton: An image-based virtual try-on network. In Proceedings of the IEEE conference on computer vision and pattern recognition. 75437552. [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems 30 (2017). [16] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [17] Li Hu. 2024. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 81538163. [18] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. 2024. In-Context LoRA for Diffusion Transformers. arXiv preprint arxiv:2410.23775 (2024). [19] Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, Chengming Xu, Jinlong Peng, Jiangning Zhang, Chengjie Wang, Yunsheng Wu, and Yanwei Fu. 2024. FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Tryon. arXiv preprint arXiv:2411.10499 (2024). [20] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. 2024. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 81768185. [21] Klemen Kotar, Stephen Tian, Hong-Xing Yu, Dan Yamins, and Jiajun Wu. 2023. Are these the same apple? comparing images based on object intrinsics. Advances in Neural Information Processing Systems 36 (2023), 4085340871. [22] Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux [23] Simon Lepage, Jérémie Mary, and David Picard. 2023. LRVS-Fashion: Extending Visual Search with Referring Instructions. arXiv:2306.02928 (2023). [24] Hong Li, Yutang Feng, Song Xue, Xuhui Liu, Bohan Zeng, Shanglin Li, Boyu Liu, Jianzhuang Liu, Shumin Han, and Baochang Zhang. 2024. UV-IDM: Identity-Conditioned Latent Diffusion Model for Face UV-Texture Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1058510595. [25] Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao Hu, Jianzhuang Liu, et al. 2024. Zone: Zero-shot instructionguided local editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 62546263. [26] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 (2022). [27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems 35 (2022), 57755787. [28] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. 2023. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378 (2023). [29] Konstantin Mishchenko and Aaron Defazio. 2023. Prodigy: An expeditiously adaptive parameter-free learner. arXiv preprint arXiv:2306.06101 (2023). [30] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. 2022. Dress code: High-resolution multi-category virtual tryon. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 22312235. [31] William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 167. Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks 9 [34] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. 2024. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024). [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [36] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020). [37] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. 2024. ProcessPainter: Learn Painting Process from Sequence Data. arXiv preprint arXiv:2406.06062 (2024). [38] Yiren Song, Xiaokang Liu, and Mike Zheng Shou. 2024. DiffSim: Taming Diffusion Models for Evaluating Visual Similarity. arXiv:2412.14580 [cs.CV] https://arxiv. org/abs/2412.14580 [39] Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, and Mike Zheng Shou. 2024. Anti-Reference: Universal and Immediate Defense Against Reference-Based Generation. arXiv preprint arXiv:2412.05980 (2024). [40] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. [41] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. 2024. OminiControl: Minimal and Universal Control for Diffusion Transformer. arXiv preprint arXiv:2411.15098 (2024). [42] Riza Velioglu, Petra Bevandic, Robin Chan, and Barbara Hammer. 2024. TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models. arXiv preprint arXiv:2411.18350 (2024). [43] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. 2024. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519 (2024). [44] Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, and Peipei Li. 2024. StableGarment: Garment-Centric Generation via Stable Diffusion. arXiv preprint arXiv:2403.10783 (2024). [45] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600612. [46] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. 2024. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 48184829. [47] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. 2023. Gp-vton: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 23550 23559. [48] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. 2024. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arXiv preprint arXiv:2403.01779 (2024). [49] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023). [50] Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianzhuang Liu, and Baochang Zhang. 2023. Ipdreamer: Appearance-controllable 3d object generation with image prompts. arXiv preprint arXiv:2310.05375 (2023). [51] Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu, and Baochang Zhang. 2024. Controllable mind visual diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 69356943. [52] Bohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, and Baochang Zhang. 2023. Face animation with an attribute-guided diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 628637. [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. [54] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586595. [55] Xujie Zhang, Ente Lin, Xiu Li, Yuxuan Luo, Michael Kampffmeyer, Xin Dong, and Xiaodan Liang. 2024. MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion Generation. arXiv preprint arXiv:2405.00448 (2024). [56] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. 2024. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 80698078. [57] Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. 2024. Fast Personalized Text to Image Synthesis with Attention Injection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 61956199. [58] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. 2024. Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model. arXiv preprint arXiv:2403.07764 (2024). [59] Yuxuan Zhang, Qing Zhang, Yiren Song, and Jiaming Liu. 2024. Stable-hair: Real-world hair transfer via diffusion model. arXiv preprint arXiv:2407.14078 (2024). Preprint, 2025, Any2AnyTryon report. 10 Guo et al. Fig. 8. Additional qualitative comparison of garment reconstruction. Fig. 9. Additional garment reconstruction results in the wild. In this Supplementary Material, we provide the details of evaluation metrics in Section. A, and in Section. B, we provide more visualization of Any2AnyTryon generation results. Validation Metrics A.1 Garment Reconstruction We follow the existing SOTA garment reconstruction method TryOffDiff [42] and leverage the full-reference metrics SSIM [45], MSSSIM, and CW-SSIM to validate the alignment between the reconstructed garment and the ground truth, while utilizing the metrics LPIPS [54], FID [15], CLIP-FID, KID [3], and the Deep Image Structure and Texture Similarity (DISTS) [11] to evaluate the quality and fidelity of the generated images. A.2 Virtual Try-on Genetation Our Any2AnyTryon supports both model-free VTON and VTON generation. In the experimental section, we conduct quantitative comparison of both tasks. For model-free VTON generation, we follow MagiClothing [5] and use MP-LPIPS and CLIP-I to measure the consistency of the garments with the generated outfitted model results. To make our quantitative comparison more compelling, we introduce more recent benchmarks, DiffSim [38] and FFA [21], to further enhance the validity of the evaluation. Preprint, 2025, Any2AnyTryon report. For VTON generation, for paired datasets with ground truth, we use LPIPS [54], SSIM [45], FID [15], and KID [3] to evaluate the quality and faithfulness of the VTON generation. For unpaired datasets without ground truth, we use FID [15] and KID [3] to validate the generation quality. More VTON Generation Results B.1 Model-free Virtual Try-on In the main paper, we presented quantitative comparison between Any2AnyTryon and other baseline methods for Model-free Virtual Try-on. To further showcase the high-quality generation results of Any2AnyTryon on the Model-free Virtual Try-on task, we provide additional visualizations of the generated outfitted model images in Fig. 11. To highlight the generated results, we use the user instruction \"model in the shop.\" The results demonstrate that, whether its for the upper garment, lower garment, or overall outfit change, our method can consistently achieve high-fidelity VTON generation. B.2 Virtual Try-on In Fig. 12, we display more generation results of Any2AnyTryon in the Virtual Try-on task. We provide six different models and six garments with distinct styles, and our Any2AnyTryon produces 36 different, rational, high-quality generated outfitted results. This Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks 11 Fig. 10. Additional garment reconstruction results in the shop. Fig. 11. Additional model-free virtual try-on results in the shop. Preprint, 2025, Any2AnyTryon report. 12 Guo et al. Fig. 12. Additional virtual try-on results in the shop. Preprint, 2025, Any2AnyTryon report. Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks 13 Fig. 13. Additional VTON in layers results in the wild. proves that our Any2AnyTryon can stably realize impressive VTON generation. B.3 Garment Reconstruction To further evaluate the quality of garment reconstruction, we provide additional qualitative comparison results in Fig. 8. The garments reconstructed using our method in Any2AnyTryon preserve the details of the input models garments better than those reconstructed by TryOffDiff, besides, we provide more garment generation in Fig. 10. Additionally, to further demonstrate the stability and generalization ability of Any2AnyTryon in garment reconstruction, we display results of garment reconstruction for in-the-wild model images in Fig. 9. The results show that our method can still generate impressive garment results even for more challenging inputs. B.4 Virtual Try-on in Layers For the more challenging task of Virtual Try-on in layers, we present Virtual Try-on in layers generation results for in-the-wild model images in Fig. 13. The results demonstrate that, even in complex scenarios such as models in the wild, our Any2AnyTryon can still produce rational, high-quality outfitted model images, proving the effectiveness of our method. Preprint, 2025, Any2AnyTryon report."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications, China",
        "National University of Singapore, China",
        "Peking University, China",
        "TiamatAI, China"
    ]
}