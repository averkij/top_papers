{
    "paper_title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers",
    "authors": [
        "Daniel D'souza",
        "Julia Kreutzer",
        "Adrien Morisot",
        "Ahmet Üstün",
        "Sara Hooker"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: \"Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?\" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations."
        },
        {
            "title": "Start",
            "content": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers Daniel Dsouza1, Julia Kreutzer1, Adrien Morisot2, Ahmet Üstün1, and Sara Hooker1 1Cohere Labs, 2Cohere Corresponding authors: {danieldsouza, ahmet, sarahooker}@cohere.com Abstract One of the most profound challenges of modern machine learning is performing well on the longtail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on fixed system prompt for maintaining performance. In this work, we ask: Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time? We revisit the divide between training and inference techniques to improve long-tail performance while providing users with set of control levers the model is trained to be responsive to. We create detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations. 5 2 0 J 7 1 ] . [ 1 2 0 7 4 1 . 6 0 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) are expected to perform well on many different tasks. Therefore, training data is heterogeneous mix, where instances can vary greatly in terms of format, contents, tasks, and languages, e.g. code generation [Lozhkov et al., 2024; Manh et al., 2023; Kocetkov et al., 2022; Zhong et al., 2017] vs. MCQA [Singh et al., 2024; Pal et al., 2022]. At inference time, First author. Principal senior advisors. Released as preprint on June 18, 2025 1 Figure 1: Tapping into Distributions: (above) illustrates the representation of various length buckets in the training distribution. (below) demonstrates the flexibility of the marker intervention on the mArena Hard test distribution. By modifying the <length_bucket>..</length_bucket> marker, the model can effectively tap into diverse training distributions, even for underrepresented length buckets. data points are not equally relevant, but it is often prohibitively expensive to go back and change the training distribution for each individual inference request. Hence, there is mismatch in the distribution at training and inference time: training time distribution is often determined by ease of access to prior data collections and prior data augmentation efforts, while at inference time, new use cases might be underrepresented in the data but highly relevant to the user. To overcome this mismatch, techniques have been proposed to improve the conditioning of the output generation at inference. These involve prompt engineering [Wu & Hu, 2023; Yu et al., 2023; Wenjuan et al., 2024], multi-shot examples [Brown et al., 2020; Lin et al., 2022; Winata et al., 2022; Logan IV et al., 2022], chain-of-thought [Wei et al., 2022; Wang et al., 2023; Ranaldi & Freitas, 2024] or decoding strategies [Shi et al., 2024; Snell et al., 2025]. However, these approaches place an enormous burden on practitioners and developers to anticipate what strategies deliver the best performance. Furthermore, the effectiveness is dependent on the exact configuration for particular model, e.g. the order of multi-shot examples plays role [Lu et al., 2022], and the wording of the prompt [Anagnostidis & Bulian, 2024]. In this work, we ask Can we optimize our training protocols to both improve controllability by the user and improve performance on rare use cases at inference time? Our approach amounts to building treasure map of hyper-detailed task-specific markers, to allow for real-time automatic targeting of long-tail features during inference. We note that some of the earliest generative models have used tags to improve performance. However, these often targeted single feature at time or were applied uniformly to an entire dataset. These early tags fell out of 2 favor over the last few years, with the focus turning to prompt engineering for users to guide the generation themselves. However, there have been few wider ecosystem changes which prompt (no pun intended) revisiting the paradigm of adding markers to training, and also motivate this work: 1) LLMs are now used by far wider group of everyday users who cant be expected to be familiar with the complexities of prompt engineering, 2) Many models are now served using an API which means training markers can be added automatically behind the API call (not visible to users), and hence can be far more complex and varied to guide and improve generations. Our work is motivated by these two trends. We take far wider view of training markers and explore setting where single data point can have up to 90 complex characteristics. We describe these as Treasure Markers, introduced at training time to provide map to guide towards higher performance at inference time. We motivate that this approach is particularly beneficial for long-tail modeling. Our goal is that the treasure map approach is robust at test-time, so we also aggressively experiment with marker dropout during training. This is akin to asking the model to still find the treasure even with missing clues. In this work, our primary contributions are as follows: 1. Introducing more general framework for controllability: We show that explicitly targeting controllability during training leads to pronounced gains at inference time, with little burden placed on the user. Training markers leads to significant downstream gains, ranging from win rates increase on open-ended generations of 5.7% on ArenaHard [Li et al., 2024] across the entire distribution of tasks relative to model with no tags. Our training marker framework offers remarkable flexibility, allowing for control over both aspects of form (output format, length) and semantic qualities (quality, tone, style) while also being completely optional at inference, because the markers can be inferred accurately. 2. Long-tail lifts: Training with explicit markers is an effective method for leveraging longtail features at inference time, unlocking high-performance even for the distributions that are underrepresented in the training data. While our framework enables relative improvement of 7.9% on Code tasks over the baseline, we observe relative lifts of up to 14.1% on tasks like CodeRepair, which are highly underrepresented in the training data. 3. Modeling underlying relationships: We demonstrate that our approach effectively models underlying relationships in the data, as evidenced by drastic reduction in length violation (36.58% 0.75%) in length-constraint instruction following, despite never seeing training sample with prompt instruction designating length constraint. While significantly reducing length violation, the training markers also boost the generation quality with 7.5% relative gain (14.36% 21.85%) in win rates."
        },
        {
            "title": "2.1 Overview of Training Time Markers",
            "content": "We condition the output sequence given an instruction with added training markers m: p(yx, m) = (cid:89) i=1 p(yi x, m, y<i). (1) 3 (a) Baseline Model (b) TreasureMarked (c) TreasureMarked (fixed) Figure 2: Modeling data features flexibly with training time markers: Results on the length instruction following on the AlpacaEval-Length-Instructed(LI) dataset. While (a) the baseline violates the length constraint with 36.58%, (b) using the TreasureMarked model and allowing to infer tags on the exact same dataset reduces the violation to 24.7%. (c) Conveying the requirement via an explicitly inserted length marker to the prompts, the TreasureMarked model violates the instruction on only 1.25% of the dataset. These markers encompass several different attributes of the data, including estimated quality scores, domains, and languages (3.1), which we store as list of markers associated with given data point (see Table 1 for an example). This template is treated as natural language and encoded with the same tokenizer as the text. We include the markers in both input (appended to the prompt) and output space (prepended to the completion), to induce the model to associate the properties of the generations with these characteristics. This reduces the burden on the practitioner or researcher at inference time, as the model learns to infer the correct markers. <MARKER_LIST> <domain> Sciences </domain> <language> French</language> /MARKER_LIST Table 1: An example list of training time markers formatted in standardized template. The finetuning objective thus becomes to minimize the negative log likelihood of the target generations including the template, given prompt with an optional input template: 1 D (cid:88) d=1 log pθ(yd, md dropout(md), xd) (2) This approach ensures that the model learns to faithfully generate and adhere to the training markers when provided on the prompt side. Training markers dropout. To avoid the model from becoming overly reliant on markers for completion or learning to trivially replicate the markers, we employ dual dropout strategies (datasetlevel, sample-level) on the prompt space. In dataset-level dropout, we completely remove the training markers from the prompt for random selection (defined as percentage of the dataset). In sample-level dropout, we completely remove random subset of training markers from each example (defined as percentage of all markers associated with given example). To ensure the model consistently produces markers at inference time, we do not introduce dropout on the generation side. 4 Figure 3: Long tail domains benefit more from training markers: (left) Domain distribution in the training data used to fine-tune the model. (right) Improvements in win rates over the baseline against Gemma2-9B on both majority and minority subsets on Arena-Hard-Auto dataset [Li et al., 2024]. We group the test data into two sets of domains that have high (>5%) and low (< 5%) presence in training data."
        },
        {
            "title": "3.1 Taxonomy of Training Markers",
            "content": "We develop comprehensive taxonomy around distinct groups of desired characteristics to capture key attributes of the training data, such as quality of the data, style, format, domain, and task. Table 2 contains the taxonomy with definitions and the set of valid marker values. We chose this selection of markers with inference-time use cases in mind: properties like quality, tone, style, and completion length are very desirable to control at inference time. We also focus on longtail attributes with the goal of specifically targeting performance on underspecified parts of the distribution. To that end, we add hyperdetailed markers for task, domain and code type which tend to have highly skewed frequencies with some instances occurring far more frequently than others. To assign markers to samples in the training dataset, we utilize dataset-related information whenever possible and use an LLM to tag missing meta-information. Specifically, we use the multilingual openweights model Command R+1 for tagging of markers for <domain>, <task>, <format> whenever unavailable from the dataset. To improve tagging performance, we use detailed definitions paired with few-shot examples to provide context for markers during annotation. We add markers across 23 languages, so we use in-language few-shot examples in each language. Our extensive set of 90 unique markers fall into categories such as Length, Style, Format, Quality, Source, Domain, Task. We include an extensive description of all markers in section 3. We describe the most frequently referenced categories below: Length: are markers that allow for control of completion length. It includes level of granularity ranging from <length_tokens> and <length_sentences> to broader categories such as concise, medium, and long. Language: <lang> describes the language the completion is written in (i.e. Arabic, Japanese), enabling the model to improve language-specific generations and reduce language switching during inference. <code_type> is specifically used to identify programming languages for coding-related tasks (i.e. python, c++). Quality: <quality> provides measurable score indicating the quality of sample, often 1Release blog of Command R+ https://cohere.com/blog/command-r-plus-microsoft-azure 5 category <quality> <quality_bucket> <length_tokens> definition the indicating Score quality assigned to sample as annotated by human or RewardModel(RM). <quality> bucketed into quartiles (calculated by language). 1 indicating highest quality and 4 indicating lowest quality # of tokens <length_sent> # of sentences <length_para> # of paragraphs <length_bucket> <length_tokens> bucketed by defined response length ranges <task> Task-related information <domain> Domain-related information <code_type> (coding tasks) Programming languages To model desired generation format <format> <source> <style> values float 1,2,3,4 int int int concise, medium, long OpenEnded, Explanation, Translation, Classification, CreativeWriting, QuestionAnswering, InformationExtraction, Summarization, Rewrite, Reasoning, CodeGeneration, CodeFix, CodeTranslation, CodeExplanation Sciences, Technology, SocialSciences, Culture, Medical, Legal, Unspecified, Conversation, Code, Math python, javascript, cpp, cobol, java, go, rust, swift, csharp, php, typescript, shell, c, kotlin, ruby, haskell, sql MCQAnswer, ChainOfThought, XML, JSON, Enumeration, Tabular, Markdown, Latex To model the source of the training data Human, Translation, Synthetic, Others To model tone and style of the generation Formal, Informal, Custom <language> The language of the completion The list of the languages are given in Section 3.2. Table 2: Comprehensive taxonomy for training time markers: Our taxonomy contains 13 categories shown with their descriptions and values. 6 derived from human annotations or Reward Model (RM). We also create categorical marker <quality_bucket> by using quartiles within language-specific subsets into {1,2,3,4}, offering broader description of quality. Domain: overarching category of the knowledge required to answer given prompt (i.e. Sciences, Technology, Medical). We annotate domain markers either using LLM tagging or derive from the source of the dataset for domains like Math and Code. Task: <task> helps capture more fine-grained differences in task characteristics within summarization, reasoning, openended, explanation). Similar to the domain domain (i.e. marker, we use LLM tagging or the data source information for obtaining task markers."
        },
        {
            "title": "3.2 Experimental Set-up",
            "content": "Training with markers. We use 7-billion parameters proprietary base model which is pretrained using data mixture that consists of texts from 23 languages covering half the worlds population. We train our base model on training corpus containing 2.7M examples made up of our mixture of instruction-style data sources. Training protocol. Training for each variant spanned 8,000 steps, employed cosine learning rate schedule with warm-up phase, using batch size of 32 and an evaluation batch size of 64. We train for 2 epochs with peak learning rate of at 2.5 104, achieved through 10 warm-up steps starting from learning rate of 0.0, and then decay back to 1.25 104. One fine-tuning run using 8,000 steps on 128 Nvidia H100 GPUs takes around 6 hours. Languages covered by the training markers. Our experiments cover 23 languages: Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian and Vietnamese. Inference settings. At inference time, we evaluate performance gains under two different settings. In the default setting, which we refer to as \"TreasureMarked\", we do not fix any of the markers at inference. This setting asks: Has the model learnt to infer the right markers without any intervention? In the second setting which we refer to as \"TreasureMarked (fixed)\", we explicitly hardcode some of the markers at inference. This asks: if we manually set the value of some markers, can we drive gains in performance? This is very reasonable for cases like quality, where we always want to steer model behavior towards higher quality generations. Baseline. We compare both \"TreasureMarked\" and \"TreasureMarked (fixed)\" against model trained on the same data, but without added markers that we refer to as Baseline. This allows for clean comparison, and controls for the same amount of data seen in both variants. Core experimental variants and ablations. In the next section, we evaluate variety of ways model trained with markers shines at inference time. We inspect three axes of control: (1) quality in section 4.1.1, (2) length in section 4.3, and (3) language in section 4.5). Furthermore, we show how long-tail examples benefit from markers, even when only inferred at inference time (section 4.1), specifically in coding tasks (section 4.2) and for long generations (section 4.3). We present key experimental ablations, including understanding the impact of dropout applied to markers on downstream performance at inference time (Section 5.3). 7 3.2.1 Evaluation Open-ended generation quality. We evaluate the impact of markers on both the English ArenaHard-Auto v0.1 [Li et al., 2024], and translated version of this dataset, m-Arena Hard [Dang et al., 2024] used for multilingual evaluation. Arena-Hard-Auto is challenging open-ended generation benchmark with prompts selected from user queries on Chatbot Arena. We measure Win Rate % against our Baseline model using GPT-4o.2 Task-specific evaluations. In addition, we evaluate the models on benchmarks specific to tasks such as code (generation, repair, translation) and length conditioned instruction following to narrow in on long-tail effects and controllability levers. We introduce each of these evaluations within the respective results sections. Length evaluations. Given the original instruction in the AlpacaEval-LI dataset [Yuan et al., 2024] contains the exact constraint, our TreasureMarked and TreasureMarked (fixed) both contain explicit reference to the contraint. For TreasureMarked, we present the original lengthinstructed prompt, allowing the model to deduce the associated tags. This approach evaluates the in contrast, for TreasureMarked (fixed), models ability to extrapolate tags from instructions. since the original instruction contains the exact constraint, we investigate an additional control strategy where we provide the constraint in the marker template if the taxonomy directly supports it. We remove the length instruction and append the corresponding <length_tokens> tag with the appropriate value. Table 3 provides an example of an edited prompt. This strategy assesses the models adherence to known templates and its ability to follow explicit length requirements that are only provided via the marker template."
        },
        {
            "title": "4.1 Impact of Treasure Markers on Open-Ended Generation",
            "content": "Open-ended performance gains. We measure Win Rates (%) of the Baseline and TreasureMarked models against Gemma2-9B [Team et al., 2024] as common point of comparison, visualized in Figure 3. We first consider our TreasureMarked variant, markers are only included in training but are inferred from the model itself during inference. Overall, we obtain an absolute increase of 5.7% in Win Rates from 32.1% to 37.8% across all tasks. This is reassuring, because it shows that markers at training time of the TreasureMarked model can already make positive change at inference time, even when only inferred by the model itself, and even if the respective markers are rarely seen during training (e.g., for underrepresented domains). Performance on the long-tail. One of our core hypotheses is that treasure markers will be particularly helpful at preserving or unearthing gains on the long-tail. To validate this hypothesis, we evaluate performance post-training on domains represented with different frequencies in the trainingset. As seen in Figure 3, SocialScience, Sciences, Finance, Medical, and Legal domains are particularly sparsely represented in the training data, each making up less than 5% of the training data. In contrast, Code is best represented in the training dataset. With inferred treasure markers, while there is an improvement of +5.7% across the higher-represented domains, we observe an even more pronounced gain of +9.1% in the underrepresented domains. 2We used gpt-4o-2024-05-13 as our judge model. Details: https://platform.openai.com/docs/models/gpt-4o 8 Figure 5: Improvement on the Long Tail for Code tasks: (left) Frequency of coding <task>s in the training dataset. (right) Despite being poorly represented in the training data, CodeRepair achieves 14.1% relative improvement by leveraging targeted markers during inference further improving on the performance from the TreasureMarked model with inferred markers. 4.1.1 Fixed Treasure Markers We also explore adding explicit markers in TreasureMarked (fixed). Here, we specifically target quality and ask Can we control the generation quality of the model as latent feature, using training time markers? To test this, we measure generation quality on m-Arena Hard [Dang et al., 2024] across 23 languages, by only adding markers related to quality. For each value [1,2,3,4] of <quality_bucket>, we also include <quality> score in conjunction with it. To obtain the <quality> score, we pick the 95% percentile calculated language-wise from the samples in the training data from each respective bucket. As evaluation, we measure the generation quality by the same Reward Model used to score the data during training to compute win rates against the Baseline model. Figure 4 demonstrates the amount of control introduced by training time markers with win rates under the RM going from 48.21% 56.5% just by changing <quality>, <quality_bucket> at inference. These results showcase the potential of our framework, where markers representing desired quality metric used during training yields control levers to leverage generations that tap into that quality metric at inference time."
        },
        {
            "title": "4.2 Impact of Treasure Markers on\nTargeted Performance of Specific Sub-tasks",
            "content": "4.2.1 Code Performance For code, we evaluate our model on three tasks from HumanEvalPack [Muennighoff et al., 2023] dataset, and measure pass@1 rates. We use CodeSynthesis, CodeRepair, and CodeTranslation3, covering python, rust, java, javascript, go, c++. These map to the following task markers in our taxonomy: Figure 4: Levers for Controlling Quality: Changing the <quality>, <quality_bucket> markers at inference time provides control over generation quality with Win Rates (as measured by internal Reward Model) going from 48.21% 56.5% over the Baseline model, demonstrating successful control over quality as annotated in the training data. 3The CodeTranslation task is created by an all-to-all mapping between the 6 languages in HumanEvalPack 9 Original AlpcaEval-LI TreasureMarked (fixed) Answer the following instruction using 199 words or less. What are the names of some famous actors that started their careers on Broadway? What are the names of some famous actors that started their careers on Broadway? <MARKER_LIST> <length_tokens>199</length_tokens> </MARKER_LIST> Table 3: Examples of length control strategies: (left) Original instruction from AlpacaEval-LI dataset; (right) Modified instruction with constraint in the marker list. CodeGeneration, CodeFix, and CodeTranslation. During training, code comprises of 27.2% of the overall training corpus. However, we specifically pick this domain because the distribution of coding subtasks differs significantly in frequency in the training corpus, as shown in Figure 5. CodeRepair and CodeTranslation are very rare coding subproblems, while CodeGeneration is heavily represented at 75.8% within the coding data. Long-tail gains. We observe the largest gains on the long-tail code tasks. As seen in Figure 5, whether we provide the markers (TreasureMarked (fixed)) or the model infers them, both rare coding problems (CodeTranslation and CodeRepair) show large lifts with up to 6.5% and 14.1% relative gain over the baseline respectively. We note that these gains are far higher than the gains observed for the far more frequent task of CodeGeneration, which only shows lifts of up to 3.2% This shows that our framework benefits all parts of the distribution, but has disproportionate success enabling large lifts to highly infrequent features during training."
        },
        {
            "title": "4.3 Length Control in Inference Time",
            "content": "To assess the impact of length conditioning during inference, we benchmark on the AlpacaEval-LI dataset [Yuan et al., 2024], which evaluates how faithfully LLMs adhere to length constraints. We complement the measurements for length violation with Win Rates (%) by evaluating valid samples against the dataset provided completions using GPT-4o. We establish our baseline using completions generated by the Baseline model. Following similar approach to Yuan et al. [2024], we assess Violation (%) as the proportion of samples exceeding the specified length constraint (See Section 3.2.1 for details). Table 4: Length Instruction Following & generation quality on Alpaca-Eval LI. Model Baseline TreasureMarked +(fixed) Violation () Win Rates () 14.36% 19.48% 21.22% 36.58% 24.74% 1.25% Improvements to length control. In Table 4, we show improvements of up to 35.3% in length violation rates. This pronounced improvement results in mere 1.25% remaining violations for this evaluation set (essentially close to saturating performance on this evaluation). Even when the treasure markers are not explicitly provided but inferred directly by the model, we observe up to 11.8% absolute decrease in violation rates. These improvements to instruction following are nontrivial, and also lead to overall win-rate gains of up to 6.86%, ensuring quality is not compromised as length constraints are enforced. 10 en xx ar cs de el es Baseline TreasureMarked (fixed ) .6865 .6844 (-0.21) .7485 .755 (+0.65) .8824 .8848 (+0.24) .7463 .7500 (+0.37) .8249 .8307 (+0.58) fa .7099 .7072 (-0.27) fr he hi id it .789 .7948 (+0.58) .7214 .7166 (-0.48) .5158 .5229 (+0.71) .776 .7874 (+1.14) .8126 .8194 (+0.68) en xx ja ko nl pl pt ro ru tr Baseline TreasureMarked (fixed ) .7368 .7342 (-0.26) .7281 .7318 (+0.37) .8103 .8117 (+0.14) .7578 .7546 (-0.32) .822 .8281 (+0.61) .8048 .8166 (+1.18) .7675 .7627 (-0.48) .6669 .6723 (+0.54) uk .7625 .7575 (-0.50) vi .7593 .756 (-0.33) zh .7176 .7200 (+0.24) Table 5: X-CometXL scores [Colombo et al., 2023] on WMT24++ test sets [Deutsch et al., 2025]. Bold differences are significant at 0.05 according to paired T-Test and bootstrap resampling [Koehn, 2004] as implemented in comet-compare."
        },
        {
            "title": "4.4 Machine Translation",
            "content": "To study the effects of the markers on machine translation, we benchmark on WMT24++[Deutsch et al., 2025] and report translation performance from English to 22 languages (en xx) based on the languages seen in pretraining. We use XCOMET-XL [Colombo et al., 2023] for evaluation, state-of-the-art machine translation evaluation metric [Freitag et al., 2024]. Table 5 shows the results with the relative improvement over the Baseline. Training the model with markers and using them at inference time improves performance on 5 languages (es, id, it, pt, ro) significantly with up to 1.18 point gains, while retaining performance on all other languages. This constitutes remarkable improvement, especially given that the training data, up to the markers, is identical. According to the metric delta analysis in [Kocmi et al., 2024], improvements of such magnitudes are very likely to be confirmed in human evaluations. Baseline TreasureMarked (fixed) ar 81.1 88.4 de 71.9 84.4 es 65.7 82.7 fr 70.4 79.8 hi 68.0 73.7 id 49.0 66.7 it 72.5 82.8 ja 68.4 83.8 ko 75.8 85.0 pt 60.6 72.2 ru 68.0 86.6 tr 84.7 78.6 vi 67.4 82.8 zh 57.1 62.6 Avg. 68.6 79.58 ( 10.98) Table 6: Line-level pass rate on Complex Prompts from the Language Confusion Benchmark [Marchisio et al., 2024]."
        },
        {
            "title": "4.5 Language Control in Inference Time",
            "content": "As the final set of results, we focus on the effect of our training markers on ensuring model responds in the language specified by the user. To evaluate this, we use the Language Confusion Benchmark [Marchisio et al., 2024] which measures the ability of model to follow cross-lingual instructions such as Respond in French..., to request completions in another language. We measure performance on the Complex Prompts subset of the cross-lingual benchmark across 14 languages. Following [Marchisio et al., 2024], we measure Line-level Pass Rate (LPR) that only deems response \"correct\" if all lines in the generation match the users desired language. During inference, we insert training markers present in the data into the prompt, but leave out the <lang> marker, since it is already present in the prompt. Table 6 shows results across 14 languages. Our model with training markers significantly improves language control performance in 13 out of 14 languages with an absolute gain of 10.98% on average across 14 languages, showcasing remarkable improvement in controllability of inference time. We observe the largest gains for Russian (+18.6%) and the lowest gains for Chinese (+5.5%). 11 Original AlpcaEval-LI TreasureMarked (on-the-fly) Answer the following instruction using 199 words or less. Answer the following instruction using 199 words or less. What are the names of some famous actors that started their careers on Broadway? What are the names of some famous actors that started their careers on Broadway? <MARKER_LIST> <domain>Culture</domain> <length_bucket>concise</length_bucket> <length_tokens>199</length_tokens> <task>QuestionAnswering</task> </MARKER_LIST> Table 7: Examples of length control strategies: (left) Original instruction from AlpacaEval-LI dataset; (right) Actual modified instruction by appending predicted markers annotated on-the-fly using Command-A"
        },
        {
            "title": "5.1 Can markers be added on-the-fly at inference?",
            "content": "Our framework of training-time markers provides significant flexibility for explicit control over generations at inference time. While users can manually insert these markers, another LLM can also automatically annotate prompts with training markers on-the-fly before the generation step. In this section, to test the effectiveness of using another LLM to enrich an incoming prompt with the relevant markers at inference, we perform an ablation where we use Command [Cohere et al., 2025] as an annotator. At inference time, we make single additional call to Command to annotate prompt with all the relevant markers using few-shot examples and then append them to the prompt. We use the AlpacaEval-LI evaluation, as it is an excellent test bed for this setup due to the existence of clearly defined requirement in the prompt. Table 7 provides an example of one such annotation. The few-shot prompt used to annotate markers on-the-fly is provided in Appendix C. Model Violation () Win Rates () Table 8 shows the results for this ablation. Similar to section 4.3, we measure Violation (%) and Win Rates (%) for evaluation. When compared to using the TreasureMarked model with the original prompts, we observe drastic reduction in violation rates from 24.74% to mere 0.75% with 2.4% relative improvement in Win Rates (from 19.48% to 21.85%). Compared to Baseline, TreasureMarked (onthe-fly) extends the gains and leads to 35.8% reduction in length violation and 7.5% improvement in Win Rates. These results demonstrate the potential gains possible by using an additional call at inference to annotate an incoming prompt with relevant markers using an external model. Table 8: On-the-fly control (Alpaca-Eval LI): Using Command to annotate markers at inference time drastically reduces violation rates to <1% while improving Win Rates by +2.3% Baseline TreasureMarked TreasureMarked (on-the-fly) 14.36% 19.48% 21.85% 36.58% 24.74% 0.75%"
        },
        {
            "title": "5.2 How do markers interact?",
            "content": "We perform an additional ablation on the AlpacaEval-LI dataset from section 4.3 to study the effect of adding more useful markers at inference time. In addition to the <length_tokens> marker that 12 conveys the explicit length constraint, we annotate and add the <domain> marker, which we suspect legal text might be longer than conversations), but should add carries implicit length biases (e.g. helpful context to the prompt. With this we ask If multiple markers are added at inference, do their effects add up or cancel out? From Table 9, we observe that the effect of adding <domain> has positive impact on the generation quality with +3.5% jump in win rates albeit at the cost of slight increase in Violation Rate(%). This indicates that there are multidimensional relationships that form between treasure markers during training and can be leveraged in conjunction to achieve desired characteristics at inference. Model Violation () Win Rates () TreasureMarked (fixed) + <domain> 1.25% 1.87% 21.22% 24.72% Table 9: Multidimensional control (Alpaca-Eval LI): Adding <domain> marker improves generation quality and hence Win Rates by +3.5% working in conjunction with <length_tokens>, without hurting the length control."
        },
        {
            "title": "5.3 What is the impact of the dropout on the marker prediction?",
            "content": "dataset_sample <domain> <task> <format> <lang> 1.2% 99.2% 99.1% To understand the impact of the marker dropout ( 2.1), we train three variants with dataset-level dropouts of [0%, 50%, 70%] while sample-level dropout is fixed to 50%. Our goal with dropout is to teach the model to infer markers without needing explicit guiding at inference time. However, too much dropout may impede the model from learning key patterns between tags and output properties. To evaluate this, we calculate the accuracy of the markers inferred by the model to the underlying markers assigned to m-Arena Hard and average across all 23 languages [Dang et al., 2024] Table 10: Effect of dropout on marker prediction. Using no dropout (dataset-level) prevents the model to learn predicting the correct marker across categories, hence, hurts the flexibility of our framework. 7.3% 47.4% 46.8% 1.9% 53.6% 51.4% 3.3% 74.9% 75.1% 0_50 50_50 70_50 In Table 10, we observe that the least extreme dataset-level dropout variant 0_50 struggles to predict the correct marker at inference time. This is expected performance, since at training time, 0% dropout of markers across the dataset implies all training sample prompts have markers associated with it which makes it overly dependent on the presence of markers at inference time. At inference time, as this is not provided, accuracy is very low at 3.42%. We note that at both 50% and 70% dataset level dropout, we observe similar final abilities to infer the correct markers. Given this, unless specified elsewhere, 50% dataset-level dropout is the default specification used throughout experiments since it strikes the best balance between learning and generalizaton."
        },
        {
            "title": "6 Related Work",
            "content": "From oneto multi-dimensional training data markers. The idea of tagging inputs with markers in neural sequence modeling goes back to early applications in machine translation and language modeling. The motivation there was to leverage discrete features during training and inference to overcome data sparsity or imbalance and introduce levers of control. In early neural LMs, special tokens were added as markers to target very specific attribute such as the topic [Mikolov & Zweig, 2012] or auxiliary features [Aransa et al., 2015] such as genre and length. In translation such markers were introduced to control attributes like the target language [Johnson et al., 2017] or desired output quality [Caswell et al., 2019; Riley et al., 2020; Marie et al., 2020; Larkin et al., 2021; 13 Freitag et al., 2022] and text complexity [Agrawal & Carpuat, 2019; Marchisio et al., 2019], but also language-specific nuances like politeness [Sennrich et al., 2016; Feely et al., 2019], voice [Yamagishi et al., 2016], gender [Kuczmarski & Johnson, 2018], domains [Kobus et al., 2017; Britz et al., 2017], or diversity [Shu et al., 2019] of translations. Other works enriched the input representation during training with discrete linguistic features [Sennrich & Haddow, 2016] or document information [Jehl & Riezler, 2018] for better contextualization at inference time. Where and how tags should be placed best differ across applications [Jehl & Riezler, 2018; Wu et al., 2021]. All of these were individual efforts that target one or two dimensions at time, highly specialized for one trained target model and with training data for one particular task. Very limited work has been done on multidimensional markers [Stergiadis et al., 2021; Ramnath et al., 2021]. In contrast, our focus is on much more general framework with vast training corpus that targets general performance. Our approach is similarly general, where instead of single feature, we want to enable flexible approach that can be used for any text generation task. Furthermore, our goal is to explicitly target improving performance on the long-tail of underrepresented features. From control in pretraining to control in instruction finetuning. In LLM research, there are several related works that experiment with adding prefixes for control in pretraining: Keskar et al. [2019] add control codes for desired text features in pretraining of LLM derived from the structure of their source, i.e., subdomains or links of online texts and specific task labels for translation and QA. At inference time, values for these control codes are specific to steer the generation. Gao et al. [2025] further propose cooldown schedule in pretraining going from marked data to unmarked data in order to not require prefixes at inference. Yuan et al. [2024] focus on length control by adding natural language length specification templates to fine-tuning data for preference optimization. In our work, we focus on the instruction finetuning stage and incorporate nuanced multi-dimensional markers (i.e. the user can specify length and domain and format). We circumvent cooldown schedule by simply introducing marker dropout, hence requiring much smaller volume of marked data at training time, and not complete population of tags at inference time. With the option to fill markers on-the-fly, our framework is highly flexible and customizable. From encoded to inferred meta-information. Related prefix and prompt tuning methods [Li & Liang, 2021; Lester et al., 2021] use continuous embeddings learned for special tokens representing markers in training to condition predictions for specific tasks at inference time. Shen et al. [2024] further break those into separate markers for domain and function. In our case, we directly embed prefixes with the same vocabulary as the LLM, smoothly integrating them into the sequence. In our experiments, we find that this helps format following even when specified in natural language and not markers (e.g. for desired output length and language sections 4.3 and 4.5). Attribute-based control in LLM generations has also been pursued with other methods, such as attribute classifiers [Dathathri et al., 2020] or learned attribute vectors [Yang et al., 2023] see [Zhang et al., 2023] for comprehensive survey."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we proposed adding markers to training data to map out potential treasures that can be retrieved at inference time, such as specific task configurations or quality characteristics. In our experiments on multilingual instruction-finetuning, we showed that these markers are powerful tool to execute control (quality, length, output language) over generations, and at the same time have beneficial effects for generation quality of underrepresented portions of the training data, such as rare coding tasks. We found that dropout of training markers trains the model to infer missing 14 markers at inference time. With this flexibility, we allow users to hunt treasures without having to tediously engineer prompts or few-shot examples for optimized performance."
        },
        {
            "title": "8 Acknowledgments",
            "content": "We thank John Dang, Yiyang Nan, Thomas Euyang, Tom Kocmi, Tom Sherbone, Manoj Govindassamy, Cécile Robert-Michon, Leila Chan Currie, and other colleagues at Cohere and Cohere Labs for their support and thoughtful feedback."
        },
        {
            "title": "References",
            "content": "Sweta Agrawal and Marine Carpuat. Controlling text complexity in neural machine translation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 15491564, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1166. URL https: //aclanthology.org/D19-1166/. Sotiris Anagnostidis and Jannis Bulian. How susceptible are llms to influence in prompts? arXiv preprint arXiv:2408.11865, 2024. Walid Aransa, Holger Schwenk, and Loïc Barrault. Improving continuous space language models auxiliary features. In Marcello Federico, Sebastian Stüker, and Jan Niehues (eds.), Proceedings of the 12th International Workshop on Spoken Language Translation: Papers, pp. 151158, Da Nang, Vietnam, December 3-4 2015. URL https://aclanthology.org/2015.iwslt-papers.5/. Denny Britz, Quoc Le, and Reid Pryzant. Effective domain mixing for neural machine translation. In Ondřej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, and Julia Kreutzer (eds.), Proceedings of the Second Conference on Machine Translation, pp. 118126, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4712. URL https://aclanthology.org/W17-4712/. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Isaac Caswell, Ciprian Chelba, and David Grangier. Tagged back-translation. In Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pp. 5363, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5206. URL https://aclanthology.org/W19-5206/. Team Cohere, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos, et al. Command a: An enterprise-ready large language model. arXiv preprint arXiv:2504.00698, 2025. Pierre Colombo, Nuno Guerreiro, Ricardo Rei, Daan Van, Luisa Coheur, and André Martins. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 2023. 15 John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, et al. Aya expanse: Combining research breakthroughs for new multilingual frontier. arXiv preprint arXiv:2412.04261, 2024. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: simple approach to controlled text generation. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=H1edEyBKDS. Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, et al. Wmt24++: Expanding the language coverage of wmt24 to 55 languages & dialects. arXiv preprint arXiv:2502.12404, 2025. Weston Feely, Eva Hasler, and Adrià de Gispert. Controlling Japanese honorifics in English-toJapanese neural machine translation. In Toshiaki Nakazawa, Chenchen Ding, Raj Dabre, Anoop Kunchukuttan, Nobushige Doi, Yusuke Oda, Ondřej Bojar, Shantipriya Parida, Isao Goto, and Hidaya Mino (eds.), Proceedings of the 6th Workshop on Asian Translation, pp. 4553, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5203. URL https://aclanthology.org/D19-5203/. Markus Freitag, David Vilar, David Grangier, Colin Cherry, and George Foster. natural diet: Towards improving naturalness of machine translation output. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 33403353, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.263. URL https://aclanthology.org/2022.findings-acl.2 63/. Markus Freitag, Nitika Mathur, Daniel Deutsch, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Frederic Blain, Tom Kocmi, Jiayi Wang, David Ifeoluwa Adelani, Marianna Buchicchio, Chrysoula Zerva, and Alon Lavie. Are LLMs breaking MT metrics? results of the WMT24 metrics shared task. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz (eds.), Proceedings of the Ninth Conference on Machine Translation, pp. 4781, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.wmt-1.2. URL https://aclanthology.org/2024.wmt-1.2/. Tianyu Gao, Alexander Wettig, Luxi He, Yihe Dong, Sadhika Malladi, and Danqi Chen. Metadata conditioning accelerates language model pre-training. arXiv preprint arXiv:2501.01956, 2025. Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear, 2017. Laura Jehl and Stefan Riezler. Document-level information as side constraints for improved neural patent translation. In Colin Cherry and Graham Neubig (eds.), Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pp. 112, Boston, MA, March 2018. Association for Machine Translation in the Americas. URL https: //aclanthology.org/W18-1802/. Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Googles multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339351, 2017. doi: 10.1162/tacl_a_00065. URL https://aclanthology.org/Q17-1024/. 16 Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. arXiv preprint Ctrl: conditional transformer language model for controllable generation. arXiv:1909.05858, 2019. Catherine Kobus, Josep Crego, and Jean Senellart. Domain control for neural machine translation. In Ruslan Mitkov and Galia Angelova (eds.), Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pp. 372378, Varna, Bulgaria, September 2017. INCOMA Ltd. doi: 10.26615/978-954-452-049-6_049. URL https://aclanthology.org/R 17-1049/. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. Tom Kocmi, Vilém Zouhar, Christian Federmann, and Matt Post. Navigating the metrics maze: Reconciling score magnitudes and accuracies. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19992014, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acllong.110. URL https: //aclanthology.org/2024.acl-long.110/. Philipp Koehn. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu (eds.), Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388395, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-3250/. James Kuczmarski and Melvin Johnson. Gender-aware natural language translation. Technical Disclosure Commons, 2018. URL https://www.tdcommons.org/dpubs_series/1577. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Samuel Larkin, Michel Simard, and Rebecca Knowles. Like chalk and cheese? on the effects of translationese in MT training. In Kevin Duh and Francisco Guzmán (eds.), Proceedings of Machine Translation Summit XVIII: Research Track, pp. 103113, Virtual, August 2021. Association for Machine Translation in the Americas. URL https://aclanthology.org/2021.mtsummit-resea rch.9/. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlpmain.243. URL https://aclanthology.org/2021.emnlp-main.243/. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 45824597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https: //aclanthology.org/2021.acl-long.353/. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 90199052, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.616. URL https://aclanthology.org/2022.emnlp-main.616/. Robert Logan IV, Ivana Balazevic, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. Cutting down on prompts and parameters: Simple few-shot learning with language models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 28242835, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.222. URL https://aclanthology .org/2022.findings-acl.222/. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 80868098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556/. Dung Nguyen Manh, Nam Le Hai, Anh TV Dau, Anh Minh Nguyen, Khanh Nghiem, Jin Guo, and Nghi DQ Bui. The vault: comprehensive multilingual dataset for advancing code understanding and generation. arXiv preprint arXiv:2305.06156, 2023. Kelly Marchisio, Jialiang Guo, Cheng-I Lai, and Philipp Koehn. Controlling the reading level of machine translation output. In Mikel Forcada, Andy Way, Barry Haddow, and Rico Sennrich (eds.), Proceedings of Machine Translation Summit XVII: Research Track, pp. 193203, Dublin, Ireland, August 2019. European Association for Machine Translation. URL https://aclantholo gy.org/W19-6619/. Kelly Marchisio, Wei-Yin Ko, Alexandre Berard, Théo Dehaze, and Sebastian Ruder. Understanding In Yaser Al-Onaizan, Mohit Bansal, and Yunand mitigating language confusion in LLMs. Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 66536677, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.380. URL https://aclanthology.org/2024.emnl p-main.380/. Benjamin Marie, Raphael Rubino, and Atsushi Fujita. Tagged back-translation revisited: Why does it really work? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5990 5997, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-m ain.532. URL https://aclanthology.org/2020.acl-main.532/. 18 Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model. In 2012 IEEE Spoken Language Technology Workshop (SLT), pp. 234239, 2012. doi: 10.1109/SLT. 2012.6424228. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale In Conference on multi-subject multi-choice dataset for medical domain question answering. health, inference, and learning, pp. 248260. PMLR, 2022. Sahana Ramnath, Melvin Johnson, Abhirut Gupta, and Aravindan Raghuveer. HintedBT: Augmenting Back-Translation with quality and transliteration hints. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 17171733, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.129. URL https://aclanthology.org/2021.emnlp-main.129/. Leonardo Ranaldi and Andre Freitas. Aligning large and small language models via chain-of-thought reasoning. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 18121827, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.109/. Parker Riley, Isaac Caswell, Markus Freitag, and David Grangier. Translationese as language in multilingual NMT. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7737 7746, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-m ain.691. URL https://aclanthology.org/2020.acl-main.691/. Rico Sennrich and Barry Haddow. Linguistic input features improve neural machine translation. In Ondřej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Aurélie Névéol, Mariana Neves, Pavel Pecina, Martin Popel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post, Lucia Specia, Karin Verspoor, Jörg Tiedemann, and Marco Turchi (eds.), Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers, pp. 8391, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2209. URL https://aclanthology.org/W16-2209/. Rico Sennrich, Barry Haddow, and Alexandra Birch. Controlling politeness in neural machine translation via side constraints. In Kevin Knight, Ani Nenkova, and Owen Rambow (eds.), Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3540, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1005. URL https: //aclanthology.org/N16-1005/. Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolò Fusi. Tag-llm: repurposing general-purpose llms for specialized domains. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. thorough examination of decoding methods in the era of LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 86018629, Miami, Florida, USA, November 2024. Association 19 for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.489. URL https://aclantho logy.org/2024.emnlp-main.489/. Raphael Shu, Hideki Nakayama, and Kyunghyun Cho. Generating diverse translations with sentence codes. In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 18231827, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1177. URL https://aclantho logy.org/P19-1177/. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al. Aya dataset: An open-access collection for multilingual instruction tuning. arXiv preprint arXiv:2402.06619, 2024. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=4F WAwZtd2n. Emmanouil Stergiadis, Satendra Kumar, Fedor Kovalev, and Pavel Levin. Multi-domain adaptation in neural machine translation through multidimensional tagging. In Janice Campbell, Ben Huyck, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky (eds.), Proceedings of Machine Translation Summit XVIII: Users and Providers Track, pp. 396420, Virtual, August 2021. Association for Machine Translation in the Americas. URL https://aclanthology.org/2 021.mtsummit-up.27/. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Improving zero-shot chain-of-thought reasoning by large language Plan-and-solve prompting: models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 26092634, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.147. URL https://aclanthology.org/2023.acl-long.147/. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlM eSB_J. Han Wenjuan, Wei Xiang, Cui Xingyu, Cheng Ning, Jiang Guangyuan, Qian Weinan, and Zhang Chi. Prompt engineering 101 prompt engineering guidelines from linguistic perspective. In Maosong Sun, Jiye Liang, Xianpei Han, Zhiyuan Liu, and Yulan He (eds.), Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 1: Main Conference), pp. 14081426, Taiyuan, China, July 2024. Chinese Information Processing Society of China. URL https://aclanthology.org/2024.ccl-1.108/. Genta Winata, Shijie Wu, Mayank Kulkarni, Thamar Solorio, and Daniel Preotiuc-Pietro. Crosslingual few-shot learning on unseen languages. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang (eds.), Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 777791, Online only, November 2022. 20 Association for Computational Linguistics. https://aclanthology.org/2022.aacl-main.59/. doi: 10.18653/v 1/2022.aaclmain.59. URL Liwei Wu, Shanbo Cheng, Mingxuan Wang, and Lei Li. Language tags matter for zero-shot neural machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 30013007, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.264. URL https://aclanthology.org/2021.findings-acl.264/. Yangjian Wu and Gang Hu. Exploring prompt engineering with GPT language models for documentlevel machine translation: Insights and findings. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 166 169, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/20 23.wmt-1.15. URL https://aclanthology.org/2023.wmt-1.15/. Hayahide Yamagishi, Shin Kanouchi, Takayuki Sato, and Mamoru Komachi. Controlling the voice of sentence in Japanese-to-English neural machine translation. In Toshiaki Nakazawa, Hideya Mino, Chenchen Ding, Isao Goto, Graham Neubig, Sadao Kurohashi, Ir. Hammam Riza, and Pushpak Bhattacharyya (eds.), Proceedings of the 3rd Workshop on Asian Translation (WAT2016), pp. 203210, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. URL https://aclanthology.org/W16-4620/. Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing Chen, and Jun Xie. Tailor: soft-prompt-based approach to attribute-based controlled text generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 410427, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.25. URL https://aclanthology.org/2023.acl-long.25/. Fangyi Yu, Lee Quartey, and Frank Schilder. Exploring the effectiveness of prompt engineering for legal reasoning tasks. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1358213596, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.858. URL https://aclanthology.org/2023.findings-acl.858/. Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, and Jing Xu. Following length constraints in instructions. arXiv preprint arXiv:2406.17744, 2024. Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, and Dawei Song. survey of controllable text generation using transformer-based pre-trained language models. ACM Comput. Surv., 56(3), October 2023. ISSN 0360-0300. doi: 10.1145/3617680. URL https://doi.org/10.1145/3617680. Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017."
        },
        {
            "title": "A Categories for Training Markers",
            "content": "Length: <length_tokens>, <length_sentences>, and <length_paragraphs> models granular control in generation length. We tokenize using language-specific Spacy models[Honnibal & Montani, 2017] to obtain token and sentence counts. For paragraph counts, we use the \"nn\" delimiter. <length_bucket> categorizes generations into broader categories such as concise (under 300 tokens), medium (between 300 and 1,000 tokens) or long (over 1,000 tokens), providing more general level of control when needed. Format: <format> is used to describe generations with specific output structures such as: JSON, Markdown, or tabular formats. This is particularly useful to condition stricter format requirements needed for real world use cases. Style: <style> captures tone and manner of communication, distinguishing between different modes of expression such as \"Formal\" and \"Informal\". We also add \"Custom\" value to model for training examples where the user specifies particular format. For instance, at inference if user asks, \"Respond like Yoda you will\" this marker will allow the model to adapt its response to match the requested style. We annotate this marker by using dataset-related information. Language: <language> describes the natural language of the generation, enabling us to model responses in specific languages during inference. We provide detailed markers across the 23 languages covered by our model. Our goal with this tag is to improve language-specific generations and reduce language switching where prompt specified by user in one language is not responded to in the same language in the completion. <code_type> is specifically used to model programming languages for coding-related tasks. We annotate this marker by using dataset-related information. Quality: <quality> provides measurable score indicating the quality of sample, often derived from human annotations or Reward Model (RM). We utilize proprietary reward model4 to assign rewards to subset of our training data. We also use these rewards to create categorical marker <quality_bucket> by using quartiles within language-specific subsets into {1,2,3,4}, offering broader description of quality. Source: <source> describes the origin of the data, distinguishing between human-generated content and other methods of data creation like synthetic and translation. We annotate this marker by using dataset-related information. Domain: <domain> ensures that domain-specific knowledge is captured from training subsets, which can then be leveraged at inference to generate content that is relevant and accurate within particular field. This is particularly crucial for inputs that could belong to multiple fields. For instance, when user asks, \"How do calculate factorial?\", specifying the <domain> as either Code or Math provides valuable context for modeling the interaction. In cases where this marker cannot be obtained from the dataset information, we employ an LLM to annotate and provide our detailed prompt in Task: <task> defines the overall objective of the generation and helps capture task-specific behaviors, especially when outputs involve complex combinations of formats or actions. This marker is useful to model dataset-wise characteristics. We hypothesize this is particularly helpful for indicating complex workflows during inference. For example, distinguishing between Translation and 4The RM is competitive with leading reward models on the RewardBench Leaderboard [Lambert et al., 2024](ht tps://huggingface.co/spaces/allenai/reward-bench) 22 CodeTranslation, or Rewrite and CodeFix, enhances the descriptiveness of datapoints within the same training pool. In cases where this marker cannot be obtained from the dataset information, we employ an LLM to annotate and provide our detailed prompt in B"
        },
        {
            "title": "B LLM Annotation",
            "content": "For the following training markers : <domain>, <task>, <format> we annotate using the multilingual open-weights model Command R+. We provide definitions and multilingual few-shot examples (except for <format>) to obtain highquality annotations from the LLM. The prompt used for tagging is as follows: B.0.1 <domain> You are helpful assistant whose goal is to classify the given prompt into single class given the following definitions (cid:44) (cid:44) (cid:44) `Sciences` : Topics related to the broad area of knowledge encompassing all scientific disciplines, including biology, chemistry, physics, earth sciences, and astronomy, which study the natural world through observation, experimentation, and analysis, aiming to understand fundamental principles and phenomena across various scales and aspects of the universe (cid:44) `Technology` : Topics related to the broad area of knowledge encompassing all (cid:44) (cid:44) (cid:44) (cid:44) engineering and technical disciplines, including Computer Science, Software Engineering, Internet of Things(IoT), Cybersecurity, Data Science, Artificial Intelligence, Machine Learning and various engineering disciplines like Mechanical Engineering, Civil Engineering and Biotechnology (cid:44) `SocialSciences` : Topics related to the broad area of knowledge encompassing all (cid:44) (cid:44) (cid:44) academic disciplines dedicated to the systematic study of human society, social relationships, and the structures that shape them, including fields like anthropology, economics, political science, psychology, and sociology, all focused on understanding how individuals and groups interact within society and the factors influencing their behavior, cultural norms, and societal institutions (cid:44) `Culture` : Topics related to the broad area of knowledge encompassing all cultural (cid:44) (cid:44) (cid:44) practices or beliefs within societies, including related concepts or behaviors that people within culture group share and understand as belonging together, like food, art, language, family structure, societal norms or religious rituals (cid:44) `Medical` : Topics related to the broad area of knowledge and practice encompassing (cid:44) (cid:44) (cid:44) all medicine and healthcare, including diagnosing and treating diseases, preventative measures, specialties like surgery, cardiology, oncology, pediatrics, and more, all built upon the foundation of basic medical sciences and patient care principles (cid:44) `Finance` : Topics related to the broad area of knowledge encompassing activities like (cid:44) (cid:44) managing money, business ethics, investing, borrowing, lending, trading, budgeting, saving, and forecasting, essentially focusing on the acquisition, allocation, and management of capital within businesses (cid:44) , individuals, and governments across various financial markets and instruments 23 (cid:44) (cid:44) `Legal` : Topics related to the broad area of knowledge encompassing Private, Public and Criminal Law, Criminal Justice, Law Enforcement, Policing, Justice Systems or Crime (cid:44) `Conversation` : Topics related to Conversation, Chit-Chat or Roleplay `Code` : Topics related to specific subject/field within computer programming where software is designed and developed to solve problems related to particular industry, business function, or area of expertise, essentially defining the target audience and unique requirements for the code being written including tasks like Code Generation, Code Fix and Code Explanation (cid:44) `Math` : Topics related to the broad field of study that uses numbers, shapes, and formulas to describe and quantify the world, including areas like Logical Reasoning, Quantitative Calculation, Pattern Recognition, Formulating Conjectures, Arithmetic, Algebra, Geometry, Number Theory, Set Theory and Analysis (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) If you are unable to confidently assign one of the above classes, you will simply respond with `Unspecified` and nothing else. (cid:44) Note: - You are only to respond with the name of the class you believe best matches the domain of the example. (cid:44) - You are only allowed to classify the example into one of the following tags : [`Sciences`, `Technology`, `SocialSciences`, `Culture`, `Medical`, `Finance`, `Legal`, `Conversation`, `Code`, `Math`, `Unspecified`] (cid:44) Here are few examples : Prompt : What is photosynthesis? Answer : `Sciences` Prompt : What is the TCP/IP protocol and how does it work ? Answer : `Technology` Prompt : How has globalization affected social cohesion ? Answer : `Social Sciences` Prompt : What is an example of popular dish that is available in multiple communities but known under different names? (cid:44) Answer : `Culture` Prompt : How long does one have to fast before fasting sugar blood test? Answer : `Medical` Prompt : Analyze the impact of microfinance initiatives on poverty alleviation in developing countries. (cid:44) Answer : `Finance` Prompt : What is the difference between first-degree crime and second-degree crime? (cid:44) Answer : `Legal` 24 Prompt : Hey! How are you? Answer : `Conversation` Prompt : Given variable x=3.142 in Python, how would use an f-string to show just 1 decimal value? (cid:44) Answer : `Code` Prompt : Solve the quadratic equation: x² + 5x - 6 = 0 Answer : `Math` Prompt : Use ABC notation to write melody in the style of folk tune. Answer : B.0.2 <task> You are helpful assistant whose goal is to classify the given prompt into single class given the following definitions (cid:44) `CodeTranslation` : Tasks related to the process of converting source code from one programming language to another while preserving the code's functionality (cid:44) `CodeExplanation` : Tasks related to the specific process of explaining snippet of code in programming language (cid:44) `CodeGeneration` : Tasks related to the specific process of generating snippet of code in programming language (cid:44) `Explanation` : Tasks related to explaining concept in any domain `CreativeWriting` : Tasks related to any form of writing that employs creative, (cid:44) literary or poetic techniques that displays imagination or invention including role-play (cid:44) `QuestionAnswering` : Tasks related to any form of question answering, including (cid:44) (cid:44) open-ended questions, closed-ended questions about given context and requests for information about particular entity. This will also generally include your `what`, 'which', 'who', 'when' type questions (cid:44) `OpenEnded` : Tasks related to any form of open-ended text generation like chat, conversation or chit-chat (cid:44) `InformationExtraction` : Tasks related to any form of information extraction usually involving some context (cid:44) `Summarization` : Tasks related to any form of summarization including but not limited (cid:44) to abstractive summarization, extractive summarization or concise descriptions of content (cid:44) `CodeFix` : Tasks related to the specific process of correcting/fixing piece of code to achieve the desired result. (cid:44) `Reasoning` : Tasks involving any form of Ideation, Reasoning, Problem Solving, (cid:44) Instruction Following or Chain-of-Thought(CoT) in order to achieve the desired result. (cid:44) `Rewrite` : Tasks involving any form of re-writing/re-phasing/re-wording/re-framing in order to achieve the desired result. (cid:44) 25 `Classification` : Tasks related to specific request of classification where you are required to assign thing to one of several groups (cid:44) `Translation` : Tasks related to specific request of translating given piece of text from one language to another language (cid:44) If you are unable to confidently assign one of the above classes, you will simply respond with `Unspecified` and nothing else. (cid:44) Note: - You are only to respond with the name of the class you believe best matches the domain of the example. (cid:44) - You are only allowed to classify the example into one of the following tags : [`CodeTranslation`, `CodeExplanation`, `CodeGeneration`, `Explanation`, `CreativeWriting`, `QuestionAnswering`, `OpenEnded`, `InformationExtraction`, `Summarization`, `CodeFix`, `Reasoning`, `Rewrite`, `Classification`, `Translation`, `Unspecified`] (cid:44) (cid:44) (cid:44) Here are few examples : Prompt : Translate the following Python function to equivalent JavaScript code that checks if string is palindrome. (cid:44) def is_palindrome(str): return str == str[::-1] Answer : `CodeTranslation` Prompt : Explain the following python function : def is_palindrome(str): return str == str[::-1] Answer : `CodeExplanation` Prompt : Generate Python function to check whether string is palindrome. Answer : `CodeGeneration` Prompt : Explain briefly how the water cycle works Answer : `Explanation` Prompt : Translate the following sentence from English to Spanish, using formal tone: 'We are pleased to announce the new partnership with our company.' (cid:44) Answer : `Translation` Prompt : You're talk show host. Pick two guests that are wildly different from each other. Briefly introduce them (cid:44) Answer : `CreativeWriting` Prompt : Classify the sentiment of the following review as positive, negative, or neutral: 'The product exceeded my expectations!' (cid:44) Answer : `Classification` Prompt : What is the capital city of France? Answer : `QuestionAnswering` Prompt : Describe your ideal work environment Answer : `OpenEnded` 26 Prompt : From the following news article, extract the names of the companies involved (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) in the recent merger, along with the date the merger was announced. (cid:44) Context:In significant development in the tech industry, two leading companies have announced their merger, marking new era of innovation and collaboration. The merger, which was officially announced on March 15, 2025, brings together TechInnovate Inc. and DigitalSolutions Corp, two giants in their respective fields. TechInnovate Inc, known for its cutting-edge research and development in artificial intelligence and machine learning, has been at the forefront of technological advancements. With team of over 5,000 engineers and scientists, the company has consistently delivered groundbreaking solutions that have transformed various industries. DigitalSolutions Corp, on the other hand, is renowned for its expertise in software development and digital transformation. The company has proven track record of helping businesses across the globe to modernize their (cid:44) operations and enhance their digital capabilities. With workforce of over 10,000 professionals, DigitalSolutions Corp. has been key player in driving digital innovation. The merger is expected to create powerhouse in the tech industry, combining the strengths of both companies to offer comprehensive solutions that address the evolving needs of businesses and consumers. The combined entity will leverage TechInnovate's AI and machine learning capabilities with DigitalSolutions' software development expertise to develop next-generation technologies. Industry analysts predict that this merger will lead to significant advancements in areas such as autonomous systems, smart cities, and personalized healthcare. The synergy between the two companies is anticipated to drive innovation, improve efficiency, and create new opportunities for growth. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Answer : `InformationExtraction` Prompt : Given the following story, provide title that summarizes the idea behind the story: (cid:44) Context: There once was girl who was frustrated with life and asked her father for advice. He (cid:44) (cid:44) (cid:44) asked her to bring an egg, two tea leaves, and potato. He then started boiling water in three separate vessels. He put the egg, potato, and tea leaves in one vessel each. After few minutes, he asked her to peel the egg and potato and strain the leaves. He explained to his daughter that: (cid:44) The soft egg was now hard. The hard potato was now soft. The tea had changed the water itself. When adversity is at our door, we can respond to it in different ways. Moral: We decide how to respond to difficult situations. Answer : `Summarization` Prompt : Fix the code below to correctly identify palindrome: def is_palindrome(str): return str == str[-1] Answer : `CodeFix` 27 Prompt : John has one pizza, cut into eight equal slices. John eats three slices, and his friend eats two slices. How many slices are left? Explain your reasoning step by step. (cid:44) (cid:44) Answer : `Reasoning` Prompt : Exaggerate this product description : 'Our new sneakers are comfortable, lightweight, and stylish.' to paragraph that can be used by the marketing team (cid:44) Answer : `Rewrite` Prompt : Use ABC notation to write melody in the style of folk tune. Answer : B.0.3 <format> You are helpful assistant whose goal is to classify the given prompt into single class given the following definitions (cid:44) `MCQAnswer` : Tasks related to multiple-choice type question answering. These prompts (cid:44) (cid:44) will typically contain multiple choices provided either in bullet form or eumerated numerically/alphabetically. This also contains multiple-choice question answer generation tasks. (cid:44) `ChainOfThought` : Tasks related to Chain-of-Thought(CoT) style question answering. This also contains CoT style question answer generation tasks (cid:44) `Enumeration` : Tasks that involve enumeration, bullet points, lists or itemization of any form (cid:44) `XML` : Tasks that involve XML generation, validation or processing in any form `Tabular` : Tasks that involve table generation, validation or processing in any form `JSON` : Tasks that involve JSON generation, validation or processing in any form `Markdown` : Tasks that involve Markdown generation, validation or processing in any form (cid:44) If you are unable to confidently assign one of the above classes, you will simply respond with `Unspecified` and nothing else. (cid:44) Note: - You are only to respond with the name of the class you believe best matches the domain of the example. (cid:44) - You are only allowed to classify the example into one of the following tags : [`MCQAnswer`, `ChainOfThought`, `Enumeration`, `XML`, `Tabular`, `JSON`, `Markdown`, `Unspecified`] (cid:44) Prompt : Use ABC notation to write melody in the style of folk tune. Answer : 28 Marker Annotation on-the-fly To annotate an incoming prompt with markers on-the-fly with Command [Cohere et al., 2025], we use the following prompt You are an expert tagger of information. You will be given list of characteristics with their definitions and possible values. (cid:44) You are to analyze the given prompt and only assign value from characteristic if it is absolutely applicable. (cid:44) The format to be returned in XML format as specified below. Strictly follow this (cid:44) format and if no tags are applicable, simply return ```<MARKER_LIST></MARKER_LIST>``` (cid:44) You will only return one value from characteristic(if applicable) The characteristics are : <length_tokens> Definition: any integer value denoting requirement of an # of words or tokens in the prompt. (cid:44) Values: int <length_sentences> Definition: any integer value denoting requirement of an # of sentences or lines in the prompt. (cid:44) Values: int <length_paragraphs> Definition: any integer value denoting requirement of an # of paragraphs in the prompt. (cid:44) Values: int <length_bucket> Definition: grouping based on generation requirement of # of tokens. if # of (cid:44) tokens < 300, then 'concise'. If 300 < # of tokens < 1000, then 'medium'. If # of tokens > 1,000, then 'long' (cid:44) Values: list = ['concise', 'medium', 'long'] <task> Definition: to assign task-related information if evident from the prompt Values: list = ['OpenEnded', 'Explanation', 'Translation', 'Classification', 'CreativeWriting', 'QuestionAnswering', 'InformationExtraction', 'Summarization', 'Rewrite', 'Reasoning', 'CodeGeneration', 'CodeFix', 'CodeTranslation', 'CodeExplanation'] (cid:44) (cid:44) (cid:44) <domain> Definition: to assign domain-related information if evident from the prompt Values: list = ['Sciences', 'Technology', 'SocialSciences', 'Culture', 'Medical', 'Legal', 'Unspecified', 'Conversation', 'Code', 'Math'] (cid:44) <code_type> Definition: to specify the coding langugage. This is only applicable if the task is coding-related. (cid:44) Values: list = ['python', 'javascript', 'cpp', 'cobol', 'java', 'go', 'rust', 'swift', 'csharp', 'php', 'typescript', 'shell', 'c', 'kotlin', 'ruby', 'haskell', 'sql'] (cid:44) (cid:44) <format> Definition: to assign format-related information if specified from the prompt 29 Values: list = ['MCQAnswer', 'ChainOfThought', 'XML', 'JSON', 'Enumeration', 'Tabular', 'Markdown', 'Latex'] (cid:44) <style> Definition: to assign style-related information if specified from the prompt. Use 'Custom' for customized styles if specified by the user. (cid:44) Values: list = ['Formal', 'Informal', 'Custom'] <lang> Definition: to assign language-related information if the generation language is specified from the prompt. (cid:44) Values: list = ['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Hebrew', 'Hindi', 'Indonesian', 'Italian', 'Japanese', 'Korean', 'Persian', 'Polish', 'Portuguese', 'Romanian', 'Russian', 'Spanish', 'Turkish', 'Ukrainian', 'Vietnamese'] (cid:44) (cid:44) (cid:44) You will only return template and nothing else. Here are some sample inputs and outputs: prompt: \"Give me 4 paragraph summary of medical improvements that have occurred over the last two decades\" (cid:44) template: ``` <MARKER_LIST> <domain>Medical</domain> <lang>English</lang> <length_bucket>medium</length_bucket> <length_paragraphs>4</length_paragraphs> <task>Summarization</task> </MARKER_LIST> ``` prompt: \"List the top 5 regions for food in Germany? Respond in German\" template: ``` <MARKER_LIST> <domain>Culture</domain> <format>Enumeration</format> <lang>German</lang> <length_bucket>concise</length_bucket> <task>QuestionAnswering</task> </MARKER_LIST> ``` prompt: \"Answer the following instruction using 5 sentences or less.nnSolve this: 55+44+33+66\" (cid:44) template: ``` <MARKER_LIST> <domain>Math</domain> <length_bucket>concise</length_bucket> 30 <length_sentences>5</length_sentences> <task>Reasoning</task> </MARKER_LIST> ``` prompt: \"Answer the following instruction using 199 words or less.nnWhat are the names of some famous actors that started their careers on Broadway?\" template:"
        }
    ],
    "affiliations": [
        "Cohere",
        "Cohere Labs"
    ]
}