{
    "paper_title": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation",
    "authors": [
        "Jiatao Gu",
        "Yuyang Wang",
        "Yizhe Zhang",
        "Qihang Zhang",
        "Dinghuai Zhang",
        "Navdeep Jaitly",
        "Josh Susskind",
        "Shuangfei Zhai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process that gradually adds noise to the input. We argue that the Markovian property limits the models ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model with the same architecture as standard language models. DART does not rely on image quantization, enabling more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 9 5 1 8 0 . 0 1 4 2 : r Preprint. Working in progress DART: DENOISING AUTOREGRESSIVE TRANSFORMER FOR SCALABLE TEXT-TO-IMAGE GENERATION Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhangδ , Dinghuai Zhangγ, Navdeep Jaitly, Josh Susskind, Shuangfei Zhai Apple, δThe Chinese University of Hong Kong, γMila {jgu32,yuyangw,yizzhang,njaitly,jsusskind,szhai}@apple.com δqhzhang@link.cuhk.edu.hk γdinghuai.zhang@mila.quebec Figure 1: Curated examples of images generated by DART at 256 256 and 512 512 pixels."
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models have become the dominant approach for visual generation. They are trained by denoising Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the models ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, transformer-based model that unifies autoregressive (AR) and diffusion within non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model that has the same architecture as standard language models. DART does not rely on image quantization, which enables more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets new benchmark for scalable, high-quality image synthesis."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in deep generative models have led to significant breakthroughs in visual synthesis, with diffusion models emerging as the dominant approach for generating high-quality Work done as part of an internship at Apple. 1 Preprint. Working in progress Figure 2: () general illustration of the proposed DART. The model autoregressively denoises image through Transformer until clean image is generated. Here, whole images are shown for visualization purpose; () We show the architecture details which integrates state-of-the-art designs similar to common language models (Dubey et al., 2024). images (Rombach et al., 2022; Esser et al., 2024). Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) operate by progressively adding Gaussian noise to an image and learning to reverse this process in sequence of denoising steps. Despite their success, these models are difficult to train on high resolution images directly, requiring either cascaded models (Ho et al., 2022), or multiscale approaches (Gu et al., 2023) or preprocessing of images to autoencoder codes at lower resolutions (Rombach et al., 2022). These limitations can stem from their reliance on the Markovian assumption, which simplifies the generative process but restricts the model only to see the generation from the previous step. This often leads to inefficiencies during training and inference, as the individual steps of denoising are unaware of the trajectory of generations from prior steps. In parallel, autoregressive models, such as GPT-4 (Achiam et al., 2023), have shown great success in modeling long-range dependencies in sequential data, particularly in the field of natural language processing. These models efficiently cache computations and manage dependencies across time steps, which has also inspired research into adapting autoregressive models for image generation. Early efforts such as PixelCNN (Oord et al., 2016) suffered from high computational costs due to pixel-wise generation. More recent models like VQ-GAN (Esser et al., 2021) and related work (Yu et al., 2022; Team, 2024; Tian et al., 2024) learn models of quantized images in compressed latent space; Li et al. (2024) propose to generate directly in such space without quantization by employing diffusion-based loss function. However, these methods fail to fully leverage the progressive denoising benefits of diffusion models, resulting in limited global context and error propagation during generation. To address these limitations, we propose Denoising AutoRegressive Transformer (DART), novel generative model that integrates autoregressive modeling within non-Markovian diffusion framework (Song et al., 2021)  (Fig. 2)  . The non-Markovian formulation in DART enables the model to leverage the full generative trajectory during training and inference, while retaining the progressive modeling benefits of diffusion models, resulting in more efficient and flexible generation compared to traditional diffusion and autoregressive approaches. Additionally, DART introduces two key improvements to address the limitations of the non-Markovian approach: (1) token-level autoregressive modeling (DART-AR), which captures dependencies between image tokens autoregressively, enabling finer control and improved generation quality, and (2) flow-based refinement module (DART-FM), which enhances the models expressiveness and smooths transitions between denoising steps. These extensions make DART flexible and efficient framework capable of handling wide range of tasks, including class conditional, text-to-image, as well as multimodal generation. DART offers scalable, efficient alternative to traditional diffusion models, achieving competitive performance on standard benchmarks for class-conditioned (e.g., ImageNet (Deng et al., 2009)) and text-to-image generation. To summarize, major contributions of our work include: We propose DART, novel non-Markovian diffusion model that leverages the full denoising trajectory, leading to more efficient and flexible image generation compared to traditional approaches. We propose two key improvements: DART-AR and DART-FM, which improve the expressiveness and coherence throughout the non-Markovian generation process. DART achieves competitive performance in both class-conditioned and text-to-image generation tasks, offering scalable and unified approach for high-quality, controllable image synthesis. 2 Preprint. Working in progress"
        },
        {
            "title": "2.1 DIFFUSION MODELS",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are latent variable models with fixed posterior distribution, and trained with denoising objective. These models have gained widespread use in image generation (Rombach et al., 2021; Podell et al., 2023; Esser et al., 2024). Diffusion models produce the entire image in non-autoregressive manner through iterative processes. Specifically, given an image x0 R3HW , we define series of latent variables xt (t = 1, , ) with Markovian process which gradually adds noise to the original image x0. The transition q(xtxt1) and the marginal q(xtx0) probabilities are defined as follows, respectively: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), q(xtx0) = (xt; αtx0, (1 αt) I), (1) where αt = (cid:81)t τ =1(1 βτ ), 0 < βt < 1 are determined by the noise schedule. The model learns to reverse this process with backward model pθ(xt1xt), which aims to denoise the image. The training objective for the model is: min LDM θ = Et[1,T ],xtq(xtx0)[ωt xθ(xt, t) x02 2], (2) where xθ(xt, t) is time-conditioned denoiser that learns to map the noisy sample xt to its clean version x0; ωt is time-dependent loss weighting, which usually uses SNR (Ho et al., 2020) or SNR+1 (Salimans & Ho, 2022). Practically, xt can be re-parameterized with noiseor vprediction (Salimans & Ho, 2022) for enhanced performance, and can be applied on pixel space (Gu et al., 2023; Saharia et al., 2022) or latent space, encoded by VAE encoder (Rombach et al., 2021). However, standard diffusion models are computationally inefficient, requiring numerous denoising steps and extensive training data. Moreover, they lack the ability to leverage generation context effectively, hindering scalability to complex scenes and long sequences like videos. 2.2 AUTOREGRESSIVE MODELS In the field of natural language processing, Transformer models have achieved notable success in autoregressive modeling (Vaswani et al., 2017b; Raffel et al., 2020). Building on this success, similar approaches have been applied to image generation (Parmar et al., 2018; Esser et al., 2021; Chen et al., 2020; Yu et al., 2022; Sun et al., 2024; Team, 2024). Different from diffusion-based methods, these methods typically focus on learning the dependencies among discrete image tokens (e.g., through Vector Quantization (Van Den Oord et al., 2017)). To elaborate, consider an image R3HW . The process begins by encoding this image into sequence of discrete tokens z1:N = E(x). These tokens are designed to approximately reconstruct the original image through learned decoder ˆx = D(z1:N ). An autoregressive model is then trained by maximizing the cross-entropy as follows: max LCE θ = (cid:88) n=1 log Pθ(znz0:n1), (3) where z0 is the special start token. During the inference phase, the autoregressive model is first used to sample tokens from the learned distribution, and then decode them into image space using D. As discussed in Kilian et al. (2024), autoregressive models offer significant efficiency advantages over diffusion models by caching previous steps in memory and enabling the entire generation process to be computed in single parallel forward pass. This reduces computational overhead and accelerates training and inference. However, the reliance on quantization can lead to information loss, potentially degrading generation quality. Additionally, the linear, step-by-step nature of token prediction may overlook the global structure, making it challenging to capture long-range dependencies and holistic coherence in complex scenes or sequences."
        },
        {
            "title": "3 DART",
            "content": "3.1 NON-MARKOVIAN DIFFUSION FORMULATION We start by revisiting the basics of diffusion models from the perspective of hierarchical variational auto-encoders (HVAEs, Kingma & Welling, 2013; Child, 2021). Given data-point x0, HVAE 3 Preprint. Working in progress process pθ of sequence of latent variables {xt}t 1 by maximizing evidence lower bound (ELBO): max LELBO θ,ϕ = Ex1:T qϕ(x) (cid:34) (cid:88) t=1 log pθ(xt1xt:T ) + log pθ(xT ) log qϕ(x1:T x0) , (4) (cid:35) where x0 is the real data, and qϕ is learnable inference model. As pointed out in VDM (Kingma et al., 2021), diffusion models can essentially be seen as HVAEs with three specific modifications: 1. fixed inference process which gradually adds noise to corrupt data x0; 2. Markovian forward and backward process where xt depends only on xt+1 (Eq (1)); 3. Noise-dependent loss weighting that reweighs ELBO with focus on perception. Only with all above simplifications combined, standard diffusion models can be formulated as Eq (2) where the generator becomes Markovian pθ(xt1xt:T ) = pθ(xt1xt) so that one can randomly sample to learn each transition independently. This greatly simplifies modeling, enabling training models with sufficiently large number of steps (e.g., = 1000 for original DDPM (Ho et al., 2020)) without suffering from memory issues. In prior research, these aspects are highly coupled, and few works attempt to disentangle them. We speculate that the Markovian assumption might not be necessary requirement for high generation quality, as long as the fixed posterior distribution and flexible loss weightings are maintained. As side evidence, one can achieve reasonable generation with much fewer steps (e.g., 100) at inference time using non-Markovian HVAE. On the contrary, the Markovian modeling forces all information compressed solely in the corrupted data from previous noise level which could be an obstacle preventing efficient learning and require more inference steps. NOn-MArkovian Diffusion Models (NOMAD) In this paper, we reconsider the original form of generator pθ(xt1xt:T ) of HVAEs while maintaining the modifications, 1 and 3, made by diffusion models. More precisely, we learn the following weighted ELBO loss (Eq (4)) with adjustable ωt: max LNOMAD θ = Ex1:T q(x0) ωt log pθ(xt1xt:T ) , (5) (cid:35) (cid:34) (cid:88) t=1 where is the pre-defined inference process. This formulation shares many similarities as autoregressive (AR) models in Eq (3), where in our case, each token represents noisy sample xt. Therefore, it is natural to implement such process with autoregressive Transformers (Vaswani et al., 2017b). However, the Markovian inference process of standard diffusion models makes it impossible for the generator θ to use the entire context except for xt. That is to say, even if we initiate our generator as pθ(xt1xt:T ), the model only needs information in xt in order to best denoise xt1. Therefore, it is critical to design fixed and non-Markovian2 inference process q(xtx0:t1) to sample the noisy sequences x1:T x0. The simplest approach is to perform an independent noising process: q(xtx0:t1) = q(xtx0) = (xt; γtx0, (1 γt)I), [1, ], (6) where γt represents the non-Markovian noise schedule. Note that, while Eq (6) may look close to the marginal distribution of the original diffusion models (Eq (1)), the underlying meaning is different as xt is conditionally independent given x0. In practice, one can also choose more complex non-Markovian process as presented in DDIM (Song et al., 2021), and we leave this exploration in future work. Additionally, we can show the following proposition: Proposition 1. non-Markovian diffusion process {xt}t with independent noising γt has bijection to Markovian diffusion process {yt}t with the same number of steps and noise level {αt}t; {yt}t achieves the maximal signal-to-noise ratio when its noise level satisfies = (cid:80)T . αt 1 αt γτ 1γτ τ =t We defer the proof to Appendix A. The above proposition indicates that we can carefully choose set of noise level γt to model the same information-destroying process as comparable diffusion baseline, while modeling with the entire generation trajectory. 1We follow the same notation of time indexing 1 . . . in diffusion models for consistency. 2The term non-Markovian refers to xt is not only related to the previous step xt1. 4 Preprint. Working in progress (a) (b) (c) Figure 3: Attention masks for (a) DART and (b) DART-AR, highlighting their different structures. (c) Comparison of learning curves, demonstrating the superior performance of DART-AR. 3.2 PROPOSED METHODS Denoising AutoRegressive Transformer (DART) We propose DART Transformer-based generative model that implements non-Markovian diffusion with an independent noising process (see Figure 2). First, following DiT (Peebles & Xie, 2022), we represent each image by first extracting the latent map with pretrained VAE (Rombach et al., 2021), patchify, and flatten the map into sequence of continuous tokens xt RKC, where is the length, and is the channel dimension. When considering multiple noise levels, we concat tokens along the length dimension. Then, DART models the generation as pθ(xt1xt:T ) = (cid:0)xt1; γt1xθ(xt:T ), (1 γt1)I(cid:1), where xθ(.) is Transformer network that takes in the concatenated sequence xt:T RK(T t)C, and predicts the mean of the next noisy image. By combining with Eq (6), we simplify Eq (5) as: min LDART θ = Ex1:T q(x0) ωt xθ(xt:T ) x02 2 , (7) (cid:35) (cid:34) (cid:88) t=1 where we define ωt = γt1 ωt to simplify the notation. Similar to standard AR models, training of 1γt1 denoising steps is in parallel, where computations across different steps are shared. chunk-based causal mask is used to maintain the autoregressive structure (see Figure 3(a)). It is evident from Eq (7) that the objective is similar to the original diffusion objective (Eq (2)), demonstrating that DART can be trained as robustly as standard diffusion models. Additionally, by leveraging the diffusion trajectory within an autoregressive framework, DART allows us to incorporate the proven design principles of large language models (Brown et al., 2020; Dubey et al., 2024). Furthermore, Proposition 1 indicates that we can select ωt according to its associated diffusion process. For instance, with SNR weighting (Ho et al., 2020), ωt can be defined as ωt = (cid:80)T . γτ 1γτ τ =t Sampling from DART is straightforward: we simply predict the mean xθ(xt:T ), add Gaussian noise to obtain the next step ˆxt1, and feed that to the following iteration. Unlike diffusion models, no complex solvers are needed. Similar to diffusion models, classifier-free guidance (CFG, Ho & Salimans, 2021) is applied to the prediction of xθ for improved visual quality. Additionally, KV-cache is employed to enhance decoding efficiency. Limitations of Naive DART Unlike diffusion models, which can be trained with large number of steps , non-Markovian modeling is constrained by memory consumption as increases. For example, using = 16 on 256 256 images will easily create over 4000 tokens even in the latent space. This fundamentally limits the modeling capacity on complex tasks such as text-to-image generation. However, the flexibility of the autoregressive structure allows us to enhance the capacity without compromising scalability. In this work, we propose two methods on top of DART: 1. DART with Token Autoregressive (DART-AR) As similarly discussed by Xiao et al. (2021), the independent Gaussian assumption of pθ(xt1xt:T ) is inaccurate to approximate the complex true 5 Preprint. Working in progress Figure 4: An illustration for the generation process of DART-FM. distribution of q(xt1xt:T ), especially when is small. straightforward solution to model image denoising as an additional autoregressive model pθ(xt1xt:T ) = (cid:81)K k=1 pθ(xk,t1x<k,t1, xt:T ): (cid:35) (cid:34) min LDART-AR θ = Ex1:T q(x0) xθ(x<k,t1, xt:T ) xk,02 2 , (8) (cid:88) (cid:88) ωt t=1 k= where x1,t1, . . . , xK,t1 are the flatten tokens of xt1. The autoregressive decomposition ensures each tokens are not independent, which is strictly stronger than the original DART. We demonstrate this by visualizing the training curve in Figure 3 (c). Training of DART-AR takes essentially the same amount of computation as standard DART with two additional modifications at the input and attention masks (see comparison in Figure 3 (a) (b)). At sampling time, DART-AR is relatively much more expensive as it requires AR steps before it outputs the final prediction. 2. DART with Flow Matching (DART-FM) The above approach models dependency across tokens, while maintaining Gaussian modeling at each step. Alternatively, we can improve the expressiveness of pθ(xt1xt:T ) by abandoning the Gaussian assumption, similar to Li et al. (2024). More precisely, we first sample xt as in regular DART. Then, we recursively apply continuous flow network, vϕ( xt, ct, τt), over multiple iterations to bridge the gap between xt and xt1 (see Figure 4). Here, vϕ( xt, ct, τ ) models the velocity field for the probabality flow between the distributions of xt and xt1, τ [0, 1] denotes the auxiliary flow timestep, and ct = cθ(xt:T ) represents the features from the last Transformer block, providing contextual information across the noisy image. Consequently, simple MLP suffices to model vϕ, adding only minimal overhead to the total training cost. We train vϕ via flow matching (Liu et al., 2022; Lipman et al., 2023; Albergo et al., 2023) due to its simplicity: min LFM ϕ,θ = Ex1:T q(x0) (cid:88) t= Eτ [0,1]vϕ((1 τ ) xt + τ xt1, ct, τ ) (xt1 xt)2 2, (9) γt1SG [xθ(xt:T )] + 1 γt1ϵ, and ϵ (0, I). SG [] is the stop-gradient operator where xt = to avoid trivial solutions in optimization. In practice, we combine Eq (9) with the original DART objectives, which can be seen as an additional refinement on top of the Gaussian-based prediction. 3.3 MULTI-RESOLUTION GENERATION DART (including both the -AR and -FM variants) is highly flexible framework that can be easily extended and applied in various scenarios with minimal changes in the formulation. For example, instead of learning fixed resolution of images, one can learn joint distribution of pθ({xi i=1) 0 RKiC is x0 with different resolution. Following the approaches proposed in Gu et al. where xi (2022; 2023); Zheng et al. (2023) for diffusion models, single DART referred to as MatryoshkaDART can model multiple resolutions by representing each image with its corresponding noisy sequence {xk }t separately, then flattening and concatenating these sequences for sequential prediction. As special case shown in Figure 5 (), we model the NOMAD objective in Eq (5) as 0}N max LMatryoshka θ = {{xi t}Ti t=0}N i=1q(x0) (cid:34) (cid:88) (cid:88) i= t=1 log pθ(xi ωi t1xi t:T , x<i (cid:35) 1:T ) , (10) where the above can be implemented using any DART variant. Note that xi on x<i 0 is not directly conditioned 0 , which not only avoids the need to handle shape changes at the boundaries but also mitigates 6 Preprint. Working in progress Figure 5: Illustrations of Matryoshka-DART () and Kaleido-DART (). By joint training models, DART can perform various applications such as multi-resolution and multi-modal generation. potential error propagation, common issue in learning cascaded diffusion (Ho et al., 2022). All low-resolution information is processed through self-attention. By this approach, the model can balance the number of noise levels with the total number of tokens to achieve better efficiency. Additionally, learning resolutions can be progressively increased by finetuning low-resolution models with extended sequences. 3.4 MULTI-MODAL GENERATION Our proposed framework, built on an autoregressive model, naturally extends to discrete token modeling tasks. This includes discrete latent modeling for image generation (Gu et al., 2024) and multi-modal generation (Team, 2024). By leveraging shared architecture, we jointly optimize for both continuous denoising and next-token prediction loss using cross-entropy for discrete latents which we call Kaleido-DART considering its architectural similarity as Gu et al. (2024) (see Figure 5). To balance inference across modalities, we reweight the discrete loss (Eq (3)) according to the relative lengths between the discrete and image tokens: LKaleido θ = λLCE θ + LDART θ , (11) where λ = # text tokens # image tokens . It is important to note that our approach is markedly distinct from several concurrent works aimed at unifying autoregressive and diffusion models within single parameter space (Zhou et al., 2024; Xie et al., 2024; Zhao et al., 2024; Xiao et al., 2024). In these efforts, the primary goal is to adapt language model architectures to perform diffusion tasks, without modifying the underlying diffusion process itself to account for the shift in model design. As we have discussed, the Markovian nature of diffusion models inherently limits their ability to leverage generation history, feature that lies at the core of autoregressive models. In contrast, DART is designed to merge the advantages of both autoregressive and diffusion frameworks, fully exploiting the autoregressive capabilities. This formulation also allows for seamless integration into LLM pipelines."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Dataset We experiment with DART on both class-conditioned image generation on ImageNet (Deng et al., 2009) and text-to-image generation on CC12M (Changpinyo et al., 2021), where each image is accompanied by descriptive caption. All images are center-cropped and resized. All models, except those used for Matryoshka-DART finetuning, are trained to synthesize images at 256 256 resolution, while the latter are trained at 512 512. For multimodal generation tasks, we augment CC12M with synthetic captions as the discrete ground-truth. Evaluation In line with prior works, we report Fréchet Inception Distance (FID) (Heusel et al., 2017) to quantify the the realism and diversity of generated images. For text-to-image generation, we also use the CLIP score (Hessel et al., 2021) to measure how well the generated images align with the given text instructions. To assess the zero-shot capabilities of the models, we report scores based on the MSCOCO 2017 (Lin et al., 2014) validation set. 7 Preprint. Working in progress Figure 6: Samples generated by text-to-image DART-FM at 256 256 and 512 152 pixels Architecture Following 3.2, we experimented with three variants of the proposed model: the default DART, along with two enhanced versions, DART-AR and DART-FM. All variants are implemented using the same Transformer blocks for consistency. As illustrated in Figure 2, our design is similar to Dubey et al. (2024), incorporating rotary positional encodings (RoPE, Su et al., 8 Preprint. Working in progress Figure 7: Comparison for class-conditioned image generation with DART and baselines trained on ImageNet. 2024) within the self-attention layers and SwiGLU activation (Shazeer, 2020) in the FFN layers. For class-conditioned generation, we follow (Peebles & Xie, 2022), adding an AdaLN block to each Transformer block to integrate class-label information. For text-to-image generation, we replace AdaLN with additional cross-attention layers over pretrained T5-XL encoder (Raffel et al., 2020). Since DART uses fixed noise schedule, there is no need for extra time embeddings as long as RoPE is active. In addition, for DART-FM, we incorporate small flow network, implemented as 3-layer MLP, which increases the total parameter count by only about 1%. Training & Inference Proposition 1 not only makes connection between DART and diffusion models, but also allows us to define the noise schedule based on any existing diffusion schedule {αt}t, which can be inversely mapped to the DART schedule {γt}t using the bijection. In this paper, we adopt the cosine schedule αt = cos (π/2 t/T ). We set = 16 while = 2563 throughout all experiments unless otherwise specified. We train all models with batch size of 128 images, resulting in total of 0.5M image tokens per update. For finetuning Matryoshka-DART, we additionally set the high-resolution part = 4 with = 1024. We use the AdamW optimizer (Loshchilov & Hutter, 2017) with cosine learning rate schedule, setting the maximum learning rate to 3e-4. During inference, we enable KV-cache with memory pre-allocated based on the total sequence length. DART performs the same number of steps for inference as in training. For the FM variants, we additionally apply 100 flow matching steps between each autoregressive step, following Li et al. (2024). More implementation details can be found in Appendix B. 4.2 RESULTS Class-conditioned Generation We report the FID scores for conditional ImageNet generation in Figure 8(a), following the approach of previous works. In line with Li et al. (2024), we apply linear CFG scheduler to DART and its variants. As shown in the figure, both -AR and -FM variants consistently outperform the default DART across all guidance scales, demonstrating the effectiveness 3K = 256 from encoding 256 256 images with StableDiffusion v1.4 VAE (https://huggingface. co/stabilityai/sd-vae-ft-ema) with patch size of 2 and = 16 channels. 9 Preprint. Working in progress (a) FID50K on ImageNet (b) FID30K on COCO (c) CLIP score on COCO Figure 8: Comparison of DART, DART-AR, DART-FM and baseline models with different CFG guidance scale on different benchmarks. * denotes models implemented and trained by us. Model Gflops Speed (s) 2.157 DART 2.112 DART-AR DART-FM 2.177 DiT (16) DiT (256) 2.073 32.384 0.075 7.44 0.32 0.065 0.84 (a) Inference speed & flops (b) Comparison of model sizes (c) Comparison of # of steps Figure 9: (a) Inference flops and speed (caulated as second per image) of different models. (b) Performance of DART of different sizes. (c) Effect of number of noise levels on DART. of the proposed improvement strategies. We also compare our methods with DiT (Peebles & Xie, 2022), using both 16 sampling steps (to match DART) and 250 steps (the suggested best setting). Notably, DART-AR achieves the best FID score of 3.98 among all variants and significantly surpasses DiT when using 16 steps, highlighting its advantage in leveraging generative trajectories, particularly when the number of sampling timesteps is limited. Not surprisingly, DiT with 250 steps performs better than DART. However, it is important to note that the official DiT model is trained for 7M steps, which is substantially more training iterations than those used for DART. Figure 7 also presents examples of generated results on ImageNet from all models using 16 sampling steps. DART-FM tends to produce sharper images with higher fidelity at higher CFG values. In contrast, DART-AR demonstrates an ability to generate more realistic samples at lower CFG values when compared to both the baselines and other variants. More samples are shown in Appendix C. Text-to-Image Generation To demonstrate the capability of DART at scale, we train the model for text-to-image generation. We also implement an in-house DiT with cross attention to text condition in comparison to our models, where we evaluate the performance on both 16 and 250 steps. As shown in Figure 8, while both -AR and -FM variants still show clear improvements against the default DART, FM achieves the best FID of 11.12, indicating its ability of handling diverse generation tasks. To validate the scalability of high-resolution image generation, we further finetune DART by jointly modeling 256256 and 512512 resolutions within single autoregressive process ( 3.3). Sampled results are shown in Figure 6 and Appendix C. Efficiency We compare both the actual inference speed (measured by wall-clock time with batch size 32 on single H100) as well as the theoretical computation (measured by GFlops) in 9(a). Since DART, DART-AR, DART-FM share the same encoder-decoder Transformer architecture, their flops are roughly the same. However, DART-AR has high wall clock inference time due to its large autoregressive steps, which have not been well parallelized in our current implementation. Integrated with recent advances in autoregressive LLMs, DART-AR can be deployed in more efficient and we leave this for future investigations. DART-FM also have inference time overhead due to the iterations of flow net in inference. Compared to DART, DiT has less flops for single pass. However, it requires sufficient many number of iterations to generate high quality samples. DART 10 Preprint. Working in progress Figure 10: Examples of multi-modal generation with Kaleido-DART. has comparable flops and inference time as DiT with 16 sampling timesteps, while DART achieves better performance than DiT (16) on ImageNet and COCO, showing the efficiency benefits of DART. Scalibility We show the scalability of our DART by training models of different sizes including small (S), base (B), large (L), and extra large (XL) on CC12M, where the configurations are listed in Appendix B.1. Figure 9(b) illustrates how the performance changes as model size increase. Across all the four models, CLIP score significantly improves by increasing the number of parameters in DART. Besides, the perform steadily increases as more training iterations are applied. This demonstrates that our proposed generative paradigm benefits from scaling as previous generative models like diffusion (Peebles & Xie, 2022) and autoregressive models (Li et al., 2024). Effects of noise levels. We experiment with different noise levels, where we vary the number of total noise levels in our denoising framework (Figure 9(c)). total of = 4 or 8 noise levels are trained in comparison with standard 16 noise levels. Not surprisingly, less noise levels lead to gradually degraded performance. However, with as few as 4 noise levels, DART can still generate plausible samples, indicating the capabilities of deploying DART in more efficient way. Variants in noise levels provides potential option for finding the optimal computation and performance trade-off especially when compute is limited. Multimodal Generation We showcase the capabilities of our proposed method in the joint generation of discrete text and continuous images, as introduced in 3.4. Figure 10 provides examples of multimodal generation using the Kaleido-DART framework. Given an input, the model generates rich descriptive texts along with corresponding realistic images, demonstrating its ability to produce diverse samples with intricate details. Notably, unlike Gu et al. (2024), our approach processes both text and images through the same model, utilizing unified mechanism to handle both modalities. This unified framework can potentially be integrated into any multimodal language models."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We presented DART, novel model that integrates autoregressive denoising with non-Markovian diffusion to improve the efficiency and scalability of image generation. By leveraging the full generation trajectory and incorporating token-level autoregression and flow matching, DART achieves competitive performance on class-conditioned and text-to-image tasks. This approach offers unified and flexible solution for high-quality visual synthesis. Our current model is restricted by the number of tokens in the denoising process. One direction is to explore more efficient architectures for long context modeling (Gu & Dao, 2023; Yan et al., 2024) which enables application to problems like video generations. Also, current work only conduct preliminary investigates on multimodal generation. Future work may train multimodal generative model based on the framework of the decoder-only language model to handle wide variety of problems with one unified model. 11 Preprint. Working in progress"
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Advances in neural information processing systems, 2020. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pp. 1691 1703. PMLR, 2020. Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. In International Conference on Learning Representations, 2021. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: Large-scale Hierarchical Image Database. IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255, 2009. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, and Josh Susskind. f-dm: multistage diffusion model via progressive signal transformation. arXiv preprint arXiv:2210.04955, 2022. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua Susskind, and Navdeep Jaitly. Matryoshka diffusion models. In The Twelfth International Conference on Learning Representations, 2023. Jiatao Gu, Ying Shen, Shuangfei Zhai, Yizhe Zhang, Navdeep Jaitly, and Joshua Susskind. Kaleido diffusion: Improving conditional diffusion models with autoregressive latent modeling. arXiv preprint arXiv:2405.21048, 2024. 12 Preprint. Working in progress Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: referencefree evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:471, 2022. Maciej Kilian, Varun Japan, and Luke Zettlemoyer. Computational tradeoffs in image synthesis: Diffusion, masked-token, and next-token prediction. arXiv preprint arXiv:2405.13218, 2024. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision, pp. 740755, 2014. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. ICML, 2016. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pp. 40554064. PMLR, 2018. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models, 2021. 13 Preprint. Working in progress Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 22562265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. Advances in Neural Information Processing Systems, pp. 59986008, 2017a. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz In Advances in neural information Kaiser, and Illia Polosukhin. Attention is all you need. processing systems, pp. 59986008, 2017b. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jing Nathan Yan, Jiatao Gu, and Alexander Rush. Diffusion models without attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82398249, 2024. 14 Preprint. Working in progress Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. Transactions on Machine Learning Research, 2022. Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, and Mingyuan Zhou. Learning stackable and skippable lego bricks for efficient, reconfigurable, and variable-resolution diffusion modeling. In The Twelfth International Conference on Learning Representations, 2023. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 15 Preprint. Working in progress PROOF OF PROPOSITION 1 Proof. Due to q(xtx0) = (xt; γtx0, (1 γt)I), we can write xt = γtx0 + 1 γtϵt. With certain coefficient {λt s}T s=t, let us define: yt (cid:88) s=t λt sxs, We study when the signal-to-noise ratio of yt achieves its maximal value: (cid:32) (cid:88) λt yt = (cid:33) γs x0 + (cid:88) s=t (cid:112)1 γsϵs λt SNR(yt) = s=t (cid:16)(cid:80)T (cid:17) s=t λt γs 2(1 γs) s=t λt (cid:80)T (cid:88) s=t γs 1 γs , (12) (13) (14) which follows from Titus lemma and the Cauchy-Schwarz inequality. It becomes an equality when: λt γs 1 γs , [t, ] (15) Next, we demonstrate that {yt}t follows Markov property when achieving the maximal signal-tonoise ratio for each yt. From Eq (15), let λt s=t ηs, we have: , ρt > 0, and ηt = γt 1γt , ηt = (cid:80)T = ρt γs 1γs (cid:32) yt = ρt ηtx0 + (cid:33) (cid:88) ηsϵs = ρt (cid:0)ηtx0 + s=t ηtϵ (cid:1) , where we use ϵ is variance preserving (VP). Next, let us assume (0, I) to equivalently simplify the noise term. When ρt = 1(cid:14)(cid:112)η + ηt, {yt}t ˆy = ρt+1 ηt+1 ρt ηt yt + σϵ, ϵ (0, I) = ρt+1 ηt+1x0 + ρt+1 ηt+1/ (cid:32) = ρt+1 ηt+1x0 + (cid:115) η2 t+1 ηt + ηtϵ σ2 ρ2 t+1 + σϵ (cid:33) ϵ , where we use ϵ to replace the noise term. So if we let ˆy match the distribution of yt+1, then (16) (17) (18) (19) (20) (21) (22) η2 t+1 ηt + = ηt+1 σ2 ρ2 t+1 ηt+1ηt ηt > 0 σ2 = ρ t+1 (cid:113) (cid:14)ηt, This implies that we can find an independent The above equation has root that σ = ρt+1 noise term added to yt to obtain yt+1, establishing that ytt constitutes Markovian forward process. (cid:18) ρt+1 ηt+1 ρt ηt p(yt+1yt) = ηt+1ηt ηt yt, ρ ηt+1ηt (23) t+1 (cid:19) Whats more, {xt}t sequence can be uniquely determined from {yt}t via (cid:18) yt ρt 1 γt γt (cid:19) 1 γt γt yt+1 ρt+1 if = T. if < xt = yt ρt , , Therefore, the two processes {xt}t and {yt}t has one-to-one correspondence. (24) Preprint. Working in progress"
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 ARCHITECTURE In this paper, we use encoder-decoder Transformer architecture (Vaswani et al., 2017a) to implement our DART model. The total number of parameters in our standard model is about 800M. For textto-image (T2I) generation, we employ pretrained Flan-T5-XL (Raffel et al., 2020; Chung et al., 2022) as the textual encoder in all experiments. During training, the encoder is frozen and only the decoder is trained. For class-conditioned (C2I) generation, the encoder is simply embedding layer, mapping class label into fixed length vector. Since we do not use text encoder for cross-attention, we pass context information through Adaptive LayerNorm. To maintain parity in the total number of parameters with text-to-image models, we set the hidden size to 1152. Below is the default configurations of DARTs. model config for DARTs: patch_size=2 hidden_size=1280 (T2I) or 1152 (C2I) num_layers=28 num_channels_per_head=64 use_swiglu_ffn=True use_rope=True rope_axes_dim=[16,24,24] use_per_head_rmsnorm=True use_adaln=False (T2I) or True (C2I) lm_feature_projected_channels=2048 Table 1 lists the configurations of different model sizes used in the scalability experiments of 4.2. For DART-FM, we implement the flow network as three MLPs (FFNs) with additional adaptive LayerNorm for modulation. Unlike standard FFN blocks in Transformers, the hidden size in our implementation remains unchanged, matching the hidden dimension of the main DART blocks. Table 1: Configurations of DART of different sizes. Model # Layers Hidden size # Heads # Params DART-S DART-B DART-L DART-XL 12 12 24 384 768 1024 1280 6 12 16 20 48M 141M 464M 812M In Matryoshka-DART upsample tuning, we follow the default setting and set the patch size as 2 for 512 512 images, which end up containing 1024 tokens each. In total, the model has 8192 tokens in total, including 4096 tokens for 256 256 images with 16 denoising steps and 4096 tokens for 512 512 images with 4 denoising steps. We also applies rotary positional embedding (RoPE) (Su et al., 2024) to embed the information of token position. We find it critical to spatially align the positional embedding of high resolution images with low resolution ones. In particular, we define the rotary matrix feature at position (i, j) in 256 256 image as R(i,j) Θ . For feature x(i,j) at position (i, j) in the corresponding high resolution image with upsample ratio r, the rotary matrix is given R(i/r,j/r) . In our case, the ratio is 2, which halves the position values on 512 512 images. Θ B.2 TRAINING In all the experiments, we share the following training configuration for our proposed DART. default training config: batch_size=128 optimizer=AdamW adam_beta1=0.9 17 Preprint. Working in progress adam_beta2=0.95 adam_eps=1e-8 learning_rate=3e-4 warmup_steps=10_000 weight_decay=0.01 gradient_clip_norm=2.0 ema_decay=0.9999 mixed_precision_training=bf16 B.3 PARAMETERIZATION OF DIFFUSION MODEL In our implementation, we employ v-prediction (Salimans & Ho, 2022) for improved performance. Namely, DART is trained to output vt = αtxtx . The prediction of the model vt is mapped to the clean image through = αtxt σt vt. The loss is then computed between and x, and the loss weighting is also applied here. σt"
        },
        {
            "title": "C ADDITIONAL SAMPLES",
            "content": "C.1 PROMPTS FOR FIGURE 1 (FROM LEFT TO RIGHT, TOP TO BOTTOM) Cinematic photo of fluffy baby Quokka with knitted hat eating large cup of popcorns, close up, studio lighting, screen reflecting in its eyes. 35mm photographs, film, bokeh, professional, 4k, highly detailed golden autumn scene, with the sun shining through canopy of orange and red leaves, path covered in fallen leaves, and soft light casting long shadows Detailed pen and ink drawing of the arrival of giant alien ship. diffusion cloud expanding in cosmic scene, with the words DART subtly emerging from the nebula, blending into the stars and dust polar bear made entirely of vanilla ice cream, standing on waffle cone iceberg with chocolate syrup dripping from its fur. lion made entirely out of swirling fire, standing majestically on rocky outcrop as the sun sets behind it. pig made out of vegetables At futuristic wedding ceremony, robot sharp and precise, holds the hand of the other robot. panda chef in professional kitchen, wearing chefs hat and apron, carefully preparing gourmet dish on the counter with steam rising from the pot. In the image, corgi dog is wearing Santa hat and is laying on fluffy rug. The dogs tongue is sticking out and it appears to be happy. There are two Christmas boxes and basket of toys nearby, indicating that the scene takes place during the winter season. The background features Christmas tree, further suggesting the holiday atmosphere. The image has warm and cozy feel to it, with the dog looking adorable in its hat and the boxes adding festive touch An astronaut standing on frozen alien planet, looking up at sky filled with two suns and swirling aurora, with ice-covered lighthouse standing tall on rocky shore, with massive waves crashing against the base, as the beam of light cuts through the night. scarecrow standing at the edge of forest in early winter, its clothes dusted with fresh snow, with crows circling above against cold, gray sky The phrase GenAI made of scattered coffee beans that form the letters, arranged on top of white kitchen table with coffee mug. An origami dragon made from rainbow-colored paper, soaring through sky filled with clouds, leaving trail of vibrant color behind it Photography closeup portrait of an adorable rusty broken down steampunk robot covered in budding vegetation, surrounded by tall grass, misty futuristic sci-fi forest environment 18 Preprint. Working in progress Figure 11: Visualization of the generation process (left: latent maps, right: decoded RGB images) for 256 256 (T = 16) and its upsampling to 512 512 (T = 4) using Matryoshka-DART. C.2 STEP-WISE GENERATIVE RESULTS Figure 11 visualizes the generative process of an image at resolution 256 256 and its upsampling to 512 512 by Matryoshka-DART. DART first iteratively refines the generative results for 16 denoising steps at resolution 256 256. It then upsamples images at resolution 512 512 through iterative denoising as well. Since the generation of high resolution is conditioned on the previous low-resolution samples, it only needs 4 denoising steps at high resolution to generate realistic images. C.3 ADDITIONAL SAMPLES ImageNet We here show more generative examples from DART variants trained on ImageNet 256 256 in Figure 12. The proposed two improvement methods, DART-AR and DART-FM, generate images with higher fidelity and more details when compared with origin DART. Text-to-Image We here show more text-to-image generative examples from DART-AR and DARTFM at resolution 256 256 in Figure 13. We also show examples at resolution 512 512 from Matryoshka-DART finetuned from DART-FM at 256 256 in Figures 14 and 15. 19 Preprint. Working in progress Figure 12: Uncurated samples from DART varints on ImageNet 256 256 with for labels of daisy, husky, coral reel, sulphur-crested cockatoo, cliff, espresso, axolotl, jay, hotdog, lionfish. Preprint. Working in progress Figure 13: Uncurated samples from DART varints on text-to-image generation at 256 256 pixels given various captions. 21 Preprint. Working in progress Figure 14: Uncurated samples from DART-FM with Matryoshka-DART fine-tuning on text-to-image generation at 512 512 pixels given various captions. Preprint. Working in progress Figure 15: Uncurated samples from DART-FM with Matryoshka-DART fine-tuning on text-to-image generation at 512 512 pixels given various captions."
        }
    ],
    "affiliations": [
        "Apple",
        "Mila",
        "The Chinese University of Hong Kong"
    ]
}