{
    "paper_title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
    "authors": [
        "Shuang Sun",
        "Huatong Song",
        "Lisheng Huang",
        "Jinhao Jiang",
        "Ran Le",
        "Zhihao Lv",
        "Zongchao Chen",
        "Yiwen Hu",
        "Wenyang Luo",
        "Wayne Xin Zhao",
        "Yang Song",
        "Hongteng Xu",
        "Tao Zhang",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 ] . [ 1 9 1 4 3 0 . 2 0 6 2 : r SWE-World: Building Software Engineering Agents in Docker-Free Environments Shuang Sun1, Huatong Song1, Lisheng Huang1, Jinhao Jiang1, Ran Le2, Zhihao Lv1, Zongchao Chen2, Yiwen Hu1, Wenyang Luo1, Wayne Xin Zhao1, Yang Song2, Hongteng Xu1, Tao Zhang2, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China. 2BOSS Zhipin, Beijing, China. sunshuang@ruc.edu.cn, batmanfly@gmail.com, songyang@kanzhun.com Abstract Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWEWorld, Docker-free framework that replaces physical execution environments with learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agentenvironment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agentenvironment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2% to 52.0% via Docker-free SFT, 55.0% with Docker-free RL, and 68.2% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World"
        },
        {
            "title": "Introduction",
            "content": "Recent years have seen rapid progress in software engineering (SWE) agents that leverage large language models (LLMs) [1], which are beginning to deliver practical value in real-world settings [2, 3]. Such agents typically operate in an iterative agentenvironment interaction loop driven by physical environment execution feedback [4, 5]. In SWE scenarios, this environment corresponds to an isolated, dependency-complete workspace instantiated from the target repository, most commonly realized via containerization frameworks such as Docker, where programs and unit tests can be executed reliably [6, 7]. Consequently, existing approaches to training and evaluating SWE agents highly depend on physical execution environments, which serve as central prerequisite for effective agent optimization [8]. In practice, execution feedback in SWE tasks encompasses both intermediate program behaviors (e.g., reproducing failures) and running unit tests for final evaluation. In particular, evaluation requires executing the agent-fixed repository and verifying that all designated tests pass. Compared to Equal contribution. Correspondence to Wayne Xin Zhao and Yang Song. lightweight execution settings that require only basic interpreter/runtime and limited dependencies (e.g., algorithmic programming tasks), SWE tasks necessitate full repository workspace together with dependency-complete environment, requiring environment setup, dependency resolution, and program execution, and thus making execution feedback substantially more resource-intensive to construct and maintain in practice [9]. To support such execution-based training, prior work has predominantly relied on Docker to instantiate task-specific environments and provide reliable execution feedback for supervised fine-tuning (SFT) or reinforcement learning (RL), enabling agents to improve their performance through multi-turn environment interaction [10, 11, 12]. Despite their effectiveness, Docker-based training paradigms introduce several fundamental scalability limitations for SWE agents. At high level, these limitations manifest across data, training, and test-time regimes. First, data scalability is constrained: many real-world repositories and pull requests cannot be readily leveraged, as complex or brittle dependency configurations often prevent projects from building or executing reliably within containerized environments [13]. Second, training scalability is hindered: the storage, management, and distribution of large numbers of Docker images impose substantial infrastructure overhead, which significantly complicates large-scale optimization, particularly reinforcement learning, in resource-constrained academic settings [11]. Third, test-time scalability is limited: because environment interactions are computationally expensive and often irreversible, it becomes difficult to fully exploit additional test-time computation through iterative exploration or trial-and-error strategies [14]. Motivated by recent advances in world modeling with LLMs [15, 16], we explore an alternative paradigm that replaces physical execution environments with learned models. This raises central question: can execution feedback traditionally obtained from Docker-based environments be approximated by LLMs, enabling Docker-free training and deployment of software engineering agents? If feasible, such an approach would fundamentally alleviate the scalability bottlenecks of Docker-centric SWE pipelines by decoupling agent optimization from costly environment instantiation. However, modeling execution at the repository level is inherently challenging: it requires reasoning over large, evolving codebases where localized edits may induce complex, non-local effects, and demands accurate prediction of diverse execution behaviors across heterogeneous projects. In this work, we address this challenge by training LLMs on real agentenvironment interaction data to predict execution outcomes, yielding learned surrogate environment that provides effective feedback for SWE agent training while overcoming the data, training, and test-time scalability constraints of Docker-based approaches. To this end, we propose SWE-World, Docker-free framework that replaces resource-intensive physical execution environments with learned surrogate for training SWE agents. The design of SWE-World is motivated by key observation: while dependency-complete, runnable environments required for program and test execution are costly to construct and maintain, many agent actionssuch as file navigation, text inspection, and code editing (e.g., ls, grep, vim)only involve lightweight file-system operations and incur negligible computational overhead. SWE-World explicitly separates these two classes of actions during the interaction loop. Lightweight operations are handled deterministically by sandbox that directly updates the repository state, whereas executionoriented commands that would traditionally require Docker are routed to an LLM-based transition model that predicts execution outcomes without instantiating runnable environments. By learning to approximate execution behavior observed in Docker, the transition model effectively replaces dependency-complete containers as the source of execution feedback, allowing agents to receive feedback without incurring the high resource cost of physical execution. Upon episode termination, i.e., when the agent submits its final code patch, an LLM-based reward model replaces containerized test execution by acting as virtual test runner that evaluates the patch and produces structured test feedback together with binary success signal. Together, these components form unified surrogate world for the agent, capturing both step-level dynamics and terminal outcomes, alleviating the scalability bottlenecks of resource-heavy execution. We conduct extensive experiments to evaluate the effectiveness of SWE-World as Docker-free training and inference environment. Using combination of open-source benchmarks and newly crawled tasks, we show that agents optimized entirely without physical execution environments achieve strong performance on SWE-bench Verified [3]. In particular, when trained with Docker-free execution feedback provided by SWE-World, Qwen2.5-Coder-32B improves from base resolve rate of 6.2% to 52.0%, and further to 55.0% with additional Docker-free RL optimization, while the smaller Qwen3-4B-Instruct reaches 25.6% and 30.0%, respectively. Beyond training, SWE-World 2 also enables effective Docker-free test-time scaling: by using the learned reward model to evaluate and select candidate solutions, Qwen2.5-Coder-32B attains resolve rate of 68.2% with TTS@8. These results demonstrate that SWE-World can match, and in some settings surpass the effectiveness of Docker-based pipelines, while drastically lowering the infrastructure and resource barrier of SWE experimentation, enabling large-scale SWE research and iteration without industrial-grade container infrastructure. Our main contributions are summarized as follows: Docker-Free SWE Environment: We propose SWE-World, Docker-free framework that replaces physical execution environments with learned surrogate for training and evaluating software engineering agents. Effective Agent Training without Execution: We show that LLM-based environment feedback can successfully support SFT and RL, achieving performance comparable to or better than training with real execution. Scalable Use of Open-Source SWE Data: By eliminating the requirement for buildable environments, SWE-World enables substantially broader utilization of real-world GitHub data for training software engineering agents."
        },
        {
            "title": "2 Related Work",
            "content": "Datasets for Software Engineering Tasks. The evaluation paradigm for large language models (LLMs) in software engineering has transitioned from isolated code generation tasks [17, 18] toward complex, repository-level issue resolution. SWE-bench [2] and its refined counterpart, SWE-benchverified [3], pioneered this shift by establishing rigorous benchmark for assessing the capacity of LLM-based code agents to resolve real-world software issues. Building upon this, several studies have focused on synthesizing issue-solving tasks from GitHub data to facilitate model training [6]. SWE-Gym [6] and SWE-rebench [7] directly harvest interactive SWE tasks from diverse GitHub repositories, while SWE-smith [8] introduces an automated pipeline for large-scale issue generation. However, significant bottleneck in these existing datasets is their heavy infrastructure dependency; each data sample typically necessitates dedicated Docker environment for execution and verification, incurring substantial computational and storage overhead [9]. In contrast, our approach streamlines this process by directly filtering issues from GitHub and utilizing only the underlying code repositories and unit tests for trajectory synthesis. By bypassing the requirement for per-sample containerization, our method significantly reduces the resource barrier while maintaining high fidelity in training data. Software Engineering LLMs and Agents. To unlock the potential of LLMs as autonomous software agents, researchers have introduced various agentic frameworks, such as SWE-agent [19] and OpenHands [4], which provide the necessary infrastructure for environment interaction. Current research in autonomous issue resolution generally follows two paradigms. The first is the agentbased approach, which situates models within authentic, sandboxed environments (e.g., Docker) to generate interaction trajectories and perform agentic SFT [20, 10, 13, 21] and RL [22, 11, 23, 24] training. While effective, this paradigm suffers from significant resource overhead, as each data sample typically demands dedicated container. The second is the agentless approach [25, 9], which decomposes software engineering tasks into structured, three-stage pipeline: fault localization, code repair, and patch verification [26, 27]. Although more efficient, these predefined workflows often restrict the agents capacity for autonomous exploration and adaptive reasoning. Diverging from these two paths, our method maintains an agentic loop to preserve exploratory autonomy but introduces an LLM-based environment simulator. By simulating environmental feedback rather than relying on heavy-weight containerization, our approach eliminates the infrastructure bottleneck of traditional agent-based systems while maintaining the flexibility of autonomous agents."
        },
        {
            "title": "3 Preliminaries",
            "content": "We study software engineering (SWE) issue resolution as an interactive process involving three components: (i) code agent that proposes edits and execution commands, (ii) codebase (repository 3 Figure 1: Overview of SWE-World. Left: We collect agent-Docker interaction data to train the SWE-World Transition Model (SWT) and SWE-World Reward Model (SWR). Middle: SWE-World forms Docker-free surrogate environment, enabling scalable agent enhancement via SFT, RL, and Test-Time Scaling. Right: Comparison of Code Agent trajectories generated based on Docker and SWE-World. workspace) to be modified, and (iii) an execution environment that provides step-level feedback and evaluates candidate fixes. SWE Task Instances. task instance is defined by an issue description and reference repository snapshot. We denote an instance as = (R, b, d, U), where is repository, is base commit, is the problem statement (e.g., bug report), and is the validation unit tests that must pass for correct fix. Additional instance metadata is provided in Appendix A. Interactive Repair Process. The agent operates in mutable workspace initialized at (R, b) and interacts with an execution environment E. At each step t, the agent produces thought zt and takes an action at; the environment executes at and returns step-level feedback yt: yt E(at; R, b, Pt). We represent code edits as patch (a set of line-level modifications to the codebase). repair episode terminates when the agent takes submit action, producing the final patch . complete trajectory is denoted as (1) τ = (cid:2)I, (z1, a1, y1), . . . , (zT , aT , yT ), (cid:3). (2) Evaluation. Given submitted patch , the environment evaluates the modified workspace by running the designated unit tests U. This yields final evaluation output yeval = test_report, E(P ; R, b, U), where test_report summarizes execution logs, and the binary reward {0, 1} follows the standard criterion: (3) = 1 all required unit tests in pass under P, (4) and = 0 otherwise."
        },
        {
            "title": "3.1 Execution Environments for SWE",
            "content": "A central challenge in SWE is the execution environment E: it must support both lightweight interactions (file navigation and editing) and heavyweight repository-specific execution (running programs and unit tests) [8]. To make this clear, we distinguish four increasingly capable layers of environment that often appear implicitly in prior work. File System. The repository workspacefiles and directories that the agent reads and modifiesis the mutable state to transform from (R, b) into correct fix. 4 Terminal. terminal provides generic interfaces (e.g., ls, grep, vim) and editing tools to navigate and modify the file system, but does not inherently provide repository-specific code execution. Sandbox. sandbox couples the file system with terminal to deterministically support navigation/editing for any repository. It typically cannot run repo-specific programs or tests due to missing dependencies and runtime setup. Docker. Docker instantiates dependency-complete environment for given repository snapshot, subsuming sandbox operations while enabling repo-specific code execution and test runs. However, building and maintaining Docker images is brittle in practice: different repositories require different installation procedures and dependency constraints, and large-scale SWE training requires spawning and managing many images and containers. As result, the storage, management, and distribution of Docker impose significant infrastructure overhead."
        },
        {
            "title": "3.2 From Docker-Based to LLM-Based Environments",
            "content": "The above decomposition suggests key observation: many agent actions in SWE only require the lightweight sandbox, while the main scalability bottleneck comes from repository-specific code execution traditionally provided by Docker. This motivates our core idea: replace the Docker execution component with learned models, while retaining deterministic sandbox for file operations. Concretely, we aim to construct surrogate environment Eworld that preserves the standard agent environment interface: the sandbox maintains the workspace state and supports navigation/editing actions, while LLMs approximate the execution feedback and test-based evaluation that would otherwise require Docker. By combining universal sandbox with LLM-based execution surrogates, we can eliminate the need for Docker-based environments during training and inference, substantially improving scalability. Details of the learned components are introduced in Section 4."
        },
        {
            "title": "4 SWE-World: LLM-Based Docker-Free Environment",
            "content": "We propose SWE-World, surrogate execution environment Eworld that approximates containerized runtimes by training LLMs to predict execution feedback from real agentenvironment interaction traces. By decoupling the agent from physical execution, SWE-World enables scalable training and inference without Docker."
        },
        {
            "title": "4.1 System Architecture",
            "content": "SWE-World replaces Docker with universal lightweight sandbox and learned LLM components. Given an agent action at, we categorize it into two types: Navigation & Editing: file exploration and edits via generic shell/tool commands (e.g., ls, cat, grep, view, create, str_replace). Code Execution: repository-specific execution commands whose outputs depend on runtime semantics (e.g., python reproduce.py, pytest). Navigation and editing actions are executed deterministically by lightweight Sandbox, which maintains the workspace state and returns step feedback yt. Code execution actions are handled by the learned SWE-World Transition Model (SWT), which predicts step-level execution feedback ˆyt. When the agent terminates by submitting final patch , the SWE-World Reward Model (SWR) acts as virtual test runner and produces test report together with the binary reward. To drive the learned models, we construct compact context κ that includes the instance information and the current workspace state; concrete context definitions are given below."
        },
        {
            "title": "4.1.1 Lightweight Sandbox for Navigation and Editing",
            "content": "The sandbox consists of the file system and terminal: it deterministically supports navigation and editing over the repository workspace. We delegate these operations to the sandbox (instead of an LLM) for two reasons: (1) Reliability: file operations are deterministic; LLM simulation may hallucinate files or contents and catastrophically mislead the agent. (2) Efficiency: navigation/editing is inexpensive and does not require semantic reasoning, making LLM inference unnecessary. As 5 result, the workspace state and the code content referenced by the learned models remain accurate and strictly consistent with the agents edit history."
        },
        {
            "title": "4.1.2 SWT: Step-Level Execution Feedback Simulation",
            "content": "For Code Execution actions, we employ the SWE-World Transition Model, denoted as MSWT. Modeling execution at the repository level is inherently challenging: it requires reasoning over large, evolving codebases where localized edits may induce complex, non-local effects, and demands accurate prediction of diverse execution behaviors across heterogeneous projects. Moreover, SWT must be sensitive to the agents incremental edits, producing meaningfully different feedback as the patch evolves. Transition Context. At step t, we construct an transition context κSWT consisting of three parts: Instance Meta Data: problem description, an initial analysis generated by powerful LLM (summarizing the core bug and intended fix), and ground-truth patch as an internal reference. This reference is strictly hidden from the agent to prevent leakage. Agent Patch: the current patch reflecting the agents modifications so far. Execution Content: the command (action at) to be simulated and the relevant code content needed to determine the runtime behavior. Given an context κSWT , SWT predicts step-level execution feedback: ˆyt = stdout, stderr, exit_code MSWT(κSWT ). (5) By conditioning on κSWT , SWT can produce realistic feedback such as error tracebacks and printed logs, enabling iterative debugging and refinement without Docker execution. See Appendix for transition context details. t"
        },
        {
            "title": "4.1.3 SWR: Test Report and Reward Generation for Evaluation Simulation",
            "content": "The SWE-World Reward Model, denoted as MSWR, validates the agents final submission. Prevalent approaches cast validation as generative classification task, prompting an LLM to map the agent trajectory directly to binary token (i.e., YES/NO) [28, 10]. During inference, they extract token log-probabilities lYES and lNO and compute scalar score via softmax: ˆr = exp(lYES) exp(lYES) + exp(lNO) . (6) This black-box formulation provides limited interpretability and can be sensitive to long, noisy trajectories. In contrast, SWR acts as virtual test runner, simulating the execution of the unit tests on the final submitted patch and generating structured test report before assigning the final binary reward. Moreover, SWR must faithfully account for complex test logic and aggregate outcomes across many test cases, where single failing case determines ˆr = 0. Evaluation Context. For trajectory τ , to drive this simulation, we construct an evaluation context κSWR . It contains the same three parts as the transition context (Instance Meta Data, Agent Patch, and τ Execution Content), and additionally includes the unit tests U: Fail to Pass (F2P): tests that reproduce the issue and must transition to passing. Pass to Pass (P2P): regression tests that must remain passing. In particular, for κSWR corresponds to the test command and the test-case content of U. Given κSWR , SWR predicts the evaluation output: τ τ , the Agent Patch is the final submitted patch , and the Execution Content ˆyeval = test_report, ˆr MSWR(κSWR τ ), ˆr {0, 1}. (7) By enforcing the generation of detailed test report prior to ˆr, SWR provides an interpretable verification signal that supports scalable Docker-free selection and optimization. See Appendix for evaluation context details."
        },
        {
            "title": "4.2 Training SWT and SWR",
            "content": "We train SWT and SWR based on Qwen2.5-Instruct-32B and Qwen2.5-Instruct-72B [29] via SFT, utilizing data collected directly from the interactions between SWE agents and real execution environments."
        },
        {
            "title": "4.2.1 Data Collection from Real Docker Rollouts",
            "content": "We construct our training corpus by generating trajectory rollouts on open-source SWE datasets [14, 6, 7] and corresponding Docker images. For each code execution step, we extract κSWT and the real execution feedback yt directly from the Docker containers output, yielding transition samples: Dtrans = {(κSWT For trajectory τ , at the terminal step, we extract κSWR outcome yeval, forming the reward dataset: execute the unit tests to obtain the evaluation , yt)}. (8) τ t Dreward = {(κSWR τ , yeval)}. (9) Crucially, this collection process achieves high data efficiency: single rollout simultaneously yields multiple interaction samples for supervising the environment models and an agent trajectory for policy training. Details are provided in Appendix B."
        },
        {
            "title": "4.2.2 Reverse-Reasoning Distillation for CoT Backfilling",
            "content": "Directly mapping complex repository contexts to precise execution feedback poses significant learning challenge. Chain-of-Thought (CoT) [30] reasoning mitigates this by providing intermediate logical steps, effectively breaking down the reasoning process and enhancing model learnability. Moreover, during inference, the explicit generation of CoT facilitates more granular analysis, leading to higher prediction accuracy [31, 32]. To obtain high-quality CoT data, we employ reverse-reasoning strategy [33, 34]. We feed both the input context κ and the ground-truth execution feedback yGT (from Docker) to powerful reasoning model, requiring it to produce strictly forward derivation without explicitly revealing the answer in the reasoning process. This model is prompted to generate step-by-step derivation CoT that reasons from the input to the specified output: CoT πteacher(Reasoning κ, yGT). (10) Before integration, we employ an LLM-as-a-Judge [35] to assess the quality of the generated CoTs, filtering out invalid reasoning, or CoTs that directly leak yGT. We backfill the high-quality thoughts into our dataset, formatting the target output by wrapping the thoughts within <think> tags followed immediately by the ground truth. Consequently, both SWT and SWR are trained to predict the sequence <think>CoT</think>yGT, enabling them to learn the underlying execution logic more effectively. Details are provided in Appendix B."
        },
        {
            "title": "5 Training SWE Agents with SWE-World",
            "content": "In this section, we leverage SWE-World to establish an end-to-end, fully Docker-free training pipeline for training SWE agents, enabling scalable data curation, supervised fine-tuning, and reinforcement learning."
        },
        {
            "title": "5.1 Software Eengineering Data Preparation",
            "content": "We construct unified instance pool from two sources. Open-Source SWE Datasets. We aggregate instances from open-source SWE datasets, including R2E-Gym, SWE-Gym, and SWE-rebench. We convert these instances into unified schema compatible with SWE-World. SWE-World Dataset. limitation of previous dataset construction pipelines is the heavy reliance on Docker environments; vast number of Pull Requests (PRs) and issues are typically discarded simply 7 because their repositories fail to build in container [8]. Leveraging SWE-Worlds ability to simulate execution without physical constraints, we bypass this bottleneck. We crawled fresh collection of PRs and issues from GitHub, applied deduplication against existing datasets, and parsed the relevant repository information. After applying heuristic filtering rules, we obtain SWE-World Dataset, curated set of 16.6K high-quality instances. As shown in Table 1, the SWE-World Dataset covers more tasks and far more repositories than prior SWE datasets, offering broader coverage and higher diversity. The detailed data processing pipeline is provided in the Appendix F. The final training set is mixture of these open-source and newly curated instances. Table 1: Comparison of SWE-World Dataset and other SWE datasets with Docker environments. Dataset SWE-rebench SWE-Gym R2E-Gym # Tasks # Repos Source 6.5k 2.4k 4.6k 1,429 11 10 3,763 Real Real Synth Real SWE-World Dataset 16.6k"
        },
        {
            "title": "5.2 Docker-Free Supervised Fine-tuning",
            "content": "We employ rejection sampling fine-tuning strategy to train the policy model. This process consists of three key steps: Trajectory Generation. For each training instance, we use powerful code agent [36, 37] to interact within the SWE-World environment (including the SWT and the Sandbox). This interaction yields agent trajectories efficiently and in parallel, bypassing the overhead of container management. Dual-Stage Filtering. To ensure the quality of the training data, we apply rigorous filtering mechanism to the generated agent trajectories: Rule-Based Filtering: We discard agent trajectories exhibiting invalid tool usage, execution timeouts, excessive context length, exceeding the maximum turn limit, or failure to produce final submission. SWR-Based Verification: We use the SWR to assess the correctness of the remaining agent trajectories. We retain only those receiving positive reward (ˆr = 1), which indicates that the agent has successfully resolved the issue according to the surrogate evaluation. Agentic Supervised Fine-Tuning. Finally, we use the high-quality agent trajectories to train our policy model (Qwen2.5-32B-Coder-Instruct [38] and Qwen3-4B-Instruct-2507 [39]). Following standard practices for agentic training, we perform agentic SFT [13] on the agents thoughts and actions. Crucially, the entire SFT pipeline is Docker-free. Training details are provided in Appendix C."
        },
        {
            "title": "5.3 Docker-Free Reinforcement Learning",
            "content": "SWE reinforcement learning [24, 22] is expensive and brittle due to its heavy reliance on Docker containers, since repeatedly spawning and managing containers for rollouts and validation consumes massive memory, CPU, and storage resources and often destabilizes the training infrastructure. SWERM [28] replaces part of the validation with learned verifier, but still relies on Docker containers for transition feedback during rollouts and mixes execution-based signals for final rewards, leaving Docker in the RL loop. In contrast, building on SWE-World, we achieve fully Docker-free RL. Agentic RL Loop with SWE-World. Starting from the SFT-trained model, we further optimize the agent with RL. We first launch SWT and SWR as inference services. During training rollouts, the agent interacts with SWE-World, receiving transition feedback from the SWT and Sandbox, while the SWR assigns the final trajectory reward. Crucially, this pipeline eliminates the need for Docker containers, requiring only the maintenance of inference servers for SWT and SWR models. 8 Optimization. We optimize the policy following the Group Relative Policy Optimization (GRPO) paradigm [40, 11]. Specifically, we employ stabilized variant that incorporates clipped ratio objectives, leave-one-out advantage estimation, and length normalization to effectively handle longhorizon reasoning tasks. By using SWT for transition feedback and SWR for terminal rewards, SWE-World supports stable, scalable, fully Docker-free RL for SWE agents. Training details are provided in Appendix D."
        },
        {
            "title": "5.4 Test-Time Scaling with SWR",
            "content": "We further improve inference performance via test-time scaling (TTS), where multiple candidate solutions are generated and verifier selects the best one. While prevalent verifiers rely on the singletoken generative classification discussed previously [14, 6], SWR enables verification grounded in simulated execution mechanics. SWR is explicitly designed to judge whether an agent trajectory has successfully resolved the issue, making it inherently suitable for TTS. Given that SWR outputs discrete binary reward, we employ multi-sample voting strategy to derive fine-grained confidence score. Specifically, for given instance, we sample candidate trajectories using the code agent. For each trajectory τ , we query SWR times to mitigate variance and calculate the mean reward: Score(τ ) ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) ˆri. (11) i=1 where test_report, ˆri Mreward(κSWR ) and κSWR denotes the evaluation context derived from the candidate trajectory τ . The trajectory with the highest average score is selected as the final submission. τ τ"
        },
        {
            "title": "6 Experiments",
            "content": "In this section, we first detail the training implementation for the SWE-World components (SWT and SWR) and SWE agents. We then present our evaluation results, ablation studies, and further analyses to validate our motivation and methodology."
        },
        {
            "title": "6.1 Experimental Settings",
            "content": "Agent Scaffolding. We build our agent on top of R2E-Gym framework [14], which is minimal scaffold adapted from OpenHands [4] and follows standard ReAct-style interaction loop [41]. The agent is equipped with three tools: str_replace_editor for file reading and editing, execute_bash for running shell commands, and submit tool for terminating the episode. We integrate SWE-World into the R2E-Gym framework execution backend, enabling the same scaffold to run either with Docker or with SWE-World as the environment. Evaluation Benchmark. We evaluate on SWE-bench Verified [2], curated split of 500 real-world GitHub issuePR tasks over 12 Python repositories. Performance is measured by resolve rate (%), i.e., the fraction of instances whose final patch passes all designated tests in the evaluation harness. SWT Evaluation. We evaluate SWT in an end-to-end setting by replacing Docker-based step feedback with SWT during rollouts, while keeping the code agent fixed (Minimax-M2.1 [42]). After each rollout terminates with submitted patch, we run the unit test in Docker to obtain the reward and report the resolve rate on SWE-bench Verified. SWR Evaluation. For SWR evaluation, we compute the Accuracy, Precision, Recall, and F1 by comparing the SWR-predicted rewards against the ground-truth rewards obtained via Docker-based test execution on held-out set of SWE-bench Verified trajectories. SWE Agents Evaluation. We evaluate SWE-World models on the SWE-bench Verified dataset, utilizing Docker execution backend for verification and benchmarking against the open-source code agents [38, 39, 6, 14, 43, 8, 22, 11, 13, 9, 44, 10, 26, 45, 46]. To prevent git hacking [47], we disallow solution-revealing git commands (e.g., git log, git show). Test-Time Scaling Setup. When using SWR for test-time scaling, we sample =8 candidate trajectories per instance and query SWR =3 times per candidate to estimate its expected reward; we report this setting as TTS@8."
        },
        {
            "title": "6.2 Main Results",
            "content": "Model/Method Scaffold Training Environment Resolve Rate (%) Qwen2.5-Coder-32B Qwen3-32B Qwen3-Coder-30B-A3B SWE-Gym-32B R2E-Gym-32B + TTS@16 Skywork-SWE-32B + TTS@ SWE-agent-LM-32B SWE-Fixer-72B SA-SWE-32B Llama3-SWE-RL-70B Lingma-SWE-GPT-72B DeepSWE-32B-Preview + TTS@16 Kimi-Dev-72B + TTS@40 SWE-Mirror-LM-32B FrogBoss-32B SWE-Lego-Qwen3-32B + TTS@16 SWE-World-4B-SFT SWE-World-4B-RL SWE-World-32B-SFT SWE-World-32B-RL + TTS@ OpenHands OpenHands OpenHands OpenHands R2E-Gym R2E-Gym OpenHands OpenHands SWE-agent Agentless OpenHands Agentless Agentless OpenHands OpenHands SWE-Agent Agentless MOpenHands SWE-Agent OpenHands OpenHands R2E-Gym R2E-Gym R2E-Gym R2E-Gym R2E-Gym - - - SFT SFT SFT SFT SFT SFT SFT RL SFT+RL SFT RL RL SFT+RL SFT+RL SFT SFT+RL SFT SFT SFT SFT+RL SFT SFT+RL SFT+RL Docker Docker Docker Docker Docker Docker Docker Docker Docker - Docker - - Docker Docker - - Docker Docker Docker Docker Sandbox + LLMs 6.2 23.2 51.6 20.6 34.4 49.4 38.0 47.0 40.2 32.8 39.4 41.0 30.2 42.2 59.0 48.6 60.4 52.2 54.6 52.6 58.8 25.6 30.0 52.0 55.0 68.2 Table 2: Performance of various models on SWE-Bench Verified. The best results are in bold and the second-best are underlined, excluding test-time scaling (TTS) results. Transition feedback Resolve Rate (%) Reward Simulation Docker (GT) Minmax-M2.1 GLM-4.7 SWT-32B SWT-72B 68.4 56.2 59.4 55.2 60.2 12.2% 9.0% 13.2% 8.2% Docker (GT) Minmax-M2.1 GLM-4.7 SWR-32B SWR-72B Acc. 1.00 0.740 0.768 0.754 0.770 Prec. Recall 1.00 0.709 0. 0.779 0.780 1.00 0.891 0.836 0.770 0.807 F1 1. 0.790 0.798 0.774 0.794 Table 3: SWE-bench Verified performance using different transition-feedback providers. Table 4: Performance of reward simulation against Docker ground truth. Performance of SWE Agents. As shown in Table 2, our method reaches the frontier performance among open-source 32B models. Starting from the Qwen2.5-Coder backbone (6.2%), SFT and RL boost performance to 52.0% and 54.8% respectively. Crucially, our purely Docker-free models significantly outperform baselines trained with real Docker execution (e.g., FrogBoss-32B at 54.6%), validating the efficacy of our pipeline. With SWR-based TTS@8, performance peaks at 68.2%, surpassing the previous best (Kimi-Dev-72B TTS@40) by large margin. SFT data source Total #Traj Resolve Rate (%) Docker (baseline) SWE-World SWE-World + Docker 5.7K 5.7K 9.3K 51.4 52.2 53.8 Table 5: SFT performance comparison using trajectories collected from Docker, SWE-World, and their mixture, under identical training settings. Effectiveness of SWT. While performance gap between simulation and reality is inevitable, SWT72B minimizes this degradation most effectively. It supports resolve rate of 60.2% for Minmax M2.1  (Table 3)  , outperforming general-purpose LLMs like GLM-4.7 (59.4%) and Minimax-M2.1 (56.2%) as more faithful environmental surrogate. Effectiveness of SWR. Table 4 validates the robustness of our reward models. SWR-32B achieves an accuracy of 0.754, surpassing Minmax-M2.1, and notably outperforms both Minmax-M2.1 and GLM4.7 in Precision (0.779). Scaling up, SWR-72B further advances performance, overtaking GLM-4.7 in both Accuracy (0.770) and Precision (0.780), thereby demonstrating the strongest comprehensive capability in reward simulation."
        },
        {
            "title": "6.3 Ablation Study",
            "content": "We investigate whether replacing the ground-truth Docker environment with SWE-World affects the quality of training data. We generated two parallel datasets of 5.7K trajectories using identical expert agents and prompt settings, differing only in the execution backend used during the rollout phase. As shown in Table 5, the model fine-tuned on SWE-World trajectories achieves resolve rate of 52.2%, slightly outperforming the model trained on Docker-generated data (51.4%). Moreover, starting from the 5.7K SWE-World trajectories, we deduplicate an additional Docker-collected set and retain 3.6K unique trajectories; mixing them yields 9.3K trajectories in total and further improves performance to 53.8%. This result suggests that our simulated environment provides supervision quality comparable to, or even better than, the physical environment. Furthermore, mixing in Docker-based trajectories yields additional gains, confirming that SWE-World is highly effective, scalable alternative for data generation."
        },
        {
            "title": "7 Further Analysis",
            "content": "This section provides further analyses on the impact of CoT (Section 7.1), RL training dynamics (Section 7.2), test-time scaling behavior (Section 7.3), and qualitative fidelity of SWT/SWR simulation (Section 7.4), to clarify the key factors behind SWE-Worlds effectiveness. 7.1 Impact of Chain-of-Thought Method Data Size Resolve Rate (%) Method Data Size Acc. Prec. Recall F1 w/o CoT w. CoT 26K 26K 55.2 56.0 w/o CoT w. CoT 4K 4K 0.578 0.609 0.704 0.653 0.712 0.754 0.704 0.728 Table 6: CoT yields only marginal improvement for SWT. Table 7: CoT substantially improves SWR reward prediction quality. We observe distinct asymmetry in the benefits of Chain-of-Thought (CoT) reasoning between the transition and reward models: Marginal Gain for SWT. As shown in Table 6, adding CoT to the transition model yields only negligible improvement (0.8%) in the downstream agents resolve rate. We attribute this to the informational robustness of the transition task. The primary role of SWT is to simulate textual feedback (i.e., stdout, stderr, exit_code). Small inconsistencies in such feedback are often tolerable: the agent can still interpret the signal, continue exploring, and correct itself via subsequent 11 Figure 2: RL training dynamics using SWT-32B. Orange lines denote average reward; Green dashed lines denote mean interaction turns. Left: Main experiment using CoT-enhanced SWR-32B shows stable learning. Right: Comparison with non-CoT SWR-32B leads to trajectory length collapse, indicating reward hacking. Figure 3: Test-time scaling on SWE-bench Verified: comparing SWR-32B with prior verifiers. actions and verification. Since CoT can substantially increase inference latency, the modest fidelity gain is typically not worth the added cost for step-level simulation. Significant Gain for SWR. In contrast, Table 7 shows that CoT is critical for the reward model, boosting accuracy by over 13% (0.578 0.712) and Precision significantly. This is due to the binary sensitivity of the reward task. The SWR must synthesize complex test execution report into strictly binary signal (Success/Failure). minor misinterpretation of test report can flip the reward label. CoT encourages more complete reasoning over repository context, patch effects, and test logic, thereby improving the fidelity of the predicted test report and making the resulting reward assignment substantially more accurate. We further analyze how this CoT integration impacts the stability of RL training in Section 7.2."
        },
        {
            "title": "7.2 RL Dynamics",
            "content": "We analyze the stability of our reinforcement learning process by tracking the reward and interaction turns over training steps. Figure 2 contrasts our main RL training with comparison run under an alternative reward model setting. Main RL Training. In the main run (Left), the agent exhibits healthy optimization dynamics: the reward steadily increases while the interaction length shows gentle, efficiency-driven decline. This suggests that the policy performance improves steadily over training, without collapsing into degenerate behaviors. 12 Figure 4: Qualitative fidelity of SWE-World simulation. Comparison Run. In contrast, the comparison run (Right), which uses non-CoT SWR, exhibits clear signs of reward hacking. The trajectory length collapses drastically after step 20, as the policy learns to exploit the reward models low precision by submitting short, invalid solutions that are mistakenly flagged as correct. These results suggest that robust and faithful reward signal is critical for stable Docker-free reinforcement learning, and CoT provides an effective way to enhance it."
        },
        {
            "title": "7.3 Test-Time Scaling",
            "content": "Figure 3 compares test-time scaling on SWE-bench Verified using agent rollouts generated by SWT-32B, where we rank candidate trajectories with verifier and select the best submission. Superior Scaling Performance. As shown in Figure 3, SWR-32B delivers strong gains under TTS: performance increases from 55.0 at K=1 to 68.2 at TTS@8, an absolute improvement of 13.2%, and substantially outperforms both R2E-Gym-Verifier-14B (59.4 at TTS@8) and OpenHands-32BVerifier (59.6 at TTS@8). Notably, the gap between SWR-32B and the Pass@K (Optimal) upper bound is much smaller than that of prior verifiers, indicating stronger reward modeling and more effective ranking signal. Moreover, SWR-32B improves monotonically from = 1 to = 8, indicating promising scaling behavior. Generative Verification vs. Regression. Beyond the final score, the scaling trend reveals fundamental difference in signal fidelity. SWR-32B improves steadily from = 1 to = 8, whereas R2E-Gym-Verifier-14B plateaus early (around TTS@5) and yields little benefit thereafter; OpenHands-32B-Verifier is even less stable, showing noticeable fluctuations after K3. This contrast highlights the advantage of our generative simulation paradigm over token-level scoring. By functioning as virtual test runner that explicitly predicts structured test report based on the evaluation context, SWR grounds its judgment in fine-grained execution logic, enabling it to robustly distinguish truly correct fixes from near-misses even within large candidate pool. In contrast, baselines that bypass this reasoning to derive scores directly from trajectory-level YES/NO log-probabilities (via softmax) are highly susceptible to the noise inherent in long interaction histories. Lacking explicit verification steps, these black-box signals lose discrimination power as the candidate set grows, leading to early saturation and unstable scaling."
        },
        {
            "title": "7.4 Qualitative Fidelity of SWT and SWR Simulation",
            "content": "We qualitatively validate the fidelity of SWE-World feedback by comparing Docker ground-truth outputs with SWT/SWR predictions under the same context (Figure 4). For readability, we omit lengthy fields in the input context and only present the essential information: (i) the command (action) to be simulated for SWT, and (ii) the test command (action) together with the F2P/P2P test sets for SWR. 13 SWT: Faithful Step-Level Execution Feedback. In the SWT example, the agent runs python reproduce_issue.py to reproduce Django issue. The simulated stdout matches the real output almost line-by-line, capturing the printed field requirements, the failure mode, and the assertion error message and exit status. It also preserves the scripts structure, indicating that SWT emulates the causal consequence of the current code state under the given command rather than producing generic-looking feedback. SWR: Semantically Consistent Test-Based Verification. For SWR, we compare real test execution with SWRs predicted report on SymPy instance. There are some formatting differences between the real test report and the output generated by SWR, as the training data adopts unified template that produces standardized pytest-style reports. Despite the format shift, the semantics align: SWR correctly predicts that all collected tests pass and explicitly lists key ones, including the F2P target and the P2P regression set. Non-essential artifacts (e.g., version strings and wall-clock time) may differ, but the core verification signal is preserved. Overall, these examples suggest that SWE-World produces realistic and trustworthy feedback: SWT closely matches real step outputs, and SWR provides content-faithful test-based verification under standardized report format."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we propose SWE-World, Docker-free framework (i.e., execution-free) for training code agents. By training and leveraging repository-level environment and reward simulation models (i.e., SWT and SWR), we establish fully Docker-free pipeline for data synthesis, supervised finetuning, and reinforcement learning. This approach effectively circumvents the deployment complexity and concurrency bottlenecks inherent in traditional Docker-based trajectory generation. Evaluation results on the SWE-bench-Verified dataset demonstrate that agents trained within this framework achieve performance comparable to those trained with ground-truth execution feedback. Furthermore, the SWR model proves highly effective for test-time scaling, providing grounded verification mechanism to select superior solutions without the need to execute actual unit tests. Additionally, by eliminating the strict dependency on buildable environments, SWE-World unlocks vast amounts of previously inaccessible open-source data resources, such as unbuildable Pull Requests and issues."
        },
        {
            "title": "References",
            "content": "[1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. [2] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. [3] Neil Chowdhury, James Aung, Chan Jun Shern, Oliver Jaffe, Dane Sherburn, Giulio Starace, Evan Mays, Rachel Dias, Marwan Aljubeh, Mia Glaese, Carlos E. Jimenez, John Yang, Leyton Ho, Tejal Patwardhan, Kevin Liu, and Aleksander Madry. Introducing swe-bench verified, August 2024. [4] Xingyao Wang, Simon Rosenberg, Juan Michelini, Calvin Smith, Hoang Tran, Engel Nyst, Rohit Malhotra, Xuhui Zhou, Valerie Chen, Robert Brennan, et al. The openhands software agent sdk: composable and extensible foundation for production agents. arXiv preprint arXiv:2511.03690, 2025. [5] SWE agent Team. Mini-swe-agent. https://github.com/SWE-agent/Mini-SWE-Agent, 2024. [6] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net, 2025. 14 [7] Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. Swerebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents, 2025. [8] John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025. [9] Zonghan Yang, Shengjie Wang, Kelin Fu, Wenyang He, Weimin Xiong, Yibo Liu, Yibo Miao, Bofei Gao, Yejie Wang, Yingwei Ma, et al. Kimi-dev: Agentless training as skill prior for swe-agents. arXiv preprint arXiv:2509.23045, 2025. [10] Chaofan Tao, Jierun Chen, Yuxin Jiang, Kaiqi Kou, Shaowei Wang, Ruoyu Wang, Xiaohui Li, Sidi Yang, Yiming Du, Jianbo Dai, et al. Swe-lego: Pushing the limits of supervised fine-tuning for software issue resolving. arXiv preprint arXiv:2601.01426, 2026. [11] Michael Luo, Naman Jain, Jaskirat Singh, Sijun Tan, Ameen Patel, Qingyang Wu, Alpay Ariyak, Colin Cai, Tarun Venkat, Shang Zhu, Ben Athiwaratkun, Manan Roongta, Ce Zhang, Li Erran Li, Raluca Ada Popa, Koushik Sen, and Ion Stoica. Deepswe: Training fully open-sourced, state-of-the-art coding agent by scaling rl. https://www.together.ai/blog/deepswe, 7 2025. Blog post. [12] Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, et al. davinci-dev: Agent-native mid-training for software engineering. arXiv preprint arXiv:2601.18418, 2026. [13] Junhao Wang, Daoguang Zan, Shulin Xin, Siyao Liu, Yurong Wu, and Kai Shen. Swemirror: Scaling issue-resolving datasets by mirroring issues across repositories. arXiv preprint arXiv:2509.08724, 2025. [14] Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2egym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint arXiv:2504.07164, 2025. [15] Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. [16] Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. Mcp-universe: Benchmarking large language models with real-world model context protocol servers. arXiv preprint arXiv:2508.14704, 2025. [17] Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [18] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [19] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [20] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis, 2025. [21] Shukai Liu, Jian Yang, Bo Jiang, Yizhi Li, Jinyang Guo, Xianglong Liu, and Bryan Dai. Context as tool: Context management for long-horizon swe-agents. arXiv preprint arXiv:2512.22087, 2025. [22] Shiyi Cao, Dacheng Li, Fangzhou Zhao, Shuo Yuan, Sumanth Hegde, Connor Chen, Charlie Ruan, Tyler Griggs, Shu Liu, Eric Tang, et al. Skyrl-agent: Efficient rl training for multi-turn llm agent. arXiv preprint arXiv:2511.16108, 2025. 15 [23] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [24] Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, et al. Training long-context, multi-turn software engineering agents with reinforcement learning. arXiv preprint arXiv:2508.03501, 2025. [25] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents, 2024. [26] Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. [27] Zhenyu He, Qingping Yang, Wei Sheng, Xiaojian Zhong, Kechi Zhang, Chenxin An, Wenlei Shi, Tianle Cai, Di He, Jiaze Chen, and Jingjing Xu. Swe-swiss: multi-task fine-tuning and rl recipe for high-performance issue resolution. https://github.com/zhenyuhe00/ SWE-Swiss, 2025. Notion Blog. [28] KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He, et al. Swe-rm: Execution-free feedback for software engineering agents. arXiv preprint arXiv:2512.21919, 2025. [29] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. [30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [31] Yu Huang, Zixin Wen, Aarti Singh, Yuejie Chi, and Yuxin Chen. Transformers provably learn chain-of-thought reasoning with length generalization. arXiv preprint arXiv:2511.07378, 2025. [32] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [33] Justin Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, et al. Reverse thinking makes llms stronger reasoners. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 86118630, 2025. [34] Honglin Lin, Qizhi Pei, Xin Gao, Zhuoshi Pan, Yu Li, Juntao Li, Conghui He, and Lijun Wu. Scaling code-assisted chain-of-thoughts and instructions for model reasoning. arXiv preprint arXiv:2510.04081, 2025. [35] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. The Innovation, 2024. [36] Z.ai. Glm-4.6: Advanced agentic, reasoning and coding capabilities. https://z.ai/blog/ glm-4.6, 2025. 2025-09-30. [37] MiniMax. Minimax m2 & agent: Ingenious in simplicity. https://www.minimax.io/news/ minimax-m2, 2025. 2025-10-27. [38] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [39] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, 16 Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. [40] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. [42] MiniMax. and with strong https://www.minimaxi.com/news/ m21-multilingual-and-multi-task-coding-with-strong-general, 2026. 2025-0104. M2.1: generalization."
        },
        {
            "title": "Multilingual",
            "content": "multi-task coding [43] Liang Zeng, Yongcong Li, Yuzhen Xiao, Changshi Li, Chris Yuhao Liu, Rui Yan, Tianwen Wei, Jujie He, Xuchen Song, Yang Liu, et al. Skywork-swe: Unveiling data scaling laws for software engineering in llms. arXiv preprint arXiv:2506.19290, 2025. [44] Atharv Sonwane, Isadora White, Hyunji Lee, Matheus Pereira, Lucas Caccia, Minseon Kim, Zhengyan Shi, Chinmay Singh, Alessandro Sordoni, Marc-Alexandre Côté, et al. Bugpilot: Complex bug generation for efficient learning of swe skills. arXiv preprint arXiv:2510.19898, 2025. [45] Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622, 2024. [46] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. [47] Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, et al. Mimo-v2-flash technical report. arXiv preprint arXiv:2601.02780, 2026."
        },
        {
            "title": "A Instance Metadata and Context Fields",
            "content": "This section details the instance-level metadata used throughout the paper, including the task definition = (R, b, d, U) and the fields involved in constructing SWT/SWR contexts. A."
        },
        {
            "title": "Instance Metadata Schema",
            "content": "Key Description repo instance_id base_commit hints_text problem_statement FAIL_TO_PASS PASS_TO_PASS gold_patch test_patch Repository name from which the task is sourced. unique identifier constructed from the repository and its PR/issue ID. Commit hash of the reference repository snapshot used to instantiate the task. Optional natural-language hints that guide the intended fix. Natural-language specification of the desired change (bug report / feature request). Unit tests expected to flip from failing to passing after correct fix (F2P). Regression unit tests that must remain passing after the fix (P2P). Reference solution in .patch format, used internally for supervision and analysis. .patch-format string that adds hidden/unseen evaluation tests. Table 8: Instance metadata fields used to define = (R, b, d, U) and to construct SWT/SWR contexts. Table 8 summarizes the key metadata fields defining each SWE task instance and used to construct the SWT and SWR contexts. A.2 Instance Meta Data in SWT/SWR Contexts Initial Analysis Generation. The Initial Analysis used in both SWT and SWR contexts is generated from the instance metadata using GLM-4.6. This analysis is designed to help SWT and SWR quickly grasp the core issue and the expected fix direction. The prompt template is shown in Figure G.1. SWT Context Meta Data. At step t, SWT takes context κSWT . Its Instance Meta Data includes: (i) the problem statement problem_statement (d), (ii) an Initial Analysis summarizing the failure behavior, likely root cause, and intended fix, and (iii) the reference solution gold_patch as an internal patch for comparison, which is never exposed to the code agent. In addition, κSWT contains the agents current patch Pt and the execution content needed to simulate the command at. The prompt template for SWT is shown in Figure G.2. SWR Context Meta Data. SWR operates on an evaluation context κSWR eval , which follows the same three-part structure as κSWT (Instance Meta Data, Agent Patch, and Execution Content) and additionally includes the unit tests U, consisting of FAIL_TO_PASS and PASS_TO_PASS. For κSWR eval , the Agent Patch corresponds to the final submitted patch , and the Execution Content corresponds to the unit-test command together with the test content in U. The prompt template for SWR is shown in Figure G.3 t"
        },
        {
            "title": "B Training Details for SWT and SWR",
            "content": "B.1 Detailed Training Setup Data Collection. We collect training data using the R2E-Gym framework, with at most 100 interaction turns per rollout. Our task pool is built upon three open-source SWE datasets: SWE-Gym [6], SWErebench [7], and R2E-Gym. We use powerful LLMs (GLM-4.6 and MinMax-M2; temperature 0.7) to interact with Docker-based environments and record real step-level execution outputs, final unit test reports and rewards. Importantly, we use both successful and failed rollouts: regardless of whether the final reward is 0 or 1, trajectory is useful as long as we can extract the corresponding context and the ground-truth"
        },
        {
            "title": "Global Batch Size Max Len",
            "content": "LR"
        },
        {
            "title": "Scheduler Warmup Epochs",
            "content": "SWT-32B (non-CoT) SWT-32B (CoT) SWR-32B (CoT) 512 512 512 98304 98304 98304 4105 4105 3105 cosine cosine cosine 0.1 0.1 0. 4 4 2 Table 9: Key SFT hyperparameters for SWT/SWR training. Docker outputs for supervision. We additionally filter out trajectories whose Docker environments are corrupted (e.g., broken images, dependency failures, or abnormal logs) that produce invalid or inconsistent outputs. For SWR data, we enforce balanced label distribution by subsampling to achieve 1:1 ratio between reward-0 and reward-1 examples. Chain-of-Thought (CoT) Augmentation. To improve reasoning fidelity, we augment both SWT and SWR training outputs with chain-of-thought generated by Qwen3-235B-A22B-Thinking (prompt shown in Figure G.4). For each example, we produce two versions of supervision: (i) non-CoT version with only the final structured output, and (ii) CoT version that prepends reasoning trace wrapped by <think> . . . </think>. After augmentation, we obtain training datasets: SWT: 26K non-CoT + 26K CoT; SWR: 21K non-CoT + 21K CoT. Models and Baselines. We train SWT and SWR using Qwen2.5-32B-Instruct and Qwen2.5-72BInstruct. As prompt-engineering baselines for both SWT and SWR, we adopt powerful open models (MinMax-M2.1 and GLM-4.7) to directly generate simulated feedback via carefully designed prompts. Prompt Templates and Output Formats. We format each training example into unified instructionfollowing template with explicit input fields. The SWT and SWR input prompt templates are shown in Figure G.2 and Figure G.3, respectively. For outputs, we enforce strict JSON format to facilitate reliable parsing: SWT: {\"stdout\": ..., \"stderr\": SWR: {\"test_report\": ..., \"reward\": ...} ..., \"exit_code\": ...} For CoT, we prepend the reasoning trace before the JSON: <think> ... </think> {...}. This design allows us to extract structured fields (e.g., stdout/exit_code or reward) without ambiguity. B.2 SFT Hyperparameters Table 9 summarizes the main SFT hyperparameters extracted from our training commands. All runs use AdamW and DeepSpeed ZeRO-3, with BF16, FlashAttention, ring attention, and gradient checkpointing enabled. Final Trained Models. Considering resource constraints and training time, we train the following models used in the main paper: SWT-32B (non-CoT; trained on 26K training examples), SWT32B-CoT (CoT; trained on 26K training examples), SWT-72B-CoT (CoT; trained on 26K training examples), SWR-32B (CoT; trained on 21K training examples), SWR-72B (CoT; trained on 21K training examples). Training Details for Docker-Free SFT with SWE-World Data Collection with SWE-World Rollouts. We collect training data by rolling out code agents in Docker-free setting using the R2E-Gym framework, with at most 100 interaction turns per rollout. The rollout task pool includes three public SWE datasetsR2E-Gym, SWE-Gym, and SWErebenchtogether with our SWE-World dataset. During rollouts, we replace the Docker backend with SWE-World: SWT provides step-level feedback, and SWR assigns the trajectory-level reward after termination. For favorable efficiencyaccuracy trade-off, we use SWT-32B and SWR-32B with 128K context window, using sampling temperature of 0. We drive rollouts with powerful LLM agents (GLM-4.6 and MinMax-M2; temperature 0.7) to generate diverse high-quality interaction traces. 19 Trajectory Filtering. We apply format-based filtering to ensure training stability. We keep only successful trajectories (reward = 1), and drop outliers that exceed 80K tokens or 100 turns to reduce OOM risks. We further discard trajectories containing malformed actions (e.g., unparsable tool/function calls or invalid repeated invocations). After filtering, we obtain 5.7K SWE-World-only trajectories for Docker-free SFT. Backbones. We fine-tune two policy backbones: Qwen2.5-32B-Coder-Instruct and Qwen3-4BInstruct-2507. SFT Hyperparameters. We train for 5 epochs with maximum context length of 80K tokens and global batch size of 256. We use AdamW with cosine learning-rate schedule, warming up for 10% of training and decaying the learning rate from 5 105 to 5 106. Training Details for Docker-Free Agent RL with SWE-World Task Pool. We conduct reinforcement learning on unified task pool constructed from three opensource SWE datasets: R2E-Gym, SWE-Gym, and SWE-rebench. Execution-Free Rollouts with SWE-World. All RL rollouts are generated in the R2E-Gym framework with the Docker backend fully replaced by the SWE-World backend. Concretely, we use SWT-32B to provide step-level execution feedback and SWR-32B to assign the terminal unit-test reward after the agent submits patch. Both SWT and SWR operate with 128K context window, and we sample both models with temperature of 0. Policy Initialization. The RL policies are initialized from our Docker-free SFT checkpoints: SWEWorld-4B-SFT and SWE-World-32B-SFT. Optimization with GRPO++. We optimize the policy using GRPO++. GRPO++ is stabilized variant of group-based policy optimization that uses clipped ratio objective with leave-one-out advantages and length normalization. For completeness, we provide the objective and related definitions below. (θ) = {oi}G i=1πold(q) (cid:34) 1 G (cid:88) i=1 1 Lmax oi (cid:88) t=1 (cid:32) min πθ(oi,t q, oi,<t) πold(oi,t q, oi,<t) (cid:32) Ri 1 1 (cid:88) (cid:33) , Rj clip (cid:18) πθ(oi,t q, oi,<t) πold(oi,t q, oi,<t) , 1 εlow, 1 + εhigh Ri (cid:19) (cid:32) 1 1 (cid:88) j=1 j=i j=1 j=i (cid:33)(cid:33)(cid:35) Rj , (12) where Lmax is fixed constant. Return Definition with SWR. Let ˆri {0, 1} denote the terminal reward predicted by SWR for rollout oi. We define the scalar return Ri as Ri = (cid:26) ˆri, if the rollout ends with submit, α ˆri, otherwise, α = 0.5. (13) RL Hyperparameters and Rollout Budget. We train with constant learning rate of 1106 and optimize on batches of 32 problems per update. For each problem, we sample 4 rollouts in parallel with policy sampling temperature of 1.0. To control cost and stabilize long-context training, each trajectory is capped at 150 interaction turns, uses maximum context length of 108K tokens during training, and is subject to per-trajectory timeout of 5,400 seconds."
        },
        {
            "title": "E Additional Evaluation Configurations",
            "content": "This section provides the concrete inference configurations used in our evaluations. 20 SWT/SWR/TTS Inference Settings. For SWT Evaluation, SWR Evaluation, and Test-Time Scaling, we use maximum context length of 128K tokens and set the sampling temperature to 0 for deterministic model outputs. SWE Agent Evaluation Settings. For SWE agent evaluation, we run the agent with sampling temperature of 0.7, maximum context length of 128K tokens, and maximum of 150 interaction turns. All final correctness judgments are obtained by running the submitted patch in the Docker evaluation harness. SWE-World Dataset Construction and Statistics We construct SWE-World dataset by extracting Python Issue-PR pairs from the GitHub Archive covering the period from 2010 to 2025. To ensure the semantic richness and quality required for downstream tasks, we apply rigorous filtering pipeline. F.1 Data Selection Heuristics We first utilize regex matching to eliminate bot-generated noise. We retain issues that contain bugrelated keywords and meet specific length constraints: titles must exceed 20 characters and issue bodies must exceed 200 characters. Additionally, we require at least three high-quality community comments to ensure sufficient context. For the linked Pull Requests (PRs), we apply the following structural constraints to ensure the tasks are solvable yet non-trivial: File Count: The number of modified files ranges from 1 to 20. Code Churn: The total code churn (additions + deletions) is between 1 and 2,000 lines. Patch Size: The maximum patch length is limited to 10,000 characters. After filtering, we deduplicate the remaining instances against existing open-source SWE datasets. This pipeline distills an initial pool of 627k records into approximately 17k high-quality samples. F.2 Dataset Statistics Table 10 presents the detailed statistics of our collected dataset, including the distribution of patch sizes and unit test breakdown. Our dataset contains 16,550 instances across 3,763 unique repositories. On average, each solution patch modifies 1.55 files and 18.76 lines of code. regarding verification, each instance includes an average of 1.98 FAIL_TO_PASS tests (verification tests) and 42.11 PASS_TO_PASS tests (regression tests). Table 11 compares our dataset with other standard benchmarks in the field. Our dataset provides significantly larger scale of training instances while maintaining reasonable complexity in terms of modified files and lines. Table 10: Detailed statistics of the collected dataset, focusing on the solution patches (Gold Patch) and evaluation unit tests. Metric Total Count Avg. per Instance General Statistics Total Samples Unique Repositories Fix Patch Statistics Lines Edited (Churn) Files Edited Unit Test Statistics Fail to Pass (F2P) Pass to Pass (P2P) 16,550 3,763 310,544 25,703 32,819 696, 21 - - 18.76 1.55 1.98 42.11 Table 11: Comparison of our dataset with related SWE datasets. We exclude execution environment details for brevity. Dataset # Tasks # Repos Source R2E R2E-gym (Subset) SWE-bench-extra SWE-bench-train SWE-fixer SWE-gym SWE-smith 0.25k 4.6k 6.38k 19k 115k 2.4k 50k 137 10 2k 37 856 11 SWE-World 16.55k 3,763 Synth Synth Real Real Real Real Both Real"
        },
        {
            "title": "Prompt for Initial Analysis",
            "content": "System Prompt: You are senior software engineer and bug-fixing expert. Your task: given bug report, human discussion, repository name, code patch, test patch, and lists of FAIL_TO_PASS / PASS_TO_PASS tests, produce *concise initial technical analysis* of the problem and the fix. **Goals of the analysis:** - Identify the core problem / bug being fixed. - Explain the key symptoms or incorrect behavior. - Describe which part of the codebase (modules / functions) is conceptually responsible. - Summarize the essence of the fix: what is changed and why it fixes the bug. - Mention how the tests (FAIL_TO_PASS / PASS_TO_PASS) relate to the fix: what behavior they verify. - Call out any subtle constraints / corner cases that are important. **Style & format requirements:** - Output MUST be plain text in English. - Use 5-10 short bullet points (markdown \"- \" style). - Each bullet should be one or two sentences, focused and technical. - Do NOT repeat the raw diff or test lists; summarize their intent. - Be specific but not verbose. User Prompt: ### Repository {repo} ### Problem Statement {problem_statement} ### Human Discussion / Hints {hints_text} ### Gold Patch (code fix) diff {gold_patch} ### Test Patch (changes to tests) diff {test_patch} ### FAIL_TO_PASS (should be failing before, passing after the fix) {f2p} ### PASS_TO_PASS (should keep passing after the fix) {p2p} ### Your Task Produce concise *initial technical analysis* that captures: * what the core bug is, * what behavior is wrong, * what this patch is fundamentally doing to fix it, * which areas of the code are conceptually involved, * and how the tests validate the fix. Remember: 5-10 bullet points, markdown - bullets, plain English, no extra commentary."
        },
        {
            "title": "Prompt for SWT",
            "content": "System Prompt: You are an expert Python code execution simulator and world-class software engineer. Your task is to predict the output of given Python command within specific code repository context. Analyze all the provided information: the initial analysis, the problem description, human hints, the agents current changes, the ideal \"gold\" solution, and the original content of the modified files. Your output MUST be single JSON object containing stdout, stderr, and exit_code. Do not add any explanations or text outside of this JSON block. ### Key Information You Must Use: 1. **Initial Analysis of the Problem**: This section contains core analysis of the issue, including the description of the error behavior, the core bug, how the issue manifests, and the intended fix. It is crucial to understanding the problem and will guide the simulation process. Use this to help you quickly identify the issue and how it should be addressed. 2. **Problem Description**: This section describes the specific issue the agent is currently working on and trying to fix. Use this to understand the exact problem the agent is attempting to resolve. 3. **Command to Simulate**: This is the command that will be executed. It contains all the information about what needs to be run, including the files to be executed and any other relevant details. Use this to simulate the execution and generate the correct output. If the \"Content of Code to be Executed\" is empty, the code is embedded within the command itself. 4. **Content of Code to be Executed**: This is the actual code that will be executed. Pay close attention to this content as the simulated output must 23 strictly correspond to the code being executed. If this section is empty, the specific code to execute is provided in the \"Command to Simulate\" section. 5. **Agents Current Code Modifications (Patch)**: This section highlights the changes that the agent has made to the codebase. These changes are the ones you need to analyze carefully to simulate the feedback. Focus on these modifications when generating your simulated output. 6. **Gold Standard Patch (For Your Reference)**: This is the correct solution to the issue. You should compare the agents current changes with the gold standard solution to ensure that the simulated result is as accurate as possible. If the agents patch is **functionally equivalent** to the gold patch (i.e., it resolves the issue in the same way), then the simulation feedback should match the expected output as defined by the gold standard. ### Your Task: - Use all the above information to generate the most realistic and accurate simulated output. - For commands that reproduce errors (e.g., python reproduce_issue.py), refer to the **Initial Analysis of the Problem** and the **Problem Description** to understand the nature of the error. Then, simulate the result based on the actual code content and the problem analysis. - For test commands (e.g., commands that include pytest), carefully compare the agents modifications with the gold standard patch. If the tests are \" FAIL_TO_PASS\" tests, analyze whether the agents changes fix the issue as described in the tests. For \"PASS_TO_PASS\" tests, ensure that the agents changes do not break existing functionality. - It is important to note that the execution result must strictly follow the current code content being executed, and no fabricated test output should be added. The same test case may have multiple versions, and the test content may change across versions. Therefore, each case should be analyzed specifically based on its content. - Your simulated output should reflect the most likely and realistic results of the command execution based on the context provided. Be precise and clear in your simulated outputs, focusing on realistic error messages, test outputs, or successful execution results. ### Format of the Output: - **stdout**: The standard output of the command, if applicable. For example, in the case of test run, this should reflect the results of the tests, such as which tests passed or failed. - **stderr**: Any error messages that might be produced by the command. For example, if the command produces syntax error or another exception, this should contain the appropriate traceback or error message. - **exit_code**: The exit code of the command. 0 indicates success, while 1 (or any other non-zero value) indicates failure. ### Example Scenarios: - **Successful Command Execution**: If the command executes successfully, stdout should contain the output generated by executing the code, and exit_code should be 0. Ensure that the output corresponds directly to the expected result of the executed code. - **Runtime Error (e.g., SyntaxError)**: If there is runtime error, such as syntax error, the stderr should contain the error traceback, and exit_code should be 1. - **Test Failures (pytest)**: If test fails, ensure the simulated output includes the failure details (e.g., pytest output) and an appropriate exit_code. Be sure to use all the context provided, including any discrepancies between the agents modifications and the gold standard patch, to generate the most accurate simulated output. 24 User Prompt: ### 1. Initial Analysis of the Problem {init_analysis} ### 2. Problem Description {problem_statement} ### 3. Command to Simulate bash {command_to_simulate} ### 4. Content of Code to be Executed {execution_code} ### 5. Agents Current Code Modifications (Patch) diff {agent_patch} ### 6. Gold Standard Patch (For Your Reference) diff {gold_patch} ### YOUR TASK Based on all the context above, provide the simulated output for the given command. Your response must be only the JSON object, with no other text."
        },
        {
            "title": "Prompt for SWR",
            "content": "System Prompt: You are an expert software engineering test runner and evaluator. Your task is to simulate running Python test command inside code repository, and then: 1. reasoning section enclosed within <think> and </think> tags that contains your full internal reasoning process. 2. Produce realistic test report summarizing which tests pass or fail. 3. Decide final reward value based on the status of specific tests. ### Key Information You Must Use: 1. **Initial Analysis of the Problem**: This section contains core analysis of the issue, including the description of the error behavior, the core bug, how the issue manifests, and the intended fix. It is crucial to understanding the problem and will guide the simulation process. Use this to help you quickly identify the issue and how it should be addressed. 2. **Problem Description**: This section describes the specific issue the agent is currently working on and trying to fix. Use this to understand the exact problem the agent is attempting to resolve. 3. **Command to Simulate**: This is the command that will be executed. It contains all the information about what needs to be run, including the files to be executed and any other relevant details. Use this to simulate the execution and generate the correct output. If the \"Content of Code to be Executed\" is empty, the code is embedded within the command itself. 4. **Content of Code to be Executed**: This is the actual code that will be executed. Pay close attention to this content as the simulated output must strictly correspond to the code being executed. If this section is empty, 25 the specific code to execute is provided in the \"Command to Simulate\" section. 5. **Agents Current Code Modifications (Patch)**: This section highlights the changes that the agent has made to the codebase. These changes are the ones you need to analyze carefully to simulate the feedback. Focus on these modifications when generating your simulated output. 6. **Gold Standard Patch (For Your Reference)**: This is the correct solution to the issue. You should compare the agents current changes with the gold standard solution to ensure that the simulated result is as accurate as possible. If the agents patch is **functionally equivalent** to the gold patch (i.e., it resolves the issue in the same way), then the simulation feedback should match the expected output as defined by the gold standard. 7. **FAIL_TO_PASS Tests**: list of tests that were failing before but are expected to **pass** after the correct fix. For the reward to be 1, **every ** test in this list must pass. 8. **PASS_TO_PASS Tests**: list of regression tests that were already passing and must **remain passing** after the fix. For the reward to be 1, **every** test in this list must pass. ### Your Task Given all of the above context: * Simulate the execution of the command under the current agent patch. * Focus especially on the tests in FAIL_TO_PASS and PASS_TO_PASS: - If **all** FAIL_TO_PASS tests pass and **all** PASS_TO_PASS tests pass, then the reward **must be 1**. - If **any** FAIL_TO_PASS test fails or errors, the reward **must be 0**. - If **any** PASS_TO_PASS test fails or errors, the reward **must be 0**. ### Output Format - **Reasoning (<think> block)**: First, provide your detailed internal reasoning, analysis, and step-by-step thought process enclosed within < think> and </think> tags. This section explains how you arrived at the simulated result. - single JSON object with the following keys: - **\"test_report\"**: concise textual description of the simulated test results. Mention which FAIL_TO_PASS and PASS_TO_PASS tests pass or fail, and any important errors. - **\"reward\"**: An integer, either 0 or 1, following the rules above. The final structure MUST look conceptually like this: <think> ...your full reasoning process here... </think> {\"test_report\": \"...\", \"reward\": 0/1} Be sure to use all the context provided, including any discrepancies between the agents modifications and the gold standard patch, to generate the most accurate simulated output. User Prompt: ### 1. Initial Analysis of the Problem {init_analysis} ### 2. Problem Description {problem_statement} ### 3. Command to Simulate bash {command_to_simulate} 26 ### 4. Content of Code to be Executed {execution_code} ### 5. Agents Current Code Modifications (Patch) diff {agent_patch} ### 6. Gold Standard Patch (For Your Reference) diff {gold_patch} ### 7. FAIL_TO_PASS Tests (Must All Pass for reward=1) {f2p} ### 8. PASS_TO_PASS Tests (Must All Pass for reward=1) {p2p} ### YOUR TASK Using all the context above, simulate running the given command, focusing on the behavior of the FAIL_TO_PASS and PASS_TO_PASS tests. Then: * Produce concise test report summarizing which tests pass or fail. * Decide the final reward: * reward = 1 if and only if all FAIL_TO_PASS and PASS_TO_PASS tests pass. * reward = 0 otherwise. Your answer must include <think>...</think> block containing your full reasoning process, followed immediately by single valid JSON object with keys \"test_report\" and \"reward\"."
        },
        {
            "title": "Prompt for Reverse CoT",
            "content": "You are an expert software engineering test runner simulator and world-class SWE bug-fix evaluator. In this task, the TRUE test execution outcome (a JSON containing the final test_report and reward) is already provided in the input. You MUST copy that JSON exactly at the end of your answer. However, your reasoning MUST be written as if you do NOT know the true outcome. You must only use the provided context (analysis, problem statement, command, code, patches, test lists, etc.) to do forward simulation that *could have produced* the true outcome. The input context you receive is organized into multiple sections. Each section has specific meaning and purpose: 1. Initial Analysis of the Problem: - What it is: high-level technical analysis of the bug, including the observed error behavior, core bug, and intended fix. - How to use it: Use this to quickly understand what is broken, what the correct behavior should be, and what the fix is aiming to change. 2. Problem Description: - What it is: more concrete description of the specific issue the agent is currently working on (bug report, task description, or failing scenario). - How to use it: Use this to understand the exact wrong behavior being observed and the expected correct behavior. 27 3. Command to Simulate: - What it is: The exact command that will be executed (for example, python script invocation or pytest command). - How to use it: Use this to determine the entry point, what tests or scripts are run, and how execution will proceed (including arguments, options, and test discovery). 4. Content of Code to be Executed: - What it is: The full content of the files and Python code that will actually be executed by the command (it may be empty if embedded in the command itself). - How to use it: Use this as the ground truth for runtime semantics, carefully simulating how this code will behave when run. 5. Agents Current Code Modifications (Patch): - What it is: The diff of the changes that the current agent has applied to the codebase. - How to use it: Use this to understand exactly what logic changed, which modules/functions are affected, and how control flow or data handling is now different. 6. Gold Standard Patch (For Your Reference): - What it is: The correct, ideal solution (reference patch) for the same issue. - How to use it: Use this to compare with the agents patch, see whether they are functionally equivalent or where they differ, and reason about possible behavioral differences. 7. FAIL_TO_PASS Tests: - What it is: list of tests that were failing before but are expected to pass after correct fix. - How to use it: Use these to judge whether the patch fixes the targeted bug: if any FAIL_TO_PASS test fails, the reward must be 0. 8. PASS_TO_PASS Tests: - What it is: list of regression tests that were already passing and must remain passing after the fix. - How to use it: Use these to check for regressions: if any PASS_TO_PASS test fails, the reward must be 0. 9. True Execution Result (JSON): - What it is: The real outcome of running the command in real docker environment, containing test_report and reward. - How to use it: You must copy this JSON exactly at the end of your answer, but you must NOT use it to guide, justify, or leak into your reasoning. Your reasoning must be written as if you have not seen this result. --- ### Your Task A) Write detailed forward-simulation reasoning trace that: - Starts from the context and code semantics. - Compares agent patch vs gold patch to judge whether they are functionally equivalent. - Predicts (as expectations) which FAIL_TO_PASS and PASS_TO_PASS tests should pass/fail and why. - For the PASS_TO_PASS test cases, you should carefully analyze and simulate whether the current agent patch would break any existing functionality in the repository, and use that to determine whether the tests would pass. 28 - For the FAIL_TO_PASS test cases, you should carefully analyze and simulate whether the current agent patch can fix the repositorys existing issue, and use that to determine whether the tests would pass. - This simulation should behave like code interpreter: walk through the execution path line by line (or block by block), follow the key execution logic, and produce complete, detailed narrative of the simulated run. - Based on the analysis, produce the test report for the PASS_TO_PASS and FAIL_TO_PASS tests. If all of them pass, the reward is 1; otherwise, it is 0. B) CRITICAL CAUSALITY REQUIREMENT (NO LEAKING / NO BACKWARD EXPLANATION) - Your reasoning MUST NOT mention that you have been given the true result. - Your reasoning MUST NOT quote, paraphrase, or reference any concrete lines from the true test_report. - Do NOT write post-hoc explanations like \"we see in the output, therefore ...\". - If you mention outcomes, phrase them as expectations derived from the code, e.g.: \"This test is expected to pass because...\", \"It likely fails due to...\". - Do NOT copy any distinctive strings from the True Execution Result into your reasoning. C) After you finish the reasoning, append the provided True Execution Result JSON EXACTLY as-is. - Do not reformat it, do not add keys, do not change whitespace, do not add extra fields. - Do not add any extra commentary after the JSON. --- ### Output Format You must output exactly: <sim_reasoning> ...plain-text forward simulation reasoning... </sim_reasoning> {PASTE_THE_PROVIDED_TRUE_EXECUTION_RESULT_JSON_HERE_EXACTLY} Rules: - Inside <sim_reasoning>: plain text only (no markdown headings, no code fences) . - After </sim_reasoning>: immediately paste the JSON exactly. - Do not output anything else. ### 1. Initial Analysis of the Problem {initial_analysis} ### 2. Problem Description {problem_statement} ### 3. Command to Simulate bash {command_to_simulate} ### 4. Content of Code to be Executed {exec_code_block_str} ### 5. Agents Current Code Modifications (Patch) diff {agent_patch} ### 6. Gold Standard Patch (For Your Reference) diff {gold_patch} ### 7. FAIL_TO_PASS Tests (Must All Pass for reward=1) {f2p} ### 8. PASS_TO_PASS Tests (Must All Pass for reward=1) {p2p} ### 9. True Execution Result (JSON) [DO NOT USE THIS IN REASONING; ONLY COPY AT THE END] {true_execution_result_json} ### YOUR TASK Write the response in the STRICT output format required by the system prompt. Remember: * Reasoning must be forward simulation based only on context. * Do NOT reference/quote the true result in reasoning. * Append the true JSON exactly after </sim_reasoning>."
        }
    ],
    "affiliations": [
        "BOSS Zhipin, Beijing, China",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}