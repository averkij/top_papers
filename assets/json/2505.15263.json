{
    "paper_title": "gen2seg: Generative Models Enable Generalizable Instance Segmentation",
    "authors": [
        "Om Khangaonkar",
        "Hamed Pirsiavash"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 3 6 2 5 1 . 5 0 5 2 : r GEN2SEG: Generative Models Enable Generalizable Instance Segmentation Om Khangaonkar Hamed Pirsiavash"
        },
        {
            "title": "UC DAVIS",
            "content": "reachomk.github.io/gen2seg"
        },
        {
            "title": "Abstract",
            "content": "By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAEs ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website."
        },
        {
            "title": "Introduction",
            "content": "Generative models, particularly diffusion models [56, 23], have recently revolutionized image synthesis, achieving unprecedented levels of fidelity and diversity. This success has been driven by substantial industry investments and advances in model architecture, training strategies, and data availability. Concurrently, scene understanding tasks, such as segmentation or parsing object relationships, remain fundamental challenges in computer vision with widespread application across disciplines. Traditionally, generative modeling and scene understanding have been treated as distinct problems. However, inspired by [28, 40, 57], we ask if the powerful representations learned by generative models can also be leveraged to improve image understanding. Large pretrained diffusion models, such as Stable Diffusion [52], have demonstrated the ability to capture intricate visual structures, enabling them to generate highly detailed and semantically coherent images. These models learn to map noisy inputs to clean, high-quality images through iterative refinement, implicitly acquiring an understanding of object boundaries, textures, and spatial relationships. We hypothesize that such models do not merely memorize pixel arrangements but also encode latent structure that organizes pixels into distinct object instances. This implicit grouping is essential for generating realistic compositions but remains unobservable when evaluating generative performance alone. We first explore finetuning pretrained diffusion models to segment object instances. Inspired by recent work [15], assuming latent diffusion model with time steps (0 . . . 1), we adapt it for single-step prediction by fixing = 1. We finetune the model end-to-end without noise. Since our loss is defined in pixel space, we decode the U-nets output with the frozen VAE, and Preprint. Under review. Figure 1: The model that generated the segmentation maps above has never seen masks of humans, animals, or anything remotely similar. We fine-tune generative models for instance segmentation using synthetic dataset that contains only labeled masks of indoor furnishings and cars. Despite never seeing masks for many object types and image styles present in the visual world, our models are able to generalize effectively. They also learn to accurately segment fine details, occluded objects, and ambiguous boundaries. directly optimize the decoded images pixels. This approach yields one-step, deterministic image segmenter that leverages Stable Diffusions large-scale generative pretraining. To tune our model, we design our instance coloring loss to encourage pixels belonging to the same object instance to share similar colors while encouraging distinct colors for different instances. Notably, we finetune using mixture of two realistic synthetic datasets that contain only labels of indoor furnishings (i.e beds, tables, chairs) [51] and cars [4], but our model still generalizes to unseen object types and image styles. This suggests that the grouping mechanism learned during pretraining is not only robust but also transferable. Crucially, our findings indicate that this capability arises not from Stable Diffusions internet-scale pretraining, but from the broader ability of generative models to understand pixel relationships while synthesizing realistic images. To validate this, we finetune Masked Autoencoder [21] (MAE) encoder-decoder pair pretrained solely on the ImageNet-1K [12] dataset for masked token reconstruction with our segmentation loss and dataset. Like Stable Diffusion, it generalizes zero-shot to object types outside our finetuning dataset. Surprisingly, it also generalizes beyond the pretraining distribution to art, luggage x-rays, mythical creatures, and more. Specifically, generalizing to \"unseen\" object types and styles means our models were never exposed to their segmentation masks. While our models may have seen images of them during pretraining (or via text conditioning, for Stable Diffusion), no segmentation supervision was provided. This highlights the remarkable ability of our models to generate high-quality masks for novel object types. MAE, in particular, was pretrained only on unlabeled ImageNet-1K, which almost certainly lacks any images from domains like impressionist art or luggage X-rays, demonstrating true zero-shot generalization as it never saw similar pixels or masks in any stage of training. This suggests learning to generate image pixels inherently leads to acquiring object instance groupings, reinforcing the idea that generative models can serve dual purposes beyond image synthesis. Our results open new avenues for leveraging the massive investment in generative AI for tasks that go beyond visual content creation. By demonstrating that generative models inherently learn transferable representations for object segmentation, we provide compelling case for their broader application in real-world perception. We hope utilizing the generative representations learned from image synthesis can pave the way for more efficient, generalizable, and data-efficient segmentation methods, enabling advances in fields where detailed scene understanding is critical, such as robotics, medical imaging, and autonomous systems. 2 Input (cid:122) DINO-B MAE-B Ours (cid:125)(cid:124) MAE-H SD-Limited (cid:123) SD SAM e O R H e S R Figure 2: To showcase the potential of generative models for instance segmentation, we highlight an example from each evaluation dataset where most or all of our models outperform SAM, despite never having seen masks of these object types. SAM often fails on fine structures (wires) or ambiguous boundaries (horses & carriage), leaving black regions where no object was detected. DINO-B also performs poorly, suggesting that generative pretraining (e.g., MAE, Stable Diffusion) learns strong priors for perceptual grouping. This is not meant to suggest that our method is inherently superior to SAM, which is trained on an annotated dataset several orders of magnitude larger. Instead, there are simply some objects which generative models are naturally better posed to segment."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Generative Models for Perception Generative models, originally developed for image synthesis, have increasingly been adapted for perception tasks in computer vision. longstanding viewpoint in the field (dating back to Hintons early work) [22] posits that learning to generate data can aid in recognizing it. Early work on GANs [17] evaluated whether representations learned by generating images [48] or videos [60] transferred well to image classification or action recognition, respectively, but performance was always far below discriminatively pretrained models. Concurrent works also utilized inpainting [45] and colorization [31, 65, 66] as pretext tasks for representation learning, but these were subsequently surpassed by discriminative pretraining [43, 16, 20]. Recent advancements have demonstrated the efficacy of diffusion models [56, 23] in various visual tasks. key advantage of recent diffusion models, such as Stable Diffusion, is the sheer scale of their pretraining; learning from over 2 billion images [52, 53] has the potential to outscale existing discriminatively trained models. Their large-scale generative pretraining has since been transferred to many perceptual tasks, such as 3D reconstruction [40, 47, 61], semantic [2, 35, 27, 58] and amodal segmentation [44, 9, 50], monocular depth [28, 50, 67], surface normals [14], optical flow [50], correspondence [57], and classification [32]. Recent work [15] also showed how to adapt these multistep models for single-step prediction, enabling task-specific losses in pixel space. Other works have shown that depth, normals, albedo, and segmentation can emerge (albeit with low quality) from generative models [3, 42, 26] without finetuning, suggesting similar representations may emerge"
        },
        {
            "title": "Image",
            "content": "MAE-H SD Figure 3: Our models assign similar colors to compositionally related parts of scene. Vaders mask and body (top), or the bowties and shirts (bottom) are separated by subtly different hues, while distinct colors partition unrelated parts such as his leg and the poles (top), or the dogs and text (bottom). This emerges without any part-level supervision, suggesting generative models learn hierarchical scene representations. More samples are provided in the high resolution figures in the appendix. from the data alone [13, 24]. However, applying generative models to instance segmentation has remained relatively unexplored. parallel line of work focused on representation learning, Masked Autoencoders (MAE) [21], has achieved SOTA in large number of visual tasks by pretraining to reconstruct masked image tokens, and then fine-tuning for discriminative tasks. However, common practice is to discard the decoder, which contains its pixel-perfect generative features, prior to finetuning. Prior works have shown the decoders generalization in solving visual prompts [1]. However, [1] focuses on MAEs ability to generate masks via prompting, while we finetune, and find it generalizes to objects whose masks are unseen in finetuning. 2. Instance Segmentation Instance segmentation has evolved from traditional two-stage methods [19, 5, 8], which first generate object proposals and then identify pixel groupings, to end-to-end approaches that directly predict masks using learned query embeddings [6, 33, 10]. While these direct methods eliminate the proposal stage, they still require extensive, class-specific annotated data, limiting their generalization. Recent advancements have introduced category-agnostic approaches, enabling models to segment objects without prior knowledge of their classes. The most well known of these is SAM [29], which learns zero-shot promptable segmentation by finetuning an MAE backbone and mask decoder on the massive SA-1B dataset. SAM and its recent successor SAM2 [49] represent breakthrough in obtaining general, category-agnostic masks without per-dataset training. SAMs zero-shot generalization provides point of comparison for our work. However, unlike SAM, which was trained on massive supervised dataset, our approach aims to leverage generative knowledge to achieve broad instance segmentation, potentially reducing the need for extensive manual mask annotations. Furthermore, our strongest model is trained for only 29 hours on 4 RTX6000 Ada (48GB) GPUs on less than 87,000 images and 3.7 million masks of only select categories, while SAM was trained for 68 hours on 256 A100 (80GB) GPUs on 11 million images and 1.1 billion masks of all types."
        },
        {
            "title": "3 Method",
            "content": "We aim to adapt pretrained generative models, such as Stable Diffusion or MAE, to perform category agnostic instance segmentation. Instance segmentation, in particular, is pixel level task, at which 4 generative models are naturally primed to excel. Specifically, we hypothesize they inherently learn to understand object boundaries and groupings because they must synthesize the objects core structure and boundaries themselves. In contrast, most SOTA models, such as SAM [29], extract features using an encoder that discards low-level details. As result, they must learn mask predictors or feature pyramid networks from scratch to gradually upsample these features to higher resolutions [37, 34]. 3.1 Instance Segmentation as Image to Image Translation Recent instance segmentation models predict sets of binary masks, each representing an object instance [6, 10]. However, it is not obvious how to enable generative models, designed to map from RW H3 RW H3, to easily decode to this style. Drawing inspiration from work on image-to-image translation [25, 68], we encode our ground truth as an RGB image with unique color for each instance and black color for the background. We find that both the Stable Diffusion VAE and MAE models can decode these masks with effectively no loss in quality. Thus, we finetune our model by forwarding our RGB image (or its latent) without adding noise or masking, decoding the output, and optimizing in pixel space with respect to the RGB ground truth. Unfortunately, one cannot simply assign each ground truth mask to color and train the model to regress it since there are many possible accurate color combinations. Thus, we propose our instance coloring loss based on two key properties of an RGB segmentation mask. First, all the pixels within mask should have no variance. Second, the color of mask should not be repeated anywhere outside the mask. By emphasizing these two properties, we can learn model of instance segmentation without needing to enforce specific colors for object masks. Simply, our finetuned model should ensure each instance is assigned unique color that is consistent across its pixels. More formally, assuming an image with instances, let Ω be the set of all pixels in the image, and {0, . . . , n} the instance index where = 0 is always the background. We define the set of pixels for instance as: Si = {j Ω pixel belongs to instance i}. For each instance, we define the mean embedding (or representative color) as µi,c = 1 Si (cid:88) jSi pj,c and µ0,c = 0; {1 . . . n}, {0, 1, 2} (1) (2) where pj,c is the cth channel of the predicted color at pixel j. We force the background masks mean to be black (µ0,c = 0) to follow standard convention and distinguish it from objects. Our loss is composed of three components: 1. Intra-Instance Variance Loss: To ensure that the predictions within an instance are consistent, we use smooth ℓ1 loss that encourages each pixels prediction to be close to the instance mean. This is defined as Lvar = (cid:88) i=0 1 Si (cid:88) 2 (cid:88) jSi c=0 Ls(pj,c, µi,c) (3) where Ls denotes the smooth ℓ1 loss. We find that using smooth ℓ1 loss over the standard ℓ2 loss converges better as it does not sharply penalize outliers. 2. Inter-Instance Separation Loss: We define the following loss to encourage the color of pixels outside the instance to be pushed away from the instances mean color, ensuring that distinct regions do not converge to similar colors: Lsep = (cid:88) i=0 1 (cid:112)Si Ti (cid:88) jTi 1 (cid:0)pj,c µi,c (cid:1)2 1 + (cid:80)2 c= (4) where Ti = Ω Si denote the set of pixels outside instance i. The loss is designed to saturate as the distance increases so that pixels far away from µi in color value do not dominate the loss. We include (cid:112)Si in the denominator to emphasize smaller objects. 5 Input MAE-B Features & Mask SimpleClick SD Features & Mask SAM Figure 4: For qualitative comparison, we showcase several results for promptable segmentation using our features. Our finetuned MAE-B and SimpleClick are trained on the same data, using the same backbone, yet our MAE-B strongly outperforms SimpleClick due to its generative prior. Our finetuned Stable Diffusion has never seen mask of the object type it is segmenting, but performs similar to SAM, which has been heavily supervised on over billion masks of all types. Prompt points are shown in green on the input. 3. Mean-Level Separation Loss: To further separate instances, we design the following loss similar to the above one, but for mask centroids: Lmean = 1 n(n + 1) (cid:88) 1 0i<jn 1 + (cid:80)2 c=0(µi,c µj,c)2 where the first fraction simply normalizes by the total number of comparisons. Finally, we finetune pretrained diffusion model by minimizing our instance coloring loss LIC: LIC = Lvar + λsepLsep + λmeanLmean where λsep and λmean are hyperparameters controlling the importance of each loss term. 3.2 Promptable Instance Segmentation (5) (6) So far, our finetuned model assigns color (x, y) to each pixel. Since pixels belonging to the same object instance are encouraged to have similar colors, we develop simple method to prompt the feature map for binary masks (similar to SAM [29]). For each prompt point R2 which contains the and location of the prompt pixel, we first compute per-pixel weight w(x, y) as Gaussian function with mean and standard deviation 0.01 (W, H), where and are the width and height of . We use to calculate the query vector qp R3 as weighted average of with high weights for pixels near as qp = (cid:80) . x,y w(x,y) (x,y) (cid:80) x,y w(x,y) We then compute query-feature similarity map: Sp(x, y) = 1 (x, y) qp2 , (7) and normalize it to [0, 1], and smooth it with joint bilateral filter [59, 46] (using as guidance), thus averaging the similarities close in both pixel location (x, y) and feature value (x, y). Finally, assuming set of point prompts, we merge all corresponding Sp by taking the per-pixel maximum across similarity maps, and threshold the merged similarity map to produce the binary mask."
        },
        {
            "title": "Method",
            "content": "COCOexc-L COCOexc-M COCOexc-S DRAM EgoHOS iShape PIDRay SAM SimpleClick DINO-B MAE-B MAE-H SD-Limited SD 57.0 1.4 35.0 44.6 50.0 53.7 57.6 59.5 0.6 11.0 17.8 23.2 35.1 38.8 56.9 0.2 1.7 2.9 3.5 7.3 8.5 50.2 2.4 29.4 34.3 40.3 40.8 48. 56.4 1.6 14.8 28.9 31.9 38.1 40.0 16.8 1.6 27.4 31.1 34.9 44.1 51.4 44.2 1.5 14.9 21.6 24.1 28.4 30.9 Table 1: We evaluate zero-shot mIoU at single prompt point on wide spread of datasets. We match or recover high percentage of performance (minimum 70%) on all datasets except COCOexc-M/S. This suggests that, for larger objects, our models have learned strong object-level representations that transfer across categories and styles. Our baselines, SimpleClick and DINO-B are far below MAE-B, suggesting this generalization is unique to generative models. Additionally, we strongly outperform SAM on the iShape dataset, which evaluates segmentation of detailed and complex structures."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Datasets Training: Inspired by previous work [28], we train our model exclusively on synthetic data. We combine two datasets: Hypersim [51] and Virtual Kitti 2 [4]. Hypersim provides rich variety of indoor scenes (i.e. bathrooms, bedrooms, libraries, and kitchens), while Virtual Kitti 2 focuses on outdoor driving scenes, with annotations limited to cars. Both datasets are realistic and do not contain other styles. After removing images with too few masks, our dataset comprises of 86,000 images (66,000 from Hypersim and 20,000 from Virtual Kitti 2), with sampling strategy that selects Hypersim images 90% of the time. Neither dataset includes annotations of people, animals, and several other categories. Additionally, while the number of images is comparable to existing instance segmentation datasets, the diversity is substantially lower. Our subset of Hypersim contains multiple views sampled from just 457 scenes, while Virtual Kitti 2 contains just 5 videos (each 15 seconds long), repeated from different viewing angles and weather conditions. list of labeled object types in Hypersim is available in Appendix D. Evaluation: Our finetuned models have seen masks of some limited object categories in single synthetic and realistic style. We aim to evaluate our models zero-shot generalization to unseen categories and styles. We select 5 datasets from [29], each of which contains very different domain from the others: COCOexc (COCO [36], but we exclude object types seen in finetuning), DRAM [11] (art), EgoHOS [64] (egocentric), iShape [62] (complex and fine structures), and PIDRay [63] (luggage x-rays). We describe the details and motivation for each dataset in Appendix A.2. 4.2 Models and Baselines Understanding the conditions under which generalization is possible is central to understanding our method. Thus, we implement our method on several generative models and compare them with some baselines. Unless specified, the models are finetuned using the loss and datasets described above. We finetune Stable Diffusion variants at 480640 (Hypersim) and 3681024 (Virtual Kitti 2) and ImageNet-pretrained models at 224224. We apply our method to four different settings that include diffusion and MAE models: Stable Diffusion v2 (SD) [52]: Finetuned end-to-end by fixing the timestep to the highest. MAE with Decoder (MAE-B/H) [21]: We finetune an MAE with its decoder end-to-end to showcase that strong generative prior learned solely from ImageNet-1K images without internet-scale pretraining or text supervision can effectively generalize. We use MAE ViT-B for direct comparison to DINOB/SimpleClick (see below) and MAE ViT-H to demonstrate scalability. Stable Diffusion v2 - Category Limited (SD-Limited): Finetuned just like SD, but we ignore loss on pixels within the bounding boxes of unknown-category masks to make sure the validation categories are not seen as the unknown category masks at the training time. This demonstrates generalization under very limited set of categories. 7 Figure 5: We evaluate segmentation quality as the number of prompt points increases. Our SD model marginally exceeds SAM at 1 prompt point and recovers >82% of SAMs performance at 9 prompt points. This is surprising as we do not use learned mask encoder for multiple prompt points, but simply merge similarity maps computed individually from each point prompt. Figure 6: We plot precision-recall of the zeroshot edge detection on BSDS500. Our strongest models outperform SAMs precision when recall is low, suggesting their segmentations lie on the exact boundaries of the corresponding objects. Our MAE-H model matches SAMs performance at low recall, despite being trained at only 4% of the resolution (224 224 vs. 1024 1024). We compare with the following baselines: SimpleClick [39]: SOTA promptable segmenter using an MAE-B ViTDet [34] backbone. We finetune SimpleClick using its released training code on our synthetic dataset to show that existing architectures cannot generalize well beyond supervised categories. DINO + VAE (DINO-B): To test whether generalizable object groupings are exclusive to generative pretraining, we attach DINO [7] to frozen VAE (from Stable Diffusion) via simple up-conv and fine-tune end-to-end. DINO provides the discriminative features, while the VAE can decode to object shapes unseen in finetuning. Segment Anything (SAM) [29]: We use SAM ViT-H off-the-shelf as high-water mark for generalization that is supervised on the huge SA-1B dataset with 1B annotated masks. Additional details are in Appendix A.1. 4.3 Qualitative Results We show several qualitative results of our method in Figure 2. These results are selected to highlight potential advantages of generative models over traditional methods. We highlight our models ability to represent scenes compositionally in Figure 3. We also showcase masks generated from our features and prompting, compared with our baselines in Figure 4. Additionally, for fair comparison, we randomly sample one page of figures without any cherry picking for each dataset in the appendix (Figures 8, 9, 10, 11, and 12). Finally, we showcase some high resolution results on challenging, in-the-wild images in Figures 13, 14, 15, 16, 17, 18, and 19 (again in the appendix). 4.4 Zero-Shot Promptable Instance Segmentation We evaluate our model on the task of zero-shot promptable instance segmentation. Following [29], we first evaluate our models ability to produce reasonable masks using only single prompt point at the ground truth object center and comparing the predicted masks IoU with the respective ground truth. Then, following the so-called \"golden\" standard of prompting [49, 29, 39, 55, 38, 54], we iteratively find the largest contiguous area of the ground truth with no mask predicted yet, and select the next prompt point in that area closest to the areas center. As promptable segmentation is highly ambiguous task, we do not necessarily expect to get high IoU, but we do hope for it to approach our high-water mark, SAM. Results. First, we examine model performance using single prompt at the object center  (Table 1)  . Except for COCOexc-M/S (Medium and Small), our model approaches or marginally exceeds SAMs performance, despite never seeing labeled masks for these categories. This indicates that generative models learn transferable object-level features, particularly for larger objects and intricate details (evidenced by superior results on the iShape dataset). Interestingly, SAM struggles in some cases because it seems to group by texture for out-of-distribution cases, such as art. Our SD-Limited model 8 Original GT SD Edges MAE-H Edges SAM Edges Figure 7: Delineating the edges of the objects in the scene is fundamentally ambiguous task. While our models outputs do not exactly match the ground truth (neither do SAMs), they represent one interpretation of the \"objects\" in the scene. Our model tends to include certain objects, such as the clouds or grass, in the background. This emerges without supervision and may be an inherent bias from generative pretraining. is able to generalize although unknown masks (over 30% of the dataset) are removed, but sometimes struggles to discriminate visually similar adjacent objects (as seen in Figure 13). Our method also surpasses both baselines, SimpleClick and DINO-B. SimpleClick, as expected, struggles with zero-shot mask generation since its feature pyramid, trained from scratch on our synthetic datasets with limited object types, lacks relevant priors. Our finetuned DINO-B model successfully activates on objects, but struggles to separate their instances. We hypothesize this is because self-distillation (and discriminative pretraining in general) over-emphasize semantics via invariant representations, meaning they enforce that the output representation does not change across augmentations. For example, discriminative pretraining forces the model to learn the same representation for both an image with two cats and its crop with only single cat. In contrast, to succeed at instance segmentation, one must learn equivariant representations, meaning they account for changes in the scale, shape, or structure of the image (and the objects within) [18]. We hypothesize that generative models are well posed to learn equivariant representations because they must learn to synthesize plausible image for every corrupted input they receive. However, our models have limitations segmenting small objects, likely due to biases from pretraining: Stable Diffusions text conditioning emphasizes large, prominent objects, while MAEs ImageNet-1K pretraining prefers central \"main\" objects. Additionally, models like SAM and other state-of-the-art segmenters [19, 10, 5] fine-tune at 10241024 resolution, whereas we fine-tune at lower resolutions: 480640 (Hypersim) and 3681024 (Virtual Kitti 2) for Stable Diffusion variants, and 224224 for ImageNet-based models. We expect that stronger generative models, such as FLUX.1, would enhance small-object segmentation. We leave this to future work due to their large parameter counts. Additionally, we explore if our masks improve with additional prompt points (Figure 5). We average IoU across datasets, excluding COCOexc-M/S (since few small objects are detected in the first place). Remarkably, our SD model achieves over 82% IoU relative to SAM at 9 prompt points, despite lacking learned prompt encoder for multi-point prompts or masks from any of the evaluated categories. 4.5 Zero-Shot Edge Detection We evaluate our segmentation features on the task of edge detection with the BSDS500 dataset [41]. It is important to note that we are not simply trying to find all edges in the image, but only object boundaries. Following [29], we simply apply Sobel filter on the predicted features followed by 9 nonmax suppression to thin the edges. We use the edge detection described above on the original image as weak baseline and on the output of SAMs AutoMaskGenerator as strong baseline. Results. Our model accurately delineates the edges of primary objects in each image despite never seeing mask of the object type. As shown in Figure 6, our Stable Diffusion variant outperforms SAM in the first quarter of the precision-recall curve, and our MAE ViT-H variant matches SAM for the first 15% of recall despite having several orders of magnitude less pre-training. However, as visualized in Figure 7, many labeled regions exist at the interface between object and background (e.g., clouds, plants, or rocks). Our model tends to include these regions in the background. As result, no edges are detected for some objects in our features, and the precision falls for higher recalls. This behavior is fully emergent, as our model has never seen masks of the overwhelming majority of objects present in the BSDS500 dataset. This hypothesis is further supported by how SD-Limited slightly outperforms SD. Specifically, the SD-Limited has no loss computed on high amount of the background (because we ignore loss on pixels inside the bounding box of unknown-category objects). As result, our SD-Limited sometimes activates on objects that SD would consider background."
        },
        {
            "title": "5 Conclusion",
            "content": "Our findings suggest that learning to synthesize visual reality inherently teaches detailed understanding of its constituent parts. Our models are able to segment objects and styles unseen in pretraining and finetuning, yet still achieve competitive performance with heavily supervised models. As we continue to scale generative models and diversify their pretraining data, their ability to perceive the visual world will only grow. Learning to leverage these powerful representations for wide variety of visual tasks has the potential to enable new frontier in generalizable perception."
        },
        {
            "title": "References",
            "content": "[1] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei A. Efros. Visual prompting via image inpainting. arXiv preprint arXiv:2209.00647, 2022. [2] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Labelefficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. [3] Anand Bhattad, Daniel McKee, Derek Hoiem, and D. A. Forsyth. Stylegan knows normal, depth, albedo, and more, 2023. [4] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. [5] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation, 2019. [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [8] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation, 2019. [9] Kaihua Chen, Deva Ramanan, and Tarasha Khurana. Using diffusion priors for video amodal segmentation. arXiv preprint arXiv:2412.04623, 2024. [10] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [11] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic Segmentation in Art Paintings. Computer Graphics Forum, 2022. 10 [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [13] Amil Dravid, Yossi Gandelsman, Alexei A. Efros, and Assaf Shocher. Rosetta neurons: Mining the common units in model zoo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [14] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision, pages 241258. Springer, 2024. [15] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and Bastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. arXiv preprint arXiv:2409.11355, 2024. [16] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In Advances in Neural Information Processing Systems, pages 26722680, 2014. [18] Kaiming He. Mask r-cnn tutorial. ICCV 2017. Presentation slides retrieved from https://people.csail. mit.edu/kaiming/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf, 2017. [19] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. [20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. [21] Kaiming He, Xiangyu Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1600016009, 2022. [22] Geoffrey Hinton. To recognize shapes, first learn to generate images. Progress in brain research, 165: 535547, 2007. [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, pages 68406851, 2020. [24] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. [25] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. [26] Markus Karmann and Onay Urfalioglu. Repurposing stable diffusion attention for training-free unsupervised interactive segmentation. arXiv preprint arXiv:2411.10411, 2024. [27] Yasufumi Kawano and Yoshimitsu Aoki. Maskdiffusion: Exploiting pre-trained diffusion models for semantic segmentation. IEEE Access, 2024. [28] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [30] Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Eq-vae: Equivariance regularized latent space for improved generative image modeling. arXiv preprint arXiv:2502.09509, 2025. [31] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Colorization as proxy task for visual understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 68746883, 2017. 11 [32] Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. arXiv preprint arXiv:2303.16203, 2023. [33] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel Ni, and Heung-Yeung Shum. Mask dino: Towards unified transformer-based framework for object detection and segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30413050, 2023. [34] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection, 2022. [35] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Open-vocabulary object segmentation with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76677676, 2023. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [37] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21172125, 2017. [38] Zheng Lin, Zheng-Peng Duan, Zhao Zhang, Chun-Le Guo, and Ming-Ming Cheng. Focuscut: Diving into focus view in interactive segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26372646, 2022. [39] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Simpleclick: Interactive image segmentation with simple vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2229022300, 2023. [40] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. [41] D. Martin, C. Fowlkes, D. Tal, and J. Malik. database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Intl Conf. Computer Vision, pages 416423, 2001. [42] Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. Emerdiff: Emerging pixellevel semantic knowledge in diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [43] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count. In Proceedings of the IEEE international conference on computer vision, pages 58985906, 2017. [44] Ege Ozguroglu, Ruoshi Liu, Dídac Surís, Dian Chen, Achal Dave, Pavel Tokmakov, and Carl Vondrick. pix2gestalt: Amodal segmentation by synthesizing wholes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [45] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 25362544, 2016. [46] Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, and Kentaro Toyama. Digital photography with flash and no-flash image pairs. ACM transactions on graphics (TOG), 23(3):664672, 2004. [47] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. [48] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. [49] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [50] Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, and Jitendra Malik. Scaling properties of diffusion models for perceptual tasks. arXiv preprint arXiv:2411.08034, 2024. [51] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. [52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [54] Konstantin Sofiiuk, Ilia Petrov, Olga Barinova, and Anton Konushin. f-brs: Rethinking backpropagating refinement for interactive segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86238632, 2020. [55] Konstantin Sofiiuk, Ilya Petrov, and Anton Konushin. Reviving iterative training with mask guidance for interactive segmentation. In 2022 IEEE International Conference on Image Processing (ICIP), pages 31413145. IEEE, 2022. [56] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, pages 22562265. PMLR, 2015. [57] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. [58] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse attend and segment: Unsupervised zero-shot segmentation using stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35543563, 2024. [59] Carlo Tomasi and Roberto Manduchi. Bilateral filtering for gray and color images. In Sixth international conference on computer vision (IEEE Cat. No. 98CH36271), pages 839846. IEEE, 1998. [60] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. Advances in neural information processing systems, 29, 2016. [61] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1261912629, 2023. [62] Lei Yang, Yan Zi Wei, Yisheng He, Wei Sun, Zhenhang Huang, Haibin Huang, and Haoqiang Fan. ishape: first step towards irregular shape instance segmentation, 2021. [63] Libo Zhang, Lutao Jiang, Ruyi Ji, and Heng Fan. Pidray: large-scale x-ray benchmark for real-world prohibited item detection, 2022. [64] Lingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-grained egocentric hand-object segmentation: Dataset, model, and applications, 2022. [65] Richard Zhang, Phillip Isola, and Alexei Efros. Colorful image colorization. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 649666. Springer, 2016. [66] Richard Zhang, Phillip Isola, and Alexei Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 10581067, 2017. [67] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 57295739, 2023. [68] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 22232232, 2017."
        },
        {
            "title": "A Experimental Models and Datasets",
            "content": "A.1 Additional Details for Models and Baselines Stable Diffusion v2 (SD): We finetune Stable Diffusion U-Net end-to-end using our loss. To train it in pixel space, we fix the timestep to the highest (999). We replace the input with our images VAE embedding instead of the noisy latent. We set the CLIP embedding to the null condition. We freeze the VAE when finetuning. Following [15], this enables deterministic one-step prediction. MAE with Decoder (MAE-B/H): We finetune MAE with the decoder end-to-end. In particular, MAE is trained only on images, so it allows us to explore if strong generative prior (without any additional labels such as text or class condition) is enough to learn strong segmentation features. Additionally, we can evaluate if internet-scale pretraining is necessary to generalize. We finetune both the ImageNet-1K pretrained MAE ViT-B (for direct comparison with DINO and SimpleClick) and the ViT-H (for rough comparison to SAM and Stable Diffusion). The masking ratio is set to 0%, so no tokens are masked. Our backbones use normalized tokens during pretraining, as this is the standard. We hypothesize this leads to the token artifacts seen in the qualitative figures. It is possible results may improve by using un-normalized tokens. Stable Diffusion v2 - Category Limited (SD-Limited) About 30% of masks in the Hypersim dataset have no labeled category. While we have manually verified this to be true, it remains impossible to empirically prove there is no mask of any given category (i.e. dolphin) in our dataset. Thus, we finetune Stable Diffusion v2 as described earlier, but we compute no loss on any pixels inside the bounding box of any mask with the \"unknown\" category. This enables us to quantitatively show that our model generalizes even in strict, category limited setting. SimpleClick: We train SimpleClick, state-of-the-art promptable instance segmenter, on the same synthetic datasets that we train our generative models on. This helps us investigate whether our generalization is due to the generative model itself, or simply an effect of training on synthetic data. Our model initializes with an MAE ViT-B encoder, along with feature pyramid learned from scratch. We finetune using its released training code. DINO + VAE (DINO-B): We investigate whether strong discriminative model can suffice for segmentation features without necessarily learning to synthesize images. However, we must pair it with model that knows how to upsample images of all types from low-dimensional space so that it will be able to generate images of objects unseen in training. To investigate this, we join ImageNet-1K pretrained DINO ViT-B with the Stable Diffusion VAE, and connect them with single up-conv layer. Additionally, the latent features of images compressed with the Stable Diffusion VAE often look like the image itself [30]. Thus, to succeed, all DINO needs to do is generate these object groupings at lower resolution. Segment Anything (SAM): SAM is large-scale promptable instance segmentation model supervised on over billion masks from very large distribution of data. We use SAM as benchmark to evaluate the extent to which our model generalizes across wide variety of images. While in some cases we outperform SAM, we do not mean for this to claim that our models are inherently superior to it. We use the publicly available checkpoint and inference library. To match their pretraining resolutions, SimpleClick, DINO, and MAE are finetuned at 224224 resolution, rather than the standard ones described in implementation details. A.2 Evaluation Datasets COCOexc: We evaluate on the COCO 2017 validation set, as is standard for large variety of segmentation tasks. To showcase our zero-shot generalization to unseen mask categories, we choose subset of COCO dataset with categories not seen in our finetuning and call it COCOexc dataset. COCOexc does not include object types that Hypersim does not have explicit category for, but we have observed to exist in the dataset (i.e. wine glass, teddy bear, potted plant). However, we have seen that our synthetic data does not contain any masks for humans or animals, so COCOexc includes such images. COCOexc contains 86% of the images and 64% of the masks in the full COCO 2017 validation set. We provide full list of categories in Appendix D. DRAM: Humans are able to perceive objects in wide variety of visual media with large amounts of abstraction. Unfortunately, most existing segmentation methods require high amounts of labeled data to generalize to art. DRAM is segmentation dataset that has annotated large variety of art pieces across styles and time periods, including many abstract styles such as impressionism, ink-and-wash, and cubism. We evaluate on the test set. EgoHOS: Egocentric vision is crucial for embodied AI and robotics. However, segmentation in egocentric tasks is challenging because the first-person views often have frequent occlusions, motion blur, and variable lighting, resulting in inconsistent and ambiguous object boundaries. The EgoHOS dataset provides segmentations of large amount of egocentric images of humans interacting with everyday objects. We evaluate on the test set. iShape: Many objects in the real world have fine, intricate structures. We evaluate our models ability to accurately segment these objects using the iShape dataset. iShape is an instance segmentation dataset which contains many occlusions and complex, fine structures such as wires or fences. We evaluate on the test set. PIDRay: Humans effortlessly apply their understanding of object shapes even in scenarios never encountered in nature. For instance, TSA agents can quickly identify dangerous items in X-ray images of luggage. To assess our models generalization to this challenging scenario, we employ the PIDRay dataset, which features labeled examples of hazardous objects in baggage. This task is particularly demanding because many dangerous items are small and deliberately concealed within other objects. We evaluate on the \"easy\" subset of the test set."
        },
        {
            "title": "B Implementation details",
            "content": "We train our models on node with four RTX6000 Ada GPUs. We train our models with batch size of 2, with gradient accumulation for 4 steps, for an effective batch size of 32. We do this intentionally, as described in [28], to mix gradients between images sampled from Hypersim and Virtual Kitti 2. We train our model for 30,000 iterations, which takes about 29 hours for SDv2 and 12 hours for MAE ViT-H. However, our models show no signs of overfitting, and performance would likely benefit from additional iterations, but we didnt explore this due to timing constraints. We sometimes struggle with memory constraints when finetuning Stable Diffusion, as some images in Hypersim have very high number of instances (2000+). Thus, we compute the loss on up to 1250 instances at most. We normalize final pixel-level outputs as we observe it improves convergence. We start our learning rate at 6e-5, after 100 steps of warmup. We then decay on cosine schedule so that we end at 1 20 of our original learning rate. We train at resolution of 480640 for Hypersim and 3681024 for Virtual Kitti 2 for our Stable Diffusion variants. For ImageNet-pretrained models (MAE, DINO, SimpleClick), we resize Hypersim images to 224224 and randomly crop 224224 region in Virtual Kitti 2 dataset. We set λsep and λmean to 300 and compute all losses in the range of [0, 255] to weight all terms equally. For Stable Diffusion, we finetune only the U-net and freeze the VAE. We set the text condition to the empty string. For all prompting experiments, we fix the threshold to 3 255 and use window size of 9 (for joint bilateral smoothing)."
        },
        {
            "title": "C Ablation Study",
            "content": "Table 2: Ablation study using mIoU at single center point on COCOexc on our SD model. Lvar Lsep Lmean smooth ℓ1 Norm mIoU 0.2 30.4 29.3 27.7 26.2 31.6 We present an ablation study in Table 2 to examine the effect of each loss component and the VAE normalization using mIoU at single center point on the COCOexc dataset. First, removing the intrainstance variance loss (Lvar) (row 1) causes performance to collapse. This is essential to our method; without it, the model does not produce uniform masks. Eliminating the pixel-level separation loss 15 (row 2) prevents the model from learning sharp object boundaries, causing it to slightly overestimate mask boundaries. Eliminating the mean-level loss (row 3) results in reduced mIoU as well, but instead affects the models ability to discriminate smaller objects. Replacing the smooth ℓ1 loss with ℓ2 loss (row 4) and the normalization of VAE outputs (Norm, row 5) results in mIoU of 27.7 and 26.2, respectively. Both smooth ℓ1 loss and normalization of VAE output helps the model converge to lower loss values earlier. Overall, the complete model that integrates all these components (row 6) achieves the highest mIoU of 31.6, confirming that each element plays role in obtaining optimal performance."
        },
        {
            "title": "D List of Object Types in Hypersim and COCOexc",
            "content": "For each dataset, we provide list of labeled object types present, along with the number of objects with that type. D.1 Hypersim This list includes only objects with instance and class annotations. Certain objects which have instance-level annotations but lack class labels, such as teddy bears or potted plants, were placed into the \"unknown\" object category. Total number of objects: 3,693,970 Objects by category: Unknown (1,375,739), books (1,149,313), chair (334,422), lamp (211,409), table (102,093), pillow (74,230), window (51,444), picture (46,102), cabinet (38,253), paper (34,420), sofa (30,895), blinds (29,462), clothes (28,917), door (20,410), box (19,769), desk (19,418), floormat (18,879), counter (14,446), bookshelf (12,887), shelves (11,886), sink (10,026), mirror (7,488), bed (6,001), towel (5,250), television (4,367), nightstand (2,803), bathtub (2,556), refrigerator (2,243), curtain (2,066), toilet (1,068), dresser (717), wall (106), whiteboard (100). D.2 COCOexc We excluded object categories that either labeled in our train set or we have observed to appear. For example, we have excluded \"wine glass\" or \"knife\" as these would be placed into Hypersims \"unknown\" category. We have personally verified all of the objects below to not exist in the subset of Hypersim we train on. Total number of objects: 23,195 Objects by category: Person (11,004), traffic light (637), handbag (540), bird (440), boat (430), truck (415), umbrella (413), cow (380), banana (379), motorcycle (371), backpack (371), carrot (371), sheep (361), donut (338), kite (336), bicycle (316), broccoli (316), cake (316), suitcase (303), orange (287), bus (285), pizza (285), horse (273), surfboard (269), zebra (268), sports ball (263), elephant (255), tie (254), skis (241), giraffe (232), tennis racket (225), dog (218), cat (202), train (190), skateboard (179), sandwich (177), baseball glove (148), baseball bat (146), airplane (143), hot dog (127), frisbee (115), fire hydrant (101), stop sign (75), bear (71), snowboard (69), parking meter (60)."
        },
        {
            "title": "Image",
            "content": "DINO-B MAE-B MAE-H SD-Limited SD"
        },
        {
            "title": "SAM",
            "content": "Figure 8: Qualitative Results on the COCOexc [36] dataset. These results are randomly chosen and not cherry-picked."
        },
        {
            "title": "Image",
            "content": "DINO-B MAE-B MAE-H SD-Limited SD"
        },
        {
            "title": "SAM",
            "content": "Figure 9: Qualitative Results on the DRAM [11] dataset. These results are randomly chosen and not cherry-picked."
        },
        {
            "title": "Image",
            "content": "DINO-B MAE-B MAE-H SD-Limited SD"
        },
        {
            "title": "SAM",
            "content": "Figure 10: Qualitative Results on the EgoHOS [64] dataset. These results are randomly chosen and not cherry-picked."
        },
        {
            "title": "Image",
            "content": "DINO-B MAE-B MAE-H SD-Limited SD"
        },
        {
            "title": "SAM",
            "content": "Figure 11: Qualitative Results on the iShape [62] dataset. These results are randomly chosen and not cherry-picked."
        },
        {
            "title": "Image",
            "content": "DINO-B MAE-B MAE-H SD-Limited SD"
        },
        {
            "title": "SAM",
            "content": "Figure 12: Qualitative Results on the PIDRay [63] dataset. These results are randomly chosen and not cherry-picked. 21 - D - M M - - i S Figure 13: Qualitative comparison for challenging in-the-wild image, displayed in high resolution. 22 DINO-B MAE-B MAE-H SD-Limited SD SAM Figure 14: Qualitative comparison for challenging in-the-wild image, displayed in high resolution. 23 - D - D E - - i e M Figure 15: Qualitative comparison for challenging in-the-wild image, displayed in high resolution. 24 - D - D E - - i e M Figure 16: Qualitative comparison for challenging in-the-wild image, displayed in high resolution. 25 - D - D E - - i e M Figure 17: Qualitative comparison for challenging in-the-wild image, displayed in high resolution. 26 DINO-B MAE-B MAE-H SD-Limited SD SAM Figure 18: Qualitative comparison for challenging in-the-wild image, displayed in high resolution. - D - D E - S - i S Figure 19: Qualitative comparison for challenging in-the-wild image, displayed in high resolution."
        }
    ],
    "affiliations": []
}