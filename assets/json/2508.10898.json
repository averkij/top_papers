{
    "paper_title": "Puppeteer: Rig and Animate Your 3D Models",
    "authors": [
        "Chaoyue Song",
        "Xiu Li",
        "Fan Yang",
        "Zhongcong Xu",
        "Jiacheng Wei",
        "Fayao Liu",
        "Jiashi Feng",
        "Guosheng Lin",
        "Jianfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 8 9 8 0 1 . 8 0 5 2 : r Puppeteer: Rig and Animate Your 3D Models Chaoyue Song1,2, Xiu Li2, Fan Yang1, Zhongcong Xu2, Jiacheng Wei1, Fayao Liu3, Jiashi Feng2, Guosheng Lin1, Jianfeng Zhang2 1Nanyang Technological University 2ByteDance Seed 3Institute for Infocomm Research, A*STAR https://chaoyuesong.github.io/Puppeteer Figure 1: Given 3D model, we apply automatic rigging to create skeleton structure with skinning weights. The input mesh is then rendered as input for video generation models [1, 34]. Finally, we produce animations guided by the generated videos. The input 3D models are generated by [73]."
        },
        {
            "title": "Abstract",
            "content": "Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces joint-based tokenization strategy for compact representation and Corresponding authors. Email: chaoyue002@e.ntu.edu.sg. Preprint. Under review. hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods."
        },
        {
            "title": "Introduction",
            "content": "From AAA games and animated films to VR/AR experiences and robotic simulations, modern interactive media demands dynamic 3D content. While recent generative AI advances have accelerated the creation of high-fidelity 3D models with intricate geometry and textures, these assets remain predominantly static. Transforming static 3D models into animated versions requires two expertdriven processes: rigging (skeleton setup and skinning weight assignment) and animation. This manual, time-intensive workflow now constitutes significant impediment to the efficiency of modern content creation pipelines. Research communities have invested considerable effort in automating the rigging process. Early template-based techniques such as Pinocchio [6] fit predefined skeletal structures to input meshes, achieving satisfactory results on specific categories but failing to generalize to arbitrary shapes. Template-free algorithms [25, 3, 7, 42, 71] extract skeletal structures directly from geometric properties but frequently produce excessively dense or topologically incompatible joint configurations unsuitable for practical animation workflows. Deep learning approaches have substantially advanced the field: RigNet [86] pioneered direct skeleton and skinning weight prediction from input shapes using graph neural networks, while MagicArticulate [67] reformulated skeleton generation as an autoregressive problem and introduced large-scale dataset with detailed rigging annotations. Despite these innovations, significant challenges persist: RigNet struggles with complex mesh topologies due to its reliance on carefully crafted features and restrictive orientation requirements. MagicArticulate suffers from computational inefficiency during inference and limited generalization in its functional diffusion process for skinning weight prediction. Critically, both approaches address only the rigging stage of the pipeline, leaving the equally challenging animation process as separate manual task that requires substantial expertise. In this work, we present Puppeteer, comprehensive framework that integrates automatic rigging and animation into unified pipeline. To address the data scarcity and limited pose diversity in existing datasets, we expand the Articulation-XL dataset [67] to 59.4k rigged models, including carefully curated subset of 11.4k diverse pose examples that enhance generalization to varied pose inputs. This expanded dataset serves as the foundation for our learning-based approach. To overcome the limitations of existing rigging approaches in handling diverse shapes and complex topologies, our system introduces key improvements to both fundamental rigging components. For skeleton generation, we employ auto-regressive transformers featuring joint-based tokenization and hierarchical sequence ordering with randomization, creating more compact representations while generating structurally coherent skeletons free from template dependencies. For skinning weight prediction, we propose an attention-based architecture incorporating topology-aware joint attention that explicitly encodes skeletal graph structure, achieving robust weight prediction with enhanced generalization and computational efficiency. Beyond rigging, we address the automatic animation challenge that previous methods have largely overlooked. We introduce differentiable optimizationbased method that requires no neural network parameters yet produces stable, high-quality animations by combining our generated rigging with reference video guidance easily obtained from off-theshelf video generation models. Our unified framework enables full automation from static meshes to animated assets, transforming the labor-intensive manual workflow into an efficient, accessible pipeline for diverse 3D content creation. 2 Extensive evaluations demonstrate the effectiveness of our approach across both rigging and animation tasks. For rigging, experiments on the expanded Articulation-XL2.0 dataset and ModelsResource benchmark [76, 85] show significant improvements over state-of-the-art methods in skeleton accuracy and skinning weight quality. The robustness of our approach is further validated through successful application to diverse 3D contentfrom professionally designed game assets to AI-synthesized geometries. For animation, direct comparisons against recent 4D generation techniques [77, 59] show that our optimization-based approach produces more temporally consistent and visually faithful results while maintaining computational efficiency. Notably, our method eliminates the jittering artifacts commonly seen in learning-based approaches during complex motion sequences. The clean and stable animation results also highlight the reliability of our automatically generated rigs. In summary, our work advances automated 3D model rigging and animation through four key contributions: (1) An expanded large-scale articulation dataset with 59.4k rigged models including diverse-pose subset; (2) novel auto-regressive skeleton generation approach featuring efficient jointbased tokenization and hierarchical sequence ordering with randomization strategies; (3) An attentionbased architecture for skinning weight prediction incorporating topology-aware joint attention; and (4) differentiable optimization-based animation method that produces stable, high-quality animation for diverse object categories without requiring extensive computational resources or manual effort."
        },
        {
            "title": "2 Related works",
            "content": "Skeleton generation. Skeleton generation methods for 3D models fall into two main groups. The first leverages templates or additional inputs. Pinocchio [6] pioneered template-fitting for automatic skeleton extraction, while Li et al. [37] employed deep learning for human joint estimation with given skeleton template. Some recent works [13, 24, 69] continue this line for humanoid skeleton generation. significant limitation of these approaches is their inability to generalize effectively to diverse object categories. Other methods in this group require additional inputs such as point cloud sequences [87], mesh sequences [14, 30], manual annotations [26], or video data [88, 81, 66, 98, 90, 65, 68, 40]. The second group works without templates or annotations. Traditional approaches [3, 7, 25, 71, 42] extract curve skeletons and often produce overly dense joints unsuitable for animation. Modern deep learning methods like Xu et al. [85] and RigNet [86] learn directly from limited datasets containing fewer than 3,000 rigged models. Despite their innovations, these methods depend extensively on carefully crafted features and impose restrictive assumptions regarding shape orientation, substantially constraining their effectiveness when confronted with complex mesh topologies. With the exponential growth of 3D datasets [15, 16] and the success of auto-regressive approaches in 3D generation [62, 11, 12, 72], the field has seen significant advances in skeleton generation. MagicArticulate [67] pioneered the formulation of skeleton generation as an auto-regressive problem and introduced Articulation-XL, large-scale 3D dataset with rigging information. Several recent works [45, 100] have also successfully incorporated auto-regressive transformer architectures for skeleton generation, further validating this approach. In our work, we substantially expand the Articulation-XL dataset from 33k to 59.4k rigged models, including diverse pose subset containing 11.4k examples. We leverage auto-regressive transformers for skeleton generation, introducing two key innovations: an efficient tokenization method for skeletal structures and hierarchical sequence ordering strategy with randomization that enhances bidirectional learning capabilities. Skinning weight prediction. Following skeleton generation, automatic rigging requires skinning weights prediction to establish joint influence on mesh vertices. Traditional geometric approaches [18, 28, 19, 6] assign weights based on vertex-joint distancesa method that proves inadequate for complex topologies. Learning-based approaches [46, 86, 54, 55] consistently integrate graph neural networks (GNN) with geometric distance cues for skinning weight prediction. However, these GNN-based methodologies face significant limitations in scalability and struggle to generalize effectively across 3D data with diverse spatial orientations. MagicArticulate [67] formulates skinning weight prediction as functional diffusion problem [97], but suffers from slow inference and limited generalization. We instead introduce an attention-based network that strategically incorporates skeleton graph distances, enabling more robust skinning weight prediction with substantially enhanced generalization across diverse object categories. Concurrent works [100, 17] similarly leverage crossattention between surface points and bones to learn skinning weights. 3 3D animation. With the generated rigging, the next step is to animate the 3D models. In contrast to earlier work that focuses on human motion generation [31, 74, 101, 23, 102, 75, 63, 64, 60, 61], we aim to animate diverse 3D object categories that can be rigged. Our pipeline uses reference video as motion guidance to animate the rigged mesh. The field of 4D generation has experienced rapid growth recently, spanning text/image-to-4D generation [91, 58, 44, 4, 50, 93, 8, 95, 41, 5, 70, 43, 84, 104] and video-to-4D generation [21, 32, 96, 82, 98, 99, 39, 20, 106, 89, 83, 9, 59, 80]. For comprehensive overview, we refer readers to the survey by Miao et al. [51]. However, most existing 4D generation approaches do not take specific 3D object as input to generate animations targeted for that object. Many notable attempts have been made to address object animation. AnyMole [94] introduced motion in-betweening method for diverse categories with context motions. AnyTop [22] proposed animating 3D rigged meshes using diffusion model without explicit motion guidance, given only an input skeleton. Millan et al. [52] presented related work but focused exclusively on humanoid models with SMPL [48] proxies. Animate3D [33] proposed animating 3D objects with Multi-view Video Diffusion Model, which requires extensive training. MotionDreamer [77] attempted to animate 3D models without rigging but produced suboptimal motion quality. The most recent related work, AKD [38], takes 3D model, manually adds skeleton, and uses [6] to predict skinning weights. They apply animation to this rigged model using video-based score distillation sampling (SDS). However, their approach is computationally intensive (requiring approximately 25 hours per object) and produces unstable animations with noticeable jittering artifacts. In contrast, we propose an optimization-based method that requires no neural network parameters to achieve more stable animations for diverse object categories by combining our generated rigging with reference video guidance."
        },
        {
            "title": "3 Automatic rigging",
            "content": "Our automatic rigging framework features two sequential modules. First, we deploy an auto-regressive transformer to infer structurally valid skeleton from raw 3D mesh (Section 3.2). Subsequently, this skeleton and the original mesh are processed by an attention-based architecture to predict precise per-vertex skinning weights (Section 3.3). To facilitate large-scale learning, we introduce ArticulationXL2.0 (Section 3.1), comprehensive dataset comprising 59.4k 3D models with high-quality rigging. 3.1 Dataset: Articulation-XL2.0 We present Articulation-XL2.0, an expanded version of Articulation-XL proposed in [67]. Our dataset incorporates multiple geometric data types from Objaverse-XL [15, 16] previously excluded, while maintaining the same data filtering process. We further improve quality by eliminating unskinned vertices and conducting manual validation, yielding over 48k high-quality rigged 3D models. Recognizing that models in our primary dataset are predominantly in rest pose configurations, thus limiting generalization capacity to novel articulations, we have constructed diverse-pose subset. By identifying the intersection between high-quality animation data from Diffusion4D [41] and our rigged model corpus, we extract 7.3k deformed meshes with corresponding rigging information from animation frames exhibiting maximal deviation from rest pose configurations. To counterbalance the predominance of humanoid morphologies in this subset, we supplement with 4.1k models generated using SMALR [107, 108] with parameterizations derived from 41 distinct animal scans and randomized valid poses. The resulting 11.4k diverse-pose dataset significantly enhances performance on unseen poses, as validated in our experiments. We will release Articulation-XL2.0, comprehensive collection of 59.4k high-quality rigged models, to facilitate future research. Dataset statistics and examples are provided in the appendix. 3.2 Auto-regressive skeleton generation We formulate skeleton generation as shape-conditioned sequence modeling problem. Given an input mesh M, we employ an auto-regressive framework (Figure 2 top) to predict skeleton consisting of 3D joint positions Rj3 and topological bone connections Nb2 defined by joint indices. Our framework consists of three key components: joint-based skeleton tokenization, hierarchical sequence ordering with randomization, and shape-conditioned auto-regressive generation. Together, these components enable accurate, efficient skeleton generation across varied object structures without relying on predefined templates. 4 Figure 2: Overview of our automatic rigging pipeline. Given 3D mesh, we first sample point clouds with normals, then generate skeleton using an auto-regressive transformer. The point clouds and skeleton are processed through an attention-based network with four key operations: 1 bone feature enhancement via topology-aware joint attention, 2 global context integration through cross-attention with shape latents, 3 bone-point interaction via cross-attention, and 4 point feature refinement. Finally, cosine similarity and softmax normalization produce the skinning weights. Joint-based skeleton tokenization. In [67], skeletons are encoded as bone-based sequences: each of the bones contributes 6 tokens (the 3D coordinates of its two endpoints), yielding total sequence length of 6b and redundantly repeating joint positions across multiple connected bones. Inspired by [45], we develop joint-based tokenization strategy that represents each of the joints by its 3D coordinates and parent index, producing sequence of length 4j. Since tree-structured skeleton satisfies = + 1, this yields 4j < 6b whenever > 3, making the joint-based representation more compact. Unlike [45], which projects joint positions into high-dimensional feature spaces via MLPs, we discretize normalized joint coordinates into 1283 grid and append the parent index, producing discretized token sequences that serve as input to our auto-regressive transformer. In practice, we assign the root joint parent index of 0 and offset all other parent indices by +1 (subtracting 1 during detokenization). Sequence ordering. While our joint-based tokenization provides compact representation, the sequential ordering of tokens significantly affects skeletal coherence and model performance. For skeleton modeling with joint positions and parent indices, tokens can be sequenced using either spatial ordering (ascending z-y-x coordinates, as in [67]) or hierarchical ordering (breadth-first traversal of the skeletal tree structure). Our experiments demonstrate that spatial ordering frequently produces disconnected skeletons, as child joints generated before their parents create invalid parent references (see Section 5.5 and appendix for comparisons). We therefore adopt hierarchical ordering, applying spatial sorting only among joints at the same hierarchical level. Additionally, inspired by [92], we enhance bidirectional learning capability through sequence randomization. We group the 4 tokens of each joint together and randomly shuffle these groups, incorporating target-aware positional indicators = [p0, p1, ..., pj1] to guide the generation process. Specifically, all tokens within joint group share positional indicator signaling which joint will be generated next. To identify the first joint group, we additionally incorporate positional indicators into shape tokens Tshape that precede skeleton tokens Tskel: = [Tshape, Tskel] + = [Tshape + p0, skel + p1, ..., Tj2 skel + pj1, Tj1 skel]. (1) Shape-conditioned auto-regressive generation. With our tokenization strategy and sequence ordering established, we now describe the auto-regressive generation process. We sample 8,192 5 points with normals from the input mesh as shape conditioning and encode them using pre-trained shape encoder [105]. This fixed-length shape token sequence Tshape precedes the transformers skeleton sequence, with < bos > and < eos > tokens marking skeleton boundaries (omitted in Equation (1)). We adopt OPT-350M [103] as our decoder-only transformer architecture, training with cross-entropy loss for next-token prediction: Lpred = CE(T, ˆT), where and ˆT represent ground truth and predicted token sequences. During inference, generation begins with shape tokens and sequential positional indicators, proceeding auto-regressively until producing the < eos > token, followed by detokenization to recover the complete skeleton. (2) 3.3 Attention-based skinning weight prediction In this section, we present an attention-based network for predicting per-vertex skinning weights that determine how the mesh deforms in response to skeleton articulation. Network architecture. The network architecture is illustrated in the bottom of Figure 2. Our pipeline begins by sampling points with normals from the input mesh. These points are processed through positional encoding and part encoder from PartField [47] to obtain part-aware point embeddings Fpoint Rnd that combine spatial information with part features. We incorporate part-aware features because parts and bones exhibit strong anatomical correspondence, providing valuable structural guidance for skinning weight prediction. In parallel, we construct bone-based coordinates Rj6 by concatenating each joints parent position with its own positionfor the root joint, its position is duplicated to fill both coordinate slots. These bone coordinates similarly undergo positional encoding to produce bone embeddings Fbone Rjd. Additionally, we feed the sampled points with normals into pre-trained shape encoder [105] to extract global shape latents Fshape R257d. The architecture then performs series of attention operations [78]:(1) Bone feature enhancement. We first apply self-attention using the topology-aware joint attention on the bone embedding to obtain enhanced bone features bone. (2) Global context integration. Cross-attention is performed between global shape latents (as context) and both point and bone features, generating updated features point and bone. (3) Bone-point interaction. Cross-attention uses the updated bone features as queries and bone. (4) Point feature refinement. Final cross-attention between refined bone features point produces the final point features point. Finally, the network computes cosine similarity scores and applies softmax normalization to produce skinning weights: point as keys/values to produce refined bone features bone (as context) and point features (cid:32) = softmax α (cid:13) (cid:13)F pointF bone (cid:13) (cid:13) point bone (cid:33) . (3) where α is learnable scaling parameter. We optimize the network using cross-entropy loss during training. Topology-aware joint attention. While the basic architecture provides effective weight prediction, our experiments revealed that explicitly modeling skeletal structure significantly enhances performance. Our ablation studies demonstrate that using bone-based coordinates Rj6 rather than joint coordinates Rj3 substantially improves performance (see Section 5.5), highlighting the importance of inter-joint relationships within the skeletal structure. To further leverage topological structure, we propose Topology-aware Joint Attention (TAJA), which augments standard self-attention with relative positional encodings derived from skeletal graph distances. To implement TAJA, we first compute graph distance matrix Rjj from the skeletal structure, then transform these distances into continuous embeddings through quantization and projection operations, yielding position embeddings Edis Rjjh, where is the number of attention heads. The attention mechanism is then modified as: Attention(Q, K, V, Edis) = softmax (cid:18) QKT dk (cid:19) + λEdis V, (4) where λ is learnable scaling parameter. This approach explicitly incorporates inter-joint topological relationships, improving the networks capacity to understand skeletal structure and generate more accurate skinning weights."
        },
        {
            "title": "4 Video-guided 3D animation",
            "content": "With the generated skeleton and skinning weights, we transform static meshes into animation-ready assets. This section presents our optimization-based approach for automatically animating rigged 3D models with video guidance. Animation pipeline. Our animation process begins by rendering the rigged mesh as the initial frame I0. Using this as conditioning image, we leverage recent text-to-video generation models [34, 1] that can maintain object identity while creating plausible motion sequences. With text prompt describing the desired animation, these models generate video sequence = {I0, I1, ..., In1} comprising frames. Given this reference video sequence , we jointly optimize per-frame joint rotations and global root motion of the 3D mesh to align the resulting animation with the generated video sequence. 0, Qi joint = {Qi Differentiable optimization framework. For each frame {1, 2, ..., 1} excluding the first frame, we optimize both root motion parameters (Qi root, Ti root) and joint-specific rotations j1}, where R4 represents rotation as unit quaternion and R3 Qi denotes translation. For the first frame (rest pose), we initialize all transformations with identity quaternions and zero translations, which remain fixed during optimization. All subsequent frames are similarly initialized before optimization begins. Our optimization process incorporates rendering losses, tracking losses, and regularization terms: 1, ..., Qi = (Lrgb + Lmask + Lf low + Ldepth) (cid:125) (cid:124) (cid:123)(cid:122) rendering losses + (Ljoint_track + Lvertex_track) (cid:123)(cid:122) (cid:125) tracking losses (cid:124) +Lreg. (5) For rendering losses, we utilize differentiable rendering via Pytorch3D [57] to generate predicted frames and compute RGB, mask, optical flow, and depth discrepancies between these predictions and the corresponding reference video frames. The optical flow and depth for video frames are extracted using off-the-shelf methods [10, 53]. The tracking losses incorporate 2D joint tracking term and 2D vertex tracking term that leverage Cotracker3 [35] to trace selected points throughout the video sequence. We project our optimized 3D joints and deformed mesh vertices into 2D space and minimize their distance to the corresponding tracked 2D keypoints. To address occlusion challenges, we implement visibility detection mechanisms for both joints and vertices. For joints, we define visibility based on ray-mesh intersection: joint is considered visible if the ray projected from the camera to the joint intersects the mesh surface exactly once. We employ the ray_mesh_intersect function from libigl [29] to compute these joint visibility masks. For vertex visibility, we leverage the rasterization output from Pytorch3D to determine visible surface points. These visibility masks, derived from the first frame, ensure that our tracking losses are applied consistently throughout the sequence based on initial visibility, preventing optimization artifacts from elements that are occluded in the reference pose. We further incorporate regularization terms that enforce frame-to-frame motion smoothness. Complete mathematical formulations of all loss components are provided in the appendix."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental setup Datasets. We train our models on the Articulation-XL2.0 dataset introduced in Section 3.1, which contains over 48k high-quality samples from Objaverse-XL [15, 16] as the main subset and 11.4k samples from the diverse-pose subset. For model training, we utilize over 46k samples from the main subset and 10.9k from the diverse-pose subset. For evaluation, we employ three distinct test sets: Articulation-XL2.0-test (2k data from the main set), ModelsResource-test [76, 86] (270 upright, front-facing models with no overlap with Articulation-XL2.0, enabling assessment of cross-dataset generalization), and 500-mesh portion of the diverse-pose subset specifically selected to evaluate model performance under varied poses. Implementation details. To enhance robustness and generalization capabilities, we apply geometric data augmentations (scaling, shifting, rotation transformations) and pose augmentationarticulating the training samples with their ground truth skeleton and skinning weights to simulate diverse poses. Further implementation details are provided in the appendix. 7 Table 1: Quantitative comparison of skeleton generation. We evaluate each method on three benchmarks using CD-J2J, CD-J2B, and CD-B2Ball reported in units of 102. Lower values indicate better alignment. * denotes models trained on Articulation-XL2.0 including the diverse-pose subset; unmarked models were trained without it. Bold and underlined numbers denote the best and second-best results, respectively."
        },
        {
            "title": "Method",
            "content": "Articulation-XL2."
        },
        {
            "title": "ModelsResource",
            "content": "Diverse-pose J2J J2B B2B J2J J2B B2B J2J J2B B2B Pinocchio RigNet MagicArti. UniRig Ours Ours* 8.324 7.618 3.264 3.305 3.033 3.109 6.612 6.076 2.503 2.611 2.300 2.370 5.485 5.279 2.123 2.180 1.923 1.983 6.852 7.223 4.114 3.964 3.841 3. 4.824 5.987 3.137 3.021 2.881 2.804 4.089 4.329 2.693 2.570 2.475 2.405 7.967 7.751 4.376 3.252 3.212 2.514 6.411 6.392 3.456 2.569 2.542 1.986 5.149 5.713 2.955 2.077 2.027 1.598 5.2 Skeleton generation results Baselines and metrics. We include four comparison methods as baselines: Pinocchio [6], which fits predefined skeleton templates to input meshes. RigNet [86], learning-based model that employs graph convolutions to infer joint locations. MagicArticulate [67], an auto-regressive framework for skeleton generation, and the concurrent method UniRig [100], which similarly uses an auto-regressive transformer approach. All methods are evaluated on Articulation-XL2.0 and ModelsResource test sets, as well as our diverse-pose subset. We evaluate skeleton generation quality using three Chamfer Distancebased metrics from [85, 86]: CD-J2J (joint-to-joint), CD-J2B (joint-to-bone) and CD-B2B (bone-to-bone). These metrics measure the spatial alignment between generated and ground truth skeletons, where lower values indicate better performance. Comparison results. Qualitative results are shown in Figure 3 for all three benchmarks. RigNet consistently produces invalid skeletonsits graph-convolutional model fails to converge well when trained on our large-scale dataset with highly varied orientations. UniRig presents missing and misaligned skeletons, such as missing bones on the turtle limbs and squirrel tail and misaligned skeletons on human hands, as marked in yellow circles. MagicArticulate matches reference skeletons closely on Articulation-XL2.0 and ModelsResource, but exhibits errors in fine details (e.g., missing bones in turtle limbs, incorrect squirrel tailbody junctions) and degrades on the diverse-pose subset, since it was trained only on predominantly rest-pose data without pose augmentation. In contrast, our method yields accurate, structurally correct skeletons across three benchmarks. Importantly, our generated skeletons can even correct omissions in artist-created skeletons, such as missing turtle headbody connection. Table 1 reports quantitative metrics, where we consistently outperform all baselines on every dataset and metric. Notably, incorporating the diverse-pose subset during training leads to marked improvements on the diverse-pose benchmark. Qualitative results on AI-generated meshes. We evaluate our methods generalization capability on AI-generated meshes from Tripo2.0 [2] and Hunyuan3D 2.0 [73]. As shown in Figure 4, we compare our method with MagicArticulate [67]. MagicArticulate loses fine details (e.g., the robots hand in rows 3 and 5, the dolphin-hummingbird chimeras tail and wings in row 4, marked in yellow) and produces misaligned skeletons (dragons tail in row 1, deers legs in row 2). By contrast, our approach consistently generates valid, robust skeletons across all categories. 5.3 Skinning weight prediction results Baselines and metrics. We compare our method for skinning weight prediction against three baselines: Geodesic Voxel Binding (GVB) [18], geometry-based technique available in Autodesk Maya [27], RigNet [86], and MagicArticulate [67]. We also evaluate these three methods on Articulation-XL2.0 and ModelsResource test sets, as well as our diverse-pose subset. Skinning weight quality is evaluated using three metrics: precision, recall, and L1-norm error. Precision is the fraction of predicted weights > 1e4 that are correct, and recall is the fraction of true weights > 1e4 we recover. The L1-norm error reports the average absolute deviation between predicted and ground truth weights over all vertices. Deformation error results are provided in the appendix. Figure 3: Qualitative skeleton generation results. The data is from Articulation-XL2.0, ModelsResource, and the diverse-pose subset from top to bottom. Table 2: Quantitative comparison of skinning weight prediction. We evaluate our approach against GVB, RigNet, and MagicArticulate. For Precision (Prec.) and Recall (Rec.), higher values indicate better accuracy and coverage. For average L1-norm error (L1), lower is better. Here, * denotes models trained on Articulation-XL2.0 with the diverse-pose subset. Method Articulation-XL2.0 ModelsResource Diverse-pose Prec. Rec. L1 Prec. Rec. L1 Prec. Rec. L1 GVB RigNet MagicArti. Ours Ours* 72.9% 65.5% 0.745 73.7% 66.1% 0.729 74.6% 71.3% 0.451 87.6% 74.0% 0.335 87.9% 73.8% 0.333 69.3% 79.2% 0.687 65.7% 80.2% 0.707 68.1% 80.7% 0.642 79.7% 81.6% 0.443 79.8% 81.5% 0.442 75.2% 64.9% 0.786 74.7% 65.4% 0.746 74.9% 68.4% 0.479 83.6% 72.2% 0.405 86.4% 72.8% 0.353 Comparison results. Figure 5 visualizes each methods predicted skinning weights alongside their L1 error maps. Our method produces more accurate weight distributions with substantially lower errors across all benchmarks. RigNet exhibits large errors on all examples, while MagicArticulates functional diffusion performs well on Articulation-XL2.0 and the diverse-pose subset but degrades on ModelsResource, revealing limited cross-dataset generalization. Quantitative results in Table 2 confirm these observations, with our method outperforming all baselines on every metric and dataset. Moreover, our approach runs fasterachieving per-example inference speeds that are 1.75, 45, and 59 those of RigNet, MagicArticulate, and GVB, respectively (see appendix for details). 5. 3D animation results Baselines. We compare our animation results with L4GM [59] for video-to-4D generation and MotionDreamer [77] for 3D mesh animation. To ensure fair evaluation, L4GM is given the same input videos and its multi-view synthesis for the first frame is replaced with ground-truth renderings of the input 3D model. MotionDreamer receives the input 3D model along with the same text prompts used for video generation. In Figure 6, some of its outputs appear untextured because its watertight mesh conversion breaks the UV mappings. Comparison results. As shown in Figure 6, we present our generated skeletons and the corresponding video-guided animations. The shapes with skeletons represent the rest poses. Although L4GMs reference views are well aligned with the source video, it repeatedly produces geometric distortions 9 Figure 4: Comparison of skeleton results on generated meshes. The meshes are generated by Tripo 2.0 [2] and Hunyuan3D 2.0 [73]. Figure 5: Qualitative skinning weight prediction results. The data is from Articulation-XL2.0, ModelsResource, and the diverse-pose subset from top to bottom. Each example shows the predicted weight visualization alongside its L1 error map. Additional results are provided in the appendix. Figure 6: Comparison of animation results. We present our generated skeletons and corresponding video-guided animations. The shapes with skeletons represent rest poses. While L4GM [59] aligns its reference views closely with the input video, it consistently exhibits distortions (highlighted in red). MotionDreamers [77] animations are subtle and can introduce unintended deformations in rigid parts (e.g., the humanoid torso). In contrast, our method delivers accurate, artifact-free animations using fully generated rigging. Videos are included in the project page. (red highlights), even when provided with ground truth multi-view renderings. MotionDreamers animations are subtle and can introduce unintended deformations in rigid parts (e.g., the humanoid torso). By contrast, our approach produces accurate, artifact-free animations using fully generated rigging. 5.5 Ablation studies In this section, we present ablation studies on both skeleton generation and skinning weight prediction. All models are trained on Articulation-XL2.0 without the diverse-pose subset. Ablation studies on skeleton generation. We ablate four componentspose augmentation, order randomization, tokenization scheme, and skeleton ordering strategyto measure their effects on skeleton generation (see Table 3). Removing pose augmentation degrades performance across all benchmarks, especially on the diverse-pose test. Disabling order randomization similarly reduces 11 Table 3: Ablation studies on skeleton generation."
        },
        {
            "title": "Method",
            "content": "Articulation-XL2."
        },
        {
            "title": "ModelsResource",
            "content": "Diverse-pose J2J J2B B2B J2J J2B B2B J2J J2B B2B w/o pose aug. w/o random Bone token Spatial order Ours 3.131 3.166 3.014 2.982 3.033 2.451 2.431 2.309 2.298 2.300 2.223 2.057 1.939 2.068 1.923 3.994 3.902 3.865 3.868 3. 3.141 3.006 2.940 2.961 2.881 2.843 2.695 2.524 2.641 2.475 4.886 3.356 3.269 3.210 3.212 4.029 2.631 2.518 2.570 2.542 3.629 2.201 2.087 2.295 2.027 accuracy. Bone-based tokenization matches our methods quality but requires 12 extra training hours and is 1.6 slower at inference. Finally, replacing hierarchical ordering with spatial ordering preserves CD-J2J and CD-J2B but markedly increases CD-B2B error and often produces disconnected skeletons; see the appendix for visualization comparisons. Table 4: Ablation studies on skinning weight prediction."
        },
        {
            "title": "Method",
            "content": "Articulation-XL2."
        },
        {
            "title": "ModelsResource",
            "content": "Diverse-pose Prec. Rec. L1 Prec. Rec. L1 Prec. Rec. L1 Joint embed w/o TAJA w/o part feat. w/o pose aug. Ours 87.1% 73.8% 0.346 86.6% 74.2% 0.348 87.4% 73.8% 0.338 88.0% 73.3% 0.337 87.6% 74.0% 0.335 79.2% 80.8% 0.458 79.2% 81.4% 0.450 79.1% 81.0% 0.451 79.3% 80.7% 0.449 79.7% 81.6% 0.443 82.8% 72.2% 0.427 82.8% 72.5% 0.414 83.2% 72.1% 0.414 82.8% 70.1% 0.444 83.6% 72.2% 0.405 Ablation studies on skinning weight prediction. We evaluate four key components of our skinning weight prediction framework (see Table 4). First, replacing bone embeddings with joint embeddings increases the average L1-norm error by 4.0% across all three benchmarks, demonstrating the importance of explicitly modeling bone information. Second, replacing Topology-aware Joint Attention (TAJA) with standard self-attention leads to performance degradation across all benchmarks, highlighting the value of modeling topological relationships between joints. Third, removing part-aware features results in consistent performance drops, confirming their contribution to accurate weight prediction. Finally, eliminating pose augmentation during training increases the L1-norm error on the diverse-pose subset by 9.6%, demonstrating that pose variation is essential for generalization to novel poses. These findings confirm that each component is crucial to our models overall accuracy."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce Puppeteer, unified rigging-and-animation pipeline built on dataset with 59.4k high-quality rigged models. Puppeteer first generates skeletons with an autoregressive transformer that uses joint-based tokenization and hierarchical ordering with randomization to capture skeletal structures. An attention-based network with topology-aware features then predicts skinning weights, followed by an efficient optimization module that produces stable, high-quality animations at low computational cost. Across multiple benchmarks, Puppeteer outperforms state-of-the-art methods in skeleton fidelity, skinning accuracy, and animation smoothness."
        },
        {
            "title": "Acknowledgements",
            "content": "This research is supported by the MoE AcRF Tier 2 grant (MOE-T2EP20223-0001) and the MoE AcRF Tier 1 grant (RG14/22)."
        },
        {
            "title": "Appendix",
            "content": "In this appendix, we provide additional details and experimental results for the main paper, including: Further details of Puppeteer (Appendix A) and Articulation-XL2.0 (Appendix B); Additional experimental results on skeleton generation, skinning weight prediction, and animation (Appendix C); discussion of the limitations of our work and future works (Appendix D), and broader impact considerations (Appendix E)."
        },
        {
            "title": "A More details of Puppeteer",
            "content": "A.1 Implementation details Skeleton generation. Our skeleton generation begins by encoding each mesh with pre-trained shape encoder [105]. We first compute its signed distance function using [79], reconstruct coarse mesh using Marching Cubes [49], and then sample 8,192 surface points (with normals). These points are finally encoded into fixed sequence of 257 shape tokens. We normalize input points to [0.5, 0.5] and apply the same scale and translation to joint positions for alignment. Joint coordinates are then discretized into 1283 grids, and parent indices are appendedyielding token sequence of length 4j. During training, we apply pose augmentation with probability of 0.5. When pose augmentation is applied, each joint has 0.3 probability of rotation, with rotation angles constrained to the range of [60, 60]. Sequence ordering randomization is annealed following [92], with permutation probability that starts at 1 and falls to 0 (reverting to hierarchical order) over training: = 1.0, 1 0.0, epoch E/2 E/4 , epoch [0, E/2], epoch [E/2, 3E/4], (6) epoch [3E/4, E], where epoch is the current epoch and the total number of epochs. Besides, we apply target-aware positional indicators that mark which joint group the model should generate next. Both shape and skeleton token sequence (except for the final joint group) are augmented with an indicator denoting their next group; the indicator on the shape tokens specifically identifies the initial joint group to generate. The auto-regressive transformer is trained on 8 NVIDIA A100 GPUs (batch size 64 per GPU, effective batch 512) for approximately 3 days and 20 hours. Skinning weight prediction. During training, we condition the attention-based network for skinning weight prediction on the ground truth skeleton and supervise it with the corresponding skinning weights. We sample 8,192 surface points (with normals) from each meshmatching our skeleton pipelineand assign each point the weights of its nearest vertex. During inference, these predicted weights are transferred back to mesh vertices through nearest-neighbor mapping. Training is performed on Articulation-XL2.0 with 8 NVIDIA A100 GPUs for roughly 1 day and 6 hours, with batch size of 16 per GPU. valid-joint masksupporting up to 70 jointsadapts the network to the varying skeleton sizes encountered during both training and evaluation. Video-guided 3D animation. Here, we describe our video-guided 3D animation process in detail. For rendering supervision, we employ four distinct loss functions. Given generated video = {I0, I1, ..., In1} and the rendered images using Pytorch3D [57], we calculate the following rendering losses: (cid:88) Lrgb = Mi (Ii i)2, Lmask = (cid:88) BCE(Mi, i), Ldepth = (cid:88) Mi (Di i)2, Lf low = Mi (Fi i)2, (7) (cid:88) 13 where Mi represents the binary mask of foreground objects in the video frames, and denotes the mask of the 3D object rendered via Pytorch3D. We extract depth maps Di using the method proposed in [10], while is obtained directly from the Pytorch3D renderer. We apply scale-shift alignment to handle the scale ambiguity between relative depth Di and metric depth i. For optical flow estimation, we compute Fi using the approach from [53], and derive by projecting the 3D vertex flow onto the 2D image plane. The element-wise multiplication operator indicates that Lrgb, Lf low, and Ldepth are all computed only within the foreground region defined by mask Mi, ensuring that our optimization focuses on the target object. The tracking losses incorporate 2D joint tracking term and 2D vertex tracking term that leverage Cotracker3 [35] to trace selected keypoints throughout the video sequence. We project visible joints and vertices of the 3D static object onto image planes to establish keypoints for the first frame. These keypoints are then tracked through the entire video sequence using Cotracker3 to obtain pi, which represents the tracked positions at each frame i. Simultaneously, is derived by projecting the deformed joints and mesh vertices onto image planes. The tracking losses are formulated as: Ljoint_track = Lpoint_track = (cid:88) (cid:88) (cid:13) (cid:13)Mj (pi,joint i,joint)(cid:13) 2 (cid:13) , (cid:13) (cid:13)Mv (pi,vertex i,vertex)(cid:13) 2 (cid:13) , (8) where Mj and Mv denote the visibility masks for joints and vertices in the first frame, respectively. These masks ensure that only visible keypoints contribute to the optimization process. Additionally, we also incorporate regularization terms that prevent temporal jittering by penalizing large transformation changes between consecutive frames. To balance these losses, we weight each term to ensure comparable magnitudes. In practice, regularization losses are down-weighted by 34 orders of magnitude relative to rendering and tracking losses to prevent over-smoothing. Our animation optimization takes approximately 20 minutes for objects with up to 10K vertices on single NVIDIA A100 GPU, processing 5-second videos (approximately 50 frames at 10 FPS) generated by Kling AI [1] or JiMeng AI [34]. Runtime scales with both mesh complexity and frame count: (1) Mesh complexity: Models with more vertices will require additional Pytorch3D rendering time. For example, the bat case (70K vertices) in our project page requires 90 minutes, while the turtle case (15K vertices) takes 35 minutes. (2) Frame count: For typical case taking 20 minutes at 50 frames (10 FPS, 5 seconds), increasing to 20 FPS (100 frames) extends optimization time to 41 minutes, while reducing to 4 FPS (20 frames) decreases it to 8 minutes, demonstrating approximately linear scaling with frame count. After optimization, our animation process follows standard skeletal animation principles [56]: (1) Forward Kinematics (FK): We compute global joint transformations from optimized local transformations using hierarchical forward kinematics, traversing the skeleton from root to leaves; (2) Linear Blend Skinning (LBS) [36]: Deform mesh vertices using weighted combinations of joint transformations, where each vertex is influenced by multiple joints according to skinning weights. This produces the final mesh animation sequence. A.2 Experimental details For baseline comparisons, we use the publicly available implementations of UniRig [100], RigNet [86], and Pinocchio [6] from their respective GitHub repositories. The Geodesic Voxel Binding (GVB) [18] comparison utilizes the implementation in Autodesk Maya [27]. RigNet and MagicArticulate [67] are trained on Articulation-XL2.0 using the original data preprocessing pipelines and training schedules specified by their authors. More details of Articulation-XL2.0 Our dataset Articulation-XL2.0 is sourced from Objaverse-XL [15, 16], specifically focusing on the GitHub and Sketchfab subsets that include file types containing rigging information (e.g., glb/gltf, fbx, dae, blend, etc.). From an initial 2.97M models, we extract 74K rigged assets (after removing over 150K duplicates) and curate 48K high-quality rigged models through quality verification. Table S5 14 Figure S7: Bone number distributions of Articulation-XL2.0. details these statistics. The number of bones in Articulation-XL2.0 ranges from 2 to 100, with the distribution illustrated in Figure S7. The diverse-pose subset in Articulation-XL2.0 is derived from two sources. The first component consists of poses extracted from data with animation, where we specifically selected frames exhibiting maximum deviation from rest pose configurations to capture extreme articulations. The second component comprises synthetically generated poses created using SMALR [107, 108] with parameterizations derived from 41 distinct animal scans and randomized valid poses. These randomized valid poses are generated by applying random rotation angles to SMALR animal joints while constraining angles within anatomically valid ranges, ensuring diverse articulation states while maintaining biological plausibility. Refer to Figure S9 for the skeleton structure and joint names of SMALR data. There are originally two joints in index 0, we merge them into single root joint. Some examples from this diverse-pose collection are illustrated in Figure S8. Note that the models in the diverse-pose test set, along with their corresponding rest poses, are entirely excluded from the training data, ensuring rigorous evaluation of generalization to both novel shapes and novel articulations. Table S5: Data statics for Articulation-XL2.0. Source All models with rigging high-quality rigging low-quality rigging Sketchfab GitHub Total 0.89M 2.08M 2.97M 64K 10K 74K 42K 6K 48K 22K 4K 26K"
        },
        {
            "title": "C Additional experimental results",
            "content": "C.1 More results of skeleton generation More qualitative results on test sets. Figure S10 presents additional qualitative comparisons of skeleton generation by our method, MagicArticulate [67], and RigNet [86] on meshes from Articulation-XL2.0, ModelsResource, and the diverse-pose subset. Our method produces more valid 15 Figure S8: Examples from the diverse-pose subset of Articulation-XL2.0. The left group showcases animation-derived samples: the top row displays the original rest poses, while the bottom row shows their corresponding deformed articulations extracted from animation sequences at frames of maximum pose deviation. The right group displays synthetically generated articulations created using SMALR [107, 108]. Figure S9: Skeleton structure and joint names of SMALR data [107, 108]. skeletons and even corrects artist-created errors, such as the missing deer legs in the first row, and the missing penguin arms in rows 4 and 5. Sequence ordering. As supplement to the ablation study in the main paper, Figure S11 compares skeletons generated with hierarchical and spatial ordering. Using spatial ordering always produces disconnected skeletons, as child joints generated before their parents create invalid parent references. Inference time. We compare average inference times on Articulation-XL2.0-test (see Table S6). Excluding data preprocessing for all methods, our approach is 2.6 faster than Pinocchio [6], 3.0 faster than RigNet [86], 1.9 faster than UniRig [100], and 1.6 faster than MagicArticulate [67]. Table S6: Inference time of skeleton generation. Method Pinocchio RigNet UniRig MagicArticulate Ours Inference time 3.9s 4.5s 2.9s 2.4s 1.5s C.2 More results of skinning weight prediction Deformation error report. In addition to the precision, recall, and L1-norm for evaluating skinning weight accuracy presented in the main paper, we also conducted comprehensive assessment of practical efficacy through deformation error analysis. This supplementary metric quantifies the mean Euclidean distance between vertices deformed via predicted skinning weights and those deformed via ground truth weights across diverse set of 10 randomly generated poses. As evidenced in Table S7, the proposed methodology exhibits better performance for all experimental datasets. Ablation studies on block depth. In addition to the ablation results in Section 5.5, we also ablate the block depth. For the green attention block in Figure 2, we vary the number of stacked blocks, which we refer to as depth. When depth is 1 (44.3M parameters), this corresponds to single block. 16 Figure S10: Comparison of skeleton generation results on test sets. From the top: six examples from Articulation-XL2.0, four from ModelsResource, and four from the diverse-pose subset. Our method produces valid skeletons and even corrects artist-created errors (e.g., missing deer legs in row 1, missing penguin arms in rows 45). Figure S11: Comparison of sequence ordering. Skeletons generated with hierarchical ordering (ours) remain connected, while spatial ordering always yields disconnected structures. Table S7: Quantitative comparison of skinning weight prediction. We evaluate our approach against GVB, RigNet, and MagicArticulate. For average L1-norm error (L1) and average distance error (avg Dist.), lower is better. Here, * denotes models trained on Articulation-XL2.0 with the diverse-pose subset."
        },
        {
            "title": "Method",
            "content": "Articulation-XL2."
        },
        {
            "title": "ModelsResource",
            "content": "Diverse-pose L1 avg Dist. L1 avg Dist. L1 avg Dist. GVB RigNet MagicArti. Ours Ours* 0.745 0.729 0.451 0.335 0.333 0.0087 0.0082 0.0051 0.0043 0.0042 0.687 0.707 0.642 0.443 0.442 0.0067 0.0078 0.0064 0.0044 0. 0.786 0.746 0.479 0.405 0.353 0.0084 0.0089 0.0067 0.0061 0.0053 Table S8: Ablation study on attention block depth in the skinning weight prediction network."
        },
        {
            "title": "Method",
            "content": "Articulation-XL2."
        },
        {
            "title": "ModelsResource",
            "content": "Diverse-pose Prec. Rec. L1 Prec. Rec. L1 Prec. Rec. L1 depth = 1 depth = 2 depth = 3 87.6% 74.0% 0.335 89.3% 73.0% 0.316 89.4% 72.5% 0.317 79.7% 81.6% 0.443 79.8% 80.2% 0.453 79.6% 79.7% 0.461 83.6% 72.2% 0.405 85.8% 71.1% 0.392 85.7% 70.8% 0.391 We incrementally increase the block depth, corresponding to larger model size, and evaluate the resulting performance. The results are shown in Table S8: when depth is set to 2 (87.7M parameters), performance on Articulation-XL2.0 and the diverse-pose subset improves noticeably, but there is slight degradation on ModelsResource. When depth is 3 (130.9M parameters), performance on Articulation-XL2.0 and the diverse-pose subset shows comparable results to depth=2 but still exhibits drop on ModelsResource. We attribute the drop on ModelsResource to the orientation distribution difference between Articulation-XL2.0 and ModelsResource: after training on Articulation-XL2.0 with highly varied orientations, the larger model may become biased toward diverse orientations, leading to degraded performance when testing on ModelsResource, which contains only front-facing orientations. For depth=3, the comparable performance to depth=2 without obvious improvement suggests that the model capacity may have reached saturation for the current dataset size. More qualitative results. Figure S12 presents additional qualitative comparisons of skinning weight predictions by our method, MagicArticulate [67], and RigNet [86] on meshes from ArticulationXL2.0, ModelsResource, and the diverse-pose subset. Each example pairs the predicted weight map with its L1 error map against artist-painted references, highlighting our methods superior accuracy across diverse object categories. Inference time. We also compare average inference times for skinning weight prediction on Articulation-XL2.0-test (see Table S9). Excluding data preprocessing, our method is 59 faster than GVB [18], 1.75 faster than RigNet [86], and 45 faster than MagicArticulate [67]. C.3 More results of animation More qualitative results. We present more animation results in Figure S13. The input meshes are generated by Tripo 2.0 [2] and Hunyuan3D 2.0 [73]. Despite well-aligned reference views, L4GM [59] consistently produces geometric distortions (highlighted in red), even with ground-truth multi-view renderings. MotionDreamer [77] generates subtle animations and introduces unintended Table S9: Inference time of skinning weight prediction. Method GVB RigNet MagicArticulate Ours Inference time 1.895s 0.056s 1.430s 0.032s 18 Figure S12: Comparison of skinning weight prediction results. From the top: three examples from Articulation-XL2.0, two from ModelsResource, and two from the diverse-pose subset. Each pair shows the predicted weight visualization alongside its L1 error map. Our predictions more closely match the artist-painted references. 19 Figure S13: Comparison of animation results on AI-generated meshes. The meshes are generated by Tripo 2.0 [2] and Hunyuan3D 2.0 [73]. The shapes with skeletons represent the rest poses. deformations in rigid parts like the turtle shell. In contrast, our approach produces accurate, artifactfree animations with our generated rigging. User study. We conducted user studies with 21 participants to evaluate animation quality across three methods: L4GM [59], MotionDreamer [77], and our approach. Participants compared 8 animation examples across three evaluation criteria: (1) Video-Animation Alignment: Which animation result shows better alignment with the input video? (2) Motion Quality: Which one has more natural and realistic motion? (3) 3D Geometry Preservation: Which method better maintains the original 3D object geometry without introducing distortions or artifacts? Results are shown in Appendix C.3. Our method outperforms both L4GM and MotionDreamer across all three evaluation dimensions. Note that Video-Animation Alignment is not evaluated for MotionDreamer since it uses text-driven motion generation rather than video guidance. Table S10: User study evaluation of animation results. Method Video-Animation Align. Motion Quality Geometry Preservation MotionDreamer L4GM Ours - 19.64% 80.36% 0 16.67% 83.33% 0 18.45% 81.55%"
        },
        {
            "title": "D Discussions",
            "content": "Despite its strong performance, our framework has two main limitations. First, it cannot capture fine-scale deformations, such as flowing hair or fluttering cloth, because no skeletons are generated for these highly deformable parts. clear example is the swimming turtle sequence on our project page. While the reference video shows very soft motion of the turtles forelimbs, our animation results appear less smooth due to insufficient joint density in these regions. This limitation stems from our skeleton generation producing few joints in areas requiring fine-scale deformations. Animation-driven joint refinement could improve smoothness but remains future work. Second, the animation stage relies on per-scene optimization, which prevents real-time deployment. An end-to-end feed-forward model that predicts animation directly would eliminate this bottleneck. Beyond these structural issues, several practical factors also affect animation quality. (1) Complex motion: Rapid movements with large joint rotations present challenges. For instance, in the seahorse case on our project page, while we capture the overall tail oscillation pattern, precise alignment with fine-scale movements remains difficult. Adaptive frame sampling based on optical flow magnitudesampling more densely during rapid motioncould potentially address these challenges. (2) Video generation quality: Although current text-to-video models (Kling AI [1], JiMeng AI [34]) 20 can generate complex motions with high success rates, video quality directly impacts animation fidelity. Motion blur or temporal inconsistencies degrade joint/vertex tracking accuracy and make optimization more challenging. We mitigate this by generating multiple video candidates and selecting the highest-quality one based on visual clarity and motion consistency. While this approach helps reduce the impact of poor-quality videos, video generation quality still represents limitation for highly complex motion scenarios. (3) Viewpoint and occlusion issues: Suboptimal camera angles can cause depth ambiguities and tracking failures. While we can select optimal viewpoints that maximize joint visibility using the input 3D mesh, single-view optimization inherently struggles when critical joints remain occluded throughout the sequence. Multi-view priors could provide better geometric understanding of occluded regions."
        },
        {
            "title": "E Broader impact",
            "content": "Beyond its technical breakthroughs, this work holds significant societal implications by removing the need for specialized expertise and empowering diverse range of creators who were once excluded from animation tools. As digital environments increasingly influence how we work, learn, and connect, expanding access to animated content creation becomes not only technically valuable but socially essential. However, democratizing animation technology also raises concerns about potential misuse in creating deceptive content and impacts on traditional animation industry employment. Our ultimate vision is to transform 3D animation from an exclusive professional skill into an intuitive creative medium accessible to everyone, while encouraging responsible use."
        },
        {
            "title": "References",
            "content": "[1] K. AI. Kling ai, 2025. URL https://klingai.com/. [2] T. AI. Tripo 3d, 2023. URL https://www.tripo3d.ai/. [3] O. K.-C. Au, C.-L. Tai, H.-K. Chu, D. Cohen-Or, and T.-Y. Lee. Skeleton extraction by mesh contraction. ACM transactions on graphics (TOG), 27(3):110, 2008. [4] S. Bahmani, X. Liu, W. Yifan, I. Skorokhodov, V. Rong, Z. Liu, X. Liu, J. J. Park, S. Tulyakov, G. Wetzstein, A. Tagliasacchi, and D. B. Lindell. Tc4d: Trajectory-conditioned text-to-4d generation. arXiv, 2024. [5] S. Bahmani, I. Skorokhodov, V. Rong, G. Wetzstein, L. Guibas, P. Wonka, S. Tulyakov, J. J. Park, A. Tagliasacchi, and D. B. Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79968006, 2024. [6] I. Baran and J. Popovic. Automatic rigging and animation of 3d characters. ACM Transactions on graphics (TOG), 26(3):72es, 2007. [7] J. Cao, A. Tagliasacchi, M. Olson, H. Zhang, and Z. Su. Point cloud skeletons via laplacian based contraction. In 2010 Shape Modeling International Conference, pages 187197. IEEE, 2010. [8] C. Chen, S. Huang, X. Chen, G. Chen, X. Han, K. Zhang, and M. Gong. Ct4d: Consistent text-to-4d generation with animatable meshes. arXiv preprint arXiv:2408.08342, 2024. [9] J. Chen, B. Zhang, X. Tang, and P. Wonka. V2m4: 4d mesh animation reconstruction from single monocular video. arXiv preprint arXiv:2503.09631, 2025. [10] S. Chen, H. Guo, S. Zhu, F. Zhang, Z. Huang, J. Feng, and B. Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv:2501.12375, 2025. [11] Y. Chen, T. He, D. Huang, W. Ye, S. Chen, J. Tang, X. Chen, Z. Cai, L. Yang, G. Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. 21 [12] Y. Chen, Y. Wang, Y. Luo, Z. Wang, Z. Chen, J. Zhu, C. Zhang, and G. Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. [13] Z. Chu, F. Xiong, M. Liu, J. Zhang, M. Shao, Z. Sun, D. Wang, and M. Xu. Humanrig: Learning automatic rigging for humanoid character in large scale dataset, 2024. URL https://arxiv.org/abs/2412.02317. [14] E. De Aguiar, C. Theobalt, S. Thrun, and H.-P. Seidel. Automatic conversion of mesh animations into skeleton-based animations. In Computer Graphics Forum, volume 27, pages 389397. Wiley Online Library, 2008. [15] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. [16] M. Deitke, R. Liu, M. Wallingford, H. Ngo, O. Michel, A. Kusupati, A. Fan, C. Laforte, V. Voleti, S. Y. Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. [17] Y. Deng, Y. Zhang, C. Geng, S. Wu, and J. Wu. Anymate: dataset and baselines for learning 3d object rigging. In SIGGRAPH, 2025. [18] O. Dionne and M. de Lasa. Geodesic voxel binding for production character meshes. In Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pages 173180, 2013. [19] A. Dodik, V. Sitzmann, J. Solomon, and O. Stein. Robust biharmonic skinning using geometric fields. arXiv preprint arXiv:2406.00238, 2024. [20] Z. Fu, J. Wei, W. Shen, C. Song, X. Yang, F. Liu, X. Yang, and G. Lin. Sync4d: Video guided controllable dynamics for physics-based 4d generation. arXiv preprint arXiv:2405.16849, 2024. [21] Q. Gao, Q. Xu, Z. Cao, B. Mildenhall, W. Ma, L. Chen, D. Tang, and U. Neumann. Gaussianflow: Splatting gaussian dynamics for 4d content creation. 2024. [22] I. Gat, S. Raab, G. Tevet, Y. Reshef, A. H. Bermano, and D. Cohen-Or. Anytop: Character animation diffusion with any topology. arXiv preprint arXiv:2502.17327, 2025. [23] C. Guo, S. Zou, X. Zuo, S. Wang, W. Ji, X. Li, and L. Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51525161, 2022. [24] Z. Guo, J. Xiang, K. Ma, W. Zhou, H. Li, and R. Zhang. Make-it-animatable: An efficient framework for authoring animation-ready 3d characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [25] H. Huang, S. Wu, D. Cohen-Or, M. Gong, H. Zhang, G. Li, and B. Chen. L1-medial skeleton of point cloud. ACM Trans. Graph., 32(4):651, 2013. [26] A. Inc. Mixamo. URL https://www.mixamo.com/. [27] A. Inc. Autodesk maya, 2024. URL https://www.autodesk.com/products/maya/ overview. Version 2024. [28] A. Jacobson, I. Baran, J. Popovic, and O. Sorkine. Bounded biharmonic weights for real-time deformation. ACM Trans. Graph., 30(4):78, 2011. [29] A. Jacobson, D. Panozzo, et al. libigl: simple C++ geometry processing library, 2018. https://libigl.github.io/. [30] D. L. James and C. D. Twigg. Skinning mesh animations. ACM Transactions on Graphics (TOG), 24(3):399407, 2005. 22 [31] B. Jiang, X. Chen, W. Liu, J. Yu, G. Yu, and T. Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [32] Y. Jiang, L. Zhang, J. Gao, W. Hu, and Y. Yao. Consistent4d: Consistent 360 {deg} dynamic object generation from monocular video. arXiv preprint arXiv:2311.02848, 2023. [33] Y. Jiang, C. Yu, C. Cao, F. Wang, W. Hu, and J. Gao. Animate3d: Animating any 3d model with multi-view video diffusion. arXiv preprint arXiv:2407.11398, 2024. [34] JiMeng AI. Jimeng ai, 2025. URL https://jimeng.jianying.com. [35] N. Karaev, I. Makarov, J. Wang, N. Neverova, A. Vedaldi, and C. Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. [36] J. P. Lewis, M. Cordner, and N. Fong. Pose space deformation: unified approach to shape interpolation and skeleton-driven deformation. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 811818. 2023. [37] P. Li, K. Aberman, R. Hanocka, L. Liu, O. Sorkine-Hornung, and B. Chen. Learning skeletal articulations with neural blend shapes. ACM Transactions on Graphics (TOG), 40(4):115, 2021. [38] X. Li, Q. Ma, T.-Y. Lin, Y. Chen, C. Jiang, M.-Y. Liu, and D. Xiang. Articulated kinematics distillation from video diffusion models. arXiv preprint arXiv:2504.01204, 2025. URL https://arxiv.org/abs/2504.01204. [39] Z. Li, Y. Chen, and P. Liu. Dreammesh4d: Video-to-4d generation with sparse-controlled gaussian-mesh hybrid representation. Advances in Neural Information Processing Systems, 37: 2137721400, 2024. [40] Z. Li, D. Litvak, R. Li, Y. Zhang, T. Jakab, C. Rupprecht, S. Wu, A. Vedaldi, and J. Wu. Learning the 3d fauna of the web. In CVPR, 2024. [41] H. Liang, Y. Yin, D. Xu, H. Liang, Z. Wang, K. N. Plataniotis, Y. Zhao, and Y. Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. [42] C. Lin, C. Li, Y. Liu, N. Chen, Y.-K. Choi, and W. Wang. Point2skeleton: Learning skeletal representations from point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42774286, 2021. [43] J. Lin, Z. Wang, Y. Hou, Y. Tang, and M. Jiang. Phy124: Fast physics-driven 4d content generation from single image. arXiv preprint arXiv:2409.07179, 2024. [44] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 85768588, 2024. [45] I. Liu, Z. Xu, W. Yifan, H. Tan, Z. Xu, X. Wang, H. Su, and Z. Shi. Riganything: Template-free autoregressive rigging for diverse 3d assets. arXiv preprint arXiv:2502.09615, 2025. [46] L. Liu, Y. Zheng, D. Tang, Y. Yuan, C. Fan, and K. Zhou. Neuroskinning: Automatic skin binding for production characters with deep graph networks. ACM Transactions on Graphics (ToG), 38(4):112, 2019. [47] M. Liu, M. A. Uy, D. Xiang, H. Su, S. Fidler, N. Sharp, and J. Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. arXiv preprint arXiv:2504.11451, 2025. [48] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: skinned multiperson linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, Oct. 2015. [49] W. E. Lorensen and H. E. Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 23 [50] Q. Miao, J. Quan, K. Li, and Y. Luo. Pla4d: Pixel-level alignments for text-to-4d gaussian splatting. arXiv preprint arXiv:2405.19957, 2024. [51] Q. Miao, K. Li, J. Quan, Z. Min, S. Ma, Y. Xu, Y. Yang, and Y. Luo. Advances in 4d generation: survey. arXiv preprint arXiv:2503.14501, 2025. [52] M. B. S. Millán, A. Dai, and M. Nießner. Animating the uncaptured: Humanoid mesh animation with video diffusion models. arXiv preprint arXiv:2503.15996, 2025. [53] H. Morimitsu, X. Zhu, R. M. Cesar-Jr., X. Ji, and X.-C. Yin. DPFlow: Adaptive optical flow estimation with dual-pyramid framework. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [54] A. Mosella-Montoro and J. Ruiz-Hidalgo. Skinningnet: Two-stream graph convolutional neural network for skinning prediction of synthetic characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1859318602, 2022. [55] X. Pan, J. Huang, J. Mai, H. Wang, H. Li, T. Su, W. Wang, and X. Jin. Heterskinnet: heterogeneous network for skin weights prediction. Proc. ACM Comput. Graph. Interact. Tech., 4(1), Apr. 2021. doi: 10.1145/3451262. URL https://doi.org/10.1145/3451262. [56] R. Parent. Computer animation: algorithms and techniques. Newnes, 2012. [57] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501, 2020. [58] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. [59] J. Ren, C. Xie, A. Mirzaei, K. Kreis, Z. Liu, A. Torralba, S. Fidler, S. W. Kim, H. Ling, et al. L4gm: Large 4d gaussian reconstruction model. Advances in Neural Information Processing Systems, 37:5682856858, 2024. [60] W. Shen, W. Yin, H. Wang, C. Wei, Z. Cai, L. Yang, and G. Lin. Hmr-adapter: lightweight adapter with dual-path cross augmentation for expressive human mesh recovery. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 60936102, 2024. [61] W. Shen, W. Yin, X. Yang, C. Chen, C. Song, Z. Cai, L. Yang, H. Wang, and G. Lin. Adhmr: Aligning diffusion-based human mesh recovery via direct preference optimization. arXiv preprint arXiv:2505.10250, 2025. [62] Y. Siddiqui, A. Alliegro, A. Artemov, T. Tommasi, D. Sirigatti, V. Rosov, A. Dai, and M. Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1961519625, 2024. [63] C. Song, J. Wei, R. Li, F. Liu, and G. Lin. 3d pose transfer with correspondence learning and mesh refinement. Advances in Neural Information Processing Systems, 34:31083120, 2021. [64] C. Song, J. Wei, R. Li, F. Liu, and G. Lin. Unsupervised 3d pose transfer with cross consistency and dual reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45 (8):1048810499, 2023. [65] C. Song, J. Wei, T. Chen, Y. Chen, C.-S. Foo, F. Liu, and G. Lin. Moda: Modeling deformable 3d objects from casual videos. International Journal of Computer Vision, pages 120, 2024. [66] C. Song, J. Wei, C. S. Foo, G. Lin, and F. Liu. Reacto: Reconstructing articulated objects from single video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53845395, 2024. [67] C. Song, J. Zhang, X. Li, F. Yang, Y. Chen, Z. Xu, J. H. Liew, X. Guo, F. Liu, J. Feng, and G. Lin. Magicarticulate: Make your 3d models articulation-ready. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 24 [68] K. Sun, D. Litvak, Y. Zhang, H. Li, J. Wu, and S. Wu. Ponymation: Learning articulated 3d animal motions from unlabeled online videos. In ECCV, 2024. [69] M. Sun, J. Chen, J. Dong, Y. Chen, X. Jiang, S. Mao, P. Jiang, J. Wang, B. Dai, and R. Huang. Drive: Diffusion-based rigging empowers generation of versatile and expressive characters. arXiv preprint arXiv:2411.17423, 2024. [70] Q. Sun, Z. Guo, Z. Wan, J. N. Yan, S. Yin, W. Zhou, J. Liao, and H. Li. Eg4d: Explicit generation of 4d object without score distillation. arXiv preprint arXiv:2405.18132, 2024. [71] A. Tagliasacchi, I. Alhashim, M. Olson, and H. Zhang. Mean curvature skeletons. In Computer Graphics Forum, volume 31, pages 17351744. Wiley Online Library, 2012. [72] J. Tang, Z. Li, Z. Hao, X. Liu, G. Zeng, M.-Y. Liu, and Q. Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. [73] T. H. Team. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025. [74] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, pages 358374. Springer, 2022. [75] G. Tevet, S. Raab, B. Gordon, Y. Shafir, D. Cohen-Or, and A. H. Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. [76] The Models-Resource. The models-resource, 2019. URL https://www.models-resource. com/. [77] L. Uzolas, E. Eisemann, and P. Kellnhofer. Motiondreamer: Exploring semantic video diffusion features for zero-shot 3d mesh animation. In International Conference on 3D Vision 2025, 2025. [78] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [79] P.-S. Wang, Y. Liu, and X. Tong. Dual octree graph networks for learning adaptive volumetric shape representations. ACM Transactions on Graphics (TOG), 41(4):115, 2022. [80] X. Wang, Y. Wang, J. Ye, F. Sun, Z. Wang, L. Wang, P. Liu, K. Sun, X. Wang, W. Xie, et al. Animatabledreamer: Text-guided non-rigid 3d model generation and reconstruction with canonical score distillation. In European Conference on Computer Vision, pages 321339. Springer, 2024. [81] S. Wu, R. Li, T. Jakab, C. Rupprecht, and A. Vedaldi. MagicPony: Learning articulated 3d animals in the wild. In CVPR, 2023. [82] Z. Wu, C. Yu, Y. Jiang, C. Cao, F. Wang, and X. Bai. Sc4d: Sparse-controlled video-to-4d generation and motion transfer. In European Conference on Computer Vision, pages 361379. Springer, 2024. [83] Y. Xie, C.-H. Yao, V. Voleti, H. Jiang, and V. Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. [84] J. L. Z. W. D. Xu and S. J. Y. G. M. Jiang. Phys4dgen: Physics-compliant 4d generation with multi-material composition perception. [85] Z. Xu, Y. Zhou, E. Kalogerakis, and K. Singh. Predicting animation skeletons for 3d articulated models via volumetric nets. In 2019 international conference on 3D vision (3DV), pages 298307. IEEE, 2019. [86] Z. Xu, Y. Zhou, E. Kalogerakis, C. Landreth, and K. Singh. Rignet: Neural rigging for articulated characters. arXiv preprint arXiv:2005.00559, 2020. [87] Z. Xu, Y. Zhou, L. Yi, and E. Kalogerakis. Morig: Motion-aware rigging of character meshes from point clouds. In SIGGRAPH Asia 2022 conference papers, pages 19, 2022. [88] G. Yang, M. Vo, N. Neverova, D. Ramanan, A. Vedaldi, and H. Joo. Banmo: Building animatable 3d neural models from many casual videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28632873, 2022. [89] C.-H. Yao, Y. Xie, V. Voleti, H. Jiang, and V. Jampani. Sv4d 2.0: Enhancing spatio-temporal consistency in multi-view video diffusion for high-quality 4d generation. arXiv preprint arXiv:2503.16396, 2025. [90] Y. Yao, Z. Deng, and J. Hou. Riggs: Rigging of 3d gaussians for modeling articulated objects in videos. In CVPR, 2025. [91] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei. 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023. [92] Q. Yu, J. He, X. Deng, X. Shen, and L.-C. Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [93] Y.-J. Yuan, L. Kobbelt, J. Liu, Y. Zhang, P. Wan, Y.-K. Lai, and L. Gao. 4dynamic: Text-to-4d generation with hybrid priors. arXiv preprint arXiv:2407.12684, 2024. [94] K. Yun, S. Hong, C. Kim, and J. Noh. Anymole: Any character motion in-betweening leveraging video diffusion models. arXiv preprint arXiv:2503.08417, 2025. [95] B. Zeng, L. Yang, S. Li, J. Liu, Z. Zhang, J. Tian, K. Zhu, Y. Guo, F.-Y. Wang, M. Xu, et al. Trans4d: Realistic geometry-aware transition for compositional text-to-4d synthesis. arXiv preprint arXiv:2410.07155, 2024. [96] Y. Zeng, Y. Jiang, S. Zhu, Y. Lu, Y. Lin, H. Zhu, W. Hu, X. Cao, and Y. Yao. Stag4d: Spatialtemporal anchored generative 4d gaussians. In European Conference on Computer Vision, pages 163179. Springer, 2024. [97] B. Zhang and P. Wonka. Functional diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47234732, 2024. [98] H. Zhang, D. Chang, F. Li, M. Soleymani, and N. Ahuja. Magicpose4d: Crafting articulated models with appearance and motion control. arXiv preprint arXiv:2405.14017, 2024. [99] H. Zhang, X. Chen, Y. Wang, X. Liu, Y. Wang, and Y. Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. Advances in Neural Information Processing Systems, 37: 1527215295, 2024. [100] J.-P. Zhang, C.-F. Pu, M.-H. Guo, Y.-P. Cao, and S.-M. Hu. One model to rig them all: Diverse skeleton rigging with unirig. arXiv preprint arXiv:2504.12451, 2025. [101] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE transactions on pattern analysis and machine intelligence, 46(6):41154128, 2024. [102] M. Zhang, D. Jin, C. Gu, F. Hong, Z. Cai, J. Huang, C. Zhang, X. Guo, L. Yang, Y. He, et al. Large motion model for unified multi-modal motion generation. In European Conference on Computer Vision, pages 397421. Springer, 2024. [103] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [104] Y. Zhao, Z. Yan, E. Xie, L. Hong, Z. Li, and G. H. Lee. Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023. [105] Z. Zhao, W. Liu, X. Chen, X. Zeng, R. Wang, P. Cheng, B. Fu, T. Chen, G. Yu, and S. Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. 26 [106] H. Zhu, T. He, X. Yu, J. Guo, Z. Chen, and J. Bian. Ar4d: Autoregressive 4d generation from monocular videos. arXiv preprint arXiv:2501.01722, 2025. [107] S. Zuffi, A. Kanazawa, D. Jacobs, and M. J. Black. 3D menagerie: Modeling the 3D shape and pose of animals. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), July 2017. [108] S. Zuffi, A. Kanazawa, and M. J. Black. Lions and tigers and bears: Capturing non-rigid, 3D, articulated shape from images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2018."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Institute for Infocomm Research, A*STAR",
        "Nanyang Technological University"
    ]
}