{
    "paper_title": "Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning",
    "authors": [
        "Awni Altabaa",
        "Siyu Chen",
        "John Lafferty",
        "Zhuoran Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 5 9 0 4 1 . 0 1 5 2 : r Unlocking Out-of-Distribution Generalization in"
        },
        {
            "title": "Zhuoran Yang",
            "content": "Department of Statistics & Data Science, Yale University {awni.altabaa, siyu.chen.sc3226, john.lafferty, zhuoran.yang}@yale.edu Abstract. Systematic, compositional generalization beyond the training distribution remains core challenge in machine learningand critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using GSM8K-style modular arithmetic on computational graphs task as testbed. We introduce and explore set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities. Date: October 17, 2025 Code: https://github.com/Awni00/algorithmic-generalization-transformer-architectures (a) Recurrence & Adaptive Computation (b) Algorithmic Supervision (c) Anchored Discrete Latent Space (d) Error Correction Figure 1. Four key mechanisms enabling robust out-of-distribution generalization in transformer architectures. (a) Recurrence and input-adaptive computation allows models to dynamically allocate computational resources based on problem complexity. (b) Algorithmic supervision guides the learning process through structured intermediate representations. (c) Anchored discrete latent spaces provide stable reference points for compositional reasoning. (d) Error correction mechanisms enable iterative refinement of predictions through feedback loops. Together, these mechanisms enable transformers to develop recursive reasoning patterns that generalize beyond their training distribution."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 Problem Setup 3.1 Task Description: Modular Arithmetic on Computational Graphs . . . . . . . . . . . . . . . . 3.2 Limitations of Standard Transformers with CoT Training . . . . . . . . . . . . . . . . . . . . . 4 Reasoning in Latent Space with Algorithmic Supervision 4.1 Mechanisms for Effective OOD Generalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 6 8 9 9 4.2 Experimental Results & Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5 Mechanistic Interpretability 6 Conclusion Experimental Details on Chain-of-Thought & End-to-End Baselines A.1 End-to-End Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Chain-of-Thought Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details on Latent State Supervision B.1 Latent State Embedding Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Latent State Supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Discretization of Intermediate States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Self-Correction Mechanism . B.5 Experiment Details & Additional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Mechanistic Interpretability Analysis C.1 Technique Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 First Layer Attention: Variable Copying . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Second Layer Attention: Value Copying . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Second Layer MLP: Module Addition in the Frequency Domain . . . . . . . . . . . . . . . . . C.5 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 24 24 27 33 33 34 34 35 38 39 42 47 51 2 1. Introduction Systematic algorithmic generalization stands as critical milestone and grand challenge in machine learning research (B. Lake and Baroni, 2018; Pollack, 1990; Socher et al., 2012; Veliˇckovic and Blundell, 2021). This ability is fundamental to human cognition, stemming from our capacity for systematic compositionality algebraically producing novel combinations from known components and making strong generalizations from limited data (Chomsky, 1957; Fodor and Pylyshyn, 1988; B. M. Lake et al., 2017). Achieving such generalization necessitates learning universal, scalable problem-solving algorithms. Even in humans, acquiring such algorithmic understanding often requires explicit step-by-step supervision. Once an algorithm is learned, however, humans can generalize its application far beyond the domain of previously encountered stimuli or problems (John Anderson, 1982; Singley and John Robert Anderson, 1989). The reasoning capabilities of artificial intelligence systems have advanced rapidly in recent years, built upon the foundation of large language models. In particular, chain-of-thought (CoT) techniques have been central to enhancing the reasoning capabilities of these systems (Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et al., 2022; Kojima et al., 2022; Liu et al., 2023; Wei et al., 2022), especially in domains like mathematics (Cobbe et al., 2021; Lewkowycz et al., 2022; Lightman et al., 2023; Shao et al., 2024). CoT enables model to receive supervision on learning reference problem-solving procedure during training and allows the model to emulate this procedure at test-time. This progress presents unique opportunity to make significant strides on foundational challenges related to reasoning in artificial intelligence. Despite these advancements, out-of-distribution (OOD) generalizationparticularly the type of length generalization involved in algorithmic reasoning (i.e., generalizing from simpler or smaller problem instances to larger or more complex ones)has remained central challenge and limitation for Transformerbased (Vaswani et al., 2017) language models (Anil et al., 2022; Jelassi et al., 2023; Kazemnejad et al., 2023; H. Zhou et al., 2024). While chain-of-thought techniques alleviate this to some degree by enabling the learning of more complex algorithmic procedures, the ability to generalize far outside the training distribution remains significant obstacle (Stechly, Valmeekam, and Kambhampati, 2024; Y. Zhou et al., 2024). In this work, we investigate the architectural and methodological mechanisms that underpin algorithmic OOD generalization in Transformer networks. To facilitate systematic investigation, we focus our study on simple yet scalable mathematical reasoning task: performing modular arithmetic on computational graphs. This task allows us to study OOD and algorithmic generalization in controlled mannerwith complexity directly parameterized by graph size and depthwhile also capturing the core essence of established mathematical reasoning benchmarks like GSM8K (Cobbe et al., 2021), which are central to evaluating the reasoning capabilities of large language models. Furthermore, this task possesses compositional nature; it can be solved by learning core set of skills (e.g., set of modular arithmetic operations and the ability to traverse the graph one layer at time) and scaling up their application to solve larger and more complex problem instances. We use this task to explore the following guiding question: What are the architectural mechanisms and inductive biases needed for robust OOD algorithmic generalization in Transformers? We find that while standard CoT training techniques enable good in-distribution performance and limited degree of OOD generalization, the learned solutions are not robust or universal, and their performance 3 rapidly degrades as test inputs grow in complexity beyond the training regime. We propose and explore set of four simple architectural and methodological mechanisms, built upon the Transformer architecture, to facilitate the learning of robust and generalizable algorithmic solutions: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via discrete bottleneck; and (iv) an explicit error-correction mechanism. When combined, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks, demonstrating robust algorithmic generalization capabilities. In particular, on our mathematical reasoning task, our method achieves perfect generalization on inputs that are several times larger than those seen during training. We complement our architectural proposal and empirical results with mechanistic interpretability analysis to reveal how these architectural proposals enable sharp OOD generalization, what circuits they learn, and why those circuits facilitate robust OOD generalization. 2. Related Work Our work is related to several strands of fundamental machine learning research, including issues of out-ofdistribution generalization, architectural mechanisms such as recurrence and discretization, chain-of-thought and intermediate supervision methods, and work on mechanistic interpretability techniques. Out-of-Distribution Generalization. Out-of-distribution (OOD) generalization, along with related capabilities such as compositionality and systematicity, poses fundamental challenge in machine learning research (Barrett et al., 2018; Baxter, 2000; Hupkes et al., 2020; Pollack, 1990; Socher et al., 2012). These capabilities are crucial for developing AI systems that can reliably apply learned knowledge to novel scenarios, hallmark of robust intelligence (Fodor and Pylyshyn, 1988; Goyal and Bengio, 2022; B. M. Lake et al., 2017). particularly important type of OOD generalization, especially for algorithmic reasoning tasks, is length generalizationthe ability to generalize from simpler or shorter training instances to significantly longer and more structurally complex instances. This has proven to be key limitation of Transformer-based (Vaswani et al., 2017) language models (Anil et al., 2022; Jelassi et al., 2023; Kazemnejad et al., 2023; H. Zhou et al., 2024). While chain-of-thought techniques alleviate this to some degree by enabling the learning of more complex algorithmic procedures, the ability to generalize far outside the training distribution remains significant obstacle (Stechly, Valmeekam, and Kambhampati, 2024; Y. Zhou et al., 2024). Recurrence. Recurrence forms foundational architectural principle in neural networks, particularly for tasks that involve sequential data or inherently iterative processes (Elman, 1990; Hochreiter and Schmidhuber, 1997; Jordan, 1997). These architectures are designed to emulate step-by-step computations by maintaining and updating an internal state, making them well-aligned with problems that have recursive or layered solution structure. Sequence-to-sequence recurrent architectures for sequence transduction and neural machine translation advanced the state of the art (Cho et al., 2014; Sutskever, Vinyals, and Le, 2014), and were instrumental to the development of attention mechanisms and the Transformer architecture (Vaswani et al., 2017). While standard Transformers do not possess recurrent structure, recurrent variants of the Transformer architecture were explored soon after its introduction (Dehghani et al., 2019). Whereas standard recurrent neural networks apply their recurrence across time or sequence length, recurrent Transformer architectures are parallel in time due to the parallel attention mechanism, but recurrent across computational depththat is, the same Transformer layer is applied iteratively to the sequence as whole. The recurrent inductive biases have been demonstrated to confer certain advantages in generalization (Fan et al., 2024; Yang 4 et al., 2024). In our work, recurrence is key architectural mechanism encoding important inductive biases that aid the discovery of scalable recursive algorithms for solving the underlying mathematical problem. Adaptive Computation. critical challenge is handling inputs with varying complexity, where fixed amount of computation may be inefficient or insufficient. This motivates the concept of adaptive computation, wherein model can dynamically adjust its computation time, for example by varying the number of recurrent iterations, based on the demands of the input. An important work in this domain is the Adaptive Computation Time (ACT) mechanism proposed by Graves (2017) for recurrent neural networks, which explicitly models and learns how many computational steps are needed as function of the input. version of the ACT mechanism is incorporated in the recurrent Transformer architecture proposed by Dehghani et al. (2019). However, drawback of such mechanisms is their complexity and difficulty of training. Although efforts have been made to explore simpler adaptive computation methods (Banino, Balaguer, and Blundell, 2021), an even simpler approach is explored by Bansal et al. (2022) and Schwarzschild et al. (2021), where the halting time is not explicitly modeled by the network, and instead the number of recurrent iterations is scaled at inference time based on the size of the input. This simpler approach can be easier to train, and has been shown to improve out-of-distribution generalization. More recently, Geiping et al. (2025) explored the viability of this approach as way to perform test-time scaling in large language models. In our work, we similarly scale computation time by proportionately scaling the number of recurrent iterations in order to solve more complex problem instances, generalizing far beyond the training distribution. Discreteness in Neural Networks. Symbolic AI systems derive their power from manipulating discrete symbols according to well-defined rules, which enables robust, precise, and interpretable reasoning (Fodor and Pylyshyn, 1988; Newel and Simon, 1976). Given this rich tradition of using discrete symbolic states in artificial intelligence, many works have subsequently explored incorporating such discrete latent representations into neural networks (Agustsson et al., 2017; Courville, Bergstra, and Bengio, 2011; Garcez, Lamb, and Gabbay, 2008; Oord, Vinyals, and Kavukcuoglu, 2018; Salakhutdinov and Hinton, 2009). Additionally, discreteness is often central characteristic of constructions of Transformer networks for specific tasks. For example, Weiss, Goldberg, and Yahav (2021) develops programming language that represents Transformerbased computation with discrete internal mechanisms. Additionally, Smolensky et al. (2024) constructs Transformer network for compositional in-context learning task, which features discreteness in both its latent states and attention mechanism. In our work, we explore the use of discrete latent states as means of anchoring the latent representation to common, depth-invariant space to enable scaling computation far beyond the training distribution while avoiding representational shift across computational depth. Chain-of-Thought & Algorithmic Supervision. Chain-of-thought techniques have been central to enhancing the reasoning capabilities of large language models. Early usage of the term chain-of-thought referred to prompting techniques that condition model to generate sequence of intermediate steps before arriving at the final answer (Kojima et al., 2022; Nye et al., 2021; Wei et al., 2022). For example, Wei et al. (2022) demonstrated that prompting the LLM with few CoT exemplars caused the model to generate an analogous step-by-step solution, which significantly improved performance on range of arithmetic, commonsense, and symbolic reasoning tasks. Kojima et al. (2022) showed that LLMs can be zero-shot reasoners in the sense that simply asking the model to reason step-by-step, without providing in-context learning CoT exemplars, can be sufficient to elicit chain-of-thought-style reasoning and improve performance. Modern usage of the term chain-of-thought has extended beyond prompting methods, as it now forms key component of the training pipeline of LLMs, wherein model is explicitly trained on demonstrations of step-by-step solutions to problems of interest, such as mathematical reasoning (Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et al., 2024; Lewkowycz et al., 2022; Liu et al., 2023). In some situations, chain-of-thought training can be interpreted as providing explicit supervision to align the model to particular algorithm or procedure for solving problem, as opposed to simply providing supervision via input-output examples. In our work, we explore traditional chain-of-thought training techniques as baselines, as well as incorporate algorithmic supervision to the internal states of our proposed method. Mechanistic Interpretability. In our work, we carry out mechanistic interpretability analysis to probe how the model has learned to solve the task and why it can do so robustly, generalizing far outside the training distribution. In recent years, there has been resurgence in work on interpretability, with new techniques being introduced that aim to understand modern large language models (Ameisen et al., 2025; Bricken et al., 2023; Elhage, Hume, et al., 2022; Elhage, Nanda, et al., 2021; Meng et al., 2022; Olsson et al., 2022). Elhage, Nanda, et al. (2021) is an influential work in this area of research, introducing conceptual framework and new terminology that continues to be used in subsequent work. key early achievement in this line of work is the discovery of induction head circuits in large language models (Olsson et al., 2022), which perform two-step copying operation that is crucial for in-context learning. In our work, we identify similar mechanism in our recurrent models that is used to copy previously computed variable values. This involves first retrieving the parent variables names in the first layer, then using these variable names to retrieve their values in the second layer, which are computed elsewhere in the sequence of latent states. Such work is often described as circuit analysis, where the goal is to identify sub-networks that are responsible for particular functions. key method for validating hypotheses about the functions of different model components is causal interventions like activation patching or ablations (Geiger, Lu, et al., 2021; Geiger, Wu, et al., 2024; Meng et al., 2022), which involves systematically modifying parts of the model or input to observe effects on behavior or internal states. We use related causal intervention techniques in our own mechanistic interpretability analysis in this work. Finally, the work by Nanda et al. (2023) and Tian (2024) is relevant as it specifically investigated how Transformers perform arithmetic, reverse-engineering modular addition algorithm learned by the feedforward network in Transformer layera phenomenon we also observe in our models. 3. Problem Setup 3.1. Task Description: Modular Arithmetic on Computational Graphs We formally introduce the task of modular arithmetic on computational graphs as follows. Task Description. computation graph is directed acyclic graph (DAG) representing network of mathematical computations, where nodes correspond to variables and edges describe the dependencies between them. As illustrated in Figure 2 with an example, the leaf nodes in this DAG are directly assigned numerical values (e.g., x7 20). All other non-leaf nodes are defined as functions of their parent nodes in the computation graph. In particular, the value of each non-leaf node is computed by applying one or more specified operations to the values of its parent nodes. In our experiments, we consider modular arithmetic operations (addition, multiplication, or subtraction), with the prime number = 23 as the modular base. For example, in Figure 2 we have x23 x7+x42(mod p) and x101 x23x91(mod p). In the following, we let and denote the total number of nodes and the number of leaf nodes, respectively. We consider graphs with up to 6 128 nodes, and let = {x1, . . . , x128} denote the set of variable names. Data Generation Process. problem instance in this task is specified by the values of the leaf nodes and computation graph depicting the computations that determine the values of all non-leaf nodes. In particular, given parameters and L, an input instance is generated as follows: (i) Randomly generate DAG with nodes, of which are leaf nodes. (ii) Randomly assign variable name from to each node. (iii) Randomly assign numerical values to the leaf nodes from = {0, 1, . . . , 22}. (iv) For each non-leaf node, randomly assign operations from = {+, , } to define its computation based on its parent nodes The instance generated by (i)(iv) is stored as token sequence, where each variable name, numerical value, and operation is assigned unique token. special separation token [sep] is used to separate different formulas. For example, the instance depicted in Figure 2 is represented as the following token sequence: 20x7 [sep] 2x42 [sep] 6x88 [sep] 14x115 x7+x42x23 [sep] x42+x88x91 [sep] x88x115x55 (1) x23x91x101 [sep] x91x88+x55x30 Target Output & Evaluation Metric. Given generated problem instance, the task is to compute the value of every node in the computation graph; these values are uniquely determined by steps (i)(iv) above. We consider the model output to be correct only if all node values are computed correctly (i.e., the input graph is fully solved). Out-of-Distribution Generalization. Our primary focus in this work is to investigate the ability of Transformer networks to learn general problem-solving procedures or algorithms that enable out-of-distribution (OOD) generalization. The complexity of each problem instance can be explicitly parameterized by graph size, enabling precise measurement of models ability to generalize to inputs more complex than those encountered during training. In particular, in this mathematx7 2 6 x42 x88 ical reasoning task, OOD generalization is evaluated by 14 x115 + x23 + x55 x101 + x30 training Transformer models on problem instances with 32 nodes and testing them on instances of varying sizes, up to = 128 (a fourfold increase). Such generalization requires the ability to process larger inputs and adaptively scale computation time during testing, beyond what was encountered in the training regime. This synthetic task captures the core essence of mathFigure 2. An illustration of an instance in modular arithmetic on computational graphs task. The goal is to compute the values of all nodes in the graph. For example, here x23 = 20 + 2 = 22 and x55 = 6 14 = 15. Recall that we consider modular arithmetic with base = 23. ematical reasoning benchmarks like GSM8K (Cobbe et al., 2021), which are pivotal for evaluating the reasoning capabilities of large language models. Similar to GSM8K, our task involves combinatorial structure combined with arithmetic computations. However, key simplification is that variable names 7 are directly tokenized, bypassing natural language representation. This focused design, while retaining the critical combinatorial structure and rule-based nature inherent in mathematical reasoning, facilitates more straightforward and modular interpretation of the learned Transformer models internal mechanisms, as will be shown in Section 5. 3.2. Limitations of Standard Transformers with CoT Training To establish baseline and motivate the need for alternative approaches, we evaluate standard Transformer architectures on our synthetic task using two primary training paradigms. End-to-End Training. The first baseline is End-to-End training, where the Transformer models are trained to directly output the final values of all nodes given the problem input, without explicit intermediate steps. The input token sequences are in the form of (1), and we employ various Transformer models with diverse architectures. See Appendix for details. Chain-of-Thought (CoT) Training. The second baseline is based on autoregressive Chain-of-Thought (CoT) training (Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et al., 2024; Cobbe et al., 2021; Lewkowycz et al., 2022; Wei et al., 2022; Ye et al., 2024), prevalent technique for enabling multi-step reasoning in LLMs. Instead of directly outputting the final answer, CoT trains model to generate sequence of intermediate reasoning steps (the thought process) that culminates in the solution. For our task, CoT intermediate steps consist of explicit demonstrations of the step-by-step computation of nodes within given computation graph. In particular, in CoT training, the Transformer model receives an input prompt consisting of the token representation of the computation graph (as in (1)), followed by special CoT token. This special token signals the beginning of the CoT reasoning, which outlines the computation of each node in topological order. Each step in the trajectory involves: (1) recalling the equation defining the nodes value, (2) recalling the values of its dependent nodes, and (3) performing the arithmetic computation. For example, computing node x101 from Figure 2 would appear in the CoT as: [...Input Prompt...]CoT[...]x101 = x23 x91 = 22 8 = 15 Here, the [...Input Prompt...] gives the description of the problem instance, and [...] denotes the preceding portion of the chain-of-thought trajectory up to node x91, which in particular includes the computation of the values of x23 and x91. An example of full CoT example from the training data is provided in Appendix A.2. Implementation. We train causal Transformer models from scratch using both End-to-End and CoT supervision on randomly generated problem instances with graph sizes 32. At inference time, models are prompted with the input and generation is performed using greedy decoding. End-to-End models directly output all node values given the input, while CoT models autoregressively generate the solution, including the full CoT trajectory. We evaluate performance based on the proportion of instances where the model computes all node values correctly, with particular focus on OOD generalization to new, randomly generated graphs of varying sizes up to = 128. For all methods, an extensive hyperparameter search was conducted (covering layers, model dimension, and positional encoding), and the best-performing configuration of each method was selected for comparison. detailed experimental setup for these baseline experiments is provided in Appendix A. 8 Observed OOD Generalization Deficiencies. We find that Chain-of-Thought training enables models to solve larger graphs compared to those trained End-to-End without chain-of-thought supervision (Figure 4). While the best-performing CoT models exhibit limited degree of OOD generalization to moderately larger 40), this capability rapidly deteriorates as graph sizes exceed the training regime. graphs (N 32 In the next section, we propose series of architectural mechanisms that address these generalization (cid:59) challenges. 4. Reasoning in Latent Space with Algorithmic Supervision 4.1. Mechanisms for Effective OOD Generalization. Effective OOD generalization on complex reasoning tasks hinges on models ability to learn and emulate an underlying scalable algorithm. This requires the model to, implicitly or explicitly, execute an iterative procedure that adapts to input complexity. Designing inductive biases to support the discovery of such scalable, compositional solutions is central challenge in machine learning (Barrett et al., 2018; Baxter, 2000; Goyal and Bengio, 2022; B. Lake and Baroni, 2018). Chain-of-thought (CoT) techniques attempt this by having the model sequentially generate token representation of computational process. However, this restriction to token-based, autoregressive format often yields brittle algorithms\" that fail to generalize robustly, especially as longer CoT sequences are needed for more complex inputs. These well-documented length generalization issues (Anil et al., 2022; Jelassi et al., 2023; Kazemnejad et al., 2023; Stechly, Valmeekam, and Kambhampati, 2024; H. Zhou et al., 2024; Y. Zhou et al., 2024) underscore CoTs limitations in effectively emulating truly scalable algorithmic procedures. This work, therefore, proposes alternative mechanisms to facilitate the learning of such iterative algorithms directly within models latent processing. Our proposal features four key architectural mechanisms: (i) recurrent Transformer blocks, (ii) algorithmic supervision, (iii) discretization in latent space, and (iv) self-correction scheme. Collectively, these mechanisms constitute an architecture enabling native latent-space reasoning, leading to effective OOD generalization. Figure 1 illustrates the four mechanisms as individual components, while Figure 3 depicts the unified architectural proposal. In the following, we present the four proposed mechanisms and the essence of their implementation, deferring certain implementation details to Appendix B. Algorithm to Emulate. To solve this task, natural algorithmic solution that is well-aligned with the Transformer architecture is to compute the values in the computation graph one layer at time. This can be realized through recursive process that iteratively applies the same computational modules. Specifically, each iteration of the algorithm computes values one layer deeper in the computation graph by fetching the necessary dependent values for nodes at the current layer and then performing the required modular arithmetic. In particular, for the example in Figure 2, in the first iteration, we evaluate variables {x7, x42, x88, x115}. In the second iteration, we evaluate {x23, x91, x55}. In the last iteration, we evaluate {x101, x30}. Note that each iteration involves the same type of computation, providing succinct and scalable recursive problem-solving algorithm. Mechanism 1: Recurrence & Input-Adaptive Computation. The iterative and recursive structure of the target layer-by-layer algorithm naturally motivates recurrent architecture. We employ recurrent Transformer block (Dehghani et al., 2019) with the goal that each application emulates one algorithmic iterationthat is, computing values for one additional layer of the computation graph. An input instance 9 Input Adaptive Recurrence Input Discrete Bottleneck Latent Space Supervision Recurrent Transformer Block Output Discretize Re-Embed Figure 3. Overview of the proposed architecture for OOD generalization. It features recurrent Transformer block, latent algorithmic supervision, and discretization mechanism to anchor representations across iterations. Self-correction mechanism is not represented here. is represented as sequence of tokens = (x1, . . . , xn), as described in (1). This is embedded to form sequence of embedding vectors E(0) , and recurrently processed with the recurrent transformer block 1 , . . . , E(0) (E(t+1) 1 , . . . , E(t+1) ) RecurrentTransformerBlock(E(t) 1 , . . . , E(t) ), = 1, 2, . . . , T. (2) The output is linearly read out from the final embedding states E(T ) . Crucially, the number of recurrent iterations, , is not fixed but adapts to input complexity, scaling linearly with the depth of the computation graph. This input-adaptive recurrence allows the model to dynamically scale its computation , . . . , E(T ) 1 time proportionate to the problems requirements, key capability for OOD generalization to larger graphs. Unlike CoT methods that scale computation by generating progressively longer linear sequences of tokens, recurrence introduces inductive biases favoring recursive solution structures, which are inherently more scalable. This recurrent structure also provides key computational advantages compared to autoregressive chain-of-thought methods: in our recurrent architecture, each step can perform parallel processing across the entire context instead of being constrained to perform computation sequentially token-by-token, yielding more efficient use of working memory since the full computational trace is not serially materialized. The use of recurrence to adaptively scale computation time is well-established concept for tackling tasks with variable complexity (Banino, Balaguer, and Blundell, 2021; Bansal et al., 2022; Dehghani et al., 2019; Fan et al., 2024; Geiping et al., 2025; Graves, 2017; Schwarzschild et al., 2021). Mechanism 2: Latent State Algorithmic Supervision. While recurrence (Mechanism 1) provides the capacity for iterative computation, it does not inherently guarantee that the model will learn the desired layer-by-layer algorithmic procedure. To instill this structure, we introduce latent state algorithmic supervision. Unlike CoT, which supervises intermediate computation in token space, our mechanism provides supervision directly within the models latent representation space at each recurrent step, steering the internal states to align with the step-by-step execution of our target algorithm. Specifically, at each recurrent iteration t, shared linear readout layer is used to predict node values from their current latent embeddings E(t) . The training loss applied to these predictions at each recurrent iteration is designed to align the model with the target layer-by-layer algorithm. In particular, for each iteration t, it penalizes errors in the predicted values 10 for nodes that are algorithmically computable within processing steps (i.e., of depth or less) as follows AlgorithmAlignmentLoss = (cid:88) (cid:88) t=1 i[n] 1{Depth(xi) t} ℓ (cid:16) Wvalue E(t) , Value(xi) (cid:17) , (3) where Depth(xi) is the nodes depth in the computation graph, Value(xi) is its ground-truth value, and ℓ is the cross-entropy loss. Thus, the algorithm alignment loss supervises the model such that at iteration t, it computes the values of all nodes in the input at computational depth less than or equal to t. For example, in Figure 2, supervision at = 1 applies to leaf nodes (e.g., x7), while at = 2 it extends to include second-layer nodes (e.g., x23), and so on. This iterative supervision encourages the model to progressively build up the solution, computing the graph one effective layer deeper with each recurrent step. Mechanism 3: Anchoring Latent Representation via Discretization. Recurrent models can suffer from representational drift across recurrent iterations during extended out-of-distribution computation, arising from error accumulation when computation scales beyond the training regime. To mitigate this and ensure stable processing across many iterations, we introduce discretization mechanism that anchors the models latent representation while scaling computation through recurrence. Specifically, after each iteration, the models continuous hidden states are projected into structured, discrete symbolic space and then immediately re-embedded to form the input for the next recurrent step. This forces the intermediate representations at each iteration to begin and end in shared structured space, thereby maintaining semantic stability even when computation extends beyond the training regime. Ultimately, this anchoring constrains the model to learn depth-invariant computational process, which is key to generalizing to longer computational depths than seen during training. We implement this anchoring using structured tokenization and embedding scheme, enabling each tokens internal state to evolve recurrently while remaining grounded in shared discrete space. In our task of modular arithmetic on computational graphs, the discrete latent space is structured as product of four factors: token syntax, variable identity, numerical value, and operation type. To illustrate the structure of the discrete space, consider the input sequence 17=x42 [sep] . This sequence is tokenized into symbolic factors as follows: syntax variable operation value 17 [ = [ x42 [ [sep] [ value = variable [sep] N/A N/A x42 N/A N/A N/A N/A N/A 17 N/A empty N/A ] ] ] ] Note that the value factor of variable tokens (e.g., x42 above) is empty at the input layer. As the model processes the input recurrently, it iteratively computes the values of different variables, updating the value factor of the discrete latent state. This yields latent representation that is discrete, shared across steps, and scalable to extended computation. To map the discrete states to distributed embeddings, we train separate embedding layer for each factor and combine the factor embeddings by summation. At each iteration, we first apply the RecurrentTransformerBlock, as in Equation (2), forming the core computation of the recurrent step. The processed distributed representations are then discretized via argmax decoding across each symbolic factor, projecting the latent representation to common structured space. We then re-embed the discrete state to form the vectorized input for the next iteration. ( E(t+1) 1 ) RecurrentTransformerBlock(E(t) 1 , . . . , E(t) ) , . . . , E(t+1) z(t+1) i,factor arg max{Wfactor i,factor FactorEmbed(z(t+1) E(t+1) E(t+1) i,syntax + E(t+1) E(t+1) E(t+1) } factor {syntax, variable, operation, value} (4) i,factor) factor {syntax, variable, operation, value} i,variable + E(t+1) i,operation + E(t+1) i,value. Mechanism 4: Learning to Self-Correct. Finally, to enhance the robustness of the learned algorithm, especially as the number of computational steps increases and makes the process more susceptible to error propagation, we introduce self-correction scheme. This mechanism aims to equip the model with the ability to recover from such intermediate mistakes. To facilitate this robustness, we train the model by intentionally introducing errors into its reasoning process. Specifically, at each recurrent iteration, with small probability, we randomly corrupt selection of the value components within the models discrete latent states. This training regimen forces the model to learn to detect when previously computed value is incorrect (due to our induced corruption or its own misstep) and then to correct this error in subsequent computational step before proceeding with the task. 4.2. Experimental Results & Discussion Combining these mechanisms yields an architecture capable of effectively generalizing far beyond the training distribution to much larger and more complex inputs. To evaluate the effects of the different mechanisms we propose, we study collection of methods, each implementing different subset of these mechanisms. These methods are listed in Table 1. The Feedforward End-to-End method does not implement any of the proposed mechanisms. The Recurrent End-to-End method partially implements Mechanism 1 as it uses recurrence but lacks input-adaptive computation. The Chain-of-Thought method partially implements Mechanism 1 since the length of the chain-of-thought trajectory scales with the complexity of the problem. It also partially implements Mechanism 2 because the next-token prediction objective on the chain-of-thought sequences provides supervision on the intermediate steps, although this supervision is not directly applied to the latent states. The Continuous Latent Space Supervision method fully implements Mechanisms 1 & 2. It is recurrent model featuring input-adaptive computation and latent state algorithmic supervision. However, we omit the discretization mechanism (Mechanism 3), thereby maintaining continuous distributed latent states. The Discrete Latent Space Supervision method incorporates the discretization mechanism, implementing Mechanisms 1, 2, & 3. Finally, the Discrete Latent Space Supervision method further incorporates the error correction mechanisms, thus implementing all four mechanisms. Enabling Robust Algorithmic OOD Generalization. Figure 4 depicts the OOD generalization performance of our methods, ablating across the ingredients described above, as well as the aforementioned Chain-ofThought and End-to-End baselines. As previously mentioned, we find that the End-to-End models (both recurrent and feedforward) fail to effectively learn the task (with respect to our stringent fully solved metric) beyond small graph sizes, even in-distribution. The recurrent models slightly outperform the feedforward models. Chain-of-Thought supervision enables significant improvement, yielding nearperfect performance in-distribution (N 32), and limited degree of out-of-distribution generalization. To 12 Table 1. Guide to Implementation of Proposed Mechanisms in Baselines. The leftmost column shows the method names of the different baselines and ablations we consider, matching the figure legends. indicates that method implements the given mechanism, indicate that it is partially implemented. indicate that the mechanism is not implemented, and (cid:32) (cid:71)(cid:35) (cid:35) Method / Mechanism Mechanism 1 Mechanism 2 Mechanism 3 Mechanism 4 Feedforward End-to-End Recurrent End-to-End Chain-of-Thought Continuous Latent Space Supervision Discrete Latent Space Supervision Discrete Latent Space Supervision (cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:32) (cid:32) (cid:32) (cid:35) (cid:35) (cid:71)(cid:35) (cid:32) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:35) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:32) Figure 4. Out-of-Distribution generalization performance of different methods on the mathematical reasoning task. Figure 5. Effective out-of-distribution generalization via input-adaptive scaling of computation time. This depicts Discrete Latent Space Supervision assess our proposed mechanisms for robust OOD generalization in Transformers, we evaluate three classes of models incorporating different subsets of those ingredients. We find that this enables dramatic improvement in OOD generalization, with performance improving further as more ingredients are incorporated. When all proposed ingredients are incorporated, i.e., Discrete Latent Space Supervision 1, the model robustly achieves near-perfect performance across all OOD splits we examined. Depth-Invariance for Scalable Reasoning. Generalizing to problem instances more complex than those seen during training requires some mechanism of scaling computation proportionately. The chain-ofthought solution to this challenge is to scale the length of the autoregressively generated CoT trace, carrying out computation through the sequential generation of tokens. While this can yield some success, it is inherently limited: computation is forced into token-by-token format rather than the models native latent representation space, constraining efficiency and robustness. In this work, we explore different approach based on recurrence with input-adaptive recurrent depth, introducing inductive biases that enforce depthinvariant structure in the learned solution. That is, the model learns solution such that the computational 1Here, denotes self-correction. description at every step of the solution process is the same, making it possible to scale it to depths far larger than those seen during training. This notion parallels other architectural invariances studied in geometric deep learning such as translation, rotation, or permutation equivariance where networks preserve behavior under transformations aligned with the task structure (Michael Bronstein et al., 2017, 2021; Gerken et al., 2023). Here, the recurrence imposes invariance under the networks own iterative action, yielding scalable, recursive algorithm capable of solving much larger and more complex instances. The Importance of Anchored Discrete Representations. In Figure 4, Continuous Latent Space Supervision denotes recurrent model where the continuous latent states receive step-by-step algorithmic supervision, but the latent states are not discretized in between recurrent block iterations as they are in Discrete Latent Space Supervision. We see that, while this outperforms the Chain-of-Thought baseline, which is limited to linear reasoning paths, its out-of-distribution performance slowly degrades as we test on progressively larger inputs, which require increasing recurrent depth and computation time. We attribute this to accumulating noise in the continuous vector representations phenomenon exacerbated when scaling test-time compute for larger problem instances which eventually causes representations to drift from the semantically meaningful manifold learned during training. In Discrete Latent Space Supervision, the model receives step-by-step algorithmic supervision as with its continuous counterpart, but now we additionally discretize the latent representation, then re-embed using common embedder that is shared across recurrent iterations. This has the effect of anchoring the latent states to common, semantically-consistent representation space, allowing the model to scale up computational depth without accumulating noise. We observe that this yields significantly improved OOD generalization. Error-Correction Leads to Greater Robustness in Scaling. In Discrete Latent Space Supervision , we introduce explicit supervision for error correction by randomly corrupting the models latent space with some small probability during training. While the model may make occasional errors, it is able to correct them in the next recurrent iteration, thereby yielding near-perfect OOD generalization. Interestingly, we find that error correction requires more layers in the recurrent block in order to succeed. An intuitive explanation is that effective error correction requires greater computational depth per step: the model must first identify and correct errors from prior steps before executing the current steps computation. Robust Test-time Scaling. On many tasks, the computation time required to solve problem instance is proportional to its size or complexity. Consequently, solving problems larger than those encountered during training necessitates scaling computation time beyond the training regime. In our setting, where the models reasoning process is latent, we achieve this by increasing the number of recurrent iterations. Figure 5 depicts the proportion of input instances solved as function of the number of recurrent iterations. Increasing the number of iterations enables solving incrementally larger and harder problem instances. Our architectural mechanisms enable this robust scaling beyond the training regime. Details, Extensions & Further Ablations. In the appendices, we provide further discussion and present additional experimental results. Here, we briefly highlight few aspects of these extensions. Across all methods, we find that hyperparameter choice can be critical. In particular, we find that the choice of positional encoding and model depth is especially important. In the above results, we always report the best model within each method after hyperparameter search, the details of which are provided in the appendix. Additionally, for the chain-of-thought baselines, we explore multiple schemes for the design of the reasoning chains and present the best results here. 14 Now that we have demonstrated the effectiveness of the proposed architectural mechanisms for robust OOD generalization, we next conduct mechanistic interpretability analysis to probe the precise computational circuits learned by each component of our model. 5. Mechanistic Interpretability In this section, we aim to answer the following questions via detailed study of the models inner workings: (i) What algorithm does the trained model implement? (ii) Why is the trained model able to generalize to OOD data? To answer these questions, we first propose hypotheses on the functionality of each model block: first-layer attention, second-layer attention, and the final MLP. For each of these hypotheses, we conduct controlled experiments where we apply causal interventions to specific parts of the input and isolate the effect on model activations to identify the function of each component. Our methodology builds on prior work on causal interpretability in neural networks (Geiger, Lu, et al., 2021; Geiger, Wu, et al., 2024; Meng et al., 2022), but is tailored specifically to interpreting recurrent transformer models. We provide complete details of our experimental methodology in the appendix. Induction Head & Modular Addition Mechanism To understand the algorithm implemented by the trained model, we analyze in detail the recurrent Transformer model trained with our proposed Discrete Latent Space Supervision method on the mathematical reasoning task. The recurrent Transformer model is configured with two layers, 16 attention heads, and hidden state dimension of 256. For more details on the model configuration, please refer to Appendix C. We summarize our mechanism analysis results in Figure 6, where we reveal an induction head mechanism operating within the two-layer attention block and modular addition mechanism in the final feedforward layer. To better understand the models behavior, let us take an example equation in the following format: [sep] var0 + var1 + var2 = rhs . We can break down the models computation into three main components at the Right-Hand Side (RHS) position: The first layer attention heads copy the variable factored embeddings of variables var0 , var1 , and var2 to the RHS position, which let the model know the variable names at the RHS position. The second layer attention heads use the copied variable names to retrieve the computed values of variables var0 , var1 , and var2 from the previous equations through an induction-head mechanism. The last feedforward layer computes the sum of the values of the variables on the LHS and outputs the result to the RHS position. First Layer Attention Performs Variable Copying. The attention heads in the first layer are grouped by the variable position they attend to, reflecting an attention pattern that is dependent on relative position, Figure 6. Illustration of the two-layer model performing the modular addition task. The colored squares represent attention heads, grouped by the variable positions they attend to. Black rectangles indicate the embedding components chosen by the value projection matrix. denotes tokens, and denotes embedding components. as illustrated in Figure 7 (left). For the token embeddings of var0 , var1 , and var2 , which comprise four separate factored embedding types (syntax, variable, operation, and value), the value and output projection matrices of each head group select subspace of these token embeddings containing only the variable embeddings. This is evident in Figure 7 (right), which plots the norm amplification for different factored embedding types. More details on the norm amplification calculation can be found in Appendix C. This shows that the first layer attention copies the variable names of its parents, which will later be used to obtain their values in the second layer. Second Layer Attention Implements Variable-Dependent Induction Head Mechanism. The second layers attention heads then retrieve the corresponding values of variables var0 , var1 , and var2 from the previous equations through an induction-head mechanism (Olsson et al., 2022). Specifically, all the attention heads are also grouped by which variable value they are retrieving. For example, let us suppose that the first head group is responsible for retrieving the value of var0 . Then, the attention heads within this group will find the first occurrence of var0 , which will be the RHS of some previous equation. This particular position is the first time the value of var0 is computed. And these attention heads will then copy the value factored embedding of var0 also to the current RHS position. In summary, the variable names copied in the first layer are used as queries to retrieve these variables values, searching over the RHS of previous equations. Feedforward Layer Performs Modular Addition. The second layer MLP implements sophisticated modular addition mechanism that computes the sum of the three variable values modulo 23. The MLP receives as input the sum of three transformed value embeddings from the attention layer one for each variable position. These embeddings exhibit periodic structure that naturally lends itself to frequency domain analysis. Through systematic experimentation where we vary all three input values from 0 to 22 and apply threedimensional Discrete Fourier Transform (DFT) analysis, we observe fascinating computational pattern. At the MLPs pre-activation stage, the representation is dominated by bias term (the (0, 0, 0) frequency component). As signals propagate through the MLP layers, this bias progressively diminishes while diagonal frequency components of the form (a, a, a) are amplified, where {1, . . . , 22}. That is, the Fourier components where var0 , var1 , and var2 have the same frequency are amplified. These diagonal frequencies encode precisely the information needed for modular arithmetic: they represent sinusoidal functions of the sum + + z. The MLP essentially performs the computation through combinations of terms like cos(2πa(x + + z)/23), where the periodic nature of trigonometric functions naturally handles the modulo operation. This frequencybased approach aligns with recent findings on how neural networks implement modular arithmetic (Doshi et al., 2024; Nanda et al., 2023; Tian, 2024). We provide detailed experimental evidence and visualizations of this mechanism in Appendix C, including DFT analysis at multiple network positions showing the progressive amplification of sum-encoding frequencies. Figure 7. Left. An illustration of the functionality of attention heads by groups in the first attention layer. Head 4 and 8 attend to the first variable position, Head 5 and 12 attend to the second variable position, Head 3, 7, 11, 14 attend to the third variable position, and the remaining heads attend to the RHS position or do not show clear attention pattern. Right. Norm amplification of each factors embeddings passed through the combined attention OV matrix by head groups. others exhibits significantly higher norm amplification, primarily because head 15 performs self-copy operation at the RHS position. OOD Generalization of the Trained Model. The models robust OOD generalization can be traced back to the architectural mechanisms of Discrete Latent Space Supervision guiding the model towards learning universal and robust algorithm. In particular, the algorithm implements variable-dependent induction head mechanism that is invariant to length, leveraging both relative-positional and variable-dependent attention patterns, which enables the model to operate over contexts of arbitrary lengths. Thus, despite being trained on graphs with limited size, the input-adaptive recurrence, intermediate supervision, and discretization mechanisms enable the model to learn scalable algorithm capable of solving problems of increased complexity. 6. Conclusion This work investigated algorithmic generalization in Transformers for scalable mathematical reasoning, domain where standard chain-of-thought approaches fail on out-of-distribution inputs. We introduced novel architecture integrating input-adaptive recurrence, latent algorithmic supervision, state discretization, and self-correction mechanisms. Collectively, these mechanisms enabled our models to achieve near-perfect OOD performance by facilitating robust, scalable reasoning directly within their internal latent representations, overcoming the brittleness of sequential token-based methods. Mechanistic interpretability further 17 illuminated how these components achieve systematic generalization. While our synthetic mathematical reasoning task offers analytical clarity for investigating fundamental principlessuch as adaptive recurrence and discrete latent bottlenecksfuture work should explore extending these principles to more diverse, less-structured, and multi-task settings."
        },
        {
            "title": "References",
            "content": "Agustsson, Eirikur, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc Gool (2017). Soft-to-hard vector quantization for end-to-end learning compressible representations. In: Advances in neural information processing systems (cited on page 5). Ameisen, Emmanuel et al. (2025). Circuit Tracing: Revealing Computational Graphs in Language Models. In: Transformer Circuits Thread (cited on page 6). Anderson, John (1982). Acquisition of cognitive skill. In: Psychological review (cited on page 3). Anil, Cem, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur (Dec. 6, 2022). Exploring Length Generalization in Large Language Models. In: Advances in Neural Information Processing Systems (cited on pages 3, 4, 9). Banino, Andrea, Jan Balaguer, and Charles Blundell (Sept. 2, 2021). PonderNet: Learning to Ponder. arXiv: 2107.05407. Pre-published (cited on pages 5, 10). Bansal, Arpit, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein (Oct. 14, 2022). End-to-End Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking. arXiv: 2202.05826 [cs]. Pre-published (cited on pages 5, 10). Barrett, David, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap (2018). Measuring abstract reasoning in neural networks. In: International conference on machine learning. PMLR (cited on pages 4, 9). Baxter, Jonathan (2000). model of inductive bias learning. In: Journal of artificial intelligence research (cited on pages 4, 9). Bricken, Trenton et al. (2023). Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. In: Transformer Circuits Thread (cited on page 6). Bronstein, Michael M, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst (2017). Geometric deep learning: going beyond euclidean data. In: IEEE Signal Processing Magazine (cited on page 14). Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veliˇckovic (2021). Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv: 2104.13478 [cs.LG] (cited on page 14). Cho, Kyunghyun, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio (Oct. 2014). Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Ed. by Alessandro Moschitti, Bo Pang, and Walter Daelemans. Doha, Qatar: Association for Computational Linguistics (cited on page 4). Chomsky, Noam (1957). Syntactic structures. Mouton de Gruyter (cited on page 3). Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. (2024). Scaling instruction-finetuned language models. In: Journal of Machine Learning Research (cited on pages 6, 8). Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. (2022). Scaling Instruction-Finetuned Language Models. arXiv: 2210.11416 [cs.LG] (cited on page 3). Cobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Łukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman (Nov. 2021). Training Verifiers to Solve Math Word Problems. arXiv:2110.14168 [cs] (cited on pages 3, 7, 8). 19 Courville, Aaron, James Bergstra, and Yoshua Bengio (2011). spike and slab restricted Boltzmann machine. In: Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings (cited on page 5). Dehghani, Mostafa, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser (Mar. 5, 2019). Universal Transformers. arXiv: 1807.03819 [cs, stat]. Pre-published (cited on pages 4, 5, 9, 10). Doshi, Darshil, Aritra Das, Tianyu He, and Andrey Gromov (2024). To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. In: Bulletin of the American Physical Society (cited on pages 17, 41). Elhage, Nelson, Tristan Hume, et al. (2022). Toy Models of Superposition. In: Transformer Circuits Thread (cited on page 6). Elhage, Nelson, Neel Nanda, et al. (2021). Mathematical Framework for Transformer Circuits. In: Transformer Circuits Thread (cited on page 6). Elman, Jeffrey (1990). Finding structure in time. In: Cognitive science (cited on page 4). Fan, Ying, Yilun Du, Kannan Ramchandran, and Kangwook Lee (Sept. 25, 2024). Looped Transformers for Length Generalization. arXiv: 2409.15647. Pre-published (cited on pages 4, 10). Fodor, Jerry and Zenon Pylyshyn (1988). Connectionism and cognitive architecture: critical analysis. In: Cognition (cited on pages 35). Garcez, Artur SDAvila, Luis Lamb, and Dov Gabbay (2008). Neural-symbolic cognitive reasoning. Springer Science & Business Media (cited on page 5). Geiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts (2021). Causal Abstractions of Neural Networks. In: Advances in Neural Information Processing Systems. Ed. by A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (cited on pages 6, 15). Geiger, Atticus, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman (2024). Finding alignments between interpretable causal variables and distributed neural representations. In: Causal Learning and Reasoning. PMLR (cited on pages 6, 15). Geiping, Jonas, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein (Feb. 17, 2025). Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach. arXiv: 2502.05171 [cs]. Pre-published (cited on pages 5, 10). Gerken, Jan E, Jimmy Aronsson, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson, Christoffer Petersson, and Daniel Persson (2023). Geometric deep learning and equivariant neural networks. In: Artificial Intelligence Review (cited on page 14). Goyal, Anirudh and Yoshua Bengio (2022). Inductive biases for deep learning of higher-level cognition. In: Proceedings of the Royal Society (cited on pages 4, 9). Graves, Alex (Feb. 21, 2017). Adaptive Computation Time for Recurrent Neural Networks. arXiv: 1603. 08983 [cs]. Pre-published (cited on pages 5, 10). He, Pengcheng, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen (2021). DeBERTa: Decoding-enhanced BERT with Disentangled Attention. arXiv: 2006.03654 [cs.CL] (cited on page 24). Hochreiter, Sepp and Jürgen Schmidhuber (1997). Long short-term memory. In: Neural computation (cited on page 4). Hupkes, Dieuwke, Verna Dankers, Mathijs Mul, and Elia Bruni (2020). Compositionality decomposed: How do neural networks generalise? In: Journal of Artificial Intelligence Research (cited on page 4). 20 Jelassi, Samy, Stéphane dAscoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François Charton (2023). Length Generalization in Arithmetic Transformers. arXiv: 2306.15400 [cs.LG] (cited on pages 3, 4, 9). Jordan, Michael (1997). Serial order: parallel distributed processing approach. In: Advances in psychology. Elsevier (cited on page 4). Kazemnejad, Amirhossein, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy (Dec. 15, 2023). The Impact of Positional Encoding on Length Generalization in Transformers. In: Advances in Neural Information Processing Systems (cited on pages 3, 4, 9, 24). Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa (2022). Large language models are zero-shot reasoners. In: Advances in neural information processing systems (cited on pages 3, 5). Lake, Brenden and Marco Baroni (2018). Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In: International conference on machine learning. PMLR (cited on pages 3, 9). Lake, Brenden M, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman (2017). Building machines that learn and think like people. In: Behavioral and brain sciences (cited on pages 3, 4). Lewkowycz, Aitor, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra (2022). Solving Quantitative Reasoning Problems with Language Models. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (cited on pages 3, 6, 8). Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe (2023). Lets Verify Step by Step. arXiv: 2305.20050 [cs.LG] (cited on page 3). Liu, Hanmeng, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang (Dec. 2023). LogiCoT: Logical Chain-of-Thought Instruction Tuning. In: Findings of the Association for Computational Linguistics: EMNLP 2023. Ed. by Houda Bouamor, Juan Pino, and Kalika Bali. Singapore: Association for Computational Linguistics (cited on pages 3, 6). Meng, Kevin, David Bau, Alex Andonian, and Yonatan Belinkov (2022). Locating and editing factual associations in gpt. In: Advances in neural information processing systems (cited on pages 6, 15). Nanda, Neel, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt (2023). Progress measures for grokking via mechanistic interpretability. In: The Eleventh International Conference on Learning Representations (cited on pages 6, 17, 41). Newel, Allen and Herbert Simon (1976). Computer science as empirical inquiry: Symbols and search. In: Communications of the ACM (cited on page 5). Nye, Maxwell, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models. arXiv: 2112.00114 [cs.LG] (cited on page 5). Olsson, Catherine et al. (2022). In-context Learning and Induction Heads. In: Transformer Circuits Thread (cited on pages 6, 16). Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu (May 30, 2018). Neural Discrete Representation Learning. arXiv: 1711.00937 [cs]. Pre-published (cited on page 5). Pollack, Jordan (1990). Recursive distributed representations. In: Artificial Intelligence (cited on pages 3, 4). Salakhutdinov, Ruslan and Geoffrey Hinton (2009). Deep boltzmann machines. In: Artificial intelligence and statistics. PMLR (cited on page 5). Schwarzschild, Avi, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein (Nov. 2, 2021). Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks. arXiv: 2106.04537 [cs]. Pre-published (cited on pages 5, 10). Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv: 2402.03300 [cs.CL] (cited on page 3). Singley, Mark and John Robert Anderson (1989). The transfer of cognitive skill. Harvard University Press (cited on page 3). Smolensky, Paul, Roland Fernandez, Zhenghao Herbert Zhou, Mattia Opper, and Jianfeng Gao (2024). Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks. arXiv: 2410.17498 [cs.AI] (cited on page 5). Socher, Richard, Brody Huval, Christopher Manning, and Andrew Ng (2012). Semantic compositionality through recursive matrix-vector spaces. In: Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning (cited on pages 3, 4). Stechly, Kaya, Karthik Valmeekam, and Subbarao Kambhampati (2024). Chain of thoughtlessness? an analysis of cot in planning. In: The Thirty-eighth Annual Conference on Neural Information Processing Systems (cited on pages 3, 4, 9). Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu (2023). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv: 2104.09864 [cs.CL] (cited on page 24). Sutskever, Ilya, Oriol Vinyals, and Quoc Le (2014). Sequence to sequence learning with neural networks. In: Advances in neural information processing systems (cited on page 4). Tian, Yuandong (2024). Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets. arXiv: 2410.01779 [cs.LG] (cited on pages 6, 17, 41). Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin (2017). Attention is all you need. In: Advances in neural information processing systems (cited on pages 3, 4, 24). Veliˇckovic, Petar and Charles Blundell (2021). Neural algorithmic reasoning. In: Patterns (cited on page 3). Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. In: Advances in neural information processing systems (cited on pages 3, 5, 8). Weiss, Gail, Yoav Goldberg, and Eran Yahav (July 19, 2021). Thinking Like Transformers. arXiv: 2106.06981 [cs]. Pre-published (cited on page 5). Yang, Liu, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos (Mar. 16, 2024). Looped Transformers Are Better at Learning Learning Algorithms. arXiv: 2311.12424 [cs]. Pre-published (cited on page 4). Ye, Tian, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu (July 29, 2024). Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process. arXiv: 2407.20311 [cs]. Pre-published (cited on page 8). 22 Zhou, Hattie, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, and Preetum Nakkiran (2024). What Algorithms can Transformers Learn? Study in Length Generalization. In: The Twelfth International Conference on Learning Representations (cited on pages 3, 4, 9). Zhou, Yongchao, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou (Feb. 14, 2024). Transformers Can Achieve Length Generalization But Not Robustly. arXiv: 2402.09371 [cs]. Pre-published (cited on pages 3, 4, 9). 23 A. Experimental Details on Chain-of-Thought & End-to-End Baselines This section provides further experimental details on the chain-of-thought and end-to-end baselines. A.1. End-to-End Baselines The end-to-end models in our experiments are causal encoder-only Transformer models with fixed depth and/or number of iterations that are trained with end-to-end supervision only. That is, they receive supervision on the final solution, but do not receive fine-grained supervision on the intermediate steps to explicitly align the models to universal algorithmic problem-solving procedure. Within the end-to-end baselines, we consider feedforward models and recurrent models. Feedforward models have fixed number of layers and independently-learned parameters at each layer. Recurrent models, on the other hand, have recurrent block consisting of some number of Transformer layers, which is applied recurrently for fixed number of iterations. Recognizing the importance of positional encoding for length generalization (Kazemnejad et al., 2023), we explore several positional encoding methods for each class of methods that we evaluate. In particular, we evaluate learned absolute positional embeddings (Vaswani et al., 2017) (AbPE), Rotary Positional Encoding (Su et al., 2023) (RoPE), No Positional Encoding (Kazemnejad et al., 2023) (NoPE), and the relative positional-encoding method proposed by (He et al., 2021) (DeBERTa). We perform hyperparameter search across each of these factors, varying the number of recurrent iterations , the number of layers per recurrent block L, the hidden state dimension D, and the positional encoding method. As described in the main text, we train on dataset of examples with up to 32 nodes, and evaluate on examples varying in size from 8 nodes to 128 nodes. Figure 8 depicts the average OOD performance as measured by the % Fully Solved metric for each baseline model configuration. The results in the main text correspond to the best-performing end-to-end models according to this metric. In particular, the best-performing recurrent model is RoPE-T4L2H16D256, and the best-performing feedforward model is DeBERTa-T1L8H16D256. Note that the naming scheme describes the positional encoding method, the number of recurrent steps , the number of layers in the Transformer block, the number of attention heads H, and the model dimension D. = 1 corresponds to feedforward model with no recurrence. Figure 9 depicts additional experimental results for the end-to-end baseline experiments. 24 Figure 8. comparison of average OOD generalization performance of different feedforward and recurrent baselines, varying architectural hyperparameters. This is computed as the average of the % Fully Solved metric computed on inputs of varying size from = 8 to = 128. 25 (a) Each line corresponds to an experimental run. Lines are color-coded by positional encoding, but other architectural hyperparameters vary and are not represented. (b) Average % Fully Solved across test splits for the best model of each positional encoding method. The relative positional encoding methods, RoPE and DeBERTa perform best. (c) % Fully solved by graph size for best model of each positional encoding method in the feedforward baselines. (d) % Fully solved by graph size for best model of each positional encoding method in the recurrent baselines. (e) % Fully solved by graph size for the best model of each architectural configuration. Recurrent models slightly outperform feedforward models. Computational depth (i.e., L) is crucial, with shallow models performing poorly even on the smallest in-distribution inputs. (f) Average attention score entropy by input size. Attention scores disperse as the input size increases. Figure 9. Further experimental results for end-to-end baselines. All end-to-end models struggle to generalize beyond the training distribution, regardless of architectural hyperparameters. A.2. Chain-of-Thought Baselines The chain-of-thought baselines in our experiments are causal Transformer language models that are trained with next-token prediction objective on sequence data that includes step-by-step solution of the problem instance. The models are evaluated by prompting them with the problem instance and autoregressively generating the entire chain-of-thought via greedy decoding procedure. We begin by providing more details on the construction of the chain-of-thought trajectories for these baselines, then provide further details on the experimental setup and present additional results. A.2.1. Chain-of-Thought Trajectories We experiment with few different types of chain-of-thought trajectories, providing different levels and styles of supervision on the intermediate computation. As described in the main text, the first part of the sequence is always the description of the input problem, which matches the format of the other methods we consider: sequence of equations that define computational graph to be solved. This is then followed by special CoT token which indicates the end of the input and the beginning of the chain-of-thought. The chain-of-thought involves solving each variable in the input in linear order, one-by-one. We experiment with two types of CoT trajectories that vary the level of detail. The first provides supervision on the values only. The CoT simply recalls that variables one-by-one and computes their values, without recalling the equation that defined them. [...Input Prompt...]CoT[...]x101=4 The second type of CoT trajectory involves first recalling the equation that defined the variable, then recalling the values of the variables in the equation, and then computing the value of the desired variable. This requires longer chain-of-thought but provides richer supervision. [...Input Prompt...]CoT[...]x101=x23+x91=22+5=4 27 Below, we provide an example of full CoT trajectory on an input with = 32 nodes. 2=x3 [sep] 2=x30 [sep] 18=x12 [sep] 14=x11 [sep] 15=x20 [sep] 8=x23 [sep] x30=x9 [sep] x23+x3=x22 [sep] x20x23=x27 [sep] x3+x22=x0 [sep] x3+x22x11=x26 [sep] x20x22+x23=x13 [sep] x22=x24 [sep] x12x23x0=x17 [sep] x11x26=x28 [sep] x13x11+x23=x21 [sep] x17x3=x25 [sep] x30x17x23=x6 [sep] x17=x16 [sep] x11+x21=x7 [sep] x28+x17x21=x14 [sep] x7=x15 [sep] x7=x31 [sep] x12+x3+x14=x5 [sep] x14=x19 [sep] x23x5x7=x29 [sep] x5=x18 [sep] x25+x23x19=x4 [sep] x14x29x5=x2 [sep] x29x28x7=x1 [sep] x3x23x18=x8 [sep] x8x28x0=x10 CoT x3=2 [sep] x30=2 [sep] x12=18 [sep] x11=14 [sep] x20=15 [sep] x23=8 [sep] x9=x30=2 [sep] x22=x23+x3=10 [sep] x27=x20x23=5 [sep] x0=x3+x22=12 [sep] x26=x3+x22x11=7 [sep] x13=x20x22+x23=13 [sep] x24=x22=10 [sep] x17=x12x23x0=17 [sep] x28=x11x26=6 [sep] x21=x13x11+x23=7 [sep] x25=x17x3=15 [sep] x6=x30x17x23=3 [sep] x16=x17=17 [sep] x7=x11+x21=21 [sep] x14=x28+x17x21=16 [sep] x15=x7=21 [sep] x31=x7=21 [sep] x5=x12+x3+x14=13 [sep] x19=x14=16 [sep] x29=x23x5x7=10 [sep] x18=x5=13 [sep] x4=x25+x23x19=7 [sep] x2=x14x29x5=9 [sep] x1=x29x28x7=16 [sep] x8=x3x23x18=1 [sep] x10=x8x28x0=6 A.2.2. Experimental Details & Additional Results We perform hyperparameter search varying: the number of recurrent iterations , the number of layers per recurrent block L, the hidden state dimension D, and the positional encoding method. As described in the main text, we train on dataset of examples with up to 32 nodes, and evaluate on examples varying in size from 8 nodes to 128 nodes. Figure 10 depicts the average OOD performance as measured by the % Fully Solved metric for each baseline model configuration. The results in the main text correspond to the best-performing CoT-supervised model according to this metric, which is the RoPE-T4L2H16D256 model. Figure 12 depicts additional experimental results for the end-to-end baseline experiments. We highlight few observations here: 28 Figure 11 shows that some models are able to recall the equation structure correctly in their CoT, but are unable to robustly compute the values correctly. This suggests that common source of error in the CoT baselines is the arithmetic computation, rather than copying equations from the input. As with the end-to-end baselines, the positional encoding method was critical for performance and length generalization. Among the methods we evaluated, we found NoPE to perform best, generalizing well to 40 nodes when trained on 32 nodes. The other positional encoding methods fail to generalize beyond the training regime. No method generalized robustly beyond 40 nodes. As with the end-to-end baselines, the computational depth of the model had significant effect on performance. In particular, 4 layer models failed to learn the task well, but 8-layer models achieved good in-distribution performance and limited degree of out-of-distribution generalization. 29 Figure 10. comparison of average OOD generalization performance of different CoT-supervised baselines, varying architectural hyperparameters. The metric is full sequence accuracy, which measures the proportion of inputs where every nodes value is computed correctly. The naming scheme matches the previous section, but adds prefix describing the format of the CoT trajectories. Val means that the CoT trajectory directly computes the values of each variable, whereas Eq-Val first recalls the equations and then computes the values. Here, (Var Len) indicates runs where the input problem size is variable and randomly sampled in 32, rather than being only = 32. 30 Figure 11. comparison of average OOD generalization performance of different CoT-supervised baselines, varying architectural hyperparameters. The metric is % Equation Structure Correct, which measures the proportion of inputs where the autoregressively generated CoT has the correct equation structure (without checking whether the values computed are correct). 31 (a) Each line corresponds to an experimental run. Lines are color-coded by positional encoding, but other architectural hyperparameters vary and are not represented. (b) Average OOD performance across test splits for the best model of each positional encoding method. (c) % Fully solved by graph size for the best model of each positional encoding method. We find NoPE to achieve the best out-of-distribution generalization performance, generalizing well to 40 nodes when trained on 32 nodes. The other positional encoding methods fail to generalize beyond the training regime. (d) % Fully solved by graph size for the best model of each architectural configuration. Computational depth (i.e., L) is crucial for good performance, with shallow models performing poorly even in-distribution on larger inputs. Figure 12. Further experimental results for CoT baselines. While chain-of-thought supervision yields improved performance over end-to-end models, out-of-distirbution generalization capabilities are limited. 32 B. Details on Latent State Supervision B.1. Latent State Embedding Structure The input to the model is presented as sequence of equations defining the value of each node in the computation graph. The vocabulary of the input includes variable names (e.g., x42), numerical values (e.g., 17), operations (e.g., +), and special symbols like equality = or equation separation [sep] . To provide the model with supervision on each part of the input, we employ special tokenization and embedding scheme. We use factored structure to tokenize each symbol in the input into 4-component tokens: syntax, variable, operation, and value. For example, the input 17=x42 [sep] ..., is tokenized as follows before the first iteration: syntax variable operation value 17 [ = [ x42 [ [sep] [ value = variable [sep] N/A N/A x42 N/A N/A N/A N/A N/A"
        },
        {
            "title": "17\nN/A\nempty\nN/A",
            "content": "] ] ] ] The syntax factor can be value, variable, operation, or the special symbols = or [sep] . The variable factor is the variable names {x0, . . . , x127}. The operation factor is the set of arithmetic operations (e.g, +, , ). The value factor is the set of numerical values (i.e., {0, . . . , 22}). We also include an N/A symbol for the variable, operation, and value factors. For example, symbols with value syntax do not have variable factor, etc. We also include special empty symbol for the value factor of variable tokens. In the input to the model, the variable tokens have empty value factors because their values have not been computed yet. As the model processes the input, it iteratively computes the values of different variables and fills in their value factor. We train separate embedder for each factor, and map the input to vector embeddings by embedding each factor and adding the embeddings. B.2. Latent State Supervision The Continuous Latent Space Supervision, Discrete Latent Space Supervision, and Discrete Latent Space Supervision methods share the same latent state supervision scheme. We train these recurrent models to learn to solve the input problem be computing node values one layer deeper in the computation graph with each recurrent iteration. We do this by defining loss function at each iteration that penalizes predictions only for variables with depth less than or equal to the current iteration. For each factor {syntax, variable, operation, value}, we learn linear read-out layer Wfactor RdmodelVfactor that maps the vector state at the end of the recurrent iteration to prediction of each factor. Here, Vfactor denotes the vocabulary for the given factor (e.g., for the value factor, this is {0, . . . , 22, N/A, empty}). We provide the model with supervision on its latent states by defining loss for each factor and at each recurrent iteration. In particular, the loss function for the value factor is defined such that the model is trained to predict the values of all variables that occur at depth in the computation graph. In particular, 33 for an input sequence = (x1, . . . , xn), the value factor loss at iteration is defined as Loss(factor = value, iteration = t) = (cid:88) (cid:16) ℓ Wvalue E(t) , Value(xi) (cid:17) . (5) i[n] Depth(xi)t where Depth(xi) is the depth of the variable xi in the input computation graph, Value(xi) is its computed value, and E(t) Rdmodel is the vector embedding of xi at recurrent iteration t. Here, ℓ is the cross-entropy loss. The overall loss used to train the models is the sum of the individual factor losses at each iteration. Loss = (cid:88) (cid:88) factor Loss(factor = value, iteration = t). (6) B.3. Discretization of Intermediate States The training procedure described above applies to the Continuous Latent Space Supervision, Discrete Latent Space Supervision, and Discrete Latent Space Supervision methods in the same way. In the methods with discrete latent bottleneck, we introduce an additional architectural mechanism where the read-out layers are used not only for computing the loss on the intermediate iterations, but also for mapping the latent representation to discrete space. In particular, letting E(t) be the embedding of the i-th token after recurrent iterations, we use argmax decoding of the linear read-outs to map the embedding to discrete prediction for each factor. This discrete state is then re-embedded using the same learned embedding module to form the vectorized input E(t+1) the next iteration. In particular, at iteration t, the models forward pass is defined as follows at ( E(t+1) 1 ) RecurrentTransformerBlock(E(t) 1 , . . . , E(t) ) , . . . , E(t+1) z(t+1) i,factor arg max{Wfactor i,factor FactorEmbed(z(t+1) E(t+1) E(t+1) i,syntax + E(t+1) E(t+1) E(t+1) } factor {syntax, variable, operation, value} (7) i,factor) factor {syntax, variable, operation, value} i,variable + E(t+1) i,operation + E(t+1) i,value. This discretization enables us to train the model with type of teacher-forcing across recurrent iterations. That is, we can teacher-force the inputs z(t) at each iteration t. This enables more efficient training. B.4. Self-Correction Mechanism In reasoning task, each reasoning step depends crucially on the prior steps in the reasoning path. If mistake is made at any stage, all subsequent computation is affected, and the error is often fatal. As the size of the problem and the number of computational steps scale, the likelihood of an error occurring at some point in the reasoning process becomes large, limiting the ability to generalize indefinitely to more complex problems. To address this challenge, reasoning model must be able to detect and correct errors as they occur in order to recover when mistake is made in its previous computation. We train the model to detect and correct errors by randomly corrupting the models latent state. That is, at each iteration, with some small probability, we corrupt random selection of the value components of the 34 models discrete states. To achieve good loss, the model must learn to detect when previously-computed value is incorrect and correct it before proceeding. B.5. Experiment Details & Additional Results As with the baselines, we explore the effect of different architectural hyperparameters, such as positional encoding and the depth of the recurrent block, on model performance. Figure 13 depicts the average OOD perfromance as measured by the % Fully Solved metric for each model configuration in the Discrete Latent Space Supervision, and Discrete Latent Space Supervision methods. The results in the main text correspond to the best-performing models according to this metric. In particular, the best-performing Discrete Latent Space Supervision model is DeBERTa-L2H16D256, and the best-performing Discrete Latent Space Supervision model is DeBERTa-L4H16D384. Figure 14 depicts additional experimental results for the Discrete Latent Space Supervision, and Discrete Latent Space Supervision methods. We highlight few observations here: The positional encoding method is critical for length generalization. The DeBERTa positional encoding method (a relative positional encoding method) performed the best by far. 2 layers for the recurrent block were sufficient for the Discrete Latent Space Supervision method. However, the recorrection mechanism of Discrete Latent Space Supervision required deeper recurrent block. We saw no significant improvement for the re-correction mechanism with 2 layers, but with 4 layers, the re-correction mechanism kicked in and enabled near-perfect OOD generalization. 35 Figure 13. Average % Fully Solved, across # nodes between 8 and 128, with training on 32 nodes, 36 (a) Each line corresponds to an experimental run. Lines are color-coded by positional encoding, but other architectural hyperparameters vary and are not represented. (b) Average OOD performance across test splits for the best model of each positional encoding method. (c) % Fully solved by graph size for best model of each positional encoding method. (d) % Fully solved by graph size for best model of each architectural configuration. Figure 14. Further experimental results for methods exploring our proposed architectural mechanisms. 37 C. Details of Mechanistic Interpretability Analysis In this section, we provide additional experimental evidence to support our claim on the mechanism learned by the model together with the error analysis of the models predictions. Notice: The following analysis is conducted only for showing the computation happening at the RightHand Side (RHS) position in each equation, as it is the place where the model is expected to compute the final result. Model Configuration. We use DeBERTa-L2H16D256 trained with our proposed Discrete Latent Space Supervision method (without the re-correction mechanism) on the mathematical reasoning task. Specifically, the recurrent Transformer model is configured with two transformer blocks, 16 attention heads, hidden dimension of 256. We use DeBERTas relative positional encoding method. The training data is the same as the one used in the main text. We choose this model setup because it is the best-performing configuration according to the % Fully Solved metric displayed in Figure 13 for two-layer model. In particular, we cherry-pick the best-performing model trained with the same configuration with different random seeds, which has % Fully Solved score of 99.98% on the OOD test set. We use this model to conduct the mechanism analysis for better interpretability. We train on modular-23 addition task with maximum graph size 32. The total number of variables in the training data is 128. The testing data used for mechanism analysis has the maximum graph size 128. Testing Data for Interpretation Analysis. To rigorously understand the inner workings of the model, we conduct controlled experiments by systematically varying the input data fed to the trained model. In particular, each testing example is sequence of arithmetic equations with number of nodes 128, appended with new probe equation to the end of the sequence with the following format: [sep] var0 + var1 + var2 = rhs . (8) where var0 , var1 , and var2 are the three variables in the probe equation, and rhs is the right-hand side of the probe equation. Thus, var0 , var1 , var2 , rhs are chosen from = {x1, . . . , x128}, and the true values of these variables are in = {0, 1, . . . , 22}. Additional Definitions and Notations. In the following, we frequently use the following definitions and notations: Head Output: For given attention head h, we define the head output for query vector qh Rdh for head dimension dh as Head Output(h) = softmax(qhK / (cid:112) dk)VhW (h) , where Kh and Vh are the key and value matrices of the head h, respectively, and (h) is the output projection matrix of the head h. In standard attention mechanism, each headsquery, key and value , (h) vector is obtained by applying linear transformation to the attention input specified by (h) , and (h) , respectively. The above definition can be applied to define the head output for any query 38 position. However, since our mechanism analysis focuses exclusively on the RHS position, we consistently define the head output as the output of the attention head at the RHS position. Here, we dont include the bias of the head output projection in the definition of the head output, as the bias applied to the final attention output is not specified to individual heads. OV Combined Matrix: For group of attention heads [16], we define the OV combined matrix as (H) OV = (cid:88) hH (h) (h) , where (h) matrix of the attention head H, respectively. Rdhd and (h) Rddh are the output projection matrix and the value projection C.1. Technique Overview To simplify the discussion, let us consider concrete example of the probe equation: [sep] x91+x88+x55=x30, (9) i.e., var0 = x91, var1 = x88, var2 = x55, and rhs = x30. To solve this equation, at the token rhs ,the model needs perform the following computations: (i) Identify that the variable names x91, x88, and x55 appear in the left-hand side of the equation; (ii) Retrieve the values of the variables x91, x88, and x55 from the previous equations, denoted by value(x91), value(x88), and value(x55), respectively; (iii) Compute the modular sum value(x91) + value(x88) + value(x55) mod 23. In the following, we show that the first layer attention is responsible for identifying the variable names, the second layer attention is responsible for retrieving the values of the variables, and the last MLP layer computes the modular sum. Before we dive into the details, we first introduce the three interpretation techniques that we will use in the sequel. C.1.1. Interpretation Technique for Attention Layers Overview. Attention mechanisms in transformers fundamentally perform information routingthey decide what information from previous token positions should be aggregated at the current position. In our arithmetic reasoning task, we hypothesize that attention heads act as specialized circuits that copy specific types of information from source tokens to the RHS position where computation occurs. To understand these circuits, we need to answer two fundamental questions: (i) Which tokens does each attention head attend to? (ii) What information is being copied from those tokens? Our approach combines controlled experimentation with mathematical analysis. We use probe equations with systematic variations to identify attention patterns, then employ linear algebra techniques to decode the information flow through the models weight matrices. This methodology reveals that attention heads 39 self-organize into functional groups, with each group specialized for specific subtask in the arithmetic computation pipeline. Application to First Layer Attention. For the first layer, we test whether attention heads identify which variables appear in the equation. We construct probe equations of the form in (8) and systematically vary the variable names (e.g., changing var0 from x91 to x42) while keeping other variables fixed. For each configuration, we measure the relative variance of each heads output, which captures how much heads output changes when we vary specific input, normalized by the heads typical output magnitude. High relative variance indicates the head is sensitive to changes in that variable position (see Section C.2.1 for the rigorous definition). Through this analysis, we discover that heads form distinct groups: heads {4, 8} attend to var0 , heads {5, 12} to var1 , and heads {3, 7, 11, 14} to var2 . To understand what these heads copy, we analyze their combined OV matrices using norm amplificationmeasuring how much each factored embedding type (among syntax, variable, operation, value) is amplified when passed through the matrix. We find that these heads specifically amplify the variable factor (with amplification 15) over other factors ( 5), confirming they copy variable identities rather than values. Our analysis suggests that the first layer attention implements variable identification, with specialized head groups that extract and route variable names from equation positions to the RHS for further processing. Application to Second Layer Attention. For the second layer, we test whether attention heads retrieve variable values needed for computation. Using the same probe equation structure, we now vary the values of variables of the probe equation (by modifying earlier equations in the sequence) while keeping variable names fixed. We again measure relative variance to identify head groups and find: heads {0, 8, 15} retrieve values for var0 , heads {5, 10} for var1 , and heads {2, 3, 4, 7, 9} for var2 . The norm amplification analysis reveals these heads strongly amplify the value factor over others, confirming they copy numerical values rather than identities. Interestingly, the copied value embeddings maintain nearorthogonality between different variables while showing circulant patterns within each variablesuggesting Fourier-like encoding that facilitates downstream arithmetic operations. In conclusion, the second layer attention implements value retrieval, with specialized head groups that fetch the numerical values of variables identified by the first layer, preparing them for arithmetic computation. C.1.2. Interpretation Technique for the Second MLP Layer After the attention layers have assembled the necessary information, the second MLP must perform the actual arithmetic computation. At this point, the residual stream at the RHS position contains structured information from both attention layers: variable identities from the first layer and their corresponding values from the second layer. To understand how the MLP transforms this information into the final answer, we analyze the structure of its input representations and apply frequency domain analysis to decode the computation. Understanding the MLP Input Structure. The input to the second MLP at the RHS position consists of value embeddings that have been copied and transformed by the second layer attention. To understand this precisely, we need to consider three key components: 40 First, recall that in our factored representation, the value factor can take 25 possible values: the numbers 0-22 for actual computations, plus special tokens empty and N/A. Each of these 25 values has learned embedding vector embedvalue(v) where {0, 1, . . . , 22, empty, N/A}. Second, the second layer attention has three distinct head groups H0 = {0, 8, 15}, H1 = {5, 10}, and H2 = {2, 3, 4, 7, 9} each responsible for copying the value of one variable position. Each group has its own combined OV matrix that transforms the value embeddings it copies. This transformation creates what we call \"new value\" embeddings: new_valuei(v) = (Hi) OV embedvalue(v) (10) where {0, 1, 2} indicates which variable position the head group attends to, and is the actual value being transformed. Intuitively, new_valuei(v) measures the contribution of of var to the residual stream at the RHS position, after second layer attention, and before the second MLP, when var has value v. Third, since each of the three head groups can potentially transform any of the 25 value embeddings, there exist 3 25 = 75 distinct transformed value embedding vectors in total. These 75 vectors essentially form lookup table: for variable position with value v, the corresponding transformed embedding is new_valuei(v). In concrete example: when the probe equation has var0 = x91 with value 7, var1 = x88 with value 15, and var2 = x55 with value 3, the second layer attention specifically copies and transforms three of these 75 vectors new_value0(7), new_value1(15), and new_value2(3) to the RHS position. The residual stream at RHS thus contains the sum of these three transformed value embeddings, which serves as the input to the second MLP. Crucially, as we will see in Appendix C.4, these transformed embeddings exhibit circulant structure that makes them amenable to frequency analysis. Frequency Domain Analysis. Motivated by the circulant structure, we propose to adopt frequency domain analysis to understand how the second layer MLP performs modular addition. Our approach involves two key steps. First, we systematically vary the inputs by creating probe equations where var0 , var1 , and var2 each take all values from 0 to 22. For each configuration (x, y, z) {0, 1, . . . , 22}3, the residual stream at the RHS position contains the sum new_value0(x) + new_value1(y) + new_value2(z). Second, we extract these representation vectors at four key network positions MLP pre-activation, postactivation, output, and final decoder and apply three-dimensional Discrete Fourier Transform (DFT). This DFT, computed independently for each coordinate of the representation vector, transforms our data into 4D tensor: three dimensions for the frequencies (a, b, c) {0, 1, . . . , 22}3 corresponding to the three input variables, and one dimension for the vector coordinates. By analyzing the norm of different frequency patterns in this tensor, we discover that the MLP performs modular addition through sinusoidal basis functions, with the diagonal frequencies (a, a, a), representing the sum + + z, becomes dominant as signals propagate through the network. Therefore, the second MLP implements modular arithmetic in the frequency domain, where the periodic nature of trigonometric functions naturally handles the modulo-23 computation. This result is consistent with the findings in the literature. See, e.g., Doshi et al. (2024), Nanda et al. (2023), and Tian (2024) for more details. C.2. First Layer Attention: Variable Copying In the followiing, we will give detailed analysis of the first layer attention mechanism. C.2.1. Group Structure in the First Layer Attention (a) Relative Variance for var0 (b) Relative Variance for var1 (c) Relative Variance for var2 (d) Relative Variance for rhs Figure 15. Relative variance heatmaps when we vary the value of var0 , var1 , var2 , and rhs . Each row corresponds to query position and each column corresponds to an attention head. To rigorously demonstrate this grouping structure, Experiment Design for Group Structure Detection. The attention heads in the first layer exhibit clear grouping pattern based on which variable position they attend to. To identify the group structure in the first layer attention and detect which heads belong to which groups, we measure each heads relative variance when we vary the variable name of each of the four variables var0 , var1 , var2 , and rhs in the probe equation in (8). As an example, to detect which heads attend to var0 , we fix var1 , var2 , and rhs while randomly sampling different variables xi with = 1, . . . , 128 for var0 in (8). Note that the variable xi must be computed in the preceding equations. Otherwise, the model cannot compute the value of xi and the probe equation is invalid. As our testing data has all variables computed in the preceding equations, we collect 128 samples that only differ in the value of var0 . We then compute the relative variance of each attention heads output at each position within the probe equation across the 128 samples. Note that relative variance is measure of how much the heads output varies in response to changes in var0 , and we give the rigorous definition in the next paragraph. The analysis can also be conducted for the other variable positions, i.e., var1 , var2 , and rhs , and the results are reported in Figure 15. Relative Variance Calculation. Let us take different sequences, e.g., the 128 sequences in the above experiment design. We only consider one RHS position for the probe equation in each sequence. For given attention head h, we define the relative variance over the sequences as Relative Variance(h) = tr(Cov(Head Output(h))) E[Head Output(h)2 2] . (11) Here, the covariance matrix for sequence of vectors v1, . . . , vn is defined as Cov(v1, . . . , vn) = 1 (cid:88) (vi v)(vi v), i= where is the mean of the sequence over the sequences, and E[] is the empirical expectation over the sequences. Intuitively, the relative variance measures how much the heads output varies relative to its overall magnitude. higher relative variance indicates that the attention heads output has larger variance relative to its overall magnitude. Since we only change var0 in the above example, larger relative variance for head means that the heads output is primarily influenced by var0 . Illustration of Figure 15. In Figure 15, we plot the relative variance heatmaps for all 16 attention heads when we vary the variable names of var0 , var1 , var2 , and rhs . Each column corresponds to different attention head, and each row corresponds to different query position. As our goal is to understand the mechanism at the rhs position, we focus on the last row, which corresponds to the rhs query position in the figures. Each subfigure plots the relative variance heatmap for altering one particular variable. higher relative variance in one subfigure indicates that the attention heads output is more sensitive to changes in the corresponding variable. Based on these results, we observe clear group structure in the first layers attention heads: Heads 4 and 8s relative variance is high only when we change the value of var0 , while the relative variance of the other heads is low. This facts suggests that heads 4 and 8 attend primarily to var0 . Similarly, heads 5 and 12 attend primarily to var1 , and heads 3, 7, 11, and 14 attend primarily to var2 . The last subfigure plots the relative variance heatmap for the rhs position. We observe that heads 2, 9, 13, and 15 attend to the RHS position rhs , and the remaining heads do not exhibit distinct pattern according to the relative variance heatmap and are unimportant. Notice that the above head groups are all disjoint. This further indicates that each head is specialized for specific variable position. This result is also backed up by the trace of the attention logits of the first layer attention heads as shown in Figure 7. Summary of the Group Structure. We observe that the attention heads in the first layer exhibit clear grouping pattern based on which variable position they attend to. Therefore, we know that the first layer attention must be copying something from the LHS variables to the RHS position. In the following, we will conduct further analysis to identify what information is being copied. 43 C.2.2. First Layer Attention Copying the Variable Identity Here, by saying copying the variable identity, we mean that the attention head is copying the factored embedding of variable among the four factored embeddings {syntax, variable, operation, value}. In the previous experiment, we have identified that the first layer attention heads are grouped into four groups, each of which attends to specific variable position. Now, we aim to identify which of the four factored embeddings is being copied by these groups. Norm Amplification Analysis. As shown in the last equation of (7), the embedding vector E(t) the four factored embeddings. We have shown that each attention head group predominantly attends to particular variable name. In this case, at rhs , vectors of the form (h) is added to the residual stream in the t-th Recurrent Transformer block, where is previous token (more specifically, the token that head attends to). Intuitively, if the OV matrix is responsible for copying particular factor, then we would expect is sum of OV E(t) i that it is more aligned to the subspace spanned by the embeddings of this particular factor. To rigorously characterize this intuition, we analyze the norm amplification for each type of factored embeddings when passed through the combined OV matrix of different head groups. Specifically, we define the norm amplification for matrix WOV on input as: Norm Amplification(WOV , x) = WOV x2 . (12) Note that the above definition can be applied to any matrix WOV and input with agreeing dimensions. For our analysis, we will consider WOV as the combined OV matrix of the attention heads in group. Specifically, let [16] be group of attention heads, and let (h) be the output projection matrix and the value projection matrix of the attention head H, respectively. The combined attention OV matrix for group [16] is then defined as and (h) (H) OV = (cid:88) hH (h) (h) . Here, the group structure is discovered by Appendix C.2.1. That is, the 16 heads are split into , {4, 8} (cid:124) (cid:123)(cid:122) (cid:125) var0 , {5, 12} (cid:124) (cid:123)(cid:122) (cid:125) var1 , {3, 7, 11, 14} (cid:125) (cid:123)(cid:122) (cid:124) var2 , {2, 9, 13, 15} (cid:125) (cid:123)(cid:122) (cid:124) rhs {0, 1, 6, 10} (cid:125) (cid:123)(cid:122) (cid:124) unimportant , (13) depending on which variable they attend to. By the definition of norm amplification, if the OV matrix is responsible for copying the identity of the variable, we expect to see large amplification for the variable factored embedding, and small amplification for the other types of embeddings {syntax, operation, value}. In that case, With slight abuse of notation, for each factored embedding type {syntax, variable, operation, value}, we can define the norm amplification for each factor type as Norm Amplification(W (H) OV , factor type) = Exfactor type (cid:104) (H) OV x2 x2 (cid:105) . Here, Exfactor type is the average over the set of all factored embeddings of the same type and is one of 44 the five groups in (13). For example, if we consider the variable factored embedding type and = {4, 8}, we have Norm Amplification(W (H) OV , variable) = Exvariable (cid:104) (H) OV x2 x2 (cid:105) , where iterates over all the 128 factored embeddings of the type variable and the OV matrix is based on the head group {4, 8}. In this case, this quantity measures the average norm amplification of the factored embeddings of the type variable when passed through the combined OV matrix of the head group {4, 8}. Comparing the Norm Amplification Across Different Groups. In Figure 7 (right), we compute the norm amplification by averaging the norm amplification over all the factored embeddings within each factor type. In Figure 16, we further provide histogram for each factored embeddings norm amplification for different factor type while different groups are highlighted in different colors, which provides more detailed view of the norm amplification across different groups. Figure 16 shows that, for each head group, the norm amplification for the variable factored embedding is significantly larger than that of the other types of embeddings. This can be seen by comparing the histograms in each subfigure with the same color. This confirms our hypothesis that the OV matrix is responsible for copying the variable factored embeddings of the variable to the RHS position. Additional Evidence on change of number of variables. We provide one interesting side-observation on how the model handles different numbers of variables in the input equations in Figure 17. Head 4 and head 8 are the two attention heads that attend to the first variable position in the first layer attention when the number of variables is 3. When the number of variables is changed to 2, we observe that head 4 now attends to the [sep] token, while head 8 attends to the equal sign token of the previous equation. This indicates that the equal sign token and the [sep] token act as attention sink for head 4 and head 8, respectively. C.2.3. First Layer MLP Residual Stream Does Not Change the Residual Stream Significantly We measure the changes brought by the first MLP layer to the residual stream by computing the Relative L2 Error as: L2 Relative Error = Residual Before MLP Residual After MLP2 Residual Before MLP2 . This metric quantifies how much the MLP alters the original residual signal. Figure 18 illustrates the heatmap of the relative L2 error computed at the rhs position across set of 256 samples. We observe that the relative L2 error is relatively small, which indicates that the MLP layer does not change the residual stream significantly. C.2.4. Conclusion By combining the results from the first layer attention and the first layer MLP, we conclude that the first transformer block plays the following role: 45 (a) Norm Amplification for OPERATION (b) Norm Amplification for SYNTAX (c) Norm Amplification for VALUE (d) Norm Amplification for VARIABLE Figure 16. Histogram of norm amplification (defined in (12)) for the embeddings in the four factored embedding types {syntax, variable, operation, value} when passed through the first attention layers combined OV matrix. Each subfigure contains five histograms in different colors, while each histogram corresponds to different group of attention heads combined OV matrix. Here, the 16 attention heads are grouped by the different variable they attend to, which are var0, var1, var2, and rhs, and an additional group for the heads that do not demonstrate clear pattern. Mechanistic Interpretation of the First Transformer Block. At the RHS position, the first transformer block copies the variable identity from the LHS of the equation to the RHS. That is, the variable names of the LHS in the probe equation (8) are copied to the RHS position, but not their values. C.3. Second Layer Attention: Value Copying Hypothesis on the Second Layer Attention Heads. As we have shown previously, the first layer attention heads copy the variable factored embeddings of the variable to the RHS position, which tells the model the identity of all the variables on the LHS of the probe equation (8). To compute the final answer for the RHS position, the model still needs to copy the values of the variables var0 , var1 , and var2 to the RHS position. We hypothesize that the second layer attention heads will copy the values of the variables var0 , var1 , and var2 to the RHS position. The Second Layer Attention Heads also Have Group Structure To test this hypothesis, we prepare data that contains probe equations of the same form as in (8) and conduct controlled experiment designed to 46 analyze how attention heads respond to changes in the values of individual variables. Different from the previous experiment where we change the variable identity, this time we fix the variable identity and only change the value of each variable var0 , var1 , and var2 one at time while keeping the other two variable values fixed. This is achieved by altering the equations before the probe equation, which compute the value of the variable, to be modified. Specifically, for each of the three variables var0 , var1 , and var2 , we conduct separate experiment where we collect samples by varying only that variables value while keeping the other two variables fixed. Then, for each variable var with = 0, 1, 2, we collect the second layer attention head outputs across the samples at the RHS position of the probe equation, and compute the following metrics: The variance of the outputs (numerator in (11)) The average squared norm (denominator in (11)) The relative variance (ratio of the above quantities) These metrics help us identify which heads are sensitive to changes in each variables value. The results are shown in Figure 19. We deduce from the results (especially the relative variance plots in the right column) that the heads {0, 8, 15} form the first group, which copy the value for var0 ; heads {5, 10} form the second group, which copy the value for var1 ; and heads {2, 3, 4, 7, 9} form the third group, which copy the value for var2 . We denote these three groups as H0, H1, and H2, respectively to simplify the notation. Second Layer Attention Heads Copy the Values of the Variables to the RHS Position. Similar to the experiment in the first layer, we compute the norm amplification coefficients for the OV matrices of the second layer attention heads, combined by groups, as shown in Figure 20. We observe that the norm amplification coefficients for the value factored embedding are significantly larger than that of the other types of embeddings. This can be seen by noticing that the bottom left subfigure has larger magnitude for all colors. This confirms our hypothesis that the OV matrix is responsible for copying the value factored embeddings of the variable to the RHS position. C.3.1. Conclusion For the second layer attention, we have the following mechanistic interpretation: Mechanistic Interpretation of the Second Layer Attention. The second layer attention heads copy the values of the variables var0 , var1 , and var2 to the RHS position. This is achieved by using the OV matrices of the second layer attention heads to write the value factored embeddings of the three variables on the LHS of (8) to the residual stream at the RHS position. C.4. Second Layer MLP: Module Addition in the Frequency Domain To study how the second layer MLP performs the modular addition operation, we first understand the structure of the input to the second layer MLP. 47 C.4.1. Understanding the Input Structure to the Second Layer MLP Having confirmed that the second layer attention successfully copies the values of variables to the RHS position, we now examine in detail how these value embeddings are structured and prepared for the MLPs arithmetic computation. The key insight is that the second layer attention transforms the value embeddings through head-specific OV matrices, creating structured representation that facilitates downstream computation. To understand this transformation, we need to consider three aspects: the original embedding space, the transformation process, and the resulting geometric structure. Original Value Embedding Space. In our factored tokenization scheme, each token is decomposed into four factors: syntax, variable, operation, and value. The value factor encompasses 25 distinct elements: the integers 0 through 22 (used for actual arithmetic computations in modulo-23 arithmetic), plus two special tokens empty (indicating variable whose value has not been computed yet) and N/A (for non-value tokens like operators). Each of these 25 possible values has learned embedding vector embedvalue(v) Rdmodel where {0, 1, . . . , 22, empty, N/A}. These embeddings are learned during training and form the basis for how numerical values are represented throughout the network. Head Group Transformation Process. As we established through our variance analysis, the second layer attention heads organize into three distinct groups based on which variable position they attend to: H0 = {0, 8, 15} attends to var0 , H1 = {5, 10} attends to var1 , and H2 = {2, 3, 4, 7, 9} attends to var2 . As result, when value embedding passes through this transformation, it becomes what we call new value embedding defined in (10), where (Hi) is the OV combined matrix for the head group Hi, {0, 1, 2}. OV Intuitively, new_value0(v) is the contribution of var0 with value to the residual stream at the RHS position after the second layer attention. By enumerating all the possible values for each variable, we have in total 75 distinct new value embeddings: Vectors 1-25: {new_value0(v) : {0, . . . , 22, empty, N/A}} for var0 Vectors 26-50: {new_value1(v) : {0, . . . , 22, empty, N/A}} for var1 Vectors 51-75: {new_value2(v) : {0, . . . , 22, empty, N/A}} for var2 During actual computation, when processing specific probe equation, the second layer attention selects and combines exactly three of these 75 vectors based on the actual values of the variables. For instance, consider the probe equation x91 + x88 + x55 = x30 where x91 has value 7, x88 has value 15, and x55 has value 3. The second layer attention performs the following: (i) Head group H0 attends to the position containing x91 and copies its value embedding, transforming it to produce new_value0(7) (ii) Head group H1 attends to the position containing x88 and produces new_value1(15) (iii) Head group H2 attends to the position containing x55 and produces new_value2(3) The residual stream at the RHS position then contains the vector sum: MLP Input = new_value0(7) + new_value1(15) + new_value2(3) + other residual components (14) This sum of three transformed value embeddings forms the primary input signal that the second MLP must decode to compute (7 + 15 + 3) mod 23 = 2. 48 Geometric Structure Analysis. To understand how these 75 vectors are organized in the embedding space and how this organization facilitates modular arithmetic, we analyze their pairwise relationships through cosine similarity, shown in Figure 21. Figure 21 exhibits two salient structures: Inter-variable near-orthogonality. Cosine similarities between value-factored embeddings belonging to different variables are near zero. Equivalently, entries outside the three 25 25 block-diagonal submatrices are close to zero. Intra-variable circulant structure. Within each variable, the first 23 value-factored embeddings (actual values) form an approximately circulant similarity submatrix: each row is cyclic right shift of the previous row. The last two embeddings, empty and N/A, are nearly orthogonal to those 23 (and to each other), yielding two low-similarity rows/columns appended to the block. Figure 21. Cosine similarity of the new value factored embeddings for all three variables in the residual stream after the second layer attention. Concretely, if we restrict attention to any one of the three blocks, the top-left 23 23 portion displays the circulant pattern. circulant matrix is determined by its first row and is diagonalized by the discrete Fourier basis, fact we will use when interpreting its spectrum. C.4.2. Module Addition in the Frequency Domain. To systematically analyze how the model performs the module addition operation, we prepare equation of the form as in (8), and we change the previous equations to alter the value of each variable var0 , var1 , and var2 . Specifically, we let var0 , var1 , and var2 to iterate over the set {0, 1, 2, . . . , 22} since the model is trained on modular-23 addition. To study how the MLP performs the module addition operation, we pick the following four positions in the model: (i) pre-activation of the second layers MLP, (ii) post-activation of the second layers MLP, (iii) the output of the second layers MLP, and (iv) the models decoder output. For each of these network positions, we take the corresponding vector in the residual stream at the RHS position of the probe equation. To simplify the notation, we denote such vector as v(x, y, z) and let denote its dimension, when the input variables are var0 = x, var1 = y, and var2 = z. We then compute the three-dimensional 23-point Discrete Fourier Transform (DFT) applied independently to each coordinate of over (x, y, z), which is defined as: DFT3(v)j,k,l = 1 233 22 (cid:88) 22 (cid:88) 22 (cid:88) x=0 y= z=0 v(x, y, z) e2πi xj+yk+zl 23 , j, k, = 0, 1, . . . , 22, Here j, k, are the frequencies in the three dimensions, and we apply the DFT to coordinatewisely. Thus, for each (j, k, l), DFT3(v)j,k,l is d-dimensional complex vector, and DFT3(v) is four-dimensional tensor with dimension 233 d. We then compute the norm of the DFT tensor along the last dimension for each (j, k, l), which represents the magnitude of the corresponding frequency component. Since the obtained DFT 49 tensor is conjugate symmetric, we have DFT3(v)j,k,l = DFT3(v)22j,22k,22l, Therefore, we only need to focus on the first half of the tensor, which has dimension 123. Studying the DFT Tensor by Frequency Group. We further partition the tensor into 7 groups by the algebraic patterns of the frequency component (j, k, l): Group 1: (0, 0, 0) Group 2: (0, 0, a), (0, a, 0), (a, 0, 0) for = 0 Group 3: (0, a, b), (a, 0, b), (a, b, 0) for nonzero = Group 4: (0, a, a), (a, 0, a), (a, a, 0) for = 0 Group 5: (a, b, c) for nonzero = b, = c, = Group 6: (a, a, b), (a, b, a), (b, a, a) for nonzero = Group 7: (a, a, a) for = We plot the histograms of the norm of the DFT tensor in the last dimension for each group, as shown in Figure 22. At the pre-activation stage of the second layer MLP, the DFT tensor shows its highest norm for the group (0, 0, 0), which suggests dominant bias term that is independent of the input variables. Progressing from the pre-activation (Figure 22a) to the MLP output (Figure 22c), this bias term gradually diminishes, while the norm corresponding to the group (a, a, a) steadily increases. This trend indicates that the MLP output contains strong frequency component of the form cos (cid:19) (cid:18) 2πax 23 cos (cid:19) (cid:18) 2πay 23 cos (cid:18) 2πaz 23 (cid:19) , (15) or similar combination involving both sine and cosine functions with the same frequency a. In (15), x, y, and denote the value of the three variables in the equation, and is the frequency. The term in (15) corresponds to degree-3 term on frequency a, indicating that the model is capable of computing terms in the form of cos(2πa(x + + z)/23 + φ) for some frequencies and phase φ, and eventually decodes to the correct answer + + mod 23. C.4.3. Conclusion For the second layer MLP, we have the following mechanistic interpretation: Mechanistic Interpretation of the Second Layer MLP. The second layer MLP performs modular addition in the frequency domain. It takes as input the sum of three transformed value embeddings (new_value0, new_value1, new_value2) from the second layer attention and computes their modular sum through sinusoidal basis functions. The MLP progressively amplifies diagonal frequency components of the form (a, a, a) with {1, . . . , 22}, which encode the sum + + mod 23, while suppressing the bias term (0, 0, 0), ultimately enabling the decoder to output the correct result. C.5. Error Analysis To better understand the probed models performance, we analyze its prediction errors. As we have three functional components in the model the first layer attention, the second layer attention, and the last feedforward layer we consider three sources of errors: (i) the first layers attention mapping copies from the wrong variable position, (ii) the second layers attention fails to copy the correct variable value, and (iii) the feedforward layer miscalculates the sum of the LHS variables. An account of the errors by source is shown in Table 2, where the major source of error is the feedforward layer calculation. Note that when considering the three sources of errors, if the error (i) occurs, we do not count towards error (ii) and (iii). Similarly, when error (ii) occurs, we dont count towards error (iii). In the following, we details how we identify the three sources of errors. C.5.1. Identifying Different Sources of Errors When our recurrent transformer model is computing the RHS value for all the equations in the sequence, we have two key concepts: Depth of equation: The depth of an equation is the number of iterations required to compute the correct RHS value. More formally, the depth of an equation is the depth of the RHS variable in the computation graph. Take Figure 2 as an example, the depth of the equation 20 = x7 is 1, as the model only needs single loop to compute the correct RHS value, and the depth of the equation x7 + x42 = x23 is 2, as the model needs two loops to compute the correct RHS value. Number of iterations: The number of iterations describes how many times the loop transformer model has iterated over the input sequence. By definition, the minimum number of iterations needed for computing the correct RHS value of an equation of depth is at least d. In fact, we observe that most of the equations can be computed with exactly the number of iterations equal to the depth. For this reason, we only consider the equations and the number of iterations such that depth of equation number of iterations, or for short, depth iter. (16) Moreover, we do not add any probe equations in this error analysis. This means that we apply the knowledge learned from the previous experiments with probe equations to identify errors happening in the whole sequence. In the following, we details how we identify the three sources of errors. First Layer Attention Error. We identify first layer attention errors by analyzing how well each attention head group focuses on its assigned variable position. For each equations RHS position, we examine the attention map (an tensor, where is the number of attention heads and is the sequence length) to extract the relevant attention probabilities. Table 2. Attribution of errors by source in the testing dataset with = 128 and 23k sentences. Error Source Count First Layer Attention Error Second Layer Copy Error Feedforward Calculation Error Total 51 9 1 40 Consider concrete example: For the head group H0 that is responsible for attending to var0 , we look at the attention probabilities where the query is at the rhs position and the key is at the var0 position, for all heads in H0. We then average these probabilities within the head group. For each equation, we can use the above strategy to obtain single group-wise attention probability for each head group at the rhs position. If this group-wise attention probability is less than our threshold of 0.9, we classify it as first layer attention error, indicating that the head group failed to maintain sufficient focus on its designated variable position. In fact, the error analysis is not very sensitive to the choice of the threshold. As we will see later in Figure 23 (Top Row), the computed group-wise attention probability is either very close to 1 or very close to 0 (for var0 and var2 , where var1 has slightly larger deviation from 1 on the high end). It is very easy to identify when an error occurs in the first layer attention. Second Layer Copy Error. For the second layer attention, we analyze the attention heads output rather than the attention map. This approach is necessary because the value factored embedding from the first layer may be distributed across multiple positions, including special tokens (like delimiters or operators), rather than being confined to the original variable position. Fortunately, we already have the extracted new value(i) factored embeddings for each var in the previous experiment. We thus treat these new value(i) factored embeddings as the ground truth value embeddings for var in the second layer attention output. For each equation containing var , we compute the cosine similarity between the ground truth value embedding for var and the designated head groups output at the rhs position in the second layer attention. We call this cosine similarity the group-wise cosine similarity. If the cosine similarity is less than our pre-determined threshold of 0.9, we consider it second layer copy error for that head group, indicating the model fails to copy the correct variable value to the RHS position. Similar to the first layer attention analysis, the choice of threshold is not critical. As shown in Figure 23 (Middle Row), the cosine similarity between the second layer attention outputs and the target value embeddings exhibits clear pattern: either very close to 1 for correct copies, or significantly lower for incorrect copies. This stark separation makes it straightforward to identify second layer copy errors. Feedforward Calculation Error. The feedforward calculation error is defined in the following way: If an equation passes the first two error checks, meaning that the first layer attention successfully attends to the correct variable position, and the second layer attention successfully copies the correct variable value to the RHS position, but the model still makes mistake when applying the factored decoder after the second layer MLP, we consider it feedforward calculation error. An account of the errors by source is shown in Table 2, where the major source of error is the feedforward layer calculation. Overall, the model demonstrates remarkable accuracy, where the total number of errors is only 40 out of 23k examples. more detailed analysis of the errors is shown in Figure 23. 52 C.5.2. Additional Error Analysis Here, we provide additional evidence for the above discussion. In Figure 23, instead of just counting the number of times specific error occurs, we histogram all the statistics used by the above error analysis procedure. Figure 23 (Top Row) is histoplot of the the group-wise attention probability in the first layer, organized by three head groups H0, H1, and H2, where each Hi is responsible for copying the value of var . See the First Layer Attention Error paragraph above for more details. We see that the attention scores generally concentrate their probability mass around 1 on the correct variable; however, the heads responsible for copying var2 are somewhat less concentrated, resulting in more errors. Moreover, for some examples where the final prediction is incorrect, we observe clear error pattern in the histogram: the attention head group completely fails to attend to the correct variable position, with the group-wise attention probability dropping to nearly 0. This stark contrast between successful and failed attention patterns makes it easy to identify first layer attention errors. In addition, Figure 23 (Middle Row) histogram the cosine similarity between the second layer attention outputs and the target value embedding, again for all three head groups. For most examples, the cosine similarity is close to 1, showing that the second layer retrieves the value embeddings. However, for some examples where the final prediction is incorrect, we also observe clear error pattern in the histogram: the cosine similarity drops to nearly 0. This stark contrast between successful and failed second layer copy patterns makes it easy to identify second layer copy errors as well. Does the Model Perform Self-Correction? The first two rows in Figure 23 are reported only for equations with depth iter. This is because the number of iterations required for computing the correct RHS value of equations is at most its depth. However, if we let the number of iterations go beyond the depth of the equations, as shown in Figure 23 (Bottom Row), the first layer attention heads are not able to concentrate their probability mass on the correct variable. This finding indicates that there is no further computation performed by the model at an equation position after the number of iterations reaches the depth of the equations, hence the model does not perform self-correction. One possible explanation for this to happen is the use of weight-decay in the training process. As the value for the rhs variable is already computed after the number of iterations reaches the depth of the equations, the model can directly pass on the computed value to the next iteration via the residual stream without any further computation. How to let the model perform self-correction? We observe that the model does not perform self-correction because we only train the model on perfect data, where the model has no need to perform any further computation beyond the depth of the equations. In fact, we can let the model perform self-correction by training the model on imperfect data, where the model has to perform some further computation beyond the depth of the equations. This motivates our proposal of Discrete Latent Space Supervision method, which trains the model with corrupted data to teach the model to recover from errors. Consequently, increasing the number of iterations beyond the depth of the input can be useful because it allows the model to correct any errors in previous iterations. (a) Head 4 for equation with 2 variables (b) Head 8 for equation with 2 variables Figure 17. Visualization of the attention maps for the head group that attends to the first variable position in the first layer attention, which includes head 4 and head 8. Each row corresponds to different query position, and each column corresponds to different key position. We only show the rows within the last probe equation, and the columns within the last 50 positions in the sequence. Here, we notice that at the RHS query position (token x127 in the last row), head 4 attends to the [sep] token and attention head 8 attends to the equal sign token 54 Figure 18. Histogram of the L2 relative error between the residual stream before and after the first layer MLP. (a) var0 statistics (b) var1 statistics (c) var2 statistics Figure 19. Attention head statistics for the second layer attention. Each subfigure shows three histograms corresponding to the variance (numerator to (11)), average norm (denominator to (11)), and relative variance for each attention heads outputs. 55 (a) Norm Amplification for OPERATION (b) Norm Amplification for SYNTAX (c) Norm Amplification for VALUE (d) Norm Amplification for VARIABLE Figure 20. Histograms of norm amplification for the four factored embedding types in the second layer attentions OV matrix. The 16 attention heads are grouped by the variable they attend to, which are var0, var1, and var2. In each subfigure, we make three histograms each corresponding to the combined OV matrix for each group of attention heads. The three histograms in each subfigure are shown in different colors, and each histogram is for all the embeddings of the corresponding factored embedding type. It can be observed that the amplification factor for the value factored embedding is significantly larger than that of the other types of embeddings, confirming our hypothesis that the OV matrix is responsible for copying the value factored embeddings of the variable to the RHS position. 56 (a) DFT of L1 MLP pre-activation (b) DFT of L1 MLP post-activation (c) DFT of L1 MLP output (d) DFT of decoder output Figure 22. Combined DFT histograms for the second layer MLP pre-activation, MLP post-activation, MLP output, and decoder output. 57 Figure 23. Error analysis. Top Row. Histograms for the group-wise attention probability in the first layer for all three head groups attending to var0 , var1 , and var2 , respectively. Here, the target equations considered all satisfy depth iter as defined in (16). We use different colors to separate the equations based on whether the decoded RHS value is correct or not after the second layer MLP. Middle Row. Histograms of the group-wise cosine similarity for the second layer attention head groups outputs with the target values embedding. Only equations with depth iter are included. Bottom Row. Histograms of the group-wise attention probability in the first layer for all three head groups. Here, the target equations considered all satisfy depth < iter, meaning that the number of iterations is beyond the depth of the equations."
        }
    ],
    "affiliations": [
        "Department of Statistics & Data Science, Yale University"
    ]
}