{
    "paper_title": "Unified Video Editing with Temporal Reasoner",
    "authors": [
        "Xiangpeng Yang",
        "Ji Xie",
        "Yiyuan Yang",
        "Yan Huang",
        "Min Xu",
        "Qiang Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF."
        },
        {
            "title": "Start",
            "content": "Xiangpeng Yang1 Ji Xie2 Yiyuan Yang1 Yan Huang1 Min Xu1 Qiang Wu1 1University of Technology Sydney 2Zhejiang University https://videocof.github.io/ 5 2 0 D 8 ] . [ 1 9 6 4 7 0 . 2 1 5 2 : r Figure 1. VideoCoFs video editing capabilities emerge from its seeing, reasoning, then editing framework. Trained on only 50k data (33 frames), this teaser shows multi-instance editing and robust 4 length generalization."
        },
        {
            "title": "Abstract",
            "content": "Existing video editing methods face critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF 1 enforces see reason edit procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-toregion alignment and fine-grained video editing. Furthermore, we introduce RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF. 1. Introduction The development of Video Diffusion Models (VDM) [14, 32, 38, 42] has enabled high-fidelity video generation across wide range of concepts. Building on these advances, video editing methods support users in designing video by adding [31], removing [18, 50], swapping [7, 41] visual concepts, and performing global style transformation [44]. Current video editing methods mainly follow two strategies: (i) expert models [1, 18, 31, 41, 47], which use adapter-based modules to feed external masks into the video generation model, yielding precise, localized edits but requiring additional inputs and per-task overhead; and (ii) unified temporal in-context learning models [11, 19, 43], which concatenate source tokens with noised edit tokens along the temporal dimension and use self-attention mechanism to guide the edit. However, without explicit spatial cues, these models often exhibit weak accuracy, especially in cases that need multi-instance recognition or spatial reasoning (Fig. 2, left). In short, there is trade-off : expert models are accurate but mask-dependent, while unified incontext models are mask-free but less precise; This raises critical question: Can we maintain formers precision and latters unification without the mask dependency? Inspired by Chain-of-Thought (CoT) multi-step reasoning [35], we compel the video diffusion model to first predict the edit region and then perform the edit, enforcing see reason edit procedure. Accordingly, we propose VideoCoF, Chain-of-Frames approach that predicts reasoning tokens (edit-region latents) before generating the target video tokens, thereby removing the need for user-provided masks while achieving precise instructionto-region alignment. To explicitly model the reasoning process, we leverage visual grounding, which is naturally suited to simulating reasoning about the edit region. Empirically, we find soft, gradually highlighted grayscale region is the most effective reasoning format. Additionally, we introduce RoPE alignment strategy. By explicitly accountFigure 2. Illustration of the difference between previous methods and our VideoCoF. We enhances the editing accuracy by forcing the video diffusion model to first predict the editing area, and then perform the editing. ing for the reasoning latent, we reset the temporal indices of the edited videos rotary position embeddings to match those of the source video, ensuring motion alignment and length extrapolation. To holistically evaluate fine-grained video editing, we further construct VideoCoF-Bench. VideoCoF trained on only 50k video pairs, outperforms strong baseline ICVE [19] that uses 1M pretraining videos plus 150k for finetuning. Specifically, we improves the instruction-following score by +15.14% and the success ratio by +18.6%. Our contributions can be summarized as follows: We propose VideoCoF, the first framework to introduce Chain of Frames approach to video editing, enabling temporal reasoning for fine-grained video editing. Building on VideoCoF, we explore an effective reasoning format for video diffusion models, and introduce RoPE alignment strategy that allowing generalization to longer frames exceeding the training duration. We demonstrate that with minimal data cost (only 50k video pairs), we achieve state-of-the-art quantitative and qualitative performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. 2. Related Work Video Editing Methods. Early training-free video editing methods [24, 38] used inversion and consistency techniques (e.g., attention manipulation [24] or optical flow [6]) but often lack precise control and struggle with complex edits. Data-driven, training-based methods [2, 5] have become the focus, offering higher quality and edit diversity. concurrent line of research [21, 34, 45] integrates MLLMs to guide the editing process, though this adds significant training and inference cost, which our pure VDM approach avoids. In-Context Video Editing. Recently, in-context learning (ICL) has emerged as promising paradigm for unified editing [12, 40, 49]. Methods like UNIC [43] and ICVE 2 Figure 3. Overview of VideoCoF framework. Our model processes source (blue), reasoning (orange), and target (green) tokens in unified sequence to reason then edit. Bottom right: Our RoPE design enables length extrapolation. [19] concatenate video conditions along the temporal axis to perform ICL. However, these methods are often limited by mask requirements [43] or, as we identify, suffer from fundamental issues with editing accuracy and lack of length extrapolation due to their naive temporal concatenation. While EditVerse [11] also explored unified in-context learning, it was built on LLaMA-style DiT backbone, whereas our work explores these capabilities within standard video diffusion transformer. Chain of Thought in Vision. Chain-of-Thought (CoT) prompting [13, 35] elicits multi-step reasoning in LLMs by having them think step-by-step. This concept of emergent reasoning has also been identified in large video generative models [4, 36] that can solve visual puzzles. However, how to leverage visual reasoning for the task of unified video editing remains unexplored. In this work, we investigate whether generative video models can perform chain of frames reasoning to achieve this. 3. Methods 3.1. VideoCoF Framework As illustrated in Figure 3, VideoCoF employs VideoDiT [32] for unified video editing. We model editing as reasoning-then-generation process: the model first reasons where to edit, then generates the intended content in that area. We call this process Chain of Frames (CoF) (Sec 3.2). All visual inputs (source, reasoning, and target frames) are encoded separately by Video VAE and then concatenated temporally. The unified frame sequence is then fed into the model, performing unified in-context learning via self-attention and language control via cross-attention. To enable video alignment and variable-length inference, we revisit the design of positional encoding. We adapt the temporal RoPE for source-to-target alignment and reasoning tokens RoPE for explicit spatial guidance (Sec 3.3). Subsequent sections detail the training and inference paradigm (Sec 3.4), and the data curation pipeline (Sec 3.5). 3.2. Chain of Frames Seeing, Reasoning, then Editing. Previous video incontext editing methods, such as UNIC [43], ICVE [19], or EditVerse [11], perform in-context learning by temporally concatenating clean source video tokens with noised editing video tokens. However, this approach lacks an explicit constraint mapping the editing instruction to the specific editing region, leading to editing accuracy problems, as shown in Fig 2. Recently, VDM have been shown to possess reasoning capabilities, as demonstrated in [36]. Inspired by this, we explicitly model the reasoning tokens, forcing the model to actively learn the relationship between the editing instruction and the target edit region first. The edit is then executed after reasoning, following seeing, reasoning, then editing process. Inspired by Chain of Thought prompting in Large Language Models (LLMs) [35], we argue that video generative model should also have an analogous chain-reasoning ability. Given the generative priors in video editing, the visual-chain should be progressive, moving from the original video to visual reference of the editing region, and finally to the edited video. Visual grounding is naturally suitable for this representation. Since video diffusion models are often insensitive to grounding masks (black or white pixels). Therefore, we choose to use gray highlight to delineate the grounding region,, which is also evidence in [8]. Finally, the gray-highlighted area as the ground truth 3 applies sequential temporal indices (e.g., 0 to 2F 1) across concatenated source and target videos. However, this hinders video length extrapolation, as the model overfits to static [0, 1] [F, 2F 1] mapping and fails to generalize to videos longer than frames. better strategy is to repeat the temporal indices. For our CoF triplet (consider = 1 for reasoning frame), straightforward reset configuration is to assign temporal indices: [0, 1] to the source, 0 to the reasoning frame, and [0, 1] to the target. However, as illustrated in Figure 4 (a), this naive reset leads to index collisions at temporal position 0, shared by the source, reasoning, and target frames. This overlap introduces visual artifacts that propagate from the reasoning tokens into the first target frame. To resolve this index collision, we set the temporal indices for both the source video and the target video to the range [1, F], while keeping the reasoning frames temporal index at 0. This isolates the reasoning token and prevents artifact leakage while maintaining length generalization. 3.4. Training and Inference Paradigm Algorithm 1: Chain of Frame (CoF) Training Input: Dataset with tuples (zs Output: Fine-tuned parameters θ (0), zr (0), ze (0), c) foreach minibatch (zs (0), ze foreach sample in minibatch do (0); (0), zr (0), c) do (0)zr (0)ze z(0) ull zs Sample U[0, 1]; Sample ε (0, I) with the same shape z(0) (ε z(0) ull); z(t) r,e (1 t)(zr (0)z(t) z(t) zs r,e; ˆv Fθ(z(t), t, c); 1 (0)) + t(εF :2F +L1); (0)ze (cid:13) (cid:13)vi ˆvi (cid:80)2F +L1 i=F (cid:13) (cid:13) L+F 2 2; ull; Update θ using gradients of L;"
        },
        {
            "title": "Given a concatenated full",
            "content": "TemporalConcat (cid:0)z(0) , z(0) ing+editing block as the generation target during training. , z(0) latent sequence z(0) (cid:1), we treat full = the reasonr,e = (1 t)(cid:0)z(0) Given timestep [0, 1] and Gaussian noise ε (0, I), we only progressively noise the reasoning and (cid:1) + εF :2F +L1, editing parts, z(t) z(0) and form the model input z(t) = z(0) z(t) r,e, The target velocity field is = ε z(0) full . Our model Fθ() predicts this velocity field from the partially noised input, and we train it by minimizing the mean squared error between predicted and true velocities. Concretely, we only supervise the reasoning and target frames, so the training loss can be written Figure 4. How our RoPE design avoid index collision. for the reasoning frames, teaching the diffusion model to reason about where the edit should occur. Consequently, the entire video editing task is reformulated as chained process: first seeing the original video, then reasoning by predicting the grounding region, and finally editing to generate the new video content within that specified area. We call this Chain of Frames (CoF). Let E() denote the video VAE encoder. We use and for frames in the source/target and reasoning latent space, respectively, and denote channel, height, and width by C, H, and . Given triplet source-reasoning-target video pair {s, r, e}, we first encode them into latent representations. The source and target video yield latent zs = E(s) and ze = E(e), both with shape RF CHW . The reasoning video yields latent zr = E(r) with shape RLCHW . This separate encoding ensures intra-causal relations and inter-video independence. Then, we perform temporal concatenation to get the unified representation: z(t) ull = z(0) (cid:124)(cid:123)(cid:122)(cid:125) seeing z(t) (cid:124)(cid:123)(cid:122)(cid:125) reasoning z(t) (cid:124)(cid:123)(cid:122)(cid:125) editing R(F +L+F )CHW , (1) where the zs = z(0) 0:F 1 denotes anchoring the source video latent at and timestep 0. ze = z(t) +L:2F +L1 mean the reasoning and target noised video latents at timestep t. At each denoising step, only the + reasoning and target frames are denoised, and the source video latents are kept clean. zr = z(t) :F +L 3.3. RoPE Design for Length Extrapolation In VideoDiT, 3D factorized RoPE [28] provides spatiotemporal positions. naive in-context learning approach 4 Figure 5. Our data curation pipeline for multi-instance data. in per-frame form as ="
        },
        {
            "title": "1\nL + F",
            "content": "2F +L1 (cid:88) i=F (cid:13) (cid:13)vi (cid:2)Fθ(z(t), t, c)(cid:3) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 , (2) where (cid:2)Fθ(z(t), t, c)(cid:3) denotes the models prediction for frame and is the text condition. The model parameters Fθ() are updated via gradient step computed from this loss. The full training procedure is summarized in Algorithm 1. , z(1) r,e During inference we initialize the reasoning+editing block from Gaussian noise, z(1) r,e (0, I) and form the full latent at = 1 by temporal concatenation with the (cid:1). An full = TemporalConcat (cid:0)z(0) clean source z(1) ODE solver guided by our model Fθ evolves z(t) full to z(0) full . The source latents z(0) are held fixed during inference, so only the reasoning/editing parts change. We then extract the edited-target latent using the same slicing index as in training: z(0) +L:2F +L1 and decode the final edited video: xedit = D(cid:0)z(0) 3.5. Video Data Curation edit = (cid:0)z(0) (cid:1). edit full (cid:1) The training of our VideoCoF requires large and diverse dataset structured as source, reasoning, and edited video triplets. However, existing video editing datasets and methods predominantly focus on single-instance-level object manipulation. This limitation is significant barrier, as real-world videos contain complex visual cues, multiple interacting instances, and intricate spatial relationships (e.g., physical left/right, object-to-object interactions). Enabling generative model to comprehend these complex, instancelevel dynamics is critical step toward true reasoning-based video editing. Therefore, we develop comprehensive data curation pipeline, illustrated in Figure 5, to specifically generate and process complex, instance-level video data. Instance-Level Curation Pipeline. Our pipeline begins with large pool of diverse videos sourced from Pexels 5 [23]. First, we employ the Qwen-VL 72B [33] to perform multi-instance identification, scanning the videos to find scenes that contain multiple, distinct objects. Once these videos are identified, we use Grounding-SAM2 [26] to perform precise segmentation, generating distinct segmentation masks for each individual instance. With these instance-specific masks, we generate triplets for variety of editing tasks: Object Addition/Removal: We utilize the Minimaxremover [50] to erase specific instance from the video. The data for object addition is then created by simply reversing this process. Object Swap and Local Style Transfer: For these tasks, we leverage the VACE-14B [10] in its inpainting mode to fill the specified masked regions. Critically, the creative prompts for these inpainting edits are generated by GPT4o[22], as we found Qwen-VL 72Bs imaginative capabilities for this specific task to be limited. Filtering and Final Dataset. All generated video pairs are rigorously evaluated to ensure quality. We use the Dover Score [37] to assess aesthetic quality and the VIE Score [15] to measure editing fidelity and coherence. weighted combination of these scores is used to filter for high-quality, successful edits. Finally, we use this pipeline to filter from the large-scale open-source Senorita 2M [51] dataset, and distill high-quality subset of 50k videos to supplement our training data. This multi-pronged approach yields our final large-scale dataset, rich in the instance-level complexity required for reasoning-based video editing. 4. Experiments 4.1. Implementation Details. VideoCoF is trained on WAN-14B [32]. We employ resolution-bucketing strategy to support multiple aspect ratios, using spatial resolutions of 336592, 400704, 400752, and 400944 (and the corresponding vertical variants, e.g., 592336). Training videos are sourced from Table 1. We compare VideoCoF with SOTA baselines on VideoCoF-Bench: InsV2V [5], Senorita [51] (an I2V model guided by InsP2P [2]), VACE-14B [10] (using GPT-4ogenerated captions), the concurrent ICVE [19] (pretrained on 1M videos and fine-tuned on 150k), and LucyEdit [30]. Despite the extensive training data used by baselines, VideoCoF is fine-tuned on only 50k video pairs and achieves superior instruction-following and success ratio."
        },
        {
            "title": "Model",
            "content": "Instruct Follow GPT-4o Score (avg.) Preservation Quality Success Ratio CLIP-T CLIP-F DINO Perceptual Quality (avg.) InsV2V [5] Senorita [51] VACE [10] ICVE [19] Lucy Edit [30] VideoCoF (Ours) 3.41 3.26 7.47 7.79 5.24 8. 6.15 6.30 5.82 8.06 6.50 8.20 5.51 5.48 7.61 8.14 6.37 7.77 6.39% 10.35% 26.60% 57.76% 29.64% 76.36% 26.19 26.04 27.02 27.49 26.98 28.00 0.988 0.994 0.994 0.992 0.991 0.992 0.978 0.988 0.990 0.986 0.986 0. Figure 6. Visual comparision between our VideoCoF and other methods on diverse video editing tasks. Senorita [51] and are 33 frames long, we only training on 50k curated video data finally. Thanks to our RoPE alignment design, the model generalizes to longer sequences at inference (e.g., 141 frames and above). By default we use 33 frames source video, 33 frames edited video, and 4 frames reasoning clip. We train with global batch size of 16 for approximately 8k iterations, optimizing with AdamW [20] and base learning rate of 1 104. 4.2. VideoCoF-Bench and Experimental Setting VideoCoF-Bench. Previous video-editing benchmarks such as V2VBench [29], TGVE [39], and FIVE-Bench [17] focus on target-prompt edits and mostly are focused on class-level object swap. They were mainly designed for training-free methods and are not suitable for instructionguided or instance-level video editing. Real-world editing requires precise instruction understanding, including instanceand part-level control (e.g., distinguishing multiple people or left vs. right), and complex reasoning. To address these gaps, we introduce VideoCoF-Bench. It contains 200 high-quality videos collected from Pexels [23], covering diverse scenes and both landscape and portrait aspect ratios. VideoCoF-Bench includes four task: Object Removal, Object Addition, Object Swap, and Local Style Transfer, each with 50 samples. Half of these samples per task are instance-level cases with instance-focused editing prompts. Evaluation Metrics. To evaluate editing performance on 6 Table 2. Ablation on Chain of frames and RoPE design. Ablation on Chain of frames and RoPE design Naive Temporal in Context 02F-1 0F-1, 0FVideoCoF 1F, 0, 1F CoF RoPE Design GPT-4o Score Instruct Follow Preservation Quality Success Ratio* 8.109 7.930 7.394 72.41% Perceptual Quality CLIP-T CLIP-F DINO 26.880 0.9907 0. 8.064 7.793 7.217 65.52% 27.088 0.9905 0.9826 8.973 8.203 7.765 76.36% 28.000 0.9915 0.9913 VideoCoF-Bench, we employ MLLM-as-a-Judge to provide holistic evaluation score. This is achieved by prompting GPT-4o [22] to assess multiple criteria given the original video, edited video, and user instruction: (1) Instruction Following (editing accuracy), (2) Preservation (unedited regions), (3) Video Quality. (4) Success ratio: we prompt the GPT-4o to provide binary Success Ratio (Yes/No) to judge the overall success of the edit. We report three perceptual quality metrics quantify lowand high-level visual similarity between source and target frames: CLIP-T for imagetext alignment, CLIP-F for temporal consistency, and DINO for structural consistency. 4.3. Comparison on VideoCoF-Bench We show qualitative and quantitative comparisons of VideoCoF-Bench in this section. As shown in Table 1, we evaluate VideoCoF against five baseline methods on the VideoCoF-Bench benchmark, which spans four distinct video editing tasks: multi-instance removal, object addition, multi-instance swap, and multi-instance local style transfer. Overall, VideoCoF demonstrates the best performance in Instruct Follow and Success Ratio across all categories. Compared to naive temporal in-context editing approaches like ICVE [19], our method achieves significantly higher success rates and better instruction adherence using only 50k reasoning pairs, whereas ICVE is pre-trained on 1M samples and fine-tuned on 150k data. Qualitatively (see Figure 6), our method also shows clearer, more faithful edits at the instance level:(a) Multiinstance removal: we precisely remove the right instance while ICVE[19] incorrectly removes the left instance. (b) the added girl is correctly placed inside Object addition: the washing machine, matching the instruction. (c) Object swap: we replace the elderly persons face and update clothing; Lucy Edit [30] changes only clothing, ICVE fails to disambiguate instances, and VACE often alters non-target people. (d) Local style (multi-instance): our model correctly identifies and edits the largest cup among several similar ob7 Figure 7. Length exploration on frames more than training. jects; other methods either fail to edit or mistakenly edit bowl. These qualitative examples demonstrate VideoCoFs stronger instance-level reasoning and higher editing fidelity. 4.4. Ablation Study To verify our novel Chain of Frames (CoF) design, particularly its reasoning frames and the RoPE design for length exploration, we conduct an ablation study on the reasoning frames, RoPE alignment strategy and reasoning format. Naive Temporal Incontext VS. CoF. As shown in Table 2, we compare VideoCoF against Naive Temporal incontext baseline. This applies temporal in-context learning by using the source video as condition through temporal concatenation, an approach similar to ICVE [19]. In contrast, our approach introduces reasoning frames as core component of the (CoF) design. This ensures the video editing follows reasoning process, i.e., forcing the model to predict the editing region first and then execute the versatile edit within that specific area. The efficacy of this design is evident when comparing the first ([0, 2F 1]) and third (VideoCoF) columns in Table 2. The inclusion of CoF brings substantial gains: the instruct follow score increases by 10.65% and the success ratio improves by 5.46%. Furthermore, the 4.16% increase in CLIP-T confirms that our reasoning frames effectively enhance the models editing accuracy and precision. Rope Design for length Extrapolation. As illustrated in Fig 7, the naive approach ([0, 2F 1]) only learns fixed temporal mapping (e.g., mapping frame 0th to frame 33th). This prevents length extrapolation, causing severe degradation (blurriness, motion misalignment, and artifacts) when 33-frame trained model is tested on 81 frames (second row). In contrast, our RoPE alignment design ([1F, 0, 1F ]) generalizes to unseen lengths without quality degradation (third row). As demonstrated in Fig 1, our model extrapolates to 141 frames (4x training length) and beyond, supporting theoretically infinite extrapolation. This effectiveness is also quantified in Table 2 (third vs. first column). We observe 3.4% relative increase in the preservation score. Furthermore, the improved DINO score confirms that our RoPE design better preserves the original videos spatio-temporal structure during editing. Table 3. Ablation on the reasoning frame format. Color Transparency Ablation on Reasoning Frame Format Gray (50%) Black (bg) (0%) Red (50%) Gray (0-75%) GPT-4o Score Instruct Follow Preservation Quality Success Ratio* Perceptual Quality CLIP-T CLIP-F DINO 7.512 7.034 6.155 52.17% 7.805 7.350 6.501 8.150 8.973 7.443 8.203 6.645 7.765 60.33% 68.45% 76.36% 26.550 26.810 27. 0.9810 0.9750 0.9855 0.9790 0.9889 0.9803 28.000 0.9915 0.9913 contrast, our progressive gray mask accurately performs the intended deletion on the left. We conclude from these experiments that the optimal reasoning format is gray mask with progressive transparency. Figure 9. Ablation on reasoning frame format. 5. Conclusion In this paper, we introduced VideoCoF, unified model for universal video editing via temporal reasoning. We identified that existing temporal in-context learning approaches often fail due to lack of explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To address these issues, we proposed the innovative Chain of Frames. VideoCoF compels the video diffusion model to follow see, reason, then edit process by first predicting the editing region before executing the versatile edit. Furthermore, to solve the length generalization challenge, we developed novel RoPE alignment paradigm that accounts for the reasoning latent. This design enables 4 times exploration in the inference. Experimental results show that VideoCoF achieves SOTA performance using mere 50k video pairs, validating the efficiency and effectiveness of our temporal reasoning design. 8 Figure 8. Motion alignment benefit by our rope design. RoPE Design for Motion Alignment. Setting the temporal index for the reasoning frame latent is critical design choice. naive approach is to set its index to 0, aligning it with the first video frame. This causes two severe issues. First, it leads to significant motion misalignment (e.g., the subject fails to perform the lifting clothes motion in Fig 8, second row). Second, this 0-index design causes interference with the first editing video frame (also index 0), leading to artifacts where the model incorrectly predicts the first frame as the reasoning frame (Fig 4). Therefore, we fix the reasoning latents index to 0, while the source and edited video indices range from 1 to (denoted as [1 F, 0, 1 ]). This strategy allows the reasoning frame to provide clear spatial guidance on where to edit, without disrupting the videos temporal structure and motion alignment. The improvements across all metrics in Tab 2 (column 3 vs. column 2) validate this design. Reasoning Frame Format. First, we explore the most suitable color for the reasoning frame mask. As shown in Table 3, we compare three formats: (1) black mask over the unedit region; (2) red, 50% transparent highlight, same as veggie [45]; and (3) gray, 50% transparent mask. The quantitative results show that using gray mask (column 3) for the edit region yields the best performance. Furthermore, we argue that the reasoning frame should act as gradual transition from the source video to the edited video. Therefore, we test progressive gray mask. Instead of single static mask, we interpolate gray mask reasoning frame and editing frame, with transparency is progressively increased (e.g., 0%, 25%, 50%, 75%). As shown by comparing column 4 and column 3 in Table 3, this progressive gray reasoning frame approach works best. Qualitatively, as shown in Figure 9, the mask format is critical. The black mask fails the deletion task, while the red mask incorrectly deletes content on the right side. In"
        },
        {
            "title": "References",
            "content": "[1] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Anylength video inpainting and editing with plug-and-play context control. arXiv preprint arXiv:2503.05639, 2025. 2 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 2, 6, 4 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 3 [4] Lan Chen, Yuchao Gu, and Qi Mao. Univid: Unifying vision tasks with pre-trained video generation models. arXiv preprint arXiv:2509.21760, 2025. 3 [5] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent videoarXiv preprint to-video transfer using synthetic dataset. arXiv:2311.00213, 2023. 2, 6, 4 [6] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: Optical flowguided attention for consistent text-to-video editing. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. [7] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7621 7630, 2024. 2 [8] Nicholas Guttenberg. Diffusion with offset noise. https: //www.crosslabs.org/blog/diffusion-withoffset-noise, 2023. 3 [9] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arxiv:2410.23775, 2024. 1 [10] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 5, 6, 2, 4 [11] Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al. Editverse: Unifying image and video arXiv editing and generation with in-context preprint arXiv:2509.20360, 2025. 2, 3 learning. [12] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907, 2025. 2 [13] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. 3 [14] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [15] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation, 2023. 5 [16] Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, and Aleksandr Gordeev. Nohumansrequired: Autonomous high-quality image editing triplet mining. arXiv preprint arXiv:2507.14119, 2025. 3 [17] Minghan Li, Chenxi Xie, Yichen Wu, Lei Zhang, and Mengyu Wang. Five: fine-grained video editing benchmark for evaluating emerging diffusion and rectified flow models. arXiv preprint arXiv:2503.13684, 2025. 6 [18] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting. arXiv preprint arXiv:2501.10018, 2025. [19] Xinyao Liao, Xianfang Zeng, Ziye Song, Zhoujie Fu, Gang In-context learning with unpaired arXiv preprint Yu, and Guosheng Lin. clips for instruction-based video editing. arXiv:2510.14648, 2025. 2, 3, 6, 7, 1, 4 [20] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5 (5):5, 2017. 6 [21] Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, and Qian He. Instructx: Towards unified visual editing with mllm guidance. arXiv preprint arXiv:2510.08485, 2025. 2, 3 [22] OpenAI. Hello gpt-4o. Blog post, 2024. 5, 7, 2 [23] Pexels. Pexels: Free stock photos, royalty free stock images & videos. https://www.pexels.com/, 2025. Accessed: 2025-11-06. 5, 6, 2 [24] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. [25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 3 [26] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 5 [27] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-scale text-to-image model with inpainting is zero-shot subject-driven image generator. 2024. 1 [28] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with 9 Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 2 [41] Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang. Videograin: Modulating space-time attention for multigrained video editing. In The Thirteenth International Conference on Learning Representations, 2025. [42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [43] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 2, 3 [44] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video In Proceedings of with artistic generation and translation. the Computer Vision and Pattern Recognition Conference, pages 26302640, 2025. 2 [45] Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, and Mohit Bansal. Veggie: Instructional editing and reasoning video concepts with grounded generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15147 15158, 2025. 2, 8 [46] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 3 [47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [48] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 1 [49] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 2 [50] Bojia Zi, Weixuan Peng, Xianbiao Qi, Jianan Wang, Shihao Zhao, Rong Xiao, and Kam-Fai Wong. Minimax-remover: Taming bad noise helps video object removal. arXiv preprint arXiv:2505.24873, 2025. 2, 5 [51] Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Se norita-2m: high-quality instructionbased dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025. 5, 6, 4 rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [29] Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, and Dacheng Tao. Diffusion model-based video editing: survey. arXiv preprint arXiv:2407.07111, 2024. [30] DecartAI Team. Lucy edit: Open-weight text-guided video editing. 2025. 6, 7, 4 [31] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video obIn Proceedings ject insertion with precise motion control. of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 2 [32] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 5 [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5 [34] Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, and Wenhu Chen. Univideo: Unified understanding, generation, and editing for videos. arXiv preprint arXiv:2510.08377, 2025. [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2, 3 [36] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 3 [37] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023. 5 [38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 76237633, 2023. 2 [39] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video editing competition. arXiv preprint arXiv:2310.16003, 2023. 6 [40] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun"
        },
        {
            "title": "Supplementary Material",
            "content": "This document provides more details of our approach and additional experimental results, which are organized as follows: Full Comparison (6) More Ablation studies (7) Implementation Details (8) Metrics (9) Discussion (10) 6. Full Comparison As shown in Tab. 7, we provide detailed breakdown of the results across four distinct tasks: Object Removal, Object Addition, Object Swap, and Local Style Transfer. Our VideoCoF consistently achieves the highest scores in instruction following and success ratio across all tasks, demonstrating superior capability in understanding and executing editing requests. We note that our scores in Preservation and Quality are slightly lower than the concurrent work ICVE [19]. This performance gap is reasonable given that ICVE benefits from large-scale pre-training on 1M pairs, and its supervised fine-tuning (SFT) dataset scale (150K) is three times larger than ours (50k). Furthermore, in terms of perceptual quality, VideoCoF achieves the highest CLIPT scores across all tasks. This further demonstrates superior video-text alignment, consistent with our leading performance in GPT-4o score. 7. More Ablation Studies In this section, we validate key design choices of VideoCoF: the length of reasoning frames and the dispatch prompt. 7.1. Ablation on Reasoning Frames Tab. 4 investigates the optimal number of reasoning frames (F ) for spatial guidance. Considering the VideoVAE temporal compression formula = (F 1)//4 + 1, frames 1 4 map to single latent frame (L = 1), while = 5 introduces latent frames = 2. Results show that = 4 achieves the best performance. This indicates maximizing spatial information within single latent frame is more effective than expanding to second latent, which introduces unnecessary temporal complexity and degradation. 7.2. Ablation on Temporal Triptych Prompt To adapt standard T2V model for instruction-based editing tasks, we draw inspiration from in-context image editing approaches [9, 27, 48]. Specifically, we implement temporal triptych prompt mechanism in VideoCoF to Table 4. Ablation on the number of Reasoning Frames. We investigate the impact of varying the number of reasoning frames from 1 to 5. Our default setting (4 frames) achieves the best balance. Frames Ablation on Reasoning Frames 3 4 (Ours) 2 5 GPT-4o Score Instruct Follow Preservation Quality Success Ratio Perceptual Quality CLIP-T CLIP-F DINO 7.915 8.281 8.219 6.542 8.191 8.115 7.692 5.274 7.735 68.47% 69.39% 68.32% 76.36% 29.06% 8.312 8.150 7.752 8.973 8.203 7.765 27.092 27.148 27.136 28.000 26.997 0.9892 0.9893 0.9899 0.9915 0.9849 0.9815 0.9827 0.9836 0.9913 0.9719 Figure 10. Input Prompt Variants for In-Context Video Editing. We evaluate two prompt formats: (a) Temporal Triptych Prompt - instructions embedded in structure video sequence showing three parts: first the original scene, then grounded {ground instruction}, and finally the same scene but {edit instruction}.) (b) Direct Instruction - explicit editing commands provided directly; describe the evolution of video content along the temporal dimension. As illustrated in Fig. 10, our prompt template is structured as follows: video sequence showing three parts: first the original scene, then grounded {ground instruction}, and finally the same scene but {edit instruction}. As evidenced in Tab. 5, this mechanism brings significant performance gains across all metrics. Crucially, unlike the concurrent work ICVE [19], which requires computationally expensive pre-training on 1M video pairs to align 1 Table 5. Ablation on Temporal Triptych Prompt. We compare the performance of our model with and without the triptych patch prompt mechanism. The inclusion of the triptych prompt significantly enhances instruction following and overall success rates. Ablation on Temporal Triptych Prompt w/o Triptych w/ Triptych (Ours) GPT-4o Score Instruct Follow Preservation Quality Success Ratio Perceptual Quality CLIP-T CLIP-F DINO 8.064 8.094 7.360 71.43% 27.07 0.989 0.980 8.973 8.203 7.765 76.36% 28.00 0.992 0.991 Table 6. Statistics of the VideoCoF Training Data. The dataset consists of 50k samples balanced across four tasks. Dataset #Samples Information in Table 6, the dataset is strategically balanced across four core editing tasks: object addition, removal, swapping, and local stylization.The data construction pipeline integrates both filtered open-source data and high-quality synthetic data:Object Addition and Removal: These subsets (25k samples total) are derived from the Senorita dataset. We employ MiniMax-Remover [50] to synthesize paired data. Specifically, for the removal task (15k), we treat the original video as the source and the object-erased version as the target. Conversely, for the addition task (10k), we invert this pair (absent present). Notably, the removal subset includes 5,000 samples featuring multi-instance objects to enhance model robustness in complex scenes. Object Swap and Local Style: To capture fine-grained structural and textural changes, we generated 25,000 samples (15k for swap, 10k for style) utilizing VACE-14B [10]. The generation process is guided by GPT-4o for diverse prompt synthesis and Grounding DINO for precise mask extraction. The swap subset encompasses both rigid and non-rigid object replacements, while the local style subset focuses on texture modification and artistic stylization. Video Editing Tasks 8.2. VideoCoF-Bench Obj. Addition 10,000 Obj. Removal 15,000 Obj. Swap 15000 Local Style 10000 Derived from filtered Senorita. Source generated by removing objects from target via MiniMaxRemover [50]. (absent present). Derived from filtered Senorita. Target generated via MiniMaxIncludes 5k multiRemover [50]. (present abinstance samples. sent). Generated via VACE-14B [10] using GPT-4o prompts and Grounding DINO masks. Covers rigid & nonrigid swaps and 5k multi-instance object swap samples. Generated via VACE-14B [10] using GPT-4o prompts and Grounding DINO masks. Focuses on texture & stylization. Total 50,000 Unified Dataset the T2V model with an instruction mode, our temporal triptych prompt approach offers practically zero-cost solution to effectively bridge the gap between generation and editing without the need for massive instruction tuning. 8. Implementation Details 8.1. Training Dataset To equip our model with robust instruction-following capabilities, we constructed unified chain-of-frames video editing dataset comprising 50k video pairs. As detailed Benchmark Construction. To strictly evaluate the generalization capability, we introduce VideoCoF-Bench, diverse evaluation set specifically curated to have no overlap with the training domain. The benchmark is constructed from three distinct sources to ensure comprehensive coverage: Pexels [23] Subset: We manually curated collection of high-quality videos from Pexels, comprising 50 samples for each editing tasks. These samples are balanced across the four core editing tasks (Addition, Removal, Swap, and Local Style) to test resolution adaptability and instruction following in varied scenes. Standard Benchmark Integration: To ensure fair comparison with existing methods, we incorporated representative samples from established benchmarks, including EditVerse [11] and UNIC-Bench [43]. Adaptation for Fairness: Notably, for samples sourced from UNIC-Bench (which typically involves ID-driven editing), we removed the reference identity images. This adaptation unifies the evaluation protocol, focusing purely on text-driven editing capabilities. This combination results in highly diverse benchmark that challenges models with unseen content and complex editing instructions. 9. Metrics GPT Evaluation. To comprehensively assess the editing performance, we employ the state-of-the-art VisionLanguage Model, GPT-4o [22], serving as an automated judge. Following the protocol of InstructX [21], we sample three frames from each video pair and utilize structured 2 and success rate using only 50k source-reasoning-editing pairs. This demonstrates remarkable data efficiency compared to existing large-scale baselines. For instance, EditVerse [11] utilizes 4M videos and 8M images, ICVE [19] leverages 2M pre-training data with 150k SFT samples, and InstructX [21] employs 200k SFT samples with joint training. Despite the significant gap in data scale, our methods superior performance suggests that the reasoning-then-editing paradigm is highly effective for Video Diffusion Models (VDMs). promising future direction is to explore the performance ceiling of VideoCoF by scaling the dataset to 200k or even millions of samples. Investigating how the reasoning capabilities evolve with larger-scale data could reveal new upper limits for precise video editing. Joint Image-Video Editing and Efficient Architectures. While our current work focuses on video data, integrating high-quality image editing datasets (e.g., MagicBrush [46], NHR-Edit[16]) presents valuable opportunity. Many recent studies have shown that joint training can enhance visual quality and concept understanding. Future work could investigate the optimal mixture ratios between image and video datasets to maximize performance. Furthermore, designing unified and efficient attention mechanisms is crucial for handling the varying temporal dimensions of images and videos within single model. Such advancements would likely improve the models cross-modal learning capabilities, allowing it to transfer fine-grained editing skills from images to complex video dynamics. Generalizing VideoCoF to Broader Tasks. VideoCoF has demonstrated exceptional performance in local editing tasks. However, the underlying reasoning framework is inherently flexible and can be extended to wider range of applications. For Global Editing (e.g., style transfer), the reasoning frame could employ full-frame gray mask to guide global transformations. For ID-Driven Editing, reference identity images could be integrated as reasoning frames to guide specific character insertions or swaps. Unifying these diverse tasksranging from local modifications to global stylization and ID injectionunder the VideoCoF paradigm represents an exciting avenue for future exploration. prompts in [21] to evaluate the results across the following dimensions: Instruction Following (Score 1-10): This metric measures the precision with which the edit adheres to the users specific command. Higher scores indicate that the editing result strictly follows the prompt instructions without ambiguity. Visual Quality (Score 1-10): This evaluates whether the edited video is visually seamless, natural-looking, and aesthetically pleasing. It penalizes artifacts, distortions, or unnatural transitions introduced during the editing process. Preservation (Score 1-10): This assesses the coherence with the original video context. It strictly penalizes unintended changes to non-edited regions, ensuring the background and non-target objects remain intact. Success Rate (Binary Yes/No): To mitigate scoring variance, we incorporate stricter discrete metric inspired [19]. GPT-4o performs binary judgment based (1) Target on rigorous three-step verification logic: Identification (confirming the target matches the descriptor/position); (2) Modification Accuracy (verifying the specific edit is applied); and (3) Strict Preservation (ensuring no other instances are altered). As presented in Table 7, VideoCoF achieves superior performance across all these metrics, validating the effectiveness of our reasoning-driven approach. Perception Quality. In addition to semantic evaluation, we report quantitative metrics to measure the visual alignment and temporal consistency: CLIP-T (Text-Image Alignment): This metric assesses the semantic alignment between the editing instruction and the output video. We compute the cosine similarity between the CLIP [25] text embedding of the instruction and the CLIP vision embedding of each output frame, reporting the average score across all frames. CLIP-F (Frame-wise Consistency): To evaluate temporal stability, we utilize the ViT-L/14 vision encoder from CLIP to extract features for each frame. The consistency score is calculated as the average cosine similarity between feature vectors of adjacent frames. DINO (Structure Consistency): While CLIP focuses on semantics, we aim to capture more fine-grained structural and textural consistency. We repeat the temporal consistency calculation using features extracted from pretrained DINOv2 [3] model. DINOs self-supervised training enables it to capture object-level details that might be overlooked by CLIP. 10. Discussion Scaling up Chain-of-Frames. Currently, VideoCoF achieves SOTA performance in instruction following 3 Table 7. Quantitative full comparison over 4 video editing tasks on VideoCoF-Bench. We compare VideoCoF with SOTA baselines: InsV2V [5]; Senorita [51] (an I2V model guided by an InstructPix2Pix [2] first frame); VACE-14B [10] (using GPT-4o generated captions); the concurrent work ICVE [19] (pre-trained 1M, fine-tuned 150k); and Lucy Edit Dev [30]. Despite extensive baseline training data, our VideoCoF is fine-tuned on only 50k source-reasoning-editing triplets and shows superior instruction following and success ratio."
        },
        {
            "title": "Model",
            "content": "Instruct Follow GPT-4o Score Preservation Quality Success Ratio CLIP-T CLIP-F DINO"
        },
        {
            "title": "Perceptual Quality",
            "content": "InsV2V [5] Senorita [51] VACE [10] ICVE [19] Lucy Edit [30] VideoCoF (Ours) InsV2V [5] Senorita [51] VACE [10] ICVE [19] Lucy Edit [30] VideoCoF (Ours) InsV2V [5] Senorita [51] VACE [10] ICVE [19] Lucy Edit [30] VideoCoF (Ours) InsV2V [5] Senorita [51] VACE [10] ICVE [19] Lucy Edit [30] VideoCoF (Ours) 3.11 3.11 N/A 5.38 2.06 9.65 2.71 2.63 7.12 8.95 6.96 9. 1.52 1.69 8.11 9.08 6.81 9.10 6.29 5.60 7.18 7.75 5.12 8."
        },
        {
            "title": "Object Removal",
            "content": "3.77 4.38 N/A 7.68 4.45 6."
        },
        {
            "title": "Object Addition",
            "content": "4.84 4.80 7.38 8.33 6.78 8."
        },
        {
            "title": "Object Swap",
            "content": "6.54 6.40 7.79 8.57 7.50 8.14 4.02 4.68 N/A 7.30 4.09 7.35 5.31 5.43 5.40 8.65 7.29 8.78 7.37 7.39 6.53 8.40 7.58 8."
        },
        {
            "title": "Local Style Transfer",
            "content": "7.89 7.69 5.53 7.89 7.05 8.29 6.89 6.33 7.65 7.98 6.73 7.71 3.92% 9.80% 0.00% 25.49% 1.96% 86.27% 2.04% 6.12% 30.61% 77.55% 44.90% 79.59% 0.00% 0.00% 34.62% 73.08% 44.23% 80.77% 19.61% 25.49% 41.18% 54.90% 27.45% 58.82% 26.85 26.96 25.57 26.64 27.37 27.50 25.50 25.26 28.01 29.13 27.39 29.60 26.22 25.97 26.93 26.54 26.46 27.10 26.19 25.97 27.56 27.64 26.71 27.80 0.984 0.995 0.996 0.994 0.992 0.988 0.985 0.990 0.990 0.987 0.987 0. 0.991 0.994 0.995 0.993 0.992 0.993 0.992 0.995 0.996 0.994 0.993 0.997 0.973 0.990 0.995 0.989 0.988 0.996 0.966 0.981 0.980 0.974 0.978 0.982 0.984 0.990 0.992 0.989 0.988 0.996 0.987 0.992 0.994 0.991 0.992 0."
        }
    ],
    "affiliations": [
        "University of Technology Sydney",
        "Zhejiang University"
    ]
}