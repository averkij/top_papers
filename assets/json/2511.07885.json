{
    "paper_title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
    "authors": [
        "Jon Saad-Falcon",
        "Avanika Narayan",
        "Hakki Orhun Akengin",
        "J. Wes Griffin",
        "Herumb Shandilya",
        "Adrian Gamarra Lafuente",
        "Medhya Goel",
        "Rebecca Joseph",
        "Shlok Natarajan",
        "Etash Kumar Guha",
        "Shang Zhu",
        "Ben Athiwaratkun",
        "John Hennessy",
        "Azalia Mirhoseini",
        "Christopher Ré"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 2 5 8 8 7 0 . 1 1 5 2 : r INTELLIGENCE PER WATT: MEASURING INTELLIGENCE EFFICIENCY OF LOCAL AI Jon Saad-Falcon * 1 Avanika Narayan * 1 Hakki Orhun Akengin 1 J. Wes Griffin 1 Herumb Shandilya 1 Adrian Gamarra Lafuente 1 Medhya Goel 1 Rebecca Joseph 1 Shlok Natarajan 1 Etash Kumar Guha 1 Shang Zhu 2 Ben Athiwaratkun 2 John Hennessy 1 Azalia Mirhoseini 1 Christopher Re 1 ABSTRACT Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances create an opportunity to rethink this paradigm: small, local LMs ( 20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) can host these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring both whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as unified metric for assessing both the capability and efficiency of local inference across model-accelerator configurations. We conduct large-scale empirical study across 20+ state-of-the-art local LMs, 8 hardware accelerators (local and cloud), and representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy (local LM win rate against frontier models), energy consumption, latency, and power. Our analysis reveals three key findings. First, that local LMs can successfully answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, longitudinal analysis from 2023-2025 shows progress in local inference viability: IPW improved 5.3, driven by both algorithmic advances and accelerator improvements, with locally-serviceable query coverage increasing from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4 lower IPW than cloud accelerators running identical models, revealing significant headroom for local accelerator optimization These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure for substantial subset of queries, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness to enable intelligence-per-watt benchmarking as local LMs and accelerators evolve."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language model (LLM) queries are predominantly processed by frontier models deployed in centralized cloud infrastructure (OpenAI, 2025; Alvarez & Marsal, 2025). This centralized approach faces mounting resource constraints as inference workloads scale from billions to trillions of queries daily (Alvarez & Marsal, 2025). History suggests an alternative path forward. From 1946-2009, computing efficiency (performance-per-watt) doubled every 1.5 years (Koomey et al., 2010), enabling redistribution of computing workloads from data center mainframes to personal computers. This transition occurred when efficiency 1Department of Computer Science, Stanford University, Stanford, CA, USA 2Together AI, San Francisco, CA, USA. Correspondence to: Jon Saad-Falcon <jonsaadfalcon@stanford.edu>, Avanika Narayan <avanikan@stanford.edu>. improvements enabled computing to meet user needs within personal device power constraints, not when PCs surpassed mainframes in raw performance. Two converging trends suggest similar inflection point may be emerging for LLM inference. First, recent advances have produced local LMs: small models ( 20B active parameters) such as QWEN3 (Team, 2025), LLAMA3.1 (Grattafiori et al., 2024), and GPT-OSS (Agarwal et al., 2025) that achieve competitive performance on many benchmarks while requiring less energy and compute than larger, frontier models (Agarwal et al., 2025). Second, local accelerators (e.g., Apple M4 Max, AMD Ryzen AI) now have sufficient memory capacity and compute throughput to host these models with interactive latencies (Apple, 2024). This raises the question: Can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring two factors: the capa-"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "bility of local LMs to accurately respond to subset of real-world queries, and the efficiency with which local accelerators convert power into useful computation. To asses this, we need unified metric that captures both the intelligence delivered (model capability) and the energy required (accelerator efficiency). We introduce intelligence per watt (IPW): task accuracy per unit of power consumption. IPW directly measures the fundamental tradeoff facing local inference: achieving sufficient task performance within constrained power budgets. This metric enables systematic comparison across model-accelerator configurations and quantifies efficiency gains from model architecture innovations (Team, 2025; OpenAI, 2025; Team et al., 2025a; IBM Research, 2025), post-training techniques (Hinton et al., 2015; Ouyang et al., 2022; Shao et al., 2024a; Dettmers et al., 2022), and accelerator improvements (Median-Group, 2019; NVIDIA Corporation, 2025b; AMD, 2025). To evaluate the viability of local inference and measure progress in IPW, we conduct large-scale empirical study addressing three questions: Q1: What fraction of current inference queries can be solved by local LMs on local accelerators, and how has this changed over time? Q2: How has intelligence per watt improved across successive generations of local models and accelerators, and what are the relative contributions of model versus accelerator advances? Q3: What resource savings (e.g. compute, energy, dollar cost) are possible by distributing workloads across local and cloud infrastructure? Our study evaluates 20+ local LMs across 8 hardware accelerators on 1M queries spanning naturalistic user conversations (Deng et al., 2024), general reasoning tasks (Yuan et al., 2025), and standardized benchmarks measuring knowledge breadth (MMLU PRO (Wang et al., 2024b)) and expertlevel reasoning (SUPERGPQA (Team et al., 2025b)). We focus on single-turn interactions because they constitute substantial portion of LLM usage (Deng et al., 2023; Wang et al., 2024a; Shirey, 2025). We compare state-of-the-art local LMs from October 2025QWEN3 (Team, 2025), GPTOSS (Agarwal et al., 2025), GEMMA3 (Team et al., 2025a), and IBM GRANITE4 (IBM Research, 2025)alongside 2023-2024 models (MIXTRAL-8X7B, LLAMA-3.1-8B) on NVIDIA, AMD, and APPLE accelerators. For each query, we measure accuracy, latency, energy, compute, cost, and memory, enabling systematic analysis of how intelligence per watt has evolved from 2023-2025 (Section 4). We release our hardware-agnostic profiling harness to support reproducible efficiency benchmarking as new models and accelerators emerge. To address Q1, we study local LM coverage: the fraction of queries correctly answered (as measured by win-rate over frontier LM) by at least one local LM in our study. Our analysis reveals that 88.7% of queries can be successfully handled by small local models as of October 2025, with coverage varying by domainexceeding 90% for creative tasks (e.g., Arts & Media) but dropping to 68% for technical fields (e.g., Architecture & Engineering) (Figure 7). Longitudinal analysis shows consistent improvement: the best local LM matched frontier model quality on 23.2% of queries in 2023, 48.7% in 2024, and 71.3% in 2025a 3.1 increase over two years  (Table 2)  . To address Q2, we track IPW improvements across successive generations of models and accelerators from 2023-2025, decomposing gains by isolating model and accelerator contributions  (Table 2)  . Holding accelerator fixed at NVIDIA H100, model improvements from MIXTRAL-8X7B (2023) to GPT-OSS-120B (2025) yield 3.1 gain in accuracy per watt; conversely, holding the model fixed at GPT-OSS-120B, accelerator improvements from H100 to BLACKWELL deliver 1.7 gain. Combined, these compounding improvements produce 5.3 overall increase (Figure 5). To address Q3, we quantify the potential resource savings from treating cloud and local infrastructure as complementary resources and routing queries to local LMs when capable, otherwise to frontier models. Oracle routing (perfect assignment of each query to the smallest capable model) could reduce energy consumption by 80.4%, compute by 77.3%, and cost by 73.8% versus cloud-only deployment (Figure 6), representing the theoretical maximum efficiency gains. Notably, routing systems need not achieve perfect accuracy to realize substantial savings while maintaining task quality. routing system with 80% accuracy (correctly assigning 80% of queries to local vs. cloud) captures 80% of theoretical maximum gains, achieving 64.3% energy reduction, 61.8% compute reduction, and 59.0% cost reduction with no degradation in answer quality. Our work makes three primary contributions. (1) We introduce intelligence per watt as unified metric for evaluating local inference viability, and conduct the first large-scale empirical study measuring its evolution across 1M+ queries, 20+ models, and 8 hardware accelerators spanning 20232025. (2) We demonstrate that local LMs can accurately answer 88.7% of single-turn chat and reasoning queries, with intelligence per watt improving 5.3 over two years through compounding model (3.2) and hardware (1.7) advances. (3) We show that hybrid local-cloud systems achieve 40 65% reductions in energy, compute, and cost with realistic routing accuracy while maintaining answer quality. Together, these findings establish local inference as practical complement to centralized infrastructure whose viability continues expanding. We release our IPW pro-"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Figure 1. Intelligence per Watt: Study of Local Intelligence Efficiency. We present the first systematic study of local AI inference efficiency across models, hardware, and real-world workloads. (Left) Intelligence efficiency is defined as task accuracy per unit of power, capturing both capabilities delivered and energy consumed. (Left-Middle) We conduct comprehensive performance profiling across 20+ state-of-the-art local LMs ( 20B active parameters), diverse hardware accelerators (APPLE, NVIDIA, AMD), multiple performance metrics, and 1M+ real-world queries spanning chat and reasoning tasks. (Right-Middle) Local LM capabilities are improving rapidly: win/tie rate versus frontier models increases from 23.2% (2023) to 71.3% (2025)a 3.1 improvement in accuracy, demonstrating that local models can accurately handle significant portions of single-turn chat and reasoning queries. (Right) Intelligence per watt improves 5.3 from 20232025, driven by advances in both model architectures and hardware accelerators, with local accelerators showing 1.5 efficiency headroom compared to enterprise-grade systems. filing harness to facilitate systematic intelligence-per-watt benchmarking as local LMs and accelerators evolve."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our work is inspired by prior work on local AI and localcloud inference system studies. See App. for an extended related work. Inference Workload Demand. The demand for AI inference is growing exponentially, placing unprecedented strain on computational infrastructure. Recent industry analyses project that global data center capacity could nearly triple by 2030, reaching 156-219 GW, with approximately 70% of new demand driven by AI workloads (McKinsey & Company, 2025). Meeting this demand will require $5.2-$7.9 trillion in capital expenditures across computing hardware, power infrastructure, and data center construction, depending on whether growth follows constrained, continued, or accelerated scenarios (McKinsey & Company, 2025). In the U.S. alone, AI systems could require over 50 GW of power capacity by 2030, approaching 5% of total U.S. power generation capacity (You et al., 2025), while global projections suggest AI data centers may need 68-100 GW by 20272030 (Pilz et al., 2025a;b). This growth reflects both the expansion of training workloadswith frontier model training compute growing at 4-5 per year (Sevilla et al., 2024)and the rising demands of inference as models deploy at scale. While training has historically dominated AI compute expenditure, inference workloads are projected to increasingly dominate total compute allocation (Sevilla et al., 2024; Belcak et al., 2025). This shift is driven by mass adoption of generative AI, enterprise integration, and the growing use of inference-time compute scaling techniques (McKinsey & Company, 2025; Sevilla et al., 2024). Organizations face the dual challenge of training next-generation models while simultaneously serving inference at scale, straining chip production, power grids, water supplies, and other critical resources (Alvarez & Marsal, 2025; OpenAI, 2025; McKinsey & Company, 2024). However, parallel trends in model and hardware efficiency offer potential relief: frontier AI performance that once required datacenter infrastructure becomes accessible on consumer hardware within approximately one year (Somala & Emberson, 2025), while hardware price-performance continues improving along historical trends (Hobbhahn & Besiroglu, 2022). These efficiency gains, combined with advances in small language models (Belcak et al., 2025), create opportunities to redistribute inference workloads from centralized cloud infrastructure to more distributed local-cloud systems. Local-Cloud Inference Systems Recent work explores collaborative protocols for splitting generation between local and cloud LMs. Prior approaches focus on tokenand layer-level collaboration: Minions proposes communication protocol where an on-device LM handles lightweight processing while cloud LM performs high-level reasoning (Narayan et al., 2025), speculative decoding employs draft model verification (Miao et al., 2023; Xu et al., 2025), and systems like SLED, HAT, and CE-CoLLM introduce"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "edge-cloud partitioning with early-exit mechanisms (Li et al., 2025; Xie et al., 2025; Jin & Wu, 2025). These techniques have been extended beyond language models to diffusion models (Yan et al., 2024), with collaboration mechanisms broadly categorized as pipeline, routing, auxiliary, distillation, and fusion strategies (Chen et al., 2025). Complementary infrastructure work addresses efficient GPU resource management for concurrent model serving (Xiang et al., 2025). Recent benchmarks have expanded beyond academic tasks to measure AI performance on real-world economically valuable work across professional domains (OpenAI, 2025a; Mercor, 2025; Snorkel AI, 2025). Evaluating local-cloud systems on such benchmarks is critical: if these systems can maintain quality on economically important tasks while offloading work from cloud to local devices, the infrastructure savings compound on the workloads that drive actual productivity and economic value. In contrast to this line of work, we seek to better understand the current limitations and improvement trajectory of local inference as means of efficiently redistributing LLM traffic between local and cloud resources."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "We formalize local and cloud inference infrastructure and introduce metrics for measuring intelligence efficiency. Inference Infrastructure: Queries, Models and Accelerators. We consider an inference infrastructure serving stream of user queries = {q1, q2, . . . , qn}, where each query qi represents user-generated request (e.g., chat messages, reasoning tasks). Let Mlocal = {m1, . . . , mk} denote set of local LMs with 20B active parameters each, and Mcloud = {M1, . . . , Mℓ} denote frontier LMs with 100B parameters. Similarly, let Hlocal represent local accelerators (e.g., APPLE M4, AMD RYZEN) and Hcloud represent cloud accelerators (e.g., NVIDIA H200, AMD MI300X). Inference Serving: Local and Cloud. We distinguish between two inference paradigms: local inference, where queries are processed by models Mlocal on accelerator Hlocal, and cloud inference, where queries are processed by models Mcloud on accelerator Hcloud. routing function : Mlocal Mcloud determines the assignment of each query to either local or cloud model. In this work we consider setup where queries can be routed between multiple local models Mlocal hosted on local accelerator Hlocal with support for small language models (up to 20B active parameters), and cloud models Mcloud hosted on cloud accelerator Hcloud with support for large-scale frontier models (at least 100B parameters). Intelligence Efficiency Metrics. We introduce family of metrics to quantify how efficiently inference systems convert energy into useful computation. For model-accelerator pair (m, h), let acc(m, q) denote the accuracy of model on query q, ppl(m, q) denote the perplexity, (m, h, q) denote the average power consumption (in watts) during inference for query q, and τ (m, h, q) denote the total latency (in seconds) for generating the response, including both prefill and decoding phases. We define four complementary efficiency metrics: Power-based metrics measure efficiency relative to instantaneous power draw: Accuracy per watt: APW(m, h) = EqQ[acc(m,q)] EqQ[P (m,h,q)] Perplexity per watt: PPW(m, h) = 1 EqQ[ppl(m,q)]EqQ[P (m,h,q)] Energy-based metrics measure efficiency relative to total energy consumed per query: Accuracy per joule: APJ(m, h) = EqQ[acc(m,q)] EqQ[P (m,h,q)τ (m,h,q)] Perplexity per joule: PPJ(m, h) = 1 EqQ[ppl(m,q)]EqQ[P (m,h,q)τ (m,h,q)] where (m, h, q) τ (m, h, q) represents the energy consumption (in joules) for processing query q. Power-based metrics (APW, PPW) capture the instantaneous efficiency of the inference system, reflecting the hardwares ability to deliver performance at given power draw. Energy-based metrics (APJ, PPJ) capture the total efficiency per query, accounting for both power consumption and generation latency. Together, these metrics provide comprehensive view of inference efficiency: intelligence per watt quantifies the steady-state efficiency of model-accelerator pairs, while intelligence per joule quantifies the end-to-end efficiency from users perspective, including the time cost of generation."
        },
        {
            "title": "4 DATASET AND PROFILING HARNESS",
            "content": "In this section, we provide details on the dataset selection and profiling harness."
        },
        {
            "title": "4.1 Dataset Selection",
            "content": "Query Curation We curate over 1M queries across four complementary benchmarks designed to measure both naturalistic deployment scenarios and controlled capability assessment. To ensure our findings about local inference efficiency generalize across task distributions, we combine"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "naturalistic queries that reflect real-world LLM usage patterns with standardized benchmarks that enable systematic evaluation of knowledge breadth and reasoning capabilities across diverse domains. For naturalistic chat tasks, we source queries from WILDCHAT (Deng et al., 2024): dataset of 1M real ChatGPT prompts, spanning 1 month of user traffic. For general reasoning tasks, we source queries from NATURALREASONING (Yuan et al., 2025), which provides approximately 1.2 million reasoning-focused queries spanning diverse domains including mathematics, physics, and chemistry. For standardized knowledge evaluation, we use MMLU PRO (Wang et al., 2024b): an enhanced version of MMLU with increased difficulty (10 vs. 4 answer choices) and improved robustness to prompt variations, measuring multi-domain knowledge understanding. For expert-level reasoning across specialized disciplines, we evaluate on SUPERGPQA (Team et al., 2025b): comprehensive benchmark spanning 285 graduate-level disciplines with emphasis on technical domains and specialized fields underrepresented in typical evaluations (e.g., light industry, agriculture, service sciences). We perform robust data cleaning and filtering (see App. B.1 for details) on each dataset before sampling queries: 500K from WILDCHAT, 500K from NATURALREASONING, 12K from MMLU PRO, and 26.5K from SUPERGPQA (see Table 1). To provide finer-grained labels beyond task categories, we use GPT-4O-MINI to annotate each query with category from the Anthropic Economic Index (Handa et al., 2025), which maps AI queries to occupations in the U.S. Department of Labors O*NET. We consider 22 categories in total, spanning Architecture and Engineering to Healthcare Support. The full list appears in App. B.1  (Table 5)  along with breakdown of category representation in the dataset."
        },
        {
            "title": "Dataset Origin",
            "content": "Category WILDCHAT NATURALREASONING MMLU PRO SUPERGPQA Chat Queries Reasoning Queries Knowledge Evaluation Graduate-Level Reasoning 500K 500K 12K 26.5K Category Items"
        },
        {
            "title": "Model Families",
            "content": "QWEN3, GPT-OSS, GEMMA, IBM GRANITE 4."
        },
        {
            "title": "Accelerators",
            "content": "NVIDIA A100, NVIDIA H200 NVIDIA GH200, NVIDIA B200 NVIDIA QUADRO RTX 6000, NVIDIA RTX 6000 ADA AMD MI300X, APPLE M4 MAX, SAMBANOVA SN40L Table 1. Dataset Overview. (Top) Query composition with split sizes. (Bottom) Models and Hardware Accelerators. Hardware Accelerators We run inference on six different AI accelerator systems: the NVIDIA A100 40 GB SXM4 (AMPERE) (NVIDIA, 2021), NVIDIA H200 SXM (HOPPER) (NVIDIA, 2024), NVIDIA GH200 GRACE HOPPER SUPERCHIP (NVIDIA Corporation, 2024), NVIDIA B200 (BLACKWELL) (NVIDIA Corporation, 2025a), NVIDIA QUADRO RTX 6000 (NVIDIA Corporation, 2019), NVIDIA RTX 6000 Ada (NVIDIA Corporation, 2023), AMD INSTINCT MI300X (CDNA 3, OAM) (AMD, 2023), SambaNova SN40L (SambaNova Systems, 2022) and APPLE MAC STUDIO (M4 MAX) (Apple, 2024). These systems were chosen because of their different memory capacities (ranging from 40 GB to 768 GB), memory bandwidth (from 546 GB/s to 8 TB/s), and power consumption (145W to 1000W) (see Table 9 for more details). collect model generations over Models We the QWEN3 (Team, 2025), GPT-OSS (OpenAI, 2025), GEMMA3 (Team et al., 2025a), and IBM GRANITE 4.0 (IBM Research, 2025) model families. For the QWEN3 family, we use QWEN3-4B, QWEN3-8B, QWEN3-14B, QWEN3-32B, and QWEN3-235B. For GPT-OSS, we consider the GPT-OSS-20B and GPT-OSS-120B models. For the GEMMA3 family, we use GEMMA3 1B INSTRUCT, GEMMA3 4B INSTRUCT, and GEMMA3 12B INSTRUCT models. For the IBM GRANITE 4.0 family, we use GRANITE-4.0-H-MICRO, GRANITE-4.0-H-TINY, GRANITE-4.0-H-SMALL, and GRANITE-4.0-H-TINY models. We also benchmark against state-of-the-art cloud models as of October 2025, including CLAUDE SONNET 4.5 (Anthropic, 2025), GEMINI 2.5 PRO (Comanici et al., 2025), and GPT-5 (2025-08-07) (OpenAI, 2025b). For our longitudinal analysis, we evaluate MIXTRAL-8X7B (Jiang et al., 2024) as well as LLAMA3.1-8B (Jiang et al., 2024). For each model, we generate responses across all dataset queries on each of the hardware backends. Full details of inference hyperparameters can be found in App. B.1. Metrics For each (query, model, hardware) triple, we collect labels for accuracy, as well as wide range of efficiency measurements: latency, throughput, timeto-first-token (TTFT), energy consumption, and more (see Table 8 for full list of metrics). We use LLM-as-a-judge (see App. B.1 for the respective prompts) to score generated responses against reference answers. For WILDCHAT, reference answers are responses from QWEN3-235B, the SOTA open-source model on LMArena (as of August 2025) (Chiang et al., 2024). For NATURALREASONING, MMLU PRO, and SUPERGPQA, we use the provided ground truth answers from each benchmark."
        },
        {
            "title": "4.2 Profiling Harness",
            "content": "We develop an end-to-end, cross-platform profiling harness for inference workloads that ensures reproducible results and easily accommodates new models, tasks, and hardware backends. It comprises three componentsdistributed"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "multi-GPU inference, response evaluation, and system-level telemetry collectionand currently supports NVIDIA, MACOS (Apple Silicon), and AMD systems. Given dataset, model, and backend, the harness orchestrates inference over all input queries, evaluates outputs (via exact match or LLM-as-a-judge), and records detailed telemetrylatency, throughput, time-to-first-token (TTFT), energy consumption, and more  (Table 8)  . Telemetry is collected via vendor APIs, synchronized at nanosecond resolution, and normalized to common units (watts, joules, megabytes). For energy measurements, we follow standard practices (Samsi et al., 2023a; Fernandez et al., 2025; Wilkins et al., 2024). On NVIDIA systems we query NVML for per-device power, energy, memory usage, and temperature; on MACOS systems we extract GPU power data from powermetrics; and on AMD systems we query ROCm SMI for power, temperature, and VRAM usage. In all cases, we compute energy via numerical integration over time and sample at 50 ms intervals, providing higher temporal resolution than prior work (100 ms (Samsi et al., 2023a) or 15 (Fernandez et al., 2025)). For multi-GPU configurations, we aggregate energy from each GPU individually rather than extrapolating from single device (Samsi et al., 2023a). However, software-based power measurements can introduce inaccuracies of 1015%, with variations distributed across different hardware components due to architectural differences in workloads between CPUs, GPUs, and NPUs (Yang et al., 2023). Even hardware wattage meters may fall short for milliwatt-level precision, though our approach aligns with established practices and provides consistent relative comparisons across configurations. Full implementation details are provided in App. B.1."
        },
        {
            "title": "INTELLIGENCE EFFICIENCY STUDY",
            "content": "We investigate whether recent advances in local LMs and local accelerators enable local inference to viably complement centralized cloud infrastructure by handling substantial fraction of inference queries. Using our curated dataset, we examine three interconnected questions: 1) the extent to which current workloads can be handled locally (Section 5.1), 2) how intelligence efficiency has evolved from 2023-2025 (Section 5.2), 3) what gains query routing across local and cloud models can deliver in practice (Section 5.3). We study single-query inference (batch size = 1), to (1) isolate intrinsic model-accelerator efficiency from systemlevel serving optimizations and (2) follow standard local inference benchmarking practices (Hao et al., 2023)."
        },
        {
            "title": "5.1 Can Local Models and Accelerators Handle",
            "content": "Current Inference Workloads? Figure 2 shows query coverage (the percentage of dataset queries correctly answered) across individual local LMs, the best-of-local ensemble (routing to the best local LM for each query), and the best-of-cloud baseline (routing to the best frontier model). Local LM coverage increases with scale and time. Across WILDCHAT, NATURALREASONING, SUPERGPQA, and MMLU PRO, individual model coverage ranges from 49.6% for QWEN3-4B, on average, to 71.4% for GPT-OSS120B, with consistent improvements at each scale point: QWEN3-8B achieves 57.5% and QWEN3-14B reaches 60.0%. Coverage has improved substantially from 2023 to 2025 (Figure 3): the best local LMs achieved 32.2% relative improvement on chat queries and 50.1% relative improvement on reasoning queries over this period. While improvements from 2023-2025 are relatively uniform across difficulty levels for chat tasks, reasoning tasks show markedly slower progress on the hardest problems (see App C). These results demonstrate that larger local LMs can handle progressively more queries without requiring cloud infrastructure, with the best individual local LM (GPT-OSS120B) successfully answering almost three-fourths of the single-turn chat and reasoning queries studied. Model diversity substantially improves coverage. Routing queries to the most appropriate local LM rather than using single model achieves 88.7% overall coveragea 28.8 percentage point improvement over QWEN3-14B and 16.3 percentage points over individual GPT-OSS-120B performance, on average. This gap between individual models and best-of-local demonstrates that architectural, pretraining, and post-training diversity captures complementary capabilities: different models excel on different query types, and intelligent routing can exploit these complementary strengths. Chat queries are more amenable to local processing than reasoning queries. Performance varies significantly by task type: WILDCHAT queries show substantially higher coverage across all models, while NATURALREASONING queries prove more challenging. The best local LM achieves 88.9% coverage on WILDCHAT versus 64.9% on NATURALREASONING 24.0 percentage point gap. This aligns with findings that 77% of real-world ChatGPT queries involve practical guidance, information seeking, or writing (Chatterji et al., 2025)tasks well-suited to local modelswhile reasoning-intensive queries more often require frontier capabilities for tasks such as Architecture, Engineering, Life & Physical Science, and Mathematics (see Figure 7 in App. B). Nevertheless, based on our local routing setting for NATURALREASONING, SUPERGPQA, and MMLU PRO (Figure 2), local LMs successfully handle over four-fifths of reasoning queries studied, suggesting significant opportunities for local inference even in technically demanding domains."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Figure 2. Local Models Rival Cloud Models Across Diverse Benchmarks: Individual model performance scales with size, ranging from 31.569.4% for IBM GRANITE4-H-SMALL, 30.083.6% for GEMMA3-12B, 51.580.4% for GPT-OSS-120B, and 66.5 89.5% for GEMINI 2.5 PRO. Local routing (best local LM per query) achieves 97.8%, 88.3%, 77.0%, and 92.4% on WILDCHAT, NATURALREASONING, SUPERGPQA, and MMLU PRO respectively, surpassing cloud routing (100%, 82.9%, 66.5%, 87.4%) on three of four benchmarks. LM viability across task distributions. To validate that our findings generalize beyond naturalistic queries, we evaluate local model coverage on MMLU PRO (multi-domain knowledge) and SUPERGPQA (graduate-level reasoning across 285 disciplines). The best individual local model (GPTOSS-120B) achieves 80.4% on MMLU PRO and 51.5% on SUPERGPQA, while routing to the best local LM for each query achieves 93.4% coverage on MMLU PRO and 83.6% coverage on SUPERGPQA, which is 13.0 and 32.1 percentage point improvement, respectively. Notably, SUPERGPQAs comprehensive disciplinary coverage reveals that technical domains remain the primary challenge for local deployment: coverage exceeds 93.1% for creative and humanities fields but drops to 60.1% for specialized technical disciplines like Architecture & Engineering (Figure 7). This domain-specific analysis confirms that while local LMs can handle the majority of conversational and knowledgerecall tasks, complex reasoning in specialized fields still benefits from frontier model capabilities. Local accelerator memory capacity is expanding rapidly. From 2012 to 2025, local accelerator memory has increased dramatically, with particularly rapid growth since 2020 (Figure 4). Local accelerators that offered 10-20 GB in 2020 now provide 128-512 GB through unified memory architectures like Apple Silicon, enabling models that previously required datacenter infrastructure to run efficiently on local hardware. This memory expansion has been the primary driver enabling local deployment of increasingly capable models: the jump from sub-20 GB to 200+ GB memory removes the key constraint that forced workloads to cloud infrastructure, allowing local accelerators to host the 8-20B active parameter models that now handle the majority of inference queries."
        },
        {
            "title": "5.2 How Intelligence Efficient is Local Inference?",
            "content": "Intelligence efficiency is improving over time. Table 2 tracks the evolution of local LM capabilities from 2023 to 2025, measuring the best available local LM ( 20B active parameters) paired with state-of-the-art accelerators each year. On our curated dataset of chat and reasoning queries, accuracy per watt has improved 5.3 over this twoyear period: in 2023, MIXTRAL-8X7B-V0.1 on NVIDIA QUADRO RTX 6000 achieved 7.92 104 accuracy per watt; by 2024, LLAMA-3.1-8B-INSTRUCT on NVIDIA RTX 6000 ADA reached 1.80 103 (a 2.27 year-overyear gain); and in 2025, GPT-OSS-120B on APPLE M4 MAX achieved 4.18 103 (a 2.32 gain). Notably, local LM coverage on single-turn chat and reasoning queries has increased in lockstep with efficiency gains: from 23.2% in 2023 to 48.7% in 2024 to 71.3% in 2025. This progression reflects compounding improvements in both model architectures, which achieve higher accuracy through advances in pretraining (Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023; DeepSeek-AI, 2024), posttraining (Bai et al., 2022; Shao et al., 2024b; DeepSeekAI, 2025), and parameter utilization via mixture-of-experts (MoE) architectures (Shazeer et al., 2017; DeepSeek-AI, 2024), and hardware accelerators, which deliver more compute (FLOPs) and memory per watt (NVIDIA, 2021; 2024). Figure 5 provides complementary view by examining accuracy per watt trends across model releases between 2023 and 2025, showing consistent efficiency improvements across multiple model families: (LLAMA, PHI, GEMMA, MISTRAL, FALCON, DEEPSEEK, QWEN,"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Figure 3. Rapid Improvement of Local LMs across Chat and Reasoning Queries: We evaluate the performance of SOTA local models released between April 2024 and August 2025 on WILDCHAT and NATURALREASONING. On WILDCHAT (left), local models show win/tie rate of 78.2% against QWEN3-235B as of August 2025, compared to just 28.0% in April 2024a 2.8 improvement in 16 months. On NATURALREASONING (right), local models achieve 80.9% accuracy by August 2025, up from 48.7% in April 2024a 66% relative improvement. 2023"
        },
        {
            "title": "SOTA\nLocal Model",
            "content": "Mixtral-8x7B-v0.1 Llama-3.1-8B-Instruct GPT-OSS-120B"
        },
        {
            "title": "SOTA\nAccelerator",
            "content": "NVIDIA Quadro RTX 6000 NVIDIA RTX 6000 Ada Apple M4 Max"
        },
        {
            "title": "Success Rate",
            "content": "23.2 1.9% 48.7 2.7% 71.3 2.2%"
        },
        {
            "title": "Intelligence\nper Watt",
            "content": "(7.92 0.32) 104 (1.80 0.21) 103 (4.18 0.53)"
        },
        {
            "title": "YoY Efficiency\nGain",
            "content": "2.27 2.32 Table 2. Increase in Intelligence per Watt for Local LMs: Accuracy per watt has improved over 5 in two years, driven by advances in both model architectures (from MIXTRAL-8X7B to GPT-OSS-120B) and accelerator hardware (from NVIDIA Quadro RTX 6000 to Apple M4 Max). While local accelerators enable deployment of capable models outside data centers, cloud-grade hardware maintains substantial efficiency advantage for the same workloads. We evaluate this gap using two complementary metrics: intelligence per watt (instantaneous power efficiency) and intelligence per joule (end-to-end energy efficiency per query). Table 3 compares intelligence per watt across QWEN3 models (4B to 32B active parameters) on APPLE M4 MAX (local) versus NVIDIA B200 and SAMBANOVA SN40L (cloud). The B200 achieves 1.40 higher intelligence per watt than the M4 MAX across all model sizes, while the SN40L achieves 1.78 higher efficiency on QWEN3-32B (3.51 103 versus 1.97 103 intelligence per watt). Table 4 extends this analysis to intelligence per joule, which accounts for both power consumption and generation laFigure 4. Increasing GPU Memory of Consumer Accelerators: Memory capacity (GB) for local accelerators. Over the past decade, local hardware has significantly closed the memory gap with cloudgrade accelerators, particularly since 2020, driven by advances in high bandwidth memory (HBM) components and unified memory architectures. and GPT-OSS). The consistent progression across multiple efficiency metricsincluding accuracy per joule and perplexity-based measurements presented in Figure 10 (App. C.2)demonstrates that intelligence efficiency is improving rapidly and predictably. As both model architectures and hardware accelerators continue to advance, the viability of local inference will expand further, enabling an increasingly large fraction of queries to be served efficiently outside centralized cloud infrastructure. Local accelerator efficiency has room for improvement:"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B Success Rate 49.3 1.8% 57.5 2.5% 59.5 1.4% 69.5 2.3% Apple M4 Max Intelligence per Watt (1.40 0.38) 103 (1.63 0.20) 103 (1.69 0.31) 103 (1.97 0.24) 103 NVIDIA Intelligence per Watt (1.95 0.14) 103 (2.27 0.18) 103 (2.35 0.09) 103 (2.75 0.14) 103 SambaNova SN40L Intelligence per Watt (3.51 0.43) 103 Table 3. Local accelerators demonstrate lower power efficiency than cloud accelerators: When running the same QWEN3 models, the APPLE M4 MAX (local) attains 1.40 lower intelligence per watt compared to the NVIDIA B200 (cloud) and SAMBANOVA SN40L (cloud), highlighting the efficiency advantage of purposebuilt cloud accelerators over local accelerators. Qwen3-8B Qwen3-32B GPT-OSS-20B GPT-OSS-120B Apple M4 Max"
        },
        {
            "title": "Intelligence\nper Joule",
            "content": "(3.80 0.40) 105 (3.51 0.38) 105 (4.38 0.31) 105 (4.23 0.45) 105 NVIDIA B"
        },
        {
            "title": "Intelligence\nper Joule",
            "content": "(8.71 0.60) 105 (5.91 0.42) 105 (7.34 0.51) 105 (6.78 0.47) 105 SambaNova SN40L"
        },
        {
            "title": "Intelligence\nper Joule",
            "content": "(2.27 0.30) 104 (3.12 0.38) 104 Table 4. Cloud accelerators demonstrate superior energy efficiency across all models: The NVIDIA B200 (cloud) achieves 1.6 to 2.3 higher intelligence per joule than the APPLE M4 MAX (local), while the SAMBANOVA SN40L (cloud) achieves 6.5 to 7.4 higher efficiency. These results highlight the substantial energy efficiency advantage of purpose-built cloud accelerators over local hardware across QWEN3 AND GPT-OSS model variants. routed between four small local LMs (QWEN3-4B, QWEN38B, QWEN3-14B, GPT-OSS-20B) running on APPLE M4 MAX devices and frontier model (QWEN3-235B) running on NVIDIA H200 infrastructure. Figure 6 shows cumulative energy consumption (left), compute usage (middle), and cost (right) over the 24-hour period for five routing strategies: routing all queries to the largest model (baseline), oracle routing (perfect assignment), and two realistic routers with 60% and 80% routing accuracy. For the realistic routers, when the system fails to correctly identify the smallest capable local model, queries default to the cloud model (QWEN3-235B) to ensure correctness. Figure 5. Increase in Intelligence per Joule for Local LMs and Accelerators: Efficiency improved 18.0 over 16 months, decomposed into 3.1 from better local LMs and 5.9 from better local accelerators. tency across QWEN3 and GPT-OSS model variants. Here the efficiency gaps widen substantially: the B200 achieves 1.6 to 2.3 higher intelligence per joule than the M4 MAX, while the SN40L achieves 6.5 to 7.4 higher efficiency. The larger gap in per-joule metrics reflects that cloud accelerators not only consume less power per unit of accuracy, but also complete queries faster, compounding their energy advantage. These efficiency gaps stem from specialized hardware optimizations in enterprise-grade accelerators: cloud accelerators like the B200 and SN40L employ purpose-built componentshigh-bandwidth memory (HBM3e), dedicated tensor processing units, and optimized memory hierarchiesthat maximize throughput per watt, whereas local accelerators use unified memory architectures that balance diverse workloads under thermal and power constraints. Notably, all measurements use batch size 1, indicating these advantages persist even in single-query settings and revealing substantial headroom for future local accelerator designs to close this gap through specialized on-device AI components. However, this efficiency disadvantage is offset by local deployment complementary system-level benefits: avoids datacenter infrastructure costs and enables 88.7% of queries that local models can handle to avoid cloud compute entirely, yielding 6080% resource reductions through intelligent routing (Section 5.3)."
        },
        {
            "title": "5.3 What Efficiency Gains Can Effective Query",
            "content": "Routing Deliver? We simulate hybrid local-cloud system serving 80.2M queries over 24-hour periodrepresentative of realistic daily inference workloads (Wang et al., 2025). Queries are Oracle routing establishes theoretical upper bounds. Assuming perfect query-to-model assignment, oracle routing reduces energy consumption by 80.4%, compute by 77.3%,"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Figure 6. Energy, Compute, and Capital Gains from Model Routing. Cumulative resource consumption over 24 hours and 80.2M LLM queries (Wang et al., 2025). Using our local-cloud router between 4 small LMs on Apple M4 Max and QWEN3-235B on an H200 yields substantial savings at various routing accuracies. For the 80% accurate router, we observe: 64.3% in energy savings, 61.8% in compute, and 59.0% in cost compared to naively routing every query to QWEN3-235B, capturing majority of the gains achievable by the theoretical best-case (Oracle Router). and cost by 73.8% versus cloud-only deployment to the largest model (Figure 6). These dramatic savings stem from assigning the 80.7% of queries that local models can handle correctly to significantly more efficient hardware, while reserving expensive frontier model compute for the remaining 19.3% of queries that truly require it. Practical routers achieve substantial gains without perfect accuracy. Crucially, routing systems need not achieve perfect accuracy to realize significant efficiency improvements. router with 80% accuracya realistic target for modern routing systemscaptures approximately 80% of oracle gains, achieving 64.3% energy reduction, 61.8% compute reduction, and 59.0% cost reduction compared to the cloud-only baseline (Figure 6). Even more conservative 60% accurate router delivers 48.4% energy savings, 46.7% compute savings, and 44.5% cost savings. Importantly, these efficiency gains come with minimal accuracy degradation: routing at 80% accuracy maintains task performance as misrouted queries fall back to the frontier model. These savings scale linearly with query volume: at current platform-scale inference demand (billions of queries daily), intelligent routing could yield annual energy savings measured in terawatt-hours."
        },
        {
            "title": "6 EXTENDED EXPERIMENTS",
            "content": "Finally, we include set of extended experiments in the Appendix, including: How does model precision affect task performance and intelligence efficiency (Appendix C.4)? How do SOTA open-source LMs compare to the SOTA closed-source LMs on Chat and Reasoning Queries (Appendix C.5)? How does performance on chat and reasoning queries connect to U.S. GDP (Appendix C.6)?"
        },
        {
            "title": "7 CONCLUSION",
            "content": "Centralized cloud infrastructure faces mounting pressure from exponential growth in LLM inference demand. Advances in local models and accelerators create an opportunity to redistribute workloads, but evaluating this transition requires quantifying both model capability and hardware efficiency. We introduce intelligence per watt as unified metric for measuring local inference viability and conduct large-scale empirical study across 20+ models, 8 hardware accelerators, and 1M real-world queries spanning 2023-2025. Our work makes three contributions. First, we demonstrate that local LMs can successfully handle 88.7% of single-turn chat and reasoning queries, with coverage varying by domainexceeding 90% for creative tasks but falling to 68% for technical fields. Second, we show that intelligence per watt has improved 5.3 from 20232025 through compounding advances in both model architectures (3.1) and hardware accelerators (1.7), with locally-serviceable query coverage increasing from 23.2% to 71.3%. Third, we demonstrate that treating local and cloud infrastructure as complementary resources and optimally routing queries across them (based on the smallest model capable of answering the queries), achieves 6080% reductions in energy, compute, and cost through intelligent query routing, even with realistic (80%) routing accuracy, while maintaining answer quality. These findings establish that local inference can meaningfully redistribute demand from centralized infrastructure for substantial and growing subset of queries. As model architectures and hardware"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "accelerators continue to advance, intelligence per watt will remain the critical metric for tracking this transition. We release our profiling harness to enable systematic efficiency benchmarking as the local inference ecosystem evolves."
        },
        {
            "title": "8 ACKNOWLEDGEMENTS",
            "content": "We thank Simran Arora, Bradley Brown, Mayee Chen, Owen Dugan, Sabri Eyuboglu, Neel Guha, Simon Guo, Jordan Juravsky, Jerry Liu, Benjamin Spector, Shayan Talaei, and Dan Zhang for their constructive feedback during the composition of the paper. We would also like to thank our collaborators at the Stanford Artificial Intelligence Laboratory (SAIL) and TogetherAI. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize); NSF under Nos. CCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-20184 (Long-context) and W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000142312633 (Deep Signal Processing); Stanford HAI under No. 247183; Google DeepMind; Google Research; Google Cloud; NXP; Xilinx; LETI-CEA; Intel; IBM; Microsoft; NEC; Toshiba; TSMC; ARM; Hitachi; BASF; Accenture; Ericsson; Qualcomm; Analog Devices; Salesforce; Total; the Laude Institute; Prime Intellect; Together; Anthropic; the HAI-GCP Cloud Credits for Research program; the Stanford Data Science Initiative (SDSI); members of the Stanford DAWN project: Meta, Google, and VMWare; and members of the Stanford SEAMS project: IBM and Felicis. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government."
        },
        {
            "title": "REFERENCES",
            "content": "Agarwal, S. et al. gpt-oss-120b and gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. URL https://arxiv.org/abs/2508.10925. Alvarez & Marsal. Rethinking ai demand part 1: Ai data centers are experiencing surge of training demand - what happens when the surge is over?, 2025. Accessed: 2025-10-06. AMD. Amd instinct mi300x accelerators product specifications. AMD official website, 2023. URL https:// www.amd.com/en/products/accelerators/ instinct/mi300/mi300x.html. 192GB HBM3, 5.3TB/s bandwidth, 750W TDP. AMD. Accelerator specifications. https://www. amd.com/en/products/specifications/ accelerators.html, 2025. Accessed: 2025-10-30. Anthony, L. F. W., Kanding, B., and Selvan, R. Carbontracker: Tracking and predicting the carbon footprint of training deep learning models. In ICML Workshop on Challenges in Deploying and Monitoring Machine Learning Systems, 2020. URL https://arxiv. org/abs/2007.03051. arXiv:2007.03051. System card: sonnet System card, Anthropic, September URL https://assets.anthropic. Anthropic. 4.5. 2025. com/m/12f214efcc2f457a/original/ Claude-Sonnet-4-5-System-Card.pdf."
        },
        {
            "title": "Claude",
            "content": "Appel, R., McCrory, P., Tamkin, A., Stern, M., McCain, M., and Neylon, T. Anthropic economic index report: Uneven geographic and enterprise ai adoption, 2025. Apple. Apple m4 max tech specs. Apple Support / Press Releases, 2024. URL https://support.apple. com/en-us/121553. Unified memory bandwidth up to 546 GB/s. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Belcak, P., Heinrich, G., Diao, S., Fu, Y., Dong, X., Muralidharan, S., Lin, Y. C., and Molchanov, P. Small language models are the future of agentic ai, 2025. URL https://arxiv.org/abs/2506.02153. Chatterji, A., Cunningham, T., Deming, D., Hitzig, Z., Ong, C., Shan, C., and Wadman, K. How people use chatgpt. Technical report, OpenAI, September 2025. Available at: https://cdn.openai.com/pdf/ a253471f-8260-40c6-a2cc-aa93fe9f142e/ economic-research-chatgpt-usage-paper. pdf. Chen, L., Zaharia, M., and Zou, J. Y. Frugalml: accuin Neural (NeurIPS URL https://proceedings. How to use ML prediction apis more rately and cheaply. Information 2020), 2020. neurips.cc/paper/2020/hash/ 789ba2ae4d335e8a2ad283a3f7effced-Abstract. html. In Advances 33 Systems"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Chen, L., Zaharia, M., and Zou, J. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023. URL https://arxiv.org/abs/2305.05176. Fernandez, J., Na, C., Tiwari, V., Bisk, Y., Luccioni, S., and Strubell, E. Energy considerations of large language model inference and efficiency optimizations. arXiv preprint arXiv:2504.17674, 2025. Chen, S., Jiang, W., Lin, B., Kwok, J., and Zhang, Y. Routerdc: Query-based router by dual contrastive learning for assembling large language models. Advances in Neural Information Processing Systems, 37:6630566328, 2024. Chen, Y., Zhao, J., and Han, H. survey on collaborative mechanisms between large and small language models, 2025. URL https://arxiv.org/abs/2505. 07460. Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhu, B., Zhang, H., Jordan, M., Gonzalez, J. E., et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. CodeCarbon Contributors. mlco2/CodeCarbon (v2.8.0). Zenodo, 2024. URL https://zenodo.org/doi/ 10.5281/zenodo.14212766. Comanici, G., Bieber, E., and et al., M. S. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507. 06261. DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Deng, Y., Zhao, N., and Huang, X. Early chatgpt user portrait through the lens of data. arXiv preprint arXiv:2312.10078, 2023. Deng, Y., Zhao, W., Hessel, J., Ren, X., Cardie, C., and Choi, Y. Wildvis: Open source visualizer for million-scale chat logs in the wild, 2024. URL https://arxiv.org/ abs/2409.03753. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:3031830332, 2022. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang,"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Handa, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., Troy, K. K., Amodei, D., Kaplan, J., Clark, J., and Ganguli, D. Which economic tasks are performed with ai? evidence from millions of claude conversations, 2025. URL https://arxiv.org/abs/2503.04761. Hao, J., Subedi, P., Ramaswamy, L., and Kim, I. K. Reaching for the sky: Maximizing deep learning inference throughput on edge devices with ai multi-tenancy. ACM Transactions on Internet Technology, 23(1):133, 2023. Henderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D., and Pineau, J. Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research, 21(248):1 43, 2020. URL http://jmlr.org/papers/v21/ 20-312.html. Hinton, G., Vinyals, O., and Dean, J. the knowledge in neural network. arXiv:1503.02531, 2015."
        },
        {
            "title": "Distilling\narXiv preprint",
            "content": "Hobbhahn, M. and Besiroglu, T. Trends in gpu priceperformance, 2022. URL https://epoch.ai/ blog/trends-in-gpu-price-performance. Accessed: 2025-10-05. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. In Advances in Neural Information Processing Systems, volume 35, pp. 3001630030, 2022. Hu, Q. J., Bieker, J., Li, X., Jiang, N., Keigwin, B., Ranganath, G., Keutzer, K., and Upadhyay, S. K. Routerbench: benchmark for multi-llm routing system. arXiv preprint arXiv:2403.12031, 2024. URL https:// arxiv.org/abs/2403.12031."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Huang, Z., Ling, G., Lin, Y., Chen, Y., Zhong, S., Wu, H., and Lin, L. Routereval: comprehensive benchmark for routing llms to explore model-level scaling up in llms. arXiv preprint arXiv:2503.10657, 2025. doi: 10.48550/arXiv.2503.10657. URL https://arxiv. org/abs/2503.10657. IBM Research. Granite 4.0 language models. https://github.com/ibm-granite/ granite-4.0-language-models, 2025. cessed: 2025-10-01. AcJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jin, H. and Wu, Y. Ce-collm: Efficient and adaptive large language models through cloud-edge collaboration, 2025. URL https://arxiv.org/abs/2411.02829. Koomey, J., Berard, S., Sanchez, M., and Wong, H. Implications of historical trends in the electrical efficiency of computing. IEEE Annals of the History of Computing, 33 (3):4654, 2010. Li, X., Spatharakis, D., Ghafouri, S., Fan, J., and Nikolopoulos, D. Sled: speculative llm decoding framework for efficient edge serving. arXiv preprint arXiv:2506.09397, 2025. URL https://arxiv. org/abs/2506.09397. McKinsey & Company. Ai power: Expanding data center capacity to meet growing demand, 2024. Accessed: 202510-06. McKinsey & Company. The cost of compute: $7 trillion race to scale data centers, 2025. Accessed: 2025-10-07. Median-Group. numbers. https://github.com/ Median-Group/numbers, 2019. Accessed: 202510-30. Mercor. Apex: The ai productivity index. https:// mercor.com/apex/, 2025. Accessed: 2025-11-02. Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Zhang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X., Shi, C., Chen, Z., Arfeen, D., Abhabkar, R., and Jia, Z. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification. In arXiv preprint arXiv:2305.09781, 2023. URL https://arxiv.org/abs/2305.09781. NVIDIA. Nvidia a100 tensor core gpu data sheet. URL NVIDIA official documentation, https://www.nvidia.com/content/dam/ en-zz/Solutions/Data-Center/a100/pdf/ nvidia-a100-datasheet-us-nvidia-1758950-r4-web. pdf. SXM4 version, 2.0TB/s memory bandwidth, 400W. 2021. NVIDIA. sheet. URL data-center/h200/. 4.8TB/s bandwidth, up to 700W (SXM variant). Nvidia h200 tensor core gpu data 2024. NVIDIA official documentation, https://www.nvidia.com/en-us/ 141GB HBM3e memory, NVIDIA Corporation. NVIDIA Quadro RTX 6000 Datasheet. NVIDIA Corporation, March 2019. URL https://www.nvidia.com/content/dam/ en-zz/Solutions/design-visualization/ quadro-product-literature/ quadro-rtx-6000-us-nvidia-704093-r4-web. pdf. Document 704093-r4. NVIDIA Corporation. NVIDIA RTX 6000 Ada NVIDIA Corporation, URL https://www.nvidia. Generation Datasheet. February 2023. com/content/dam/en-zz/Solutions/ design-visualization/rtx-6000/ proviz-print-rtx6000-datasheet-web-2504660. pdf. Document 2647623. NVIDIA Corporation. NVIDIA Grace Hopreport, Technical URL https://resources.nvidia. per Superchip Architecture. 2024. com/en-us-data-center-overview-mc/ en-us-data-center-overview/ grace-hopper-superchip-datasheet-partner. Accessed: 2025-01-15. NVIDIA Corporation. NVIDIA DGX B200 System Architecture. 2025a. Technical https://resources.nvidia.com/ URL en-us-dgx-systems/dgx-b200-datasheet. Accessed: 2025-01-15. report, NVIDIA Corporation. Nvidia data center gpu resource https://resources.nvidia.com/l/ center. en-us-gpu, 2025b. Accessed: 2025-10-30. Ong, I., Almahairi, A., Wu, V., Chiang, W.-L., Wu, T., Gonzalez, J. E., Kadous, M. W., and Stoica, I. Routellm: Learning to route llms with preference data, 2024. Narayan, A., Biderman, D., Eyuboglu, S., May, A., Linderman, S., Zou, J., and Re, C. Minions: Cost-efficient collaboration between on-device and cloud language models. arXiv preprint arXiv:2502.15964, 2025. URL https://arxiv.org/abs/2502.15964. Ong, I., Almahairi, A., Wu, V., Chiang, W.-L., Wu, T., Gonzalez, J. E., Kadous, M. W., and Stoica, I. Routellm: In ProLearning to route llms with preference data. ceedings of the International Conference on Learning Representations (ICLR), 2025."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. OpenAI. Gdpval: Evaluating ai model performance on real-world economically valuable tasks. Technical report, OpenAI, 2025a. Available at: https://openai. com/index/gdpval/. OpenAI. 2025b. introducing-gpt-5/. Accessed 2025-09-13. Introducing GPT-5. OpenAI Blog, URL https://openai.com/index/ OpenAI. gpt-oss-120b and gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. OpenAI."
        },
        {
            "title": "Announcing",
            "content": "project. announcing-the-stargate-project/, uary 2025. Accessed: 2025-10-06. stargate https://openai.com/index/ Janthe OpenRouter. OpenRouter. https://openrouter.ai, 2025. Accessed: 23 September 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Oviedo, F., Kazhamiaka, F., Choukse, E., Kim, A., Luers, A., Nakagawa, M., Bianchini, R., and Ferres, J. M. L. Energy use of ai inference: Efficiency pathways and testtime compute, 2025. URL https://arxiv.org/ abs/2509.20241. Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021. URL https://arxiv.org/abs/2104.10350. Pilz, K. F., Mahmood, Y., and Heim, L. Ais power requirements under exponential growth: Extrapolating ai data center power demand and assessing its potential impact on u.s. competitiveness. Research Report RRA3572-1, RAND Corporation, Santa Monica, CA, January 2025a. URL https://www.rand.org/pubs/ research_reports/RRA3572-1.html. Pilz, K. F., Sanders, J., Rahman, R., and Heim, L. Trends in ai supercomputers, 2025b. URL https://arxiv. org/abs/2504.16026. SambaNova Systems. Sambanova datascale sn40l: The hardware system for running high performance ai workloads. Product datasheet, SambaNova Systems, Inc., Palo Alto, California, 2022. URL https://sambanova. ai/hubfs/23945802/downloads/Product% 20Collateral/SambaNova_SambaFlow_ Datasheet_021122_EN.pdf. Accessed: November 10, 2025. Samsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W., Kepner, J., Tiwari, D., and Gadepally, V. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pp. 19. IEEE, 2023a. Samsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W., Kepner, J., Tiwari, D., and Gadepally, V. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pp. 19, Boston, MA, USA, 2023b. IEEE. doi: 10.1109/HPEC58863.2023.10363447. URL https:// arxiv.org/abs/2310.03003. Schuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D., Tran, V. Q., Tay, Y., and Metzler, D. Confident adaptive language modeling. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022. Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. Green AI. Communications of the ACM, 63(12):5463, 2020. doi: 10.1145/3381831. URL https://dl.acm. org/doi/10.1145/3381831. Sevilla, J., Besiroglu, T., Cottier, B., You, J., Roldan, E., Villalobos, P., and Erdil, E. Can ai scaling continue through 2030?, 2024. URL https://epoch.ai/blog/ can-ai-scaling-continue-through-2030. Accessed: 2025-10-06. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024a. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. Shirey, T. How people use chatgpt: Stats from 13,252 conversations. https://www.webfx.com/blog/ai/ chatgpt-usage-statistics/, September 2025. Accessed: November 2025."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Snorkel AI. https: Snorkel ai //leaderboard.snorkel.ai/, 2025. Expertverified benchmarks including SnorkelUnderwrite and SnorkelSpatial. leaderboards. Somala, V. and Emberson, L. Frontier ai performance becomes accessible on consumer hardware within year, 2025. URL https://epoch.ai/ data-insights/consumer-gpu-model-gap. Accessed: 2025-10-06. Somerstep, S., Polo, F. M., de Oliveira, A. F. M., Mangal, P., Silva, M., Bhardwaj, O., Yurochkin, M., and Maity, S. Carrot: cost aware rate optimal router, 2025. URL https://arxiv.org/abs/2502.03261. Song, Y., Mi, Z., Xie, H., and Chen, H. Powerinfer: Fast large language model serving with In Proceedings of the ACM consumer-grade GPU. SIGOPS 30th Symposium on Operating Systems Principles (SOSP 24). ACM, 2024. doi: 10.1145/3694715. 3695964. URL https://dl.acm.org/doi/10. 1145/3694715.3695964. Strubell, E., Ganesh, A., and McCallum, A. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 36453650, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1355. URL https: //aclanthology.org/P19-1355. Tambe, T., Hooper, C., Pentecost, L., Jia, T., Yang, E.-Y., Donato, M., Sanh, V., Whatmough, P. N., Rush, A. M., Brooks, D., and Wei, G.-Y. Edgebert: Sentence-level energy optimizations for latency-aware multi-task NLP inference. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture. ACM, 2021. doi: 10.1145/3466752.3480095. URL https://dl. acm.org/doi/10.1145/3466752.3480095. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., Rouillard, L., Mesnard, T., Cideron, G., bastien Grill, J., Ramos, S., Yvinec, E., Casbon, M., Pot, E., Penchev, I., Liu, G., Visin, F., Kenealy, K., Beyer, L., Zhai, X., Tsitsulin, A., Busa-Fekete, R., Feng, A., Sachdeva, N., Coleman, B., Gao, Y., Mustafa, B., Barr, I., Parisotto, E., Tian, D., Eyal, M., Cherry, C., Peter, J.-T., Sinopalnikov, D., Bhupatiraju, S., Agarwal, R., Kazemi, M., Malkin, D., Kumar, R., Vilar, D., Brusilovsky, I., Luo, J., Steiner, A., Friesen, A., Sharma, A., Sharma, A., Gilady, A. M., Goedeckemeyer, A., Saade, A., Feng, A., Kolesnikov, A., Bendebury, A., Abdagic, A., Vadi, A., Gyorgy, A., Pinto, A. S., Das, A., Bapna, A., Miech, A., Yang, A., Paterson, A., Shenoy, A., Chakrabarti, A., Piot, B., Wu, B., Shahriari, B., Petrini, B., Chen, C., Lan, C. L., Choquette-Choo, C. A., Carey, C., Brick, C., Deutsch, D., Eisenbud, D., Cattle, D., Cheng, D., Paparas, D., Sreepathihalli, D. S., Reid, D., Tran, D., Zelle, D., Noland, E., Huizenga, E., Kharitonov, E., Liu, F., Amirkhanyan, G., Cameron, G., Hashemi, H., Klimczak-Plucinska, H., Singh, H., Mehta, H., Lehri, H. T., Hazimeh, H., Ballantyne, I., Szpektor, I., Nardini, I., Pouget-Abadie, J., Chan, J., Stanton, J., Wieting, J., Lai, J., Orbay, J., Fernandez, J., Newlan, J., yeong Ji, J., Singh, J., Black, K., Yu, K., Hui, K., Vodrahalli, K., Greff, K., Qiu, L., Valentine, M., Coelho, M., Ritter, M., Hoffman, M., Watson, M., Chaturvedi, M., Moynihan, M., Ma, M., Babar, N., Noy, N., Byrd, N., Roy, N., Momchev, N., Chauhan, N., Sachdeva, N., Bunyan, O., Botarda, P., Caron, P., Rubenstein, P. K., Culliton, P., Schmid, P., Sessa, P. G., Xu, P., Stanczyk, P., Tafti, P., Shivanna, R., Wu, R., Pan, R., Rokni, R., Willoughby, R., Vallu, R., Mullins, R., Jerome, S., Smoot, S., Girgin, S., Iqbal, S., Reddy, S., Sheth, S., Poder, S., Bhatnagar, S., Panyam, S. R., Eiger, S., Zhang, S., Liu, T., Yacovone, T., Liechty, T., Kalra, U., Evci, U., Misra, V., Roseberry, V., Feinberg, V., Kolesnikov, V., Han, W., Kwon, W., Chen, X., Chow, Y., Zhu, Y., Wei, Z., Egyed, Z., Cotruta, V., Giang, M., Kirk, P., Rao, A., Black, K., Babar, N., Lo, J., Moreira, E., Martins, L. G., Sanseviero, O., Gonzalez, L., Gleicher, Z., Warkentin, T., Mirrokni, V., Senter, E., Collins, E., Barral, J., Ghahramani, Z., Hadsell, R., Matias, Y., Sculley, D., Petrov, S., Fiedel, N., Shazeer, N., Vinyals, O., Dean, J., Hassabis, D., Kavukcuoglu, K., Farabet, C., Buchatskaya, E., Alayrac, J.-B., Anil, R., Dmitry, Lepikhin, Borgeaud, S., Bachem, O., Joulin, A., Andreev, A., Hardin, C., Dadashi, R., and Hussenot, L. Gemma 3 technical report, 2025a. URL https://arxiv.org/abs/2503.19786. Team, P., Du, X., Yao, Y., Ma, K., Wang, B., Zheng, T., Zhu, K., Liu, M., Liang, Y., Jin, X., Wei, Z., Zheng, C., Deng, K., Gavin, S., Jia, S., Jiang, S., Liao, Y., Li, R., Li, Q., Li, S., Li, Y., Li, Y., Ma, D., Ni, Y., Que, H., Wang, Q., Wen, Z., Wu, S., Hsing, T., Xu, M., Yang, Z., Wang, Z. M., Zhou, J., Bai, Y., Bu, X., Cai, C., Chen, L., Chen, Y., Cheng, C., Cheng, T., Ding, K., Huang, S., Huang, Y., Li, Y., Li, Y., Li, Z., Liang, T., Lin, C., Lin, H., Ma, Y., Pang, T., Peng, Z., Peng, Z., Qi, Q., Qiu, S., Qu, X., Quan, S., Tan, Y., Wang, Z., Wang, C., Wang, H., Wang, Y., Wang, Y., Xu, J., Yang, K., Yuan, R., Yue, Y., Zhan, T., Zhang, C., Zhang, J., Zhang, X., Zhang, X., Zhang, Y., Zhao, Y., Zheng, X., Zhong, C., Gao, Y., Li, Z., Liu, D., Liu, Q., Liu, T., Ni, S., Peng, J., Qin, Y., Su, W., Wang, G., Wang, S., Yang, J., Yang, M., Cao, M., Yue, X., Zhang, Z., Zhou, W., Liu, J., Lin, Q., Huang, W., and Zhang, G. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025b. URL"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Xie, Z., Xu, Y., Xu, H., Liao, Y., and Yao, Z. novel hat-shaped device-cloud collaborative inference frameIn arXiv preprint work for large language models. arXiv:2503.18989, 2025. URL https://arxiv. org/abs/2503.18989. Xu, J., Pan, J., Zhou, Y., Chen, S., Li, J., Lian, Y., Wu, J., and Dai, G. Specee: Accelerating large language model inference with speculative early exiting. arXiv preprint arXiv:2504.08850, 2025. URL https: //arxiv.org/abs/2504.08850. Yan, C., Liu, S., Liu, H., Peng, X., Wang, X., Chen, F., Fu, L., and Mei, X. Hybrid sd: Edge-cloud collaborative inference for stable diffusion models, 2024. URL https://arxiv.org/abs/2408.06646. Yang, Z., Adamek, K., and Armour, W. Part-time power measurements: nvidia-smis lack of attention. arXiv preprint arXiv:2312.02741, 2023. You, J., Owen, D., Porter, D., and Wilson, T. Scaling intelligence: The exponential growth of ais power needs, 2025. URL https://www.epri.com/research/ products/000000003002033669. Yuan, W., Yu, J., Jiang, S., Padthe, K., Li, Y., Wang, D., Kulikov, I., Cho, K., Tian, Y., Weston, J. E., and Li, X. Naturalreasoning: Reasoning in the wild with 2.8m challenging questions, 2025. URL https://arxiv. org/abs/2502.13124. Zhang, Y. The avengers: simple recipe for uniting smaller language models to challenge proprietary giants, 2025a. Zhang, Y. Beyond gpt-5: Making llms cheaper and better via performance-efficiency optimized routing, 2025b. https://arxiv.org/abs/2502.14739. Team, Q. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Tschand, A., Rajan, A. T. R., Idgunji, S., Ghosh, A., Holleman, J., Kiraly, C., Ambalkar, P., Borkar, R., Chukka, R., Cockrell, T., Curtis, O., Fursin, G., Hodak, M., Kassa, H., Lokhmotov, A., Miskovic, D., Pan, Y., Manmathan, M. P., Raymond, L., St. John, T., Suresh, A., Taubitz, R., Zhan, S., Wasson, S., Kanter, D., and Reddi, V. J. MLPerf power: Benchmarking the energy efficiency of machine learning systems from µwatts to mwatts for susIn 2025 IEEE International Symposium tainable AI. on High-Performance Computer Architecture (HPCA), pp. 12011216, Las Vegas, NV, USA, 2025. IEEE. doi: 10.1109/HPCA61900.2025.00092. U.S. Bureau of Economic Analysis. GDP by industry, 2024. URL https://www.bea.gov/data/gdp/ gdp-industry. Wang, X., Chen, Z., Ren, J., Li, Y., Zhang, J., Sun, J., Mi, Y., et al. MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations, 2024a. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging multitask language understanding benchmark, 2024b. URL https://arxiv.org/abs/2406.01574. Wang, Y., Chen, Y., Li, Z., Kang, X., Fang, Y., Zhou, Y., Zheng, Y., Tang, Z., He, X., Guo, R., Wang, X., Wang, Q., Zhou, A. C., and Chu, X. BurstGPT: real-world workload dataset to optimize llm serving systems. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD 25), Toronto, ON, Canada, 2025. ACM. doi: https://doi.org/10. 1145/3711896.3737413. URL https://doi.org/ 10.1145/3711896.3737413. Wilkins, G., Keshav, S., and Mortier, R. Hybrid heterogeneous clusters can lower the energy consumption of llm inference workloads. In Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems, pp. 506513, 2024. Xiang, Y., Li, X., Qian, K., Yang, Y., Zhu, D., Yu, W., Zhai, E., Liu, X., Jin, X., and Zhou, J. Aegaeon: Effective gpu pooling for concurrent llm serving on the market. In Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles, SOSP 25, Seoul, Republic of Korea, 2025. ACM. doi: 10.1145/3731569.3764815."
        },
        {
            "title": "A EXTENDED RELATED WORKS",
            "content": "Below we provide an extended treatment of related works. LLM Routing central challenge in local-cloud routing systems is determining which model should handle given query so as to maximize efficiency. Prior work spans broad design space, but much of it can be organized around two families of approaches: embedding-based routers (Zhang, 2025a; Somerstep et al., 2025; Chen et al., 2024) and generative/decoderbased routers (Ong et al., 2025). Embedding-based methods rely on encoding queries (and sometimes models) into vector space and then applying similarity search or lightweight classification. Early work largely adopted binary routing, where queries are directed between just two models. For example, RouteLLM (Ong et al., 2024) demonstrated that simple supervised classification can yield up to 85% cost reduction while maintaining GPT-4level performance, but this setting was restricted to two-model scenarios. More recent systems generalize routing to multi-model settings: ensemble-style methods such as FrugalGPT (Chen et al., 2023), RouterDC (Chen et al., 2024), and Avengers Pro (Zhang, 2025a;b) show that intelligently combining smaller models can approximate or even surpass larger frontier LMs. Decoder-based methods leverage small language model to directly generate the routing decision. Causal LLM Routing, suggests that incorporating richer querymodel interaction signals via generative modeling or cross-attention can yield more robust routing than static embeddings (Chen et al., 2024). In this work, we are inspired by these novel approaches to routing, and evaluate their performance in the local-cloud routing setup. LLM Routing Benchmarks Recent work has explored benchmarks for LLM query routing, primarily targeting costquality tradeoffs across multiple models. RouterBench (Hu et al., 2024) provides comprehensive suite of curated academic tasks ( 405K samples) to evaluate routing policies along costquality Pareto frontiers. RouteLLM (Ong et al., 2024) introduces preference-trained routing framework evaluated on academic benchmarks like MMLU and MT-Bench, with focus on achieving quality under token cost constraints, though it remains limited to token-level metrics. RouterEval (Huang et al., 2025) emphasizes model selection accuracy at scale, compiling over 200M performance records across 8.5K models and 12 benchmarks to study generalization, yet lacks coverage of real-world queries. In contrast, our curated dataset targets routing under naturalistic conditions, leveraging 1M real user queries from WILDCHAT and NATURALREASONING. It uniquely supports the exploration of local-cloud routing tradeoffs beyond just cost and quality, to metrics such as latency, energy, memory, throughput, and more generated on local accelerators and enterprise-grade accelerators. Moreover, in contrast to existing benchmarkswhich provide stale performance records limited to models released prior to July 2024our curated dataset evaluates several state-of-the-art models, including Qwen3 (Team, 2025) and GPT-OSS (Agarwal et al., 2025), all released after May 2025. To support ongoing benchmarking, we release our efficiency profiling harness, hardware-agnostic toolkit for generating fresh telemetry and evaluation records as new models become available. LocalCloud Inference Systems Beyond model selection, recent work explores collaborative inference protocols that split generation between local and cloud models. Minions (Narayan et al., 2025) proposes two-stage protocol where small on-device LM handles lightweight processing and frontier LM performs high-level reasoning, with an extended version introducing task decomposition and aggregation for improved quality. Such collaborative schemes offer large energy and cost savings but require careful protocol design to avoid performance loss. parallel line of work centers on speculative decoding, where small draft model generates candidate continuations that are verified or refined by larger target LM (Miao et al., 2023; Xu et al., 2025). These approaches primarily target latency and throughput, particularly in constrained hardware settings, and typically assume that generation will ultimately invoke large LM. Other hybrid protocols like SLED (Li et al., 2025) and HAT (Xie et al., 2025) introduce edgecloud model partitioning with intermediate state exchange to balance device limitations with quality needs. While these systems explore fine-grained collaboration at the token or layer level, our work investigates the limitations of coarser-grained alternative: query-level routing across multiple small and large LMs, where we measure not only accuracy and cost, but also latency, memory, and energy across diverse hardware accelerators. Efficient AI We are inspired by work on Green AI which proposes treating energy as first-class metric alongside accuracy and cost, with calls for standardized reporting and tooling for reproducible accounting of power use and emissions during training and inference (Schwartz et al., 2020; Strubell et al., 2019; Patterson et al., 2021; Henderson et al., 2020; Anthony et al., 2020; CodeCarbon Contributors, 2024; Oviedo et al., 2025). Complementary to our focus on localcloud routing, cost and efficiency-driven model selection strategies such as FrugalML and FrugalGPT for API and model cascades,"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "and CALM for token-wise early exit, dynamically allocate workloads to cheaper or smaller models while preserving quality (Chen et al., 2020; 2023; Schuster et al., 2022). On-device and edge studies demonstrate algorithmhardware co-design for lower latency and energy consumption, exemplified by EdgeBERTs optimizations and PowerInfers efficient LLM serving on commodity GPUs (Tambe et al., 2021; Song et al., 2024). Finally, hardware-aware benchmarking efforts such as From Words to Watts and MLPerf Power quantify inference energy across accelerators and standardize power measurement protocols (Samsi et al., 2023b; Tschand et al., 2025)."
        },
        {
            "title": "B DATASET AND PROFILING HARNESS",
            "content": "In this section, we provide additional details on our dataset curation for our study of hybrid local-cloud LM systems. B.1 Dataset Curation Here, we provide additional details on the Anthropic Economic Index (Handa et al., 2025) categories used as labels (see Table 5), the hardware platforms profiled, and the metrics recorded in our curated dataset. Life, physical, and social science Architecture and engineering Installation, maintenance, and repair Legal services Arts, design, sports, entertainment, and media Farming, fishing, and forestry Food preparation and serving related Community and social service Office and administrative support Protective service Construction and extraction"
        },
        {
            "title": "Computer and mathematical\nEducation instruction and library\nBusiness and financial operations\nTransportation and material moving\nProduction services\nHealthcare support\nHealthcare practitioners and technical\nSales and related\nGeneral management\nBuilding grounds cleaning and maintenance\nPersonal care and service",
            "content": "Table 5. Anthropic Economic Index Categories (Handa et al., 2025). This taxonomy categorizes occupations into 22 standardized economic domains, adapted from U.S. Bureau of Labor Statistics frameworks. It is designed to support AI impact analysis by aligning labor categories with distinct task structures. Query Curation When sourcing queries from the WILDCHAT and NATURALREASONING datasets, we apply robust data cleaning and filtering to ensure the quality and consistency of the sampled queries. For NATURALREASONING, we filter out all queries that dont contain ground truth answers. For WILDCHAT, we eliminate non-English entries to maintain linguistic uniformity across the dataset. Queries that are malformed, nonsensical, or otherwise unintelligible (as determined by an LLM judgei.e., GPT-4O-MINI) are discarded to prevent noise. Additionally, duplicate queries are removed to reduce redundancy and avoid overrepresentation of specific prompts. Finally, we filter out excessively long queries that exceed 32,000-character limit. Dataset Statistics Table 6 reveals significant differences in how the two datasets are distributed across domains. WILDCHAT is dominated by Arts, design, sports, entertainment, and media queries (47.1%), followed by Computer and mathematical (18.1%), while NATURALREASONING is primarily composed of Life, physical, and social science (36.0%) and Computer and mathematical (34.8%) queries. We use GPT-4O-MINI to bucket each query into its economic categorization using the prompt below. 1 You are query categorizer. Your task is to categorize the following user query into one of the predefined categories based on the job/occupation domain it relates to most closely. 3 4 5 6 7 9 Query: \"{query}\" Available Categories: - Office and administrative support - Transportation and material moving - Sales and related - Food preparation and serving related 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "- General management - Business and financial operations - Healthcare practitioners and technical - Production services - Education instruction and library - Healthcare support - Construction and extraction - Installation, maintenance, and repair - Computer and mathematical - Building grounds cleaning and maintenance - Protective service - Personal care and service - Architecture and engineering - Community and social service - Arts, design, sports, entertainment, and media - Life, physical, and social science - Legal services - Farming, fishing, and forestry - None Instructions: 1. Read the query carefully 2. Determine which job/occupation category the query relates to most closely 3. If the query doesnt clearly relate to any specific occupation category, use \"None\" 4. Respond with ONLY the category name, exactly as listed above Category: Solvability rates vary dramatically by domain and dataset type, where querys solvability is defined as its ability to be answered correctly by any of the available local LMs (e.g. Qwen models or GPT OSS). WILDCHAT queries show consistently high solvability across most domains (generally > 94%), with particularly strong performance in creative and social domains. In contrast, NATURALREASONING exhibits more variable solvability, with technical domains like Architecture and engineering showing only 41.5% solvability compared to 99.4% for the same domain in WILDCHAT. This disparity reflects the complexity difference between open-ended chat queries and analytical reasoning tasks, supporting our findings that chat queries are more amenable to local model routing than reasoning-intensive queries. Figure 7. Local Win/Tie-Rate vs. Cloud LMs by Domain. Stacked bars show the fraction of single-turn chat and reasoning queries handled by local LMs (< 20B active parameters; blue) versus those routed to frontier models in the cloud (red), computed per economic index domain (Appel et al., 2025) Metrics We detail all the metrics collected via our profiling harness in Table 8. For our correctness evaluations on WILDCHAT, we use an LLM-as-a-judge approach to evaluate model generated answers against ground truth answer from QWEN3-235B. For our correctness evaluations of NATURALREASONING, we use another LLM-judge prompt, but compare"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Domain Computer and mathematical Arts, design, sports, entertainment, and media Life, physical, and social science None Education instruction and library Architecture and engineering Business and financial operations Healthcare practitioners and technical Office and administrative support Legal services Community and social service Transportation and material moving Sales and related Food preparation and serving related General management Installation, maintenance, and repair Farming, fishing, and forestry Protective service Construction and extraction Healthcare support Production services Personal care and service Building grounds cleaning and maintenance TOTAL WC Count 90,662 WC % 18.1 WC Solv % NR Count 99.5 174, NR % 34.8 NR Solv % 67.3 235,658 47.1 98.7 2, 0.5 52.9 28,079 49,014 23,196 5,782 19,628 8,905 6, 5,208 6,125 1,890 4,689 3,308 3,364 1,677 1,315 991 1,112 546 648 278 5.6 9.8 4.6 1.2 3.9 1.8 1. 1.0 1.2 0.4 0.9 0.7 0.7 0. 0.3 0.3 0.2 0.2 0.1 0.1 0.1 98.8 180,065 36.0 97.3 97.2 98.9 97.8 98. 91.6 98.6 97.0 95.1 97.8 98.3 96. 97.1 99.5 97.9 97.2 97.5 100.0 92.9 100.0 79,752 13,864 28,762 8,779 16.0 2.8 5.8 1.8 1, 0.4 25 1,349 404 3,914 218 340 2,037 411 237 147 12 334 19 70 0.0 0.3 0. 0.8 0.0 0.1 0.1 0.4 0.1 0.0 0.0 0.0 0.1 0. 0.0 500K 100.0 98.4 500K 100.0 60.5 65.6 80.4 40.8 55. 66.3 48.0 69.0 76.2 53.0 67.4 61. 67.9 56.2 65.5 56.5 60.5 100.0 65.3 0.0 72.9 63.0 Table 6. Dataset Domain Composition and LM Coverage ( 20B Active Parameter Models). Comparison of domain distribution and model solvability rates across WILDCHAT (WC) and NATURALREASONING (NR) datasets. Solvability indicates the percentage of problems that can be solved correctly by at least one model with 20B active parameters. against ground truth answers provided in the original dataset. We provide both LLM-judge prompts below. The LLM used for each respective evaluation is GPT-4O. For SUPERGPQA and MMLU PRO, we simply compare the multiple choice answer selected in the response to the multiple choice answer of the reference response. WILDCHAT LLM-judge Prompt 1 2 You are an impartial judge evaluating the quality of two AI-assistant replies to the same user prompt. 3 4 Step 1: Generate your own answer 5 Write the response *you* would give to the user. Keep it separate from later analysis. 6 7 Step 2: Decide the query type 8 Classify the user prompt as either 9 - **Subjective / open-ended** (creative writing, opinion, advice, brainstorming) 10 - **Objective / technical** (code, math, logical derivations with single correct outcome ) 11 If uncertain, default to \"Subjective\". 12 13 Step 3 - Score each assistant with the correct rubric 14 15 Query type Criteria 16 ---------------------- 17 Subjective / open-ended 1. Correctness / factual soundness 2. Helpfulness 3. Relevance 4. Conciseness 5. Creativity & novelty 18 Objective / technical 19 20 When using the multi-criteria rubric, note strengths and weaknesses for **each** dimension 1. Correctness only . 21 When using the single-criterion rubric, focus exclusively on factual / functional accuracy"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Category WILDCHAT MMLU PRO SUPERGPQA Average Computer and mathematical Life, physical, and social science Sales and related Business and financial operations Production services Office and administrative support Healthcare practitioners and technical Installation, maintenance, and repair Architecture and engineering Protective service Education instruction and library Farming, fishing, and forestry None General management Transportation and material moving Construction and extraction Food preparation and serving related Community and social service Arts, design, sports, entertainment, and media Building grounds cleaning and maintenance Legal services Healthcare support Personal care and service 93.4% 91.1% 86.8% 89.3% 89.7% 88.5% 88.9% 90.4% 90.0% 88.6% 90.6% 87.2% 86.5% 88.2% 91.1% 91.0% 86.1% 87.4% 84.8% 90.1% 87.1% 86.5% 89.4% 90.6% 84.7% 74.2% 82.9% 85.7% 83.3% 78.8% 80.0% 73.2% 75.0% 77.4% 77.8% 77.4% 76.3% 66.7% 66.7% 82.6% 64.1% 75.5% 71.4% 61.5% 60.0% 50.0% 72.8% 50.4% 64.3% 52.5% 48.8% 44.8% 48.0% 44.4% 51.6% 47.6% 43.2% 45.9% 42.4% 41.6% 44.9% 41.4% 30.3% 45.5% 36.2% 30.8% 43.6% 38.9% 25.0% 85.6% 75.4% 75.1% 74.9% 74.7% 72.2% 71.9% 71.6% 71.6% 70.4% 70.4% 70.3% 68.8% 68.7% 67.5% 66.4% 66.3% 65.7% 65.5% 64.1% 64.1% 61.8% 54.8% Table 7. GPT-OSS-120B Performance across Datasets. Performance metrics across WILDCHAT, NATURALREASONING, MMLU PRO, and SUPERGPQA benchmarks, organized by Anthropic Economic Index categories (Handa et al., 2025). and ignore style or flair. 22 23 Step 4: Compare & justify 24 Explain which assistant is better and why, correcting any mistakes you find. Highlight missing but important details. **Be concise.**"
        },
        {
            "title": "Verdict",
            "content": "25 26 Step 5: 27 1. Assistant is significantly better: [[A>>B]] 28 2. Assistant is slightly better: [[A>B]] 29 3. Tie, Assistant is equal: [[A=B]] 30 4. Assistant is slightly better: [[B>A]] 31 5. Assistant is significantly better: [[B>>A]] 32 33 Choose exactly one token from: [[A>>B]], [[A>B]], [[A=B]], [[B>A]], [[B>>A]]. 34 35 --- 36 37 ### Output format (strict) 38 Return **only** JSON object that matches the provided schema: NATURALREASONING LLM-judge Prompt 1 You are evaluating response to scientific/technical question against reference answer. 2 3 Your task is to determine if the response is factually correct and complete compared to the reference. 4 5 Consider: 6 1. Scientific accuracy of facts and concepts 7 2. Mathematical correctness (if applicable) 8 3. Completeness of the answer 9 4. Technical precision 10 11 Question: {question} 12 13 Response: {response} 14 15 Reference Answer: {reference}"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "16 17 Return ONLY True if the response is correct and complete, False otherwise. Description FLOPs per query. MACs per query; proxy for compute. Energy per query (J). Total energy across queries. Latency per token (ms). Token output rate (toks/s). Time to first token (s). Total time per query (s). CPU memory usage (MB). GPU memory usage (MB). Model load time (s). Query batch size. GPU memory use (01). Max token length allowed. Max batch token count. Max output tokens. Number of threads. Sampling temperature. Top-k cutoff. Top-p (nucleus) threshold. Warm-up steps. Metric flops per request macs per request per query joules total joules per token ms throughput tokens per sec time to first token seconds total query seconds cpu mb.avg / max / median / min gpu mb.avg / max / median / min initialization duration seconds batch size gpu memory utilization max model len max num batched tokens max output tokens num workers temperature top top warmup steps per query watts.avg / max / median / min GPU power draw per query (W). total watts.avg / max / median / min cpu count cpu brand host name os name / os version / kernel version temperature.avg / max / median / min input output Session GPU power draw (W). CPU core count. CPU model. Machine hostname. OS and kernel info. Device temperature (C). Input tokens per query. Output tokens per query. Table 8. Dataset Metrics. Summary of compute, latency, memory, and energy profiling metrics. Hardware Backends Details regarding profiled hardware can be found in Table 9. Data Generation Procedure We generate model outputs using consistent decoding settings across all tasks: temperature = 0.6, top-p = 0.95, top-k = 20, min-p = 0.0, and 32768-token output limit. For NATURALREASONING, SUPERGPQA and GPQA queries, we enable deliberative prompting (use thinking = True); for WILDCHAT, we disable it. For QWEN models, we apply repetition penalty of 1.1 and length penalty of 1.0. Telemetry Collection We collected telemetry by instrumenting host-level samplers that interface directly with vendorsupported system APIs on each platform. Data were obtained from NVML on NVIDIA-equipped hosts, from the powermetrics facility on macOS, and from ROCm SMI on AMD-equipped hosts. Each sampler queried the respective system interface to obtain GPUand system-level measurements and produced synchronized records suitable for downstream quantitative analysis. On NVIDIA systems, we interface directly with NVML and enumerate all visible GPUs. For each device, we query instantaneous power as reported by the driver, read cumulative energy from the on-device counter, obtain GPU temperature from the hardware sensor, and retrieve memory usage from the devices memory interface. Units are normalized (e.g., milliwatts and millijoules mapped to watts and joules; bytes to megabytes). In multi-GPU hosts, power and memory are"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Hardware NVIDIA A100 40 GB SXM4 (Ampere) NVIDIA H200 SXM (Hopper) NVIDIA B200 (Blackwell) NVIDIA GH200 (Grace Hopper) NVIDIA Quadro RTX 6000 (Turing) NVIDIA RTX 6000 Ada Generation AMD Instinct MI300X (CDNA 3, OAM) Apple Mac Studio (M4 Max) SambaNova SN40L RDU Bandwidth 1,555 GB/s 4.8 TB/s 8 TB/s Memory 40 GB HBM2 141 GB HBM3e 192 GB HBM3e 144 GB HBM3e + 624 GB LPDDR5X 4.8 TB/s (GPU) 24 GB GDDR6 48 GB GDDR6 192 GB HBM3 128 GB unified 64 GB HBM2E"
        },
        {
            "title": "672 GB/s\n960 GB/s\n5.3 TB/s (peak)\n546 GB/s\n1.6 TB/s",
            "content": "Power 400 TDP Up to 700 TDP 1000 TDP 1000 TDP 295 TDP 300 TDP 750 TBP 480 (continuous) 500 TDP Table 9. Accelerator Details. Memory, bandwidth, and power specifications of evaluated accelerators and systems. Cost Savings Compute Savings Energy Savings Size Threshold () Qwen + GPT-OSS 4B 8B 14B 20B 32B 120B 65.2% 80.8% 89.0% 90.5% 91.3% 91.1% Qwen 65.2% 80.8% 89.0% 91.9% Qwen + GPT-OSS 65.1% 83.1% 93.0% 97.4% 97.4% 97.7% Qwen 65.1% 83.1% 93.0% 92.8% Qwen + GPT-OSS 63.5% 79.6% 87.0% 89.4% 90.4% 90.7% Qwen 63.5% 79.6% 87.0% 90.5% Table 10. Cost, Compute, and Energy Savings from Local-Cloud Routing on WildChat: Savings across different resources while maintaining task accuracy of SOTA open-source cloud model (i.e. Qwen3 235B-A22B). summed across devices and temperature is averaged to yield single aggregate view. Each record also includes host memory usage from OS counters, nanosecond timestamp, and device identity and backend provenance. On macOS, we execute powermetrics with elevated privileges and ingest its continuous plist stream. Each plist frame is parsed to extract the GPU power value exposed by the system (Apple Silicon: processor power.actual; Intel: processor.combined power), which is normalized to watts. Energy (joules) is obtained by numerically integrating the power signal over successive frames using the measured inter-frame wall-clock interval. In parallel, system memory usage is sampled from OS counters. Every observation is timestamped and annotated with Apple device identity and an explicit powermetrics backend tag. On AMD systems, we use ROCm SMI to query current GPU power (watts), read temperature from junction or edge sensors (C), and obtain VRAM usage from the device memory interface (bytes to megabytes). Energy (joules) is computed by integrating the power signal over time using consecutive sampling intervals. System memory usage is read from OS counters. In multi-GPU machines, the primary device under observation is explicitly selected (GPU index 0 in our setup), and all records carry precise timestamps together with device identity and backend metadata. To ensure measurement precision and account for variance in inference-time behavior, we execute each query 10 times and aggregate power measurements across runs. For each query, we compute the mean power draw (watts) and mean energy consumption (joules) per query by averaging across these 10 independent executions. This repeated sampling approach reduces measurement noise and provides robust estimates of per-query resource consumption that account for system-level variability in accelerator utilization, thermal conditions, and memory allocation patterns. LOCAL-CLOUD EXPERIMENTS C.1 How has local LM task coverage changed over different difficulty slices of the data Using labels for query difficulty we quantify the rate of improvement of local LMs across task difficulty slices. We label each query by the minimum model size (in parameters) required to solve it when considering the SOTA LMs as of August 2025, categorizing queries into five difficulty levels: level 1 ( 4B params), level 2 ( 8B params), level 3 ( 20B params), level 4 ( 235B params), and level 5 (unsolvable). For chat tasks (see Figure 8), we observe near-universal performance gains across all difficulty levels, with 2025 models"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Figure 8. Chat Task Performance by Difficulty Level and Year. Model success rates across four difficulty levels and three model generations (2023, 2024, 2025). The data reveals dramatic progress across all difficulty levels, with 2023 models achieving 28.79% overall success rising to 98.12% by 2025. Notably, Levels 1-3 approach near-perfect performance (98-99%), while Level 4 shows the largest relative improvement (+210.4% per year) despite starting from the lowest baseline (17.77%). Figure 9. Reasoning Task Performance by Difficulty Level and Year. Model success rates on across five difficulty levels and three model generations. The benchmark shows three-tier saturation pattern: near-complete (98-99% on Levels 1-2), approaching saturation (85-92% on Levels 3-4), and wide-open frontier (51% on Level 5)."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Cost Savings Compute Savings Energy Savings Size Threshold () Qwen + GPT-OSS 4B 8B 14B 20B 32B 120B 52.9% 60.5% 68.7% 73.3% 76.9% 86.7% Qwen 52.9% 60.5% 68.7% 75.9% Qwen + GPT-OSS 54.5% 62.5% 70.1% 72.2% 75.1% 86.0% Qwen 54.5% 62.5% 70.1% 75.1% 86.0% Qwen + GPT-OSS 46.3% 54.0% 62.5% 67.8% 72.4% 85.9% Qwen 46.3% 54.0% 62.5% 71.6% Table 11. Cost, Compute, and Energy Savings from Local-Cloud Routing on NaturalReasoning: Savings across different resources while maintaining task accuracy of SOTA open-source cloud model (i.e. Qwen3 235B-A22B). Figure 10. Perplexity and Accuracy per Joule Trends across WILDCHAT and NATURALREASONING. achieving 98-99% success on levels 1-3 and 92.6% on level 4. Absolute improvements range from +55.4 percentage points (pp) for level 1 to +76.4 pp for level 3, indicating relatively uniform capability gains. For reasoning tasks (see Figure 9, the pattern differs substantially. While levels 1-3 show strong improvements (+24.0, +37.8, and +53.9 pp respectively), levels 4 and 5 exhibit markedly slower progress. Level 4 improves by only +23.8 pp (7.93% to 31.72%), and level 5 remains largely unsolved with just +1.5 pp improvement (3.27% to 4.72%). This suggests that while local models have rapidly closed the gap on moderately difficult reasoning tasks, the hardest reasoning problemsthose requiring either massive scale or capabilities beyond current architecturesremain significant frontier. The presence of 134 level 5 problems (16.5% of the reasoning dataset) that remain 95% unsolved indicates substantial headroom for future model development in complex reasoning domains. C.2 How much has intelligence efficiency changed over time? Figure 10 presents results across single-turn chat and reasoning queries, evaluating intelligence efficiency across modelhardware pairs from April 2024 through August 2025. We measure both perplexity (left panel) and accuracy (right panel) normalized by energy consumption in joules per query, tracking nine distinct model families (LLAMA, PHI, GEMMA, MISTRAL, FALCON, DEEPSEEK, QWEN, and GPT-OSS) deployed on various GPU configurations including NVIDIA A100 (40GB/80GB PCIe/SXM), H100 (80GB SXM), H200 (141GB HBM3e), and L40S (48GB) accelerators. Energy measurements capture end-to-end inference costs. The temporal snapshots at April 2024, August 2024, and August 2025 enable direct comparison of efficiency trajectories, revealing how successive generations of models and hardware migrate from suboptimal regions (high energy, low performance) toward optimal regions (low energy, high performance) as indicated by the shaded quadrants in each panel. C.3 What efficiency gains can effective query routing deliver? We compute cost per query using pricing available on OpenRouterAI (OpenRouter, 2025). Table 12 lists the token pricing used."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Model Input Cost (USD / 1M tokens) Output Cost (USD / 1M tokens) Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B Qwen3-235B GPT-OSS-20B GPT-OSS-120B 0.000 0.035 0.060 0.100 0.220 0.03 0.15 0.000 0.138 0.124 0.450 0.880 0.14 0. Table 12. Model pricing from OpenRouterAI. Costs are in USD per 1M tokens for input and output, as of August 2025. C.4 How does model precision affect performance and efficiency? Model quantizationreducing numerical precision from FP16 to FP8 or FP4decreases memory requirements and energy consumption during inference while introducing approximation error that may degrade model accuracy. To quantify this tradeoff, we evaluate eight open-source models from the QWEN3 and GEMMA families across three precision levels: FP16 (full precision), FP8 (8-bit floating point), and FP4 (4-bit floating point). For each model-precision pair, we measure accuracy on three reasoning-focused datasets: NATURALREASONING (N = 10, 000), SuperGPQA (N = 10, 000), and MMLU Pro (N = 10, 000). Figure 11 shows that quantization from FP16 to FP4 yields energy reductions of 3 3.5 with accuracy degradation of approximately 2.5 percentage points per precision step across all models and datasets. For example, on SuperGPQA, QWEN3-14B achieves 54.5% (FP16), 52.0% (FP8), and 49.0% (FP4)a total degradation of 5.5 percentage points despite 3.23 reduction in energy consumption. Larger models maintain their relative performance advantage even at lower precision: QWEN3-14B at FP4 (49.0% accuracy) outperforms QWEN3-4B at FP16 (48.5% accuracy) on SuperGPQA, indicating that model scale matters more than precision for reasoning tasks. These results demonstrate that FP8 and FP4 quantization enable practical deployment of local models with predictable performance tradeoffs, allowing system designers to select precision levels based on application-specific requirements while capturing most of the energy savings identified in Section 5.3. C.5 How do SOTA open-source LMs compare to the SOTA closed-source LMs on Chat and Reasoning Queries? To evaluate the competitiveness of open-source models for local deployment, we compare the performance of state-of-the-art open-source models against leading closed-source models across single-turn chat and reasoning queries. We evaluate three closed-source frontier modelsGPT-5-2025-08-07, GEMINI-2.5-PRO, and CLAUDE-SONNET-4-5against eight open-source models ranging from QWEN3-8B to QWEN3-235B-A22B, measuring performance on NATURALREASONING, MMLU Pro, and SuperGPQA. Table 13 shows that the best open-source model (QWEN3-235B-A22B) achieves 71.8% average accuracy across benchmarks, trailing the best closed-source model (GPT-5-2025-08-07, 77.9%) by 6.1 percentage points. Performance gaps vary substantially by task type, as shown in Table 14. On MMLU Pro and SuperGPQA, open-source models nearly match closed-source performance: QWEN3-235B-A22B achieves 82.3% versus 87.4% on MMLU Pro (5.1% gap) and 63.1% versus 66.5% on SuperGPQA (3.4% gap). However, on NATURALREASONING, the gap widens to 12.9% (70.0% versus 82.9%), indicating that closed-source models maintain substantial advantage on naturalistic reasoning tasks. Local models with deployment constraints ( 20B active parameters) face larger gaps: the best local model (QWEN3-14B) trails closed-source models by 11.813.2% across benchmarks, with the smallest gap on MMLU Pro (11.8%) and the largest on NATURALREASONING (13.2%). These results demonstrate that while open-source models at frontier scale (235B parameters) approach closed-source performance on knowledge and reasoning benchmarks, practical local deployment using smaller models (14B parameters) requires accepting 1113% accuracy degradation relative to closed-source alternatives. C.6 How does performance on chat and reasoning queries connect to U.S. GDP? To assess the economic relevance of local model performance improvements, we compute GDP-weighted accuracy for each model by weighting its performance on each economic category by that sectors contribution to the 2024 U.S. GDP of $29.18 trillion (U.S. Bureau of Economic Analysis, 2024). This metric attempts to quantify what proportion of economic value is relevant and addressable by local LMs, given the local model performances across single-turn chat and reasoning queries. Figures 12 and 13 shows that model improvements translate directly into expanded GDP coverage: on SuperGPQA,"
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Figure 11. Minimal Accuracy Degradation Shifting from FP16 to FP4 for Open-Source Local Models: Evaluation across three reasoning datasets (N = 10, 000 each) shows 2 3% accuracy loss per precision step, demonstrating that 8/F 4 quantization enables efficient deployment with acceptable performance tradeoffs. QWEN3-235B-A22B achieves 59.2% accuracy covering $9.3T in relevant GDP (31.9% of total U.S. GDP), while on MMLU Pro, it reaches 84.5% accuracy covering $7.6T (26.0% of total U.S. GDP). The strong positive correlation between model accuracy and GDP coverage across both benchmarks demonstrates that scaling model capabilities systematically expands the set of economically valuable tasks that can be automated. Task type substantially affects GDP coverage: chat tasks in WILDCHAT show the highest coverage with GPT-OSS-120B reaching 89.2% accuracy and covering $20.3T (69.6% of U.S. GDP), while reasoning tasks in NATURALREASONING show lower coverage with QWEN3-235B-A22 achieving 69.3% accuracy but only covering $6.8T (23.3% of U.S. GDP). This disparity reveals that current models excel at creative and conversational tasks that dominate economic activity, but struggle with technical reasoning tasks concentrated in specialized sectors like architecture, engineering, and physical sciences. The gap between chat coverage (69.6%) and reasoning coverage (23.3%) represents both limitation of open-source local models and an economic opportunity: improving reasoning capabilities could unlock an additional $13.5T in GDP-relevant tasks, suggesting that advances in technical reasoning would have substantial economic impact beyond current model capabilities."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Model GPT-5-2025-08-07 GEMINI-2.5-PRO CLAUDE-SONNET-4-5 QWEN/QWEN3-235B-A22B (BEST OSS) QWEN/QWEN3-32B OPENAI/GPT-OSS-120B QWEN/QWEN3-30B-A3B QWEN/QWEN3-14B OPENAI/GPT-OSS-20B QWEN/QWEN3-8B Type WildChat NaturalReasoning MMLU Pro SuperGPQA Average Closed-Source Closed-Source Closed-Source Open-Source Open-Source Open-Source Open-Source Open-Source Open-Source Open-Source 81.9% 89.5% 88.1% N/A 76.1% 89.2% 47.3% 48.9% 77.3% 50.2% 82.9% 77.9% 76.9% 70.0% 69.7% 65.0% 64.3% 60.0% 67.3% 57.9% 86.5% 87.4% 86.4% 82.3% 77.9% 78.3% 76.9% 75.6% 73.4% 73.3% 64.4% 66.5% 60.1% 63.1% 56.5% 55.3% 57.4% 56.2% 48.9% 51.8% 78.9% 80.3% 77.9% 71.8% 70.1% 72.0% 61.5% 60.2% 66.7% 58.3% Table 13. Model performance comparison across benchmarks. Scores represent performance on WILDCHAT, NATURALREASONING, MMLU PRO, and SUPERGPQA benchmarks. Qwen3-235B-A22B is used as the reference model for WILDCHAT evaluation. Metric Closed Best Best Closed Model Open Best Best Open Model Gap WildChat NaturalReasoning MMLU Pro 89.5% GEMINI-2.5-PRO 82.9% GPT-5-2025-08-07 87.4% GEMINI-2.5-PRO SuperGPQA 66.5% GEMINI-2.5-PRO 89.2%* OPENAI/GPT-OSS-120B 0.3% 70.0% QWEN/QWEN3-235B-A22B 12.9% 82.3% QWEN/QWEN3-235B-A22B 5.1% 63.1% QWEN/QWEN3-235B-A22B 3.4% Local Best ( 20B Active) Best Local Model Local Gap 89.2% OPENAI/GPT-OSS-120B 0.3% 67.3% OPENAI/GPT-OSS-20B 5.6% 80.3% OPENAI/GPT-OSS-120B 7.1% 50.5% QWEN/QWEN3-14B 16.0% Table 14. Closed-Source vs. Open-Source Performance Gap by Task. Comparison between closed-source, open-source (all sizes), and local ( 20B active parameters) models. *Excludes Qwen3-235B-A22B (reference model for WildChat). Figure 12. Open-Source Local LMs Performance vs. U.S. GDP - WildChat and Natural Reasoning: Model accuracy on WildChat and Natural Reasoning benchmarks plotted against relevant GDP in trillions of dollars. Both benchmarks show continued performance improvements as training compute scales across models from Qwen3B-4B to Qwen3B-A22B-235B. For our calculations, we compute the weighted sum of an LMs accuracy on each U.S. Labor category vs. the U.S. GDP associated with that category."
        },
        {
            "title": "Measuring Intelligence Efficiency of Local AI",
            "content": "Figure 13. Open-Source Local LMs Performance vs. U.S. GDP - SuperGPQA and MMLU Pro: Model accuracy on SuperGPQA and MMLU Pro benchmarks plotted against relevant GDP in trillions of dollars. Both benchmarks show continued performance improvements as training compute scales across models from Qwen3B-4B to Qwen3B-A22B-235B. For our calculations, we compute the weighted sum of an LMs accuracy on each U.S. Labor category vs. the U.S. GDP associated with that category."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}