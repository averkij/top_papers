{
    "paper_title": "Training Video Foundation Models with NVIDIA NeMo",
    "authors": [
        "Zeeshan Patel",
        "Ethan He",
        "Parth Mannan",
        "Xiaowei Ren",
        "Ryan Wolf",
        "Niket Agarwal",
        "Jacob Huffman",
        "Zhuoyao Wang",
        "Carl Wang",
        "Jack Chang",
        "Yan Bai",
        "Tommy Huang",
        "Linnan Wang",
        "Sahil Jain",
        "Shanmugam Ramasamy",
        "Joseph Jennings",
        "Ekaterina Sirazitdinova",
        "Oleg Sudakov",
        "Mingyuan Ma",
        "Bobby Chen",
        "Forrest Lin",
        "Hao Wang",
        "Vasanth Rao Naik Sabavat",
        "Sriharsha Niverty",
        "Rong Ou",
        "Pallab Bhattacharya",
        "David Page",
        "Nima Tajbakhsh",
        "Ashwath Aithal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Foundation Models (VFMs) have recently been used to simulate the real world to train physical AI systems and develop creative visual experiences. However, there are significant challenges in training large-scale, high quality VFMs that can generate high-quality videos. We present a scalable, open-source VFM training pipeline with NVIDIA NeMo, providing accelerated video dataset curation, multimodal data loading, and parallelized video diffusion model training and inference. We also provide a comprehensive performance analysis highlighting best practices for efficient VFM training and inference."
        },
        {
            "title": "Start",
            "content": "2025-3-18 NVIDIA"
        },
        {
            "title": "Abstract",
            "content": "Video Foundation Models (VFMs) have recently been used to simulate the real world to train physical AI systems and develop creative visual experiences. However, there are significant challenges in training large-scale, high quality VFMs that can generate high-quality videos. We present scalable, open-source VFM training pipeline with NVIDIA NeMo, providing accelerated video dataset curation, multimodal dataloading, and parallelized video diffusion model training and inference. We also provide comprehensive performance analysis highlighting best practices for efficient VFM training and inference. 1. Introduction The evolution of generative AI has moved beyond text-based models with new multimodal models. These models can now handle tasks like image generation, captioning, and visual question-answering, signaling progression towards AI systems that can interact with humans across various modalities. Recently, the focus has shifted from text and images to video, unlocking new potential applications across various industries. Video Foundation Models (VFMs) will revolutionize several industries including robotics, autonomous vehicles, and entertainment. Building VFMs comes with unique challenges due to the scale and diversity of video data. Training VFMs requires scalable training pipelines that can ingest large amounts of data and train model capable of understanding temporal and spatial dynamics to simulate the world. In this paper, we present scalable framework for VFM training with NVIDIA NeMo. Our end-to-end training framework allows users to create or fine-tune their own VFMs. We provide high-throughput data curation with NeMo Curator, efficient multimodal data loading functionality through Megatron Energon, scalable diffusion model training leveraging Megatron Core, and parallelized video generation pipeline in NeMo. 5 2 0 2 M 7 1 ] . [ 1 4 6 9 2 1 . 3 0 5 2 : r Figure 1: VFM Training Stack. NeMo provides an end-to-end stack for training video foundation models, leveraging NeMo Curator for video curation, Megatron Core for scaling transformer models, and the NeMo Framework for pre-training, fine-tuning, and accelerated inference. 1A detailed list of contributors and acknowledgments can be found in Section of this paper. 2025 NVIDIA. All rights reserved. Training Video Foundation Models with NVIDIA NeMo 2. High-Throughput Video Curation with NeMo Curator Training VFMs effectively requires internet-scale high-quality video data. NeMo Curator allows users to efficiently clip, annotate and filter 100PB+ of videos. To accomplish this, NeMo Curator provides two modular pipelines that can ingest raw videos and output high quality VFM training / fine-tuning datasets. Each pipeline is composed of sequence of stages, where each stage performs step in the curation process. The modularity of these pipelines allows users to customize different parts of the data curation process with custom models. Figure 2: Video Curation Pipeline. The video curation pipeline clips and processes large amounts of raw video. Then, the clips are sharded and stored on the cloud in the Webdataset format. 2.1. Clipping Pipeline The clipping pipeline takes as input raw, uncurated videos and outputs short, continuous (no jump-cuts or other transitions) clips with associated metadata. It uses an aggressive method of splitting clips, analyzing the color changes between frames, which is smoothed out by computing the similarity between image embeddings of adjacent clips to potentially merge them back together. These clips are then transcoded to the high-quality video encoding (H264), and they are annotated with video embeddings to facilitate semantic search capabilities. We can create synthetic captions from VLM when no caption for the video has been provided, which has been shown to improve the quality of the downstream model over pre-written captions (Sharifzadeh et al., 2024). After this pipeline, it is recommended that users manually inspect the generated captions to verify their quality and make any alterations as they see fit. 2.2. Sharding Pipeline Sharding generates text embeddings for captions and creates the final WebDataset used for training. WebDataset divides large datasets into smaller, manageable shards. This division facilitates parallel data access and processing, allowing multiple GPUs to handle different shards concurrently. This is especially necessary when training VFM on thousands of GPUs with petabyte-scale multimodal data. The sharding pipeline splits the data into different POSIX tar archive files. During training, these tar files can be accessed with purely sequential read operations, which can signifcantly boost I/O performance from cloud storage solutions. 2.3. Pipeline Optimizations The pipelines in NeMo Curator are built to scale to hundreds of nodes while running at maximum throughput. To perform effectively at scale, we use GPU acceleration in all stages of our pipelines and leverage Ray (Moritz 2 Training Video Foundation Models with NVIDIA NeMo et al., 2018) to automatically scale the number of workers per stage to balance the throughput. Beyond running the model inference on GPUs, we found that utilizing the hardware video decoder (NVDEC) and hardware video encoder (NVENC) on NVIDIA GPUs brought 3x speedup in the decoding and transcoding stages. Since each of the stages has different throughput, it is easy to be limited by the stage with the lowest throughput. For example, in the clipping pipeline, we use different models for generating video embeddings and video captions. Video captions are generated with vision-language models that can have several billions of parameters. On the other hand, video embeddings are generated with models much smaller, around few hundred million parameters in size. In this case, the video captioning model has much lower throughput than the video embedding model and would be rate-limiting stage in the pipeline. Performing inference with these models without wasting GPU cycles requires system to balance the workloads automatically. To solve this, we built an auto-balancing system as shown in Figure 3 that will deploy an optimal number of workers for each stage in the pipeline. This causes lower throughput stages to end up with more workers to keep pace with the higher throughput stages. As seen in Figure 3, we can achieve significant speedups across the overall pipeline by leveraging our auto-balancing system along with Ray for streaming distributed execution across GPU workers. Figure 3: Auto-Balanced Curation Pipeline. Certain curation stages can be rate-limiting the throughput of the entire curation pipeline. We created an auto-balancing system to match the throughput of the overall pipeline by allocating the optimal number of workers depending on the curation stage. NeMo Curator integrates the clipping, sharding, and auto-balancing systems together in single platfrom, enabling users to scale their video curation efforts. These systems are essential to training high-quality video foundation models as the quality of the model is directly dependent on the training data. NeMo Curator streamlines the process of experimenting with various data curation strategies, which accelerates the VFM research and development process. 3 Training Video Foundation Models with NVIDIA NeMo 3. Efficient Multimodal Dataloading with Megatron Energon VFMs are typically trained with ùí™(1B) images and ùí™(100M) videos. An efficient dataloading strategy is necessary for high throughput during training. Our VFM training framework takes advantage of the efficient multimodal dataloading capabilities of Megatron Energon to achieve this. In this section, we discuss the core Megatron Energon features that enable reliable and scalable VFM training in NeMo. 3.1. Dataset Preparation and Blending Preparing and storing large-scale data for VFM training brings many challenges. Assuming tokenizer that compresses the temporal and spatial dimensions by factor of 8 and fixed image/video resolution and length, one would need ùí™(100TB) of storage. Storing this much data on most compute clusters is typically infeasible, so we use cloud storage solutions like AWS S3 to store our training datasets. Energon uses the WebDataset format to pull dataset shards from cloud storage. Energon also enables blending multiple data sources together, allowing users to experiment and find the optimal data blend for their VFM. One of the core applications of dataset blending is mixed image-video training with sequence packing. Our datasets contain both images and videos with variable sequence lengths. Typically, we can use the SBHD attention format and group sequences with the same length together in the same training batch. However, this requires that we split our video model training into different stages, introducing complex dataloading logic into our workflow. Sequence packing is technique that concatenates multiple training samples along the sequence dimension. This method eliminates the need for excessive padding. Since more tokens are processed in each micro batch, GPU compute and GPU memory utilization are significantly increased. This is especially helpful for VFM training since it allows packing images and videos with varied length and resolution in the same micro batch as shown in Figure 4. Although sequence packing limits the micro batch size to 1, it helps minimize compute wastage and removes the need for complicated logic, which significantly simplifies the dataloading process. Figure 4: Mixed Resolution Image-Video Training. We utilize sequence packing with padding to enable joint training of images and videos with different resolutions and video length. 3.2. Optimized Network Usage Storing large-scale video data on cloud services like AWS S3 often strains network bandwidth during distributed training, since each model-parallel rank typically downloads the same data shards to keep their dataloaders identical. This issue intensifies as we scale to thousands of GPUs, reducing overall training throughput. Our solution is to assign unique data shard to each rank and then use an all-gather operation to distribute these shards across ranks. This strategy maintains identical dataloaders for each rank while cutting down on redundant downloads, leading to higher training throughput in low-bandwidth environmentseven with the added communication overhead. 4 Training Video Foundation Models with NVIDIA NeMo 4. Scaling Video Foundation Model Training NeMo provides user-friendly interface to scale video diffusion model training. Video foundation models can be trained with autoregressive next-token prediction or full sequence diffusion objectives. NeMos well-established suite of tools on large language models (LLMs) can be reused for autoregressive models. In this section, we discuss our new scalable diffusion training capabilities that includes implementations of state-of-the-art diffusion transformers such as NVIDIA Cosmos diffusion world foundation models (NVIDIA et al., 2025). Our diffusion training framework provides various model parallelism options, achieving over 40% Model FLOPs Utilization (MFU) for several model configurations. 4.1. Diffusion Formulation Diffusion models learn to generate high-quality videos from random noise through an iterative sampling process. Specifically, diffusion models are trained to reverse fixed time-dependent stochastic process that corrupts data by adding noise. We train neural network ùúñùúÉ that estimates the noise ùúñùë° ùí© (0, I) added to create the corrupted video latent ùëßùë° = ùõºùë°ùë•0 + ùúéùë°ùúñùë°. Our diffusion model can be conditioned on multiple signals, such as the noise schedule timestep ùë° and text embedding ùë¶. We train the model with the following denoising score matching loss formulation: ‚Ñí (ùúÉ) = Eùë°ùí∞ (1,ùëá ),ùúñùë°ùí© (0,I) [ùë§(ùë°)ùúñùë° ùúñùúÉ(ùëßùë°; ùë°, ùë¶)2] , where ùë§(ùë°) is function that weights the different denoising tasks contributing to the training objective. Additionally, we use the same denoising network preconditioning, loss weighting, and noise distribution as proposed in EDM (Karras et al., 2022, 2024). We can sample videos from our diffusion model by iteratively running this denoising process for ùëÅ steps on random noise with given conditioning signal ùë¶. Here, we can utilize various stochastic samplers to generate videos. Our framework provides both the second-order EDM Heun sampler (Karras et al., 2022) the higher-order RES sampler Zhang et al. (2023) for accelerated video generation. 4.2. Training Pipeline Figure 5: Video Diffusion Transformer. Our pipeline consists of various input signals such as text, videos, and noise timestep which are compressed and used to train video diffusion transformer. Training video diffusion transformers requires several different components. Figure 5 displays the end-to-end process to train and generate video samples from video diffusion transformers. 5 Training Video Foundation Models with NVIDIA NeMo The first step is video tokenization. Given an input image or video, we generate spatiotemporal tokens with causal temporal 3D tokenizer. We flatten the 3D tokens into sequence of 3D spacetime patches using 3D patchify operation. We also provide video tokenizer fine-tuning capabilities in our framework. Details on tokenizer training can be found in Section 4.3. Next, we train the diffusion transformer (DiT) (Peebles and Xie, 2022) to denoise the 3D spacetime patches that have been corrupted with Gaussian noise. The diffusion transformer will output the denoised version of the full sequence of visual tokens. During training, we condition the diffusion transformer with the diffusion noise schedule timestep ùë° and the text embedding of the prompt describing the input video sample. The timestep ùë° is applied through an Adaptive LayerNormalization (AdaLN) mechanism, in which we condition the gamma and beta parameters of the LayerNorm on ùë°. One issue with using AdaLN is that significant portion of the total model parameters consist of the MLP parameters from Adaptive LayerNorm layers. We provide the option to use AdaLN-LoRA (Gupta et al., 2023), which reduces the total parameters used by the AdaLN layers by decomposing the operation into two low-rank matrix multiplications. We observe that AdaLN-LoRA also improves MFU during training. The text conditioning is applied through cross attention layer in each transformer block, in which we compute the attention between the video query tensor and the text embedding key-value tensors. Finally, we compute the diffusion loss with parallelized EDM diffusion pipeline using the noise prediction from the diffusion transformer. Our custom EDM diffusion pipeline is compatible with model parallelisms such as tensor parallel, pipeline parallel, and context parallel. 4.3. Training Custom Video Tokenizer Training directly on original videos or high-resolution images is often computationally prohibitive. widely adopted approach involves encoding the original data into latent space using video tokenizer, which can reduce the total dataset size by over 100 times, thereby significantly accelerating the learning process. As shown in Fig. 4, the training process begins with 3D causal video tokenizer to compress video inputs, enabling efficient VFM training. Recognizing the critical role of video tokenizers, our framework offers comprehensive toolkit that allows users to customize or fine-tune video tokenizer models to better suit their specific needs. 4.3.1. Customizing Tokenizer Architecture Our framework leverages open-source models from Hugging Face Diffusers (von Platen et al., 2022) to enable seamless video tokenizer customization. Users can modify the video tokenizer architecture by simply adjusting few lines in the configuration JSON files. For instance, the default video tokenizer provides an 8x spatial compression. Users can increase the spatial compression rate by adding and fine-tuning additional encoder/decoder blocks, which can significantly reduce downstream VFM training costs by further reducing total dataset size. This approach provides users with the flexibility to balance generation quality against available computational resources. 4.3.2. Finetuning Video Tokenizers We support fine-tuning either pretrained tokenizers or customized tokenizers with partial weight initialization from pretrained tokenizer on proprietary user data. Fine-tuning pretrained tokenizer enables users to quickly adapt an existing model to their specific video data. Fine-tuning customized tokenizer allows users to modify the video tokenizer architecture, enabling adjustments to the level of data compression based on their computational resources and downstream task requirements. We also provide users the option to use multiple loss functions, including Mean Squared Error (MSE), Kullback-Leibler (K-L) divergence loss, Learned Perceptual Image Patch Similarity (LPIPS) loss (Zhang et al., 2018), and Generative Adversarial Network (GAN) loss (Goodfellow et al., 2014). 6 Training Video Foundation Models with NVIDIA NeMo 4.4. Parallelizing Diffusion Transformers Diffusion modeling has shown impressive scaling behaviors in terms of data, model size, and compute Li et al. (2024). The DiT architecture has shown desirable scaling properties for generative and perceptual tasks Liang et al. (2024); Ravishankar et al. (2024). Modern video foundation models have adopted similar transformer-based architectures Brooks et al. (2024); Gupta et al. (2023); Kong et al. (2025); Polyak et al. (2025). However, the DiT architecture presents several challenges during both training and inference. Training DiT models with several billions of parameters on long sequences of videos has high activation memory usage, necessitating the use of various model parallelism strategies. In this section, we will discuss the various training parallelism options in NeMo enabled by Megatron Core for video diffusion training. Tensor Parallel (TP). Increasing the overall size of DiT models also increases the memory requirements for storing activations during training. TP splits the parameter tensors for each layer distributes them across multiple GPUs (Shoeybi et al., 2020). Thus, each GPU holds and processes only fraction of the total parameters. This strategy not only alleviates memory demandssince both model parameters and activations occupy less space per GPUbut it also helps accommodate extremely large models that would otherwise exceed single GPUs capacity. However, due to the smaller kernel workloads for each GPU, there is an increase in CPU overhead. The additional CPU overhead along with the communication cost of TP can potentially cause drop in overall MFU. Fully Sharded Data Parallel (FSDP). FSDP shards models parameters and gradients and the optimizer states across multiple GPUs (Zhao et al., 2023). In traditional data-parallel setup, each GPU maintains full copy of the model parameters. FSDP splits model parameters into shards and stores each shard on different GPU. As result, no single GPU holds the entire parameter set. During the forward pass, each GPU only needs the local shards it owns, reducing memory usage. FSDP utilizes collective operations across data-parallel GPUs. For parameter computation, an all-gather is applied during both the forward and backward pass. For parameter gradients, reduce-scatter is applied only during back-propagation. FSDP overlaps all communications with model compute, providing seamless model parallelism experience that does not require finding an optimal configuration as when using 3D Parallelism. However, FSDP performance heavily relies on the training clusters inter-GPU communication speed especially for large model training, where the communication overhead is quite large. This issue can be mitigated by pairing FSDP with TP. By restricting the FSDP sharding group to the data-parallel group size, TP can help reduce the model state and activation size per-GPU, which effectively reduces the communication and activation memory overhead for FSDP. Context Parallel (CP). In order to train our DiT model on long sequences of video, we must parallelize the transformer computation along the sequence dimension due to high activation memory costs. To enable long-context training, we utilize CP to parallelize the network activations across multiple GPUs by sharding the input tensors along the sequence dimension (Liu et al., 2023). This effectively shards the activations of all layers, unlike sequence parallelism (SP), which only shards the LayerNorm and Dropout layer activations. When using CP, all model components besides attention are not modified since they do not have inter-token operations. For attention, we must compute each query token (Q) with the keys and values (KV) of all tokens in the sequence. CP splits and KV into multiple blocks, so we must ensure that the computation is done correctly by leveraging communication collectives. Specifically, we arrange the GPU hosts in ring topology, where each GPU processes block of and iteratively computes blockwise attention with each KV chunk. During the forward pass, an all-gather operation assembles the full KV sequence so that every block can attend to all tokens. In the backward pass, reduce-scatter distributes gradients for KV back to their respective shards. To reduce activation memory cost, we only store one KV block of sequence chunk on each GPU during the forward pass, and then we gather KV again during the backward pass. Implementing the all-gather and reduce-scatter operations as peer-to-peer (P2P) communications via TransformerEngine (NVIDIA, 2024) allows for overlapping KV communication with blockwise attention computation, further improving training efficiency. Training Video Foundation Models with NVIDIA NeMo Pipeline Parallel (PP). With PP, we can shard the layers of transformer model across multiple GPU devices, assigning an equal number of transformer blocks to each device (Narayanan et al., 2021; Shoeybi et al., 2020). We split data batch into smaller microbatches, and then pipeline model execution across microbatches. One of the major challenges with PP is the pipeline bubble, which is the blocks of time where GPUs are idle. Various pipelining schedules have been proposed in recent years. In NeMo, we offer default pipelining with the GPipe algorithm (Huang et al., 2019) and interleaved pipelining (Narayanan et al., 2021) to reduce the pipeline bubble size. For the DiT model, we specifically explore how to efficiently pipeline conditioning information for the model across pipeline stages. PP for transformer models typically only communicates the hidden states across different PP stages. However, for DiT-based video foundation models, we may have several additional conditioning signals, including text embeddings, image embeddings, camera parameter embeddings etc. These embeddings are typically used in cross-attention blocks throughout each transformer block. To ensure correct training, we must be able to compute crossattention with these conditioning signals at each pipeline stage. Figure 6: Parallel Conditioning Scheme. The DiT model uses additional conditioning signals that must be available at each pipeline stage. We determined that recomputing these embeddings at each pipeline stage is more efficient than communicating them across pipeline stages during training. We consider two approaches to mitigate this issue as shown in Figure 6. The first method is communicating the additional conditioning signals along with the hidden states across pipeline stages. This requires computing the embeddings once before the first DiT block and then communicating them across each PP stage. The second method is computing each embedding at each PP stage separately, and then conditioning the model with the computed embeddings. In the first method, we must modify the PP schedule each time we incorporate new conditioning signal by increasing the communication buffer size to add the new embedding, which increases communication cost. In the second method, we do not modify the PP schedule, but we must spend more compute since we recompute conditioning signals at each PP stage. We observed that the additional computation cost of the second method has better tradeoff with MFU than the first method with the additional communication cost. It is also simpler to implement and easily scales as new conditioning signals are added. 4.5. DiT Parallelization Performance Study This section provides detailed evaluation and performance benchmarks for our framework. We first explore the algorithm-system co-design space to find the best performing model configurations. This is followed by detailed performance benchmarking results, comparison with other open-source alternatives like Fast-DiT (Fang et al., 2024) and scaling experiments. We end this section highlighting the importance of flexible training framework for diffusion models and brief summary of our learnings. 4.5.1. Algorithm-system co-design. To achieve high compute efficiency while scaling both model sizes and context lengths, we explore algorithmsystem co-design space along with parallelization strategies. AdaLN-LoRA architecture. One of the key changes we make to the DiT Architecture is altering the AdaLN projection. The AdaLN projection consumes vast majority of the models parameters [Hidden Dim 9*Hidden Dim]. 8 Training Video Foundation Models with NVIDIA NeMo For example, using base DiT model architecture, 7B model would have an AdaLN projection that alone would consume 2.7B parameters. Additionally, given that the AdaLN is performed on the timestep embedding (instead of the full context length) and is singular GEMM, it also doesnt allow for straightforward model parallelism strategy that would allow for balancing the compute with communication. To improve this, we implement AdaLN-LoRA (Gupta et al., 2023). This not only allows us to reduce the parameter density focused in AdaLN but also allows us to use larger hidden dimensions for given fixed model size. This results in more efficient GPU compute utilization and better compute-communication ratios which helps us improve our compute performance by up to 1.2x as shown in Figure 7. Figure 7: Compute Performance Improvement with AdaLN-LoRA. We benchmark the DiT training speedup with AdaLN-LoRA for 7B and 28B models on sequence lengths of 8k and 74k tokens. We observe noticeable speedup across all configurations and up to 1.2x for DiT-7B at 8k sequence length. Modularized Context Parallelism. Another choice we make while exploring parallelization strategies is to disable context parallelism for the CrossAttention portion of the layer when CP is used. We make this decision as the KV context length is not sufficiently large to require parallelism or overlap communication under compute if parallelism was used. QK Normalization. Another challenge we face is the normalization of ùëÑ and ùêæ before dot product attention compute. Under Tensor Parallelism, ùëÑ and ùêæ are normally sharded across the number of attention heads and computing normalization across the entire hidden dimension would require excessive communication. We choose to limit this normalization per head in order to improve compute throughput and find no adverse effect on model performance. 4.5.2. Performance Benchmarking We study the compute throughput performance of training workloads with varying model sizes, and context lengths with multiple parallelization strategies and compute scales and show that our training framework can achieve up to 48.2% MFU. Performance Evaluation. Table 1 describes the workload configurations used. We report best achieved performance on 8 8xH100 nodes for all workloads except 28B - Stage 3 which is reported on 32 8xH100 nodes. We take Fast-DiT (Jin and Xie, 2024) as baseline to compare with our framework. For fair comparison, we enable AdaLN-LoRA and FSDP support using Huggingface Accelerate (Gugger et al., 2022) in Fast-DiT. Figure 8 shows the best compute performance for each workload using our framework and compares it to the best compute performance achieved using Fast-DiT. Our training framework is able to achieve high throughput consistently across all workloads and outperforms Fast-DiT by up to 1.85x on the 7B model. Our framework 9 Training Video Foundation Models with NVIDIA NeMo Table 1: DiT Model Performance Benchmarking Configurations Workload Layers Hidden Size Heads Context Length 7B Stage 2 7B Stage 3 28B Stage 2 28B Stage 28 28 48 48 4096 4096 6144 6144 32 32 48 48 8192 73728 8192 73728 maintains strong performance when scaling the model size to 28B, while Fast-DiT is unable to run the 28B model as it runs out of memory capacity. Figure 8: Best Compute Performance per Workload. We benchmark our training framework and compare it with the Fast-DiT framework. Our framework consistently delivers higher training throughput than Fast-DiT, and it also supports training larger models that Fast-DiT cannot handle. Scaling Efficiency. Our framework also achieves high strong scaling efficiency. We continue to measure compute throughput while scaling up the number of 8xH100 nodes given fixed workload and find that our framework achieves 95% or higher strong scaling efficiency as shown in Figure 9. Figure 9: DiT Training Scaling Efficiency. We measure the scaling efficiency across four workloads, scaling from 8 to 32 nodes of 8xH100 GPUs. All configurations exceed 95% efficiency, with most surpassing 98%. These results highlight our frameworks near-linear scaling across training workloads. Observations and Learnings. As shown in Figure 8, it is clear that each workload requires unique combination of parallelism strategies to achieve high efficiency, highlighting the need for flexible training framework. To further underscore the importance of full 4D parallelism (TP, CP, PP and FSDP), we study the best throughput achieved by DiT-7B with AdaLN-LoRA under various parallelism configurations while scaling context length. Figure 10 shows performance of different strategies normalized to the best performing 4D parallel strategy. 10 Training Video Foundation Models with NVIDIA NeMo The best performing parallelism strategy is unique at each context length, and no single parallelism strategy alone is sufficient to consistently achieve good performance. At shorter context lengths and smaller model sizes, FSDP is typically sufficient to achieve decent training throughput. However, as the context length and activation memory footprint grows, FSDP alone is no longer the best performing configuration and requires combination of TP, CP and FSDP. PP becomes necessary strategy as well to distribute large models while maintaining high compute efficiency. We briefly summarize our learnings below. Figure 10: Context Scaling Performance Across Parallelism Strategies. We observe that the optimal parallelism configuration changes based on the context length. FSDP is performant for shorter context lengths, but 4D parallelism becomes necessary for efficient long-context training. Tradeoffs Between Parallelism Strategies. When scaling diffusion transformer training, it is important to consider the tradeoffs between specific types of parallelism strategies. TP, CP, and PP have various advantages and disadvantages when applied to the DiT architecture: 1. Context Parallelism: CP primarily reduces the memory footprint of activations on each device during training. In scenarios with long video sequences, we can effectively hide the communication overhead of CP. However, when sequences are short, this overhead becomes largely exposed. 2. Tensor Parallelism + Sequence Parallelism: TP and SP together effectively minimize the memory usage of parameters and activations. However, due to the widespread use of Adaptive Layer Normalization (AdaLN) in the DiT architecture, TP communication cannot be well overlapped with computation. Additionally, when model size is not large (i.e. 10B parameters), excessive splitting of parameters can lead to reduced efficiency in matrix multiplications. 3. Pipeline Parallelism: While PP does not help in reducing activation memory usage and introduces bubbles of idle GPU time at the start and end of processing, it can effectively decrease parameter memory usage. Therefore, it is beneficial when dealing with larger model sizes, especially in settings with low communication bandwidth between GPUs. In summary, after weighing the advantages and disadvantages of the parallel techniques, the following recommendations can be made: 1. When both the model size and context lengths are relatively small, FSDP can be sufficient. 2. When the model size is relatively small and the video sequences are large, CP should be prioritized. 3. As context length grows, CP should be prioritized, and as model size grows, TP/FSDP should be prioritized for any model sharding. 11 Training Video Foundation Models with NVIDIA NeMo 4. If model size is large, TP should be prioritized intra-node to an extent where smaller GEMMs are still efficient. Beyond that, PP should be used. 5. In cases where the model size and context length are particularly large, combination of TP, PP and CP may be required. As general rule of thumb, TP/SP communications should be kept intra-node while CP and PP groups can perform inter-node communication while still sufficiently hiding the communication overhead. 4.6. Spatial-Temporal DiT (ST-DiT) In addition to full attention, ST-DiT (Spatial-Temporal DiT) (Zheng et al., 2024) introduces spatial and temporal attentions, which are applied to intra-frame and inter-frame tokens respectively. These attentions are stacked in transformer layer, and their computational complexity varies lot for their substantially different batch sizes and sequence lengths. Full attention flattens all tokens of all frames into sequence, which results in small batch size (e.g., 1 or 2) and long sequence length (e.g., 256K, 512K, etc.). Spatial attention is applied to each frame separately which translates to relatively large batch size (e.g., hundreds of frames) but short sequence length (e.g., few thousands of tokens in each frame). Temporal attention is like spatial attention, because their batch sizes and sequence lengths are transposed to each other. Input shape variance leads to different tradeoffs on parallelization strategies: larger batch sizes prefer DP (data parallelism), and longer sequence lengths need CP (context parallelism). Since DP splits the batch dimension, it does not trigger any inter-GPU communication for attention. Thus, each GPU can work independently with its assigned batch split. CP splits the long sequence into chunks and distributes them between GPUs, so each GPU only needs to compute and save activations of sequence chunk. However, CP inserts an all-gather to collect the full sequence of KV, because of each token needs to compute with the KV of all tokens in the same sequence. The CP implementation in TransformerEngine changes the all-gather to P2P communications arranged in ring-topology, which hides the communication overhead under the attention compute if the sequence is long enough. We propose to apply CP to full attention, but DP to spatial and temporal attentions as shown in Figure 11. This cannot work out directly due to the mismatched input/output shapes. Assume that the input shape of each transformer layer is [b, h*w*t, d], where ùëè is the batch size, ‚Ñé and ùë§ are the height and width of each frame, ùë° is the number of frames, and ùëë is the hidden size. Sequence length for each attention is determined by ‚Ñé, ùë§, and ùë°. The output shape of full attention with CP is [b, (h*w*t)/CP, d], but the following spatial attention requires input shape of [(b*t)/CP, h*w, d], and the last temporal attention expects input shape of [(b*h*w)/CP, t, d]. To get the expected input shape for each attention, we insert all-to-all communications between attentions for tensor shape transitions. Figure 11: ST-DiT Hybrid Parallelism. We propose new hybrid parallelism approach for ST-DiT models that reduces the total necessary all-to-all (A2A) collectives while fully hiding the communication overhead of full attention and minimizing the communication overhead for both spatial and temporal attention. 12 Training Video Foundation Models with NVIDIA NeMo Existing ST-DiT implementations either did not consider all three attentions simultaneously, i.e. DSP (Zhao et al., 2024), or had significant communication overhead exposed, i.e. 4 all-to-all collectives per attention in DeepSpeed Ulysses (Jacobs et al., 2023). Compared to prior implementations, our proposal not only takes all attention modules into consideration, but also can fully hide communication overheads of full attention and introduces the minimum overheads to spatial and temporal attentions (only two all-to-all collectives). Table 2 compares the performance of different parallel strategies for spatial-temporal attention. It can be seen that using all-to-all for spatial-temporal attention achieves better performance than CP, and it is able to achieve up to 40% MFU on NVIDIA H100 GPUs for our 12B ST-DiT configuration. Table 2: ST-DiT Benchmark Results"
        },
        {
            "title": "Training Config",
            "content": "GPU MFU (TFLOPS/s) Throughput (token/s/GPU)"
        },
        {
            "title": "Speedup",
            "content": "STDiT-7B STDiT-12B STDiT-12B STDiT-7B STDiT-12B STDiT-7B STDiT-12B STDiT-12B STDiT-7B STDiT-12B 35K 35K 35K 74K 74K 35K 35K 35K 74K 74K TP=2 SP PP=4 VPP=2 CP=4 FSDP CP=8 TP=2 SP PP=4 CP=4 TP=2 SP PP=2 CP=8 TP=2 SP PP=4 CP=8 TP=2 SP PP=4 VPP=2 All2All=4 FSDP All2All=8 TP=2 SP PP=4 All2All=4 TP=2 SP PP=2 All2All=8 TP=2 SP PP=4 All2All=8 284.99 327.61 359.92 139.4 168.8 318.7 402.5 385.0 335.6 381.1 7138.9 4670.7 5131.6 3371.4 2342.8 7984.5 5738.5 5489.2 8113.6 5288.5 - - - - - 1.118 1.229 1.070 2.407 2. 5. Efficient Video Generation Inference Video diffusion models must denoise long sequences of 3D spatiotemporal tokens during inference time. To generate high resolution video samples, these models must denoise tens of thousands of tokens, which may only amount to few seconds of actual video. The sampling process is iterative, which can lead to long generation times per sampled video. We introduce an efficient approach for video generation using diffusion models that leverages context parallelization across multiple GPUs. In the following sections, we detail our inference approach and performance results across variety of configurations. 5.1. Parallelized Inference Algorithm We apply simple, yet effective approach to parallelize video diffusion model inference with context parallelism. This method significantly accelerates the inference process by distributing the denoising workload across GPUs and denoising sequence chunks in parallel. We visualize the process to generate single video in Figure 12. We begin the video generation process by partitioning the input noise latent representations along the sequence dimension and allocate each chunk to specific GPU. Once the input noise latents are distributed, the DiT on each GPU independently performs the denoising process on its assigned subsequence. This process is repeated ùëá times, where ùëá is the total number of denoising steps. The denoising operation is core component of diffusion models, involving iterative refinement to produce coherent and high-quality outputs. By executing these operations in parallel across GPUs, we effectively reduce the overall computation time proportional to the number of GPUs. Finally, the denoised latent tensors are gathered and concatenated to reconstruct the full sequence of the video in latent space. The combined latent sequence is then decoded with the Cosmos Video Tokenizer (NVIDIA et al., 2025). 13 Training Video Foundation Models with NVIDIA NeMo Figure 12: Parallelized Inference with Context Parallel. To generate high-resolution videos efficiently, we leverage CP to parallelize the diffusion denoising process on multiple GPUs. This strategy provides near-linear scaling in inference speedup with respect to the number of GPUs. 5.2. Performance Study We benchmark inference performance of our video diffusion model across variety of configurations. Specifically, we analyze the performance of the NVIDIA Cosmos-1.0-Diffusion-7B-Text2World model (NVIDIA et al., 2025). We use classifier-free guidance (CFG) (Ho and Salimans, 2022) during inference, which requires both conditional and unconditional output simultaneously. Thus, we use global and micro batch size of 2 during all experiments. We observe that using context parallelism and tensor parallelism at inference has 80-90% scaling efficiency up to 32 H100 GPUs. We can further improve the speedup by enabling compute/communication overlap for context parallelism to achieve near linear scaling. We also note that on H100 GPUs, FP8 Multi-Head Attention (MHA) from TransformerEngine can improve performance by 28% over BF16 on 1 GPU and 48% on 32 GPUs. Our inference pipeline enables fast video generation with large models even across multi-node systems, which is crucial to serve these models to customers and utilize them for synthetic data generation pipelines for physical AI applications. There are several other strategies that can also improve inference performance, including model quantization (Li et al., 2023), CFG parallel (Fang et al., 2024), and model distillation (Xie et al., 2024; Zhou et al., 2024). We leave these explorations to future works. Figure 13: Inference Performance Analysis. We benchmark the Cosmos-1.0-Diffusion-7B-Text2World model across different compute, precision, and model parallelism settings. Our experiments show that we can achieve near-linear scaling with our proposed CP-accelerated inference algorithm. 14 Training Video Foundation Models with NVIDIA NeMo 6. Conclusion In this paper, we introduced comprehensive and scalable framework for training VFMs using NVIDIA NeMo. Our framework integrates powerful video curation capabilities via NeMo Curator, multimodal dataloading with Megatron Energon, and scalable video diffusion model training with Megatron Core. We demonstrated the importance of algorithm-system co-design, highlighting the need for advanced 4D parallelization to efficiently train large diffusion transformers. We applied and showed the benefits of algorithm-system co-design in our new hybrid parallelism approach for custom diffusion transformers. Our extensive benchmarking demonstrated superior performance and scalability compared to existing methods at both training and inference time, achieving near-linear scaling and significantly higher MFU than previous baselines. By systematically exploring various parallelism strategies, we established guidelines for optimal parallel configurations across different model sizes and sequence lengths. Our flexible framework sets strong foundation by providing users the tools needed to explore and scale video foundation models to accelerate novel research and applications. 15 Training Video Foundation Models with NVIDIA NeMo A. Contributors A.1. Core Contributors NeMo Curator Ryan Wolf, Niket Agarwal, Jacob Huffman Multimodal Dataloading Ethan He, Zeeshan Patel Tokenizer Training Pipeline Linnan Wang, Ethan He Diffusion Training Pipeline Zeeshan Patel, Ethan He, Parth Mannan, Jack Chang DiT Performance Study Parth Mannan, Yan Bai, Zeeshan Patel, Ethan He ST-DiT Hybrid Parallel Algorithm Design Zeeshan Patel, Ethan He, Xiaowei Ren ST-DiT Hybrid Parallel Implementation & Benchmarking Zhuoyao Wang, Carl Wang Inference Performance Study Tommy Huang, Zeeshan Patel, Ethan He A.2. Contributors Sahil Jain, Shanmugam Ramasamy, Joseph Jennings, Ekaterina Sirazitdinova, Oleg Sudakov, Mingyuan Ma, Bobby Chen, Forrest Lin, Hao Wang, Vasanth Rao Naik Sabavat, Sriharsha Niverty, Rong Ou, Pallab Bhattacharya, David Page, Nima Tajbakhsh, Ashwath Aithal A.3. Acknowledgements We thank Ming-Yu Liu, Yen-Chen Lin, Qinsheng Zhang, and Wenwen Gao for their help and support throughout this project. 16 Training Video Foundation Models with NVIDIA NeMo References [1] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. URL https://openai.com/research/ video-generation-models-as-world-simulators. 7 2024. [2] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transformers (dits) with massive parallelism. arXiv preprint arXiv:2411.01738, 2024. 8, 14 [3] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https://arxiv.org/abs/ 1406.2661. 6 [4] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. 9 [5] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos√© Lezama. Photorealistic video generation with diffusion models, 2023. URL https://arxiv.org/abs/ 2312.06662. 6, 7, 9 [6] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv.org/abs/ 2207.12598. [7] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism, 2019. URL https://arxiv.org/abs/1811.06965. 8 [8] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023. URL https://arxiv.org/abs/2309.14509. 13 [9] Chuanyang Jin and Saining Xie. Fast-dit: Fast diffusion models with transformers. https://github. com/chuanyangjin/fast-DiT, 2024. 9 [10] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models, 2022. URL https://arxiv.org/abs/2206.00364. [11] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models, 2024. URL https://arxiv.org/abs/2312.02696. 5 [12] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. 7 [13] Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R. Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-based text-to-image generation, 2024. URL https://arxiv.org/abs/2404.02883. 7 17 Training Video Foundation Models with NVIDIA NeMo [14] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models, 2023. URL https://arxiv.org/abs/2302.04304. [15] Zhengyang Liang, Hao He, Ceyuan Yang, and Bo Dai. Scaling laws for diffusion transformers, 2024. URL https://arxiv.org/abs/2410.08184. 7 [16] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context, 2023. URL https://arxiv.org/abs/2310.01889. 7 [17] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: distributed framework for emerging ai applications, 2018. URL https://arxiv.org/abs/1712.05889. 2 [18] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm, 2021. URL https://arxiv.org/abs/2104.04473. [19] NVIDIA. Transformer engine, 2024. URL https://github.com/NVIDIA/TransformerEngine. 7 [20] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Kl√°r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai, 2025. URL https://arxiv.org/abs/2501.03575. 5, 13, 14 [21] William Peebles and Saining Xie. arXiv:2212.09748, 2022. 6 Scalable diffusion models with transformers. arXiv preprint [22] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2025. URL https://arxiv.org/abs/2410.13720. 18 Training Video Foundation Models with NVIDIA NeMo [23] Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, and Jitendra Malik. Scaling properties of diffusion models for perceptual tasks, 2024. URL https://arxiv.org/abs/2411.08034. 7 [24] Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, and Andrea Banino. Synth2: Boosting visual-language models with synthetic captions and image embeddings, 2024. URL https://arxiv.org/abs/2403.07750. 2 [25] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. URL https://arxiv.org/abs/1909.08053. 7, [26] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022. 6 [27] Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Patrick Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models, 2024. URL https: //arxiv.org/abs/2405.16852. 14 [28] Qinsheng Zhang, Jiaming Song, and Yongxin Chen. Improved order analysis and design of exponential integrator for diffusion models sampling, 2023. URL https://arxiv.org/abs/2308.02157. 5 [29] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. URL https://arxiv.org/abs/1801.03924. 6 [30] Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, and Yang You. Dsp: Dynamic sequence parallelism for multi-dimensional transformers, 2024. URL https://arxiv. org/abs/2403.10266. 13 [31] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/2304.11277. [32] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora. 12 [33] Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen, and Siwei Lyu. Simple and fast distillation of diffusion models, 2024. URL https://arxiv.org/abs/2409.19681."
        }
    ],
    "affiliations": []
}