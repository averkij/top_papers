{
    "paper_title": "IHEval: Evaluating Language Models on Following the Instruction Hierarchy",
    "authors": [
        "Zhihan Zhang",
        "Shiyang Li",
        "Zixuan Zhang",
        "Xin Liu",
        "Haoming Jiang",
        "Xianfeng Tang",
        "Yifan Gao",
        "Zheng Li",
        "Haodong Wang",
        "Zhaoxuan Tan",
        "Yichuan Li",
        "Qingyu Yin",
        "Bing Yin",
        "Meng Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models' ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 5 4 7 8 0 . 2 0 5 2 : r IHEval: Evaluating Language Models on Following the Instruction Hierarchy Zhihan Zhang(cid:0)1,2*, Shiyang Li2, Zixuan Zhang2, Xin Liu2, Haoming Jiang2, Xianfeng Tang2, Yifan Gao2, Zheng Li2, Haodong Wang2, Zhaoxuan Tan1,2, Yichuan Li2,3, Qingyu Yin2, Bing Yin2, Meng Jiang1, 1University of Notre Dame 2Amazon 3Worcester Polytechnic Institute zzhang23@nd.edu"
        },
        {
            "title": "Abstract",
            "content": "The instruction hierarchy, which establishes priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is lack of comprehensive benchmarks for evaluating models ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive opensource model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs."
        },
        {
            "title": "Introduction",
            "content": "Instruction-tuned language models (LMs) are increasingly deployed as interactive services across various applications (OpenAI, 2023; Yang et al., 2024; Bi et al., 2024). To ensure consistent performance and safety, developers typically seek to regulate the models behavior, such as finetuning the model on responding to certain instructions (Touvron et al., 2023), using post-processing techniques to edit model outputs (Song et al., 2024), and detailing system messages to impose constraints(Anthropic, 2024b). However, in realworld applications, these pre-defined regulations frequently struggle to cover the full range of possible user inputs. For instance, users may request tasks beyond the models intended scope (OpenAI, * This work was done when Zhihan, Zhaoxuan, and Yichuan were interns at Amazon. 1 2024), or the integrated tools may return unexpected content (Zhan et al., 2024). The LM may risk misbehavior if higher-level instructions, such as regulative system messages, are overridden by subsequent conflicting inputs. This highlights the need for LMs to possess an inherent capacity to follow an instruction hierarchy, where instructions of high-level regulations are always prioritized over low-level inputs. The order of priority ranked from highest to lowest should be: system messages, user messages, conversation history, and tool outputs. This hierarchy was first proposed by Wallace et al. (2024) and aligns with typical practices when using language models. model that adheres to this hierarchy would be able to spontaneously resolve instruction conflicts by prioritizing higher-order ones, which reduces the dependency on complex regulation methods while also mitigating the risk of misbehavior1. Despite its significance, the instruction hierarchy paradigm does not receive much attention in LM research and evaluation. In some models, system messages an important tool for giving highlevel instructions are either not supported (Rivi√®re et al., 2024), not distinguished from user messages (Mistral, 2024), or exhibit limited variation during training (Ivison et al., 2023). While many recent models have supported multi-level inputs, the related training details are rarely disclosed. notable advancement in this area comes from OpenAIs study (Wallace et al., 2024), but their evaluation was limited to GPT models and unreleased proprietary data, focusing solely on safety-related instructions. This constrains its general applicability to wider range of use cases. To date, there remains no comprehensive benchmark to evaluate how well different LMs adhere to the instruction hierarchy. In order to bridge this gap and highlight 1We target scenarios where users cant modify developerset system messages (e.g., ChatGPT). In contrast, when users can customize prompts, conflicts are rarer unless by mistake. Figure 1: Four categories of the instruction hierarchy and the corresponding priority orders of instructions. Conflict instructions are shown in red. Models are expected to follow the instruction with the higher priority. Figure 2: Results of mainstream LMs on IHEval. The reference setting represents original task performance without hierarchical inputs. We observe large performance drops when models face conflicting hierarchical instructions. the vital role of the instruction hierarchy, we create IHEval, comprehensive benchmark for Instruction Hierarchy Evaluation. It is designed with the following characteristics: 1. Diverse scenarios: Consisting of 3,538 examples and nine tasks, it spans four key scenarios involving hierarchical instructions: rule following, task execution, safety defense, and tool use. 2. Comprehensive input hierarchy: It covers four types of input: system messages, user messages, conversation history, and tool outputs. 3. Instruction alignments and conflicts: It includes both settings where (1) low-priority inputs align with high-level regulations, and (2) low-priority inputs contain additional instructions that conflict with those regulations. 4. Varied task difficulties: It offers various task difficulty settings by adjusting the strictness of the instruction phrasing, such as intensifying conflicts by requiring the model to exclusively follow specific instructions. 5. Programmable evaluation: All tasks are evaluated programmatically, ensuring the efficiency and reproducibility of the evaluation process."
        },
        {
            "title": "We evaluate a variety of mainstream LMs using",
            "content": "IHEval and observe several key insights: (1) LMs struggle to prioritize high-level instructions when conflicts arise, with open-source models showing less than 50% accuracy in resolving these conflicts. This performance significantly lags behind both GPT-4o and their original instruction-following accuracy, as shown in Figure 2; (2) Even without conflicts, model performance on hierarchical inputs is inconsistent with the single-input reference setting; (3) Models handling of conflicts is easily influenced by superficial factors like the strictness of instructions, and does not scale effectively with model size. These findings suggest that current LMs are not fully optimized for following the instruction hierarchy, leading to performance degradation or even unsafe behavior. We hope that our study can spark deeper research into this direction."
        },
        {
            "title": "We summarize the main contributions of this",
            "content": "work as follows: We design comprehensive evaluation for assessing LMs compliance with the instruction hierarchy, covering diverse scenarios where LMs face instructions of different priorities. We collect benchmark to support this evaluation, including settings where hierarchical instructions either align or conflict, all of which 2 are programmatically evaluated. We evaluate wide selection of LMs and find that they are not sufficiently optimized for the instruction hierarchy, highlighting potential risks in their real-world applications."
        },
        {
            "title": "2.1 Evaluation on Instruction Following",
            "content": "The ability to follow instructions is crucial assessment of instruction-tuned LMs. Early research in this area adopted straightforward approach that leverages an expert LM (e.g., GPT-4) to holistically judge the quality of models response to an instruction (Dubois et al., 2023; Zheng et al., 2023a). More recent work focused on disentangling instruction-following evaluation from other factors, such as response detailedness and factuality, by proposing more fine-grained assessments on whether the response adheres to the constraints specified in the user query. Some studies, for instance, required LMs to follow strict rules regarding response formats (Zhou et al., 2023; Mu et al., 2023; Li et al., 2023), while others designed casespecific constraints to regulate the content of model outputs (Jiang et al., 2023; Qin et al., 2024b; Wen et al., 2024). Recent studies also explored scenarios where instructions are embedded within the task input to assess whether LMs can correctly differentiate between instructions and data (Zverev et al., 2024; Yi et al., 2023). In contrast to these works in the general domain, researchers in LM safety focused on whether models can effectively reject malicious instructions, whether directly provided by attackers (Schulhoff et al., 2023; Toyer et al., 2024) or injected via external information (Zhan et al., 2024; Debenedetti et al., 2024). Despite the rich amount of work in this area, none of them systematically analyzed the LMs ability in the instruction hierarchy. Notably, IHEval includes various scenarios where LMs face hierarchical inputs, especially those with conflicting instructions, bridging gap in the current evaluation of LMs."
        },
        {
            "title": "2.2 System Prompts in LMs",
            "content": "System prompts (Ramlochan, 2024) are commonly employed to guide LMs behavior from high level. System prompts typically define the LMs role, task, output format, and safety guidelines, all of which are intended to be followed throughout the entire interaction. In many models, system prompts have been introduced as separate input field from the user instruction (OpenAI, 2023; Touvron et al., 2023; Mukherjee et al., 2023), but details about its training process such as the types and diversity of system prompts used are rarely disclosed. Subsequent research demonstrated that system prompts can be used to improve the performance of LMs in general-domain instruction following (Zheng et al., 2023b), personalized response generation (Lee et al., 2024), rule adherence (Lu et al., 2024), and defending jailbreaks (Zou et al., 2024). Inspired by this line of work, we investigate whether LMs consistently prioritize system prompts over user instructions and extend this evaluation to the broader context of instruction hierarchy. During the development of IHEval, concurrent work (Qin et al., 2024a) introduced SysBench which evaluates LMs adherence to system prompts. Compared to Sysbench, IHEval features more comprehensive evaluation of the instruction hierarchy concept, encompassing system prompts, user instructions, conversation history, and tool outputs. Moreover, IHEval is fully equipped with programmatic evaluation, offering better cost-efficiency and reproducibility than the GPT-based evaluation used in SysBench."
        },
        {
            "title": "IHEval",
            "content": "Definition In this paper, we denote inputs to be the text segments that the model receives, which may contain both instructions that control the models behavior, and data that the model needs to process. IHEval is designed around the instruction hierarchy, which assigns priority to instructions from four types of input: system messages, user messages, conversation history, and tool output, ranked from highest to lowest priority. We define hierarchical inputs as input sequences composed of more than one type of input, such as sequence that includes both system message and user message. When facing instruction conflicts, we refer to the higher-priority instruction as the main instruction, which defines the primary task the model should prioritize. The conflicting instruction refers to the lower-priority instruction whose request is incompatible with the main instruction. Task Settings For comprehensive evaluation, we design three input settings for each IHEval task: Aligned Setting: The model receives hierarchical inputs where all low-priority inputs align with the highest-priority instruction. This tests 3 Figure 3: IHEval covers four categories and nine tasks. Detailed examples and instructions are in Figures 816. the models ability to process hierarchical inputs in normal scenarios without conflicts. Conflict Setting: There are conflicts among different priorities of instructions within hierarchical input. Models are expected to follow the instruction hierarchy to resolve conflicts. Reference Setting: We notice that models response to hierarchical instructions is affected by both its original task performance and its ability to follow the instruction hierarchy (IHfollowing). To better assess IH-following performance, we add reference setting that isolates the original task performance by removing hierarchical inputs. Specifically, the model is evaluated in standard single-input setting, where all hierarchical instructions from the aligned setting are merged into single user message. Task Design IHEval tasks are selected to encompass diverse range of application scenarios and input types. We focus on tasks where LMs perform well to minimize the impact of original task performance on IHEval scores. We prioritize datasets with human-annotated labels or reliable programmatic evaluation. Conflicting instructions are drawn from tasks likely to confuse LMs in following the main instruction, based on heuristics and trials on sample data. As result, total of nine tasks is created and grouped into four categories based on the type of content in the instructions: Rule Following: Instructions specify formatting rules for model outputs. We adapt data from IFEval (Zhou et al., 2023) into our single-turn task, where the original data is split into formatting rules (system message) and user queries (user message). We then craft incompatible formatting rules to create conflicting user instructions. The single-turn data is further extended to multiturn setting by crafting both response to the first turn and follow-up user message. Data crafting in this category is initially performed by Claude (Anthropic, 2024a), after which we manually review all AI-written messages and re-write low-quality ones. Task Execution: In this category, the system message outlines specific NLP task that the model should perform on the users input, while the user message may provide conflicting instruction that requests different task. This category covers typical NLP tasks that span extraction, generation, and classification. Safety Defense: Following the setting of TensorTrust (Toyer et al., 2024), this category simulates the model as security system. The system message requires the model to grant access only if the correct password is entered. Normal user inputs involve password attempts, but conflicts arise when malicious users launch adversarial attacks to make the model respond with Ac4 Figure 4: The original data source, the evaluation metric, and the data size of each task. cess Granted (Hijack task) or attempt to extract the system message that contains the password (Extraction task). Tool Use: This category focuses on tasks where the model needs to call external tools to complete the users request. We simulate tool call and the corresponding tool output, where the tool output may either align with the users request or contain conflicting instructions. Such instructions can be intrinsic, i.e., originating from the tool-retrieved content itself, or injected by an external attacker and is concatenated with the normal tool-retrieved content. The illustration of all tasks is listed in Figure 3, with data examples and task instructions shown in Figures 816, respectively. Data sources and statistics are outlined in Figure 4. Details about data collection for each individual task and the motivation of task selection are provided in Appendix A. Task Difficulties IHEval introduces multiple difficulty levels for each task by crafting instructions with different imperative strictness. This approach not only provides comprehensive evaluation of model performance but also reduces the randomness brought by the phrasing of instructions. The stricter version of instructions requires the model to exclusively adhere to the given instruction (e.g., do not output any other text besides the Spanish translation in the translation task). All these instructions are shown in Figures 816. Evaluation IHEval evaluates models based on their performance in completing the main instruction, as outlined in Figure 4. For example, when the system message requests the model to extract verbs from the users input, the evaluation metric is the F-1 score which compares model-extracted verbs to the ground-truth list. Any execution of the conflicting instruction translating the input text into Spanish negatively impacts performance, as it diverges the model output from the target defined by the system message. For tasks that are not evaluated by exact-match accuracy (excluding safety tasks that check the defense success rate using the whole model response), we calculate both strict metric and loose metric, following the practice in IFEval (Zhou et al., 2023). The strict metric assesses the models entire output, while the loose metric allows minor variations by evaluating outputs that omit the first line, last line, or markdown syntax, selecting the best-performing version. The loose metric accommodates brief introductory phrases (e.g., Im sorry, but can only extract verbs from your message) that explain the models behavior. However, overly interactive responses such as asking for clarification or answering both instructions are discouraged, as they treat the hierarchical instructions with the same level of priority (see Appendix for more detailed discussion). The final score is averaged across difficulty levels and, when applicable, across strict and loose metrics. As previously mentioned, the reference setting decouples models baseline task performance from its IH-following ability. To quantify this distinction, we calculate the score difference ( in Table 1) between the reference setting and the other two settings. Specifically, we report both the mean difference, which reflects the models average IH-following performance (where smaller performance drops indicate better IH-following), and the mean absolute difference, which captures performance fluctuation between single-input vs. hierarchical-input settings."
        },
        {
            "title": "4 Experiments",
            "content": "In this study, we evaluate 13 widely used LMs from five different model families, including both proprietary and open-source models: GPT (3.5turbo, 4o-2024-0806, 4o-mini-2024-0718, OpenAI, 2023), Claude-3 (Haiku, Sonnet, Anthropic, 2024a), LLaMA-3.1 (8B, 70B, Dubey et al., 2024), LLaMA-3 (8B, 70B), Mistral (7B-v0.3, Large2407, Mistral, 2024), and Qwen-2 (7B, 72B, Yang et al., 2024). The decoding temperature is set to 0 to ensure deterministic outputs. 5 Model Setting Rule Following Task Execution Safety Defense Single. Multi. Ext. Gen. Class. Hijack Extract Intrinsic Injected Tool Use Avg. Mean Abs. GPT-4o (2024-0806) GPT-4o mini (2024-0718) Claude-3 Sonnet LLaMA-3.1 70B Mistral-Large (2407) Qwen-2 72B reference aligned conflict reference aligned conflict reference aligned conflict reference aligned conflict reference aligned conflict reference aligned conflict 89.0 85.6 49.5 84.5 82.3 33.9 80.9 68.4 10.8 88.3 82.9 14. 83.6 81.7 25.2 81.4 82.1 35.8 86.5 86.8 51.0 86.1 80.2 35.7 83.9 69.5 21.1 88.4 76.6 24. 85.2 87.1 60.0 85.0 81.3 39.5 90.0 78.0 87.3 73.9 77.2 38.3 90.5 78.4 84.0 72.0 47.7 31.1 84.9 76.9 77.4 79.8 2.3 29.7 89.1 77.0 84.3 59.5 15. 0 85.2 78.5 76.0 78.3 11.0 20.2 74.9 75.0 73.4 75.3 53.7 58.4 100 100 99.7 99.6 100 41.1 100 100 9. 100 100 6.2 100 100 78.4 100 100 99.5 99.5 99.2 91.2 99.1 98.6 70.3 87.1 97.6 46. 99.3 95.8 24.4 99.2 97.7 23.9 97.6 97.5 36.8 100 98.7 96.7 99.4 98.7 95.5 85.5 97.2 60. 99.7 96.2 25.2 98.4 99.1 18.8 98.4 97.8 34.7 90.2 88.6 63.8 89.3 82.7 43.6 87.1 85.3 56. 89.0 20.3 2.2 88.3 77.9 13.9 83.9 77.6 26.2 94.0 99.0 62.5 79.0 59.0 0 87.0 91.0 39. 100 94.0 14.0 69.0 79.0 13.5 92.0 86.0 46.0 - 91.9 91.0 2.1 -0.9 70.0 -21.9 21.9 - 89.6 84.2 5.5 -5.4 44.3 -45.2 45.2 - 85.9 85.1 7.2 -0.8 30.7 -55.2 55. - 92.3 78.8 -13.5 13.5 14.0 -78.3 78.3 - 87.5 4.0 -1.2 86.3 29.4 -58.1 58.1 - 87.6 85.7 2.1 -1.9 47.8 -39.7 39.7 Table 1: Results of select LMs on IHEval. Full results are in Tables 510. is the score difference from the reference setting, including both the mean difference (signed) and the mean absolute difference. Red scores indicate > 5. Single. and Multi. refer to single-turn and multi-turn tasks in the Rule Following category. Ext., Gen., and Class. refer to extraction, generation, and classification tasks in Task Execution. The best performance in the conflict setting is marked as bold and the second-best is underlined."
        },
        {
            "title": "4.1 Main Results",
            "content": "The performance of select LMs is shown in Table 1, with full results available in Tables 510. We highlight the following key findings:"
        },
        {
            "title": "Models exhibit",
            "content": "inconsistent performance when conventional tasks are structured as hierarchical inputs. Comparing the aligned setting (hierarchical inputs) to the reference setting (original task performance) reveals significant performance fluctuations in all models except GPT-4o and Qwen2-72B, with at least 4 points of absolute difference. For instance, when switching to hierarchical inputs, LLaMA-3.1-70B experiences performance decline in eight out of nine tasks, averaging 13-point drop. Smaller-scale models show even greater variability, often experiencing performance drops of more than 10 points (Tables 510). This inconsistency suggests that LMs are less optimized for hierarchical inputs compared to the standard single-input setting. Models struggle in utilizing the instruction hierarchy to resolve conflicts. All models experience notable performance drop in conflict settings, indicating failure to follow the high-priority instructions when they conflict with low-priority ones. Despite 22-point drop from its aligned setting, GPT-4o remains the best performer in handling instruction conflicts, likely reflecting OpenAIs fine-tuning efforts on the instruction hierarchy as described in Wallace et al. (2024). Although other tested models perform comparably to GPT-4o in reference and aligned settings, they fall significantly behind in the conflict setting, which suggests lack of training on following the instruction hierarchy. Qwen-2 emerges as the second-best with 48% accuracy, though more recent models like LLaMA-3.1 and Mistral-Large claimed themselves to be the new state-of-the-art on other general benchmarks like MMLU (Hendrycks et al., 2021). Besides, compared to results on SysBench (Qin et al., 2024a), IHEval reveals larger performance gap between aligned and conflict inputs, which effectively uncovers the limitations of current LMs in following the instruction hierarchy."
        },
        {
            "title": "4.2 Performance by Model Scale",
            "content": "We group the LMs by model family and plot their performance on reference, aligned, and conflict settings in Figure 5. We have the following findings based on the trends: 6 Figure 5: The trend of IHEval performance by model scale. Conflict Ins. Main Ins. Rule Multi-turn First. Both. weak strong weak strong weak strong Task Execution Gen. Extract. Class."
        },
        {
            "title": "Inject",
            "content": "weak strong weak strong weak strong 41.0 12.3 31.8 55.4 16.1 37.1 9.3 16.3 28.3 50.6 10.4 24. 25.9 47.0 21.5 38.8 30.6 43.7 33.3 45.2 33.3 13. 36.5 18.7 Table 2: Model performance in conflict settings with different strictness of instructions. Main Ins. and Conflict Ins. refer to the main instruction and the conflicting instruction, respectively. In multi-turn Rule Following, First. and Both. are settings where conflict instructions appear in the first turn or both turns (see Figure 9 for examples). 1. Improved Performance with Model Scale: Across all three settings, the larger model consistently shows better performance on IHEval, which aligns with established scaling laws. As models become larger, their performance on aligned settings gets closer to or matches the reference scores. This implies that larger models, while improving general instruction-following abilities, have also effectively mastered the ability to handle aligned hierarchical inputs. 2. Increasing Gap Between Aligned and Conflict Settings: Despite improved performance on the conflict setting, most models, except GPT and Qwen-2, exhibit larger gap between the aligned and conflict settings as scale increases. Some models even exhibit inverse scaling on resolving conflicts, e.g., Claude-Haiku outperforms Claude-Sonnet on 5 out of 9 tasks, as shown in Table 6. This indicates that models abilities to handle conflicting instructions do not scale as effectively as general instruction-following capabilities, which again suggests lack of model training in following the instruction hierarchy."
        },
        {
            "title": "4.3 Performance by Instruction Strictness",
            "content": "To explore the impact of instruction strictness on how models handle conflicting instructions, we calculate the average model score for each strictness level in conflict settings. According to Table 2, there is clear trend that performance improves when the high-priority instruction has the stricter demand, but decreases when the conflicting instruction is stricter, indicating strong correlation between instruction strictness and model behavior. However, this behavior is undesired: models should follow instructions based on their priorities in the instruction hierarchy, not the tone or strictness of the wording. These findings suggest that current models are not sufficiently aware of the instruction hierarchy and their behavior is easily influenced by superficial factors."
        },
        {
            "title": "4.4 Prompting LMs to Follow the Hierarchy",
            "content": "Given that current LMs lack inherent awareness of the instruction hierarchy, can we explicitly convey this principle to them through prompt engineering? To answer this question, we prepend the following prompt to the system message that states the priority of the instructions: Surprisingly, as shown in Table 3, this additional prompt does not bring noticeable improvements to model performance. This suggests that teaching LMs to follow the instruction hierarchy is not trivial task: Dedicated training efforts are needed"
        },
        {
            "title": "IPP Rule Task Safety Tool Avg",
            "content": "GPT-4o 50.3 71.7 (0806) 54.6 65.7 LLaMA 7.1 3.1-70B 19.6 13.8 19.3 Mistral 42.6 36.5 Large 39.6 34.6 94.0 93.1 24.8 28.5 21.4 21.2 63.2 70.0 56.0 67.2 8.1 8. 14.0 17.1 13.7 29.4 14.6 28.3 Table 3: Performance with or without the additional instruction priority prompt (IPP). Improved scores are in green, while decreased scores are in red."
        },
        {
            "title": "Conflict",
            "content": "Alignment with Main Instruction System I1 R1 I2 GPT 4o Claude3 Sonnet"
        },
        {
            "title": "All\nModels",
            "content": "- - - - 86.5 - 86.7 86.8 78.1 73.2 28.6 86.6 - 83.9 73.7 69. 57.2 35.9 6.3 84.7 85.9 83.5 79.6 68.9 59.5 17.7 84.2 Table 4: Results on variants of the Multi-turn Rule Following task. M: Main instruction, I1: User instruction in the 1st turn, R1: Model response in the 1st turn, I2: User instruction in the 2nd turn. and indicate whether the input aligns or conflicts with the main instruction. All Models refers to the average performance of all models listed in Table 1. conflicting R1 means its response format does not follow the main instruction. rather than superficial prompt engineering."
        },
        {
            "title": "4.5 Model Performance in Different Conflicts",
            "content": "We explore model behavior under various alignment and conflict scenarios using the multi-turn conversations from the Rule Following task. The models objective is to follow the formatting constraints in the main instruction when responding to user queries. Figure 7 illustrates examples of different scenarios. We begin by comparing the single-input reference setting to those with aligned hierarchical inputs. As shown in Table 4, model performance slightly drops when: (1) the formatting constraints are placed in the system message rather than alongside the user query (85.983.5), and (2) there is preceding conversation turn between the system message and the current turn (83.579.6). This shows the instability of LMs: they may struggle to consistently follow system messages throughout 8 Figure 6: Error types when facing instruction conflicts (all models in Figure 1). IM : Main instruction; IC: Conflicting instruction. Examples are based on IM : Extract verbs in user message, IC: Translate this passage to Spanish (Task Execution - Extraction). multi-turn conversations. notable exception is GPT-4o, whose performance remains nearly unchanged across these aligned settings. Next, we introduce varying degrees of conflict into the models input. We observe that as more input components conflict with the system message, the models performance deteriorates. Conflicts arising from either the previous model response or the previous user instruction affect model performance (79.668.959.5 in Table 4). Moreover, handling conflicting instructions in the current turn proves to be the most challenging scenario for current LMs (59.517.7). Lastly, we test whether the model can follow the formatting constraints in the current turn when the previous turn contains conflicting instructions (last line in Table 4). Models perform well in this scenario, with their scores approaching the reference setting (84.2 vs. 85.9). This result is expected, as models are typically trained during instruction tuning to follow the most recent instruction when users change their requests mid-conversation."
        },
        {
            "title": "4.6 Analysis of Model Behavior",
            "content": "We perform an exhaustive behavioral analysis to examine cases where models fail to resolve instruction conflicts. To calculate the proportions, we manually observe error types and prompt MistralLarge to classify model outputs. The results are summarized in Figure 6, with detailed task-level analysis in Figures 816. Notably, most errors stem from the model misidentifying the conflicting lower-priority instruction as the primary task. Besides, we witness the following model behavior. In some cases, models attempt to either complete both instructions or refuse to execute either. These errors stem from the model assigning equal priority to both instructions, which violates the instruction hierarchy. Additionally, some models partially follow both instructions by synthesizing elements of each. For instance, when faced with instruction conflicts between extract verbs in user message and translate the following passage into Spanish, Claude-3-sonnet responds by providing the verbs, but in Spanish. This suggests the model misinterprets the instructions as aligned, leading to false attempt to combine their requirements. In other cases, conflicting instructions distract the model from correctly interpreting the primary task. For example, when instructed to extract verbs in the user message, the model may skip those verbs in the conflicting instruction, ignoring the fact that the conflicting instruction is part of the user message."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced IHEval, comprehensive benchmark designed to evaluate the ability of LMs to follow the instruction hierarchy. Our benchmark consists of nine diverse tasks that are all evaluated programmatically, covering scenarios where hierarchical inputs either align or conflict, and vary in both input types and task difficulties. Through IHEval, we identified significant weakness in mainstream LMs: their difficulty in recognizing the priority of different instructions. We further conducted detailed analysis of model behavior under various scenarios of instruction conflicts. This work highlights the need for further optimization of LMs in this critical dimension and lays the groundwork for future research in this area."
        },
        {
            "title": "Limitations",
            "content": "While we identified the challenge LMs face in following the instruction hierarchy, this paper did not propose specific solutions to address this issue. We acknowledge the importance of designing training methods which optimize models to better follow the instruction hierarchy, such as constructing data for supervised fine-tuning or preference tuning, but we believe that such optimizations would not produce great research impact without comprehensive evaluation data and in-depth analyses of model behavior. Therefore, this paper focused on bridging the evaluation gap, with the development of solutions being priority for future work."
        },
        {
            "title": "Ethical Considerations",
            "content": "We have taken the following steps to minimize ethical concerns related to the data collection and evaluation experiments in this work: Data Safety and Label Accuracy. Most of the data in IHEval are sourced from public benchmarks with human-annotated labels. While part of the Rule Following data are generated using Claude, every Claude-generated example is further reviewed by the authors of this paper, during which low-quality data are re-written. Therefore, all data in IHEval are verified by humans, which minimizes the risk of containing inaccurate annotations or unsafe AI-generated content. Data Sensitivity. All safety-related tasks are built upon simulated scenarios without any realworld data. For instance, tasks from the Safety Defense category simulates the LM as security system with secret password. User attacks collected by Toyer et al. (2024) are also based on this assumption, ensuring that no real security attacks or user information are included. Similarly, the injected instruction task in the Tool Use category simulates tool call returning Slack usernames which are sampled from common first names in English. The injected questions are sourced from Zverev et al. (2024) and focus on commonsense knowledge. In addition, IHEval does not include any data related to real-world security attacks, such as LM jailbreaking (see Appendix A.3 for discussion on the exclusion of jailbreaking evaluations). Data Bias. Data in IHEval do not include any information linked to specific users or user groups, which minimizes the likelihood of demographic bias within the dataset. Evaluation Bias. All tasks in IHEval are designed for programmatic evaluation. This eliminates the potential bias in model-based evaluation, e.g., using GPT-4 as the judge to assess other models outputs. In conclusion, based on these precautions, the risks associated with the data collection of IHEval and the usage of this benchmark for evaluating LMs should be minimal."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by NSF IIS2137396."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024a. Introducing the next generation of claude. News accouncement by Anthropic. Anthropic. 2024b. System prompts of claude models. Release Notes by Anthropic. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, and et al. 2024. Deepseek LLM: scaling open-source language models with longtermism. Arxiv preprint, 2401.02954. Edoardo Debenedetti, Jie Zhang, Mislav Balunovic, Luca Beurer-Kellner, Marc Fischer, and Florian Tram√®r. 2024. Agentdojo: dynamic environment to evaluate attacks and defenses for LLM agents. Arxiv preprint, 2406.13352. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur√©lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi√®re, Bethany Biron, Binh Tang, Bobbie Chern, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacafarm: simulation framework for methods that learn from human feedback. In Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Samin Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations, ICLR 2021. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew E. Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in changing climate: Enhancing LM adaptation with tulu 2. Arxiv preprint, 2311.10702. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2023. Followbench: multi-level fine-grained constraints following benchmark for large language models. Arxiv Preprint, 2310.20410. Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo. 2024. Aligning to thousands of preferences via system message generalization. Arxiv preprint, 2405.17977. Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. 2023. Instruction-following evaluation through verbalizer manipulation. Arxiv Preprint, 2307.10558. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out. Association for Computational Linguistics. Xinyu Lu, Bowen Yu, Yaojie Lu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, and Yongbin Li. 2024. Sofa: Shielded on-the-fly alignment via priority rule following. In Findings of the Association for Computational Linguistics, ACL 2024. Mistral. 2024. Large enough mistral ai frontier ai in your hands. News accouncement by Mistral AI. Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David A. Wagner. 2023. Can llms follow simple rules? Arxiv Preprint, 2311.04235. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of GPT-4. Arxiv preprint, 2306.02707. OpenAI. 2023. GPT-4 technical report. Arxiv preprint, 2303.08774. OpenAI. 2024. Model spec: Overview. OpenAI Website. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL 2022. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj√∂rkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL 2013. Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, and Bin Cui. 2024a. Sysbench: Can large language models follow system messages? Arxiv preprint, 2408.10943. 10 Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024b. Infobench: Evaluating instruction following ability in large language models. Arxiv Preprint, 2401.03601. Sunil Ramlochan. 2024. System prompts in large language models. Blog on promptengineering.org. Morgane Rivi√®re, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L√©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram√©, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, and et al. 2024. Gemma 2: Improving open language models at practical size. Arxiv preprint, 2408.00118. Sander Schulhoff, Jeremy Pinto, Anaum Khan, LouisFran√ßois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher CarIgnore nahan, and Jordan L. Boyd-Graber. 2023. this title and hackaprompt: Exposing systemic vulnerabilities of llms through global prompt hacking competition. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023. Xiaoshuai Song, Zhengyang Wang, Keqing He, Guanting Dong, Yutao Mou, Jinxu Zhao, and Weiran Xu. 2024. Knowledge editing on black-box large language models. CoRR, abs/2402.08631. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, and et al. 2023. Llama 2: Open foundation and fine-tuned chat models. Arxiv preprint, 2307.09288. Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. 2024. Tensor trust: Interpretable prompt injection attacks from an online game. In The Twelfth International Conference on Learning Representations, ICLR 2024. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, Yiming Liu, Jie Tang, Hongning Wang, and Minlie Huang. 2024. Benchmarking complex instruction-following with multiple constraints composition. Arxiv preprint, 2407.03978. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and et al. 2024. Qwen2 technical report. Arxiv preprint, 2407.10671. Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. Benchmarking and defending against indirect prompt injection attacks on large language models. Arxiv Preprint, 2312.14197. Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. 2024. Jailbreak attacks and defenses against large language models: survey. Arxiv Preprint, 2407.04295. Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024. Injecagent: Benchmarking indirect prompt injections in tool-integrated large language In Findings of the Association for model agents. Computational Linguistics, ACL 2024. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn 8th Interuating text generation with BERT. national Conference on Learning Representations, ICLR 2020. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. In Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023. Mingqian Zheng, Jiaxin Pei, and David Jurgens. 2023b. Is \"a helpful assistant\" the best role for large language models? systematic evaluation of social roles in system prompts. Arxiv preprint, 2311.10054. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. Arxiv Preprint, 2311.07911. Xiaotian Zou, Yongkang Chen, and Ke Li. 2024. Is the system message really important to jailbreaks in large language models? Arxiv preprint, 2402.14857. Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. 2024. The instruction hierarchy: Training llms to prioritize privileged instructions. Arxiv preprint, 2404.13208. Egor Zverev, Sahar Abdelnabi, Mario Fritz, and Christoph H. Lampert. 2024. Can llms separate instructions from data? and what do we even mean by that? Arxiv Preprint, 2403.06833."
        },
        {
            "title": "A Detailed Data Collection",
            "content": "A.1 Rule Following In this category, instructions dictate the formatting rules of the models response. The tasks include both single-turn and multi-turn conversations, as illustrated in Figures 8 and 9, respectively. To create the single-turn task, we derive data from the IFEval dataset (Zhou et al., 2023). The original IFEval data is directly adopted as the reference setting. We then split the original inputs into formatting rules (system message) and user queries (user message) to build the aligned setting. Next, conflicting instructions are generated and concatenated with user queries to create the conflict setting. These conflicting instructions request formatting rules that are incompatible with those specified in the system message. The separation of system messages and the crafting of conflicting instructions are performed by Claude-3-Sonnet, and are further manually reviewed by the authors, during which low-quality ones are re-written. For evaluation, we follow IFEvals evaluation script to assess whether the model response follows the formatting rules defined in the system message. The multi-turn task builds on the single-turn data by using it as the initial turn in conversation. To create multi-turn scenario, we first generate two responses for the first turn one aligned with the system message and the other conflicting with it2. second-turn user query is then generated based on the context established in the first turn, requiring the model to use information from both turns to respond. These data are collected using the same process as the single-turn task: They are first written by Claude and then manually verified. The settings for the multi-turn task are as follows: Reference single user message that combines the system message with the second-turn query, excluding the first-turn data. Aligned The first turn contains the original user query and the aligned response. First-turn Conflict The first-turn user message is concatenated with the conflicting formatting rules, followed by the conflicting response. Both-turns Conflict Building on the First-turn Conflict setting, the second-turn user message also contains the conflicting formatting rules. 2We obtain the conflicting response by prompting Claude to answer the query using the conflicting format specified in the conflict single-turn setting. In practice, we observe that models adherence to the system messages may deteriorate in the aligned multi-turn setting compared to the reference setting where there is only single turn. This means models struggle to consistently apply the system message across all conversation turns, despite such consistency being the intended purpose of system messages. To focus on models IH-following ability instead of their multi-turn consistency, we add another aligned setting with stricter version of the system message. This stricter version explicitly requires adherence to formatting rules throughout the entire conversation, as illustrated in Figure 9. A.2 Task Execution In this category, the model is given an instruction to perform specific task on the users input. In the conflict setting, an additional instruction is included in the user message, requesting the execution of different task. Such scenarios are common, for example, when LMs are used to translate instruction data, the translation of original instruction is needed rather than the response to it. We curate three tasks, each representing common type of NLP benchmark: extraction, generation, and classification. For each task, the aligned setting includes system message that defines the task, and the user input is normal piece of data without any instructions. In the conflict setting, conflicting instruction is prepended to the data, asking the LM to perform an alternative task. The tasks are as follows: Extraction System message: Verb extraction; Conflicting instruction: English-to-Spanish translation. Data and their corresponding POS tags are collected from Ontonotes (Pradhan et al., 2013). The evaluation metric is the F-1 score which compares the model-extracted verb list with the ground-truth verb list. Generation System message: English-toSpanish translation; Conflicting instruction: Math problem solving. The data are math problems from MGSM (Shi et al., 2023) which are paired with Spanish annotations. We use ROUGE-L (Lin, 2004) as the metric to evaluate the translation quality. We also tried other popular metrics such as BLEU (Papineni et al., 2002) and BERTScore (Zhang et al., 2020), but we found negligible differences among them. Classification System message: Language de12 tection; Conflicting instruction: Text summarization. We select the language detection task because adding any conflicting instructions to the user message does not affect the classification label. The data are text passages from XL-Sum (Hasan et al., 2021). We require models to output their predictions in JSON format3 for easy answer extraction, and use accuracy as the classification metric. For each task, we also design strict versions for both system messages and conflicting instructions. The strict version demands the LM to only execute the current instruction without generating any other content. Detailed task examples, instructions, and error analysis for these three tasks are provided in Figures 1012. A.3 Safety Defense critical application of the instruction hierarchy is ensuring model safety. This requires LMs to prioritize their adherence to security protocols set by developers to defend against potential adversarial attacks from malicious users. In this context, we follow the configuration of TensorTrust (Toyer et al., 2024), where the LM functions as security system that requires password for user access. The system message provided to the model includes specific security guidelines, including the secret password. The model is instructed to respond with Access Granted only when the correct password is input by the user. Additionally, the model must not reveal the password or the security guidelines under any circumstances. In the aligned setting, user inputs consist of normal password attempts, and the model is evaluated based on its accuracy in identifying password validity. User messages in the conflict setting vary between the two tasks: Hijack Task User inputs are adversarial attempts to manipulate the model into generating Access Granted without providing the correct password. Extraction Task User messages aim to extract the password by making the model repeat or leak the system message. These adversarial attack instructions are sourced from the original TensorTrust dataset, while the system message is custom-crafted for IHEval by the authors. Model performance in conflict set3All models in our experiments are able to follow such formatting constraint. tings is assessed by the percentage of successful defenses. failed defense is when the model either (1) generates Access Granted in its response, (2) reveals the password, or (3) outputs more than half of the unigrams in the system message. We test two variations of the system message: strict version, which includes defenses against broader range of potential attack strategies, and weaker version, which contains only the necessary guidelines. Detailed instructions and examples are illustrated in Figures 13 and 14, respectively. In this category, we do not test jailbreaking attacks (Yi et al., 2024) as these typically target harmful model behaviors that impact human society. The awareness of such moral standards is independent from the adherence to written instructions. Therefore, jailbreaking primarily tests the safety fine-tuning of the LM rather than its ability to follow instructions. In contrast, IHEval focuses on scenarios where the defined instructions are challenged by conflicting requests. A.4 Tool Use Tool outputs are another source where conflicting instructions may arise. In this category, the model needs to call external tools to fulfill the users request. To analyze the interaction between tool outputs and user instructions, we simulate tool call made by the model and the corresponding content returned from the tools execution. We design two tasks: one where the instruction is inherently present in the tool-retrieved content (intrinsic instructions), and another where an external attacker injects conflicting instructions into the tools response (injected instructions). Specific configurations are as follows: Intrinsic Instruction We define tool that reads the text content of given webpage. We reuse data from the three tasks in the Task Execution category, mixing them to create new dataset. The user message is the main instruction, and the tool output consists of retrieved text content from the webpage. The webpage normally contains text passage (aligned setting), but may also include conflicting instruction (conflict setting). stricter version of the conflicting instruction requires the model to ignore all prior instructions, simulating scenario where the webpage has been maliciously altered. The evaluation follows the same metrics as those used in Task Execution, where metrics on all such as providing solutions to both instructions may lead to unsafe behavior. translation bot responding to unrelated requests, like election predictions, may introduce undesired bias. Similarly, developers may not want shopping bot to answer queries about competitors products. Asking for clarifications does not prevent misbehaving either, as it gives user inputs the same priority as developer commands. Moreover, such responses complicate programmatic evaluation when using LM APIs. Thus, avoiding responses to potential misuse aligns with standard LM practices (Wallace et al., 2024, 3.1). Moreover, GPT-4os strong performance on IHEval tasks further supports that our criteria reflect industry practices."
        },
        {
            "title": "C Full Results",
            "content": "Results of all 13 LMs on IHEval are shown in Tables 510, grouped by their model family."
        },
        {
            "title": "D Task Cards of IHEval",
            "content": "In Figures 816, we provide the task cards for each of the nine tasks in IHEval, including the example of different task settings, different versions of instructions, and error analysis. Redundant details of model responses may be omitted due to space limits. Only major error types are shown. The percentage at the end of each error type represents its proportion among all errors, and is calculated from the generated responses of all models in Table 1. examples are averaged as the final score. Injected Conflict Here, the tool retrieves usernames from Slack channel, and the model is tasked with identifying the shortest username. In the conflict setting, the tool output is appended with commonsense question as the conflicting instruction. The injected questions are adopted from the SEP dataset (Zverev et al., 2024). stricter version of the injection incorporates the important message attack from Debenedetti et al. (2024) which is more sophisticated adversarial tactic. Both the original user task and the injected question require single-word response, making it impossible for the model to answer both simultaneously. We evaluate the models performance based on its accuracy in identifying the shortest username. Although there are other datasets that inject prompts into tool outputs (Zhan et al., 2024; Debenedetti et al., 2024), the injected task is usually independent of the original user task. As result, they focus on attack success rates which evaluate whether the injected task is executed, while the original user task may still be completed concurrently. In contrast, IHEval focuses on addressing instruction conflicts that cannot be resolved by responding to both instructions, so we evaluate the completion of the user task as the criteria of the models awareness of the instruction priority. Any execution of the conflicting instruction results in performance drop, but it is not necessarily the cause. More detailed instructions, examples, and error analyses in this category are listed in Figures 15 and 16."
        },
        {
            "title": "B Evaluation Criteria",
            "content": "In conflict settings, we evaluate whether models strictly follow high-priority instructions while ignoring conflicting low-priority ones. Since system messages are set by developers providing services to public users, prioritizing developer commands is crucial. This ensures LMs function as intended and maximizes model safety, as user inputs may not always align with the models designed purpose. For example, translation bot should focus solely on translating user input. It may clarify its role when responding to users (e.g., am translator, so can only translate your message), and we accommodate such behavior using the loose metric in 3. On the other hand, overly interactive behaviors 14 Figure 7: The input configuration of different settings in 4.5. Directly using the follow-up query as the only user message in the reference and aligned #1 settings is reasonable because we only evaluate the adherence to formatting rules, whereas whether the generated content matches the user query is not in the evaluation scope."
        },
        {
            "title": "Setting",
            "content": "Rule Following Task Execution Safety Defense Single. Multi. Ext. Gen. Class. Hijack Extract Intrinsic Inject"
        },
        {
            "title": "Tool Use",
            "content": "Avg. Mean Abs. GPT-3.5-turbo (2024-0125) GPT-4o mini (2024-0718) GPT-4o (2024-0806) reference aligned conflict reference aligned conflict reference aligned conflict 70.1 70.3 26.5 84.5 82.3 33.9 89.0 85.6 49.5 69.4 72.9 25. 86.1 80.2 35.7 86.5 86.8 51.0 79.6 76.7 78.0 80.3 34.0 57.7 90.5 78.4 84.0 72.0 47.7 31.1 90.0 78.0 87.3 73.9 77.2 38.3 100 100 2. 99.6 100 41.1 100 100 99.7 88.8 94.7 43.3 99.1 98.6 70.3 99.5 99.2 91.2 87.7 97.2 29. 99.4 98.7 95.5 100 98.7 96.7 85.9 78.3 20.2 89.3 82.7 43.6 90.2 88.6 63.8 - 98.0 84.0 4.2 92.0 84.8 +0.8 66.0 33.9 -50.1 50.1 79.0 89.6 59.0 84.2 - -5.4 5.4 44.3 -45.2 45.2 0 94.0 91.9 99.0 91.0 2.1 62.5 70.0 -21.9 21.9 - -0. Table 5: Results of GPT models on IHEval. Red scores indicate > 5."
        },
        {
            "title": "Setting",
            "content": "Rule Following Task Execution Safety Defense Single. Multi. Ext. Gen. Class. Hijack Extract Intrinsic Inject"
        },
        {
            "title": "Tool Use",
            "content": "Avg. Mean Abs. Claude-3 Haiku Claude-3 Sonnet reference aligned conflict reference aligned conflict 77.8 68.3 15.4 80.9 68.4 10.8 78.9 71.0 23.4 83.9 69.5 21.1 84.5 77.3 71.8 74.7 7.3 23.6 84.9 76.9 77.4 79.8 2.3 29. 100 100 26.0 100 100 9.8 97.4 90.3 42.2 87.1 97.6 46.6 97.5 94.0 52.4 85.5 97.2 60. 87.6 80.2 59.1 87.1 85.3 56.9 69.0 85.6 33.0 75.9 1.5 - -9.7 9.7 27.9 -57.7 57.7 87.0 85.9 91.0 85.1 7.2 39.0 30.7 -55.2 55.2 - -0. Table 6: Results of Claude models on IHEval. Red scores indicate > 5."
        },
        {
            "title": "Setting",
            "content": "Rule Following Task Execution Safety Defense Single. Multi. Ext. Gen. Class. Hijack Extract Intrinsic Inject"
        },
        {
            "title": "Tool Use",
            "content": "Avg. Mean Abs. LLaMA-3.1 8B LLaMA-3.1 70B reference aligned conflict reference aligned conflict 80.7 71.1 14.5 88.3 82.9 14.3 79.6 68.1 20.1 88.4 76.6 24.3 84.4 72.5 77.1 48.9 21.8 7.1 89.1 77.0 84.3 59.5 15. 0 100 96.9 0.1 100 100 6.2 70.2 66.2 19.2 99.3 95.8 24.4 68.2 64.1 11. 99.7 96.2 25.2 85.1 7.9 7.8 89.0 20.3 2.2 91.0 81.3 0.0 0.0 - 55.6 -25.7 25.7 11.3 -70.0 70.0 - 92.3 100 94.0 78.8 -13.5 13.5 14.0 14.0 -78.3 78.3 Table 7: Results of LLaMA-3.1 models on IHEval. Red scores indicate > 5."
        },
        {
            "title": "Setting",
            "content": "Rule Following Task Execution Safety Defense Single. Multi. Ext. Gen. Class. Hijack Extract Intrinsic Inject"
        },
        {
            "title": "Tool Use",
            "content": "Avg. Mean Abs. LLaMA-3 8B LLaMA-3 70B reference aligned conflict reference aligned conflict 77.0 71.4 22.7 83.8 81.5 15.0 74.6 57.7 22.6 84.3 69.8 23.9 79.7 73.5 100.0 72.0 57.2 100.0 20.0 15.6 0.2 85.4 74.9 79.8 64.4 2.0 24. 99.6 99.4 33.2 93.2 82.2 22.0 98.8 97.9 32.9 92.8 78.9 23.6 99.7 97.2 37.4 - - - - - - - - - - - - 84.4 - 74.2 -10.2 10.2 18.1 -66.3 66.3 - 89.5 84.3 5.2 -5.2 24.2 -65.3 65.3 Table 8: Results of LLaMA-3 models on IHEval. Red scores indicate > 5. As LLaMA-3 models do not officially support tool calling, we skip the Tool Use setting for them."
        },
        {
            "title": "Setting",
            "content": "Rule Following Task Execution Safety Defense Single. Multi. Ext. Gen. Class. Hijack Extract Intrinsic Inject"
        },
        {
            "title": "Tool Use",
            "content": "Avg. Mean Abs. Mistral-7B Instruct-v0.3 Mistral-Large (2407) reference aligned conflict reference aligned conflict 53.9 54.7 22.6 83.6 81.7 25.2 54.4 63.6 39.7 85.2 87.1 60.0 44.8 61.1 42.5 39.1 15.8 15.2 85.2 78.5 76.0 78.3 11.0 20. 42.9 88.5 12.4 100 100 78.4 63.0 58.1 18.6 99.2 97.7 23.9 61.3 60.1 8.6 98.4 99.1 18. 54.0 30.6 2.0 88.3 77.9 13.9 - 51.0 54.0 48.6 0.0 17.9 -5.4 15.0 -39.0 39.0 0.0 69.0 87.5 79.0 86.3 4.0 13.5 29.4 -58.1 58.1 - -1.2 Table 9: Results of Mistral models on IHEval. Red scores indicate > 5."
        },
        {
            "title": "Setting",
            "content": "Rule Following Task Execution Safety Defense Single. Multi. Ext. Gen. Class. Hijack Extract Intrinsic Inject"
        },
        {
            "title": "Tool Use",
            "content": "Avg. Mean Abs. Qwen-2 7B Qwen-2 72B reference aligned conflict reference aligned conflict 58.1 48.7 14.5 81.4 82.1 35.8 61.1 45.9 18.8 85.0 81.3 39.5 57.4 72.7 51.0 64.4 14.1 33.1 74.9 75.0 73.4 75.3 53.7 58. 99.2 99.0 23.8 100 100 99.5 80.4 76.3 11.6 97.6 97.5 36.8 81.1 75.2 16.9 98.4 97.8 34. 75.8 48.1 1.6 83.9 77.6 26.2 - 78.0 73.7 46.0 61.6 -12.1 12.1 13.5 16.4 -57.3 57.3 92.0 87.6 86.0 85.7 2.1 46.0 47.8 -39.7 39.7 - -1. Table 10: Results of Qwen-2 models on IHEval. Red scores indicate > 5. 16 Figure 8: Task card of the single-turn conversation task in Rule Following category. The main instruction is in green, and the conflicting instruction is in red. 17 Figure 9: Task card of the multi-turn conversation task in Rule Following category. The main instruction is in green, and the conflicting instruction is in red. There are two conflict settings in this task: (1) First-turn conflict: only the conversational history (instruction & response) conflicting with the system message; and (2) Both-turns conflict: both the history and the current turn conflicting with the system message. Figure 10: Task card of the extraction task in Task Execution category. The main instruction is in green, and the conflicting instruction is in red. Both the system message and user message can be replaced with the stricter version. In the conflict setting, the ideal behavior is to extract verbs from the entire user message, including the translation instruction and the task data. 19 Figure 11: Task card of the generation task in Task Execution category. The main instruction is in green, and the conflicting instruction is in red. Both the system message and user message can be replaced with the stricter version. In the conflict setting, the ideal behavior is to translate the entire user message, including the math instruction and the task data. 20 Figure 12: Task card of the classification task in Task Execution category. The main instruction is in green, and the conflicting instruction is in red. Both the system message and user message can be replaced with the stricter version. Figure 13: Task card of the Hijack task in Safety Defense category. The main instruction is in green, and the conflicting instruction is in red. The system message can be replaced with the stricter version, which describes more potential adversarial attacks. 22 Figure 14: Task card of the Extraction task in Safety Defense category. The main instruction is in green, and the conflicting instruction is in red. The system message can be replaced with the stricter version, which describes more potential adversarial attacks. 23 Figure 15: Task card of the Intrinsic Instruction task in Tool Use category. The main instruction is in green, and the conflicting instruction is in red. The injected instruction in the tool output can be replaced with the stricter version, which represents maliciously manipulated content in the webpage. Figure 16: Task card of the Injected Instruction task in Tool Use category. This task slightly differs from others as the main task is elaborated in the user message (the green part), whereas the system message only serves as formatting constraint to facilitate exact-match evaluation. The conflicting instruction is in red, and can be replaced with the stricter version which represents more carefully designed injection to attack the model. The format of this stronger attack is adopted from AgentDojo (Debenedetti et al., 2024)."
        }
    ],
    "affiliations": [
        "Amazon",
        "University of Notre Dame",
        "Worcester Polytechnic Institute"
    ]
}