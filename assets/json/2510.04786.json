{
    "paper_title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning",
    "authors": [
        "Jonas Hübotter",
        "Leander Diaz-Bone",
        "Ido Hakimi",
        "Andreas Krause",
        "Moritz Hardt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 6 8 7 4 0 . 0 1 5 2 : r Preprint. Under review. LEARNING ON THE JOB: TEST-TIME CURRICULA FOR TARGETED REINFORCEMENT LEARNING Jonas Hübotter,1 Leander Diaz-Bone,1 Ido Hakimi1 Andreas Krause1 Moritz Hardt2 1ETH Zürich, Switzerland 2Max Planck Institute for Intelligent Systems, Tübingen, Germany"
        },
        {
            "title": "ABSTRACT",
            "content": "Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can model do the same? We propose an agent that assembles task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from large pool of available training data. Our experiments demonstrate that reinforcement learning on test-time curriculum consistently improves the model on its target tasks, across variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of testtime curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time. Code Models Data Logs Figure 1: Test-time curricula (TTCs) lead to remarkable improvements in math and coding by practicing on self-curated task-related problems at test-time. The plots show the pass@1 test accuracy of Qwen3-8B throughout its test-time training. Our method, TTC-RL (solid red line), consistently improves performance, learning faster and achieving higher final accuracy than standard RL post-training (dashed gray line). Notably, the final pass@1 accuracy of TTC-RL approaches the models initial pass@8 performance (dotted gray line), which represents proxy for the performance ceiling of the initial model. The stars indicate the final pass@8 values after TTC-RL, demonstrating significant improvement over the initial pass@8, which indicates that the model learns new solution strategies at test-time. Equal contribution. Correspondence to Jonas Hübotter jonas.huebotter@inf.ethz.ch. 1 Preprint. Under review."
        },
        {
            "title": "INTRODUCTION",
            "content": "We study how large language models (LLMs) can continually improve at reasoning on their target tasks at test-time. Increasing test-time compute, for example, by extended use of context as scratch space, has recently emerged as key direction for improving LLMs on challenging tasks such as math and coding (Jaech et al., 2024; Guo et al., 2025; Kimi et al., 2025). Test-time scaling has been driven primarily by extensive general-purpose reinforcement learning (RL; Guo et al., 2025), where the LLM learns how to effectively use its context for reasoning. However, since the context of LLMs is bounded and becomes exceedingly expensive to expand, an LLM cannot learn in-context from experience over long timeframes. One promising technique for overcoming this challenge is test-time training (TTT; Sun et al., 2020; Hardt & Sun, 2024), which continues training the model at test-time after being given task. Previous work has studied TTT via supervised fine-tuning on human-created or expert data, either retrieved (Hardt & Sun, 2024; Hübotter et al., 2025) or provided as few-shot examples (Akyürek et al., 2025). Other work has instead focused on TTT in the context of recurrent neural networks (Sun et al., 2025; von Oswald et al., 2025; Zhang et al., 2025b), aiming to replace the costly attention-based context in Transformers (Vaswani et al., 2017) with fixed-size state (i.e., the model itself), but losing some of the advantages of reasoning over an uncompressed scratchpad. We explore complementary approach to test-time scaling, where an LLM is continually trained on self-curated training tasks related to its target task, while practicing on each individual training task in-context. This leverages the Transformers attention as an uncompressed scratchpad for short-term ideation, while meta-learning strategies for leveraging that context across long-term, task-specific experience. We propose test-time curriculum (TTC) agent that automatically designs its own curriculum of training tasks by selecting the relevant tasks for the job from large corpus of existing tasks. The agent then attempts tasks in its curriculum, and compresses the gathered experience into its weights via RL. The automatic self-guided curriculum design avoids laborious human curation of datasets, and enables training on purpose-built curricula at test-time. We find that this reinforcement learning on test-time curricula (TTC-RL) leads to remarkably improved reasoning on target tasks. In particular, we find that TTC-RL improves the pass@1 of several strong LLMs across diverse reasoning tasks, covering competition math, coding, and scientific reasoning (cf. Figure 1). We further identify that TTC-RL is complementary to other means of test-time scaling, effectively improving pass@k and maj@k even at large k. Notably, we find that TTC-RL can overcome the limitation of fixed context windows by observing that non-thinking model (limited to 8k context tokens) with TTC-RL can perform similarly to the same model thinking for 30k tokens in-context. This demonstrates that during TTC-RL, the model continues learning how to think effectively for its target tasks. Our results suggest such targeted RL as promising new direction for LLM agents that continually improve at test-time through many interactions with an environment. Figure 2: TTC-RL performs targeted practice on similar problems to the target task at test-time. The agent is given target task (red) and self-curates curriculum of related tasks (blue). It then explores solution strategies on this curriculum, reinforcing successful approaches (). This experience enables the agent to more effectively solve the original, more difficult target task. We summarize our contributions as follows: 1. We propose TTC agent for targeted RL (3): We propose test-time curriculum agent which at test-time when given target task, self-selects related training tasks from diverse corpus. The agent then learns from its own experience of attempting those tasks via RL. 2. TTC-RL improves reasoning on target tasks (4): Across several models and tasks, TTCRL consistently improves pass@1 substantially faster than general-purpose RL post-training on standard RL datasets, and saturates at higher accuracy. Next, we identify that TTC-RL substantially raises the performance ceiling of the model (pass@k) and demonstrate that it is complementary to existing approaches to test-time scaling. Finally, we find that TTC-RL 2 Preprint. Under review. yields strongly specialized models that perform remarkably well on their target tasks, even when compared to models that are allowed to think for tens of thousands of tokens in context. 3. Measuring latent improvements in reasoning (5): The evaluation of RL-trained models faces the challenge of estimating whether improved scores are due to better reasoning or merely learning the expected output format. We introduce new metric, latent improvement, which computes lower bound on the improvement in reasoning due to RL training, and find that TTC-RL leads to substantial improvements in latent reasoning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Test-time scaling and general-purpose RL training. common strategy for improving LLM performance in challenging domains is to allocate additional test-time compute, for instance, through majority voting (Snell et al., 2025), search with reward model (Lightman et al., 2023; Wang et al., 2024a; Setlur et al., 2025a), or by identifying consistent patterns among parallel rollouts (Wang et al., 2023; Huang et al., 2025a). The potential of such methods is often measured by pass@k, which describes the performance ceiling with generations (Chen et al., 2025b). More recently, scaling test-time compute via in-context reasoning (Brown et al., 2020; Wei et al., 2022) has significantly improved performance in domains like math and coding (Jaech et al., 2024). This capability is commonly enabled by large-scale, general-purpose RL training on diverse tasks (Lambert et al., 2024; Ma et al., 2025; Guo et al., 2025; Kimi et al., 2025), during which models learn to reason within their bounded context (Setlur et al., 2025b), which connects to the broad topic of meta-learning (Schmidhuber, 1987; Duan et al., 2017; Finn et al., 2017). This paradigm is related to goal-conditioned RL (Schaul et al., 2015; Andrychowicz et al., 2017) where several works have studied automatic curriculum learning (Warde-Farley et al., 2018; Pitis et al., 2020; Pong et al., 2020), first proposed by Bengio et al. (2009). In contrast to improving general-purpose models, our work employs RL to train specialized reasoners for particular target task at test-time. Self-play. specialized form of curriculum learning has proven highly successful in domains like games through the use of self-play (Schmidhuber, 1991; Silver et al., 2016), where an agent is repeatedly challenged by playing against itself. Seminal works show that this approach can lead to superhuman performance (e.g., Mnih et al., 2015; Silver et al., 2016; 2017; Berner et al., 2019). Several recent works aim to generalize this paradigm to LLMs and more general domains such as coding by self-generating training curriculum (Zhao et al., 2025; Huang et al., 2025b; Chen et al., 2025a; Fang et al., 2025). While recent work has studied test-time curricula as an extension of self-play to goal-conditioned RL settings (Diaz-Bone et al., 2025), its evaluation has focused on simple robotic navigation tasks. We extend this line of work to challenging reasoning tasks by self-curating training curriculum, enabling LLMs to continually learn from extensive experience on single task (Silver & Sutton, 2025; Shen et al., 2025). Test-time training and test-time RL. Training model at test-time for given input has been widely studied as TTT (Sun et al., 2020), using supervised (Hardt & Sun, 2024; Hübotter et al., 2025; Yu et al., 2025a; Bertolissi et al., 2025; Bagatella et al., 2025a) or self-supervised losses (Sun et al., 2025; Dalal et al., 2025). Several methods perform TTT in purely unsupervised manner, i.e., without real-world data or feedback (Wang et al., 2021; Zhang et al., 2022). Most relevant to our work, Zuo et al. (2025) recently extended unsupervised TTT to perform RL on the test set, leveraging the models majority votes as pseudo-labels. This connects to broader theme of unsupervised RL (Zhang et al., 2025a; Shao et al., 2025; Zhou et al., 2025; Prabhudesai et al., 2025) and self-improvement in LLMs (Zelikman et al., 2022; Gulcehre et al., 2023; Lee et al., 2025)."
        },
        {
            "title": "3 TEST-TIME CURRICULA",
            "content": "We consider the set of target tasks = {x 1, . . . , } given at test-time, and our goal is to specialize an existing model through further training to those tasks. For training, as in general-purpose RL, we rely on an existing large corpus of training tasks = {(xi, vi)}N i=1, for each of which vi() {0, 1} verifies whether an attempt was correct. To specialize, it is common practice to construct particular subset (cid:98)D from D, and we call such targeted subset test-time curriculum for D. We seek to make test-time training on such curriculum scalable. To this end, we propose to go beyond human-curated test-time curricula and let the initial model craft its own test-time curriculum. 3 Preprint. Under review. The previous works of Hardt & Sun (2024) and Hübotter et al. (2025) have studied self-curated testtime curricula with supervised fine-tuning (SFT), and have shown that this can improve language modeling, i.e., lead to lower perplexity. However, this approach is limited since it requires the corpus to specify how training tasks are to be solvednot only to verify whether solution is correct. Moreover, mirroring recent observations on the robustness of on-policy RL (Shenfeld et al., 2025), we observe that SFT on expert traces often leads to an initial drop in performance on downstream tasks, suggesting that SFT is ill-suited for TTT with LLMs. We provide further details in Appendix A."
        },
        {
            "title": "3.1 AUTOMATIC TTCS FOR TARGETED RL",
            "content": "We therefore focus on on-policy RL and extend the previous work on automatic data selection for TTC-SFT (Hardt & Sun, 2024; Hübotter et al., 2024; 2025) to automatic task selection in TTC-RL. We adopt SIFT (Hübotter et al., 2025), which selects those examples from the corpus that the model deems most informative for the target tasks. SIFT has hyperparameter λ, which reflects the models ability to learn from the seen examples, and which explicitly trades between diversity of the selected examples and their relevance to the target tasks. We find that our results are robust to the choice of λ and generally set λ = 0.1 in our experiments. To determine which examples are most informative, SIFT leverages latent representation space ϕ of token sequences for which we use the normalized last-token last-layer embeddings of the initial model. Appendix gives examples for such self-curated test-time curricula. This pipeline leverages the semantic understanding of the initial model to self-curate test-time curriculum for the target tasks. We then train on this test-time curriculum via GRPO (Shao et al., 2024), as shown in Algorithm 1.1 Note that test-time training does not necessitate the model to stay close to its initialization since it needs to generalize only to its target tasks, and hence, we omit the KL penalty of GRPO. We include background on SIFT and GRPO in Appendix B. In an extension, we evaluate test-time curriculum that automatically selects tasks of the right difficulty, which we show to further accelerate learning on weaker models (cf. Appendix C). Algorithm 1 Test-Time Curriculum for Targeted RL Require: Test tasks 1: {(xt, vt)} SIFTλ,ϕ,T,D(D) 2: for = 0, 1, . . . , 1 do {ˆyt+1,i} πt( xt+1) 3: {rt+1,i} vt+1({ˆyt+1,i}) 4: θt+1 GRPO(θt, {ˆyt+1,i}, {rt+1,i}) 5: 6: end for attempt verify RL step select curriculum 3.2 DIVERSE CORPUS FOR GENERAL-PURPOSE RL POST-TRAINING To study the effectiveness of our proposed adaptive test-time curriculum, we leverage large corpus of high-quality verifiable training data, suitable for post-training model across diverse domains. We assemble new meta-dataset, which we call the verifiable-corpus and which combines approximately 265k diverse training tasks, spanning three environments: Exact answer match / Math: For math problems with numerical answer, we determine answer equivalence using math-verify. Our corpus contains the training splits of GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), and the DAPO math dataset (Yu et al., 2025b), covering numerically verifiable math problems for wide range of difficulties. Judged answer match / General reasoning: Measuring the validity of complex reasoning requires more robust verification than symbolic equivalence checks. Given (potentially long) golden answer, we use 1.5B-parameter verifier model trained by Ma et al. (2025) to determine whether attempted and golden answers are semantically equivalent. Our corpus contains the Webinstruct-verified dataset (Ma et al., 2025), which covers wide variety of subjects ranging from natural sciences to history. Unit tests / Code: Finally, we combine several sources of coding tasks. Each coding task is verified by set of unit tests. Our corpus combines tasks from APPS (Hendrycks et al., 2021a), code contests (Li et al., 2022), TACO (Li et al., 2023), PrimeIntellect (Mattern et al., 2025), Leetcode (Xia et al., 2025), the Codeforces training split (Penedo et al., 2025) and all LiveCodeBench tasks (Jain et al., 2024) prior to February 1, 2025. 1Algorithm 1 abstracts that we perform each RL step over batch of training tasks and that we perform RL training for multiple episodes. Preprint. Under review. Model AIME24 AIME25 MATH500 Codeforces CodeElo LCBv6 GPQA-D Qwen3-8B + RL post-training + TTC-RL 21.67 41.67 50.83+29.2 23.33 38.33 41.67+18.3 Qwen3-4B-Instruct-2507 52.50 55.83 + RL post-training 60.00+7.5 + TTC-RL Qwen3-8B-Base + RL post-training + TTC-RL 15.83 22.50 30.00+14.2 40.83 47.50 45.83+5. 14.17 20.83 21.67+7.5 69.55 82.50 85.10+15.6 72.00 86.30 88.50+16.5 63.10 76.85 78.15+15.1 20.85 27.83 33.35+12.5 26.70 28.39 34.99+8. 9.92 17.46 17.84+7.9 13.73 22.67 29.34+15.6 20.27 21.18 27.20+6.9 6.67 9.97 11.33+4.7 20.61 25.95 27.29+6.7 21.56 25.95 26.91+5. 11.26 18.51 17.94+6.7 49.11 56.47 58.38+9.3 61.93 62.82 61.93+0.0 29.70 42.77 45.94+16.2 Table 1: Performance of TTC-RL on reasoning benchmarks. We evaluate TTC-RL across benchmarks for math (AIME24, AIME25, MATH500), coding (Codeforces, CodeElo, LCBv6), and scientific reasoning (GPQA-D). Numbers in bold denote the best performance for given model backbone, and we use + to denote the improvement over the initial model in percentage points. We perform filtering step where we remove training tasks with empty answers or less than 5 unit tests, to ensure reliable training signal. Finally, we deduplicate and decontaminate the corpus, as detailed in Appendix E.1. We openly share the corpus and our environment implementations to support future research. To our knowledge, the verifiable-corpus is one of the first public corpora of high-quality verifiable tasks, spanning several domains and environments. We envision that, building on this work, future efforts will ultimately enable TTC agents to utilize any relevant training tasks they find on the web (similarly to retrieval-augmented generation; Lewis et al., 2019), or to self-generate their own training tasks (see, e.g., Zhao et al., 2025)."
        },
        {
            "title": "4 RESULTS",
            "content": "We focus our evaluation on diverse set of target tasks in math, coding, and scientific reasoning. Specifically, we evaluate test-time curricula for high-school-level competition math questions in AIME 24 & 25 and MATH500 (Hendrycks et al., 2021b). We evaluate coding ability on Codeforces (Penedo et al., 2025), CodeElo (Quan et al., 2025), and on LiveCodeBench v6 (Jain et al., 2024), i.e., tasks released after February 1, 2025. Finally, we evaluate scientific reasoning with GPQA-Diamond (Rein et al., 2024) which covers questions in biology, physics, and chemistry. TTC-RL can be applied to each task within benchmark individually or to the entire benchmark on aggregate, treating it as set of target tasks. We primarily evaluate TTC-RL per-benchmark as this yields greater statistical significance under limited compute budget. We then perform an ablation, indicating that per-task TTCs performs at least on-par with per-benchmark TTCs (cf. Section 4.2). To ensure that our evaluation is accurate, we adopt evalchemy (Raoof et al., 2025) and synthesize system prompts to be consistent across benchmarks (cf. Appendix E.2). We generally train for two episodes with batch size 8 and 16 rollouts per train task,2 and measure avg@4 on the set of test tasks once every ten steps. To further reduce noise, we compute moving average across three validation steps. Finally, in our summarized numeric results, we report the highest averaged avg@4, and include detailed plots of avg@4 per step in Appendix D.2. We perform our main evaluation on the non-thinking models Qwen3-8B (Yang et al., 2025) and the more recent Qwen3-4B-Instruct-2507, whose responses we limit to 8192 tokens. We additionally evaluate on the Qwen3-8B base model. We opt for non-thinking models due to the high computational cost of running thinking models over long contexts, typically of up to 32k tokens. The goal of our TTC framework is to show that models can improve at test-time, even without further expanding their context. We hypothesize that our results extend to thinking models, which simply have larger maximum response length. 2We summarize all training hyperparameters in Appendix E.3. Preprint. Under review. Figure 4: TTC-RL scales test-time compute in way that is complementary to other means of test-time scaling. Left: The pass@k of TTC-RL on Qwen3-8B, averaged over benchmarks, increases substantially for small and large k, indicating that TTC-RL raises the models performance ceiling. Middle: TTC-RL also improves the performance of majority voting (across math and GPQA-D), with the initial pass@1 significantly outperforming maj@64 on the initial model. Right: We evaluate Qwen3-8B in non-thinking and thinking mode, as well as the non-thinking model + TTC-RL. The color indicates the relative accuracy per column. We find that TTC-RL significantly improves the non-thinking model, allowing it to perform close to the thinking variant in several domains, despite reasoning over 8k rather than 30k context tokens. Main results. We summarize our main results in Figure 1 and Table 1. We find that TTC-RL significantly improves accuracy across range of models and all benchmarks. Notably, it also leads to significant performance gains on top of Qwen3-8B-Base within only relatively few RL steps, indicating that TTCs lead to sample-efficient training. Our main baseline is model that is trained on 1k uniformly chosen training tasks from the corpus, to which we refer to as standard RL post-training, since this method yields general-purpose model. We compare this to TTC-RL with curriculum of size 1k and find that training on test-time curriculum accelerates learning significantly and leads to saturation at substantially higher performance.3 Notably, Qwen3-8B with TTC-RL performs on-par with strong closed-source non-thinking models; for example, it approximately matches GPT-4o2024-08-06 on LCBv6. In Figure 3, we further ablate the size of the curriculum and find that TTC-RL consistently outperforms general-purpose RL post-training across wide range of curriculum sizes. Interestingly, at dataset size 1though performing poorlythe general-purpose RL post-training outperforms TTC-RL. We suspect that this may result from TTC-RL picking practice task that is very similar to the test tasks, in which case overfitting may harm more than when overfitting to less related task. Figure 3: TTC-RL substantially outperforms general-purpose RL post-training for range of data sizes. We evaluate Qwen3-8B on all seven benchmarks and report the average test accuracy when training for 250 steps. Takeaway 1 TTC-RL substantially improves accuracy on wide variety of models and benchmarks, compared to models initial performance and after (continued) RL post-training on our corpus. 4.1 TTCS ARE COMPLEMENTARY TO EXISTING APPROACHES TO TEST-TIME SCALING Next, we demonstrate that TTC-RL improves the LLMs ability for test-time scaling. TTCs raise the models performance ceiling. While the improvement in accuracy demonstrates that during TTC-RL, the model learns to better reason within context, we ask whether the model improves more broadly. common metric to understand models performance ceiling for testtime scaling is the pass@k metric, which measures whether any one of attempts is correct (Chen 3In Appendix D.3, we additionally compare to an RL post-training baseline that only samples training tasks from the test environment and show that this yields comparable results. Preprint. Under review. et al., 2025b). Recent work has repeatedly shown that RL-training tends not to improve pass@k at large (Yue et al., 2025), leading to the concern that RL-training is simply distilling pass@k into pass@1. In Figure 4 (left), we instead observe that TTC-RL significantly improves pass@k across wide range of k. Similarly, TTC-RL also improves the realized performance gains of majority voting, as can be seen in Figure 4 (middle), and notably increases the pass@1 well beyond the maj@64 after continued RL post-training. Our results indicate that two key factors lead to the performance of TTC-RL: Improvements to the RL training algorithm that also apply to our general-purpose RL-training baseline, as well as the specific data selected by the TTC agent, as indicated by the strong improvement in majority voting. We provide more detailed discussion in Appendix D.1. Developing better understanding of the circumstances under which RL-training can discover new behavior, leading to improved pass@k, is an exciting direction for future research. TTC-RL with short-context LLM can perform close to long-context LLM. We also seek to better understand how TTC-RL relates to reasoning over long contexts. To this end, we evaluate the non-thinking and thinking variants of Qwen3-8B, limited to 8k and 30k tokens per response, respectively. In Figure 4 (right), we find that TTC-RL on the non-thinking model performs close to the thinking model in several domains, particularly in coding and GPQA.4 Further, note that the asymptotic cost of growing context in Transformer is quadratic (Vaswani et al., 2017), whereas the asymptotic cost of TTC-RL is linear (since experience is compressed into the models weights). This suggests that there is regime in which, given fixed compute budget, TTC-RL outperforms further scaling of context size. We believe that studying this compute-optimal Pareto frontier is an exciting topic for future research. Our results indicate that to further improve the performance of LLMs, test-time curricula may eventually be advantageous over continued scaling of context size. Takeaway 2 Test-time curricula substantially increase the pass@k performance ceiling of model and can perform similarly to models which are reasoning over much larger context. This indicates the potential of TTCs to complement existing approaches to test-time scaling. 4.2 TTCS EFFECTIVELY SPECIALIZE MODELS To determine whether the test-time curriculum specializes the model to its target tasks, we conduct straightforward experiment: We evaluate each final checkpoint of TTC-RL on all benchmarks, including those that were not part of the set of target tasks. We summarize the results in Figure 5 (right), with columns corresponding to evaluation and rows corresponding to training. We find that after TTC-RL, models perform best on their target tasks, while severely underperforming on tasks that are unrelated to the target tasks. Moreover, we identify block-diagonal structure, where models generalize better across mutually related groups of tasks, particularly among similar math benchmarks. We also find that models appear to generalize better from coding to math than vice versa, and models generalize better from code and math to GPQA than vice versa. TTCs for individual tasks. Aspirationally, we anticipate test-time curricula to enable continual learning for single test task over long timeframe. While we focus our main evaluation on the setting where test-time curricula are applied per benchmark, we run an ablation with 30 separate TTCsone per AIME 25 question. The results in Figure 5 (left) demonstrate that specializing to an individual test task can outperform broader specialization to group of test tasks. This shows that TTC-RL does not depend on larger set of test tasks to implicitly lead to diverse data and robust training, and instead seamlessly extends to fully test-time setting with only single task given. We find, however, that more fine-grained specialization does not always lead to further performance gains. We evaluate training separate TTCs for each of biology, physics, and chemistry in GPQA, leading to approximately the same performance as joint TTC. In our view, gaining better understanding for how much specialization is helpful is an exciting direction for further research. Takeaway 3 Test-time curricula effectively specialize the model to their target tasks. When applied to an individual target task, TTC-RL can be seen directly as method for test-time scaling. 4In MATH500, non-thinking Qwen3-8B + TTC-RL (85%) even outperformed the thinking variant (77%). Preprint. Under review. Figure 5: Left: Per-task TTC-RL outperforms benchmark-level TTC in AIME25. We perform TTC-RL and maj-TTRL (cf. Section 5.2) on Qwen3-8B, and find that per-task TTC-RL even outperforms the benchmark-level TTC. Middle: TTC-RL improves correctness of reasoning, not only learning the answer format. We evaluate the difference in accuracy between TTC-RL and the initial Qwen3-8B, averaged over benchmarks. The latent improvement is lower bound on the accuracy gain that is not due to merely learning the format (cf. Section 5.1). Right: TTC-RL yields models that are specialized to their target tasks. We plot the accuracy of Qwen3-8B trained for given target tasks (rows) when evaluated on other benchmarks (columns). We normalize accuracies across all evaluations of particular benchmark. Notably, the model trained via TTC-RL for the right target tasks (i.e., the diagonal) always performs best."
        },
        {
            "title": "5 FURTHER ANALYSIS",
            "content": "5.1 ESTIMATING REAL IMPROVEMENT When evaluating RL-trained models on verifiable tasks, reasonable concern is whether the Indeed, we find that if the initial model simply learns to adhere to the expected output format. model is not able to consistently produce well-formed responses, RL-training tends to quickly teach the model the expected output format. Therefore, disentangling shallow learning of format from improvements in models latent reasoning is critical for accurate evaluation. Ideally, we would like to measure whether the models reasoning improves throughout trainingregardless of whether we can automatically parse and evaluate responses. We propose to measure models latent improvement (LI) during RL training as follows. Consider the event of an answer being marked as accurate by the verifier, which occurs if it is well-formed (i.e., it can be extracted and interpreted) and if the models latent reasoning is correct. Based on this, straightforward lower bound on correctness is simply P(correct) P(accurate). To measure the improvement in correctness throughout RL training, we make the following intuitive assumption: Assumption 1. We assume that being well-formed does not reduce the chance of being correct. Formally, we assume P(correct well-formed) P(correct), i.e., non-negative association of formedness and correctness. Intuitively, this assumption states that an ill-formed response does not increase the likelihood of correct latent reasoning. This yields straightforward upper bound on the probability of correct latent reasoning: P(correct) P(accurate)/P(well-formed) if P(well-formed) > 0. Thus, the improvement in correctness after RL steps is lower bounded as Latent Improvement := P(correctT ) P(correct0) P(accurateT ) P(accurate0) P(well-formed0) . (1) Measuring latent improvement. We consider response as ill-formed if we cannot extract an answer, e.g., because the response was truncated at the max-token limit or because the completed response did not contain an extractable answer. We note that to reliably measure LI, it is essential 8 Preprint. Under review. to ensure that answer extraction is strict.5 In Figure 5 (middle), we measure the latent improvement of Qwen3-8B, and find that under Assumption 1, TTC-RL leads to substantial latent improvement. We include our complete results in terms of LI in Table 7 of Appendix D."
        },
        {
            "title": "5.2 TOWARDS CONTINUAL SELF-IMPROVEMENT AT TEST-TIME",
            "content": "We consider this work as first step towards agents that continue learning at test-time and specialize without requiring human supervision. The recent work of Zuo et al. (2025) can also be seen as step in this direction by proposing to train on the test set directly, using majority votes as surrogate rewards (maj-TTRL). Since Maj-TTRL relies on majority votes as its training signal, it can be applied only to environments with structured outputs such as our math environment with numerical answers or the multiple choice GPQA. In contrast, our proposed TTCs can be applied in any environment where reward signal can be defined. We perform comparison to Zuo et al. (2025) in Table 2 and find that Maj-TTRL leads to significant gains in accuracy across math benchmarks, but helping less in GPQA. We emphasize that Maj-TTRL and test-time curricula are complementary approaches, e.g., one can perform Maj-TTRL directly after TTC-RL, which we find to outperform Maj-TTRL alone (cf. Figure 12 in Appendix D.4). Notably, the performance gains of Maj-TTRL on the strong Qwen3-4B-Instruct-2507 model in AIME 24 & 25 suggest that the returns from our proposed implementation of TTC-RL are constrained by the scope of its fixed training corpus. This saturation does not imply ceiling on the models capabilities; rather, it may indicate promising opportunity for self-improvement methods such as Maj-TTRL or synthetic data generation (e.g., Zhao et al., 2025; Zweiger et al., 2025), which may be combined with or extend TTCs. 5.3 ON CONTAMINATION AND REWARD HACKING Model Math Code GPQA-D Qwen3-8B-Instruct + Maj-TTRL 52.63 + TTC-RL 59. 29.99 Qwen3-4B-Instruct-2507 + Maj-TTRL 69.49 64.78 + TTC-RL 29.70 51.14 58.38 62.44 61.93 Table 2: The competitive performance of Maj-TTRL on our strongest model suggests that TTC-RLs effectiveness is constrained by its fixed training corpus. Combining our approach with selfimprovement techniques is therefore an exciting direction for future work. The performance gains from TTC-RL are remarkable: for example, in AIME24 and CodeElo, the pass@1 of the strong Qwen3-8B more than doubles within only few hundred training steps. This naturally raises the question of potential confounding factors. To mitigate this risk, we took several steps: we extensively decontaminated our corpus by removing tasks that overlap with the test sets, implemented safeguards against reward hacking within our code environment, and manually reviewed several model responses. While we base our evaluation on the widely used evalchemy package (Raoof et al., 2025), we found significant flaw in the evaluation of Codeforces and CodeElo, where some (and frequently all) private test cases were leaked into the prompt as examples. This enables strong model to solve task simply by handling each test case individually. To mitigate this, we removed all input/output examples from the prompts of Codeforces and CodeElo, and also ensured that private test cases are not leaked in tasks from our training corpus. remaining limitation is that we cannot guarantee the cleanliness of the models original pretraining data. To account for this possibility, we evaluate on LCBv6, which consists of coding tasks that were released since February 2025. Hence, TTC-RLs performance gains on LCB makes pre-existing contamination less likely explanation for our results. Furthermore, we compare TTCRL to an oracle that trains directly on the test tasks, finding that our method learns slightly more slowly and levels off at lower accuracy (cf. Figure 14 in Appendix D). We believe our findings on the importance of data selection (cf. Figure 1) and improvements to the RL training algorithm to facilitate exploration (cf. Appendix D.1) offer plausible explanations for these results. We further include qualitative examples demonstrating the improvements in reasoning in Appendix F. 5If answers are extracted, which are not intended as answers by the model, this artificially inflates LI and violates Assumption 1. To ensure this, we only extract the contents of boxed{} or the contents wrapped in , for math and code, respectively. 9 Preprint. Under review."
        },
        {
            "title": "6 DISCUSSION",
            "content": "We propose test-time curriculum agent that self-curates sequence of training tasks to specialize towards specific target task via reinforcement learning. We demonstrate that TTCs achieve remarkable performance gains across multiple models and diverse reasoning benchmarks, significantly raising the performance ceiling of strong initial models through specialization to their target task. To better evaluate these gains, we introduce the latent improvement metric, which measures genuine improvements in reasoning correctness. Our experiments confirm that TTCs yield substantial gains in latent improvement. This highlights the potential of currently underutilized compute regime: targeted test-time training, which sits between large-scale general-purpose training and frozen test-time scaling. While standard next-token prediction relies on models intuition and reasoning allows it to leverage context for deliberation, our proposed test-time curriculum enables the model to meta-learn how to reason for particular target task at test-time. Similarly, when humans begin new job, they often train for weeks or months before being able to solve all required tasks. During this time, they collect experience on dozens of tasks that are similar, becoming more efficient at solving their jobs target tasks. In demonstrating the potential of such targeted test-time training, our work opens up several exciting research directions. natural direction is to move beyond the bottleneck of fixed task corpus through self-generated TTCs, which may still use human-created tasks as inspiration. Further avenues include improving the sampleand step-efficiency of TTC-RL through advancing methods for RL training. This also raises questions about scaling laws for this new regime: for instance, at what context length does it become more advantageous to scale TTC-RL rather than increasing the context window? Looking beyond single-task specialization, TTCs might be extended to dynamic settings where an agent must adapt to an evolving set of target tasks. Finally, TTC-RL could be used to unconfound benchmark evaluations by providing standardized method for specializing all models to test task (Dominguez-Olmedo et al., 2025), enabling fairer comparison of their core capabilities."
        },
        {
            "title": "CONTRIBUTIONS",
            "content": "JH and IH conceived of the project, with input from MH on the direction of TTC-SFT. JH led the project and writing, with significant help of LDB. JH and LDB worked equally on implementation and experiments. IH supported infrastructure. IH, AK, and MH advised the project."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Akira Yoshiyama who supported our implementation of curricula (i.e., dynamic datasets) in verl. We further thank Matthias Otth who developed our results with SFT on GSM8K, indicating that SFT is ill-suited for TTT in LLMs and motivating this project. Finally, we thank Yu Sun for helpful discussions and Marco Bagatella for feedback on an early version of this paper. This project was supported through the Swiss AI compute grant a156. JH was supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545. IH was supported by an ETH AI Center Postdoctoral fellowship."
        },
        {
            "title": "REFERENCES",
            "content": "Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning. In ICML, 2025. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In NeurIPS, 2017. Marco Bagatella, Mert Albaba, Jonas Hübotter, Georg Martius, and Andreas Krause. Test-time offline reinforcement learning on goal-related experience. arXiv preprint arXiv:2507.18809, 2025a. 10 Preprint. Under review. Marco Bagatella, Jonas Hübotter, Georg Martius, and Andreas Krause. Active fine-tuning of multitask policies. In ICML, 2025b. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, 2009. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Ryo Bertolissi, Jonas Hübotter, Ido Hakimi, and Andreas Krause. Local mixtures of experts: Essentially free test-time training via model merging. In COLM, 2025. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020. Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Self-questioning language models. arXiv preprint arXiv:2508.03682, 2025a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@k training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025b. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. One-minute video generation with test-time training. In CVPR, 2025. Leander Diaz-Bone, Marco Bagatella, Jonas Hübotter, and Andreas Krause. Discover: Automated curricula for sparse-reward reinforcement learning. In NeurIPS, 2025. Ricardo Dominguez-Olmedo, Florian Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. In ICLR, 2025. Yan Duan, John Schulman, Xi Chen, Peter Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. In ICLR, 2017. Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, and Dacheng Tao. Serl: Self-play reinforcement learning for large language models with limited data. arXiv preprint arXiv:2505.20347, 2025. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 11 Preprint. Under review. Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models. In ICLR, 2024. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. In NeurIPS, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In NeurIPS, 2021b. Audrey Huang, Adam Block, Dylan Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan Ash, and Akshay Krishnamurthy. Self-improvement in language models: The sharpening mechanism. In ICLR, 2025a. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025b. Jonas Hübotter, Bhavya Sukhija, Lenart Treven, Yarden As, and Andreas Krause. Transductive active learning: Theory and applications. In NeurIPS, 2024. Jonas Hübotter, Sascha Bongni, Ido Hakimi, and Andreas Krause. Efficiently learning at test-time: Active fine-tuning of llms. In ICLR, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Kimi, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Nayoung Lee, Ziyang Cai, Avi Schwarzschild, Kangwook Lee, and Dimitris Papailiopoulos. Selfimproving transformers overcome easy-to-hard and length generalization challenges. In ICML, 2025. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS, 2019. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR, 2023. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, et al. Deepcoder: fully open-source 14b coder at o3-mini level. Together AI Blog, 2025. URL https://www.together.ai/blog/deepcoder. Preprint. Under review. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025. David JC MacKay. Information-based objective functions for active data selection. Neural computation, 4(4), 1992. Justus Mattern, Manveer, Jannik, Matthew, Felix, Johannes, and Vincent. Synthetic-1: Scaling distributed synthetic data generation for verified reasoning. PrimeIntellect Blog, 2025. URL https://www.primeintellect.ai/blog/synthetic-1. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540), 2015. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces dataset, 2025. URL https://huggingface.co/datasets/open-r1/ codeforces. Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In ICML, 2020. Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skewfit: State-covering self-supervised reinforcement learning. In ICML, 2020. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257, 2025. Qwen. Qwq-32b: Embracing the power of reinforcement learning. Qwen Blog, 2025. URL https: //qwenlm.github.io/blog/qwq-32b. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Negin Raoof, Etash Kumar Guha, Ryan Marten, Jean Mercat, Eric Frankel, Sedrick Keh, Hritik Bansal, Georgios Smyrnis, Marianna Nezhurina, Trung Vu, et al. Evalchemy, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In COLM, 2024. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In ICML, 2015. Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987. Jürgen Schmidhuber. Learning to generate sub-goals for action sequences. In Artificial neural networks, 1991. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. In ICLR, 2025a. Amrith Setlur, Yuxiao Qu, Matthew Yang, Lunjun Zhang, Virginia Smith, ral Kumar. lem. optimizing-llm-test-time-compute-involves-solving-a-meta-rl-problem. Optimizing llm test-time compute involves CMU MLD Blog, and Avisolving meta-rl probURL https://blog.ml.cmu.edu/2025/01/08/ 2025b. Preprint. Under review. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, et al. Thinking vs. doing: Agents that reason by scaling test-time interaction. arXiv preprint arXiv:2506.07976, 2025. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259, 2025. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 2025. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587), 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. In ICLR, 2025. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. In ICML, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, arXiv preprint et al. Mesanet: Sequence modeling by locally optimal test-time training. arXiv:2506.05233, 2025. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. ICLR, 2021. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In ACL, 2024a. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In NeurIPS, 2024b. David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018. Preprint. Under review. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms. arXiv preprint arXiv:2504.14655, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Hongzhou Yu, Tianhao Cheng, Ying Cheng, and Rui Feng. Finemedlm-o1: Enhancing the medical reasoning ability of llm from supervised fine-tuning to test-time training. In COLM, 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In NeurIPS, 2022. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In NeurIPS, 2022. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025a. Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025b. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. In NeurIPS, 2025. Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Selfadapting language models. In NeurIPS, 2025. 15 Preprint. Under review."
        },
        {
            "title": "CONTENTS",
            "content": "A Why Imitation Learning is ill-suited for TTCs"
        },
        {
            "title": "B Background",
            "content": "B.1 SIFT . . B.2 GRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Autobalancing Achievability with TTCs"
        },
        {
            "title": "D Extended Results",
            "content": "D.1 Increasing clip-high in GRPO is essential for learning . . . . . . . . . . . . . . . . D.2 Performance vs. step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 RL post-training baseline restricted to the test environment . . . . . . . . . . . . D.4 Extended comparison and combination of TTC-RL with Maj-TTRL . . . . . . . . D.5 Additional benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6 Further results and ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7 Unsuccessful attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment Details E.1 Dataset . . . . . E.2 System prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Details of the RL training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Qualitative Examples F.1 CodeElo, Question 85 . F.2 AIME 25, question 26 . F.3 TTC for CodeElo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 18 18 19 21 21 21 21 24 24 24 27 27 28 30 30 31 35 Preprint. Under review. WHY IMITATION LEARNING IS ILL-SUITED FOR TTCS While we focus on RL-training with test-time curriculum, the prior works of Hardt & Sun (2024) and Hübotter et al. (2025) have proposed to instead perform supervised fine-tuning on human-produced data (TTC-SFT), retrieved from large corpus. Next to being impractical since requiring reasoning traces for training tasks, we make the observation that the distribution-shift of off-policy SFT appears to make it fundamentally ill-suited for test-time training of LLMs. To test this, we train Qwen2.5-7B-Instruct model (Qwen et al., 2024) on the test sets of the AMC23 and AIME25 math competitions, using expert traces generated by QwQ-32B (Qwen, 2025) using the SFT pipeline from OpenThinker3 (Guha et al., 2025). OpenThinker3-7B is simply the fine-tuned Qwen2.5-7B-Instruct when trained to convergence on curated training set of QwQ-32B (Yang et al., 2025) traces (Guha et al., 2025). Although OpenThinker3 demonstrates that at convergence, an SFT-trained Qwen2.5-7B-Instruct can achieve strong performance, Figure 6 shows that even when training directly on the test set, it takes hundreds of gradient steps before the accuracy starts to increase, while initially dropping to close to 0%. Intuitively, even though perplexity decreases smoothly throughout training, the models behavior undergoes phase transitions, and begins by only reproducing superficial reasoning patterns such as repeatedly generating Wait, ...: Figure 6: Training on the test set with SFT leads to an initial accuracy drop, indicating that SFT is ill-suited for TTT. Excerpts from reasoning traces for AIME 25 after 200 SFT steps . . . be 2025. Wait, actually, actually, actually, actually, actually, actually, actually, actually, actually, actually, . . . . . . numerator.nnWait, numerator numerator is numerator denominator * denominator numerator.nnWait, numerator numerator . . . This phenomenon is closely related to recent observations that off-policy SFT appears to induce greater distribution shift of the policy than on-policy RL (Shenfeld et al., 2025), indicating that TTC-SFT is less robust and may even be less efficient than TTC-RL. Validating this phenomenon with expert solutions in GSM8K. To validate that this behavior is because of the behavior SFT rather than the specifics of QwQ-32Bgenerated reasoning traces, we perform similar experiment on GSM8K (Cobbe et al., 2021). GSM8Ks test set has not only verified numerical answers, but also humangenerated expert reasoning traces. In Figure 7, we train Llama-3.2-1B-Instruct on these expert solutions with SFT and observe that after the first full epoch, performance on the GSM8K test set has dropped. At first this may seem puzzling, but looking at qualitative example suggests that the model first learns superficial cues (the human-generated GSM8K solutions are very succinct) before in this case overfitting to the SFT data: Figure 7: Training Llama-3.2-1BInstruct on the GSM8K test set with SFT. After the first full epoch, the performance has dropped significantly compared to the initial model, before then overfitting to the SFT data. Attempt of GSM8K question after 1 epoch of SFT on the test set Problem There are three trees in Eddys backyard. The shortest tree has height of 6 feet, and the second tree has height of 5 feet more than the shortest tree. The height of the tallest tree is twice the height of the two trees combined. How tall is the tallest tree? 17 Preprint. Under review."
        },
        {
            "title": "Ground Truth",
            "content": "Tallest Tree: 108 feet Middle sized tree:(108/2)-6=48 feet Smallest tree:48/4=12 feet"
        },
        {
            "title": "Initial Answer",
            "content": "The tallest tree is 108 feet. The middle-sized tree is 6 feet less than half the height of the tallest tree. Half of 108 is 54. So the middle-sized tree is 54 - 6 = 48 feet tall. The smallest tree is 1/4 the height of the middle tree, which is 1/4 48 = 12 feet tall. The final answer is"
        },
        {
            "title": "Final Answer",
            "content": "The middle tree is 108/2 - 6 = 60 feet tall. The smallest tree is 60/4 = 15 feet tall. The final answer is 15 Hyperparameter Learning rate Batch size Max. sequence length in tokens Packing Adams β-values Value 1e-5 32 16384 No (0.9, 0.999) Table 3: Hyperparameters for SFT training on the test sets of AMC23 and AIME25. This corresponds to the micro configuration of OpenThinker (Guha et al., 2025)."
        },
        {
            "title": "B BACKGROUND",
            "content": "B.1 SIFT Several works studied how to optimally select data for imitation learning, e.g., the early seminal work of MacKay (1992) and recent extensions (Hübotter et al., 2024; 2025; Bagatella et al., 2025b). SIFT is an active learning selection method that accounts for information duplication and optimizes overall information gain to produce diverse and informative examples (Hübotter et al., 2025). Given feature map ϕ, we define the inner-product kernel k(x, x) := ϕ(x)ϕ(x). SIFT greedily selects data from corpus to minimize measure of uncertainty about how to respond to specific prompt x. This uncertainty (posterior variance) given selected set is quantified as: (x) := k(x, x) σ2 (x)(KX + λI)1kX (x), (2) where KX is the kernel matrix of X, kX (x) is the vector of kernel evaluations between the inputs in and x, and λ > 0 is regularization coefficient. SIFT iteratively selects the next point xn+1 by greedily minimizing this posterior uncertainty: xn+1 := arg min xD Xn{x}(x). σ2 (3) The regularization coefficient λ modulates the trade-off between relevance (favored by large λ) and diversity (favored by small λ). Full details, including theoretical guarantees and empirical results, are presented in the SIFT paper (Hübotter et al., 2025). B.2 GRPO For RL-training, we adopt GRPO (Shao et al., 2024) without KL penalty. For specific training task x, the behavior policy πθold samples group of individual responses {oi}G i=1. Then, we 18 Preprint. Under review. calculate the advantage of the i-th response by normalizing the group-level rewards {ri}G i=1: ˆAi,t = ri mean({Ri}G std({Ri}G i=1) i=1) . GRPO then maximizes clipped objective: JGRPO(θ) = (cid:98)D,{oi}G i=1πθold (x) (cid:34) 1 (cid:88) i= 1 oi oi (cid:88) (cid:16) t=1 with importance weights min (cid:0)wi,t(θ) ˆAi,t, clip(wi,t(θ), 1 ϵlow, 1 + ϵhigh) ˆAi,t wi,t(θ) = πθ(oi,t x, oi,<t) πθold(oi,t x, oi,<t) . (4) (cid:35) (cid:1)(cid:17) , (5) (6) Maximizing the learning signal in GRPO. When training on selected dataset we aim to provide maximal learning signal to the model. One simple way to determine whether provided data sample provides useful information is via the norm of GRPOs gradient. The gradient of the GRPO objective, in the on-policy setting (πθ = πθold) is given by: θJGRPO(θ) = (cid:88) 1 1 oi oi (cid:88) ˆAi,tθ log πθ(oi,t x, oi,<t) (7) t=1 This formulation reveals that the advantages ˆAi,t are closely tied to the gradient norm of GRPO, θJGRPO(θ). Intuitively, by selecting data with high absolute advantage we maximize the gradient norm and provide strong learning signal to the model. i=1 In the sparse-reward setting for fixed question x, the reward is distributed according to Bernoulli distribution Ber(px). The expected absolute advantage for this question can be derived as follows, where we assume for simplicity: (cid:21) [A] = (cid:20) E[R] σ(R) Therefore, the absolute advantage is maximized for px = 1 2 . This simple argument suggests that, in order to maximize the learning signal, we should choose questions on which the current model has success rate 50%. 1 px σ(R) px σ(R) px(1 px) + (1 px) = px = 2 (8) (cid:112) AUTOBALANCING ACHIEVABILITY WITH TTCS The goal of targeted test-time curriculum is to teach the LLM skills that are directly useful for solving the target tasks. Naively selecting the test-time curriculum, however, may result in training tasks that are either too easy or too hard for the current model. Prior work on curricula for sparsereward reinforcement learning (e.g., Pitis et al., 2020; Zhao et al., 2025; Huang et al., 2025b; DiazBone et al., 2025) has shown that selecting tasks at an appropriate level of difficulty can dramatically accelerate learning. In line with these findings, we demonstrate that balancing task relevance with task difficulty can lead to better-performing TTC if the model is initially significantly weaker than required to solve most target tasks. Intuitively, success rate of 50% provides the most detailed differentiation as to which approaches work. Indeed, in expectation, success rate of 50% leads to the largest possible absolute advantage in GRPO (cf. Appendix B.2), which implies large gradient norm and strong and informative learning signal for the model. Estimating the success rate online. This raises the question of how to estimate the difficulty αx of given training task from the corpus at time t. We assume access to an initial estimate of difficulty αx recursively to track the approximate success rate of the model for each question: (cid:40) 0 (0, 1). We then update αx if was within the last batch αx t+B := rx t+B σ(σ1(αx ) + σ1(t+B)) otherwise, (9) 19 Preprint. Under review. where t+B is the mean reward across the batch and σ(z) = 1/(1 + ez) the sigmoid function. Intuitively, if > 0.5, the achievability estimate of all unseen questions is increased, indicating that tasks are becoming easier for the agent. Conversely, if < 0.5, the achievability estimates are decreased, reflecting that training tasks are currently too difficult. Trading off achievability & relevance to the test task. We can now leverage the achievability estimates to ensure that the selected tasks are of an appropriate difficulty. To this end, we propose Achievable Test-Time Curricula (A-TTCs), which balance relevance to the target tasks, as identified by SIFT, with achievability: ABt {(x, v) αx Bt [amin, amax]} {(xBt, vB(t+1)1)} arg min SIFTλ,ϕ,B,ABt(D) where [amin, amax] determines the interval of task difficulty we consider for the task selection with SIFT. This selection strategy offers simple way to select batches of problems online, which are of the right difficulty while remaining relevant to the target tasks. In practice, we choose [amin, amax] = [0.2, 0.6], with the goal of achieving approximately 50% of tasks over the batch, obtain prior difficulty estimates by computing the success rates of the Qwen3-8B model on all questions and enforce minimum subset size of 1000 to select from. The results in Figure 8 show that on the weaker Qwen3-0.6B model trading-off achievability with relevance yields higher training reward and furthermore improves test score across the three math benchmarks, AIME 24 & 25 and MATH500. We note that this procedure appears useful primarily if the difficulty level in the dataset is wrongly calibrated with respect to the models capabilities. Figure 8: Comparison of train and test accuracy of standard TTC-RL vs. ATTC-RL averaged across math benchmarks (MATH500, AIME24, AIME25) on the Qwen3-0.6B model. Modeling assumptions. To motivate our online achievability estimation, we consider the logits ) of the achievability values and make the assumption that at each time step the ϕx = σ1(αx change in the logits dt is jointly gaussian across all tasks: t+1 ϕx dx = ϕx (10) dt (0, Σ) with Σ = (v c)In + c11 (11) That is, we consider fixed variance for all tasks and assume that the update has constant correlation among all tasks. After observing the achievabilities for batch of problems at time t, we can compute the update in the logits for the observed tasks and are able to estimate the update for the unobserved problems. Consider batch of problems = {y1, . . . , ym} and an unobserved problem / B, then: E[dx dy , B] = c1((v c)IB + c11)1dB (cid:18) Bc2 (v c)(v + (B 1)c) (cid:19) (cid:88) dy yB = = dy (cid:88) yB + (B 1)c (cid:123)(cid:122) (cid:125) (cid:124) ψ (cid:88) dy t+B = ϕx ϕx + ψ Under the assumed covariance structure and letting t+B = σ(ψ (cid:80) yB dy ), our update becomes: yB (cid:40) αx t+B := rx t+B σ(σ1(αx ) + σ1(t+B)) otherwise. if was within the last batch 20 (12) (13) (14) (15) (16) Preprint. Under review."
        },
        {
            "title": "D EXTENDED RESULTS",
            "content": "In this section, we present additional experiments and ablations. D.1 INCREASING CLIP-HIGH IN GRPO IS ESSENTIAL FOR LEARNING Maintaining sufficient level of entropy in the policy is key for any on-policy exploration method. When training with GRPO with symmetrical clipping on verifiable rewards it has been observed (Yu et al., 2025b; Luo et al., 2025), that the policys entropy quickly goes to 0, preventing effective It has been found that an increase of the clip-high (ϵhigh) parameter in GRPO can exploration. lead to stabilization of the entropy and improved performance during training (Luo et al., 2025). Intuitively, if correct answers are rewarded more strongly than incorrect answers are penalized, the agent is incentivized to maintain higher entropy in its action distribution, promoting exploration. In Figure 9 we evaluate the effect of the clip-high parameter on the policy entropy and test accuracy during training. We find that symmetric clipping (ϵhigh = 0.2) leads to constant decrease in policy entropy and poor performance on the test tasks. When increasing the clip-high parameter, the policy entropy starts increasing, and the test accuracy is dramatically improved. In our preliminary experiments on Codeforces, ϵhigh = 0.32 improved significantly over ϵhigh = 0.28, which was suggested in Yu et al. (2025b) and used in our other experiments. Figure 9: Increasing the ϵhigh to 0.28 prevents the collapse of policy entropy and leads to strong performance on the test set. We plot the test accuracy and the policy entropy over the course of the training for various values of ϵhigh on the Qwen3-8B model trained on the Codeforces dataset. GRPOs default value is ϵhigh. D.2 PERFORMANCE VS. STEP In Figure 10, we provide further detail on the performance of all models across the main benchmarks. The plots reveal substantial variation in test accuracy development in response to training with the same TTC, indicating that models have varying initial capabilities and potential of training via RL. This is the case, as each model has been subject to different post-training techniques and therefore responds differently to the RL training on the TTC. To address these differences, we propose an algorithm in Appendix C, which aims to calibrate the difficulty of the curriculum to the capabilities of the model. D.3 RL POST-TRAINING BASELINE RESTRICTED TO THE TEST ENVIRONMENT simple heuristic to improve models domain-specific capabilities is to restrict training to tasks from the target domain. This can be seen as primitive version of TTC that conditions on the environment type but ignores instance-level task characteristics. Accordingly, we include baseline that samples random subset of the training setanalogous to RL post-trainingbut restricted to the target domain. Figure 11 demonstrates that filtering the training questions to the code domain is insufficient to achieve comparable performance to TTC-RL on Codeforces and CodeElo. 21 Preprint. Under review. Figure 10: TTC-RL shows strong improvements over standard RL Post-Training across most considered models on the math and coding benchmarks. We plot the individual performance of all considered models on the main benchmarks. 22 Preprint. Under review. Model AIME24 AIME25 MATH500 Codeforces CodeElo LCB GPQA-D Qwen3-8B-Instruct + RL post-training + Maj-TTRL (Zuo et al., 2025) + TTC-RL Qwen3-4B-Instruct-2507 + RL post-training + Maj-TTRL (Zuo et al., 2025) + TTC-RL Qwen3-8B-Base + RL post-training + Maj-TTRL (Zuo et al., 2025) + TTC-RL 21.67 41.67 42.50 50.83 52.50 55.83 65.83 60.00 15.83 22.50 20.83 30. 23.33 38.33 30.00 41.67 40.83 47.50 55.83 45.83 14.17 20.83 20.00 21.67 69.55 82.50 85.40 85.10 72.00 86.30 86.80 88.50 63.10 76.85 74.55 78. 20.85 27.83 33.35 26.70 28.39 34.99 9.92 17.46 17.84 13.73 20.61 25.95 22.67 27.29 29.34 20.27 21.56 25.95 21.18 26.91 27.20 6.67 11.26 9.97 18.51 17. 11.33 49.11 56.47 51.14 58.38 61.93 62.82 62.44 61.93 29.70 42.77 29.70 45.94 Table 4: Extended comparison of TTC-RL with Maj-TTRL across models and benchmarks. Figure 11: Restricting RL post-training to include only problems in code environment explains only fraction of the improvement on challenging coding tasks (Codeforces, CodeElo) seen by TTC-RL. D.4 EXTENDED COMPARISON AND COMBINATION OF TTC-RL WITH MAJ-TTRL Majority voting Test-Time Reinforcement Learning (Maj-TTRL), recently introduced by Zuo et al. (2025), provides an alternative way to train the model at test time using majority labels as rewards on the target tasks. This approach applies only to domains with structured labels, such In Taas math or multiple-choice and is therefore not applicable to our coding benchmarks. ble 4, we compare the performance of Maj-TTRL with TTC-RL across our main benchmarks and all considered models. TTC-RL outperforms Maj-TTRL on most benchmarks for Qwen3-8B and Qwen3-4B-Instruct-2507. The only model, where Maj-TTRL achieves higher performance than TTC-RL is the Qwen3-4B-Instruct-2507 model, which is the strongest among all considered models. This reveals the dataset as the main bottleneck for improving performance and suggests to move beyond the bottleneck of fixed task corpus through self-generated TTCs. Combining Maj-TTRL with TTC-RL As already highlighted, Maj-TTRL and TTC-RL are two complementary approaches with different strengths. Intuitively, TTC-RL aims to learns from the most relevant tasks in the given corpus to improve on the target tasks, while Maj-TTRL is able to improve the performance on the target tasks directly by continuously aiming to match the majority prediction of the model. Beyond comparing them in isolation, Figure 12 shows that initializing MajTTRL from the final TTC-RL checkpoint and training on the target benchmark yields the strongest results on all math benchmarks. 23 Preprint. Under review. Figure 12: Combining TTC-RL and Maj-TTRL combines the strengths of both methods and yields the strongest results on all math benchmarks. We show the results on the Qwen3-8B for math. D.5 ADDITIONAL BENCHMARKS While our main evaluation focuses on the most challenging benchmarks in math, code and general reasoning, aiming to push the capabilities of frontier models, we additionally provide implementation and results for set of simpler benchmarks. These include in the math domain, GMS8K (Cobbe et al., 2021) and AMC23. For coding we add the HumanEval+ (Chen et al., 2021) and MBPP+ (Chen et al., 2021). Finally, for wide range of general reasoning task we include the MMLU-Pro (Wang et al., 2024b) benchmark. The results in Table 5 show that TTC-RL yields substantial gains on math and coding, especially for the weaker Qwen3-8B-Base model. For Qwen3-8B, the improvements are less pronounced, suggesting that the verifiable-corpus may contain fewer useful tasks at the level of complexity required by these benchmarks, or that these benchmarks are too simple to see substantial further improvement in reasoning. Model GSM8K AMC23 HumanEval+ MBPP+ MMLU-Pro* 83.19 Qwen3-8B + RL post-training 93.06 + TTC-RL 94.01+10.8 73.09 Qwen3-8B-Base + RL post-training 92.80 + TTC-RL 93.25+20.2 63.12 86.25 88.75+25.6 46.25 63.12 72.50+26. 79.88 82.77 80.64+0.8 35.82 81.10 81.25+45.4 44.88 63.23 61.64+16.8 38.83 60.44 63.56+24.8 66.00 69.30 68.71+2.8 45.46 62.21 61.86+16. Table 5: Performance of TTC-RL on easier benchmarks. (*) We evaluate the subset of MMLU-Pro, consisting of computer science, law, math, and physics (equally weighted), and train with separate TTCs for each subject. D.6 FURTHER RESULTS AND ABLATIONS In Figure 13, we show the marginal improvement in percentage points throughout training when using TTC-RL over general-purpose RL post-training, and find that this difference remains large throughout training for all models. In Figure 14, we perform an ablation, comparing to oracle training on the test set. In Table 6, we provide detailed breakdown of values for pass@k. In Table 7, we report additional results on latent improvement. D.7 UNSUCCESSFUL ATTEMPTS The strong improvements observed when increasing the clip-high parameter ϵhigh suggest that the exploration phase requires stabilization of the policy entropy. We evaluated cooldown of entropy via continued training with ϵhigh = 0.2. However, in Figure 15, we find that the cooldown appears to slightly improve performance in math, but not generally. 24 Preprint. Under review. Figure 13: Improvement of TTC-RL over RL post-training across several models. Figure 14: Training on the test set vs TTC-RL (Codeforces & AIME25). Qwen3-8B AIME AIME25 MATH500 Codeforces CodeElo LCB GPQA-D 21.67/50.83 23.33/41.67 69.55/85.10 20.85/33.35 13.73/29.34 20.61/27.29 49.11/58.38 Pass@1 31.87/52.10 28.31/48.37 77.57/86.91 24.96/31.82 17.71/33.75 23.55/28.74 60.94/64.45 Pass@2 39.11/60.45 34.11/56.01 82.63/88.34 29.61/35.32 23.11/38.90 27.10/31.03 72.04/73.49 Pass@4 Pass@8 46.47/67.43 40.13/62.10 85.68/89.37 33.57/38.31 28.28/43.01 30.12/33.06 80.60/80.67 Pass@16 53.21/73.19 45.91/68.27 87.65/90.22 37.06/40.65 32.88/46.39 32.22/34.75 86.49/85.94 Pass@32 58.98/77.06 51.52/73.78 89.09/90.91 40.09/42.45 36.75/49.20 33.25/35.92 90.09/89.33 Pass@64 63.23/79.03 56.67/78.51 90.10/91.43 42.57/43.74 39.74/51.43 33.79/36.73 92.37/91.43 Table 6: TTC-RL consistently improves the pass@k across math and code for large k. We show the pass@k for Qwen3-8B before and after the TTC-RL training on our main benchmarks. Figure 15: Continued training with decreased clip-high parameter (ϵhigh = 0.2) does not yield improved performance. We plot the average performance averaged over the main math, code and general reasoning benchmarks on the Qwen3-8B model. 25 Preprint. Under review. Model AIME24 AIME25 MATH500 Codeforces CodeElo LCB GPQA-D Qwen3-8B + TTC-RL Latent improvement Qwen3-4B-Instruct-2507 + TTC-RL Latent improvement Qwen3-8B-Base + TTC-RL Latent improvement 21.67 50.83 +20.95 52.50 60.00 -26.30 15.83 30.00 +9.79 23.33 41.67 +15. 40.83 45.83 -18.64 14.17 21.67 +3.96 69.55 85.10 +6.02 72.00 88.50 +3.69 63.10 78.15 +10.30 20.85 33.35 +7. 26.70 34.99 +5.27 9.92 17.84 +5.36 13.73 20.61 29.34 27.29 +15.38 +5.53 20.27 21.56 27.20 26.91 +2.10 +1.34 6.67 11.26 11.33 17.94 +2.57 +3.69 49.11 58.38 +9. 61.93 61.93 0.00 29.70 45.94 +14.49 Table 7: On most benchmarks and models TTC-RL yields strong latent improvement, which normalized for learning the correct output format. 26 Preprint. Under review."
        },
        {
            "title": "E EXPERIMENT DETAILS",
            "content": "E.1 DATASET We curate multi-domain training corpus from math (DAPO-Math-17k, Hendrycks MATH, GSM8K), code (LiveCodeBench up until August 1, 2024, TACO, PrimeIntellect, Codeforces train, CodeContests, LeetCode), and WebInstruct-verified. All samples are cast into unified schema with fields kind, dataset, description, problem, answer, and tests, with light task-specific preprocessing (e.g., GSM8K answer extraction). For simplicity we compute embeddings for SIFT using Qwen3-8B across all runs. Decontamination. We decontaminate our entire corpus except for Webinstruct-verified against our held-out evaluation benchmarks using single, conservative procedure: 1. Text normalization: Lowercase, whitespace collapse, and answer normalization by removing TeX wrappers such as boxed{}. 2. Candidate pruning via small n-grams: We tokenize benchmark texts and index 12-gram shingles6 to retrieve small candidate set for each training item. 3. Contamination tests: An item is marked contaminated if it either (i) shares any exact 32gram shingle with benchmark item or (ii) achieves sequence-similarity ratio of at least 0.75 (difflib-style) with any candidate. 4. Removal: For math, we additionally require the normalized training answer to match the benchmark answer before removal. For code, if training item matches multiple distinct benchmark tasks from single benchmark, we keep it to avoid removing generic boilerplate or templates. Deduplication. Within-domain duplicates are removed via fast token-coverage deduplication: we keep the first occurrence and drop later item when at least threshold fraction of its normalized token set is covered by another items tokens (or vice versa), requiring identical normalized answers when answers are present. We use threshold 0.80 for math and 0.95 for code; WebInstruct-verified is deduplicated within itself at 1.00. Extraction of problem descriptions. For each training task, we extract description as its main identifier. For tasks unlike coding, the description coincides with the problem field, without any system prompts. For coding tasks, we extract the description from problem to avoid any superfluous selection of tasks based on the formatting of input-output examples or other formatting. TTCs are self-curated via SIFT based on the models last-token last-layer representation of the description field. To each description, we append information about the environment: The solution will be evaluated in {math/verifier/code} environment.. Filtering. We remove low-signal or malformed items with the following rules: Code training tasks require at least 5 executable tests, non-empty descriptions. We also drop cases where the description trivially duplicates the problem text, indicating that the problem was wrongly parsed or is missing input-output examples. We drop items with missing or empty answers, except for code tasks with unit tests. We enforce minimum description length for code of at least 100 characters to prevent underspecified tasks. We exclude all items whose prompt length exceeds our max-token limit of 2048. E.2 SYSTEM PROMPTS We use the following system prompts, which we adapted from evalchemy (Raoof et al., 2025) and simplified slightly. We did not tune system prompts for better performance. General system prompt {problem} Please reason step by step, and put your final answer within boxed{}. 6That is, any consecutive sequence of 12 tokens. 27 Preprint. Under review."
        },
        {
            "title": "Hyperparameter",
            "content": "Data & setup Episodes Dataset size SIFT λ Generation limits Max. prompt length (tokens) Max. response length (tokens) Max. response length of verifier (tokens) Optimization & objective Advantage estimator GRPO clip-low / clip-high Adam β-values Learning rate Gradient clip KL coefficient Training sampling Batch size # rollouts Temperature Validation sampling # rollouts Temperature Top-p"
        },
        {
            "title": "Value",
            "content": "2 1000 0.1 2048 8192 2048 GRPO 0.2 / 0.28 (0.9, 0.999) 1e-6 1.0 0.0 8 16 1.0 4 0.6 0.95 Table 8: Hyperparameters for TTC-RL training. Code system prompt You are coding expert. You will be given coding problem, and you need to write correct Python program that matches the specification and passes all tests. The time limit is 1 second. You may start by outlining your thought process. In the end, please provide the complete code in code block enclosed with .nn{problem} GPQA system prompt Return your final response within boxed{} and only include the letter choice (A, B, C, or D) as your final response. Problem: {problem} Options: {options} Answer: E.3 DETAILS OF THE RL TRAINING We summarize our hyperparameters for RL training in Table 8. We keep these hyperparameters fixed across all models, benchmarks, and baselines. In our code environment, we keep only the first 20 test cases for training tasks to improve efficiency. Training reward. We include format penalty in the train reward if our answer extraction fails (i.e., we extract an empty string) to encourage well-formed responses. Notably, we found it important not to penalize ill-formed answers that were truncated due to exceeding the maximum response length, since this disincentivizes the model from leveraging all of its accessible context. 28 Preprint. Under review. For training tasks from Webinstruct-verified, we additionally include length penalty as proposed by Ma et al. (2025). Denoting the number of tokens in the extracted answer of an attempt by and the number of tokens of the golden answer by l, the length penalty is defined as ℓ := 0.05 min{l l, 10}. We set ℓ = 0 for math and code environments."
        },
        {
            "title": "Our training reward for a given attempt is",
            "content": "r := 1 ℓ 1 2 0 if the attempt is correct if the attempt is ill-formed and was not truncated otherwise. (17) (18) 29 Preprint. Under review."
        },
        {
            "title": "F QUALITATIVE EXAMPLES",
            "content": "In this section we provide qualitative examples of single runs, which showed interesting behavior and provide examples of parts of the curricula used for training for various code and math problems. F.1 CODEELO, QUESTION"
        },
        {
            "title": "Problem",
            "content": "Description: You have an array of non-negative integers a1, a2, . . . , an. The value of sub-array of length 2, a[l, r] = [al, al+1, . . . , ar] is the minimum value of ai aj such that < r, where is the xor (exclusive-or) operator. You have to find the k-th smallest value over all sub-arrays of length 2. Input: The first line of the input contains multiple test cases (1 2 104). The first line of each test case contains integer numbers and (2 105, 1 n(n1) The second line of the input contains non-negative integer numbers a1, a2, . . . , an (0 ai 109) the array itself. It is guaranteed that the sum of over all test cases does not exceed 105. Output: Print the k-th smallest value obtained over all subarrays of length at least 2. Note: In the first testcase, we have subarrays with their smallest exclusive-or pair as:[1, 2] : 3, [2, 3] : 1, [3, 4] : 7, [4, 5] : 1, [1, 2, 3] : 1, [2, 3, 4] : 1, [3, 4, 5] : 1, [1, 2, 3, 4] : 1, [2, 3, 4, 5] : 1, [1, 2, 3, 4, 5] : 1. The sorted order would be: 1, 1, 1, 1, 1, 1, 1, 1, 3, 7. Therefore, the second smallest element would be 1. ). 2 Initial Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import sys import heapq def main(): input = sys.stdin.read data = input().split() idx = 0 = int(data[idx]) idx += 1 results = [] for _ in range(t): n, = int(data[idx]), int(data[idx + 1]) idx += 2 = list(map(int, data[idx:idx + n])) idx += xor_values = [] for in range(n): for in range(i + 1, n): xor_values.append(a[i] ^ a[j]) xor_values.sort() results.append(str(xor_values[k - 1])) print(\"n\".join(results)) if __name__ == \"__main__\": main() Final Answer 1 2 def main(): import sys Preprint. Under review. 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 input = sys.stdin.read().split() idx = 0 = int(input[idx]) idx += 1 for _ in range(t): # Read and n, = map(int, input[idx:idx+2]) idx += 2 # Read the array = list(map(int, input[idx:idx+n])) idx += # Step 1: Generate all subarrays of length $geq$2 subarray_values = [] # For each possible subarray for in range(n): for in range(i + 1, n): # Subarray from to (inclusive) sub_len = - + 1 if sub_len < 2: continue # Extract the subarray sub = a[i:j+1] # Generate all pairs (x, y) with < # For this subarray, compute min XOR min_xor = float(inf) # Iterate over all pairs (p, q) in subarray indices for in range(len(sub)): for in range(p + 1, len(sub)): current_xor = sub[p] ^ sub[q] if current_xor < min_xor: min_xor = current_xor # Add to the list of values subarray_values.append(min_xor) # Sort the list subarray_values.sort() # Output the k-th smallest (1-based index) print(subarray_values[k - 1]) main() F.2 AIME 25, QUESTION 26 The following problem is the 26th task from the AIME25 competition. Additionally, we provide the initial answer of Qwen3-8B for this problem, as well as the answer of the model after 250 training steps on the TTC curated specifically for this specific question. Finally, we list the first 10 selected training problems for this task. Problem Let A1A2 . . . A11 be non-convex 11-gon such that The area of AiA1Ai+1 is 1 for each 2 10, cos(AiA1Ai+1) = 12 13 for each 2 10, The perimeter of A1A2 . . . A11 is 20. If A1A2 + A1A11 can be expressed as for positive integers m, n, p, with squarefree and gcd(m, p, q) = 1, find + + + q. Please reason step by step, and put your final answer within . np Preprint. Under review."
        },
        {
            "title": "Ground Truth",
            "content": "19 np Initial Answer . . . tates that the perimeter is 20, and the sum x2 + x11 is what we need, and the answer is in the form , perhaps we can proceed with the assumption that the perimeter is related to the sum of the side lengths, which can be expressed in terms of a, and then solve for a. However, without going through the full calculation, which would be very lengthy, perhaps we can consider that the perimeter includes the sum of the side lengths, which can be expressed as function of a, and then solve for a. Alternatively, since the problem gives the answer in the form involving square roots, it suggests that the expression for + 26 involves square root. But since we have x2 + x11 = + 26 , and the perimeter is 20, which includes other terms, but since the problem gives the answer in terms of radicals, we can assume that the perimeter is expressed in terms of and the square root, and solving gives us the required expression. However, since this is too vague, and given that the problem is from competition, lets assume that the perimeter can be expressed in terms of a, and the sum + 26 But since this is too open-ended, and given the time constraints, Ill proceed to assume that the perimeter is related to the sum of the xis, and the sum of the side lengths, and that after solving, the value of + 26 However, without going through the full derivation, which is quite involved, Ill conclude that the answer is likely , and from the given information, the answer is likely: is expressed as can be found. np . np 13 10 5 But since the problem states that is square-free, and the answer is in the form the given answer format matches this, the final answer would be: np , and = 52, = 13, = 10, = Thus, + + + = 52 + 13 + 10 + 5 = 80. But to check, lets verify: If the sum is + 26 the expression would involve square roots. Assuming that the final expression is 52 1310 5 , and the perimeter is 20, then solving for would lead to quadratic, and , then the sum is 52 1310 5 , and the answer is: So the final answer is 52 + 13 + 10 + 5 = 80. Thus, the final answer is: = 52, = 13, = 10, = 5 80 Final Answer . . . 1620 = 4 405 = 4 5 81 = 4 5 9² So: 1620 = 4 405 = 2 405 = 2 81 5 = 2 9 5 = 18 5 Thus: 2025 09 1618 : 51 : 53t = 2 18 5 8 2025 09 1618 : 51 : We are looking for positive value of t, since its sum of positive lengths. So take the positive root: 32 Preprint. Under review. 2 + 18 8 = = 18 5 2 9 = 5 1 4 51 4 , which is the value of + 26 5a , which is the expression we need for Therefore, = 9 A1A2 + A1A11. So the answer is: 9 5 1 4 Which matches the form is square-free. Thus, + + + = 9 + 5 + 1 + 4 = 19. np , with = 9, = 5, = 1, = 4. Check gcd(9,1,4) = 1, and 19 Training problem 1 Let BCBC be rectangle, let be the midpoint of BC , and let be point on the circumcircle of the rectangle. Let triangle ABC have orthocenter H, and let be the foot of the perpendicular from to line AM . Suppose that AM = 2, [ABC] = 2020, and BC = 10. Then AT = , where and are positive integers with gcd(m, n) = 1. Compute 100m + n. The solution will be evaluated in math environment. Training problem 2 Let ABC be triangle with C = 30. Let be the point where the A-excircle touches line BC, the circumcenter of triangle ABC, and X, the intersections of the altitude from with the incircle with in between and . Suppose points A, and are collinear. If the ratio AO for positive integers a, b, c, with gcd(a, b, d) = 1 and not divisible by the square of any prime, find + + + d. The solution will be evaluated in math environment. AX can be expressed in the form a+b Training problem 3 Robert is robot who can move freely on the unit circle and its interior, but is attached to the origin by retractable cord such that at any moment the cord lies in straight line on the ground connecting Robert to the origin. Whenever his movement is counterclockwise (relative to the origin), the cord leaves coating of black paint on the ground, and whenever his movement is clockwise, the cord leaves coating of orange paint on the ground. The paint is dispensed regardless of whether there is already paint on the ground. The paints covers 1 gallon/unit 2, and Robert starts at (1, 0). Each second, he moves in straight line from the point (cos(θ), sin(θ)) to the point (cos(θ + a), sin(θ + a)), where changes after each movement. starts out as 253o and decreases by 2o each step. If he takes 89 steps, then the difference, in gallons, between the amount of black paint used and orange paint used can be written as . . . Training problem 4 There are players in round-robin ping-pong tournament (i.e. every two persons will play exactly one game). After some matches have been played, it is known that the total number of matches that have been played among any 2 people is equal to 3k (where is fixed integer). Find the sum of all possible values of n. The solution will be evaluated in math environment. Training problem 5 Let ABC be triangle with AB = 4 and AC = 7 2 . Let ω denote the A-excircle of ABC. Let ω touch lines AB, AC at the points D, E, respectively. Let Ω denote the circumcircle of 33 Preprint. Under review. ADE. Consider the line ℓ parallel to BC such that ℓ is tangent to ω at point and such that ℓ does not intersect Ω. Let ℓ intersect lines AB, AC at the points X, , respectively, with XY = 18 and AX = 16. Let the perpendicular bisector of XY meet the circumcircle of AXY at , Q, where the distance from to is smaller than the distance from toF . Let ray for relatively prime positive integers m, n, find + n. The solution will be evaluated in math environment. meet Ω for the first time at the point Z. If 2 = Training problem"
        },
        {
            "title": "13 LHS Students attend the LHS Math Team tryouts. The students are numbered 1, 2, . . . , 13.\nTheir scores are s1, s2, . . . , s13, respectively. There are 5 problems on the tryout, each of which\nis given a weight, labeled w1, w2, . . . , w5. Each score si is equal to the sum of the weights of all\nproblems solved by student i. On the other hand, each weight wj is assigned to be\n, where\nthe sum is over all the scores of students who solved problem j. (If nobody solved a problem,\nthe score doesn’t matter). If the largest possible average score of the students can be expressed\n√\na\nb , where a is square-free, find a + b. The solution will be evaluated in a math\nin the form\nenvironment.",
            "content": "1 (cid:80) si Training problem 7 Let ABCDE be pentagon with area 2017 such that four of its sides AB, BC, CD, and EA have integer length. Suppose that = = = 90o, AB = BC, and CD = EA. The maximum possible perimeter of ABCDE is + c, where a, b, and are integers and is not divisible by the square of any prime. Find + + c. The solution will be evaluated in math environment. Training problem 8 Let ABC be triangle with AB = 4 and AC = 7 2 . Let ω denote the A-excircle of ABC. Let ω touch lines AB, AC at the points D, E, respectively. Let Ω denote the circumcircle of ADE. Consider the line ℓ parallel to BC such that ℓ is tangent to ω at point and such that ℓ does not intersect Ω. Let ℓ intersect lines AB, AC at the points X, , respectively, with XY = 18 and AX = 16. Let the perpendicular bisector of XY meet the circumcircle of AXY at , Q, where the distance from to is smaller than the distance from toF . for relatively prime positive Let ray integers m, n, find + n. The solution will be evaluated in math environment. meet Ω for the first time at the point Z. If 2 = Training problem 9 Point is in the interior of ABC. The side lengths of ABC are AB = 7, BC = 8, CA = 9. The three feet of perpendicular lines from to sides BC, CA, AB are D, E, respectively. + AB Suppose the minimal value of BC c, where gcd(a, b) = 1 and is square-free, calculate abc. The solution will be evaluated in math environment. can be written as + CA Training problem 10 Billy the baker makes bunch of loaves of bread every day, and sells them in bundles of size 1, 2, or 3. On one particular day, there are 375 orders, 125 for each bundle type. As such, Billy goes ahead and makes just enough loaves of bread to meet all the orders. Whenever Billy makes loaves, some get burned, and are not sellable. For nonnegative less than or equal to the total number of loaves, the probability that exactly loaves are sellable to customers is inversely proportional to 2i (otherwise, its 0). Once he makes the loaves, he distributes out all of the sellable loaves of bread to some subset of these customers (each of whom will only accept their desired bundle of bread), without worrying about the order in which he gives them out. If the expected number of ways Billy can distribute the bread is of the form ab 2c1 , find + + c. The solution will be evaluated in math environment. 34 Preprint. Under review. F.3 TTC FOR CODEELO In the following, we list the 10 most relevant problems selected by SIFT to improve performance on the CodeElo benchmark. Training problem 1 There are monsters standing in row. The i-th monster has ai health points. Every second, you can choose one alive monster and launch chain lightning at it. The lightning deals damage to it, and also spreads to the left (towards decreasing i) and to the right (towards increasing i) to alive monsters, dealing damage to each. When the lightning reaches dead monster or the beginning/end of the row, it stops. monster is considered alive if its health points are strictly greater than 0. For example, consider the following scenario: there are three monsters with health equal to [5, 2, 7], and = 3. You can kill them all in 4 seconds: - launch chain lightning at the 3-rd monster, then their health values are [2, 1, 4]; - launch chain lightning at the 1-st monster, then their health values are [1, 1, 4]; - launch chain lightning at the 3-rd monster, then the . . . Training problem 2 Eshag has an array consisting of integers. Eshag can perform the following operation any number of times: choose some subsequence of and delete every element from it which is strictly larger than AV G, where AV is the average of the numbers in the chosen subsequence. For example, if = [1, 4, 3, 2, 4] and Eshag applies the operation to the subsequence containing a1, a2, a4 and a5, then he will delete those of these 4 elements which are larger than a1+a2+a4+a5 4 , so after the operation, the array will become = [1, 3, 2]. 4 = 11 Your task is to find the maximum number of elements Eshag can delete from the array by applying the operation described above some number (maybe, zero) times. sequence is subsequence of an array if can be obtained from by deletion of several (possibly, zero or all) elements. The solution will be evaluated in code environment. Training problem 3 There are squares drawn from left to right on the floor. The i-th square has three integers pi, ai, bi, written on it. The sequence p1, p2, dots, pn forms permutation. Each round you will start from the leftmost square 1 and jump to the right. If you are now on the i-th square, you can do one of the following two operations: 1. Jump to the + 1-th square and pay the cost ai. If = n, then you can end the round and pay the cost ai. 2. Jump to the j-th square and pay the cost bi, where is the leftmost square that satisfies > i, pj > pi. If there is no such then you can end the round and pay the cost bi. There are rounds in the game. To make the game more difficult, you need to maintain square set (initially it is empty). You must pass through these squares during the round (other squares can also be passed through). The square set for . . . 35 Preprint. Under review. Training problem 4 YouKn0wWho has an integer sequence a1, a2, . . . an. Now he will split the sequence into one or more consecutive subarrays so that each element of belongs to exactly one subarray. Let be the number of resulting subarrays, and h1, h2, . . . , hk be the lengths of the longest increasing subsequences of corresponding subarrays. For example, if we split [2, 5, 3, 1, 4, 3, 2, 2, 5, 1] into [2, 5, 3, 1, 4], [3, 2, 2, 5], [1], then = [3, 2, 1]. YouKn0wWho wonders if it is possible to split the sequence in such way that the bitwise XOR of h1, h2, . . . , hk is equal to 0. You have to tell whether it is possible. The longest increasing subsequence (LIS) of sequence b1, b2, . . . , bm is the longest sequence of valid indices i1, i2, . . . , ik such that i1, i2, . . . , ik and bi1 , bi2, . . . , bik . For ex . . . Training problem 5 Eve is beginner stand-up comedian. Her first show gathered grand total of two spectators: Alice and Bob. Eve prepared a1 + a2 + a3 + a4 jokes to tell, grouped by their type: type 1: both Alice and Bob like them; type 2: Alice likes them, but Bob doesnt; type 3: Bob likes them, but Alice doesnt; type 4: neither Alice nor Bob likes them. Initially, both spectators have their mood equal to 0. When spectator hears joke he/she likes, his/her mood increases by 1. When spectator hears joke he/she doesnt like, his/her mood decreases by 1. If the mood of spectator becomes negative (strictly below zero), he/she leaves. When someone leaves, Eve gets sad and ends the show. If no one leaves, and Eve is out of jokes, she also ends the show. Thus, Eve wants to arrange her jokes in such way that the show lasts as long as possible. Help her to calculate the maximum number of jokes she can tell before the show ends. The solution will be evalu . . . Training problem Solve the following coding problem using the programming language python: zscoder has deck of + custom-made cards, which consists of cards labelled from 1 to and jokers. Since zscoder is lonely, he wants to play game with himself using those cards. Initially, the deck is shuffled uniformly randomly and placed on the table. zscoder has set which is initially empty. Every second, zscoder draws the top card from the deck. If the card has number written on it, zscoder removes the card and adds to the set S. If the card drawn is joker, zscoder places all the cards back into the deck and reshuffles (uniformly randomly) the + cards to form new deck (hence the new deck now contains all cards from 1 to and the jokers). Then, if currently contains all the elements from 1 to n, the game ends. Shuffling the deck doesnt take time at all. 36 Preprint. Under review. What is the expected number of seconds before the game ends? We can sho . . . Training problem 7 pupils, who love to read books, study at school. It is known that each student has exactly one best friend, and each pupil is the best friend of exactly one other pupil. Each of the pupils has exactly one interesting book. The pupils decided to share books with each other. Every day, all pupils give their own books to their best friends. Thus, every day each of the pupils has exactly one book. Your task is to use the list of the best friends and determine the exchange of books among pupils after days. For simplicity, all students are numbered from 1 to in all tests. The solution will be evaluated in code environment. Training problem You are given rooted tree, consisting of vertices. The vertices are numbered from 1 to n, the root is the vertex 1. You can perform the following operation at most times: choose an edge (v, u) of the tree such that is parent of u; remove the edge (v, u); add an edge (1, u) (i. e. make with its subtree child of the root). The height of tree is the maximum depth of its vertices, and the depth of vertex is the number of edges on the path from the root to it. For example, the depth of vertex 1 is 0, since its the root, and the depth of all its children is 1. Whats the smallest height of the tree that can be achieved? The solution will be evaluated in code environment. Training problem 9 Back in time, the seven-year-old Nora used to play lots of games with her creation ROBOHead-02, both to have fun and enhance his abilities. One day, Noras adoptive father, Phoenix Wyle, brought Nora boxes of toys. Before unpacking, Nora decided to make fun game for ROBO. She labelled all boxes with distinct integers a1, a2, . . . , an and asked ROBO to do the following action several (possibly zero) times: Pick three distinct indices i, and k, such that aiaj and aiak. In other words, ai divides both aj and ak, that is aj mod ai = 0, ak mod ai = 0. After choosing, Nora will give the k-th box to ROBO, and he will place it on top of the box pile at his side. Initially, the pile is empty. After doing so, the box becomes unavailable for any further actions. Being . . . Training problem 10 This is an interactive problem You are given grid n, where is odd. Rows are enumerated from 1 to from up to down, columns are enumerated from 1 to from left to right. Cell, standing on the intersection of row 37 Preprint. Under review. and column y, is denoted by (x, y). Every cell contains 0 or 1. It is known that the top-left cell contains 1, and the bottom-right cell contains 0. We want to know numbers in all cells of the grid. To do so we can ask the following questions: x1y1x2y2, where 1 x1 x2 n, 1 y1 y2 n, and x1 + y1 + 2 x2 + y2. In other words, we output two different cells (x1, y1), (x2, y2) of the grid such that we can get from the first to the second by moving only to the right and down, and they arent adjacent. As response to such question you will be told if there exists path between (x1, y1) and (x2, y2), going only to the right or down, numbers in cells of which form palindrome. For example, paths, shown in gr . . ."
        }
    ],
    "affiliations": [
        "ETH Zürich, Switzerland",
        "Max Planck Institute for Intelligent Systems, Tübingen, Germany"
    ]
}