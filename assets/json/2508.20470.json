{
    "paper_title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
    "authors": [
        "Xiaochuan Li",
        "Guoguang Du",
        "Runze Zhang",
        "Liang Jin",
        "Qi Jia",
        "Lihua Lu",
        "Zhenhua Guo",
        "Yaqian Zhao",
        "Haiyang Liu",
        "Tianqi Wang",
        "Changsheng Li",
        "Xiaoli Gong",
        "Rengang Li",
        "Baoyu Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 0 7 4 0 2 . 8 0 5 2 : r Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation Xiaochuan Li1,, Guoguang Du1,, Runze Zhang1,, Liang Jin1,, Qi Jia1,, Lihua Lu1, Zhenhua Guo1, Yaqian Zhao1, Haiyang Liu, Tianqi Wang, Changsheng Li, Xiaoli Gong2 Rengang Li3,1, , Baoyu Fan2,1, 2 Nankai University https://dropletx.github.io 1 IEIT System Co., Ltd. 3 Tsinghua University Figure 1: Droplet3D achieves creative 3D content generation based on both image and text input. Commonsense priors including spatial consistency and semantic knowledge facilitate the 3D generation abilities of our method."
        },
        {
            "title": "Abstract",
            "content": "Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper *Equal contribution. Corresponding author. explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/."
        },
        {
            "title": "Introduction",
            "content": "Thanks to the increasing availability of massive online data, large-scale pre-trained models have demonstrated such generative potential in multi-modalities, such as text[43, 46], image[50, 32], and video[44, 74]. However, the scaling of 3D generative models faces significant bottleneck due to data scarcity, as the collection of 3D data is considerably more challenging than for other, more readily available modalities. For example, the largest open-source 3D dataset, Objaverse-XL [8, 6, 35], contains merely 10 million samples, orders of magnitude smaller than the imagetext paired datasets[51] which usually consists of billions of samples. This presents two main challenges. First, the limited coverage of existing 3D datasets hinders the comprehensive capture of the real world, leading to scarcity of learnable spatial consistency information that is constrained by data distribution biases. Second, compared to modalities such as text and images, 3D generative models learn less semantic knowledge, restricting their capacity to generate diverse content. Consequently, while the scaling law has been extensively validated in other modalities, the progress of this paradigm in the 3D domain remains comparatively slow. Fortunately, the abundance of online video data offers promising solution to mitigate 3D data scarcity[59, 27, 62]. First, vast video resources inherently contain multi-view spatial consistency information, as illustrated by the toy and dancing child examples in Fig. 1 where objects are captured from multiple views. From this viewpoint, such videos can be regarded as specific representation of 3D data. Introducing these videos significantly enhances the foundational capability of 3D generative models to maintain cross-view consistency. Second, the scale advantage of video datasets enables broader semantic knowledge than 3D data. As demonstrated by the QR code case in Fig. 1, by learning from video samples, 3D model can comprehend semantic concepts not present in its 3D training corpus and successfully generate objects that conform to textual descriptions. This indicates that the diverse semantic knowledge encapsulated in videos provides 3D models with critical generalizable commonsense priors. Motivated by this, we hope to study video-driven paradigm for 3D generation, which creates 3D content based on an image and detailed text prompts. promising approach involves fine-tuning pretrained video generation models with 3D data, enabling the adapted 3D generation models to inherit beneficial commonsense priors from the video domain. promising paradigm involves fine-tuning pre-trained video generation model using 3D data, enabling the resulting 3D model to inherit the rich commonsense priors from its video counterpart, thereby enhancing its generative capabilities. Although existing work[4, 58] recognized this potential, their specific fine-tuning datasets, base models, and strategies exhibited limitations. Specifically, these approaches fail to adequately leverage text supervision and lack explicit mechanisms to preserve the capabilities transferred from the video backbone. In light of this, our research on this issue encompasses from dataset construction to model development. We proposed large-scale 3D dataset, Droplet3D-4M. It comprises approximately 4 million 3D models, each accompanied by 360 orbital rendering video and detailed multi-view level text annotation. Each video consists of 85 frames captured along circular camera path around the object, which covers complete 360-degree view. On the text side, each video is paired with dense natural language caption averaging about 260 words in length. This caption meticulously characterizes the objects overall appearance and elaborates on the unique details of different views, such as front, side, or back views, from sequential viewpoints as the camera orbits. This dense multi-view-level text annotation provides rich supervisory signals for the model to understand and generate each viewpoint, which benefits 3D generative models developing. 2 Furthermore, we introduce novel 3D generative model, Droplet3D, which supports both image and dense text input, allowing our model to generate content that meets user needs with greater granularity. It is fine-tuned from DropletVideo[74], video diffusion model capable of modeling integral spatio-temporal consistency. Droplet3D is capable of generating sequence of 85 dense multi-view images capturing full 360-degree orbit of the object. In addition, to adapt to the input requirements of Droplet3D, we introduce text rewriting module and an initial image canonical view alignment module to preprocess the users prompt, thereby aligning it with the distribution of Droplet3D-4M. Finally, based on the generated orbital views, we employed downstream applications using both 3D Gaussian splatting and textured mesh reconstruction algorithms, surpassing the fixed output format commonly used in current prevailing 3D generation solutions[77, 31]. Comprehensive qualitative and quantitative experiments demonstrate the superiority of our proposed approach for 3D content generation tasks based on images and text. Extensive visualization experiments show various interesting capabilities of Droplet3D, demonstrating the potential of video-driven 3D generation methods for creative generation. Notably, in contrast to the prevailing object-level 3D solutions[23, 64], our model demonstrates considerable potential for scene-level generation. The contributions of this work are as follows: We observe that commonsense priors learned from videos can facilitate 3D generation, and we provide an initial exploration and validation of this video-driven 3D generative solution. We release Droplet3D-4M, large-scale 3D dataset paired with circumferential descriptions, containing 4 million 3D models, dense multi-view images, and detailed orbital text captions. We introduce Droplet3D, 3D multi-view generation model. It inputs both image and dense text and generates multi-view images, which supports downstream 3D Gaussian splatting and textured meshes reconstructions. We have open-sourced the dataset, code, the model weights, and the complete technical solution. We hope this initiative fosters algorithmic innovation in the public domain."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 3D Datasets The 3D dataset aims to construct collection of 3D content encompassing various data formats to support the research and evaluation of intelligent algorithms such as reconstruction and understanding. Existing 3D datasets are primarily constructed based on meshes or multi-view images. Mesh-based 3D datasets, including Objaverse[8], Objaverse-XL[7], ShapeNet[3], Thingi10K[78], 3D-FUTURE[14], and OmniObject3D[63], provide mesh models defined by vertices, edges, and faces. This format makes them highly suitable for tasks such as 3D rendering and geometric analysis. For example, the widely used GSO[11] dataset falls into this category, which constructs 3D mesh structures based on scans of everyday objects. Additionally, the currently largest dataset, ObjaverseXL[7], also belongs to this category, containing over 10.2 million 3D objects sourced from variety of origins, ranging from artificially designed models to photogrammetric scans, significantly exceeding its base version which composes of 800 thousand models[8]. However, despite its large scale, the quality of its samples varies considerably. Many samples contain only limited geometric patches and lack sufficient visual textures and text annotation information. This limitation restricts the application of such datasets in highly controllable 3D generation tasks requiring precise perspective-aware descriptions. On contrast, multi-view image-based 3D datasets, such as MVImgNet[73], MVImgNet2.0[17], and Co3D[49], provide 2D images or video frames capturing objects or scenes from multiple angles, supporting tasks such as Structure-from-Motion and 3D reconstruction. Among them, MVImgNet[73] includes 6.5 million frames from 219,188 videos across 238 categories, along with annotations such as masks and camera parameters. The further expanded MVImgNet2.0[17] covers approximately 300K objects with complete 360-degree views. Although these datasets provide rich multi-view data, their scale still lags significantly behind that of Objaverse-XL. Moreover, they also lack detailed text captions describing the appearance from specific perspectives. For example, although Cap3D[40] provides multi-view images, it provides short captions that describe the objects overall appearance, thereby overlooking the variations that should inherently exist in descriptions across 3 different viewpoints. This hinders tasks requiring the integration of visual-text understanding, such as text-guided 3D or video generation. In light of this, we propose the large-scale 3D dataset Droplet3D-4M, designed to support the training of image-and-text-based 3D generation tasks. This dataset includes high-quality mesh models strictly selected from Objaverse-XL, as well as densely distributed multi-view rendered images surrounding the objects. More importantly, Droplet3D-4M provides detailed text annotations at the surrounding multi-view level with an average length of 260 words, offering critical multi-modal supervision annotations for 3D generation tasks. 2.2 3D Generation Models In recent years, the task of generating 3D content from text or single image has advanced rapidly, underscoring the remarkable potential of AIGC to expand our repository of 3D digital assets. Existing research falls broadly into two paradigms. The first paradigm, termed native 3D generation methods [42, 26, 77], can directly generate editable 3D assets represented by meshes or voxels from image or text prompts. These methods typically require training on large-scale datasets and employ feedforward inference mechanisms to achieve efficient generation. Although highly efficient and multi-view consistent, their performance heavily depends on the scale and quality of the training data, which often face accessibility and acquisition limitations. Harnessing the remarkable generative power of large-scale pre-trained models in images [50, 32], and videos [44, 74], the second paradigm unifies 2D generative models to distill or reconstruct multi-view observations into 3D representations such as NeRF and 3DGS, yielding an efficient 3D generation framework. In the line of combining image generation models, pioneering work DreamFusion [45] and subsequent studies [57, 71, 34, 5, 72, 60] employ techniques like Score Distillation Sampling (SDS) [45] or its improved variants to distill 3D information from 2D diffusion priors. To enhance multi-view consistency, more recent studies [55, 37, 67] introduce conditional controls, such as camera embeddings [54, 37] and epipolar constraints [22], to fine-tune pre-trained diffusion models. Meanwhile, other works [19, 56, 65] propose to learn 3D via two-stage pipeline: first generating intermediate multi-view images and then utilizing these images to assist in feedforward 3D reconstruction. However, the generated intermediate images in such methods are typically sparse, and even with the limited range, leading to the spatial consistency required for high-quality 3D reconstruction remains elusive. More recently, video generation models [2, 18] have effectively maintained inter-frame consistency by integrating temporal continuity, making them particularly suitable for generating coherent multiview outputs. Inspired by this, several studies (e.g., V3D[4], SV3D[58], VideoMV[80], etc. [4, 58, 41, 80, 68, 16]) suggest adopting fine-tuned video diffusion models to generate multi-view outputs for subsequent 3D generation. Although impressive generation results, these approaches fail to adequately leverage text supervision and lack explicit mechanisms to preserve the capabilities transferred from the video backbone. Additionally, some methods [21, 33, 66, 41, 36, 69] consider jointly modeling both text and image prompts to produce 3D assets with better appearance and geometry control. For example, IM-3D [41] generates multi-view images from text and image pairs with the aid of video diffusion model, followed by 3D reconstruction to generate high-quality 3D assets. Nevertheless, videos and video generation models are still underexplored in 3D generation due to simple text or image prompts. Therefore, in this work, we introduce novel 3D generative model, Droplet3D, which supports both image and dense text input, to further explore the potential of video modality in advancing 3D generation. Our model allows generate content that meets user needs with greater granularity, and supports downstream 3D Gaussian splatting and textured meshes reconstructions."
        },
        {
            "title": "3 The Droplet3D-4M Dataset",
            "content": "To take advantage of the commonsense priors acquired by video generation models trained in largescale video data and thereby enhance the generalization capability of 3D generative models, we construct novel dataset Droplet3D-4M to bridge the gap between the video and 3D domains. Specifically, qualifying 3D sample must satisfy two criteria. First, it should support dense, coherent 4 Figure 2: sample from Droplet3D-4M comprises 85-frame multi-view rendered video and fine-grained, multi-view-level text annotation. The blue, red, and green text illustrate viewpoint and appearance changes at three consecutive time steps, with corresponding left-side color markers indicating the specific moments of transformation. multi-view sequences. This ensures compatibility with the output interface of video generation models, facilitating the generation of spatially consistent 3D content. Second, it should be accompanied by dense text annotations. This provides stronger supervision, enabling the 3D generator to maximally retain the video models semantic understanding of text. As the 3D generator inherits broader semantic knowledge from video data, this dense text supervision allows the generative model to effectively exploit this inherited capability. Table 1: Comparison of Droplet3D-4M and other 3D-language datasets. Droplet3D is largescale dataset of 4 million 3D objects, distinguished by its provision of dense 85-view renderings and precisely aligned, multi-view level dense text annotations for each object, which is novel among existing 3D datasets. Note that MVI stands for Multi-view Images. Year Objects Format Images Views Annotations Anno-Grain GSO[11] OmniObject3D[63] Thingi10K[78] 3D-FUTURE[14] Objaverse-OA[39] ShapeNet[3] Anymate[9] Objaverse[8] TexVerse[75] Objaverse-XL[7] CO3D[49] MV-Video[25] uCO3D[38] G-Objaverse[79] MVImgNet[73] MVImgNet2.0[17] Objaverse++[35] Cap3D[40] Droplet3D-4M 2024 2024 2016 2021 2025 2015 2025 2022 2025 2023 2021 2024 2025 2024 2023 2024 2025 2023 2025 Mesh 1K Mesh 6K Mesh 10K Mesh 10K Mesh 14.8K Mesh 51K Mesh 230K Mesh 800K 858K Mesh 10.2M Mesh 19K 53K 170K 200K 220K 300K 500K 791K 4M MVI MVI MVI MVI MVI MVI MVI MVI MVI 1.5M 88M 34M 30M 6.5M 9M 20M 6M 900M 79 48 200 38 30 30 40 8 85 3 sentence Object Level <15.0 words 260.0 words Multi-view Level Object Level Object Level Accordingly, our proposed dataset, Droplet3D-4M, comprises dense multi-view rendered videos and fine-grained, multi-view-level text annotations, as illustrated in Fig. 2. Each rendered video consists of an 85-frame image sequence captured from uniformly distributed 360 orbital viewpoints. The angular difference between adjacent frames is strictly controlled to be within 5, ensuring video coherence, which is critical factor In training video-driven generative models. In terms of text 5 annotations, we provide dense descriptions with an average length of 260 words, far exceeding those in existing 3D datasets. More importantly, the annotations not only cover holistic appearances such as shape and style but also specifically describe appearance variations induced by changes in viewpoint. For example in Fig. 2, the second paragraph of the text annotation details the side and rear features of the figurine, describing that the yellow backpack becomes partially visible from side view and is only fully revealed from the back. This fine-grained, view-aware annotation paradigm provides unprecedented supervisory signals for the 3D generative model, effectively guiding and preserving the backbone networks capacity for complex semantic understanding. In contrast to current prevailing 3D datasets, which typically lack text annotations or provide only brief descriptions at object-level, Droplet3D-4M achieves significant enhancement in information richness, which has been illustrated in Tab. 1. For the construction of Droplet3D-4M, we propose data pre-processing pipeline that balances quality and efficiency, and construct new dataset, Droplet3D-4M. This pipeline employs adaptive sampling techniques instead of resource-intensive rendering during the initial screening phase, followed by targeted high-fidelity rendering only for validated assets. Compared to conventional workflows, this approach reduces computational overhead by factor of 4 to 7 while generating metadatarich outputs of superior quality for multimodal learning tasks. The pipeline consists of three key parts: multi-view video rendering, image evaluation metric filtering, and multi-view-level caption generation, as shown in Fig. 3. Note that the raw 3D models we collected are from Objaverse-XL[7], comprising 6.3 million models sourced from GitHub and Sketchfab. Figure 3: The pipeline we proposed to curate the Droplet3D-4M dataset. The proposed pipeline encompasses three key stages: multi-granularity rendering, filtering, and caption generating. 3.1 Multi-View video Rendering We designed both coarse and fine rendering strategies for multi-view videos to mitigate the explosive computational consumption caused by rendering all data directly using the Blender engine. For the coarse rendering, we initially rendered the 6.3 million 3D models sourced from Objaverse-XL at low quality using the pyrender library. For the fine rendering, we employed BlenderProc to render the filtered 4 million models that met the quality and aesthetic score criteria. The fundamental rendering hyperparameters were kept consistent across both rendering processes. The perspective camera configuration was set to 50-degree vertical field of view and maintained fixed aspect ratio of 1.0. Prior to rendering, the 3D meshes underwent geometric normalization, including aligning the centroid to the world origin, uniformly scaling to unit maximum extent, and optional axial rotation. The scene lighting utilized multi-light setup: directional ambient light with an intensity of 2.0 provided base illumination, supplemented by three positional point lights, each with an intensity of 8.0, strategically placed on the left, right, and rear of the object at distance of 2 units. The coarse rendering process leveraged PyRenders off-screen rendering capabilities and EGL backend acceleration, while the fine rendering process utilized Blenders EEVEE renderer to enhance quality. For the coarse rendering process, only 8 views were rendered for metric calculation and filtering. For the high-quality rendering, 85 camera positions were uniformly distributed along circular trajectory with fixed 0 elevation angle, with radial distances randomly varying between 1.6 and 2.0 units. All cameras were focused on the scene origin, with the +Y axis serving as the upward reference direction. 6 3. Image-Assessment Metric Filtering We filtered the dataset by selecting high-quality multi-view rendering videos based on aesthetics and image quality. We utilize the publicly available LAION aesthetics model[51] to compute aesthetic scores and the DOVER-Technical model[61] to evaluate image quality. Only clips surpassing predefined thresholds are retained. Notably, nearly 77% of clips achieve an aesthetic score above 4.0, while approximately 81% exceed score of 4.0 in image quality, underscoring the datasets high visual fidelity. 3.3 Multi-View consistent captions Generation 3.3.1 Supervised-finetuning VLM captioner The proposed framework employs an AI-driven approach to convert video content into detailed textual descriptions, significantly reducing the need for human annotation. Traditional video captioning methods often produce brief outputs that fail to capture nuanced visual elements across different perspectives. To address this, our methodology incorporates dual-phase enhancement mechanism. First, we construct curated video dataset featuring comprehensive annotations that precisely track object states, camera movements, and most importantly variations in detail levels across viewpoint transitions. These preliminary descriptions are then linguistically refined using GPT-4 to ensure grammatical precision and stylistic consistency, resulting in high-quality training material. For the technical implementation, we fine-tuned multiple cutting-edge multimodal models within the 7-9 billion parameter spectrum, considering both performance and efficiency. The generated annotations exhibit variations in detail levels across viewpoint transitions including precise camera operations, object-surface changes, illumination changes, and environmental characteristics. Such meticulous documentation offers vital training cues for keeping the multi-view correspondence, especially concerning appearance changes. Quality assurance measurements indicate that the automated captions match or surpass human-generated annotations in terms of multi-view correpondence. 3.3.2 Enhancing MLLMs for Multi-View Captioning via GRPO Figure 4: An example of reinforcement learning samples for GRPO. We constructed constrained model to annotate the think and answer processes. For the think process, we provided annotations on themes, materials, functions, details, OCR, etc. For the answer process, we identified two types of scoring points based on dense multi-view level captions: scoring points marked in blue denote those aligned with the Think process, while those marked in red indicate detailed descriptions of perspective changes. Recent studies have demonstrated that test-time scaling can significantly enhance the reasoning abilities of large-scale language models. This technique has been successfully implemented in state-of-the-art systems such as OpenAIs o1[24] and Deepseek-R1[15], both of which achieve strong 7 performance on challenging tasks involving mathematical reasoning and programming. However, vision-language models (VLMs) fine-tuned using conventional approaches frequently exhibit problematic behaviors, including incorrect object type predictions, misleading functional descriptions, and hallucinations of non-existent visual details. To mitigate these issues, the GRPO algorithm[52] has proven particularly effective when applied to supervised fine-tuning for specialized vision tasks. Empirical results confirm its advantages across multiple domains, including visual question answering[76], spatial relationship understanding[53], and temporal sequence analysis in video content[13]. Therefore, we designed the VLM model enhancements throught the second stage GRPO Reinforcement learning. We designed five dimensions to ensure the output descriptions meet our requirements for multi-perspective image captioning. They are Subject(Judging the type of the object), material(what the object is made of), Functional(what the object is used for), Details(changes due to the camera motion), OCR(whether the visual text during the view change). Also, similar to DeepseekR1, we designed <think>dimensions</think><answer>caption</answer> as the output format. The dimensions contain the descriptions of the object for the five dimensions above. The caption part is the fluent mutli-view caption for this sample, combining the dimension contents above with the view changes. Fig. 4 shows an example. Figure 5: An caption illustration from Droplet3D-4M. Each 3D model is ultimately rendered into an encompassing multi-view video comprising 85 frames with uniformly distributed angles. The caption typically consists of two paragraphs: the first describes the overall style and key details of the object, while the second provides multi-view-level description, emphasizing the appearance variations induced by the circular capture. Expressions related to viewpoint changes are highlighted in different colors. For the Reward funtions, we mainly focus on the content metric and the multiview-match format metric. The content metric formula is as: = 1 (cid:88) i= = 1 (cid:88) i=1 sign(i) sim(pi, gi) sign(i) sim(pi, gi) 1 = 2 + 8 (1) (2) (3) The multiview-match format metric is as: (cid:18) = min 1.0, (cid:80) matches(pattern : Vpattern) 5 (cid:19) (4) The Multi-view pattern can be described as the more key words, like front, circle-around, side, back, self-rotation, 360-degree, appear, the more pattern award will be obtained. The final reward is determined as: Reward = 1+M . Ultimately, captions that are insufficiently detailed are compelled to incorporate additional information. As illustrated in Fig. 5 with samples from Droplet3D-4M, it is evident that the captions contain substantial details regarding the appearance and perspectives. 3.4 Data Overall and Statistics In conclusion, the constructed Droplet3D-4M dataset comprises meticulously curated collection of 4 million fully-covered multi-view video sequences, encompassing approximately 900 million individual frames with total runtime exceeding 8,000 hours. Its unprocessed raw 3D models were collected from Objaverse-XL[7], comprising 6.3 million models sourced from GitHub and Sketchfab. This comprehensive visual dataset has been publicly released with the aim of advancing research in spherical vision systems and multi-view analysis. It should be noted that due to the origin of some source models being sampled from the internet, the dataset is published under the CC BY-NC-SA 4.0 license. Researchers are advised that although the material is freely available for academic research, all content within it remains subject to the original distribution permissions, restricting its use to non-commercial purposes only. Figure 6: The aesthetics distribution and the image quality distribution of Droplet3D-4M. These distributions demonstrate that our dataset achieves high scores in both aesthetics and image quality, indicating an overall high-quality standard for the dataset. In addition, we computed the aesthetic and quality scores of the rendered videos in the dataset, as shown in Fig. 6. As previously mentioned, Droplet3D-4M ultimately retained only those samples with both scores above 4.0."
        },
        {
            "title": "4 The Droplet3D Model",
            "content": "To inherit the spatial consistency and semantic knowledge priors that are readily accessible to video models into 3D generation algorithms, we designed and trained Droplet3D. To this end, we specifically employ multi-view rendered videos from Droplet3D-4M to interface with the video backbone model. Furthermore, we use multi-view level annotations to provide fine-grained supervision for model fine-tuning, thereby preserving the models cross-modal semantic understanding capability. The architecture of Droplet3D is illustrated in Fig. 7. The technical details of the 3D generation process during Droplet3D is illustrated in Fig. 8. For any given text or image prompt, Droplet3D first aligns it with the model. Initially, we expand the users input requirements based on lightweight large language model, rewriting them into dense textual descriptions that follow the same distribution as the multi-view level captions in the Droplet3D-4M dataset. Simultaneously, to accommodate inputs from arbitrary viewpoints, we designed anonical viewpoint alignment module to adjust the perspective of the input images. For the backbone network, we introduce 3D causal VAE to achieve implicit space encoding and decoding of the video, and employ multi-modal diffusion transformer to facilitate the fusion of text and video modality features while constraining their independence. Droplet3D demonstrates creativity in the process of generating surrounding multi-view videos based on images and text prompts. As an application, we adapted and integrated various converters 9 Figure 7: Droplet3D Framework: Inheriting spatial-semantic priors from massive videos for high-fidelity 3D generation. Droplet3D employs DropletVideo as its video backbone, effectively leveraging its commonsense priors of spatial consistency and semantic knowledge. This enables novel 3D object generation paradigm conditioned on the joint input of an initial image and dense text, while achieving superior generalization performance. to reconstruct different 3D modalities, such as textured mesh and Gaussian splatting, serving as applications of the content generated by Droplet3D. Figure 8: Overview of the Droplet3D Techniques. To enhance performance, Droplet3D supports alignment module to transfer the user input to fit the model. Subsequently, the aligned text and image features are fed into the backbone network to generate multi-view images with 3D consistency. At the end, the created multi-view content can be integrated into reconstruction modules for various 3D modalities, thereby generating production-ready 3D assets. 4.1 Multi-View Image Generation The generation of multi-view images, or surround-view videos, is core step in Droplet3D, and it is crucial for creative 3D generation based on images and text. From our perspective, this step can be viewed as bridge that transfers the general capabilities of video models to 3D generation. 3D Causal VAE 10 Figure 9: The surrounding multi-view video generation backbone of Droplet3D. It consists of two components: 3D causal VAE and vision-text modality-expert Transformer architecture. Within the backbone network of Droplet3D, videos are initially processed by 3D causal Variational Autoencoder (VAE) to be transformed into latent space features. We extend the 3D Causal VAE using VAE framework coupled with 3D structural architecture, designed for encoding and decoding video frames. Unlike traditional autoencoders, the outputs of the VAEs encoder and decoder are modeled as parameterized probability density distributions. Yang et al. [70] applied 3D convolutions to video reconstruction, demonstrating that 3D structure can effectively mitigate the flickering artifacts often present in reconstructed videos. Inspired by this, we employ 3D causal VAE to enhance the computational efficiency of Droplet3D, while ensuring the generated videos possess temporal continuity and stability. Simultaneously, on the long-text side, we extract linguistic features based on the encoder structure of the T5 model[47], feeding these features concurrently into the subsequent 3D Modality-expert Transformers. 3D Modality-Expert Transformer Within the Transformer backbone network, we employ 3D multi-modal attention mechanism to simultaneously process both text and video inputs. Specifically, for the video input, given that 3D full attention is technique that has evolved alongside the widespread adoption of Transformers in computer vision, we incorporate 3D positional embeddings within the Transformer architecture. Concurrently, the text input is encoded through conventional Transformer encoder to facilitate the smooth fusion of multi-modal information. Compared to previous decoupled approaches, this integrated methodology enables more effective capture of dynamic changes within the video and enhances the generated contents performance in terms of semantic consistency and diversity. Further Training Based on DropletVideo Given that the fundamental rationale for introducing the video generation step is to incorporate the general capabilities of video generation models into 3D asset generation, we proceed with further training based on an appropriate video generation backbone model. DropletVideo[74] is video generation model that considers integral spatio-temporal consistency, supporting the generation process given an initial frame image and dense text input. This aligns formally with the multi-view image generation pipeline of Droplet3D. Additionally, because its training data includes large-scale video segments constrained by spatial consistency, such as certain street scene or character surround shots, it exhibits strong potential for 3D consistency. Therefore, we conduct secondary training based on its weights to fit the latent space feature distributions for 3D surround-view generation. 4.2 User Input Alignments 4.2.1 Multi-View-Level Text Alignment To effectively align the variations in linguistic style and length of user-provided prompts and to provide detailed guidance for the surround-view video generation of the target object, we implement 11 dense prompt generation preprocessing step. Specifically, considering the superior performance of large language models in tasks such as text reasoning and image summarization, we employ the LoRA [20] technique to fine-tune an open-source model via instruction tuning. Experimental results indicate that approximately 500 in-domain samples are sufficient to achieve the desired fine-tuning level. This module is designed to rewrite user prompts while preserving the original semantics. It transforms the prompts into standardized information structure similar to the captions used during training. Similar to Droplet3D-4M, the rewritten descriptions typically consist of two denser paragraphs. The first paragraph describes the overall style of the object and as many details as possible, while the second paragraph elaborates on multi-view level details based on the users prompt, as illustrated in Fig. 10. Users can review or modify the generated text for proofreading before inputting it into Droplet3D. Besides, this module supports multiple languages. Figure 10: An illustration of rewritten samples. Text describing viewpoint changes is highlighted in red. Green text indicates how user-provided brief or detailed descriptions have been rewritten into form consistent with the training samples, showcasing their appearance across different viewpoints to provide more specific design requirements. 4.2.2 Image Perspective Alignment Figure 11: Image alignment module converts users arbitrary inputs into canonical perspectives. It employs LoRA fine-tuning based on the FLUX.1-Kontext-dev model to rectify arbitrary input viewpoints into canonical perspectives, such as front, back, left, and right. 3D generation methods typically achieve optimal performance only when provided with canonical view, demonstrating poor adaptability to arbitrary input perspectives. This imposes strict requirements on user input. For instance, when user aims to perform 3D generation from an image captured at rotated angle, the performance of existing generation algorithms significantly degrades. Motivated by 12 this limitation, we introduce view alignment module in Droplet3D, enabling it to be seamlessly invoked with images from any viewpoint. This module fine-tunes the input view to one of the canonical viewpoints: front, left, right, or back. We selected some samples with high image-quality from the Droplet3D-4M dataset and collected some AIGC generated samples from Internet, making small dataset containing 200 examples. Then, we constructed the canonical viewpoints generation dataset from the small dataset. For each example, we manually selected four standard orthogonal views as ground truth, and constructed image pairs using initial images rendered by rotating [15, 40] degrees clockwise or counterclockwise from the chosen ground truth viewpoint. Finally, we frame this task as an image editing problem, with the fixed editing prompt: Convert this image to the closest canonical view, such as front, left, right or back. For the model selection and fine-tuning, we selected the FLUX.1-Kontext-dev [30] as the base model, training it with Low-Rank Adaptation (LoRA)[20]. As shown in Fig. 11, the model architecture In the dual-stream employs hybrid design combining dual-stream and single-stream blocks. configuration, separate weight matrices process image and text tokens independently, with crossmodal fusion achieved through attention mechanisms applied to concatenated token sequences. Following dual-stream processing, the integrated token sequence undergoes joint encoding via successive single-stream blocks before final text token pruning and image token decoding."
        },
        {
            "title": "Implementation and Experiments",
            "content": "5.1 Implementation Details We employ the parameters of the pre-trained DropletVideo-5B[74] model as the weight initialization for Droplet3D. Similar to DropletVideo, we employ t5-v1_1-xxl[47] as the text encoder, and set the maximum token length to 400 instead of 226, to accommodate longer captions. The model architecture is based on the MMDiT series[12], consisting of 42 layers, with 48 attention heads per layer, and each head has dimension of 64. The time step embedding dimension is set to 512. For optimization, we use Adam[29] with weight decay of 3 102 and an eps value of 1 1010. The learning rate is set to 2 105. The number of sampled frames ((N)) is fixed at 85. The model is trained using the bfloat16 mixed precision method, similar to the DeepSpeed framework[48]. During inference, the classifier-free guidance scale is set to 6.5 to enhance the motion smoothness of the generated surround-view videos. When trained on the Droplet3D-4M dataset, our model supports an image resolution of 512. For the Canonical View Alignment training, The LoRA rank (network dimension) was set to 128. The learning rate is 1e-4 and the optimizer is AdamW8bit[10]. 5.2 Evaluation 5.2.1 TI-to-3D Quantitative Evaluation We conducted quantitative comparative analysis based on the GSO dataset[11]. To validate the accuracy of the generated content by Droplet3D given single image and text prompt, we compared it with other TI-to-3D methods, including LGM[56], MVControl[33] as shown in Tab. 2. Given the bias in the coverage of the GSO dataset, we performed down-sampling and selected subset of 200 samples for the evaluation. The sampled instances can cover all categories in the original dataset while ensuring, to the greatest extent possible, that their distribution is confirmed uniform. Table 2: Quantitative comparison of Droplet3D with other 3D generation methods that simultaneously support both text and image inputs. Droplet3D outperforms the other two methods. SSIM () LPIPS () MSE () CLIP-S () PSNR () LGM[56] MVControl[33] Droplet3D (Ours) 21.38 22.31 28.36 0.137 0.31 0.03 0.011 0.009 0.0017 0.737 0.61 0.866 0.84 0.88 0. 13 Figure 12: Comparison of generated content from different generative methods that support simultaneous image and text input. The experimental cases demonstrate that the generation performance of Droplet3D is significantly superior to that of the two baseline methods. As shown in Tab. 2, our model significantly outperforms the other two methods in terms of generation performance. On the one hand, metrics such as PSNR and SSIM confirm that the content generated by Droplet3D is of higher quality and closer to the ground truth. On the other hand, it demonstrates stronger semantic understanding capabilities, as evidenced by the CLIP scores in the last column. We attribute this to our use of T5 as the text encoder and extensive pre-training on video data. Fig. 12 presents qualitative comparison between Droplet3D and two other methods, LGM and MVControl, both of which also support simultaneous input of image and text. The results demonstrate that our method outperforms the others in terms of aesthetic quality, precision, and consistency between the generated content and the input image. This further validates the superiority of our Droplet3D in the TI-2-3D task. 5.2.2 Ablation Study Considering that the video generation backbone network is core module of Droplet3D, we conduct quantitative comparisons using different backbone networks. These include DropletVideo[74], which serves as the genetic origin of Droplet3D, and Cogvideox-Fun[1], model of comparable scale. Additionally, we compare against Wan2.1-I2V-14B and Step-Video-TI2V-30B, which are currently among the most state-of-the-art open-source video generation models. Tab. 3 demonstrates the effectiveness of the proposed Droplet3D-4M dataset, showing that models further trained with it exhibit improved generation consistency. It can be observed that while the 14 Figure 13: Comparison between Droplet3D and its predecessor, DropletVideo. The performance enhancement of Droplet3D over DropletVideo in terms of spatial consistency underscores the critical role of continued training on the Droplet3D-4M dataset. Table 3: Comparison of Droplet3D and the backbone it inherits from, DropletVideo, in terms of generation performance. Droplet3D performs better. SSIM () LPIPS () MSE () CLIP-S () PSNR () Droplet3D-5B DropletVideo-5B[74] 28.36 20.51 0.76 0.87 0.03 0.12 0.0017 0.02 0.866 0. original DropletVideo, which was not trained on Droplet3D-4M, also demonstrates commendable performance across various metrics, it still falls short of the Droplet3D proposed in this work. Furthermore, as illustrated in Fig. 13, DropletVideo demonstrates notable capability in generating camera-rotating surround-view videos, with the synthesized content exhibiting high fidelity and geometric accuracy. This proficiency suggests that DropletVideo is well-suited to serve as robust initialization for the weights of backbone network. Nevertheless, the model exhibits certain shortcomings for 3D. Specifically, it tends to generate inconsistencies in dynamic objects, such as panda depicted with its eyes closed in static pose. However, this inherent limitation underscores the critical necessity for subsequent fine-tuning with the proposed Droplet3D-4M to enhance the models performance in 3D generations. In Fig. 13, it can be observed that, in the provided examples, Droplet3D is capable of generating superior 3D content. plausible explanation for this is that the generated surround-view images by Droplet3D are both plausible and rich in detail, while maintaining high spatial consistency. This endows our model with commendable foundational capabilities for 3D generation. Additionally, Quantitative results shown in Tab. 4 indicate that DropletVideo is indeed suitable for further training in 3D generation than other video generators. This may be related to its consideration of integral spatio-temporal consistency, as the large number of samples incorporating spatial consis15 Figure 14: Comparison between DropletVideo and other video backbones. DropletVideo demonstrates superior performance in circumnavigation shooting tasks. Using the chimney in red circle as an example, DropletVideo successfully generates surrounding-camera-motion videos, whereas Step and WanX only support minor rotational movements, and the similarly-sized Cogvideox-Fun lacks this capability entirely. Concurrently, DropletVideo also surpasses all competing models in preserving the original images style, such as its background color. Table 4: Comparison of different video generation backbone models. We compared video models including DropletVideo, in terms of generation performance. Params LPIPS () MSE () CLIP-S () PSNR () SSIM () DropletVideo[74] Cogvideox-Fun[1] Wan2.1-I2V Step-Video-TI2V 5B 5B 14B 30B 20.51 15.16 12.97 15.32 0.87 0.50 0.89 0.79 0.12 0.21 0.19 0.30 0.02 0.09 0.067 0.08 0.76 0.68 0.67 0.64 tency endows it with inherent 3D generation capabilities. It significantly outperforms other models of comparable scale, such as Cogvideox-Fun, and achieves performance comparable to or exceeding that of the other two larger and more advanced models, even without fine-tuning. Tab. 4 also demonstrates the video generation capabilities of DropletVideo-5B alongside Step-Video-TI2V-30B and Wan2.1-I2V-14B, currently the most popular open-source video generation models, despite their larger parameter counts. The results indicate that DropletVideo indeed possesses higher capacity for maintaining 3D consistency compared to models of similar scale. While Step and WanX, due to their larger weights, also exhibit considerable ability, they may exhibit more pronounced object movements, 16 such as the bears head turning, the gold coin onion blinking or jumping, possibly due to the training samples being more inclined towards narrative changes. In contrast, CogVideoX-Fun-v1.5, with similar parameter count to DropletVideo-5B, demonstrates poorer performance. Visual examples are shown in Fig. 14. DropletVideo demonstrates strong capability to generate multi-angle views of objects from input images, consistently achieving high-quality results for most cases. Note that considering the trade-offs between spatial consistency, foundational generation capabilities, and model size, we selected DropletVideo as the backbone for subsequent training in the 3D domain. 5.3 Potential and Applications in 3D Generation 5.3.1 Feasible Controllable-Creativity Driven by Language Prompts Figure 15: Controllable-Creativity based on the initial image of the Panda Astronaut case and different given texts. The three rows in the figure respectively demonstrate the generation based on the given space backpack, orange backpack, and energy ball. Figure 16: Controllable-Creativity based on the initial image of the Castle case and different given texts. The three rows in the figure respectively demonstrate the generation based on the given stone door, blue gate, and garden with red door. Current mainstream 3D generation models typically only support image or text input[77, 64]. However, Droplet3D conditions on both an initial image and dense text, enabling it to support more targeted creative design, which is quite interesting. This is reflected in the variations of the generated assets when we employ AIGC images along with different textual descriptions. Fig. 1518 present four sets of examples. In each set, Droplet3D generates different samples by utilizing the same AIGC-generated image as input, accompanied by distinct text guidance. As shown in Fig. 15, when different text descriptions are provided for the background of the panda astronaut, the generated effects are both distinct and plausible. Droplet3D can generate his space backpack, orange backpack, and vibrant energy ball that match the pandas physique and appearance based on its body shape and visual characteristics. Notably, in the example of generating energy ball, Droplet3D can produce objects that are either dazzling or semi-transparent. Compared to generating textured meshes, images or videos can more intuitively produce content with richer colors. This is also one of the factors we believe contributes to the feasibility of video-based generative models in 3D generation. Similarly, as illustrated in Fig. 17, Droplet3D can produce varying details behind the axe depending on whether the text description specifies gems or runes. Besides, as shown in Fig. 18, when different decorations such as wind-up key, crystal orb, or even QR code are input, the model is able to understand the semantic meaning of the text. Additionally, its color generation 18 Figure 17: Controllable-Creativity based on the initial image of the Battle Axe case and different given texts. The three rows in the figure respectively demonstrate the generation based on the given symmetrical side, purple crystal, and green life gem. is also relatively accurate. This capability for controllable 3D content generation stems from two main sources. On the one hand, our Droplet3D inherently supports both image and text interfaces at the data level. On the other hand, it benefits from richer prior learned from vast volume of video media. First, the greater amount of spatial consistency data extracted from these videos helps to enhance the 3D generators ability to produce accurate content. Second, the more general semantic knowledge integrated from the video backbone model enables our model to better comprehend textual requirements. prime example is the QR code generation task. When prompted to generate QR 19 Figure 18: Controllable-Creativity based on the initial image of the Coin Onion case and different given texts. The three rows in the figure respectively demonstrate the generation based on the given wind-up key, crystal orb, and QR code. code, an object scarcely found in 3D model datasets, Droplet3D can still accurately identify what it is and generate it correctly, drawing upon its knowledge derived from videos. These experimental results demonstrate the potential of our proposed method in the field of 3D asset generation. The ability to create based on both images and expressions is one of the attributes we find particularly compelling, and it also validates the feasibility of leveraging video to facilitate 3D creation. Unlike generation algorithms that support only single modality, either image or text, simultaneously referencing image priors and supplementing with textual descriptions also holds practical utility for 3D content generation. 5.3.2 3D Lifting from Stylized Inputs Droplet3D exhibits relatively robust capability in lifting 2D content into 3D. Although the training data for Droplet3D-4M is entirely based on renderings from Objaverse-XL, which consists solely of 20 Figure 19: Droplet3D demonstrates its lifting capability on 2D sketch paintings. Our model can transform objects such as characters and buildings into three-dimensional representations based on simple line drawings. From up to bottom: sketch of girl dressed in minimalist style, Japanese anime character, and traditional Chinese architectural structure. Figure 20: Droplet3D demonstrates its lifting capability on 2D images styled as comics paintings. Our model can elevate objects within given 2D comic into 3D, thereby achieving cross-dimensional effect. From top to bottom: gentle girl in comic style, male student with animal ears, an American comic-style superhero, and scarecrow in the style of Hayao Miyazaki. 21 object-level 3D files, Droplet3D still demonstrates certain degree of robustness, particularly when the input images differ significantly from the training data distribution, such as stylized AIGC images like comics or sketches. We believe this capability may stem from its prior video training, which has endowed it with extensive general knowledge, making its 3D generation more versatile. This observation is particularly interesting, as it validates, to some extent, our hypothesis that video can facilitate 3D generation. Figs. 19 and 20 demonstrate the lifting capacity for sketches and comic-style images, respectively. On the left of each figure are the stylized image inputs, while the right shows the surrounding perspective segments generated by our model based on it. For example, as shown in the first row of Fig. 20, our model can identify the body shape and posture of the comic girl and smoothly transform it into 3D form without altering its style. We believe that this capability to support stylized images contributes to the generation of 3D assets. For instance, as shown in Fig. 19, it holds significant potential to form well-developed 3D object based on just few simple strokes from designer or illustrator, whether it be character (1st and 2nd rows) or an architecture (3rd row). 5.3.3 Textured Mesh Synthesis We tested the potential of Droplet3D to create practical and usable 3D assets. Fig. 21 illustrates the results of generating textured meshes using Droplet3D. We employed Hunyuan3D-2[77] as the synthesis tool. Specifically, we utilized Hunyuan3D-2.0 DiT model for the geometric mesh synthetic and employed Hunyuan3D-2.1 painting model to generate textured meshes. We applied this algorithm to convert the multi-view images from Droplet3D into an industrially applicable representation. Compared to other mesh generation models that directly utilize single images, our pipeline enables the creation of more diverse range of appearances based on long or short text descriptions. 5.3.4 3D Gaussian Splatting Points Synthesis In addition, we also tested the potential to create 3D Gaussian splatting modality contents. Fig. 22 presents examples of 3D Gaussian splatting generated using Droplet3D. We used the native optimization-based approach[28] for this implementation. As can be observed, the re-rendered images maintain high quality, demonstrating the effectiveness of our proposed method. The generated surrounding viewpoints exhibit strong spatial consistency. Considering that we only employ the most basic reconstruction algorithm without any post-processing steps such as denoising, completion or camera pose estimation, we argue that plausible reason is that our model can produce dense viewpoints, with the surrounding motion poses being relatively uniform and accurate. 5.3.5 Scene-level 3D Content Generation Additionally, what excites us most is that, as illustrated in Fig. 23, our model can perform lifting on images with scene-level styles, which validates that our technical approach is fundamentally distinct from other currently popular native 3D generation methods, which are typically object-level. In contrast, Droplet3D has the potential to convert scene-level content into 3D. Fig. 23 sequentially demonstrates scene-level 3D generation for manor, an island with lightning, tranquil riverside at night, and scene deep within space station. Similar to cinematic freeze-frame effect, our model can automatically lift these scenes to 3D, thereby reducing labor costs. From this perspective, it can also be regarded as another form of 3D asset creation. However, what is even more intriguing is that the training set, Droplet3D-4M, contains no scene-level samples. Therefore, this capability can be considered entirely inherited from its ancestral source, the DropletVideo video generation model. Fig. 23 demonstrates the reconstruction of scene-level results using Gaussian splatting. It is interesting that we can further develop more applications based on these generated contents. For example, users can use these scene contents and combine it with the object-level point set. In terms of implementation, we can embed objects into the scene by controlling the scale, displacement, and rotation of both point sets. This intriguing application is intrinsically linked to Droplet3Ds potential for generating scene-level content. 22 Figure 21: Mesh reconstruction based on content generated by Droplet3D. Note that the models used for mesh generation and texture mapping originate from the open-source Hunyuan3D-2.0 and Hunyuan3D-2.1, respectively. 23 Figure 22: 3D Gaussian splatting reconstruction based on content generated by Droplet3D. We employed the native optimization-based approach for this implementation. Figure 23: Gaussian splatting edition based on scene-level content generated by Droplet3D. Compared to existing mainstream object-level 3D generation methods, Droplet3D not only generates individual objects but also demonstrates significant potential for scene-level content generation."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose technical approach for 3D generation based on video backbone models, aiming to leverage the vast repository of video media materials to enhance the general knowledge capabilities of 3D generation, thereby facilitating the creation of 3D assets. On one hand, we introduce large-scale 3D training dataset, termed Droplet3D-4M, comprising 4 million 3D models, accompanied by high-quality rendered surround-view videos and viewpoint-level textual descriptions. On the other hand, we present the technical framework of Droplet3D and achieve the training of 3D model weights using the DropletVideo, video generation backbone network with spatio-temporal consistency. Concurrently, we design text rewriting module and an image canonical viewpoint alignment module to refine the pipeline. Finally, we substantiate the value of our proposed approach through extensive visualization experiments. Furthermore, comprehensive quantitative experiments demonstrate its foundational capabilities and effectiveness. We hope that the ideas presented in this paper will contribute to the advancement of the 3D generation algorithm community, and the relevant dataset, code, and model weights associated with this work have been made open-source."
        },
        {
            "title": "References",
            "content": "[1] AIGC-Apps. Cogvideox-fun. https://github.com/aigc-apps/CogVideoX-Fun, 2024. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 25 [4] Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion models are effective 3d generators. arXiv preprint arXiv:2403.06738, 2024. [5] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. LuarXiv preprint ciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. [6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. [7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. [9] Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, and Jiajun Wu. Anymate: dataset and baselines for learning 3d object rigging. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. [10] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. [11] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv. org/abs/2403.03206, 2. [13] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [14] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:33133337, 2021. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d: Learning scalable 3d generative models from video diffusion models. In European Conference on Computer Vision, pages 333350. Springer, 2024. [17] Xiaoguang Han, Yushuang Wu, Luyue Shi, Haolin Liu, Hongjie Liao, Lingteng Qiu, Weihao Yuan, Xiaodong Gu, Zilong Dong, and Shuguang Cui. Mvimgnet2. 0: larger-scale dataset of multi-view images. arXiv preprint arXiv:2412.01430, 2024. [18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [19] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 26 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [21] Junchao Huang, Xinting Hu, Zhuotao Tian, Shaoshuai Shi, and Li Jiang. Edit360: 2d image edits to 3d assets from any angle. arXiv preprint arXiv:2506.10507, 2025. [22] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97849794, 2024. [23] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. [24] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [25] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d model with multi-view video diffusion. arXiv preprint arXiv:2407.11398, 2024. [26] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. [27] Donggoo Kang, Jangyeong Kim, Dasol Jeong, Junyoung Choi, Jeonga Wi, Hyunmin Lee, Joonho Gwon, and Joonki Paik. Consistent zero-shot 3d texture synthesis using geometry-aware diffusion and temporal video models. arXiv preprint arXiv:2506.20946, 2025. [28] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [29] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [30] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [31] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. [32] Xiaochuan Li, Baoyu Fan, Runze Zhang, Liang Jin, Di Wang, Zhenhua Guo, Yaqian Zhao, and Rengang Li. Image content generation with causal reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1364613654, 2024. [33] Zhiqi Li, Yiming Chen, Lingzhe Zhao, and Peidong Liu. Controllable text-to-3d generation via surface-aligned gaussian splatting. arXiv preprint arXiv:2403.09981, 2024. [34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 300309, 2023. [35] Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, and Cindy Le. Objaverse++: Curated 3d object dataset with quality annotations. arXiv preprint arXiv:2504.07334, 2025. [36] Jiantao Lin, Xin Yang, Meixi Chen, Yingjie Xu, Dongyu Yan, Leyi Wu, Xinli Xu, Lie Xu, Shunsi Zhang, and Ying-Cong Chen. Kiss3dgen: Repurposing image diffusion models for 3d asset generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 58705880, 2025. [37] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 27 [38] Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Zhang, Natalia Neverova, et al. Uncommon objects in 3d. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1410214113, 2025. [39] Yichong Lu, Yuzhuo Tian, Zijin Jiang, Yikun Zhao, Yuanbo Yang, Hao Ouyang, Haoji Hu, Huimin Yu, Yujun Shen, and Yiyi Liao. Orientation matters: Making 3d generative models orientation-aligned. arXiv preprint arXiv:2506.08640, 2025. [40] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36:7530775337, 2023. [41] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation. arXiv preprint arXiv:2402.08682, 2024. [42] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. [43] OpenAI. Gpt-4 technical report, 2023. [44] OpenAI. Openai-sora. https://openai.com/sora, 2024. [45] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [46] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [48] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. [49] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [51] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. [52] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [53] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [54] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. [55] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multiview diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [56] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 28 [57] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2281922829, 2023. [58] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. [59] Hongsheng Wang, Xinrui Zhou, and Feng Lin. Nova-3d: Non-overlapped views for 3d anime character reconstruction. In Proceedings of the 6th ACM International Conference on Multimedia in Asia Workshops, pages 17, 2024. [60] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [61] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. [62] Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, and Anpei Chen. Genfusion: Closing the loop between reconstruction and generation via videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60786088, 2025. [63] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803814, 2023. [64] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. [65] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [66] Xinli Xu, Wenhang Ge, Jiantao Lin, Jiawei Feng, Lie Xu, HanFeng Zhao, Shunsi Zhang, and Ying-Cong Chen. Flexgen: Flexible multi-view generation from text and image inputs. arXiv preprint arXiv:2410.10745, 2024. [67] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217, 2023. [68] Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, and Tao Mei. Hi3d: Pursuing high-resolution image-to-3d generation with video diffusion models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 68706879, 2024. [69] Jiahui Yang, Donglin Di, Baorui Ma, Jianxun Cui, Xun Yang, Yongjia Ma, Wenzhang Sun, Wei Chen, Zhou Xue, Meng Wang, et al. Tv-3dg: Mastering text-to-3d customized generation with visual prompt. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [70] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [71] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67966807, 2024. [72] Taoran Yi, Jiemin Fang, Zanwei Zhou, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Xinggang Wang, and Qi Tian. Gaussiandreamerpro: Text to manipulable 3d gaussians with highly enhanced quality. arXiv preprint arXiv:2406.18462, 2024. 29 [73] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. [74] Runze Zhang, Guoguang Du, Xiaochuan Li, Qi Jia, Liang Jin, Lu Liu, Jingjing Wang, Cong Xu, Zhenhua Guo, Yaqian Zhao, et al. Dropletvideo: dataset and approach to explore integral spatio-temporal consistent video generation. arXiv preprint arXiv:2503.06053, 2025. [75] Yibo Zhang, Li Zhang, Rui Ma, and Nan Cao. Texverse: universe of 3d objects with high-resolution textures. arXiv preprint arXiv:2508.10868, 2025. [76] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84758489, 2025. [77] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. [78] Qingnan Zhou and Alec Jacobson. Thingi10k: dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016. [79] Qi Zuo, Xiaodong Gu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Lingteng Qiu, Liefeng Bo, and Zilong Dong. High-fidelity 3d textured shapes generation by sparse encoding and adversarial decoding. In European Conference on Computer Vision, pages 5269. Springer, 2024. [80] Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Weihao Yuan, Rui Peng, Siyu Zhu, Liefeng Bo, Zilong Dong, Qixing Huang, et al. Videomv: Consistent multi-view generation based on large video generative model. 2024."
        }
    ],
    "affiliations": [
        "IEIT System Co., Ltd.",
        "Nankai University",
        "Tsinghua University"
    ]
}