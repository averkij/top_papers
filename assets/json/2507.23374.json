{
    "paper_title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
    "authors": [
        "Shuangkang Fang",
        "I-Chao Shen",
        "Takeo Igarashi",
        "Yufeng Wang",
        "ZeSheng Wang",
        "Yi Yang",
        "Wenrui Ding",
        "Shuchang Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation."
        },
        {
            "title": "Start",
            "content": "NeRF Is Valuable Assistant for 3D Gaussian Splatting Shuangkang Fang1, I-Chao Shen2, Takeo Igarashi2, Yufeng Wang1, ZeSheng Wang1, Yi Yang3, Wenrui Ding1, Shuchang Zhou3 1Beihang University 2The University of Tokyo 3StepFun 5 2 0 2 1 3 ] . [ 1 4 7 3 3 2 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce NeRF-GS, novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its perforIn NeRF-GS, we revisit the design of 3DGS and mance. progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation. 1. Introduction Neural Radiance Fields (NeRF) [41] and 3D Gaussian Splatting (3DGS) [27] have emerged as two prominent methodologies in 3D scene reconstruction, photorealistic rendering, and virtual reality applications [6, 13, 14, 16, 17, 19, 37, 41, 42, 51, 54, 58]. NeRF represents 3D scenes through continuous volumetric field, capturing intricate details by an MLP-based encoding of color and density at any position in space. However, it requires numerous forward passes through the MLP, limiting its applicability in real-time scenarios. In contrast, 3DGS [27] represents scene through set of discrete Gaussians to approximate points in space, which capitalizes on point-based rendering for computational efficiency, providing real-time performance. Nonetheless, the reliance on Gaussian initialization and limited spatial perception can lead to instability in Figure 1. NeRF-GS establishes bridge of communication between NeRF and 3DGS, leveraging information sharing, modeling of distinct characteristics, and joint optimization to enable 3DGS to achieve higher fidelity representation. In this case, NeRF-GS outperforms the vanilla 3DGS by 1.8dB in PSNR. 3DGS training [12, 18, 20, 46]. Moreover, the weak correlation between discrete Gaussians results in lack of smooth spatial transitions [7, 8, 40], which negatively affects the visual quality of the rendered outputs. To address these deficiencies, existing studies have sought to improve both NeRF and 3DGS. For example, some researches focus on accelerating rendering for NeRF [6, 19, 22, 42, 53, 64], while others improve 3DGS in terms of visual quality [9, 35, 40, 65]. However, most of these studies treat NeRF and 3DGS as independent scene representation paradigms, concentrating on their separate enhancements. Several researchers have attempted to leverage the properties of NeRF to enhance 3DGS, such as initializing 3DGS with NeRF [18, 46], embedding NeRF attributes within 3DGS [36, 40, 46], or creating networks to implicitly estimate 3DGS parameters [8, 35]. However, these approaches primarily focus on individually enhancing variants of 3DGS, without systematically exploring the potential components in the full NeRF pipeline that could benefit 3DGS. They also overlook the modeling of structural differences between the two, resulting in limited performance gains. Thus, the integration of NeRF and 3DGS methods and the combination of their respective strengths remain underexplored. To this end, we propose NeRF-GS, novel framework that integrates the NeRF network into the training of the 3DGS model, leveraging specific NeRF properties to address 3DGS inherent limitations. In revisiting the design space of the 3DGS model, we identify and implement three critical components in the hybrid NeRF-GS framework. (1) Sharing Mechanism (Sec 4.1): we first introduce Hash-based network for encoding features in continuous space optimized by NeRF volume rendering, and design strategies to identify potential Gaussian positions. Subsequently, both NeRF and 3DGS share these features to decode additional attributes for their respective spatial points. (2) Residual Vectors (Sec 4.2): due to inherent differences between the NeRF and 3DGS forms, directly using NeRF-optimized features and NeRF-initialized Gaussian positions does not adequately adapt to the 3DGS branch. To address this, we propose explicitly modeling their discrepancies by optimizing residual vectors for both features and positions to personalize and enhance 3DGS performance. (3) Joint Optimization (Sec 4.3): we align the attributes and rendering results of spatial points along rays passing through the important Gaussian in the NeRF branch with those in the 3DGS, which reduces feature confusion and ensures mutually beneficial constraints on shared features. Additionally, we leverage NeRFs continuous spatial query capability to assist in adaptive Gaussian growth, achieving efficient joint optimization across different branches. The hybrid design of NeRF-GS is not mere combination of NeRF and 3DGS, but rather systematic and comprehensive integration that considers the interrelations and differences between the two, maximizing the auxiliary role of NeRF in enhancing 3DGS. It is structurally flexible, allowing the 3DGS branch to be independently separated after joint optimization, thus preserving its real-time rendering capability. Experiments conducted on benchmark datasets demonstrate that NeRF-GS significantly outperforms the original 3DGS method in both quantitative and qualitative evaluation. Additionally, the mutual regularization between the dual branches in NeRF-GS notably improves the rendering quality of the 3DGS branch under sparse-view conditions. These findings indicate that these seemingly disparate scene representation methods are, in fact, complementary rather than competitive, providing new insights for exploring further hybrid 3D scene representation techniques. 2. Related work Implicit Volume Rendering. Implicit methods provide continuous spatial modeling capability to represent 3D scenes, eliminating the need for discretization [2, 4, 33, 38, 39, 41, 42, 45, 47, 49, 50, 56, 67]. Many methods have been developed on this basis to improve the visual quality and rendering speed. Plenoctrees [64] and Plenoxels [19] render faster than vanilla NeRF by pre-tabulating Tensor. DeRF [52] and KiloNeRF [53] accelerate speed by partitioning the target scene into smaller MLPs. Instant-NGP [42] introduces learnable, multi-resolution hash encoding to fit scenes efficiently. Mip-NeRF [4] enhances NeRF with cone tracing multi-scale properties and automatic antialiasing. Several methods have demonstrated that the features extracted by NeRF contain significant scene information. For example, Unisurf [48] achieves detailed mesh by sharing features between NeRF and SDF. DecomNeRF [30] enables semantic-level scene decomposition through feature embedding. PVD [14, 15] facilitates the conversion between different forms of NeRF by distilling features. Point-based Representations. Recent advances in pointbased 3D rendering have shown substantial improvements in rendering efficiency [21, 2427, 35, 36, 40, 63]. RAINGS [26], Agg [60], and NPGs [10] have proposed novel initialization strategies to address the limitations of the initialization from SfM in the original 3DGS. MS3DGS [62], Analytic-Splatting [32], and SA-GS [55] enhance 3DGS performance by introducing strategies to reduce aliasing. Additionally, to mitigate the issues of storage demands in 3DGS, some approaches have achieved lightweight Gaussian representations through parameter compression [3, 34, 43, 44, 66] and pruning [1, 11, 69]. Several studies have explored the complementarity and transfer of characteristics between different 3D representations. Notable examples [18, 46] utilize points extracted from NeRF for 3DGS initialization. VDGS [36] incorporates NeRF concepts by employing implicit MLPs to make 3DGS opacity view-dependent. SplatFields [40] samples implicit features from triplanes, establishing an autocorrelated feature space for estimating Gaussian sphere parameters. Scaffold-GS [35] derives the possible positions and attributes of Gaussian from set of candidate anchors. Hash-GS [8] and Compact-3DGS [31] leverages NeRF attributes for 3DGS parameter compression. However, these methods mainly adopt certain NeRF-inspired features to independently optimize 3DGS variants, which differ significantly from our methodology. Moreover, these methods typically implement direct transfer of NeRF characteristics to 3DGS without considering their inherent differences, thereby failing to fully exploit the models potential. 3. Preliminaries Neural Radiance Fields. NeRF represents scenes using an implicit function that maps spatial points = (x, y, z) and view directions = (θ, ϕ) to density σ and color c. For ray originating at with direction d, the RGB value Figure 2. Overview of NeRF-GS. (a) We first pretrain Hash-based NeRF network to acquire continuous spatial encoding capabilities and implicit scene representation. (b) Utilizing the preliminary scene carved by NeRF, we resample rays corresponding to image edges to obtain potential Gaussian positions, facilitating Gaussian initialization. (c) During joint optimization, the GS branch queries corresponding features from the Hash grid for each Gaussian sphere. These features, combined with positions and their respective residual terms (f , p), decode additional Gaussian attributes A, including color, opacity, scale, and rotation vectors. For the NeRF branch, rendering is exclusively performed on rays (GS-Rays) passing through important Gaussian spheres within the view frustum. The two branches are aligned by opacity and RGB values (Lop reg, Lpos reg ). Simultaneously, we leverage ray attributes from the NeRF branch along with gradient information from the GS branch to achieve adaptive control over Gaussian spheres. The purple dashed box marks the parameters to be trained. joint), further supervised by reconstruction(Lgs, Lnerf), and residual regularization (Lfea joint, Lrgb Cnerf of the corresponding pixel is computed by numerically integrating the colors ci and densities σi of the spatial points xi = + tid sampled along the ray: Cnerf = (cid:88) i=1 Ti(1 exp(σiδi))ci, (1) where Ti = exp( (cid:80)i1 j=1 σjδj) is the accumulated transmittance up to the i-th sample, and δ is the distance between adjacent samples. Gaussian Splatting. In 3DGS, the scene is represented by set of anisotropic 3D Gaussian functions, inheriting the EWA volume splatting method [70] and allowing efficient rendering through tile-based rasterization approach. Typically, 3DGS are initialized from set of points generated by SfM and can be described by the central position and covariance matrix Σ that is parameterized using rotation matrix and scaling matrix as follows: Σ = RSST RT . (2) 3DGS uses quaternion to represent rotation and vector for scaling. Each Gaussian is also associated with an opacity α [0, 1] and set of spherical harmonics (SH) coefficients to define the view-dependent color c. By projecting the 3D Gaussian into 2D, the color and opacity coverage of each projected Gaussian is evaluated and the pixel color Cgs can be calculated by sequentially blending all 2D Gaussians that contribute to the pixel, as follows: Cgs = (cid:88) iN i1 (cid:89) ciαi (1 αj). j=1 (3) 4. NeRF-GS Our objective is to integrate the full NeRF pipeline into 3DGS model training and utilize specific properties of NeRF to address the limitations of 3DGS. Fig. 2 illustrates an overview of our method. We start by independently training the NeRF branch to model spatially continuous Hash feature and initialize the Gaussian spheres in the 3DGS branch, enabling spatial awareness and feature sharing between the two branches (Sec 4.1). We design neural GS branch derived from these shared features. To fill the gap between NeRF and 3DGS representations, we further optimize each Gaussian with residual feature vector and position offset vector, using the refined vectors to infer additional Gaussian attributes (Sec 4.2). During joint optimization, we introduce GS-Rays, defined as rays connecting important Gaussian centers within the view frustum to the camera origin, serving as query rays for the NeRF branch. Along these GS-Rays, we achieve mutual constraint between NeRF and GS branches by minimizing differences in their spatial attributes and rendering results. Furthermore, we leverage NeRF to facilitate Gaussian adaptive growth in regions challenging for 3DGS perception (Sec 4.3). 4.1. Sharing Mechanism in Dual-branch NeRF for Prior Sharing. NeRF represents the scene as continuous volumetric field, allowing arbitrary queries of spatial points to obtain density σ and color c. This guarantees that every Gaussian has corresponding NeRF feature, and volume rendering creates strong spatial correlations that address 3DGSs limitations in discrete point representation and weak spatial relationships. To achieve efficient feature sharing, we construct hash feature extraction network that extracts multi-scale features from spatial point x. As in INGP [42], the density σ is derived from the spatial feature , with the color derived by combining and the direction vector d, as shown below. = H(x), σ = Fσ(f ), = Fc(f , d). (4) Once σ and are obtained, the images can be rendered by Eq. 1. After the NeRF branch has been pre-trained, the features can be shared with the 3DGS branch to capture similar information at corresponding spatial points. Edge-based Initalization. Similar to RadSplat [46], we estimate initial Gaussian positions by computing the median ray depth z. However, unlike RadSplat, which uniformly samples one million points from all rays for Gaussian initialization, we assign higher sampling weights to rays corresponding to high-frequency image textures, as these textures define scene contours. Specifically, we apply edge detection to extract image textures, designate their corresponding rays as edge rays, and then estimate the potential Gaussian position set Ginit using the following approach: Ginit = {pi pi {Pedge, Prandom}}, (5) where Pedge and Prandom are points in edge rays and random rays, respectively, and Gaussian position pi = + di zi. Our design tightly integrates the NeRF and GS branches, ensuring that the initialized points continue to share spatial information and undergo further joint optimization, which is completely different from RadSplat and NeRF-init [18] that treat initialization as an independent step. Neural GS Derivation from Shared Features. Unlike the vanilla 3DGS [27], which directly optimizes Gaussian properties, we embed shared features into the GS branch. Specifically, we use tight MLP Fgs transforms and into Gaussian attributes as follows: = Fgs(p, ), (6) where including color SH, opacity α, rotation and scale s. Note that the shared information and the newly introduced network are used only during training. The Gaussian attributes can be directly used for inference rendering without compromising the real-time advantage. 4.2. Residual Vectors in GS branch Residual Feature. The feature at the same spatial point yields distinct information in NeRF and GS branches: NeRF uses the feature to predict density and color, while GS requires an additional derivation of the geometric properties (rotation and scale). Therefore, NeRF-optimized features may lack adaptability for predicting Gaussian properties. To address this, we optimize residual feature vector for each Gaussian to capture information discrepancies in the shared features. This refined feature vector maintains consistency with the NeRF branch features while enabling individual Gaussians to fine-tune specific information, thus enhancing the rendering quality of the GS branch. Residual Position. Due to potential NeRF fitting errors and differing spatial perception caused by the GS branch, the Gaussian position derived from NeRF branch initialization may not fully suit the GS branch. Therefore, in addition to the residual feature, we optimize residual position for each Gaussian to capture subtle spatial adjustments. After introducing the discrepancy modeling, Gaussian attributes can be derived based on the adjusted position and feature vectors. = Fgs(p + p, + ), (7) where and are modeled as trainable parameters as shown in Fig. 2. 4.3. Joint Optimization in Dual-branch GS-Rays. NeRF requires dense sampling and network queries, which preclude rendering an entire image in single pass like in 3DGS. To synchronize optimization, we propose rendering NeRF using only partial rays in each iteration. We select rays that connect Gaussian positions with the high opacity in the current view frustum space to the camera origin, which we refer to as GS-Rays: Rgs. For the k-th training view, its GS-Rays are determined by the corresponding camera origin ok and the ray directions dk . Rk gs = {ok, dk }, dk = pk ok, (8) where pk is visible Gaussian positions with the high opacity in the k-th view. This design ensures that the sampling points in the NeRF branch are distributed as closely as possible to the Gaussian spheres, thereby aligning the scene perception and enhancing the effectiveness of the shared information across different branches, as well as providing the necessary data for subsequent joint optimization. Growing and Pruning Operation. In the original 3DGS, Gaussian growth occurs via gradient evaluation, which restricts gradient awareness to regions containing Gaussian spheres, potentially overlooking important blank areas. Inspired by Point-NeRF [61], we leverage NeRF spatial continuity to address this limitation. Specifically, we evaluate the opacity at sampling points in the NeRF branch as follows. αnerf = 1 exp(σiδi). (9) New Gaussian spheres are then added at points with high opacity and far from existing Gaussian spheres. We regulate NeRF-driven growing to achieve an optimal balance between the number of Gaussian spheres and scene representation accuracy, which mitigates the 3DGS limitation of localized growth perception. For pruning, we adopt the original 3DGS strategy, relying solely on GS branch information without NeRF assistance. This is because the pruning regions already include Gaussian-perceptible areas. Loss Design. During joint training, we design loss functions for single-branch optimization and dual-branch collaboration. For the NeRF branch, we use an L1 norm loss Lrgb nerf [28] for density, which promotes more concentrated distribution pattern by discouraging density dispersion. nerf for rendered RGB values and an entropy loss Len Lnerf = Lrgb nerf + λenLen nerf. (10) For the GS branch, we use an L1 norm loss Lrgb loss LSSIM gs larization Lvol gs and SSIM for rendered images, along with volume regugs [35] to minimize Gaussian sphere overlap. Lgs = Lrgb gs + λSSIMLSSIM gs + λvolLvol gs . (11) For dual-branch collaborative loss, we use L1 norm Lrgb joint to constrain the rendered pixel values along GS-Rays in the NeRF branch with corresponding GS branch rendered pixels. Additionally, we align the Gaussian opacity (α in Eq. 3) with corresponding NeRF opacities (αnerf in Eq. 9) by L1 norm loss Lop joint. Residual features and position residuals are further constrained by L2 norm regularization, denoted as reg and Lpos Lfea reg , to encourage NeRF and GS branches to learn common spatial properties while providing mutual regularization against overfitting. The overall loss function during joint optimization is as follows: Ltotal =Lgs + λnerfLnerf + λrgbLrgb λopLop joint + λfeaLfea joint+ reg + λposLpos reg . (12) 5. Experiments 5.1. Implementation Details Training and Optimization Details. Following the original 3DGS method configurations, we implement NeRF-GS in PyTorch. For the Hash network, we set 16 different level grids, each outputting 2-dimensional features, yielding 32-dimensional feature vector for spatial point. During the NeRF branch pre-training, each batch contains 8,192 rays and is trained for 10 epochs. For real-world datasets, we initialize using 1,000,000 points sampled at an 8:2 ratio from edge rays and random rays, while Blender datasets are initialized with 100,000 points. To enhance efficiency during joint training, the NeRF branch renders 4,096 rays sampled from GS-Rays. We set λen, λSSIM, and λvol to 1e-4, 0.2, and 1e-3, respectively, while λnerf, λrgb, λop, λfea, and λpos are set to 0.1, 0.05, 1e-3, 1e-4, and 1e-4 respectively. NeRFassisted Gaussian growth occurs every 100 iterations, with maximum of 200 new additions. Joint training iterates 30k for full-view datasets and 8k for sparse-view scenes. All experiments are conducted on an NVIDIA A100 GPU. Datasets and Metrics. We report experimental results including Mip-NeRF360 (all 9 on real-world datasets, scenes) [5], Tanks&Temples [29] DeepBlending [23], and the Blender dataset [41]. Evaluation metrics include PSNR, SSIM [59], and LPIPS [68]. Additionally, we compare metrics for training time (minutes), storage size (MB), and rendering speed (FPS) to assess the models compactness and efficiency. Baselines. Our method is focused on enhancing GS branch performance, so we primarily compare it with 3DGS [27] and its variants, including C3DGS [44], Scaffold-GS [35], Mip3DGS [65], and 2DGS [24]. We also compare methods incorporating NeRF properties such as Hash-GS [8], and VDGS [36], as well as SplatFields [40], which is specifically designed for sparse-view scenes. 5.2. Comparison We conduct extensive quantitative and qualitative comparisons with state-of-the-art methods on both full and sparse datasets. As our primary focus is on the impact of NeRF integration on GS performance, all results, unless otherwise noted, use the GS branch as the final output of NeRF-GS. Full View Scene. We optimize NeRF-GS using the default full training data on multiple benchmark datasets. Comparative results are shown in Table 1, where our approach significantly outperforms the vanilla 3DGS model and other state-of-the-art methods across PSNR, SSIM, and LPIPS metrics. Qualitative experiments in Fig. 3 demonstrate our methods superior capability in capturing high-frequency textures and fine geometric details while better reflecting lighting conditions. Notably, compared to other methods that incorporate NeRF-like concepts, such as VDGS and Hash-GS, NeRF-GS achieves even more substantial improvements. This indicates that our dual-branch joint optimization framework is more effective than simple NeRF initialization or directly adapting NeRF implicit concepts, validating NeRF-GS as robust framework for integrating diverse 3D representation approaches. Sparse View Scene. Through shared spatial positions and corresponding encoded features, different branches within NeRF-GS can more comprehensively perceive and learn Table 1. Quantitative comparison on real-world datasets. Colors denote the 1st , 2nd , and 3rd best-performing model."
        },
        {
            "title": "Method",
            "content": "INGP [42] 3DGS [27] C3DGS [44] Scaffold-GS [35] Hash-GS [8] VDGS [36] Ours"
        },
        {
            "title": "DeepBlending",
            "content": "Mip-NeRF360 Tanks&Temples PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 23.62 29.42 29.79 30.21 29.98 29.54 30.70 0.797 0.899 0.901 0.906 0.902 0.906 0.912 0.423 0.247 0.258 0.254 0.269 0.243 0.237 26.43 27.49 27.08 27.5 27.53 27.64 28. 0.725 0.813 0.798 0.806 0.807 0.813 0.817 0.339 0.222 0.247 0.252 0.238 0.220 0.210 21.72 23.69 23.32 23.96 24.04 24.02 24.44 0.723 0.844 0.831 0.853 0.846 0.851 0.860 0.330 0.178 0.201 0.177 0.187 0.176 0.161 Figure 3. Qualitative comparison on real-world datasets. The numbers indicate the PSNR. Our method demonstrates significant advantage over 3DGS and its variants, achieving more faithful representation of scene details. from limited 3D scene information. Additionally, the collaborative optimization between NeRF and GS branches, facilitated by this shared information, creates mutual constraints and regularization effects, mitigating overfitting, which is crucial for modeling scenes under sparse views. To validate this, we perform sparse-view comparisons with baseline methods, as shown in Table 2. Across various sparsity levels, NeRF-GS consistently surpasses corresponding baselines. Remarkably, NeRF-GS achieves performance comparable to or even surpassing the SplatField method, which is specifically designed for sparse-view settings. Fig. 4 also provides qualitative illustration of these improvements. Furthermore, it can be observed that the performance gap between NeRF-GS and baseline methods in sparse views is more pronounced than in full views, affirming the effectiveness of our methods regularization. We Figure 4. Qualitative comparison under 12 input views on the Blender dataset. The numbers indicate the PSNR. Table 2. Quantitative comparison of using different numbers of input views on Blender dataset. Our NeRF-GS maintains high performance when the scene input views are reduced. Method SparseNeRF [57] INGP [42] 3DGS [27] Mip3DGS [65] Scaffold-GS [35] 2DGS [24] Hash-GS [8] VDGS [36] SplatFields [40] Ours Full views 12 views 8 views PSNR SSIM PSNR SSIM PSNR SSIM 32. 33.18 33.32 33.36 33.41 33.07 33. 33.37 33.25 33.71 0.957 0.960 0. 0.969 0.966 0.964 0.967 0.969 0. 0.970 22.92 22.68 25.29 24.86 23. 25.62 25.36 24.77 25.80 26.34 0. 0.875 0.900 0.898 0.874 0.911 0. 0.898 0.911 0.912 22.20 21.87 22. 22.37 21.53 23.04 23.14 22.88 23. 23.92 0.861 0.860 0.866 0.862 0. 0.877 0.879 0.872 0.889 0.881 Figure 5. Impact of feature share and joint optimization on sparse view scenes. These two key designs enable mutual regularization constraints between NeRF and GS branches, significantly improving the visual quality of NeRF-GS in sparse views. Table 3. Comparison of model efficiency with 3DGS. We report the FPS, model size (MB), training time (minutes) and PSNR. The 3DGSL denotes longer iterative training (50k) for 3DGS. DeepBlending Mip-NeRF360 Method 3DGS 3DGSL Ours FPS Size Time PSNR FPS Size Time PSNR 105 104 122 672 678 526 36. 55.7 51.7 29.42 29.50 30.70 101 102 729 733 564 41.5 67.9 60.3 27. 27.57 28.32 further conduct relevant experiments in the next subsection. Figure 6. Visualization of position residuals. The points represent the initial Gaussian positions, with the top 20% of points having the largest optimized residuals highlighted in red. We compare this with the results obtained by fixed Gaussian positions during training, demonstrating the importance of the residual vectors for personalized adaptation in the GS branch. 5.3. Qualitative Analysis of NeRF-GS Regularization Effect. By introducing the spatial continuity of NeRF, NeRF-GS establishes self-correlation across different Gaussian spheres in 3DGS. Additionally, feature sharing, cross-branch loss constraints, and joint optimization enable mutual regularization between NeRF and 3DGS. In Fig. 5, we illustrate the critical role of NeRF-GS in preventing overfitting to the scene. When associations between two branches are directly removed, such as feature sharing, loss constraints during joint training, etc., the NeRF-GS shows large visual quality degradation. Visualization of Discrepancy. Errors introduced during NeRF pre-training and inherent disparities between NeRF and 3DGS can impede the GS branchs ability to effectively model 3D scene from NeRF-shared information. NeRFGS addresses this challenge through residual mechanism. An example is shown in Fig. 6, incorporating positional residuals allows the GS branch to adjust Gaussian positions, avoiding artifacts that could arise from only adjusting Gaussian shapes under fixed positions. Model Efficiency. While NeRF-GS bridges two distinct 3D representation models, it maintains their independence. Post-joint training, each branch can retain only its effecTable 4. Ablation of different components in NeRF-GS on Tank&Temples and DeepBlending datasets. Numbers represent the PSNR metric. See Section 5.4 for discussion. Tanks&Temples"
        },
        {
            "title": "Ablation of sharing mechanisms",
            "content": "w/o Edge-based Init w/o Feature Share 24.06 21.17 22.61 25.74 22.12 23.93 28.65 29.54 29.8 30.91 29.8 30."
        },
        {
            "title": "Ablation of residual vectors",
            "content": "w/o Residual Feature w/o Residual Position 25.88 22.31 24.09 25.97 22.35 24.16 29.76 29.89 30.84 31.01 30.3 30.45 w/o Lfea joint w/o Lpos joint w/o Lop joint w/o Lrgb joint"
        },
        {
            "title": "Ablation of joint optimization",
            "content": "26.16 22.51 24.33 30.05 30.88 30.46 26.09 22.46 24.27 30. 31.03 30.56 26.3 22.4 24.35 30.02 31. 30.60 26.14 22.48 24.31 w/o GS-Rays 25.82 22.26 24."
        },
        {
            "title": "Full",
            "content": "26.27 22.61 24.44 30.21 29.85 30.17 30.97 30. 30.6 30.22 31.23 30.7 tive components, preserving the original single-branch inference speed. As shown in Table 3, our method maintains GS real-time rendering capabilities while requiring less storage than the original 3DGS approach. This is because our initialization and Gaussian growing strategies reduce Gaussian spheres. For example, on the DeepBlending dataset, vanilla 3DGS uses 2,461,023 Gaussians, while ours uses only 1,926,336. We also compare it with an extendedtraining version of 3DGSL, showing NeRF-GS outperforms 3DGS even with similar training time. This suggests that integrating the NeRF branch is worthwhile trade-off despite the increase in training time. 5.4. Ablation Studies Impact of Sharing Mechanisms. In NeRF-GS, information exchange manifests through spatial co-utilization and feature sharing. We propose scene-edge-based initialization scheme as in Eq. 5 and compare it with the alternative initialization from SFM, denoted as w/o Edge-based Init. Moreover, to examine the effect of feature sharing, we directly train the GS branch with learnable feature parameters, remarked as w/o Feature Share. The ablation results in Table 4 indicate that our proposed initialization significantly outperforms the alternatives. Likewise, the feature-sharing scheme across NeRF and GS exhibits irreplaceable positive impacts on the full scene. Impact of Residual Vectors. As discussed in Sec 4.2, the GS branch needs to derive different geometric information from NeRFs shared features and initial points, suggesting the need for differentiated information encoding. To achieve this, we introduce residual strategies for both features and Gaussian positions. Removing these terms results in significant performance degradation, as shown in Table 4. This indicates that, in addition to information sharing, enabling each branch to learn adapted and differentiated information is also critical. In contrast, previous methods such as Scaffold-GS, Hash-GS and VDGS that merely incorporate NeRF characteristics overlooked this distinction, thereby offering limited performance improvement. Impact of Joint Optimization Strategy. Our joint optimization process incorporates several key components to ensure efficient and effective training between the NeRF and GS branches. The GS-Rays strategy directs NeRF to focus on areas that are essential for the GS branch during rendering, effectively enabling efficient information exchange and mutual enhancement between branches. The term w/o GS-Rays in Table 4 shows performance decline when replacing GS-Rays with an equal number of random rays. We also evaluate the effectiveness of newly introduced loss terms, as indicated in Table 4. Removing mutual constraints between branch outputs leads to performance degradation. Furthermore, applying regularization constraints to shared information to prevent excessive branch discrepancy enhances model performance. 6. Limitations Although NeRF-GS fundamentally turns NeRF and 3DGS from competitors into collaborators and achieves superior performance, it also increases method complexity and computational overhead. Certain components within the two full pipelines may be redundant when combined. Developing more compact and streamlined integration strategy could enhance our frameworks applicability and improve its interpretability in future research. 7. Conclusion In this study, we introduce NeRF-GS, novel framework that combines implicit neural radiance fields with Gaussian splatting. Its core innovation lies in dual-branch collaborative design, comprising three key components: shared information in positional spaces and features, residual vectors to model inherent inter-branch differences, and joint optimization via GS-Rays alignment of intermediate results and rendering outputs, as well as adaptive Gaussian controls. These strategies effectively address several limitations of 3DGS, including initialization dependency, limited spatial awareness, insufficient Gaussian sphere correlation, and overfitting in sparse-view scenes. Experimental results demonstrate that NeRF-GS achieves state-of-the-art performance, offering new insights into the fusion of NeRF and 3DGS as an efficient hybrid approach for 3D scene representation."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by the National Natural Science Foundation of China (U24B6013) and China Scholarship Council (202406020139)."
        },
        {
            "title": "References",
            "content": "[1] Muhammad Salman Ali, Maryam Qamar, Sung-Ho Bae, and Enzo Tartaglione. Trimming the fat: Efficient compression of 3d gaussian splats through pruning. arXiv preprint arXiv:2406.18214, 2024. 2 [2] Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, and Yaron Lipman. Controlling neural level sets. Advances in Neural Information Processing Systems(NIPS), 2019. 2 [3] Milena Bagdasarian, Paul Knoll, Florian Barthel, Anna Hilsmann, Peter Eisert, and Wieland Morgenstern. 3dgs. zip: survey on 3d gaussian splatting compression methods. arXiv preprint arXiv:2407.09510, 2024. 2 [4] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58555864, 2021. 2 [5] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of anti-aliased neural radiance fields. the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 5 [6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European conference on computer vision, pages 333350. Springer, 2022. [7] Guikun Chen and Wenguan Wang. survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890, 2024. 1 [8] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Hac: Hash-grid assisted context for 3d gaussian splatting compression. In European Conference on Computer Vision, pages 422438. Springer, 2025. 1, 2, 5, 6, 7 [9] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2025. 1 [10] Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, and Jan Eric Lenssen. Neural parametric gaussians for monocular non-rigid object reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1071510725, 2024. 2 [11] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprint arXiv:2311.17245, 2023. [12] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2, 2024. 1 [13] Shuangkang Fang, Yufeng Wang, Yi Yang, Yi-Hsuan Tsai, Wenrui Ding, Shuchang Zhou, and Ming-Hsuan Yang. Editing 3d scenes via text prompts without retraining. arXiv eprints, pages arXiv2309, 2023. 1 [14] Shuangkang Fang, Yufeng Wang, Yi Yang, Weixin Xu, Heng Wang, Wenrui Ding, and Shuchang Zhou. Pvd-al: Progressive volume distillation with active learning for efficient conversion between different nerf architectures. arXiv preprint arXiv:2304.04012, 2023. 1, 2 [15] Shuangkang Fang, Weixin Xu, Heng Wang, Yi Yang, Yufeng Wang, and Shuchang Zhou. One is all: Bridging the gap between neural radiance fields architectures with progressive volume distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 597605, 2023. 2 [16] Shuangkang Fang, Dacheng Qi, Weixin Xu, Yufeng Wang, Zehao Zhang, Xiaorong Zhang, Huayu Zhang, Zeqi Shao, and Wenrui Ding. Efficient implicit sdf and color reconstruction via shared feature field. In Proceedings of the Asian Conference on Computer Vision, pages 34993516, 2024. 1 [17] Shuangkang Fang, Yufeng Wang, Yi-Hsuan Tsai, Yi Yang, Wenrui Ding, Shuchang Zhou, and Ming-Hsuan Yang. Chatedit-3d: Interactive 3d scene editing via text prompts. arXiv preprint arXiv:2407.06842, 2024. [18] Yalda Foroutan, Daniel Rebain, Kwang Moo Yi, and Andrea Tagliasacchi. Does gaussian splatting need sfm initialization? arXiv preprint arXiv:2404.12547, 2024. 1, 2, 4 [19] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55015510, 2022. 1, 2 [20] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20796 20805, 2024. 1 [21] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXiv preprint arXiv:2311.16043, 2023. 2 [22] Stephan Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neuIn Proceedings of the IEEE/CVF ral rendering at 200fps. international conference on computer vision, pages 14346 14355, 2021. 1 [23] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):115, 2018. 5 [24] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. arXiv preprint arXiv:2403.17888, 2024. 2, 5, [25] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, and Yuexin Ma. Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53225332, 2024. [26] Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, and Seungryong Kim. Relaxing accurate initialization constraint for 3d gaussian splatting. arXiv preprint arXiv:2403.09413, 2024. 2 [27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4):114, 2023. 1, 2, 4, 5, 6, 7 [28] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot neural volume renIn Proceedings of the IEEE/CVF Conference on dering. Computer Vision and Pattern Recognition, pages 12912 12921, 2022. 5 [29] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36 (4):113, 2017. [30] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. Advances in Neural Information Processing Systems, 35:2331123330, 2022. 2 [31] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21719 21728, 2024. 2 [32] Zhihao Liang, Qi Zhang, Wenbo Hu, Ying Feng, Lei Zhu, and Kui Jia. Analytic-splatting: Anti-aliased 3d gaussian splatting via analytic integration. arXiv preprint arXiv:2403.11056, 2024. 2 [33] David Lindell, Julien NP Martel, and Gordon Wetzstein. Autoint: Automatic integration for fast neural volume renIn Proceedings of the IEEE/CVF Conference on dering. Computer Vision and Pattern Recognition, pages 14556 14565, 2021. 2 [34] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. Compgs: Efficient 3d scene representation via compressed gaussian splatting. arXiv preprint arXiv:2404.09458, 2024. 2 [35] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 1, 2, 5, 6, 7, 3 [36] Dawid Malarz, Weronika Smolak, Jacek Tabor, Sławomir Gaussian splatting arXiv preprint Tadeja, and Przemysław Spurek. with nerf-based color and opacity. arXiv:2312.13729, 2023. 1, 2, 5, 6, 7 [37] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In CVPR, 2021. 1 [38] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), 2019. 2 [39] Mateusz Michalkiewicz, Jhony Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson. Implicit surface representations as layers in neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV), 2019. 2 [40] Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, and Edmond Boyer. Splatfields: Neural gaussian splats for sparse 3d and 4d reconstruction. In European Conference on Computer Vision, pages 313332. Springer, 2025. 1, 2, 5, 7 [41] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision(ECCV), 2020. 1, 2, [42] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1 102:15, 2022. 1, 2, 4, 6, 7, 3 [43] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv preprint arXiv:2311.18159, 2023. 2 [44] Simon Niedermayr, Josef Stumpfegger, and Rudiger Westermann. Compressed 3d gaussian splatting for accelerated novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1034910358, 2024. 2, 5, 6 [45] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV), 2019. 2 [46] Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, and Federico Tombari. Radsplat: Radiance field-informed gaussian splatting for robust real-time rendering with 900+ fps. arXiv preprint arXiv:2403.13806, 2024. 1, 2, 4 [47] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning texIn Proceedings of ture representations in function space. the IEEE/CVF International Conference on Computer Vision(ICCV), 2019. 2 [48] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance In Proceedings of fields for multi-view reconstruction. the IEEE/CVF International Conference on Computer Vision(ICCV), 2021. [49] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), 2019. 2 Vision and Pattern Recognition, pages 2092320931, 2024. 2 [63] Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, and Qi Tian. Gaussianobject: Just taking four images to get high-quality 3d object with gaussian splatting. arXiv preprint arXiv:2402.10259, 2024. 2 [64] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5752 5761, 2021. 1, 2 [65] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1944719456, 2024. 1, 5, [66] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An endto-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. 2 [67] Jian Zhang, Yuanqing Zhang, Huan Fu, Xiaowei Zhou, Bowen Cai, Jinchi Huang, Rongfei Jia, Binqiang Zhao, and Xing Tang. Ray priors through reprojection: Improving neural radiance fields for novel view extrapolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1837618386, 2022. 2 [68] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [69] Zhaoliang Zhang, Tianchen Song, Yongjae Lee, Li Yang, Cheng Peng, Rama Chellappa, and Deliang Fan. Lp-3dgs: arXiv preprint Learning to prune 3d gaussian splatting. arXiv:2405.18784, 2024. 2 [70] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VIS01., pages 29538. IEEE, 2001. 3 [50] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy In Proceedings of the European Conference on networks. Computer Vision(ECCV), 2020. [51] Quentin Picard, Stephane Chevobbe, Mehdi Darouich, and Jean-Yves Didier. survey on real-time 3d scene reconstruction with slam methods in embedded systems. arXiv preprint arXiv:2309.05349, 2023. 1 [52] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. Derf: Decomposed radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1415314161, 2021. 2 [53] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with In Proceedings of the IEEE/CVF thousands of tiny mlps. international conference on computer vision, pages 14335 14345, 2021. 1, 2 [54] Taha Samavati and Mohsen Soryani. Deep learning-based 3d reconstruction: survey. Artificial Intelligence Review, 56(9):91759219, 2023. 1 [55] Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, and Hao Zhao. Sags: Scale-adaptive gaussian splatting for training-free antialiasing. arXiv preprint arXiv:2403.19615, 2024. 2 [56] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5459 5469, 2022. [57] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 90659076, 2023. 7 [58] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. 1 [59] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 5 [60] Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, and Arash Vahdat. Agg: Amortized generative 3d gaussians for single image to 3d. arXiv preprint arXiv:2401.04099, 2024. 2 [61] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Pointnerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54385448, 2022. 4 [62] Zhiwen Yan, Weng Fei Low, Yu Chen, and Gim Hee Lee. Multi-scale 3d gaussian splatting for anti-aliased rendering. In Proceedings of the IEEE/CVF Conference on Computer NeRF Is Valuable Assistant for 3D Gaussian Splatting"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 7. Impact of NeRF-assisted Gaussian growth. We initialize 3DGS using point clouds with missing regions to evaluate its scene perception range and sensitivity to initialization. Without NeRF-assisted Gaussian growth, 3DGS exhibits insufficient reconstruction (a) or incomplete reconstruction (b) in the missing areas. However, when employing the proposed NeRF-assisted Gaussian growth strategy in our method, these missing regions are successfully reconstructed. This demonstrates that NeRF significantly enhances the perception range of 3DGS, reducing its sensitivity to initialization and improving visual quality. 8. Analysis of Gaussian Adaptive Control from"
        },
        {
            "title": "NeRF Branch",
            "content": "The continuous spatial representation of NeRF enables queries at any spatial location, allowing it to perceive the entire 3D scene. In contrast, individual Gaussian sphere in 3DGS has limited perceptual range, making 3DGS sensitive to initialization and less effective in adaptive control. As shown in Figure 7, we deliberately design Gaussian initialization with missing regions in certain spatial areas. Figure 8. Comparison of initialization with RadSplat. NeRF-GS focuses more on the contours of the scene during ray sampling, alleviating the burden of position optimization in the GS branch while achieving superior visual results in regions with complex textures. Figure 9. Impact of joint optimization on the NeRF branch. The dashed line indicates the mean PSNR. Given equivalent training iterations, the NeRF obtained through NeRF-GS outperforms training this NeRF independently. This demonstrates that dual-branch training not only benefits the GS branch but also enhances the performance of the NeRF branch. After iterative optimization, it can be observed that GS allocates limited number of Gaussians to these regions without assistance from the NeRF branch, and in extreme cases, it fails to perceive the missing areas entirely, resulting in poor or incomplete scene reconstruction. Conversely, our NeRF-assisted adaptive control strategy successfully senses these regions, significantly enhancing the global perceptual capability of the GS branch and reducing its sensitivity to initialization. 9. Analysis of Edge-based Initialization NeRF-GS utilizes pre-trained NeRF to obtain candidate Gaussian positions. To enhance initialization efficiency, we incorporate an edge detection step that pre-identifies critical rays and increases their sampling probability during initialization. This design is predicated on the observation that Gaussian spatial distribution should ideally align with the contours of the actual 3D scene, with more Gaussians in textured areas and fewer in blank areas. In the baseline RadSplat, rays are sampled uniformly at random without discrimination, which we consider inefficient. To illustrate this, we conduct visualization experiment in Figure 8, showing that our approach yields Gaussian distribution that clusters around areas rich in texture, with fewer Gaussians in low-texture or empty regions. The rendering results demonstrate that our edge-based initialization method effectively captures complex scene textures, outperforming uniform sampling in accurately representing the scene. formance drop when Lnerf is removed, demonstrating that jointly optimizing the NeRF branch benefits the GS branch. Similarly, direct optimization of GS after initialization leads to performance degradation, validating the effectiveness of our proposed joint optimization strategy. Moreover, we observe that Drandom 3dgs, further confirming the superiority of our initialization strategy. underperforms compared to Dedge 3dgs 12. Per-scene Breakdown Results of NeRF-GS We provide detailed quantitative assessment of NeRF-GS across various scenes in Tables 7, 8 and 9, including metrics such as PSNR, SSIM, and LPIPS. Table 7. Per-scene results of Blender dataset of our method. Full views chair drums ficus hotdog lego materials mic ship Avg PSNR 35.36 SSIM 0.985 0.012 LPIPS 26.34 0.948 0.047 35.15 0.9852 0.013 37.81 0.984 0.019 36.45 0.983 0.014 30.873 0.962 0.036 36.78 0.988 0. 30.9 0.887 0.111 33.71 0.970 0.032 12 views chair drums ficus hotdog lego materials mic ship Avg PSNR 28.32 SSIM 0.950 0.040 LPIPS 22.67 0.8991 0.082 26.48 0.9371 0.035 29.58 0.942 0.063 24.26 0.888 0.106 29.02 0.966 0.027 24.21 0.799 0. 26.34 0.912 0.080 26.18 0.912 0.081 8 views chair drums ficus hotdog lego materials mic ship Avg PSNR 25.95 SSIM 0.917 0.061 LPIPS 20.58 0.871 0.114 23.12 0.892 0.101 27.27 0.937 0.099 25.01 0.885 0.101 20.83 0.834 0.184 25.72 0.941 0. 22.93 0.773 0.225 23.92 0.881 0.124 Table 8. Per-scene results of Tanks&Temples and DeepBlending datasets of our method. Tanks&Temples DeepBlending Truck Train Avg Dr Johnson Playroom Avg PSNR 26.27 SSIM 0.887 0.127 LPIPS 22.61 0.833 0. 24.44 0.860 0.161 30.17 0.91 0.235 31.23 0.914 0.238 30.70 0.912 0.237 Table 9. Per-scene results of Mip-NeRF360 dataset of our method. bicycle bonsai counter graden kitchen room stump flowers treehill Avg PSNR 25.52 SSIM 0.695 0.327 LPIPS 33.97 0.957 0.145 30.5 0.93 0.144 27.84 0.868 0. 32.56 0.939 0.102 32.78 0.941 0.155 27.08 0.785 0.206 21.71 0.613 0.314 22.99 0.626 0.395 28.32 0.817 0. Table 5. Additional comparisons. We evaluate the performance of the NeRF branch in NeRF-GS and compare it to InstantNGP [42], which also utilizes hash-based structure. DeepBlending Tanks&Temples Mip-NeRF360 Method PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Instant-NGP BranchNeRF BranchGS 23.62 22.43 30.70 0.797 0.784 0.910 0.423 0.441 0.245 21.72 21.11 24.44 0.723 0.718 0.860 0.330 0.338 0. 26.43 25.12 28.32 0.725 0.722 0.824 0.339 0.343 0.217 and Dedge Table 6. Additional ablation studies. Numbers are PSNR metric. Drandom 3dgs denote direct optimizing 3DGS after initializa3dgs tion using the random initialization and the proposed edge-based initialization, respectively. Tanks&Temples"
        },
        {
            "title": "Playroom Avg",
            "content": "w/o Lvol gs w/o Lnerf Drandom 3dgs Dedge 3dgs 26.10 25.44 22.48 21.15 24.29 23.30 25.46 21. 23.65 25.87 22.11 23."
        },
        {
            "title": "Full",
            "content": "26.27 22.61 24.44 30.02 28.79 29.07 29. 30.17 31.07 29.46 29.92 30.38 31.23 30.55 29. 29.50 29.89 30.70 10. Analysis of NeRF Branch Performance Mutual Promotion between NeRF and GS Branches. While the primary aim of this work is to leverage NeRF characteristics to address 3DGS limitations, we have found that the GS branch also positively impacts the NeRF branch during joint training. As depicted in Figure 9, the NeRF branch trained jointly with the GS branch outperforms an independently optimized NeRF under the same number of iterations. This improvement arises from feature sharing and joint loss constraints between NeRF and GS branches, which enhance NeRF optimization as well. The simultaneous performance gains of both branches further confirm the complementary relationship between NeRF and 3DGS, offering insights for exploring integration with other forms of 3D representation. Compare with Structurally Similar NeRF Method. We further compare the NeRF branch to the GS branch and the Instant-NGP [42] based on the same hash structure. It should be noted that this article focuses more on the improvement of the GS branch performance by NeRF, where we observe significant performance improvement in the GS branch. 11. Additional Ablation Studies loss We further conduct ablation studies on additional terms, including the introduced volume regularization [35] and the overall loss term of the NeRF branch, Lnerf. Additionally, we evaluate the performance of directly optimizing 3DGS after initialization using the random initialization (Drandom 3dgs). 3dgs The results, presented in Table 6, indicate significant per- ) and the proposed edge-based initialization (Dedge"
        }
    ],
    "affiliations": [
        "Beihang University",
        "StepFun",
        "The University of Tokyo"
    ]
}