{
    "paper_title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
    "authors": [
        "Ruihang Chu",
        "Yefei He",
        "Zhekai Chen",
        "Shiwei Zhang",
        "Xiaogang Xu",
        "Bin Xia",
        "Dingdong Wang",
        "Hongwei Yi",
        "Xihui Liu",
        "Hengshuang Zhao",
        "Yu Liu",
        "Yingya Zhang",
        "Yujiu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 5 6 7 8 0 . 2 1 5 2 : r Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance Ruihang Chu1,2 Yefei He1 Dingdong Wang4 Bin Xia4 Yu Liu1 Tongyi Lab, Alibaba Group 1 Zhekai Chen3 Hongwei Yi Yingya Zhang1 Shiwei Zhang1 Xihui Liu Yujiu Yang2 Xiaogang Xu4 Hengshuang Zhao3 2Tsinghua University 3HKU 4CUHK Github: https://github.com/ali-vilab/Wan-Move"
        },
        {
            "title": "Abstract",
            "content": "We present Wan-Move, simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frames features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pros commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Moves superior motion quality. Code, models, and benchmark data are made available."
        },
        {
            "title": "Introduction",
            "content": "Motion lies at the heart of video generation as it fundamentally transforms static images into dynamic visual narratives. Recognizing its importance, both the research community [1, 2, 3] and commercial players [4, 5, 6] have devoted considerable effort to controlling motion in video generative models. The essence of motion control lies in injecting motion guidance signal into the video generation process. Thus, the two key choices are (i) how to represent the guidance signal and (ii) how to integrate it into the generator. First, existing motion guidance representations can be broadly classified into sparse and dense types. Sparse representations include bounding boxes [7, 8] and segmentation masks [1, 9, 10]. Although these signals can steer an objects global movement, they fail to control Equal contribution Corresponding authors Project leader 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: Wan-Move is image-to-video generation framework that supports diverse motion control applications. The generated samples (832480p, 5s) exhibits high visual fidelity and accurate motion. local motions. In contrast, dense representations, such as pixel-wise optical flow [2, 11, 12, 3] and point trajectories [13, 14], offer more fine-grained control ability. Yet, optical flow requires an additional model for flow estimation during inference, which adds cumulative errors across frames and hampers scalability. While point trajectories are easy to specify during inference, each track is only single-pixel thread and lacks surrounding spatial context. This makes it hard to align textures and motion patterns across neighboring regions. Second, to inject the guidance signal into generative models, range of motion encoders have been designed [14, 15, 1, 13, 16, 17], with ControlNet [18] being popular way to fuse motion cues. However, all of these pipelines introduce extra motion-aware modules, which may degrade the motion signal during processing and make it harder to fine-tune the video-generation backbone at scale. To tackle these challenges, we present Wan-Move, novel motion-control framework that builds on the existing image-to-video (I2V) generation model without adding auxiliary motion-processing modules. Our core idea is to inject motion information by directly editing the image condition features. We turn it into an updated latent guide that conveys both appearance and motion throughout video generation. Thanks to this simple and effective design, Wan-Move delivers high-quality motion control and scales easily by fine-tuning the powerful I2V backbone. Specifically, we represent motion trajectories using point tracks [13] as they capture fine-grained local and global movement. Unlike prior work [13] that embeds point trajectories into latent features, we transfer each trajectory from pixel space into latent coordinates. As I2V generation aims to animate the first frame, we guide this process by copying the first-frame feature at each tracked position to its corresponding location in later frames along the latent trajectory. Each copied feature preserves rich context, thus the propagated signal drives more natural local motion, as verified in Sec. 5.3. Moreover, since motion guidance is injected by editing the image condition features, we add no extra modules. As result, Wan-Move can plug straight into the I2V backbone, such as Wan-I2V-14B [19], and support scalable fine-tuning with fast convergence. Fig. 1 shows that Wan-Move generates highfidelity video clips (832480p, 5s) with precise motion control, enabling diverse set of applications, as illustrated in Sec. 5.4. To our best knowledge, it is the first research model (to be open-sourced) to match the visual quality of commercial products such as Kling 1.5 Pros Motion Brush [4]. 2 To set rigorous, comprehensive evaluation for motion-control methods, we introduce free-license benchmark termed MoveBench. Compared with existing benchmarks [20, 21, 22] that offer fewer clips, shorter durations, and incomplete motion annotations, MoveBench provides more data, greater diversity, and reliable motion annotations  (Fig. 5)  . Concretely, we design curation pipeline to categorize the video library into 54 content categories, 10-25 videos each, giving rise to over 1000 cases to ensure broad scenario coverage. All video clips maintain 5-second duration to facilitate evaluation of long-range dynamics. Every clip is paired with detailed motion annotations for single or multiple objects. They include both precise point trajectories and sparse segmentation masks to fit wide range of motion-control models. We ensure annotation quality by developing an interactive labeling pipeline. It combines human labeling with SAM [23] predictions, marrying annotation precision with automated scalability. In summary, our contributions are as follows: We propose Wan-Move for motion control in image-to-video generation. Unlike prior approaches that require motion encoding, it injects the motion guidance by editing condition features, adding no new modules and allowing the easy fine-tuning of base models at scale. We introduce MoveBench, comprehensive and well-curated benchmark to assess motion control. hybrid human+SAM labeling pipeline ensures annotation quality. Extensive experiments on MoveBench and public datasets show that Wan-Move supports diverse motion-control tasks and delivers commercial-grade results with scaled training."
        },
        {
            "title": "2 Related Work",
            "content": "Video generation models. Visual generation [24, 25, 26, 27, 28] has attracted increasing attention in recent years. Video diffusion models [29] pioneer the extension of denoising diffusion probabilistic models (DDPMs) to video generation through 3D U-Net architecture. Subsequent advancements, such as Imagen Video [30] and Phenaki [31], enhance this framework to produce longer and higherresolution sequences. Nevertheless, these CNN-based approaches [32, 33, 34] face limitations in capturing long-range spatiotemporal dependencies. Transformer-based architectures [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51] overcome this bottleneck and greatly improve training scalability. Recent innovations, including CogVideoX [52] and HunyuanVideo [6], further validate the efficacy of spatio-temporal attention mechanisms for coherent video synthesis. Notably, Wan [19] introduces an efficient framework for both text-to-video and image-to-video generation, setting new standard for open-source video models. Our Wan-Move, introduces how to leverage latent trajectory guidance to enable motion control upon the image-to-video diffusion model, enabling precise motion control while preserving visual fidelity. Motion-controllable video generation. To adapt pretrained video generation models for motioncontrollable synthesis, training-free methods [53, 54, 55, 56] optimize input noisy latents or manipulate attention mechanisms, enabling zero-shot control. However, these approaches often exhibit performance degradation when controlling fine-grained or multi-object motion. In contrast, finetuning based methods [57, 2, 15, 1, 3, 14, 20, 58, 16, 59, 60, 61, 10, 12, 62, 63, 64] leverage diverse motion signals and introduce various techniques to integrate them into the base model. While these methods significantly enhance output quality, they typically require auxiliary encoders or fusion modules, complicating the model architecture and limiting training scalability. Among these studies, the most relevant to our work is Motion Prompting [13], as both employ point trajectories to represent motion guidance. However, we differ in two key aspects. First, Motion Prompting [13] encodes point tracks via random embeddings in pixel space, where the guidance is pixel-level threads that lack surrounding context to offer local control. We express point trajectories in latent space using the image feature, providing rich local information and finer control. Second, Motion Prompting [13] integrates motion guidance through separate ControlNet [18], whereas we directly use the pretrained base model without architectural modifications, which facilitates its scalable fine-tuning. Sec. 5.3 provides quantitative and qualitative evidence of our advantages. For more specific robotic scenarios, works [65, 66] rely on pretrained DINOv2 features [67] to transfer object representations across frames for motion control in generated videos, yet both delivers DINOv2s limitation in representing motion signals. DINOv2 excels at high-level semantic encoding but lacks fine-grained object details. Thus, GEM [1] employs additional identity embeddings to distinguish objects and train an ObjectNet to bridge the domain gap between DINOv2 and the UNets feature space. Moreover, the 14 patch size in DINOv2 may restrict the granularity of the proposed 3 Figure 2: (a) To inject motion guidance, we transfer point trajectories from videos to latent space, then replicate the first frame feature into subsequent zero-padded frames along each latent trajectory. (b) Wan-Move is trained upon an existing image-to-video generation model (e.g., work [19, 34]), with an efficient latent feature replication step (as in (a)) to update the condition feature. The CLIP [70] image encoder and umT5 [71] text encoder from the base model are omitted for simplicity. control. In contrast, our approach mitigates these limitations by employing native VAE in I2V foundation models without relying on any auxiliary modules (e.g., identity embeddings). Benchmark for motion-controllable video generation. Most motion-controllable work evaluates on small, task-specific datasets, which are typically ad hoc, limited in scope, or insufficient for evaluating long-range dynamics and multi-object interactions. For example, datasets such as DAVIS [21] and VIPSeg [22] have been repurposed for trajectory control methods, yet their short clip durations and sparse annotations make them inadequate for assessing long-term consistency or complex interactions. While MagicBench [20] expands to 600 video clips, it categorizes samples solely by object count and relies on automatically generated labels from noisy pipelines, limiting the annotation precision. To address these limitations, we introduce MoveBench, comprehensive benchmark for motioncontrollable video generation. It includes carefully selected 1018 videos with extensive annotations, long-range dynamics, and 54 well-classified content patterns."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary Video diffusion models [29, 19, 6, 68] apply Gaussian noise to clean data during the forward process and learn reverse process to denoise and generate videos. To reduce computational costs, the denoising network typically operates on latent video representations obtained from pretrained VAE [69]. Given an input video R(1+T )HW 3, the encoder compresses both the temporal and spatial dimensions with compression ratios ft (temporal) and fs (spatial), while expanding the channel dimension to C, yielding = E(V) R(1+ C. The decoder then reconstructs the video from the latent representation as ˆV = D(x). ) fs fs ft Our work focuses on motion-controllable image-to-video (I2V) generation, where models are required to generate motion-coherent videos based on the input first-frame image and motion trajectories. While the first frame will be encoded into the condition feature zimage by the VAE, motion trajectories, which can be represented in diverse formats, remain in pixel space. Thus, the key challenge lies in effectively encoding motion trajectories into the condition feature zmotion and injecting it into the generative model. To avoid the signal degradation and training difficulties associated with additional motion encoder and fusion modules, we aim to develop motion-control framework that leverages existing I2V models without architectural modifications, as detailed below. 4 3.2 Latent Trajectory Guidance To enable video generation conditioned on the first frame, an effective approach of popular I2V models [19, 34] concatenates the latent noise xt and the first-frame condition feature zimage along the channel dimension. zimage is obtained by encoding the first frame along with zero-padded subsequent frames 0T HW using pretrained VAE encoder E: zimage = (concat [I, 0T HW 3]) R(1+ ft ) fs fs C. (1) For motion guidance representation, we adopt point trajectories, following prior studies [13, 14], as they provide fine-grained control and capture both local and global motion. Formally, point trajectory of length 1 + can be represented as R(1+T )2, where p[n] = (xn, yn) specifies the trajectory location in the n-th frame in the pixel space. Existing methods often employ auxiliary modules to encode and integrate trajectories into the backbone. Yet, this approach may degrade motion signals during motion encoding. In addition, training extra modules increases the complexity of fine-tuning the base model at scale. This raises key question: Can we inject pixel-space motion guidance without auxiliary modules? Intuitively, I2V generation aims to animate the first frame, while motion trajectories specify object positions in each subsequent frame. Given the translation equivariance of VAE models, latent features at corresponding trajectory positions should closely resemble those in the first frame. Motivated by this, we propose encoding trajectories directly into latent space via spatial mapping, eliminating the need for an extra motion encoder: p[n] = p[n] fs (cid:80)nft i=(n1)ft+1 p[i] ftfs if = 0, 1 ft , (2) The latent trajectory position at the first frame is derived by spatial mapping, while for subsequent frames, it is averaged over each consecutive ft frames. This deterministically transforms pixel-space trajectories into latent space. To inject the obtained latent trajectories, we extract the latent features of the first frame at the initial trajectory point p[0] and replicate them across subsequent frames according to p, leveraging the translation equivariance of latent features, as shown in Fig. 2 (a): zimage [n, p[n, 0], p[n, 1], :] = zimage [0, p[0, 0], p[0, 1], :] for = 1, . . . , ft . (3) Here, zimage[t, h, w, :] denotes the feature vector at temporal index t, height h, and width w. This operation efficiently injects motion guidance into the condition feature by updating zimage, eliminating the need for explicit motion condition features and injection modules. An overview of the WanMove generation framework is presented in Fig. 2(b). When multiple visible point trajectories coincide at given spatiotemporal position, we randomly select one trajectorys corresponding first-frame feature. 3.3 Training and Inference Training data. We curate high-quality training dataset, which undergoes rigorous two-stage filtering to ensure both visual quality and motion consistency. First, we manually annotate the visual quality of 1,000 samples and use them to train an expert scoring model for initial quality assessment. To further enhance temporal coherence, we introduce motion quality filtering stage. Specifically, for each video, we extract SigLIP [72] features from the first frame and compute the mean SigLIP features for the remaining frames. The cosine similarity between these features serves as our stability metric. Based on empirical analysis of 10,000 samples, we establish threshold to retain only videos where the content remains consistent with the initial frame. This two-stage pipeline produces final dataset of 2 million high-quality 720p videos with strong visual quality and motion coherence. Additional details on the training data sources are provided in the supplementary material. Modeling training. Based on our training dataset, we use CoTracker [73] to track the trajectories of dense 3232 grid of points. For each training iteration, we sample trajectories from 5 Figure 3: Construction pipeline of MoveBench to obtain high-quality samples with rich annotations. Figure 4: Balanced sample number per class. Figure 5: Comparison with related benchmarks. mixed distribution: with 5% probability, no trajectory is used (k = 0); with 95% probability, is uniformly sampled from 1 to 200. Notably, we retain 5% probability of dropping motion conditions, which effectively preserves the models original image-to-video generation capability. For the selected trajectories, we extract the first-frame features and replicate them to subsequent zero-padded frames, as formalized by Eq. (3). Since CoTracker distinguishes between visible and occluded point trajectories, we perform feature replication only along the visible trajectories. During training, the model parameters θ is initialized from the I2V model [19] and fine-tuned to predict the vector field vt(xt) that transports samples from the noise distribution to the data distribution [74]: LFM(θ) = Et,xt,c (cid:2)vθ(xt, t, c) vt(xt)2(cid:3) , (4) where denotes the union of the generation condition. Inference with Wan-Move. The inference process closely resembles the original I2V model, with an additional latent feature replication operation. Specifically, Wan-Move conditions generation on three inputs: (1) text prompt, (2) an input image as the first frame, and (3) sparse or dense point trajectories for motion control. Pretrained umT5 [71] and CLIP [70] models are employed to encode global context from the text prompt and first frame, respectively. The resulting image embedding zglobal and text embeddings ztext are then injected into the DiT backbone via decoupled crossattention [18]. Additionally, VAE is used to extract the first-frame condition feature zimage, which will be injected through latent feature replication (as detailed in Sec. 3.2). Classifier-free guidance is applied to enhance alignment with conditional information. Formally, let unconditional vector field vuncond= vθ(xt, t, zimage, zglobal), and conditional vector field vcond=vθ(xt, t, zimage, zglobal, ztext). The guided vector field vθ(xt, t, zimage, zglobal, ztext) is weighted combination of the conditional and unconditional outputs, with the guidance scale w: vθ(xt, t, zimage, zglobal, ztext) = vuncond + w(vcond vuncond) (5)"
        },
        {
            "title": "4 MoveBench",
            "content": "Current benchmarks for motion-controllable video generation suffer from small scale, short duration, and lack precise, comprehensive motion annotations, thus introducing bias and limiting granularity. To address these gaps, we introduce MoveBench, high-quality benchmark with 1018 videos (480832 resolution, 5-second duration), designed for comprehensive evaluation of motion-controllable generation, as illustrated in Fig. 3-5. The evaluation videos are selected from Pexels [75], large-scale, high-quality dataset containing about 400K videos, all released under free license. MoveBench combines algorithmic curation with human expertise to ensure diverse, representative, and precisely annotated motion data. Compared to prior works, it offers three key features: (i) High quality. We curate videos through rigorous four-stage pipeline, as illustrated in Fig. 3(a). We first utilize the expert scoring model obtained from Sec. 3.3 to score videos based on visual quality, 6 Table 1: Performance comparisons on MoveBench and DAVIS. Wan-Move consistently yields substantial improvements in both visual fidelity and motion quality across all metrics. Method MoveBench DAVIS FID FVD PSNR SSIM EPE FID FVD PSNR SSIM EPE ImageConductor [58] LeviTor [16] Tora [14] MagicMotion [20] Wan-Move (Ours) 34.5 18.1 22.5 17.5 12.2 424.0 98.8 100.4 96.7 83.5 13.4 15.6 15.7 14.9 17. 0.49 0.54 0.55 0.56 0.64 15.6 3.4 3.3 3.2 2.6 54.2 22.0 25.9 24.2 14.7 513.6 115.4 129.2 113.4 94.3 11.6 13.3 13.7 12.8 16.5 0.47 0.51 0.49 0.53 0. 14.8 3.7 3.5 3.5 2.5 Figure 6: Qualitative comparisons between Wan-Move and recent approaches, including both academic methods [58, 16] and commercial solutions [4]. Motions that deviate from the specified trajectories and major visual artifacts are marked with red dots and boxes, respectively. filtering out low-quality content. Then, the selected videos are cropped to 480p and uniformly sampled to 81 frames to ensure temporal consistency. Finally, videos are clustered into 54 content categories and we manually select the 15-25 most representative examples for each category, balancing diversity and quality  (Fig. 4)  . (ii) Precise annotations. As shown in Fig. 5, we provide both point and mask annotations, so that methods using mask guidance signals can also be evaluated using our benchmark. To precisely annotate motion regions, we design an interactive annotation interface as shown in Fig. 3. Annotators click on target region in the first frame, prompting SAM [23] to generate an initial mask. When the mask exceeds the desired area, annotators add negative points to exclude irrelevant regions. This is critical for isolating articulated motions or small objects in cluttered scenes. After annotation, each video contains at least one point indicating representative motion, with 192 videos additionally including multiple-object motion trajectories. (iii) Detailed captions. As captions [76, 77, 78] are validated critical for generation tasks, we use powerful Gemini [79] to generate dense descriptions covering objects, actions, and camera dynamics. Unlike segmentation datasets like DAVIS [21], our captions are tailored for video generation tasks."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experimental Setup Wan-Move is implemented on top of Wan-I2V-14B [19], state-of-the-art image-to-video (I2V) generation model. As described in Sec. 3.3, we fine-tune Wan-Move on high-quality dataset consisting of 2M high-quality videos. Only the DiT backbone is trainable, while the image and text encoders remain frozen. During inference, we use classifier-free guidance scale of 5.0 unless otherwise specified. Detailed training configurations are provided in the supplementary material. To quantitatively evaluate the fidelity of generated videos, we compute standard video quality metrics including FID [80], FVD [81], PSNR, and SSIM [82]. To assess motion accuracy, we measure the L2 distance between ground truth tracks and those estimated from generated videos, following [13] in denoting this metric as end-point error (EPE). All evaluations are performed at resolution of 480p. 7 Figure 7: Qualitative comparisons with MagicMotion [20], controlling motions using sparse signals (i.e., bounding boxes) as input. Motions that break the guidance are marked with red dots. Table 2: MoveBench multi-object motion results. Table 3: Our win rates in 2AFC human study. Method FID FVD PSNR SSIM EPE Method Motion accuracy Motion quality Visual quality ImageConductor Tora Wan-Move (Ours) 77.5 53.2 28.8 764.5 350.0 226.3 13.9 14.5 16.7 0.51 0.54 0. 9.8 3.5 2.2 LeviTor Tora MagicMotion Kling 1.5 Pro 98.2 96.2 89.4 47.8 98.0 93.8 96.4 53.4 98.8 98.4 98.2 50.2 Table 4: Ablation on motion guidance strategies. Motion guidance FID FVD PSNR SSIM EPE Pixel replication 17.3 91.0 Random track embedding 15.4 89.2 Latent feature replication 12.2 83.5 15.3 16.1 17.8 0.56 0.59 0.64 3.7 2.7 2.6 Table 5: Ablation on condition fusion strategies. Cond. fusion FID FVD EPE Latency (s) ControlNet 12.4 Concat. (Ours) 12.2 84.6 83.5 2.5 2.6 987 (+225) 765 (+3) 5.2 Main Results Single-object motion control. We present an extensive comparison between Wan-Move and recent motion-controllable video generation methods [58, 16, 14, 20, 4]. Quantitative results on MoveBench and the public DAVIS [21] are shown in Table 1. Qualitative visualizations are presented in Fig. 6. Unlike other methods that rely on point tracks for motion guidance, MagicMotion [20] takes as input sparse masks and bounding boxes. Since boxes can be directly derived from segmentation masks, they are inherently compatible with MoveBench that covering mask annotations. Hence, we also conduct comparisons using box-based inputs in Fig. 7. Among these methods, ImageConductor [58] exhibits poor performance in both image and motion quality, which can be attributed to its reliance on direct pixel-level trajectory injection, where single-pixel features lack sufficient semantic and texture information. The remaining methods report similar EPE (3.23.4), despite differing motion guidance approaches: Levitor [16] and MagicMotion [20] utilize the complex ControlNet [18], while Tora [14] adopts the lightweight adaLN [35]. Notably, our method achieves the best motion control performance (lowest EPE) and video quality (highest PSNR and SSIM) through latent trajectory replication without introducing additional parameters. This underscores the effectiveness of our latent trajectory guidance in adhering to motion constraints. Consistent results on the DAVIS dataset further validate the robustness of our approach. Multi-object motion control. As MoveBench includes 192 cases with annotated multi-object motion, we further evaluate Wan-Move against baselines [58, 14] on this challenging setting, as presented in Table 2. Our method achieves significantly lower FVD and reduced EPE compared to other methods, highlighting its precise adherence to motion constraints in more complex scenarios. Human study. We conduct two-alternative forced-choice (2AFC) human evaluation comparing Wan-Move with SOTA approaches [14, 4, 16, 20]. Each method generated 50 conditioned samples, which are evaluated by 20 participants. The results, presented in Table 3, report Wan-Moves win rates across three metrics: motion accuracy, motion quality, and visual quality. Compared to Tora [14], Wan-Move achieves win rates exceeding 96% in all categories. When evaluated against the commercial model Kling 1.5 Pro, our method demonstrates competitive performance, with superior win rates in motion quality. This narrows the gap between research-oriented and commercial models. 5.3 Ablation Study Trajectory guidance strategy. We investigate the impact of motion guidance strategies on video Table 6: Ablation on maximum number of point trajectories (see Sec. 3.3) during training. FVD Number PSNR SSIM EPE FID Table 7: Ablation on actual number of point trajectories during inference. FID SSIM EPE Number PSNR FVD = 10 = 100 = 200 = 500 = 12.8 12.9 12.2 13.3 13.4 86.6 84.7 83.5 83.9 83.7 17.6 17.7 17.8 17.6 17.2 0.62 0.65 0.64 0.63 0.61 3.3 2.7 2.6 3.0 3.9 = 0 = 1 = 16 = 512 = 12.8 12.2 10.6 7.7 6.2 87.9 83.5 78.3 51.0 45.2 17.9 17.8 18.2 20.3 21.9 0.64 0.64 0.67 0.75 0.79 12.4 2.6 2.2 1.5 1.1 quality and motion consistency. Quantitative and qualitative results as presented in Table 4 and Fig. 8, respectively. Pixel replication applies pixel-level copy-paste along the original trajectory, followed by VAE encoding. Yet, since single-pixel features contain limited semantic and texture information, the resulting motion control is weak, as reflected by high EPE value of 3.7 and generation failures (see Fig. 8). The random track embedding approach, originally proposed for pixel space representations [13], is adapted to assign randomly initialized embeddings in latent space for injecting motion guidance. While effective for rigid single-region control, this approach fails to incorporate contextual information from surrounding regions, resulting in suboptimal video quality (lower PSNR and SSIM) and stiff motion near tracked points. For example, the hand moves, but surrounding bread remains static in Fig. 8. In contrast, our proposed latent feature replication method achieves superior video quality (highest PSNR of 17.8) and precise motion control (lowest EPE of 2.6). Figure 8: Visualization of various guidance strategies. Condition fusion strategy. We compare different motion condition approaches, namely ControlNet [18] and direct concatenation (our approach). The results are presented in Table 5. Notably, simple concatenation of motion conditions with input noise achieves performance comparable to ControlNet in motion-controllable generation. Yet, ControlNet introduces significant additional modules, substantially increasing inference latency by 225 seconds over the original I2V model. In contrast, Wan-Move preserves the base model architecture and only adds one-time trajectory extraction process, increasing just 3-second inference time. Number of point trajectories during training. Table 6 evaluates the impact of the maximum number of point tracks (N ) during training. As increases from 10 to 200, the models motion-following capability improves progressively, evidenced by the decreasing EPE. The optimal performance, in terms of both structural similarity (SSIM) and EPE, is achieved at =200. However, further increasing the number of point tracks leads to rise in EPE. This can be attributed to the mismatch between the dense point tracks in the training and the sparse point tracks during evaluation. Number of point tracks during inference. Table 12 ablates the performance of WanMove across varying numbers of point tracjectories over MoveBench. As the number of tracks increases, EPE drops significantly, indicating better motion guidance and enhanced temporal coherence. When reaching the maximum number of point trajectories extracted by CoTracker, Wan-Move achieves the lowest EPE of 1.1. Though it is trained with at most 200 tracks, the model shows strong generalization capability. Notably, naive I2V inference (with no point tracks) yields PSNR and SSIM scores comparable to motioncontrolled generation, confirming that our model strongly retains its inherent I2V quality. Naive I2V samples generated by Wan-Move are presented in Fig. 9. Figure 9: I2V results of Wan-Move (no point tracks). Backbones and data scale. In pursuit of the best generation quality, we initially train Wan-Move with large-scale dataset and strong backbone. To ensure fair comparison with the leading approaches MagicMotion and Tora, we align Wan-Moves backbone and training data scale with them. This yields two variants, i.e., Wan-Move-Cog-23K and Wan-Move-Cog-630K, which are trained on 23K and 630K data samples respectively, using CogVideoX1.5-5B-I2V [52] as backbone. The 9 Table 8: Ablation on different I2V backbones and training data scale. Wan-Move attains better results under the same setting. Method Data scale FID FVD PSNR SSIM EPE Backbone MagicMotion CogVideoX-5B Wan-Move-Cog-23K CogVideoX-5B CogVideoX-5B Tora Wan-Move-Cog-630K CogVideoX-5B Wan-Move 14.9 17.5 96.7 23K 23K 16.8 16.0 92.3 630K 22.5 100.4 15.7 17.2 630K 14.1 87.3 17.8 Wan2.1-I2V-14B 2000K 12.2 83. 0.56 0.59 0.55 0.61 0.64 3.2 2.8 3.3 2.8 2.6 Table 9: Large-motion and outof-distribution-motion subset. Subset Method FID FVD EPE Large OOD Tora 29.1 MagicMotion 24.6 14.5 Wan-Move Tora 28.9 MagicMotion 23.5 13.5 Wan-Move 126.3 119.3 86.6 120.2 115.7 86.0 4.3 4.1 3.0 4.0 3.9 2.8 detailed comparison on MoveBench is shown in Table 8. Under the same backbone and data scale, Wan-Move still outperforms these two powerful methods. Evaluation on large-motion and out-of-distribution motion scenarios. To further verify the model generalizability, we curate subsets from MoveBench containing high-amplitude and out-ofdistribution motion control cases. For each video, its motion amplitude score is computed as the average of the top 2% largest optical flow values extracted by RAFT [83]. The top 20% highest-score videos are selected as large-motion videos. Besides, we manually curate 50 uncommon motion cases as out-of-distribution subset, including complex foregroundbackground interactions, objects moving out of frame, and rare camera motions. Evaluation results on these challenging examples are shown in Table 9. Notably, Wan-Move consistently outperforms two leading baselines, with performance gaps further widening under these difficult condition. In addition, Wan-Moves performance only marginally drops compared to its results on the full benchmark, demonstrating its robustness. 5.4 Motion Control Applications As point trajectories can flexibly represent various types of motion, Wan-Move supports wide range of motion control applications, as showcased in Fig. 1. First, rows 12 show object control using single or multiple point trajectories. For camera control (row 3), we can either drag background elements directly or follow the approach of work [13]. The latter estimates point cloud from monocular depth predictor [84], projects it along camera pose trajectory, and applies z-buffering to obtain camera-aligned 2D trajectories. Following work [13], we perform primitive-level control by rotating virtual sphere to generate projected 2D trajectories for globe motion (row 4). In row 5, we enable motion transfer by applying trajectories extracted from one video to update the condition features of different image. Row 6 shows 3D rotation control by estimating depth-based positions, applying rotation, and projecting the results to 2D. We refer readers to the supplementary file for more visualizations and full videos."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "We propose Wan-Move, simple and scalable framework for precise motion control in video generation. It represents motion with point trajectories and transfers them into latent coordinates through spatial mapping, requiring no extra motion encoder. We then inject trajectory guidance into first-frame condition features via latent feature replication, achieving effective motion control without architectural changes. For rigorous evaluation, we further present MoveBench, comprehensive and well-curated benchmark featuring diverse content categories with hybrid-verified annotations. Extensive experiments on MoveBench and public datasets show that Wan-Move generates highquality, long-duration (5s, 480p) videos with motion controllability on par with commercial tools like Kling 1.5 Pros Motion Brush. We believe our open-sourced solution offers an efficient path to scale motion-controllable video generation and will empower wide range of creators. Limitations and broader impacts. Wan-Move uses point trajectories to guide motion, which can be unreliable when tracks are missing due to occlusion. While we observe that short-term occlusions can be recovered once the point reappears, showing degree of generalization, prolonged absence may lead to loss of control (see Appendix). As with other generative models, Wan-Move carries dual-use potential. Its ability to produce realistic, controllable videos can benefit creative industries, education, and simulation, but also risks misuse for generating misleading or harmful content."
        },
        {
            "title": "7 Acknowledgment",
            "content": "This work was supported by the National Natural Science Foundation of China (Grant No. 62576191). 10 Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance"
        },
        {
            "title": "Contents",
            "content": "8 Implementation Details"
        },
        {
            "title": "8.1 Training Data Details .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "8.3 Training and Inference Configuration . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "9 Additional Experiments 9.1 Choice of Feature Replication Strategies under Trajectory Overlap . . . . . . . . . 9.2 Choice of Different Training Strategies . . . . . . . . . . . . . . . . . . . . . . . . 9.3 Model Performance Under Trajectory Disappearance . . . . . . . . . . . . . . . . 9.4 Failure Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Qualitative Visualizations 10.1 More Qualitative Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2 More Camera Control Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3 More Motion Transfer Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4 More 3D Rotation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 11 11 13 13 13 13 13 14 14 14"
        },
        {
            "title": "Implementation Details",
            "content": "8.1 Training Data Details Table 10 presents the composition of the filtered training datasets, which are sourced from Panda70M [85], Pixabay [86], Pexels [75], and YouTube. YouTube videos are independently collected for this study. To prevent data leakage, the videos from Pexels are strictly separated from those in the proposed MoveBench. All videos for training are captioned using Qwen2.5-VL [87], with the prompt structure illustrated in Fig. 10. This prompt emphasizes motion and camera attributes while preserving the fundamental scene descriptions, ensuring that the model semantically understands the context and generates physically plausible motions. The same captioning prompt is applied to the videos in MoveBench. 8.2 MoveBench Construction Details Video content clustering. Following the initial filtering stage, we conduct rigorous content clustering process to ensure broad scenario coverage in our benchmark. Specifically, we sample 16 frames per filtered video and compute the average of their SigLip [72] features. Using k-means clustering, we group these features into 54 distinct content categories. Each category label, e.g., Tennis, is then automatically captioned using Qwen2.5-VL [87]. Finally, we manually select the 1525 most representative videos per category to maintain balance of diversity. Interactive labeling. Existing models often fail to accurately identify representative motion regions in videos, as the most prominent motion may not be optimal, and many motions terminate prematurely. 11 Table 10: The statistics of the training datasets. Figure 10: Prompt for video caption. Dataset source Number Captioner Panda70M [85] 0.56M Qwen2.5-VL Pixabay [86] 0.42M Qwen2.5-VL Pexels [75] 0.25M Qwen2.5-VL YouTube 0.75M Qwen2.5-VL Figure 11: The interactive annotation interface displays the video (left) and its first frame (right). Users click green positive points to specify the start point of motion trajectory, and red negative points to exclude irrelevant regions if needed. SAM segments the mask of moving objects & regions for user review. To annotate multiple motion trajectories, users must assign different object IDs. To facilitate precise annotation of motion regions, we introduce an interactive labeling interface  (Fig. 11)  for selecting the initial motion point and its corresponding mask in the first frame. Annotators begin by selecting target point in the initial frame, prompting SAM [23] to generate preliminary segmentation mask. If the mask extends beyond the desired area, negative points can be added to exclude irrelevant regions. This method effectively isolates articulated motions or small objects. For subsequent frames, point trajectories are automatically extracted using CoTracker [73]. 8.3 Training and Inference Configuration As illustrated in Sec. 5.1 of the main paper, we employ Wan-I2V-14B [19] as the base I2V model. During training, both the DiT and umT5 components of Wan are wrapped with Fully Sharded Data Parallel (FSDP) [88], with parameters cast to torch.bfloat16 for memory efficiency. The training employs the AdamW optimizer [89] with weight decay of 1e-3 and base learning rate of 5e-6. The first 2,000 steps are used for linear warm-up to enable smooth transition from the initial I2V generation (corresponding to 0 point trajectories) to motion-controllable video generation. We adopt flow matching objective for optimization, where the number of time sampling steps is set to 1,000 during training. To enable large-scale training with long sequences (e.g., 5s video clip), we adopt the Ulysses sequence parallelism strategy [90] following Wan, setting the sequence parallel size to 4. We train our model using 64 NVIDIA A100 GPUs, with each GPU processing quarter of 12 Table 11: Impact of feature replication strategies when multiple motion trajectories overlap. FVD strategy PSNR SSIM EPE FID Table 12: Impact of using dense-to-sampling training strategy. Strategy FID FVD PSNR SSIM EPE Average 13.1 Random 12.2 83.4 83.5 17,5 17.8 0.63 0. 2.7 2.6 Dense-to-Sampling 12.9 84.2 12.2 83.5 Sampling 17.5 17.8 0.62 0.64 2.6 2.6 Figure 12: Wan-Move generalizes to continue controlling motion trajectories when they temporarily disappears. Green circles indicate visible segments, while red circles mark invisible segments, e.g., occluded or out-of-frame parts. the sequence length, for total of 30,000 steps. During inference, we follow Wans sampling scheme with 50 sampling steps."
        },
        {
            "title": "9 Additional Experiments",
            "content": "9.1 Choice of Feature Replication Strategies under Trajectory Overlap We analyze the impact of feature replication strategies in cases of trajectory overlap. The results, presented in Table 11, demonstrate that randomly selecting single trajectorys first-frame feature for replication when multiple trajectories coincide yields superior video quality and motion control. This is evidenced by lower FVD and EPE compared to feature averaging. We hypothesize that averaging features from overlapping trajectories leads to information loss, thereby degrading performance. 9.2 Choice of Different Training Strategies This subsection evaluates the performance differences between dense-to-sampling and direct sampling training strategies, as presented in Table 12. Prior work [14, 15, 2, 58] commonly employs two-stage dense-to-sampling pipeline, where the first stage uses dense motion trajectories to enhance motion control followed by sparse trajectories in the second stage. However, we find that our model, trained with randomly sampling of 1-200 points (as refer to Sec. 3.3 in the main paper), achieves comparable EPE and lower FVD compared to the two-stage approach. These results demonstrate that our method provides generalization capability in point trajectory numbers while simplifying the training process. This generalization ability is also verified in Table 12 of the main paper. 9.3 Model Performance Under Trajectory Disappearance Fig. 12 illustrates our models capability to generate motion-coherent videos when handling temporarily invisible trajectories. The Wan-Move maintains stable generation quality in these challenging scenarios, which we attribute to both the presence of similar cases in the training data and the models inherent generalization capacity. 9.4 Failure Cases This subsection analyzes and visualizes three primary failure modes of Wan-Move, as illustrated in Fig. 13. First, control degradation occurs when motion trajectories remain invisible for extended durations, causing the model to lose conditional guidance. Second, performance deteriorates in visually complex scenes with multiple interacting objects in crowded environments. Third, implausible 13 Figure 13: Three primary failure modes of Wan-Move. (a) Loss of motion control due to persistent trajectory disappearance; (b) Visual artifacts in overly complex, crowded environments; and (c) motion outputs that violate rigorous physical laws. motion trajectories that violate fundamental physical laws result in out-of-distribution predictions. Further, erroneous tracking points identified by CoTracker [73] may compound these failure modes."
        },
        {
            "title": "10 Qualitative Visualizations",
            "content": "10.1 More Qualitative Comparisons We present additional qualitative comparisons with state-of-the-art academic [14] and commercial [4] approaches, as shown in Fig. 14. 10.2 More Camera Control Results As demonstrated in Fig. 15, Wan-Move enables camera control. This can be accomplished, following the work [13], by estimating point cloud using monocular depth predictor [84], projecting it along predefined camera trajectory, and applying z-buffering to derive occlusion flags and camera-aligned 2D trajectories. 10.3 More Motion Transfer Results This subsection presents dense motion transfer visualizations generated by Wan-Move using dense point trajectories (1,024 in our implementation). As illustrated in Fig. 16, Wan-Move achieves nearly identical appearance quality and motion alignment compared to the original videos given dense trajectory conditions and the same first frame. Moreover, Wan-Move also enables video editing by 14 Figure 14: Additional qualitative comparisons with Tora [14] and the commercial model Kling 1.5 Pro [4]. Wan-Move demonstrates superior motion accuracy and visual quality. Major motion control failures or visual artifacts are denoted with red boxes. copying the motion while using an additional image editing model to modify the content in the first frame, maintaining the original videos motion trajectories, as shown in Fig. 17. 10.4 More 3D Rotation Results As illustrated in Fig. 18, Wan-Move additionally supports 3D object rotation. This capability is realized by first estimating depth-based 3D positions, applying rotational transformation, and then reprojecting the results into 2D trajectories. These trajectories subsequently serve as conditioning inputs for our model to rotate the objects in videos. Figure 15: Wan-Move enables effective and flexible camera control through different point trajectories, such as linear displacement, dolly in, and dolly out. Figure 16: Wan-Move enables accurate video motion copy using dense point trajectories (e.g., 1024 points). The synthesized video preserves high fidelity in both appearance and object-level motion alignment with the original video, even under complex environmental conditions."
        },
        {
            "title": "References",
            "content": "[1] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In ECCV, 2024. [2] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv:2308.08089, 2023. 16 Figure 17: Wan-Move enables video editing through motion copy and additional image editing models. It first applies the image editing model (e.g., ControlNet [18], GPT-4o [91]) to modify the style or content of the first frame, then uses the original videos motion trajectories to animate the edited image frame. Figure 18: Wan-Move enables object 3D rotation by estimating depth-based positions, applying rotation, and projecting the results to 2D. [3] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, and Ning Yu. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. In CVPR, 2025. [4] Kuaishou. Kling ai. https://klingai.kuaishou.com, 2024.06. [5] Runway. Gen-3. https://runwayml.com, 2024.06. [6] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv:2412.03603, 2024. [7] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. NeurIPS, 2024. 17 [8] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv:2402.01566, 2024. [9] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Animateanything: Fine-grained open domain image animation with motion guidance. arXiv:2311.12886, 2023. [10] Haitao Zhou, Chuang Wang, Rui Nie, Jinlin Liu, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. In AAAI, 2025. [11] Mathis Koroglu, Hugo Caselles-Dupré, Guillaume Jeanneret Sanmiguel, and Matthieu Cord. Onlyflow: Optical flow based motion conditioning for video diffusion models. arXiv:2411.10501, 2024. [12] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In SIGGRAPH, 2024. [13] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv:2412.02700, 2024. [14] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv:2407.21705, 2024. [15] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, 2024. [16] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. arXiv:2412.15214, 2024. [17] Yuqing Chen, Junjie Wang, Lin Liu, Ruihang Chu, Xiaopeng Zhang, Qi Tian, and Yujiu Yang. O-disco-edit: Object distortion control for unified realistic video editing. arXiv preprint arXiv:2509.01596, 2025. [18] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [19] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [20] Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, and Zuxuan Wu. Magicmotion: Controllable video generation with dense-to-sparse trajectory guidance. arXiv:2503.16421, 2025. [21] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. [22] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic segmentation in the wild: benchmark. In CVPR, 2022. [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment Anything. In ICCV, 2023. [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 18 [25] Zhekai Chen, Ruihang Chu, Yukang Chen, Shiwei Zhang, Yujie Wei, Yingya Zhang, and Xihui Liu. Tts-var: test-time scaling framework for visual auto-regressive generation. arXiv preprint arXiv:2507.18537, 2025. [26] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [27] Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nießner, Chi-Wing Fu, and Jiaya Jia. Diffcomplete: Diffusion-based generative 3d shape completion. Advances in neural information processing systems, 36:7595175966, 2023. [28] Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, and Ziwei Liu. Freescale: Unleashing the resolution of diffusion models via tuning-free scale fusion. arXiv preprint arXiv:2412.09626, 2024. [29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. [30] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. arXiv:2210.02303, 2022. [31] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv:2210.02399, 2022. [32] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. [33] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv:2308.06571, 2023. [34] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia, 2024. [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [36] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022. [37] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [38] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. [39] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv:2501.00103, 2024. [40] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. TMLR, 2025. [41] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv:2412.00131, 2024. 19 [42] OpenAI. Video generation models as world simulators, 2024. [43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv:2410.13720, 2024. [44] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv:2405.04233, 2024. [45] MiniMax. Hailuo ai. https://hailuoai.com/video, 2024.09. [46] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. [47] GenmoTeam. Mochi 1. https://github.com/genmoai/models, 2024. [48] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv:2502.10248, 2025. [49] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. [50] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. [51] Yu Li, Menghan Xia, Gongye Liu, Jianhong Bai, Xintao Wang, Conglang Zhang, Yuxuan Lin, Ruihang Chu, Pengfei Wan, and Yujiu Yang. Adaviewplanner: Adapting video diffusion models for viewpoint planning in 4d scenes. arXiv preprint arXiv:2510.10670, 2025. [52] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In ICLR, 2025. [53] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David Lindell. Sg-i2v: Self-guided trajectory control in image-to-video generation. arXiv:2411.04989, 2024. [54] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv:2406.16863, 2024. [55] Wan-Duo Kurt Ma, John Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. In SIGGRAPH Asia, 2024. [56] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via masked-diffusion. In CVPR, 2024. [57] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. VideoComposer: Compositional video synthesis with motion controllability. In NeurIPS, 2023. [58] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Ying Shan, and Yuexian Zou. Image conductor: Precision control for interactive video synthesis. In AAAI, 2025. [59] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In ICLR, 2024. 20 [60] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. arXiv:2502.08639, 2025. [61] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv:2501.03847, 2025. [62] Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Perception-ascontrol: Fine-grained controllable image animation with 3d-aware motion representation. arXiv:2501.05020, 2025. [63] Xiang Wang, Shiwei Zhang, Haonan Qiu, Ruihang Chu, Zekun Li, Yingya Zhang, Changxin Gao, Yuehuan Wang, Chunhua Shen, and Nong Sang. Replace anyone in videos. arXiv preprint arXiv:2409.19911, 2024. [64] Bin Xia, Jiyang Liu, Yuechen Zhang, Bohao Peng, Ruihang Chu, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamve: Unified instruction-based image and video editing. arXiv preprint arXiv:2508.06080, 2025. [65] Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro Rezende, Yasaman Haghighi, David Brüggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, et al. Gem: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2240422415, 2025. [66] Aram Davtyan, Sepehr Sameni, Björn Ommer, and Paolo Favaro. Cage: Unsupervised visual composition and animation for controllable video generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1616316171, 2025. [67] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [68] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv:2311.15127, 2023. [69] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [71] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv:2304.09151, 2023. [72] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. [73] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In ECCV, 2024. [74] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv:2210.02747, 2022. [75] JovianZM. Pexels-400k dataset. https://huggingface.co/datasets/jovianzm/Pexels-400k, 2024. [76] Yiming Ren, Zhiqiang Lin, Yu Li, Gao Meng, Weiyun Wang, Junjie Wang, Zicheng Lin, Jifeng Dai, Yujiu Yang, Wenhai Wang, et al. Anycap project: unified framework, dataset, and benchmark for controllable omni-modal captioning. arXiv preprint arXiv:2507.12841, 2025. [77] Dingdong Wang, Jin Xu, Ruihang Chu, Zhifang Guo, Xiong Wang, Jincenzi Wu, Dongchao Yang, Shengpeng Ji, and Junyang Lin. Inserter: Speech instruction following with unsupervised interleaved pre-training. arXiv preprint arXiv:2503.02769, 2025. [78] Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, and Helen Meng. Mmsu: massive multi-task spoken language understanding and reasoning benchmark. arXiv preprint arXiv:2506.04779, 2025. [79] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. [80] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. [81] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphal Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. In ICLR, 2019. [82] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. [83] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. [84] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024. [85] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [86] Pixabay. Discover and download free videos - pixabay, 2025. [87] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [88] PyTorch Team. Pytorch fully sharded data parallel (fsdp). https://pytorch.org/docs/stable/fsdp. html, 2021. Accessed: [Insert Date]. [89] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. [90] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. [91] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKU",
        "Tongyi Lab, Alibaba Group",
        "Tsinghua University"
    ]
}