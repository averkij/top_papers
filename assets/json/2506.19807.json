{
    "paper_title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
    "authors": [
        "Baochang Ren",
        "Shuofei Qiao",
        "Wenhao Yu",
        "Huajun Chen",
        "Ningyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL."
        },
        {
            "title": "Start",
            "content": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang* Zhejiang University Tencent AI Seattle Lab baochangren@gmail.com wenhaowyu@global.tencent.com {shuofei, huajunsir, zhangningyu}@zju.edu.cn 5 2 0 J 4 2 ] . [ 1 7 0 8 9 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities1."
        },
        {
            "title": "Introduction",
            "content": "DeepSeek-R1 (Guo et al., 2025) demonstrates that through meticulously designed reward functions, Reinforcement Learning (RL) can significantly enhance the reasoning ability of models. Slow-thinking models often exhibit severe hallucinations, particularly during extended chains of *Corresponding author. 1Our code is available at: https://github.com/zjunlp/ KnowRL. Figure 1: KnowRL reduces hallucinations in slowthinking models. thought or complex reasoning processes, where initial inaccuracies can become significantly amplified. This vulnerability is underscored by the performance of even large-scale models; for instance, the DeepSeek-R1-Distill-Qwen-32B (Guo et al., 2025) achieves an accuracy of only 6.64% on the SimpleQA (Wei et al., 2024) dataset. RL driven by outcome rewards often neglects the LLMs reasoning process, leading to correct outputs from flawed internal reasoning. Generalizing such incorrect reasoning patterns significantly increases hallucination risk. This issue is exacerbated in slow-thinking models trained via RL on mixed task types, partly because simple open-domain factual tasks and complex reasoning tasks (e.g., mathematics) have differing training requirements. Fundamentally, models lacking factual guidance during training may resort to any means to obtain rewards, resulting in highly unreliable thinking processes. Existing hallucination mitigation methods have limitations: Supervised Fine-Tuning (SFT; Ouyang et al., 2022) with high-quality data is costly and difficult to scale; Retrieval Augmented Generation (RAG; Lewis et al., 2020) encounters efficiency and fusion challenges in long Chain-Of-Thought (COT) reasoning; and decoding-stage interventions (Chuang et al., 2023) can disrupt strategies learned through RL. Crucially, these approaches do not consistently preserve strong reasoning capabilities while reducing hallucinations. To address this, we propose Knowledgeenhanced Reinforcement Learning (KnowRL), method that integrates factuality reward into the RL training process. This reward, inspired by FactScore (Min et al., 2023) and obtained by evaluating textual support against an external knowledge base, guides the model to perform fact-based slow thinking. KnowRL cooperatively optimizes reasoning and fact-following in slow-thinking models by helping them learn better knowledge boundaries to avoid fabricating facts, as illustrated in Figure 1. Our extensive experiments validate KnowRLs significant impact. For instance, distillation-based slow-thinking models, after KnowRL training, demonstrate leading accuracy on several hallucination benchmarks, including TruthfulQA (Lin et al., 2021) and SimpleQA, and importantly, they either improve or preserve their advanced reasoning skills on datasets like GPQA (Rein et al., 2024) and AIME 2025. Likewise, slow-thinking models initially developed through RL methodologies also reap considerable benefits, showing strong performance on ChineseSimpleQA (He et al., 2024) where accuracy reaches 16.23%, alongside enhanced reasoning capabilities post-KnowRL application. Across these evaluations, KnowRL consistently outperforms baseline methods in mitigating hallucinations while often elevating the models reasoning performance beyond their original levels. The main contributions of this paper are as follows: We introduce KnowRL, novel framework using factuality rewards from external knowledge to reduce hallucinations in LLMs while preserving their reasoning ability. We demonstrate through comprehensive experiments that KnowRL effectively mitigates hallucinations and maintains reasoning in diverse slow-thinking models. We contribute meticulously constructed, high-quality training dataset for factual tasks, crucial for initializing and training KnowRL."
        },
        {
            "title": "2 Hallucination in Slow-thinking Models",
            "content": "Slow-thinking models, often created via knowledge distillation to enhance complex reasoning capabilities, represent \"double-edged sword\" in the context of factuality. While their elongated, multi-step reasoning process is designed for thoroughness, it paradoxically creates more opportunities for factual deviation. Unlike human slow thinking, which relies on constantly queried factual memory, an LLMs reasoning chain often draws upon its own parametric knowledgean amalgam of fact and statistical artifact. This makes each step potential point of failure, where minor initial inaccuracy can \"snowball\" into confidently asserted, yet entirely fabricated, conclusion. The core issue is factual supervision gap created by current training paradigms. Distillation, for instance, teaches student model to replicate teachers reasoning patterns, but provides no explicit guarantee that the factual assertions within those patterns are correct. This results in models that are skilled at sounding logical but are not necessarily grounded in reality. This supervision gap manifests in range of well-documented, nuanced hallucination issues, including misrepresenting problem features (Heyman and Zylberberg, 2025), decrease in accuracy as reasoning depth increases (Patel et al., 2024), and the generation of unfaithful chain-of-thought processes that merely rationalize errors (Arcuschin et al., 2025). This factual supervision gap renders traditional post-training methods, particularly outcome-based RL, insufficient and potentially harmful. An RL agent rewarded only for correct final answer has no incentive to maintain factual integrity during its reasoning steps and may even learn to reinforce \"lucky\" chains of hallucination. Thus, enhancing the reasoning of slow-thinking models without simultaneously amplifying their capacity for factually ungrounded fabrication remains critical and unresolved challenge for current training paradigms."
        },
        {
            "title": "Learning",
            "content": "Given the significant hallucination issues in slowthinking models, we propose that more reliable mitigation method involves introducing knowledge to directly supervise the models thinking process during reinforcement learning. KnowRL is our proposed Knowledge-enhanced Reinforcement Learning framework; it mitigates hallucinations by integrating external knowledge and corresponding factuality reward directly into the RL training loop, as illustrated in Figure 2, thereby promoting more Figure 2: KnowRL framework. We begin by constructing the training dataset, followed by an initial cold-start SFT stage, and then conduct RL training guided by three distinct reward signals. reliable, fact-based reasoning. Its methodology involves careful data construction, composite reward signal, and two-stage training process. 3.1 Data Construction 3.1.1 RL Data Construction We use part of the data extracted from NqOpen (Kwiatkowski et al., 2019; Lee et al., 2019), as well as data from WebQuestions (Berant et al., 2013) and ComplexQuestions (Bao et al., 2016), as factual question data sources. Multiple De-duplication and Entropy Filtering. First, we filter questions that Qwen2.5-7B-Instruct (Team, 2024) answers correctly on its first attempt. The NqOpen dataset, due to its large size and less stringent prior cleaning, then undergoes de-duplication to ensure data diversity. We use Semhash on this filtered NqOpen data for both exact and semantic de-duplication, employing potionbase-8M (min, 2024) as the sentence embedding model for the semantic step. Finally, this deduplicated NqOpen data is combined with the initially filtered WebQuestions and ComplexQuestions data to form the filtered data. Refinement and Entity Extraction. In this step, we use GPT-4o and predefined rules to check the data from step 1, excluding questions that are informal, lack sufficient information for reasoning, or use informal language. Guided by carefully designed prompts (see Appendix A), GPT-4o accepts or rejects items and, for those accepted, normalizes, corrects, and extracts entities. To ensure manageable training difficulty, DeepSeek-R1 then further filters this data, retaining only questions it answers correctly on its first attempt; this process also provides the long COT data needed for SFT baseline training. Wikipedia-grounded Data Filtering. This step filters out data that cannot be matched to our knowledge source, Wikipedia2 (6.4 million entries). Based on each data items entities, we retrieve corresponding Wikipedia entriesrequiring an exact entity match or full containment within Wikipedia entry title for successful link. We then add their textual content to our knowledge base. To avoid redundancy, each entity keyword links to no more than three Wikipedia entries, and items with entities failing to match any entry are filtered out. Additionally, to ensure SFT training stability, we retain only the 3,177 data items where DeepSeekR1s output length is between 300 and 700 tokens, thus completing the training data construction. 3.1.2 Cold Start Data Construction We process another portion of NqOpen data using the same filtering method as our RL training data construction: employing DeepSeek-R1 to dis220231101.en, https://dumps.wikimedia.org/. till long CoT responses while retaining only firstattempt correct answers. The filtered outputs (300700 tokens) that mismatch RL training knowledge become our cold-start dataset, yielding 1,798 qualified entries. 3.2 Reward Function Design We design special reward function to guide the model in generating outputs that are not only factual in the reasoning process but also provide accurate final answers. The final reward for model output is denoted as Rtotal(o). Format Reward. The format reward is to justify the specific <think>othink</think> and <answer>oanswer</answer> structure, where othink denote the thinking process and oanswer denote the final answer. Rformat(o) = (cid:40) +1 if has valid format 1 otherwise . (1) Correct Reward. The correct reward evaluates the accuracy of the final answer oanswer, which is assessed by the GPT-4o-mini. final answer is incorrect: Rcombined(o) = (cid:40) +2 1 + Rfact(o) if oanswer is correct if oanswer is incorrect . (4) This ensures that even if an answer is marked incorrect, the model can still receive relatively positive reward based on the factuality of its thought. Finally, the total reward is defined as: Rtotal(o) = Rformat(o) + Rcombined(o). (5) 3.3 RL Training. We first cold-start the model with SFT to establish more stable foundation for RL training. For the subsequent RL stage, we employ the Group Relative Policy Optimization (GRPO; Shao et al., 2024) algorithm on our own constructed dataset. The GRPO process optimizes the models policy using combination of correct reward and our designed fact reward. This fact-based reward guidance is crucial, as it teaches the model to ground its reasoning in facts, thereby effectively mitigating hallucination while maintaining or enhancing its reasoning capabilities4. Rcorrect(o) = (cid:40) +2 1 if oanswer is correct otherwise ."
        },
        {
            "title": "4 Experiments",
            "content": "(2) 4.1 Experimental Settings. Fact Reward. The fact reward quantifies the factuality of the reasoning process articulated in othink. We achieve this by using FactScore, which decomposes othink into atomic facts and verifies their support against an external knowledge base K. We use the Wikipedia text matched with each entry in the training data to construct K. We first utilize GPT4o-mini to decompose the input text into several atomic facts. Subsequently, we use an NLI model3 to determine if each atomic fact is supported by text retrieved from the K. Nsupported(othink, K) is the number of atomic facts in othink supported by K. Rfact(o) = min (cid:18) Nsupported(othink, K) 15 (cid:19) , 1.0 . (3) This encourages the model to ground its reasoning in verifiable facts. The Rcombined(o) is then formulated to avoid excessively penalizing factual reasoning when the 3https://huggingface.co/MoritzLaurer/ DeBERTa-v3-base-mnli-fever-anli. Datasets and Metrics. We use TruthfulQA, SimpleQA, and ChineseSimpleQA to evaluate hallucination, and GPQA (general domain) and AIME 2025 (mathematical reasoning) to evaluate reasoning ability. For evaluation, TruthfulQA is assessed with the Bleu metric. For SimpleQA, ChineseSimpleQA, and AIME 2025 datasets, we employ GPT4o-mini to judge the correctness of the model outputs, with the special condition that SimpleQA accuracy is calculated only on non-rejected answers. Lastly, for the GPQA dataset, we test exclusively on the diamond category data, extracting the answer from specific output format to determine its correctness. All evaluations are conducted using OpenCompass (Contributors, 2023). We evaluate all models with the temperature set to 0. Models and Baselines. We select the SkyworkOR1-7B-Preview (He et al., 2025) and the DeepSeek-R1-Distill-Qwen-7B for experiments. These models represent the two most popular slow 4Qwen3 (Yang et al., 2025) is also trained through RL with multiple reward combinations. In the same period, we independently propose KnowRL and construct our own training data for factual tasks. We will open source our training data. Methods Direct Generation Self-Refine FactTune-FS DPO SFT KnowRL-Zero KnowRL Direct Generation Self-Refine FactTune-FS DPO SFT KnowRL-Zero KnowRL Hallucination Reasoning TruthfulQA SimpleQA ChineseSimpleQA GPQA Diamond AIME 2025 34.39 30.00 34.76 34.64 38.43 34.39 37.70 33.05 33.90 35.50 36.96 37.09 33.54 38.19 Skywork-OR1-7B-Preview 5.19 2.90 5.33 4.60 4.90 5.55 4.58 13.27 11.57 12.30 15.57 14.87 15.17 16.23 DeepSeek-R1-Distill-Qwen-7B 1.92 2.60 2.46 3.21 4.12 1.34 4.56 11.17 9.97 10.27 12.33 12.70 11.00 13.90 31.31 46.97 32.83 34.34 32.32 30.30 34.34 34.34 45.45 36.87 37.37 30.30 34.34 37.37 6.67 36.67 6.67 6.67 16.67 10.00 10.00 16.67 33.33 26.67 13.33 16.67 6.67 13.33 Table 1: Main Experiment. KnowRL-Zero is method of RL training without cold-start training. KnowRL represents RL training on cold-start trained model. The best results are marked in bold. Figure 3: KnowRL Training Curve. The Combined Reward is Rcombined. thinking training methods: RL, represented by the Skywork-OR1-7B-Preview model, and distillation, represented by the DeepSeek-R1-Distill-Qwen-7B model. We select Self-Refine (Madaan et al., 2023) as the baseline for prompt engineering, and SFT, DPO, and FactTune-FS (Tian et al., 2023) as the baselines for post-training methods. For DPO, we use the distilled DeepSeek-R1 data as the chosen data; for FactTune-FS, the chosen data is composed of the original models outputs, which are filtered for high factuality using FactScore. Training Details. We use Low-Rank Adaptation (LoRA; Hu et al., 2022) to train all models. For the reinforcement learning training stage, we train for 150 steps, with the learning rate set to 1.0e-5. More details can be seen in Appendix C. 4.2 Main Results KnowRL effectively mitigates hallucination while maintaining the reasoning ability of slow-thinking models. As shown in Table 1, KnowRLs performance improves significantly. The DeepSeek-R1-Distill-Qwen-7B trained by KnowRL achieves the highest accuracy on 3 hallucination evaluation datasets. Simultaneously, its accuracy on GPQA datasets not only doesnt decrease but even improves compared to the original model. Regarding the AIME 2025 dataset, given its small data size and the slight decrease in score, we consider the models mathematical reasoning ability to be maintained at its original level. Similarly, the Skywork-OR1-7B-Preview trained by KnowRL also achieves top accuracy on ChineseSimpleQA (16.23%) and performs near the highest on the SimpleQA and TruthfulQA datasets. Furthermore, this model also shows improved accuracy on the GPQA and AIME 2025 datasets compared to its original version. This indicates that KnowRL is effective in mitigating hallucination and can also enhance the existing strong reasoning ability of both distillationbased and RL-trained models. Reward Hallucination Reasoning Rf ormat Rf act Rcorrect TruthfulQA SimpleQA ChineseSimpleQA GPQA Diamond AIME 2025 DeepSeek-R1-Distill-Qwen-7B 33.41 34.03 34.76 33.54 2.36 1.55 1.74 1.34 10.47 9.70 10.97 11. DeepSeek-R1-Distill-Qwen-7B-ColdStart 36.84 37.82 37.82 38.19 14.37 14.60 13.80 13.90 3.73 6.16 4.04 4.56 35.86 31.31 37.88 34.34 35.86 35.86 33.33 37.37 10.00 16.67 20.00 6. 16.67 13.33 16.67 13.33 Table 2: Reward Ablation Experiment. DeepSeek-R1-Distill-Qwen-7B is trained with cold start to obtain DeepSeek-R1-Distill-Qwen-7B-ColdStart. The best results are marked in bold. For the reward components, indicates inclusion and indicates exclusion. Figure 4: Performance of the Skywork-OR1-7B-Preview and DeepSeek-R1-Distill-Qwen-7B on 5 datasets with different number of training steps. ColdStart training is necessary for factual tasks. the Skywork-OR1-7BAs shown in Table 1, Preview trained by KnowRL-Zero only performs better on the SimpleQA dataset; on the other datasets, its performance decreases compared to the original model. The same situation occurs with the DeepSeek-R1-Distill-Qwen-7B. This indicates that within limited number of steps, RL training struggles to independently discover correct reasoning paths without an initial SFT phase. This conclusion coincides with the findings of Gandhi et al. (Gandhi et al., 2025), who also emphasize that cold-start or initial warm-up phase is vital for effective subsequent reinforcement learning. Therefore, for open-domain factual tasks, performing ColdStart SFT first is highly necessary. OR1-7B-Previews reward curve during KnowRL training converges to only approximately -0.4. This indicates better training performance for distillation-based models like DeepSeek-R1-DistillQwen-7B. This is further supported by Table 1, where the KnowRL-trained DeepSeek-R1-DistillQwen-7B outperforms all baselines on three hallucination evaluation datasets, while the KnowRLtrained Skywork-OR1-7B-Preview only exceeds all baselines on the ChineseSimpleQA dataset. KnowRLs training effect is less pronounced on the initially RL-trained Skywork-OR1-7B-Preview model compared to the distilled DeepSeek-R1Distill-Qwen-7B. This suggests that RL-trained models might approach capability ceiling due to converging to local optimum during their initial RL process, making further improvement by KnowRL difficult due to search space constraints. The findings from Reinforcement Learning with Verifiable Rewards (RLVR; Yue et al., 2025) support the conclusion that distilled models possess higher performance ceiling. This is because, unlike methods that merely improve sampling efficiency Distillation-based slow-thinking models exhibit higher training ceiling compared to slowthinking models trained by RL. The training curves in Figure 3 show DeepSeek-R1-DistillQwen-7B achieving higher combined reward during KnowRL training, with its output length decreases around 110 steps. In contrast, Skyworkby narrowing the reasoning search space, distillation allows models to learn genuinely new knowledge from an unreduced search space, enabling more significant enhancement via subsequent RL. Thus, our method effectively reduces hallucination in distilled models, demonstrating its practical utility, and also confirms that stacked RL training can help mitigate hallucinations. 4.3 Ablation Study To explore the impact of RL training with different reward combinations on model reasoning ability and hallucination, we evaluate the RL performance of DeepSeek-R1-Distill-Qwen-7B and DeepSeekR1-Distill-Qwen-7B-ColdStart under different reward combinations. Cold start training is crucial for motivating the fact reward signals effectiveness. As shown in Table 2, applying the Format+Fact reward to the DeepSeek-R1-Distill-Qwen-7B-ColdStart yields notable gains across multiple hallucination evaluation datasets when compared to results obtained using only the Format reward. In contrast, for the DeepSeek-R1-Distill-Qwen-7B, incorporating the Fact reward alongside the Format reward leads to only minor improvements on one metric and decreased performance on others, relative to using the Format reward alone. This significant difference in outcomeswhere substantial positive gains from the fact reward are observed only after ColdStartstrongly indicates that ColdStart training is vital to motivate or unlock the beneficial effects of the fact reward signal for mitigating hallucinations. SFT pre-aligns the model with desired factual behaviors, making specialized fact rewards more informative and interpretable, which in turn improves the models sample efficiency in learning to achieve factual outcomes during RL. The superposition of rewards can lead to suboptimal outcomes due to conflicting optimization pressures. Our ablation study demonstrates that simply aggregating rewards can be counterproductive. As shown in Table 2, augmenting the highperforming Format+Correct combination with an additional Fact reward on the DeepSeek-R1-DistillQwen-7B model severely degrades its advanced reasoning capabilities. This is starkly evidenced by the GPQA Diamond accuracy falling from 37.88% to 34.34% and, most dramatically, the AIME 2025 score plummeting from 20.00% to mere 6.67%. This performance drop stems from the Fact reward introducing conflicting optimization goal that encourages reward hacking and over-constrains the model. In essence, forcing rigid process (how to reason) obstructs the achievement of correct outcome (what to answer), demonstrating that targeted signals are superior to simple superposition. Correct reward boosts reasoning but Not factuality. Our ablation study reveals distinct trade-off with the correctness reward. As shown in Table 2, reward combinations involving Correct rewards significantly enhance reasoning, pushing the DeepSeek-R1-Distill-Qwen-7B model to top accuracy on the GPQA and AIME 2025 datasets. However, this potent stimulus for reasoning does little to mitigate hallucinations, with the model showing unremarkable performance on the factuality benchmarks. This suggests the model learns to produce correct answers without factual reasoning process, which contrasts with findings in mathematical reasoning where models can still gain performance by capitalizing on spurious signals (Shao et al., 2025). Therefore, we conclude that relying on outcome-based correctness alone is insufficient for mitigating hallucinations, underscoring the need for more comprehensive approach like the full KnowRL framework. 4.4 Training Step Analysis To explore the impact of different training steps of KnowRL on the reasoning ability and hallucination of slow-thinking models, we test both the SkyworkOR1-7B-Preview and the DeepSeek-Distill-Qwen7B trained by KnowRL for 100, 150, and 200 steps. The results are shown in Figure 4. Our analysis of training steps reveals clear trade-off between factual accuracy and reasoning ability. Both models generally achieve optimal hallucination mitigation around 150 training steps. Extending training to 200 steps often leads to decline in performance on factual datasets like TruthfulQA and SimpleQA. Concurrently, reasoning ability can continue to improve, as seen with the DeepSeek-Distill-Qwen-7B model, whose GPQA performance peaks at 200 steps even as its factuality on other metrics declines. This behavior can be attributed to several factors: simple factual tasks may require fewer training steps than complex reasoning, and prolonged mixed-task training can lead to overfitting on data artifacts (Berglund et al., 2023), which harms generalization. This prioritizes reasoning gains at the expense of peak factual accuracy. This phenomenon can be accompanied by progressive shift towards enhancing reasoning ability on other tasks, notably illustrated by the DeepSeek-Distill-Qwen-7B model, to improve and achieves its peak performance on the GPQA reasoning dataset at 200 training steps, even as its performance on some hallucination metrics declines at this stage. Therefore, we conclude that Overtraining is key factor causing high hallucination in slow-thinking models. 4.5 Cold Start Data Size Analysis To analyze the impact of ColdStart data size, we expanded the dataset to 3K items and conducted experiments on the DeepSeek-R1-Distill-Qwen-7B model, with results shown in Figure 5. Figure 5: Performance of DeepSeek-R1-Distill-Qwen7B after ColdStart training with 1.8K and 3K data sizes. The experiment reveals distinct trade-off. While increasing the ColdStart data size from 1.8K to 3K did not significantly improve SFT performance and resulted in less pronounced gains on hallucination datasets, it reversed the trend for reasoning tasks. Notably, after KnowRL training, the 3K version showed improved accuracy on GPQA and AIME 2025 over its ColdStart counterpart, whereas the 1.8K version showed decrease. This pattern indicates that larger volume of cold-start data biases the subsequent RL training phase towards enhancing reasoning ability. Therefore, we can draw conclusion: Excessive cold start data size leads to RL training shifting its focus towards improving reasoning ability."
        },
        {
            "title": "5 Related Work",
            "content": "Hallucination Mitigation. central challenge in LLM development is that increased reasoning ability often paradoxically leads to more severe hallucinations (Yao et al., 2023). This tendency is especially problematic in \"slow-thinking\" models, where capacity for complex reasoning is intertwined with higher propensity for factual errors that can amplify in \"snowball effect\" (Ji et al., 2023; Huang et al., 2025; Zhang et al., 2023; Cheng et al., 2025; Yao et al., 2025; Zheng et al., 2025; Zhang et al., 2025a). The causes for these hallucinations are diverse, including noise in training data (Mündler et al., 2023), unintended consequences of reinforcement learning (Song et al., 2025; Ouyang et al., 2022), conflicts where internal knowledge \"overshadows\" factual context (Zhang et al., 2024, 2025b), and decoding uncertainties (Kuhn et al., 2023). Fundamentally, these issues stem from core failure of LLMs to recognize their own knowledge boundaries (Zhang, 2023; Tonmoy et al., 2024; Liang et al., 2024; Manakul et al., 2023; Kadavath et al., 2022), which has spurred research into solutions like knowledge-aware optimization (Chen et al., 2022; Chen, 2023) and alignment for honesty (Yang et al., 2024). RL for Reasoning RL enhances LLM reasoning, enabling complex strategies like reflection and verification (Xie et al., 2025; Yeo et al., 2025). To achieve more reliable reasoning (Mei et al., 2025), recent work increasingly focuses on supervising the reasoning process itself. Fine-grained guidance methods, such as step-level value preference optimization (Chen et al., 2024) and tree search (Feng et al., 2023), are proving valuable for enhancing model reliability."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper studies the high levels of hallucination in both distillation-based and RL-trained slowthinking models. We analyze how the current outcome-reward-driven RL paradigm, despite enhancing reasoning, fails to ensure fact-based thinking. To address this, we propose KnowRL, knowledge-enhanced RL training method, and validate its effectiveness on multiple datasets. Our findings demonstrate that directly supervising the models thinking process with factual rewards is more robust strategy for building reliable models than solely optimizing for correct final answers. It should be noted that KnowRL represents preliminary exploration into mitigating hallucinations in reasoning models through subsequent post-training. We hope KnowRL offers the community an effective technical pathway to mitigate hallucinations in these slow-thinking models."
        },
        {
            "title": "Limitations",
            "content": "Despite our best efforts, this work still has certain limitations that point to promising directions for future exploration and improvement. Fundamental Mechanism Studies. Further research is needed to investigate core theoretical questions, such as determining the performance ceiling of RL and identifying scenarios where RL approaches may not be optimal. Particularly, the underlying mechanisms behind multilingual hybrid reasoning phenomena warrant in-depth study, as such understanding could significantly advance interpretability research for slow-thinking models and reveal fundamental insights about the nature of chain-of-thought reasoning. Efficiency Challenges. The current KnowRL framework faces efficiency limitations due to its reliance on knowledge base retrieval and frequent API calls during training, which inevitably impacts computational performance. Future work should explore more efficient methods for factual verification to address these bottlenecks while maintaining model accuracy. Scalability. An important direction is investigating how to extend the KnowRL paradigm to: Larger-scale model architectures. More diverse task domains. Broader application scenarios. This expansion would test the frameworks generalizability while potentially unlocking new capabilities in knowledge-grounded reasoning systems."
        },
        {
            "title": "References",
            "content": "2024. Model2vec: Turn any sentence transformer into small fast model. Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Chain-of-thought reasoning in Conmy. 2025. arXiv preprint the wild is not always faithful. arXiv:2503.08679. Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. 2016. Constraint-based question answering with knowledge graph. In Proceedings of COLING 2016, the 26th international conference on computational linguistics: technical papers, pages 25032514. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 15331544. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse: Llms trained on\" is b\" fail to learn\" is a\". arXiv preprint arXiv:2309.12288. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024. Step-level value preference optimization for mathematical reasoning. arXiv preprint arXiv:2406.10858. Huajun Chen. 2023. Perspectives and challenges. arXiv:2312.02706. Large knowledge model: arXiv preprint Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022. Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction. In Proceedings of the ACM Web conference 2022, pages 27782788. Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2025. Think more, hallucinate less: Mitigating hallucinations via dual process of fast and slow thinking. arXiv preprint arXiv:2501.01306. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting layers improves factuarXiv preprint ality in large language models. arXiv:2309.03883. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. 2025. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou. 2025. Skywork open reasoner series. Notion Blog. Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. 2024. Chinese simpleqa: chinese factuality evaluaarXiv preprint tion for large language models. arXiv:2411.07140. Alex Heyman and Joel Zylberberg. 2025. Reasoning large language model errors arise from hallucinating critical problem features. arXiv preprint arXiv:2505.12151. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):1 55. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):138. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Language models Tran-Johnson, et al. 2022. arXiv preprint (mostly) know what they know. arXiv:2207.05221. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. arXiv preprint arXiv:1906.00300. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474. Yuxin Liang, Zhuoyang Song, Hao Wang, and Jiaxing Zhang. 2024. Learning to trust your feelings: Leveraging self-awareness in llms for hallucination mitigation. arXiv preprint arXiv:2401.15449. Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594. Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896. Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xing Gao, Yu Yang, Chengjun Xie, et al. 2025. o2-searcher: searching-based agent model for open-domain arXiv preprint open-ended question answering. arXiv:2505.16582. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251. Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. arXiv preprint arXiv:2305.15852. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, and Chitta Baral. 2024. Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models. arXiv preprint arXiv:2406.17169. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. 2025. Spurious rewards: Rethinking training signals in rlvr. Notion Blog. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Linxin Song, Taiwei Shi, and Jieyu Zhao. 2025. The hallucination tax of reinforcement finetuning. arXiv preprint arXiv:2505.13988. Mike Zhang, Johannes Bjerva, and Russa Biswas. 2025a. Scaling reasoning can improve factuality in large language models. arXiv preprint arXiv:2505.11140. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah Smith. 2023. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534. Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi Fung, Jing Li, Manling Li, and Heng Ji. 2024. Knowledge overshadowing causes amalgamated hallucination in large language models. arXiv preprint arXiv:2407.08039. Yuji Zhang, Sha Li, Cheng Qian, Jiateng Liu, Pengfei Yu, Chi Han, Yi Fung, Kathleen McKeown, Chengxiang Zhai, Manling Li, et al. 2025b. The law of knowledge overshadowing: Towards understanding, predicting, and preventing llm hallucination. arXiv preprint arXiv:2502.16143. Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, and Kai Yu. 2025. Enhancing llm reliability via explicit knowledge boundary modeling. arXiv preprint arXiv:2503.02233. Qwen Team. 2024. Qwen2.5: party of foundation models. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher Manning, and Chelsea Finn. 2023. Finetuning language models for factuality. In The Twelfth International Conference on Learning Representations. SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313, 6. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2024. Alignment for honesty. Advances in Neural Information Processing Systems, 37:6356563598. Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Yu-Yang Liu, and Li Yuan. 2023. Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469. Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. 2025. Are reasoning models more prone to hallucination? arXiv preprint arXiv:2505.23646. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. 2025. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837. Chen Zhang. 2023. User-controlled knowledge fusion in large language models: Balancing creativity and hallucination. arXiv preprint arXiv:2307.16139."
        },
        {
            "title": "A Prompts",
            "content": "Prompt Used by the GPT-4O for Data Filtering You are an entity extraction assistant that identifies key entities in questions. TASK: 1. First normalize the query by properly capitalizing names, titles, and other named entities 2. Determine if the query has sufficient context to be answered meaningfully 3. Extract only the most important entities from the query that are essential for answering it RULES: 1. Extract MAXIMUM of 2 specific entities (people, places, objects, works, etc.) 2. Output the MOST important entity first, then the secondary entity (if any) 3. Extract precise named entities, not general concepts or phrases 4. Keep related entities together as single entity (e.g., character names with their roles) 5. Return individual entities rather than relationships or possessive forms 6. Only extract truly representative entities - ignore generic terms that dont specifically define the query 7. Only REJECT queries that meet the rejection criteria below ONLY reject queries in these specific cases: 1. When the entity in the query is completely ambiguous (e.g., \"Who is that person?\") 2. When the query lacks necessary qualifying information (e.g., \"Who will win?\" with no mention of what contest) 3. When the query is too vague to determine its intent (e.g., \"What happened to him?\") 4. When the query is time-sensitive and contains temporal references like \"now\", \"current\", \"latest\", \"recent\", etc. 5. When the query lacks sufficient information to determine single definitive answer, potentially leading to multiple correct interpretations or answers 6. Be careful not to extract purely numerical information such as year as an entity Note: Queries with historical context, pop culture references, geographical locations, or other well-defined entities should be ACCEPTED. EXAMPLES: Example 1: Original Query: \"who played barbara gordon batgirl?\" Normalized Query: \"Who played Barbara Gordon Batgirl?\" Output: Normalized Query: \"Who played Barbara Gordon Batgirl?\" Entities: [\"Barbara Gordon Batgirl\"] NOT: [\"Barbara Gordon\", \"Batgirl\"] - This is incorrect because \"Barbara Gordon Batgirl\" is single character entity. Example 2: Original Query: \"what continent does armenia belong to?\" Normalized Query: \"What continent does Armenia belong to?\" Output: Normalized Query: \"What continent does Armenia belong to?\" Entities: [\"Armenia\"] NOT: [\"Armenia\", \"continent\"] - The term \"continent\" is generic category, not specific entity representative of this query. Example 3: Original Query: \"who is niall fergusons wife?\" Normalized Query: \"Who is Niall Fergusons wife?\" Output: Normalized Query: \"Who is Niall Fergusons wife?\" Entities: [\"Niall Ferguson\"] Example 4: Original Query: \"who was the italian leader in ww1?\" Normalized Query: \"Who was the Italian leader in WW1?\" Output: Normalized Query: \"Who was the Italian leader in WW1?\" Entities: [\"Italian leader\", \"WW1\"] Example 5: Original Query: \"who will play mr gray in the film?\" Normalized Query: \"Who will play Mr. Gray in the film?\" Output: Normalized Query: \"Who will play Mr. Gray in the film?\" REJECT (insufficient context - which film?) Example 6: Original Query: \"who is in charge of libya now?\" Normalized Query: \"Who is in charge of Libya now?\" Output: Normalized Query: \"Who is in charge of Libya now?\" REJECT (time-sensitive query with temporal reference \"now\") Example 7: Original Query: \"what did werner heisenberg discover?\" Normalized Query: \"What did Werner Heisenberg discover?\" Output: Normalized Query: \"What did Werner Heisenberg discover?\" REJECT (lacks sufficient specificity - Heisenberg made multiple discoveries) Please try to output in this format: Normalized Query: \"The normalized version of the query\" Entities: [\"entity1\", \"entity2\"] If you need to reject, still include the normalized query: Normalized Query: \"The normalized version of the query\" REJECT (reason for rejection) Extract key entities from this query: \"query\""
        },
        {
            "title": "B Case Analysis",
            "content": "To explore KnowRLs impact on reasoning in factual tasks, we analyzed reasoning case from the KnowRL training process, detailing the models emergent reasoning behaviors. As shown in Figure 6, these include Mixed-Language Reasoning, Knowledge Anchoring, Cross-verification, and Key Information Extraction. For simple factual tasks, distinct from mathematical reasoning, the model typically first proposes an initial answer (knowledge anchoring) and subsequently verifies this initial answer through reasoning behaviors such as reflection and cross-verification. This observed process aligns with human cognitive approaches when facing factual tasks, which further suggests the suitability of an outcome-based, reward-driven reinforcement learning training paradigm for open-domain factual tasks. Figure 6: Case analysis of the KnowRL training process."
        },
        {
            "title": "C Training Setups",
            "content": "We are training two 7B models, DeepSeek-R1-Distill-Qwen-7B and Skywork-OR1-7B-Preview, with Reinforcement Learning on 1A800. Parameter DeepSeek-R1-Distill-Qwen-7B Skywork-OR1-7B-Preview SFT Training Hyperparameters epoch cutoff_len lora_rank lora_alpha per_device_train_batch_size gradient_accumulation_steps learning_rate num_train_epochs lr_scheduler_type warmup_ratio fp16 lora_rank lora_alpha torch_dtype per_device_train_batch_size gradient_accumulation_steps learning_rate beta lr_scheduler_type warmup_ratio vllm_gpu_memory_utilization optim 4 3072 256 512 2 1 1.0e-4 4.0 cosine 0.1 true RL Training Hyperparameters 256 512 bfloat16 24 4 1.0e-5 0.001 cosine 0.03 0.5 adamw_8bit 4 3072 256 512 2 1 1.0e-4 4.0 cosine 0.1 true 128 256 bfloat16 18 4 1.0e-5 0.01 cosine 0.03 0.5 adamw_8bit Table 3: Hyperparameter settings for SFT and RL training of models."
        },
        {
            "title": "D ColdStart SFT Overtraining",
            "content": "To investigate the effects of prolonged ColdStart SFT, DeepSeek-R1-Distill-Qwen-7B was trained for 1200 steps in this initial stage, creating the DeepSeek-R1-Distill-Qwen-7B-ColdStart1200steps model. Examination of this models 50-step RL training curve, presented in Figure 7, reveals key challenge: the model exhibits significant difficulty in learning even the most basic format rewards within this RL timeframe. This observation suggests that over-training during the cold-start SFT phase can seriously diminish the efficacy of the ensuing reinforcement learning phase. Figure 7: DeepSeek-R1-Distill-Qwen-7B-ColdStart1200steps Training Curve."
        }
    ],
    "affiliations": [
        "Tencent AI Seattle Lab",
        "Zhejiang University"
    ]
}