{
    "paper_title": "SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts",
    "authors": [
        "Gengze Zhou",
        "Yicong Hong",
        "Zun Wang",
        "Chongyang Zhao",
        "Mohit Bansal",
        "Qi Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the latter concentrates on following detailed textual commands. Despite the differing focuses of these tasks, the underlying requirements of interpreting instructions, comprehending the surroundings, and inferring action decisions remain consistent. This paper consolidates diverse navigation tasks into a unified and generic framework -- we investigate the core difficulties of sharing general knowledge and exploiting task-specific capabilities in learning navigation and propose a novel State-Adaptive Mixture of Experts (SAME) model that effectively enables an agent to infer decisions based on different-granularity language and dynamic observations. Powered by SAME, we present a versatile agent capable of addressing seven navigation tasks simultaneously that outperforms or achieves highly comparable performance to task-specific agents."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 2 5 5 5 0 . 2 1 4 2 : r SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts Gengze Zhou1 Yicong Hong2 Zun Wang3 Chongyang Zhao4 Mohit Bansal3 Qi Wu 1The University of Adelaide 2Adobe Research 3UNC, Chapel Hill 4UNSW Sydney {gengze.zhou, qi.wu01}@adelaide.edu.au https://github.com/GengzeZhou/SAME"
        },
        {
            "title": "Abstract",
            "content": "The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the latter concentrates on following detailed textual commands. Despite the differing focuses of these tasks, the underlying requirements of interpreting instructions, comprehending the surroundings, and inferring action decisions remain consistent. This paper consolidates diverse navigation tasks into unified and generic framework we investigate the core difficulties of sharing general knowledge and exploiting task-specific capabilities in learning navigation and propose novel State-Adaptive Mixture of Experts (SAME) model that effectively enables an agent to infer decisions based on different-granularity language and dynamic observations. Powered by SAME, we present versatile agent capable of addressing seven navigation tasks simultaneously that outperforms or achieves highly comparable performance to task-specific agents. 1. Introduction Acquiring the capability to understand natural language commands and navigate in unfamiliar environments constitutes fundamental competency for embodied intelligence. In recent years, great variety of navigational tasks has emerged, each defined by distinct navigation objectives, from broad, high-level goals [8, 126] to detailed, low-level directives [6, 52, 80, 99, 125], highlighting exploration and instruction-following, respectively. However, these tasks are mostly formulated as isolated research problems, and the specialized methods developed for each are typically not generalizable to others (Figure 1a). For example, structured memory tailored for efficient target object exploration [10, 11, 86], contextual guidance for vague instructions [30, 65, 95], and episodic vision-language alignment for instruction following [28, 38, 98, 103]. Subsequent works leverage generic vision-language representations [18, 59, 61, 96, 97] to pretrain vision-language-action policies [14, 16, 32, 34, 36, 60, 74, 81] (Figure 1b), finetuning parameters for specific tasks while maintaining the same model architecture. In this paper, we argue that the essential difference between these tasks lies in the granularity of instruction, and the learning problems should be unified under the broader concept of language-guided visual navigation (VLN), where the overarching goal is to create versatile system that can interpret and execute arbitrary language instructions (Figure 1c). Apart from the practical perspective, in which the agents capability should not be constrained by the users instruction format, unifying navigational problems can potentially create an effective learning paradigm that enables training an agent on larger amount of data covering wider skill sets. An intuitive approach towards this is to integrate distinct navigational datasets and use them to train an existing agent for multitasking; however, our study reveals that simply mixing data yields inconsistent performance variation across tasks due to conflicting learning objectives (2.2). As result, we resort to more adaptive framework capable of sharing general knowledge and retrieving task-specific skills to infer appropriate navigation decisions. Inspired by recent success in the Mixture of Experts (MoE) [41, 43, 93] approach for natural language processing, in particular, enhancing the transformer-based Large Language Models [21, 42, 111], we incorporate MoE to build our generalizable navigation agent. Unlike previous works that typically implement task-wise or token-wise MoE, which we found ineffective in multi-task navigation learning, we propose novel MoE formulation for sequential embodied agents in which the experts are selected based on the agents state (i.e., attended language and visual observation at certain timestep, see 3.3). We further show that applying MoE on visual queries gives better results than applying it on feed-forward networks as in LLMs, aligning 1 Figure 1. We consolidate diverse navigation tasks into unified language-guided navigation framework sorted by language granularity. Previous approaches utilize task-specific designs tailored to address particular types of language instructions, as shown in (a) and (b). In contrast, we propose versatile system that can interpret and execute arbitrary language instructions as shown in (c). with the fact that many navigational agents rely on multiview visual attention for decision-making. We termed this method State-Adaptive Mixture of Experts, or SAME, suggesting applying the same model to solve wide range of navigation problems. Powered by SAME, we train versatile agent across seven major language-guided navigation tasks, including R2R [6], RxR-EN [53], REVERIE [80], OBJECTNAV1, CVDN [99], SOON [125], and R2R-CE [50], achieving state-of-the-art or highly comparable performance to models tailored for single task. 2. Background In this section, we begin by introducing the formulation of language-guided visual navigation tasks under varying levels of language granularity. We employ DUET [16] to illustrate general cross-modality navigation model for language-guided navigation. Through series of contrastive experiments, we analyze the interconnections among these navigation tasks, understanding their underlying contradictions and providing proof of concept for our approach. {s0, a0, s1, a1, . . . , sT , aT } to navigate to the target position vT , as specified by the instruction W. Each action at transit the agent from the current state st = vt, θt, ϕt to st+1 = vt+1, θt+1, ϕt+1 which includes its spatial location vt V, heading angle θt, and elevation angle ϕt, and generate new visual observation Ot. Additionally, the agent maintains record of the state history ht and adjusts the conditional transition probability between states, defined as St = (st+1 at, st, ht, O, W), where denotes the conditional transition probability distribution. We categorize the language instruction into three classes by granularity as: Fine-grained VLN: describes the sequence of actions {s0, a0, s1, a1, . . . , sT , aT } step-by-step. Coarse-grained VLN: refers to an out-of-sight target at vT , e.g., the cold tap in the first bedroom on level two. Zero-grained VLN: refers to single term indicating the target (e.g., an object category in OBJECTNAV). 2.1. Navigation Tasks Formulation Given an instruction represented by sequence of word embeddings = {wi}L i=1, an agent navigates on predefined undirected graph = V, E, where represents the navigable nodes and represents the connectivity edges. The agent is expected to execute sequence of actions 1We consider Object-Goal Navigation [8] as form of zero-grained language-guided navigation, as will be specified in 2.1. Multimodal Navigation Policy At each step, the agent receives local visual observation Ot = {oi}36 i=0, consisting of 36 view images, and language instruction W. These are encoded separately by vision encoder and language encoder into visual feature ˆOt and language feature ˆW. DUET [16] incorporates ˆOt and agents state st to obtain node embedding ˆVt and maintain topological map ˆGt = { ˆVi}it as navigation history, details are provided in the supplementary. local cross-modal encoder is utilized 2 to excite visual features conditioned on language features: CrossAttn( ˆOt, ˆW) = Softmax (cid:32) ˆOtWq( ˆWWk)T (cid:33) ˆWWv, (1) The output embedding from the final layer of view oi is denoted as ˆo i. Similarly, parallel global cross-modal encoder is implemented to encode language-conditioned map ˆGt = CrossAttn( ˆGt, ˆW). Denote the output embedding of node Vi as ˆv i, the navigation score given by the local and global cross-modal encoder is calculated as: = FFNg(ˆv = FFNl(ˆo sl si = σtsl i), sg + (1 σt)sg where FFN is two-layer feed-forward network and σ is learnable parameter. Datasets We select three typical datasets for our contrastive experiments based on instruction granularity: R2R [6]: The fine-grained VLN task which consists of 22k human-annotated navigational instructions. On average, an instruction contains 32 words, and the groundtruth path is formed by 7 steps, totaling 10 meters. i), (3) (2) REVERIE [80]: The coarse-grained VLN task inherits the trajectories from R2R but provides high-level instructions that describe target object. On average, instructions contain 21 words, and the length of ground-truth paths ranges from 4 to 7 steps. OBJECTNAV-MP3D [8]: We use the standard split of 11 validation scenes from the Habitat OBJECTNAV dataset [92] in MP3D [9], which consists of 21 goal categories. We utilized human demonstration from HabitatWeb [87] as training data, details are discussed in Section 2.2. Evaluation Metrics We follow 5 standard metrics in VLN literature to assess the agent performance, including Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), normalized inverse of the Path Length (SPL) [5], normalized Dynamic Time Warping (nDTW) [40]. 2.2. What are the Conflicts in Navigation Multitasks Learning? To successfully train versatile navigation agent, it is essential to understand the underlying contradictions that prevent unified model learning and to gain insights into the best practices for learning from diverse data sources. Following the discussion in Section 1, we unify the training data by transferring 70k human demonstration OBJECTNAV data from Habitat-Web into trajectories in the discrete environment. Data Transformation in Discrete Environment The original trajectories in OBJECTNAV are constructed as sequences of continuous viewpoint positions, averaging 243 3 Training Data R2R REVERIE MP3D R2R (Val Unseen) REVERIE (Val Unseen) OBJECTNAV-MP3D (Val) TL NE SR SPL TL NE SR SPL TL NE SR SPL 14.33 17.55 20.76 14.03 19.17 14. 3.82 6.22 8.55 4.01 7.13 4.10 67 42 16 64 34 65 55 32 9 55 26 54 19.61 17.91 20.00 15.22 19.46 16.62 7.55 6.56 10.11 7.78 6.24 6.11 39 41 13 38 35 28 32 9 31 26 27 15.30 10.46 22.17 25.91 21.50 22.97 4.69 5.91 3.67 3.28 3.29 3.54 55 43 68 72 70 68 24 23 29 28 33 27 Table 1. Comparison of single-run performance with different mixture of training data for DuET [16]. indicates utilizing Habitat-rendered images. Numbers in gray indicated zero-shot inference results on held-out datasets. steps per demonstration. Following Hong et al. [37], we discretize the Habitat-MP3D [91] environment into connectivity graph G. We match each viewpoint in the trajectory from Habitat-Web to the nearest nodes on based on Euclidean distance and merge sequentially repeated nodes. Disconnected paths and paths with an ending position more than 0.5m away from the original endpoint are removed. This results in 58,803 trajectories with an average of 20 steps. Similarly, we transfer the data from the MP3D validation split to evaluate model performance in discrete environments. Fine-grain Language Understanding Benefits Targetoriented Navigation We conduct multi-task training on DUET [16] initialized from LXMERT [97] using various data mixtures and report held-out inference results for datasets not included in the training. To address the visual gap between Habitat-rendered images and Matterport3Drendered images, we map the R2R and REVERIE trajectories onto and employ Habitat-rendered images for this experiment. The results are presented in Table 1. Our findings reveal several key insights: (1) Mixing training data across different tasks reduces performance compared to training on individual tasks requiring higherlevel language understanding capacity. For example, in the R2R task, incorporating additional data results in 2-3% drop in success rate (SR), while in REVERIE, combining OBJECTNAV data leads to substantial SR decrease of 67%, even lower than the zero-shot performance achieved when training exclusively with R2R data (39% SR vs. 35% SR). (2) Training with fine-grained human-annotated instruction-trajectory pairs proves advantageous for OBJECTNAV, yielding 2-4% SR improvement. (3) Models trained exclusively on VLN data achieve strong zero-shot performance on OBJECTNAV (above 40% SR); however, models trained only on OBJECTNAV data perform poorly on tasks that demand sophisticated language understanding (with only 15% SR). Additionally, models trained solely on R2R (fine-grained VLN) data achieve 39% SR on REVERIE (coarse-grained VLN) and 55% SR on OBJECTNAV. This suggests that when the target is visible or only minor exploration is needed, R2R-trained models can successfully infer the location. These observations lead to two main conclusions: 1. Fine-grained language understanding improves target-oriented navigation, as visual-semantic understanding is enhanced through learning vision-language alignment; however, models trained exclusively on target-oriented data lack the language comprehension required to follow complex instructions. 2. Training with simple data mixing is insufficient to achieve optimal performance for tasks demanding both exploration and detailed instruction interpretation (coarse-grained VLN). 3. Mixture of Experts for Versatile Languageguided Visual Navigation The insights from Section 2.2 motivate the need for method to manage conflicts that arise during multi-task learning. To address this, we propose new State-Adaptive Mixture of Experts (SAME) approach. SAME employs multiple specialized expert networks = {f1, , fN } which could be switched during each step in navigation episode conditioned on the agents state by routing mechanism R. In this way, we differentiate the learning of distinct navigation skills, such as exploration and instructionfollowing, while facilitating the sharing of common navigational knowledge like visual semantic understanding. 3.1. MoE Formulation subset of experts is activated in sparsely gated MoE layer during each forward pass. The router predicts the probability of each expert being assigned: P(xr) = Softmax (R (xr)) R(xr) = xr (4) (5) where xr is the routing feature extracted from the input x, RdN is trainable layer, is the hidden dimension and is the number of experts. The weighted sum of the outputs from experts with top-k routing scores noted as set is computed as the output: MoE(x, xr) = (cid:88) iT (xr)i fi(x) (6) Typically load balancing loss [27] is implemented to encourage an even distribution of input across the experts: Lbalance = (cid:88)"
        },
        {
            "title": "FiDi",
            "content": "(7) where represents the fraction of inputs processed by each expert fi, and represents the fraction of router probability allocated for expert fi. Task-wised MoE and Token-wised MoE Recently, most research on Mixture of Experts (MoE) has focused on transformer-based Large Language Models (LLMs), where MoE operates at the token level to process each individual input token, as illustrated in Figure 2. Concurrently, MoE is also extensively explored in computer vision multi-task learning, where expert modules are routed at the task level. These two routing features are formulated as follows: xtoken = xi = Etask = WtT xtask (10) (11) where xi is the i-th token in input x, and Etask is the task embedding for task with index . Beyond assigning each task to specific experts through hard assignment, it might be helpful for an agent to select appropriate navigation skills based on the language instruction. To this end, we formulate language-aware expert selection mechanism, represented as follows: = ˆW CLS xtext (12) where ˆW CLS denotes the [CLS] token for text feature ˆW. 3.2. State-Adaptive Experts Selection Our initial experiments with the above token-wise and taskwise MoE in multi-task navigation learning yielded suboptimal results (to be presented in 3.3), prompting us to reconsider more feasible MoE formulation for the sequential decision-making process in navigation. We observed that the density of language information the agent receives which must align with visual observations to determine an action can vary significantly across different timesteps in different tasks. In other words, the agents state interpretation should be inherently generalizable to address distinct navigation problems. In light of this, we introduce SAME, multimodal State-Adaptive expert selection mechanism: xmulti = Wm (cid:34)"
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) i=0 (cid:35) ˆOt; ˆW CLS (13) where linear layer Wm R2hh is implemented to merge ˆOt the concatenation of the mean visual feature 1 over different views and the text [CLS] token ˆW CLS. (cid:80)L i=0 i=0 (cid:88) i=1 (cid:88) i="
        },
        {
            "title": "1\nK",
            "content": "F = = 1 {arg max P(xr) = i} , (8) Besides, we also investigate the effect of adding task information for expert selection: P(xr)i. (9) xtext task xmulti task = xtext = xmulti + Etask + Etask (14) (15) 4 Figure 2. Illustration of MoE position and experts routing methods. SAME routing based on multimodal features from visual observations and language instructions allows the agent to dynamically adapt to environmental visual changes. Routing Condition R2R (Val Unseen) REVERIE (Val Unseen) OBJECTNAV-MP3D (Val) TL NE SR SPL TL NE SR SPL TL NE SR SPL w/o MoE 11.76 2.98 73. 65.79 13.17 5.90 40.39 35.40 16. 3.16 72.31 42.72 Token-wised MoE [93]: Token Embedding Task-wised MoE: Task Embedding Text [CLS] w/ Task Embedding State-Adaptive MoE (ours): SAME w/ Task Embedding w/o Pretrain 13.28 2.98 73.99 64.81 16. 5.60 43.45 35.40 15.69 3.04 74. 44.97 12.98 14.45 15.80 13.51 14.00 14.71 3.13 2.92 2.95 2.90 2.89 4.66 72.84 74.67 73. 73.69 74.50 59.05 64.81 64.12 62.80 64.92 64.66 49.02 14.90 19.22 21.50 16.32 17.65 14.19 5.71 5.46 5. 5.38 5.66 6.26 43.71 45.50 43.85 45.67 42.32 38.43 36.58 36.56 34.10 37.95 33.61 32.40 14.96 15.63 18. 15.60 17.20 19.93 3.17 3.11 3.00 3.10 2.86 3.08 71.13 71.32 72.40 71.43 73.39 70.94 43.88 42.75 39. 43.39 42.07 38.48 Table 2. Comparison of single-run performance with different MoE routing conditions on R2R, REVERIE, and OBJECTNAVMP3D. 3.3. Comparison on MoE Routing Following the above, we can see the key to distinguishing specific and shared navigational knowledge learning is learning the routing mechanism in MoE. In this section, we investigate the routing strategies for versatile languageguided navigation policy. Experiment Setup We conduct contrastive experiments to compare the routing mechanism, Matterport3D-rendered images [6] are used for R2R and REVERIE. MoE experts are deployed to the visual queries (will be discussed in Section 3.4), and results are shown in Table 2. As discussed in Section 2.2, we demonstrate that learning fine-grained language understanding can enhance targetoriented navigation learning, motivating us to take advantage of the powerful vision-language-action pre-train conducted in VLN [14, 16, 32, 34, 74, 81, 106]. Throughout the subsequent experiments, we initialized our model from ScaleVLN [106], which scales the VLN data from 14K to 4.9M by generating synthetic data to perform pertaining. VLN Pretrain Benefits Multiple Navigation Task Learning As shown in Table 2, there is significant improvement on all tasks when initializing with VLN pre-trained weights compared to directly performing multi-task tuning on SAME initialized from general vision-language pretrain LXMERT [97] (w/o Pretrain), featured by 15% SR increase on R2R. SAME Facilitates Navigation Skill Sharing Comparing different MoE routing types, we highlight significant improvement of 3% SR and 2% SPL in SAME routing on REVERIE. We hypothesize that this improvement is due to the nature of the coarse-grained VLN task, where the agent must alternate between exploration, in cases where language guidance lacks detail, and adhering closely to language instructions when precisely localizing the target. SAME routing enables flexible selection of experts to manage these distinct navigation behaviors based on the current observation and language input, allowing the agent to learn transferable knowledge across tasks. This flexibility also results in the SAME agent achieving the highest average SPL of 48.31% across all tasks. Compared to the multi-task-tuned DUET (w/o MoE) under the same experimental conditions, all MoE methods show significant performance increase (3-5% SR) on the REVERIE task, with no notable performance drops on other tasks. This aligns with our findings in Section 2.2, suggesting that coarse-grained VLN performance is influenced when co-trained with other language instruction types, likely due to model overfitting to the R2R task. Our proposed MoE approach addresses this issue effectively. Furthermore, we examine hard assignments with specific experts dedicated to different tasks by directly routing through task embeddings. This approach results in perMoE Experts Position R2R (Val Unseen) REVERIE (Val Unseen) OBJECTNAV-MP3D (Val) TL NE SR SPL TL NE SR SPL TL NE SR SPL Feed Forward Visual Query Textual Key & Value 13.18 13.51 15.58 3.08 2.90 2.82 73.44 73.69 75.35 64.86 64.92 63. 16.02 16.32 20.30 5.62 5.38 5.36 42.77 45.67 45.61 35.28 37.95 34.75 16.02 15.60 17.57 3.09 3.10 3. 71.37 71.43 72.17 42.24 43.39 42.67 Table 3. Comparison of single-run performance with different MoE experts positions on R2R, REVERIE, and OBJECTNAVMP3D. and xmulti formance drops across all tasks compared to other routing methods. Additionally, incorporating task embeddings into xtext reduces performance, indicating that routr ing based on samples from different tasks provides strong but ineffective bias, preventing the router from effectively learning which experts to select based on the instruction or observation, thus impeding the development of shared knowledge across tasks. 3.4. Which Part of the Navigation Policy Learns Different Navigation Behaviour? Since the initial application of MoE in transformer architectures [27, 54, 127], MoE has acted as an enhancement for Feed-Forward Network (FFN) modules within these models. Concurrently, some studies have integrated multi-head attention layers with MoE to further enhance performance while managing computational costs. In this section, we analyze the impact of MoE applied at different components of the transformer model, specifically focusing on the FFN, visual queries Wq, textual key Wk, and value Wv with SAME under the same experiment setup described in Section 3.3. The results are shown in Table 3. MoE applied to different components yields varying performance across tasks. Notably, the best overall performance is observed when applying MoE to the visual query, achieving the highest SPL on all tasks with fewer parameters. This suggests that utilizing MoE at the visual query level within the cross-attention layer is particularly effective. This effectiveness likely stems from the multimodal policys control over diverse navigation behaviors within the cross-attention layer, where specialized visual query experts allow the agent to more accurately determine the next action by adjusting attention scores over visual embeddings from multiple viewpoints. While FFN-based MoE enhances performance compared to non-MoE models, it is surpassed by MoE configurations that integrate with attention layers, highlighting the crucial role of cross-modal attention in successful action selection. In summary, the experimental results indicate that employing MoE with visual query experts and routing based on multimodal features from visual observations and language instructions (SAME) allows the agent to dynamically adapt to environmental visual changes while staying aligned with language guidance, thereby enhancing robust performance across different language-guided navigation tasks. 4. Experiments In this section, we conduct multi-task tuning based on the previous discussion. We conduct thorough evaluation of SAME across seven major navigation benchmarks, complemented by series of ablation studies on training details to establish best practices for effective multi-task training in language-guided navigation. Datasets Besides the R2R, REVERIE, and OBJECTNAVMP3D, we include 4 other datasets for evaluation. RxR-EN [52]: English split of the RxR dataset, which contains longer instructions compared to R2R and nonshortest trajectory from starting point to ending point. CVDN [99] requires the agent to comprehend the conversation history and infer the correct next actions based on the dialogue context. For evaluation, we use the standard metric, Goal Progress (GP), which calculates the average difference between the completed trajectory length and the remaining distance to the goal. SOON [125]: Similar to REVERIE, the instructions describe target rooms and objects, with an average length of 47 words. The expert paths vary in length from 2 to 21 steps, with an average of 9.5 steps. R2R-CE [50]: Transfering the discrete trajectories in R2R to continuous 3D scans rendered by Habitat [92], allowing an agent to navigate freely in open space while requiring interaction with obstacles. Implementation Details We build upon the DUET architecture, and replace all the visual query layers in the crossattention with MoE layers. We adopt SAME routing for each MoE layer. We initialize from the pre-trained weights of ScaleVLN [106] and utilize CLIP ViT-B/16 [84] as the visual encoder. We use ScaleVLN, R2R, RxR-EN, CVDN, REVERIE, SOON, and Habitat-Web as the training data with sampling ratio of 10:1:1:1:1:1:2 without mixing different data in batch. We follow [3, 16, 44] to utilize DAgger [90] algorithm to obtain interactive supervision from the simulator. The training objective = LDAG + λLbalance is the combination of DAgger loss and MoE load balancing loss in Equation 7, balanced by coefficient λ = 0.8. The model is fine-tuned using AdamW optimizer [71] with learning of 1 105 for 20k iterations with batch size of 16 on single 80G NVIDIA A100 GPU. 4.1. Comparison with State-of-the-Art Models Comparison in Discrete Environment We first compare our method with current SoTA methods in the discrete MP3D environment [6]. The datasets are organized in Table 4 by language instruction granularity and complexity, ranging from fine-grained and complex to coarse-grained and simple, from left to right. We classify previous methods into two categories, the first one is separate model for each task, which fine-tunes distinct set of parameters for CVDN RxR-EN R2R SOON REVERIE OBJECTNAV-MP3D Methods Val Test Val unseen Val unseen Test unseen Val unseen Test unseen Val unseen Test unseen Val GP GP SR nDTW SR SPL SR SPL SR SPL SR SPL SR SPL SR SPL SR SPL Separate Model for Each Task: SF [28] RCM [103] EnvDrop [98] PREVALENT [34] VLNBERT [36] HAMT [15] HOP+ [81] DUET [16] AutoVLN [17] BEVBert [3] GridMM [107] VER [67] GOAT [101] ScaleVLN [106] - - - 3.15 - 5.13 - - - - - - - 6.12 - - - 2.44 - 5.58 - - - - - - 6.97 Unified Model for All Tasks: MT-RCM [104] NaviLLM [121] ScaleVLN SAME (ours) 4.65 6.16 5.93 6. 3.91 7.90 - 7.07 - - - - - 56.4 - - - 66.7 - - 68.2 - - - 46.7 50.5 - - - - - 63.0 - - - 69.6 - - 66.8 - - - 49.7 51.2 36 43 52 58 63 66 67 72 - 75 75 76 78 47 67 76 76 - - 48 53 57 61 61 60 - 64 64 65 68 70 41 59 67 66 35 43 51 54 63 65 66 69 - 73 73 76 75 77 45 68 - 74 28 38 47 51 57 60 60 59 - 62 62 66 65 40 60 - 64 - - - - - - - 36.3 41.0 - - - 40.4 - - 38.3 33.2 36.1 - - - - - - - 22.6 30.7 - - - 28.1 - - 29.2 25.4 25.4 - - - - - - - 33.4 40.4 - - - 40.5 - - 35.0 - 38.2 - - - - - - - 21.4 27.9 - - - 25.2 - - 26.3 - 27.1 - 9.3 - - 25.5 33.0 36.1 47.0 55.9 51.8 - 56.0 53.4 57.0 - 42.2 41.9 46.4 - 7.0 - - 21.1 30.2 31.1 33.7 40.9 36.4 - 39.7 36.7 41. - 35.7 34.4 36.1 - 7.8 - - 24.6 30.4 33.8 52.5 55.2 52.8 - 56.8 57.7 56.1 - 39.8 - 48.6 - 6.7 - - 19.5 26.7 28.2 36.1 38.9 36.4 - 38.8 40.5 39.5 - 32.3 - 37.1 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 72.3 76.3 - - 43.4 42.7 Table 4. Agents performance across all tasks in the discrete environment [6]. indicates our implementation of multi-task tuning. Note that existing methods tailored for OBJECTNAV-MP3D are proposed in continuous environments, which will be evaluated in Table 5 below. each downstream task. The second one is unified model for all tasks, which includes our baseline methods: MT-RCM [104]: This method performs multi-task training on R2R and CVDN using the RCM model [103]. NaviLLM [121]: hybrid model of DUET and Large Language Model (LLM) that employs stronger vision encoder, EVA-CLIP-02-Large [26], and replaces the 12layer cross-modal encoder in DUET with LLM, Vicuna7B [20], to facilitate multi-task instruction tuning. ScaleVLN: We conduct multi-task tuning on DUET [16] with ScaleVLN [106] initilization. This serves as baseline for multi-task tuning on DUET without using MoE. As shown in Table 4, SAME achieve State-of-the-Art performance on multiple benchmarks with unified model. Compared to separate models for each task, SAME achieve SoTA performance on CVDN with 7.07 GP on test 13% GP increase on validation compared to ScaleVLN. SAME performs at the same level as VER and GOAT comparing the SPL on R2R and REVERIE. Compared to other unified models, SAME outperform the baseline multi-task tuned ScaleVLN on all tasks, with an average of 3% SR increase among all tasks. SAME peform significantly better than NaviLLM on R2R and REVERIE. Comparison in Continuous Environment We further compare our method with current SoTA methods in the continuous Habitat environment [92]. We take the same model reported in Table 4 and examine the zero-shot inference results on held-out datasets in continuous environment. We follow Hong et al. [37] and deploy the waypoint predictor to bridge the gap between discrete and continuous. As shown in Table 5, SAME surpass Habitat-Web, which performs imitation learning on OBJECTNAV human demonstration,"
        },
        {
            "title": "Methods",
            "content": "R2R-CE (Val unseen) MP3D (Val) NE SR SPL SR SPL NaVid [117] ScaleVLN [106] ETPNav [4] BEVBert [3] SemExp [10] PONI [86] Habitat-Web [87] 5.47 4.80 4.71 4.57 - - - 37 55 57 - - - 36 51 49 50 - - - SAME (ours) 5.31 38 - - - - 28 32 35 43 - - - - 11 12 21 Table 5. Comparison with previous methods in the continuous environment [92]. We report the zero-shot inference results of SAME using the same checkpoint from Table 4. when zero-shot transferring to the continuous environment. This aligned with our findings in 2.2 that fine-grain language understanding improves target-oriented navigation. 4.2. Ablation Studies We conduct series of experiments to establish best practices for training with multiple navigation tasks. These ablations are performed using the same experimental setup as described in Section 3.3. Effect of Training Schema We investigate different training schemas for SAME, specifically the training algorithm and data mixing strategy. For the training algorithm, we compare training with imitation learning only on teacher actions [6] to training with DAgger, where at each time step, an agent performs an action sampled from the predicted probability of its action space and minimizes the loss between the sampled action and the ground truth. This method Algorithm Batch R2R (Val Unseen) REVERIE (Val Unseen) OBJECTNAV-MP3D (Val) TL NE SR SPL TL NE SR SPL TL NE SR SPL Imitation Imitation DAgger DAgger Mix Sequential Mix Sequential 13.77 9.35 16.09 13.51 3.82 4. 4.07 2.90 65.51 61.09 62.36 73.69 56.57 58.88 52.18 64.92 19.02 9. 25.05 16.32 5.79 7.37 5.89 5.38 35.64 28.37 30.45 45.67 28.15 26. 21.91 37.95 31.31 26.57 27.57 15.60 2.64 3.51 2.81 3.10 75.83 71. 74.85 71.43 26.77 24.06 31.70 43.39 Table 6. Comparison of single-run performance with different training schema on R2R, REVERIE, and OBJECTNAV-MP3D. λ 0.2 0.5 0.8 1. R2R (Val Unseen) REVERIE (Val Unseen) OBJECTNAV-MP3D (Val) TL NE SR SPL TL NE SR SPL TL NE SR SPL 12.63 13.77 13.51 13.56 2.91 2.89 2.90 3.02 73.61 74.29 73.69 73. 65.73 65.00 64.92 64.52 15.47 19.45 16.32 16.19 5.55 5.56 5.38 5.24 42.86 44.70 45.67 45.81 35.22 34.89 37.95 37.89 15.25 18.10 15.60 18. 3.13 3.05 3.10 2.67 71.79 72.26 71.43 74.66 43.51 40.50 43.39 44.81 Table 7. Comparison of single-run performance with different MoE balance coefficients λ on R2R, REVERIE, and OBJECTNAV-MP3D. allows an agent to learn from paths that cover wide space and reduces the exposure bias caused by teacher forcing [46]. For the data mixing strategy, we investigate mixing different datasets in the same batch versus sampling different data for training. As shown in Table 6, training with DAgger significantly improves the performance when sequentially sampling data. Compare row 4 to row 2 in Table 6, 12.6% SR and 6.04% SPL increase on R2R, 17.3% SR and 11.53% SPL increase on REVERIE, 19.33% SPL increase on OBJECTNAV-MP3D is observed. We also noticed that mixing up training data in the same batch would significantly affect DAgger training, comparing row 4 to row 3. This indicates utilizing Dagger without mixing up training data in the same batch is the best practice. Effect of MoE Routing Balance Coefficient We investigate the effect of MoE routing balance loss coefficient λ in Table 7. When λ is large, the balancing loss would force the model to evenly select different experts for samples in the same batch, when λ decreases, such constraint is weakened. The model trained with λ = 0.2 in row 1 performs worse than all other variants, demonstrating the significance of the balancing loss in MoE training. 5. Related Work 5.1. Vision-and-Language Navigation The development of navigation agent capable of interpreting and acting upon unrestricted linguistic instructions to navigate through unfamiliar, photorealistic environments has been longstanding goal within the field of Visionand-Language Navigation [7, 35, 52, 80, 99, 118, 125]. Approaches to this problem have primarily addressed two main areas: (1) Vision-Language Alignment: Some studies [14, 32, 34, 36, 57, 60, 74, 81, 119] leverage generic visual-linguistic representations [18, 59, 61, 96, 97]. Others incorporate additional supervision through data augmentation [5, 24, 32, 55, 56, 58, 79, 98, 106], along with training strategies [39, 72, 102, 103, 124] to enhance cross-modal alignment. Recently, some work [12, 64, 69, 70, 78, 82, 83, 116, 117, 121123] integrate LLMs with navigation policy for generalizable world knowledge. (2) Efficient Action Planning Mechanisms: These involve historical state memorization [15, 36], self-correction strategies [45, 73], navigation map construction [2, 13, 16, 66, 67, 100, 107, 120], and the use of external knowledge prompts [62, 63]. 5.2. ObjectGoal Navigation Approaches to learning to understand visual semantics and perform object goal navigation [9, 22, 23, 25, 29, 46 48, 85, 110, 115] could be categorized into two streams: (1) Modular Pipelines with Learned Modules [10, 11, 33, 86, 87, 108]: This paradigm integrates learning into specific modules by leveraging explicit scene representations like semantic map [11, 33, 86], or simply employ object detectors or segmentors [75, 87]. (2) End-to-end Learning with RL or IL [1, 88, 109, 112, 114]: these methods benefit from visual representation [76, 113], auxiliary task [114], and data augmentation [75] to generalize to unseen environments. 5.3. Mixture of Experts Mixture of Experts (MoE) [41, 43, 93] models utilize multiple specialized experts along with routing network that dynamically assigns tasks based on their complexity. Taskoriented MoE enhances the models capacity to learn both specific and shared knowledge in computer vision, without significantly increasing computational costs [19, 31, 68, 77, 89, 94]. Sparsely activated MoE is widely employed in LLMs to reduce computational costs while enabling the training of gigantic models with trillions of parameters through sparse activation [27, 54, 93]. MoE assigns different experts for instruction tuning in recent LLMs [21, 42, 111], optimizing the learning process by focusing on specific tasks within the overall model architecture. To address task conflicts and enhance generalization in unseen tasks, some methods [19, 31, 105] employ various strategies to optimize the selection and aggregation of expert outputs in MoE. 6. Conclusion This paper unifies diverse range of navigation tasks within cohesive language-guided navigation framework. It examines the fundamental challenges of sharing common knowledge while leveraging task-specific capabilities in navigation learning. We propose the State-Adaptive Mixture of Experts (SAME), which enables an agent to make decisions by integrating multi-granularity language inputs and dynamic observations. We believe SAME can guide the learning towards versatile language-guided navigation agents."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the Centre for Augmented Reasoning, an initiative by the Department of Education, Australian Government."
        },
        {
            "title": "References",
            "content": "[1] Ziad Al-Halah, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Zero experience required: Plug & play modular transfer learning for semantic visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1703117041, 2022. 8 [2] Dong An, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, and Tieniu Tan. Neighbor-view enhanced model for viIn Proceedings of the sion and language navigation. 29th ACM International Conference on Multimedia, pages 51015109, 2021. 8 [3] Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, and Jing Shao. Bevbert: Topo-metric map pre-training for language-guided navigation. arXiv preprint arXiv:2212.04385, 2022. 6, 7 [4] Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, and Liang Wang. Etpnav: Evolving topological planning for vision-language navigation in continuous environments. arXiv preprint arXiv:2304.03047, 2023. 7 [5] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 3, 8 [6] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674 3683, 2018. 1, 2, 3, 5, 6, 7, 15, 16 [7] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3674 3683, 2018. [8] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects. In arXiv:2006.13171, 2020. 1, 2, 3, 15 [9] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning In 2017 Interfrom rgb-d data in indoor environments. national Conference on 3D Vision (3DV), pages 667676. IEEE, 2017. 3, 8 9 [10] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33:42474258, 2020. 1, 7, 8 [11] Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, and Saurabh Gupta. Neural topological slam for In Proceedings of the IEEE/CVF convisual navigation. ference on computer vision and pattern recognition, pages 1287512884, 2020. 1, 8 [12] Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, and Kwan-Yee Wong. Mapgpt: Mapguided prompting for unified vision-and-language navigation. arXiv preprint arXiv:2401.07314, 2024. [13] Kevin Chen, Junshen Chen, Jo Chuang, Marynel Vazquez, and Silvio Savarese. Topological planning with In Protransformers for vision-and-language navigation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1127611286, 2021. 8 [14] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. Advances in Neural Information Processing Systems, 34:58345847, 2021. 1, 5, 8 [15] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. Advances in Neural Information Processing Systems, 34:58345847, 2021. 7, 8 [16] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Think global, act local: Dual-scale graph transformer for vision-and-language navIn Proceedings of the IEEE/CVF Conference on igation. Computer Vision and Pattern Recognition, pages 16537 16547, 2022. 1, 2, 3, 5, 6, 7, 8, 15 [17] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Learning from unlabeled 3d environments for vision-and-language navigation. In European Conference on Computer Vision, pages 638 655. Springer, 2022. 7 [18] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. In Uniter: Universal image-text representation learning. European conference on computer vision, pages 104120. Springer, 2020. 1, 8 [19] Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang, Yu Qiao, and Jing Shao. Octavius: Mitigating task interference in mllms via moe. arXiv preprint arXiv:2311.02684, 3, 2023. 8 [20] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. [21] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. 1, 8 [22] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, et al. Robothor: An open simulation-to-real embodied ai platform. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 31643174, 2020. 8 [23] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. Advances in Neural Information Processing Systems, 35: 59825994, 2022. 8 [24] Zi-Yi Dou and Nanyun Peng. Foam: follower-aware speaker model for vision-and-language navigation. arXiv preprint arXiv:2206.04294, 2022. 8 [25] Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: framework for visual object manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44974506, 2021. 8 [26] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1935819369, 2023. [27] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. 4, 6, 8 [28] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. Advances in Neural Information Processing Systems, 31, 2018. 1, 7 [29] SY Gadre, Wortsman, Ilharco, Schmidt, and Song. Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. arXiv preprint arXiv:2203.10421, 2022. 8 [30] Chen Gao, Jinyu Chen, Si Liu, Luting Wang, Qiong Zhang, and Qi Wu. Room-and-object aware knowledge reasoning for remote embodied referring expression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30643073, 2021. 1 [31] Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James Kwok, and Yu Zhang. Mixture of cluster-conditional lora experts arXiv preprint for vision-language instruction tuning. arXiv:2312.12379, 2023. 8 [32] Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, and Cordelia Schmid. Airbert: In-domain pretraining for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16341643, 2021. 1, 5, [33] Meera Hahn, Devendra Singh Chaplot, Shubham Tulsiani, Mustafa Mukadam, James Rehg, and Abhinav Gupta. No rl, no simulation: Learning to navigate without navigating. Advances in Neural Information Processing Systems, 34:2666126673, 2021. 8 [34] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning generic agent for visionand-language navigation via pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1313713146, 2020. 1, 5, 7, 8 [35] Yicong Hong. Learning Language-Guided Visual Navigation. PhD thesis, The Australian National University (Australia), 2023."
        },
        {
            "title": "In Proceedings of",
            "content": "[36] Yicong Hong, Qi Wu, Yuankai Qi, Cristian RodriguezOpazo, and Stephen Gould. recurrent vision-andthe language bert for navigation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16431653, 2021. 1, 7, 8 [37] Yicong Hong, Zun Wang, Qi Wu, and Stephen Gould. Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1543915449, 2022. 3, 7, 15 [38] Ronghang Hu, Daniel Fried, Anna Rohrbach, Dan Klein, Trevor Darrell, and Kate Saenko. Are you looking? grounding to multiple modalities in vision-and-language navigation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 65516557, 2019. 1 [39] Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel Magalhaes, Jason Baldridge, and Eugene Ie. Transferable representation learning in vision-and-language navigation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 74047413, 2019. 8 [40] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. arXiv preprint arXiv:1907.05446, 2019. 3 [41] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. 1, 8 [42] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 1, 8 [43] Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994. 1, [44] Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh, Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, and Zarana Parekh. new path: Scaling visionand-language navigation with synthetic instructions and imitation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1081310823, 2023. 6 10 [45] Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. Tactical rewind: Self-correction via In Probacktracking in vision-and-language navigation. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67416749, 2019. 8 [46] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel Chang, and Manolis Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1638416393, 2024. 8 [47] Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, and Roozbeh Mottaghi. Goat-bench: benchmark for multi-modal lifelong navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1637316383, 2024. [48] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 8 [49] Jacob Krantz and Stefan Lee. Sim-2-sim transfer for visionIn and-language navigation in continuous environments. European Conference on Computer Vision, pages 588603. Springer, 2022. [50] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision-andIn Eulanguage navigation in continuous environments. ropean Conference on Computer Vision, pages 104120. Springer, 2020. 2, 6, 15 [51] Jacob Krantz, Aaron Gokaslan, Dhruv Batra, Stefan Lee, and Oleksandr Maksymets. Waypoint models for instruction-guided navigation in continuous environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1516215171, 2021. 15 [52] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotempoIn Proceedings of the 2020 Conference ral grounding. on Empirical Methods in Natural Language Processing (EMNLP), pages 43924412, 2020. 1, 6, 8, 16 [53] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotempoIn Proceedings of the 2020 Conference ral grounding. on Empirical Methods in Natural Language Processing (EMNLP), pages 43924412, 2020. 2 [54] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. 6, 8 [55] Jialu Li and Mohit Bansal. Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation. arXiv preprint arXiv:2305.19195, 2023. 8 [56] Jialu Li and Mohit Bansal. Improving vision-and-language navigation by generating future-view image semantics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1080310812, 2023. 8 [57] Jialu Li, Hao Tan, and Mohit Bansal. Improving crossmodal alignment in vision language navigation via syntactic information. arXiv preprint arXiv:2104.09580, 2021. 8 [58] Jialu Li, Hao Tan, and Mohit Bansal. Envedit: Environment editing for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1540715417, 2022. 8 [59] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: simple and perforarXiv preprint mant baseline for vision and language. arXiv:1908.03557, 2019. 1, 8 [60] Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, and Yejin Choi. Robust navigation with language pretraining and stochastic sampling. arXiv preprint arXiv:1909.02244, 2019. 1, 8 [61] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pretraining for vision-language tasks. In European Conference on Computer Vision, pages 121137. Springer, 2020. 1, 8 [62] Xiangyang Li, Zihan Wang, Jiahao Yang, Yaowei Wang, and Shuqiang Jiang. Kerm: Knowledge enhanced reasoning for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25832592, 2023. 8 [63] Bingqian Lin, Yi Zhu, Zicong Chen, Xiwen Liang, Jianzhuang Liu, and Xiaodan Liang. Adapt: Visionlanguage navigation with modality-aligned action prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1539615406, 2022. 8 [64] Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, and Xiaodan Liang. Navcot: Boosting llm-based vision-andlanguage navigation via learning disentangled reasoning. arXiv preprint arXiv:2403.07376, 2024. [65] Xiangru Lin, Guanbin Li, and Yizhou Yu. Scene-intuitive agent for remote embodied visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70367045, 2021. 1 [66] Rui Liu, Xiaohan Wang, Wenguan Wang, and Yi Yang. Birds-eye-view scene graph for vision-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1096810980, 2023. 8 [67] Rui Liu, Wenguan Wang, and Yi Yang. Volumetric environment representation for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1631716328, 2024. 7, 8 11 [68] Zhili Liu, Kai Chen, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, and James Kwok. Task-customized masked autoencoder via mixture of cluster-conditional experts. arXiv preprint arXiv:2402.05382, 2024. 8 [69] Yuxing Long, Xiaoqi Li, Wenzhe Cai, and Hao Dong. Discuss before moving: Visual language navigation via multiexpert discussions. arXiv preprint arXiv:2309.11382, 2023. 8 [70] Yuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Instructnav: Zero-shot system for Zhan, and Hao Dong. generic instruction navigation in unexplored environment. arXiv preprint arXiv:2406.04882, 2024. [71] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018. 6 [72] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Selfmonitoring navigation agent via auxiliary progress estimation. arXiv preprint arXiv:1901.03035, 2019. 8 [73] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. The regretful agent: Heuristic-aided navigation through progress estimation. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 67326740, 2019. 8 [74] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving visionand-language navigation with image-text pairs from the web. In European Conference on Computer Vision, pages 259274. Springer, 2020. 1, 5, 8 [75] Oleksandr Maksymets, Vincent Cartillier, Aaron Gokaslan, Erik Wijmans, Wojciech Galuba, Stefan Lee, and Dhruv Batra. Thda: Treasure hunt data augmentation for seIn Proceedings of the IEEE/CVF Inmantic navigation. ternational Conference on Computer Vision, pages 15374 15383, 2021. 8 [76] Arsalan Mousavian, Alexander Toshev, Marek Fiˇser, Jana Koˇsecka, Ayzaan Wahid, and James Davidson. Visual representations for semantic target driven navigation. In 2019 International Conference on Robotics and Automation (ICRA), pages 88468852. IEEE, 2019. [77] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. Advances in Neural Information Processing Systems, 35:95649576, 2022. 8 [78] Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, and Yoon Kim. Langnav: Language as perceptual representation for navigation. arXiv preprint arXiv:2310.07889, 2023. 8 [79] Amin Parvaneh, Ehsan Abbasnejad, Damien Teney, Javen Qinfeng Shi, and Anton van den Hengel. Counterfactual vision-and-language navigation: Unravelling the unseen. Advances in Neural Information Processing Systems, 33:52965307, 2020. 8 [80] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring exIn Proceedings of pression in real indoor environments. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99829991, 2020. 1, 2, 3, 8, 16 [81] Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, and Qi Wu. Hop+: History-enhanced and order-aware pre-training for vision-and-language navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 1, 5, 7, [82] Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, and Qi Wu. March in chat: Interactive prompting for remote embodied referring expression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15758 15767, 2023. 8 [83] Yanyuan Qiao, Qianyi Liu, Jiajun Liu, Jing Liu, and Qi Wu. Llm as copilot for coarse-grained vision-and-language navigation. In European Conference on Computer Vision, pages 459476. Springer, 2025. 8 [84] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 6 [85] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 8 [86] Santhosh Kumar Ramakrishnan, Devendra Singh Chaplot, Ziad Al-Halah, Jitendra Malik, and Kristen Grauman. Poni: Potential functions for objectgoal navigation with interaction-free learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1889018900, 2022. 1, 7, 8 [87] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51735183, 2022. 3, 7, 8, [88] Ram Ramrakhya, Dhruv Batra, Erik Wijmans, and Abhishek Das. Pirlnav: Pretraining with imitation and rl finetuning for objectnav. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1789617906, 2023. 8, 15 [89] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. 8 [90] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction In Proceedings of the fourto no-regret online learning. teenth international conference on artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. 6 12 [91] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: Platform for Embodied AI Research. ICCV, 2019. 3 [92] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 93399347, 2019. 3, 6, 7, [93] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. 1, 5, 8 [94] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling vision-language arXiv preprint models with sparse mixture of experts. arXiv:2303.07226, 2023. 8 [95] Gunnar Sigurdsson, Jesse Thomason, Gaurav Sukhatme, and Robinson Piramuthu. Rrex-bot: Remote referring expressions with bag of tricks. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 52035210. IEEE, 2023. 1 [96] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Vl-bert: Pre-training of arXiv preprint Furu Wei, and Jifeng Dai. generic visual-linguistic representations. arXiv:1908.08530, 2019. 1, [97] Hao Tan and Mohit Bansal. Lxmert: Learning crossmodality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 51005111, 2019. 1, 3, 5, 8, 16 [98] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environIn Proceedings of NAACL-HLT, pages mental dropout. 26102621, 2019. 1, 7, 8 [99] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning, pages 394406, 2020. 1, 2, 6, 8, 16 [100] Hanqing Wang, Wenguan Wang, Tianmin Shu, Wei Liang, and Jianbing Shen. Active visual information gathering for In European Conference on vision-language navigation. Computer Vision, pages 307322. Springer, 2020. 8 [101] Liuyi Wang, Zongtao He, Ronghao Dang, Mengjiao Shen, Chengju Liu, and Qijun Chen. Vision-and-language navigation via causal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1313913150, 2024. 7 [102] Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before you leap: Bridging model-free and model-based reinforcement learning for In Proplanned-ahead vision-and-language navigation. ceedings of the European Conference on Computer Vision (ECCV), pages 3753, 2018. 8 [103] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language In Proceedings of the IEEE/CVF Conference navigation. on Computer Vision and Pattern Recognition, pages 6629 6638, 2019. 1, 7, [104] Xin Eric Wang, Vihan Jain, Eugene Ie, William Yang Wang, Zornitsa Kozareva, and Sujith Ravi. Environment-agnostic multitask learning for natural language grounded navigation. In European Conference on Computer Vision, pages 413430. Springer, 2020. 7 [105] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameterefficient model tuning. arXiv preprint arXiv:2205.12410, 2022. 8 [106] Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, and Yu Qiao. Scaling data generation in vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1200912020, 2023. 5, 6, 7, 8 [107] Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang. Gridmm: Grid memory map for In Proceedings of the vision-and-language navigation. IEEE/CVF International Conference on Computer Vision, pages 1562515636, 2023. 7, 8 [108] Justin Wasserman, Karmesh Yadav, Girish Chowdhary, Abhinav Gupta, and Unnat Jain. Last-mile embodied visual navigation. In Conference on Robot Learning, pages 666 678. PMLR, 2023. 8 [109] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations (ICLR), 2020. 8 [110] Fei Xia, Amir Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world In Proceedings of the perception for embodied agents. IEEE Conference on Computer Vision and Pattern Recognition, pages 90689079, 2018. [111] Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024. 1, 8 [112] Karmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-Pierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, and Oleksandr Maksymets. Offline visual representation learning for embodied navigation. In Workshop on Reincarnating Reinforcement Learning at ICLR 2023, 2023. 8 [113] Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, and Roozbeh Mottaghi. Visual semantic navigation using scene priors. arXiv preprint arXiv:1810.06543, 2018. 8 [114] Joel Ye, Dhruv Batra, Abhishek Das, and Erik Wijmans. Auxiliary tasks and exploration enable objectgoal navigation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1611716126, 2021. 8 13 [127] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Stmoe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. [115] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, TsungYen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. Homerobot: Open-vocabulary mobile manipulation. arXiv preprint arXiv:2306.11565, 2023. 8 [116] Zhaohuan Zhan, Lisha Yu, Sijie Yu, and Guang Tan. Mc-gpt: Empowering vision-and-language navigation with arXiv preprint memory map and reasoning chains. arXiv:2405.10620, 2024. 8 [117] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and Wang He. Navid: Video-based vlm plans the next step for vision-and-language navigation. arXiv preprint arXiv:2402.15852, 2024. 7, 8 [118] Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, and Parisa Kordjamshidi. Vision-and-language navigation today and tomorrow: survey in the era of foundation models. arXiv preprint arXiv:2407.07035, 2024. 8 [119] Chongyang Zhao, Yuankai Qi, and Qi Wu. Mind the gap: Improving success rate of vision-and-language navigation by revisiting oracle success routes. In Proceedings of the 31st ACM International Conference on Multimedia, pages 43494358, 2023. 8 [120] Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong Yang, Haibing Ren, Huaxia Xia, and Si Liu. Targetdriven structured transformer planner for vision-language navigation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 41944203, 2022. 8 [121] Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. Towards learning generalist model for embodied navigation. arXiv preprint arXiv:2312.02010, 2023. 7, 8 [122] Gengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 76417649, 2024. [123] Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pages 260278. Springer, 2025. 8 [124] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1001210022, 2020. 8 [125] Fengda Zhu, Xiwen Liang, Yi Zhu, Qizhi Yu, Xiaojun Chang, and Xiaodan Liang. Soon: Scenario oriented object navigation with graph-based exploration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1268912699, 2021. 1, 2, 6, 8, 16 [126] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE international conference on robotics and automation (ICRA), pages 33573364. IEEE, 2017. 1 14 SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we aim to provide additional details to support the main content of our paper: Comparison of Training Methods: In Section A, we compare and discuss the training methodologies employed in ObjectGoal Navigation (OBJECTNAV) and Vision-and-Language Navigation (VLN) research. This comparison highlights the motivation behind the chosen training strategies and the data selection for SAME. Illustration of the DUET Method: Section offers comprehensive explanation of the DUET [16] approach utilized in our study, elucidating its design and integration within our framework. Full Results: The complete results of SAME are provided in Table 8, showcasing the performance of our method across various metrics and datasets. A. OBJECTNAV and VLN Training Besides learning shareable knowledge and task-specific skills from the model design of SAME, another challenge under the unified language-guided navigation framework is to determine the most effective approach to facilitate the learning of agents language comprehension capacity and grounding it in action prediction. Analyzing the rationality within the contrasting research focuses on ObjectGoal Navigation and VLN offers insights into this challenge. Specifically, we observe that the primary differences lie in the training data and training methods used. In this section, we discuss and make strategic decisions in SAME training regarding these two aspects, to address the above challenge. When language instructions are minimal, the task reduces to OBJECTNAV [8], where the learning objective is the semantic affinity between the target object and the visual perception, and leveraging episodic memory for strategic exploration without redundant revisits, since no extract information is provided from the language instruction. From the data aspect, it is proven to be effective to learn strategical exploration through human demonstration, and data collection is done in several works [87, 88]. From the training aspect, OBJECTNAV combines learning where and how to move, incorporating semantic perception and low-level collision avoidance (FORWARD, TURN LEFT, TURN RIGHT, STOP) within continuous environment [92]. On the contrary, VLN requires higher-level language understanding, where the agents not only need to understand the visual semantics of the environment but also need to align past observation and action sequence with the language description. From the data aspect, VLN agents learn higher-level language comprehension capacity from human-annotated instructions for navigation episodes. From the training aspect, such alignment is hard to learn directly in continuous environment, evident by the low performance ( 35% SR) on VLN-CE benchmark [50] of the methods that directly operate in continuous environments. Therefore, VLN research typically decouples Vision-Language alignment from collision avoidance by learning to navigate in discretized environment [6], where the navigable viewpoints are densely sampled from the environment at an average separation of 2.25m to form navigation graph. The learned multimodal navigation policy performs high-level action by selecting view directions that contain navigable viewpoint and teleporting between viewpoints on the graph. waypoint predictor [37, 49, 51] is employed to bridge action space discrepancies in continuous settings. Decoupling the learning of Vision-LanguageAction alignment with low-level action control significantly benefits the learning of language understanding capacity, improving VLN-CE success rates by 20%. To bridge action space discrepancies in continuous settings, modular designs employ waypoint predictors to propose navigable waypoints based on current observation, while the multimodality navigation policy performs view selection conditioned on these waypoints, with heuristic controller executing low-level actions to move to the waypoint. Decoupling the learning of vision-language-action alignment with low-level action control significantly benefits the language understanding capacity, improving VLNCE success rates by approximately 20%. In this work, we hypothesize such modular setups optimize the learning of language understanding capacity, which guides us to perform unified policy training in the discrete environment. Building on the aforementioned discussion, this work concentrates on solving the high-level decision-learning problem by decoupling it from tasks such as collision avoidance and low-level control. This direction motivates the adoption of VLN training methods for SAME training within discrete environment. Regarding training data, we combine OBJECTNAV human demonstration data with VLN human-annotated instructions to capture and learn distinct navigation behaviors. B. DUET Revisit SAME builds upon the design of the Dual-scale Graph Transformer (DUET) [16]. DUET incorporates text en15 coder to process instructions and employs both global and local branches to facilitate cross-modal reasoning at coarse and fine scales. B.1. Text and Visual Embedding DUETs text encoder leverages 12-layer transformer, initialized with LXMERT [97]. For visual embedding, each nodes visual observation comprises 36 view images, covering 12 horizontal and 3 vertical directions. To differentiate between these views, directional embedding Eang is added to the visual features ˆOt, which are extracted by the vision encoder. Since DUET incorporates all 36 view images to construct the spatial observation, navigable adjacent nodes are only visible in subset of these views, referred to as navigable views. To account for this, navigable embedding Enav is also included. The final visual embedding is processed by 2-layer transformer to encode spatial relationships between views, producing panoramic view embeddings: Benchmark Val Unseen Test Unseen TL NE nDTW SR SPL TL NE GP SR SPL R2R [6] RxR-EN [52] REVERIE [80] SOON [125] CVDN [99] 13.65 22.69 18.87 34.42 30. 2.73 6.53 5.18 8.12 12.72 71.05 51.20 48.54 76.25 50.52 46.35 36.11 24.48 66.16 42.19 36.12 25.42 17.23 14.80 19.47 37.99 3.03 7.07 73.92 48.60 38.18 18.15 64.41 37.10 27.11 12.18 Table 8. Full results of SAME on all VLN benchmarks. tric position of node on the map, capturing its orientation and distance relative to the current node. On the other hand, the navigation step encoding assigns value corresponding to the latest visited timestep for previously visited nodes, while unexplored nodes are encoded with value of 0. This encoding scheme enables the model to differentiate nodes based on their navigation history, thereby enhancing alignment with the provided instructions. Additionally, special stop node is introduced into the graph to signify the stop action. This node is connected to all other nodes in the graph. Opano = SelfAttn (cid:16) ˆOt + Eang + Enav(cid:17) . (16) B.3.2. Global Cross-Modal Encoding B.2. DUET Local Branch This section focuses on the local branch of DUET, which predicts actions based on the current nodes instruction and egocentric observation. Unlike the global branch, no graphlevel information is utilized beyond local observations. B.2.1. Local Visual Embedding The panoramic view embedding Opano is augmented with two types of location embeddings. The first represents the relative location of the current node with respect to the starting node, encoding long-distance directional relationships. The second represents the egocentric directions of adjacent views at the current node, enabling actions such as turn right. B.2.2. Local Cross-Modal Encoding The local branch employs standard 4-layer cross-modal transformer to capture relationships between vision and language. During action prediction, mask is applied to exclude unnavigable views and action logits are computed only for the navigable views at the current node. B.3. DUET Global Branch The encoded node features and word embeddings are processed through 4-layer graph-aware cross-modal transformer, which is composed of the following two key components, as illustrated in Figure 2. Cross-Attention Layer This layer models the relationships between the global map and the instruction, enabling crossmodal alignment. SAME examine applying State-Adaptive MoE on the visual query Wq or textual key Wk and value Wv in this layer. Graph-Aware Self-Attention Layer (GASA) Unlike standard self-attention mechanisms which rely solely on visual similarity, the GASA module incorporates the graphs structural information to refine attention computation, formulated as follows: GASA(V) = Softmax (cid:18) VWq(VWk)T (cid:19) + A(Et) VWv, (17) where A(Et) represents the spatial affinity matrix, comprised of pairwise L2 distances among all observed nodes. By incorporating this spatial context, GASA ensures that the model prioritizes spatially or topologically proximate nodes, which are often more contextually relevant than visually similar but distant nodes. This section introduces the global branch of DUET, which tasks the topological map representation ˆGt and encoded language instruction ˆW to predict actions by selecting any nodes on the graph. Each block in the global branch concludes with FeedForward Network (FFN). Additionally, SAME explores applying the State-Adaptive MoE mechanism to this FFN, as depicted in Figure 2 of the main paper. B.3.1. Node Embedding For each node on the graph, two additional encodings are applied: location encoding Eloc and navigation step encoding Estep. The location encoding represents the egocenC. Full Results on All VLN Tasks We show the full results of SAME on all the tested VLN benchmarks in Table 8."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "The University of Adelaide",
        "UNC, Chapel Hill",
        "UNSW Sydney"
    ]
}