{
    "paper_title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification",
    "authors": [
        "Zhaopan Xu",
        "Pengfei Zhou",
        "Jiaxin Ai",
        "Wangbo Zhao",
        "Kai Wang",
        "Xiaojiang Peng",
        "Wenqi Shao",
        "Hongxun Yao",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 5 0 5 2 1 . 3 0 5 2 : r MPBench: Comprehensive Multimodal Reasoning Benchmark for"
        },
        {
            "title": "Process Errors Identification",
            "content": "Zhaopan Xu1,2, Pengfei Zhou3, Jiaxin Ai4, Wangbo Zhao3, Kai Wang3, Xiaojiang Peng5 Wenqi Shao2,4, Hongxun Yao1*, Kaipeng Zhang2,4* 1HIT, 2Shanghai AI Laboratory,3NUS,4Shanghai Innovation Institude,5SZTU h.yao@hit.edu.cn, zhangkaipeng@pjlab.org.cn https://mpbench.github.io"
        },
        {
            "title": "Abstract",
            "content": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs."
        },
        {
            "title": "Introduction",
            "content": "Artificial intelligence (AI) has not yet effectively addressed complex reasoning tasks such as mathematics, programming, and planning. barrier to achieving artificial general intelligence (AGI) remains this. To address this challenge, the recent release of the GPT-o1 (OpenAI, 2024) proposed learning to reason to master human-like reasoning processes. Unlike traditional LLMs thinking, reasoning models generate long chains of thought. Consequently, recent research has increasingly paid * Corresponding author. attention to process-level (i.e., stepwise) reasoning analysis and further introduced various reasoning models to achieve significant enhancements in tasks such as mathematics and code generation. Identifying process errors can facilitate effective reasoning trajectories. PRMs, introduced by (Uesato et al., 2022) and (Lightman et al., 2023), can provide stepwise feedback for multi-step reasoning results. By rewarding intermediate steps, PRMs can enhance LLMs reasoning capabilities during both training and inference. Specifically, PRMs facilitate reinforcement learning or chain-of-thought data generation by offering fine-grained rewards, thus reducing the dependence on human-annotated data and improving performance. Moreover, during inference, PRMs guide LLMs in evaluating and exploring intermediate thoughts,\" promoting the generation of more deliberate reasoning steps and ultimately leading to improved reasoning accuracy. Despite the critical role of PRMs, they have not been adequately evaluated. Best-of-N performance, widely employed evaluation paradigm, is time-consuming, lacks finer-grained inspection, and its evaluation reliability can be significantly affected by the underlying solution generation model. This limited scope of evaluation hinders comprehensive understanding of PRMs potential to enhance complex reasoning tasks. Recent benchmarks (Zheng et al., 2024; Song et al., 2025) were introduced to evaluate PRMs. However, they typically focus on single scenario, such as step-bystep error identification, evaluating the ability of PRMs to detect erroneous steps accurately. These evaluation paradigms inadequately assess PRM performance in some scenarios. For example, during the reasoning process search in LLM inference, PRMs are tasked with selecting the correct step from candidates without access to the complete reasoning trajectory and the final answer. Furthermore, whereas current PRM benchmarks are text-centric, multimodal benchmarks are deficient, while multiPRM Benchmarks? Multimodal Benchmarks? Evaluation Paradigms Step Annotation Annotator Test Case Size MR-GSM8K (Zeng et al., 2023) CriticBench (Lin et al., 2024) MathCheck-GSM (Zhou et al., 2024) M3CoT (Chen et al., 2024a) ProcessBench (Zheng et al., 2024) PRMBENCH (SONG ET AL., 2025) MPBENCH (OURS) 1 1 1 1 1 1 Human - Synthetic Human Human Synthetic + Human Synthetic + Human 2,999 - 516 5,975 3,400 6,216 9, Table 1: Comparison between reasoning-related LLM benchmarks with our MPBench benchmark. modal contents are common in real-world tasks. These issues motivate our development of MPBench, comprehensive multimodal benchmark for evaluating the efficacy of multimodal PRMs across three scenarios. MPBench focuses on three key evaluation paradigms: Step Correctness, which evaluates the ability of PRMs to evaluate the correctness of each intermediate reasoning step and provide stepwise rewards to support reinforcement learning; Answer Aggregation, where PRMs aggregate per-step scores from multiple candidate solutions to select the best one; and Reasoning Process Search, which examines the ability of PRMs to guide the search for optimal reasoning steps by enabling structured exploration of potential solutions. With 9,745 fine-grained data instances across six sub-categories, MPBench offers robust framework for comprehensively assessing PRM performance in real-world reasoning tasks, providing valuable insights into their role in improving the reasoning capabilities of MLLMs. We conduct extensive experiments on MPBench with 12 MLLMs (prompted as critic models), including closed-source models, GPT-4o and Gemini-2.0, and open-source models like InternVL (Chen et al., 2024b), QWenVL (Team, 2025), and QVQ (Team, 2024). They struggle to achieve satisfactory results, and we believe our benchmarks benefit the future development of multimodal PRMs and process-level analysis. Our key contributions are summarized as follows: We present MPBench, the first comprehensive multimodal process-level reward model benchmark, comprising 9,745 fine-grained data instances across diverse subjects, tasks, and challenges. MPBench incorporates three distinct evaluation paradigms that comprehensively assess the role of PRMs in enhancing MLLM reasoning during both training and inference. These paradigms include step correctness, answer aggregation, and reasoning process search. We in-depth analyze the performance of 12 MLLMs and reveal distinct performance characteristics across different scenarios, providing valuable insights to assist future research on the development of multimodal PRMs."
        },
        {
            "title": "2.1 Reasoning Benchmarks",
            "content": "al., 2021) (Cobbe and et GSM8K MATH (Hendrycks et al., 2021) have served as prominent benchmarks for evaluating the mathematical reasoning capabilities of LLMs, primarily focusing on assessing the final correctness of generated solutions. Subsequent work has explored synthesizing solutions and evaluating intermediate steps. For instance, MathCheck (Zhou et al., 2024) focuses on judging the correctness of individual reasoning steps. CriticBench (Lin et al., 2024) evaluates language models abilities to critique solutions and extends evaluation to various reasoning tasks. With the growing interest in the development of process reward models (PRMs) to enhance model reasoning capabilities, several benchmarks are specifically designed for PRMs. such as ProcessBench (Zheng et al., 2024) and PRMBench (Song et al., 2025). These reasoning benchmarks are primarily textbased. With the increasing demand for evaluating multimodal reasoning, there has been drive to develop multimodal reasoning benchmarks across diverse domains (Zeng et al., 2024). However, research on benchmarks specifically tailored to multimodal PRMs remains limited. To address this gap, we introduce MPBench to benchmark multimodal process reward models. As highlighted in Table 1, MPBench encompasses wide range of tasks, along with large-scale, diverse set of erroneous steps, enabling more comprehensive evaluation of multimodal PRMs reasoning capabilities."
        },
        {
            "title": "2.2 Process Reward Models",
            "content": "The increasing prevalence of Process Reward Models (PRMs) highlights their crucial role in enhancing reasoning by providing step-wise rewards that facilitate reinforcement learning and data generation. To evaluate the accuracy of these step-wise rewards, benchmarks like ProcessBench and PRMBench have been developed. These benchmarks focus on constructing reasoning processes with erroneous steps for mathematical problems and evaluating the PRMs ability to identify the first error or the fine-grained error category of each step. Recently, GPT-o1 has shown impressive reasoning trajectories, highlighting the potential of scaling testtime computation to enhance LLM reasoning accuracy. Building on this idea, several studies (Snell et al., 2024; Xiang et al., 2024) have explored using PRMs to guide search within the interactive process during inference, further boosting model reasoning capabilities. However, evaluating PRMs in this application scenario remains limited. This is primarily due to the need for group and treestructured reasoning process data, which existing PRM benchmarks do not adequately address. This gap in evaluation is precisely one of the aspects our work intends to address."
        },
        {
            "title": "3 MPBench",
            "content": "Reasoning Process Search: This paradigm evaluates the PRMs ability to guide the search for optimal reasoning steps during inference. As illustrated in Fig. 1c, PRMs provide step-wise predictions that enable tree search over the solution space. This guided search encourages LLMs to generate more deliberate reasoning steps during inference, ultimately leading to improved reasoning accuracy. MPBench includes 9,745 fine-grained instances across six sub-categories, designed to comprehensively evaluate models across the three key evaluation paradigms, which enables robust assessment of PRM in handling realistic reasoning."
        },
        {
            "title": "3.1.1 Step Correctness",
            "content": "This assessment takes annotated question-solution pairs and corresponding ground truth answers as input. The task is to evaluate the correctness of each individual step, generating step-level scores. predefined threshold is then applied to these scores, yielding binary prediction of step correctness. Consequently, this evaluation is framed as binary classification problem, with the F1 score serving as the evaluation metric. Following ProcessBench (Zheng et al., 2024), which uses the negative F1 score as metric for error detection, we also combine the negative F1 score (F 1neg) with the F1 score to compute comprehensive evaluation metric called RMScore. MPBench employs three evaluation paradigms, each targeting specific role of PRMs in enhancing reasoning: Step Correctness: This paradigm evaluates the PRMs ability to judge the correctness of each intermediate step within solution, as illustrated in Fig. 1a. Accurate step correctness evaluation enables PRMs to provide step-wise rewards, facilitating reinforcement learning for LLM reasoning (Xia et al., 2024) and reducing reliance on human-annotated data. Answer Aggregation: Given multiple solutions from the generator, PRMs are tasked with aggregating the per-step scores for each answer to determine the best candidate solution (Fig 1 b). This capability allows PRMs to assess the overall quality of each solution and guide the model in selecting the most appropriate response from set of candidates. RM -Score = w1 1neg + w2 1, (1) where w1 and w2 are weights, both set to 0.5 by default. Based on diverse application scenarios, we further categorize this assessment into two subcategories: First Error Identification and All Error Identification, as detailed below: First Error Identification requires PRMs to identify the first error encountered in reasoning process. This evaluation method is commonly employed in both PRM testing (Zheng et al., 2024) and training (Hwang et al., 2024). All Error Identification This sub-category evaluates the PRMs ability to identify all errors within given solution. This comprehensive error identification is crucial for providing fine-grained rewards during training, enabling effective reinforcement learning. Figure 1: An overview of our MPBench. Left: data curation procedure. Right: evaluation paradigms: Step Correctness, Answer Aggregation, and Reasoning Process Search, highlighting the assessment of PRM performance through various tasks such as identifying errors, aggregating answers, and guiding reasoning steps"
        },
        {
            "title": "3.1.2 Answer Aggregation",
            "content": "In this scenario, the PRM receives question and multiple candidate solutions as input, and the task is to select the correct solution. We evaluate the performance of two search approaches: Best-of-N and Majority Voting. Best-of-N applies PRMs to score each candidate solution independently and selects the solution with the highest individual score as the final answer. Majority Voting selects the answer that the majority of inference traces support. The scores of responses associated with the same answer are aggregated, and the answer with the highest aggregated score is chosen as the final answer."
        },
        {
            "title": "3.1.3 Reasoning Process Search",
            "content": "This evaluation assesses search performance during inference. The PRM receives as input the question, the corresponding history of reasoning steps, and candidate steps for the current step under evaluation. In our dataset, this component consists of paired tree-structured data, where each step presents binary choice. We initially use the F1 score as the primary evaluation metric. Furthermore, following the work of MR-Score (Zeng et al., 2024), we employ the Matthews Correlation Coefficient (MCC) (Matthews, 1975) for the binary classification of search correctness. CC = N N (cid:112)(T + )(T + )(T + )(T + ) , (2) where TP, TN, FP, and FN stand for true positive, true negative, false positive, and false negative. The MCC score ranges from -1 to +1 with -1 meaning total disagreement between prediction and observation, 0 indicating near random performance, and +1 representing perfect prediction."
        },
        {
            "title": "3.2 Data Curation",
            "content": "We curated the dataset by extracting metadata and constructing test cases according to our defined evaluation paradigms. Detailed statistics of MPBench are displayed in Appendix A. Our dataset is based on M3CoT (Chen et al., 2024a), large-scale multimodal dataset comprising 17 topics and 263 categories across three primary domains: science knowledge, mathematics, and commonsense. M3CoT provides questions, ground truth answers, and ground truth step-level solution processes. Following CoMT (Cheng et al., 2024), we filtered out low-quality instances (e.g., vague expressions) to establish our ground truth answers, ensuring the reliability and accuracy of our dataset. Corresponding to our evaluation paradigms, MPBench comprises three distinct data categories: (1) Erroneous Steps: This category evaluates PRM capacity to provide step-level supervision by identifying incorrect steps within solution sequences. (2) Multi-solution: This component tests PRM ability to rerank candidate answers during inference, selecting the most promising solution. (3) Action Trees: This structured data assesses PRM guidance during reasoning process search, where PRMs navigate tree of possible actions to identify the optimal reasoning path. The construction of erroneous steps, multi-solution, and action trees is detailed below. Visualizations of example data for the three evaluation paradigms are provided in Appendix Figs 6, 7, and 8. Erroneous Steps To generate erroneous steps, we leveraged GPT-4o to introduce reasonable errors into the ground truth reasoning processes. Multi-solution To generate diverse set of candidate solutions, including incorrect ones, we employed four readily available multimodal language models: two open-source (LLaVa and QWen) and two closed-source (GPT-4o and Gemini). Each model generated three solutions per problem. From these generated solutions, we randomly selected one solution from each model, along with the original ground truth solution, to create final set of five candidate solutions. For problems where all generated solutions were correct, we randomly selected incorrect solutions from the pool of all generated incorrect solutions for that problem. Action Trees To construct action trees, we prompted GPT-4o to expand each incorrect action into the corresponding ground truth steps. This expansion process yielded multiple action pairs for the Reasoning Process Search evaluation."
        },
        {
            "title": "3.3.1 Filtering",
            "content": "To ensure high data quality and validity, and to maintain reasonable level of challenge, we implemented multi-stage filtering process, incorporating rule-based filtering, GPT-4 review, and simple problem filter. Specifically, this process included: 1) defining rules to ensure adherence to the required format; 2) manually developing in-context (IC) examples to guide GPT-4 in identifying and filtering unreasonable instances; and 3) using Gemini for testing and filtering solutions where the absolute difference between the scores of incorrect and correct steps was greater than 1, thereby ensuring dataset difficulty. Further details regarding these steps are provided in Appendix B."
        },
        {
            "title": "4 Experiments",
            "content": "To provide comprehensive evaluation of various models on MPBENCH, we selected diverse set, encompassing both open-source and proprietary LLMs. This selection includes advanced models such as GPT-4 and multi-step reasoningenhanced LLMs like the Gemini-2-Thinking (DeepMind, 2024). Given the complexity of the tasks, few-shot demonstration setups were employed to facilitate model adaptation to the required output format through In-Context Learning (ICL). Specifically, we utilized two-shot examples when prompting general-purpose LLMs. The impact of varying fewshot settings is discussed in Section 4.1.4. The prompts for the above phases are detailed in Appendix D."
        },
        {
            "title": "4.1 Results and Analysis",
            "content": "Table 2 showcases the performance of MLLMs on MPBench. Specifically, we are interested in exploring the following research questions: RQ1: How do model architecture and scale impact performance? RQ2: What correlations exist between different reasoning abilities (step correctness identity, answer aggregation, reasoning process search)? RQ3: How does performance vary across different steps in multi-step reasoning process? RQ4: What is the effect of in-context learning (ICL) settings on the models performance? RQ5: How does domain knowledge (science, mathematics, commonsense) influence performance? In the following sections, we will discuss these research questions in turn."
        },
        {
            "title": "4.1.1 RQ1: Influence of Model & Scale\nModel As shown in Fig 2, they fail to generalize\nacross the different tasks, especially on the First\nError Identification (FEI) of step correctness and\nMajority Voting (MV) of answer aggregation, in-\ndicating potential limitations in their ability to ef-\nfectively process and integrate multimodal infor-",
            "content": "Model Name Overall Step Correctness Answer Aggregation Reasoning Process Search FEI AEI Avg. BoN MV Avg. F1 MCC Avg. 9.1 Random 50.0 29.6 InternVL2.5-1B InternVL2.5-8B Qwen2.5-VL-3B Qwen2-VL-7B InternVL2.5-26B InternVL2.5-38B 31.8 22.2 49.1 35.6 31.30.5 48.3+16.5 36.7 56.9 46.8 42.4+10.6 10.3 51.7 31.0 49.7+17.9 55.3 28.2 38.9 48.2 24.9 1.6 37.6+5.8 51.3 29.1 6.8 42.4+10.6 45.7+13.9 9.8 54.2 32.0 55.8+24.0 24.8 65.2 45.0 41.8+10.0 15.5 55.6 35.5 65.4+33.6 53.8 74.9 64.4 Gemini-2.0-flash-exp Gemini-2.0-thinking-exp 64.6+32.8 54.0 74.9 64.4 71.2+39.4 51.2 74.4 62.8 GPT-4o Qwen2.5-VL-72B QVQ Qwen2.5-VL-7B 40.9 40.9 47.1 33.3 79.5 58. 67.5 55.6 74.6 57.2 76.8 55.0 85.2 50.6 59.0 52.5 81.6 69.8 63.6 51.7 82.2 69.6 81.7 67.7 85.3 62.2 40.9 40.2 68. 61.5 66.9 65.9 67.9 55.8 75.7 57.7 75.9 74.7 73. 50.0 45.5 53.1 56.5 55.4 48.1 53.6 66.3 64.4 54. 70.5 69.8 84.7 0 -9.1 6.3 12.9 11.2 -3.8 7.1 32. 28.8 9.6 41.1 39.6 69.5 25.0 18.2 29.7 34.7 33.5 22.1 30. 49.4 46.6 32.2 55.8 54.7 77.1 Table 2: Performances comparison of models on MPBENCH. The best performance for each category and task is in bold, while the second-best performance is underlined. Random denotes the performance of random reward generation. In the Overall category, + (or ) indicates the performance gain compared to the random baseline. tently maintaining comparability, its performance in Reasoning Process Search (F1 and MCC) is significantly higher than existing open-source and proprietary models, indicating that stronger model capabilities are required for reward models to search paths during the inference stage. Scale Model performance on MPBench generally scales with size, most notably for Step Correctness and Reasoning Process Search  (Table 2)  . Weaker models (e.g., Qwen2.5-VL-3B) even perform below random chance on these assessments. This suggests that larger model capacity is crucial for complex reasoning, enabling better learning of correct/incorrect steps and navigation of the solution space. The disproportionate impact of scale on Step Correctness and Search suggests these tasks are cognitively demanding, requiring deeper reasoning process understanding and step-level evaluation."
        },
        {
            "title": "4.1.2 RQ2: Correlations of different Abilities\nThe capabilities of step correctness, answer aggre-\ngation, and reasoning process search show a posi-\ntive correlation. Fig 3 illustrates the interrelation-\nship among these capabilities. A positive linear\nrelationship is observed between step correctness\nand reasoning process search, with the improve-\nment rates in both generation and critique being\nnearly identical, despite step correctness focusing\nprimarily on training parsing. However, the linear\ncorrelation between step correctness and answer",
            "content": "Figure 2: Performance breakdown on MPBench. mation for error detection and answer selection. On the other hand, their performance on All Error Identification (AEI) and Best-of-N (BoN) is significantly higher, suggesting that directly leveraging LLMs for reward process reasoning during both training and inference might be more effectively achieved through AEI and BoN. This discrepancy in performance across different evaluation paradigms raises interesting questions about the optimal design and utilization of PRMs for multimodal reasoning tasks. Among the overall metrics, the state-of-the-art model GPT-4o performs the best. While consisFigure 3: Interrelationship between models capabilities in step correctness identify, answer aggregation, and reasoning process search. Each point on the graph represents model, with coordinates indicating its performance in step correctness identify(SC), answer aggregation (AA), and reasoning process search (RS). The graph features fitted lines for the scatter plots, denoted by blue lines for SC/AA, SC/RS, and AA/RS, while red dashed line represents the ideal growth line. The slope of this ideal growth line is the ratio of the random values of each metric. Figure 4: Impact of Error Position on Model Performance. (a) Distribution of error positions within the dataset. (b) Model performance on reasoning process search, measured by average F1 score and MCC, across different error positions. (c) Model performance on Step Correctness, measured by F1 score, across different error positions. Note: Step 1 and steps beyond 10 are truncated for improved visualization. aggregation is less pronounced. Although increases in step correctness lead to improvements in answer aggregation, the growth rate of answer aggregation is slower, suggesting that its enhancement requires targeted exploration beyond simply improving step correctness. As seen in Fig 3 (c), there is notable increase in reasoning process search as answer aggregation improves, indicating that reasoning process search benefits from enhanced answer aggregation capabilities."
        },
        {
            "title": "4.1.3 RQ3: Performance at different step",
            "content": "positions Fig 4 (a) displays the distribution of error positions within the dataset. As illustrated, the highest concentration of errors occurs at step 4, with gradual decline in frequency towards both earlier and later steps. While errors are present across all steps shown (2 through 10), the distribution is clearly skewed towards the mid-range of the reasoning process. The performance of different models on reasoning process search and step correctness at various error positions is shown in Figures 4(b) and (c), respectively. For reasoning process search (b), we observe general downward trend in performance as the error position increases. This suggests that models may struggle to effectively navigate the search space as the reasoning process lengthens, potentially due to the accumulation of errors or the exponential expansion of possible paths. In contrast, the performance on Step Correctness (c) appears less sensitive to the error position and, in some cases, even shows improvement in later steps. This could indicate that models become more confident in their step evaluations with more context or that training data might be biased toward later steps. Comparing the two evaluation paradigms, we can see clear divergence in their performance patterns across different error positions. While reasoning process search emphasizes the ability to identify the correct path early in the reasoning process, step correctness focuses on evaluating the correctness of individual steps, regardless of their Evaluation Model Science Commonsense Mathematics Step Qwen2-VL-7B-Instruct Correctness Gemini-2-thinking R:29.6 Answer GPT-4o Qwen2-VL-7B-Instruct Aggregation Gemini-2-thinking R:40.9 Process Search R:25.0 GPT-4o Qwen2-VL-7B-Instruct Gemini-2-thinking GPT-4o 29.00.6 67.3+37.7 63.1+33.5 66.3+26.9 77.8+36.9 75.7+34.8 29.3+4.3 53.1+28.1 77.8+52.8 31.3+1.7 64.7+35.1 66.2+36.6 71.2+30.3 68.6+27.7 72.9+32.0 32.7+7.7 59.4+34.4 77.5+52.5 29.20.4 38.2+8.6 46.3+16.7 68.5+27.6 72.7+31.8 68.6+27.7 24.30.7 42.9+17.9 70.0+45. Table 3: Performance of Models on MPBench Across Science, Commonsense, and Mathematics Domains. The table presents average scores for each evaluation paradigm (Step Correctness, Answer Aggregation, and Reasoning Process Search), compared to the performance of random baseline (R). advantage in this category. This suggests that GPT4os stronger capabilities are more effectively leveraged when dealing with complex, mathematically oriented problems within the process reward framework. In contrast, Qwen2-VL-7B-Instructs performance in mathematics falls below the random baseline, highlighting the difficulties faced by less capable models in this domain. This underscores the importance of robust model capacity for effectively utilizing process rewards, particularly when dealing with challenging problem domains. The performance differences across domains suggest that future research could benefit from developing domain-specific PRMs or training strategies that better equip models to handle diverse reasoning demands. Furthermore, the substantial gap between GPT-4o and other models in the mathematics domain indicates that this area remains significant challenge and promising direction for future advancements in process reward modeling."
        },
        {
            "title": "5 Conclusions",
            "content": "This work presents MPBench, novel multimodal benchmark designed for evaluating error identification in reasoning processes. Through detailed analysis of MLLM performance on MPBench, we have shed light on the strengths and weaknesses of these models when used as criteria for enhancing reasoning. Specifically, we examined three key aspects: step correctness identification, answer aggregation, and reasoning process search. Our investigation revealed linear correlation coupled with subtle inconsistencies between them. Additionally, our analysis across diverse domains (science, commonsense, and mathematics) demonstrated that mathematical reasoning presents significant challenge for current MLLMs, with performance lagging behind that observed in other domains. This Figure 5: The impact of ICL few-shot numbers on model performance. position. This suggests that these two paradigms capture different aspects of reasoning ability and may be relevant at different stages of the reasoning process."
        },
        {
            "title": "4.1.5 RQ5: Impacts of Domain\nTable 3 presents the results across different do-\nmains. The questions are categorized into three pri-\nmary domains: science knowledge, mathematics,\nand commonsense, consistent with the categories\nalready identified in M3CoT. As shown in the table,\nperformance generally declines in the mathematics\ndomain, which is arguably more challenging. No-\ntably, GPT-4o exhibits a significant performance",
            "content": "suggests that future research should prioritize the development of PRMs and training methodologies that are more robust to the complexities of mathematical reasoning."
        },
        {
            "title": "Limitations",
            "content": "Despite our best efforts throughout the entire benchmark construction process, MPBench may still contain inaccurate labels of error locations, particularly for the more challenging complex math problems."
        },
        {
            "title": "References",
            "content": "Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. 2024a. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024b. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng, Wanxiang Che, Min Li, and Libo Qin. 2024. Comt: novel benchmark for chain of multi-modal thought on large vision-language models. arXiv preprint arXiv:2412.12932. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. DeepMind. 2024. Gemini 2.0 flash experimenhttps://deepmind.google/technologies/ tal. gemini/flash/. Accessed: 2024-12-25. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. 2024. Selfexplore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards. arXiv preprint arXiv:2404.10346. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. 2024. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809. Brian Matthews. 1975. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)- Protein Structure, 405(2):442451. 2024. OpenAI. llms. learning-to-reason-with-llms/. Learning reason with https://openai.com/index/ to Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. 2025. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124. Qwen Team. 2024. Qvq: To see the world with wisdom. Qwen Team. 2025. Qwen2.5-vl. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Evaluating mathematiarXiv preprint and Pengfei Liu. 2024. cal reasoning beyond accuracy. arXiv:2404.05692. Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, et al. 2024. Atomthink: slow thinking framework for multimodal mathematical reasoning. arXiv preprint arXiv:2411.11930. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. 2023. Mr-gsm8k: metareasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. 2024. Mr-ben: meta-reasoning benchmark for evaluating system-2 thinking in llms. CoRR, abs/2406.13975. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek Wong, Xiaowei Huang, QiIs your ufeng Wang, and Kaizhu Huang. 2024. model really good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733. uate volunteers, assessed the validity of generated erroneous reasoning steps. Annotators reviewed original data, generated erroneous steps, and reasoning context, judging plausibility and alignment with expected error types. >95% agreement rate across paradigms confirms the datasets high quality and suitability for evaluating and training process reward models. Minor discrepancies likely reflect the subjective nature of error evaluation in complex reasoning."
        },
        {
            "title": "D Prompts",
            "content": "As introduced in Section 3, MPBench employs three evaluation paradigms to assess the capabilities of PRMs in the reasoning process: (1) Step Correctness, (2) Answer Aggregation, and (3) Reasoning Process Search. To evaluate these capabilities, we carefully designed prompts to query MLLMs (e.g., GPT-4o) and assess their performance as PRMs. Below, we provide an example prompt for each paradigm. Due to space limitations, we display only one example for each paradigm. D.1 Prompts for Step Correctness As discussed in Section 3.1.1, the Step Correctness paradigm evaluates PRMs ability to assess the correctness of each intermediate reasoning step. To test this capability, we designed the few-shot prompt as shown in Table 5. D.2 Prompts for Answer Aggregation The Answer Aggregation paradigm, as introduced in Section 3.1.2, examines PRMs ability to aggregate scores from multiple solutions and select the best candidate response. To evaluate this capability, we used the prompt detailed in Table 6. D.3 Prompts for Reasoning Process Search Finally, the Reasoning Process Search paradigm, described in Section 3.1.3, evaluates PRMs ability to guide the search for optimal reasoning steps during inference. To assess this capability, we employed the prompt as in Table 7."
        },
        {
            "title": "A Statistics of our benchmark",
            "content": "Evaluation Step Correctness Answer Aggregation Reasoning Process Search Science Commonsense Mathematics 2248 1535 2232 1133 496 973 322 437 369 All 3703 2468 Table 4: Statistics of MPBench."
        },
        {
            "title": "B Filtering",
            "content": "Step 1: Rule-Based Filtering for Format Adherence To ensure dataset consistency and usability, we implemented rule-based filtering, defining explicit rules for data instance format (question structure, solution format, metadata). Automated scripts flagged and removed instances violating these rules, ensuring uniformity and preparing the data for further quality control. Step 2: GPT-4 Review with In-Context Examples. Following the rule-based filtering, we employed more nuanced approach using GPT-4, to identify and remove unreasonable or nonsensical instances. To effectively guide GPT-4 in this task, we manually curated set of in-context (IC) examples. These examples consisted of both valid and invalid data instances, carefully chosen to illustrate the types of issues we were looking to identify, such as logical inconsistencies, factual errors, or nonsensical reasoning steps. We then presented GPT-4 with the remaining data instances and asked it to classify each instance as either \"reasonable\" or \"unreasonable\" based on the patterns observed in the IC examples. Step 3: Gemini-Based Difficulty Filtering To ensure dataset difficulty, we used Gemini-1.5-pro to score solution steps, calculating the absolute score difference between incorrect and correct steps. Instances with difference > 1 (deemed potentially too easy) were manually reviewed and removed if necessary. This multi-stage filtering process, combining automated rules, LLM review, and difficulty assessment, yielded high-quality dataset suitable for evaluating and training process reward models."
        },
        {
            "title": "C Human Verification",
            "content": "To ensure dataset quality, we conducted human verification of 300 randomly sampled instances (100 per evaluation paradigm). Three independent annotators, including one co-author and two undergrad1. System Prompt You are reasoning evaluator. Your task is to analyze problem-solving steps and provide structured assessments in JSON format. For each solution step, you need to evaluate: Score (-1 to +1): * +1: Completely correct reasoning * 0: Partially correct with some mistakes * -1: Completely incorrect * Use any value in between to indicate varying degrees of correctness Requirements: - Evaluate each step independently - Provide scores as floating-point numbers - Return results in strict JSON format: \"Score\": [scores] - Ensure arrays have the same length with the number of solution steps - Consider logical accuracy, mathematical coherence, and solution efficiency Example output format: {\"Score\": [0.8, -0.5, 1.0]} You will be presented with problem, its step-by-step solution and its final answer. Please analyze each step and provide your evaluation in the specified JSON format. 2. Few Shots User 1 Question: [Question] What might be the possible function of the area? [Choices] (A) Putting on makeup (B) Conducting business meetings (C) Taking rest (D) Displaying artwork Solution: Step 1. We can infer that the area closed off by the long red rope fence in the museum exhibit is used to display artwork as there is large painting hanging on the wall to the left of the bed with golden drape, which suggests that the person who owned this place loved artwork. Step 2. Additionally, the very large bed with long golden blanket draped around it on each side in dimly lit room is luxury item indicating that the person who owned it was wealthy, further supporting the idea that the area is museum exhibit. Step 3. The lights in the area may serve as spotlights to focus on the artwork. Step 4. The lights is suit for conducting business meetings Step 5. Therefore, we can conclude that the correct answer is D, displaying artwork Answer: Assistant 1 {\"Score\": [1.0, 0.7, 0.8, -1.0, 1.0]} User 2 Question: [Question] Which property do these three objects have in common? [Choices] (A) scratchy (B) flexible (C) fragile Solution: Step 1: Examine each object. Step 2: Determine if each object possesses the specified property. Step 3: flexible object is capable of being folded or bent without breaking easily. None of the objects meet this criteria. Step 4: scratchy object feels rough and causes itchiness when it comes into contact with the skin. None of the objects fit this description. Step 5: fragile object will shatter into multiple pieces if dropped. However, none of the objects are fragile. Step 6: All three objects are actually flexible. Step 7: Therefore, while previously concluded as option C, the steps now imply contradiction indicating they are flexible. Answer: Assistant 2 {\"Score\": [1, 0.6, 0.5, 0.8, -1, -1, 1]} Table 5: Few-shot Prompt for evaluating PRMs ability to assess the correctness of each intermediate reasoning step. 1. System Prompt You are reasoning evaluator. Your task is to analyze problem-solving steps and provide structured assessments in JSON format. For each solution step, you need to evaluate: Score (-1 to +1): * +1: Completely correct reasoning * 0: Partially correct with some mistakes * -1: Completely incorrect * Use any value in between to indicate varying degrees of correctness Requirements: - Evaluate each step independently - Provide scores as floating-point numbers - Return results in strict JSON format: \"Score\": [scores] - Ensure arrays have the same length with the number of solution steps - Consider logical accuracy, mathematical coherence, and solution efficiency Example output format: {\"Score\": [0.8, -0.5, 1.0]} You will be presented with problem and its step-by-step solution. Please analyze each step and provide your evaluation in the specified JSON format. 2. Few Shots User 1 Question: [Question] What might be the possible function of the areay? [Choices] (A) Putting on makeup (B) Conducting business meetings (C) Taking rest (D) Displaying artwork Solution: Step 1. We can infer that the area closed off by the long red rope fence in the museum exhibit is used to display artwork as there is large painting hanging on the wall to the left of the bed with golden drape, which suggests that the person who owned this place loved artwork. Step 2. Additionally, the very large bed with long golden blanket draped around it on each side in dimly lit room is luxury item indicating that the person who owned it was wealthy, further supporting the idea that the area is museum exhibit. Step 3. The lights in the area may serve as spotlights to focus on the artwork. Step 4. The lights is suit for conducting business meetings Step 5. Therefore, we can conclude that the correct answer is D, displaying artwork Assistant 1 {\"Score\": [1.0, 0.7, 0.8, -1.0, 1.0]} User 2 Question: [Question] If you had to select one option as the correct answer for the Precalculus problem shown in the picture, which one would you choose and why? [Choices] (A) None of the choices given in the text are correct. (B) the answer is option in the image (C) option in the image should be selected (D) the answer is option in the image Solution: Let = (α, 0, 0), = (0, β, 0), and = (0, 0, γ). α + Then the equation of plane ABC is given by β + Assuming the relationship between the coordinates is incorrect, we might hypothesize different approach where = α r2 = 3 By equating algebraic blunders, assume 3 α2 + 3 Then, the incorrect conclusion is 1 , misreading the relations and not equating correctly. Determine the incorrect answer based on the flawed assumption in the provided image Choices: (A) 9, (B) 5, (C) 36, (D) 1 Based on incorrectly assumed calculations, the answer would be option in the image. Consequently, due to faulty reasoning, the incorrect solution is choice (B) in the given text. Assistant 2 {\"Score\": [1.0, 0.6, 0.3, -1.0, -1.0, 0.0, -1.0]} γ2 = 3, without correct reasoning. 3 ,and = γ 3 gives 1 β2 + 3 γ2 . 3 ,q = β α2 + 3 β2 + 3 p2 + 1 q2 + 1 γ = 1. Table 6: Few-shot Prompt for evaluating PRMs ability to aggregate scores from multiple solutions and select the best candidate response. 1. System Prompt Your task is to evaluate the next step of reasoning or calculation based on THE GIVEN QUESTION and HISTORICAL REASONING STEPS. You will be provided with: 1. QUESTION. 2. HISTORICAL REASONING STEPS. 3. Candidates of next step. For each solution step, you need to evaluate: Score (-1 to +1): * +1: Completely correct reasoning * 0: Partially correct with some mistakes * -1: Completely incorrect * Use any value in between to indicate varying degrees of correctness Requirements: - Evaluate each candidate independently - Provide scores as floating-point numbers - Return results in strict JSON format: \"Score\": [scores] - Ensure arrays have the same length with the number of candidates - Consider logical accuracy, mathematical coherence, and solution efficiency Example output format: {\"Score\": [0.8, -0.5]} 2. Few Shots User 1 Question: [Question] What kind of snowboarders is the mountain in the picture suitable for? [Choices] (A) Beginner only (B) Not sure (C) Advanced only (D) All levels Historical reasoning steps: Step 1. The snowboarder in the picture is seen touching the ground with his arm. Step 2. It indicates that they are struggling to maintain their balance. Candidates of next step: So it suggests that the mountain is challenging for the man in the image. However, the snowboarder might also be intentionally leaning to perform trick, which some beginners may attempt, but it could also imply deceptive appearance of difficulty. Assistant 1 {\"Score\": [0.8, -0.7]} User 2 Question: [Question] Which property do these four objects have in common? [Choices] (A) salty (B) stretchy (C) transparent Historical reasoning steps: Step 1: Examine each object individually. Step 2: Determine if each object possesses the specified property. Step 3: stretchy object elongates when force is applied. However, the potato chips and the pretzel are not stretchy. Step 4: Potato chips are known for their salty taste. It is important to note that all four objects are salty. Candidates of next step: Step 5: While transparent object allows clear visibility through it, note that ocean water can appear transparent under certain conditions. Consequently, only the ocean water might be considered transparent, while the potato chips, the pretzel, and the fries are not. Step 5: transparent object allows clear visibility through it. However, the potato chips, the pretzel, and the fries are not transparent. Assistant 2 {\"Score\": [-0.5, 0.6]} Table 7: Few-shot Prompt for evaluating PRMs ability to guide the search for optimal reasoning steps during inference. 17 Figure 6: Erroneous Steps. 18 Figure 7: Multi-solution. 19 Figure 8: Action Trees."
        }
    ],
    "affiliations": [
        "HIT",
        "NUS",
        "SZTU",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute"
    ]
}