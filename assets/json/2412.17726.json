{
    "paper_title": "VidTwin: Video VAE with Decoupled Structure and Dynamics",
    "authors": [
        "Yuchi Wang",
        "Junliang Guo",
        "Xinyi Xie",
        "Tianyu He",
        "Xu Sun",
        "Jiang Bian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation. In this paper, we propose a novel and compact video autoencoder, VidTwin, that decouples video into two distinct latent spaces: Structure latent vectors, which capture overall content and global movement, and Dynamics latent vectors, which represent fine-grained details and rapid movements. Specifically, our approach leverages an Encoder-Decoder backbone, augmented with two submodules for extracting these latent spaces, respectively. The first submodule employs a Q-Former to extract low-frequency motion trends, followed by downsampling blocks to remove redundant content details. The second averages the latent vectors along the spatial dimension to capture rapid motion. Extensive experiments show that VidTwin achieves a high compression rate of 0.20% with high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and performs efficiently and effectively in downstream generative tasks. Moreover, our model demonstrates explainability and scalability, paving the way for future research in video latent representation and generation. Our code has been released at https://github.com/microsoft/VidTok/tree/main/vidtwin."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 6 2 7 7 1 . 2 1 4 2 : r VidTwin: Video VAE with Decoupled Structure and Dynamics Yuchi Wang1,2*, Junliang Guo2, Xinyi Xie2,3, Tianyu He2, Xu Sun1, Jiang Bian2 1Peking University 2Microsoft Research Asia 3CUHK (SZ) wangyuchi@stu.pku.edu.cn, 120040057@link.cuhk.edu.cn, xusun@pku.edu.cn {junliangguo, tianyuhe, jiang.bian}@microsoft.com https://github.com/microsoft/VidTok"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation. In this paper, we propose novel and compact video autoencoder, VidTwin, that decouples video into two distinct latent spaces: Structure latent vectors, which capture overall content and global movement, and Dynamics latent vectors, which represent fine-grained details and rapid movements. Specifically, our approach leverages an Encoder-Decoder backbone, augmented with two submodules for extracting these latent spaces, respectively. The first submodule employs Q-Former to extract low-frequency motion trends, followed by downsampling blocks to remove redundant content details. The second averages the latent vectors along the spatial dimension to capture rapid motion. Extensive experiments show that VidTwin achieves high compression rate of 0.20% with high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and performs efficiently and effectively in downstream generative tasks. Moreover, our model demonstrates explainability and scalability, paving the way for future research in video latent representation and generation. 1. Introduction The latent diffusion model has recently revolutionized the popular text-to-image field, with representative models such as the Stable Diffusion series [8, 28, 29, 31]. In this paradigm, the image autoencoder plays critical role by encoding the image into compact latent space, thereby alleviating modeling complexity and improving training efficiency of the diffusion model. Recently, there has been growing interest in adapting this paradigm for video latent representation and downstream video generation tasks [3, 4, * Work done during an internship at Microsoft Research Asia. Junliang Guo and Xu Sun are corresponding authors. Figure 1. An example illustrating the Structure and Dynamics latents. We select two frames, t1 and t2, and show the original and reconstructed video frames, labeled Orig. and Recon., respectively. S. Recon. and D. Recon. refer to the reconstructed frames decoded using only the corresponding Structure or Dynamics latents. The Structure latent captures the main semantic content and overall motion trends, while the Dynamics latent encodes local details and rapid movements. 12, 16, 26, 54]. However, due to the extra temporal consistency of videos compared to static images, simultaneously modeling visual content and temporal dependencies into latent space presents challenging problem. Upon reviewing previous works that explore the conversion of video into latent representations using autoencoders [16, 19, 26, 52, 54], we identify two main design philosophies. First, classical approaches, represent each frame (or group of frames) as latent vectors or tokens of uniform size [43, 51, 52]. This method is straightforward but overlooks the redundancy between frames. Video inherently exhibits continuity, indicating that adjacent frames typically differ only slightly in details, suggesting significant potential for further compression. The second emerging approach addresses this problem by dividing the representation into two types, i.e., single or few content frame(s) along with several motion latent vectors [19, 48, 54]. However, these decoupling methods over1 simplify the dynamic nature of video content, leading to unsatisfactory generation results, such as blurred frames. In this paper, we propose novel approach that encodes videos into two distinct latent spaces: Structure Latent, which represents the global content and movement, and Dynamics Latent, which captures fine-grained details and rapid motions. For instance, in the video of tightening screw shown in Fig. 1, the main semantic content, such as the table and screw, corresponds to Structure Latent, while fine-grained details, such as color, texture, and rapid local movementslike the screws downward motion and rotationare captured by Dynamics Latent. These components are combined to form the reconstructed video. To achieve this, we introduce the VidTwin model, designed to effectively learn these interdependent latent representations. Our approach addresses the shortcomings of previous methods that often neglect dynamic content, enabling video autoencoder capable of achieving high compression without compromising reconstruction quality. Specifically, we utilize the Spatial-Temporal Transformer [2] as the backbone of our video autoencoder and introduce two submodules to extract Structure Latent and Dynamics Latent, respectively. For the former, leveraging the powerful information extraction capabilities of the QFormer [24] architecture, we apply it solely to the temporal dimension to extract the low-frequency changing motion trends independently of spatial location. We then further downsample the latent in the spatial dimension to remove redundant details while retaining the most important object information. For the latter, since rapid motion information can be represented in low dimensions, we first downsample the latent vectors obtained from the encoder spatially, and then we average these vectors along both the height and width dimensions to further reduce their dimensionality. Additionally, we design mechanism to adapt the obtained latents for diffusion models by patchifying the two latent vectors and concatenating them as the training target for the diffusion model. Through experiments, we demonstrate that our model offers several advantages: (1) High compression rate: The decoupling design and more compact latent representation of VidTwin yield superior compression rate, achieving around 500 compression factor while maintaining high reconstruction quality. This significantly alleviates memory and computational burdens for downstream models, which are often challenged by the high dimensionality of video data. (2) Effectiveness for downstream tasks: Video autoencoders are commonly used within generative models, which require smooth latent space. We validate this with the UCF-101 [35] dataset, where our model performs comparably to some well-established models, demonstrating its (3) Explainability and adaptability to generative tasks. Scalability: As shown in Fig. 1, we carefully design the latent space to ensure meaningful and explainable representations, and preliminary experiments also suggest that the model exhibits scalability, both of which provide opportunities for further research and improvements. In conclusion, the main contributions of our work are summarized as follows: (1) Building on the philosophy of decoupling video representation into structure and dynamics, we propose novel video codec foundation model, VidTwin, which demonstrates effective decoupling with compact design. (2) Our VidTwin achieves high compression rate and strong reconstruction ability, and has been verified for its applicability and efficiency in generative models. (3) We highlight the importance of video latent representation in current research trends and hope our VidTwin can inspire or facilitate further related research. 2. Related Works 2.1. Visual Autoencoder With the rapid advancement of visual generation, there has been increasing attention on visual latent representation techniques. Given that the dominant methods for generation are now diffusion models and autoregressive approaches, two primary types of corresponding representations have emerged: (1) Continuous latent vectors: Stable Diffusion [29] was one of the pioneering works to utilize Variational Autoencoder (VAE) [21] for image encoding, with diffusion model then modeling this latent space. This approach has since inspired numerous subsequent works [5, 6, 8, 28]. For video data, several studies have incorporated 3D convolutions [1, 4, 16, 33, 36] or spatio-temporal attention mechanisms [15, 18, 26, 49] into the backbone, resulting in latent space specifically designed for video data, which facilitates more effective video generation. (2) Discrete tokens: In separate line of work, influenced by the success of language modeling in the NLP community, several models have explored discrete representations of visual information. VQ-VAE [38] introduced codebook into the VAE [21] training procedure to discretize the representation, while VQ-GAN [7] incorporated adversarial training to improve the quality of generated images. Later models further refined their architectures, such as replacing CNNs with Transformers [50] or improving quantization methods [25, 52]. For video data, some approaches treat frames as independent images for tokenization [10, 42, 56], while others incorporate 3D architectures to capture spatio-temporal features [9, 40, 51, 54]. Among these, MAGVIT-v2 [52] has emerged as prominent video tokenizer, proposing look-up-free quantizer and has been widely adopted in recent models. 2 Figure 2. Details of our model. After obtaining the latent from the Encoder, the process branches into two flows. The Structure Latent extraction module, FS, which consists of Q-Former and convolutional networks, extracts the Structure Latent component zS. The Dynamics Latent extraction module, FD, comprising convolutional networks and an averaging operator, extracts the Dynamics Latent component zD. Finally, using the decoding module, we align all latents to the same dimension and combine them before passing them into the Decoder. 2.2. Video Compression and Decoupling 3. Methodology Video compression is critical challenge in computer vision, and the philosophy of decoupling has been employed in traditional video codecs for many years. For instance, MPEG-4 [22] uses I-frames to represent key frames and macroblock motion to capture movement. Building on this concept, Video-LaViT [19] recently designed pipeline that transforms key frames and motion vectors into tokens, integrating them with large language models. Other representative methods for motion representation include MotionI2V [32], which uses pixel trajectories to capture motion, and [23], which employs optical flow for frame interpolation. Some approaches focus on specific video types, such as GAIA series [11, 44, 53] focuses on talking-face videos and uses self-cross reenactment to disentangle identity and motion, or iVideoGPT [48], which explores embodied videos. CMD [54] utilizes weighted average of all frames to represent content, while motion is learned by neural network. However, we identify several limitations in these methods, such as incompatibility with generative models, reliance on complex architectures, or unsatisfactory results due to excessive prior knowledge in some models. In contrast, we revisit the decoupling mechanism and propose novel approach. Experiments demonstrate that our method has great promise, and we hope it will inspire further innovation in the community. In this section, we introduce the VidTwin model. In Sec. 3.1, we provide an overview of the architecture of VidTwin. Subsequently, Sec. 3.2 describes the process of converting video into Structure Latent and Dynamics Latent, while Sec. 3.3 delineates the process of reconstructing the video from these two latents. In Sec. 3.4, we outline the training and inference pipelines, and lastly, in Sec. 3.5, we discuss design for adapting our proposed latents for use with diffusion models. 3.1. Overall Architecture classical autoencoder consists of an encoder and decoder D. Given video RCF HW , where C, , H, represent the channel, number of frames, height, and width, respectively, the encoder produces latent vector = E(x) Rcf hw, where c, , h, are corresponding dimensions with but with lower dimensions. The decoder attempts to reconstruct the input as ˆx = D(z) = D(E(x)). The encoder and decoder are jointly trained to minimize the reconstruction loss Lrec = ˆx x. In our VidTwin model, we propose decoupling video into Structure Latent and Dynamics Latent components. As illustrated in Fig. 2, after obtaining the latent vector z, we introduce two processing functions, FS and FD, which generate the desired latent representations zS and 3 zD. These procedures are described in detail in Sec. 3.2.1 and Sec. 3.2.2. For decoding, we employ two functions, HS and HD, to align these latents to the same dimensional space before combining them and passing them to the decoder. The overall procedure is summarized as follows: zS, zD = FS (cid:0)E(x)(cid:1), FD (cid:0)E(x)(cid:1) ˆx = D(cid:0)[HS(zS); HD(zD)](cid:1) 3.2. Encode Video into Latents We will use Structure function and Dynamics function to extract Structure Latent and Dynamics Latent, respectively. 3.2.1. Structure Latent Extraction To extract the temporal low-frequency representation from the encoders output latent Rcf hw, we employ the Q-Former, classical interface proposed in BLIP-2 [24] that serves as bridge between different modalities. We choose this module due to its elegant architecture and proven ability to extract semantic information from visual input. It is Transformer [39] architecture with learned queries as input. In each block, the latent serves as condition to perform cross-attention, and the last hidden states are taken as the output. In our scenario, as shown in Fig. 2, we define the query as nq tokens (nq ) with dimension dq as input. Then, for the latent z, we turn it into sequence by merging the spatial dimensions into the batch dimension, resulting in dimension (hw, f, c). We then use an MLP to convert the channel dimension into dq, and perform standard QFormer operations along the temporal dimension. This process dynamically selects nq representative features from the frames. The final output is obtained as: = Qformer(z, q) R(hw)nqdq Notably, when we combine the height and width dimensions into the batch dimension, it compels the Q-Former to learn the general temporal motion trends independently of location, which aligns with our expectations. We now have the intermediate latent S, but it still faces two challenges: (1) Spatial compression has not been performed, resulting in high product of and w, and (2) the dimensionality of the Q-Formers hidden state, dq, remains high. To address these, we reshape into shape (nq, dq, h, w) and apply several convolutional layers to downsample the spatial dimensions while using bottleneck to reduce the channel dimension dq to smaller size dS. These operations reduce the dimensionality of the final Structure Latent while preserving main content information by eliminating detailed spatial information. Finally, we obtain the final Structure Latent zS RnqdS hS wS . 3.2.2. Dynamics Latent Extraction For dynamic local details, we consider that rapid motion information should be low-dimensional and distributed across each frame. Therefore, instead of manipulating the temporal dimension, we primarily focus on the spatial dimensions. natural approach to reduce the dimensions is to use spatial Q-Former to extract the most relevant spatial locations, similar to the method used for the Structure Latent. However, this approach disrupts spatial consistency, leading to performance degradation in our experiments. Instead, we design an alternative approach. As shown in Fig. 2, we first downsample the latent along the spatial dimensions using convolutional layers, obtaining an intermediate result with dimensions (f, D, hD, wD). Inspired by [54], we then average along the height and width dimensions to eliminate these spatial dimensions. The resulting vectors are concatenated and passed through head to reduce the channel dimension to dD: zD = ([avgh(z D); avgw(z D)]) Rf dD (wD +hD ) This results in the Dynamics Latent zD. Notably, this approach reduces the latent dimension from O(wD hD) to O(wD + hD), effectively extracting compact dynamic details while preserving spatial integrity. 3.3. Decode Latents to Video With the expected latents Structure Latent and Dynamics Latent obtained, we need to find way to combine them before inputting them into the decoder. For Structure Latent zS with shape (nq, dS, hS, wS), we apply upsampling layers to recover the spatial size and MLPs to adjust the channel dimension dS and query token number nq, yielding uS Rcf hw. For Dynamics Latent zD with shape (f, dD, wD + hD), we process the latents for height and width separately. Specifically, for latents z(h) Rf dD hD , we use MLPs to recover the corresponding spatial and channel dimensions, followed by repeating along the missing spatial dimension: Rf dD wD and z(w) = Repw(T (z(h) u(h) = Reph(T (z(w) u(w) )) Rcf hw )) Rcf hw Subsequently, we perform an element-wise addition of these latents and pass them to the decoder to obtain the final output video: ˆx = D(uS + u(h) + u(w) ) RCF HW 3.4. Training and Inference We train all modules, including the Encoder E, Decoder D, latent extraction modules FS, FD, and decoding heads HS, HD, jointly to recover the input. Following the standard loss definition for image autoencoders proposed in VQ-GAN [7], we employ the basic reconstruction loss Lrec 4 Table 1. Quantitative comparison with baseline methods. The bold values indicate the best results, while the underlined values represent the second-best. Sem., Tempo., and Deta. refer to semantic preservation, temporal consistency, and detail retention, respectively. Our model outperforms the baselines across multiple metrics, demonstrating its superior reconstruction ability. Method Compress. Rate PSNR LPIPS SSIM FVD Sem. Tempo. Deta. iVideoGPT [48] MAGVIT-v2 [52] CMD [54] EMU-3 [43] VidTwin (Ours) 1.50% 0.65% 6.85% 0.53% 0.20% 19.353 24.351 27.332 25.359 0.4677 0.3347 0.2732 0.2543 0.5752 0.6877 0.7746 0.7260 1693.10 653.88 468.47 353.71 4.28 4.43 4.51 4. 28.137 0.2414 0.8044 388.86 4.71 4.33 4.46 4.35 4. 4.62 3.59 3.97 4.22 4.60 4.73 along with feature-level perceptual loss Lp and adversarial losses LGAN . Considering that VidTwin is likely to be integrated into generative model, we expect the latent space to be sufficiently smooth. Thus, we adopt VAE paradigm, wherein instead of directly inputting the latents into the decoder, we introduce randomness around the latents, namely v(z) = µ(z) + σ(z) ϵ, where ϵ (0, I), and µ(z) and σ(z) are learnable modules predicting the mean and standard deviation. To regularize this distribution, we use the KL divergence loss with the standard Gaussian distribution LKL = KL(N (µz, σz)N (0, I)). More detailed explanations of the VAE model can be found in Appendix D.2. The final loss is defined as: = Lrec + λpLp + λGAN LGAN + λKLLKL During sampling, we use the mean of the latent, i.e., µ(z). If the required latents are predicted from generative model, we simply follow the decoding method mentioned in Sec. 3.3 to generate the final video. 3.5. Conditional Video Generation with VidTwin Typically, VidTwin is expected to connect with generative model. Here, we present basic design to adapt Structure Latent and Dynamics Latent for use in diffusion model and welcome other designs from the community. Given video, we first apply the trained VidTwin to obtain the Structure Latent latent zS and the Dynamics Latent latent zD. The dimension of zS is (dS, nq, hS, wS), which resembles video-like data. For zD, we combine the latents along the height and width dimensions, introducing pseudo-dimension in the second dimension to yield (dD, 1, f, hD+wD), effectively treating it as single-frame video. We then apply 3D patchification method to convert both latents into two sequences of tokens, each with dimension dDiff. Since these token embeddings originate from different latents, we align them to similar scale through normalization and then concatenate them along the length dimension to form the training target. With the ground-truth latent training target y0 and any relevant conditions (such as text or video class), we perform the standard diffusion training procedure [14]. This involves sampling noise, adding it to the latent to get the noisy version yt, and then attempting to remove the noise using learnable model Di. We utilize the current popular mechanism to predict x0 directly, defined as LDiff = Di(yt, c) y0. During sampling, we follow the DDIM [34] method to predict ˆy0. We also employ classifier-free guidance [13] to further enhance the models conditioning capabilities. More details about the diffusion model can be found in Appendix C.3. Finally, we input the predicted latents into the decoder of VidTwin to generate the final output video. 4. Experiments We conduct experiments to validate the proposed VidTwin model, from aspects including the compression rate, reconstruction ability, as well as the effectiveness and efficiency on downstream tasks. 4.1. Setup 4.1.1. Datasets For training, we utilize self-collected large-scale textvideo dataset, containing 10 million video-text pairs. Considering the broad variety of content and motion speed in this dataset, we believe training on this dataset is good choice to fulfill our design philosophy. For evaluation, we use the MCL-JCV dataset [41], which is classical dataset for evaluating video compression quality. Moreover, to verify the adaptability of the latent emitted by our model to generative models, we evaluate the class-conditioned video generation ability on the UCF-101 [35] dataset, which provides 101 different classes of motion videos. 4.1.2. Implementation Details We train our model on 8 fps, 16-frame, 224 224 video clips and evaluate on 25 fps, 16-frame, 224 224 video clips. The backbone of our model is Spatial-Temporal Transformer [2] with hidden dimension of 768 and patch size of 16. Both the encoder and decoder consist of 16 layers, each with 12 attention heads, resulting in total of 5 Figure 3. Qualitative comparison with baseline methods. Two examples are presented: gradually rotating photo and fast-motion boxing scene. VidTwin demonstrates the ability to reconstruct fine details and accurately capture rapid motion. about 300M parameters. For the latent size, in one of our configurations, we set hS = hD = 7, with dimensions 4 and 8, respectively, resulting in two latents with dimensions 7 7 16 4 and 16 14 8 for 16 3 224 224 video clip. The model is trained on 4 A100 GPUs. We use the Adam optimizer with learning rate of 1.6e-4. Additional hyperparameters and model settings are provided in Appendix C. Each evaluator is presented with 20 samples and asked to rate each on scale from 1 to 5. The final evaluation score is computed as the Mean Opinion Score (MOS), representing the average rating across evaluators. Moreover, we also introduce the Compression Rate (Compress. Rate) metrics, which we define as the ratio between the dimension of the latent space (or token embeddings) used in the downstream generative model, and the input videos dimension. 4.2. Baselines and Evaluation Metrics 4.3. Reconstruction Quality We select several state-of-the-art baselines, including models that represent videos as latents with uniform size, such as MAGVIT-v2 [52] and the visual tokenizer of EMU3 [43], as well as models that decouple content and motion, like CMD [54] and iVideoGPT [48]. We compare these baselines with our model using standard reconstruction metrics, including PSNR [17], SSIM [46], LPIPS [55], and FVD [37]. Additionally, we conducted human evaluation by inviting 15 professional evaluators to assess the results based on three criteria: semantic preservation (Sem.), temporal consistency (Tempo.), and detail retention (Deta.). As shown in Tab. 1, our model achieves state-of-the-art performance across most objective and subjective metrics, demonstrating strong capabilities in video reconstruction. The vision tokenizer of EMU-3 achieves the best FVD score and good reconstruction ability, likely due to the large dataset it was trained on (InternVid [45]). While most models perform well in semantic preservation and temporal consistency, they vary significantly in detail retention, where our model outperforms the others. Additionally, our model utilizes highly compact latent space, approximately 2.5 It is ento 30 times smaller than those of the baselines. 6 Figure 4. An illustration of cross-replacement example, where Video is generated using the Structure Latent from Video and the Dynamics Latent from Video B. couraging to see that our model achieves comparable or superior reconstruction quality with such low-dimensional latent space, highlighting the efficiency and effectiveness of VidTwin. We also train our architecture at different parameter scales, and larger models perform even better. This scalability is likely due to our Transformer-based architecture; further details can be found in Appendix A.4. Furthermore, we conduct case studies, with qualitative results shown in Fig. 3 (zoom in to observe finer details). For the left case, we observe that our model effectively captures local details and the gradual rotation of the object. In comparison, baselines such as CMD show blurred edges and incomplete rotation. In the right case, featuring the fast motion of man boxing, all baselines struggle to accurately capture the rapid movements, resulting in ghosting artifacts. In contrast, VidTwin produces significantly clearer results, demonstrating the effectiveness of our decoupling strategy for capturing both low-frequency changing objects and rapid local motion. More cases are presented in Appendix A.1. 4.4. Further Analysis As highlighted in Sec. 1, our VidTwin not only demonstrates strong reconstruction capabilities but also excels in explainability, efficiency, and adaptability with generative models. In this section, we provide evidence to support these claims. 4.4.1. Explorations on the Roles of Latents In VidTwin, we design two distinct latents: Structure Latent for the main object and overall movement trend, and Dynamics Latent, which captures local details and rapid motions. We present two experiments that provide insight into their respective roles. 7 Figure 5. We present the FLOPs and training memory costs of the unified generative model, as applied to our model and the baselines. First, as discussed in Sec. 3.3, we perform element-wise addition of the latents before inputting them into the decoder. This setup enables us to explore the outputs generated when each latent is passed through the decoder individually, i.e., generating results from D(uS) and D(uD). An example provided in Fig. 1 of the Sec. 1 illustrates the distinct differences between the two latents using scenario involving the screwing process. As observed, the Structure Latent captures the main semantic content, such as the table and screw, while the Dynamics Latent captures fine-grained details, including color and rapid local movements of the screw. Notably, in frame t2, where the screw drops, the video generated by the Structure Latent shows only slight change, whereas the one generated by the Dynamics Latent captures this immediate movement. This demonstrates the distinction between low-frequency and high-frequency movement trends. , uB Second, we conduct cross-reenactment experiment in which we combine the Structure Latent from one video, A, with the Dynamics Latent from another video, B, to observe the generated output from the decoder, i.e., generating D(uA D). As shown in Fig. 4, the generated video inherits the main object (house) and overall structure from Video A, which provides Structure Latent, while the local color comes from Video B, which provides Dynamics Latent. Notably, we observe that the movement in the generated video inherits the rapid rotation from Video B, while adjusting the gradually downward camera view according to the scene in Video A. This further validates our motivation to decouple video content into overall structure and detailed dynamics. We provide additional examples for both settings in Appendix A. One additional note is that, as suggested by the name VidTwin, the Structure Latent and Dynamics Latent latents work together to generate the final video. These separate Table 2. The generative ability of our model and the baselines, as tested on UCF-101. Table 3. Ablation studies on the proposed techniques. Models TATS [9] MAGVIT-v2 [52] Video-LaViT [19] Ours FVD 332 58 275 analyses are intended to offer glimpse into the roles of each latent, but it is important to note that isolating them inevitably introduces information loss. In future work, we plan to explore additional methods for better understanding the intrinsic information stored in these separate latents. 4.4.2. Computation Resource Analysis for Generative Methods VidTwin PSNR SSIM 26.116 0.731 (a) w/o Disentanglement (b) w/o D. Latent Avg. (c) w/o S. Latent Qformer (d) w/o S. Latent Move Spa. 23.512 24.835 25.386 23.169 0.654 0.693 0.702 0.630 ing training, generation model based on our latent space will achieve even better performance. Models 4.5. Ablation Studies Through our decoupling design, we reduce redundancy, resulting in compact latents with high compression rate. key advantage of having lower-dimensional latents is the reduced computational resource requirements for downstream tasks. To demonstrate this, we compare the FLOPs and memory consumption of generative models based on representative baselines. For fair comparison, instead of using the original generative models from their respective papers, which vary significantly, we construct pseudo uniform DiT [27] architecture with uniform patch size, focusing solely on resource consumption rather than generative ability. The results are shown in Fig. 5. As observed, the downstream diffusion model that fits our latent space, which has higher compression rate, requires significantly fewer FLOPs and less training memory (4 to 8 times and 2 to 3 times smaller than the baselines, respectively). This reduction in resource consumption leads to improved deployment efficiency. Furthermore, given the smaller dimension of our latent space, it is possible to use smaller diffusion model to fit the distribution, further reducing resource requirements. Additional details about the pseudo DiT model used can be found in Appendix B.2. 4.4.3. Generative Quality of Diffusion Models As shown in Sec. 3.5, we design basic method to adapt our latent representations to the generation framework of DiT-based diffusion model. We evaluate the proposed method on the UCF-101 dataset [35] for class-conditional video generation, with the results reported in Tab. 2. Our model achieves performance comparable to several existing methods. It is important to note that the main focus of this paper is not on generation, and we have implemented only simple baseline model to evaluate the adaptability of our approach to the diffusion framework. Despite this, the results are promising and demonstrate that the latent space in VidTwin is well-suited for downstream generative tasks. We believe that with more refined design, larger dataset, and the incorporation of additional techniques durWe conduct an ablation study to assess the impact of our proposed designs by removing each one. The experiments are evaluated using the same number of training steps, and the results are presented in Tab. 3. The findings can be summarized as follows: (a) When we omit the disentangling paradigm and use single latent with similar compression rate, performance drops significantly, demonstrating that our decoupling approach not only produces meaningful latent representations but also enhances performance at the same compression rate. (b) As discussed in Sec. 3.2.2, replacing the averaging method with Spatial Q-Former to further compress the spatial dimensions of Dynamics Latent results in poorer performance, likely due to the disruption of spatial arrangement. (c) We propose using Q-Former to extract Structure Latent. When we replace it with simple convolution layers and an MLP to decrease the temporal dimension, performance degrades, highlighting the superior semantic extraction capability of the Q-Former. (d) As mentioned in Sec. 3.2.1, moving the spatial dimensions into the batch dimension to obtain location-independent latents is crucial. Without this, and by placing them into the hidden states dimension instead, we observe noticeable performance loss. 5. Conclusion In this paper, we present VidTwin, novel foundation model for video latent representation. VidTwin incorporates carefully designed submodules within an Encoder-Decoder framework to effectively separate Structure and Dynamics latent spaces. Through extensive experiments, we demonstrate that VidTwin achieves high compression rates, has simple architecture, and performs well in downstream generative tasks. Additionally, inspired by [47], the Structure Latent space in our model appears well-suited for visual understanding tasks, which we plan to explore in future work. Finally, our approach provides explainability and scalability, making it valuable for future research. We hope that our 8 work will inspire new decoupling techniques in the video community and contribute to advancements in both video generation and broader multimodal applications."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation, 2024. 2 [2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding?, 2021. 2, 5, 13 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. 1 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models, 2023. 1, 2 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. [6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. 2 [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis, 2021. 2, 4, 11 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 1, 2 [9] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer, 2022. 2, 8 [10] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Martın-Martın, and Li Fei-Fei. Maskvit: Masked visual pre-training for video prediction, 2022. 2 [11] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, Sheng Zhao, and Jiang Bian. Gaia: Zeroshot talking avatar generation, 2024. 3 [12] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation, 2023. [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. 5, 14 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 5, 15 [15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022. 2 [16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. 1, [17] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th International Conference on Pattern Recognition, pages 23662369, 2010. 6 [18] Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, Haiyang Sun, Kun Zhan, Peng Jia, and Miao Zhang. Dive: Dit-based video generation with enhanced control, 2024. 2 [19] Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, and Yadong Mu. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization, 2024. 1, 3, 8 [20] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. 13, 14 [21] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 2, 15 [22] Didier Le Gall. Mpeg: video compression standard for multimedia applications. Communications of the ACM, 34 (4):4658, 1991. 3 [23] Jaihyun Lew, Jooyoung Choi, Chaehun Shin, Dahuin Jung, and Sungroh Yoon. Disentangled motion modeling for video frame interpolation, 2024. 3 [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. 2, 4, 13 [25] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple, 2023. 2 [26] OpenAI. Video generation models as world simulators. 2024. 1, 2 [27] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 8, 13 [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 1, 2 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2 [30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. 9 Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation, 2024. 6 [46] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [47] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024. 8 [48] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, ivideogpt: Interactive Jianye Hao, and Mingsheng Long. videogpts are scalable world models, 2024. 1, 3, 5, 6, 11 [49] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 76237633, 2023. 2 [50] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan, 2022. [51] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer, 2023. 1, 2 [52] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 1, 2, 5, 6, 8, 11, 13 [53] Runyi Yu, Tianyu He, Ailing Zhang, Yuchi Wang, Junliang Guo, Xu Tan, Chang Liu, Jie Chen, and Jiang Bian. Make your actor talk: Generalizable and high-fidelity lip sync with motion and appearance disentanglement, 2024. 3 [54] Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, and Anima Anandkumar. Efficient video diffusion models via content-frame motion-latent decomposition, 2024. 1, 2, 3, 4, 5, 6, 11 [55] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. 6 [56] Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, and Jiang Bian. Video in-context learning. arXiv preprint arXiv:2407.07356, 2024. 2 [31] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023. 1 [32] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling, 2024. [33] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022. 2 [34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. 5, 14, 15 [35] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild, 2012. 2, 5, 8, 14 [36] Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, and Jiang Bian. Vidtok: versatile and open-source video tokenizer. arXiv preprint arXiv:2412.13061, 2024. 2 [37] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges, 2019. 6 [38] Aaron van den Oord, Oriol Vinyals, and Koray Neural discrete representation learning, Kavukcuoglu. 2018. 2 [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. 4, 15 [40] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description, 2022. 2 [41] Haiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang, Ioannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. Mcl-jcv: jnd-based h. 264/avc video quality assessment dataset. In 2016 IEEE international conference on image processing (ICIP), pages 15091513. IEEE, 2016. 5 [42] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video transformers, 2022. [43] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024. 1, 5, 6, 11 [44] Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, and Jiang Bian. Instructavatar: Textguided emotion and motion control for avatar generation, 2024. 3 [45] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui 10 A. Additional Experimental Results B. Additional Information on Experimental To enhance the visual experience, we strongly encourage viewing the videos on the website https://github. com/microsoft/VidTok/tree/main/vidtwin. A.1. Additional Reconstruction Examples Fig. 6 presents additional reconstruction examples. By zooming in, one can observe that our VidTwin effectively captures intricate details, such as raindrops in the first and second cases. Moreover, by decoupling structural and dynamic motion features, our model excels at preserving rapid motion dynamics. For example, in the third case, VidTwin accurately reproduces the light trails of fast-moving car, where other baselines fail to do so. A.2. Additional Decoupling Examples In Sec. 4.4.1, we demonstrated the ability to separately recover the Structure Latent and Dynamics Latent components. Additional examples are shown in Figure 7. Videos generated using Structure Latent predominantly capture primary structures and main objects, while those generated with Dynamics Latent focus on colors and rapid movements. notable example is observed in the bottom-right case, where fireworks visible in the first frame disappear in the second. However, the Structure Latent-generated video retains the fireworks from the first frame, demonstrating that Structure Latent effectively encodes low-frequency, gradually evolving information. A.3. Additional Cross-Reenactment Examples Fig. 8 provides further examples of the cross-reenactment experiments described in Sec. 4.4.1. In these examples, the generated videos inherit the basic structure from Video while incorporating local details and motions from Video B. Notably, motion patterns such as horizontal movements and wave-like motions, as seen in the two bottom cases, are effectively transferred. A.4. Initial Scalability Exploration In Sec. 4.3, we described training our architecture at varying parameter scales and observed consistent performance improvements with larger models. Tab. 4 summarizes the configurations of each model, evaluated at the same training step. The results demonstrate steady enhancement in reconstruction quality with increasing model size. In future work, we plan to explore additional model scales and investigate potential scaling laws, including exponential trends and other patterns."
        },
        {
            "title": "Settings",
            "content": "B.1. Baselines and Compression Rates This section provides details on the baselines used in our evaluation and discusses their compression rates, as outlined in Sec. 4.2. Notably, MAGVIT-v2 [52], iVideoGPT [48], and CMD [54] do not offer official code or pretrained checkpoints. Therefore, we reimplement these methods based on the descriptions provided in their respective papers. MAGVIT-v2 [52]: MAGVIT-v2 employs 3D causal CNN layers to downsample videos into latents, with temporal downsampling factor of 4 and spatial downsampling factor of 8. The latent dimension is set to 5, as reported in the paper, resulting in compression rate of: 5 3 4 8 8 0.65%. EMU-3 [43]: EMU-3 is generative model proposed by BAAI1. For our evaluation, we primarily utilize its video tokenizer, which is based on SBER-MoVQGAN2. This tokenizer incorporates two temporal residual layers with 3D convolutional kernels in both the encoder and decoder modules, enhancing video tokenization. Similar to MAGVITv2, it achieves 4 temporal compression and 88 spatial compression. The compression rate, with latent size of 4, is calculated in the same manner. CMD [54]: CMD decouples video representations into content frames and motion latents. For video of size (c, f, h, w), the content frame has dimensions (c, h, w), and the motion latent is (d, + w, ), where is the dimension of the motion vector. Based on the settings described in the paper, the compression rate is: 1 + d(h + w) chw = 1 + 2 224 32 3 224 224 6.9%. The primary bottleneck lies in the content frame, and we hypothesize that longer video clips could reduce the compression rate (though at the potential cost of performance). iVideoGPT [48]: iVideoGPT employs conditional VQGAN [7] with dual encoders and decoders. The context frames 1 : T0 are encoded using N0 tokens, while subsequent frames are encoded with fewer tokens (n), condi1https://www.baai.ac.cn/ 2https://github.com/ai-forever/MoVQGAN 11 Table 4. Settings and performance of VidTwin at different scales. Models Depth Num. Heads Dim. Hidden Num. Params. PSNR SSIM VidTwin small VidTwin base VidTwin large 12 16 16 8 12 12 512 768 1536 126M 335M 1.3B 24.83 26.13 27.16 0.683 0.732 0. Figure 6. Additional reconstruction cases comparing our VidTwin model with baselines. Zoom in to observe finer details. tioned on the context tokens to capture the essential dynamics. The compression rate is given by: and, based on the information in the paper, we calculate it as: N0d + n(T T0)d , 2 162 64 + 14 42 64 3 16 1.5%. 12 Figure 7. Additional examples of decoupling Structure Latent and Dynamics Latent. B.2. Pseudo DiT for Resource Consumption Evaluation Our VidTwin model offers highly compressed latent space, significantly reducing the resource requirements of downstream generative models. To validate this, in Sec. 4.4.2, we compare the performance of generative model applied to the latent spaces produced by VidTwin and the baselines. For fair comparison, we utilize the same DiT [27] architecture in all experiments. The configuration includes 6 layers, 8 attention heads, hidden dimension size of 512, and feed-forward network (FFN) dimension of 2048, resulting in total of 12,610,560 parameters. Additionally, unified patch size of 2 is used for all dimensions. We calculate the FLOPs using single sample (batch size = 1). For memory consumption, we employ the Adam [20] optimizer and record the maximum GPU memory usage during training. C. Implementation Details C.1. Model Details sion. Temporal attention uses causal masking, ensuring that earlier frames do not attend to later ones, similar to the configuration in MAGVIT-v2 [52]. We evaluate three different scales (outlined in Tab. 4) by adjusting the depth, hidden state dimensions, and other parameters. For spatial dimensions, patch size of 16 is used for both height and width, while for the temporal dimension, the patch size is set to 1. The Q-Former [24], employed for extracting Structure Latent components, consists of 6 layers with hidden dimension of 64 and 8 attention heads. For downsampling, we primarily use convolutional layers with stride of 2, while upsampling is performed using Upsample layers with factor of 2. By varying the number of convolutional layers, latents of different sizes can be generated. Recommended latent size settings are as Tab. 5. From our experiments, we see that these configurations exhibit minimal performance differences, allowing users to select setting based on specific requirements. C.2. Data and Training Details The key hyperparameters for training data and optimization are summarized as Tab. 6. As described in Sec. 3.1, our VidTwin adopts an EncoderDecoder architecture. Specifically, we utilize SpatialTemporal Transformer [2] backbone. In each block, spatial attention is first applied to the height and width dimensions, followed by temporal attention along the temporal dimenC.3. Diffusion Model Details In Sec. 4.4.3, we describe the design of diffusion model tailored to the latent space of our VidTwin model. This model adopts the DiT [27] architecture with 18 layers and Figure 8. Additional examples of cross-reenactment. Table 5. Recommended settings for latent sizes. Setting Structure Latent Dynamics Latent 2 3 hS = wS = 7, nq = 16, dS = 4 hD = wD = 7, dD = 8 hS = wS = 7, nq = 16, dS = 4 hD = wD = 4, dD = hS = wS = 7, nq = 12, dS = 4 hD = wD = 7, dD = 8 hidden state size of 1152. Conditioning is introduced via cross-attention, and for the UCF-101 dataset [35], we use 256-dimensional vector to encode the class information. The diffusion process consists of 1000 steps, with DDIM [34] used as the sampling strategy and 50 steps for inference. Classifier-free guidance [13] is applied, where conditioning is randomly dropped in 20% of the samples during training. The classifier-free guidance weight is set to 5 during sampling. For training, we use the Adam optimizer [20] with β1 = 0.9, β2 = 0.999. The learning rate is managed with Lambda scheduler and includes 10,000 warmup steps. Training is conducted on 8 40G A100 GPUs, with an input configuration of 16 video frames at resolution of 224. D. Basics for Diffusion Models and VAE D.1. Basics for Diffusion Models Diffusion models are class of emerging generative models designed to approximate data distributions. The train14 Table 6. Training Configuration Parameter Value Input Video Resolution 224 Input Video Frames Input Video FPS 8 Optimizer Learning Rate Warmup Steps Adam; β1 = 0.9, β2 = 0.99 1.6 104 Learning Rate Scheduler Cosine Annealing Lp Weight Decay LGAN LKL 0. 0.0001 0.05 0.001 Training Batch Size 6 Training Device 4 80G A100 GPUs ing process consists of two phases: the forward diffusion process and the backward denoising process. Given data point sampled from the real data distribution, x0 q(x)3, the forward diffusion process gradually adds Gaussian noise to the sample, generating sequence of noisy samples x1, . . . , xT . The noise scales are controlled by variance schedule βt (0, 1), and the density can be expressed as: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI). Using the reparameterization trick [14], this process allows for sampling at any arbitrary time step in closed form: q(xtx0) = (xt; αtx0, 1 αtI), where αt = 1 βt and αt = (cid:81)t i=1 αi. From this, it is evident that as , xT converges to an isotropic Gaussian distribution, aligning with the initial condition used during inference. However, obtaining closed form for the reverse process q(xt1xt) is challenging. When βt is sufficiently small, the posterior also approximates Gaussian distribution. In this case, model pθ(xt1xt) can be trained to approximate these conditional probabilities: pθ(xt1xt) = (xt1; µθ(xt, t), Σθ(xt, t)), where µθ(xt, t) and Σθ(xt, t) are parameterized by denoising network fθ, such as U-Net [30] or Transformer [39]. By deriving the variational lower bound to 3We follow the notation and derivation process of https : / / lilianweng.github.io/posts/20210711diffusionmodels. optimize the negative log-likelihood of x0, Ho et al. [14] introduces simplified DDPM learning objective: Lsimple = (cid:88) t=1 Eq (cid:2)ϵt(xt, x0) ϵθ(xt, t)2(cid:3), where ϵt represents the noise added to the original data x0. In our work, we adopt simpler architecture that directly predicts x0, with the loss function defined as: = x0 fθ(xt, t). During inference, the reverse process begins by sampling noise from Gaussian distribution, p(xT ) = (xT ; 0, I), and iteratively denoising it using pθ(xt1xt) until x0 is obtained. DDIM [34] refines this process by ensuring its marginal distribution matches that of DDPM. Consequently, during generation, only subset of diffusion steps {τ1, . . . , τS} is sampled, significantly reducing inference latency. D.2. Basics for VAE Variational Autoencoders (VAEs) [21] are class of generative models that combine probabilistic reasoning with neural networks to learn the underlying distribution of highdimensional data. VAE consists of two components: an encoder and decoder. The encoder maps input data to latent variable characterized by probabilistic distribution q(zx), typically parameterized as Gaussian. The decoder reconstructs the input by sampling from the latent space and generating data through p(xz). To ensure that the latent space conforms to structured prior distribution, typically standard Gaussian p(z) = (0, I), VAEs optimize the Evidence Lower Bound (ELBO): = Eq(zx)[log p(xz)] DKL(q(zx)p(z)), where the first term represents the reconstruction loss, ensuring that the generated data resembles the input, and the second term is the Kullback-Leibler divergence, which regularizes the latent space. key point of VAEs is the reparameterization trick, which facilitates gradient-based optimization by expressing the latent variable as: = µ + σ ϵ, ϵ (0, I), where µ and σ are outputs of the encoder network. VAEs have found applications in areas such as image synthesis, data compression, and representation learning due to their ability to generate diverse, high-quality samples while maintaining interpretability of the latent space. In our work, we employ VAE as the backbone model and introduce two submodules to decouple the video latent representation effectively."
        }
    ],
    "affiliations": [
        "CUHK (SZ)",
        "Microsoft Research Asia",
        "Peking University"
    ]
}