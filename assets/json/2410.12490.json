{
    "paper_title": "Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective",
    "authors": [
        "Yongxin Zhu",
        "Bocheng Li",
        "Hang Zhang",
        "Xin Li",
        "Linli Xu",
        "Lidong Bing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling by applying K-Means on the latent features of self-supervised learning models. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at \\url{https://github.com/DAMO-NLP-SG/DiGIT}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 3 ] . [ 2 0 9 4 2 1 . 0 1 4 2 : r Stabilize the Latent Space for Image Autoregressive Modeling: Unified Perspective Yongxin Zhu1,3, Bocheng Li2,3, Hang Zhang4, Xin Li, Linli Xu 2,3, Lidong Bing 1School of Data Science, University of Science and Technology of China 2School of Computer Science and Technology, University of Science and Technology of China 3State Key Laboratory of Cognitive Intelligence, 4Zhejiang University zyx2016@mail.ustc.edu.cn,bcli@mail.ustc.edu.cn,hangzhang_scu@foxmail.com lixin4ever@gmail.com,linlixu@ustc.edu.cn,binglidong@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established commanding presence. To address this discrepancy, we introduce unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling by applying K-Means on the latent features of self-supervised learning models. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at https://github.com/DAMO-NLP-SG/DiGIT."
        },
        {
            "title": "Introduction",
            "content": "In recent years, remarkable advancements have been achieved in the field of image generation, principally propelled by the development of latent-based generative models, such as Latent Diffusion Models (LDMs) [34, 30] and Mask Image Models (MIMs) [7, 26]. By employing reconstructive autoencoders such as VQGAN [15] or VAE [23] to compress images into manageable low dimensional latent space, these models can generate highly realistic and imaginative samples. Concurrently, in light of the transformative impact of autoregressive (AR) generative models, such as Large Language Models [31, 32, 5, 27] in NLP, it becomes compelling to investigate the feasibility of similar paradigms to images. Despite the advances in image autoregressive pre-training, exemplified by Corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Figure 1: (a): Linear probe and class-unconditional generation performance of different methods trained and evaluated on ImageNet-1K. (b): Class-conditional generation performance of different methods on ImageNet-1k. The size of the bubbles indicates the number of parameters in the models. DiGIT achieves SOTA performance in linear probing and establishes new SOTA in image generation within single model. models such as iGPT [8], AIM [14] and GIVT [36] in the pixel space, VIM [41] and LVM [1] in the latent space, their performance are still inferior to the leading models [34, 12, 7] in image generation, or self-supervised learning models [18, 10, 6, 28] in image understanding tasks. An intriguing observation emerges regarding the presumed optimality of current practices in latent space: as illustrated in Figure 1(b), though sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in the image generation task. This discrepancy prompts reevaluating our understanding of latent spaces and their interaction with generative models, suggesting unexplored avenues worth investigating. central premise in learning theory [20] is that the latent distribution should retain as much critical information of the data distribution as possible, akin to compression goal. This leads to common misconception that an optimal latent space for reconstruction equates to optimal generative performance. Nevertheless, in the investigation regarding the reconstruction and generation ability with the popular VQGAN model [42], it is observed that the generation FID will deteriorate when the reconstruction FID becomes lower (where lower value indicates better performance), challenging the above assumption. To address these intriguing discrepancies, we introduce unified perspective on the relationship between latent spaces and generative models to analyze what constitutes an optimal latent space for generative models. Our findings reveal that, beyond the compression techniques employed by prevalent latent generative models, an optimal latent space should also aim to minimize the distance between distributions under the condition of incorporating generative model, which is an aspect often overlooked. We critically assess prevalent methodologies and reveal that the stability of latent space is important for generative models. We argue that the reason why autoregressive models underperform iterative models such as LDMs and MIMs is that iterative models can correct errors brought by the instability of latent space. Drawing from this insight, we propose straightforward method to stabilize the existing latent space methods for image autoregressive generative models. Unlike conventional autoencoder-style approaches, our approach disentangles the concurrent training of encoders and decoders and commences with encoder-only training through discriminative self-supervised model [28]. This phase does not necessitate decoder for pixel reconstruction, enabling the encoder to discern the intrinsic and distinguishable features present within the data. Subsequently, separate decoder of the autoencoder [15] is trained and tasked solely with the pixel reconstruction process, conditioned on the features identified by the encoder. By focusing initially on the encoders capability to extract meaningful data features independently of pixel reconstruction, we lay foundation for more stable and feature-rich latent 2 space. The subsequent independent training phase of the decoder ensures that these captured features can be accurately translated back to pixels. In support of the autoregressive generative model, which requires discrete tokens for next token prediction, we employ strategy inspired by VQGAN [15] to discretize the encoders latent feature space with the K-Means clustering algorithm. With this novel image tokenizer induced from the stabilized latent space, the performance of autoregressive generative models in images is enhanced significantly in both image understanding and image generation tasks. We refer to this approach as call Discriminative Generative Image Transformer (DiGIT). Notably, when scaling up the model size, substantial improvements can be achieved. To the best of our knowledge, this is the first evidence that image autoregressive generative models behave analogously to GPT. In essence, this work endeavors to redefine the boundaries of what is possible in image autoregressive modeling through unified perspective of latent space. In summary, our contributions to the field of image generative models include: We introduce unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. We propose novel method to stabilize latent space by disentangling the encoder and decoder training processes. Furthermore, simple yet effective discrete image tokenizer is proposed to improve the image autoregressive generative models performance under the philosophy of next token prediction. The experimental results show that the image autoregressive modeling with our tokenizer leads to SOTA performance in image understanding and generation, with further improvements witnessed when scaling up the model size."
        },
        {
            "title": "2 Problem Formulation",
            "content": "In section 2.1, we formalize the latent space requirements for generative models and categorize current latent-based generative models. Furthermore, in section 2.2, we analyze the stability of different induced latent spaces and propose to stabilize the latent space for autoregressive generative models instead of stabilizing the generation process with an iterative decoding strategy like LDMs. 2.1 Latent Space for Generative Models Drawing inspiration from the complexity perspective of latent space induced from autoencoders in Hu et al. [21], we delve into the latent space for generative models. Generative models aim at learning distribution to approximate the data distribution PX . Formerly, given tractable prior distribution PZ and distance metric D(, ) between distributions, the purpose of generative model is to minimize the distance between data distribution PX and the distribution generated by g(Z): min D(Pg(Z), PX ). (1) For example, GANs [16] employ the Gaussian distribution as their prior and utilize discriminator network as the distance metric. However, the optimal strategy for data representation in generative models is still under-explored. Recent studies on latent diffusion models [34] have identified that direct learning in the pixel space of images is suboptimal. They propose to learn in latent space induced by constrained autoencoder model such as VAE [23] or VQGAN [15], which has been demonstrated to improve the perceptual quality. simple method to construct the latent space is using an encoder : Rd Rdz to map raw data samples Rd into latent space (X) of dimension dz. Consequently, the goal of latent-based generative models is to learn the distribution as per the following formula: min D(Pg(Z), Pf (X)), (2) where Pf (X) denotes the data distribution in the latent space induced by the encoder . Despite these advances, determining the optimal latent space configuration for generative models remains unresolved. Distance between distributions in different spaces. Given the ultimate goal of the generative model is to produce image pixels, decoder model : Rdz Rd, paired with the encoder model , is necessary to convert latent representations back into pixels. We define generalized distance between different spaces associated with decoder as: DH(Pf (X), PX ) := inf hH D(Ph(f (X)), PX ). (3) By employing DH to compare distributions across different spaces, we can define the ideal latent space (X) as the one that minimizes DH(Pf (X), PX ). This implies selecting latent representation that minimizes the empirical objective L(hPf (X)) with the same family of decoder models. Such latent configuration depends on both the data and the decoder training methodology. We can formalize it with an autoencoder framework by looking at the encoder and decoder together, and the primary goal becomes the reconstruction of the input sample x, formulated as minf,h L(h(f (x)), x). Once generative model successfully approximates the latent distribution Pf (X), the generated sample can be efficiently transformed back into pixels using the decoder h. Similarly, we can define the distance between distributions in the latent space (X) Rdz and data space Rd conditioned on the latent-based generative model G, DG(Pf (X), PX ) := inf gG D(Pg(Z), Pf (X)), (4) which aims to minimize the empirical objective with the generative model family G. Optimal Latent Space for Generative Models. Now that we have characterized the ideal latent distribution given the family of generative models and the data, the next step is to determine how to find the optimal latent space. At the population level, the objective for the latent-based generative models with decoder is: min hH,gG D(Ph(g(Z)), PX ) = min hH,gG D(Ph(f (X)), PX ) + D(Pg(Z), Pf (X)), (5) where the first term focuses on optimizing the decoder to enhance the reconstruction quality and the second one is directed towards optimizing the generative model to more accurately approximate the latent space distribution. Inspired by this observation, we can characterize the optimal latent distribution (X) for given PX from the perspective of minimizing the distance between distributions in different spaces by defining as argmin Dlatent(Pf (X), PX ) := (X) = argmin F DG(Pf (X), PX ) + DH(Pf (X), PX ) (6) Notice that Pf (X) depends on multiple factors, including PX , the distance metric D, and the constructed model families and H. By integrating DG and DH, we arrive at comprehensive measure of the distance between the distribution in the latent space and the original data distribution as Dlatent(Pf (X), PX ). The second term, DH(Pf (X), PX ), is exactly the objective of reconstructive autoencoders. Ultimately, from examining the learning objective pertinent to identifying the optimal latent space for generative models, it becomes evident that: reconstructive autoencoder does not necessarily establish an advantageous latent space for generative models. Two Pathways of the Latent Space Construction. Although we theoretically analyze the optimization of the optimal latent space for generative models, it is challenging to implement in practice because optimizing (f, g, h) simultaneously is computationally complex. practical solution is to optimize DG and DH separately, allowing for tractable training. When DH(Pf (X), PX ) is not target for optimization, it implies that the optimization of the decoder within the generative model framework is bypassed. The encoder independently forms latent space, aligning with self-supervised learning (SSL) strategies aimed at uncovering lower-dimensional features from unlabeled data. However, learning the generative models in the latent space induced by SSL models remains relatively unexplored. On the other hand, when DG(Pf (X), PX ) remains fixed, the primary objective becomes optimizing the encoder and decoder to effectively learn and represent the latent space, 4 where Dlatent(Pf (X), PX ) degrades into an autoencoder learning objective. This approach is evident in recent latent-space-oriented generative models, such as LDM [34, 30], VQGAN (AR)[15], and MaskGIT (MIM) [7], all of which concentrate on learning in the latent space with the encoder and decoder frozen. 2 While latent generative models such as LDM [34], MaskGIT [7], and VQGAN [15] share the same latent space induced by reconstructive autoencoder [15] to minimize DH(Pf (X), PX ), their image generation performances differ significantly. In the next section, we analyze the reason behind it from the perspective of the latent space. 2.2 Stability of the Latent space We first describe the decoding mechanism of various latent generative models. Both LDM [34] and MaskGIT [7] can be depicted as an iterative sampling procedure given by: p(xT ) = (cid:89) i=1 p(xixi1). (7) The intermediate states xi in LDMs represent images infused with Gaussian noise of varying variance, whereas for MaskGIT, they denote discretely tokenized images augmented with masks. In contrast, the autoregressive framework of VQGAN (AR) is described as: p(x) = p(x1, . . . , xN ) = (cid:89) j=1 p(xjx<j), (8) where xi represents the i-th patch in the image sequence. Notice that xi denotes the entire image while xj means the local patch tokens. In the autoregressive decoding process, if the previously sampled tokens are incorrect, the accuracy of subsequent tokens would be affected due to error aggregation. In contrast, the iterative decoding approach allows for the revision of earlier misjudged tokens. When the latent space is unstable that small perturbation in pixels can change the latent distribution significantly, the iterative decoding mechanism employed by LDM and MaskGIT can alleviate the error aggregation problem by allowing for revision of earlier misjudged tokens while autoregressive models cannot. Consequently, stable latent space is required to reduce errors introduced in the generation process of autoregressive models. This principle forms the foundation of our methodology for developing metric to evaluate latent spaces with an emphasis on the stability of the latent representations. We examine two primary types of latent spaces: (1) autoencoder induced by minimizing DH(Pf (X), PX ) and (2) self-supervised learning (SSL) model induced by minimizing DG(Pf (X), PX ). By analyzing network parameters of these models in linear regime, we derive the following propositions. Proposition 2.1. The latent space spanned by linear autoencoder is congruent with that spanned by the principal component loading vectors derived in Principal Component Analysis (PCA). Furthermore, the principal component loading vectors can be elucidated from the autoencoders weights. Proposition 2.2. The discriminative self-supervised model learns to separate data distributions in the latent space as Linear Discriminant Analysis in principle. Motivated by these theoretical insights, we introduce metric to assess the stability of the latent space induced by different encoder models. To exemplify these concepts, we refer to an example consisting of two Gaussian distributions in two-dimensional space, as depicted in Figure 6(a). The results attained from applying the PCA and LDA algorithms are visually depicted in Figure 6(b) and (c) respectively. The distribution embedded by the LDA model exhibits greater separability than that by the PCA model. To quantitatively evaluate stability, we add Gaussian noise of different variances to the original 2D data and subsequently train linear classifier on the latent space. As Figure 6(d) illustrates, the accuracy of the LDA model consistently surpasses that of PCA. 2DG and DH can achieve zero simultaneously. For example, in the well-known posterior collapse phenomenon in the VAE literature, the latent space (X) is tractable Gaussian distribution and DG(Pf (X), PX ) can be zero by simply setting the generative models as Gaussian sampler. If the decoder is strong enough and directly generates samples without conditioning on the encoder output, DH(Pf (X), PX ) can be zero as well. 5 Table 1: The stability of latent spaces induced from VQ Token and Discriminative Token (introduced in Section 3), assessed across different Signal-to-Noise Ratio (SNR) levels to evaluate performance under varying signal and noise conditions. SNR 30 25 20 15 5 1 VQ Token change Disc Token change VQ Token cos-sim Disc Token cos-sim 0.187 0.114 0.972 0.975 0.317 0.178 0.949 0.960 0.487 0.260 0.910 0. 0.663 0.355 0.853 0.916 0.805 0.457 0.777 0.888 0.901 0.570 0.682 0.855 0.948 0.687 0.594 0.816 0.01 0.956 0.721 0.571 0. Figure 2: The architecture of DiGIT. To evaluate the stability of latent space induced from autoencoders and SSL models, we add Gaussian noise to image pixels and then feed the noisy images to VQGAN encoder and an SSL encoder DINOv2 [28]. This experiment aims to examine the resilience of the latent spaces induced by these encoders to such disturbances. We measure the rate of change in discrete tokens, specifically VQ tokens for the latent space induced from the VQGAN encoder and discriminative tokens for the latent space induced from the SSL model, and the cosine similarity in conjunction with the strength of the noise introduced. The experimental results in Table 1 demonstrate that the latent space induced from the SSL model exhibits heightened stability compared to that derived from the VQGAN autoencoder. Therefore, we propose to replace the unstable latent space induced by the reconstructive model with stable latent space induced by the discriminative self-supervised model for autoregressive models."
        },
        {
            "title": "3 Stabilize the Latent Space with Self-supervised Learning Model",
            "content": "In this section, we present simple but effective image tokenizer that discretizes the feature representations of discriminative SSL models to form discrete tokens for autoregressive models. The architecture of our model is illustrated in Figure 2. Discrete Image Discriminative Tokenizer Drawing inspiration from the VQGAN tokenizer [15], which employs an implicit K-Means clustering algorithm within the latent space to generate discrete tokens for autoregressive modeling, we propose straightforward approach to perform K-Means clustering on the feature space of discriminative SSL models to obtain discrete tokens. To process given dataset, our initial step involves gathering the features of image patches, akin to the hidden states produced by SSL models. Then we employ clustering algorithm to group these patches, resulting in collection of clustering centers. These centers constitute the codebook for the discrete tokenizer. To determine the discrete tokens for an image patch at inference, we identify its nearest neighbor in the codebook, which is then designated as the discrete token for the respective patch. Image Autoregressive Modeling After converting images into discrete tokens with the discriminative tokenizer, we treat each image as sequence by flattening the discrete tokens from images into 1D sequence in raster order. We train causal Transformer [38] model with the next token prediction objective, which is the same as the standard approach for language models. 6 Table 2: Linear-probe accuracy of image autoregressive generative models on ImageNet [11]. Methods # Tokens Features # Params Top-1 Acc. iGPT-L [8] iGPT-XL [8] VIM+VQGAN [41] VIM+dVAE [41] GIVT [36] VIM+ViT-VQGAN [41] VIM+ViT-VQGAN [41] AIM [14] DiGIT (Ours) DiGIT (Ours) 32 32 64 64 32 32 32 32 16 16 32 32 32 32 16 16 16 16 16 1536 3072 1024 1024 1024 1024 2048 1536 1024 1536 1362M 6801M 650M 650M 304M 650M 1697M 0.6B 219M 732M 60.3 68.7 61.8 63.8 65.1 65.1 73.2 70.5 71.7 80."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details We take the discriminative SSL model DINOv2 [28] as the encoder for all experiments. The K-Means model is trained on the randomly selected 10% subset of the ImageNet [11] training set. We use the autoregressive model with the same architecture as GPT-2 [32]. We train the DiGIT models with the base and large sizes. The vocabulary size of the tokenizer for the base is 8192 and 16000 for the large size. More implementation details and hyper-parameters are provided in Appendix A.3. 4.2 Image Understanding The GPT model is famous for learning semantic features by generative training objective of next token prediction. We compare the image understanding ability of different image autoregressive models with linear-probe as described in iGPT [8]. We train linear classifier on top of the frozen features average from each layer on the ImageNet training set. We report the Top-1 accuracy compared with other image autoregressive models in Table 2. Remarkably, with only 219M parameters, DiGIT achieves Top-1 accuracy of 71.7%, surpassing both iGPT and VIM-Base, which have greater number of parameters and operate at more visual tokens. Despite representing images with smaller token grid size (16 16 as opposed to 32 32), DiGIT still delivers superior top-1 accuracy, demonstrating the effectiveness of our tokenizer. Moreover, when we scale DiGITs parameters from 219M to 732M, the Top-1 accuracy shows an additional increase of 8.6% and reaches 80% for the first time. The improvement indicates that DiGIT with the proposed discriminative tokenizer has the potential for the development of large vision models. 4.3 Image Generation Since the SSL models do not have paired decoder to recover pixels from latent space, the generative models trained with our discriminative tokenizer require an auxiliary image decoder to render pixels. The discriminative tokenizer can be seamlessly integrated with any existing image generative models trained with tokenizer induced from reconstructive autoencoder. In our experiment, we train an autoregressive model VQGAN, and an MIM model MaskGIT as the pixel decoder respectively. The results are presented in Table 3 and Table 4. The autoregressive model equipped with our discriminative tokenizer achieves the SOTA performance with FID reaching 3 for the first time. Furthermore, the performance significantly improves as the model size increases, demonstrating the potential of large vision model with next token prediction. Interestingly, when utilizing the DiGIT as the conditioning factor, the performance of both the autoregressive and MaskGIT decoders becomes close (4.62 and 4.79). This observation suggests that stabilizing the latent space produces effects analogous to the iterative stabilization decoding mechanism. 4.4 Ablation Study We conduct the ablation study to present comprehensive analysis of the proposed discriminative tokenizer in image generation and understanding. The results are illustrated in Table 3(a) and Table 3: Class-unconditional image generation on ImageNet with resolution 256 256. DiGIT + VQ indicates that we utilize golden discriminative tokens alongside VQ generated by autoregressive models. Type GAN Diff. Diff. MIM MIM MIM MIM Methods BigGAN [4] LDM [34] ADM [12] MAGE [26] MAGE [26] MaskGIT [7] DiGIT (+MaskGIT) AR AR AR AR AR validation data DiGIT + VQ validation data VQ only VQGAN [15] GIVT [36] GIVT [36] DiGIT (+VQGAN) DiGIT (+VQGAN) #Param #Epoch FID IS 70M 395M 554M 200M 463M 227M 219M 214M 304M 1.67B 219M 732M - - - - - 1600 1600 300 200 200 500 500 400 200 - - 38.6 39.1 26.2 11.1 9.10 20.7 9.04 24.38 17.70 11.02 9.13 4.59 1.92 1.67 24.70 22.83 39.70 81.17 105.1 42.08 75.04 30.93 - - 73.85 141.29 184.40 175. Table 4: Class-conditional image generation on ImageNet with resolution 256 256. denotes the model is trained with classifier-free guidance while all the other models are not. Methods #Param #Epoch FID IS Type GAN Diff. Diff. Diff. Diff. MIM MIM+AR MIM+AR MIM+AR MIM AR AR AR AR AR AR AR MIM MIM BigGAN [4] ADM [12] LDM-4 [34] DiT-XL/2 [30] L-DiT-7B [30] Contextual RQ-Trans [25] VAR [35] VAR [35] VAR [35] MAGVIT-v2 [42] VQVAE-2 [33] RQ-Trans [24] RQ-Trans [24] ViTVQGAN [41] ViTVQGAN [41] GIVT [36] GIVT [36] MaskGIT [7] DiGIT (+MaskGIT) AR AR AR validation data DiGIT + VQ validation data VQ only VQGAN [15] DiGIT (+VQGAN) DiGIT (+VQGAN) 160M 554M 400M 675M 7B 371M 310M 310M 600M 307M 13.5B 480M 3.8B 650M 1.7B 304M 1.67B 227M 219M 227M 219M 732M - - - - - - - 300 200 200 250 1080 - - - 360 360 500 500 300 200 300 400 200 - - 6.95 10.94 10.56 9.62 6.09 5.45 4.64 3.60 2.95 3.65 31.11 15.72 7.55 11.20 5.3 5.67 3.46 6.18 4.62 18.65 4.79 3.39 1.92 1.67 198.2 101.0 103.5 121.50 153.32 172.6 - 257.5 306.1 200.5 45 86.8 134.0 97.2 149.9 - - 182.1 146.19 80.4 142.87 205.96 184.40 175. Figure 3(b). For image generation tasks, we take the autoregressive model trained with the VQGAN tokenizer as the baseline. Introducing discriminative tokens leads to significant improvement, reducing FID to 9.66 and increasing IS to 69.15, underscoring the effectiveness of stabilizing latent space for autoregressive models. Further extending the training duration to 400 epochs yielded additional improvements of 0.53. substantial advancement is observed when scaling up the model size to 732M, resulting in FID dropping dramatically to 4.59 and IS more than doubling to 141.29. This indicates that increasing the models capacity significantly enhances its ability to model complex relationships within the data, which is similar phenomenon in GPT models. Overall, the study highlights the latent space stabilization and the potential of large-scale training of autoregressive modeling in images with our discriminative tokenizer. 8 VQ Token + Discriminative Token + Longer Training (400 epoch) + Scale up (732M) FID 24.38 9.66 9.13 4.59 IS 30.93 69.15 73.85 141.29 (a) (b) Figure 3: Ablation study of DiGIT. (a) The comparison of tokenizer, training steps, and model size in the image generation task. (b) Linear-probe accuracy from different layers in the pre-trained DiGIT-base with different number of K-Means clusters. For the image understanding task, we investigate the effect of K-Means clusters and features learned in the different layers of DiGIT. We can see that increasing the cluster number can further improve the accuracy of the linear probe, which means the image autoregressive model can benefit from larger vocabulary. The linear probe accuracy increases quickly from the first transformer block, reaches its peak at the middle layers, and finally decreases little bit for the last few blocks. This observation connects the image autoregressive model to the text language model where the semantic information is learned in the middle layers of the transformer. 4.5 Comparison of Discrete Tokenizers We conduct an experiment to investigate the effect of different SSL models on latent space. We generally categorize the SSL models into two types according to their pre-training objectives: (1) Global level (MoCo) and patch level (MAE,iBOT), (2) reconstructive (MAE) and discriminative (MoCo, iBOT). At the global level, the loss function is computed using an aggregate output such as [CLS] token or mean pooling. In contrast, patch-level models involve patches directly in loss computation. Reconstructive models, such as MAE, aim to recover image pixels in manner akin to autoencoders, while discriminative models are optimized to learn the distinguishable features. As demonstrated in Table 4(a), the discriminative objective plays pivotal role in image generation in that it can stabilize the latent space. Furthermore, because generative models need to predict patches, the inclusion of patch-level loss function can enhance performance. To assess the stability of latent space induced by our discriminative tokenizer and reconstructive tokenizer. We pre-train two auto-regressive generative models on the ImageNet dataset [11], employing the proposed discriminative tokenizer and VQGAN tokenizer respectively. We provide each model with the upper half of the target image as conditional prompt for generation, challenging them to complete the lower half of the image. stable latent space should be able to help the autoregressive model generate the lower half more robustly, maintaining thematic and aesthetic coherence. As shown in Figure 4(b), the FID decreases for both models when given longer prefix context. However, when the prefix length is reduced from 75% to only 12.5% of the image, the model trained with the VQGAN tokenizer encounters difficulties in producing images that adhere to the specified prompt. In contrast, the model utilizing the discriminative tokenizer effectively continues to produce congruent visual tokens, maintaining low FID scores even with significantly truncated prefix."
        },
        {
            "title": "5 Related Work",
            "content": "Image Tokenizer The image tokenizer [37, 33, 15] is essential in converting pixels into discrete tokens for autoregressive generative modeling. VQVAE [37] first proposes to assign latent features learned by an encoder to the nearest entry in the learnable codebook embeddings, followed by decoder to reconstruct the original image pixels. VQGAN [15] further incorporates adversarial loss and perceptual loss to improve the image synthesis quality. RQ-Transformer [24] extends the single-layer quantizer to the multi-layer residual quantizer to augment the visual tokenizers ability to capture fine-grained details. ViT-VQGAN [41] incorporates the modern Vision-Transformer [13] into VQGAN to enhance the reconstruction quality. MAGVIT-v2 [42] substitute the online update codebook in VQGAN with lookup-free quantizer to enable larger vocabulary for generative language models. 9 SSL Type FID IS Acc@LP Acc@OL MAE MoCo iBOT VQGAN P+R 45.51 G+D 20.38 P+D 16.81 24.38 - 31.40 59.22 61.10 - 75.8 76.7 76.0 - 18.39 45.02 57.88 30.93 (a) (b) Figure 4: (a): The comparison of tokenizers induced from different SSL models. Acc@LP is obtained by linear probing on the autoregressive model (model size of 39M for 100 epochs) trained with tokenizers. Acc@OL is the linear probe score of the SSL model. P: patch, D: discriminative, R: reconstructive. (b): Generation quality curves in FID on ImageNet 256 256 valid set when scaling the prefix length with discriminative tokenizer and reconstructive VQGAN tokenizer. Both are autoregressive models with 219M parameters. Image Autoregressive Modeling Inspired by the success of autoregressive Transformer [38] in text generation tasks, there have been several efforts to replicate it in image generation tasks. One of the pioneering works is iGPT [8], which pre-trains an autoregressive model on pixels with the same architecture as GPT2 [32], achieving promising results in unsupervised visual representation learning. LVM [1] proposes large-scale vision dataset composed of images and videos, based on which large vision model is trained. Empirical observations indicate that the model scales effectively across various tasks with in-context learning [40]. Similarly, AIM [14] follows ViT [13] and represents images in the patch. It is observed that the performance of image recognition continues to increase as the model size scales up. VAR [35] proposes next-scale prediction to generate images from coarse to fine in hybrid of autoregressive and nonautoregressive manner. Self-supervised Learning Models Self-supervised learning (SSL) [9, 17] plays an important role in learning fundamental visual representations for downstream tasks. Among them, SimCLR [9], MoCo [9, 18, 10] compute losses at the image level through [CLS] token aggregation or pooling operations with contrastive learning. iBOT-style models [43, 6, 28] extend the loss to the patch level, achieving improved performance in dense prediction tasks. BEiT [3] uses VQGAN tokenized sequences as the training target. MAE [19] randomly masks some patches of images and reconstructs the pixels with unmasked patches as the condition."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we make an exploration in the latent space for generative modeling. We introduce unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Subsequently, we propose simple but effective discriminative image tokenizer, followed by an image autoregressive generative model DiGIT. Empirical results indicate that our tokenizer achieves superior performance across both image understanding and image generation tasks. Notably, when DiGIT is scaled up in model size, it exhibits even greater enhancements, indicating the potential for the development of large vision models. Our findings challenge the conventional wisdom that proficiency in reconstruction equates to an effective latent space for auto-regressive generation. Through this work, we aim to rekindle interest in the generative pre-training of image auto-regressive models and encourage reevaluation of the fundamental components that define latent space for generative models."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This research was supported by the National Key Research and Development Program of China (Grant No. 2022YFB3103100), the National Natural Science Foundation of China (Grant No. 62276245)."
        },
        {
            "title": "References",
            "content": "[1] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan L. Yuille, Trevor Darrell, Jitendra Malik, and Alexei A. Efros. Sequential modeling enables scalable learning for large 10 vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2286122872, June 2024. [2] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2(1):5358, 1989. ISSN 0893-6080. doi: https://doi.org/10.1016/0893-6080(89)90014-2. URL https://www.sciencedirect. com/science/article/pii/0893608089900142. [3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=p-BhZSz59o4. [4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm. [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 96509660, October 2021. [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1131511325, June 2022. [8] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 16911703. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/chen20s.html. [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 15971607. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/chen20j.html. [10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. doi: 10.1109/CVPR.2009.5206848. [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 87808794. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/ 2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy. 11 [14] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Ángel Bautista, Vaishaal Shankar, Alexander Toshev, Joshua M. Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=c92KDfEZTg. [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, June 2021. [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherIn jil Ozair, Aaron Courville, and Yoshua Bengio. Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/ 5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf. Generative adversarial nets. [17] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 2127121284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf. [18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1600016009, June 2022. [20] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006. doi: 10.1126/science.1127647. URL https: //www.science.org/doi/abs/10.1126/science.1127647. [21] Tianyang Hu, Fei Chen, Haonan Wang, Jiawei Li, Wenjia Wang, Jiacheng Sun, and Zhenguo Li. Complexity matters: Rethinking the latent space for generative modeling. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 2955829579. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 5e8023f07625374c6fdf3aa08bb38e0e-Paper-Conference.pdf. [22] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [23] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [24] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1152311532, June 2022. [25] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draftand-revise: Effective image generation with contextual rq-transformer. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3012730138. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ c276c3303c0723c83a43b95a44a1fcbf-Paper-Conference.pdf. [26] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 21422152, June 2023. 12 [27] OpenAI. Gpt-4 technical report, 2023. [28] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. [29] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019. [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205, October 2023. [31] Alec Radford. Improving language understanding by generative pre-training. 2018. [32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [33] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/ 2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf. [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. [35] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. [36] Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision ECCV 2024, pages 292309, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72998-0. [37] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/ 2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf. [38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [39] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 99299939. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/wang20k.html. 13 [40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. [41] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=pfNyExj7z2. [42] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=gzqrANCF4g. [43] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image BERT pre-training with online tokenizer. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=ydopy-e6Dg."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Limitation and Broader Impact Our proposed discriminative tokenizer exhibits remarkable capabilities in both the image understanding and the image generation tasks, which is challenging for single model. However, the discriminative tokenizer induced by the SSL model cannot directly render pixels. Consequently, we need to train another decoder model to convert tokens from discriminative tokenizers to VQGAN tokens. The potential for direct generation of RGB pixels from the tokens produced by the discriminative tokenizer remains an uncharted avenue, which we leave for future work. Image generative models possess dichotomous nature, particularly within the realm of visual media. On the positive side, they foster myriad of creative endeavors, and methodologies aim to minimize the costs of training and inference, holding the potential to broaden access and democratize the use of this technology. On the negative side, the simplicity with which manipulated content can be crafted and propagated raises serious concerns. This includes the proliferation of misinformation and spam. Furthermore, generative models may inadvertently unveil aspects of their training data, particularly troubling issue when that data includes sensitive or personal information collected without explicit consent. A.2 Proof of claims in Section 2 Proposition A.1. Consider the optimization problem for Rn: W2W1X2 , min W1Rmn,W2Rnm (9) which is linear autoencoder. W2 is minimizer of the problem if and only if its column space is spanned by the first loading vectors of X. Proof. First, we derive the condition for the optimal W1 in the context of this optimization problem. Setting the gradient of the objective function with respect to W1 to zero leads to W1 being the left Moore-Penrose pseudoinverse of W2 [2]. Similarly, setting the gradient with respect to W2 to zero would identify W2 as the right pseudoinverse of W1: W1 = 2 = (W This finding indicates that the optimization can be simplified to focus on single matrix (either W1 or W2), hence removing the redundancy in the parameters: W2W 2 W2)1W 2 . (11) (10) 2 X2 . min W2Rnm 2 W2)1W 2 = W2(W The term W2W 2 is recognized as the matrix form of the orthogonal projection operator onto the column space of W2. This property holds true even when the column vectors of W2 are not orthonormal. By performing QR decomposition on W2, W2 = QR where is an orthogonal matrix (QT = I) and is an upper triangular matrix, we effectively transform the problem into optimizing over orthogonal matrices. The objective can thus be restated as: min Rmn T X2 , subject to = Inn. (12) This revelation explicitly demonstrates that minimizing the reconstruction error in the space Rmn demands that (equivalent to W2 in our context) projects onto space spanned by its most significant structural components (in terms of variance), which are precisely the first loading vectors, or principal components, of X. Proposition A.2. The discriminative self-supervised model learns to separate data distributions in latent space as LDA in principle. Proof. We first consider the objective of Fisher LDA for two-class classification. LDA seeks to find linear projection that maximizes the separation between the two classes: 15 J(w) = wT SBw wT SW , (13) where SB is the between-class scatter matrix, SW is the within-class scatter matrix, and is the projection vector. We take contrastive learning with InfoNCE for analysis because it is the most popular learning objective in discriminative SSL models. For contrastive learning using the asymptotic form of the InfoNCE objective [39], we have two components: = 1 τ E(x,x+)ppos (cid:2)f (x)f (x+)(cid:3) + Expdata (cid:104) log Expdata (cid:104) ef (x)f (x)/τ (cid:105)(cid:105) , (14) where the first term encourages similarity between positive pairs. In LDA, this is analogous to minimizing SW because we want points from the same class to be close to each other when projected onto w. The second term, when expanded using Jensens inequality, represents an upper bound on the regularized sum of all pairwise inner products between different embeddings, effectively encouraging dissimilarity between all samples: Expdata (cid:104) log Expdata (cid:104) ef (x)f (x)/τ (cid:105)(cid:105) = 1 (cid:88) i=1 log 1 (cid:88) j=1 eh hj /τ 1 τ m2 (cid:88) (cid:88) i=1 j= hj. (15) When hi = (xi) are normalized, optimizing this term aims to decrease Sum(WW) = (cid:80)m hj. This term, being an upper bound for the largest eigenvalue of WW, when minimized, encourages \"flatter\" singular value distribution of the embedding space. This flattening makes the embedding space more isotropic, which in turn increases the between-class scatter SB. j=1 (cid:80)m i=1 To make the connection explicit, consider that in LDA, we want to maximize SB, the between-class scatter, which is typically represented as: SB = (µ1 µ2)(µ1 µ2)T , (16) where µ1 and µ2 are the means of the two classes. Analogously, in the contrastive learning framework, minimizing the second term scatters the embeddings more uniformly in the high-dimensional space, akin to maximizing between-class scatter. The uniform distribution of the embeddings across the space increases the separation between classes or clusters of points, akin to the effect of maximizing SB. A.3 Implementation Details We extract the hidden states from the third-to-last layer of DINOv2 for discriminative tokenizer training, as it suggests more generalized representations in the intermediate layers compared to the last layer. All DiGIT models are trained from scratch with batch size of 2048 for the base model and 1024 for the large model, over duration of 200 epochs. For the image generation task, the decoder model for pixel rendering is trained with batch size of 2048 for 400 epochs, while the large model uses batch size of 1024 for 200 epochs. The base model consists of 16 layers with dimension of 1024 and hidden dimension of 4096. The large model consists of 24 layers with dimension of 1536 and hidden dimension of 6144. Both configurations feature 16 attention heads. For all models, we utilize the Adam [22] optimizer with β = (0.9, 0.98). We employ an inverse square root decay schedule for the learning rate with peak value of 3e 4 for 10 epochs warm-up. We use the same VQGAN model as MAGE Li et al. [26]. During training, we apply data augmentations including random crops resized to the short edge and horizontal flips. For the decoding strategy, we apply top-k sampling with = 400 for the large model and = 200 for the base model. In the autoregressive decoder for pixel rendering, we use probability of 0.8 with temperature of 1.0 for top-p sampling. All experiments are conducted using the FAIRSEQ [29] library. 16 (a) (b) Figure 5: FID and Inception Score as function of top-k, top-p sampling on the image generation task with DiGIT-base. The decoding temperature is fixed to 1.0. The stage2 denotes the autoregressive model for pixel rendering. Figure 6: Toy example of PCA and LDA. 17 A.4 Qualitative Cases Figure 7: Class-unconditional image generation results on ImageNet 256256 by DiGIT. 18 Figure 8: Class-conditional image generation results on ImageNet 256256 by DiGIT, where the images in the same row share the same class label."
        }
    ],
    "affiliations": [
        "School of Computer Science and Technology, University of Science and Technology of China",
        "School of Data Science, University of Science and Technology of China",
        "State Key Laboratory of Cognitive Intelligence",
        "Zhejiang University"
    ]
}