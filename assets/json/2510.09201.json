{
    "paper_title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
    "authors": [
        "Yumin Choi",
        "Dongki Kim",
        "Jinheon Baek",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs."
        },
        {
            "title": "Start",
            "content": "MULTIMODAL PROMPT OPTIMIZATION: WHY NOT LEVERAGE MULTIPLE MODALITIES FOR MLLMS Yumin Choi1 Dongki Kim1 Jinheon Baek1 1KAIST {yuminchoi, cleverki, jinheon.baek, sungju.hwang}@kaist.ac.kr Sung Ju Hwang1,2 2DeepAuto.ai 5 2 0 2 0 ] . [ 1 1 0 2 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in Bayesianbased selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as crucial step to realizing the potential of MLLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated outstanding capabilities across diverse range of tasks and domains (OpenAI, 2024; Grattafiori et al., 2024; Yang et al., 2025). We note that central factor in unlocking their full potential lies in the design of prompts, which directly influence model performance. However, crafting high-quality prompts is often labor-intensive and iterative process that requires substantial human intervention. To address this limitation, the field of Automatic Prompt Optimization (APO) has emerged, whose goal is to automate the discovery of effective prompts (Zhou et al., 2023; Pryzant et al., 2023; Yang et al., 2024; Fernando et al., 2024). For example, one representative approach (APE) frames this challenge as an iterative search problem, where at each step, set of new candidate prompts is generated or updated, evaluated on target task, and the best-performing prompts are selected to guide the next round of generation. Recently, on top of the LLMs, Multimodal Large Language Models (MLLMs) have been proposed, which process not only text but also images, videos, and other modalities (such as molecules) (OpenAI, 2023; Zhu et al., 2025; Bai et al., 2025; Gemini, 2025). Yet, despite these advances and their wide-ranging applications, existing prompt optimization methods remain restricted to the textual modality (Pryzant et al., 2023; Guo et al., 2024; Cui et al., 2025), and overlook the richer expressive capacity afforded by multimodal inputs (that text alone cannot capture). For instance, as illustrated in Figure 1, describing the distinct characteristics of specific bird may require long and potentially ambiguous text, while single image can convey the same information far more directly. By limiting optimization to text, existing methods are prone to generating less effective, suboptimal prompts that fail to fully exploit the multimodal space that MLLMs are inherently capable of leveraging. Motivated by this limitation, we first define the novel problem of multimodal prompt optimization, which expands the prompt optimization space beyond text to incorporate multiple modalities. However, while this expanded space opens new opportunities, it also introduces couple of challenges Equal contribution; Code is available at https://github.com/Dozi01/MPO. 1 Figure 1: Concept Figure. (A) Existing prompt optimization approaches restrict the optimization to the textual space, leaving MLLMs underutilized by failing to provide rich contextual signals. (B) Our multimodal prompt optimization expands the optimization space into multimodality, allowing the discovery of salient multimodal context and fully leveraging the expressive capacity of MLLMs. for automatic optimization. First, exploring the larger, combinatorial space of multimodal prompts requires prompt update strategy that can efficiently navigate candidate prompts while maintaining cross-modal consistency. Furthermore, selecting promising candidates becomes substantially more difficult, as the enlarged search space makes optimal prompts increasingly sparse, given the need to account for both the effectiveness within each modality and the alignment across modalities, which, in turn, calls for evaluation strategies that are both efficient and accurate. To address these challenges, we propose Multimodal Prompt Optimizer (MPO), unified framework for optimizing prompts across both the textual and non-textual modalities, which consists of the two key components: (i) alignment-preserving exploration and (ii) prior-inheritance-based selection. Specifically, for exploration, the proposed MPO jointly updates the textual prompt, as well as its associated non-textual counterparts by generating instructions to create (or revise) the non-textual components of the multimodal prompt (unlike prior approaches that refine only text), and notably, their updates are guided by the single semantic gradient (i.e., feedback) to ensure their alignment derived from the failure analysis of the current prompt. Moreover, these updates are further diversified through complementary operations, namely generation, editing, and mixing, to ensure the broad and expressive exploration of the multimodal prompt space. Then, building on this exploration with multiple candidate prompts updated, MPO leverages the prior-inherited Bayesian-UCB as prompt selection strategy, which utilizes the performance score of parent prompts as prior (unlike conventional approaches that treat each candidate independently), to reliably identify the high-performing prompts by biasing the selection process toward more promising regions of the multimodal space. To validate MPO, we conduct extensive experiments benchmarking it against leading text-only optimization methods across 10 datasets, and our evaluation suite spans not only images and videos but also molecular structures, ensuring broad coverage of diverse modalities. Across all domains, MPO demonstrate consistent and significant performance gains, empirically confirming our core hypothesis: expanding the prompt search space into the multimodal domain is crucial to exploit the expanded capacity of MLLMs. Further analyses show the efficacy of MPO components: alignment-preserving exploration with complementary operators facilitates the discovery of optimal multimodal prompts by not only ensuring cross-modal consistency but also thoroughly probing the search space; and the prior-inherited Bayesian-UCB accurately and efficiently selects high-performing prompts, reducing evaluation budget by 42% compared with prior-free baseline. These results highlight MPO as an effective framework for optimizing multimodal prompts, unlocking the full capabilities of MLLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal Large Language Models The development of MLLMs has significantly extended the capabilities of traditional LLMs by enabling them to process and reason over diverse non-textual modalities, including images, videos, audio, and more (Liu et al., 2023; OpenAI, 2023; Chu et al., 2023). In particular, these models are typically trained through large-scale multimodal pre-training, which aligns modality-specific encoders (e.g., vision or audio) with LLM backbones, followed by post-training stages such as supervised fine-tuning and preference optimization to endow them with multimodal instruction-following abilities (Gemini, 2025; Bai et al., 2025; Zhu et al., 2025). Moreover, leveraging these capabilities, MLLMs have achieved strong performance on broad range of tasks, from foundational ones such as classification and captioning, to domain-specific, highstakes applications such as medical image question answering and pharmacological property prediction (Martin et al., 2019; Liu et al., 2021; Corbi`ere et al., 2025; Huang et al., 2021). 2 Figure 2: Overview of MPO, consisting of two components. (A) Alignment-preserving exploration analyzes failure set to generate feedback, which is then used both to refine the textual prompt and to guide modality-specific generator to create new non-textual prompt with one of three operators. (B) Prior-Inherited Bayesian UCB Selection leverages the parents performance as an informative prior, warm-starting the search to effectively identify high-performing prompts among candidates. Automatic Prompt Optimization To reduce the burden of manual prompt engineering and systematically uncover the effective prompts, the field of Automatic Prompt Optimization (APO) has emerged. Existing works can be broadly categorized into two paradigms. The first is gradient-based optimization, which learns continuous embedding vectors (i.e., soft prompts) that are prepended to model inputs to steer behavior (Khattak et al., 2023; Zeng et al., 2024; Wang et al., 2024). Yet, while effective, they are computationally costly, yield uninterpretable numerical vectors, and are restricted to open-source models with accessible parameters. To overcome these drawbacks, gradient-free approaches have been proposed, which iteratively generate, evaluate, and refine candidate prompts using LLMs themselves (Zhou et al., 2023; Yang et al., 2024). Also, some recent works enhance this process by analyzing prompt failures to guide improvements (Pryzant et al., 2023; Ye et al., 2024; Cui et al., 2025; Yuksekgonul et al., 2025), while others borrow ideas from evolutionary algorithms (e.g., mutation and crossover) to explore the prompt space (Guo et al., 2024; Fernando et al., 2024). Despite this progress, current APO techniques are limited to text-only settings, restricting optimization to purely linguistic information. In contrast, our work expands prompt optimization into the multimodal domain, enabling the prompt discovery that fully exploits the capabilities of MLLMs. Instance-Specific Prompting and Optimization Distinct from task-level prompt optimization, another line of research focuses on instance-specific prompting strategies that operate at inference time to enhance reasoning on per-query basis. For example, MM-CoT (Zhang et al., 2024b) guides the model to generate an intermediate textual rationale before producing the final answer. Also, other methods augment visual inputs with query-dependent signals, such as bounding boxes or points, to guide attention toward relevant regions of an image (Zhou et al., 2024; Jiang et al., 2024; Lin et al., 2024). Similar ideas have been explored in text-to-image and text-to-video generation, where prompts are crafted and refined to produce outputs more faithfully aligned with user intent (Manas et al., 2024; Mo et al., 2024; Gao et al., 2025). However, these techniques are queryspecific, designed to improve model performance for single instance at time. By contrast, APO pursues different objective: discovering single, reusable prompt that boosts performance across an entire task, and our work advances this paradigm by extending it into the multimodal domain."
        },
        {
            "title": "3 METHODOLOGY: MULTIMODAL PROMPT OPTIMIZER",
            "content": "We present Multimodal Prompt Optimizer (MPO), composed of two modules: alignment-preserving exploration of multimodal prompt space and prompt selection with prior-inherited Bayesian UCB. 3.1 PROBLEM DEFINITION We begin by formally describing MLLMs and then proposing novel problem of multimodal prompt optimization, which redefines and expands the notion of existing prompt optimization beyond text. Multimodal Large Language Models Multimodal Large Language Models (MLLMs) extend the capabilities of LLMs by processing inputs that combine text with non-textual modalities. Formally, an MLLM can be represented as parametric function MLLM : (T M) , where denotes the textual input space, denotes the non-textual input space, and denotes the Kleene Star (representing finite sequence over the combined spaces). In other words, given multimodal query 3 and prompt (each potentially containing both textual and non-textual components), the model generates textual output = MLLM(p, q). It is worth noting that prior work on prompt optimization has generally restricted to purely textual form (p = ), leaving the non-textual dimensions of unused. This restriction underutilizes the expressive capacity of MLLMs and fails to provide richer contextual signals that are often crucial for real-world multimodal tasks (See Figure 1). Multimodal Prompt Optimization Building on the expanded space of MLLMs, we extend the notion of prompt for optimization from text-only to multimodal. Specifically, we define multimodal prompt as pair = (t, m) M, where is the textual prompt and is the non-textual prompt. Then, given task dataset consisting of queryanswer pairs (q, a), the objective of multimodal prompt optimization is to discover the optimal prompt (t, m) that maximizes performance: (t, m) = argmax (t,m)T E(q,a)D (cid:104) (cid:0)MLLM(t, m, q), a(cid:1)(cid:105) , where is function for task-specific evaluation metric, such as accuracy or F1 scores. Notably, compared to optimizing only textual prompts, the joint search space introduces an entirely new axis of non-textual information, which in turn raises two fundamental challenges. First, multimodal prompts must maintain cross-modal consistency: textual and non-textual components should provide complementary, not conflicting signals; however, expanding to the combinatorial space greatly increases the risk of semantic misalignment. Second, the enlarged space amplifies the difficulty of candidate selection: high-quality prompts become sparse, and low-quality prompts dominate, making it harder to efficiently identify promising candidates. To overcome these, we now explain the proposed multimodal prompt optimizer, designed to navigate this enlarged space below. 3.2 ALIGNMENT-PRESERVING EXPLORATION OF MULTIMODAL PROMPT SPACE The first challenge in multimodal prompt optimization lies in exploring the enlarged search space while preserving semantic consistency across modalities; thus, naive approach that independently updates textual and non-textual components risks producing misaligned prompts, where one modality contradicts the other. To tackle this, we introduce an exploration framework that couples the update of textual and non-textual prompts while supporting diverse operations (Figure 2). Joint Optimization of Multimodal Prompt Our MPO jointly updates the textual and non-textual prompts to ensure that both evolve coherently, achieved through the following two mechanisms: Cohesive Backpropagation. We begin by identifying failure set = {(q, a, y) = a} for multimodal prompt = (t, m). Instead of treating errors separately for text and non-textual inputs, we then generate unified feedback = (t, m) = MLLM(t, m; ), which encodes cross-modal weaknesses in textual form. By doing so, we obtain the single supervisory signal that guides both modalities simultaneously, mitigating the risk of overfitting updates to one modality. Joint Multimodal Update. Using the feedback, MPO jointly refines the textual prompt while deriving modality-specific conditions (in the textual form) that direct non-textual revisions. Specifically, the MLLM produces an updated textual prompt and further modality-specific condition describing how the non-textual prompt should adapt: (t, c) = MLLM(t, m; F, p). The condition is then passed to modality-specific generators (such as text-to-image or text-to-molecule modules), which yield updated non-textual prompts = g(c). This guarantees that updates to remain consistent with the revised textual prompt t, rather than being optimized in isolation. Exploration Operators Ensuring that generated outputs remain consistent with the guiding textual conditions is necessary baseline, and effective optimization further requires that actively explores diverse regions of the multimodal space. To achieve this, we design three operators (namely, generation, edit, and mix), which systematically expand, refine, and recombine non-textual prompts. Generation operator. This operator explores entirely new non-textual prompts, e.g., novel spatial arrangements in visual inputs or unique substructures in molecules. Specifically, conditioned only on the generation signal cgen, it creates prompt from scratch without referencing prior candidates: = g(cgen, ), where (cgen, t) = MLLM(t, m; p, F). By decoupling from past candidates, it explores unexplored regions and avoids local optima, especially in early stages (where initial prompts are unavailable) or when the candidate pool is biased. 4 Edit operator. This operator performs fine-grained refinements of non-textual prompts (e.g., textures) while retaining useful structures from the prior prompt. Specifically, given the edit condition cedit, the update is performed by conditioning on the prior non-textual prompt: = g(cedit, {m}), where (cedit, t) = MLLM(t, m; p, F). This enables targeted, incremental refinements, making it particularly effective when prompt is already strong but requires adjustment on specific attributes rather than complete redesign. Mix operator. This operator blends the complementary strengths of multiple multimodal prompts. Specifically, it first leverages feedback from multiple prompts to generate mixing condition cmix, which is then used by the generator to combine non-textual prompts as follows: = g(cmix, {mi}K i=1), where (cmix, t) = MLLM({ti, mi; pi, Fi}K i=1). By synthesizing multiple candidates, it yields balanced compositions, avoids over-reliance on single candidate, and enables exploration of intermediate solutions better than individual ones."
        },
        {
            "title": "3.3 EFFECTIVE PROMPT SELECTION BY PRIOR-INHERITED BAYESIAN UCB",
            "content": "Another challenge in multimodal prompt optimization is to identify which candidates should be prioritized for evaluation and carried forward. Yet, this step is non-trivial with the enlarged multimodal space, since high-quality prompts become relatively sparse, and large portion of the evaluation budget risks being wasted on low-potential candidates. Existing approaches typically adopt either (i) uniform allocation, where each candidate is evaluated equally regardless of its prior likelihood of success (Zhou et al., 2023; Cui et al., 2025), or (ii) bandit-based allocation, such as UCB (Auer, 2002; Pryzant et al., 2023), which adaptively balances exploration and exploitation. However, both paradigms suffer from an inefficient cold-start problem: newly generated prompts are treated as independent arms with no prior information, leading to unproductive evaluations in the early rounds. Parent-Child Correlation We address this cold-start inefficiency by introducing informative priors that warm-start the evaluation process. In particular, our hypothesis is that the performance of parent prompt is positively correlated with that of its children. To test this, we analyze the optimization trajectory, measuring the correlation between the performance of parent prompts and the average performance of their children. As shown in Figure 3, we observe strong positive correlation (Pearsons = 0.88), providing concrete evidence that parent scores could serve as highly informative priors for estimating child performance. Figure 3: Correlation of parent and child scores. Prior-Inherited Bayesian UCB Motivated by this finding, we propose prior-inherited Bayesian UCB, selection strategy that initializes the score distribution of new child prompt based on the posterior of its parent (rather than uniform). Specifically, we model the expected score of each multimodal prompt pi as Beta distribution, Beta(αi, βi), where αi and βi correspond to (pseudo-) counts of successful and failure outcomes, respectively. Then, for child prompt pi originated from parent prompt ppar(i), we initialize its prior proportionally to the posterior mean performance of the parent ˆµpar(i), scaled by prior strength hyperparameter > 0, formalized as follows: αi = ˆµpar(i) + 1, βi = (1 ˆµpar(i)) + 1, where ˆµpar(i) = αpar(i) αpar(i) + βpar(i) . (1) This prior-inherited mechanism provides pseudo-observations to newly generated child prompts, effectively warm-starting the evaluation process. With fixed total budget, it then proceeds iteratively: at each round, we select the prompt with the highest UCB score (an upper quantile of its Beta posterior), evaluate it on small batch of data, and update its posterior parameters αi and βi. Once the budget is exhausted, the candidate prompt with the highest expected score is selected as the new parent for the next iteration of optimization. Please refer to Algorithm 2 for the complete procedure. The following proposition guarantees that our proposed selection strategy leverages an informative parent prior (better than random chance) to accelerate the selection of the best-promising prompt. Proposition 3.1. (Fewer Pulls via Prior-Inherited Bayesian UCB) With the prior of Equation 1, 2 )(cid:3) 0), the best-arm and if the prior is more informative than uniform (Ei identification cost of Bayesian UCB is nonincreasing, where d(p, q) is the Bernoulli KL divergence. (cid:2)d(µi, ˆµpar(i)) d(µi, 1 Table 1: Main Results. Comparison of MPO with manual prompting, few-shot prompting, and text-only APO baselines on diverse benchmarks across image, video, and molecular modalities. Results are averaged over three independent runs. * denotes the average performance across multiple subtasks within the benchmark. Avg. denotes the average accuracy over all datasets except F1. PlantVillage* CUB* SLAKE* DrivingVQA RSVQA Drive&Act VANE. Absorption* BBBP CYP Inhibit.* Image Video Molecule Methods Human CoT 1-Shot 3-Shot 5-Shot APE OPRO EvoPrompt PE2 ProTeGi SEE MPO (Ours) Acc. 42.2 43.1 39.7 48.2 46.5 55.8 54.1 56.1 67.9 64.4 69.0 76.4 Acc. Acc. 47.9 49.0 54.7 58.8 58.1 67.3 59.7 59.6 71.6 70.0 71.6 78.6 35.2 30.8 31.4 30.6 28.0 34.3 33.9 34.8 35.8 35.4 35.0 38. Acc. 49.7 52.9 54.5 53.9 45.9 52.8 52.7 52.9 53.7 54.4 52.2 56.0 Acc. 51.0 49. 48.5 52.2 49.2 54.4 51.0 50.5 55.2 54.2 53.4 55.9 Acc. 47.3 37.2 50.4 54.2 54.3 50.3 46.4 46.7 50.8 53.0 51.7 58. Acc. Acc. F1 Acc. F1 Acc. 47.0 31.6 62.4 56.0 61.4 64.3 51.0 56.5 63.0 65.5 57.9 71.2 38.5 39.6 37.8 46.1 48.1 45.7 37.6 48.2 64.5 71.1 71.4 76. 36.3 36.7 35.7 44.2 45.5 40.4 35.4 46.5 56.8 58.2 60.0 64.5 39.4 33.6 36.1 42.7 49.3 36.0 39.2 38.7 61.3 72.1 67.0 75. 38.6 32.5 34.8 42.6 49.3 34.7 38.3 37.7 58.2 65.7 62.3 67.6 43.1 40.1 56.2 51.9 52.0 52.3 43.0 51.1 58.5 59.8 61.4 64. F1 37.1 32.3 48.3 47.3 47.0 50.9 37.1 49.7 55.1 57.0 56.7 60.2 Avg. 44.1 40. 47.2 49.5 49.3 51.3 46.9 49.5 58.2 60.0 59.1 65.1 The proof and detailed analysis are provided in Appendix B. Intuitively, this guarantee demonstrates that informative parent priors accelerate the discovery of high-quality prompts by reducing wasted evaluations on low-potential candidates, which is particularly beneficial for multimodal prompt optimization, where the combinatorial search space is far larger than text-only settings. In other words, by rapidly eliminating unpromising candidates and reallocating the budget toward more promising regions, our method enables efficient exploration of the vast multimodal prompt landscape."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets We conduct an extensive evaluation on MPO across diverse set of modalities, including images, videos, and molecules. For the image modality, we consider both image classification and visual question answering (VQA) tasks. Specifically, we use PlantVillage (Mohanty et al., 2016) for diseased leaf identification and CUB-200-2011 (Wah et al., 2011) for fine-grained bird classification; meanwhile, for VQA, we evaluate on SLAKE (Liu et al., 2021), RSVQA (Lobry et al., 2020), and DrivingVQA (Corbi`ere et al., 2025), which cover radiology, remote sensing, and dynamic driving scenes, respectively. For the video modality, we evaluate on Drive&Act (Martin et al., 2019) for driver action recognition and VANE-Bench (Gani et al., 2025) for abnormality detection in videobased VQA. Finally, for the molecular modality, we include three different property prediction tasks from TDC (Huang et al., 2021), namely, Absorption (Hou et al., 2007; Ma et al., 2008; Broccatelli et al., 2011; Siramshetty et al., 2021), BBBP (Martins et al., 2012), and CYP inhibition tasks (Veith et al., 2009). Detailed configurations for each dataset are provided in Appendix A.1. Baselines We benchmark MPO against both manually designed prompts and representative automatic prompt optimization methods. For manual prompting, we include Human, simple handcrafted prompt, Chain-of-Thought (CoT) (Wei et al., 2022), which uses the widely adopted phrase Lets think step by step, and Few-Shot, which supplies in-context examples drawn from the training data. For automatic methods, we compare against leading LLM-based text-only optimizers, including APE (Zhou et al., 2023), OPRO (Yang et al., 2024), EvoPrompt (Guo et al., 2024), PE2 (Ye et al., 2024), ProTeGi (Pryzant et al., 2023), and SEE (Cui et al., 2025). Detailed descriptions of all baselines are provided in Appendix A.2. Implementation Details For answer generation, we use Qwen2.5-VL (7B) (Bai et al., 2025) as the base model for image and video tasks, and Qwen3 (8B) (Yang et al., 2025) for molecular tasks. During optimization, GPT-4o mini (OpenAI, 2024) serves as the prompt optimizer, responsible for analyzing failures and refining multimodal prompts. For modality-specific generation, we employ GPT-Image (OpenAI, 2025) for images, Wan2.1 (1.3B) (Wan et al., 2025) for videos, and again GPT-4o mini for molecules. For the implementation of the iterative optimization loop, we use the beam search (Pryzant et al., 2023) with the beam size of = 3 and the number of iterations of = 13. Also, at each iteration (except the first), b2 child prompts are produced by evenly applying the generation, edit, and mix operators, after which the top-b prompts are selected via prior-inherited Bayesian-UCB. Meanwhile, in the first iteration, only the generation operator is used to initialize 6 Table 2: Generalizability results of MPO across components with different backbones: (Top) base models; (Bottom Left) optimizer models; (Bottom Right) modality-specific generators. InternVL-3.5 (14B) GPT-4.1 nano Qwen2.5-VL (72B) Gemma3 (12B) Human 1-shot 3-shot 5-shot ProTeGi SEE MPO 55.7 66.8 69.6 72. 74.1 73.6 80.4 45.6 56.7 64.6 68.9 68.2 68.1 73.1 51.6 34.7 36.5 34. 71.9 70.8 73.2 46.8 46.1 37.7 42.6 61.0 61.6 65.9 Optimizer Model SEE MPO T2I Generator PlantVillage* Qwen2.5-VL (7B) Gemini 2.5 Flash GPT-4o mini GPT-4o 65.2 68.2 69.0 69.2 69.1 74.8 76.4 78.0 SEE (Text-only) SANA1.5 (1.6B) Nano Banana GPT-Image-Low GPT-Image-Medium 69.0 71.8 72.9 76.4 76.6 Figure 4: Relationship between cross-modal alignment and performance gain. We report median values alongside Q1 and Q3. multimodal prompts, since no non-textual prompts exist yet. The complete optimization process is summarized in Algorithm 1. To ensure fairness, we keep the number of explored prompts consistent across all methods. In our case, each candidate prompt is allocated an evaluation budget of 100, and the prior strength for our prior inheritance is set to 10% of this budget (S = 10). Reported results are averaged over three independent runs. Please see Appendix A.3 for additional details. 4.2 EXPERIMENTAL RESULTS AND ANALYSES Main Results As shown in Table 1, MPO consistently outperforms all baselines across image, video, and molecular domains, confirming its effectiveness in discovering prompts that more effectively harness the capabilities of MLLMs. Specifically, compared to existing text-only optimization methods, MPO achieves substantial gains, demonstrating that incorporating non-textual signals into prompts provides stronger contextual grounding and enhances task-specific reasoning. Moreover, MPO outperforms exemplar-based Few-Shot prompting, showing that it can capture richer crossmodal information and its underlying dependencies beyond simple queryanswer demonstrations. In both image and video domains, MPO performs strongly on classification and QA tasks, underscoring its robustness across diverse real-world scenarios. Likewise, on molecular tasks, MPO surpasses all baselines, highlighting its effectiveness in highly specialized applications. Generalizability to Diverse Backbone Models We further validate the generalizability of MPO by varying the backbone models used in each component, namely, base models, optimizer models, and modality-specific generators, and assessing its robustness under these variations. First, as shown in Table 2 (Top), MPO maintains strong performance across different architectures and exhibits even greater effectiveness as model size increases, for example, with Qwen2.5-VL (72B). Also, Table 2 (Bottom Left) further shows that MPO remains effective regardless of the optimizer model, surpassing state-of-the-art text-only methods (e.g., SEE) under diverse backbone models for optimization. Finally, Table 2 (Bottom Right) demonstrates that MPO generalizes well to modality-specific generators, including lightweight open-source models such as SANA1.5 (1.6B), where it continues to outperform textual optimization methods. These results highlight MPO as broadly generalizable and robust framework, effective across wide variety of base models and practical scenarios. Analysis on Cross-Modal Alignment Recall that MPO uses the alignment-preserving exploration to jointly refine textual and non-textual components of multimodal prompts, and we further analyze how this cross-modal alignment strategy contributes to performance gains. To isolate this effect, we consider four variants: (1) Sequential, where the textual prompt is optimized first and the non-textual prompt is refined afterward; (2) Random Image Prompt, where the image component is replaced with another optimized image prompt (i.e., not jointly optimized with the text); (3) In-Distribution Image Query, where it is replaced with an image sampled from the same task; and (4) OOD Image Query, where it is replaced with an image sampled from different task. After that, we measure the relationship between performance gain over the Human baseline and the DSG score designed to quantify alignment (Cho et al., 2024). As shown in Figure 4, MPO achieves both the highest alignment score and the largest performance gains, followed by Sequential optimization and Random Image Prompt, while In-Distribution and OOD Image Query lag significantly behind. These results 7 Table 3: Ablation on the contribution of each modality in the optimized multimodal prompt. Table 4: Ablation on three exploration operators, utilizing each one of them individually. Text Image PlantVillage* CUB* Apple Corn Grape Potato Avg. Human - Human MPO MPO - MPO MPO 42.2 50.4 55. 76.4 47.9 58.2 64.2 78.6 SEE 76.4 75.9 48.0 75.7 69.0 Generation 76.9 77.9 53.7 83.6 73.3 Edit Mix 77.2 76.3 56.2 80.1 72.5 74.0 77.9 65.1 79.8 74.8 MPO (Full) 77.7 78.2 65.9 84.0 76. Figure 5: Efficiency comparison of selection strategies. Figure 6: Image prompt optimization process of the best-performing multimodal prompt on subtask (i.e., grosbeak species classification) of CUB. Task Classes box contains the examples of four species: Rose Breasted Grosbeak, Pine Grosbeak, Blue Grosbeak, and Evening Grosbeak. confirm that stronger cross-modal alignment directly translates to better task performance, and that alignment-preserving updates (included in MPO) are crucial in promoting modality consistency. Ablation on Modality Contributions in Prompts To examine the contribution of each modality within optimized prompts, we ablate the textual and non-textual components from the final multimodal prompt. As shown in Table 3, using only single modality (either MPO text without image or human text combined with MPO image) already surpasses the Human baseline, confirming that both modalities independently provide useful signals. However, the full multimodal prompt yields substantially higher performance, demonstrating that the two modalities are not merely additive but mutually reinforcing, which underscores the importance of jointly leveraging textual and non-textual information to achieve performance gains beyond what either modality can deliver alone. Effect of Exploration Operators To assess the contribution of the proposed exploration operators (such as generation, edit, and mix), we conduct both qualitative and ablation analyses. Qualitatively, as illustrated in Figure 6, we observe that each operator serves distinct role: the generation operator introduces novel visual compositions, the edit operator fine-tunes local features such as textures or visual characters, and the mix operator blends broader attributes such as background or spatial layout. In addition to this, the ablation study in Table 4 further confirms their complementary effects: while each operator individually improves over the baseline, combining all three within MPO leads to the best performance. This demonstrates that the proposed operators jointly enable more comprehensive exploration of the multimodal prompt space, facilitating the discovery of the optimal prompts. We observe similar pattern in molecular prompt optimization, shown in Figure 14, with concrete examples of operator-driven updates (including textual conditions) provided in Table 8. Selection Strategies We evaluate the effectiveness of our prior-inherited Bayesian UCB strategy for candidate prompt selection by comparing it against three alternatives: Uniform, which distributes the evaluation budget evenly across candidates; UCB (Auer, 2002), standard bandit algorithm; and an ablated variant of ours w/o Prior. As shown in Figure 5, MPO achieves the same performance as the Uniform strategy while using only 30% of the evaluation budget, yielding 70% reduction in resource cost. Moreover, MPO consistently outperforms both UCB and w/o Prior, reaching their performance levels with 52% and 42% less budget, respectively. These results confirm that the warm start enabled by prior inheritance is crucial for both efficiency and accuracy, allowing MPO to scale effectively over the enlarged multimodal search space and reliably identify high-quality prompts. 8 Figure 7: Train Curve of MPO compared to ProTeGi on CUB. Figure 8: Visualization of hidden states in MLLMs by PCA. Figure 9: Analysis of the prior strength (S) on performance. Train Dynamics of MPO To better understand how MPO improves over the course of optimization, we analyze its training dynamics in comparison to ProTeGi by tracking the test performance of the top-1 prompt on the CUB dataset. As shown in Figure 7, both methods improve during the early iterations; however, ProTeGi quickly plateaus after the third iteration, with only marginal additional gain of 1.1 points. In contrast, MPO continues to improve steadily, ultimately achieving much higher final score, including an additional 6.4-point gain beyond the third iteration. This comparison result highlights that MPO effectively overcomes the performance ceiling of text-only optimization methods by effectively navigating the multimodal prompt space, enabling it to escape local optima (imposed by the text-only strategy) and discover prompts closer to the global optimum. Hidden State Visualization To gain deeper insight into why optimized multimodal prompts yield greater performance improvements than text-only prompts, we visualize the hidden state of MLLMs by averaging intermediate-layer embeddings, following Zhang et al. (2024a). As shown in Figure 8, hidden states obtained from text-only methods (including the text-only component of MPO) cluster together, suggesting that they guide the reasoning of MLLMs within similar yet limited semantic space. In contrast, the full multimodal prompt from MPO shifts the hidden states into distinct region, indicating that the non-textual component introduces information unavailable from text alone. In other words, the multimodal prompt alters the internal representation space of models, enabling richer reasoning pathways and ultimately leading to superior task performance. Analysis of Prior Strength Recall that in our prior-inherited selection strategy, the prior strength determines the number of pseudo-observations used to initialize the score distributions of child prompts, and we study its effect by varying and reporting the resulting performance. As shown in Figure 9, we first observe that small under-utilizes the parent prior, resulting in weaker guidance and suboptimal performance. In contrast, an excessively large causes the model to over-rely on the parent prior, limiting its ability to adapt to the actual performance of child prompts. Consequently, the performance is maximized at an intermediate S, where inherited knowledge provides strong warm start while still allowing sufficient flexibility to incorporate new observations. Qualitative Result We provide qualitative examples for the optimized multimodal prompts for the image modality in Table 9 of Appendix. From this, we observe that the optimized multimodal prompts consistently supply task-critical context in both textual and visual forms. Also, more importantly, the textual prompts explicitly instruct the model to leverage non-textual signals (e.g., Use the hybrid reference image for guidance), thereby unlocking the full multimodal capacity of MLLMs. Additional examples for the video and molecular modalities are presented in Tables 10, 11, and 12."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced the novel problem of multimodal prompt optimization, extending the optimization space beyond text to fully leverage the capability of MLLMs. To tackle this, we proposed the Multimodal Prompt Optimizer (MPO), unified framework that jointly refines textual and non-textual components through alignment-preserving exploration with multiple generation operations and efficiently identifies high-quality prompts via prior-inherited Bayesian UCB strategy. Experiments across diverse modalities (including images, videos, and molecules) demonstrate that MPO consistently surpasses leading text-only prompt optimization methods, validating its efficacy in diverse real-world multimodal problems. We believe our work establishes multimodal prompt optimization as key direction for advancing the use of MLLMs, moving beyond text-only prompting paradigms."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our study does not involve human subjects, personally identifiable data, or sensitive information. All experiments were conducted on public datasets and models under research-permissive licenses."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We attach the code to ensure the reproducibility of our work in the supplementary materials. Additionally, we provide detailed description of the experimental setup in Section 4.1. We further provide additional implementation details in Appendix A.3, the dataset configuration in Appendix A.1, the meta prompts to operationalize MPO in Appendix A.4, and the full algorithms in Appendix A.5."
        },
        {
            "title": "REFERENCES",
            "content": "Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 2002. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, et al. Qwen2.5-vl technical report. ArXiv, 2025. Fabio Broccatelli, Emanuele Carosati, Annalisa Neri, Maria Frosini, Laura Goracci, Tudor Oprea, and Gabriele Cruciani. novel approach for predicting p-glycoprotein (abcb1) inhibition using molecular interaction fields. Journal of medicinal chemistry, 2011. Jaemin Cho, Yushi Hu, Jason M. Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. In The Twelfth International Conference on Learning Representations, ICLR, 2024. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. ArXiv, 2023. Charles Corbi`ere, Simon Roburin, Syrielle Montariol, Antoine Bosselut, and Alexandre Alahi. DRIVINGVQA: analyzing visual chain-of-thought reasoning of vision language models in realworld scenarios with driving theory tests. ArXiv, 2025. Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley A. Malin, and Kumar Sricharan. SEE: strategic exploration and exploitation for cohesive in-context prompt optimization. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics ACL 2025, 2025. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktaschel. Promptbreeder: Self-referential self-improvement via prompt evolution. In Forty-first International Conference on Machine Learning, ICML, 2024. Hanan Gani, Rohit Bharadwaj, Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan. Vanebench: Video anomaly evaluation benchmark for conversational lmms. In Findings of the Association for Computational Linguistics: NAACL 2025, 2025. Bingjie Gao, Xinyu Gao, Xiaoxue Wu, Yujie Zhou, Yu Qiao, Li Niu, Xinyuan Chen, and Yaohui Wang. The devil is in the prompts: Retrieval-augmented prompt optimization for text-to-video generation. In Conference on Computer Vision and Pattern Recognition, 2025. Team Gemini. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. ArXiv, 2025. Aaron Grattafiori et al. The llama 3 herd of models. ArXiv, 2024. 10 Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth International Conference on Learning Representations, ICLR, 2024. Tingjun Hou, Junmei Wang, Wei Zhang, and Xiaojie Xu. Adme evaluation in drug discovery. 7. prediction of oral absorption by correlation and classification. Journal of chemical information and modeling, 2007. Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. Proceedings of Neural Information Processing Systems, NeurIPS Datasets and Benchmarks, 2021. Songtao Jiang, Yan Zhang, Chenyi Zhou, Yeying Jin, Yang Feng, Jian Wu, and Zuozhu Liu. Joint visual and text prompting for improved object-centric perception with multimodal large language models. ArXiv, 2024. Muhammad Uzair Khattak, Hanoona Abdul Rasheed, Muhammad Maaz, Salman H. Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Conference on Computer Vision and Pattern Recognition, 2023. Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, and Lu Yuan. Rethinking visual prompting for multimodal large language models with external knowledge. ArXiv, 2024. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 18th IEEE International Symposium on Biomedical Imaging, ISBI 2021, 2021. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Annual Conference on Neural Information Processing Systems 2023, NeurIPS, 2023. Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. RSVQA: visual question answering for remote sensing data. Transactions on Geoscience and Remote Sensing, 2020. Chang-Ying Ma, Sheng-Yong Yang, Hui Zhang, Ming-Li Xiang, Qi Huang, and Yu-Quan Wei. Prediction models of human plasma protein binding rate and oral bioavailability derived by using gacgsvm method. Journal of pharmaceutical and biomedical analysis, 2008. Oscar Manas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-to-image consistency via automatic prompt optimization. Transactions on Machine Learning Research, 2024. Manuel Martin, Alina Roitberg, Monica Haurilet, Matthias Horne, Simon Reiß, Michael Voit, and Rainer Stiefelhagen. Drive&act: multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles. In International Conference on Computer Vision, ICCV, 2019. Ines Filipa Martins, Ana Teixeira, Luis Pinheiro, and Andre Falcao. bayesian approach to in silico blood-brain barrier penetration modeling. Journal of chemical information and modeling, 2012. Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, and Qing Yang. Dynamic prompt optimizing for text-to-image generation. In Conference on Computer Vision and Pattern Recognition, 2024. Sharada P. Mohanty, David P. Hughes, and Marcel Salathe. Using deep learning for image-based plant disease detection. Frontiers in Plant Science, 2016. OpenAI. Gpt-4v(ision) system card. 2023. URL https://cdn.openai.com/papers/ GPTV_System_Card.pdf. OpenAI. Gpt-4o system card. ArXiv, 2024. 11 OpenAI. Introducing 4o image generation, 2025. URL https://openai.com/index/ introducing-4o-image-generation/. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with gradient descent and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP. Association for Computational Linguistics, 2023. Vishal Siramshetty, Jordan Williams, Dac-Trung Nguyen, Jorge Neyra, Noel Southall, Ewy Mathe, Xin Xu, and Pranav Shah. Validating adme qsar models using marketed drugs. SLAS DISCOVERY: Advancing the Science of Drug Discovery, 2021. Henrike Veith, Noel Southall, Ruili Huang, Tim James, Darren Fayne, Natalia Artemenko, Min Shen, James Inglese, Christopher Austin, David Lloyd, et al. Comprehensive characterization of cytochrome p450 isozyme selectivity across chemical libraries. Nature biotechnology, 2009. C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds 200. Technical report, California Institute of Technology, 2011. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, et al. Wan: Open and advanced large-scale video generative models. ArXiv, 2025. Taowen Wang, Yiyang Liu, James Chenhao Liang, Junhan Zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, and Dongfang Liu. M2PT: Multimodal prompt tuning for zero-shot instruction learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35, NeurIPS, 2022. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. ArXiv, 2025. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, ICLR, 2024. Qinyuan Ye, Mohamed Ahmed, Reid Pryzant, and Fereshte Khani. Prompt engineering prompt engineer. In Findings of the Association for Computational Linguistics, ACL 2024, 2024. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639:609616, 2025. Fanhu Zeng, Fei Zhu, Haiyang Guo, Xu-Yao Zhang, and Cheng-Lin Liu. Modalprompt:dualmodality guided prompt for continual learning of large multimodal models. ArXiv, 2024. Lechen Zhang, Tolga Ergen, Lajanugen Logeswaran, Moontae Lee, and David Jurgens. SPRIG: improving large language model performance by system prompt optimization. ArXiv, 2024a. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research, 2024b. Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought prompting for visual reasoning refinement in multimodal large language models. ArXiv, 2024. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, ICLR, 2023. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. 2025."
        },
        {
            "title": "A ADDITIONAL EXPERIMENTAL DETAILS",
            "content": "A.1 DETAILS ON DATASETS We provide detailed description of the datasets used in our experiments. To conduct comprehensive evaluation, we compile diverse set of benchmarks for classification and question-answering tasks across various modalities, including images, videos, and molecules. We use the official training/test splits where available, and if not, we create our own splits. For the image and video modality tasks, we sample 300 test examples, whereas for the molecule modality, we use the entire test set. PlantVillage The PlantVillage dataset (Mohanty et al., 2016) contains 54,306 images of plant leaves, spanning 38 disease categories across 14 crop species. To construct focused, fine-grained classification task, we design subtasks by selecting four crop species, each having at least three distinct classes (e.g., one healthy and two or more diseases). This setup allows for more controlled evaluation of the models ability to identify specific plant diseases. Due to the lack of an official split, we split this subset using the 50/50 ratio for training and testing. CUB-200-2011 The CUB-200-2011 dataset (Wah et al., 2011) is standard benchmark for finegrained bird species classification. To evaluate the capability of MLLMs in distinguishing between visually similar species, we group birds that share common family name (e.g., Hummingbird), and select groups containing three or four distinct species to ensure balanced level of difficulty, resulting in total of 12 subtasks. Then, we divide the samples for each subtask using 50/50 ratio, curating them to contain at least 80 instances for both training and test. SLAKE The SLAKE dataset (Liu et al., 2021) is an open-ended visual question answering benchmark tailored for the medical domain from various radiological modalities. To assess the performance of MLLMs across these different modalities, we partition the dataset into distinct subsets based on the modality, creating separate tasks for CT, MRI, and X-Ray images. DrivingVQA The DrivingVQA dataset (Corbi`ere et al., 2025) is closed-ended visual question answering benchmark with 3,931 multiple-choice questions based on real-world driving scenarios. To avoid ambiguity in the evaluation process, we filter the dataset to exclusively retain instances with single correct answer, resulting in final dataset of 2,039 training and 521 test instances. RSVQA We use the RSVQA dataset (Lobry et al., 2020) to evaluate performance on the openended visual question answering task for remote sensing images. Notably, the questions are designed to evaluate models understanding of various geospatial concepts, including land cover classification, object counting, and relational reasoning between objects. For our experiments, we utilize the low-resolution image set from the benchmark. Drive&Act For the video classification task, we use the Drive&Act dataset (Martin et al., 2019), which provides comprehensive labels for driver behaviors inside vehicles. We adhere to the official split of 6,642 training and 2,222 test instances, and preprocess the video clips by sampling frames at rate of 1 frame per second (fps). VANE-Bench The VANE-Bench dataset (Gani et al., 2025) is closed-ended question answering benchmark for video anomaly detection, whose samples (each with 10-frame clip from synthetic or real-world videos) show various irregularities or distortions. We split the dataset into training and test sets using the 60/40 ratio, resulting in 293 training and 263 test instances. Absorption The Absorption task (Huang et al., 2021) is categorized into molecular property prediction, designed to evaluate models ability to estimate pharmacokinetic characteristics related to drug absorption. It is composed of four subtasks: PAMPA (Parallel Artificial Membrane Permeability Assay), HIA (Human Intestinal Absorption), Pgp (P-glycoprotein substrate classification), and Bioavailability, and we use the official random split. 13 BBBP BBBP (Martins et al., 2012) is molecular classification task to predict whether the given molecule can penetrate the blood-brain barrier (BBB), which is highly selective system. We use the official random split from Huang et al. (2021), consisting of 1,453 train and 382 test examples. CYP Inhibit The CYP Inhibition task (Veith et al., 2009) involves classifying whether molecule can inhibit Cytochrome P450 (CYP) enzymes, which play key roles in metabolism. It comprises five subtasks: inhibition of CYP 2C19, CYP 2D6, CYP 3A4, CYP 1A2, and CYP 2C9. We adopt the official random split provided in Huang et al. (2021). A.2 DETAILS ON BASELINES This subsection details the baseline methods used in our experiments. APE (Zhou et al., 2023) generates candidate prompts by reverse-engineering instructions from examples and by paraphrasing existing prompts. OPRO (Yang et al., 2024) leverages the LLM as an optimizer, guiding it with pairs of prompts and their performance scores to generate progressively better instructions. EvoPrompt (Guo et al., 2024) utilizes an evolutionary algorithm where the LLM performs mutation and crossover operations on population of prompts. PE2 (Ye et al., 2024) focuses on optimizing the meta-prompt used to steer the LLM optimizer. It provides guidance through structured template containing detailed task descriptions, context specification, and step-by-step reasoning format. ProTeGi (Pryzant et al., 2023) simulates gradient descent for discrete prompts. It uses the LLM to generate natural language critiques based on prompt failures (termed textual gradients), and subsequently edits the prompt in the opposite semantic direction. SEE (Cui et al., 2025) performs cohesive optimization of both the prompt instructions and the incontext examples. The method follows four-phase process that strategically alternates between global exploration and local exploitation. A.3 ADDITIONAL IMPLEMENTATION DETAILS In this subsection, we provide the additional implementation details in our experiments. Regarding model temperature, we use temperature value of 0 for the base model to ensure consistency and 0.7 for the optimizer model to encourage the generation of diverse candidate prompts. The failure set size in the cohesive backpropagation process is fixed at 3. While the evaluation budget is generally set to 100, for CUB subtasks with fewer than 100 training samples, the budget for our MPO method is specifically set to one-third of the available instances. For modality-specific handling, we implement several strategies. In the video task, when the video query is part of the failure set, we sample three representative frames (first, middle, and last) from queries. In video generation, to mitigate the high complexity of video editing and mixing, we employ only the generation operator. We generate 5-second videos at 16 fps, then downsample them to 5 frames at 1 fps to construct the video prompt. For the molecule tasks, we represent chemical structures using the 1D representation (i.e., SMILES) and utilize GPT-4o mini for the molecule generator. Regarding optimization objectives, we use accuracy for image and video modalities, and F1 for the molecular modality to handle the class imbalance. Finally, to measure answer correctness, we adopt task-specific evaluation criteria: the final predefined label is extracted for standard classification, strict formatting rules are applied for binary and closed-ended QA tasks, and exact match is used for open-ended QA tasks. We select the best-performing prompts on the training set and report their performance on the test set. Our experiments are conducted on NVIDIA H100 80GB GPUs. 14 A.4 META PROMPTS TO IMPLEMENT MPO This subsection details the meta-prompts to instantiate MPO, which include cohesive backpropagation prompt and three operator prompts (generation, edit, mix) for update. We provide the meta prompt from image modality as representative example. The prompts for other modalities, such as video and molecule, are based on this structure, with minor, modality-specific wordings adjusted. Prompt for Cohesive Backpropagation You are Prompt Failure Analysis Agent specialized in multimodal prompt optimization. Your task is to analyze the failure case of Multimodal Large Language Model (MLLM) and identify the potential reasons in the prompt for the models incorrect prediction. Based on the given input, output, and ground truth, analyze both the Text Prompt and the Image Prompt used in the task. ### Input Structure for MLLM: - Text Prompt: task-specific textual instruction for the MLLM. - Image Prompt: reference image that supports task understanding. - Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an answer. ### Prompts: - Text Prompt : {text_prompt} - Image Prompt : {modality_prompt} ### Wrong Examples: {wrong_examples} ### Output Format: Text Prompt Analysis: - Identify missing information, vague instructions, or ambiguous wording that could have misled the model. - Explain how weaknesses in the Text Prompt may have contributed to the wrong output. - Suggest specific improvements (e.g., clearer task definition, additional constraints, better examples) to help the model produce the correct answer. Image Prompt Analysis: - If an image Prompt was used, analyze its effectiveness. - Identify problems such as lack of clarity, poor composition, irrelevant details, or missing key features. - If no image Prompt was used, suggest what kind of image (visual content, attributes, composition) would help correct the failure. Figure 10: Meta Prompt for Cohesive Backpropagation in MPO. Prompt for Generation Operator You are Prompt-Improvement Agent specializing in multimodal prompt optimization. Your task is to design improved prompts for both image generation and text instruction, aimed at enhancing the performance of Multimodal Large Language Model (MLLM). ### Input Structure for MLLM: - Text Prompt: task-specific textual instruction for the MLLM. - Image Prompt: reference image that supports task understanding. - Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an answer. ### Provided Material - Text Prompt: {text_prompt} - Image Prompt: {modality_prompt} - Wrong Examples: {wrong_examples} - Failure Analysis: {analysis} ### Your Task Your task is to review the failure analysis carefully to understand the issues and create two improved prompts that directly address the issues in the failure analysis: 1. Image Generation Prompt - Write detailed prompt for an image generator. - Enhance or redesign the reference image to resolve issues found in the analysis. - Ensure the image highlights critical visual features necessary for success. - If no reference image is provided, suggest an appropriate one based on the failure analysis. 2. Improved Text Prompt - Write clear, concise, and unambiguous instruction for the MLLM. - Resolve ambiguities found in the failure analysis. - Elaborate on how the reference image should be interpreted. ### Output Format <image_generation_prompt>{image_generation_prompt}</image_generation_prompt> <improved_text_prompt>{improved_text_prompt}</improved_text_prompt> Figure 11: Meta Prompt for Generation Operator in MPO. 15 Prompt for Edit Operator You are Prompt-Improvement Agent specializing in multimodal prompt optimization, with focus on prompt editing. Your task is to design improved prompts for both image editing and text instruction, aimed at enhancing the performance of Multimodal Large Language Model (MLLM). ### Input Structure for MLLM: - Text Prompt: task-specific textual instruction for the MLLM. - Image Prompt: reference image that supports task understanding. - Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an answer. ### Provided Material - Text Prompt: {text_prompt} - Image Prompt: {modality_prompt} - Wrong Examples: {wrong_examples} - Failure Analysis: {analysis} ### Your Task Your task is to review the failure analysis carefully to understand the issues and create two improved prompts that directly address the issues in the failure analysis: 1. Image Editing Prompt: - Write precise and context-aware prompt instructing the image editor to modify the given reference image. - Specify which visual components (e.g., objects, colors, textures, lighting, perspective, composition) should be added, removed, or replaced based on the failure analysis. - Clearly identify any undesirable visual elements that led to the failure. - Guide the editor on how to retain key features, proportions, or stylistic elements that are critical to the intended outcome. 2. Improved Text Prompt - Write clear, concise, and unambiguous instruction for the MLLM. - Resolve ambiguities found in the failure analysis. - Elaborate on how the reference image should be interpreted. ### Output Format <image_edit_prompt>{image_edit_prompt}</image_edit_prompt> <improved_text_prompt>{improved_text_prompt}</improved_text_prompt> Figure 12: Meta Prompt for Edit Operator in MPO. Prompt for Mix Operator You are Prompt-Improvement Agent specializing in multimodal prompt optimization, with focus on cross-prompt fusion. Your task is to create improved, mixed prompts for both image prompt and text instruction, aimed at enhancing the performance of Multimodal Large Language Model (MLLM). ### Input Structure for MLLM: - Text Prompt: task-specific textual instruction for the MLLM. - Image Prompt: reference image that supports task understanding. - Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an answer. ### Provided Material #### Prompt - Text Prompt A: {text_prompt_A} - Image Prompt A: {modality_prompt_A} - Wrong Examples from Prompt A: {wrong_examples_A} - Failure Analysis for Prompt A: {analysis_A} #### Prompt - Text Prompt B: {text_prompt_B} - Image Prompt B: {modality_prompt_B} - Wrong Examples from Prompt B: {wrong_examples_B} - Failure Analysis for Prompt B: {analysis_B} ### Your Task Your task is to review the failure analysis carefully to understand the issues and create two improved prompts that directly address the issues in the failure analysis: 1. Image Mixing Prompt: - Write guidance for the image generator to combine and improve both reference images. - Address visual issues identified in both failure analyses. - Guide the model to create new hybrid image that merges key beneficial visual features from both references while mitigating their weaknesses. - Explicitly state which visual elements from each image should be retained, modified, or discarded to achieve task success. 2. Improved Text Prompt - Write clear, concise, and unambiguous instruction for the MLLM. - Incorporate key visual or task-relevant features identified in both failure analysis. - Explain how the reference image should be used to assist the task. ### Output Format <image_mixing_prompt>{image_mixing_prompt}</image_mixing_prompt> <mixed_text_prompt>{mixed_text_prompt}</mixed_text_prompt> Figure 13: Meta Prompt for Mix Operator in MPO. A.5 FULL ALGORITHM OF MPO We provide the overall algorithm for MPO, with alignment-preserving exploration (including the operators) described in Algorithm 1 and the prior-inherited Bayesian UCB selection in Algorithm 2. Algorithm 1 MPO: Multimodal Prompt Optimizer Require: Initial prompt (t0, ), Number of iterations , Beam size Train dataset Dtr, Metric function for = 1..b do Fp {(q, a, y) (q, a) Dtr, = MLLM(p, q), = a} (t, cgen) MLLM.Generation(t0, ; p, Fp); g(cgen, ) {(t, m)} 1: (t0, ), {p}, , ˆµ E(q,a)Dtr [f (MLLM(t0, , q), a)] 2: for = 1..b2 do 3: 4: MLLM.Feedback(t0, ; Fp) 5: 6: 7: end for 8: BayesianUCBSelect(P, C, b) 9: for iter = 1..T do 10: for all = (t, m) do 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end for 29: return (t, m) where = arg maxpP ˆµp, Fp {(q, a, y) (q, a) Dtr, = MLLM(p, q), = a} MLLM.Feedback(t, m; Fp) op RandomSample({generation, edit, mix}) if op = generation then end for BayesianUCBSelect(P, C, b) (t, cedit) MLLM.Edit(t, m; p, Fp); g(cedit, {m}) end if {(t, m)} else if op = mix then else if op = edit then ˆµp = αp end for αp+βp (t, cgen) MLLM.Generation(t, m; p, Fp); g(cgen, ) Select prompts for next step Cohesive backpropagation Joint multimodal update RandomSample(P {p}) (t, cmix) MLLM.Mix((t, m; p, Fp), (t, m; p, p)); g(cmix, {m, m}) Select prompts for next step Algorithm 2 Prior-Inherited Bayesian UCB Selection Require: Parent Prompts P, set of child prompts = {pi}k i=1, Beam Size Parents performance {ˆµpar(i)}k Total evaluation budget , Prior strength S, Exploration parameter i=1, Train dataset Dtr, Metric function , Batch size αi ˆµpar(i) + 1 , 1: Initialize Beta priors for each child prompt pi P: 2: for = 1, . . . , do 3: 4: end for 5: for = 1, 2, . . . , (N/B) do 6: βi (1 ˆµpar(i)) + 1 1 t(log )c qt 1 arg maxi{1,..,k} BetaQuantile(qt; αi, βi) Choose prompt with highest UCB 7: 8: Dmini Sample(Dtr, B) 9: 10: 11: end for 12: Return top-b prompts from sorted by posterior mean ˆµi = αi st E(q,a)Dmini [f (MLLM(t, m, q), a)] αj αj + st , βj βj + (1 st) αi+βi Evaluate on small data batch Update posterior Inherit prior from parent 17 THEORETICAL ANALYSIS ON PRIOR-INHERITED BAYESIAN UCB In this section, we provide the proof for Proposition 3.1, starting with the formal problem setting. Setting. Let each arm {1, . . . , k} have an unknown Bernoulli mean reward µi (0, 1) and let arg maxi µi be an optimal arm. Write the suboptimality gap as = µi µi > 0 for = i. As shown in algorithm 2, the Bayesian UCB algorithm maintains Beta posterior distribution for each arms mean reward. At each round t, Bayesian UCB selects the arm with the highest upper posterior quantile, qt, observes the resulting Bernoulli reward, and updates the corresponding posterior. Prior inheritance For each child arm i, we initialize Beta prior using the parents posterior mean ˆµpar(i) (0, 1) and pseudo-count > 0: α0,i = ˆµpar(i) + 1, β0,i = (1 ˆµpar(i)) + 1. (2) For comparison, uninformative (or uniform) prior is Beta(1, 1). After Ni(t) pulls with Xi(t) successes by time t, the posterior parameters are αi(t) = α0,i + Xi(t) and βi(t) = β0,i + Ni(t) Xi(t). Denote the posterior mean ˆµt,i = αi(t)/(αi(t) + βi(t)), the upper quantile qt,i = BetaQuantile(qt; αi(t), βi(t)), and the lower quantile ℓt,i = BetaQuantile(1 qt; αi(t), βi(t)). Average KL-closeness assumption. Our analysis relies on the assumption that parents posterior provides useful inductive bias for its children. We formalize this concept using the KullbackLeibler (KL) divergence for Bernoulli distributions, defined as d(p, q) = log + (1 p) log 1p 1q . Let be the population of child arms produced during optimization. We assume the parent estimate is, on average over children, KL-closer to the truth than the mean of the uninformative prior: (cid:1) d(cid:0)µi, 1 2 The assumption is empirically supported by the strong positive correlation observed between parent and child scores (Figure 3). (cid:2)d(cid:0)µi, ˆµpar(i) for some γ > 0. (cid:1)(cid:3) γ EiI (3) B.1 TWO AUXILIARY LEMMAS Lemma B.1 (Pseudo-counts shrink one-sided credible widths). There exists universal constant > 0 such that for all 2 and all arms i, qt,i ˆµt,i (cid:115) log Ni(t) + , ˆµt,i ℓt,i (cid:115) log Ni(t) + . (4) The key implication is that the credible interval width scales with 1/(cid:112)Ni(t) + rather than 1/(cid:112)Ni(t). Thus, the prior strength acts as an additive effective sample size, shrinking the interval as if we had additional observations. proof sketch. The proof relies on standard concentration bounds for Beta posteriors. The conjugacy of the Beta-Binomial model makes the posterior tractable, allowing for the specific application of Chernoff tail bound. For any ε (0, 1), the probability that the upper posterior quantile underestimates the true mean by at least P{ qt,i µi ε } exp(cid:0) (Ni(t) + S) d(µi ε, µi)(cid:1), (5) with symmetric bound holding for the lower quantile ℓt,i. The result is obtained by using the approximation d(µi ε, µi) ε2 for small ε and selecting the quantile level 1 qt = Θ(1/t) yields the stated (cid:112)log t/(Ni(t) + S) bounds. Lemma B.2 (Effect of Informative Priors on Posterior Quantiles). Under equation 3, for any fixed real counts (n, s) with Binom(n, µi), the posterior under prior inheritance Beta(α0,i+s, β0,i+ s) is, on average over I, better centered around µi than the posterior under uninformative prior Beta(1 + s, 1 + s). Consequently, (cid:2)ℓ(par) (cid:3) 0, (i = i), (cid:3) 0 (cid:2)q(par) (6) t,i q(unif) t,i t,i ℓ(unif) EiI with strict inequalities whenever γ > 0 and > 0. EiI t,i 18 proof sketch. The posterior mean under prior inheritance is ˆµ(par) , while the posS+n+2 terior mean under uninformative prior is ˆµ(unif) n,i = 1+s n+2 . Taking expectation over and then over yields convex combinations of µi with ˆµpar(i) versus 1/2. Our KL-closeness assumption equation 3 directly implies that the posterior mean under prior inheritance is, in expectation, better estimate of µi. Since the posterior quantiles are centered around this mean, Lemma B.1 ensures that an improvement in the means centering translates to the stated shifts in the quantiles, holding in expectation. n,i = ˆµpar(i)+s+1 B.2 SUFFICIENT CONDITION FOR OPTIMAL ARM IDENTIFICATION For the algorithm to correctly identify the optimal arm by the final round , sufficient condition is that the credible intervals for the optimal and suboptimal arms are well-separated. Formally, this occurs if the lower quantile of the optimal arm exceeds the upper quantile of every suboptimal arm. This is the separation event: ℓT,i > max i=i qT,i. (7) If this separation event fails, it implies that for some suboptimal arm i, the credible intervals overlap. This allows us to bound the suboptimality gap by the sum of the one-sided credible widths: (µi ℓT,i ) + (qT,i µi) (cid:115) (cid:115) log Ni (T ) + + log Ni(T ) + , (8) where the second inequality follows from Lemma B.1. This implies that to guarantee separation, the credible interval widths must be sufficiently small relative to the gap. Therefore, deterministic sufficient condition for equation 7 is: there exists universal constant > 0 such that, (cid:115) log Ni (T ) + + (cid:115) log Ni(T ) + < i. (9) Crucially, combining this condition with the quantile shift from Lemma B.2 reveals the benefit of our approach. Because the prior inheritance yields better quantile estimates, the sample allocation required to satisfy equation 9 is achieved no later than with an uninformative prior. B.3 PROOF OF PROPOSITION 3.1 Proof. The prior inheritance improves the performance of Bayesian UCB through two synergistic mechanisms: (i) Tighter credible intervals at fixed counts. For any given allocation of pulls, the prior strength acts as an additive effective sample size. As established in Lemma B.1, this shrinks the credible interval widths by effectively replacing the sample size Ni(T ) with Ni(T )+S. This directly reduces the left-hand side of the deterministic condition equation 9, making it easier to satisfy. (ii) More efficient sample allocation. The informative prior also leads to better allocation of pull over time. Lemma B.2 shows that the quantiles are favorably shifted on average: the lower bound for the optimal arm increases, while the upper bounds for suboptimal arms decrease. This improved estimation guides the UCB policy to allocate more pulls to and waste fewer on suboptimal arms, particularly in the early stages. Consequently, in expectation: (cid:105) (cid:104) (unif) (T ) (cid:104) (par,S) (unif) N (par,S) (cid:104) (i = i), (cid:105) (T ) (cid:105) (T ) (cid:105) (T ) (cid:104) (10) , with strict inequalities when the prior is strictly beneficial (γ > 0 and > 0). Together, these two mechanisms ensure that the separation condition equation 7 is met more efficiently. The tighter intervals (i) make the condition easier to satisfy for any given sample allocation, and the improved allocation strategy (ii) finds sufficient allocation faster. As result, for sufficiently larger budget , the total expected number of pulls on suboptimal arms is reduced: (cid:88) (par,S) i=i (T ) i=i (cid:88) (unif) (T ) , (11) This is equivalent to stating that the expected cost of identifying the best arm is non-increasing, and strictly decreases whenever the average KL-closeness assumption holds."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS AND ANALYSIS",
            "content": "C.1 COMPARISON OF COMPUTATIONAL COSTS Table 5: Comparison of the number of model requests (or calls) for MPO and other baselines. Model Requests (Calls) Methods Base Model Optimizer Model Modality-Specific Generator Avg. Performance APE ProTeGi SEE MPO (Ours) 11.7k 11.7k 11.7k 11.7k 117 234 153 234 N/A N/A N/A 117 51.3 60.0 59.1 65.1 We analyze the number of model requests (or model calls) as proxy for computational cost, and report the results in Table 5. First, the base model call is the same for all methods, as we fix the number of explored prompts and the evaluation budget. For the optimizer model calls, APE uses the one-step exploration (e.g., paraphrasing), requiring the number of calls to be equal to the generated candidates. ProTeGi and our MPO utilize two-step process (e.g., feedback generation and refinement), requiring twice the number of calls. SEE combines both approaches and falls in between. Note that, although MPO incurs an additional computational cost by calling modality-specific generator to explore non-textual prompts, this cost is manageable, as this process can utilize lightweight, open-source generators such as SANA1.5 (1.6B) to minimize the additional expense, while still outperforming text-only prompt optimization methods as validated in Table 2 (Bottom Right). In other words, despite the marginal increase in computation (which is also manageable), MPO achieves substantial performance improvement unattainable by existing text-only optimization methods. C.2 FULL MAIN RESULTS We provide the full results, including performance on individual subtasks. The results for the image modality are presented in Table 6, and for the molecule modality in Table 7. Table 6: Full experimental results on image modality benchmarks, including subtasks, with all scores reported as the average accuracy over three independent experiments. PlantVillage CUB SLAKE DrivingVQA RSVQA Apple Corn Grape Potato Avg hummingbird albatross bunting jay cuckoo cormorant swallow blackbird auklet grosbeak oriole grebe Avg CT MRI X-Ray Avg DrivingVQA RSVQA Human CoT 1-shot 3-shot 5-shot APE OPRO EvoPrompt PE2 ProTeGi SEE MPO 47.4 57. 48.6 72.2 69.8 70.7 68.2 70.9 74.0 75.4 76.4 77.7 40.5 35.6 35.4 37.5 38.8 66.0 63.1 65.7 74.8 71.0 75. 78.2 29.1 34.9 27.1 27.5 23.2 33.8 31.2 32.8 43.7 38.4 48.0 65.9 51.7 44. 47.8 55.4 54.3 52.9 53.9 55.1 79.2 72.6 75.7 84.0 42.2 43.1 39.7 48.2 46.5 55.8 54.1 56.1 67.9 64.4 69. 76.4 50.4 49.6 56.3 62.6 67.0 80.4 57.4 61.3 78.5 83.3 78.9 82.2 42.8 39. 49.6 36.8 46.2 56.4 47.7 41.5 60.6 51.9 56.8 61.4 74.0 80.6 68.2 74.0 78.3 89.2 87.6 90.5 94.6 91.5 93. 94.6 90.4 81.6 87.0 92.0 95.1 96.9 90.2 87.9 94.6 97.7 95.0 97.3 12.2 30. 48.4 55.7 56.9 46.1 34.6 41.3 46.8 60.6 61.0 68.3 35.0 39.1 44.0 50.2 48.5 56.0 53.9 45.3 54.3 53.5 60. 58.4 50.0 49.4 33.6 51.4 53.4 54.4 56.7 50.6 67.2 62.5 64.7 71.1 32.5 44. 69.3 53.1 36.8 80.0 75.1 62.6 89.6 81.4 86.7 85.5 31.8 34.3 32.3 35.9 45.5 41.9 34.3 34.3 45.5 42.4 47. 73.7 68.3 56.7 72.5 80.6 71.7 87.5 77.8 88.3 98.9 98.6 98.1 98.6 40.1 32. 46.8 45.9 46.8 45.7 45.1 44.3 53.2 48.5 48.2 68.4 46.9 50.3 47.8 66.9 51.4 73.1 55.6 67.2 75.8 68.6 69. 84.2 47.9 49.0 54.7 58.8 58.1 67.3 59.7 59.6 71.6 70.0 71.6 78.6 35.7 31. 32.6 29.7 26.3 35.9 35.2 35.2 36.8 36.2 36.3 36.1 30.0 27.9 22.8 21.5 16.2 28.9 28.0 29.9 31.9 33.2 31. 37.5 39.9 33.1 38.9 40.6 41.3 38.1 38.3 39.3 38.9 36.9 36.9 41.0 35.2 30. 31.4 30.6 28.0 34.3 33.9 34.8 35.8 35.4 35.0 38.2 49.7 52.9 54.5 53.9 45.9 52.8 52.7 52.9 53.7 54.4 52. 56.0 51.0 49.6 48.5 52.2 49.2 54.4 51.0 50.5 55.2 54.2 53.4 55.9 Table 7: Full experimental results on molecule modality benchmarks, including subtasks, with all scores reported as the average accuracy over three independent experiments. PAMPA HIA Pgp Bioavail. Avg Absorption BBBP BBBP CYP 2C19 CYP 2D6 CYP 3A4 CYP 1A CYP 2C9 Avg CYP Inhibition Human CoT 1-shot 3-shot 5-shot APE OPRO EvoPrompt PE2 ProTeGi SEE MPO Acc 18.2 30.2 16.0 23.3 23.3 17.7 18.0 36.4 52.7 74.8 68.8 78. F1 16.8 31.1 14.0 22.7 23.1 16.2 16.6 35.8 45.9 54.1 52.5 56.1 Acc 41.1 40.2 40.2 56.9 66.7 73.6 40.2 55.8 82.5 84.5 85.1 89.1 F1 40.7 40. 39.7 52.7 59.2 55.2 39.9 51.8 65.8 64.4 69.7 76.3 Acc 55.6 51.3 58.6 58.2 58. 52.4 56.3 52.4 63.1 59.9 65.4 71.0 F1 49.4 36.1 55.4 56.0 56.3 51.7 50.0 50.6 62.0 59.5 65. 70.6 Acc 39.1 36.7 36.2 46.1 43.8 39.1 35.9 48.4 59.6 65.1 66.4 68. F1 38.2 38.9 33.6 45.6 43.5 38.6 34.9 47.6 53.3 54.8 52.6 55.1 Acc 38.5 39.6 37.8 46.1 48.1 45.7 37.6 48.2 64.5 71.1 71.4 76.7 F1 36.3 36. 35.7 44.2 45.5 40.4 35.4 46.5 56.8 58.2 60.0 64.5 Acc 39.4 33.6 36.1 42.7 49. 36.0 39.2 38.7 61.3 72.1 67.0 75.3 F1 38.6 32.5 34.8 42.6 49.3 34.7 38.3 37.7 58.2 65.7 62. 67.6 Acc 52.6 48.7 56.8 53.1 55.7 54.3 52.6 52.8 57.6 58.8 56.4 60. F1 46.2 39.1 50.9 48.1 53.1 53.6 46.3 52.1 57.4 58.3 56.4 59.2 Acc 25.8 22.8 61.4 48.5 44.2 49.4 25.7 46.3 57.9 57.6 70.7 67.6 F1 25.1 21. 37.6 40.0 38.3 44.7 25.0 42.8 46.1 48.6 51.1 51.9 Acc 43.6 42.8 54.9 47.1 48. 49.3 43.5 48.8 58.0 60.3 57.7 64.2 F1 33.5 32.7 52.5 40.3 42.8 49.1 33.4 46.9 56.7 57.2 57. 63.5 Acc 52.9 50.0 56.3 60.4 55.3 56.2 52.9 57.7 60.6 61.7 62.1 64. F1 43.7 38.7 50.4 58.6 48.3 56.2 43.7 57.2 60.3 61.5 61.7 63.6 Acc 40.5 36.2 51.8 50.7 56.4 52.3 40.5 49.8 58.6 60.4 59.8 65.4 F1 37.1 29. 50.4 49.2 52.7 50.9 37.1 49.4 55.1 59.3 56.8 62.5 Acc 43.1 40.1 56.2 51.9 52. 52.3 43.0 51.1 58.5 59.8 61.4 64.3 F1 37.1 32.3 48.3 47.3 47.0 50.9 37.1 49.7 55.1 57.0 56. 60.2 20 C.3 QUALITATIVE RESULTS In this section, we provide qualitative results for MPO. Figure 14 illustrates the optimization process in the molecular domain, and Table 8 shows examples of textual conditions for the modality-specific generator and the resulting image prompts. The optimized multimodal prompts from MPO are presented for the image  (Table 9)  , video  (Table 10)  , and molecular domains (Table 11 and Table 12). Figure 14: The optimization process for the best multimodal prompt on the BBBP task. Inherited substructures from the parent molecule are marked with the same colored circles. USE OF LARGE LANGUAGE MODELS (LLMS) We use large language models merely as writing assistant. Its role is confined to improving grammar and paraphrasing sentences for clarity, and all the core ideas regarding problem definition, MPO framework, experimental design, and interpretation of results are entirely our own. 21 cgen: Textual Condition for Generation Operator Create detailed reference image showcasing various species of grosbeaks, ensuring each bird is clearly labeled with its common name and distinctive features. Highlight key visual characteristics such as: - Beak shape (e.g., wide, pointed) - Color patterns (e.g., primary colors, unique markings) - Body size (e.g., small, medium, large) Use arrows or circles to emphasize these features, especially where they differ from other species. The composition should be organized in grid format for easy comparison, with neutral background to avoid distractions from the birds themselves. cedit: Textual Condition for Edit Operator Enhance the reference image of grosbeaks by implementing the following modifications to improve clarity and usability for classification tasks: 1. Increase Resolution and Clarity: Use higher-resolution image to ensure that all details of the birds are crisp and easily identifiable. 2. Labeling: Clearly label each grosbeak species with bold, legible text. Ensure that the labels are positioned close to the respective birds and distinguishable from the background. 3. Consistent Postures: Arrange the birds in similar postures and angles to facilitate direct visual comparisons. Consider uniform side view to best showcase the beak shapes and body sizes. 4. Highlight Distinctive Features: Add visual cues such as arrows or circles that point to unique characteristics (e.g., beak shape, wing colors, and markings). Include brief descriptions of these features near the labels. 5. Simplify Background: Remove any distracting elements from the background, opting for neutral color that allows the birds to stand out more prominently. 6. Maintain Proportions: Ensure that the proportions of the birds remain accurate and consistent with their actual sizes to aid in the visual comparison. These changes aim to create more effective reference image that enhances the classification of grosbeaks by making distinguishing features more accessible. cmix: Textual Condition for Mix Operator Create new reference image that combines the best visual elements from both provided grosbeak images. Retain the clear anatomical labeling from Reference Image B, ensuring each grosbeak species is distinctly identified. Highlight key features such as color patterns and beak shapes using arrows and concise annotations. From Reference Image A, incorporate the variety of grosbeak species but arrange them in less cluttered format, allowing for clearer comparison of unique characteristics. Focus on using high-resolution images that showcase the birds in similar poses and angles. Ensure that the primary color, distinctive markings, and beak shapes are clearly visible and easily comparable to aid in accurate classification. Discard any elements that create visual confusion or do not add value to the identification process. Table 8: Operation examples for the image prompt update, including parent image prompts, resulting child image prompts, and the textual condition to the modality-specific generator, i.e., GPT-Image. 22 PlantVillage: Plant Leaf Images Analyze the provided grape leaf image and classify it into one of the following categories: [Healthy, Leaf Blight, Black Rot, Esca]. Use the hybrid reference image for guidance, focusing on the following critical visual features: 1. Healthy: Look for vibrant, uniform green color and smooth texture without blemishes. 2. Leaf Blight: Identify distinct yellowing edges along with well-defined small dark spots that are clearly visible. 3. Black Rot: Check for sharply defined, dark, sunken lesions that are prominent on the leaf surface, often accompanied by slight shriveling. 4. Esca: Look for distinct irregular brown patches, significant necrosis, and curling of the leaf edges. In cases where symptoms overlap, prioritize the most severe characteristics. For example, if both dark spots and sunken lesions are present, classify based on the prominence of the lesions. Ensure that you assess each feature carefully, referencing the hybrid image to visualize these distinctions accurately. CUB: Bird Images Classify the bird in the target image by comparing it with the hybrid reference image of grosbeaks. Follow these refined steps for accurate classification. 1. Identify the Grosbeak Group: Refer to the hybrid image that displays the Rose Breasted Grosbeak, Pine Grosbeak, Blue Grosbeak, and Evening Grosbeak. Familiarize yourself with the specific traits of each species, including color patterns and markings. 2. Analyze Visual Features: Focus on these critical features of the target bird: Dominant Color and Markings: Note the primary color and any distinctive patterns, such as throat colors or wing designs. Beak Characteristics: Compare the shape and size of the beak with those in the reference image, as these can vary significantly among species. Body Size Comparison: Assess the body size of the target bird relative to the reference birds, ensuring accurate size comparisons. 3. Feature Prioritization: Prioritize color patterns first, as they are often the most telling feature. If colors are similar, evaluate beak shape and size next. Finally, consider body size. If the target bird does not closely match any reference species, provide the name of the closest match or indicate unknown, based on the following criteria: Closeness is determined by the degree of similarity across all analyzed features, with color being the primary factor, followed by beak shape and size. After analyzing these features, provide the name of the bird species that most closely matches the visual characteristics observed in the target image, supported by specific observations from the hybrid reference image. SLAKE: Radiological Images Given the MRI scan image, identify and list all visible abnormalities present in the brain. Your response should include specific conditions, such as edema and tumors, along with their locations (e.g., Right Lobe for edema). Refer to the labeled markers in the image to assist with your analysis. Aim to provide comprehensive answer that covers all relevant conditions without omitting any visible feature. Ensure your response is clear and concise. DrivingVQA: Driving Images Examine the provided image of road scenario and determine the most appropriate action based on specific visual cues. Pay close attention to the following details: 1. Lane Markings: Identify the lane markings; solid lines indicate no-overtaking zone, while dashed lines indicate safe area for overtaking. Clearly explain how these markings influence your decision. 2. Position of Vehicles: Assess the positions and distances of the vehicles. Determine if there is enough space and time to safely execute an overtaking maneuver based on their speeds and proximity. 3. Traffic Signs: Observe all visible traffic signs, particularly their meanings. For example, triangular sign may indicate hazard ahead, while circular sign specifies speed limits. Explain how each sign influences your decision. Based on your observations, decide whether you would (A) continue the overtaking maneuver or (B) move to the right. Justify your choice with specific details from the image, ensuring clarity in your reasoning. Conclude your response with The answer is [answer]. Use the image as reference to support your analysis of lane markings, vehicle positions, and relevant traffic signs in this driving scenario. RSVQA: Remote Sensing Images Analyze the provided neighborhood map and respond to the following questions with accurate counts and concise answers: 1. Count the total number of small roads (less than 5 feet wide), medium roads (5-10 feet wide), and large roads (greater than 10 feet wide). Indicate which category has the highest count. Use the legend provided to classify each road accurately. 2. Identify if there is commercial building (a structure used for business purposes, such as shops or offices) located to the left of any farmland area. Ensure you consider the top-down perspective of the map when determining placement. 3. For any presence or absence questions, provide direct Yes or No response. Refer to the mapss colors and labels, ensuring you utilize the legend for accurate identification of each category. Pay special attention to spatial relationships as defined in the map to avoid misinterpretations. Table 9: Qualitative examples of the optimized multimodal (image and text) prompts. 23 Drive&Act: Drivers Action Videos Classify the primary action being performed in the video, focusing specifically on interactions with objects or devices. If multiple actions are present, prioritize the action that is most visually prominent or contextually relevant. For example, if person is both eating and using phone, classify the action of using the phone. Use the definitions provided for each action to guide your decision. Consider visual cues such as hand movements, object handling, and the overall context of the scene to help determine the primary action. If two actions appear equally relevant, choose the one that is visually dominant or crucial to understanding the situation. VANEBench: Abnormal Videos Analyze the provided video to identify and describe specific actions or behaviors depicted. Focus particularly on actions that diverge from common social norms or expectations. For instance, typical actions might include greeting someone or making eye contact, while atypical actions could involve unexpected emotional reactions, erratic movements, or interactions that seem out of place. Consider the following examples: - A) person suddenly laughing in serious situation. - B) Someone avoiding eye contact in social setting. Please select the most striking anomaly from the provided options and present your answer in the format: The answer is [answer]. Table 10: Qualitative examples of the optimized multimodal (video and text) prompts. Absorption: Drug Absorption to Human You are drug discovery assistant tasked with predicting the human intestinal absorption (HIA) of newly designed hybrid molecule. Your analysis should focus on the following physicochemical properties, taking into account both the strengths and limitations of previous reference molecules: 1. Molecular Weight (MW): Calculate the molecular weight of the hybrid molecule. molecular weight below 500 Da is generally favorable for absorption. 2. Lipophilicity (LogP): Estimate the LogP value of the hybrid molecule. Aim for value between -2 and 5, ensuring that it balances contributions from both polar and non-polar functional groups. 3. Polarity and Solubility: Analyze the overall polarity of the molecule. While polar functional groups can enhance solubility, ensure that their presence does not excessively hinder absorption through lipid-rich environments. 4. Functional Groups: Identify and describe key functional groups present in the hybrid molecule. Focus on ionizable groups that can enhance solubility while ensuring that non-polar groups are balanced to facilitate membrane permeability. Discuss how these groups interact to affect absorption. 5. Stereochemistry: Note any chiral centers present in the molecule. Different enantiomers may exhibit varying absorption profiles, so explain how stereochemistry could influence absorption. At the end of your analysis, provide conclusion formatted as either Final answer: Absorbed or Final answer: Not absorbed. Ensure that your evaluation is comprehensive and considers the combined properties derived from the reference molecules to accurately predict the absorption potential of the hybrid molecule. Utilization of the Reference Molecule: This hybrid molecule is designed to improve predictions for human intestinal absorption (HIA) by integrating key features that enhance solubility and membrane permeability. The presence of two carboxylic acid groups increases the likelihood of ionization, which can improve solubility in the gastrointestinal tract, while the tertiary amine enhances interaction with transporters, facilitating absorption into the bloodstream. The molecular weight is kept below 500 Da, aligning with favorable absorption criteria, and the LogP is balanced to ensure optimal lipophilicity. This design allows for comprehensive analysis of the hybrid molecules physicochemical properties, which can be used to inform predictive models for HIA. By leveraging the strengths of both reference molecules, the hybrid is expected to yield more accurate predictions regarding absorption potential, thereby aiding in drug discovery efforts. The combination of polar and non-polar functional groups ensures that the molecule can effectively navigate the lipid-rich environments of the intestinal membrane while maintaining sufficient solubility for absorption. Table 11: Qualitative examples of the optimized multimodal (molecule and text) prompts. 24 BBBP: Penetration to Blood-Brain Barrier You are drug discovery assistant responsible for predicting the blood-brain barrier (BBB) penetration capability of new hybrid molecule based on its physicochemical properties. Follow these detailed instructions to conduct your analysis effectively: ### Key Considerations for BBB Penetration: 1. Lipophilicity (LogP): Estimate the lipophilicity of the hybrid molecule. Aim for LogP between 1 and 5, which is optimal for BBB crossing. Use computational tools like ALOGPS or ChemDraw to report the estimated LogP value. 2. Molecular Weight: Check the molecular weight of the hybrid molecule. It should be below 450 Da. Provide the exact molecular weight in your analysis. 3. Hydrogen Bonding: Assess the number of hydrogen bond donors (HBDs) and acceptors (HBAs). Aim for 1-2 HBDs and 3-5 HBAs to enhance the likelihood of BBB penetration. If the counts exceed these ranges, note how this may affect permeability. 4. Ionization State: Evaluate if the hybrid molecule is neutral or charged at physiological pH (7.4). Clearly state the estimated ionization state and include pKa values for relevant groups. 5. Presence of Polar Groups: Identify polar functional groups and assess their overall impact on BBB permeability. Ensure balanced presence to avoid excessive hydrophilicity. ### Analysis Instructions: - Analyze the provided hybrid molecular structure and evaluate how these properties collectively influence its ability to cross the BBB. Provide specific quantitative measures where applicable. - Compare the hybrid molecules characteristics with those of well-characterized reference molecule known to cross the BBB, noting key differences in properties that may influence permeability. - Summarize your findings clearly, stating the implications of the physicochemical properties on BBB crossing capability. ### Final Answer Format: Conclude your analysis with clear statement formatted as either: Final answer: Can cross BBB or Final answer: Cannot cross BBB. Ensure that your analysis is thorough and based on the specific physicochemical properties outlined above to enhance the accuracy of your predictions. Utilization of the Reference Molecule: This modified molecule enhances predictions for blood-brain barrier (BBB) penetration by addressing several key physicochemical properties. 1. Lipophilicity (LogP): The addition of methyl group (C) at the terminal position increases the hydrophobic character of the molecule, which can improve its LogP value, making it more favorable for BBB crossing. This change is aimed at achieving LogP within the optimal range of 1-5. 2. Molecular Weight: The modified molecule maintains molecular weight below 450 Da, ensuring compliance with critical criterion for BBB penetration. This is essential as larger molecules often struggle to cross the barrier. 3. Hydrogen Bonding: The modification retains balanced number of hydrogen bond donors (HBDs) and acceptors (HBAs). By keeping the HBDs to minimum (1-2) and ensuring HBAs are within the optimal range (3-5), the likelihood of effective BBB penetration is increased. 4. Ionization State: The structural modifications aim to maintain the molecule in neutral or partially ionized state at physiological pH, which is critical for enhancing lipophilicity and reducing the likelihood of charge-related hindrances to BBB penetration. 5. Presence of Polar Groups: The molecule has been adjusted to balance polar functional groups, reducing excessive hydrophilicity while retaining necessary polar characteristics for biological activity. By focusing on these modifications, the molecule is better positioned for predictive modeling of BBB penetration capabilities, as it aligns with established physicochemical parameters known to influence permeability. This can inform computational predictions and improve the accuracy of models assessing BBB crossing potential. CYP Inhibition: Inhibitory Effect to Cytochrome P450 (CYP) Enzymes You are drug discovery assistant tasked with predicting the CYP2C19 inhibition potential of target molecule. Your analysis should focus on identifying and evaluating specific structural features that correlate with CYP2C19 inhibition. Address the following key characteristics: 1. Aromatic Rings: Identify the number and type of aromatic rings present in the target molecule. Multiple aromatic rings are important for π-π stacking interactions with the CYP2C19 enzyme, enhancing inhibition potential. 2. Functional Groups: Assess for the presence of functional groups known to enhance binding, such as: - Sulfonamide groups (S(=O)2N) and amide groups (C(=O)N), which facilitate hydrogen bonding and electrostatic interactions. - Highlight any other functional groups that may support or hinder binding. 3. Basic Nitrogen Atoms or Heterocycles: Determine if the molecule includes basic nitrogen atoms or heterocycles that could enhance binding affinity through electrostatic interactions. 4. Comparison with Known Inhibitors: Compare the structural features of the target molecule with those of known CYP2C19 inhibitors like Omeprazole or Voriconazole. Pay close attention to similarities and differences in aromaticity, functional groups, and other relevant characteristics. Conclude your analysis with clear statement regarding the inhibition status of the target molecule, formatted as follows: Final answer: Inhibits CYP2C19 or Final answer: Does not inhibit CYP2C19. Ensure your comparisons and conclusions are supported by your structural analysis. Utilization of the Reference Molecule: This generated molecule, which contains multiple aromatic rings, sulfonamide group, and an amide group, can significantly improve predictions for CYP2C19 inhibition potential. The presence of two aromatic rings enhances π-π stacking interactions with the CYP2C19 enzyme, which is crucial for binding affinity. The sulfonamide group (S(=O)2N) is known to facilitate hydrogen bonding, while the amide group (C(=O)N) can participate in additional hydrogen bonding interactions, both of which are important for stabilizing the enzyme-inhibitor complex. Furthermore, the molecule incorporates basic nitrogen atoms in the amide and sulfonamide groups, which can engage in electrostatic interactions with the enzyme, further enhancing binding affinity. This structural design aligns with the characteristics observed in known CYP2C19 inhibitors like Omeprazole and Voriconazole, which also feature multiple aromatic systems and functional groups that promote hydrogen bonding. By comparing the generated molecules structural features with those of established inhibitors, we can derive insights into the molecular descriptors that correlate with inhibition potential. This molecule serves as reference point for evaluating new compounds in terms of their predicted CYP2C19 inhibition, helping to refine predictive models and improve the accuracy of virtual screening processes. The combination of aromaticity, functional groups, and basic nitrogen atoms in this molecule provides robust framework for understanding and predicting CYP2C19 inhibition. Table 12: Qualitative examples of the optimized multimodal (molecule and text) prompts."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST"
    ]
}