{
    "paper_title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding",
    "authors": [
        "Minsoo Kim",
        "Kyuhong Shim",
        "Jungwook Choi",
        "Simyung Chang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . e [ 1 5 4 7 5 1 . 6 0 5 2 : r InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding Minsoo Kim1 Kyuhong Shim2 Jungwook Choi1 Simyung Chang3 1Hanyang University 2Sungkyunkwan University 3Qualcomm AI Research, Qualcomm Korea YH {minsoo2333, choij}@hanyang.ac.kr khshim@skku.edu simychan@qti.qualcomm.com"
        },
        {
            "title": "Abstract",
            "content": "Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their keyvalue (KV) cache grows linearly with timequickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces hard, lengthindependent memory cap for streaming video understanding. During video encoding it monitors the cache and, once user-set threshold is reached, runs lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracyeven in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in multimodal large language models (MLLMs) have dramatically expanded the scope of visual reasoning. Visionlanguage instruction tuning now allows single backbone to answer open-ended questions over long video sequences [25, 28, 46], while context-extension techniques such as FlashAttention-2 and RingAttention push the effective window into the million-token regime [7, 24, 31]. These breakthroughs underpin new generation of streaming video assistants and humanoid robots that promise continuous, real-time scene understanding on mobile phones, AR glasses and edge robots [14, 29, 40, 35]. Streaming video understanding (SVU) diverges from conventional offline video understanding (OVU). Offline models see the entire clip and user query before inference, so they can tailor every compression or retrieval step. In streaming, frames arrive incrementally and future queries are unknown, forcing all pre-query processing to be query-agnostic. In addition, device memory is fixed, yet the transformer emits hundreds of tokens per frame, so the keyvalue (KV) cache grows linearly. For example, 15-min, 10 fps clip processed by LLaVA-Next-Video-7B already needs demands 100 GB of KV storage, far beyond the tens of gigabytes available on mobile or robotic platforms [50, 19]. Work done during an internship at Qualcomm Technologies, Inc. Corresponding authors. Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc. Preprint. Under review. Prior work tackles long-video memory constraints at three stages  (Fig.1)  . Frame Sampling [10] drops frames before encoding, reducing memory but severely degrading temporal coverage and accuracy. Input-Vision Compression (IVC) [34, 38] prunes redundant vision tokens after encoding, lowering Prefill load but still requiring the full vision token set to be stored in memory. KV cache Compression (KVC) [23, 12] selects query-relevant tokens after the Prefill step, offering the highest accuracy but only after materializing the full KV cache. The challenge intensifies in streaming scenarios: memory usage for Frame Sampling, IVC, and KVC grows almost linearly with video length, eventually exceeding device limits. KV cache offloading (e.g., ReKV [33]) expands memory space yet incurs costly data transfer, repeated for each query. Thus, no existing approach delivers the key property SVU needs: length-independent and query-agnostic streaming video compression. natural approach to address memory constraints in streaming video is to exploit the strong spatiotemporal redundancy of video streams. We introduce InfiniPot-V, the first framework specifically designed for memory-constrained SVU. InfiniPot-V is training-free, query-agnostic, and operates continuously during inference. When the KV cache reaches user-defined memory threshold , it performs an in-place compression that frees space for new frames while preserving the semantic essence of prior context. This compression is guided by two lightweight and complementary metrics. Temporal-axis Redundancy (TaR) models Key embeddings as 3D tensor over time and removes tokens with high cosine similarity to recent frames, effectively pruning static or repetitive content. Value-Norm Importance (VaN) ranks the remaining tokens by the ℓ2 norm of their Value vectorsa strong, model-agnostic proxy for semantic salienceand applies layer-adaptive pooling strategy. This compression is highly efficient, adding negligible latency while strictly enforcing memory limits. Extensive evaluation confirms the effectiveness of this design. Across four open-source visionlanguage models (3B and 7B) and six long-video benchmarkscovering both offline (VideoMME, EgoSchema, MLVU, LongVideoBench) and streaming (RVS-Ego, RVS-Movie) tasksInfiniPot-V reduces input context length usage to as low as 6K for 50K-token contexts, with accuracy matching or exceeding full-cache baselines. It maintains real-time performance at 14 frames per second with only 0.5% compression overhead. Additionally, its query-agnostic nature offers clear benefits in multi-turn dialogue settings (Appendix. C). By eliminating the KV cache bottleneck without retraining or query dependency, InfiniPot-V paves the way for practical, on-device multimodal assistants."
        },
        {
            "title": "2 Background",
            "content": "We aim to deploy streaming video understanding (SVU) applications [47, 33] in memory-constrained environments. Unlike offline video understanding (OVU) [52, 11, 43], which assumes access to the entire video, SVU must process arbitrarily long video streams and answer questions at any time step using only the frames observed up to that point. Given video stream VT := [v1, v2, . . . , vT ] with frames and set of questions = q1, q2, . . . , qN , SVU answers each question qi at time (1 ) using only the observed frames Vt := [v1, v2, . . . , vt]. As SVU deals with unbounded video streams, memory-efficient processing is essential. In this section, we describe how multimodal large language models (MLLMs) handle long videos, review prior approaches to memory reduction in OVU, and analyze their limitations when applied to SVU. (See Appendix. for detailed discussion of related work.)"
        },
        {
            "title": "2.1 Preliminary: Offline Long Video Understanding",
            "content": "Video Processing in MLLMs. Multimodal Large Language Models (MLLMs) [50, 46, 41] process offline videos through structured pipeline (Fig. 1(a)). Given video VT := [v1, v2, . . . , vT ] of uniformly sampled frames, vision encoder fViT transforms each frame into visual tokens: = fViT(VT ) = [x1, x2, . . . , xN ] RN D, (1) where = denotes the total number of sampled tokens, where is the number of tokens per frame (determined by input resolution and ViT patch size), and is the token embedding dimension. The token sequence is then passed to the LLM in two phases: Prefill and Decoding. During the Prefill phase (Fig. 1(a), step 2), all tokens are processed at once to construct the initial key-value (KV) cache. The attention operation computes: = XWq, = XWk, = XWv, Oattn = Softmax (cid:18) QK (cid:19) + V, (2) 2 Figure 1: MLLMs Video Understanding and Compression. (a) OVU pipeline; (b) IVC: compresses vision tokens after encoding; (c) KVC: compresses KV cache after prefill; (d) CKV: iteratively processes and compresses KV caches to constrain memory usage; (e) Accuracy vs. GPU memory consumption for compression across four token reduction ratios (50%, 25%, 12.5%, 6.25%) on MLVU using Qwen-2-VL-7B. LongVU[34] is used for IVC, SnapKV[23] for KVC; ; (f) GPU memory usage as input video stream length increases. IVC/KVC/CKV target 6K cache; Sampling uses 1/4 of input frames. Measured on A100 (80GB) where Wq, Wk, Wv RDD are projection matrices and is causal mask enforcing autoregressive decoding. In the Decoding phase (Fig. 1(a), step 3), the model generates tokens one at time using cached keys and values from the prefill phase. To avoid redundant computation, the KV cache = (K, ) is updated incrementally: Ct+1 = ({K, kt+1}, {V, vt+1}) , (3) where kt+1, vt+1 correspond to the KV embeddings of the newly processed token."
        },
        {
            "title": "2.2 Offline Long-Video Compression Strategies",
            "content": "Long videos produce extremely long token sequences X, leading to prohibitive GPU memory and latency during decoding. Prior works tackle this bottleneck in the offline setting through three classes of methods (Fig. 1ac): (1) Frame Sampling [10]. Uniformly sampling shorter clip hence, memory usage is also reduced proportional to compression rate. VT reduces the input length and, (2) Input-Vision Compression (IVC) [34, 38]. After vision encoding, IVC aggressively prunes redundant vision tokens, keeping only salient subset (Fig. 1b) to shrink the context fed into the language decoder for memory-compressed Prefill. (3) KV cache Compression (KVC) [23, 12, 4]. Conduct compression after prefill: KVC computes importance scores ut = (cid:80)N i=N wAttn(xi xt) over the last tokens and retain top-M entries for the memory budget by applying eviction policy π, yielding compressed cache = π(C) for memory-compressed Decoding. (Fig. 1c). Note that the π eviction policy is highly dependent on the content of the last tokens, reflecting the user query, and is thus referred to as query-dependent cache compression method (see Appendix. for further analysis). These techniques are effective when the entire video is available upfront, but they implicitly assume (i) unconstrained memory for compression and (ii) known or easily approximated query."
        },
        {
            "title": "2.3 Challenges in Streaming Video Understanding",
            "content": "Fig. 1(e) compares three offline compression methods on fixed 50 K-token video at four compression ratios (darker shades indicating higher ratios: 50%, 25%, 12.5%, 6.25%), revealing fundamental trade-off between memory usage and accuracy. Frame sampling skips frames to save memory, but severely degrades recognition accuracy. Increasing the sample ratio improves accuracy but quickly inflates memory usage. IVC starts with large memory footprint for all vision tokens before selecting which to discard. KVC, which operates on more expressive keyvalue features, achieves the highest accuracy but requires the largest Prefill cache. Notably, even under favorable offline settingwith full video access and an offline querynone of the methods achieve both high accuracy and low memory usage. This trade-off becomes more severe in the streaming video understanding (SVU) setting. As shown in Fig.1(f), peak GPU memory usage increases with stream length. KVC exhibits near-linear memory growth, as it must materialize all vision tokens and build the full KV cache before compression. Furthermore, due to its query-dependent nature, KVC must re-execute the memory-intensive prefill stage whenever the user query changes. Frame sampling and IVC also grow linearly, albeit more slowly, eventually exceeding the memory capacity of practical edge devices (e.g., 32GB[19]) as the stream continues. ReKV [33], recent KVC method, addresses this by offloading the KV cache to CPU memory, but this introduces substantial offloading overhead and compression latency. These findings highlight two core requirements for SVU: (1) fixed memory budget that does not grow with stream length, and (2) query-agnostic token retention strategies. Existing methods fail to meet at least one of these, limiting their suitability for SVU. To overcome this, we propose Continual KV cache Compression (CKV), illustrated in Fig. 1(d). CKV processes frames in small blocks and compresses the cache whenever the fixed memory limit is reached, ensuring constant memory usage throughout streaming. Additionally, for query-agnostic token retention, our approach employs lightweight spatiotemporal metrics to identify and preserve semantically significant tokens without relying on future queries. As result, despite operating under strict memory constraints, CKV achieves accuracy on par with or better than KVC (Fig.1(e)), while consuming far less memory than IVC or frame sampling (Fig. 1(f)). The algorithmic details are described in Sec.3. Algorithm 1 Continual KV cache Compression (CKV) with InfiniPot-V Require: Memory budget , target cache size C, TaR ratio α Initialize K, while video stream continues do Empty KV cache (1) Process: Knew, Vnew Process new frame; [K; Knew], [V ; Vnew] Append new tokens if len(K) then Memory budget exceeded len(K) = len(V ) = (2) Extract: Krecent, Vrecent recent frames from K, (3) TaR: sTaR ComputeTaRScores(K); ITaR TopK(sTaR, αC len(Krecent)) Sec. 3.1 (4) VaN: sVaN ComputeAdaptiveVaNScores(V ); IVaN TopK(sVaN, (1 α)C) Sec. 3.2 (5) Combine: ITaR IVaN Indices(Krecent); K[I], [I] Compress to size end if if user query arrives then Generate response using current K, end if end while 3 InfiniPot-V: Memory-Constrained Streaming Video Understanding We present InfiniPot-V, CKV framework designed for memory-constrained SVU. As shown in Fig. 1(d) and Algorithm 1, InfiniPot-V processes video streams by applying continual KV cache compression within fixed memory budget. In this framework, KV embeddings from incoming frames are stored until the memory limit is reached. At that point, compression reduces the cache to smaller target size (with C), retaining only the most essential vision tokens based on two criteria. The freed space (M C) accommodates new frames. This process repeats continuously, enabling efficient stream processing under strict memory constraints. When user query is issued, the model answers using the compressed cache that summarizes visual context from 4 Figure 2: Spatio-Temporal KV cache Compression (TaR and VaN). (a) Temporal redundancy across adjacent frames, showing static patches that can be evicted from past frames; (b) Layerwise cosine similarity of Key/Value embeddings for static patches between consecutive frames in LLaVA-Next-Video-7B; (c) InfiniPot-V performs query-agnostic spatiotemporal compression, reducing temporal redundancy with TaR and selecting tokens via VaN spatial scoring. all prior frames. Notably, compression adds only 0.5% overhead relative to input frames processing time. InfiniPot-V leverages two token eviction criteria: Temporal-axis Redundancy (TaR) and Value Norm (VaN) for identifying crucial tokens for compressing KV cache. In the following subsections, we detail each criterion, and finally describe how to effectively combine them."
        },
        {
            "title": "3.1 Temporal-axis Redundancy (TaR) Reduction via Patch-wise Similarity",
            "content": "Video streams exhibit inherent spatiotemporal redundancy across frames [42, 34, 38]. In this section, we focus on exploiting temporal redundancy, as illustrated in Fig. 2(a) where static patches5 (e.g., background) persist across frames. For MLLMs processing videos with fixed memory usage, identifying this redundancy in KV caches is crucial. Our analysis in Fig. 2(b) reveals that Key embeddings effectively capture temporal redundancy, exhibiting higher cosine similarity for static patches between adjacent frames compared to Value embeddings, across all layers. Building on this insight, we propose TaR, technique that performs patch-wise comparison of Key embeddings along the temporal axis to detect and reduce redundant tokens. As shown in Fig. 2(c), we introduce 3D reshaping of Key embeddings to enable direct comparison of corresponding patches across frames. Based on this structured KV cache, the TaR implementation starts with memory constraint of tokens, processing consecutive video frames, each containing = /f vision tokens. To maintain temporal continuity, we designate the latest frames as recent frames and retain them in full. The older past frames (f frames) are selectively compressed based on their patch-wise similarity to recent frames. To measure the patch-wise similarity between frames, we divide the current Key embeddings RH(f p)D into Krecent RHrpD and Kpast RH(f r)pD, representing the recent and past frames respectively. For each spatial coordinate (i, j), we compute the ℓ2-normalized cosine similarity between recent and past frames of the same patch coordinate: sT aR(t, i, j) = 1 (cid:88) t=1 (cid:16) cos (t,i,j) past , (t,i,j) recent (cid:17) . (4) Here, sT aR(t, i, j) is the importance score of the patch in t-th frame at (i, j) coordinate. The negative sign is applied so that higher computed score indicates lower redundancy (i.e., the token is more 5In MLLMs, each vision patch corresponds to single token, so we use these terms interchangeably. distinctive). This ensures that tokens with less temporal similarity to recent frames are prioritized. We then select the least redundant tokens (i.e., higher score) in past frames using the Top-K operator: ITaR = TopK(sT aR, Krecent), (5) where is the target cache compression size and Krecent = rp accounts for the recent frame tokens that are always retained. The compressed key-value pairs are formed by concatenating the selected key frame tokens with all recent frame tokens: (cid:1), KTaR = Concat(cid:0)K[:, ITaR, :], Krecent VTaR = Concat(cid:0)V [:, ITaR, :], Vrecent (6) (cid:1). By fully preserving the most recent frames, we maintain complete information on rapidly changing or newly introduced content, while selectively retaining distinctive visual elements from the past."
        },
        {
            "title": "3.2 Spatial Semantic Importance Preserving with Value Norm (VaN)",
            "content": "While TaR focuses on reducing temporal redundancy, VaN serves complementary role: identifying and preserving semantically salient regions within each video frame, independent of the query. To achieve this, we employ Value embeddings (V ), which inherently capture semantic information in transformer attention [39]. Specifically, we introduce Value Norm (VaN) as metric for token-level semantic importance: sV aN = (t,i,j)2. Analysis of Value Norm. We hypothesize that tokens with higher VaN contain richer semantic information, making them more valuable for video understanding. To quantify semantic importance, we project vision token representations from each layer into the vocabulary space [27] and compute the entropy of the resulting word probability distribution, where the higher entropy implies greater informativeness [9, 3]. As shown in Fig. 3 (a), tokens with higher VaN consistently exhibit higher entropy, confirming their semantic significance. This advantage translates to improved performance: Fig. 3 (b) shows that retaining high-VaN tokens achieves substantially higher video understanding accuracy across various compression ratios compared to low-VaN tokens. Figure 3: Value Norm (VaN) Analysis. (a) Entropy analysis of vision token representations grouped by their VaN scores. (b) VideoMME performance under varying cache compression ratios using either VaN or reverse-VaN for token selection. (c) Layer-wise locality of VaN, measured by center distance and coefficient of variation (CV); lower values indicate stronger spatial consistency. LLaVA-Next-7B with Video-MME used. Layer-wise Adaptive Pooling. An analysis of VaN distributions reveals strong spatial locality patterns in early to middle layers, which gradually diminish in deeper layers as shown in Fig. 3(c). To measure spatial locality patterns across layers, we employ two methods: (1) compute the average distance between the center point and surrounding points within 3 3 window spanning the VaN values of each frame (center-dist.), and (2) measure the Coefficient of Variance (CV) to quantify dispersion of VaN distributions. Lower values in both metricssmaller center-dist. and CVindicate that VaN scores are closely clustered, implying high spatial locality, whereas higher values reflect greater dispersion and lower locality. As shown in Fig. 3 (c), both metrics consistently indicate strong locality in early to middle layers, while gradually diminishing in deeper layers. Based on this observation, we design an adaptive spatial pooling mechanism that adjusts the average pooling kernel size per layer. To implement this, we design mapping function that assigns kernel sizes in inverse relation to each layers CV: PoolSize(CVl) = g(CVl) where : R+ 1, 3, 5, 7 This approach assigns larger pooling kernels (e.g., 7) to lower layers with smaller CV values (higher spatial locality), and smaller kernels (e.g., 1, implying no pooling) to upper layers with larger CV 6 Method Max Duration GPT4-V* GPT4-o* LLaVA-OV* LongVU* LongVU* Qwen-2-VL Qwen-2-VL + Ours LLaVA-Next LLaVA-Next + Ours Qwen-2.5-VL Qwen-2.5-VL + Ours Size # Frames Budget EgoSchema MLVU 120 min 3 min VideoMME 60 min LVB 60 min 7B 7B 3B 7B 7B 7B 7B 3B 3B 1fps 1fps 32 1fps 1fps 768 768 128 128 768 768 8K 8K 8K 50K 6K 25K 6K 50K 6K 55.6 72.2 60.1 67.6 59. 65.2 65.6 67.6 65.8 64.4 61.8 66.2 64.7 65.4 55.9 65.8 65.8 68.7 65.2 63.3 62.1 60.7 77.2 58.2 60.6 51. 63.9 62.8 62.8 61.1 60.3 59.3 66.7 58.8 58.4 63.5 60.9 59.9 56.5 Table 1: Comparison of various MLLMs accuracy on four Offline Video Understanding (OVU) benchmarks. * denotes the numbers from official paper. Compression Budget Video MME MLVU Method FullKV TTC [38] (IVC) STC [34] (IVC) InfiniPot-V (CKV) 50K 3K 6K 3K 6K 3K 6K Short Med Long Holistic Single Multi Avg. 74.68 62. 55.00 66.78 72.55 67.89 72.55 73.89 74.11 51.22 55.00 51.00 56. 57.78 60.78 47.89 51.67 49.33 51.55 51.78 53.44 76.34 72.05 76. 71.54 74.30 77.73 77.16 73.91 58.78 60.88 58.56 61.09 70.38 72. 43.29 64.22 33.22 36.66 33.90 35.94 43.15 44.75 54.84 58. 55.02 57.86 63.09 64.26 Table 2: Comparison under memory-constrained settings (3, 6K memory-budget) with Input Video Compression (IVC) methods: TTC from DyCoke [38] and STC from LongVU [34]. Qwen-2-VL-7B used across VIdeoMME and MLVU benchmarks. Results comparing memory-unconstrained IVC methods (without cache compression) with InfiniPot-V are provided in Tab. A6. values, thus preserving fine-grained details where needed. Detailed kernel size mapping is provided in Algorithm A. For KV cache compression, we select tokens using VaN scores processed through our adaptive pooling mechanism, retaining the Top-C tokens with highest pooled VaN values as described in Fig. 2(c): IVaN = TopK(VaNpool, C) KVaN = K[:, IVaN, :], VVaN = [:, IVaN, :]. (7)"
        },
        {
            "title": "3.3 Design Space Exploration",
            "content": "Combining TaR and VaN for Token Selection. TaR and VaN capture complementary aspects of spatio-temporal redundancy in streaming video. To integrate them, we prioritize TaR-based selection by first allocating αC tokens to TaR, then filling the remaining (1 α)C with VaN-selected tokens. This two-stage selection strategy effectively balances temporal and feature importance. detailed hyperparameter exploration, including sweeps over α and the size of the recent frame window r, is provided in Appendix. A.2. Comparison with Memory-Constrained Alternatives. natural question is whether IVC or KVC can be adapted for SVU under memory constraints. To explore this, we apply query-agnostic methods such as spatial token compression (STC) and token temporal merging (TTC) from LongVU [34] and DyCoke [38]. InfiniPot-V outperforms all these baselines by notable accuracy margin, demonstrating the strength of continual compression over expressive key-value embeddings (details in Tab. 2). 7 Figure 4: KV cache Compression (KVC) methods evaluation results with offline long video understanding tasks under Continual KV Cache Compression (CKV) framework. Performance across four compression ratios (1/16, 1/8, 1/4, 1/2) for LLaVA-Next-7B (top row) and Qwen-2-VL-7B (bottom row) on VideoMME, MLVUdev, and LongVideoBenchdev (LVBdev) tasks. The full evaluation results are shown in Table A5."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Benchmarks. We evaluate our InfiniPot-V on both offline video understanding (OVU) and streaming video understanding (SVU) tasks. For OVU, we utilize representative long video understanding benchmarks (ranging from 3 minutes to over 2 hours): VideoMME [11], MLVU [52], LongVideoBenchmark (LVB) [43], and Egoschema [26]. For SVU, we employ RVS-Ego/Movie streaming VideoQA benchmark [47] featuring open-ended questions paired with timestamps and evaluate the answers using the GPT-3.5-turbo-0125 following [47, 33]. Models. We apply our method on four state-of-the-art MLLMs capable of long-video understanding: Qwen-2-VL-7B [41], Qwen-2.5-VL-3B [46], LLaVA-OV-7B [22], and LLaVA-Next-Video [50]. Details on input video sampling settings and benchmark details are provided in Appendix. B."
        },
        {
            "title": "4.2 Evaluatal Results",
            "content": "Offline Video Understanding. To assess the absolute compression capability of our method, we evaluate InfiniPot-V with both commercial MLLMs (GPT-4V [28], GPT-4o [29]) and state-ofthe-art public models designed for offline video understanding, including LLaVA-OV [22], and LongVU [34]. Unlike these specialized, fully trained models, InfiniPot-V is training-free, plugin framework compatible with MLLMs of various scales, enabling high performance under fixed memory budgets. As shown in Tab. 1, InfiniPot-V reduces memory usage to just 25% (6K tokens) for LLaVA-Next (originally 25K tokens) and 12.5% for Qwen-VL series (6K vs. 50K), with minimal performance loss. Notably, it achieves comparable or better accuracy than LongVU at the 7B scale and significantly outperforms it at 3B, demonstrating both efficiency and scalability. Comparison with IVC under Memory Constraints. To evaluate recent query-agnostic IVC methods under memory-constrained CKV, we adopt unified setup on VideoMME and MLVU: token temporal merging (TTC) from DyCoke [38] and spatial token compression (STC) from LongVU [34] are applied to compress vision tokens to fit the target memory budget , while KV cache is managed using sliding window attention (SWA) [2]. When operated under such constraints, these IVC methods suffer from notable accuracy degradation. In contrast, InfiniPot-V performs KV cache compression using TaR and VaN, leveraging expressive key-value representations to achieve superior average accuracy under 6K memory budgetcorresponding to an 88% lossless compression rate. 8 RVS-Ego RVS-Movie Execution Time Total Memory Usage LLaVA-OV-7B Acc Score Acc Score Video Enc. (msec/Frame) GPU CPU ReKV ReKV w/o off. InfiniPot-V 60.1 55.8 57.9 3.9 3.3 3. 53.4 50.8 51.4 3.8 3.4 3.5 285.7 74.6 76. 37.5 GB + 18.8GB/h 27.2 GB 27.8 GB 0 0 Table 3: Streaming benchmark comparison to offloading-based KV cache control method. (ReKV) Video Enc. shows execution time per frame, GPU indicates peak memory usage, and CPU denotes the size of video KV-Cache offloaded to CPU per hour. Results based on an 1-hour video processed with 0.5 fps sampling rate in streaming mode. LLaVA-OV-7B is used. MLVUdev Holistic Reasoning Single Detail Multi Detail Ablation Study Topic Anomaly Plot Needle Full KV Uniform Select TaR Reverse TaR Frame TaR VaN Reverse VaN VaN + Pool TaR + VaN + Pool 85. 83.7 79.0 82.9 85.9 78.3 84.4 85.2 86.3 67.5 66. 64.5 66.0 66.5 66.5 68.0 68.0 68.0 72.7 67.9 56.9 67.0 71. 56.2 68.6 71.4 72.7 83.9 76.1 65.6 78.9 78.0 66.8 76.6 77. 80.3 Ego 65.1 58.5 55.1 63.6 62.2 53.4 61.9 63. 63.9 AO 54.1 51.0 45.2 51.0 51.7 46.3 52.5 52. 54.1 AC 32.5 27.2 21.8 31.1 35.4 17.5 29.1 31. 35.4 Avg 65.9 61.5 55.5 62.9 64.5 55.0 63.0 64. 65.8 Table 4: Ablation study of TaR, VaN, and their combination. Experiments conducted on MLVU using Qwen-2-VL-7B with 6K memory budget. Comparison with KVC under Memory Constraints. Fig.4 evaluates KVC methods within our CKV framework under constrained memory across offline video understanding tasks. Compression ratios (1/16, 1/8, 1/4, 1/2) are defined based on each models maximum frame capacity (e.g., 128 frames for LLaVA-Next, 768 for Qwen-2-VL). Our InfiniPot-V consistently outperforms all baseline methods (Uniform Select, SnapKV, InfiniPot) across all tasks for both LLaVA-Next-7B and Qwen-2-VL-7B, demonstrating superior video understanding performance. Under CKV constraintswhere actual query access is not availablequery-dependent methods like SnapKV[23] degrade significantly. In contrast, InfiniPot-V maintains strong accuracy even at high compression ratios (e.g., 1/16), thanks to its query-agnostic selection via TaR and VaN. Streaming Video Understanding. We evaluate InfiniPot-V on streaming video understanding (SVU) using two popular StreamingVQA benchmarks, RVS-Ego and RVS-Movie, with LLaVA-OV-7B. As baseline, we compare against ReKV[33], state-of-the-art SVU method, under two system settings: (1) CPU-GPU system with CPU offloading, which allows spilling KV cache to CPU memory, and (2) CPU-GPU system without CPU offloading, simulating shared-memory devices where CPU memory is either unavailable or pre-occupied[19]. Tab. 3 reports SVU accuracy, compression time, and memory usage. With CPU offloading, ReKV can retain the full KV cache in CPU memory but suffers from high data transfer overhead, causing substantial delays. Without offloading, ReKV is restricted to fixed local cache and exhibits sharp accuracy degradation. In contrast, InfiniPot-V operates entirely within GPU memory, eliminating offloading overhead while outperforming ReKV in accuracymaking it highly practical solution for memory-constrained or shared-memory systems."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Tab. 4 validates our design decisions for TaR and VaN. Reversed strategies (T Reverse and Reverse) significantly degrade performance by discarding distinctive or semantically important tokens. Within TaR, patch-wise similarity proves more effective than frame-level similarity (64.5 vs. 62.9). VaN alone surpasses the baseline, and its performance improves further with adaptive pooling (64.1 vs. 63.0). Combining TaR and VaN yields the highest accuracy, significantly outperforming the baseline. Additional integration explorations are discussed in Appendix. A.2."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we proposed InfiniPot-V, training-free KV cache control framework for streaming video processing in memory-constrained environments. Built around practical constraintsunavailable queries and strict memory budgets during compressionInfiniPot-V employs two novel token eviction criteria, TaR and VaN, achieving significant improvements in long video understanding under streaming scenarios."
        },
        {
            "title": "References",
            "content": "[1] Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, and Bo Ji. Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models, 2024. [2] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020. [3] David Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, and Trevor Darrell. Analyzing the language of visual tokens, 2025. [4] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models, 2024. [5] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. LongVILA: Scaling long-context visual language models for long videos. In The Thirteenth International Conference on Learning Representations, 2025. [6] Giulio Corallo, Orion Weller, Fabio Petroni, and Paolo Papotti. Beyond rag: Task-aware kv cache compression for comprehensive knowledge reasoning, 2025. [7] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [8] Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. simple and effective l_2 norm-based strategy for KV cache compression. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1847618499, November 2024. [9] Sebastian Farquhar, Jannik Kossen, Livia Kuhn, et al. Detecting hallucinations in large language models using semantic entropy. Nature, 630:625630, 2024. [10] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition, 2019. [11] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [12] Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. In The Thirteenth International Conference on Learning Representations, 2025. [13] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. In The Twelfth International Conference on Learning Representations, 2024. [14] Google DeepMind. Project ASTRA. https://deepmind.google/technologies/ project-astra/, 2024. [15] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina González, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Koláˇr, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbeláez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1899519012, June 2022. [16] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael Mahoney, Kurt Keutzer, and Amir Gholami. Squeezed attention: Accelerating long context length llm inference. arXiv preprint arXiv:2411.09688, 2024. [17] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 709727. Springer, 2020. [18] Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, and Zhiyuan Liu. Locret: Enhancing eviction in long-context llm inference with trained retaining heads on consumer-grade devices, 2025. [19] Leela S. Karumbunathan. NVIDIA Jetson AGX Orin Series Technical Brief. Technical Report TB_10749-001_v1.2, NVIDIA Corporation, July 2022. Version 1.2. [20] Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, and Hyun Oh Song. Kvzip: Query-agnostic kv cache compression with context reconstruction, 2025. [21] Minsoo Kim, Kyuhong Shim, Jungwook Choi, and Simyung Chang. InfiniPot: Infinite context processing on memory-constrained LLMs. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1604616060, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [22] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [23] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. SnapKV: LLM knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [24] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for near-infinite context. In The Twelfth International Conference on Learning Representations, 2024. [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3489234916. Curran Associates, Inc., 2023. [26] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 11 [27] Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, and Fazl Barez. Towards interpreting visual information processing in vision-language models. In The Thirteenth International Conference on Learning Representations, 2025. [28] OpenAI. Gpt-4v(ision) system card, 2023. [29] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. [30] Junyoung Park, Dalton Jones, Matthew Morse, Raghavv Goel, Mingu Lee, and Chris Lott. Keydiff: Key similarity-based kv cache eviction for long-context llm inference in resourceconstrained environments, 2025. [31] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context In The Twelfth International Conference on window extension of large language models. Learning Representations, 2024. [32] Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024, 2024. [33] Zhelun Yu Shangzhe Di. Streaming video question-answering with in-context video KV-cache retrieval. In The Thirteenth International Conference on Learning Representations, 2025. [34] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding, 2024. [35] Morgan Stanley. Humanoids: report, Morgan Stanley, Technical Accessed via Future Management Group: https://www.futuremanagementgroup.com/wp-content/uploads/240626-HumanoidRobots-Morgan-Stanley.pdf. implications of embodied ai. Investment June 2024. [36] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. [37] Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Shikuan Hong, Danning Ke, Yiwu Yao, and Gongyi Wang. Razorattention: Efficient KV cache compression through retrieval heads. In The Thirteenth International Conference on Learning Representations, 2025. [38] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models, 2024. [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [40] Ethan Waisberg, Joshua Ong, Mouayad Masalkhi, Nasif Zaman, Prithul Sarker, Andrew Lee, and Alireza Tavakkoli. Meta smart glasseslarge language models and the future for assistive glasses for individuals with vision impairments. Eye, 38(6):10361038, 2024. [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [42] T. Wiegand, G.J. Sullivan, G. Bjontegaard, and A. Luthra. Overview of the h.264/avc video coding standard. IEEE Transactions on Circuits and Systems for Video Technology, 13(7):560 576, 2003. 12 [43] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for longcontext interleaved video-language understanding. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [44] Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, and Afshin Dehghan. Slowfast-llava-1.5: family of token-efficient video large language models for long-form video understanding, 2025. [45] Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, and Doyen Sahoo. Think: Thinner key cache by query-driven pruning. In The Thirteenth International Conference on Learning Representations, 2025. [46] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [47] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams, 2024. [48] Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, Minqi Wang, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. [cls] attention is all you need for training-free visual token pruning: Make vlm inference faster, 2024. [49] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. Sparsevlm: Visual token sparsification for efficient vision-language model inference, 2024. [50] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. [51] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [52] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 13 Algorithm 2 InfiniPot-V Algorithm Require: Video stream , memory constraint , target KV cache size C, recent frame count r, CV thresholds {τ1, τ2, τ3}, TaR ratio α [0, 1], frames corresponding to tokens, vision token number per single frame = /f // Temporal-axis Redundancy (TaR) Reshape Kl into Krecent,l RHrpD and Kpast,l RH(f r)pD for each patch (t, i, j) in past frames do t=1 cos(K (t,i,j) , (t,i,j) recent,l ) l=1 (cid:80)r s(t, i, j) = 1 Accumulate KV embeddings until reaching for each layer do Ensure: Compressed KV cache { Kl, Vl}L 1: Let CT aR = αC be the TaR selection budget 2: Let CV aN = (1 α)C be the VaN selection budget 3: Initialize empty KV cache for each layer {1, . . . , L} 4: while processing video stream do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end for Il TopK(Sl, CT aR) // Value Norm (VaN) with Adaptive Pooling VaNl Vl2 // Compute CV for adaptive pooling µl mean(VaNl) σl std(VaNl) CVl σl/µl // Determine pooling size using mapping function pool_sizel g(CVl) 7, 5, 3, 1, if CV < τ1 if τ1 CV < τ2 if τ2 CV < τ3 if CV τ3 where g(CV) = 21: past,l Select least redundant tokens Using thresholds {τ1, τ2, τ3} VaNpool,l AveragePool2d(VaNl, pool_sizel) // Combine TaR and VaN by prioritizing TaR-selected tokens VaNpool,l[Il] max(VaNpool,l) Jl TopK(VaNpool,l, C) Kl Kl[:, Jl, :], Vl Vl[:, Jl, :] Prioritize TaR tokens Final token selection Update layer KV cache with compressed KV cache 22: 23: 24: 25: 26: end for 27: 28: end while InfiniPot-V Algorithm and Configuration A.1 Algorithm Description Algorithm 2 presents the complete process of InfiniPot-Vs cache control framework along with its compression formulation. InfiniPot-V processes video streams by continuously pre-filling and compressing the KV cache using two token selection strategies: Temporal-axis Redundancy (TaR) and Value Norm (VaN). For TaR, the algorithm splits video frames into recent frames (the latest frames) and past frames, then computes cosine similarities between corresponding patches to identify and remove redundant visual tokens. (Line 10) For spatial semantic importance token selection, layer-wise adaptive pooling mechanism based on VaN is employed. The pooling size is dynamically determined by the Coefficient of Variation (CV) of the VaN, (Line 18) where higher CV indicates sparser or more distinct feature distribution. Precomputed model-specific CV thresholds {τ1, τ2, τ3} determine pooling sizes from the set {1,3,5,7}, selecting larger windows for uniform (low CV) VaN distributions and smaller ones for sparse (high CV) VaN distributions (Line 21). To integrate both criteria, TaR-selected tokens are prioritized by assigning them the maximum VaN score before the final token selection. Specifically, by setting VaNpool,l[Il] = max(VaNpool,l) (Line 24) and then applying TopK selection, the algorithm ensures that temporally distinctive tokens are preserved while allowing VaN to select additional tokens based on semantic importance. VideoMME = αTaR + (1 α)VaN, = 6K Qwen-2-VL-7B α = 0 (VaN) α = 0.2 α = 0. α = 0.6 α = 0.8 α = 1 (TaR) Short (-3 min) Medium (3 - 30 min) Long (30 - 120 min) Average 74.4 59.9 51. 62.1 74.3 59.3 51.0 61.6 74.1 61.3 52.4 62.6 74.9 61.4 53. 63.1 74.4 61.2 53.4 63.0 73.8 58.2 53.2 61.7 LLaVA-Next-7B α = 0 (VaN) α = 0.2 α = 0.4 α = 0.6 α = 0.8 α = 1 (TaR) Short (-3 min) Medium (3 - 30 min) Long (30 - 120 min) Average 69.8 59.3 52.1 60.4 69.8 59.3 52.1 60. 72.0 59.2 52.7 61.3 71.1 58.7 52.0 60.6 71.9 57.7 51.4 60. 68.8 57.3 51.0 59.0 Table A1: TaR and VaN Combination Ratio: Sweep over combination ratio α in TaR and VaN combination under 6K memory budget (M ) on VideoMME. α=0 and α=1 correspond to VaNonly and TaR-only, respectively. The best-performing configurations are shown in bold, while the second-best results are underlined. Qwen-2-VL-7B MLVU VideoMME = 6K Holistic Single Multi Avg. Short Medium Long Avg. = 0.125 = 0.25 = 0.50 /C = 0.75 /C = 0.50 /C = 0. 77.6 77.8 78.6 78.2 76.8 73.2 66.2 67.1 64.6 71.7 68.2 65.9 43.9 43.5 39.0 44.6 42.3 39. 63.1 63.4 61.3 65.8 63.3 60.3 68.7 68.0 65.1 74.4 74.1 73.6 57.3 57.2 56.7 59.9 60.3 56. 51.0 51.1 49.7 51.9 52.9 50.4 59.0 58.8 57.2 62.1 62.4 60.3 Table A2: Recent Frame and Compression Ratio Exploration: Top: Sweep over recent frame numbers determined by multiplying various ratios (0.125, 0.25, 0.5) to (frame number corresponding to memory budget ) in TaR. Bottom: Performance under varying compression ratios /C across MLVU and VideoMME with Qwen-2-VL-7B. TaR performs best with 0.25 and compression ratio 0.5. The highest values are shown in bold, with the second-highest underlined. A.2 Hyper-Parameter Exploration InfiniPot-V involves three main hyper-parameters: the TaR and VaN budget allocation ratio α, the number of recent frames used in TaR, and the target compression size applied at each continual KV cache compression step. This section presents comparative experiments exploring each hyper-parameter. TaR and VaN Budget Ratio (α) We compare the accuracy of offline video understanding (OVU) task across different values of α, which determines the budget allocation between TaR and VaN under fixed memory budget (M = 6K), for both Qwen-2-VL-7B and LLaVA-Next-7B models. As shown in Tab. A1, performance peaks when α is between 0.4 and 0.6, outperforming the use of either VaN-only (α = 0) or TaR-only (α = 1). This confirms the effectiveness of our approach, which jointly considers both spatial and temporal dimensions for KV cache compression. Recent Frames (r) and Compression Ratio (M /C) Tab. A2 presents exploration experiments for two key hyperparameters: the recent frames number r, which determines the proportion of recent frames within the memory budget in TaR, and the compression ratio /C, which defines what proportion of the compression size to maintain relative to the memory budget in continual KV cache compression (CKV). For the recent frame number (Tab. A2, Top), we observe optimal performance on both MLVU and VideoMME benchmarks when 0.25f . Setting = 0.5f results in an excessive number of frames being designated as the latest frames for temporal redundancy measurement, which limits the effectiveness of redundancy reduction. This limitation is reflected in the decreased performance 15 metrics. (61.3 vs 63.4 in MLVU and 57.2 vs 59.0 in VideoMME) Note that the sweep experiments are conducted using TaR-only settings (α = 1). For the compression ratio (M /C), we conduct comparative experiments across three ratios (0.75, 0.50, and 0.25). As shown in Tab. A2 Bottom, an excessive compression ratio such as 0.25 in CKV results in noticeable performance degradation. These findings confirm that ratio of 0.5 or higher represents an appropriate configuration for CKV. Based on these explorations, we standardize the hyperparameter values at α = 0.5, = 0.125, and /C = 0.75 for all main experimental results when evaluating InfiniPot-V."
        },
        {
            "title": "B Experimental Setting Details",
            "content": "B.1 MLLMs Video Sampling Details. For all benchmarks, we employ consistent uniform frame sampling strategy to ensure maximized long video understanding performance across all settings. For Qwen-2-VL [41], which supports dynamic image resizing based on the number of frames, we use the hyper-parameter configuration reported to yield the best performance in their original work: FPS_MAX_FRAMES = 768, VIDEO_MIN_PIXEL = 128 28 28 and VIDEO_MAX_PIXEL = 768 28 28. Although theoretically larger token budgets could be set, we adopt this configuration to match the optimal context length of 50K as reported in the original paper [41], on top of which we applied KV cache compression. For LLaVA, we set the number of sampled frames to 128 to ensure it remained within the models trained context length (<32K). With this video sampling configuration, Qwen-2-VL [41] uses 384 frames with 130 tokens per frame, resulting in total context length of 49,920 tokens, while LLaVA-Next [50] and LLaVA-OV [22] use 128 frames with 196 tokens per frame, yielding total of 25,088 for offline long video inputs. B.2 Long Video Understanding Benchmark Details Offline Video Understanding(OVU) We evaluate our method on four multiple-choice based offline video question answering benchmarks: Video-MME [11], MLVU [52], EgoSchema [26], and LongVideoBench [43]. For MLVU and EgoSchema, we use the development sets for evaluation. For Video-MME, we report results without subtitles version. This is because prepending subtitles for all video frames as single context block directly before the question represents an unrealistic setting that is incompatible with streaming scenarios, where subtitles are typically unavailable during real-time video processing and would not be accessible as complete context in advance. Streaming Video Understanding (SVU) For SVU evaluation, we use two benchmarks: RVSEgo and EVS-Movie [47]. RVS-Ego is constructed from 10 videos from the Ego4D [15] dataset, while RVS-Movie uses 22 long videos from MovieNet [17]. Each benchmark consists of QA set containing open-ended generation questions and their corresponding timestamps indicating when each question should be presented during video streaming. The evaluation process works as follows: during CKV processing, when the video stream reaches the timestamp of given question sample, we present the question and generate an answer based on the compressed KV cache accumulated up to that point. The generated answers are then compared against ground-truth answers using GPT-3.5-turbo-0125 to produce accuracy and score metrics. B.3 Baseline Settings Input Video Compression (IVC) Details. For the comparison with Input Video Compression (IVC) methods in Tab. 2 and A6, we implement LongVU [34] and DyCoke [38] as follows: For LongVU, we apply Spatial Token Compression (STC) every 8 frames as specified in the original paper. STC compresses vision token embeddings by identifying temporally redundant patches using cosine similarity between patches. We adjust the similarity threshold to control the compression rate while maintaining the original methodology. For DyCoke, we implement Token Temporal Merging (TTM) which, similar to LongVU, compresses vision encoder output features. TTM calculates cosine similarity between patches in adjacent frames to eliminate redundant patch embeddings. Following 16 the original paper, we process compression every 4 frames and adjust the similarity threshold to control compression rates. For fair comparison in the Continual KV cache compression (CKV) framework in Tab. 2, we adapt both methods to work within memory constraint . Specifically, we compress each input video stream to size and implement sliding window attention [2] to evict older KV cache entries once the cache size reaches the predefined memory limit (M ). This adaptation ensures all methods operate under identical memory constraints for fair comparison with InfiniPot-V. For benchmark comparing InfiniPot-V with IVC methods that use full vision encoding without cache compression, see Tab. A6. KV Cache Compression (KVC) Details. In Fig. 2, we compare three KV cache compression methods within Continual KV cache compression (CKV). First, Uniform Select, inspired by uniform video sampling approaches, selects frames at regular intervals and retains all KV cache tokens corresponding to those frames. For SnapKV [23], we follow the original method configuration under the CKV process, using the last 32 tokens of the given budget tokens as an observation window (w) to calculate attention scores for token selection (see Eq. 1 in Appendix. D.1). Additionally, we apply 1D pooling with kernel size of 7 to these scores, as done in the original implementation6. For InfiniPot [21], we design proxy prompt for video compression: \"Provide detailed description of this video.\" This prompt is utilized in the CaP method to generate attention scores and apply KV cache compression. Detailed experimental results are provided in Tab. A5. FastV Hyper-Parameter Settings. To provide additional performance comparison with compression methods specialized for MLLMs, we also include performance comparisons with FastV [4] in Tab.A5. FastV requires two hyper-parameters, and R, which specify the layer where token pruning begins and the percentage of tokens to prune. For fair comparison, we adjust the of FastV to ensure that the total number of KV cache entries across layers matches the total entry count of other baselines that maintain the same number of KV-cache entries across each layer. Specifically, for Qwen-2-VL, the (L, R) pairs corresponding to memory budgets of 3K, and 6K are set to (2, 2.8%) and (2, 5.8%) respectively. B.4 Positional Encoding Details. MLLM backbone LLMs utilize positional encoding to differentiate vision token positions. LLaVANext [50] and LLaVA-OV [22] use standard 1D RoPE [36], while Qwen-2-VL [41] employs 3D RoPE for multimodal encoding. For Offline Video Understanding(OVU), we apply KV cache compression after positional encoding (i.e., Post RoPE). However, Streaming Video Understanding(SVU) presents challenge: continuous video stream processing can exceed the models maximum positional range. For example, in LLaVA models with 196 tokens per frame, streaming more than 6 minutes of video at 0.5 FPS exceeds the 32K context window (note that RVS-Ego and RVS-Movie average over 60 minutes). To address this, we adopt strategies from InfiniPot [21] and ReKV [33], re-ordering positional indices to fit within the memory budget at each CKV step. Specifically, we cache the pre-positional encoded KVs hidden states and re-assign positional indices during decoding, ensuring they never exceed position regardless of video length. While this enables SVU for arbitrarily long videos, it discards the original positional information of vision tokens. In particular, additional handling is required for Qwen-2-VLs 3D RoPE. Developing methods that preserve the original spatial and temporal position encoding while supporting streaming video lengths beyond the models positional capacity remains an open direction for future work. Multi-Turn Video Understanding Analysis Fig. A1 presents qualitative comparison between query-dependent (SnapKV)[23] and query-agnostic (InfiniPot-V) KV cache compression approaches in multi-turn conversations with streaming video input. When SnapKV performs compression based on Q1, it generates answers almost identical to the Full KV cache for that specific query (Q1), answering that the butter was placed in the refrigerator. 6https://github.com/FasterDecoding/SnapKV 17 Figure A1: Qualitative Results of Multi-Turn Conversation: Full-KV uses 16K cache while InfiniPot-V and SnapKV employ 3K compressed KV cache. SnapKV performs query-guided cache compression based on Q1 before proceeding with multi-turn conversation. The video sample is from the MLVU ego reasoning subtask, using the Qwen-2-VL-7B model. 128 frame sampling is used. However, this query-guided compression strategy reveals significant limitations when handling different types of queries (Q2, Q3) about the same video content. Specifically, SnapKV makes critical errors in subsequent queries - misidentifying kettle as \"two spoons\" in Q2 and incorrectly counting the total number of bread pieces in Q3. In contrast, InfiniPot-V maintains accurate answers consistently across all three queries using the same 3K compressed KV cache. It correctly identifies that the butter was placed in the fridge (Q1), recognizes the kettle on the stove (Q2), and counts all 4 pieces of bread throughout the cooking process (Q3), demonstrating the effectiveness of query-agnostic compression for multi-turn streaming video scenarios. Why Query-Agnostic KV Cache Compression Matters for SVU? In this section, we provide detailed analysis of why query-agnostic compression is essential for Streaming Video Understanding (SVU), building upon the requirements discussed in Sec. 2. To demonstrate how these SVU-specific constraints impact existing KV cache compression methods, we present case study across three representative scenarios. D.1 Preliminary: Attention-based KV Cache Compression Eviction-based KV cache compression reduces cache size by removing tokens with the lowest importance scores. Employing attention scores for computing token importance scores is the predominant approach in previous methods [23, 4, 12, 16]. In methods such as SnapKV [23], the importance scores ut of token xt are computed by aggregating attention scores from the last tokens (i.e., observation window) which contain the user instruction: (cid:88) ut = Attn(xi xt), (1) i=N where is the current sequence length. Using these scores, the KV cache is compressed by retaining the top-M tokens with the highest aggregated attention scores. Here, defines the memory budget: = TopK(u, ) and = [u1, , uN ] indicates the importance scores of all tokens. The compressed Key and Value caches are then formed by extracting tokens at indices I: = K[:, I, :], (2) where K, RHN are the uncompressed Key and Value caches with heads, tokens, and per-head dimension D. This approach has two characteristics: (1) it requires computing the full KV cache for all tokens before compression, and (2) it requires the user query to be present at the end = [:, I, :] 18 Figure A2: KV Cache Compression Case Study with SVU: Illustration of cache control strategies under three conditions, differing in the presence of two core requirements for Streaming Video Understanding (SVU): memory constrained (MC) and query agnostic (QA). (a) Case 1: Queryguided compression retains relevant (GT) frames for accurate responses. (b) Case 2: Without query guidance, compression fails to preserve critical frames, resulting in inaccurate responses. (c) Case 3 (Streaming scenario): In streaming video processing, where frames arrive continuously, continual KV cache compression (CKV) is necessary, but queries are unavailable during compression. of the context. We refer to these approaches as query-guided or attention-based cache compression methods.7 D.2 Case Study: Towards Streaming Video Understanding with CKV To investigate the applicability of attention-based KV cache compression methods to streaming video understanding, we examine three cache control strategies (Fig. A2). Case 1. Recent KV cache compression methods [23, 12] assume full access to context and queries at compression time, as shown in Fig. A2(a). In this memory-unconstrained setting, the model observes the full input before compression. Previous works [23] have demonstrated that attention scores effectively identify query-relevant tokens KV cache (orange box corresponding to GT Frame in Fig. A2(a)), enabling compression that retains critical information while discarding less important tokens. As shown in Tab. A3, this approach maintains performance comparable to the uncompressed cache setup (68.01 vs 68.75) at the cost of large memory usage at compression, detailed in Fig. 1. xt Attention Scoring Prefill Gen. = 3K Gen. = 6K Case Full KV Case 1 Case 2 n/a Attn(q xt) Attn(q xt) Attn(q xt) Attn(qv xt) 25K 25K 25K Case 3 Attn(qv xt) 3K/6K 68.75 () 68.01 60.35 60.60 60.32 57.55 68.40 63.42 63.50 62.28 59. Table A3: Case study of Attention Scoring: conducted on MLVU benchmark with LLaVA-Next-Video-7B. Note that memoryconstrained setting (Case 3) shares the same budget during prefill and generation stages. Case 2. Fig. A2(b) illustrates how attention-based cache compression fails when user queries are unavailable during compression. Under this scenario, although the memory budget is assumed unconstrained, the KV cache is compressed without consideration of (future) queries, causing important visual tokens (orange tokens cache corresponding to the GT Frame) lost during compression. To quantify this degradation and explore alternatives, we test compression with generic queries (q: \"What is happening in this video?\", :\"What are the key events in this video?\") and the last vision tokens (qv) for importance scoring: 7Throughout this paper, \"query\" refers to the users instruction or question related to the given video. 19 Context Length Mem (GB) TTFT (s) Mem (GB) TTFT (s) Case 3 (Ours) Case 1, 2 5K 25K 50K 100K 21.29 33.76 58.55 79.38 0.98 1.21 2.12 3.27 20.93 21.60 22.16 22.85 1.08 1.12 1.17 1. Table A4: Peak GPU Memory and TTFT: Comparison of peak memory usage and TimeTo-First-Token(TTFT) across different context lengths for memory-unconstrained (Case 1, 2) and memory-constrained (Case 3) approaches. Figure A3: Jaccard Similarity between KV Caches: Compare KV cache sets selected by different queries (q1, q2, q3) across layers. ualt = Attn(qalt xt), qalt {q, q, qv} (3) Tab. A3 and Fig. A2(a) show that these alternatives significantly degrade performance (60.32 vs 68.75), even with unconstrained memory. Case 3: Streaming Scenario. Beyond the query-agnostic challenge in Case 2, deploying streaming video understanding on resource-constrained devices requires fixed memory usage for the KV cache. For input video streams, these constraints necessitate continual compression when new frames arrive and memory capacity is reached, as shown in Fig. A2(c). To evaluate this scenario, we use the query-agnostic approach from Case 2 with vision tokens (qv) for importance scoring, while compressing the KV cache whenever memory limits are reached. As shown in Tab. A3, this combined constraint further degrades performance (57.55 vs 60.32), highlighting the challenge of preserving key information under both query-agnostic and memory-constrained settings. This case study reveals two key challenges for KV cache compression in streaming video: 1) the need for query-agnostic compression due to continuous incoming video, and 2) the requirement to maintain fixed memory constraints. These challenges cause significant performance drops in previous methods [23, 21, 4], motivating Continual KV cache compression (CKV) specifically designed for memory-constrained streaming video. Attention Scoring Analysis We further analyze the query-dependent nature of attention-based KV cache compression using the VideoMME benchmark. To investigate why performance varies with different queries, we compute the Jaccard similarity between token sets selected for different queries across layers using attention scores at each layer. For this analysis, q1, q2, q3 represent three distinct questions associated with the same video sample in the VideoMME benchmark. As shown in Fig. A3, the similarity between token sets decreases significantly in the middle-to-late layers, dropping to around 0.4. This indicates that each query selects different set of tokens, particularly in deeper layers. This analysis highlights that attention-based scoring methods inherently select query-specific tokens, explaining the performance degradation when query information is unavailable or changes during streaming video scenarios."
        },
        {
            "title": "E Memory and Latency Measurement Results",
            "content": "Table A4 presents measurements of peak memory consumption and Time-To-First-Token (TTFT) during the prefill stage, conducted on single NVIDIA A100-80GB GPU using PyTorch. The experiments averaged over five runs with three warmup iterations, compare the performance of memory-unconstrained (Case 1, 2) and memory-constrained (Case 3) approaches across various context lengths. For memory-unconstrained methods, we observe linear growth in memory requirements, escalating from 21.29 GB at 5K tokens to 79.38 GB at 100K tokens, accompanied by proportional increase in TTFT from 0.98 to 3.27 seconds. 20 Our memory-constrained continual KV cache compression (Case 3) exhibits remarkably different behavior. Despite the increasing context length, the peak memory usage shows only minimal growth, rising modestly from 20.93 GB at 5K tokens to 22.85 GB at 100K tokens. Similarly, the TTFT remains relatively stable, increasing from 1.08 to 1.20 seconds across the same range. These detailed measurements demonstrate that our approach effectively maintains near-constant resource utilization while processing extended video frames."
        },
        {
            "title": "F Related Work",
            "content": "F.1 MLLMs for Long Video Understanding Recent advances in long-context MLLMs have attracted significant attention. Notable examples include Gemini-2.0 [32], supporting streaming video; LongVILA [5], capable of handling up to 6,000 video frames; LLaVA-Next-Video [50], which leverages high-quality synthetic instruction data; and Qwen-2-VL [41], enabling hour-long video analysis via multimodal RoPE. F.2 Input-Vision Compression (IVC) To address the computational demands of long-form video processing, several approaches have been proposed to compress redundant visual information before it enters the backbone LLM. LongVU [34] adopts query-dependent input frame sampling and redundant pixel removal for finegrained video understanding, but the two-tower vision encoding results in high latency during input sampling, making it impractical for streaming scenarios. Additionally, this approach requires training specialized models to operate in the proposed manner, limiting its applicability to existing pre-trained models. DyCoke [38] reduce redundancies between adjacent frames at the input video level and dynamically updates query-related tokens in the KV cache from external storage. Slow-Fast-LLaVA-1.5 [44] proposes dividing input video processing into separate slow and fast pathways, using different projection methods to reduce input vision tokens. However, this approach still suffers from the limitation of requiring all input vision tokens to be processed simultaneously and necessitates additional model training. F.3 KV Cache Compression (KVC) Understanding the long context in MLLMs demands efficient KV cache control to manage memory growth and latency overhead. KV cache compression methods can be broadly categorized into query-dependent and query-agnostic approaches. Query-Dependent KV cache Compression. Methods like SnapKV [23], H2O [51], HeadKV [12] and ThinK [45] leverage query-to-context attention scores to identify crucial KV entries but require the full context to be prefilled before compression, making them impractical under memory constraints. In the multimodal domain, FastV [4] accelerates prefill by pruning vision tokens at certain layers based on their attention scores from the final query token. SparseVLM [49] selects visual tokens relevant to user queries via cross-attention. Overall, query-dependent methods effectively compress context but struggle to handle diverse queries for the given context after compression [37]. ReKV [33] addresses streaming video scenarios by offloading video-related KV cache to CPU memory and retrieving query-dependent cache entries on demand. This approach relies on external storage and suffers from data transfer overhead, making it unsuitable for memory-constrained streaming video understanding. Query-Agnostic KV cache Compression. Recent works pursue query-agnostic KV cache compression to eliminate reliance on future queries [13, 8, 16, 20, 6, 30]. In particular, SqueezedAttention [16] uses key-based clustering but requires full-context encoding, limiting its applicability to memoryconstrained settings. InfiniPot [21] compresses context by approximating potential user queries through task-specific proxy prompt, but its fixed prompt restricts flexibility. In the vision domain, HiRED [1] and FasterVLM [48] utilize [CLS] token attention scores for compression decisions. However, their reliance on special tokens restricts their application to recent MLLMs that lack such tokens [46, 50], limiting their broader applicability. 21 Streaming MC QA Compression Method Prefill Budget Decoding Budget VideoMME MLVU Short Medium Long Avg. Holistic Single Multi Avg. LVB Avg. 1 C 2 C ) ( a 1 C 2 C ) ( 3 C - - Full KV FastV [4] (L = 2) 50K 50K 48/3K (R = 2.8) 48/6K (R = 5.8) SnapKV [23] Uniform SnapKV Uniform SnapKV InfiniPot [21] InfiniPot-V - - Full KV Uniform SnapKV [23] Uniform SnapKV Uniform SnapKV InfiniPot [21] InfiniPot-V 50K 50K 50K 50K 50K 50K 3K 6K 12K 24K 3K 6K 12K 24K 3K 6K 12K 24K 3K 6K 12K 24K 25K 25K 25K 25K 25K 25K 25K 25K 25K 1.5K 3K 6K 12K 1.5K 3K 6K 12K 1.5K 3K 6K 12K 1.5K 3K 6K 12K 3K 6K 3K 6K 3K 6K 3K 6K 12K 24K 3K 6K 12K 24K 3K 6K 12K 24K 3K 6K 12K 24K 25K 3K 6K 3K 6K 3K 6K 3K 6K 1.5K 3K 6K 12K 1.5K 3K 6K 12K 1.5K 3K 6K 12K 1.5K 3K 6K 12K Qwen-2-VL-7B 74. 54.11 59.67 74.00 74.22 70.33 72.00 69.00 72.11 66.00 72.33 74.00 74.22 66.67 72.00 74.44 74. 67.11 72.89 74.00 74.22 73.89 74.11 74.22 74.22 62.11 50.11 54.55 61.00 60.55 54.67 58. 54.00 57.56 52.44 53.33 55.33 59.22 52.22 55.33 58.89 61.00 54.55 57.33 57.78 60.55 57.78 60.78 62.68 63.22 55. 63.93 48.67 50.78 54.22 54.33 49.55 52.11 50.67 52.22 48.00 48.67 51.44 53. 49.89 51.33 52.89 53.78 51.00 51.33 53.22 53.56 51.78 53.44 53.89 53.11 50.96 55.00 63.07 63.03 58.18 60. 57.89 60.63 55.48 58.11 60.26 62.22 56.26 59.55 62.07 63.00 57.55 60.52 61.67 62.78 61.11 62.78 63.59 63.52 LLaVA-Next-Video-7B 74.33 74.33 73.89 74.44 74.44 66.33 71.00 64.00 69.55 56.22 59.22 64.89 72. 52.40 62.11 66.33 72.11 53.22 58.22 62.44 70.55 63.89 67.78 72.44 73.89 60.11 62.33 62.00 59.89 60. 54.00 56.33 54.55 58.44 46.89 51.55 55.67 59.89 58.00 54.55 56.11 58.00 51.11 51.78 53.89 59.22 52.55 56.22 59.55 58. 54.11 62.85 55.00 54.78 53.78 53.78 49.67 51.55 51.11 52. 44.00 47.44 49.78 53.00 51.33 48.55 51.11 53.11 47.55 49.33 51.11 52.55 47.11 50.33 51.33 52.11 63.89 63.56 62.70 62. 56.67 59.63 56.55 60.26 49.04 52.74 56.78 61.85 47.89 55.07 57.85 61.07 53.11 54.22 55.81 60.77 54.52 58.11 61.11 61. 76.34 69.59 72.00 77.08 77.59 72.29 77.08 75.88 76.46 72.54 72.55 75.94 77. 75.88 76.46 75.71 77.66 74.94 75.02 74.46 76.03 77.73 77.16 76.90 76.91 80.60 80.29 80.66 80.41 80. 75.12 77.84 78.53 80.86 69.72 74.30 76.71 80.03 74.92 76.94 79.60 79.71 69.89 72.42 76.46 79.84 77.08 77.88 80.03 80. 73.91 59.40 64.08 67.49 73.91 59.06 67.49 63.48 66.43 59.00 62.19 65.53 71. 63.48 66.43 68.61 71.82 61.80 63.18 66.46 71.11 70.38 72.31 73.41 73.97 43.29 65.85 58. 62.85 33.84 33.47 39.07 42.90 33.51 39.07 35.35 36.22 33.51 33.67 37.01 40. 35.35 36.22 35.98 39.90 36.60 37.09 38.30 40.29 43.15 44.75 43.97 42.18 55.01 57.60 62.11 66.10 55.54 62. 58.99 60.66 55.59 57.00 60.36 64.18 58.99 60.66 61.31 64.37 58.36 59.11 60.70 63.71 64.70 65.82 65.99 65.73 47.94 50. 59.06 58.80 59.80 59.11 56.70 56.72 55.21 55.82 57.91 58.60 54.91 55.15 56.89 59.09 54.00 54.64 56.94 57. 57.64 58.40 59.18 58.94 51.30 54.38 61.42 62.64 57.84 60.73 57.86 59.34 55.43 56.98 59.51 61. 56.72 58.45 60.09 62.15 56.64 58.09 59.77 61.44 61.15 62.33 62.92 62.73 73.73 49.43 68. 63.55 65.05 72.38 72.25 73.01 73.45 59.65 65.60 59.73 63. 52.53 57.25 61.14 67.33 56.89 59.18 62.15 67.99 56.44 55.88 57.97 67.81 57.32 65.74 69.41 71.16 49.19 49.62 49.67 49. 38.55 43.92 41.69 45.07 36.53 36.48 34.55 44.31 32.62 35.71 37.12 44.89 30.54 34.45 37.07 45.57 34.64 40.31 43.93 51. 68.01 68.19 68.46 68.64 58.04 62.90 59.94 63.26 52.87 56.19 57.99 64.38 55.11 57.55 59.98 64. 52.88 54.48 57.28 64.89 56.49 61.94 65.16 68.35 62.35 62.55 62.34 62.34 59.14 61.69 56.19 59. 54.92 54.40 57.72 61.04 53.65 54.71 57.81 58.83 52.14 52.43 55.58 59.23 56.48 58.37 60.86 61.84 64.75 64.76 64.50 64. 57.95 61.41 57.56 61.14 52.28 54.44 57.50 62.42 52.22 55.78 58.55 61.55 52.71 53.71 56.22 61.63 55.83 59.47 62.38 63. Table A5: InfiniPot-V vs KVC Offline long video understanding evaluation results under memoryconstrained scenario (case 3), with MC (Memory-Constrained) and QA (Query-Agnostic) conditions marked. Results are reported on (1) Video-MME - Short: -3min, Medium: 3-30min, Long: 30min-2h, (2) MLVU - Holistic, Single-Detail, Multi-Detail LVU, and (3) LVB (LongVideoBenchmark)."
        },
        {
            "title": "G Experimental Results Data",
            "content": "G.1 Comparison between InfiniPot-V and KVC Tab. A5 provides detailed performance comparison between KV cache compression (KVC) methods and InfiniPot-V across offline video understanding (OVU) benchmarks under various compression ratios for two models: Qwen-2-VL and LLaVA-Next. In Case 1, where the full prefill is conducted and the final query is accessible at compression time, FastV demonstrates significantly inferior performance at similar compression ratios due to its aggressive token-pruning strategy. In contrast, SnapKV shows robust performance at high compression ratios across both models by utilizing the full context KV cache and retaining vision tokens that are highly correlated with the given query. 22 Qwen-2-VL Vision Decoding MLVU VideoMME IVC Methods Budget Budget Holistic Single Multi Avg. Short Med Long Avg. Avg. Full KV Uniform TTM [38] STC [34] InfiniPot-V Uniform TTM [38] STC [34] InfiniPot-V 50K 50K 50K 50K 6K 50K 50K 50K 3K 50K 6K 6K 6K 6K 3K 3K 3K 3K 76.3 77.7 78.2 77. 77.2 75.7 77.3 76.9 77.7 73.9 69.8 70.0 71.5 72. 66.5 67.8 68.2 70.4 43.3 41.6 42.7 44.7 44.7 38.6 39.5 41. 43.2 65.9 64.0 64.5 65.7 65.8 61.1 62.4 63.1 64. 74.7 74.9 74.9 74.3 74.1 72.2 72.7 71.2 73.9 62. 58.0 59.2 59.6 60.8 53.4 56.2 55.9 57.8 55.0 52.8 52.7 54. 53.4 50.0 52.2 53.7 51.8 63.9 61.9 62.3 62.8 62. 58.6 60.4 60.3 61.1 64.2 62.5 62.9 63.8 63.8 59.4 61.0 61. 62.5 Table A6: InfiniPot-V vs IVC: Performance comparison between Input-Vision Compression (IVC) methodology and InfiniPot-V. Vision budget denotes the vision token length before IVC, while decoding budget refers to the input token length used during decoding. Evaluated using Qwen-2-VL with MLVU and VideoMME datasets. Case 2 examines the query-agnostic setting, where, as explored in our earlier case study in Appendix. D.2, SnapKV exhibits notable performance degradation across both models when applied in query-agnostic manner, showing performance comparable to uniform selection baseline. In Case 3, which represents the CKV framework scenario where the constrained memory budget is used for both prefill and decoding stages, InfiniPot-V significantly outperforms all three baselines across various compression ratios on both models, as showcased in Fig. 2. G.2 Comparison between InfiniPot-V and IVC Table A6 presents performance comparison between Input-Vision Compression (IVC) methods and InfiniPot-V on the MLVU and Video-MME benchmarks using the Qwen-2-VL model. Under 6K decoding budget, the IVC methods demonstrate robust overall performance by utilizing the full vision encoding budget (50K tokens). InfiniPot-V achieves comparable or slightly superior performance to these methods while operating under constrained memory budgets for both vision encoding and decoding stages (6K tokens). When the decoding budget is compressed to 3K tokens, the IVC methods exhibit performance degradation, with LongVUs STC methodology achieving the highest performance among the IVC approaches. Notably, InfiniPot-V demonstrates both efficiency and effectiveness by achieving higher accuracy than IVC methods that utilize the full vision encoding budget, while operating under constrained budgets (3K) for both vision encoding and decoding stages."
        },
        {
            "title": "H Limitation and Future Work",
            "content": "InfiniPot-V introduces the first training-free, query-agnostic framework for memory-constrained streaming video understanding, enabling length-independent KV cache compression with minimal accuracy loss across long-form, real-time scenarios. However, several avenues exist for further advancement. Current approaches focus primarily on vision tokens, yet real-world streaming applications involve multiple modalities including speech, text, and video simultaneously. Future work could extend our framework to unified multimodal compression, enabling more realistic and comprehensive streaming understanding systems that efficiently manage diverse input types within fixed memory constraints. Additionally, our current fixed budget allocation between TaR and VaN components could benefit from adaptive mechanisms that dynamically adjust compression ratios based on input characteristicsallocating more resources to temporal redundancy reduction for static scenes or prioritizing spatial importance for content-rich frames. Furthermore, while InfiniPot-Vs training-free nature ensures broad applicability, end-to-end learning approaches could optimize models specifically for continual compression scenarios, potentially enabling more aggressive compression ratios through learned token importance estimation [18] tailored to streaming video understanding tasks."
        }
    ],
    "affiliations": [
        "Hanyang University",
        "Qualcomm AI Research, Qualcomm Korea",
        "Sungkyunkwan University"
    ]
}