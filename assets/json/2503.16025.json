{
    "paper_title": "Single Image Iterative Subject-driven Generation and Editing",
    "authors": [
        "Yair Shpitzer",
        "Gal Chechik",
        "Idan Schwartz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Personalizing image generation and editing is particularly challenging when we only have a few images of the subject, or even a single image. A common approach to personalization is concept learning, which can integrate the subject into existing models relatively quickly, but produces images whose quality tends to deteriorate quickly when the number of subject images is small. Quality can be improved by pre-training an encoder, but training restricts generation to the training distribution, and is time consuming. It is still an open hard challenge to personalize image generation and editing from a single image without training. Here, we present SISO, a novel, training-free approach based on optimizing a similarity score with an input subject image. More specifically, SISO iteratively generates images and optimizes the model based on loss of similarity with the given subject image until a satisfactory level of similarity is achieved, allowing plug-and-play optimization to any image generator. We evaluated SISO in two tasks, image editing and image generation, using a diverse data set of personal subjects, and demonstrate significant improvements over existing methods in image quality, subject fidelity, and background preservation."
        },
        {
            "title": "Start",
            "content": "Single Image Iterative Subject-driven Generation and Editing Yair Shpitzer Bar-Ilan University Gal Chechik Bar-Ilan University, NVIDIA Idan Schwartz Bar-Ilan University 5 2 0 2 M 0 2 ] . [ 1 5 2 0 6 1 . 3 0 5 2 : r Figure 1. SISO is an inference-time optimization method to personalize images from single subject image without training . SISO can personalize the subject of given image or generate new images with the personal subject."
        },
        {
            "title": "Abstract",
            "content": "Personalized image generation and image editing from an image of specific subject is at the research frontier. It becomes particularly challenging when one only has few images of the subject, or even single image. common approach to personalization is concept learning, which can integrate the subject into existing models relatively quickly but produces images whose quality tends to deteriorate quickly when the number of subject images is small. Quality can be improved by pre-training an encoder, but training restricts generation to the training distribution, and is time consuming. It is still difficult and open challenge to personalize image generation and editing from single image without training. Here, we present SISO, new, training-free approach based on optimizing similarity score with an input subject image. More specifically, SISO iteratively generates images and optimizes the model based on loss of similarity to the given subject image until satisfactory level of similarity is achieved, allowing plug-and-play optimization to any image generator. We evaluated SISO in two tasks, image editing and image generation, using diverse data set of personal subjects, and demonstrate significant improvements over existing methods in image quality, subject fidelity, and background preservation. 1. Introduction Subject-driven text-conditioned image generation and editing combines the ease-of-use of prompt-conditioning with the superior visual control provided when creating visual content using personalized elements. It is crucial for creative expression, from advertising to digital art, but remains challenging task when only few images of the personal element are available. The most common approach to personalization is concept learning, where pre-trained model is fine-tuned on few images of specific concept [10, 47]. While effective when multiple training samples are available, these methods struggle when given only single image, failing to generalize and often overfitting to the specific details of the input. This leads to style leakage and structural distortions rather than accurate personalization. encoder-based methods [11, 31, 67] adapt better to single image by training on diverse set of concepts. However, this training requires significant computational resources and datasetspecific tuning, delaying their public availability. As result, subject-driven generation and editing remain largely inaccessible for newer models, leaving the challenge of efficient single-image personalization unsolved. To address these challenges, we describe new method called SISO (Single Image Subject Optimization) . During generation, SISO directly optimizes subject similarity score between the generated image and single image. Specifically, we show that by using similarity score based on DINO [37] and IR features [51]SISO excels at capturing identity features and filtering out the background even with single image. By optimizing this score, our method focuses on preserving the identity of the concept rather than other elements of the scene. Employing pre-trained score models for fine-tuning diffusion model presents significant challenges. Current approaches [8, 47] continue the standard optimization of diffusion process, which can be viewed as predicting the noise of given latent. They do not work with pixel-level input because they operate on the latent space. In contrast with these previous methods, our optimization process iteratively takes as input decoded generated images during inference. We generate an image at each step, compute similarity loss, and update the parameters. After each step, we generate new image and repeat the process until satisfactory level is achieved. Our method steers the model at inference time by backpropagating through the diffusion process. With the rise of distilled diffusion models that require as few as one denoising step [30, 42], our approach becomes significantly more practical. We further describe how SISO can be efficiently applied to standard diffusion processes like Sana [64], which can be computationally expensive. We describe two-stage training simplification: first, training in an efficient setup with low number of denoising steps and simple prompts; then, at inference, applying the optimized model with more denoising steps and varied prompts to enhance output quality. Fig. 1 demonstrates the effectiveness of SISO, personalizing with single subject image. SISO allows for highly natural edits, such as accurately replacing the cat while keeping the original cats stance. For the plush images, we successfully replaced the subject without altering the background, maintaining natural pose on the tree. Additionally, our image-generation variant showcases the subjects versatility in various complex prompts. Beyond improving accuracy and image quality, the testtime optimization approach presented here offers two benefits: (i) it is plug-and-play, meaning both the similarity loss and the generative model can easily be replaced, making it very suitable for the high-paced release cycles of image generators; and (ii) the optimization generates an image at each step, making the optimization process visible and able to stop at each point, enhancing user control. We ran SISO with single subject image for both generating and editing images on the ImageHub benchmark, demonstrating significant improvements in image naturalness while maintaining high fidelity in identity and background preservation. Our human evaluations support these results, showing better prompt alignment and naturalness in image generation, as well as enhanced background preservation and naturalness in image editing. We also provide qualitative results illustrating the significant improvements. This work has the following contributions: (i) We propose SISO, novel inference-time iterative optimization technique that alters the subject of vanilla image generator using only one reference subject image. (ii) We show that SISO can be applied to two popular tasks: subject-driven image generation and editing, with minor adaptations to the regularization of penalties. (iii) Our results demonstrate significant improvements in single-image subject-driven personalization, opening up new thread for research in image personalization that, to our knowledge, has not been explored yet. 2. Related Work Concept Learning. Concepts are typically trained using small set of up to 20 images. Various fine-tuning techniques have been proposed. Initial attempts used prompt tuning, i.e., learning token representation [10], and learning negative prompts as well [9]. The following approach updates the entire model [47]. Newer variations learn style and content separately [50] or consider multiple concepts [17]. However, these methods often leak style or fail to learn complex objects needed for subject-driven generation, especially with limited training image set. Encoder Learning. Early methods trained an encoder to generate an initial subject embedding or to adjust network weights and then fine-tuning during inference for highquality personalization. However, these methods were often restricted to specific concepts [11, 31, 48]. Recent approaches studied how to bypass inference-time optimization [4, 26, 34, 52, 62, 67]. Significant efforts have focused on personalizing human faces, utilizing identity recognition networks or incorporating them as auxiliary losses to enhance identity preservation [12, 16, 41, 61, 63, 68]. Some recent studies have explored adding cross-attention layers [12, 16, 67]. However, methods that encode subjects into existing cross-attention layers tend to preserve the original content more effectively [1, 34, 40, 57, 60, 63]. Despite their advancements, training such encoders still requires substantial computational resources. Recent stateFigure 2. SISO workflow for image generation. SISO generates images by iteratively optimizing based on pre-trained identity metrics IR and DINO. The added LoRA parameters are updated at each step, while the rest of the models remain frozen. The left panel shows the progress of subject-driven optimization for the prompt image of dog by displaying the initial image, followed by the 15th, 25th, and 35th iteration steps. Similarity to the subject image (top) increases during optimization. We find that optimizing with simple prompt is effective, since the optimized model generates novel images of the subject without further optimization, even with complex prompts, as shown on the right. of-the-art encoder solutions, such as the ones proposed for Flux [30], require large-scale datasets and extended training times [3, 56]. Furthermore, to the best of our knowledge, no encoder solution currently exists for Sana [64]. In contrast, our proposed method is plug-and-play, allowing for rapid adaptation to variety of generative models. Subject-driven Image Editing. Initial methods train an adapter to align image encodings with text encodings [54, 66]. These methods fail on novel concepts. Later methods replaced semantic representations with identity features [5]. Other works add more control via camera parameters or text prompts [38, 65, 69, 70]. Following, identity preservation was improved with part-level composition [6]. Recent works leverage rectified flow models and tailored diffusion noise schedules to enable fast, zero-shot inversion and highquality semantic image editing [7, 35, 45]. Another thread explored concept learning from set of images instead of training an adapter [14, 15, 33]. These approaches are closely related to ours; however, learning concept requires up to 20 images, while SISO uses single image. Another recent method is training-free, creating collage of the reference on the background image [32]. However, this approach is better suited for insertion rather than subject replacement. Instead, we leverage subject similarity score that modifies image subjects. Training Free Image Editing. Refers to methods with no separate learning phase, commonly used in image editing tasks. Style-transfer methods employ an inversion technique and transfer attention key-and-value representations from reference style image [18, 23, 25, 53] or use an encoder [59]. recent method fuses content and style without inversion [46]. While training-free methods may enable single image reference for edits, they mostly focus on style. Our approach, which steers the model at inference time, can learn subjects from input reference images. 3. Method We introduce SISO (Single Image Subject Optimization), subject-driven conditioning method operating with single subject image. SISO operates by fine-tuning the diffusion model at inference time, using loss function computed over the generated image. Specifically, since SISO operates over images in pixel space, we can use high-quality pretrained models that measure object similarity and encourage the model to produce images similar to the desired subject. This approach is different from existing approaches that operate by predicting the noise, as done during the training of the diffusion model. 3.1. Preliminaries: Conditioned Latent Diffusion conditioned latent diffusion model (LDM) generates an image p(xy), where is the conditioning term, such as text. Training the model is typically achieved by adding This update rule is simplified for brevity. In practice, we use Adam optimizer [27]. After updating the model parameters, we repeat this process iteratively. Since this iterative process involves generating well-formed images, rather than noisy latent, it can be used in an interactive manner. Users can observe and stop the optimization process based on the optimized image displayed at each step, or it can stop automatically using standard early stopping strategies (see Appendix C). By default, backpropagation through LDM is performed through the entire diffusion process, significantly increasing memory requirements. Our approach is particularly well suited for efficient distilled turbo variants that require only single diffusion step [42]. To support non-distilled models and reduce computational costs, we stop backpropagation after several denoising steps. For instance, with Sana, we backpropagated through the last three denoising steps, which we find sufficient for personalization. This is probably because the final diffusion steps primarily refine local appearance details [20]. We now discuss in detail how SISO can be used for (i) image generation and (ii) image editing. 3.3. Subject-driven Image Generation To use SISO for generation, we expect two inputs: conditioning prompt and single reference image of the subject. We define the similarity loss as Lsim(ˆxi, xs) = δDINO(ˆxi, xs) + δIR(ˆxi, xs), (2) where ˆxi is the generated image at optimization step i, δDINO and δIR are distances in DINO [37] and IR [51] embedding spaces, and a, are calibration hyper-parameters. IR and DINO are suited for assessing the identity distance of objects independent of background influences. Using two metrics in our loss function serves two purposes. (i) They enhance performance thanks to an ensemble effect; and (ii) they serve as form of penalty regularization, mitigating the risk of mode collapse that might occur when optimizing based on single metric. Training Simplification. To enhance training stability, we find generating simple images using simple prompt beneficial, as similarity metrics often struggle in complex scenes. Additionally, we observe that training with low number of denoising steps, even single step, is sufficient for efficiency. Notably, the optimized LoRA weights, even when trained with simple prompt and minimal denoising steps, can be used for inference with different prompts and more denoising steps to enhance quality. This insight inspired two-stage approach for handling detailed scenes: (1) first, optimize with simple prompt and low number of denoising steps, then (2) use the fine-tuned Figure 3. SISO workflow for image editing. The main differences from generation  (Fig. 2)  are: (1) Use diffusion inversion to map the input image into latent begins (bottom); and (2) it adds background preservation regularization term (Eq. 3) noise to an image and learning to predict the added noise: minθ ˆϵθ(zt, t, y) ϵt2 2 . Here, zt is an intermediate noisy latent, ϵt is the added noise up to step t, and θ represents the learnable weights. In many personalization approaches, fine-tuning the model is achieved using the same objective followed during training, namely, reconstruction loss over the latents. In personalization tasks, one is given set of images of specific subject that one wishes the model to learn. Here, we assume that only single subject image is given and denote it by xs. 3.2. SISO: Single-Image Subject Optimization SISO optimizes the image generation model during inference using the generated images to compute the loss. By defining the loss in pixel space, we enable using highquality pre-trained models to measure the similarity between the subject in the generated image and in the input. SISO operates iteratively (Fig. 2 left). We start with randomly initializing low-rank adaptation parameters θLoRA and adding them to diffusion model following LoRA [21]. We also fix specific seed for the noise latent zT and use deterministic sampler [53]. Then, at step of the iterative process, we generate an image ˆxi using the diffusion model. The generated image is the output of differentiable and deterministic LDM, hence any differentiable loss L(θLoRA, ˆxi) computed over the image can be used for propagating gradients back to the model parameters θLoRA. To preserve subject identity, we set to be subject similarity loss that takes the generated image ˆxi and reference subject image xs as input and computes the similarity of subjects across images. We then update the parameters with gradient descent step: θLoRA θLoRA + α θLoRA L(ˆxi, xs) L(ˆxi, xs)2 . (1) Table 1. Comparison of two baselines for subject-driven image generation using single reference image per subject. We evaluate identity preservation (DINO, IR), prompt adherence (CLIP-T), and naturalness (FID, KID, CMMD). Identity Preservation Prompt Adherence DINO CLIP-T IR AttnDreamBooth ClassDiffusion SISO (ours) 0.47 0.50 0.48 0.51 0. 0.53 0.29 0.29 0.31 Naturalness FID KID CMMD 164.4 166.6 0.004 0. 149.2 0.002 0.41 0.18 0.18 Table 2. Comparing SISOwith Dreambooth using three backbone models: SDXL-Turbo, Flux Schnell and Sana. for subject-driven image generation using single reference image. SISO improves prompt adherence while maintaining image fidelity. Backbone Identity Preservation Prompt Adherence DINO CLIP-T IR DreamBooth SISO (ours) SDXL-Turbo SDXL-Turbo DreamBooth SISO (ours) FLUX Schnell FLUX Schnell DreamBooth SISO (ours) Sana Sana 0.58 0.48 0.33 0.51 0.45 0. 0.67 0.53 0.45 0.56 0.46 0.51 0.28 0.31 0.25 0.31 0.29 0. Naturalness Diversity FID KID CMMD MSE 177.69 149.2 227.1 149.5 149.5 149. 0.010 0.002 0.023 0.002 0.003 0.003 0.85 0.18 1.09 0.14 0.23 0. 0.05 0.11 0.05 0.12 0.16 0.19 model to generate images with more complex prompts and additional denoising steps. As shown in Fig. 2 (right), after optimizing LoRA weights for the prompt image of dog, the learned subject can be generated for various prompts without further optimization. 3.4. Subject-driven Image Editing In subject-driven image editing, the model swaps the subject of given image x0 with reference image xs while crucially preserving the background, unlike in image generation, where background coherence with the prompt suffices. Additionally, editing an image requires converting it into the domain of the diffusion model (see Fig. 3). We begin with inversion using ReNoise inversion [13], which yields faithful inversions (more details in section of the Appendix). Let ˆx0 = Inversion(x0) be the inverted image of x0. To preserve the background, we first generate subject mask Ms by classifying the image xs and employing object detection with Grounding DINO to identify objects of the same class [36]. We then extract segmentation mask from the detected bounding box using SAM [28]. The background loss is defined as follows: Lbg(xi, xs, ˆx0) = MSE( Ms(xi), Ms(ˆx0)), (3) where Ms is the inverse subject mask, i.e., the subjects background. Intuitively, this loss acts as penalty for maintaining the background of the original image x0. Overall, the loss for subject-driven image editing is: L(xi, xs, ˆx0) = Lsim(xi, xs) + Lbg(xi, xs, ˆx0), (4) where is hyperparameter. We optimize the loss with our iterative inference-time optimization technique. 4. Experiments Benchmark Dataset and evaluation protocol. We use the benchmark dataset and the experimental protocol from ImagenHub [29]. For subject-driven image editing, their setup consists of 154 samples, each featuring one of 22 unique subjects from various categories. These include as animals (cat, dog) and day-to-day objects like backpack, sunglasses, or teapot. Subject images were taken from DreamBooth [47]. For subject-driven image generation, the setup comprises of 150 prompts with 29 unique sample subjects with similar categories. Implementation details. For image generation, we used SDXL-Turbo [49], the distilled version of SDXL [42]. For image editing, we used SD-Turbo1, distilled version of Stable Diffusion 2.1 [44]. We set the loss calibration hyperparameters to = 1, = 1, = 10, and the learning rate to α = 3e4. The resolution in all our experiments is 512 512. Baselines. Since our task is to efficiently adapt pretrained image generator using single image of reference subject, we compare SISO against baselines that can operate without requiring to train an encoder learning. For image generation, we compared with AttnDreamBooth [39]. It improves over DreamBooth [47] with three-stage process, optimizing textual embedding, cross-attention layers, and the U-Net. We also compared with ClassDiffusion, which uses semantic preservation loss [22]. For image 1https://huggingface.co/stabilityai/sd-turbo Table 3. Subject-driven image editing. All experiments used single reference image per subject. We report identity preservation (DINO, IR, CLIP-I), background preservation (LPIPS), and naturalness (FID, KID, CMMD). Table 6. User study for image editing (left) and generation (right). values are the win rate of our method (fraction of preferred cases) against the leading baseline. denotes the standard error of the mean (SEM) based on binomial distribution. Identity Preservation Background Preservation LPIPS DINO IR CLIP-I Naturalness FID KID CMMD TIGIC SwapAnything SISO (ours) 0.51 0.45 0. 0.58 0.60 0.75 0.77 0.74 0.80 0.22 0.11 0.14 143.26 0.0066 185.74 0.0277 114.83 0.0031 0.759 1.101 0.475 Table 4. Ablation for image generation. We report identity preservation (DINO, IR) and prompt adherence (CLIP-T) Identity Preservation Prompt Adherence DINO CLIP-T IR SISO (ours) Ours - w/o Prompt Simpl. Ours - w/o DINO Ours - w/o IR 0.48 0.52 0.44 0.49 0.53 0.62 0.50 0. 0.31 0.29 0.31 0.31 Table 5. Ablation for image editing. We report identity preservation (DINO, IR, CLIP-I) and background preservation (LPIPS) Identity Preservation DINO IR CLIP-I BG Preservation LPIPS SISO (ours) Ours - w/o BG Pres. Ours - w/o DINO Ours - w/o IR 0.55 0.55 0.49 0.54 0.75 0.76 0.74 0. 0.80 0.80 0.78 0.78 0.14 0.18 0.13 0.12 editing, we used SwapAnything, which employs masked latent blending and appearance adaptation [15]. All the methods above use concept learning to depict the subject and typically require up to 20 subject images for accurate performance. However, here, we use them with single image. We also compared with TIGIC, training-free technique that uses an attention-blending strategy during denoising [32]. 4.1. Evaluation Metrics Identity Preservation. To evaluate subject similarity, we crop the subject using Grounding DINO [36] and compare it using: (i) DINO distance for instance similarity, particularly for animals, (ii) IR features, effective in item similarity [51], and (iii) CLIP-I, which measures class-level similarity [43]. Naturalness. To assess image realism, we compare generated images against reference set: vanilla Stable Diffusion outputs for generation and input images for editing. We compute three metrics: FID [19], KID [2], which has been shown to be more stable in small datasets, and CMMD [24] for semantically richer CLIP-based evaluation. TIGIC (Editing) ClassDiffusion (Generation) Identity Preservation Naturalness Background Preservation Prompt Adherence 0.45 0.05 0.58 0.05 0.60 0.05 - 0.47 0.05 0.65 0.05 - 0.69 0.05 Prompt adherence. In image generation, we also measure alignment with the input prompt using CLIP-T, the CLIP score between the generated image and the input prompt. Diversity. Single-image concept learning often leads to overfitting, limiting diversity in generated images due to reconstruction loss. To quantify this, we compute the mean squared error (MSE) between generated and subject images. Background Preservation. For image editing, maintaining the background while altering the subject is crucial. We assess this using LPIPS [71], where lower scores indicate higher similarity. To exclude the edited region, we mask the subject using Grounding DINO and SAM [28] before computing LPIPS. 4.2. Quantitative Results Image Generation. Table 1 shows results for image generation, comparing SISO to two subject-driven baselines that typically learn from multiple subject images but are tested here with single reference. SISO significantly improves naturalness metrics, suggesting that baselines degrade image quality due to overfitting. Additionally, SISO enhances prompt adherence while maintaining subject identity. This suggests that aligning the image directly, rather than splitting the process into separate optimization and generation stages, improves identity preservationalbeit with slight trade-off in naturalness or prompt accuracy. Next, in Table 2, we further evaluate the adaptability of different models for subject-driven generation using single image. To our knowledge, DreamBooth is the only baseline that can be easily adapted across models, as others are tailored specifically for Stable Diffusion 2.1. Our results show that our method outperforms DreamBooth in identity preservation for FLUX and Sana. Although DreamBooth achieves better identity preservation with SDXL-Turbo, this is mainly due to overfitting, as indicated by the diversity metrics (0.05 vs. 0.11)."
        },
        {
            "title": "ClassDiffusion AttnDreamBooth DreamBooth",
            "content": "TI a . . . d o a . . . a g n . . . Figure 4. Qualitative results for subject-driven image generation using single subject image. The subject image is shown on the left, followed by the given prompt and the generated results from our method and various baselines. Input Image Subject Image Ours TIGIC SwapAnything DreamBooth TI Figure 5. Qualitative results for subject-driven image editing using single subject image. Each row shows an original input image to be edited, reference subject image, and results generated by our method SISO and four baselines. Image Editing. Table 3 compares our approach against subject-driven image editing baselines. TIGIC blends the subject into the image during diffusion, often resulting in background corruption (0.22 vs. 0.14). SwapAnything learns the subject concept, but when only single subject image is used, its identity preservation significantly declines (0.55 vs. 0.80 on DINO). Additionally, naturalness metrics are low, with an FID score of 185.7, suggesting that fewer input subject images can substantially drop image quality. Ablation. In Table 4, we examine prompt simplification. We observe trade-off: Simplifying the prompt improves adherence, while direct optimization with the full prompt better preserves subject identity. Table 5 evaluates the impact of background preservation loss (Eq. 3) on editing. Adding this loss improves background consistency (LPIPS: 0.14 vs. 0.18) without compromising identity preservation. We also assess using DINO and IR in an ensemble, which enhances identity preser-"
        },
        {
            "title": "Sana",
            "content": "k a d . . . . . . t s g p r t u e i Figure 6. Subject-driven image generation using three backbone models (single reference image) vation with only slightly reduced background consistency (LPIPS: 0.12 vs. 0.14). User study. In addition to automated metrics, we conducted user study to measure identity preservation, background preservation, prompt adherence, and naturalness. We used Amazon MTurk for 100 images, with five raters per image. See full details in the appendix (Sec. B). Two user studies were conducted separately, one for editing and one for generation, comparing our method against the best available baseline of each task. The results of the user study are given in Table 6. For editing, TIGIC better preserves subject identity because it often acts almost as copy-paste of the subject into the given input image. This is reflected in SISO obtaining higher scores for both naturalness and background preservation, with win rates of 58% and 60%, respectively. In the generation task, we see slight improvement for the baseline in subject preservation (47% win rate). However, SISO produces significantly more natural images (65% win rate) and shows prompt adherence (69% win rate). 4.3. Qualitative Results We begin by showing the results of our generative model compared to popular baselines  (Fig. 4)  . We evaluate subject-driven image generation on three subjects: plush toy, glasses and dog. Only our method correctly places the plush in Paris, while others overfit to the input image. Textual Inversion (TI) avoids this but fails to capture identity. Similar issues arise with the glasses, where most methods retain background elements, except ours and TI, though TI lacks detail. Our method preserves subject identity while generating diverse backgrounds. In the final row, the baselines fail to depict the subject and follow the prompt. In Fig. 5, we compare baselines for image editing by learning subject concepts, inverting, and regenerating images. In the first row, our method accurately preserves the wolf plush, while baselines either blend unnaturally (TIGIC), leak background details (SwapAnything), or distort both subject and background (DreamBooth, TI). In the second row, our method correctly replaces the black cat, though with slight eye color mismatch, while baselines fail entirely. In the third row, all methods perform better, but ours best preserves the background. In Fig. 6, we present subject-driven image generation using single reference image with SDXL Turbo, FLUX Schnell, and Sana models. DreamBooth, the only baseline adaptable across models, shows several limitations when trained on single image: (i) low diversity, with generations closely resembling the subject (e.g., the dog generated by SDXL and FLUX, the cat by FLUX), (ii) artifacts and unnatural attributes (e.g., the cats generated by SDXL and FLUX), and (iii) poor identity preservation (e.g., the dog in Sana). We also assess the stability of our method using various seeds (see Figures 12 and 13 in the appendix). 5. Conclusion We present SISO, novel optimization technique that employs single subject image and enables subject-driven image generation and subject-driven image editing by leveraging pre-trained image similarity score models. We show that in all previous baselines, enabling such capability with single image in an existing diffusion model is far from being solved. While our method still has room for improvement in subject identity preservation, it opens up new research thread that may make the personalization of image generators as simple as possible with the use of only single image. 6. Acknowledgments This work was supported by Vatat datascience grant."
        },
        {
            "title": "References",
            "content": "[1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. neural space-time representation for text-to-image personalization, 2023. 2 [2] Mikołaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. 6 [3] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon Wetzstein. Diffusion self-distillation for zero-shot customized image generation. arXiv preprint arXiv:2411.18616, 2024. 3 [4] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W. Subject-driven text-to-image generaCohen. arXiv preprint tion via apprenticeship learning. arXiv:2304.00186, 2023. 2 [5] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 65936602, 2023. 3 [6] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation. ArXiv, abs/2406.07547, 2024. 3 [7] Zhi Deng, Yibo He, Yulun Zhang, Yunfu Zhang, Zhen Li, Sifei Liu, Zhangyang Wang, Xiaolong Wang, and Yulun Wang. Fireflow: Fast inversion of rectified flow for image semantic editing. arXiv preprint arXiv:2412.07517, 2024. [8] Prafulla Dhariwal and Alexander Nichol. Diffusion Advances models beat gans on image synthesis. in Neural Information Processing Systems, 34:8780 8794, 2021. 2 [9] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via positive-negative prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 2 [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. 1, 2 [11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023. 2 [12] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel CohenOr. Lcm-lookahead: Encoder-based text-to-image arXiv preprint arXiv:2401.12345, personalization. 2024. [13] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real arXiv image inversion through iterative noising. preprint arXiv:2403.14602, 2024. 5 [14] Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, and Xin Eric Wang. Photoswap: Personalized subject swapping in images, 2023. 3 [15] Jing Gu, Nanxuan Zhao, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Yilin Wang, and Xin Eric Wang. Swapanything: Enabling arbitrary object swapping in personalized image editing. ECCV, 2024. 3, 6 [16] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. arXiv preprint arXiv:2404.16022, 2024. 2 [17] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73237334, 2023. 2 [18] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention.(2023). 2023. 3 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 4 [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 4 [22] Jiannan Huang, Jun Hao Liew, Hanshu Yan, Yuyang Yin, Yao Zhao, and Yunchao Wei. Classdiffusion: More aligned personalization tuning with explicit arXiv preprint arXiv:2405.17532, class guidance. 2024. 5 [23] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. 3 [24] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation In Proceedings of the metric for image generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93079315, 2024. 6 [25] Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Visual style promptarXiv preprint Lee, and Youngjung Uh. ing with swapping self-attention. arXiv:2402.12974, 2024. [26] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. 2 [27] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 4 [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, In Proceedings of the et al. IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 5, 6 Segment anything. [29] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. arXiv preprint arXiv:2310.01596, 2023. 5 [30] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large lanarXiv preprint arXiv:2301.12597, guage models. 2023. 2 [32] Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu, Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, and Feng Zheng. Tuning-free image customization with image and text guidance. In European Conference on Computer Vision, 2024. 3, 6 [33] Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. Dreamedit: Subject-driven image editing, 2023. 3 [34] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. arXiv preprint arXiv:2312.04461, 2023. 2 [35] Haonan Lin, Yan Chen, Jiahao Wang, Wenbin An, Mengmeng Wang, Feng Tian, Yong Liu, Guang Dai, Jingdong Wang, and Qianying Wang. Schedule your edit: simple yet effective diffusion noise schedule for image editing. arXiv preprint arXiv:2410.18756, 2024. 3 [36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2025. 5, [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin Dinov2: Learning robust viEl-Nouby, et al. arXiv preprint sual features without supervision. arXiv:2304.07193, 2023. 2, 4 [38] Yulin Pan, Chaojie Mao, Zeyinzi Jiang, Zhen Han, and Jingfeng Zhang. Locate, assign, refine: Taming customized image inpainting with text-subject guidance. arXiv preprint arXiv:2403.19534, 2024. 3 [39] Lianyu Pang, Jian Yin, Baoquan Zhao, Feize Wu, Fu Lee Wang, Qing Li, and Xudong Mao. Attndreambooth: Towards text-aligned personalized text-toimage generation. Advances in Neural Information Processing Systems, 37:3986939900, 2025. 5 [40] Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Semantic-aware attention valNested attention: arXiv preprint ues for concept personalization. arXiv:2501.01407, 2025. 2 [41] Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. Portraitbooth: versatile portrait model for fast identity-preserved personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2708027090, 2024. 2 [42] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2, 4, 5 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 87488763. PMLR, 2021. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF Conference els. on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 5 [45] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 3 [46] Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control. arXiv preprint arXiv:2405.17401, 2024. 3 [47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 1, 2, 5 [48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023. 2 [49] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87 103. Springer, 2025. 5 [50] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422438. Springer, 2025. [51] Shihao Shao and Qinghua Cui. 1st place solution in google universal images embedding, 2022. 2, 4, 6 [52] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023. 2 [53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. 3, 4 [54] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Generative object compositing. arXiv preprint arXiv:2212.00932, 2022. 3 [55] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. In Proceedings of Going deeper with convolutions. the IEEE conference on computer vision and pattern recognition, pages 19, 2015. 14 [56] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. [57] Yotam Tewel, Omer Sadik, Amit H. Bermano, and Key-locked rank one editing arXiv preprint Daniel Cohen-Or. for text-to-image personalization. arXiv:2305.01644, 2023. 2 [58] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https: / / github . com / huggingface / diffusers, 2022. 13 [59] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. 3 [60] Kuan-Chieh Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, and Kfir Aberman. Moa: Mixtureof-attention for subject-context disentanglement in arXiv preprint personalized image generation. arXiv:2404.11565, 2024. 2 [61] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2 [62] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoders. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1628416294, 2023. 2 [63] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuningfree multi-subject image generation with localized atInternational Journal of Computer Vision, tention. 2024. [64] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient highresolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 2, 3 [65] Shaoan Xie, Yang Zhao, Zhisheng Xiao, Kelvin CK Chan, Yandong Li, Yanwu Xu, Kun Zhang, and Tingbo Hou. Dreaminpainter: Text-guided subjectdriven image inpainting with diffusion models. arXiv preprint arXiv:2312.03771, 2023. 3 [66] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editIn Proceedings of the ing with diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023. 3 [67] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter arXiv preprint for text-to-image diffusion models. arXiv:2308.06721, 2023. 2 [68] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, and Huicheng Zheng. Inserting anybody in diffusion models via celeb basis. arXiv preprint arXiv:2306.00926, 2023. 2 [69] Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, and Ying Shan. Customnet: Zero-shot object customization with variableviewpoints in text-to-image diffusion models. arXiv preprint arXiv:2310.19784, 2023. 3 [70] Bo Zhang, Yuxuan Duan, Jun Lan, Yan Hong, Huijia Zhu, Weiqiang Wang, and Li Niu. Controlcom: Controllable image composition using diffusion model. arXiv preprint arXiv:2308.10040, 2023. [71] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 In this supplementary material, we present additional experiment results. The supplementary comprises the following subsections: 1. Sec. A, details the inversion method we used for image editing. 2. Sec. B, details about the user study. 3. Sec. C, details about early-stopping method used in our experiments. 4. Sec. D, details about the implementation of the baselines. 5. Sec. E, details about adaptation of SISO to various bacbone models. 6. Sec. F, details about the attempt to use SISO for subjectdriven face swapping. A. Diffusion Inversion We employ ReNoise for diffusion inversion in our image editing solution. ReNoise hyperparameters include strength, calibrating noise addition, balancing reconstruction, and editability. High values harm reconstruction while improving the ability to edit, and low values hinder object changes but improve reconstruction. We tuned the default setting from 1 to 0.75 in all experiments. Although this setting slightly reduces editing potential, subject-driven editing demands changes to the subject, not the background. Thus, this value empirically proved optimal for both reconstruction and subject editing without altering the background. B. User Study According to the task, workers in Amazon MTurk were presented with subject image, condition (a prompt or an input image), and two generated images - one from SISO and the other from the baseline. The study was conducted on 100 images from the benchmark, with five workers rated each image. The method used for the study was Twoalternative forced choice, where raters must choose the preferred output between two options. In our case, the workers were presented three questions per image. Each question requested the worker to choose between two generated images (the order between the generated images was randomly picked). For subject-driven image generation, the questions tested the following criteria: (i) object similarity (what we refer in the paper as identity preservation), (ii) prompt alignment (what we refer as prompt adherence) and (iii) naturalness. See Fig. 7 for illustration of the user study interface. C. Early Stopping Figure 7. Illustration of the user study interface for Subject-driven image generation task. all iterations and stop the optimization process when satisfactory result is obtained. To achieve fully automated process, we used simple early-stopping strategy, where the process ends if the loss has not improved by percent on the last iterations. Specifically, we set = 3 and = 7 in all of our experiments, both for generation and editing. D. Baselines Here, we describe how we implemented the baselines used in the paper. Subject-driven image generation. We compared our (i) DreamBooth, which method against three baselines: fine-tunes the diffusion model parameters according to set of reference images. We used the code given in Diffusers [58] library for all different base models (SDXL, FLUX, and Sana). (ii) AttnDreamBooth, which improves on DreamBooth with three-stage process, optimizing textual embedding, cross-attention layers, and the U-Net. (iii) ClassDiffusion, which utilizes semantic preservation loss. For both AttnDreamBooth and ClassDiffusion we used the official implementation published by the authors, using their default hyper-parameters. SISO generates well-formed image at each iteration, rather than noisy latent. This enables using the method in an interactive manner. One option is to display images from Subject-driven image editing. We compared our method (i) SwapAnything, which employs with two baselines: (ii) masked latent blending and appearance adaptation. Subject Image Output 1 Output 2 Figure 8. Optimizing on FLUX schnell using four denoising steps results in low quality images. this direction has potential, it did not show satisfactory results (see Fig. 9). Subject Image Input Image Output Figure 9. Results for subject-driven face swapping. TIGIC, training-free technique that uses an attentionblending strategy during the denoising process. TIGIC was initially designed for subject insertion, where the user wants to insert the subject to an empty area in the input image. To adapt to the subject replacement task, we used state-of-the-art inpainting model (LaMa2) to remove the original object and then applied TIGIC. For both methods, we used the official implementation published by the authors, using their default hyper-parameters. E. Adaptation to Various Backbone Models key advantage of SISO is its ability to be used with different backbone models with limited adaptation. In this section, we will describe the main differences in implementation between the different backbones we used (SDXLTurbo, FLUX schnell, Sana). First, SDXL-Turbo and FLUX schnell are distilled versions, meaning that they generate images using small number of steps (1-4). Sana, on the other hand, does not have distilled version and requires 20 steps to generate high-quality image. We found that when using distilled versions, backpropogating through the final denoising step is sufficient. However, when using non-distilled version, like Sana, it may be beneficial to backpropagate through more than one denoising step. Specifically, we set the number of steps to backpropogate through to 3. Also, even when using distilled version, the number of denoising steps used in each iteration may be important, and different models behave differently in this context. We will denote this number as t. SDXL-Turbo is less noisy to different values of t, but FLUX schnell showed significant difference when using various values of t. More specifically, setting >= 2 resulted in low-quality generated images, even when trying to backpropogate through more denoising steps (see Fig. 8). However, FLUX schnell generates blurred images when used with one denoising step. naive approach to overcome the blurriness is to use model trained for up-scaling resolution. But this requires loading another model and may complicate the process. We solved the issue using the training simplification (Sec. 3.3 in the paper). Although the weights were optimized using = 1, they can be used in inference with different values of t, thus producing high-quality images. F. Subject-driven Face Swapping natural use-case for SISO is subject-driven face swapping. We tried to adapt our method to this task by using different feature extractor more suitable for face recognition. Specifically, we employed InceptionResnet [55], using the implementation from pytorch-facenet library3). While 2https://github.com/advimman/lama 3https://github.com/timesler/facenet-pytorch"
        },
        {
            "title": "Subject Image",
            "content": "... on wooden counter ... on finger ... in box .. on glass table ... in the beach ... as pirate ... in gondolla ... as dj ... in the bedroom ... in the kitchen ... in the living room ... in store ... on an armchair ... on shelf ... on couch ... on fence ... in Coachella ... in Fuji mountain ... in the beach ... in Paris ... on desk ... in the kitchen ... on nightstand ... on book Figure 10. More Qualitative results on Subject Driven Image Generation"
        },
        {
            "title": "Output",
            "content": "Figure 11. More qualitative results on Subject Driven Image Editing. Subject Image e t . . . p s . . . a a n . . . b . . . 10 20 30 35 50 100 120 seed value Figure 12. We show the stability of our method across eight seeds for Subject Driven Image Generation. Subject Image Input Image 10 30 35 42 50 100 seed value Figure 13. We show the stability of our method across eight seeds for Subject Driven Image Editing."
        }
    ],
    "affiliations": [
        "Bar-Ilan University",
        "NVIDIA"
    ]
}